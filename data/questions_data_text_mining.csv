Natural Language/Text Mining and Reddit/social news site,"<p>I think there is a wealth of natural language data associated with sites like reddit or digg or news.google.com.</p>

<p>I have done a little bit of research with text mining, but can't find how I could use those tools to parse something like reddit.</p>

<p>What kind of applications can you come up with?</p>
","nlp, information-retrieval, text-mining","<p>I have found in the past that the best way to mine data on sites like Reddit or Digg is to first use the developer API that they provide. Typically you have a focused interest in either a topic or trend, and the only way to get that data is through an established public interface. You can also parse feeds, and combine them both to uncover 90% of what you would want to know. If you want to do deep research on data not available through an API, then you should be prepared to spend a significant amount of time writing custom wrappers around a tool like cURL. If you have the budget you can also call them and ask if they offer paid research data on users.</p>
",3,3,1052,2008-10-23 00:32:49,https://stackoverflow.com/questions/228042/natural-language-text-mining-and-reddit-social-news-site
Best clustering algorithm? (simply explained),"<p>Imagine the following problem:</p>

<ul>
<li>You have a database containing about 20,000 texts in a table called ""articles""</li>
<li>You want to connect the related ones using a clustering algorithm in order to display related articles together</li>
<li>The algorithm should do flat clustering (not hierarchical)</li>
<li>The related articles should be inserted into the table ""related""</li>
<li>The clustering algorithm should decide whether two or more articles are related or not based on the texts</li>
<li>I want to code in PHP but examples with pseudo code or other programming languages are ok, too</li>
</ul>

<p>I've coded a first draft with a function check() which gives ""true"" if the two input articles are related and ""false"" if not. The rest of the code (selecting the articles from the database, selecting articles to compare with, inserting the related ones) is complete, too. Maybe you can improve the rest, too. But the main point which is important to me is the function check(). So it would be great if you could post some improvements or completely different approaches.</p>

<p><strong>APPROACH 1</strong></p>

<pre><code>&lt;?php
$zeit = time();
function check($str1, $str2){
    $minprozent = 60;
    similar_text($str1, $str2, $prozent);
    $prozent = sprintf(""%01.2f"", $prozent);
    if ($prozent &gt; $minprozent) {
        return TRUE;
    }
    else {
        return FALSE;
    }
}
$sql1 = ""SELECT id, text FROM articles ORDER BY RAND() LIMIT 0, 20"";
$sql2 = mysql_query($sql1);
while ($sql3 = mysql_fetch_assoc($sql2)) {
    $rel1 = ""SELECT id, text, MATCH (text) AGAINST ('"".$sql3['text'].""') AS score FROM articles WHERE MATCH (text) AGAINST ('"".$sql3['text'].""') AND id NOT LIKE "".$sql3['id']."" LIMIT 0, 20"";
    $rel2 = mysql_query($rel1);
    $rel2a = mysql_num_rows($rel2);
    if ($rel2a &gt; 0) {
        while ($rel3 = mysql_fetch_assoc($rel2)) {
            if (check($sql3['text'], $rel3['text']) == TRUE) {
                $id_a = $sql3['id'];
                $id_b = $rel3['id'];
                $rein1 = ""INSERT INTO related (article1, article2) VALUES ('"".$id_a.""', '"".$id_b.""')"";
                $rein2 = mysql_query($rein1);
                $rein3 = ""INSERT INTO related (article1, article2) VALUES ('"".$id_b.""', '"".$id_a.""')"";
                $rein4 = mysql_query($rein3);
            }
        }
    }
}
?&gt;
</code></pre>

<p><strong>APPROACH 2 [only check()]</strong></p>

<pre><code>&lt;?php
function square($number) {
    $square = pow($number, 2);
    return $square;
}
function check($text1, $text2) {
    $words_sub = text_splitter($text2); // splits the text into single words
    $words = text_splitter($text1); // splits the text into single words
    // document 1 start
    $document1 = array();
    foreach ($words as $word) {
        if (in_array($word, $words)) {
            if (isset($document1[$word])) { $document1[$word]++; } else { $document1[$word] = 1; }
        }
    }
    $rating1 = 0;
    foreach ($document1 as $temp) {
        $rating1 = $rating1+square($temp);
    }
    $rating1 = sqrt($rating1);
    // document 1 end
    // document 2 start
    $document2 = array();
    foreach ($words_sub as $word_sub) {
        if (in_array($word_sub, $words)) {
            if (isset($document2[$word_sub])) { $document2[$word_sub]++; } else { $document2[$word_sub] = 1; }
        }
    }
    $rating2 = 0;
    foreach ($document2 as $temp) {
        $rating2 = $rating2+square($temp);
    }
    $rating2 = sqrt($rating2);
    // document 2 end
    $skalarprodukt = 0;
    for ($m=0; $m&lt;count($words)-1; $m++) {
        $skalarprodukt = $skalarprodukt+(array_shift($document1)*array_shift($document2));
    }
    if (($rating1*$rating2) == 0) { continue; }
    $kosinusmass = $skalarprodukt/($rating1*$rating2);
    if ($kosinusmass &lt; 0.7) {
        return FALSE;
    }
    else {
        return TRUE;
    }
}
?&gt;
</code></pre>

<p>I would also like to say that I know that there are lots of algorithms for clustering but on every site there is only the mathematical description which is a bit difficult to understand for me. So coding examples in (pseudo) code would be great.</p>

<p>I hope you can help me. Thanks in advance!</p>
","algorithm, text, cluster-analysis, data-mining, text-mining","<p>The most standard way I know of to do this on text data like you have, is to use the 'bag of words' technique. </p>

<p>First, create a 'histogram' of words for each article. Lets say between all your articles, you only have 500 unique words between them. Then this histogram is going to be a vector(Array, List, Whatever) of size 500, where the data is the number of times each word appears in the article. So if the first spot in the vector represented the word 'asked', and that word appeared 5 times in the article, vector[0] would be 5:</p>

<pre><code>for word in article.text
    article.histogram[indexLookup[word]]++
</code></pre>

<p>Now, to compare any two articles, it is pretty straightforward. We simply multiply the two vectors:</p>

<pre><code>def check(articleA, articleB)
    rtn = 0
    for a,b in zip(articleA.histogram, articleB.histogram)
        rtn += a*b
    return rtn &gt; threshold
</code></pre>

<p>(Sorry for using python instead of PHP, my PHP is rusty and the use of zip makes that bit easier)</p>

<p>This is the basic idea. Notice the threshold value is semi-arbitrary; you'll probably want to find a good way to normalize the dot product of your histograms (this will almost have to factor in the article length somewhere) and decide what you consider 'related'. </p>

<p>Also, you should not just put every word into your histogram. You'll, in general, want to include the ones that are used semi-frequently: Not in every article nor in only one article. This saves you a bit of overhead on your histogram, and increases the value of your relations. </p>

<p>By the way, this technique is described in more detail <a href=""http://en.wikipedia.org/wiki/Vector_space_model"" rel=""noreferrer"">here</a></p>
",15,19,10605,2009-05-12 14:38:50,https://stackoverflow.com/questions/853139/best-clustering-algorithm-simply-explained
Which NLP toolkit to use in JAVA?,"<p>i'm working on a project that consists of a website that connects to the NCBI(National Center for Biotechnology Information) and searches for articles there.  Thing is that I have to do some text mining on all the results. 
I'm using the JAVA language for textmining and AJAX with ICEFACES for the development of the website.
 What do I have :
A list of articles returned from a search.
Each article has an ID and an abstract.
The idea is to get keywords from each abstract text.
And then compare all the keywords from all abstracts and find the ones that are the most repeated. So then show in the website the related words for the search.
Any ideas ? 
I searched a lot in the web, and I know there is Named Entity Recognition,Part Of Speech tagging, there is teh GENIA thesaurus for NER on genes and proteins, I already tried stemming ... Stop words lists, etc...
I just need to know the best aproahc to resolve this problem.
Thanks a lot.</p>
","java, nlp, text-mining","<p>i would recommend you use a combination of POS tagging and then string tokenizing to extract all the nouns out of each abstract.. then use some sort of dictionary/hash to count the frequency of each of these nouns and then outputting the N most prolific nouns.. combining that with some other intelligent filtering mechanisms should do reasonably well in giving you the important keywords from the abstract<br>
for POS tagging check out the POS tagger at <a href=""http://nlp.stanford.edu/software/index.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/index.shtml</a></p>

<p>However, if you are expecting a lot of multi-word terms in your corpus.. instead of extracting just nouns, you could take the most prolific <a href=""http://en.wikipedia.org/wiki/N-gram"" rel=""noreferrer"">n-grams</a> for n=2 to 4</p>
",5,7,9956,2009-05-22 00:09:24,https://stackoverflow.com/questions/895893/which-nlp-toolkit-to-use-in-java
Background reading for parsing sloppy / quirky / &quot;almost structured&quot; data?,"<p>I'm maintaining a program that needs to parse out data that is present in an ""almost structured"" form in text.  i.e. various programs that produce it use slightly different formats, it may have been printed out and OCR'd back in (yeah, I know) with errors, etc. so I need to use heuristics that guess how it was produced and apply different quirks modes, etc.  It's frustrating, because I'm somewhat familiar with the theory and practice of parsing if things are well behaved, and there are nice parsing frameworks etc. out there, but the unreliability of the data has led me to write some very sloppy ad-hoc code.  It's OK at the moment but I'm worried that as I expand it to process more variations and more complex data, things will get out of hand.  So my question is:</p>

<p>Since there are a fair number of existing commercial products that do related things (""quirks modes"" in web browsers, error interpretation in compilers, even natural language processing and data mining, etc.) I'm sure some smart people have put thought into this, and tried to develop a theory, so what are the best sources for background reading on parsing unprincipled data in as principled a manner as possible?</p>

<p>I realize this is somewhat open-ended, but my problem is that I think I need more background to even know what the right questions to ask are.</p>
","parsing, text-mining","<p>Given the choice between what you've proposed and fighting a hungry crocodile while covered in raw-beef-flavored marmalade and both hands tied behind my back, I'd choose the ...</p>

<p>Well, OK on a more serious note, if you have data that doesn't abide by the any ""sane"" structure, you have to study the data and find frequencies of quirks in it and correlate the data for the given context (i.e. how it was generated)</p>

<p>Print to OCR to get the data in is almost always going to lead to heart break.  The company I work for employs a veritable army of people who manually read such documents and hand ""code"" (i.e. enter by hand) the data for known problematic OCR scenarios, or documents our customers detect the original OCR failed on.  </p>

<p>As for leveraging ""Parsing Frameworks"" these tend to expect data that will always follow the grammar rules you've laid out. The data you've described has no such guarantees.  If you go that route be prepared for unexpected - though not always obvious - failures.</p>

<p>By all means if there is any way possible to get the original data files, do so. Or if you can demand that those providing the data make their data come in a single well defined format, even better. (It might not be ""YOUR"" format, but at least it's a regular and predictable format you can convert from)</p>
",1,3,142,2009-09-02 17:25:43,https://stackoverflow.com/questions/1369073/background-reading-for-parsing-sloppy-quirky-almost-structured-data
Looking for an information retrival / text mining application or library,"<p>We extract various information from e-mails - flights, car rentals, hotels and more. the method is to extract the body of the mail, usually in HTML form but sometime it's text or we use the information in a PDF/Word/RTF attachment. We then apply regular expressions (sometimes in several steps) in order to get information, which is provided in a tabular form (you can think of a flight table, hotel table, etc.). Notice, even though we parse HTML, this is not web scraping.</p>

<p>Currently we are using QL2's WebQL engine, but we are looking to replace it from business reasons. Can you recommend on another engine? It must run on Linux and be accessible from Java (a Java API would be the the best, but Web services are good solution as well). It also must support regular expressions for text extraction and not just to be based on the HTML structure.</p>
","parsing, information-retrieval, html-content-extraction, text-mining, information-extraction","<p>Just wanted to update - our final decision was to implement the parsing in <a href=""http://groovy.codehaus.org/"" rel=""nofollow noreferrer"">groovy</a>, and to add some required functionality (html to text, pdf to text, clean whitespace, etc.) either by implementing it in Java ot by relying on 3rd party libraries. </p>
",0,1,1546,2009-09-23 11:05:40,https://stackoverflow.com/questions/1465331/looking-for-an-information-retrival-text-mining-application-or-library
"Besides NLTK, what is the best information retrieval library for Python?","<p>For use to analyze documents on the Internet!</p>
","python, information-retrieval, text-mining","<p>Alternatively, <a href=""http://www.r-project.org"" rel=""nofollow noreferrer"">R</a> has many tools available for text mining, and it's easy to <a href=""http://rpy.sourceforge.net/rpy2.html"" rel=""nofollow noreferrer"">integrate with Python using RPy2</a>.</p>

<p>Have a look at the <a href=""http://cran.r-project.org/web/views/NaturalLanguageProcessing.html"" rel=""nofollow noreferrer"">Natural Language Processing view on CRAN</a>.  In particular, look at the <code>tm</code> package.  Here are some relevant links:</p>

<ul>
<li>Paper about the package in the Journal of Statistical Computing: <a href=""http://www.jstatsoft.org/v25/i05/paper"" rel=""nofollow noreferrer""><a href=""http://www.jstatsoft.org/v25/i05/paper"" rel=""nofollow noreferrer"">http://www.jstatsoft.org/v25/i05/paper</a></a>.  The paper includes a nice example of an analysis of the R-devel
mailing list (<a href=""https://stat.ethz.ch/pipermail/r-devel/"" rel=""nofollow noreferrer"">https://stat.ethz.ch/pipermail/r-devel/</a>) newsgroup postings from 2006.</li>
<li>Package homepage: <a href=""http://cran.r-project.org/web/packages/tm/index.html"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/packages/tm/index.html</a></li>
<li>Look at the introductory vignette: <a href=""http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf</a></li>
</ul>

<p>In addition, R provides many tools for parsing HTML or XML.  Have <a href=""https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package"">a look at this question for an example using the <code>RCurl</code> and <code>XML</code> packages</a>.</p>
",5,0,3516,2009-10-28 03:13:53,https://stackoverflow.com/questions/1635014/besides-nltk-what-is-the-best-information-retrieval-library-for-python
text-mine PDF files with Python?,"<p>Is there a package/library for python that would allow me to open a PDF, and search the text for certain words?</p>
","python, pdf, text-mining","<p>Using <a href=""https://github.com/mstamy2/PyPDF2/"" rel=""nofollow noreferrer"">PyPdf2</a> you can use <a href=""http://code.activestate.com/recipes/511465/"" rel=""nofollow noreferrer"">extractText()</a> method to extract pdf text and work on it.</p>

<p>Update: Changed text to refer to PyPdf2, thanks to @Aditya Kumar for heads up.</p>
",12,5,8058,2009-11-04 07:30:02,https://stackoverflow.com/questions/1672202/text-mine-pdf-files-with-python
What is &quot;entropy and information gain&quot;?,"<p>I am reading this book (<a href=""http://www.nltk.org/book"" rel=""noreferrer"">NLTK</a>) and it is confusing.  <strong>Entropy</strong> is <a href=""http://www.nltk.org/book/ch06.html#entropy-and-information-gain"" rel=""noreferrer"">defined as</a>:</p>

<blockquote>
  <p>Entropy is the sum of the probability of each label
  times the log probability of that same label</p>
</blockquote>

<p>How can I apply <em>entropy</em> and <em>maximum entropy</em> in terms of text mining?  Can someone give me a easy, simple example (visual)?</p>
","math, text, computer-science, nltk, text-mining","<p>I assume entropy was mentioned in the context of building <a href=""https://en.wikipedia.org/wiki/Decision_tree_learning"" rel=""noreferrer""><strong>decision trees</strong></a>.</p>

<p>To illustrate, imagine the task of <a href=""https://en.wikipedia.org/wiki/Supervised_learning"" rel=""noreferrer"">learning</a> to <a href=""https://en.wikipedia.org/wiki/Statistical_classification"" rel=""noreferrer"">classify</a> first-names into male/female groups. That is given a list of names each labeled with either <code>m</code> or <code>f</code>, we want to learn a <a href=""https://en.wikipedia.org/wiki/Predictive_modelling"" rel=""noreferrer"">model</a> that fits the data and can be used to predict the gender of a new unseen first-name.</p>

<pre><code>name       gender
-----------------        Now we want to predict 
Ashley        f              the gender of ""Amro"" (my name)
Brian         m
Caroline      f
David         m
</code></pre>

<p>First step is <a href=""https://en.wikipedia.org/wiki/Feature_selection"" rel=""noreferrer"">deciding</a> what <a href=""https://en.wikipedia.org/wiki/Feature_%28machine_learning%29"" rel=""noreferrer""><strong>features</strong></a> of the data are relevant to the target class we want to predict. Some example features include: first/last letter, length, number of vowels, does it end with a vowel, etc.. So after feature extraction, our data looks like:</p>

<pre><code># name    ends-vowel  num-vowels   length   gender
# ------------------------------------------------
Ashley        1         3           6        f
Brian         0         2           5        m
Caroline      1         4           8        f
David         0         2           5        m
</code></pre>

<p>The goal is to build a <a href=""https://en.wikipedia.org/wiki/Decision_tree"" rel=""noreferrer"">decision tree</a>. An example of a <a href=""https://en.wikipedia.org/wiki/Tree_%28data_structure%29"" rel=""noreferrer"">tree</a> would be:</p>

<pre><code>length&lt;7
|   num-vowels&lt;3: male
|   num-vowels&gt;=3
|   |   ends-vowel=1: female
|   |   ends-vowel=0: male
length&gt;=7
|   length=5: male
</code></pre>

<p>basically each node represent a test performed on a single attribute, and we go left or right depending on the result of the test. We keep traversing the tree until we reach a leaf node which contains the class prediction (<code>m</code> or <code>f</code>)</p>

<p>So if we run the name <em>Amro</em> down this tree, we start by testing ""<em>is the length&lt;7?</em>"" and the answer is <em>yes</em>, so we go down that branch. Following the branch, the next test ""<em>is the number of vowels&lt;3?</em>"" again evaluates to <em>true</em>. This leads to a leaf node labeled <code>m</code>, and thus the prediction is <em>male</em> (which I happen to be, so the tree predicted the outcome <a href=""https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers"" rel=""noreferrer"">correctly</a>).</p>

<p>The decision tree is <a href=""https://en.wikipedia.org/wiki/ID3_algorithm"" rel=""noreferrer"">built in a top-down fashion</a>, but the question is how do you choose which attribute to split at each node? The answer is find the feature that best splits the target class into the purest possible children nodes (ie: nodes that don't contain a mix of both male and female, rather pure nodes with only one class).</p>

<p>This measure of <em>purity</em> is called the <a href=""https://en.wikipedia.org/wiki/Information_theory"" rel=""noreferrer""><strong>information</strong></a>. It represents the <a href=""https://en.wikipedia.org/wiki/Expected_value"" rel=""noreferrer"">expected</a> amount of <a href=""https://en.wikipedia.org/wiki/Self-information"" rel=""noreferrer"">information</a> that would be needed to specify whether a new instance (first-name) should be classified male or female, given the example that reached the node. We calculate it
based on the number of male and female classes at the node.</p>

<p><a href=""https://en.wikipedia.org/wiki/Information_entropy"" rel=""noreferrer""><strong>Entropy</strong></a> on the other hand is a measure of <em>impurity</em> (the opposite). It is defined for a <a href=""https://en.wikipedia.org/wiki/Binary_classification"" rel=""noreferrer"">binary class</a> with values <code>a</code>/<code>b</code> as:</p>

<pre><code>Entropy = - p(a)*log(p(a)) - p(b)*log(p(b))
</code></pre>

<p>This <a href=""https://en.wikipedia.org/wiki/Binary_entropy_function"" rel=""noreferrer"">binary entropy function</a> is depicted in the figure below (random variable can take one of two values). It reaches its maximum when the probability is <code>p=1/2</code>, meaning that <code>p(X=a)=0.5</code> or similarly<code>p(X=b)=0.5</code> having a 50%/50% chance of being either <code>a</code> or <code>b</code> (uncertainty is at a maximum). The entropy function is at zero minimum when probability is <code>p=1</code> or <code>p=0</code> with complete certainty (<code>p(X=a)=1</code> or <code>p(X=a)=0</code> respectively, latter implies <code>p(X=b)=1</code>).</p>

<p><img src=""https://i.sstatic.net/OUgcx.png"" alt=""https://en.wikipedia.org/wiki/File:Binary_entropy_plot.svg""></p>

<p>Of course the definition of entropy can be generalized for a discrete random variable X with N outcomes (not just two):</p>

<p><img src=""https://i.sstatic.net/vIFD7.png"" alt=""entropy""></p>

<p><em>(the <code>log</code> in the formula is usually taken as the <a href=""https://en.wikipedia.org/wiki/Binary_logarithm"" rel=""noreferrer"">logarithm to the base 2</a>)</em></p>

<hr>

<p>Back to our task of name classification, lets look at an example. Imagine at some point during the process of constructing the tree, we were considering the following split:</p>

<pre><code>     ends-vowel
      [9m,5f]          &lt;--- the [..,..] notation represents the class
    /          \            distribution of instances that reached a node
   =1          =0
 -------     -------
 [3m,4f]     [6m,1f]
</code></pre>

<p>As you can see, before the split we had 9 males and 5 females, i.e. <code>P(m)=9/14</code> and <code>P(f)=5/14</code>. According to the definition of entropy:</p>

<pre><code>Entropy_before = - (5/14)*log2(5/14) - (9/14)*log2(9/14) = 0.9403
</code></pre>

<p>Next we compare it with the entropy computed after considering the split by looking at two child branches. In the left branch of <code>ends-vowel=1</code>, we have:</p>

<pre><code>Entropy_left = - (3/7)*log2(3/7) - (4/7)*log2(4/7) = 0.9852
</code></pre>

<p>and the right branch of <code>ends-vowel=0</code>, we have:</p>

<pre><code>Entropy_right = - (6/7)*log2(6/7) - (1/7)*log2(1/7) = 0.5917
</code></pre>

<p>We combine the left/right entropies using the number of instances down each branch as <a href=""https://en.wikipedia.org/wiki/Weighted_arithmetic_mean"" rel=""noreferrer"">weight factor</a> (7 instances went left, and 7 instances went right), and get the final entropy after the split:</p>

<pre><code>Entropy_after = 7/14*Entropy_left + 7/14*Entropy_right = 0.7885
</code></pre>

<p>Now by comparing the entropy before and after the split, we obtain a measure of <a href=""https://en.wikipedia.org/wiki/Information_gain_in_decision_trees"" rel=""noreferrer""><strong>information gain</strong></a>, or how much information we gained by doing the split using that particular feature:</p>

<pre><code>Information_Gain = Entropy_before - Entropy_after = 0.1518
</code></pre>

<p><em>You can interpret the above calculation as following: by doing the split with the <code>end-vowels</code> feature, we were able to reduce uncertainty in the sub-tree prediction outcome by a small amount of 0.1518 (measured in <a href=""https://en.wikipedia.org/wiki/Bit"" rel=""noreferrer"">bits</a> as <a href=""https://en.wikipedia.org/wiki/Units_of_information"" rel=""noreferrer"">units of information</a>).</em></p>

<p>At each node of the tree, this calculation is performed for every feature, and the feature with the <em>largest information gain</em> is chosen for the split in a <a href=""https://en.wikipedia.org/wiki/Greedy_algorithm"" rel=""noreferrer"">greedy</a> manner (thus favoring features that produce <em>pure</em> splits with low uncertainty/entropy). This process is applied recursively from the root-node down, and stops when a leaf node contains instances all having the same class (no need to split it further).</p>

<p>Note that I skipped over some <a href=""https://en.wikipedia.org/wiki/C4.5_algorithm"" rel=""noreferrer"">details</a> which are beyond the scope of this post, including how to handle <a href=""https://en.wikipedia.org/wiki/Discretization_of_continuous_features"" rel=""noreferrer"">numeric features</a>, <a href=""https://en.wikipedia.org/wiki/Missing_data"" rel=""noreferrer"">missing values</a>, <a href=""https://en.wikipedia.org/wiki/Overfitting"" rel=""noreferrer"">overfitting</a> and <a href=""https://en.wikipedia.org/wiki/Pruning_%28decision_trees%29"" rel=""noreferrer"">pruning</a> trees, etc..</p>
",1064,355,219702,2009-12-07 11:54:32,https://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain
Find HEX patterns and number of occurrences,"<p>I'd like to find patterns and sort them by number of occurrences on an HEX file I have.</p>

<p>I am not looking for some specific pattern, just to make some statistics of the occurrences happening there and sort them.</p>

<pre><code>DB0DDAEEDAF7DAF5DB1FDB1DDB20DB1BDAFCDAFBDB1FDB18DB23DB06DB21DB15DB25DB1DDB2EDB36DB43DB59DB32DB28DB2ADB46DB6FDB32DB44DB40DB50DB87DBB0DBA1DBABDBA0DB9ADBA6DBACDBA0DB96DB95DBB7DBCFDBCBDBD6DB9CDBB5DB9DDB9FDBA3DB88DB89DB93DBA5DB9CDBC1DBC1DBC6DBC3DBC9DBB3DBB8DBB6DBC8DBA8DBB6DBA2DB98DBA9DBB9DBDBDBD5DBD9DBC3DB9BDBA2DB84DB83DB7DDB6BDB58DB4EDB42DB16DB0DDB01DB02DAFCDAE9DAE5DAD9DAE2DAB7DA9BDAA6DA9EDAAADAC9DACADAC4DA92DA90DA84DA89DA93DAA9DA8CDA7FDA62DA53DA6EDA
</code></pre>

<p>That's an excerpt of the HEX file, and as an example I'd like to get:</p>

<p>XX occurrences of BDBDBD</p>

<p>XX occurrences of B93D</p>

<p>Is there a way to mine the file to generate that output?</p>
","hex, data-mining, text-mining","<p>Sure. Use a <a href=""http://search.cpan.org/~ambs/Text-Ngram-0.11/lib/Text/Ngram.pm"" rel=""nofollow noreferrer"">sliding window</a> to create the counts (The link is for Perl, but it seems general enough to understand the algorithm). Your patterns are named <a href=""http://en.wikipedia.org/wiki/N-gram"" rel=""nofollow noreferrer"">N-grams</a>. You will have to limit the maximal pattern, though.</p>
",2,3,3108,2009-12-14 21:32:01,https://stackoverflow.com/questions/1903648/find-hex-patterns-and-number-of-occurrences
Perl within Python?,"<p>There is a Perl library I would like to access from within Python.
How can I use it?</p>

<p>FYI, the software is <a href=""http://webascorpus.sourceforge.net/PHITE.php?sitesig=FILES&amp;page=FILES_10_Software"" rel=""noreferrer"">NCleaner</a>. I would like to use it from within Python to transform an HTML string into text. (Yes, I know about aaronsw's Python html2text. NCleaner is better, because it removes boiler-plate.)</p>

<p>I don't want to run the Perl program as a script and call it repeatedly, because it has an expensive initial load time and I am calling it many times.</p>
","python, perl, text-mining","<p><a href=""http://search.cpan.org/~gaas/pyperl-1.0/perlmodule.pod"" rel=""noreferrer"">pyperl</a> provides perl embedding for python, but honestly it's not the way I'd go. I second Roboto's suggestion -- write a script that runs NCleaner (either processing from stdin to stdout, or working on temporary files, whichever one is more appropriate), and run it as a subprocess.</p>

<p>Or, since I see from the NCleaner page that it has a C implementation, use whatever facilities Python has for binding to C code and write a Python module that wraps the NCleaner C implementation. Then in the future the answer to invoking NCleaner from Python will just be ""here, use this module.""</p>

<p><em>Footnote: <a href=""http://search.cpan.org/perldoc?Inline::Python"" rel=""noreferrer"">Inline::Python</a> is better code than pyperl, and I would suggest using that instead, but it only supports having Python call back to Perl when Python is invoked from Perl in the first place -- the ability to embed Perl into Python is listed as a possible future feature, but it's been so since 2001, so don't hold your breath.</em></p>
",13,7,2211,2009-12-16 20:55:18,https://stackoverflow.com/questions/1917656/perl-within-python
N-gram function in vb.net -&gt; create grams for words instead of characters,"<p>I recently found out about n-grams and the cool possibility to compare frequency of phrases in a text body with it. Now I'm trying to make an vb.net app that simply gets an text body and returns a list of the most frequently used phrases (where n >= 2).</p>

<p>I found an C# example of how to generate a n-gram from a text body so I started out with converting the code to VB. The problem is that this code does create one gram per character instead of one per word. The delimiters I want to use for the words is: VbCrLf (new line), vbTab (tabs) and the following characters: !@#$%^&amp;*()_+-={}|\:\""'?¿/.,&lt;>’¡º×÷‘;«»[]</p>

<p>Does anyone have an idea how I can rewrite the following function for this purpose:</p>

<pre><code>   Friend Shared Function GenerateNGrams(ByVal text As String, ByVal gramLength As Integer) As String()
    If text Is Nothing OrElse text.Length = 0 Then
        Return Nothing
    End If

    Dim grams As New ArrayList()
    Dim length As Integer = text.Length
    If length &lt; gramLength Then
        Dim gram As String
        For i As Integer = 1 To length
            gram = text.Substring(0, (i) - (0))
            If grams.IndexOf(gram) = -1 Then
                grams.Add(gram)
            End If
        Next

        gram = text.Substring(length - 1, (length) - (length - 1))
        If grams.IndexOf(gram) = -1 Then
            grams.Add(gram)

        End If
    Else
        For i As Integer = 1 To gramLength - 1
            Dim gram As String = text.Substring(0, (i) - (0))
            If grams.IndexOf(gram) = -1 Then
                grams.Add(gram)

            End If
        Next

        For i As Integer = 0 To (length - gramLength)
            Dim gram As String = text.Substring(i, (i + gramLength) - (i))
            If grams.IndexOf(gram) = -1 Then
                grams.Add(gram)
            End If
        Next

        For i As Integer = (length - gramLength) + 1 To length - 1
            Dim gram As String = text.Substring(i, (length) - (i))
            If grams.IndexOf(gram) = -1 Then
                grams.Add(gram)
            End If
        Next
    End If
    Return Tokeniser.ArrayListToArray(grams)
End Function
</code></pre>
","vb.net, text-mining, n-gram","<p>An <em>n</em>-gram for words is just a list of length <em>n</em> that stores these words. A list of <em>n</em>-grams is then simply a list of list of words. If you want to store frequency then you need a dictionary that is indexed by these <em>n</em>-grams. For your special case of 2-grams, you can imagine something like this:</p>

<pre><code>Dim frequencies As New Dictionary(Of String(), Integer)(New ArrayComparer(Of String)())
Const separators as String = ""!@#$%^&amp;*()_+-={}|\:""""'?¿/.,&lt;&gt;’¡º×÷‘;«»[] "" &amp; _
                             ControlChars.CrLf &amp; ControlChars.Tab
Dim words = text.Split(separators.ToCharArray(), StringSplitOptions.RemoveEmptyEntries)

For i As Integer = 0 To words.Length - 2
    Dim ngram = New String() { words(i), words(i + 1) }
    Dim oldValue As Integer = 0
    frequencies.TryGetValue(ngram, oldValue)
    frequencies(ngram) = oldValue + 1
Next
</code></pre>

<p><code>frequencies</code> should now contain a dictionary with all two consecutive word pairs contained in the text, and the frequency with which they appear (as a consecutive pair).</p>

<p>This code requires the <code>ArrayComparer</code> class:</p>

<pre><code>Public Class ArrayComparer(Of T)
    Implements IEqualityComparer(Of T())

    Private ReadOnly comparer As IEqualityComparer(Of T)

    Public Sub New()
        Me.New(EqualityComparer(Of T).Default)
    End Sub

    Public Sub New(ByVal comparer As IEqualityComparer(Of T))
        Me.comparer = comparer
    End Sub

    Public Overloads Function Equals(ByVal a As T(), ByVal b As T()) As Boolean _
            Implements IEqualityComparer(Of T()).Equals
        System.Diagnostics.Debug.Assert(a.Length = b.Length)
        For i As Integer = 0 to a.Length - 1
            If Not comparer.Equals(a(i), b(i)) Then Return False
        Next

        Return True
    End Function

    Public Overloads Function GetHashCode(ByVal arr As T()) As Integer _
            Implements IEqualityComparer(Of T()).GetHashCode
        Dim hashCode As Integer = 17
        For Each obj As T In arr
            hashCode = ((hashCode &lt;&lt; 5) - 1) Xor comparer.GetHashCode(obj)
        Next

        Return hashCode
    End Function
End Class
</code></pre>

<p>Unfortunately, this code doesn’t compile on Mono because the VB compiler has problems finding the generic <code>EqualityComparer</code> class. I’m therefore unable to test whether the <code>GetHashCode</code> implementationw works as expected but it should be fine.</p>
",2,2,4034,2010-03-10 11:46:23,https://stackoverflow.com/questions/2416550/n-gram-function-in-vb-net-create-grams-for-words-instead-of-characters
Retrieve Information From Different Unstructured Text Files - Text Mining?,"<p>I need some help in solving this problem.</p>

<p>We have a large amount of documents of a given specified domain. These documents are from differente sources and therefore their structure can be very different too. On the other side I have a table with some specified fields where some figures has to be filled from the extract of the documents.</p>

<p>For example:</p>

<blockquote>
  <p>Company x had a business volume of
  $20mio in 2010. $1,000,000 was the exchange of
  company y this year.</p>
</blockquote>

<p>The result should something like this</p>

<pre><code>|| Company | Year | Volume  
||  X      | 2010 |  200,000  
||  Y      | 2010 | 1000,000  
</code></pre>

<p>Can you point me please to some links or topics, where I can find further informations how to solve such a problem.</p>

<p>I know that there is no out of the box solution for this, but where should i start to look for.</p>

<p>Thanks in advance.</p>
","c#, data-mining, text-mining","<p>Ok. There are entire computer science labs devoted to that sort of stuff!
Maybe start by looking a tool called <a href=""http://en.wikipedia.org/wiki/RapidMiner"" rel=""nofollow noreferrer"">RapidMiner</a></p>

<p>Also here are a couple of research paper titles I have as PDF's (which I don't have links for anymore sadly):</p>

<p><strong>1. Automated Understanding of Financial Statements
Using Neural Networks and Semantic Grammars</strong></p>

<p>James Markovitch
Dun &amp; Bradstreet, Search Technologies
April 1995
Email: jsmarkovitch@yahoo.com
Copyright  1995 James Markovitch</p>

<p><strong>2. An Integrated Approach for Automatic Semantic Structure Extraction in Document Images</strong></p>

<p>Margherita Berardi, Michele Lapi, and Donato Malerba
Dipartimento di Informatica – Università degli Studi di Bari
via Orabona 4 - 70126 Bari
{berardi,lapi,malerba}@di.uniba.it</p>

<p>I think the first one would be of greatest interest in terms of what you are after. Not quite sure how much value it will be though :)</p>
",2,2,1471,2010-03-17 10:34:35,https://stackoverflow.com/questions/2461477/retrieve-information-from-different-unstructured-text-files-text-mining
Text mining on large database (data mining),"<p>I have a large database of resumes (CV), and a certain table <strong>skills</strong> grouping all users skills.</p>

<p>inside that table there's a field <strong>skill_text</strong> that describes the skill in full text.</p>

<p>I'm looking for an algorithm/software/method to extract significant terms/phrases from that table in order to build a new table with standarized skills..</p>

<p>Here are some examples skills extracted from the DB :</p>

<ul>
<li>Sectoral and competitive analysis</li>
<li>Business Development (incl. in international settings)</li>
<li>Specific structure and road design software - Microstation, Macao, AutoCAD (basic knowledge)</li>
<li>Creative work (Photoshop, In-Design, Illustrator)</li>
<li>checking and reporting back on campaign progress</li>
<li>organising and attending events and exhibitions</li>
<li>Development : Aptana Studio, PHP, HTML, CSS, JavaScript, SQL, AJAX</li>
<li>Discipline: One to one marketing, E-marketing (SEO &amp; SEA, display, emailing, affiliate program) Mix marketing, Viral Marketing, Social network marketing.</li>
</ul>

<p>The output shoud be something like :</p>

<ul>
<li>Sectoral and competitive analysis</li>
<li>Business Development</li>
<li>Specific structure and road design software - </li>
<li>Macao</li>
<li>AutoCAD </li>
<li>Photoshop</li>
<li>In-Design</li>
<li>Illustrator</li>
<li>organising events </li>
<li>Development </li>
<li>Aptana Studio</li>
<li>PHP</li>
<li>HTML</li>
<li>CSS</li>
<li>JavaScript</li>
<li>SQL</li>
<li>AJAX</li>
<li>Mix marketing</li>
<li>Viral Marketing</li>
<li>Social network marketing</li>
<li>emailing</li>
<li>SEO</li>
<li>One to one marketing</li>
</ul>

<p>As you see only skills remains no other representation text.</p>

<p>I know this is possible using text mining technics but how to do it ?
the database is realy large.. it's a good thing because we can calculate text frequency and decide if it's a real skill or just meaningless text...
The big problem is .. how to determin that ""blablabla"" is a skill ? </p>

<p>Edit :
please don't tell me to use standard things like a text tokinzer, or regex .. because users input skills in a very arbitrary way !!</p>

<p>thanks</p>
","database, data-mining, text-mining","<p>If I was doing this programmatically I would:</p>

<p>Extract all punctuation delimited data (or perhaps just brackets and commas) into a new table (with no primary key, just skill) so <code>Creative work (Photoshop, In-Design, Illustrator)</code> becomes</p>

<pre><code> Skill            
 -------------
 Creative work    
 Photoshop        
 In-Design        
 Illustrator      
</code></pre>

<p>Then, after you've proceed all CVs, query for the most common skills (this is MySQL)</p>

<pre><code>SELECT skill, COUNT(1) cnt FROM newTable GROUP BY skill ORDER BY cnt DESC;
</code></pre>

<p>Which may look like this contrived example</p>

<pre><code> Skill            Cnt
 ---------------------
 Photoshop        3293
 Illustrator      2134
 Creative work     932
 In-Design         123
</code></pre>

<p>Then you decide, from the top X skills, which you want to capture, which must map to other skills (<code>Indesign</code> and <code>In-design</code> should map to the same skill, for example) and which to discard, then script the process using a data map. </p>

<p>Use the data map to write a new word frequency table (this time skill_id, skill, frequency) and the second time when parsing the data also write to a lookup table (cv_id,skill_id). Your data will then be in a state where each CV is mapped to a number of skills, and each skill to a number of CVs. You can query for the most popular skills, CVs matching certain criteria etc.</p>
",5,2,3061,2010-04-13 22:16:15,https://stackoverflow.com/questions/2633598/text-mining-on-large-database-data-mining
n-grams from text in PostgreSQL,"<p>I am looking to create n-grams from text column in PostgreSQL. I currently split(on white-space) data(sentences) in a text column to an array.</p>

<p><code>enter code here</code>select regexp_split_to_array(sentenceData,E'\s+') from tableName</p>

<p>Once I have this array, how do I go about:</p>

<ul>
<li>Creating a loop to find n-grams, and write each to a row in another table</li>
</ul>

<p>Using unnest I can obtain all the elements of all the arrays on separate rows, and maybe I can then think of a way to get n-grams from a single column, but I'd loose the sentence boundaries which I wise to preserve.</p>

<p>Sample SQL code for PostgreSQL to emulate the above scenario</p>

<pre><code>create table tableName(sentenceData  text);

INSERT INTO tableName(sentenceData) VALUES('This is a long sentence');

INSERT INTO tableName(sentenceData) VALUES('I am currently doing grammar, hitting this monster book btw!');

INSERT INTO tableName(sentenceData) VALUES('Just tonnes of grammar, problem is I bought it in TAIWAN, and so there aint any englihs, just chinese and japanese');

select regexp_split_to_array(sentenceData,E'\\s+')   from tableName;

select unnest(regexp_split_to_array(sentenceData,E'\\s+')) from tableName;
</code></pre>
","sql, postgresql, text-mining","<p>Check out <a href=""http://www.postgresql.org/docs/current/static/pgtrgm.html"" rel=""nofollow noreferrer"">pg_trgm</a>: ""The pg_trgm module provides functions and operators for determining the similarity of text based on trigram matching, as well as index operator classes that support fast searching for similar strings.""</p>
",4,4,5080,2010-06-15 12:59:55,https://stackoverflow.com/questions/3045336/n-grams-from-text-in-postgresql
Keeping Track of Word Proximity,"<p>I am working on a small project which involves a dictionary based text searching within a collection of documents. My dictionary has positive signal words (a.k.a good words) but in the document collection just finding a word does not guarantee a positive result as there may be negative words for example (not, not significant) that may be in the proximity of these positive words. I want to construct a matrix such that it contains the document number,positive word and its proximity to negative words. </p>

<p>Can anyone please suggest a way to do that. My project is at a very very early stage so I am giving a basic example of my text. </p>

<pre><code>No significant drug interactions have been reported in studies of candesartan cilexetil given with other drugs such as glyburide, nifedipine, digoxin, warfarin, hydrochlorothiazide.   
</code></pre>

<p>This is my example document in which <strong>candesartan cilexetil, glyburide, nifedipine, digoxin, warfarin, hydrochlorothiazide</strong> are my positive words and no significant is my negative word. I want to do a proximity (word based) mapping between my positive and nevative words. </p>

<p>Can anyone give some helpful pointers? </p>
","r, text-mining","<p>First of all I would suggest not to use R for this task. R is great for many things, but text manipulation is not one of those. Python could be a good alternative.</p>

<p>That said, if I were to implement this in R, I would probably do something like (very very rough):</p>

<pre><code># You will probably read these from an external file or a database
goodWords &lt;- c(""candesartan cilexetil"", ""glyburide"", ""nifedipine"", ""digoxin"", ""blabla"", ""warfarin"", ""hydrochlorothiazide"")
badWords &lt;- c(""no significant"", ""other drugs"")

mytext &lt;- ""no significant drug interactions have been reported in studies of candesartan cilexetil given with other drugs such as glyburide, nifedipine, digoxin, warfarin, hydrochlorothiazide.""
mytext &lt;- tolower(mytext) # Let's make life a little bit easier...

goodPos &lt;- NULL
badPos &lt;- NULL

# First we find the good words
for (w in goodWords)
    {
    pos &lt;- regexpr(w, mytext)
    if (pos != -1)
        {
        cat(paste(w, ""found at position"", pos, ""\n""))
        }
    else    
        {
        pos &lt;- NA
        cat(paste(w, ""not found\n""))
        }

    goodPos &lt;- c(goodPos, pos)
    }

# And then the bad words
for (w in badWords)
    {
    pos &lt;- regexpr(w, mytext)
    if (pos != -1)
        {
        cat(paste(w, ""found at position"", pos, ""\n""))
        }
    else    
        {
        pos &lt;- NA
        cat(paste(w, ""not found\n""))
        }

    badPos &lt;- c(badPos, pos)
    }

# Note that we use -badPos so that when can calculate the distance with rowSums
comb &lt;- expand.grid(goodPos, -badPos)
wordcomb &lt;- expand.grid(goodWords, badWords)
dst &lt;- cbind(wordcomb, abs(rowSums(comb)))

mn &lt;- which.min(dst[,3])
cat(paste(""The closest good-bad word pair is: "", dst[mn, 1],""-"", dst[mn, 2],""\n""))
</code></pre>
",5,3,934,2010-06-21 14:47:31,https://stackoverflow.com/questions/3085673/keeping-track-of-word-proximity
Decision Trees For Document Classification,"<p>I wanted to know that is it possible to use decision trees for document classification, and if yes then how should the data representation be?
I know the use of the R package <a href=""http://cran.r-project.org/web/packages/party/index.html"" rel=""nofollow noreferrer""><code>party</code></a> for Decision Trees.</p>
","r, nlp, classification, text-mining, document-classification","<p>One way is to have a huge matrix where each row is a document, and each column is a word.  And the values in the cells are the number of times that word showed in that document.</p>
<p>Then, if you are dealing with &quot;supervised learning&quot; case, you should have another column for the classifier, and from there on you can use a command like &quot;rpart&quot; (from the rpart package), to create your classification tree.  The command would be entering a formula to rpart, in a similar fashion as you would to a linear model (lm).</p>
<p>If you want, you could also try to first group your words to &quot;groups of words&quot;, and then have each column belonging to a different group of words, with a number indication how many words in the document belonged to that group.  For that I would have a look at the &quot;tm&quot; package.  (If you end up doing something with that, please consider maybe posting about it here, so we could learn from it)</p>
",2,1,5011,2010-06-24 23:57:18,https://stackoverflow.com/questions/3114734/decision-trees-for-document-classification
Large scale Machine Learning,"<p>I need to run various machine learning techniques on a big dataset (10-100 billions records)
The problems are mostly around text mining/information extraction and include various kernel techniques but are not restricted to them (we use some bayesian methods, bootstrapping, gradient boosting, regression trees -- many different problems and ways to solve them)</p>

<p>What would be the best implementation? I'm experienced in ML but do not have much experience how to do it for huge datasets
Is there any extendable and customizable Machine Learning libraries utilizing MapReduce infrastructure 
Strong preference to c++, but Java and python are ok
Amazon Azure or own datacenter (we can afford it)?</p>
","java, c++, machine-learning, mapreduce, text-mining","<p>Unless the classification state space you are attempting to learn is extremely large, I would expect that there is significant redundancy in a text-mining-focused dataset with 10-100 billion records or training samples.  As a rough guess, I would doubt that one would need much more than a 1-2% random sample subset to learn reliable classifiers that would hold up well under cross-validation testing.</p>
<p>A quick literature search came up with the following relevant papers.  The Tsang paper claims <strong>O(n)</strong> time complexity for <strong>n</strong> training samples, and there is software related to it available as the <a href=""http://www.cse.ust.hk/%7Eivor/cvm.html"" rel=""nofollow noreferrer"">LibCVM toolkit</a>.  The Wolfe paper describes a distributed EM approach based on MapReduce.</p>
<p>Lastly, there was a <a href=""https://web.archive.org/web/20170417131341/http://www.select.cs.cmu.edu:80/meetings/biglearn09/"" rel=""nofollow noreferrer"">Large-Scale Machine Learning workshop</a> at the NIPS 2009 conference that looks to have had lots of interesting and relevant presentations.</p>
<p><strong>References</strong></p>
<p>Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung (2005). &quot;<a href=""http://www.cse.ust.hk/%7Eivor/publication/tsang05a.pdf"" rel=""nofollow noreferrer"">Core Vector Machines: Fast SVM Training on Very Large Data Sets</a>&quot;, Journal of Machine Learning Research, vol 6, pp 363–392.</p>
<p>J Wolfe, A Haghighi, D Klein (2008). &quot;<a href=""http://www.cs.berkeley.edu/%7Earia42/pubs/icml08-distributedem.pdf"" rel=""nofollow noreferrer"">Fully Distributed EM for Very Large Datasets</a>&quot;, Proceedings of the 25th International Conference on Machine Learning, pp 1184-1191.</p>
<p>Olivier Camp, Joaquim B. L. Filipe, Slimane Hammoudi and Mario Piattini (2005). &quot;<a href=""https://doi.org/10.1007/1-4020-2673-0_21"" rel=""nofollow noreferrer"">Mining Very Large Datasets with Support Vector Machine Algorithms </a>&quot;, Enterprise Information Systems V, Springer Netherlands, pp 177-184.</p>
",22,28,3144,2010-07-08 23:58:02,https://stackoverflow.com/questions/3208927/large-scale-machine-learning
How does Shingleprinting work in practice?,"<p>I'm trying to use shingleprinting to measure document similarity. The process involves the following steps:</p>

<ol>
<li>Create a <a href=""http://en.wikipedia.org/wiki/W-shingling"" rel=""nofollow noreferrer"" title=""5-shingling"">5-shingling</a> of the two documents D1, D2</li>
<li>Hash each shingle with a 64-bit hash</li>
<li>Pick a random permutation of the numbers from 0 to 2^64-1 and apply to shingle hashes</li>
<li>For each document find the smallest of the resulting values</li>
<li>If they match count it as a positive example, if not count it as a negative example</li>
<li>Repeat 3. to 5. a few times</li>
<li>Use <code>positive_examples / total examples</code> as the similarity measure</li>
</ol>

<p>Step 3 involves generating a random permutation of a very long sequence. Using a Knuth-shuffle seems out of the question. Is there some shortcut for this? Note that in the end we need only a single element of the resulting permutation.</p>
","performance, random, permutation, information-retrieval, text-mining","<p>Warning: I'm not 100% positive about this, but I've read some of the papers and I believe this is how it works.  For instance, in ""A small approximately min-wise independent family of hash functions"" by Piotr Indyk, he writes ""In the implementation integrated with Altavista, the set H was chosen to be a pairwise independent family of hash functions.""</p>

<p>In step 3, you don't actually need a random permutation on [n] (the integers from 1 to n).  It turns out that a pairwise-independent hash function works in practice.  So what you do is pick a pairwise-independent hash function h.  And then apply h to each of the shingle hashes.  You can take the min of those values in step 4.</p>

<p>A standard pairwise-independent hash function is h(x) = ax + b (mod p), where a and b are chosen randomly and p is a prime.</p>

<p>References: <a href=""http://www.cs.princeton.edu/courses/archive/fall08/cos521/hash.pdf"" rel=""nofollow"">http://www.cs.princeton.edu/courses/archive/fall08/cos521/hash.pdf</a> and <a href=""http://people.csail.mit.edu/indyk/minwise99.ps"" rel=""nofollow"">http://people.csail.mit.edu/indyk/minwise99.ps</a></p>
",4,4,697,2010-07-09 08:57:21,https://stackoverflow.com/questions/3211185/how-does-shingleprinting-work-in-practice
Text classification/categorization algorithm,"<p>My objective is to [semi]automatically assign texts to different categories. There's a set of user defined categories and a set of texts for each category. The ideal algorithm should be able to learn from a human-defined classification and then classify new texts automatically. 
Can anybody suggest such an algorithm and perhaps .NET library that implements ше? </p>
","algorithm, text-mining, document-classification","<p>Doing this is not trivial. Obviously you can build a dictionary that maps certain keywords to categories. Just finding a keyword would suggest a certain category.</p>

<p>Yet, in natural language text, the keywords would usually not be in their stem form. You would need some morphology tools to find the stem form and use it on the dictionary.</p>

<p>But then somebody could write something like: ""This article is not about ..."". This would introduce the need for syntax and semantical analysis.</p>

<p>And then you would find that certain keywords can be used in several categories: ""band"" could be used in musics, Technics, or even handicraft work. You would therefore need an ontology and statistical or other methods to weigh the probability of the category to choose if not definite.</p>

<p>Some of the keywords might not even be easy to fit into an ontology: is mathematician closer to programmer or gardener? But you said in your question that the categories are built by men, so they could also help building the ontology.</p>

<p>Have a look on <a href=""https://stackoverflow.com/questions/tagged/computational-linguistics"">computational linguistics</a> here and in <a href=""http://en.wikipedia.org/wiki/Computational_linguistics"" rel=""nofollow noreferrer"">Wikipedia</a> for further studies.</p>

<p>Now, the more narrow the field your texts are from, the more structured they are, and the smaller the vocabulary, the easier the problem becomes.</p>

<p>Again some keywords for further studies: morphology, syntax analysis, semantics, ontology, computational linguistics, indexing, keywording</p>
",19,15,15583,2010-08-27 13:12:11,https://stackoverflow.com/questions/3584472/text-classification-categorization-algorithm
Topic modeling using mallet,"<p>I'm trying to use topic modeling with Mallet but have a question.</p>

<p>How do I know when do I need to rebuild the model? For instance I have this amount of documents I crawled from the web, using topic modeling provided by Mallet I might be able to create the models and infer documents with it. But overtime, with new data that I crawled, new subjects may appear. In that case, how do I know whether I should rebuild the model from start till current? </p>

<p>I was thinking of doing so for documents I crawled each month. Can someone please advise?</p>

<p>So, is topic modeling more suitable for text under a fixed amount of topics (the input parameter k, no. of topics). If not, how do I really determine what number to use?</p>
","nlp, text-mining, topic-modeling, mallet","<p>The answers to your questions depend in large part on the kind of data you're working with and the size of the corpus.</p>

<p>Regarding frequency, I'm afraid you'll just have to estimate how often your data changes in a meaningful way and remodel at that rate. You could start with a week and see if the new data lead to a significantly different model. If not, try two weeks and so on. </p>

<p>The number of topics you select is determined by what you're looking for in the model. The higher the number, the more fine-grained the results. If you want a broad overview of what's in your corpus, you could select say 10 topics. For a closer look, you could use 200 or some other suitably high number.</p>

<p>I hope that helps.</p>
",3,2,1607,2010-09-03 13:13:12,https://stackoverflow.com/questions/3636067/topic-modeling-using-mallet
Clustering text in MATLAB,"<p>I want to do hierarchical agglomerative clustering on texts in MATLAB. Say, I have four sentences, </p>

<pre><code>I have a pen.
I have a paper. 
I have a pencil.
I have a cat. 
</code></pre>

<p>I want to cluster the above four sentences to see which are more similar. I know Statistic toolbox has command like <code>pdist</code> to measure pair-wise distances, <code>linkage</code> to calculate the cluster similarity etc. A simple code like:</p>

<pre><code>X=[1 2; 2 3; 1 4];
Y=pdist(X, 'euclidean');
Z=linkage(Y, 'single');
H=dendrogram(Z)
</code></pre>

<p>works fine and return a dendrogram.  </p>

<p>I wonder can I use these command on the texts as I mentioned above. Any thoughts ? </p>

<hr>

<p><strong>UPDATES:</strong></p>

<p>Thanks to Amro. Read Understood and computed the distance among strings. Code follows:</p>

<pre><code>clc
S1='I have a pen'; % first String

f_id=fopen('events.txt','r'); %saved strings to compare with
events=textscan(f_id, '%s', 'Delimiter', '\n');
fclose(f_id); %close file.
events=events{1}; % saving the text read.

ii=numel(events); % selects one text randomly.
% store the texts in a cell array

for kk=1:ii

   S2=events(kk);
   S2=cell2mat(S2);
   Z=levenshtein_distance(S1,S2);
   X(kk)=Z;

end 
</code></pre>

<p>I input a string and I had 4 saved strings. Now I calculated the pairwise distance using <code>levenshtein_distance</code> function. It returns a matrix <code>X=[  17     0    16    18    16]</code>. </p>

<p>** I guess this is my pair wise distance matrix. Similar to what pdist does. Is it ?</p>

<p>** Now, I'm trying to input X to compute the linkage like </p>

<pre><code>Z=linkage(X, 'single);
</code></pre>

<p>Output I'm getting is:</p>

<blockquote>
  <p>Error using ==> linkage at 93 Size of
  Y not compatible with the output of
  the PDIST function.</p>
  
  <p>Error in ==> Untitled2 at 20
  Z=linkage(X,'single') .</p>
</blockquote>

<p>Why so ? Can use the linkage function at all ? Help appreciated. </p>

<p><strong>UPDATE 2</strong> </p>

<pre><code>clc
S1='I have a pen';

f_id=fopen('events.txt','r');
events=textscan(f_id, '%s', 'Delimiter', '\n');
fclose(f_id); %close file.
events=events{1}; % saving the text read.

ii=numel(events)+1; % total number of strings in the comparison

D=zeros(ii, ii); % initialized distance matrix;
for kk=1:ii 

    S2=events(kk);

    %S2=cell2mat(S2);

    for jk=kk+1:ii

  D(kk,jk)= levenshtein_distance(S1{kk},S2{jk});

    end

end

D = D + D';       %'# symmetric distance matrix

%# linkage expects the output format to match that of pdist,
%# so we convert D to a row vector (lower/upper part of matrix)
D = squareform(D, 'tovector');

T = linkage(D, 'single');
dendrogram(T).
</code></pre>

<p><em>Error: ??? Cell contents reference from a non-cell array object.
Error in ==> Untitled2 at 22
  D(kk,jk)= levenshtein_distance(S1{kk},S2{jk});</em></p>

<p>Also, Why am I reading the event from the file inside the first loop ? Doesn't seem logical. Bit confused, if I can work this way or only solution is to input all strings inside the code. Help much appreciated. </p>

<p><strong>UPDATE</strong></p>

<p>code to compare two sentences:</p>

<pre><code>clc
    str1 = 'Fire in NY';
    str2= 'Jeff is sick';

D=levenshtein_distance(str1,str2);
D = D + D';       %'# symmetric distance matrix

%# linkage expects the output format to match that of pdist,
%# so we convert D to a row vector (lower/upper part of matrix)

%D = squareform(D, 'tovector');

T = linkage(D, 'complete');
[H,P] = dendrogram(T,'colorthreshold','default');  
</code></pre>

<p>Output D=18.</p>

<p>WITH Different strings:</p>

<pre><code>clc
str1 = 'Fire in NY';
str2= 'NY catches fire';

D=levenshtein_distance(str1,str2);
D = D + D';       %'# symmetric distance matrix

%# linkage expects the output format to match that of pdist,
%# so we convert D to a row vector (lower/upper part of matrix)

%D = squareform(D, 'tovector');

T = linkage(D, 'complete');
[H,P] = dendrogram(T,'colorthreshold','default'); 
</code></pre>

<p>D=28. </p>

<p>Based on distance, a completely different sentence looks similar. What I'm trying to do, If I have stored <strong>Fire in NY</strong>, I wont store <strong><code>NY catches fire</code></strong>. However, for the first case, I would store as the information is new. </p>

<p>IS LD sufficient to do this ? Help appreciated.</p>
","matlab, cluster-analysis, text-mining","<p>What you need is a distance function that can handle strings. Check out the <a href=""http://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein distance</a> (edit distance). There are plenty of implementations out there:</p>

<ul>
<li><a href=""http://en.wikibooks.org/wiki/Algorithm_implementation/Strings/Levenshtein_distance#Octave_And_MATLAB"" rel=""nofollow noreferrer"">Wikibooks.org</a></li>
<li><a href=""http://www.mathworks.com/matlabcentral/fileexchange/17585-calculation-of-distance-between-strings"" rel=""nofollow noreferrer"">""Calculation of distance between strings"" on FEX</a></li>
</ul>

<p>Alternatively, you should extract some interesting features (ex: number of vowels, length of string, etc..) to build a vector space representation, then you can apply any of the usual distance measures (euclidean, ...) on the new representation.</p>

<hr>

<p><strong>EDIT</strong></p>

<p>The problem with your code is that <a href=""http://www.mathworks.com/help/toolbox/stats/linkage.html"" rel=""nofollow noreferrer"">LINKAGE</a> expects the input distances format to match that of <a href=""http://www.mathworks.com/access/helpdesk/help/toolbox/stats/pdist.html"" rel=""nofollow noreferrer"">PDIST</a>, namely a row vector corresponding to pairs of observations in the order 1-vs-2, 1-vs-3, 2-vs-3, etc.. which is basically the lower half of the complete distance matrix (since its supposed to be symmetric as <code>dist(1,2) == dist(2,1)</code>)</p>

<pre><code>%# instances
str = {'I have a pen.'
    'I have a paper.'
    'I have a pencil.'
    'I have a cat.'};
numStr = numel(str);

%# create and fill upper half only of distance matrix
D = zeros(numStr,numStr);
for i=1:numStr
    for j=i+1:numStr
        D(i,j) = levenshtein_distance(str{i},str{j});
    end
end
D = D + D';       %'# symmetric distance matrix

%# linkage expects the output format to match that of pdist,
%# so we convert D to a row vector (lower/upper part of matrix)
D = squareform(D, 'tovector');

T = linkage(D, 'single');
dendrogram(T)
</code></pre>

<p>Please refer to the documentation of the functions in question for more information...</p>
",5,3,8835,2010-09-05 13:16:41,https://stackoverflow.com/questions/3646169/clustering-text-in-matlab
How to compute similarity between two sentences (syntactical and semantical),"<p>I'm supposed to take two sentences each time and compute if they are similar. By similar I mean, both syntactically and semantically. </p>

<blockquote>
  <p>INPUT1: Obama signs the law.
        A new law is signed by Obama.</p>
  
  <p>INPUT2:
        A Bus is stopped here.
        A vehicle stops here. </p>
  
  <p>INPUT3: Fire in NY.
         NY is burnt down. </p>
  
  <p>INPUT4: Fire in NY.
         50 died in NY fire.</p>
</blockquote>

<p>I don't want to use ontology tree as a soul. I wrote a code to compute <a href=""http://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein distance</a> (LD) between sentences and then decide if the 2nd sentence:</p>

<ul>
<li>can be ignored (INPUT1 and 2), </li>
<li>should replace the first sentence (INPUT 3), or</li>
<li>store along with the first sentence (INPUT4). </li>
</ul>

<p>I'm not happy with the code as LD only computes syntactical level (what other methods ?). How can semantic be incorporated (like bus is sort of a vehicle?) .</p>

<p>The code goes here:</p>

<pre><code>%# As the difference is computed, a decision is made on the new event
%# (string 2) to be ignored, to replace existing event (string 1) or to be
%# stored separately. The higher the LD metric, the higher the difference
%# between two strings. Of course, lower difference indices either identical
%# or similar events. However, the higher difference indicates the new event
%# as a fresh event.

%#.........................................................................
%# Calculating the LD between two strings of events.
%#.........................................................................
L1=length(str1)+1;
L2=length(str2)+1;
L=zeros(L1,L2);   %# Initializing the new length.

g=+1;             %# just constant
m=+0;             %# match is cheaper, we seek to minimize
d=+1;             %# not-a-match is more costly.

% do BC's
L(:,1)=([0:L1-1]*g)';
L(1,:)=[0:L2-1]*g;

m4=0;             %# loop invariant
%# Calculating required edits.
for idx=2:L1;
    for idy=2:L2
        if(str1(idx-1)==str2(idy-1))
            score=m;
        else
            score=d;
        end
        m1=L(idx-1,idy-1) + score;
        m2=L(idx-1,idy) + g;
        m3=L(idx,idy-1) + g;
        L(idx,idy)=min(m1,min(m2,m3)); % only minimum edits allowed.
    end
end
%# The LD between two strings.
D=L(L1,L2);

%#....................................................................
%# Making decision on what to do with the new event (string 2).
%#...................................................................
if (D&lt;=4)     %# Distance is so less that string 2 seems identical to string 1.
    store=str1;        %# Hence string 2 is ignored. String 1 remains stored.
elseif (D&gt;=5 &amp;&amp; D&lt;=15) %# Distance is larger to be identical but not enough to
    %# make string 2 an individual event.
    store= str2;       %# String 2 is somewhat similar to string 1.
                       %# So, string 1 is replaced with string 2 and stored.
else
    %# For all other distances, string 2 is stored along with string 1.
    store={str1; str2};
end
</code></pre>

<p>Any help is appreciated.</p>
","matlab, semantics, text-mining","<p>""Semantically"". <strong>No simple text-book algorithm for that. Natural language (esp. English) is a very complicated and fickle beast.</strong> Let's look at (just a small part of) the provided cases:</p>

<pre><code>INPUT1: Obama signs the law. A new law is signed by Obama.
</code></pre>

<p>Signing a law makes it a 'new' law.</p>

<pre><code>INPUT2: A Bus is stopped here. A vehicle stops here.
</code></pre>

<p>Need to know a bus is a type if vehicle as well as some sort of time relation. Also, what if the bus <em>did</em> stop but does not normally stop or is no longer stopped? It can be taken several ways.</p>

<pre><code>INPUT3: Fire in NY. NY is burnt down.
</code></pre>

<p>Need to know that fires can burn things down.</p>

<pre><code>INPUT4: Fire in NY. 50 died in NY fire.
</code></pre>

<p>Need to know that fires can kill things (see next). Need to associated the ""news headline"" (50 WHAT?) with people. The brain can do this somewhat trivially. Computer programs are not brains.</p>

<p>And I'm no English major :-)</p>
",2,0,2995,2010-09-07 03:41:36,https://stackoverflow.com/questions/3655612/how-to-compute-similarity-between-two-sentences-syntactical-and-semantical
C# algorithm for N-gram,"<p>I am intending to use the n-gram code from <a href=""http://www.codeproject.com/KB/cs/tfidf.aspx"" rel=""nofollow"">this article</a>. The algorithm produces these tri-gram results:</p>

<pre><code>t, th, the, he, e, q, qu, qui, uic, ick, ck, k, r, re, red, ed, d
</code></pre>

<p>for the text <code>the quick red</code></p>

<p>However <a href=""https://en.wikipedia.org/wiki/Trigram#Examples"" rel=""nofollow"">wikipedia</a>, reckons it should be:</p>

<pre><code>the  qui  k_r
he_  uic  _re
e_q  ick  red
_qu  ck_
</code></pre>

<p>(space indicated by ‘_’).</p>

<p>What is correct? Are there any other C# implementation out there?</p>
","c#, text-mining, information-retrieval, n-gram","<p>The second example is correct.</p>

<p>ps. Why do you generate trigrams for the complete text and not only for words? What is your use case?</p>
",2,0,7712,2010-09-30 08:53:33,https://stackoverflow.com/questions/3829110/c-algorithm-for-n-gram
Is there a better way to create Keyword frequency table in R?,"<p>I want to take a csv export of my bibtex literature database and analyse the correlation between keywords and Journals.  I start off with a csv file containing one row per piece of literature, each one with a Journal name, and a keyword list, which is a slash deliminated list.  I want to end up with either a matrix of Journal by Keyword and counts.</p>

<p>Currently I've written this code, but there must be a better way, anyone have any ideas ?</p>

<pre><code>sortframe&lt;-function(df,...){df[do.call(order,list(...)),]}
library(ggplot2)
library(plyr)

bib&lt;-read.csv(""/home/paul/workspace/Test_R_statet/data/bib.csv"") # read csv file
</code></pre>

<p>So, here's the structure of my data, I've taken twenty rows that (seem) to be representative of the three thousand I've got in total.</p>

<pre><code>dput(bib)


structure(list(BibliographyType = c(7L, 7L, 7L, 7L, 7L, 7L, 7L, 
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L), Author = structure(c(19L, 21L, 
22L, 23L, 24L, 25L, 20L, 28L, 26L, 27L, 1L, 2L, 2L, 2L, 3L, 4L, 
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 15L, 16L, 17L, 
18L), .Label = c(""Constantinos, Apostolou; Dotsikas, Yannis; Kousoulos, Constantinos &amp; Loukas, Yannis L."", 
""Constantinos, Apostolou; Kousoulos, Constantinos; Dotsikas, Yannis; Soumelas, Georgios Stefanos; Kolocouri, Filomila; Ziaka, Afroditi &amp; Loukas, Yannis L."", 
""Constantinos, Kousoulos; Tsatsou, Georgia; Apostolou, Constantinos; Dotsikas, Yannis &amp; Loukas, Yannis"", 
""Corine, Ekhart; Gebretensae, Abadi; Rosing, Hilde; Rodenhuis, Sjoerd; Beijnen, Jos H. &amp; Huitema, Alwin D. R."", 
""Costa, Ferreira Sergio Luis; Bruns, Roy Edward; da Silva, Erik Galvpo Paranhos; dos Santos, Walter Nei Lopes; Quintella, Cristina Maria; David, Jorge Mauricio; de Andrade, Jailson Bittencourt; Breitkreitz, Marcia Cristina; Jardim, Isabel Cristina Sales Fontes &amp; Neto, Benicio Barros"", 
""Costa, Queiroz Regina Helena; Bertucci, Carlo; Malfarb, Wilson Roberto; Dreossi, S�nia Aparecida Carvalho; Chaves, Andrqa Rodrigues; Valqrio, Daniel Augusto Rodrigues &amp; Queiroz, Maria EugWnia Costa"", 
""Cui, Shuangjin; Fang, Feng; Han, Liu &amp; Ming, Ma"", ""D., Blessborn; Neamin, G.; Bergqvist, Y. &amp; Lindegsrdh, N."", 
""D., Fraier; Frigerio, E.; Brianceschi, G. &amp; James, C. A."", ""D., Grotto; Santa Maria, L. D.; Boeira, S.; Valentini, J.; Charpo, M. F.; Moro, A. M.; Nascimento, P. C.; Pomblum, V. J. &amp; Garcia, S. C."", 
""D., Hawker Charles; Garr, Susan B.; Hamilton, Leslie T.; Penrose, John R.; Ashwood, Edward R. &amp; Weiss, Ronald L."", 
""D., Hawker Charles; Roberts, William L.; Garr, Susan B.; Hamilton, Leslie T.; Penrose, John R.; Ashwood, Edward R. &amp; Weiss, Ronald L."", 
""D., Heath Dennis; Pruitt, Milagros A.; Brenner, Dean E.; Begum, Aynun N.; Frautschy, Sally A. &amp; Rock, Cheryl L."", 
""D., Jovanovic &amp; Vukovic, S."", ""D., McCullough B."", ""D., McCullough B. &amp; Vinod, H. D."", 
""D., McCullough B. &amp; Wilson, B."", ""D., Mendes Gustavo; Hamamoto, Daniele; Ilha, Jaime; Pereira, Alberto dos Santos &amp; De Nucci, Gilberto"", 
""do, Borges Ney Carter; Mendes, Gustavo D.; Barrientos-Astigarraga, Rafael E.; Galvinas, Paulo; Oliveira, Celso H. &amp; De Nucci, Gilberto"", 
""hui, Liu Chang; Huang, Xiao tao; Zhang, Rong; Yang, Lei; Huang, Tian lai; Wang, Ning sheng &amp; Mi, Sui qing"", 
""jing, Chen Zhang; Zhang, Jing; Yu, Ji cheng; Cao, Guo ying; Wu, Xiao jie &amp; Shi, Yao guo"", 
""jun, Dao Yi; Jiao, Zheng &amp; Zhong, Ming kang"", ""lan, Feng Shi; Hu, Fang di; Zhao, Jian xiong; Liu, Xi &amp; Li, Y."", 
""ming, Huang Jian; Wang, Guo quan; Jin, Yu; Shen, Teng &amp; Weng, Weiyu"", 
""nhaug, Halvorsen Trine Gr; Pedersen-Bjergaard, Stig &amp; Rasmussen, Knut E."", 
""qing, Liu Hua; Su, Meng xiang; Di, Bin; Hang, Tai jun; Hu, Ying; Tian, Xiao qin; Zhang, Yin di &amp; Shen, Jian ping"", 
""qing, Liu Yun; Chen, Qi yuan; Chen, Ben Mei; Liu, Shao gang; Deng, Fu liang &amp; Zhou, Ping"", 
""ying, Lee Chun &amp; Lee, Yung-jin""), class = ""factor""), Title = structure(c(29L, 
23L, 24L, 9L, 10L, 15L, 8L, 18L, 11L, 21L, 20L, 3L, 3L, 3L, 12L, 
25L, 26L, 19L, 16L, 2L, 14L, 22L, 6L, 7L, 27L, 13L, 5L, 4L, 28L, 
17L, 1L), .Label = c(""Anastrozole quantification in human plasma by high-performance liquid chromatography coupled to photospray tandem mass spectrometry applied to pharmacokinetic studies"", 
""A new approach to evaluate stability of amodiaquine and its metabolite in blood and plasma"", 
""An improved and fully validated LC-MS/MS method for the simultaneous quantification of simvastatin and simvastatin acid in human plasma"", 
""Assessing the reliability of statistical software: Part I"", 
""Assessing the reliability of statistical software: Part II"", 
""Automated Transport and Sorting System in a Large Reference Laboratory: Part 1. Evaluation of Needs and Alternatives and Development of a Plan"", 
""Automated Transport and Sorting System in a Large Reference Laboratory: Part 2. Implementation of the System and Performance Measures over Three Years"", 
""Determination of CQP propionic acid in rat plasma and study of pharmacokinetics of CQP propionic acid in rats by liquid chromatography"", 
""Determination of eleutheroside E and eleutheroside B in rat plasma and tissue by high-performance liquid chromatography using solid-phase extraction and photodiode array detection"", 
""Determination of palmatine in canine plasma by liquid chromatography-tandem mass spectrometry with solid-phase extraction"", 
""Development and validation of a liquid chromatography-tandem mass spectrometry method for the determination of xanthinol in human plasma and its application in a bioequivalence study of xanthinol nicotinate tablets"", 
""Development of a high-throughput method for the determination of itraconazole and its hydroxy metabolite in human plasma, employing automated liquidG��liquid extraction based on 96-well format plates and LC/MS/MS"", 
""Generation of quasi-stationary magnetic fields in turbulent plasmas"", 
""LC-MS-MS determination of nemorubicin (methoxymorpholinyldoxorubicin, PNU-152243A) and its 13-OH metabolite (PNU-155051A) in human plasma"", 
""Liquid-phase microextraction and capillary electrophoresis of citalopram, an antidepressant drug"", 
""New method for high-performance liquid chromatographic determination of amantadine and its analogues in rat plasma"", 
""On the accuracy of statistical procedures in Microsoft Excel 97"", 
""PKfit - A Pharmacokinetic Data Analaysis Tool in R"", ""Quantification of carbamazepine, carbamazepine-10,11-epoxide, phenytoin and phenobarbital in plasma samples by stir bar-sorptive extraction and liquid chromatography"", 
""Quantitative determination of donepezil in human plasma by liquid chromatography/tandem mass spectrometry employing an automated liquid-liquid extraction based on 96-well format plates: Application to a bioequivalence study"", 
""Quantitative determination of erythromycylamine in human plasma by liquid chromatography-mass spectrometry and its application in a bioequivalence study of dirithromycin"", 
""Rapid quantification of malondialdehyde in plasma by high performance liquid chromatography-visible detection"", 
""Selective method for the determination of cefdinir in human plasma using liquid chromatography electrospray ionization tandam mass spectrometry"", 
""Simultaneous determination of aciclovir, ganciclovir, and penciclovir in human plasma by high-performance liquid chromatography with fluorescence detection"", 
""Simultaneous quantification of cyclophosphamide and its active metabolite 4-hydroxycyclophosphamide in human plasma by high-performance liquid chromatography coupled with electrospray ionization tandem mass spectrometry (LC-MS/MS)"", 
""Statistical designs and response surface techniques for the optimization of chromatographic systems"", 
""Tetrahydrocurcumin in plasma and urine: Quantitation by high performance liquid chromatography"", 
""The numerical reliability of econometric software"", ""Verapamil quantification in human plasma by liquid chromatography coupled to tandem mass spectrometry: An application for bioequivalence study""
), class = ""factor""), Journal = structure(c(7L, 7L, 7L, 5L, 7L, 
6L, 7L, 1L, 7L, 7L, 7L, 9L, 9L, 9L, 2L, 7L, 6L, 9L, 9L, 9L, 9L, 
9L, 3L, 3L, 7L, 10L, 11L, 11L, 8L, 4L, 7L), .Label = c("""", ""Analytical and Bioanalytical Chemistry"", 
""Clinical Chemistry"", ""Computational Statistics and Data Analysis"", 
""European Journal of Pharmaceutics and Biopharmaceutics"", ""Journal of Chromatography A"", 
""Journal of Chromatography B"", ""Journal of Economic Literature"", 
""Journal of Pharmaceutical and Biomedical Analysis"", ""Physica B+C"", 
""The American Statistician""), class = ""factor""), Custom3 = structure(c(8L, 
9L, 11L, 17L, 25L, 19L, 24L, 27L, 12L, 22L, 2L, 5L, 5L, 6L, 3L, 
1L, 20L, 23L, 13L, 14L, 4L, 7L, 21L, 16L, 15L, 26L, 28L, 28L, 
28L, 10L, 18L), .Label = c(""4-Hydroxycyclophosphamide/Accuracy/Active metabolite/Assay/Chromatography/Cyclophosphamide/Determination/Electrospray/Electrospray ionization/Electrospray ionization tandem mass spectrometry/High performance liquid chromatography/High-performance liquid chromatography/Human/Human plasma/Internal standard/LC-MS/MS/Liquid chromatography/Liquid chromatography tandem mass spectrometry/Mass spectrometry/Metabolite/Pharmacokinetic/Pharmacokinetics/Plasma/Precipitation/Precision/Protein precipitation/Quantification/Sample preparation/Tandem mass spectrometry"", 
""96-Well/96-Well format/Analytical/Automated liquid-liquid extraction/bioequivalence/Bioequivalence study/Determination/Donepezil/Electrospray/Electrospray ionization/Extraction/Freezing/High throughput/High-throughput/Human/Human plasma/Human-plasma/LC-MS/MS/Liquid chromatography/tandem mass spectrometry/Liquid-liquid extraction/Loratadine/Mass spectrometry/Plasma/Plasma samples/Quantitative/Sample preparation/Tablet/Validation"", 
""96-Well/96-Well format/Assay/bioequivalence/Bioequivalence study/Determination/Electrospray/Electrospray ionization/Extraction/Freezing/High throughput/High-throughput/Human/Human plasma/Human-plasma/Interface/Internal standard/LC/MS/MS/LLE/Mass spectrometry/Metabolite/Monitoring/MRM/Parallel sample processing/Plasma/Plasma sample/Plasma samples/Precision/Quality control/Quantification/Simultaneous quantification/Tablet"", 
""96-Well/96-well plates/Accuracy/Analysis/Determination/Doxorubicin/Doxorubicin derivative/Extraction/Human/Human plasma/Human-plasma/In vivo/Interface/Interference/Internal standard/Ionspray/LC-MS-MS/LC-MS-MS determination/Liquid chromatography tandem mass spectrometry/Liquid chromatography-tandem mass spectrometry/Mass spectrometry/Metabolite/Methoxymorpholinyldoxorubicin/Monitoring/Multiple reaction monitoring/Nemorubicin/Patients/Plasma/Plasma samples/Precision/Quantitative/Quantitative determination/Residue/Solid phase extraction/SPE"", 
""96-Well/Analysis/Analytical/APCI/Atmospheric pressure chemical ionization/bioequivalence/Bioequivalence study/Determination/Electrospray/ESI/Extraction/Fully automated/High throughput/High-throughput/Human/Human plasma/Human-plasma/Improved/Internal standard/LC-MS/MS/LC-MS/MS method/Linearity/Liquid chromatography/tandem mass spectrometry/Liquid-liquid extraction/LLE/Lovastatin/Mass spectrometry/Plasma/Plasma sample/Plasma samples/Polarity switch/Precipitation/Precision/Protein precipitation/Quantification/Sample preparation/Simultaneous determination/Simultaneous quantification/Simvastatin/Simvastatin acid/Specificity/Tablet/Two-step extraction"", 
""96-Well/Atmospheric pressure chemical ionization/bioequivalence/Electrospray/High throughput/High-throughput/Human plasma/LC-MS/MS/Liquid-liquid extraction/Plasma/Polarity switch/Protein precipitation/Sample preparation/Simvastatin/Two-step extraction"", 
""Accuracy/Alkaline hydrolysis/Analytical/Assay/Bias/Deproteinization/Derivatization/Determination/Extraction/HPLC-VIS/Human plasma/Malondialdehyde/MDA/n-Butanol extraction/Phosphate/Plasma/Quantification/Reproducibility/Stability"", 
""Accuracy/Analysis/Analytical/bioequivalence/Bioequivalence study/Chromatography/Determination/Electrospray/Electrospray ionization/ESI/Extraction/Formulation/Human/Human plasma/Human-plasma/Imprecision/Internal standard/LC-MS/MS/Liquid chromatography/Liquid-liquid extraction/Mass spectrometry/Metoprolol/Monitoring/MRM/Plasma/Plasma samples/Quantification/Tablet/Tandem mass spectrometry/Verapamil"", 
""Accuracy/Cefdinir/Chromatography/Determination/Electrospray/Electrospray ionization/Healthy volunteer/Human/Human plasma/Human-plasma/LC/LC-MS/MS/Liquid chromatography/Mass spectrometry/Method validation/Monitoring/MS/MS/Pharmacokinetic/Pharmacokinetic profile/Plasma/Precipitation/Protein precipitation/Quantification/Three/Triple quadrupole/Validation/Water/Waters"", 
""Accuracy/statistics/reliability/testing"", ""Aciclovir/Assay/Bias/Chromatography/Determination/Ganciclovir/High performance liquid chromatography/High-performance liquid chromatography/HPLC/HPLC method/Human/Human plasma/Liquid chromatography/Penciclovir/Pharmacokinetic/Pharmacokinetic study/Plasma/Precipitation/Protein precipitation/Three"", 
""Acyclovir/bioequivalence/Electrospray/Extraction/Human/Human plasma/Liquid chromatography-tandem mass spectrometry/Liquid chromatography/tandem mass spectrometry/Mass spectrometry/Plasma/Precipitation/Protein precipitation/Quantification/Validation/Xanthinol nicotinate"", 
""Amantadine/Anthraquinone-2-sulfonyl chloride/Derivatization/Determination/HPLC/Memantine/Pharmacokinetic/Pharmacokinetic studies/Pharmacokinetic study/Plasma/Quantification/Rat/Rat plasma/Rimantadine/Three/UV/UV detection"", 
""Amodiaquine/Analysis/Antimalarial/Bias/Blood/Chloroquine/Desethylamodiaquine/Liquid chromatography/Metabolite/Plasma/Simultaneous analysis/solid-phase extraction/Stability/Whole blood"", 
""Analysis/Analytical/Blood/Chromatography/Curcumin/High performance liquid chromatography/HPLC/Internal standard/Liquid chromatography/Metabolite/Metabolites/Methods/Plasma/Quantification/Quantitation/Tetrahydrocurcumin/Urine/UV/UV detection/UV-detection"", 
""Analysis/Automation/Linear/Methods/Three"", ""Analysis/Blood/Chromatography/Determination/Eleutherococcus injection/Eleutheroside B/Eleutheroside E/Extraction/High performance liquid chromatography/High-performance liquid chromatography/HPLC/HPLC method/Liquid chromatography/Model/Pharmacokinetic/Pharmacokinetic studies/Pharmacokinetic study/Pharmacokinetics/Plasma/Rat/Rat plasma/Rats/Sample preparation/Solid phase extraction/solid-phase extraction/Tissue distribution"", 
""Analytical/Anastrazole/Anastrozole/Chromatography/Extraction/Healthy volunteer/High performance liquid chromatography/High-performance liquid chromatography/HPLC-MS-MS/Human/Human plasma/Human-plasma/Internal standard/Liquid chromatography/Liquid-liquid extraction/Mass spectrometry/Pharmacokinetic/Pharmacokinetic studies/Pharmacokinetic study/Pharmacokinetics/Photospray/Plasma/Quantification/Tandem mass spectrometry"", 
""Antidepressant/Antidepressant drug/Basic drugs/Capillary electrophoresis/CE/Citalopram/Detection/Drugs/Extraction/Hollow fibre/HPLC/HPLC method/Human/Human plasma/Human-plasma/liquid phase microextraction/Liquid-phase microextraction/LPME/Metabolite/Methods/Microextraction/N-Desmethylcitalopram/Phosphate/Plasma/Plasma sample/Plasma samples/Proteins/Quantification"", 
""Applications/Box-Behnken design/Box-Benhken design/Central composite design/Chromatographic methods/Determination/DOE/Doehlert matrix/Extraction/Methodology/Methods/Model/Multivariate techniques/Optimization/paper/Review/Sample preparation/Validation"", 
""Automation/Improved/Methods"", ""bioequivalence/Bioequivalence study/Determination/Dirithromycin/Electrospray/Electrospray ionization/Erythromycylamine/Extraction/Human/Human plasma/LC-MS/Plasma/Precision/Quantification/Residue"", 
""Carbamazepine/Carbamazepine-10,11-epoxide/Extraction/High-performance liquid chromatography/Liquid chromatography/Optimization/Phenobarbital/Phenytoin/Plasma/Quantification/Stir bar-sorptive extraction/Therapeutic drug monitoring/Three"", 
""Chromatography/CQP propionic acid/Determination/Extraction/Liquid chromatography/Pharmacokinetic/Pharmacokinetic studies/Pharmacokinetic study/Pharmacokinetics/Plasma/Rat/Rat plasma/Solid phase extraction/solid-phase extraction/UV"", 
""Determination/Dog/Electrospray/Electrospray ionization/Extraction/High-performance liquid chromatography-tandem mass spectrometry/HPLC-MS-MS/Internal standard/Jatrorrhizine/LC-MS-MS/Liquid chromatography tandem mass spectrometry/Liquid chromatography-tandem mass spectrometry/Mass spectrometry/Oasis/Palmatine/Pharmacokinetic/Pharmacokinetic studies/Pharmacokinetic study/Pharmacokinetics/Plasma/Plasma samples/Quantification/Solid phase extraction/solid-phase extraction/SPE/Water/Waters"", 
""Interaction/Plasma/Turbulence/Turbulent plasmas"", ""Pharmacokinetic/R"", 
""Software/statistics/reliability/testing""), class = ""factor"")), .Names = c(""BibliographyType"", 
""Author"", ""Title"", ""Journal"", ""Custom3""), row.names = c(1L, 2L, 
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 270L, 271L, 272L, 273L, 274L, 
275L, 276L, 277L, 278L, 279L, 280L, 281L, 282L, 283L, 284L, 285L, 
286L, 287L, 288L, 289L, 290L), class = ""data.frame"")
</code></pre>

<p>So that's my example data.  Here I manually loop over the data and build a narrow result dataframe, that I could turn into my desired result using melt/reshape.</p>

<pre><code>rm(""res"",""l"",""keyw"",""resu"")

res&lt;-data.frame(Journal=NA) #  Create result dataframe
res[keywordlist]&lt;-NA # Create keyword columns

l&lt;-1
resu&lt;-data.frame(Journal=NA,Keyword=NA, Count=0)

for(n in 1:nrow(bib)){ #  Loop over entries,
    message(n)
    keyw&lt;-strsplit(as.character(bib$Custom3[n]),""/"")[[1]]
    if(length(keyw)&gt;0){ #  If there was a keyword....
        for(i in 1:length(keyw)){ #  for each keyword, add a line with Journal, Keyword, 1
            message(paste(""i is "",i,sep=""""))
            message(paste(""l is "",l,sep=""""))
            message(paste(""journal "",bib$Journal[n],sep=""""))
            resu[l,]&lt;-c(as.character(bib$Journal[n]),keyw[i],1)
            message(paste(""Keyword is "",keyw[i],sep=""""))
            resu$Count[l]&lt;-1
            l&lt;-l+1
        }}
}

#Now use ddply to summarise
keywordtable&lt;-ddply(resu, c(""Journal"",""Keyword""),function(df) {
            result&lt;-data.frame(Journal=df$Journal[1], Keyword=df$Keyword[1], Count=sum(as.numeric(df$Count)))
            return(result)
        })
</code></pre>

<p>Now I can take the highest scores and plot a 'heatmap' style graph.</p>

<pre><code>trimkeyword&lt;-subset(keywordtable,!(Journal == """") &amp; Count &gt; 5, drop=TRUE)
trimkeyword$Journal&lt;-droplevels(trimkeyword$Journal)
trimkeyword$Keyword&lt;-droplevels(trimkeyword$Keyword)

qplot(data=trimkeyword, x=Journal, y=Count)

p &lt;- ggplot(trimkeyword, aes(Journal, Keyword)) + geom_tile(aes(fill = Count),colour = ""white"") + 
        scale_fill_gradient(low = ""white"", high = ""steelblue"")

base_size &lt;- 6
p&lt;-p + theme_grey(base_size = base_size) + labs(x = """", y = """") + 
        scale_x_discrete(expand = c(0, 0)) +
        scale_y_discrete(expand = c(0, 0)) + 
        opts(legend.position = ""none"", axis.ticks = theme_blank(), 
                axis.text.x = theme_text(size = base_size *0.8, 
                        angle = 330, hjust = 0, colour = ""grey50""))

print(p)
</code></pre>

<p>Anyone else want to suggest anything ?</p>

<p>Other things that come to mind are;</p>

<ul>
<li>Are there any good ways (in R) to
weed out sets of keywords that have
the same meaning (ie cat, cats,
feline, pussy could all be replaced
with cat) </li>
<li>Is there a way to build the
table without looping</li>
</ul>

<p>EDIT: I've replaced the dummy data with something that's more representative.</p>
","r, text-mining, heatmap","<p>here is a much shorter piece of code to get to keywordtable from the data frame bib</p>

<pre><code># create list of keywords by journal
res = dlply(bib, .(Journal), summarize, 
            keyw = strsplit(as.character(Custom3), ""/""));

# convert into dataframe
res = melt(unlist(res));


res$journal   = rownames(res);
names(res)[1] = 'keyword';
rownames(res) = NULL;

res$journal = with(res, gsub('.keyw', """", journal));
res$journal = with(res, gsub('[[:digit:]]', """", journal));
res$keyword = tolower(res$keyword);

keywordtable = ddply(res, .(journal, keyword), summarize, 
               count = length(keyword));
</code></pre>

<p>An alternate visualization would be to create a word cloud of keywords using the snippets package. Here is the code to do that:</p>

<pre><code>library(snippets);
keywords = table(res$keyword);
cloud(keywords, col = col.br(keywords, fit=TRUE))
</code></pre>
",2,3,1537,2010-10-23 17:00:18,https://stackoverflow.com/questions/4005092/is-there-a-better-way-to-create-keyword-frequency-table-in-r
Lucene Entity Extraction,"<p>Given a finite dictionary of entity terms, I'm looking for a way to do Entity Extraction with intelligent tagging using Lucene. Currently I've been able to use Lucene for:<br>
 - Searching for complex phrases with some fuzzyness<br>
 - Highlighting results</p>

<p>However, I 'm not aware how to:<br>
 -Get accurate offsets of the matched phrases<br>
 -Do entity-specific annotaions per match(not just <B></B> tags for every single hit)</p>

<p>I have tried using the explain() method - but this only gives the terms in the query which got the hit - not the offsets of the hit within the original text.</p>

<p>Has anybody faced a similar problem and is willing to share a potential solution?</p>

<p>Thank you in advance for you help!</p>
","lucene, text-mining, information-extraction, lucene-highlighter","<p>For the offset, see this question: <a href=""https://stackoverflow.com/questions/2930339/how-get-the-offset-of-term-in-lucene"">How get the offset of term in Lucene?</a></p>

<p>I don't quite understand your second question. It sounds to me like you want to get the data from a <a href=""http://lucene.apache.org/java/3_0_2/api/core/org/apache/lucene/document/Field.html"" rel=""nofollow noreferrer"">stored field</a> though. To get the data from a stored field:</p>

<pre><code>TopDocs results = searcher.Search(query, filter, num);
foreach (ScoreDoc result in results.scoreDocs)
{
    Document resultDoc = searcher.Doc(result.doc);
    string valOfField = resultDoc.Get(""My Field"");
}
</code></pre>
",2,4,1483,2010-11-16 21:50:56,https://stackoverflow.com/questions/4199382/lucene-entity-extraction
Runtime pompt for Rapidminer,"<p>I have been using Rapidminer and created a series of processes which preform a standard set of tasks. Now, I want allow the user to dynamically set the parameters of a process at the start. </p>

<p>For example, when writing  a CSV, I want to prompt the user to type a string containing the location where it should be saved via some prompt (either at the start of the script, or at some other stage during the process.</p>

<p>Is this possible via Rapidminer, or should I be creating some script to generate and runt he process on the fly?</p>
","data-mining, text-mining, rapidminer","<p>To change the parameters you want passed to your processes, I believe that you must edit the .xml file of your process. For example,</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;process version=""5.0""&gt;
  &lt;context&gt;
    &lt;input/&gt;
    &lt;output/&gt;
    &lt;macros/&gt;
  &lt;/context&gt;
  &lt;operator activated=""true"" class=""process"" compatibility=""5.0.10"" expanded=""true"" name=""Process""&gt;
    &lt;process expanded=""true"" height=""145"" width=""212""&gt;
      &lt;operator activated=""true"" class=""generate_data"" compatibility=""5.0.10"" expanded=""true"" height=""60"" name=""Generate Data"" width=""90"" x=""112"" y=""30""&gt;
        &lt;parameter key=""number_examples"" value=""10""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""write_csv"" compatibility=""5.0.10"" expanded=""true"" height=""60"" name=""Write CSV"" width=""90"" x=""271"" y=""39""&gt;
        &lt;parameter key=""csv_file"" value=""C:\Users\wessel\Desktop\gendata.csv""/&gt;
      &lt;/operator&gt;
      &lt;connect from_op=""Generate Data"" from_port=""output"" to_op=""Write CSV"" to_port=""input""/&gt;
      &lt;connect from_op=""Write CSV"" from_port=""through"" to_port=""result 1""/&gt;
      &lt;portSpacing port=""source_input 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 2"" spacing=""0""/&gt;
    &lt;/process&gt;
  &lt;/operator&gt;
&lt;/process&gt;
</code></pre>

<p>You can see that for the write_csv operator, the value contains the path of the file to be written to. Changing the parameters as you describe would involve writing a script to get the values from the user, edit the corresponding values in the .xml file to these desired values and then throwing the process at RapidMiner.</p>
",1,1,742,2010-12-15 12:36:53,https://stackoverflow.com/questions/4449924/runtime-pompt-for-rapidminer
Full Text PDFs for PubMed Articles,"<p>While working on a project I need to download and process full text articles for PubMed abstracts, is there any implemented code or tool that allows the user to input a set of PubMed ids and downloads the free full text articles for the same.  Any kind of help or tips is greatly appreciated. </p>
","pdf, nlp, text-mining, pubmed","<p>I don't think it's possible in general, due to the nature of PubMed. The best you are going to do is get articles from the Open Access subset of PubMedCentral. PubMedCentral have a <a href=""https://www.ncbi.nlm.nih.gov/pmc/tools/textmining/"" rel=""nofollow noreferrer"">number of online utilities</a> for doing the job.</p>
",8,6,6000,2011-01-14 16:20:52,https://stackoverflow.com/questions/4693226/full-text-pdfs-for-pubmed-articles
Find 3-8 word common phrases in body of text using PHP,"<p>I'm looking for a way to find common phrases within a body of text using PHP. If it's not possible in php, I'd be interested in other web languages that would help me complete this.</p>

<p>Memory or speed are not an issues. </p>

<p>Right now, I'm able to easily find keywords, but don't know how to go about searching phrases. </p>
","php, data-mining, text-mining","<p>I've written a PHP script that does just that, <a href=""https://github.com/Core-Xii/freqwordseq-legacy-"" rel=""nofollow"">right here.</a> It first splits the source text into an array of words and their occurrence count. Then it counts common sequences of those words with the specified parameters. It's old code and not commented, but maybe you'll find it useful.</p>
",5,4,4453,2011-01-26 04:37:27,https://stackoverflow.com/questions/4801449/find-3-8-word-common-phrases-in-body-of-text-using-php
How to classify words to their correspoding categories?,"<p>I've to implement text classification for a long list of words. I've some categories defined e.g. If the word ""UK"" is in the list, it will come under ""Regions"". If the word is ""Pizza"", it will come under category ""food"".</p>

<p>How can I classify the words under different categories? Is there any open source tool available to do that? </p>
","java, nlp, classification, ontology, text-mining","<p>I'm not entirely sure what you're trying to do, but if what you want is to build up a list representative words for a number of categories then you could do this by selecting the top N most frequent words, excluding <a href=""http://en.wikipedia.org/wiki/Stop_words"" rel=""nofollow"">stop words</a>, from a set of documents representative of each category. This is an easy way of creating a very basic ontology.</p>

<p>For example, to create a set of words about <strong>food</strong> you could crawl the web for <strong><a href=""http://allrecipes.co.uk/"" rel=""nofollow"">recipies</a> and <a href=""http://www.just-eat.co.uk/"" rel=""nofollow"">menus</a></strong> and then select the most frequent words from these. I'd expect that once you have excluded stop words you'll have a good list of food related words.  For words related to <strong>programming</strong> you could crawl <strong>stackoverflow.com</strong>, etc etc...</p>

<p>Then again, this may not be what you're trying to do...</p>
",2,4,2717,2011-01-30 19:02:24,https://stackoverflow.com/questions/4844825/how-to-classify-words-to-their-correspoding-categories
Text Mining an Outlook Email Archive,"<p>I am considering preforming some text-mining on a set of large individual .pst file containing >4 years of communication.</p>

<p>Initially, I would like to just extract the header information to identify social networks, but ultimately would like to begin to classify emails based on key-words or create some structured output that would support some further analysis.</p>

<p>Does anyone have any suggestions where to begin?</p>
","outlook, text-mining","<p>You should check the research done on the publicly available <a href=""http://www.cs.cmu.edu/~enron/"" rel=""nofollow"">Enron Email Dataset</a> -> The page has link to some interesting papers</p>
",2,1,3206,2011-01-31 14:59:57,https://stackoverflow.com/questions/4852353/text-mining-an-outlook-email-archive
Converting data into information:Where to start?,"<p>We (my company) runs a website which have lots of data recorded like user registration, visits, clicks, what the stuff they post etc etc but so far we don't have a tool to find out how to monitor entire thing or how to find patterns in it so that we can understand what kind of information we can get from it? So that Mgmt can take decisions based on it. In short, the people do at Amazon or Google based on data they retrieve, we want a similar thing.</p>

<p>Now, after the intro, I would like to know what technology could it be called;is it Data Mining,Machine Learning or what? Where should we start to convert meaningless data into useful Information?</p>
","text, data-mining, text-mining, information-retrieval","<p>I think what you need enters in the ""realm"" of: parsing data, creating graphs, showing statistics about some elements, etc.</p>

<p>There is no ""easy"" answer, I can only answer parts of your question.</p>

<ul>
<li><p>There are no premade magical analytical tools, big companies have their own backend tools tunned to parse the large amounts of data and spit out data summaries that are then used to build graphs or for statistical analysis.</p></li>
<li><p>I think the domain you are searching for is statistical data analysis. But there are many parts that go together here.</p></li>
</ul>

<p>Best advice I can give you is to set up specific goals for you analysis and then try to see what is the best solution, you question is too open.</p>

<p>ie. if you are interested in visits/clicks/website related statistics Google Analytics is a great tool, and very easy to use.</p>
",1,0,679,2011-03-04 05:11:16,https://stackoverflow.com/questions/5189954/converting-data-into-informationwhere-to-start
What is a good approach for extracting keywords from user-submitted text?,"<p>I'm building a site that allows users to make sense of a debate by graphically representing arguments for and against a particular issue. (<a href=""http://wrangl.com"" rel=""nofollow"">Wrangl</a>)</p>

<p>I'd like to categorise these debates so they are more easily found and connected. I don't want to irritate the person creating the debate by asking them to add tags and categories before they see any benefit, so I'm looking at a way of automatically extracting keywords.</p>

<p>What's a good approach for taking the debate's title and description (and possibly the content of the arguments themselves once there are some) to pull out, say, ten strong keywords that could be used as metadata to connect similar debates together, or even as the content of the ""meta"" keywords tag in the head of the HTML page where the debate is viewable. Eg. <a href=""http://wrangl.com/datamapper-activerecord"" rel=""nofollow"">Datamapper vs ActiveRecord</a></p>

<p>The site is coded in Ruby with Sinatra, using DataMapper for data storage. I'm ideally looking for something which will work on Heroku (I don't have a way of writing files to disk dynamically), and I'd consider a web service, an API or ideally a Ruby gem.</p>
","ruby, metadata, sinatra, keyword, text-mining","<p>Maybe you can use <a href=""http://martin.ankerl.com/2007/01/09/textanalyzer-automatically-extract-characteristic-words/"" rel=""noreferrer"">TextAnalyzer</a>.</p>
",7,6,2112,2011-03-16 07:43:14,https://stackoverflow.com/questions/5322317/what-is-a-good-approach-for-extracting-keywords-from-user-submitted-text
Tools to generating a grammar using examples?,"<p><a href=""https://stackoverflow.com/questions/1245485/algorithms-or-patterns-for-reading-text/1247649#1247649"">This answer</a> shows a pretty example of using a parser generator to look through text for some patterns of interest.  In that example, it's product prices.</p>

<p>Does anyone know of tools to generate the grammars given training examples (document + info I want from it)?  I found a couple papers, but no tools.  I looked through <a href=""http://en.wikipedia.org/wiki/ANTLR"" rel=""nofollow noreferrer"">ANTLR</a> docs a bit, but it deals with grammars; a ""recognizer"" takes as input a grammar, not training examples.</p>
","grammar, text-mining","<p>This is a machine learning problem.  You can at best get an approximation.  But I don't think anybody has done this well, let alone released a tool.  (I actively track what people do to build grammars for computer languages, and this idea has been proposed many times, but I have yet to see a useful implementation).</p>

<p>The problem is that for any fixed set of examples, there's a huge number of possible grammars.  It is easy to construct a naive one:  for the fixed set of examples, simply propose a grammar that has one rule to recognize each example.  That works, but is hardly helpful.  Now the question is, how many ways can you generalize this, and which one is the best?  In fact you can't know, because your next new example may be a total surprise in terms of structure.  (Theory definition: A language is the set of sentences that comprise it).</p>

<p>We haven't even talked about the  simpler problem of learning the <em>lexemes</em> of the language.  How would you propose to learn what legal strings for floating point numbers are?</p>
",4,6,2054,2011-03-29 15:58:38,https://stackoverflow.com/questions/5475408/tools-to-generating-a-grammar-using-examples
Mallet: features contribution on each prediction,"<p>I'm developing a NER system on Mallet using CRFs.</p>

<p>Do you know if it is possible to collect the features contribution for each prediction?
I need to know and understand the precise behavior of the CRF model.</p>

<p>Any suggestions?</p>

<p>Thanks.</p>

<p>Cheers,
ukrania</p>
","java, machine-learning, text-mining, mallet, crf","<p>Yes you can do this, just do  print of CRF model and check weights of model, if value is big by absolute value then it has big influence in classification result.  </p>
",0,2,383,2011-04-29 21:57:23,https://stackoverflow.com/questions/5837818/mallet-features-contribution-on-each-prediction
How are dictionaries for topic-related crawlers defined?,"<p>I am wondering what is the best method to define a dictionary to calculate relevance of a specific website. At least dictionaries with words seem to be an important method of measuring relevance for new websites found via links (e.g. if a website is linked to, but it does not contain any word about soccer, it is probably irrelevant for my soccer crawler).</p>

<p>I came to the following ideas, but all of them have major drawbacks:</p>

<ul>
<li>Write a dictionary by hand -> you might forget a lot of words and it is very time consuming</li>
<li>Take the most important words from the first website as dictionary -> a lot of words would probably be missing</li>
<li>Take the most important words on all websites as entries in the dictionary and weight them by relevance (e.g. a website which is only relevant 0.4 would not have such a big impact on the dictionary as a website that is relevant 0.8) -> seems pretty complicated and could lead to unexpected results</li>
</ul>

<p>The last method seems the best to me, but maybe there are better and more common methods?</p>
","web-crawler, text-mining","<p>I would recommend that you build a common-word dictionary from a list of known sites. Suppose you have 100 sites and you <strong>know</strong> that they're all talking about soccer. You can build unigram and bigram (or n-gram) maps of the content and use it as a baseline from which you measure some type of ""deviation"" with regards to every new observation you make. Note that you would have to remove the common stopwords in order to eliminate irrelevant words; in English there are quite a few, here is a list: <a href=""http://www.ranks.nl/resources/stopwords.html"" rel=""nofollow"">http://www.ranks.nl/resources/stopwords.html</a></p>

<p>N-grams are frequency counts of words or combinations of words. Unigrams creates a map where the key is the word and the value is the number of occurrence for each word. Bigrams are usually constructed by combining two consecutive words and using them as the key, so forth for trigrams and n-grams.</p>

<p>You can take the top n-grams from your known sites and compare them against the top n-grams of the site you're currently evaluating. The more similar they are, the more likely that the site is with the same topic.</p>
",1,1,146,2011-05-08 15:36:44,https://stackoverflow.com/questions/5928417/how-are-dictionaries-for-topic-related-crawlers-defined
Algorithms for mapping data in data mining,"<p>I need to scrape some webpages and extract content from them. I'm planning to select some specific keywords and map the data that has some relationship b/w them. But I have no Idea, how I could do that. Could anyone suggest me some algorithms for doing it?.</p>

<p>For example I need to download some webpages about apples and map the relevant data about apples to it and store in database so that, if someone needs specific information about it, I could provide it fastly and accurately.</p>

<p>Also it would be helpful pointing out helpful libraries too. I'm planning to do it in python.</p>
","python, web-scraping, data-mining, text-mining","<p>Have a look at <a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a>, <a href=""http://www.clips.ua.ac.be/pages/pattern"" rel=""nofollow"">Pattern</a> or <a href=""http://orange.biolab.si/"" rel=""nofollow"">Orange</a> modules.</p>

<p>As a start ""<a href=""http://oreilly.com/catalog/9780596529321"" rel=""nofollow"">Programming collective intelligence: building smart web 2. 0 applications</a>"" by Toby Segaran is a good book to read.</p>
",1,2,1174,2011-05-14 12:30:08,https://stackoverflow.com/questions/6001834/algorithms-for-mapping-data-in-data-mining
Installing R Text Mining package on Ubuntu Lucid,"<p>New to R, and am trying to install the text mining package (tm). However when I do</p>

<blockquote>
  <blockquote>
    <p>install.packages(""tm"",dependencies=TRUE) </p>
  </blockquote>
</blockquote>

<p>I get the following error:</p>

<pre><code>  Warning in install.packages(""tm"", dependencies = TRUE) :
  argument 'lib' is missing: using '/usr/local/lib/R/site-library'
  --- Please select a CRAN mirror for use in this session ---
  Loading Tcl/Tk interface ... done
  Warning message:
  In getDependencies(pkgs, dependencies, available, lib) :
  package ‘tm’ is not available
</code></pre>

<p>Any thoughts?</p>

<p>Thanks in advance.</p>
","r, text-mining","<p>There's a thread on this in the <a href=""http://ubuntuforums.org/showthread.php?t=1644889"" rel=""nofollow"">Ubuntu forums</a>.</p>

<p>Most likely answer is that tm requires a more recent version of R (>= 2.11.0) than the one that you have installed. Try adding an R repository as <a href=""http://cran.r-project.org/bin/linux/ubuntu/README"" rel=""nofollow"">described here</a> and upgrading, then try to install tm again.</p>
",1,0,1644,2011-05-18 00:52:28,https://stackoverflow.com/questions/6038756/installing-r-text-mining-package-on-ubuntu-lucid
How to access Wikipedia from R?,"<p>Is there any package for R that allows querying Wikipedia (most probably using Mediawiki API) to get list of available articles relevant to such query, as well as import selected articles for text mining?</p>
","r, wikipedia, text-mining, wikipedia-api, mediawiki-api","<p>Use the <code>RCurl</code> package for retreiving info, and the <code>XML</code> or <code>RJSONIO</code> packages for parsing the response.</p>

<p>If you are behind a proxy, set your options.</p>

<pre><code>opts &lt;- list(
  proxy = ""136.233.91.120"", 
  proxyusername = ""mydomain\\myusername"", 
  proxypassword = 'whatever', 
  proxyport = 8080
)
</code></pre>

<p>Use the <code>getForm</code> function to access <a href=""https://secure.wikimedia.org/wikipedia/en/w/api.php"" rel=""noreferrer"">the API</a>.</p>

<pre><code>search_example &lt;- getForm(
  ""http://en.wikipedia.org/w/api.php"", 
  action  = ""opensearch"", 
  search  = ""Te"", 
  format  = ""json"",
  .opts   = opts
)
</code></pre>

<p>Parse the results.</p>

<pre><code>fromJSON(rawToChar(search_example))
</code></pre>
",8,11,6878,2011-05-23 10:28:16,https://stackoverflow.com/questions/6095952/how-to-access-wikipedia-from-r
Liblinear how to use it,"<p>I'm fairly new at machine learning and text mining in general. It has come to my attention the presence of a ruby library called Liblinear <a href=""https://github.com/tomz/liblinear-ruby-swig"" rel=""nofollow"">https://github.com/tomz/liblinear-ruby-swig</a>.</p>

<p>What I want to do so far is train the software to identify whether a text mentions anything related to bicycles or not.</p>

<p>Can someone please highlight the steps that I should be following (i.e: preprocessing text and how), share resources and ideally share a simple example to get me going.</p>

<p>Any help will do, thanks!</p>
","ruby, machine-learning, classification, text-mining","<p>The classical approach is:</p>

<ol>
<li>Collect a representative sample of input texts, each labeled as related/unrelated.</li>
<li>Divide the sample into training and test sets.</li>
<li>Extract all the terms in all the documents of the training set; call this the vocabulary, <em>V</em>.</li>
<li>For each document in the training set, convert it into a vector of booleans where the <em>i</em>'th element is true/1 iff the <em>i</em>'th term in the vocabulary occurs in the document.</li>
<li>Feed the vectorized training set to the learning algorithm.</li>
</ol>

<p>Now, to classify a document, vectorize it as in step 4. and feed it to the classifier to get a related/unrelated label for it. Compare this with the actual label to see if it went right. You should be able to get at least some 80% accuracy with this simple method.</p>

<p>To improve this method, replace the booleans with term counts, normalized by document length, or, even better, <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Tf%E2%80%93idf"" rel=""nofollow"">tf-idf</a> scores.</p>
",2,4,542,2011-05-24 20:49:54,https://stackoverflow.com/questions/6116718/liblinear-how-to-use-it
Classify words to &quot;good&quot; and &quot;bad&quot;,"<p>I have a list of domain names and want to determine is name of domain looks like it is porno site or not. What the better way to do this? List of porn domains looks like <a href=""http://dumpz.org/56957/"" rel=""nofollow"">http://dumpz.org/56957/</a> . This domains can be used to teach the system how porno domains should look like. Also I have other list - <a href=""http://dumpz.org/56960/"" rel=""nofollow"">http://dumpz.org/56960/</a> - many domains of this list also is porno and I want to determine them by name.</p>
","python, algorithm, scala, classification, text-mining","<p>Use a bayesian filter eg: <a href=""http://spambayes.sourceforge.net/"" rel=""noreferrer"">SpamBayes</a> or Divmods Reverend. You train it with the list you have and could score how likely it is for a given domain, if it is porn.</p>

<p>For a short overview look at <a href=""http://www.larsen-b.com/Article/244.html"" rel=""noreferrer"">this</a> article.</p>
",5,4,1679,2011-05-29 18:35:45,https://stackoverflow.com/questions/6169621/classify-words-to-good-and-bad
Mining Wikipedia for mapping relations for text mining,"<p>I am planning to develop a web-based application which could crawl wikipedia for finding relations and store it in a database. By relations, I mean searching for a name say,'Bill Gates' and find his page, download it and pull out the various information from the page and store it in a database. Information may include his date of birth, his company and a few other things. But I need to know if there is any way to find these unique data from the page, so that I could store them in a database. Any specific books or algorithms would be greatly appreciated. Also mentioning of good opensource libraries would be helpful. </p>

<p>Thank You</p>
","python, pattern-matching, data-mining, wikipedia, text-mining","<p>If you haven't already, you should have a look at DBpedia.  Many categories of wiki articles have ""Infoboxes"" for the kinds of information you describe, and they've made a database out of it:</p>

<p><a href=""http://en.wikipedia.org/wiki/DBpedia"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/DBpedia</a></p>

<p>You might also leverage some of the information in Metaweb's <a href=""http://www.youtube.com/watch?v=tBSdYi4EY3s"" rel=""nofollow noreferrer"">Freebase</a> (which overlaps and I believe may even integrate the info from DBpedia.)  They have an API for querying their graph database, and there's a Python wrapper for it called <a href=""http://code.google.com/p/freebase-python/"" rel=""nofollow noreferrer"">freebase-python</a>.</p>

<blockquote>
  <p><strong>UPDATE:</strong> Freebase is no more; they were acquired by Google and eventually folded into the <a href=""https://en.wikipedia.org/wiki/Knowledge_Graph"" rel=""nofollow noreferrer"">Google Knowledge Graph</a>.  There is <a href=""https://developers.google.com/knowledge-graph/"" rel=""nofollow noreferrer"">an API</a> but I don't think they have anything like the formal sync'ing Freebase had with public sources like Wikipedia.  I'm personally disappointed in how this looks to have turned out.  :-/</p>
</blockquote>

<p>As for the natural language processing bit, if you do make headway on that problem you might consider these databases as repositories for any information you do mine.</p>
",6,2,3186,2011-05-30 02:24:31,https://stackoverflow.com/questions/6171764/mining-wikipedia-for-mapping-relations-for-text-mining
extracting postal addresses from pdf files,"<p>Are there any libraries/toolkits that would help me in the task of extracting postal address information from unstructured PDF documents (e.g. letters)? If not, how would you approach this task?</p>

<p>I thought about using an open source PDF library and searching for the information with regex patterns, but I'm not sure if it's possible to reliably identify addresses with this simple approach. Unfortunately, the data mining course I attended didn't touch text mining, but only dealt with highly structured data. Maybe someone working on natural language processing knows a useful library or toolkit?</p>
","regex, pdf, text, data-mining, text-mining","<p>I would recommend <a href=""http://pdfbox.apache.org"" rel=""nofollow"">http://pdfbox.apache.org</a> for reading pdf(i.e converting to text) and <a href=""http://code.google.com/p/graph-expression/"" rel=""nofollow"">http://code.google.com/p/graph-expression/</a> for writting Post address grammar. </p>
",1,4,4078,2011-07-05 12:07:23,https://stackoverflow.com/questions/6582398/extracting-postal-addresses-from-pdf-files
The relationship between latent Dirichlet allocation and documents clustering,"<p>I would like to clarify the relationship between latent Dirichlet allocation (LDA) and the generic task of document clustering. </p>

<p>The LDA analysis tends to output the topic proportions for each document. If my understanding is correct, this is not the direct result of document clustering. However, we can treat this probability proportions as a feature reprsentation for each document. Afterwards, we can invoke other established clustering method based on the feature configurations generated by LDA analysis.</p>

<p>Is my understanding correct? Thanks.</p>
","nlp, machine-learning, data-mining, text-mining, lda","<p>Yes, you can treat the output of LDA as features for your documents; this is exactly what Blei, Ng and Jordan did in the <a href=""http://jmlr.csail.mit.edu/papers/v3/blei03a.html"" rel=""noreferrer"">paper that introduced LDA</a>. They did it for classification, but for clustering the procedure is the same.</p>

<p>(In machine learning terminology, this use of LDA is called <em>dimensionality reduction</em> because it reduces the feature space's number of dimensions from |<em>V</em>|, the vocabulary size, to some number <em>k</em> of topics selected by the user.)</p>
",10,9,1965,2011-07-07 14:17:37,https://stackoverflow.com/questions/6612062/the-relationship-between-latent-dirichlet-allocation-and-documents-clustering
"Pure statistical, or Natural Language Processing engine?","<p>What are the statistical engines that yield better results than the OpenNLP suite of tools, if any? What I'm looking for is an engine that picks keywords from texts and provides stemming on those verbs &amp; nouns, perhaps Natural Language Processing is not the way to go here. The engine should also work with different languages.</p>
","nlp, text-mining, opennlp, statistics","<p>LingPipe is probably worth a look as complete NLP tool.</p>

<p>However, if all you need to do is find verbs and nouns and stem them, then you could just
1) tokenize text
2) run a POS tagger
3) run a stemmer</p>

<p>The Stanford tools can do this for multiple languages I believe, and NLTK would be a quick way to try it out.</p>

<p>However, you want to be careful of just going after verbs and nouns- what do you do about noun phrases and multiword nouns? Ideally an nlp package can handle this, but a lot of it depends on the domain you are working in. Unfortunately a lot of NLP is how good your data is.</p>
",2,2,721,2011-07-08 19:59:30,https://stackoverflow.com/questions/6629812/pure-statistical-or-natural-language-processing-engine
Are there APIs for text analysis/mining in Java?,"<p>I want to know if there is an API to do text analysis in Java. Something that can extract all words in a text, separate words, expressions, etc.  Something that can inform if a word found is a number, date, year, name, currency, etc.</p>

<p>I'm starting the text analysis now, so I only need an API to kickoff. I made a web-crawler, now I need something to analyze the downloaded data. Need methods to count the number of words in a page, similar words, data type and another resources related to the text.</p>

<p>Are there APIs for text analysis in Java?</p>

<p>EDIT: Text-mining, I want to mining the text. An API for Java that provides this.</p>
","java, api, nlp, analysis, text-mining","<p>For example - you might use some classes from standard library <code>java.text</code>, or use <code>StreamTokenizer</code> (you might customize it according to your requirements). But as you know - <strong>text data from internet sources is usually has many orthographical mistakes</strong> and for better performance you have to use something like <strong>fuzzy tokenizer</strong> - <em>java.text and other standart utils has too limited capabilities in such context</em>.</p>

<p><em>So, I'd advice you to use <strong>regular expressions</strong> (java.util.regex) and create own kind of tokenizer according to your needs.</em></p>

<p><strong>P.S.</strong>
According to your needs - you might create state-machine parser for recognizing templated parts in raw texts. You might see simple state-machine recognizer on the picture below (you can construct more advanced parser, which could recognize much more complex templates in text).</p>

<p><img src=""https://i.sstatic.net/VifsP.png"" alt=""enter image description here""></p>
",10,25,16969,2011-07-23 12:56:34,https://stackoverflow.com/questions/6800509/are-there-apis-for-text-analysis-mining-in-java
"How I classify a word of a text in things like names, number, money, date,etc?","<p>I did some questions about text-mining a week ago, but I was a bit confused and still, but now I know wgat I want to do.</p>

<p><strong>The situation:</strong> I have a lot of download pages with HTML content. Some of then can bean be a text from a blog, for example. They are not structured and came from different sites.</p>

<p><strong>What I want to do:</strong> I will split all the words with whitespace and I want to classify each one or a group of ones in some pre-defined itens like names, numbers, phone, email, url, date, money, temperature, etc.</p>

<p><strong>What I know:</strong> I know the concepts/heard about about Natural Language Processing, Named Entity Reconigzer, POSTagging, NayveBayesian, HMM, training and a lot of things to do classification, etc., but there is some different NLP libraries with differents classifiers and ways to do this and I don't know what use or what do.</p>

<p><strong>WHAT I NEED:</strong> I need some code example from a classifier, NLP, whatever, that can classify each word from a text separetely, and not a entire text. Something like this:</p>

<pre><code>//This is pseudo-code for what I want, and not a implementation

classifier.trainFromFile(""file-with-train-words.txt"");
words = text.split("" "");
for(String word: words){
    classifiedWord = classifier.classify(word);
    System.out.println(classifiedWord.getType());
}
</code></pre>

<p>Somebody can help me? I'm confused with various APIs, classifiers and algorithms.</p>
","java, nlp, classification, text-mining, named-entity-recognition","<p>You should try <a href=""http://incubator.apache.org/opennlp/"" rel=""nofollow"">Apache OpenNLP</a>. It is easy to use and customize. </p>

<p>If you are doing it for Portuguese there are information on how to do it on the project documentation using Amazonia Corpus. The types supported are:</p>

<p>Person, Organization, Group, Place, Event, ArtProd, Abstract, Thing, Time and Numeric.</p>

<ol>
<li><p>Download the OpenNLP and the <a href=""http://www.linguateca.pt/floresta/ficheiros/gz/amazonia.ad.gz"" rel=""nofollow"">Amazonia Corpus</a>. Extract both and copy the file <code>amazonia.ad</code> to the <code>apache-opennlp-1.5.1-incubating</code> folder.</p></li>
<li><p>Execute the TokenNameFinderConverter tool to convert the Amazonia corpus to the OpenNLP format:</p>

<pre><code>bin/opennlp TokenNameFinderConverter ad -encoding ISO-8859-1 -data amazonia.ad -lang pt &gt; corpus.txt
</code></pre></li>
<li><p>Train you model (Change the encoding to the encoding of the corpus.txt file, that should be your system default encoding. This command can take several minutes):</p>

<pre><code>bin/opennlp TokenNameFinderTrainer -lang pt -encoding UTF-8 -data corpus.txt -model pt-ner.bin -cutoff 20
</code></pre></li>
<li><p>Executing it from command line (You should execute only one sentence and the tokens should be separated):</p>

<pre><code>$ bin/opennlp TokenNameFinder pt-ner.bin 
Loading Token Name Finder model ... done (1,112s)
Meu nome é João da Silva , moro no Brasil . Trabalho na Petrobras e tenho 50 anos .
Meu nome é &lt;START:person&gt; João da Silva &lt;END&gt; , moro no &lt;START:place&gt; Brasil &lt;END&gt; . &lt;START:abstract&gt; Trabalho &lt;END&gt; na &lt;START:abstract&gt; Petrobras &lt;END&gt; e tenho &lt;START:numeric&gt; 50 anos &lt;END&gt; .
</code></pre></li>
<li><p>Executing it using the API:</p>

<pre><code>InputStream modelIn = new FileInputStream(""pt-ner.bin"");

try {
  TokenNameFinderModel model = new TokenNameFinderModel(modelIn);
}
catch (IOException e) {
  e.printStackTrace();
}
finally {
  if (modelIn != null) {
    try {
       modelIn.close();
    }
    catch (IOException e) {
    }
  }
}

// load the name finder
NameFinderME nameFinder = new NameFinderME(model);

// pass the token array to the name finder
String[] toks = {""Meu"",""nome"",""é"",""João"",""da"",""Silva"","","",""moro"",""no"",""Brasil"",""."",""Trabalho"",""na"",""Petrobras"",""e"",""tenho"",""50"",""anos"","".""};

// the Span objects will show the start and end of each name, also the type
Span[] nameSpans = nameFinder.find(toks);
</code></pre></li>
<li><p>To evaluate your model you can use 10-fold cross validation: (only available in 1.5.2-INCUBATOR, to use it today you need to use the SVN trunk) (it can take several hours)</p>

<pre><code>bin/opennlp TokenNameFinderCrossValidator -lang pt -encoding UTF-8 -data corpus.txt -cutoff 20
</code></pre></li>
<li><p>Improve the precision/recall by using the Custom Feature Generation (check documentation), for example by adding a name dictionary.</p></li>
</ol>
",5,1,8058,2011-08-01 02:55:44,https://stackoverflow.com/questions/6893858/how-i-classify-a-word-of-a-text-in-things-like-names-number-money-date-etc
Rattle loading String to Vector file from WEKA,"<p>I have been using WEKA to do some text classification work and I want to try
out R.</p>

<p>The problem is I cannot load the String to Vector ARFF files created by
WEKA's string parser into Rattle .</p>

<p><strong>Looking at the logs I get something like:</strong></p>

<pre><code>/Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,

: scan() expected 'a real', got '2281}'/
</code></pre>

<p><strong>My ARFF data file looks a bit like this:</strong></p>

<pre><code>@relation 'reviewData'

@attribute polarity {0,2}
.....
@attribute $$ numeric
@attribute we numeric
@attribute wer numeric
@attribute win numeric
@attribute work numeric

@data
{0 2,63 1,71 1,100 1,112 1,140 1,186 1,228 1}
{14 1,40 1,48 1,52 1,61 1,146 1}
{2 1,41 1,43 1,57 1,71 1,79 1,106 1,108 1,133 1,146 1,149 1,158 1,201 1}
{0 2,6 1,25 1,29 1,42 1,49 1,69 1,82 1,108 1,116 1,138 1,140 1,155 1}
..../
</code></pre>

<p>Any ideas how I can convert this into an R readable format?</p>

<p>Cheers!</p>
","r, file-io, machine-learning, weka, text-mining","<p>When you save the result of the <code>StringToWordVector</code> attribute filter, it will be saved as a <a href=""http://weka.wikispaces.com/ARFF+%28stable+version%29#Sparse%20ARFF%20files"" rel=""nofollow"">sparse ARFF</a> file.</p>

<p>You need to check if <a href=""http://rattle.togaware.com/"" rel=""nofollow"">Rattle</a> supports reading this format. If not, you can apply the <code>SparseToNonSparse</code> instance filter, which will convert it to a dense matrix format (file size will be much larger).</p>

<p><em>Example:</em> if the sparse data looks like:</p>

<h3>sparse.arff</h3>

<pre><code>@relation name
@attribute word1 numeric
@attribute word2 numeric
..
@attribute word10 numeric
@data
{0 1,3 3,8 1,9 1}
{2 2,5 1,8 1,9 1}
</code></pre>

<p>it will be converted to:</p>

<h3>nonsparse.arff</h3>

<pre><code>@relation name
@attribute word1 numeric
@attribute word2 numeric
..
@attribute word10 numeric
@data
1,0,0,3,0,0,0,0,1,1
0,0,2,0,0,1,0,0,1,1
</code></pre>
",0,1,600,2011-08-04 15:02:30,https://stackoverflow.com/questions/6943756/rattle-loading-string-to-vector-file-from-weka
RapidMiner Sentiment Analysis ,"<p>I have a collection of short messages classified as positive or negative which is saved in an ARFF file genereated in WEKA. I would like to move this data to RapidMiner for classification and processing purposes. </p>

<p>Being a complete newbie in RapidMiner does anyone have examples on how to build a classifier for these messages. The messages are in one file in the format:</p>

<pre><code>""MSG 1 TEXT"", categoryX
""MSG 2 TXT"", categoryX
</code></pre>

<p>Thanks!</p>
","string, machine-learning, weka, text-mining, rapidminer","<p>Lots of rapidminer videos here:</p>

<p><a href=""http://www.youtube.com/user/VancouverData?ob=0&amp;feature=results_main"" rel=""nofollow"">http://www.youtube.com/user/VancouverData?ob=0&amp;feature=results_main</a></p>

<p>There is a series on text mining</p>
",1,1,969,2011-08-05 11:57:20,https://stackoverflow.com/questions/6955870/rapidminer-sentiment-analysis
Word Net - Word Synonyms &amp; related word constructs - Java or Python,"<p>I am looking to use WordNet to look for a collection of like terms from a base set of terms. </p>

<p>For example, the word <strong><em>'discouraged'</em></strong> - potential synonyms could be: <code>daunted, glum, deterred, pessimistic</code>. </p>

<p>I also wanted to identify potential bi-grams such as; <code>beat down, put off, caved in</code> etc.</p>

<p>How do I go about extracting this information using Java or Python? Are there any hosted WordNet databases/web interfaces which would allow such querying?</p>

<p>Thanks!</p>
","java, python, nlp, text-mining, wordnet","<p>It is easiest to understand the WordNet data by looking 
at the Prolog files. They are documented here:</p>

<p><a href=""http://wordnet.princeton.edu/wordnet/man/prologdb.5WN.html"" rel=""nofollow"">http://wordnet.princeton.edu/wordnet/man/prologdb.5WN.html</a></p>

<p>WordNet terms are group into synsets. A synset is a maximal 
synonym set. Synsets have a primary key so that they can be used 
in semantic relationships. </p>

<p>So answering your first question, you can list the different 
senses and corresponding synonyms of a word as follows:</p>

<pre><code>Input X: Term
Output Y: Sense  
Output L: Synonyms in this Sense  

s_helper(X,Y) :- s(X,_,Y,_,_,_).  
?- setof(H,(s_helper(Y,X),s_helper(Y,H)),L).  
</code></pre>

<p>Example:</p>

<pre><code>?- setof(H,(s_helper(Y,'discouraged'),s_helper(Y,H),L).  
Y = 301664880,  
L = [demoralised, demoralized, discouraged, disheartened] ;  
Y = 301992418,  
L = [discouraged] ;  
No  
</code></pre>

<p>For the second part of your question, WordNet terms are 
sequences of words. So you can search this WordNet terms 
for words as follows:</p>

<pre><code>Input X: Word  
Output Y: Term

s_helper(X) :- s(_,_,X,_,_,_).  
word_in_term(X,Y) :- atom_concat(X,' ',H), sub_atom(Y,0,_,_,H).
word_in_term(X,Y) :- atom_concat(' ',X,H), atom_concat(H,' ',J), sub_atom(Y,_,_,_,J).
word_in_term(X,Y) :- atom_concat(' ',X,H), sub_atom(Y,_,_,0,H).
?- s_helper(Y), word_in_term(X,Y).
</code></pre>

<p>Example:</p>

<pre><code>?- s_helper(X), word_in_term('beat',X).  
X = 'beat generation' ;  
X = 'beat in' ;  
X = 'beat about' ;  
X = 'beat around the bush' ;  
X = 'beat out' ;  
X = 'beat up' ;  
X = 'beat up' ;  
X = 'beat back' ;  
X = 'beat out' ;  
X = 'beat down' ;  
X = 'beat a retreat' ;  
X = 'beat down' ;  
X = 'beat down' ;  
No
</code></pre>

<p>This would give you potential n-grams, but no so much
morphological variation. WordNet does also exhibit some
lexical relations, which could be useful. </p>

<p>But both Prolog queries I have given are not very efficient.
The problem is the lack of some word indexing. A Java
implementation could of course implement something better.
Just imagine something along:</p>

<pre><code>class Synset {  
    static Hashtable&lt;Integer,Synset&gt; synset_access;  
    static Hashtable&lt;String,Vector&lt;Synset&gt;&gt; term_access;  
}
</code></pre>

<p>Some Prolog can do the same, by a indexing directive, it is
possible to instruct the Prolog system to index on multiple
arguments for a predicate.</p>

<p>Putting up a web service shouldn't be that difficult, either
in Java or Prolog. Many Prologs systems easily allow embedding
Prolog programs in web servers, and Java champions servlets.</p>

<p>A list of Prologs that support web servers can be found here:</p>

<p><a href=""http://en.wikipedia.org/wiki/Comparison_of_Prolog_implementations#Operating_system_and_Web-related_features"" rel=""nofollow"">http://en.wikipedia.org/wiki/Comparison_of_Prolog_implementations#Operating_system_and_Web-related_features</a></p>

<p>Best Regards</p>
",3,6,2633,2011-08-08 15:10:32,https://stackoverflow.com/questions/6984264/word-net-word-synonyms-related-word-constructs-java-or-python
Latent Semantic Analysis concepts,"<p>I've read about using Singular Value Decomposition (SVD) to do Latent Semantic Analysis (LSA) in corpus of texts. I've understood how to do that, also I understand mathematical concepts of SVD. </p>

<p>But I don't understand why does it works applying to corpuses of texts <em>(I believe - there must be linguistical explanation)</em>. Could anybody explain me this with linguistic point of view?</p>

<p>Thanks</p>
","algorithm, nlp, data-mining, text-mining, latent-semantic-indexing","<p>There is no linguistic interpretation, there is no syntax involved, no handling of equivalence classes, synonyms, homonyms, stemming etc. Neither are any semantics involved, it is just words-occuring-together.
Consider a ""document"" as a shopping cart: it contains a combination of words (purchases). And words tend to occur together with ""related"" words. </p>

<p>For instance: The word ""drug"" can occur together with either of {love, doctor, medicine, sports, crime}; each will point you in a different direction. But combined with many other words in the document, your query will probably find  documents from a similar field.</p>
",14,13,3079,2011-08-14 21:49:26,https://stackoverflow.com/questions/7059954/latent-semantic-analysis-concepts
text mining in sqlite,"<p>I have sqlite database and need to find the most frequent words in it.
Example,</p>

<blockquote>
  <p>text<br/>
  table chair floor<br/>
  table chair<br/>
  table <br/></p>
</blockquote>

<p>Solution needed</p>

<blockquote>
  <p>word number<br/>
  table &nbsp;3<br/>
  chair &nbsp;2<br/>
  floor &nbsp;1<br/> </p>
</blockquote>

<p>The database is big (several Gb). I am looking for solution in SQL. Also maybe using C++ or other approach.</p>
","c++, sql, sqlite, data-mining, text-mining","<p><a href=""http://www.sqlite.org/fts3.html"" rel=""nofollow"">http://www.sqlite.org/fts3.html</a></p>

<p>Check out the  <em>Fts4aux - Direct Access to the Full-Text Index</em> section.</p>
",0,0,605,2011-08-24 07:33:00,https://stackoverflow.com/questions/7171954/text-mining-in-sqlite
WEKA - Classifying New Data from Java - IDF Transform,"<p>We are trying to implement a WEKA classifier from inside a Java program. So far so good, everything works well however when building the classifier from the training set in Weka GUI we used the StringToWordVector IDF transform to help improve classification accuracy. </p>

<p>How, from within Java for new instances do I calculate the IDF transform to set for each token value in the new instance before passing the instance to the classifier?</p>

<p>The basic code looks like this:</p>

<pre><code>Instances ins = vectorize(msg);
Instances unlabeled = new Instances(train,1);
Instance inst = new Instance(unlabeled.numAttributes());

String tmp = """";

for(int i=0; i &lt; ins.numAttributes(); i++) {
    tmp = ins.attribute(i).name();
    if(unlabeled.attribute(tmp)!=null)
      inst.setValue(unlabeled.attribute(tmp), 1.0); //TODO: Need to figure out the IDF transformed value to put here NOT 1!!
}

unlabeled.add(inst);

unlabeled.setClassIndex(classIdx);

.....cl.distributionForInstance(unlabeled.instance(i));
</code></pre>

<p>So how do I go about coding this so that I put the correct value in the new instance I want to classify?</p>

<p>Just to be clear the line <code>inst.setValue(unlabeled.attribute(tmp), 1.0);</code> needs to be changed from <code>1.0</code> to the IDF transformed number...</p>
","java, machine-learning, weka, text-mining, tf-idf","<p>You need to use FilteredClassifier for this purpose. The code snippet is :</p>

<pre>
<code>
    StringToWordVector  strWVector = new StringToWordVector();   
    filteredClassifier fcls = new FilteredClassifier();
    fcls.setFilter(strWVector);
    fcls.setClassifier(new SMO());
    fcls.buildClassifier(yourdata)
     //rest of your code 

</code>
</pre>

<p>This is much easier as you can pass your instances all at once.FilteredClassifier takes care of all other details. The code is not tested but it will get you started.</p>

<p>Edit : You can do in the following  way too. This is code snippet from weka tutorial See http://weka.wikispaces.com/Use+WEKA+in+your+Java+code#Filter-Filtering%20on-the-fly  Batch Mode for details </p>

<pre>
<code>
Instances train = ...   // from somewhere
 Instances test = ...    // from somewhere
 Standardize filter = new Standardize();
 filter.setInputFormat(train);  // initializing the filter once with training set
 Instances newTrain = Filter.useFilter(train, filter);  // configures the Filter based on train instances and returns filtered instances
 Instances newTest = Filter.useFilter(test, filter);    // create new test se
</code>
</pre>

<p>HTH</p>
",1,2,1682,2011-08-30 05:00:06,https://stackoverflow.com/questions/7238879/weka-classifying-new-data-from-java-idf-transform
Text Mining on huge list of strings,"<p>I have list of strings. (pretty big list of ids and strings scattered in 4-5 big files. around a GB each). These strings are formatted like this:</p>

<p>1,Hi</p>

<p>2,Hi How r u?</p>

<p>2,How r u?</p>

<p>3,where r u?</p>

<p>3,what does this mean</p>

<p>3,what it means</p>

<p>Now I want to do text mining on these strings and want to prepare a dendrogram which I want to display the strings in the following way</p>

<p>1-Hi</p>

<p>2-Hi How r u?</p>

<pre><code> ----How r u?
</code></pre>

<p>3-What does this mean?</p>

<pre><code> ----what it means?
</code></pre>

<p>3-Where are you?</p>

<p>This output is based on the similarities of strings following the comma after an id(suppose ID of a person who used those strings) for a particular person. If some other person used same words, then it should be grouped according to strings he used.</p>

<p>Now, it seems to be a simple task. But I want something to be done like this on hadoop/Mahout or something which can support huge set of data on clustered linux machines.
and also how should I approach this problem for the solution. I have tried different approaches in Mahout already, wherein i tried to create sequence file and seq2sparse vectores and then trying to do clustering. but it didn't work for me. Any help or pointers in the direction would be a great help. </p>

<p>Thanks &amp; Regards,
Atul</p>
","hadoop, data-mining, text-mining, mahout","<p>I think that what you really need is hierarchical clustering. There was <a href=""https://issues.apache.org/jira/browse/MAHOUT-19"" rel=""nofollow"">one implementation</a> proposed for Mahout, one is also implemented in <a href=""http://www.shogun-toolbox.org/"" rel=""nofollow"">Shogun Toolbox</a> (also designed for large-scale computation). But it's hard to guarantee that it will work, because the input seems to be hard.</p>
",2,2,925,2011-09-04 22:46:19,https://stackoverflow.com/questions/7302594/text-mining-on-huge-list-of-strings
VIM: How can I search match a line which doesn&#39;t have a particular character?,"<p>I have some lines like this from an ldiff file,</p>

<pre><code>dn: cn=dkalland_directs_ww,cn=org_groups,cn=beehive_groups,cn=groups,dc=oracle
,dc=com
businesscategory: open
cn: dkalland_directs_ww
description: Directs Group for daniel.kallander@oracle.com
displayname: dkalland_directs_ww
mail: dkalland_directs_ww@oracle.com
objectclass: top
objectclass: orclGroup
objectclass: groupOfUniqueNames
orclglobalid: modified
orclnormdn: cn=dkalland_directs_ww,cn=org_groups,cn=beehive_groups,cn=groups,d
c=oracle,dc=com
owner: cn=BHGRPADMIN_WW,L=AMER,DC=ORACLE,DC=COM
uniquemember: cn=mattias_tobiasson,dc=us,dc=oracle,dc=com
uniquemember: cn=mattias_joelson,dc=us,dc=oracle,dc=com
uniquemember: cn=markus_persson,dc=us,dc=oracle,dc=com 
...
</code></pre>

<p>Now as there are some lines which are continuation of the previous line. I want to join them back to their respective line.</p>

<p>What I am confused about is how can I search a line without the <code>"":""</code> character so that I can join it with previous line.</p>

<p>Plz help.</p>
","search, vim, design-patterns, vi, text-mining","<p>I believe you want to do a </p>

<pre><code>:v/:/-1j
</code></pre>

<p>The <code>v</code> command selects all lines that <em>don't</em> match the patterh <code>/:/</code>. The <code>-1</code> selects the lines one above. and the <code>j</code> joins this line with the next line (i.e. the one selected with the <code>v</code> command)</p>

<p><strong>Edit</strong> Benoit and dash-tom-bang have provided substantial improvements in their comments: the <code>1</code> is not necessary, since it is the default, and the <code>!</code> does not join the lines with a space. So, this leads to the following, better version:</p>

<pre><code>:v/:/-j!
</code></pre>
",11,3,2578,2011-09-08 10:07:14,https://stackoverflow.com/questions/7346317/vim-how-can-i-search-match-a-line-which-doesnt-have-a-particular-character
How to count the number of words in a paragraph and exclude some words (from a file)?,"<p>I've just started to learn Python so my question might be a bit silly. I'm trying to create a program that would:
<br>- import a text file (got it) 
<br>- count the total number of words (got it), 
<br>- count the number of words in a specific paragraph, starting with a specific phrase (e.g. ""P1"", ending with another participant ""P2"") and exclude these words from my word count. Somehow I ended up with something that counts the number of characters instead :/
<br>- print paragraphs separately (got it) 
<br>- exclude ""P1"" ""P2"" etc. words from my word count.</p>

<p>My text files look like this:
<em><br>P1: Bla bla bla.
<br>P2: Bla bla bla bla.
<br>P1: Bla bla.
<br>P3: Bla.</em></p>

<p>I ended up with this code:</p>

<pre><code>text = open (r'C:/data.txt', 'r')
lines = list(text)
text.close()
words_all = 0
for line in lines:
    words_all = words_all + len(line.split())
print 'Total words:   ', words_all

words_par = 0
for words_par in lines:
    if words_par.startswith(""P1"" or ""P2"" or ""P3"") &amp; words_par.endswith(""P1"" or ""P2"" or ""P3""):
        words_par = line.split()
    print len(words_par)
    print words_par.replace('P1', '') #doesn't display it but still counts
else:
    print 'No words'
</code></pre>

<p>Any ideas how to improve it?</p>

<p>Thanks</p>
","python, count, text-mining","<p>The first part is ok where you get the total words and print the result.</p>

<p>Where you fall down is here</p>

<pre><code>words_par = 0
for words_par in lines:
    if words_par.startswith(""P1"" or ""P2"" or ""P3"") &amp; words_par.endswith(""P1"" or ""P2"" or ""P3""):
        words_par = line.split()
    print len(words_par)
    print words_par.replace('P1', '') #doesn't display it but still counts
else:
    print 'No words'
</code></pre>

<p>The <i>words_par</i> is at first a string containing the line from the file.  Under a condition which will never be meet, it is turned into a list with the </p>

<pre><code>line.split()
</code></pre>

<p>expression.  This, if the expression </p>

<pre><code>words_par.startswith(""P1"" or ""P2"" or ""P3"") &amp; words_par.endswith(""P1"" or ""P2"" or ""P3"")
</code></pre>

<p>were to ever return True, would always be splitting the last line in your file, due to the last time it was assigned to was in the first part of your program where you did a full count of the number of words in the file.  That should really be </p>

<pre><code>words_par.split()
</code></pre>

<p>Also</p>

<pre><code>words_par.startswith(""P1"" or ""P2"" or ""P3"")
</code></pre>

<p>will always be </p>

<pre><code>words_par.startswith(""P1"")
</code></pre>

<p>since</p>

<pre><code>""P1"" or ""P2"" or ""P3""
</code></pre>

<p>always evaluates to the first one which is True, which is the first string in this case.  Read <a href=""http://docs.python.org/reference/expressions.html"" rel=""nofollow"">http://docs.python.org/reference/expressions.html</a> if you want to know more.</p>

<p>While we are at it, unless you are wanting to do bitwise comparisons avoid doing</p>

<pre><code>something &amp; something
</code></pre>

<p>instead do</p>

<pre><code>something and something
</code></pre>

<p>The first will evaluate both expressions no matter what the result of the first, where as the second will only evaluate the second expression if the first is True.  If you do this your code will operate a little more efficiently.</p>

<p>The </p>

<pre><code>print len(words_par)
</code></pre>

<p>on the next line is always going to counting the number of characters in the line, since the if statement is always going to evaluate to False and the word_par never got split into a list of words.</p>

<p>Also the else clause on the for loop will always be executed no matter whether the sequence is empty or not.  Have a look at <a href=""http://docs.python.org/reference/compound_stmts.html#the-for-statement"" rel=""nofollow"">http://docs.python.org/reference/compound_stmts.html#the-for-statement</a> for more information.</p>

<p>I wrote a version of what I think you are after as a example according to what I think you want.  I tried to keep it simple and avoid using things like list comprehension, since you say you are just starting to learn, so it is not optimal, but hopefully will be clear.  Also note I made no comments, so feel free to hassle me to explain things for you.</p>

<pre><code>words = None
with open('data.txt') as f:
    words = f.read().split()
total_words = len(words)
print 'Total words:', total_words

in_para = False
para_count = 0
para_type = None
paragraph = list()
for word in words:
  if ('P1' in word or
      'P2' in word or
      'P3' in word ):
      if in_para == False:
         in_para = True
         para_type = word
      else:
         print 'Words in paragraph', para_type, ':', para_count
         print ' '.join(paragraph)
         para_count = 0
         del paragraph[:]
         para_type = word
  else:
    paragraph.append(word)
    para_count += 1
else:
  if in_para == True:
    print 'Words in last paragraph', para_type, ':', para_count
    print ' '.join(paragraph)
  else:
    print 'No words'
</code></pre>

<p>EDIT:</p>

<p>I actually just noticed some redundant code in the example.  The variable para_count is not needed, since the words are being appended to the paragraph variable.  So instead of</p>

<pre><code>print 'Words in paragraph', para_type, ':', para_count
</code></pre>

<p>You could just do</p>

<pre><code>print 'Words in paragraph', para_type, ':', len(paragraph)
</code></pre>

<p>One less variable to keep track of.  Here is the corrected snippet.</p>

<pre><code>in_para = False
para_type = None
paragraph = list()
for word in words:
  if ('P1' in word or
      'P2' in word or
      'P3' in word ):
      if in_para == False:
         in_para = True
         para_type = word
      else:
         print 'Words in paragraph', para_type, ':', len(paragraph)
         print ' '.join(paragraph)
         del paragraph[:]
         para_type = word
  else:
    paragraph.append(word)
else:
  if in_para == True:
    print 'Words in last paragraph', para_type, ':', len(paragraph)
    print ' '.join(paragraph)
  else:
    print 'No words'
</code></pre>
",2,3,12269,2011-09-09 09:19:00,https://stackoverflow.com/questions/7359510/how-to-count-the-number-of-words-in-a-paragraph-and-exclude-some-words-from-a-f
How to sum up the word count for each person in a dialogue?,"<p><br>I'm starting to learn Python and I'm trying to write a program that would import a text file, count the total number of words, count the number of words in a specific paragraph (said by each participant, described by 'P1', 'P2' etc.), exclude these words (i.e. 'P1' etc.) from my word count, and print paragraphs separately.</p>
<p>Thanks to @James Hurford I got this code:</p>
<pre><code>words = None
with open('data.txt') as f:
   words = f.read().split()
total_words = len(words)
print 'Total words:', total_words

in_para = False
para_type = None
paragraph = list()
for word in words:
  if ('P1' in word or
      'P2' in word or
      'P3' in word ):
      if in_para == False:
         in_para = True
         para_type = word
      else:
         print 'Words in paragraph', para_type, ':', len(paragraph)
         print ' '.join(paragraph)
         del paragraph[:]
         para_type = word
  else:
    paragraph.append(word)
else:
  if in_para == True:
    print 'Words in last paragraph', para_type, ':', len(paragraph)
    print ' '.join(paragraph)
  else:
    print 'No words'
</code></pre>
<p>My text file looks like this:</p>
<blockquote>
<p>P1: Bla bla bla.</p>
<p>P2: Bla bla bla bla.</p>
<p>P1: Bla bla.</p>
<p>P3: Bla.</p>
</blockquote>
<p>The next part I need to do is summing up the words for each participant. I can only print them, but I don't know how to return/reuse them.</p>
<p>I would need a new variable with word count for each participant that I could manipulate later on, in addition to summing up all the words said by each participant, e.g.</p>
<pre><code>P1all = sum of words in paragraph
</code></pre>
<p>Is there a way to count &quot;you're&quot; or &quot;it's&quot; etc. as two words?</p>
<p>Any ideas how to solve it?</p>
","python, count, sum, nlp, text-mining","<p>Congrats on beginning your adventure with Python!  Not everything in this post might make sense right now but bookmark it and comeback to it if it seems helpful later.  Eventually you should try to move from scripting to software engineering, and here are a few ideas for you!  </p>

<p>With great power comes great responsibility, and as a Python developer you need to be more disciplined than other languages which don't hold your hand and enforce ""good"" design.</p>

<p>I find it helps to start with a top-down design.  </p>

<pre><code>def main():
    text = get_text()
    p_text = process_text(text)
    catalogue = process_catalogue(p_text)
</code></pre>

<p>BOOM!  You just wrote the whole program -- now you just need to back and fill in the blanks!  When you do it like this, it seems less intimidating.  Personally, I don't consider myself smart enough to solve very big problems, but I'm a pro at solving small problems.  So lets tackle one thing at a time.  I'm going to start with 'process_text'.</p>

<pre><code>def process_text(text):
    b_text = bundle_dialogue_items(text)   
    f_text = filter_dialogue_items(b_text)
    c_text = clean_dialogue_items(f_text)
</code></pre>

<p>I'm not really sure what those things mean yet, but I know that text problems tend to follow a pattern called ""map/reduce"" which means you perform and operation on something and then you clean it up and combine, so I put in some placeholder functions.  I might go back and add more if necessary.</p>

<p>Now let's write 'process_catalogue'.  I could've written ""process_dict"" but that sounded lame to me.</p>

<pre><code>def process_catalogue(p_text): 
    speakers = make_catalogue(c_text)
    s_speakers = sum_words_per_paragraph_items(speakers)
    t_speakers = total_word_count(s_speakers)
</code></pre>

<p>Cool.  Not too bad.  You might approach this different than me, but I thought it would make sense to aggregate the items, the count the words per paragraph, and then count all the words.  </p>

<p>So, at this point I'd probably make one or two little 'lib' (library) modules to back-fill the remaining functions.  For the sake you being able to run this without worrying about imports, I'm going to stick it all in one .py file, but eventually you'll learn how to break these up so it looks nicer.  So let's do this.  </p>

<pre><code># ------------------ #
# == process_text == #
# ------------------ #

def bundle_dialogue_items(lines):
    cur_speaker = None
    paragraphs = Counter()
    for line in lines:
        if re.match(p, line):
            cur_speaker, dialogue = line.split(':')
            paragraphs[cur_speaker] += 1
        else:
            dialogue = line

        res = cur_speaker, dialogue, paragraphs[cur_speaker]
        yield res


def filter_dialogue_items(lines):
    for name, dialogue, paragraph in lines:
        if dialogue:
            res = name, dialogue, paragraph
            yield res

def clean_dialogue_items(flines):
    for name, dialogue, paragraph in flines:
        s_dialogue = dialogue.strip().split()
        c_dialouge = [clean_word(w) for w in s_dialogue]
        res = name, c_dialouge, paragraph
        yield res
</code></pre>

<p>aaaand a little helper function</p>

<pre><code># ------------------- #
# == aux functions == #
# ------------------- #

to_clean = string.whitespace + string.punctuation
def clean_word(word):
    res = ''.join(c for c in word if c not in to_clean)
    return res
</code></pre>

<p>So it may not be obvious but this library is designed as a data processing pipeline.  There several ways to process data, one is pipeline processing and another is batch processing.  Let's take a look at batch processing.</p>

<pre><code># ----------------------- #
# == process_catalogue == #
# ----------------------- #

speaker_stats = 'stats'
def make_catalogue(names_with_dialogue):
    speakers = {}
    for name, dialogue, paragraph in names_with_dialogue:
        speaker = speakers.setdefault(name, {})
        stats = speaker.setdefault(speaker_stats, {})
        stats.setdefault(paragraph, []).extend(dialogue)
    return speakers



word_count = 'word_count'
def sum_words_per_paragraph_items(speakers):
    for speaker in speakers:
        word_stats = speakers[speaker][speaker_stats]
        speakers[speaker][word_count] = Counter()
        for paragraph in word_stats:
            speakers[speaker][word_count][paragraph] += len(word_stats[paragraph])
    return speakers


total = 'total'
def total_word_count(speakers):
    for speaker in speakers:
        wc = speakers[speaker][word_count]
        speakers[speaker][total] = 0
        for c in wc:
            speakers[speaker][total] += wc[c]
    return speakers
</code></pre>

<p>All these nested dictionaries are getting a little complicated.  In actual production code I would replace these with some more readable classes (along with adding tests and docstrings!!), but I don't want to make this more confusing than it already is!  Alright, for your convenience below is the whole thing put together.  </p>

<pre><code>import pprint
import re
import string
from collections import Counter

p = re.compile(r'(\w+?):')


def get_text_line_items(text):
    for line in text.split('\n'):
        yield line


def bundle_dialogue_items(lines):
    cur_speaker = None
    paragraphs = Counter()
    for line in lines:
        if re.match(p, line):
            cur_speaker, dialogue = line.split(':')
            paragraphs[cur_speaker] += 1
        else:
            dialogue = line

        res = cur_speaker, dialogue, paragraphs[cur_speaker]
        yield res


def filter_dialogue_items(lines):
    for name, dialogue, paragraph in lines:
        if dialogue:
            res = name, dialogue, paragraph
            yield res


to_clean = string.whitespace + string.punctuation


def clean_word(word):
    res = ''.join(c for c in word if c not in to_clean)
    return res


def clean_dialogue_items(flines):
    for name, dialogue, paragraph in flines:
        s_dialogue = dialogue.strip().split()
        c_dialouge = [clean_word(w) for w in s_dialogue]
        res = name, c_dialouge, paragraph
        yield res


speaker_stats = 'stats'


def make_catalogue(names_with_dialogue):
    speakers = {}
    for name, dialogue, paragraph in names_with_dialogue:
        speaker = speakers.setdefault(name, {})
        stats = speaker.setdefault(speaker_stats, {})
        stats.setdefault(paragraph, []).extend(dialogue)
    return speakers


def clean_dict(speakers):
    for speaker in speakers:
        stats = speakers[speaker][speaker_stats]
        for paragraph in stats:
            stats[paragraph] = [''.join(c for c in word if c not in to_clean)
                                for word in stats[paragraph]]
    return speakers


word_count = 'word_count'


def sum_words_per_paragraph_items(speakers):
    for speaker in speakers:
        word_stats = speakers[speaker][speaker_stats]
        speakers[speaker][word_count] = Counter()
        for paragraph in word_stats:
            speakers[speaker][word_count][paragraph] += len(word_stats[paragraph])
    return speakers


total = 'total'


def total_word_count(speakers):
    for speaker in speakers:
        wc = speakers[speaker][word_count]
        speakers[speaker][total] = 0
        for c in wc:
            speakers[speaker][total] += wc[c]
    return speakers


def get_text():
    text = '''BOB: blah blah blah blah
blah hello goodbye etc.

JERRY:.............................................
...............

BOB:blah blah blah
blah blah blah
blah.
BOB: boopy doopy doop
P1: Bla bla bla.
P2: Bla bla bla bla.
P1: Bla bla.
P3: Bla.'''
    text = get_text_line_items(text)
    return text


def process_catalogue(c_text):
    speakers = make_catalogue(c_text)
    s_speakers = sum_words_per_paragraph_items(speakers)
    t_speakers = total_word_count(s_speakers)
    return t_speakers


def process_text(text):
    b_text = bundle_dialogue_items(text)
    f_text = filter_dialogue_items(b_text)
    c_text = clean_dialogue_items(f_text)
    return c_text


def main():

    text = get_text()
    c_text = process_text(text)
    t_speakers = process_catalogue(c_text)

    # take a look at your hard work!
    pprint.pprint(t_speakers)


if __name__ == '__main__':
    main()
</code></pre>

<p>So this script is almost certainly overkill for this application, but the point is to see what (questionably) readable, maintainable, modular Python code might look like.  </p>

<p>Pretty sure output looks something like:</p>

<pre><code>{'BOB': {'stats': {1: ['blah',
                       'blah',
                       'blah',
                       'blah',
                       'blah',
                       'hello',
                       'goodbye',
                       'etc'],
                   2: ['blah',
                       'blah',
                       'blah',
                       'blah',
                       'blah',
                       'blah',
                       'blah'],
                   3: ['boopy', 'doopy', 'doop']},
         'total': 18,
         'word_count': Counter({1: 8, 2: 7, 3: 3})},
 'JERRY': {'stats': {1: ['', '']}, 'total': 2, 'word_count': Counter({1: 2})},
 'P1': {'stats': {1: ['Bla', 'bla', 'bla'], 2: ['Bla', 'bla']},
        'total': 5,
        'word_count': Counter({1: 3, 2: 2})},
 'P2': {'stats': {1: ['Bla', 'bla', 'bla', 'bla']},
        'total': 4,
        'word_count': Counter({1: 4})},
 'P3': {'stats': {1: ['Bla']}, 'total': 1, 'word_count': Counter({1: 1})}}
</code></pre>
",1,2,2367,2011-09-15 11:13:07,https://stackoverflow.com/questions/7429845/how-to-sum-up-the-word-count-for-each-person-in-a-dialogue
Data Mining situation,"<p>Suppose I have the data as mentioned below.</p>

<p>11AM user1 Brush</p>

<p>11:05AM user1 Prep Brakfast</p>

<p>11:10AM user1 eat Breakfast</p>

<p>11:15AM user1 Take bath</p>

<p>11:30AM user1 Leave for office</p>

<p>12PM user2 Brush</p>

<p>12:05PM user2 Prep Brakfast</p>

<p>12:10PM user2 eat Breakfast</p>

<p>12:15PM user2 Take bath</p>

<p>12:30PM user2 Leave for office</p>

<p>11AM user3 Take bath</p>

<p>11:05AM user3 Prep Brakfast</p>

<p>11:10AM user3 Brush</p>

<p>11:15AM user3 eat Breakfast</p>

<p>11:30AM user3 Leave for office</p>

<p>12PM user4 Take bath</p>

<p>12:05PM user4 Prep Brakfast</p>

<p>12:10PM user4 Brush</p>

<p>12:15PM user4 eat Breakfast</p>

<p>12:30PM user4 Leave for office</p>

<p>This data tell me about the daily routine of different people. From this data it seems user1 and user2 behave similarly (though there is a difference in time they perform the activity but they are following the same sequence). With the same reason, User3 and User4 behave similarly.
Now I have to group such users into different groups. In this example, group1- user1 and USer2 ... followed by group2 including user3 and user4</p>

<p>How should I approach this kind of situation. I am trying to learn data mining and this is an example I thought of as a data mining problem. I am trying to find an approach for the solution, but I can not think of one. I believe this data has the pattern in it. but I am not able to think of the approach which can reveal it.
Also, I have to map this approach on the dataset I have, which is pretty huge but similar to this :) The data is about logs stating occurrence of events at a time. And I want to find the groups representing similar sequence of events.</p>

<p>Any pointers would be appreciated.</p>
","data-mining, text-mining","<p>It looks like <strong>clustering</strong> on top of <strong>associating mining</strong>, more precisely <a href=""http://en.wikipedia.org/wiki/Apriori_algorithm"" rel=""nofollow"">Apriori</a> algorithm. Something like this: </p>

<ol>
<li>Mine all possible associations between actions, i.e. sequences Bush -> Prep Breakfast, Prep Breakfast -> Eat Breakfast, ..., Bush -> Prep Breakfast -> Eat Breakfast, etc. Every pair, triplet, quadruple, etc. you can find in your data. </li>
<li>Make separate attribute from each such sequence. For better performance add boost of 2 for pair attributes, 3 for triplets and so on. </li>
<li>At this moment you must have an attribute vector with corresponding boost vector. You can calculate feature vector for each user: set 1 * boost at each position in the vector if this sequence exists in user actions and 0 otherwise). You will get vector representation of each user. </li>
<li>On this vectors use clustering algorithm that fits your needs better. Each found class is the group you use. </li>
</ol>

<p><strong>Example:</strong> </p>

<p>Let's mark all actions as letters: </p>

<p>a - Brush<br>
b - Prep Breakfast<br>
c - East Breakfast<br>
d - Take Bath<br>
...  </p>

<p>Your <em>attributes</em> will look like</p>

<p>a1: a->b<br>
a2: a->c<br>
a3: a->d<br>
...<br>
a10: b->a<br>
a11: b->c<br>
a12: b->d<br>
...<br>
a30: a->b->c->d<br>
a31: a->b->d->c<br>
...  </p>

<p>User <em>feature vectors</em> in this case will be: </p>

<pre><code>attributes   = a1, a2, a3, a4, ..., a10, a11, a12, ..., a30, a31, ...
user1        =  1,  0,  0,  0, ...,   0,   1,   0, ...,   4,   0, ...
user2        =  1,  0,  0,  0, ...,   0,   1,   0, ...,   4,   0, ...
user3        =  0,  0,  0,  0, ...,   0,   0,   0, ...,   0,   0, ...
</code></pre>

<p>To compare 2 users some distance measure is needed. The simplest one is <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow"">cosine distance</a>, that is just value of cosine between 2 feature vectors. If 2 users have exactly the same sequence of actions, their similarity will equal 1. If they have nothing common - their similarity will be 0. </p>

<p>With distance measure use clustering algorithm (say, <a href=""http://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow"">k-means</a>) to make groups of users. </p>
",2,7,239,2011-09-30 17:23:00,https://stackoverflow.com/questions/7613863/data-mining-situation
unsupervised Named entity recognition (NER) with custom controlled vocabulary for crosslink-suggestions in Java,"<p>I'm looking for a Java library that can do Named entity recognition (NER) with a custom controlled vocabulary, without needing labeled training data first. I searched some on SE, but most questions are rather unspecific.</p>

<p>Consider the following use-case:</p>

<ul>
<li>an editor is inputting articles in a CMS (about 500 words).</li>
<li>the text may contain references (in plain text) to entities of a specific domain.  e.g: 
<ul>
<li>names of points of interest, like bars, restaurants, as well as neighborhoods, etc. </li>
</ul></li>
<li>a controlled vocabulary of these entities exist (about 5.000 entities) . 
<ul>
<li>I imagine an entity to be a -tuple in the vocabulary</li>
</ul></li>
<li>after finishing the text, the user should be able to save the document. </li>
<li>This triggers the workflow to scan the piece of text against the vocabulary, by comparing against the name of the entity. It's not required to have a 100% match: 97% on Jarao-winkler or whatever (I'm not familiar with what algo's NER uses) may be enough, I need this to be configurable. </li>
<li>Hits are returned to the controller server-side. This in return returns JSON to the client containing  of the entities, which are represented as suggested crosslinks to the editor. </li>
</ul>

<p>Ideally, I'm looking for a project that uses NRE to suggests crosslinks within a CMS-environment to piggyback on. (I'm sure plugins for wordpress exist for example) not so sure if something similar exists in Java. </p>

<p>All other more general pointers to NRE-libraries which work with controlled custom vocabularies are welcome as well.  </p>
","java, information-retrieval, text-mining, named-entity-recognition","<p>For people looking this up in the future: </p>

<p>""Approximate Dictionary-Based Chunking""
see: <a href=""http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html"" rel=""nofollow"">http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html</a></p>

<p>(URL edited.)</p>
",3,6,1681,2011-10-05 15:02:39,https://stackoverflow.com/questions/7663428/unsupervised-named-entity-recognition-ner-with-custom-controlled-vocabulary-fo
About data mining by using twitter data,"<p>I plan to write a thesis about using sentiment information to enhance the predictivity of some financial trading model for currency. </p>

<p>The sentiment data should be twitter threads including some keyword, like ""EUR.USD"". And I will filter out some sentiment words to identify the sentiment. Simple idea. Then we try to see whether here is any relation between the degree of sentiment and the movement of EUR.USD.</p>

<p>My big concern is on twitter data. As we all know that the twitter set up the limit to see the history data. You could only browser back for like 5 days. It is not enough since our strategy based on daily sentiment.</p>

<p>I noticed that google have some fantastic thing like timeline about the twitter updates: <a href=""http://www.readwriteweb.com/archives/googles_twitter_timeline_lets_you_explore_the_past.php"" rel=""nofollow"">http://www.readwriteweb.com/archives/googles_twitter_timeline_lets_you_explore_the_past.php</a></p>

<p>But first of all, I am in Switzerland and seems I have no such function on my google which is too smart to identify my location and may block some US google version function like this. Secondly, even I could see some fancy interactive google timeline control on my firefox, How could I dig out data from my query and save them? Does google supply such api?</p>
","twitter, text-mining","<p>The Google service you mentioned has shut down recently so you won't be able to use it. (<a href=""http://www.searchenginejournal.com/google-realtime-shuts-down-as-twitter-deal-expires/31007/"" rel=""nofollow"">http://www.searchenginejournal.com/google-realtime-shuts-down-as-twitter-deal-expires/31007/</a>)</p>

<p>If you need a longer timespan of data to analyze I see the following options:</p>

<ul>
<li>pay for historical data :) (<a href=""https://dev.twitter.com/docs/twitter-data-providers"" rel=""nofollow"">https://dev.twitter.com/docs/twitter-data-providers</a>)</li>
<li>if you don't want to pay, you need to fetch tweets containing EUR/USD whatever else (you could use the streaming API for this) and store them somehow. Run this service for a while (if possible) and you'll have more than just 5 days of data.</li>
</ul>
",2,0,805,2011-10-15 00:17:39,https://stackoverflow.com/questions/7774838/about-data-mining-by-using-twitter-data
Using Regexpr with $,"<p>Just a quick question, does anyone know how to use regexpr with ""\$"" ?  Essentially, I want to parse out strings and figure out what numeric value came after the \$ (for example ""Get $50 off on purchases of new bed frames"").</p>
",text-mining,"<p>In regular expressions, <code>$</code> would denote the end of the string, and so if you want to match an actual $, you'd need to ""escape"" it, like <code>\$</code>.</p>

<p>In <code>grep</code> in R, you need to use <code>\\</code>, as follows:</p>

<pre><code>x &lt;- ""Get $50 off on purchases of new bed frames""
grep(""\\$\\d+"", x)
</code></pre>
",4,0,77,2011-10-15 12:59:51,https://stackoverflow.com/questions/7779065/using-regexpr-with
URL path similarity/string similarity algorithm,"<p>My problem is that I need to compare URL paths and deduce if they are similar. Below I provide example data to process:</p>

<pre><code># GROUP 1
/robots.txt

# GROUP 2
/bot.html

# GROUP 3
/phpMyAdmin-2.5.6-rc1/scripts/setup.php
/phpMyAdmin-2.5.6-rc2/scripts/setup.php
/phpMyAdmin-2.5.6/scripts/setup.php
/phpMyAdmin-2.5.7-pl1/scripts/setup.php
/phpMyAdmin-2.5.7/scripts/setup.php
/phpMyAdmin-2.6.0-alpha/scripts/setup.php
/phpMyAdmin-2.6.0-alpha2/scripts/setup.php

# GROUP 4
//phpMyAdmin/
</code></pre>

<p>I tried Levenshtein distance to compare, but for me is not enough accurate. I do not need 100% accurate algorithm, but I think 90% and above is a must.</p>

<p>I think that I need some sort of classifier, but the problem is that each portion of new data can containt path that should be classified to the new unknown class.</p>

<p>Could you please direct me to the right thoutht?</p>

<p>Thanks</p>
","algorithm, data-mining, classification, levenshtein-distance, text-mining","<p>When checking @jakub.gieryluk suggestion I accidentally have found solution that satisfy me - ""Hobohm clustering algorithm, originally devised to reduce redundancy of biological sequence data sets.""</p>

<p>Tests of PERL library implemented by <a href=""http://search.cpan.org/~brunov/String-Cluster-Hobohm-0.112890/lib/String/Cluster/Hobohm.pm"" rel=""nofollow"">Bruno Vecchi</a> gave me really good results. The only problem is that I need Python implementation, but I belive that I can either find one on the Internet or reimplement code by myself.</p>

<p>Next thing is that I have not checked active learning ability of this algorithm yet ;)</p>
",1,3,3820,2011-10-18 14:43:24,https://stackoverflow.com/questions/7809134/url-path-similarity-string-similarity-algorithm
What are the basic algorithms for text mining?,"<p>I'm trying to do an application for mining some texts from the web, but I'm not sure of what is the best way to perform text mining.</p>

<p>What I want with this question is know about what are the most used techniques/algorithms to perform text mining and do some information retrieval in documents (not for indexing). </p>
","nlp, information-retrieval, text-mining","<p><em>Text mining</em> is a rather broad term, it roughly means machine learning applied to text. Common techniques include <em>k</em>-means clustering, Naive Bayes and linear SVM classification, tf-idf vectorization, <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Singular_value_decomposition"" rel=""noreferrer"">SVD</a> (called <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Latent_semantic_analysis"" rel=""noreferrer"">LSA</a> when applied to text), latent Dirichlet allocation. So, performing ""some text mining"" might mean just about anything, just like doing ""some information retrieval"".</p>

<p>See Bing Liu's book <em>Web Data Mining</em> for a good intro to the field.</p>
",7,-2,9530,2011-11-05 01:43:45,https://stackoverflow.com/questions/8017572/what-are-the-basic-algorithms-for-text-mining
package tm. problems with kmeans,"<p>I have a question about k-means clustering in R. Actually i'm doing everything according to this <a href=""http://www.jstatsoft.org/v25/i05/paper"" rel=""nofollow"" title=""article"">article</a>. Everything is based on examples within the tm package so it's required no data import. acq contains 50 documents and crude 20 documents.</p>

<pre><code>library(tm)
data(""acq"")
data(""crude"")
ws &lt;- c(acq, crude)
wsTDM &lt;- Data(TermDocumentMatrix(ws)) #First problem here
wsKMeans &lt;- kmeans(wsTDM, 2)
wsReutersCluster &lt;- c(rep(""acq"", 50), rep(""crude"", 20))
cl_agreement(wsKMeans, as.cl_partition(wsReutersCluster), ""diag"")

Error in lapply(X, FUN, ...) : 
(list) object cannot be coerced to type 'integer'
</code></pre>

<p>I actually want to create cross agreement matrix. But this article was wrote in 2008 since then a lot have changed.  The Data function is only available in RSurvey package, but i'm kinda doubt is it the same. And i think that the main problem is that TermDocumentMatrix was S4 class and now it's S3. I know it's possibly to do this having text only. But I wanna do it like this since in TDM it's possible to remove stopwords, punct, etc for better results. So if someone has any solution that would be terrific.</p>
","r, statistics, cluster-analysis, k-means, text-mining","<p>The TDM is stored as a sparse matrix, as described in <code>?TermDocumentMatrix</code>. This can also be seen from just inspecting the object like <code>str(wsTDM)</code>. That old <code>Data()</code> function was just a way to access the contents as a regular matrix. It is not needed anymore. Just do <code>kmeans(wsTDM, 2)</code> and you'll see that the output is as expected, with clusters identified for 2775 observations (terms) on 70 features (documents). Good luck!</p>
",3,2,938,2011-11-09 23:18:24,https://stackoverflow.com/questions/8073106/package-tm-problems-with-kmeans
Detect text language in R,"<p>I have a list of tweets and I would like to keep only those that are in English.</p>
<p>How can I do this?</p>
","r, text-mining","<p>The <a href=""http://cran.r-project.org/web/packages/textcat/index.html""><code>textcat</code></a> package does this. It can detect 74 'languages' (more properly, language/encoding combinations), more with other extensions. Details and examples are in this freely available article:</p>

<p>Hornik, K., Mair, P., Rauch, J., Geiger, W., Buchta, C., &amp; Feinerer, I. <a href=""http://www.jstatsoft.org/v52/i06/"">The textcat Package for n-Gram Based Text Categorization</a> in R. Journal of Statistical Software, 52, 1-17. </p>

<p>Here's the abstract:</p>

<blockquote>
  <p>Identifying the language used will typically be the first step in most
  natural language processing tasks. Among the wide variety of language
  identification methods discussed in the literature, the ones employing
  the Cavnar and Trenkle (1994) approach to text categorization based on
  character n-gram frequencies have been particularly successful. This
  paper presents the R extension package textcat for n-gram based text
  categorization which implements both the Cavnar and Trenkle approach
  as well as a reduced n-gram approach designed to remove redundancies
  of the original approach. A multi-lingual corpus obtained from the
  Wikipedia pages available on a selection of topics is used to
  illustrate the functionality of the package and the performance of the
  provided language identification methods.</p>
</blockquote>

<p>And here's one of their examples:</p>

<pre><code>library(""textcat"")
textcat(c(
  ""This is an English sentence."",
  ""Das ist ein deutscher Satz."",
  ""Esta es una frase en espa~nol.""))
[1] ""english"" ""german"" ""spanish"" 
</code></pre>
",50,46,32665,2011-11-10 11:11:14,https://stackoverflow.com/questions/8078604/detect-text-language-in-r
hashes of ngrams: document fingerprinting,"<p>I am trying to implement the winnowing algorithm for document fingerprinting in R. </p>

<p>Here the reference <a href=""http://www.ida.liu.se/~TDDC03/oldprojects/2005/final-projects/prj10.pdf"" rel=""nofollow"">http://www.ida.liu.se/~TDDC03/oldprojects/2005/final-projects/prj10.pdf</a></p>

<p>My question: </p>

<p>how do I get hashes of n-gram and how do I select those </p>

<pre><code>nGrams &lt;- c(""adoru"", ""dorun"", ""orunr"", ""runru"", ""unrun"", ""nrunr"" ,""runru"",
  ""unrun"",""nruna"", ""runad"", ""unado"", ""nador"", ""adoru"", ""dorun"", ""orunr"" ,""runru"" ,
  ""unrun"")
</code></pre>
","r, hash, text-mining, fingerprinting","<p>It seems as though</p>

<pre><code>library(digest)
v &lt;- sapply(nGrams,digest,algo=""crc32"")
uv &lt;- unique(v)
(as.integer(as.hexmode(uv))-1) %% 4 == 0
</code></pre>

<p>would be a good start. (CRC32 is always odd, so subtracting 1 is necessary.) </p>
",1,0,462,2011-11-12 13:29:48,https://stackoverflow.com/questions/8104719/hashes-of-ngrams-document-fingerprinting
Algorithms/methods to compile forum discussions into categorized articles or information?,"<p>I'm designing and coding a knowledge based community sharing system (forum, Q&amp;A, article sharing between students, professors and experts) in Java, for the web.</p>

<p>I need to use some data mining/text processing techniques/algorithms to analyse the discussions between experts and students (discussions are categorized using tags) and create proper notes and compilations on specific similar topics.</p>

<p>I'm not an expert regarding such algorithms or tools available. It'd be great if anyone can provide me with some pointers or explain how I can proceed with this problem.</p>

<p>Thanks!!</p>
","java, data-mining, text-mining","<p>For categorization of articles you can use LSA (Latent Semantic Analysis) technique .</p>

<p>You can check these tools for text processing.</p>

<ol>
<li><p><a href=""http://alias-i.com/lingpipe/index.html"" rel=""nofollow"">LingPipe</a> : Tool kit for processing text.</p></li>
<li><p><a href=""http://lucene.apache.org/java/docs/index.html"" rel=""nofollow"">Lucene</a> : Text mining </p></li>
<li><p><a href=""http://lucene.apache.org/solr/"" rel=""nofollow"">Solr</a> : Powerful text search tool</p></li>
</ol>
",0,0,212,2011-11-27 17:06:35,https://stackoverflow.com/questions/8287314/algorithms-methods-to-compile-forum-discussions-into-categorized-articles-or-inf
"NLP for java, which toolkit should I use?","<p>I'm working on a project that needs to count the occurrence of every word of a txt file.
For example, I have a text file like this:</p>

<blockquote>
  <p>What Silver Lake Looks For in IPO Candidates
  3 Companies Crushed by Earnings: Apple, Cirrus Logic, IBM
  IBM's Palmisano: How You Get To Be A 100-Year Old Company</p>
</blockquote>

<p>If there are 3 sentences shown above in the file and I want to calculate every word's occurrence. Here, Companies and company should be considered as the same word ""company""(lowercase), so the total occurrence for the word ""company"" is 2. </p>

<p>Is there any NLP toolkit for java that can tell two words like ""families"" and ""family"" are actually from the same word ""family""?</p>

<p>I'll count the occurrence of every word to further do the Naive Bayes training, so it's very important to get the accurate numbers of occurrences of each word.</p>
","java, text, nlp, text-mining","<p><a href=""http://lucene.apache.org/java/docs/index.html"" rel=""nofollow"">Apache Lucene</a> and <a href=""http://opennlp.sourceforge.net/"" rel=""nofollow"">OpenNLP</a> provide good stemming algorithm implementations. You can review and use the best one that suites you. I've been using Lucene for my projects.</p>
",4,1,1155,2011-12-15 04:54:25,https://stackoverflow.com/questions/8515281/nlp-for-java-which-toolkit-should-i-use
Counting syllables,"<p>I'm looking to assign some different readability scores to text in R such as the Flesh Kincaid.  </p>

<p>Does anyone know of a way to segment words into syllables using R?  I don't necessarily need the syllable segments themselves but a count.</p>

<p>so for instance:</p>

<pre><code>x &lt;- c('dog', 'cat', 'pony', 'cracker', 'shoe', 'Popsicle')
</code></pre>

<p>would yield:
1, 1, 2, 2, 1, 3</p>

<p>Each number corresponding the the number of syllables in the word.</p>
","r, text-mining","<p>Some tools for NLP are available here:</p>

<p><a href=""http://cran.r-project.org/web/views/NaturalLanguageProcessing.html"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/views/NaturalLanguageProcessing.html</a></p>

<p>The task is non-trivial though.  More hints (including an algorithm you could implement) here:</p>

<p><a href=""https://stackoverflow.com/questions/405161/detecting-syllables-in-a-word"">Detecting syllables in a word</a></p>
",5,19,4385,2011-12-17 23:36:03,https://stackoverflow.com/questions/8553240/counting-syllables
need an idea about text mining for mining data from bulk of files,"<p>I am new for data mining. I am doing my B.Tech final year, my final year project title is ""Extraction and analysis of faculty performance of management discipline from student feedback using text mining"". Here we will have number of files which contains feedback given by students, each student will have one single file. From all these files we have to retrieve useful information.</p>

<p>can any one suggest me how to start, what are the tools to be used?
what are the technologies to be used? 
I am familiar with JAVA(jse), can i achieve this using java programming language, how?</p>

<p>regards...
Upendra.S</p>
","data-mining, text-mining","<p>Some ideas:</p>

<ul>
<li>which are the most frequently used words or phrases?</li>
<li>which words often co-occur (association analysis)</li>
<li>word/phrase frequency by student grade (which words do the top and bottom students use in their prof reviews?)</li>
<li>word/phrase frequency by faculty rating (which words are associated with the score that students give the faculty member?)</li>
<li>word/phrase frequency by faculty tenure (which words are associated with new and older profs?)</li>
</ul>

<p>here is my five part series on text mining with rapidminer:</p>

<p><a href=""http://vancouverdata.blogspot.com/2010/11/text-analytics-with-rapidminer-loading.html"" rel=""nofollow"">http://vancouverdata.blogspot.com/2010/11/text-analytics-with-rapidminer-loading.html</a></p>
",3,1,1472,2011-12-21 15:59:48,https://stackoverflow.com/questions/8592684/need-an-idea-about-text-mining-for-mining-data-from-bulk-of-files
Identification of an important document,"<p>I have a set of text documents in java .  I have to identify the most important document (just as what an expert would identify) using a computer.</p>

<p>eg.  I have 10 books on java , the system identifies Java complete reference as the most important document or the most relevant.(based on similarities with the wikipedia page about java)  </p>

<p>One method would be to have a reference document and find similarities between this document and the set of documents at hand (as mentioned in the previous example). And provide a result saying the one which has maximum similarity is the most important docuemnt . </p>

<p>I want to identify other more efficient methods of performing this. 
please suggest other methods for finding the relevant document (in a unsupervised way if possible) . </p>
",text-mining,"<p>I think another mechanism would be, have a dictionary of words and ranking map associated with each document. </p>

<p>For example, in Java complete reference book case, there will be a dictionary of keywords and its ranking.</p>

<p>Java-10
J2ee-5
J2SDK-10
Java5-10 etc.,</p>

<p>Note:If your documents are dynamic streams and names also dynamic, I am not sure how to handle it.</p>
",0,0,152,2011-12-26 04:51:18,https://stackoverflow.com/questions/8632990/identification-of-an-important-document
ranking documents from my database,"<p>Everytime i seach for papers / documents on document ranking or text classification i am redirected to pages about related to web pages, but i want to rank documents in a repository. </p>

<p>Can someone suggest a book/paper that talks about ranking documents which are present in a database of documents (Every search result returns page rank or someother algorithm pertaining to the internet)</p>

<p>My aim is to rank the documents from my database based on their relevance to a query or based on a user's reference document(No internet or web sites involved)</p>
","ranking, text-mining","<p>You should probably stick to an existent document ranking library or database. Most SQL databases have a full-text search mechanism. If you are working with text indexing only, you might as well look into many text searching/document ranking solutions, such as Lucene (there are many others around as well).<br>
If you want to understand how ranking algorithms work, it could be worth taking a look at <a href=""http://en.wikipedia.org/wiki/Tf-idf"" rel=""nofollow"">http://en.wikipedia.org/wiki/Tf-idf</a> and <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow"">http://en.wikipedia.org/wiki/Cosine_similarity</a>.<br>
If you want to understand how indexing such information to make searching efficient, you should look at <a href=""http://en.wikipedia.org/wiki/Inverted_index"" rel=""nofollow"">http://en.wikipedia.org/wiki/Inverted_index</a>.<br>
Please note, however, that I am no expert on the matter, and many other approaches exist, although they shouldn't be too different in its basic form.<br>
Using a system that does this dirty job for you will not only save you time, but will also give you more robust and reliable querying capabilities then you would be able to implement on your own in a decent amount of time.</p>
",0,1,241,2011-12-29 03:17:26,https://stackoverflow.com/questions/8663649/ranking-documents-from-my-database
search engine to search documents from a local database,"<p>I am looking for a search engine that finds documents from my local database on the basis of a search query. The process does not involve any web pages. They include only a set of text documents(pdf and doc files). 
   Please suggest a few efficient search engines for this process.</p>
","search-engine, text-mining","<p>I would try <a href=""http://lucene.apache.org/solr/"" rel=""nofollow"">Solr</a> first (it is built on top of Lucene). Comes with a nice web based admin tool also. </p>
",1,1,2442,2012-01-02 03:55:51,https://stackoverflow.com/questions/8697273/search-engine-to-search-documents-from-a-local-database
Explicit Semantic Analysis,"<p>I came across this term called 'Explicit Semantic Analysis ' which uses Wikipedia as a reference and finds the similarity in documents and categorizes them into classes (correct me if i am wrong). </p>

<p>The link i came across is <a href=""http://www.cs.technion.ac.il/~gabr/resources/code/esa/esa.html"" rel=""noreferrer"">here</a> </p>

<p>I wanted to learn more about it. Please help me out with it !</p>
","text, similarity, text-mining","<p>This explicit semantic analysis works on similar lines as <a href=""http://en.wikipedia.org/wiki/Semantic_similarity"" rel=""nofollow"">semantic similarity</a> . I got hold of this <a href=""http://en.wikipedia.org/wiki/Semantic_similarity"" rel=""nofollow"">link</a> which provides a clear example of ESA</p>
",3,5,1425,2012-01-03 03:33:13,https://stackoverflow.com/questions/8707624/explicit-semantic-analysis
What platform / tool / software / language should i use for text mining?,"<p>I am a begineer to the field of text mining . 
I need to perform work on the document similarity .I aim at comparing two documents and then providing the similarity between them in terms of a number. I have  read a lot of theory about this . I am planning to start with the cosine similarity </p>

<p>Can any of you help me with these basics questions : 
1. What platform ? (windows/linux)
2. What tool (People talk about weka / mahout / hadoop ) - i have no idea on what to use 
3. What language ? 
Some questions might sound absurd , but i have to start from scratch and i need some help </p>
","hadoop, weka, similarity, mahout, text-mining","<p>For software, I highly recommend RapidMiner, which you can grab from <a href=""http://rapid-i.com"" rel=""nofollow"">http://rapid-i.com</a>. Some quick pros:</p>

<ul>
<li>Open source and implemented in Java (works on any platform)</li>
<li>Intuitive graphical ""operator pipeline"" for hundreds of data mining tasks</li>
<li>Excellent text mining support. See this <a href=""http://www.youtube.com/watch?v=TyZjom46yGA"" rel=""nofollow"">video tutorial</a></li>
</ul>

<p>In my experience data mining requires some real discipline to achieve desirable results. RapidMiner should help.</p>
",2,0,994,2012-01-05 04:53:44,https://stackoverflow.com/questions/8737828/what-platform-tool-software-language-should-i-use-for-text-mining
How to recover currency information from broken data set?,"<p>This is so not my area, so I apologize if this is not in scope for this stack.</p>

<p>I am cleaning up (for personal entertainment and making visualization to share with others) survey data (<a href=""http://s.wordpress.org/resources/survey/wp2011-survey.tar.gz"" rel=""nofollow noreferrer"">download, 9MB</a>) that went through some manipulations to be anonymized before getting released to the public.</p>

<p>One of the questions was about hourly payment rate and allowed free form text answer. Some of those answers got badly broken characters, two most common cases shown in image below:</p>

<p><img src=""https://i.sstatic.net/0xgw9.png"" alt=""enter image description here""></p>

<p>I would hate to discard those answers, but I am at loss how to revert them to meaningful state.</p>

<ol>
<li><p>Ask for better data dump - poked related people about it, but not too hopeful.</p></li>
<li><p>Try to determine which characters ended up this way. Dealing with encodings is always troublesome and these don't look like any broken characters I ever seen before so I have no idea where to start and if there are tools available to help with this. That might not even be a valid characters or currency symbols at all.</p></li>
<li><p>Try to match broken characters to valid currency characters. I strongly suspect one of the two might be &euro; character and other might be &pound; given that survey was slanted towards English-speaking countries. However will I be able to reliably back up such guess by relative quantity of character to other answers? Unfortunately geo data was not provided, so I can't match answers to countries.</p></li>
</ol>
",text-mining,"<p>It was confirmed that this was caused by export bug in survey software and characters do correspond to euro and pound.</p>

<blockquote>
  <p>As you suspected.</p>
  
  <p><img src=""https://i.sstatic.net/6WIzw.png"" alt=""enter image description here""></p>
  
  <p>it's a <a href=""https://twitter.com/#!/Polldaddy"" rel=""nofollow noreferrer"">@Polldaddy</a> export to csv bug</p>
  
  <p><em><a href=""https://twitter.com/#!/pjadavies/status/155479113770082306"" rel=""nofollow noreferrer"">Pete Davies</a></em></p>
</blockquote>
",0,0,33,2012-01-06 23:44:44,https://stackoverflow.com/questions/12913501/how-to-recover-currency-information-from-broken-data-set
R Regular Expression Lookbehind,"<p>I have a vector filled with strings of the following format: <code>&lt;year1&gt;&lt;year2&gt;&lt;id1&gt;&lt;id2&gt;</code></p>

<p>the first entries of the vector looks like this:</p>

<pre><code>199719982001
199719982002
199719982003
199719982003
</code></pre>

<p>For the first entry we have: year1 = 1997, year2 = 1998, id1 = 2, id2 = 001. </p>

<p>I want to write a regular expression that pulls out year1, id1, and the digits of id2 that are not zero. So for the first entry the regex should output: 199721.</p>

<p>I have tried doing this with the stringr package, and created the following regex:</p>

<pre><code>""^\\d{4}|\\d{1}(?&lt;=\\d{3}$)""
</code></pre>

<p>to pull out year1 and id1, however when using the lookbehind i get a ""invalid regular expression"" error. This is a bit puzzling to me, can R not handle lookaheads and lookbehinds?</p>
","regex, r, package, text-mining","<p>Since this is fixed format, why not use substr? <code>year1</code> is extracted using <code>substr(s,1,4)</code>, <code>id1</code> is extracted using <code>substr(s,9,9)</code> and the <code>id2</code> as <code>as.numeric(substr(s,10,13))</code>. In the last case I used <code>as.numeric</code> to get rid of the zeroes.</p>
",9,7,11109,2012-01-12 11:41:31,https://stackoverflow.com/questions/8834872/r-regular-expression-lookbehind
Is there an algorithm for determining the relevance of a text to a theme?,"<p>I want to know what can be used to determine the relevance of a page for a theme like games, movies, etc.</p>

<p>Is there some research in this area or is there only counting how many times some relevant words appear?</p>
","nlp, information-retrieval, text-mining, relevance","<p>The common choice is supervised document classification on bag of words (or bag of n-grams) features, preferably with tf-idf weighting.</p>

<p>Popular algorithms include Naive Bayes and (linear) SVMs.</p>

<p>For this approach, you'll need labeled training data, i.e. documents annotated with relevant themes.</p>

<p>See, e.g., <a href=""http://www-nlp.stanford.edu/IR-book/"" rel=""noreferrer""><em>Introduction to Information Retrieval</em></a>, chapters 13-15.</p>
",5,1,1532,2012-01-16 12:49:02,https://stackoverflow.com/questions/8880265/is-there-an-algorithm-for-determining-the-relevance-of-a-text-to-a-theme
Forums for text mining/similarity,"<p>I am aware that stackoverflow is a programming oriented forum that helps coders to solve bugs . But i have plenty of conceptual doubts in text mining techniques / information retrieval and semantic similarity . </p>

<p>Please suggest a forum where they discuss these concepts so that i can putforth my doubts in that! </p>
","text-mining, forums","<p>Maybe <a href=""https://stats.stackexchange.com/"">https://stats.stackexchange.com/</a></p>
",3,3,668,2012-01-23 03:17:03,https://stackoverflow.com/questions/8966853/forums-for-text-mining-similarity
Text classification extract tags from text,"<p>I have a lucene index with a lot of text data, each item has a description, I want to extract the more common words from the description and generate tags to classify each item based on the description, is there a lucene.net library for doing this or any other library for text classification?</p>
","c#, .net, data-mining, text-mining","<p>No, lucene.net can make search, index, text normalization, ""find more like this"" funtionalty, but not a text classification.</p>

<p>What to suggest to you depends from your requirements. So, maybe more description needed.
But, generally, easiest way try to use external services. All external services have REST API, and it's very easy to interact with it using C#.</p>

<p>From external services:</p>

<ul>
<li><a href=""http://opencalais.com/"" rel=""nofollow"">Open Calais</a> </li>
<li><a href=""http://uclassify.com"" rel=""nofollow"">uClassify</a> </li>
<li><a href=""http://code.google.com/apis/predict"" rel=""nofollow"">Google Prediction API</a> </li>
<li><a href=""http://textclassify.com"" rel=""nofollow"">Text Classify</a> </li>
<li><a href=""http://alchemyapi.com"" rel=""nofollow"">Alchemy API</a> </li>
</ul>

<p>Also there good Java SDK like Mahout. As I remember interactions with Mahout could be also done like with service, so integration with it is not a problem at all.</p>

<p>I had similar ""auto tagging"" task using c#, and I've used for that Open Calais. It's free to make 50,000 transactions per day. It was enough for me. Also uClassify has good pricing, as example ""Indie"" license 99$ per year.</p>

<p>But maybe external services and Mahout is not your way. Than take a look at <a href=""http://wiki.dbpedia.org"" rel=""nofollow"">DBpedia</a> project and RDF. 
And the last, you can use some implementations of Naive Bayes algorithm, at least. It's easy, and all will be under your control. </p>
",2,2,2539,2012-01-24 16:55:51,https://stackoverflow.com/questions/8990804/text-classification-extract-tags-from-text
"Who are the major authors in Information Extraction, Text Mining and Natural Language Processing area?","<p>This is not a code question, but about concepts. I want to know who are the main author/researches for Information Extraction, Natural Language Processing and Text Mining to read his papers/books/works.</p>
","nlp, text-mining, information-extraction","<p>You will find very good references on Quora under:</p>

<p><a href=""http://www.quora.com/Natural-Language-Processing/What-are-the-most-important-research-papers-which-all-NLP-students-should-definitely-read?q=best%20papers%20natural%20lan"" rel=""nofollow"">What are the most important research papers which all NLP students should definitely read?</a></p>
",2,0,454,2012-02-02 02:04:48,https://stackoverflow.com/questions/9106165/who-are-the-major-authors-in-information-extraction-text-mining-and-natural-lan
How to extract textual contents from a web page?,"<p>I'm developing an application in java which can take textual information from different web pages and will summarize it into one page.For example,suppose I have a news on different web pages like Hindu,Times of India,Statesman,etc.Now my application is supposed to extract important points from each one of these pages and will put them together as a single news.The application is based on concepts of web content mining.As a beginner to this field,I can't understand where to start off.I have gone through research papers which explains noise removal as first step in buiding this application.</p>

<p>So,if I'm given a news web page the very first step is to extract main news from the page excluding hyperlinks,advertisements,useless images,etc. My question is how can I do this ? Please give me some good tutorials which explains the implementation of such kind of application using web content mining.Or at least give me some hint how to accomplish it ? </p>
","java, web, web-scraping, text-mining, web-mining","<p>You can use <a href=""https://github.com/buriy/python-readability"" rel=""noreferrer"">readability</a> or <a href=""http://code.google.com/p/boilerpipe/"" rel=""noreferrer"">boilerpipe</a>, two open source tools for this task. For a tutorial you should read the code &amp; documentation for those two projects.</p>
",9,0,2892,2012-02-09 17:14:04,https://stackoverflow.com/questions/9215505/how-to-extract-textual-contents-from-a-web-page
how can one increase size of plotted area wordclouds in R,"<p>trying to replicate the example here; </p>

<p><a href=""http://onertipaday.blogspot.com/2011/07/word-cloud-in-r.html"" rel=""noreferrer"">http://onertipaday.blogspot.com/2011/07/word-cloud-in-r.html</a></p>

<p>Need help figuring out how to increase the plotted area of the word cloud. Changing the height and width parmeters in  png(""wordcloud_packages.png"", width=1280,height=800) only changes the height and width of the canvas..but the plotted area remains small. </p>

<pre><code>require(XML)
require(tm)
require(wordcloud)
require(RColorBrewer)
u = ""http://cran.r-project.org/web/packages/available_packages_by_date.html""
t = readHTMLTable(u)[[1]]
ap.corpus &lt;- Corpus(DataframeSource(data.frame(as.character(t[,3]))))
ap.corpus &lt;- tm_map(ap.corpus, removePunctuation)
ap.corpus &lt;- tm_map(ap.corpus, tolower)
ap.corpus &lt;- tm_map(ap.corpus, function(x) removeWords(x, stopwords(""english"")))
ap.tdm &lt;- TermDocumentMatrix(ap.corpus)
ap.m &lt;- as.matrix(ap.tdm)
ap.v &lt;- sort(rowSums(ap.m),decreasing=TRUE)
ap.d &lt;- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
pal2 &lt;- brewer.pal(8,""Dark2"")
png(""wordcloud_packages.png"", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
</code></pre>
","r, text-mining, tag-cloud, word-cloud","<p>Try using the <code>res</code> parameter, instead:</p>

<pre><code>...
png(""wordcloud_packages.png"", width=12,height=8, units='in', res=300)
...
</code></pre>

<p><img src=""https://i.sstatic.net/weVQ9.png"" alt=""enter image description here""></p>
",22,16,27930,2012-02-12 00:59:54,https://stackoverflow.com/questions/9245519/how-can-one-increase-size-of-plotted-area-wordclouds-in-r
Develop algorithm to analyze words,"<p>I have am working on a project where I have seven ""posts."" The posts are just a sentence or two about the subject. What I need to do is to develop an algorithm which looks through the posts and identifies certain trends.
For example, ""A is good but causes B."" I need to develop an algorithm which would identify the link between A and B.</p>

<p>However, as you may be able to tell from my sub par description, I have no idea how to address this problem. Can anybody point me in the right direction? I looked at data mining but I'm not sure if that is what I need. </p>
","algorithm, nlp, data-mining, text-mining","<p>What you are asking is a hot research topic in text mining and natural language processing. However, your question is too general imo.</p>

<p>The simplest thing you could start with might be identifying the words that frequently appear together in a sentence (or consecutive sentences). That would at least provide you with some kind of a correlation. Look into Association Rule Learning as user1161595 suggested. Clustering techniques might be of help too. To begin with, have a look at <a href=""http://en.wikipedia.org/wiki/Cluster_analysis"">cluster analysis</a> and <a href=""http://www2.parc.com/istl/projects/ia/sg-clustering.html"">text clustering</a>.</p>

<p>To extract relationships between the words, you need to dive deep into natural language processing. You can use <a href=""http://scholar.google.com/scholar?as_q=twitter%20trend&amp;num=10&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_sdt=1&amp;as_subj=eng&amp;as_sdtf=&amp;as_sdts=5&amp;btnG=Search%20Scholar&amp;hl=en"">Google Scholar</a> for searching related research on text mining, trend analysis and information retrieval.</p>

<p>In addition, the following links might give you an idea of what you are dealing with:</p>

<ul>
<li><a href=""http://www.guardian.co.uk/technology/2011/dec/05/monday-note-twitter"">Datamining Twitter</a> - an article on The Guardian</li>
<li><a href=""http://isc.sans.edu/diary.html?storyid=5728"">How to Use Twitter for Information Mining</a></li>
<li><a href=""http://practicalquant.blogspot.com/2010/04/text-mining-and-twitter.html"">Text Mining and Twitter</a></li>
<li><a href=""http://heuristically.wordpress.com/2011/04/08/text-data-mining-twitter-r/"">Text Data Mining With Twitter And R</a> - mentions a Python module written for this purpose.</li>
</ul>

<p>As for the implementation, you might look into the open source frameworks mentioned on <a href=""http://en.wikipedia.org/wiki/Text_mining#Open_source"">Wikipedia</a>. Furthermore, there are lots of machine learning and NLP libraries out there.</p>

<p>Last but not least, I believe there is a good chance that something similar to what you are looking for has already been implemented somewhere by someone. So a through Google search with the right keywords might get you what you want without diving deep into research.</p>
",8,2,737,2012-03-02 21:58:12,https://stackoverflow.com/questions/9540944/develop-algorithm-to-analyze-words
Removing everything but html tags from a corpus,"<p>I'm using the package <code>tm</code>. I have a corpus full of html document and I would like to remove everything but the html tags. I've been trying to do that for a few days but I don't seem to be able to find any good solution.</p>

<p>For example, let's say I have a document like this :</p>

<pre><code>&lt;html&gt;
&lt;body&gt;

&lt;h1&gt;hello&lt;/h1&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>I would want the document to become like this:</p>

<pre><code>&lt;html&gt; &lt;body&gt; &lt;h1&gt;
</code></pre>

<p>(Or with the closing tags, I don't really mind.)</p>

<p>My goal is to count how many times each tag is used in a document.</p>
","regex, r, text-mining, tm","<p>I'm not familiar with tm, but here's how you could do it using Regular Expressions.</p>

<p>(Presupposition: your string starts and ends with an HTML tag)</p>

<pre><code>str &lt;- ""&lt;html&gt;&lt;body&gt;&lt;p&gt;test&lt;p&gt;test2&lt;/body&gt;&lt;/html&gt;""
str &lt;- gsub(""&gt;[^&lt;^&gt;]+&lt;"", ""&gt; &lt;"", str) # remove all the text in between HTML tags, leaving only HTML tags (opening and closing)
str &lt;- gsub(""&lt;/[^&lt;^&gt;]+&gt;"", """", str) #remove all closing HTML tags.
</code></pre>

<p>That would leave you with your desired string.</p>

<p>If you're new to RegEx, check out <a href=""http://www.regular-expressions.info/"" rel=""nofollow"">this site</a> for additional info getting started. Basically, the first gsub above is going to replace all text in between <code>&gt;</code> and <code>&lt;</code> which isn't an open or close bracket (i.e. all non-tag text). The second gsub will replace all text which starts with <code>&lt;/</code> and ends with <code>&gt;</code> with nothing -- removing the closing tags from the string</p>
",2,1,1744,2012-03-26 15:51:36,https://stackoverflow.com/questions/9875366/removing-everything-but-html-tags-from-a-corpus
algorithm to extract simple sentences from complex(mixed) sentences?,"<p>Is there an algorithm that can be used to extract simple sentences from paragraphs?</p>

<p>My ultimate goal is to later run another algorithm on the resulted simple sentence to determine the author's sentiment.</p>

<p>I've researched this from sources such as Chae-Deug Park but none discuss preparing simple sentences as training data.</p>

<p>Thanks in advance</p>
","nlp, extract, text-mining, text-extraction, information-extraction","<p>I have just used openNLP for the same.</p>

<pre><code>public static List&lt;String&gt; breakIntoSentencesOpenNlp(String paragraph) throws FileNotFoundException, IOException,
        InvalidFormatException {

    InputStream is = new FileInputStream(""resources/models/en-sent.bin"");
    SentenceModel model = new SentenceModel(is);
    SentenceDetectorME sdetector = new SentenceDetectorME(model);

    String[] sentDetect = sdetector.sentDetect(paragraph);
    is.close();
    return Arrays.asList(sentDetect);
}
</code></pre>

<p>Example</p>

<pre><code>    //Failed at Hi.
    paragraph = ""Hi. How are you? This is Mike."";
    SentenceDetector.breakIntoSentencesOpenNlp(paragraph).forEach(sentence -&gt; System.out.println(sentence));

    //Failed at Door.Noone
    paragraph = ""Close the Door.Noone is out there"";
    SentenceDetector.breakIntoSentencesOpenNlp(paragraph).forEach(sentence -&gt; System.out.println(sentence));//not able to break on noone

    paragraph = ""Really!! I cant believe. Mr. Wilson can come any moment to receive mrs. watson."";
    SentenceDetector.breakIntoSentencesOpenNlp(paragraph).forEach(sentence -&gt; System.out.println(sentence));

    //Failed at dr.
    paragraph = ""Radhika, Mohan, and Shaik went to meet dr. Kashyap to raise fund for poor patients."";
    SentenceDetector.breakIntoSentencesOpenNlp(paragraph).forEach(sentence -&gt; System.out.println(sentence));//breaking on dr.

    paragraph = ""This is how I tried to split a paragraph into a sentence. But, there is a problem. My paragraph includes dates like Jan.13, 2014 , words like U.S. and numbers like 2.2. They all got splitted by the above code."";
    SentenceDetector.breakIntoSentencesOpenNlp(paragraph).forEach(sentence -&gt; System.out.println(sentence));//breaking on dr.

    paragraph = ""www.thinkzarahatke.com is the second site I developed. You can send mail to admin@thinkzarahatke.com"";
    SentenceDetector.breakIntoSentencesOpenNlp(paragraph).forEach(sentence -&gt; System.out.println(sentence));
</code></pre>

<p>It failed only when there is a human mistake. Eg. ""Dr."" abbreviation should have capital D, and there is at least 1 space is expected between 2 sentences.</p>

<p>You can also achieve it using <a href=""https://stackoverflow.com/a/21430792/453767"">RE</a> in following way;</p>

<pre><code>public static List&lt;String&gt; breakIntoSentencesCustomRESplitter(String paragraph){
    List&lt;String&gt; sentences = new ArrayList&lt;String&gt;();
    Pattern re = Pattern.compile(""[^.!?\\s][^.!?]*(?:[.!?](?!['\""]?\\s|$)[^.!?]*)*[.!?]?['\""]?(?=\\s|$)"", Pattern.MULTILINE | Pattern.COMMENTS);
    Matcher reMatcher = re.matcher(paragraph);
    while (reMatcher.find()) {
        sentences.add(reMatcher.group());
    }
    return sentences;

}
</code></pre>

<p>Example</p>

<pre><code>    paragraph = ""Hi. How are you? This is Mike."";
    SentenceDetector.breakIntoSentencesCustomRESplitter(paragraph).forEach(sentence -&gt; System.out.println(sentence));

    //Failed at Door.Noone
    paragraph = ""Close the Door.Noone is out there"";
    SentenceDetector.breakIntoSentencesCustomRESplitter(paragraph).forEach(sentence -&gt; System.out.println(sentence));

    //Failed at Mr., mrs.
    paragraph = ""Really!! I cant believe. Mr. Wilson can come any moment to receive mrs. watson."";
    SentenceDetector.breakIntoSentencesCustomRESplitter(paragraph).forEach(sentence -&gt; System.out.println(sentence));

    //Failed at dr.
    paragraph = ""Radhika, Mohan, and Shaik went to meet dr. Kashyap to raise fund for poor patients."";
    SentenceDetector.breakIntoSentencesCustomRESplitter(paragraph).forEach(sentence -&gt; System.out.println(sentence));

    //Failed at U.S.
    paragraph = ""This is how I tried to split a paragraph into a sentence. But, there is a problem. My paragraph includes dates like Jan.13, 2014 , words like U.S. and numbers like 2.2. They all got splitted by the above code."";
    SentenceDetector.breakIntoSentencesCustomRESplitter(paragraph).forEach(sentence -&gt; System.out.println(sentence));

    paragraph = ""www.thinkzarahatke.com is the second site I developed. You can send mail to admin@thinkzarahatke.com"";
    SentenceDetector.breakIntoSentencesCustomRESplitter(paragraph).forEach(sentence -&gt; System.out.println(sentence));
</code></pre>

<p>But errors are competitively high. Another way is using BreakIterator;</p>

<pre><code>public static List&lt;String&gt; breakIntoSentencesBreakIterator(String paragraph){
    List&lt;String&gt; sentences = new ArrayList&lt;String&gt;();
    BreakIterator sentenceIterator =
            BreakIterator.getSentenceInstance(Locale.ENGLISH);
    BreakIterator sentenceInstance = sentenceIterator.getSentenceInstance();
    sentenceInstance.setText(paragraph);

    int end = sentenceInstance.last();
     for (int start = sentenceInstance.previous();
          start != BreakIterator.DONE;
          end = start, start = sentenceInstance.previous()) {
         sentences.add(paragraph.substring(start,end));
     }

     return sentences;
}
</code></pre>

<p>Example:</p>

<pre><code>    paragraph = ""Hi. How are you? This is Mike."";
    SentenceDetector.breakIntoSentencesBreakIterator(paragraph).forEach(sentence -&gt; System.out.println(sentence));

    //Failed at Door.Noone
    paragraph = ""Close the Door.Noone is out there"";
    SentenceDetector.breakIntoSentencesBreakIterator(paragraph).forEach(sentence -&gt; System.out.println(sentence));

    //Failed at Mr.
    paragraph = ""Really!! I cant believe. Mr. Wilson can come any moment to receive mrs. watson."";
    SentenceDetector.breakIntoSentencesBreakIterator(paragraph).forEach(sentence -&gt; System.out.println(sentence));

    //Failed at dr.
    paragraph = ""Radhika, Mohan, and Shaik went to meet dr. Kashyap to raise fund for poor patients."";
    SentenceDetector.breakIntoSentencesBreakIterator(paragraph).forEach(sentence -&gt; System.out.println(sentence));


    paragraph = ""This is how I tried to split a paragraph into a sentence. But, there is a problem. My paragraph includes dates like Jan.13, 2014 , words like U.S. and numbers like 2.2. They all got splitted by the above code."";
    SentenceDetector.breakIntoSentencesBreakIterator(paragraph).forEach(sentence -&gt; System.out.println(sentence));

    paragraph = ""www.thinkzarahatke.com is the second site I developed. You can send mail to admin@thinkzarahatke.com"";
    SentenceDetector.breakIntoSentencesBreakIterator(paragraph).forEach(sentence -&gt; System.out.println(sentence));
</code></pre>

<p><strong>Benchmarking</strong>:</p>

<ul>
<li>custom RE : 7 ms </li>
<li>BreakIterator : 143 ms</li>
<li>openNlp : 255 ms</li>
</ul>
",1,4,1856,2012-04-04 22:58:15,https://stackoverflow.com/questions/10020451/algorithm-to-extract-simple-sentences-from-complexmixed-sentences
How to tell if two web contents are similar?,"<p>Given 2 html sources, I want to first extract the main content out of it using something like <a href=""http://code.google.com/p/boilerpipe/"" rel=""nofollow"">this</a>. Are there any <a href=""http://tomazkovacic.com/blog/122/evaluating-text-extraction-algorithms/"" rel=""nofollow"">other better libraries</a> - I am specifically looking for Python/Javascript ones?</p>

<p>Once I have the two extracted contents, I want to return a score between 0 and 1 denoting how similar they are e.g. news articles on the same topic from CNN and BBC would have higher similarity scores since they are on the same topic or webpages pertaining to the same product on Amazon.com and Walmart.com would have a high score too. How can I do this? Are there existing libraries that do this already? What are some good libraries I can use? Basically I am looking for a combination of <a href=""http://en.wikipedia.org/wiki/Automatic_summarization"" rel=""nofollow"">automatic summarization</a>, <a href=""http://en.wikipedia.org/wiki/Terminology_extraction"" rel=""nofollow"">keyword extraction</a>, <a href=""http://en.wikipedia.org/wiki/Named_entity_recognition"" rel=""nofollow"">named-entity recognition</a> and <a href=""http://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""nofollow"">sentiment-analysis</a>.</p>
","python, nlp, machine-learning, text-mining, semantic-analysis","<p>There are many things embedded in your question. I will try to provide you with a library or else will suggest you Algorithms that can solve your tasks (which you can Google and you will get many python implementations)</p>

<p><strong>Point 1</strong>. To extract main content out of html (http://nltk.googlecode.com/svn/trunk/doc/book/ch03.html) &amp; for other NLP related stuff you can check out NLTK. Its written in Python. You can also check out for a library called BeautifulSoup, its awesome (http://www.crummy.com/software/BeautifulSoup/)</p>

<p><strong>Point 2</strong>. When you say:</p>

<p>Once I have the two extracted contents, I want to return a score between 0 and 1 denoting how similar they are....</p>

<p>For this I suggest you can cluster your document set using any unsupervised learning clustering technique. Since your problem falls under the distance-metric based clustering so it should be really easy for you to cluster similar documents and then assign a score to them based on their similarity with the cluster centroid. Try either K-Means or Adaptive Resonance Theory. In the latter you dont need to define the number of clusters in advance. OR as larsman points out in his comments you can simply use TF-IDF (http://www.miislita.com/term-vector/term-vector-3.html)</p>

<p><strong>Point 3</strong>.When you say:</p>

<p>Basically I am looking for a combination of automatic summarization, keyword extraction, named-entity recognition and sentiment-analysis</p>

<p>For Automatic Summarization use Non Negative Matrix Factorization</p>

<p>For Keyword extraction use NLTK</p>

<p>For Named-Entity Recognition use NLTK</p>

<p>For Sentiment Analysis use NLTK</p>
",5,3,504,2012-04-05 20:09:36,https://stackoverflow.com/questions/10035280/how-to-tell-if-two-web-contents-are-similar
XPath for each Document in R Corpus,"<p>I have a corpus, x, in R created from a directory using DirSource. Each document is a text file containing the full HTML of a related vBulletin forum webpage. Since it is a thread, each document has multiple separate posts that I want to capture with my XPath. The XPath seems to work, but I cannot put all my captured nodes back into the corpus.</p>

<p>If my corpus has 25 documents that have an average 4 posts each, then my new corpus should have 100 documents. I'm wondering if I have to do a loop and create a new corpus.</p>

<p>Here is my messy work so far. Any source from a thread in www.vbulletin.org/forum/ is an example of the structure.</p>

<pre><code>#for stepping through
xt &lt;- x[[5]]
xpath &lt;- ""//div[contains(@id,'post_message')]""

getxpath &lt;- function(xt,xpath){
  require(XML)

  #either parse
  doc &lt;- htmlParse(file=xt)
  #doc &lt;- htmlTreeParse(tolower(xt), asText = TRUE, useInternalNodes = TRUE)

  #don't know which to use
  #result &lt;- xpathApply(doc,xpath,xmlValue)
  result &lt;- xpathSApply(doc,xpath,xmlValue)

  #clean up
  result &lt;- gsub(pattern=""\\s+"",replacement="" "",x=gsub(pattern=""\n|\t"",replacement="" "",x=result))

  result &lt;- c(result[1:length(result)])

  free(doc)

  #converts group of nodes into 1 data frame with numbers before separate posts
  #require(plyr)
  #xbythread &lt;- ldply(.data=result,.fun=function(x){unlist(x)})

  #don't know what needs to be returned
  result &lt;- Corpus(VectorSource(result))
  #result &lt;- as.PlainTextDocument(result)

  return(result)
}

#call
x2 &lt;- tm_map(x=x,FUN=getxpath,""//div[contains(@id,'post_message')]"")
</code></pre>
","xml, r, xpath, text-mining, corpus","<p>Figured it out a while ago. htmlParse needs isURL=TRUE.</p>

<pre><code>getxpath &lt;- function(xt,xpath){
  require(XML);require(tm)
  x &lt;- htmlParse(file=u,isURL=TRUE)
  resultvector &lt;- xpathSApply(x,xpath,xmlValue)
  result &lt;- gsub(pattern=""\\s+"",replacement="" "",x=gsub(pattern=""\n|\t"",replacement="" "",x=resultvector))
  return(result)
}

res &lt;- getxpath(""http://url.com/board.html"",""//xpath"")
</code></pre>

<p>To get all the files, I use list.files to get the file list, Map/clusterMap with getxpath() to put them in a list, do.call to get them in a vector, and Corpus(VectorSource(res)) to put them in a Corpus.</p>
",1,0,358,2012-04-13 16:00:09,https://stackoverflow.com/questions/10144280/xpath-for-each-document-in-r-corpus
Word Semantic-Similarity (distance measures) webservices?,"<p>Is there any web-service that provides a word semantic-similarity measurements?</p>

<p>I'm aware of <a href=""http://www.linguatools.de/disco/disco_en.html"" rel=""nofollow noreferrer"">Disco</a>, but I'd prefer a service with an ongoing growing base (and it would be most helpful if you had tried it in your projects).</p>

<p>I'm also aware of WordNet-based algorithms, but installing and managing it as part of the project resources brings heavy weight.</p>
","semantics, similarity, text-mining, wordnet","<p>You could consider using the google index API, there is a sample application here - <a href=""http://www.mechanicalcinderella.com/"" rel=""nofollow"">http://www.mechanicalcinderella.com/</a> uses it and has links to the original paper describing the technique. Source code here - <a href=""http://complearn.org/index.html"" rel=""nofollow"">http://complearn.org/index.html</a>. </p>

<p>i have not used it myself, other than for fun... but it seems pretty decent. </p>
",0,1,943,2012-04-19 21:54:46,https://stackoverflow.com/questions/10237399/word-semantic-similarity-distance-measures-webservices
How do I search for a pattern within a text file using Python combining regex &amp; string/file operations and store instances of the pattern?,"<p>So essentially I'm looking for specifically a 4 digit code within two angle brackets within a text file. I know that I need to open the text file and then parse line by line, but I am not sure the best way to go about structuring my code after checking ""for line in file"". </p>

<p>I think I can either somehow split it, strip it, or partition, but I also wrote a regex which I used compile on and so if that returns a match object I don't think I can use that with those string based operations. Also I'm not sure whether my regex is greedy enough or not...</p>

<p>I'd like to store all instances of those found hits as strings within either a tuple or a list. </p>

<p>Here is my regex: </p>

<pre><code>regex = re.compile(""(&lt;(\d{4,5})&gt;)?"")
</code></pre>

<p>I don't think I need to include all that much code considering its fairly basic so far. </p>
","python, regex, file-io, text-mining, string-parsing","<pre><code>import re
pattern = re.compile(""&lt;(\d{4,5})&gt;"")

for i, line in enumerate(open('test.txt')):
    for match in re.finditer(pattern, line):
        print 'Found on line %s: %s' % (i+1, match.group())
</code></pre>

<p>A couple of notes about the regex:</p>

<ul>
<li>You don't need the <code>?</code> at the end and the outer <code>(...)</code> if you don't want to match the number with the angle brackets, but only want the number itself</li>
<li>It matches either 4 or 5 digits between the angle brackets</li>
</ul>

<p><strong>Update:</strong> It's important to understand that the <em>match</em> and <em>capture</em> in a regex can be quite different. The regex in my snippet above matches the pattern <em>with</em> angle brackets, but I ask to capture only the internal number, <em>without</em> the angle brackets.</p>

<p>More about regex in python can be found here : 
<a href=""https://docs.python.org/2/howto/regex.html"" rel=""noreferrer"">Regular Expression HOWTO</a></p>
",78,70,295087,2012-05-07 05:53:47,https://stackoverflow.com/questions/10477294/how-do-i-search-for-a-pattern-within-a-text-file-using-python-combining-regex
Removing an &quot;empty&quot; character item from a corpus of documents in R?,"<p>I am using the <code>tm</code> and <code>lda</code> packages in R to topic model a corpus of news articles. However, I am getting a ""non-character"" problem represented as <code>""""</code> that is messing up my topics. Here is my workflow:</p>

<pre><code>text &lt;- Corpus(VectorSource(d$text))
newtext &lt;- lapply(text, tolower)
sw &lt;- c(stopwords(""english""), ""ahram"", ""online"", ""egypt"", ""egypts"", ""egyptian"")
newtext &lt;- lapply(newtext, function(x) removePunctuation(x))
newtext &lt;- lapply(newtext, function(x) removeWords(x, sw))
newtext &lt;- lapply(newtext, function(x) removeNumbers(x))
newtext &lt;- lapply(newtext, function(x) stripWhitespace(x))
d$processed &lt;- unlist(newtext)
corpus &lt;- lexicalize(d$processed)
k &lt;- 40
result &lt;-lda.collapsed.gibbs.sampler(corpus$documents, k, corpus$vocab, 500, .02, .05,
compute.log.likelihood = TRUE, trace = 2L)
</code></pre>

<p>Unfortunately, when I train the lda model, everything looks great except the most frequently occurring word is """". I try to remedy this by removing it from the vocab as given below and reestimating the model just as above: </p>

<pre><code>newtext &lt;- lapply(newtext, function(x) removeWords(x, """"))
</code></pre>

<p>But, it's still there, as evidenced by:</p>

<pre><code>str_split(newtext[[1]], "" "")

[[1]]
 [1] """"              ""body""          ""mohamed""       ""hassan""       
 [5] ""cook""          ""found""         ""turkish""       ""search""       
 [9] ""rescue""        ""teams""         ""rescued""       ""hospital""     
[13] ""rescue""        ""teams""         ""continued""     ""search""       
[17] ""missing""       ""body""          ""cook""          ""crew""         
[21] ""wereegyptians"" ""sudanese""      ""syrians""       ""hassan""       
[25] ""cook""          ""cargo""         ""ship""          ""sea""          
[29] ""bright""        ""crashed""       ""thursday""      ""port""         
[33] ""antalya""       ""southern""      ""turkey""        ""vessel""       
[37] ""collided""      ""rocks""         ""port""          ""thursday""     
[41] ""night""         ""result""        ""heavy""         ""winds""        
[45] ""waves""         ""crew""          """"             
</code></pre>

<p>Any suggestions on how to go about removing this? Adding <code>""""</code> to my list of stopwords doesn't help, either.</p>
","r, text-mining, text-analysis, lda, topic-modeling","<p>I deal with text a lot but not tm so this is 2 approaches to get rid of the """" you have.  likely the extra """" characters are because of a double space bar between sentences.  You can treat this condition before or after you turn the text into a bag of words.  You could replace all "" ""x2  with "" ""x1 before the strsplit or you could do it afterward (you have to unlist after strsplit).</p>

<pre><code>x &lt;- ""I like to ride my bicycle.  Do you like to ride too?""

#TREAT BEFORE(OPTION):
a &lt;- gsub("" +"", "" "", x)
strsplit(a,  "" "")

#TREAT AFTER OPTION:
y &lt;- unlist(strsplit(x, "" ""))
y[!y%in%""""]
</code></pre>

<p>You might also try:</p>

<pre><code>newtext &lt;- lapply(newtext, function(x) gsub("" +"", "" "", x))
</code></pre>

<p>Again I don't use tm so this may not be of help but this post hadn't seen any action so I figured I'd share possibilities.</p>
",4,5,7367,2012-05-07 20:02:53,https://stackoverflow.com/questions/10488343/removing-an-empty-character-item-from-a-corpus-of-documents-in-r
Choose or generate canonical variant from multiple sentences,"<p>I'm working with an API that maps my GTIN/EAN queries to product data.</p>

<p>Since the data returned originates from merchant product feeds, the following is almost universally the case:</p>

<ul>
<li>Multiple results per GTIN</li>
<li>Products' titles are pretty much unstructured</li>
<li>Products' titles are ""polluted"" with
<ul>
<li>SEO-related stuff,</li>
<li>information about the quantity contained,</li>
<li>""buy two, get one free"" offers,</li>
<li>etc.</li>
</ul></li>
</ul>

<p><strong>I'm looking for a programmatic way to either</strong></p>

<ul>
<li><em><strong>choose</em> the ""cleanest""/most canonical version available</strong></li>
<li><strong>or <em>generate</em> a new one that represents the ""lowest common denominator"".</strong></li>
</ul>

<p>Consider the following example results for a single EAN query:</p>

<ul>
<li>Nivea Deo Roll-On Dry Impact for Men</li>
<li>NIVEA DEO Roll on Dry/blau</li>
<li>Nivea Deo Roll-On Dry Impact for Men, 50 ml, 3er Pack (3 x 50 ml)</li>
<li>Nivea Deo Roll on Dry/blau 50 ml</li>
<li>Nivea Deoroller 50ml dry for Men blau   Mindestabnahme: 6 Stück (1 VE)</li>
<li>NIVEA Deoroller, Dry Impact for Men</li>
<li>NIVEA DEO Roll on Dry/blau_50 ml</li>
</ul>

<p><strong>My homebrew approach looks like this:</strong></p>

<ul>
<li>Basic cleanup:
<ul>
<li>Lowercase the titles,</li>
<li>strip excessive whitespace,</li>
<li>throw out apparent stopwords such as ""buy"" and ""click""</li>
</ul></li>
<li>Build an array for <code>word =&gt; global occurence</code>
<ul>
<li><code>""Nivea"" =&gt; 7</code></li>
<li><code>""Deo"" =&gt; 5</code></li>
<li><code>""Deoroller"" =&gt; 2</code></li>
<li><code>…</code></li>
<li><code>""VE"" =&gt; 1</code></li>
</ul></li>
<li>Calculate the ""cumulative word value"" for each of the titles
<ul>
<li><code>""Nivea Deo"" =&gt; 12</code></li>
<li><code>""Nivea Deoroller VE"" =&gt; 10</code></li>
</ul></li>
<li>Divide the cumulative value by the length of the title, resulting in a score
<ul>
<li><code>""Nivea Deo"" =&gt; 6</code></li>
<li><code>""Nivea Deoroller VE"" =&gt; 3.34</code></li>
</ul></li>
</ul>

<p>Obviously, my approach is pretty basic, error-prone and biased towards short sentences with frequently used words – yielding more or less satisfactory results.</p>

<ul>
<li><strong>Would you choose a different approach?</strong></li>
<li><strong>Is there some NLP magic way to take care of the problem that I don't know of?</strong></li>
</ul>
","php, text-mining, information-extraction, nlp","<p>Since your existing metric seems to bias towards shorter phrases, you should consider factoring in bigrams into the mix. So instead of considering scores for just individual words, additionally consider the score for consecutive pairs of words as well (e.g. 'nivea deo', deo roll-on', 'roll-on dry', etc). When computing the score for each title, factor in the scores for every unigram and bigram you can generate out of the title together, but maybe give the bigrams more weight, and this should encourage your algorithm to prefer longer phrases.</p>

<p>If you have large existing corpus of lots of names like these at your disposal, consider using something like <a href=""http://en.wikipedia.org/wiki/Tf%2aidf"">TF-IDF</a><br>
What you are doing right can be likened to just using TF. Using your global corpus, you can compute the idf of each unigram and bigram, which is basically a measure of unique or rare a word or phrase is across the entire corpus.<br>
tf = the number of times you have seen an ngram within these results<br>
idf = a global measure of how unique an ngram might be across all results (or atleast a very large number of them)<br>
So when computing the score for a title, instead of simply adding up the tf's of each ngram in it, you add up the tf*idf of each ngram instead. Rarer ngrams (which possibly do a better job at distinguishing this item from all other items) have a higher idf, so your algorithm should give higher weight to them. A lot of junk terms (like Mindestabnahme) would have really high idf, but they would have a really small tf, so they might not make a big difference. Alternatively prune off tokens you see fewer than k times, to get rid of noise.  </p>

<p>Another NLP trick to know about is <a href=""http://en.wikipedia.org/wiki/Levenshtein_distance"">Levenshtein distance</a> .. which is a way to quantify how similar two strings are. You can compute the levenshtein distance between every pair of strings within your results, and then try preferring the result which has the lowest average distance from all the other strings. This might not work well by itself... but factoring this score in with your existing approach might help you navigate some tricky cases.  </p>
",6,23,866,2012-06-01 20:25:02,https://stackoverflow.com/questions/10856896/choose-or-generate-canonical-variant-from-multiple-sentences
Mahout - Clustering - &quot;naming&quot; the cluster elements,"<p>I'm doing some research and I'm playing with Apache Mahout 0.6</p>

<p>My purpose is to build a system which will  name different categories of documents based on user input. The documents are not known in advance and I don't know also which categories do I have while collecting these documents. But I do know, that all the documents in the model should belong to one of the predefined categories.</p>

<p>For example: 
Lets say I've collected a N documents, that belong to 3 different groups : </p>

<ul>
<li>Politics</li>
<li>Madonna (pop-star)</li>
<li>Science fiction</li>
</ul>

<p>I don't know what document belongs to what category, but I know that each one of my N documents belongs to one of those categories (e.g. there are no documents about, say basketball among these N docs)</p>

<p>So, I came up with the following idea:</p>

<ul>
<li><p>Apply mahout clustering (for example k-mean with k=3 on these documents)
This should divide the N documents to 3 groups. This should be kind of my model to learn with. I still don't know which document really belongs to which group, but at least the documents are clustered now by group</p></li>
<li><p>Ask the user to find any document in the web that should be about 'Madonna' (I can't show to the user none of my N documents, its a restriction). Then I want to measure 'similarity' of this document and each one of 3 groups. 
I expect to see that the measurement for similarity between user_doc and documents in Madonna group in the model will be higher than the similarity between the user_doc and documents about politics.</p></li>
</ul>

<p>I've managed to produce the cluster of documents using 'Mahout in Action' book.
But I don't understand how should I use Mahout to measure similarity between the 'ready' cluster group of document and one given document.</p>

<p>I thought about rerunning the cluster with k=3 for N+1 documents with the same centroids (in terms of k-mean clustering) and see whether where the new document falls, but maybe there is any other way to do that?</p>

<p>Is it possible to do with Mahout or my idea is conceptually wrong? (example in terms of Mahout API would be really good)</p>

<p>Thanks a lot and sorry for a long question (couldn't describe it better)</p>

<p>Any help is highly appreciated</p>

<p>P.S. This is not a home-work project :)</p>
","java, mahout, text-mining","<p>This might be possible, but a much easier solution (IMHO) would be to hand-label a few documents from each category, then use those to bootstrap k-means. I.e., compute the centroids of the hand-labeled politics/Madonna/scifi documents and let k-means take it from there.</p>

<p>(In fancy terms, you would be doing <a href=""https://en.wikipedia.org/wiki/Semi-supervised_learning"" rel=""nofollow"">semisupervised</a> <a href=""https://en.wikipedia.org/wiki/Nearest_centroid_classifier"" rel=""nofollow"">nearest centroids classification</a>.)</p>
",2,1,492,2012-06-12 12:18:05,https://stackoverflow.com/questions/10996505/mahout-clustering-naming-the-cluster-elements
Storing text mining data,"<p>I am looking to track topic popularity on a very large number of documents. Furthermore, I would like to give recommendations to users based on topics, rather than the usual bag of words model.
To extract the topics I use natural language processing techniques that are beyond the point of this post.</p>

<p>My question is how should I persist this data so that:
I) I can quickly fetch trending data for each topic (in principle, every time a user opens a document, the topics in that document should go up in popularity)
II) I can quickly compare documents to provide recommendations (here I am thinking of using clustering techniques)</p>

<p>More specifically, my questions are:
1) Should I go with the usual way of storing text mining data? meaning storing a topic occurrence vector for each document, so that I can later measure the euclidean distance between different documents.
2) Some other way?</p>

<p>I am looking for specific python ways to do this. I have looked into SQL and NoSQL databases, and also into pytables and h5py, but I am not sure how I would go about implementing a system like this. One of my concerns is how can I deal with an ever growing vocabulary of topics?</p>

<p>Thank you very much</p>
","python, database, data-mining, text-mining","<p>I would suggest that you do this work in a SQL database.  You may not want to store the documents there, but the topics are appropriate.</p>

<p>You want one table just for the topics:</p>

<pre><code>create table Topics (
    TopicId int identity(1,1), -- SQL Server for auto increment column
    TopicName varchar(255),
    CreatedBy varchar(255) default system_user,
    CreatedAt datetime default getdate()
</code></pre>

<p>)</p>

<p>You want another table for the topics assigned to documents, assuming that you have some sort of document id to identify documents:</p>

<pre><code>create table DocumentTopics (
    DocumentTopicId int identity(1,1), -- SQL Server for auto increment column
    TopicId int,
    DocumentID int,
    CreatedBy varchar(255) default system_user,
    CreatedAt datetime default getdate()
</code></pre>

<p>)</p>

<p>And another table for document views:</p>

<pre><code>create table DocumentView (
    DocumentViewId int identity(1,1), -- SQL Server for auto increment column
    DocumentId int,
    ViewedAt datetime,
    viewedBy int, -- some sort of user id
    CreatedBy varchar(255) default system_user,
    CreatedAt datetime default getdate()
</code></pre>

<p>)</p>

<p>Now you can get the topics by popularity for a given date range using a query such as:</p>

<pre><code>select t.TopicId, t.TopicName, count(*) as cnt
from DocumentUsage du join
     DocumentTopics dt
     on du.DocumentId = dt.DocumentId join
     Topics t
     on dt.TopicsId = t.TopicsId
where du.ViewedAt between &lt;date1&gt; and &lt;date2&gt;
group by t.TopicId, t.TopicName
order by 3 desc
</code></pre>

<p>You can also get information about users, changes over time, and other information.  You could have a users table, which could provide weights for the topics (more reliable users, less reliable users).  This aspect of the system should be done in SQL.</p>
",1,3,656,2012-06-29 18:32:58,https://stackoverflow.com/questions/11267143/storing-text-mining-data
R Regular Expression : extracting speaker in a script,"<p>I would like to use R to extract the speaker out of scripts formatted like in the following example:</p>

<p>""Scene 6: Second Lord: Nay, good my lord, put him to't; let him have his way. First Lord: If your lordship find him not a hilding, hold me no more in your respect. Second Lord: On my life, my lord, a bubble. BERTRAM: Do you think I am so far deceived in him? Second Lord: Believe it, my lord, in mine own direct knowledge, without any malice, but to speak of him as my kinsman, he's a most notable coward, an infinite and endless liar, an hourly promise-breaker, the owner of no one good quality worthy your lordship's entertainment.""</p>

<p>In this example, I would like to extract: (""Second Lord"", ""First Lord"", ""Second Lord"", ""BERTRAM"", ""Second Lord""). The rule is obvious: it is the group of words situated between the end of a sentence and a semi-column.</p>

<p>How can I write this in R ?</p>
","regex, r, text-mining","<p>Maybe something like this:</p>

<pre><code>library(stringr)  
body &lt;- ""Scene 6: Second Lord: Nay, good my lord, put him to't; let him have his way. First Lord: If your lordship find him not a hilding, hold me no more in your respect. Second Lord: On my life, my lord, a bubble. BERTRAM: Do you think I am so far deceived in him? Second Lord: Believe it, my lord, in mine own direct knowledge, without any malice, but to speak of him as my kinsman, he's a most notable coward, an infinite and endless liar, an hourly promise-breaker, the owner of no one good quality worthy your lordship's entertainment."" 
p &lt;- str_extract_all(body, ""[:.?] [A-z ]*:"")

# and get rid of extra signs
p &lt;- str_replace_all(p[[1]], ""[?:.]"", """")
# strip white spaces
p &lt;- str_trim(p)
p
""Second Lord"" ""First Lord""  ""Second Lord"" ""BERTRAM""     ""Second Lord""

# unique players
unique(p)
[1] ""Second Lord"" ""First Lord""  ""BERTRAM""  
</code></pre>

<h2>Explanations of regex: (which are not perfect)</h2>

<p><code>str_extract_all(body, ""[:.?] [A-z ]*:"")</code> a match is started with either <code>:</code> or <code>.</code> or <code>?</code> (<code>[:.?]</code>) followed by a whitespace. Any character and whitespace is matched until the next <code>:</code>.</p>

<h2>Get position</h2>

<p>You can use <code>str_locate_all</code> with the same regex:</p>

<pre><code>str_locate_all(body, ""[:.?] [A-z ]*:"")
</code></pre>
",4,2,506,2012-07-06 07:54:55,https://stackoverflow.com/questions/11358100/r-regular-expression-extracting-speaker-in-a-script
Using LIBSVM grid.py for unbalanced data?,"<p>I'm having a three class problem with unbalanced data (90%, 5%, 5%). Now I want to train a classifier using LIBSVM.</p>

<p>The problem is that LIBSVM optimizes its parameter gamma and Cost for optimal accuracy, which means that 100% of the examples are classified as class 1, which is of course not what I want.</p>

<p>I've tried modifying the weight parameters -w without much success.</p>

<p>So what I want is, modifying grid.py in a way that it optimizes Cost and gamma for precision and recall separated by classes rather than for overall accuracy. Is there any way to do that? Or are there other scripts out there that can do something like this?</p>
","machine-learning, libsvm, text-mining, svm","<p>The -w parameter is what you need for unbalanced data. What have you tried so far?</p>

<p>If your classes are:</p>

<ul>
<li>class 0: 90% </li>
<li>class 1: 5% </li>
<li>class 2: 5%</li>
</ul>

<p>You should pass the following params to svm:</p>

<pre><code>-w0 5 -w1 90 -w2 90
</code></pre>
",8,7,2479,2012-07-10 09:10:44,https://stackoverflow.com/questions/11410086/using-libsvm-grid-py-for-unbalanced-data
text mining/analyse user commands/questions algorithm or library,"<p>I got a financial application and I wish to add to it the ability to get user command or input in textbox and then take the right action. for example, wish the user to write ""show the revenue in the last 10 days"" and it'll show the revenue to him/her - the point is that I wish it to really understand the meaning of the question, so the previus statement will bring the same results as ""do I got any revenue in the last 10 days"" or something like that - BI (something like the Wolfram|Alpha engine).</p>

<p>I wonder if there's any opensource library or algorithm books or whatever that I can use to learn the subject. Regards to opensource libraries - I don't mind which language it'll be written in.</p>

<p>I've read about this subject and saw many engines and services (OpenNLP, Apache UIMA, CoreNLP etc.) but did not figure out if they're right for my needs.</p>

<p>Any answer or suggestion is welcome.
Many thanks!</p>
","algorithm, libraries, text-mining","<p>The field you're talking about is usually called ""natural language processing"".  It's hard, and an active field of research.  There are various libraries which you could consider based on your preferred programming language and use case:</p>

<p><a href=""http://en.wikipedia.org/wiki/List_of_natural_language_processing_toolkits"" rel=""nofollow"">http://en.wikipedia.org/wiki/List_of_natural_language_processing_toolkits</a></p>

<p>I've used NLTK a little bit.  This field is seriously difficult to get right, so you might want to try to restrict your application to some small set of verbs and nouns such that people are using a controlled vocabulary in the first instance, and then try to extend it beyond that.</p>
",0,1,131,2012-07-13 11:22:45,https://stackoverflow.com/questions/11469519/text-mining-analyse-user-commands-questions-algorithm-or-library
R tm package create matrix of Nmost frequent terms,"<p>I have a <code>termDocumentMatrix</code> created using the <code>tm</code> package in R.</p>

<p>I'm trying to create a matrix/dataframe that has the 50 most frequently occurring terms.</p>

<p>When I try to convert to a matrix I get this error:</p>

<pre><code>&gt; ap.m &lt;- as.matrix(mydata.dtm)
Error: cannot allocate vector of size 2.0 Gb
</code></pre>

<p>So I tried converting to sparse matrices using Matrix package:</p>

<pre><code>&gt; A &lt;- as(mydata.dtm, ""sparseMatrix"") 
Error in as(from, ""CsparseMatrix"") : 
  no method or default for coercing ""TermDocumentMatrix"" to ""CsparseMatrix""
&gt; B &lt;- Matrix(mydata.dtm, sparse = TRUE)
Error in asMethod(object) : invalid class 'NA' to dup_mMatrix_as_geMatrix
</code></pre>

<p>I've tried accessing the different parts of the tdm using:</p>

<pre><code>&gt; freqy1 &lt;- data.frame(term1 = findFreqTerms(mydata.dtm, lowfreq=165))
&gt; mydata.dtm[mydata.dtm$ Terms %in% freqy1$term1,]
Error in seq_len(nr) : argument must be coercible to non-negative integer
</code></pre>

<p>Here's some other info:</p>

<pre><code>&gt; str(mydata.dtm)
List of 6
 $ i       : int [1:430206] 377 468 725 3067 3906 4150 4393 5188 5793 6665 ...
 $ j       : int [1:430206] 1 1 1 1 1 1 1 1 1 1 ...
 $ v       : num [1:430206] 1 1 1 1 1 1 1 1 2 3 ...
 $ nrow    : int 15643
 $ ncol    : int 17207
 $ dimnames:List of 2
  ..$ Terms: chr [1:15643] ""000"" ""0mm"" ""100"" ""1000"" ...
  ..$ Docs : chr [1:17207] ""1"" ""2"" ""3"" ""4"" ...
 - attr(*, ""class"")= chr [1:2] ""TermDocumentMatrix"" ""simple_triplet_matrix""
 - attr(*, ""Weighting"")= chr [1:2] ""term frequency"" ""tf""
&gt; mydata.dtm
A term-document matrix (15643 terms, 17207 documents)

Non-/sparse entries: 430206/268738895
Sparsity           : 100%
Maximal term length: 54 
Weighting          : term frequency (tf)
</code></pre>

<p>My ideal output is something like this:</p>

<pre><code>term      frequency
the         2123
and         2095
able         883
...          ...
</code></pre>

<p>Any suggestions?</p>
","r, text-mining, tm, term-document-matrix","<p>The term-document matrices in tm are already created as sparse matrices. Here, <code>mydata.tdm$i</code> and <code>mydata.tdm$j</code> are the vectors of indexes of the matrix and <code>mydata.tdm$v</code> is the related vector of frequencies. So that you can create a sparse matrix writing :</p>

<pre><code>sparseMatrix(i=mydata.tdm$i, j=mydata.tdm$j, x=mydata.tdm$v)
</code></pre>

<p>Then you can use <code>rowSums</code> and link the rows, you're interested in, to the terms, they stand for, with <code>$Terms</code>.</p>
",9,7,5481,2012-07-16 16:42:46,https://stackoverflow.com/questions/11508728/r-tm-package-create-matrix-of-nmost-frequent-terms
Extract terminology from sentences quickly,"<p>I am working in Text Mining and my work is focused on biomedical entities (genes, proteins, drugs and diseases). I would like to share with you some questions.</p>

<p>Now, my goal is to find biomedical entities in biomedical text (from Medline) and through of dictionaries of terms, I can identify each entity found with its unique identifier.</p>

<p>To store text, dicitionaries and results, I am using MongoDB (a nonSQL database). Each abstract is splitted in sentences, and each sentence is store in a new record (with list of tokens, chunks and Part-of-Speech tags). To find entities, I get all senteces and for each one I create a regular expresion for each term in the dictionary (in Python):</p>

<pre><code>for term in dicitonary:
     matches = re.finditer(r'(' + term + ')', sentence)
     for m in matches:
          ini = m.start()
          end = m.end()
          result.append(ini, end, dictionary.get_identification[term])
</code></pre>

<p>But it is really slow, I have several subsets of 150,000 abstracts (>1,000,000 of sentences).</p>

<p>For me, it is really interesting soft-matching to extract more entities where their terminology is not exactly in my dictionary, but it can increase my running time.</p>

<p>I think that my problem is to do a lot of regular expressions (I have dictionary with 300,000 entries) for each sentence, because I have to find the terms in sentence. Without Machine Learning algorithm, how could you resolve this problem? And with ML algorithms? Now, I am flexible to change my programming language, databases...</p>

<p>Thank you very much!!!</p>

<p>Regards,</p>

<p>àlex.</p>
","python, mongodb, nlp, text-mining","<p>Instead of building one RE per <code>term</code>, build a single, disjunctive one that can catch all of them:</p>

<pre><code>pattern = re.compile(""(%s)"" % ""|"".join(re.escape(term) for term in dictionary))
</code></pre>

<p>then use <code>pattern.finditer</code>.</p>

<p>As for ""how to use machine learning"", that's far too broad a question, IMHO. Start out by googling for ""biomedical named entity recognition"" -- there's a vast amount of literature about that problem and assorted tools.</p>
",4,2,608,2012-07-19 09:31:46,https://stackoverflow.com/questions/11557863/extract-terminology-from-sentences-quickly
Loglikelihood similarity for document clustering,"<p>I’m using following loglikelihood formula to compare the similarity between a document and a cluster:
log p(d|c) = sum (c(w,d) * log p(w|c)); 
c(w,d) is the frequency of a word in a document and p(w|c) is the likelihood of word w being generated by a cluster c.</p>

<p>The problem is that based on this similarity the document is often assigned to the wrong cluster. If I assign the document to the cluster with the highest log p(d|c) (as it is usually negative value I take –log p(d|c)) then it will be the cluster that contains a lot of words from a document but the probability of these words in the cluster is low. 
If I assign the document to the cluster with the lowest log p(d|c) then it will be the cluster that has intersection with a document only in one word. 
Can someone explain me how to use the loglikelihood correctly? I try to implement this function in java. I already looked on google scholar, but didn’t found suitable explanation of loglikelihood in text mining. 
Thanks in advance</p>
","machine-learning, cluster-analysis, similarity, text-mining","<p>Your log likelihood formulation is correct for describing a document with a multinomial model (words in each document are generated independently from a multinomial distribution).</p>

<p>To get the maximum likelihood cluster assignment, you should be taking the cluster assignment, c, that maximizes log p(d|c). log p(d|c) should be a negative number - the maximum is the number closest to zero.</p>

<p>If you are getting cluster assignments that don't make sense, it is likely that this is because the multinomial model does not describe your data well. So, the answer to your question is most likely that you should either choose a different statistical model or use a different clustering method.</p>
",1,1,807,2012-07-21 10:23:21,https://stackoverflow.com/questions/11591355/loglikelihood-similarity-for-document-clustering
Replace string within a variable in R using fuzzy matching,"<p>I have data with 3 relevant variables: The first is an activity (x1), the second is the respondents rating of that activity (x2), and the third is the proper name of the activity in x1 (x3).  The x1 variables are respondent written, and very close matches to the reference variable of the activity x3, but all a little bit different.  I would like to match and replace all x1's with the reference x3 - I was thinking of using a loop referring to each reference activity x3 and replacing the x1 respondent written activity using a program like agrep.  However, agrep seems only to tell me what the matches are.  How can I replace the x1 variables with the ""correct"" string title in x3?</p>
","r, text-mining, matching","<p>in R, the function <code>agrep</code> returns the indices where it found a match, not the number of matches</p>

<pre><code>agrep('chrg', c('charge', 'trapper', 'friend', 'charger'))
# [1] 1 4
</code></pre>

<p>If you would like to have the value instead of the index, you can pass <code>value=TRUE</code>.</p>

<pre><code>agrep('chrg', c('charge', 'trapper', 'friend', 'charger'), value=TRUE)
# [1] ""charge""  ""charger""
</code></pre>

<p><strong>EDIT after your update:</strong></p>

<p>If <code>x1</code> and <code>x3</code> are in phase (for each index you have the names of the <em>same</em> activity) here is a snippet that will do the trick.</p>

<pre><code>subs &lt;- function(x, old, new) {
    # Replace 'old' by 'new' in 'x'.
    matchv &lt;- match(x, old, nomatch=0)
    replace(x, matchv &gt; 0, new[matchv])
}
# y is any vector that contains short names.
subs(y, x1, x3)
</code></pre>

<p>If they are not in phase you can create the <code>old</code> and <code>new</code> vectors as follows with <code>agrep</code>.</p>

<pre><code>oldnew &lt;- sapply(x1, function(x) { agrep(x, x3, value=TRUE)[1] })
subs(y, names(oldnew), oldnew)
</code></pre>
",1,2,1108,2012-07-28 17:54:14,https://stackoverflow.com/questions/11703810/replace-string-within-a-variable-in-r-using-fuzzy-matching
Why loading a model takes so much time for me in R?,"<p>For a personal project I need to run several machine learning algorithms against different texts in order to classify them.</p>

<p>I used to do this using RapidMiner but I decided to move all my development to R as I feel I have more control with it.</p>

<p>The issue I am seeing now (which I did not notice with RapidMiner) is that loading the models is taking a lot of time.</p>

<p><strong>For example:</strong></p>

<p>I have a model which checks if it the text refers to sports. 
The model is <code>37.7 MB</code> and it takes <code>8:34</code> with my <code>2.2 GH i7 Mac with 4GB of RAM</code></p>

<p><strong>The way I am calling the model is the following:</strong></p>

<pre><code>fileNameMatrix = paste(query,query1,""-matrix.Rd"", sep ="""")
fileNameModel= paste(query,query1,""-model.Rd"", sep ="""")

load(fileNameMatrix)
load(fileNameModel)
</code></pre>

<p>The model was generated using <code>RTextTools</code></p>

<p>Those query variables you read are because I need to call almost 20 models and compare them against different datasets. That is why although 8 minutes is not a lot, when I read all of them its almost 3 hours just on loading which makes my task almost useless considering its an almost real time task.</p>

<p>Which factors should I consider to reduce loading time if reducing the size of the model is not an option? </p>

<p>One other thing I consider suspicious is that while the matrix file is rather small <code>64KB</code> the model is still <code>37.7MB</code>. Is it possible that the model file is bigger than necessary? Have anyone experienced something similar using RTextTools?</p>

<p>This is one of my firsts tasks using models in R so excuse me if I am doing somethings which is obviously wrong.</p>

<p>Thanks a lot for your time and any tip in the right direction will be much appreciated!</p>
","r, machine-learning, text-mining","<p>Have you checked the RAM usage in your Activity Monitor? Compressed <code>RData</code> files are relatively tiny, but they uncompress to be massive. For instance, an <code>n x n</code> matrix of all <code>0</code>'s will take up essentially no space for any <code>n</code> (that may explain your small matrix size).  Your loaded model might then be huge; I have some <code>RData</code> files that amount to maybe <code>200 MB</code> but that cannot be loaded in memory in <code>R</code>. This could become a problem if you're running low on RAM, as your computer may attempt to use drive space to load the files.</p>
",2,1,1044,2012-08-23 03:21:52,https://stackoverflow.com/questions/12084303/why-loading-a-model-takes-so-much-time-for-me-in-r
does mallet have a GUI?,"<p>Has anyone seen a GUI for <a href=""http://mallet.cs.umass.edu"" rel=""noreferrer"">Mallet</a>?  </p>

<p>Thanks</p>
","user-interface, machine-learning, text-mining, mallet","<p><a href=""http://code.google.com/p/topic-modeling-tool/"" rel=""nofollow"">http://code.google.com/p/topic-modeling-tool/</a></p>

<p>you might also be interested in:</p>

<p><a href=""http://electricarchaeology.ca/2011/11/11/topic-modeling-with-the-java-gui-gephi/"" rel=""nofollow"">http://electricarchaeology.ca/2011/11/11/topic-modeling-with-the-java-gui-gephi/</a></p>
",4,5,1062,2012-09-13 22:55:25,https://stackoverflow.com/questions/12415950/does-mallet-have-a-gui
How can i cluster document using k-means (Flann with python)?,"<p>I want to cluster documents based on similarity.</p>

<p>I haved tried ssdeep (similarity hashing), very fast but i was told that k-means is faster and flann is fastest of all implementations, and more accurate so i am trying flann with python bindings but i can't find any example how to do it on text (it only support array of numbers).</p>

<p>I am very very new to this field (k-means, natural language processing). What i need is speed and accuracy.</p>

<p>My questions are: </p>

<ol>
<li>Can we do document similarity grouping / Clustering using KMeans (Flann do not allow any text input it seems )</li>
<li>Is Flann the right choice? If not please suggest me High performance library that support text/docs clustering, that have python wrapper/API.</li>
<li>Is k-means the right algorithm?</li>
</ol>
","nlp, cluster-analysis, data-mining, k-means, text-mining","<p>You need to represent your document as an array of numbers (aka, a vector).  There are many ways to do this, depending on how sophisticated you want to be, but the simplest way is just to represent is as a vector of word counts.</p>

<p>So here's what you do:</p>

<ol>
<li><p>Count up the number of times each word appears in the document.</p></li>
<li><p>Choose a set of ""feature"" words that will be included in your vector.  This should exclude extremely common words (aka ""stopwords"") like ""the"", ""a"", etc.</p></li>
<li><p>Make a vector for each document based on the counts of the feature words.</p></li>
</ol>

<p>Here's an example.</p>

<p>If your ""documents"" are single sentences, and they look like (one doc per line):</p>

<pre><code>there is a dog who chased a cat
someone ate pizza for lunch
the dog and a cat walk down the street toward another dog
</code></pre>

<p>If my set of feature words are <code>[dog, cat, street, pizza, lunch]</code>, then I can convert each document into a vector:</p>

<pre><code>[1, 1, 0, 0, 0]  // dog 1 time, cat 1 time
[0, 0, 0, 1, 1]  // pizza 1 time, lunch 1 time
[2, 1, 1, 0, 0]  // dog 2 times, cat 1 time, street 1 time
</code></pre>

<p>You can use these vectors in your k-means algorithm and it will hopefully group the first and third sentence together because they are similar, and make the second sentence a separate cluster since it is very different.</p>
",20,11,13716,2012-09-19 14:51:13,https://stackoverflow.com/questions/12497252/how-can-i-cluster-document-using-k-means-flann-with-python
How to count the number of sentences in a text in R?,"<p>I read a text into R using the <code>readChar()</code> function. I aim at testing the hypothesis that the sentences of the text have as many occurrences of letter ""a"" as occurrences of letter ""b"". I recently discovered the <code>{stringr}</code> package, which helped me a great deal to do useful things with my text such as counting the number of characters and the total number of occurrences of each letter in the entire text. Now, I need to know the number of sentences in the whole text. Does R have any function, which can help me do that? Thank you very much!</p>
","r, text-mining","<p>Thank you @gui11aume for your answer. A very good package I just found that can help do the work is <code>{openNLP}</code>. This is the code to do that:</p>

<pre><code>install.packages(""openNLP"") ## Installs the required natural language processing (NLP) package
install.packages(""openNLPmodels.en"") ## Installs the model files for the English language
library(openNLP) ## Loads the package for use in the task
library(openNLPmodels.en) ## Loads the model files for the English language

text = ""Dr. Brown and Mrs. Theresa will be away from a very long time!!! I can't wait to see them again."" ## This sentence has unusual punctuation as suggested by @gui11aume

x = sentDetect(text, language = ""en"") ## sentDetect() is the function to use. It detects and seperates sentences in a text. The first argument is the string vector (or text) and the second argument is the language.
x ## Displays the different sentences in the string vector (or text).

[1] ""Dr. Brown and Mrs. Theresa will be away from a very long time!!! ""
[2] ""I can't wait to see them again.""

length(x) ## Displays the number of sentences in the string vector (or text).

[1] 2
</code></pre>

<p>The <code>{openNLP}</code> package is really great for natural language processing in R and you can find a good and short intro to it <a href=""https://docs.google.com/viewer?a=v&amp;q=cache:8-Jur-gt8YAJ%3acran.r-project.org/web/packages/openNLP/vignettes/openNLP.pdf%20&amp;hl=fr&amp;gl=us&amp;pid=bl&amp;srcid=ADGEEShvBkPPtxSrvvMx7HDpGyj_H7y70aUg8BshK5XqMZ6ZGqttMdwfJ9rbWBU-4XDx_1JnhezTbkkhQ8EguEsK1D5dm3f3v9SfJlEJAezW4zVbh6kHAMfFyKQ7zqvH_l4pYrR3ypQC&amp;sig=AHIEtbTsZrcI1ahhYG1arZ6GNLTuXd5aKA"">here</a> or you can check out the package's documentation <a href=""https://docs.google.com/viewer?a=v&amp;q=cache%3ac3QktuUpX3oJ%3acran.r-project.org/web/packages/openNLP/openNLP.pdf%20&amp;hl=fr&amp;gl=us&amp;pid=bl&amp;srcid=ADGEEShYHnZ2a0hvH_Gli-5OO7V_3-xAmPe2IEM_sSrZheAIACd-123jau2Eql4KW5Fuc6jyuPxq64KSnofr9ara8bnzRkGj6O-Upu2nzlcktuTtI6N4tnnQ09ZHrQf2GUThICf9P4Af&amp;sig=AHIEtbSSjE27yDgWRsrm-cLTndWLgKH6dA"">here</a>.</p>

<p>Three more languages are supported in the package. You just need to install and load the corresponding model files.</p>

<ol>
<li><code>{openNLPmodels.es}</code> for Spanish </li>
<li><code>{openNLPmodels.ge}</code> for German </li>
<li><code>{openNLPmodels.th}</code> for Thai</li>
</ol>
",13,8,7634,2012-09-26 08:51:12,https://stackoverflow.com/questions/12602652/how-to-count-the-number-of-sentences-in-a-text-in-r
Why am I getting error? ValueError: chunk structures must contain tagged tokens or trees,"<p>I've been tinkering with NLTK with the aim of extracting entities from some news articles, but I keep getting an error:</p>
<blockquote>
<p>ValueError: chunk structures must contain tagged tokens or trees.</p>
</blockquote>
<p>Here's my code:</p>
<pre><code>import lxml.html
import nltk, re, pprint 



def ie_preprocess(document):
    &quot;&quot;&quot;This function takes raw text and chops and then connects the process to break     
       it down into sentences, then words and then complete part-of-speech tagging&quot;&quot;&quot;
    sentences = nltk.sent_tokenize(document)
    sentences = [nltk.word_tokenize(sent) for sent in sentences]
    sentences = [nltk.pos_tag(sent) for sent in sentences]
    return sentences


    #import story
    base_url = &quot;http://www.thisisstaffordshire.co.uk/Yobs-pelt-999-crews-bottles-fireworks-Shelton/story-17256383-detail/story.html&quot;
    page = lxml.html.parse(base_url)
    story = page.xpath('//*[@id=&quot;story&quot;]/div[2]/div[1]')
    raw_text = story[0].text_content()
    #tokenize
    output = ie_preprocess(raw_text)
    print output
    #chunk
    grammar = r'''
       NP: 
       {&lt;DT&gt;&lt;NN.*&gt;&lt;.*&gt;*&lt;NN.*&gt;} 
       '''
    cp = nltk.RegexpParser(grammar)

    chunked = cp.parse(output)

    print chunked
</code></pre>
<h2>Update</h2>
<p>Here's the error message in full:</p>
<pre><code>Traceback (most recent call last):
  File &quot;geo_locator.py&quot;, line 30, in &lt;module&gt;
    chunked = cp.parse(output)
  File &quot;/Users/davidelks/pythontests/venv/lib/python2.7/site-packages/nltk/chunk/regexp.py&quot;, line 1183, in parse
    chunk_struct = parser.parse(chunk_struct, trace=trace)
  File &quot;/Users/davidelks/pythontests/venv/lib/python2.7/site-packages/nltk/chunk/regexp.py&quot;, line 999, in parse
     chunkstr = ChunkString(chunk_struct)
  File &quot;/Users/davidelks/pythontests/venv/lib/python2.7/site-packages/nltk/chunk/regexp.py&quot;, line 93, in __init__
tags = [self._tag(tok) for tok in self._pieces]
  File &quot;/Users/davidelks/pythontests/venv/lib/python2.7/site-packages/nltk/chunk/regexp.py&quot;, line 103, in _tag
    raise ValueError('chunk structures must contain tagged '
ValueError: chunk structures must contain tagged tokens or trees
</code></pre>
","python, nltk, text-mining","<p>The <code>parse()</code> function can only handle one sentence at a time. </p>

<p>This works: </p>

<pre><code>chunked = []
for s in output:
    chunked.append(cp.parse(s))
</code></pre>

<p>Result:</p>

<pre><code>[Tree('S', [(u'POLICE', 'NN'), (u'are', 'VBP'), (u'hunting', 'VBG'), ... 
</code></pre>
",9,4,2868,2012-11-07 12:11:22,https://stackoverflow.com/questions/13269543/why-am-i-getting-error-valueerror-chunk-structures-must-contain-tagged-tokens
Weka ARFF generation,"<p>I am trying to generate an .arff file from a csv data file I have. Now I am totally new to Weka and have started using it just a day back. I am trying out a simple twitter sentiment analysis with this for starters. I have generated training data in CSV. Contents of CSV file are as follows:</p>

<pre><code>  tweet,affinScore,polarity
 ATAUTHORcfoblog is giving away a $25 Amex gift card (enter to win over $600 in prizes!) http://t.co/JD8EP14c ,4,4
""American Express has always been my dark horse acquirer of  ATAUTHORFoursquare. Bundle in Square-like payments &amp; its a lite-retailer platform, no? "",0,1
African-American Demos Express Ethnic Identity Differently http://t.co/gInv4bKj via  ATAUTHORmediapost ,0,3
Google ???????? Visa ? American Express  http://t.co/eEZTSiHY ,0,4
Secrets to Success from Small-Business Owners : Lifestyle :: American Express OPEN Forum http://t.co/b85F8JX0 via  ATAUTHOROpenForum ,2,1
RT  ATAUTHORhunterwalk: American Express has always been my dark horse acquirer of  ATAUTHORFoursquare. Bundle in Square-like payments &amp; its a lite ... ,0,1
Winning Surveys $1500 american express Huggies Sweeps http://t.co/WoaTFowp ,4,1
I root for Square mostly because a small business that takes Square is also one that takes American Express. ,0,1
I dont know how bitch be acting American Express but they cards be saying DEBIT ON IT HAVE A ?? PLEASE!!! ,-5,2
Uh oh... RT  ATAUTHORBlackArrowBella: I dont know how bitch be acting American Express but they cards be saying DEBIT ON IT HAVE A ?? PLEASE!!! ,-5,2
Just got another credit card. A Blue Sky card with American Express. Its gonna help pay for the honeymoon!  ATAUTHORAmericanExpress ,-1,1
Follow  ATAUTHORShaveMagazine and ReTweet this msg to be entered to #Win an American Express Gift card. Winners contacted bi-weekly by direct msg! ,2,4
American Express Gold zakelijk aanvragen: http://t.co/xheZwmbt ,0,3
RT  ATAUTHORhunterwalk: American Express has always been my dark horse acquirer of  ATAUTHORFoursquare. Bundle in Square-like payments &amp; its a lite ... ,0,1
</code></pre>

<p>Here first attribute is actual tweet, second is AFFIN score and third is actual classification class (1- Positive, 2-Negative, 3-Neutral, 4-Spam)</p>

<p>Now I try to generate .arff format from it using code:</p>

<pre><code>import weka.core.Instances;
import weka.core.converters.ArffSaver;
import weka.core.converters.CSVLoader;

import java.io.File;

public class CSV2Arff {
  /**
   * takes 2 arguments:
   * - CSV input file
   * - ARFF output file
   */
  public static void main(String[] args) throws Exception {
    if (args.length != 2) {
      System.out.println(""\nUsage: CSV2Arff &lt;input.csv&gt; &lt;output.arff&gt;\n"");
      System.exit(1);
    }

    // load CSV
    CSVLoader loader = new CSVLoader();
    loader.setSource(new File(args[0]));
    Instances data = loader.getDataSet();

    // save ARFF
    ArffSaver saver = new ArffSaver();
    saver.setInstances(data);
    saver.setFile(new File(args[1]));
    saver.setDestination(new File(args[1]));
    saver.writeBatch();
  }
}
</code></pre>

<p>This generates .arff file that looks somewhat like:</p>

<pre><code>   @relation file

@attribute tweet {_ATAUTHORcfoblog_is_giving_away_a_$25_Amex_gift_card_(enter_to_win_over_$600_in_prizes!)_http://t.co/JD8EP14c_,'American_Express_has_always_been_my_dark_horse_acquirer_of__ATAUTHORFoursquare._Bundle_in_Square-like_payments_&amp;_its_a_lite-retailer_platform,_no?_',African-American_Demos_Express_Ethnic_Identity_Differently_http://t.co/gInv4bKj_via__ATAUTHORmediapost_,Google_????????_Visa_?_American_Express__http://t.co/eEZTSiHY_,Secrets_to_Success_from_Small-Business_Owners_:_Lifestyle_::_American_Express_OPEN_Forum_http://t.co/b85F8JX0_via__ATAUTHOROpenForum_,RT__ATAUTHORhunterwalk:_American_Express_has_always_been_my_dark_horse_acquirer_of__ATAUTHORFoursquare._Bundle_in_Square-like_payments_&amp;_its_a_lite_..._

@data
_ATAUTHORcfoblog_is_giving_away_a_$25_Amex_gift_card_(enter_to_win_over_$600_in_prizes!)_http://t.co/JD8EP14c_,4,4
'American_Express_has_always_been_my_dark_horse_acquirer_of__ATAUTHORFoursquare._Bundle_in_Square-like_payments_&amp;_its_a_lite-retailer_platform,_no?_',0,1
African-American_Demos_Express_Ethnic_Identity_Differently_http://t.co/gInv4bKj_via__ATAUTHORmediapost_,0,3
Google_????????_Visa_?_American_Express__http://t.co/eEZTSiHY_,0,4
Secrets_to_Success_from_Small-Business_Owners_:_Lifestyle_::_American_Express_OPEN_Forum_http://t.co/b85F8JX0_via__ATAUTHOROpenForum_,2,1
RT__ATAUTHORhunterwalk:_American_Express_has_always_been_my_dark_horse_acquirer_of__ATAUTHORFoursquare._Bundle_in_Square-like_payments_&amp;_its_a_lite_..._,0,1
</code></pre>

<p>I am new to Weka but from what I have read, I have a suspicion that this ARFF is not correctly formed. Can anyone comment on it?</p>

<p>Also if it is wrong, can someone point me to where exactly am I going wrong?</p>
","machine-learning, weka, text-mining, arff","<p>Make sure to set the type of the <code>tweet</code> attribute to be arbitrary strings, not a <em>categorial</em> attribute, which seems to be the default. This doesn't scale well, as it puts a copy of every tweet in the type definition otherwise.</p>

<p>Note that for actual analysis of the tweet contents, you will likely need to preprocess them much further. You will likely want a sparse vector representation of the text instead of a long string.</p>
",0,0,1393,2012-11-13 20:41:23,https://stackoverflow.com/questions/13368424/weka-arff-generation
looking for twit and text message style stopwords,"<p>I have used R for mining the tweets and I got the most frequent words used in the tweets. However the most frequent words are like this:</p>

<pre><code> [1] ""cant""     ""dont""     ""girl""     ""gonna""    ""lol""      ""love""    
 [7] ""que""      ""thats""    ""watching"" ""wish""     ""youre""  
</code></pre>

<p>I am looking for trends and names and events in the texts.
I am wondering if there is a way to remove this text message style words (such as gonna,wanna, ...) from the corpus? Is there any stopwords for them? 
any help would be appreciated.</p>
","r, nlp, text-mining, stop-words","<p>The text mining package maintains it's own list of stopwords and provides useful tools for managing and summarizing this type of text. </p>

<p>Let's say your tweets are stored in a vector. </p>

<pre><code>library(tm)
words &lt;- vector_of_strings
corpus &lt;- Corpus(VectorSource(words))
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, function(x) tolower(x))
corpus &lt;- tm_map(corpus, function(x) removeWords(x, 
                stopwords()))
</code></pre>

<p>You can use the last line with your own list of stopwords(): </p>

<pre><code>stoppers &lt;- c(stopwords(), ""gonna"", ""wanna"", ""lol"", ... ) 
</code></pre>

<p>Unfortunately, you'll have to generate your own list of ""text messaging"" or ""internet messaging"" stopwords. </p>

<p>But, you could cheat a bit, by borrowing from NetLingo ( <a href=""http://vps.netlingo.com/acronyms.php"" rel=""nofollow"">http://vps.netlingo.com/acronyms.php</a> ) </p>

<pre><code>library(XML)
theurl &lt;- ""http://vps.netlingo.com/acronyms.php""
h &lt;- htmlParse(theurl)
h &lt;- getNodeSet(h,""//ul/li/span//a"")
stoppers &lt;- sapply(h,xmlValue)
</code></pre>
",4,3,364,2012-11-26 04:33:27,https://stackoverflow.com/questions/13558703/looking-for-twit-and-text-message-style-stopwords
number of words in a corpus,"<p>I am looking for a way to find the most frequent words in a text and I am using R.
by most frequent, I mean words which their low frequency are 1% of the words in the corpus. So I need to calculate the number of words in a corpus. </p>

<p>Here is my code, so far: </p>

<pre><code>#!/usr/bin/Rscript
library('tm')
library('wordcloud')
library('RColorBrewer')
twittercorpus &lt;- system.file(""stream"",""~/txt"", package = ""tm"")
twittercorpus &lt;- Corpus(DirSource(""~/txt""),
                        readerControl=list(languageEl = ""en""))
twittercorpus &lt;- tm_map(twittercorpus, removeNumbers)
twittercorpus &lt;- tm_map(twittercorpus,tolower)
twittercorpus &lt;- tm_map(twittercorpus,removePunctuation)
my_stopwords &lt;- c(stopwords(""SMART""))
twittercorpus &lt;-tm_map(twittercorpus,removeWords,my_stopwords)
mydata.dtm &lt;- TermDocumentMatrix(twittercorpus)
</code></pre>

<p>I need something like:</p>

<pre><code>freqmatrix &lt;-findFreqTerms(mydata.dtm, lowfreq=rowSums(mydata.dtm)/100)
</code></pre>
","r, text-mining","<p>if you look at <code>str(mydata.dtm)</code> there is a named  component called <code>nrow</code>.  Use that:</p>

<pre><code>freqmatrix &lt;- findFreqTerms(mydata.dtm, lowfreq=mydata.dtm$nrow/100)
</code></pre>
",7,4,2105,2012-11-26 22:34:50,https://stackoverflow.com/questions/13574341/number-of-words-in-a-corpus
How to read a stop word list from a text file in R,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/3804292/read-a-text-file-into-r"">Read a Text File into R</a>  </p>
</blockquote>



<p>I have a custom stopword list which is in a text file separated with newline character.How can I use the that file in my R script?</p>

<p><strong>Code:</strong></p>

<pre><code>my_stopwords &lt;- c(stopwords(),""aint"",""wanna"",""gonna"",...)
</code></pre>

<p>The only problem is I want to read the words from the file instead of hardcoding them like above. My text file looks like this:</p>

<pre><code>""aint""
""wanna""
""gonna""
...
</code></pre>

<p>Thanks in advance.</p>
","r, text-mining, stop-words","<p>A new-line-separated file could technically be considered a valid CSV file. Try <code>read.csv()</code> to get the list in as a data.frame. You may want to <code>unlist</code> it or just access the first column to get it in an array like what you have.</p>
",1,1,2324,2012-11-29 05:24:44,https://stackoverflow.com/questions/13619402/how-to-read-a-stop-word-list-from-a-text-file-in-r
R: Extract value and lines after key word (text file mining),"<p>Setting:
I have (simple) .csv and .dat files created from laboratory devices and other programs storing information on measurements or calculations. I have found this for other languages but nor for R</p>

<p>Problem:
Using R, I am trying to extract values to quickly display results w/o opening the created files. Hereby I have two typical settings:</p>

<p>a) I need to read a priori unknown values after known key words
b) I need to read lines after known key words or lines</p>

<p>I can't make functions such as scan() and grep() work.</p>

<p>c) Finally I would like to loop over dozens of files in a folder and give me a summary (to make the picture complete: I will manage this part)</p>

<p>I woul appreciate any form of help.</p>
","r, keyword, text-mining","<p>ok, it works for the key value (although perhaps not very nice)</p>

<pre><code> variable&lt;-scan(""file.csv"", what=character(),sep="""")
</code></pre>

<p>returns a charactor vector of everything</p>

<pre><code> variable[grep(""keyword"", ks)+2]     # + 2 as the actual value is stored two places ahead
</code></pre>

<p>returns characters of seaked values.</p>

<pre><code> as.numeric(lapply(variable, gsub, patt="","", replace="".""))
</code></pre>

<p>for completion: data had to be altered to number and "","" and ""."" problem needed to be solved.</p>

<p>in a line:
     data=as.numeric(lapply(ks[grep(""Ks_Boden"", ks)+2], gsub, patt="","", replace="".""))</p>

<p>Perseverence is not to bad of an asset ;-)</p>

<p>The rest isn't finished, yet, I will post once finished.</p>
",2,4,2470,2012-12-09 15:24:47,https://stackoverflow.com/questions/13788774/r-extract-value-and-lines-after-key-word-text-file-mining
"Databases for filtering pop culture quotes or celebrity-related data out of text (e.g., tweets)?","<p>I am trying to mine social media data, such as tweets. However, social media data have a lot of noise- for example people discussing celebrities or quoting a movie/TV/song, that is something most generally that is not about themselves or somebody they actually know personally. </p>

<p>So, is: <strong>are there any dynamic (i.e., automatically updated) databases on the most popular current celebrities</strong>? Movie quotes that they are in or song lyrics that they sing would also be relevant. </p>
","twitter, social-networking, text-mining, social-media","<p>I don't think such a curated list exists. Smaller ones do exist, for example the 100 top movies quotes on Wikipedia. However, these are not updated. </p>

<p>One possibility is to filter out the aspects of your input that appear on another social media site that tracks trends, such as <a href=""http://delicious.com/"" rel=""nofollow"">Delicious</a>. Unless you are looking for trends, something that rises to the top of two trending sites likely ... is just a trend.</p>

<p>Delicious has a <a href=""http://code.google.com/p/pydelicious/"" rel=""nofollow"">nice Python wrapper</a> for its API.</p>

<p>In Pythonic pseudocode, </p>

<pre><code> data = social-media.content
 data = filter(lambda datum: datum not in delicious.content-list,data)
</code></pre>
",1,2,158,2012-12-24 00:33:42,https://stackoverflow.com/questions/14015871/databases-for-filtering-pop-culture-quotes-or-celebrity-related-data-out-of-text
How do I create a DTM-like text matrix from a list of text blocks?,"<p>I have been using the <code>textmatrix()</code> function for a while to create DTMs which I can further use for LSI. </p>

<pre><code>dirLSA&lt;-function(dir){
  dtm&lt;-textmatrix(dir)
  return(lsa(dtm))
}

textdir&lt;-""C:/RProjects/docs""
dirLSA(textdir)

&gt; tm
$matrix
                  D1 D2 D3 D4 D5 D6 D7 D8 D9
1. 000             2  0  0  0  0  0  0  0  0
2. 20              1  0  0  1  0  0  1  0  0
3. 200             1  0  0  0  0  0  0  0  0
4. 2014            1  0  0  0  0  0  0  0  0
5. 2015            1  0  0  0  0  0  0  0  0
6. 27              1  0  0  0  0  0  0  1  0
7. 30              1  0  0  0  1  0  1  0  0
8. 31              1  0  2  0  0  0  0  0  0
9. 40              1  0  0  0  0  0  0  0  0
10. 45             1  0  0  0  0  0  0  0  0
11. 500            1  0  0  0  0  0  1  0  0
12. 600            1  0  0  0  0  0  0  0  0
728. bias          0  0  0  2  0  0  0  0  0
729. biased        0  0  0  1  0  0  0  0  0
730. called        0  0  0  1  0  0  0  0  0
731. calm          0  0  0  1  0  0  0  0  0
732. cause         0  0  0  1  0  0  0  0  0
733. chauhan       0  0  0  2  0  0  0  0  0
734. chief         0  0  0  8  0  0  1  0  0
</code></pre>

<p>Textmatrix() is a function which takes a directory(folder path) and returns a document-wise term frequency. This is used in further analysis like Latent Semantic Indexing/Allocation(LSI/LSA)</p>

<p>However, a new problem that came across me is that if I have tweet data in batch files (~500000 tweets/batch) and I want to carry out similar operations on this data.</p>

<p>I have code modules to clean up my data, and I want to pass the cleaned tweets directly to the LSI function. The problem I face is that the <code>textmatrix()</code> does not support it.</p>

<p>I tried looking at other packages and code snippets, but that didn't get me any further. Is there any way I can create a line-term matrix of sorts?</p>

<p>I tried sending <code>table(tokenize(cleanline[i]))</code> into a loop, but it wont add new columns for words not already there in the matrix. Any workaround?</p>

<p>Update: I just tried this:</p>

<pre><code>a&lt;-table(tokenize(cleanline[10]))
b&lt;-table(tokenize(cleanline[12]))

df1&lt;-data.frame(a)
df1
df2&lt;-data.frame(b)
df2

merge(df1,df2, all=TRUE)
</code></pre>

<p>I got this:</p>

<pre><code>&gt; df1
    Var1 Freq
1           6
2      ""    2
3    and    1
4   home    1
5   mabe    1
6 School    1
7   then    1
8   xbox    1
&gt; b&lt;-table(tokenize(cleanline[12]))
&gt; df2&lt;-data.frame(b)
&gt; df2
        Var1 Freq
1              13
2          ""    2
3  BillGates    1
4       Come    1
5       help    1
6        Mac    1
7       make    1
8  Microsoft    1
9     please    1
10   Project    1
11    really    1
12   version    1
13      wish    1
14     would    1
&gt; merge(df1,df2)
  Var1 Freq
1    ""    2
&gt; merge(df1,df2, all=TRUE)
        Var1 Freq
1               6
2              13
3          ""    2
4        and    1
5       home    1
6       mabe    1
7     School    1
8       then    1
9       xbox    1
10 BillGates    1
11      Come    1
12      help    1
13       Mac    1
14      make    1
15 Microsoft    1
16    please    1
17   Project    1
18    really    1
19   version    1
20      wish    1
21     would    1
</code></pre>

<p>I think I'm close.</p>
","r, text-mining, corpus","<p>Something that works for me:</p>

<pre><code>textLSA&lt;-function(text){

  a&lt;-data.frame(table(tokenize(text[1])))
  colnames(a)[2]&lt;-paste(c(""Line"",1),collapse=' ')
  df&lt;-a

  for(i in 1:length(text)){
    a&lt;-data.frame(table(tokenize(text[i])))
    colnames(a)[2]&lt;-paste(c(""Line"",i),collapse=' ')
    df&lt;-merge(df,a, all=TRUE)
  }

  df[is.na(df)]&lt;-0
  dtm&lt;-as.matrix(df[,-1])
  rownames(dtm)&lt;-df$Var1

  return(lsa(dtm))
}
</code></pre>

<p>What do you think of this code?</p>
",0,1,931,2013-01-03 08:25:44,https://stackoverflow.com/questions/14135303/how-do-i-create-a-dtm-like-text-matrix-from-a-list-of-text-blocks
Math of tm::findAssocs how does this function work?,"<p>I have been using <code>findAssoc()</code> with textmining (<code>tm</code> package) but realized that something doesn't seem right with my dataset. </p>

<p>My dataset is 1500 open ended answers saved in one column of csv file.
So I called the dataset like this and used typical <code>tm_map</code> to make it to corpus. </p>

<pre><code>library(tm)
Q29 &lt;- read.csv(""favoritegame2.csv"")
corpus &lt;- Corpus(VectorSource(Q29$Q29))
corpus &lt;- tm_map(corpus, tolower)
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, removeNumbers)
corpus&lt;- tm_map(corpus, removeWords, stopwords(""english""))
dtm&lt;- DocumentTermMatrix(corpus)

findAssocs(dtm, ""like"", .2)
&gt; cousin  fill  ....
  0.28    0.20      
</code></pre>

<p>Q1. When I find Terms associated with <code>like</code>, I don't see the output <code>like = 1</code> as part of the output. However, </p>

<pre><code>dtm.df &lt;-as.data.frame(inspect(dtm))
</code></pre>

<p>this dataframe consists of 1500 obs. of 1689 variables..(Or is it because the data is save in a row of csv file?) </p>

<p>Q2. Even though <code>cousin</code> and <code>fill</code> showed up once when the target term <code>like</code> showed up once, the score is different like this. Shouldn't they be same?</p>

<p>I'm trying to find the math of <code>findAssoc()</code> but no success yet. Any advice is highly appreciated!</p>
","r, text-mining","<pre><code> findAssocs
#function (x, term, corlimit) 
#UseMethod(""findAssocs"", x)
#&lt;environment: namespace:tm&gt;

methods(findAssocs )
#[1] findAssocs.DocumentTermMatrix* findAssocs.matrix*   findAssocs.TermDocumentMatrix*

 getAnywhere(findAssocs.DocumentTermMatrix)
#-------------
A single object matching ‘findAssocs.DocumentTermMatrix’ was found
It was found in the following places
  registered S3 method for findAssocs from namespace tm
  namespace:tm
with value

function (x, term, corlimit) 
{
    ind &lt;- term == Terms(x)
    suppressWarnings(x.cor &lt;- cor(as.matrix(x[, ind]), as.matrix(x[, 
        !ind])))
</code></pre>

<p>That was where self-references were removed.</p>

<pre><code>    findAssocs(x.cor, term, corlimit)
}
&lt;environment: namespace:tm&gt;
#-------------
 getAnywhere(findAssocs.matrix)
#-------------
A single object matching ‘findAssocs.matrix’ was found
It was found in the following places
  registered S3 method for findAssocs from namespace tm
  namespace:tm
with value

function (x, term, corlimit) 
sort(round(x[term, which(x[term, ] &gt; corlimit)], 2), decreasing = TRUE)
&lt;environment: namespace:tm&gt;
</code></pre>
",7,3,11702,2013-01-10 21:01:20,https://stackoverflow.com/questions/14267199/math-of-tmfindassocs-how-does-this-function-work
partial string matching reification in R,"<p>I need to reconcile similar string to one the longest string. I am stuck with agrep, how would you do it?</p>

<p><strong>Update: better problem definition</strong>
I have a .csv of addresses of bakeries and I want to know (1) how many branches a bakery has and (2) how many businesses there are.</p>

<p>I therefore extract only the names of the bakeries (see sample below) and now I need to find all the strings that are so similiar that they represent the same business. Then I count the business and the number of branches.</p>

<pre><code>c(""5 Meister Bäcker"", ""5 Meister Bäcker H. Breunung GmbH &amp; Co. KG"", 
""5 Meister Bäcker Hubert Breunung Amthorpassage Ba..."", ""5 Meister Bäcker Hubert Breunung Bäckerei"", 
""5 Meister Bäcker Hubert Breunung Backshop"", ""5 Meister Bäcker Hubert Breunung Backshop"", 
""5 Meister Bäcker Hubert Breunung Backshop"", ""5 Meister Bäcker Hubert Breunung Backshop"", 
""5 Meister Bäcker Hubert Breunung Backshop"", ""5 Meister Bäcker Hubert Breunung Backshop"", 
""5 Meister Bäcker Hubert Breunung GmbH &amp; Co. KG"", ""5 Meister Bäcker Hubert Breunung GmbH &amp; Co. KG"", 
""5 Meister Bäcker Hubert Breunung GmbH &amp; Co. KG"", ""5 Meister Bäcker Hubert Breunung GmbH &amp; Co. KG"", 
""5 Meister Bäcker Hubert Breunung GmbH &amp; Co. KG"", ""5 Meister Bäcker Hubert Breunung"", 
""5 Meister Bäcker Verwaltung"", ""Abel Backwaren"", ""Abele Bäckerei Filiale"", 
""Abele Bruno Bäckerei"", ""Abele Eugen Bäckerei"", ""Abele Stefan Bäckerei"", 
""Abel Lutz Feinbäckerei"", ""Abelmann Guido Bäckerei"", ""Abelmann Guido Bäckerei"", 
""Abeln"", "" 05432 6 79"", ""Abeln Der City Bäcker"", ""Abeln"", "" 05933 18 12"", 
""Abeln"", "" 05432 90 46 10"", ""Abeln"", "" 04471 72 93"", ""Abeln"", 
"" 05431 90 40 87"", ""Abeln"", "" 04471 93 03 47"", ""Abeln der City Bäcker"", 
""Abeln Der City Bäcker"", ""Abeln Der City Bäcker"", ""Abeln"", 
"" 05961 95 53 30"", ""Abeln Der City Bäcker"", ""Abeln mein City-Bäcker Bäckerei"", 
""Abeln"", "" 05933 64 76 23"", ""Abel Rupert Bäckerei und Konditorei"", 
""Abels Bäckerei"", ""Abenthum Heinrich Bäckerei und Konditorei"", 
""Aberle Hanno Bäckerei"", ""Abholzer Bäckerei Inh. Bernard u. Rosi Sproß"")
</code></pre>
","r, text-mining","<p>Is is not clear what do you want to do. I assume taht for each elements in you vector data , you try to find the similar longest string, or the longest string within the similars(agrep result).</p>

<ol>
<li><p>You can first remove all the dupliacted( no sens for me to do a grep , when there is exact matching)</p>

<pre><code>stats.nd &lt;- dat[!duplicated(dat)]   #I remove duplicated, but below
                                    # I will do the job in the origin vector
</code></pre></li>
<li><p>Assuming <code>dat</code> is your string vector , you can do smething like this :</p>

<pre><code>ll.lmatch &lt;- sapply(seq_along(dat),function(x){
         ll &lt;- agrep(pattern=dat[x],         # for each string
                     dat[-x],                # I search between the others strings 
                     value=T,max.distance=0.5)    # I set the Levenshtein distance
         ll[which.max(rank(ll))]                  # I choose the longest using rank
})
res &lt;- cbind(dat,dat.match)
</code></pre></li>
</ol>
",1,0,843,2013-01-11 13:18:48,https://stackoverflow.com/questions/14279042/partial-string-matching-reification-in-r
How to write custom removePunctuation() function to better deal with Unicode chars?,"<p>In the source code of the tm text-mining R-package, in file <a href=""https://r-forge.r-project.org/scm/viewvc.php/pkg/R/transform.R?view=markup&amp;root=tm"" rel=""nofollow"">transform.R</a>, there is the <code>removePunctuation()</code> function, currently defined as:</p>

<pre><code>function(x, preserve_intra_word_dashes = FALSE)
{
    if (!preserve_intra_word_dashes)
        gsub(""[[:punct:]]+"", """", x)
    else {
        # Assume there are no ASCII 1 characters.
        x &lt;- gsub(""(\\w)-(\\w)"", ""\\1\1\\2"", x)
        x &lt;- gsub(""[[:punct:]]+"", """", x)
        gsub(""\1"", ""-"", x, fixed = TRUE)
    }
}
</code></pre>

<p>I need to parse and mine some abstracts from a science conference (fetched from their website as UTF-8). The abstracts contain some unicode characters that need to be removed, particularly at word boundaries. There are the usual ASCII punctuation characters, but also a few Unicode Dashes, Unicode Quotes, Math Symbols... </p>

<p>There are also URLs in the text, and there the punctuation the intra-word punctuation characters need to be preserved. tm's built-in <code>removePunctuation()</code> function is too radical.</p>

<p>So I need a custom <code>removePunctuation()</code> function to do removal according to my requirements.</p>

<p>My custom Unicode function looks like this now, but it does not work as expected. I am using R only rarely, so getting things done in R takes some time, even for the simplest tasks. </p>

<p><strong>My function:</strong></p>

<pre><code>corpus &lt;- tm_map(corpus, rmPunc =  function(x){ 
# lookbehinds 
# need to be careful to specify fixed-width conditions 
# so that it can be used in lookbehind

x &lt;- gsub('(.*?)(?&lt;=^[[:punct:]’“”:±&lt;/&gt;]{5})([[:alnum:]])',"" \\2"", x, perl=TRUE) ;
x &lt;- gsub('(.*?)(?&lt;=^[[:punct:]’“”:±&lt;/&gt;]{4})([[:alnum:]])',"" \\2"", x, perl=TRUE) ;
x &lt;- gsub('(.*?)(?&lt;=^[[:punct:]’“”:±&lt;/&gt;]{3})([[:alnum:]])',"" \\2"", x, perl=TRUE) ;
x &lt;- gsub('(.*?)(?&lt;=^[[:punct:]’“”:±&lt;/&gt;]{2})([[:alnum:]])',"" \\2"", x, perl=TRUE) ;
x &lt;- gsub('(.*?)(?&lt;=^[[:punct:]’“”:±&lt;/&gt;])([[:alnum:]])',"" \\2"", x, perl=TRUE) ; 
# lookaheads (can use variable-width conditions) 
x &lt;- gsub('(.*?)(?=[[:alnum:]])([[:punct:]’“”:±]+)$',""\1 "", x, perl=TRUE) ;

# remove all strings that consist *only* of punct chars 
gsub('^[[:punct:]’“”:±&lt;/&gt;]+$',"""", x, perl=TRUE) ;

}
</code></pre>

<p>It does not work as expected. I think, it doesn't do anything at all.
The punctuation is still inside the terms-document matrix, see:</p>

<pre><code> head(Terms(tdm), n=30)

  [1] ""&lt;&gt;&lt;/&gt;""                      ""---""                       
  [3] ""--,""                        "":&lt;/&gt;""                      
  [5] "":()""                        ""/).""                       
  [7] ""/++""                        ""/++,""                      
  [9] ""..,""                        ""...""                       
 [11] ""...,""                       ""..)""                       
 [13] ""“”,""                        ""(|)""                       
 [15] ""(/)""                        ""(..""                       
 [17] ""(..,""                       ""()=(|=).""                  
 [19] ""(),""                        ""().""                       
 [21] ""(&amp;)""                        ""++,""                       
 [23] ""(0°""                        ""0.001),""                   
 [25] ""0.003""                      ""=0.005)""                   
 [27] ""0.006""                      ""=0.007)""                   
 [29] ""000km""                      ""0.01)"" 
...
</code></pre>

<p><strong>So my questions are:</strong></p>

<ol>
<li>Why doesn't the call to my function(){} have the desired effect? How can my
function be improved? </li>
<li>Are Unicode regex pattern classes such as if
<code>\P{ASCII}</code> or <code>\P{PUNCT}</code> supported in R's perl-compatible regular
expressions? I think they aren't (by default) by <a href=""http://www.regular-expressions.info/pcre.html"" rel=""nofollow"">PCRE:</a>: "" Only the support for various Unicode properties with \p is incomplete, though the most important ones are supported.""</li>
</ol>
","r, unicode, text-mining, tm","<p>As much as I like Susana's answer it is breaking the Corpus in newer versions of <em>tm</em> (No longer a PlainTextDocument and destroying the meta)</p>

<p>You will get a <em>list</em> and the following error:</p>

<pre><code>Error in UseMethod(""meta"", x) : 
no applicable method for 'meta' applied to an object of class ""character""
</code></pre>

<p>Using</p>

<pre><code>tm_map(your_corpus, PlainTextDocument)
</code></pre>

<p>will give you back your corpus but with broken $meta (in particular document ids will be missing.</p>

<p><strong>Solution</strong></p>

<p>Use <em>content_transformer</em></p>

<pre><code>toSpace &lt;- content_transformer(function(x,pattern)
    gsub(pattern,"" "", x))
your_corpus &lt;- tm_map(your_corpus,toSpace,""„"")
</code></pre>

<p><em>Source:</em>
Hands-On Data Science with R,
Text Mining,
Graham.Williams@togaware.com <a href=""http://onepager.togaware.com/"" rel=""nofollow noreferrer"">http://onepager.togaware.com/</a></p>

<h2>Update</h2>

<p>This function removes everything that is not alpha numeric (i.e. UTF-8 emoticons etc.)</p>

<pre><code>removeNonAlnum &lt;- function(x){
  gsub(""[^[:alnum:]^[:space:]]"","""",x)
}
</code></pre>
",2,8,5365,2013-01-11 15:26:34,https://stackoverflow.com/questions/14281282/how-to-write-custom-removepunctuation-function-to-better-deal-with-unicode-cha
R: merge text documents by index,"<p>I have a data frame that looks like this:</p>

<pre><code>_________________id ________________text______
    1   | 7821             | ""some text here""
    2   | 7821             |  ""here as well""
    3   | 7821             |  ""and here""
    4   | 567              |   ""etcetera""
    5   | 567              |    ""more text""
    6   | 231              |   ""other text""
</code></pre>

<p>And I would like to group the texts by IDs, so I can run a clustering algorithm:</p>

<pre><code>________________id___________________text______
    1   | 7821             | ""some text here here as well and here""
    2   | 567              |   ""etcetera more text""
    3   | 231              |   ""other text""
</code></pre>

<p>Is there any way to do this? I am importing from a database table and I have a lot of data, so I can't do it manually. </p>
","r, text-mining","<p>You're actually looking for <code>aggregate</code>, not <code>merge</code>, and there should be <em>many</em> examples on SO demonstrating different options for aggregation. Here's the most basic and direct approach, using the formula approach to specify which columns to <code>aggregate</code>.</p>

<p>Here's your data in a copy-and-pasteable form</p>

<pre><code>mydata &lt;- structure(list(id = c(7821L, 7821L, 7821L, 567L, 567L, 231L), 
    text = structure(c(6L, 3L, 1L, 2L, 4L, 5L), .Label = c(""and here"", 
    ""etcetera"", ""here as well"", ""more text"", ""other text"", ""some text here""
    ), class = ""factor"")), .Names = c(""id"", ""text""), class = ""data.frame"", 
    row.names = c(NA, -6L))
</code></pre>

<p>Here's the aggregated output.    </p>

<pre><code>aggregate(text ~ id, mydata, paste, collapse = "" "")
#     id                                 text
# 1  231                           other text
# 2  567                   etcetera more text
# 3 7821 some text here here as well and here
</code></pre>

<p>Of course, there is also <code>data.table</code>, which has nice compact syntax (and awesome speed):</p>

<pre><code>&gt; library(data.table)
&gt; DT &lt;- data.table(mydata)
&gt; DT[, paste(text, collapse = "" ""), by = ""id""]
     id                                   V1
1: 7821 some text here here as well and here
2:  567                   etcetera more text
3:  231                           other text
</code></pre>
",10,3,190,2013-01-28 16:37:43,https://stackoverflow.com/questions/14566703/r-merge-text-documents-by-index
Using Ruby to tag records that contain repeat phrases in a table,"<p>I'm trying to use Ruby to 'tag' records in a CSV table, based on whether or not a particular field contains a certain phrase that is repeated. I'm not sure if there are libraries to assist with this kind of job, and I recognize that Ruby might not be the most efficient language to do this sort of thing.</p>

<p>My CSV table contains a unique ID and a text field that I want to search:</p>

<pre><code>ID,NOTES
1,MISSING DOB; ID CANNOT BE BLANK
2,INVALID MEMBER ID - unable to verify
3,needs follow-up
4,ID CANNOT BE BLANK-- additional info needed
</code></pre>

<p>From this CSV table, I've extracted keywords and assigned them a tag, which I've stored in another CSV table.</p>

<pre><code>PHRASE,TAG
MISSING DOB,BLANKDOB
ID CANNOT BE BLANK,BLANKID
INVALID MEMBER ID,INVALIDID
</code></pre>

<p>Note that the NOTES column in my source contains punctuation and other phrases in addition to the phrases I have identified and want to map. Additionally, not all records have phrases that will match.</p>

<p>I want to create a table that looks something like this:</p>

<pre><code>ID, TAG
1, BLANKDOB
1, BLANKID
2, INVALIDID
4, BLANKID
</code></pre>

<p>Or, alternately with the tags delimited with another character:</p>

<pre><code>ID, TAG
1, BLANKDOB; BLANKID
2, INVALIDID
4, BLANKID
</code></pre>

<p>I have loaded the mapping table into a hash, with the phrase as the key.</p>

<pre><code>phrase_hash = {}
    CSV.foreach(""phrase_lookup.csv"") do |row|
        phrase, tag = row
        next if name == ""PHRASE""
        phrase_hash[phrase] = tag
    end
</code></pre>

<p>The keys of the hash are then the search phrases that I want to iterate through. I'm having trouble expressing what I want to do next in Ruby, but here's the idea:</p>

<p>Load the NOTES table into an array. For each phrase (i.e. key), select the records from the array that contain the phrase, gather the IDs associated with these rows, and output them with the associated tag for that phrase, as above.</p>

<p>Can anyone help?</p>
","ruby, tagging, text-mining","<p>I'll give you an example using hash inputs instead of CSV:</p>

<pre><code>notes = { 1 =&gt; ""MISSING DOB; ID CANNOT BE BLANK"",
          2 =&gt; ""INVALID MEMBER ID - unable to verify"",
          3 =&gt; ""needs follow-up"",
          4 =&gt; ""ID CANNOT BE BLANK-- additional info needed""
        }

tags =  { ""MISSING DOB"" =&gt; ""BLANKDOB"",
          ""ID CANNOT BE BLANK"" =&gt; ""BLANKID"",
          ""INVALID MEMBER ID"" =&gt; ""INVALIDID""
        }

output = {}

tags.each_pair do |tags_key,tags_value|
    notes.each_pair do |notes_key, notes_value|
        if notes_value.match(tags_key)
            output[notes_key] ||= []
            output[notes_key] &lt;&lt; tags_value 
        end
    end
end 

puts output.map {|k,v| ""#{k}, #{v.join(""; "")}""}.sort
</code></pre>
",0,0,80,2013-01-31 07:19:05,https://stackoverflow.com/questions/14620442/using-ruby-to-tag-records-that-contain-repeat-phrases-in-a-table
"How To: Check Frequency Weighting in a Term Document Matrix, in &#39;topicmodels&#39;","<p>I'm attempting to do some topic modeling using the R package <code>topicmodels</code></p>

<p>I've done my pre-processing using the 'tm' package, per these instructions <a href=""https://stackoverflow.com/questions/7927367/r-text-file-and-text-mining-how-to-load-data"">R text file and text mining...how to load data</a>. </p>

<p>However, when I go to run my correlated topic model (CTM) using <code>topicmodels</code> in R, I receive the following error </p>

<pre><code> ""Error in CTM...DocumentTermMatrix needs to have a term frequency weighting. 
</code></pre>

<p>I've triple checked the structure of my DocumentTermMatrix shows it does have a frequency weighting: </p>

<pre><code> A document-term matrix (26 documents, 413 terms)

 Non-/sparse entries: 4804/5934
 Sparsity           : 55%
 Maximal term length: 13 
 Weighting          : term frequency - inverse document frequency (normalized) (tf-idf)
</code></pre>

<p>Any suggestions on how to get this working would be appreciated!</p>
","r, text-mining","<p>You need to specify the weighting parameter to be weightTf if you use the slam package before:</p>

<pre><code>m=as.simple_triplet_matrix(mm);
dtm &lt;- as.DocumentTermMatrix(m,weighting =weightTf)
</code></pre>
",3,3,3344,2013-02-04 22:50:19,https://stackoverflow.com/questions/14697218/how-to-check-frequency-weighting-in-a-term-document-matrix-in-topicmodels
Java based Word mapping (semantic) application,"<p>I want to develop an Java based application to map synonyms to a unique code or a word. For example in medical terms, the word  <code>heart attack</code>  or  <code>cardiac arrest</code> etc. means the same thing. So I want to build a database ( need no be an RDBMS) to store such mappings. Such mappings have to be added/ modified or deleted later on. </p>

<p>The primary objective is to develop an application to map semantically together entities. </p>

<p>My input will be like a two or three word phrase and it will be mapped to a standard code. example <strong>heart attack</strong> and <strong>cardiac arrest</strong> mapped to a disease code <strong>HA50122445</strong>.  Tomorrow, if I find a new phrase  say <strong>myo cardiac inflammation</strong>, I want to map it to the code <strong>HA50122445</strong></p>

<p>I looked at <a href=""http://lucene.apache.org/core/index.html"" rel=""nofollow"">Apache Lucene</a>, but it is for text mining , primarity based on search keywords. My requirements is kind of similar to <a href=""http://wordnet.princeton.edu/wordnet/"" rel=""nofollow"">Wordnet</a>. Is the Wordnet database editable or modifiable ? My research says no. Is it right ? </p>

<p>It would be really helpful, if you could guide me. Thank you</p>
","java, semantics, semantic-web, text-mining, wordnet","<p>You might want to check out <a href=""http://www.nlm.nih.gov/pubs/factsheets/umlsmeta.html"" rel=""nofollow"">UMLS metathesaurus</a> and <a href=""http://metamap.nlm.nih.gov/"" rel=""nofollow"">MetaMap</a>. The former is ""a large, multi-purpose, and multi-lingual thesaurus that contains millions of biomedical and health related concepts"". That must have some identifiers in it. You can base your ""database"" on that, because building it yourself over time is going to be extremely laborious. The latter is a natural language processing toolkit which scans text, discovers medical concepts and maps them to entries in the UMLS thesaurus. </p>

<p>Disclaimer: I have not used either of these resources. My team has built similar resources for a client, but there are not freely available, thus my not recommending them.</p>
",3,2,965,2013-02-20 11:30:16,https://stackoverflow.com/questions/14978741/java-based-word-mapping-semantic-application
R Text mining - how to change texts in R data frame column into several columns with word frequencies?,"<p>I have a data frame with 4 columns. Column 1 consists of ID's, column 2 consists of texts (about 100 words each), column 3 and 4 consist labels. </p>

<p>Now I would like to retrieve word frequencies (of the most common words) from the texts column and add those frequencies as extra columns to the data frame. I would like the column names to be the words themselves and the columns filled with their frequencies (ranging from 0 to ... per text) in the texts.</p>

<p>I tried some functions of the tm package but until now unsatisfactory. 
Does anyone has any idea how to deal with this problem or where to start? Is there a package that can do the job?</p>

<pre><code>id  texts   label1    label2
</code></pre>
","r, text-mining, tm","<p>Well let's work through the issues then...</p>

<p>I'm guessing you have a data.frame that looks like this:</p>

<pre><code>       person sex adult                                 state code
1         sam   m     0         Computer is fun. Not too fun.   K1
2        greg   m     0               No it's not, it's dumb.   K2
3     teacher   m     1                    What should we do?   K3
4         sam   m     0                  You liar, it stinks!   K4
5        greg   m     0               I am telling the truth!   K5
6       sally   f     0                How can we be certain?   K6
7        greg   m     0                      There is no way.   K7
8         sam   m     0                       I distrust you.   K8
9       sally   f     0           What are you talking about?   K9
10 researcher   f     1         Shall we move on?  Good then.  K10
11       greg   m     0 I'm hungry.  Let's eat.  You already?  K11
</code></pre>

<p>This data set comes from the qdap package.  to get qdap use <code>install.packages(""qdap"")</code>.</p>

<p>Now to make the reproducible example I was talking about with your data set do what I'm doing here with the <code>DATA</code> data set from qdap.</p>

<pre><code>DATA
dput(head(DATA))
</code></pre>

<p>Ok now for your original problem I think <code>wfm</code> will do what you want:</p>

<pre><code>freqs &lt;- t(wfm(DATA$state, 1:nrow(DATA)))
data.frame(DATA, freqs, check.names = FALSE)
</code></pre>

<p>If you only wanted the top so many words use an ordering technique like I use here:</p>

<pre><code>freqs &lt;- t(wfm(DATA$state, 1:nrow(DATA)))
ords &lt;- rev(sort(colSums(freqs)))[1:9]      #top 9 words
top9 &lt;- freqs[, names(ords)]                #grab those columns from freqs  
data.frame(DATA, top9, check.names = FALSE) #put it together
</code></pre>

<p>The outcome looks like this:</p>

<pre><code>&gt; data.frame(DATA, top9, check.names = FALSE)
       person sex adult                                 state code you we what not no it's is i fun
1         sam   m     0         Computer is fun. Not too fun.   K1   0  0    0   1  0    0  1 0   2
2        greg   m     0               No it's not, it's dumb.   K2   0  0    0   1  1    2  0 0   0
3     teacher   m     1                    What should we do?   K3   0  1    1   0  0    0  0 0   0
4         sam   m     0                  You liar, it stinks!   K4   1  0    0   0  0    0  0 0   0
5        greg   m     0               I am telling the truth!   K5   0  0    0   0  0    0  0 1   0
6       sally   f     0                How can we be certain?   K6   0  1    0   0  0    0  0 0   0
7        greg   m     0                      There is no way.   K7   0  0    0   0  1    0  1 0   0
8         sam   m     0                       I distrust you.   K8   1  0    0   0  0    0  0 1   0
9       sally   f     0           What are you talking about?   K9   1  0    1   0  0    0  0 0   0
10 researcher   f     1         Shall we move on?  Good then.  K10   0  1    0   0  0    0  0 0   0
11       greg   m     0 I'm hungry.  Let's eat.  You already?  K11   1  0    0   0  0    0  0 0   0
</code></pre>
",7,2,4122,2013-03-06 22:01:00,https://stackoverflow.com/questions/15258954/r-text-mining-how-to-change-texts-in-r-data-frame-column-into-several-columns
Sentiment Analysis on LARGE collection of online conversation text,"<p>The title says it all; I have an SQL database bursting at the seams with online conversation text. I've already done most of this project in Python, so I would like to do this using Python's NLTK library (unless there's a <strong>strong</strong> reason not to).</p>

<p>The data is organized by <strong>Thread</strong>, <strong>Username</strong>, and <strong>Post</strong>. Each thread more or less focuses on discussing one ""product"" of the Category that I am interested in analyzing. Ultimately,  when this is finished, I would like to have an estimated opinion (like/dislike sort of deal) from each user for any of the products they had discussed at some point.</p>

<p>So, what I would like to know:</p>

<p>1) <strong>How can I go about determining what product each thread is about?</strong> I was reading about keyword extraction... is that the correct method?</p>

<p>2) <strong>How do I determine a specific users sentiment based on their posts?</strong> From my limited understanding, I must first ""train"" NLTK to recognize certain indicators of opinion, and then do I simply determine the context of those words when they appear in the text?</p>

<p>As you may have guessed by now, I have no prior experience with NLP. From my reading so far, I think I can handle learning it though. Even just a basic and crude working model for now would be great if someone can point me in the right direction. Google was not very helpful to me.</p>

<p><strong>P.S.</strong> I have permission to analyze this data (in case it matters)</p>
","python, nlp, nltk, text-mining, sentiment-analysis","<p>Training any classifier requires a <strong>training set</strong> of <strong>labeled data</strong> and a feature extractor to obtain <strong>feature sets</strong> for each text. After you have a trained classifier, you can apply it to previously unseen text (unlabeled) and obtain a classification based on the machine learning algorithm used. NLTK <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html"" rel=""nofollow"">gives a good explanation and some samples to play around with</a>.</p>

<p>If you are interested in building a classifier for positive/negative sentiment, using your own training dataset, I would avoid simple keyword counts, as <a href=""http://sentiment.christopherpotts.net/lingcog.html#commitment"" rel=""nofollow"">they aren't accurate for a number of reasons</a> (eg. negation of positive words: ""not happy""). An alternative, where you can still use a large training set without having to manually label anything, is <strong>distant supervision</strong>. Basically, this approach uses <em>emoticons</em> or other specific text elements as <strong>noisy labels</strong>. You still have to choose which features are relevant but many studies have had good results with simply using <em>unigrams</em> or <em>bigrams</em> (individual words or pairs of words, respectively).</p>

<p>All of this can be done relatively easily with Python and NLTK. You can also choose to use a tool like <a href=""https://github.com/japerk/nltk-trainer"" rel=""nofollow"">NLTK-trainer</a>, which is a wrapper for NLTK and requires less code.</p>

<p>I think <a href=""http://cs.wmich.edu/~tllake/fileshare/TwitterDistantSupervision09.pdf"" rel=""nofollow"">this study</a> by Go et al. is one of the easiest to understand. You can also read other studies for <a href=""http://scholar.google.com/scholar?q=distant%20supervision"" rel=""nofollow"">distant supervision</a>, <a href=""http://scholar.google.com/scholar?q=distant%20supervision%20sentiment%20analysis"" rel=""nofollow"">distant supervision sentiment analysis</a>, and <a href=""http://scholar.google.com/scholar?q=twitter%20sentiment%20analysis"" rel=""nofollow"">sentiment analysis</a>. </p>

<p>There are a few built-in classifiers in NLTK with both training and classification methods (<a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.classify.naivebayes.NaiveBayesClassifier-class.html"" rel=""nofollow"">Naive Bayes</a>, <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.classify.maxent.MaxentClassifier-class.html"" rel=""nofollow"">MaxEnt</a>, etc.) but if you are interested in using Support Vector Machines (SVM) then you should look elsewhere. Technically NLTK provides you with an <a href=""http://nltk.org/_modules/nltk/classify/svm.html"" rel=""nofollow"">SVM class</a> but its really just a wrapper for <a href=""https://bitbucket.org/wcauchois/pysvmlight"" rel=""nofollow"">PySVMLight</a>, which itself is a wrapper for <a href=""http://svmlight.joachims.org"" rel=""nofollow"">SVMLight</a>, written in C. I had numerous problems with this approach though, and would instead recommend <a href=""http://www.csie.ntu.edu.tw/~cjlin/libsvm/"" rel=""nofollow"">LIBSVM</a>.</p>

<p>For determining the topic, many have used simple keywords but there are some more complex methods available.</p>
",5,10,5841,2013-03-10 19:44:20,https://stackoverflow.com/questions/15326694/sentiment-analysis-on-large-collection-of-online-conversation-text
Cosine similarity returning wrong distance,"<p>I have two vectors represented as a HashMap and I want to measure the similarity between them. I use the cosine similarity metric as in the following code:</p>

<pre><code>public static void cosineSimilarity(HashMap&lt;Integer,Double&gt; vector1, HashMap&lt;Integer,Double&gt; vector2){
double scalar=0.0d, v1Norm=0.0d, v2Norm=0.0d;

for(int featureId: vector1.keySet()){
   scalar+= (vector1.get(featureId)* vector2.get(featureId));
   v1Norm+= (vector1.get(featureId) * vector1.get(featureId));
   v2Norm+= (vector2.get(featureId) * vector2.get(featureId));
}

 v1Norm=Math.sqrt(v1Norm);
 v2Norm=Math.sqrt(v2Norm);

 double cosine= scalar / (v1Norm*v2Norm);
 System.out.println(""v1 is: ""+v1Norm+"" , v2 is: ""+v2Norm+"" Cosine is: ""+cosine);    
}
</code></pre>

<p>Strangely, two vectors that are supposed to be dissimilar come close to .9999 result which is just wrong! </p>

<p>Please note that the keys are exactly the same for both maps.</p>

<p>data file is here: <a href=""http://ge.tt/3TJFuqa/v/0?c"" rel=""nofollow"" title=""Link to GE.TT website"">file</a></p>

<p>File format:</p>

<p>FeatureId vector1_value vector2_value</p>
","java, text-mining, cosine-similarity","<p>Your code is fine.</p>

<p>The vectors are dominated by several large features. In those features, the two vectors are almost collinear, which is why the similarity measure is close to <code>1</code>.</p>

<p>I include the six largest features below. Look at the ratio of <code>vec2</code> over <code>vec1</code>: it's almost identical across those features.</p>

<pre><code>feature     vec1    vec2        vec2/vec1

64806110    2875    1.85E+07    6.43E+03
64806108    5750    3.68E+07    6.40E+03
64806107    8625    5.49E+07    6.37E+03
64806106    11500   7.29E+07    6.34E+03
64806111    14375   9.07E+07    6.31E+03
64806109    17250   1.08E+08    6.28E+03
</code></pre>
",4,1,824,2013-03-12 07:27:57,https://stackoverflow.com/questions/15355560/cosine-similarity-returning-wrong-distance
How to represent bi-grams,"<p>I'm working on a system/algorithm that will detect topics in a stream of tweets. </p>

<p>What I'll do is remove the stop words, emoticons, urls, etc. and I'm thinking about representing the tweet as follows:</p>

<pre><code>terms = (t1, t2, ..., tk)
hashtags = (h1, h2, ..., hn)
date = date of tweet
</code></pre>

<p>and then use some similarity measures between the tweets when applying some clustering algorithms, combining those 3 values. This will be a little more complex than that, since I'll handle replies (eg. when you reply to some tweet, most of the time you keep talking about the same topics, etc).</p>

<p>I don't know if that will work or not, but the problem I'm seeing so far is that I'm not identifying <strong>n-grams</strong>, so <em>Barack Obama</em> appear most of time together, and in my system it will be two separate terms (<em>Barack</em> and <em>Obama</em>).</p>

<p>My question is:</p>

<p><strong>How can I also represent bi-grams? I mean, how is it usually modeled?</strong></p>

<p>I thought about having something like the following:</p>

<pre><code>Tweet = `Some words here`
terms = `[some, words, here, some words, words here]`
...
</code></pre>

<p>but I don't know if that is the correct way to go, if I have to do that for every possible bi-gram, etc.</p>

<p><strong>Edit</strong>:</p>

<p>In my database, I will have all the terms stored. Should I also store the bi-grams as if they were terms?</p>
","python, twitter, nlp, cluster-analysis, text-mining","<p>Let's say one of your documents is ""the quick brown fox jumped over the lazy dog"".</p>

<p>the bi-grams and uni-grams would be:</p>

<pre><code>the_quick
quick_brown
brown_fox
fox_jumped
jumped_over
over_the
the_lazy
lazy_dog
the
quick
brown
fox
jumped
over
lazy
dog
</code></pre>

<p>You could then put all the unique grams of all of your documents in a word vector to analyze, like this:</p>

<pre><code>Document the_quick  quick_brown  ... lazy  dog   some_other_gram

1        0.01       0.02             0.1   0.05  0.0
2        0          0                0.12  0.0   0.1
3        0.5        0.4              0     0     0
</code></pre>

<p>where the numbers in the cells represent the count, binary count, frequency, or TFIDF score of the terms in the documents. </p>

<p>You could then compare documents for similarity, or do clustering, or classification on them.</p>
",3,-1,374,2013-03-16 19:27:05,https://stackoverflow.com/questions/15453689/how-to-represent-bi-grams
findassoc function in tm package of R giving error,"<p>I have a term document matrix build from youtube comments</p>

<p>about 977 documents</p>

<p>created a term document matrix using:</p>

<pre><code>dtm &lt;- DocumentTermMatrix(doc_corpus)
</code></pre>

<p>and then on applying the function</p>

<pre><code>findAssocs(dtm,""hello"", 0.6)
</code></pre>

<p>getting the following (translated from french)</p>

<pre><code>Error in which (x [term],&gt; corlimit): index out of bounds
</code></pre>

<p>all the other functions are working like findFreqTerms and hclus etc</p>
","r, text-mining","<p>You get an error beacause the word does not exist in the terms sets. You can check this using :</p>

<pre><code>'hello' %in% Terms(dtm)
</code></pre>

<p>which should return FALSE.</p>
",2,0,493,2013-03-18 14:34:52,https://stackoverflow.com/questions/15479393/findassoc-function-in-tm-package-of-r-giving-error
Make dataframe of top N frequent terms for multiple corpora using tm package in R,"<p>I have several <code>TermDocumentMatrix</code>s created with the <code>tm</code> package in R.</p>

<p>I want to find the 10 most frequent terms in each set of documents to ultimately end up with an output table like:</p>

<pre><code>corpus1   corpus2
""beach""   ""city""
""sand""    ""sidewalk""
...        ...
[10th most frequent word]
</code></pre>

<p>By definition, <code>findFreqTerms(corpus1,N)</code> returns all of the terms which appear N times or more. To do this by hand I could change N until I got 10 or so terms returned, but the output for <code>findFreqTerms</code> is listed alphabetically so unless I picked exactly the right N, I wouldn't actually know which were the top 10. I suspect that this involves manipulating the internal structure of the TDM that you can see with <code>str(corpus1)</code> as in <a href=""https://stackoverflow.com/questions/11508728/r-tm-package-create-matrix-of-nmost-frequent-terms"">R tm package create matrix of Nmost frequent terms</a> but the answer here was very opaque to me so I wanted to rephrase the question.</p>

<p>Thanks!</p>
","r, text-mining, corpus, tm","<p>Here's one way to find the top N terms in a document term matrix. Briefly, you convert the dtm to a matrix, then sort by row sums:    </p>

<pre><code># load text mining library    
library(tm)

# make corpus for text mining (data comes from package, for reproducibility) 
data(""crude"")
corpus &lt;- Corpus(VectorSource(crude))

# process text (your methods may differ)
skipWords &lt;- function(x) removeWords(x, stopwords(""english""))
funcs &lt;- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a &lt;- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 &lt;- TermDocumentMatrix(a, control = list(wordLengths = c(3,10))) 
</code></pre>

<p>Here's the method in your Q, which returns words in alpha order, not always very useful, as you note...</p>

<pre><code>N &lt;- 10
findFreqTerms(a.dtm1, N)

[1] ""barrel""     ""barrels""    ""bpd""        ""crude""      ""dlrs""       ""government"" ""industry""   ""kuwait""    
[9] ""market""     ""meeting""    ""minister""   ""mln""        ""month""      ""official""   ""oil""        ""opec""      
[17] ""pct""        ""price""      ""prices""     ""production"" ""reuter""     ""saudi""      ""sheikh""     ""the""       
[25] ""world""
</code></pre>

<p>And here's what you can do to get the top N words in order of their abundance:</p>

<pre><code>m &lt;- as.matrix(a.dtm1)
v &lt;- sort(rowSums(m), decreasing=TRUE)
head(v, N)

oil prices   opec    mln    the    bpd   dlrs  crude market reuter 
86     48     47     31     26     23     23     21     21     20 
</code></pre>

<p>For several document term matrices, you could do something like this:</p>

<pre><code># make a list of the dtms
dtm_list &lt;- list(a.dtm1, b.dtm1, c.dtm1, d.dtm1)
# apply the rowsums function to each item of the list
lapply(dtm_list, function(x)  sort(rowSums(as.matrix(x)), decreasing=TRUE))
</code></pre>

<p>Is that what you want to do?</p>

<p>Hat-tip to Ian Fellows' <a href=""http://cran.r-project.org/web/packages/wordcloud/index.html"">wordcloud</a> package where I first saw this method.</p>

<p><strong>UPDATE: following the comment below, here's some more detail...</strong></p>

<p>Here's some data to make a reproducible example with multiple corpora:</p>

<pre><code>examp1 &lt;- ""When discussing performance with colleagues, teaching, sending a bug report or searching for guidance on mailing lists and here on SO, a reproducible example is often asked and always helpful. What are your tips for creating an excellent example? How do you paste data structures from r in a text format? What other information should you include? Are there other tricks in addition to using dput(), dump() or structure()? When should you include library() or require() statements? Which reserved words should one avoid, in addition to c, df, data, etc? How does one make a great r reproducible example?""

examp2 &lt;- ""Sometimes the problem really isn't reproducible with a smaller piece of data, no matter how hard you try, and doesn't happen with synthetic data (although it's useful to show how you produced synthetic data sets that did not reproduce the problem, because it rules out some hypotheses). Posting the data to the web somewhere and providing a URL may be necessary. If the data can't be released to the public at large but could be shared at all, then you may be able to offer to e-mail it to interested parties (although this will cut down the number of people who will bother to work on it). I haven't actually seen this done, because people who can't release their data are sensitive about releasing it any form, but it would seem plausible that in some cases one could still post data if it were sufficiently anonymized/scrambled/corrupted slightly in some way. If you can't do either of these then you probably need to hire a consultant to solve your problem"" 

examp3 &lt;- ""You are most likely to get good help with your R problem if you provide a reproducible example. A reproducible example allows someone else to recreate your problem by just copying and pasting R code. There are four things you need to include to make your example reproducible: required packages, data, code, and a description of your R environment. Packages should be loaded at the top of the script, so it's easy to see which ones the example needs. The easiest way to include data in an email is to use dput() to generate the R code to recreate it. For example, to recreate the mtcars dataset in R, I'd perform the following steps: Run dput(mtcars) in R Copy the output In my reproducible script, type mtcars &lt;- then paste. Spend a little bit of time ensuring that your code is easy for others to read: make sure you've used spaces and your variable names are concise, but informative, use comments to indicate where your problem lies, do your best to remove everything that is not related to the problem. The shorter your code is, the easier it is to understand. Include the output of sessionInfo() as a comment. This summarises your R environment and makes it easy to check if you're using an out-of-date package. You can check you have actually made a reproducible example by starting up a fresh R session and pasting your script in. Before putting all of your code in an email, consider putting it on http://gist.github.com/. It will give your code nice syntax highlighting, and you don't have to worry about anything getting mangled by the email system.""

examp4 &lt;- ""Do your homework before posting: If it is clear that you have done basic background research, you are far more likely to get an informative response. See also Further Resources further down this page. Do help.search(keyword) and apropos(keyword) with different keywords (type this at the R prompt). Do RSiteSearch(keyword) with different keywords (at the R prompt) to search R functions, contributed packages and R-Help postings. See ?RSiteSearch for further options and to restrict searches. Read the online help for relevant functions (type ?functionname, e.g., ?prod, at the R prompt) If something seems to have changed in R, look in the latest NEWS file on CRAN for information about it. Search the R-faq and the R-windows-faq if it might be relevant (http://cran.r-project.org/faqs.html) Read at least the relevant section in An Introduction to R If the function is from a package accompanying a book, e.g., the MASS package, consult the book before posting. The R Wiki has a section on finding functions and documentation""

examp5 &lt;- ""Before asking a technical question by e-mail, or in a newsgroup, or on a website chat board, do the following:  Try to find an answer by searching the archives of the forum you plan to post to. Try to find an answer by searching the Web. Try to find an answer by reading the manual. Try to find an answer by reading a FAQ. Try to find an answer by inspection or experimentation. Try to find an answer by asking a skilled friend. If you're a programmer, try to find an answer by reading the source code. When you ask your question, display the fact that you have done these things first; this will help establish that you're not being a lazy sponge and wasting people's time. Better yet, display what you have learned from doing these things. We like answering questions for people who have demonstrated they can learn from the answers. Use tactics like doing a Google search on the text of whatever error message you get (searching Google groups as well as Web pages). This might well take you straight to fix documentation or a mailing list thread answering your question. Even if it doesn't, saying “I googled on the following phrase but didn't get anything that looked promising” is a good thing to do in e-mail or news postings requesting help, if only because it records what searches won't help. It will also help to direct other people with similar problems to your thread by linking the search terms to what will hopefully be your problem and resolution thread. Take your time. Do not expect to be able to solve a complicated problem with a few seconds of Googling. Read and understand the FAQs, sit back, relax and give the problem some thought before approaching experts. Trust us, they will be able to tell from your questions how much reading and thinking you did, and will be more willing to help if you come prepared. Don't instantly fire your whole arsenal of questions just because your first search turned up no answers (or too many). Prepare your question. Think it through. Hasty-sounding questions get hasty answers, or none at all. The more you do to demonstrate that having put thought and effort into solving your problem before seeking help, the more likely you are to actually get help. Beware of asking the wrong question. If you ask one that is based on faulty assumptions, J. Random Hacker is quite likely to reply with a uselessly literal answer while thinking Stupid question..., and hoping the experience of getting what you asked for rather than what you needed will teach you a lesson.""
</code></pre>

<p>Now let's process the example text a little, in the usual way. First convert the character vectors to corpora.</p>

<pre><code>library(tm)
list_examps &lt;- lapply(1:5, function(i) eval(parse(text=paste0(""examp"",i))))
list_corpora &lt;- lapply(1:length(list_examps), function(i) Corpus(VectorSource(list_examps[[i]])))
</code></pre>

<p>Now remove stopwords, numbers, punctuation, etc.</p>

<pre><code>skipWords &lt;- function(x) removeWords(x, stopwords(""english""))
funcs &lt;- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
list_corpora1 &lt;- lapply(1:length(list_corpora), function(i) tm_map(list_corpora[[i]], FUN = tm_reduce, tmFuns = funcs))
</code></pre>

<p>Convert processed corpora to term document matrix:</p>

<pre><code>list_dtms &lt;- lapply(1:length(list_corpora1), function(i) TermDocumentMatrix(list_corpora1[[i]], control = list(wordLengths = c(3,10))))
</code></pre>

<p>Get the most frequently occuring words in each corpus:</p>

<pre><code>top_words &lt;- lapply(1:length(list_dtms), function(x)  sort(rowSums(as.matrix(list_dtms[[x]])), decreasing=TRUE))
</code></pre>

<p>And reshape it into a dataframe according to the specified form:</p>

<pre><code>library(plyr)
top_words_df &lt;- t(ldply(1:length(top_words), function(i)  head(names(top_words[[i]]),10)))
colnames(top_words_df) &lt;- lapply(1:length(list_dtms), function(i) paste0(""corpus"",i))
top_words_df

    corpus1    corpus2      corpus3    corpus4     corpus5    
V1  ""example""  ""data""       ""code""     ""functions"" ""answer""   
V2  ""addition"" ""people""     ""example""  ""prompt""    ""help""     
V3  ""data""     ""synthetic""  ""easy""     ""relevant""  ""try""      
V4  ""how""      ""able""       ""email""    ""book""      ""question"" 
V5  ""include""  ""actually""   ""include""  ""keywords""  ""questions""
V6  ""what""     ""bother""     ""recreate"" ""package""   ""reading""  
V7  ""when""     ""consultant"" ""script""   ""posting""   ""answers""  
V8  ""are""      ""cut""        ""check""    ""read""      ""people""   
V9  ""avoid""    ""form""       ""data""     ""search""    ""search""   
V10 ""bug""      ""happen""     ""mtcars""   ""section""   ""searching""
</code></pre>

<p>Can you adapt that to work with your data? If not, please edit your question to more accurately show what your data look like.</p>
",30,15,22232,2013-03-19 17:12:23,https://stackoverflow.com/questions/15506118/make-dataframe-of-top-n-frequent-terms-for-multiple-corpora-using-tm-package-in
Produce a DocumentTermMatix that includes given terms in R,"<p>I'm producing a DocumentTermMatrix using <code>tm</code> on a corpus, using only terms which occur quite frequently. (ie MinDocFrequency=50)</p>

<p>Now I want to produce a DTM with a different corpus, but counting exactly the same terms as the previous one, no extra and no fewer. (to cross-validate)</p>

<p>If I use the same method to produce the DTM as with the first corpus, I end up including either more or less terms, or just different ones because they're at a different frequency to the original corpus.</p>

<p>How can I go about doing this? I need to specify which terms to count somehow, but I don't know how.</p>

<p>Thanks to anyone who can point me in the right direction,</p>

<p>-N</p>

<p>EDIT: I was asked for a reproducible example, so I've pasted some example code here <a href=""http://pastebin.com/y3FDHbYS"" rel=""nofollow"">http://pastebin.com/y3FDHbYS</a>
Re-edit:</p>

<pre><code> require(tm)
 text &lt;- c('saying text is good',
          'saying text once and saying text twice is better',
          'saying text text text is best',
          'saying text once is still ok',
          'not saying it at all is bad',
          'because text is a good thing',
          'we all like text',
          'even though sometimes it is missing')

validationText &lt;- c(""This has different words in it."",
                     ""But I still want to count"",
                     ""the occurence of text"",
                     ""for example"")

TextCorpus &lt;- Corpus(VectorSource(text))
ValiTextCorpus &lt;- Corpus(VectorSource(validationText))

Control = list(stopwords=TRUE, removePunctuation=TRUE, removeNumbers=TRUE, MinDocFrequency=5)

TextDTM = DocumentTermMatrix(TextCorpus, Control)
ValiTextDTM = DocumentTermMatrix(ValiTextCorpus, Control)
</code></pre>

<p>This, however just shows the method I'm already familiar with for producing a corpus, and as a result the two DTMs, (TextDTM and ValiTextDTM) contain different terms. What I'm trying to achieve is counting the same terms in both corpuses, even if they are much less frequent in the validation one. In the example then, I'd be trying to count occurrences of the word ""text"", even though this would produce a very sparse matrix in the validation case.</p>
","r, data-mining, text-mining, tm","<p>Here's one approach... does it work for your data? <strong>see further down for details that include the OP's data</strong></p>

<pre><code># load text mining library    
library(tm)

# make first corpus for text mining (data comes from package, for reproducibility) 
data(""crude"")
corpus1 &lt;- Corpus(VectorSource(crude[1:10]))

# process text (your methods may differ)
skipWords &lt;- function(x) removeWords(x, stopwords(""english""))
funcs &lt;- list(tolower, removePunctuation, removeNumbers, 
              stripWhitespace, skipWords, MinDocFrequency=5)
crude1 &lt;- tm_map(corpus1, FUN = tm_reduce, tmFuns = funcs)
crude1.dtm &lt;- TermDocumentMatrix(crude1, control = list(wordLengths = c(3,10))) 

# prepare 2nd corpus
corpus2 &lt;- Corpus(VectorSource(crude[11:20]))

# process text as above
skipWords &lt;- function(x) removeWords(x, stopwords(""english""))
funcs &lt;- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
crude2 &lt;- tm_map(corpus2, FUN = tm_reduce, tmFuns = funcs)
crude2.dtm &lt;- TermDocumentMatrix(crude1, control = list(wordLengths = c(3,10))) 

crude2.dtm.mat &lt;- as.matrix(crude2.dtm)

# subset second corpus by words in first corpus
crude2.dtm.mat[rownames(crude2.dtm.mat) %in% crude1.dtm.freq, ]
    Docs
 Terms    reut-00001.xml reut-00002.xml reut-00004.xml reut-00005.xml reut-00006.xml
 oil                 5             12              2              1              1
 opec                0             15              0              0              0
 prices              3              5              0              0              0
    Docs
Terms    reut-00007.xml reut-00008.xml reut-00009.xml reut-00010.xml reut-00011.xml
oil                 7              4              3              5              9
opec                8              1              2              2              6
prices              5              1              2              1              9
</code></pre>

<p><strong>UPDATE after data provided and comments</strong> I think this a bit closer to your question.  </p>

<p>Here's the same process using <em>document term matrices</em> instead of TDMs (as I used above, a slight variation):</p>

<pre><code># load text mining library    
library(tm)

# make corpus for text mining (data comes from package, for reproducibility) 
data(""crude"")
corpus1 &lt;- Corpus(VectorSource(crude[1:10]))

# process text (your methods may differ)
skipWords &lt;- function(x) removeWords(x, stopwords(""english""))
funcs &lt;- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
crude1 &lt;- tm_map(corpus1, FUN = tm_reduce, tmFuns = funcs)
crude1.dtm &lt;- DocumentTermMatrix(crude1, control = list(wordLengths = c(3,10))) 


corpus2 &lt;- Corpus(VectorSource(crude[11:20]))

# process text (your methods may differ)
skipWords &lt;- function(x) removeWords(x, stopwords(""english""))
funcs &lt;- list(tolower, removePunctuation, removeNumbers, 
              stripWhitespace, skipWords, MinDocFrequency=5)
crude2 &lt;- tm_map(corpus2, FUN = tm_reduce, tmFuns = funcs)
crude2.dtm &lt;- DocumentTermMatrix(crude1, control = list(wordLengths = c(3,10))) 

crude2.dtm.mat &lt;- as.matrix(crude2.dtm)
crude2.dtm.mat[,colnames(crude2.dtm.mat) %in% crude1.dtm.freq ]

Terms
Docs             oil opec prices
reut-00001.xml   5    0      3
reut-00002.xml  12   15      5
reut-00004.xml   2    0      0
reut-00005.xml   1    0      0
reut-00006.xml   1    0      0
reut-00007.xml   7    8      5
reut-00008.xml   4    1      1
reut-00009.xml   3    2      2
reut-00010.xml   5    2      1
reut-00011.xml   9    6      9
</code></pre>

<p><strong>And here's a solution using the data added into the OP's question</strong></p>

<pre><code>text &lt;- c('saying text is good',
          'saying text once and saying text twice is better',
          'saying text text text is best',
          'saying text once is still ok',
          'not saying it at all is bad',
          'because text is a good thing',
          'we all like text',
          'even though sometimes it is missing')

validationText &lt;- c(""This has different words in it."",
                    ""But I still want to count"",
                    ""the occurence of text"",
                    ""for example"")

TextCorpus &lt;- Corpus(VectorSource(text))
ValiTextCorpus &lt;- Corpus(VectorSource(validationText))

Control = list(stopwords=TRUE, removePunctuation=TRUE, removeNumbers=TRUE, MinDocFrequency=5)

TextDTM = DocumentTermMatrix(TextCorpus, Control)
ValiTextDTM = DocumentTermMatrix(ValiTextCorpus, Control)

# find high frequency terms in TextDTM
(TextDTM.hifreq &lt;- findFreqTerms(TextDTM, 5))
[1]   ""saying""    ""text""     

# find out how many times each high freq word occurs in TextDTM
TextDTM.mat &lt;- as.matrix(TextDTM)
colSums(TextDTM.mat[,TextDTM.hifreq])
saying   text 
6        9
</code></pre>

<p><strong>Here are the key lines</strong>, subset the second DTM based on the list of high-frequency words from the first DTM. In this case I've used the <code>intersect</code> function since the vector of high frequency words includes a word that is not in the second corpus at all (and <code>intersect</code> seems to handle that better than <code>%in%</code>)</p>

<pre><code># now look into second DTM
ValiTextDTM.mat &lt;- as.matrix(ValiTextDTM)
common &lt;- data.frame(ValiTextDTM.mat[, intersect(colnames(ValiTextDTM.mat), TextDTM.hifreq) ])
names(common) &lt;- intersect(colnames(ValiTextDTM.mat), TextDTM.hifreq)
     text
1    0
2    0
3    1
4    0
</code></pre>

<p>How to find the <strong>total count of the high freq word(s)</strong> in the second corpus:</p>

<pre><code>colSums(common)
text 
   1
</code></pre>
",4,2,1044,2013-03-23 16:47:19,https://stackoverflow.com/questions/15589362/produce-a-documenttermmatix-that-includes-given-terms-in-r
Scanning through large text corpus counting phrases in vocabulary,"<p>I need to scan a corpus of around 1.5 million text documents and match any set of words, of length between 1 and 4, against a vocabulary of around 1.8 million phrases.</p>

<p>Extracting the word n-grams from each text file is not a problem, and is quite fast; the bottleneck is in the check against the vocabulary. </p>

<p>My vocabulary is stored in a MySQL table, properly indexed, and I'm querying each phrase like this: </p>

<pre><code>SELECT (1) FROM vocab WHERE phrase=%s;
</code></pre>

<p>The whole thing runs veeery slowly. Any optimization ideas?</p>

<p>Thanks!</p>
","python, mysql, text-mining","<p>My suggestion would be to read in the entire vocabulary into memory once, and then you'll be able to run some queries on it pretty quickly. You could throw it in a <code>set</code> or a <code>dict</code>, or if you're feeling fancy you could even use a bloom filter to check for inclusion. I'm biased on this point, but I recommend <a href=""https://github.com/axiak/pybloomfiltermmap"" rel=""nofollow"">pybloomfiltermmap</a> if you choose to go that route.</p>

<p>If you use a bloom filter, one advantage is that it's much more compact than having to store a <code>set</code> in memory and they're quite fast. They do have bounds on the false-positive rate and in practice can be controlled to acceptable levels.</p>
",2,0,241,2013-03-27 18:44:24,https://stackoverflow.com/questions/15666919/scanning-through-large-text-corpus-counting-phrases-in-vocabulary
how to read text in a table from a csv file,"<p>I am new using the tm package. I want to read a csv file which contents one column with 2000 texts and a second column with a factor variable yes/no into a Corpus. My intention is to convert the text as a matrix and use the factor variable as target for prediction. I would need to divide the corpus in train and test sets as well. 
  I read several documents like tm.pdf etc. and found the documentation relatively limited. This is my attempt following another threat on the same subject,</p>

<pre><code>TexTest&lt;-read.csv(""C:/Test.csv"")
 m &lt;- list(Text = ""Text"", Clasification = ""Classification"")
 corpus1 &lt;-
Corpus(x=TexTest,readerControl=list(reader=readTabular(mapping=m),language=""en""))

Error in if (x$Length &gt; 0) vector(""list"", as.integer(x$Length)) else list() : 
  argument is of length zero
</code></pre>

<p>Using </p>

<pre><code>corpus1 &lt;- Corpus(VectorSource(TexTest))
</code></pre>

<p>results in </p>

<pre><code>A corpus with 2 text documents
</code></pre>

<p>instead of 2000 texts. </p>

<p>How is the standard procedure here? 
Thanks</p>
","r, text-mining, tm","<p>You need to use <code>DataframeSource</code> in the <code>Corpus</code> function, that's where your example differs from the example on p. 2 of the PDF <a href=""http://cran.r-project.org/web/packages/tm/vignettes/extensions.pdf"" rel=""nofollow"">Extensions: How to Handle Custom File Formats</a> in the <code>tm</code> package.</p>

<p>Some reproducible data:</p>

<pre><code>TexTest &lt;- structure(list(Text = c(""When discussing performance with colleagues, teaching, sending a bug report or searching for guidance on mailing lists and here on SO, a reproducible example is often asked and always helpful. What are your tips for creating an excellent example? How do you paste data structures from r in a text format? What other information should you include? Are there other tricks in addition to using dput(), dump() or structure()? When should you include library() or require() statements? Which reserved words should one avoid, in addition to c, df, data, etc? How does one make a great r reproducible example?"", 
""Sometimes the problem really isn't reproducible with a smaller piece of data, no matter how hard you try, and doesn't happen with synthetic data (although it's useful to show how you produced synthetic data sets that did not reproduce the problem, because it rules out some hypotheses). Posting the data to the web somewhere and providing a URL may be necessary. If the data can't be released to the public at large but could be shared at all, then you may be able to offer to e-mail it to interested parties (although this will cut down the number of people who will bother to work on it). I haven't actually seen this done, because people who can't release their data are sensitive about releasing it any form, but it would seem plausible that in some cases one could still post data if it were sufficiently anonymized/scrambled/corrupted slightly in some way. If you can't do either of these then you probably need to hire a consultant to solve your problem"", 
""You are most likely to get good help with your R problem if you provide a reproducible example. A reproducible example allows someone else to recreate your problem by just copying and pasting R code. There are four things you need to include to make your example reproducible: required packages, data, code, and a description of your R environment. Packages should be loaded at the top of the script, so it's easy to see which ones the example needs. The easiest way to include data in an email is to use dput() to generate the R code to recreate it. For example, to recreate the mtcars dataset in R, I'd perform the following steps: Run dput(mtcars) in R Copy the output In my reproducible script, type mtcars &lt;- then paste. Spend a little bit of time ensuring that your code is easy for others to read: make sure you've used spaces and your variable names are concise, but informative, use comments to indicate where your problem lies, do your best to remove everything that is not related to the problem. The shorter your code is, the easier it is to understand. Include the output of sessionInfo() as a comment. This summarises your R environment and makes it easy to check if you're using an out-of-date package. You can check you have actually made a reproducible example by starting up a fresh R session and pasting your script in. Before putting all of your code in an email, consider putting it on http://gist.github.com/. It will give your code nice syntax highlighting, and you don't have to worry about anything getting mangled by the email system."", 
""Do your homework before posting: If it is clear that you have done basic background research, you are far more likely to get an informative response. See also Further Resources further down this page. Do help.search(keyword) and apropos(keyword) with different keywords (type this at the R prompt). Do RSiteSearch(keyword) with different keywords (at the R prompt) to search R functions, contributed packages and R-Help postings. See ?RSiteSearch for further options and to restrict searches. Read the online help for relevant functions (type ?functionname, e.g., ?prod, at the R prompt) If something seems to have changed in R, look in the latest NEWS file on CRAN for information about it. Search the R-faq and the R-windows-faq if it might be relevant (http://cran.r-project.org/faqs.html) Read at least the relevant section in An Introduction to R If the function is from a package accompanying a book, e.g., the MASS package, consult the book before posting. The R Wiki has a section on finding functions and documentation"", 
""Before asking a technical question by e-mail, or in a newsgroup, or on a website chat board, do the following:  Try to find an answer by searching the archives of the forum you plan to post to. Try to find an answer by searching the Web. Try to find an answer by reading the manual. Try to find an answer by reading a FAQ. Try to find an answer by inspection or experimentation. Try to find an answer by asking a skilled friend. If you're a programmer, try to find an answer by reading the source code. When you ask your question, display the fact that you have done these things first; this will help establish that you're not being a lazy sponge and wasting people's time. Better yet, display what you have learned from doing these things. We like answering questions for people who have demonstrated they can learn from the answers. Use tactics like doing a Google search on the text of whatever error message you get (searching Google groups as well as Web pages). This might well take you straight to fix documentation or a mailing list thread answering your question. Even if it doesn't, saying “I googled on the following phrase but didn't get anything that looked promising” is a good thing to do in e-mail or news postings requesting help, if only because it records what searches won't help. It will also help to direct other people with similar problems to your thread by linking the search terms to what will hopefully be your problem and resolution thread. Take your time. Do not expect to be able to solve a complicated problem with a few seconds of Googling. Read and understand the FAQs, sit back, relax and give the problem some thought before approaching experts. Trust us, they will be able to tell from your questions how much reading and thinking you did, and will be more willing to help if you come prepared. Don't instantly fire your whole arsenal of questions just because your first search turned up no answers (or too many). Prepare your question. Think it through. Hasty-sounding questions get hasty answers, or none at all. The more you do to demonstrate that having put thought and effort into solving your problem before seeking help, the more likely you are to actually get help. Beware of asking the wrong question. If you ask one that is based on faulty assumptions, J. Random Hacker is quite likely to reply with a uselessly literal answer while thinking Stupid question..., and hoping the experience of getting what you asked for rather than what you needed will teach you a lesson.""
), Classification = c(""Yes"", ""No"", ""Yes"", ""No"", ""Yes"")), .Names = c(""Text"", 
""Classification""), class = ""data.frame"", row.names = c(NA, -5L
))
</code></pre>

<p>Make a corpus of five documents (one for each row in the CSV file)</p>

<pre><code># TexTest&lt;-read.csv(""Test.csv"", stringsAsFactors = FALSE)
m &lt;- list(Content = ""Text"", Topic = ""Classification"")
library(tm)
myReader &lt;- readTabular(mapping = m)
(corpus &lt;- Corpus(DataframeSource(TexTest), readerControl = list(reader = myReader)))

A corpus with 5 text documents
# as expected, one doc per row of the CSV file

corpus[[1]]

When discussing performance with colleagues, teaching, sending a bug report or searching for guidance on mailing lists and here on SO, a reproducible example is often asked and always helpful. What are your tips for creating an excellent example? How do you paste data structures from r in a text format? What other information should you include? Are there other tricks in addition to using dput(), dump() or structure()? When should you include library() or require() statements? Which reserved words should one avoid, in addition to c, df, data, etc? How does one make a great r reproducible example?

# as expected, the first row of the CSV file
</code></pre>

<p>Is that what you wanted to do?</p>
",4,4,3811,2013-03-28 15:18:10,https://stackoverflow.com/questions/15685574/how-to-read-text-in-a-table-from-a-csv-file
Emoticons in Twitter Sentiment Analysis in r,"<p>How do I handle/get rid of emoticons so that I can sort tweets for sentiment analysis?</p>

<p>Getting:
Error in sort.list(y) : 
  invalid input </p>

<p>Thanks</p>

<p>and this is how the emoticons come out looking from twitter and into r:</p>

<pre><code>\xed��\xed�\u0083\xed��\xed��
\xed��\xed�\u008d\xed��\xed�\u0089 
</code></pre>
","r, text-mining, iconv, sentiment-analysis","<p>This should get rid of the emoticons, using <code>iconv</code> as suggested by ndoogan.</p>

<p>Some reproducible data:</p>

<pre><code>require(twitteR) 
# note that I had to register my twitter credentials first
# here's the method: http://stackoverflow.com/q/9916283/1036500
s &lt;- searchTwitter('#emoticons', cainfo=""cacert.pem"") 

# convert to data frame
df &lt;- do.call(""rbind"", lapply(s, as.data.frame))

# inspect, yes there are some odd characters in row five
head(df)

                                                                                                                                                text
1                                                                      ROFLOL: echte #emoticons [humor] http://t.co/0d6fA7RJsY via @tweetsmania  ;-)
2 “@teeLARGE: when tmobile get the iphone in 2 wks im killin everybody w/ emoticons &amp;amp; \nall the other stuff i cant see on android!"" \n#Emoticons
3                      E poi ricevi dei messaggi del genere da tua mamma xD #crazymum #iloveyou #emoticons #aiutooo #bestlike http://t.co/Yee1LB9ZQa
4                                                #emoticons I want to change my name to an #emoticon. Is it too soon? #prince http://t.co/AgmR5Lnhrk
5  I use emoticons too much. #addicted #admittingit #emoticons &lt;ed&gt;&lt;U+00A0&gt;&lt;U+00BD&gt;&lt;ed&gt;&lt;U+00B8&gt;&lt;U+00AC&gt;&lt;ed&gt;&lt;U+00A0&gt;&lt;U+00BD&gt;&lt;ed&gt;&lt;U+00B8&gt;&lt;U+0081&gt; haha
6                                                                                         What you text What I see #Emoticons http://t.co/BKowBSLJ0s
</code></pre>

<p><strong>Here's the key line that will remove the emoticons:</strong></p>

<pre><code># Clean text to remove odd characters
df$text &lt;- sapply(df$text,function(row) iconv(row, ""latin1"", ""ASCII"", sub=""""))
</code></pre>

<p>Now inspect again, to see if the odd characters are gone (see row 5)</p>

<pre><code>head(df)    
                                                                                                                               text
1                                                                     ROFLOL: echte #emoticons [humor] http://t.co/0d6fA7RJsY via @tweetsmania  ;-)
2 @teeLARGE: when tmobile get the iphone in 2 wks im killin everybody w/ emoticons &amp;amp; \nall the other stuff i cant see on android!"" \n#Emoticons
3                     E poi ricevi dei messaggi del genere da tua mamma xD #crazymum #iloveyou #emoticons #aiutooo #bestlike http://t.co/Yee1LB9ZQa
4                                               #emoticons I want to change my name to an #emoticon. Is it too soon? #prince http://t.co/AgmR5Lnhrk
5                                                                                 I use emoticons too much. #addicted #admittingit #emoticons  haha
6                                                                                        What you text What I see #Emoticons http://t.co/BKowBSLJ0s
</code></pre>
",22,19,14894,2013-04-01 17:25:32,https://stackoverflow.com/questions/15748190/emoticons-in-twitter-sentiment-analysis-in-r
CLUTO Document Term Matrix to tm DocumentTermMatrix,"<p>I have a document term matrix in cluto format:</p>

<pre><code>#Document #Term #TotalItem
term-x weight-x term-y weight-y (for only nonzeros terms, a row per document)
</code></pre>

<p>Instead of a corpus, I want to create DocumentTermMatrix(tm package) from this file, is this possible?</p>

<pre><code>Cluto File:
2 3 3
1 3 3 4
2 8

Row File:
car
plane

Column File:
x
y
z
</code></pre>

<p>Solution:</p>

<pre><code>dtm = as.DocumentTermMatrix(read_stm_CLUTO(file), weightTf);
rows &lt;- scan(""rows.txt"", what="""", sep=""\n"");
columns &lt;- scan(""columns.txt"", what="""", sep=""\n"");

dtm$dimnames = list(rows,columns);
</code></pre>
","r, text-mining, tm, cluto","<p>This should do it: </p>

<pre><code>require(slam)
as.DocumentTermMatrix(read_stm_CLUTO(file), weightTf)
</code></pre>

<p>If you can link to your CLUTO file or an add an excerpt of it to your Q we can look at row and column names. </p>

<p>hat-tip: <a href=""https://r-forge.r-project.org/scm/viewvc.php/pkg/R/foreign.R?root=tm&amp;view=diff&amp;r1=1127&amp;r2=1127&amp;diff_format=s"" rel=""nofollow"">https://r-forge.r-project.org/scm/viewvc.php/pkg/R/foreign.R?root=tm&amp;view=diff&amp;r1=1127&amp;r2=1127&amp;diff_format=s</a></p>
",1,0,410,2013-04-02 14:55:12,https://stackoverflow.com/questions/15768562/cluto-document-term-matrix-to-tm-documenttermmatrix
Cluster text documents in database,"<p>I do have 20.000 text files loaded in PostgreSQL database, one file in one row, all stored in table named <code>docs</code> with columns <code>doc_id</code> and <code>doc_content</code>.</p>

<p>I know that there is approximately 8 types of documents. Here are my questions:</p>

<ul>
<li>How can I find these groups?</li>
<li>Are there some similarity, dissimilarity measures I can use?</li>
<li>Is there some implementation of longest common substring in PostgreSQL?</li>
<li>Are there some extensions for text mining in PostgreSQL? (I've found only <a href=""http://www.sai.msu.su/~megera/wiki/Tsearch2"" rel=""nofollow"">Tsearch</a>, but this seems to be last updated in 2007)</li>
</ul>

<p>I can probably use some <code>like '%%'</code> or <code>SIMILAR TO</code>, but there might be better approach.</p>
","postgresql, data-mining, text-mining, document-classification","<p>You should use <a href=""http://www.postgresql.org/docs/9.2/static/textsearch.html"" rel=""nofollow"">full text search</a>, which is part of PostgreSQL 9.x core (aka Tsearch2).</p>

<p>For some kind of measure of longest common substring (or similarity if you will), you might be able to use <code>levenshtein()</code> function - part of <a href=""http://www.postgresql.org/docs/9.2/static/fuzzystrmatch.html"" rel=""nofollow""><code>fuzzystrmatch</code></a> extension.</p>
",1,1,951,2013-04-04 08:02:24,https://stackoverflow.com/questions/15805643/cluster-text-documents-in-database
How to fetch information in XML format from Nutch spidered webpages database,"<p>I'm trying to build books aggregation portal. Nutch provides me excellent web crawler, but I want very specific information like, book title, book price, ISBN, author etc. How to extract that information from the crawled pages? I would like to fetch this information in XML format if possible.</p>

<p>In addition to the above, I would like to ask if this is the right approach! Can it be done in better way with other open source software?</p>
","xml, nutch, aggregation, text-mining","<p>It depends on how structured the data is.</p>

<p>I assume you are crawling mostly HTML pages.</p>

<p>Oftentimes you can use <a href=""https://developer.mozilla.org/en/docs/XPath"" rel=""nofollow"">XPath</a> to grab portions of the page, such as ""//div[@class='books']/a/text()""</p>

<p>If much of the text is unstructured (no structured HTML patterns to grab), then you are going to have to use regular expressions or information extraction.</p>

<p>If you're lucky, you can do some/most of it using regular expressions.</p>

<p>For some more complicated structures, you'd need to use information extraction / named entity recognition.</p>

<p>You would have to train an IE tool, such as <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford's CoreNLP</a> to recognize, say, book titles and annotate them in your documents. Also check out <a href=""http://brat.nlplab.org/index.html"" rel=""nofollow"">BRAT Rapid Annotation Tool</a>.</p>

<p>Services like Mozenda can do the crawling and X-Path work for you, but I haven't seen a company that provides IE services. </p>
",0,1,156,2013-04-09 18:21:37,https://stackoverflow.com/questions/15909558/how-to-fetch-information-in-xml-format-from-nutch-spidered-webpages-database
Find the 10 most frequently occurring words with morphology,"<p>Please tell me how to solve this problem better.</p>

<p>I have the text of the Russian and I want to find the 10 most common words <strong>with morphology</strong>.
Maybe there is any open source libraries to solve this issue in <strong>python</strong>?</p>
","python, text-mining, text-analysis","<p>You can use one of Python morphology analyzers for Russian to normalize the word:</p>

<ul>
<li><a href=""https://github.com/kmike/pymorphy2"" rel=""nofollow"">https://github.com/kmike/pymorphy2</a></li>
<li><a href=""https://github.com/kmike/pymorphy"" rel=""nofollow"">https://github.com/kmike/pymorphy</a></li>
<li><a href=""https://github.com/irokez/Pyrus"" rel=""nofollow"">https://github.com/irokez/Pyrus</a></li>
</ul>

<p>There is also a Porter stemmer for Russian in <a href=""https://github.com/nltk/nltk"" rel=""nofollow"">https://github.com/nltk/nltk</a>. Also, you could employ <a href=""http://company.yandex.ru/technologies/mystem/"" rel=""nofollow"">http://company.yandex.ru/technologies/mystem/</a> from a command line.</p>

<p>I'd recommend pymorphy2 for your task, but I'm a bit biased :)</p>
",4,3,367,2013-04-12 12:35:34,https://stackoverflow.com/questions/15971578/find-the-10-most-frequently-occurring-words-with-morphology
Visualise distances between texts,"<p>I'm working on a research project for school. I've written some text mining software that analyzes legal texts in a collection and spits out a score that indicates how similar they are.  I ran the program to compare each text with every other text, and I have data like this (although with many more points):</p>

<pre><code>codeofhammurabi.txt crete.txt      0.570737
codeofhammurabi.txt iraqi.txt      1.13475
codeofhammurabi.txt magnacarta.txt 0.945746
codeofhammurabi.txt us.txt         1.25546
crete.txt iraqi.txt                0.329545
crete.txt magnacarta.txt           0.589786
crete.txt us.txt                   0.491903
iraqi.txt magnacarta.txt           0.834488
iraqi.txt us.txt                   1.37718
magnacarta.txt us.txt              1.09582
</code></pre>

<p>Now I need to plot them on a graph.  I can easily invert the scores so that a small value now indicates texts that are similar and a large value indicates texts that are dissimilar: the value can be the distance between points on a graph representing the texts.</p>

<pre><code>codeofhammurabi.txt crete.txt      1.75212
codeofhammurabi.txt iraqi.txt      0.8812
codeofhammurabi.txt magnacarta.txt 1.0573
codeofhammurabi.txt us.txt         0.7965
crete.txt iraqi.txt                3.0344
crete.txt magnacarta.txt           1.6955
crete.txt us.txt                   2.0329
iraqi.txt magnacarta.txt           1.1983
iraqi.txt us.txt                   0.7261
magnacarta.txt us.txt              0.9125
</code></pre>

<p>SHORT VERSION:
Those values directly above are distances between points on a scatter plot (1.75212 is the distance between the codeofhammurabi point and the crete point).  I can imagine a big system of equations with circles representing the distances between points.  What's the best way to make this graph?  I have MATLAB, R, Excel, and access to pretty much any software I might need.</p>

<p>If you can even point me in a direction, I'll be infinitely grateful.</p>
","r, matlab, graph, distance, text-mining","<p>Your data are really distances (of some form) in the multivariate space spanned by the corpus of words contained in the documents. Dissimilarity data such as these are often ordinated to provide the best <em>k</em>-d mapping of the dissimilarities. Principal coordinates analysis and non-metric multidimensional scaling are two such methods. I would suggest you plot the results of applying one or the other of these methods to your data. I provide examples of both below.</p>

<p>First, load in the data you supplied (without labels at this stage)</p>

<pre><code>con &lt;- textConnection(""1.75212
0.8812
1.0573
0.7965
3.0344
1.6955
2.0329
1.1983
0.7261
0.9125
"")
vec &lt;- scan(con)
close(con)
</code></pre>

<p>What you effectively have is the following distance matrix:</p>

<pre><code>mat &lt;- matrix(ncol = 5, nrow = 5)
mat[lower.tri(mat)] &lt;- vec
colnames(mat) &lt;- rownames(mat) &lt;-
  c(""codeofhammurabi"",""crete"",""iraqi"",""magnacarta"",""us"")

&gt; mat
                codeofhammurabi  crete  iraqi magnacarta us
codeofhammurabi              NA     NA     NA         NA NA
crete                   1.75212     NA     NA         NA NA
iraqi                   0.88120 3.0344     NA         NA NA
magnacarta              1.05730 1.6955 1.1983         NA NA
us                      0.79650 2.0329 0.7261     0.9125 NA
</code></pre>

<p>R, in general, needs a dissimilarity object of class <code>""dist""</code>. We could use <code>as.dist(mat)</code> now to get such an object, or we could skip creating <code>mat</code> and go straight to the <code>""dist""</code> object like this:</p>

<pre><code>class(vec) &lt;- ""dist""
attr(vec, ""Labels"") &lt;- c(""codeofhammurabi"",""crete"",""iraqi"",""magnacarta"",""us"")
attr(vec, ""Size"") &lt;- 5
attr(vec, ""Diag"") &lt;- FALSE
attr(vec, ""Upper"") &lt;- FALSE

&gt; vec
           codeofhammurabi   crete   iraqi magnacarta
crete              1.75212                           
iraqi              0.88120 3.03440                   
magnacarta         1.05730 1.69550 1.19830           
us                 0.79650 2.03290 0.72610    0.91250
</code></pre>

<p>Now we have an object of the right type we can ordinate it. R has many packages and functions for doing this (see the <a href=""http://cran.r-project.org/web/views/Multivariate.html"" rel=""noreferrer"">Multivariate</a> or <a href=""http://cran.r-project.org/web/views/Environmetrics.html"" rel=""noreferrer"">Environmetrics</a> Task Views on CRAN), but I'll use the <strong>vegan</strong> package as I am somewhat familiar with it...</p>

<pre><code>require(""vegan"")
</code></pre>

<h2>Principal coordinates</h2>

<p>First I illustrate how to do principal coordinates analysis on your data using <strong>vegan</strong>.</p>

<pre><code>pco &lt;- capscale(vec ~ 1, add = TRUE)
pco

&gt; pco
Call: capscale(formula = vec ~ 1, add = TRUE)

              Inertia Rank
Total           10.42     
Unconstrained   10.42    3
Inertia is squared Unknown distance (euclidified) 

Eigenvalues for unconstrained axes:
 MDS1  MDS2  MDS3 
7.648 1.672 1.098 

Constant added to distances: 0.7667353
</code></pre>

<p>The first PCO axis is by far the most important at explaining the between text differences, as exhibited by the Eigenvalues. An ordination plot can now be produced by plotting the Eigenvectors of the PCO, using the <code>plot</code> method</p>

<pre><code>plot(pco)
</code></pre>

<p>which produces</p>

<p><img src=""https://i.sstatic.net/jpAMR.png"" alt=""enter image description here""></p>

<h2>Non-metric multidimensional scaling</h2>

<p>A non-metric multidimensional scaling (nMDS) does not attempt to find a low dimensional representation of the original distances in an Euclidean space. Instead it tries to find a mapping in <em>k</em> dimensions that best preserves the <strong>rank</strong> ordering of the distances between observations. There is no closed-form solution to this problem (unlike the PCO applied above) and an iterative algorithm is required to provide a solution. Random starts are advised to assure yourself that the algorithm hasn't converged to a sub-optimal, locally optimal solution. Vegan's <code>metaMDS</code> function incorporates these features and more besides. If you want plain old nMDS, then see <code>isoMDS</code> in package <strong>MASS</strong>.</p>

<pre><code>set.seed(42)
sol &lt;- metaMDS(vec)

&gt; sol

Call:
metaMDS(comm = vec) 

global Multidimensional Scaling using monoMDS

Data:     vec 
Distance: user supplied 

Dimensions: 2 
Stress:     0 
Stress type 1, weak ties
No convergent solutions - best solution after 20 tries
Scaling: centring, PC rotation 
Species: scores missing
</code></pre>

<p>With this small data set we can essentially represent the rank ordering of the dissimilarities perfectly (hence the warning, not shown). A plot can be achieved using the <code>plot</code> method</p>

<pre><code>plot(sol, type = ""text"", display = ""sites"")
</code></pre>

<p>which produces</p>

<p><img src=""https://i.sstatic.net/DFD6z.png"" alt=""enter image description here""></p>

<p>In both cases the distance on the plot between samples is the best 2-d approximation of their dissimilarity. In the case of the PCO plot, it is a 2-d approximation of the real dissimilarity (3 dimensions are needed to represent all of the dissimilarities fully), whereas in the nMDS plot, the distance between samples on the plot reflects the rank dissimilarity not the actual dissimilarity between observations. But essentially distances on the plot represent the computed dissimilarities. Texts that are close together are most similar, texts located far apart on the plot are the most dissimilar to one another.</p>
",10,11,5069,2013-04-14 21:57:09,https://stackoverflow.com/questions/16004847/visualise-distances-between-texts
Text-mining with the tm-package - word stemming,"<p>I am doing some text mining in R with the <code>tm</code>-package. Everything works very smooth. However, one problem occurs after stemming (<a href=""http://en.wikipedia.org/wiki/Stemming"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Stemming</a>). Obviously, there are some words, which have the same stem, but it is important that they are not ""thrown together"" (as those words mean different things). </p>

<p>For an example see the 4 texts below. Here you cannnot use ""lecturer"" or ""lecture"" (""association"" and ""associate"") interchangeable. However, this is what is done in step 4. </p>

<p>Is there any elegant solution how to implement this for some cases/words manually (e.g. that ""lecturer"" and ""lecture"" are kept as two different things)?</p>

<pre><code>texts &lt;- c(""i am member of the XYZ association"",
""apply for our open associate position"", 
""xyz memorial lecture takes place on wednesday"", 
""vote for the most popular lecturer"")

# Step 1: Create corpus
corpus &lt;- Corpus(DataframeSource(data.frame(texts)))

# Step 2: Keep a copy of corpus to use later as a dictionary for stem completion
corpus.copy &lt;- corpus

# Step 3: Stem words in the corpus
corpus.temp &lt;- tm_map(corpus, stemDocument, language = ""english"")  

inspect(corpus.temp)

# Step 4: Complete the stems to their original form
corpus.final &lt;- tm_map(corpus.temp, stemCompletion, dictionary = corpus.copy)  

inspect(corpus.final)
</code></pre>
","r, text-mining, tm","<p>I'm not 100% sure what you're after and don't totally get how <code>tm_map</code> works.  If I understand then the following works.  As I understand you want to supply a list of words that should not be stemmed.  I'm using the qdap package mostly because I'm lazy and it has a function <code>mgsub</code> I like.</p>
<p>Note that I got frustrated with using <code>mgsub</code> and <code>tm_map</code> as it kept throwing an error so I just used <code>lapply</code> instead.</p>
<pre><code>texts &lt;- c(&quot;i am member of the XYZ association&quot;,
    &quot;apply for our open associate position&quot;, 
    &quot;xyz memorial lecture takes place on wednesday&quot;, 
    &quot;vote for the most popular lecturer&quot;)

library(tm)
# Step 1: Create corpus
corpus.copy &lt;- corpus &lt;- Corpus(DataframeSource(data.frame(texts)))

library(qdap)
# Step 2: list to retain and indentifier keys
retain &lt;- c(&quot;lecturer&quot;, &quot;lecture&quot;)
replace &lt;- paste(seq_len(length(retain)), &quot;SPECIAL_WORD&quot;, sep=&quot;_&quot;)

# Step 3: sub the words you want to retain with identifier keys
corpus[seq_len(length(corpus))] &lt;- lapply(corpus, mgsub, pattern=retain, replacement=replace)

# Step 4: Stem it
corpus.temp &lt;- tm_map(corpus, stemDocument, language = &quot;english&quot;)  

# Step 5: reverse -&gt; sub the identifier keys with the words you want to retain
corpus.temp[seq_len(length(corpus.temp))] &lt;- lapply(corpus.temp, mgsub, pattern=replace, replacement=retain)

inspect(corpus)       #inspect the pieces for the folks playing along at home
inspect(corpus.copy)
inspect(corpus.temp)

# Step 6: complete the stem
corpus.final &lt;- tm_map(corpus.temp, stemCompletion, dictionary = corpus.copy)  
inspect(corpus.final)
</code></pre>
<p><strong>Basically it works by:</strong></p>
<ol>
<li>subbing out a unique identifier key for the supplied &quot;NO STEM&quot; words (the <code>mgsub</code>)</li>
<li>then you stem (using <code>stemDocument</code>)</li>
<li>next you reverse it and sub the identifier keys with the &quot;NO STEM&quot; words (the <code>mgsub</code>)</li>
<li>last complete the Stem (<code>stemCompletion</code>)</li>
</ol>
<p><strong>Here's the output:</strong></p>
<pre><code>## &gt;     inspect(corpus.final)
## A corpus with 4 text documents
## 
## The metadata consists of 2 tag-value pairs and a data frame
## Available tags are:
##   create_date creator 
## Available variables in the data frame are:
##   MetaID 
## 
## $`1`
## i am member of the XYZ associate
## 
## $`2`
##  for our open associate position
## 
## $`3`
## xyz memorial lecture takes place on wednesday
## 
## $`4`
## vote for the most popular lecturer
</code></pre>
",13,12,41622,2013-04-17 20:15:16,https://stackoverflow.com/questions/16069406/text-mining-with-the-tm-package-word-stemming
extract semi-structured text from Word documents,"<p>I want to text-mine a set of files based on the below form.  I can create a corpus where each file is a document (using <code>tm</code>), but I'm thinking it might be better to create a corpus where each section in the 2nd form table was a document having the following meta data:</p>

<pre><code>  Author       : John Smith
  DateTimeStamp: 2013-04-18 16:53:31
  Description  : 
  Heading      : Current Focus
  ID           : Smith-John_e.doc Current Focus
  Language     : en_CA
  Origin       : Smith-John_e.doc
  Name         : John Smith
  Title        : Manager
  TeamMembers  : Joe Blow, John Doe
  GroupLeader  : She who must be obeyed 
</code></pre>

<p>where Name, Title, TeamMembers and GroupLeader are extracted from the first table on the form. In this way, each chunk of text to be analyzed would maintain some of its context.</p>

<p>What is the best way to approach this?  I can think of 2 ways:</p>

<ul>
<li>somehow parse the corpus I have into child corpora.</li>
<li>somehow parse the document into subdocuments and make a corpus from those.</li>
</ul>

<p>Any pointers would be much appreciated.</p>

<p>This is the form:
<img src=""https://i.sstatic.net/ojVNs.png"" alt=""HR form""></p>

<p><a href=""https://docs.google.com/file/d/0B1krGGny7BduT2tVeGxCaW5XT2c/edit?usp=sharing"" rel=""nofollow noreferrer"">Here is an RData file</a> of a corpus with 2 documents. exc[[1]] came from a .doc and exc[[2]] came from a docx. They both used the form above.</p>
","r, text-mining, tm","<p>Here's a quick sketch of a method, hopefully it might provoke someone more talented to stop by and suggest something more efficient and robust... Using the <code>RData</code> file in your question, I found that the <code>doc</code> and <code>docx</code> files have slightly different structures and so require slightly different approaches (though I see in the metadata that your <code>docx</code> is 'fake2.txt', so is it really <code>docx</code>? I see in your other Q that you used a converter outside of R, that must be why it's <code>txt</code>). </p>

<pre><code>library(tm)
</code></pre>

<p>First get custom metadata for the <code>doc</code> file. I'm no regex expert, as you can see, but it's roughly 'get rid of trailing and leading spaces' then 'get rid of ""Word""', then get rid of punctuation...</p>

<pre><code># create User-defined local meta data pairs
meta(exc[[1]], type = ""corpus"", tag = ""Name1"") &lt;- gsub(""^\\s+|\\s+$"","""", gsub(""Name"", """", gsub(""[[:punct:]]"", '', exc[[1]][3])))
meta(exc[[1]], type = ""corpus"", tag = ""Title"") &lt;- gsub(""^\\s+|\\s+$"","""", gsub(""Title"", """", gsub(""[[:punct:]]"", '', exc[[1]][4])))
meta(exc[[1]], type = ""corpus"", tag = ""TeamMembers"") &lt;- gsub(""^\\s+|\\s+$"","""", gsub(""Team Members"", """", gsub(""[[:punct:]]"", '', exc[[1]][5])))
meta(exc[[1]], type = ""corpus"", tag = ""ManagerName"") &lt;- gsub(""^\\s+|\\s+$"","""", gsub(""Name of your"", """", gsub(""[[:punct:]]"", '', exc[[1]][7])))
</code></pre>

<p>Now have a look at the result</p>

<pre><code>    # inspect
    meta(exc[[1]], type = ""corpus"")
Available meta data pairs are:
  Author       : 
  DateTimeStamp: 2013-04-22 13:59:28
  Description  : 
  Heading      : 
  ID           : fake1.doc
  Language     : en_CA
  Origin       : 
User-defined local meta data pairs are:
$Name1
[1] ""John Doe""

$Title
[1] ""Manager""

$TeamMembers
[1] ""Elise Patton Jeffrey Barnabas""

$ManagerName
[1] ""Selma Furtgenstein""
</code></pre>

<p>Do the same for the <code>docx</code> file</p>

<pre><code># create User-defined local meta data pairs
meta(exc[[2]], type = ""corpus"", tag = ""Name2"") &lt;- gsub(""^\\s+|\\s+$"","""", gsub(""Name"", """", gsub(""[[:punct:]]"", '', exc[[2]][2])))
meta(exc[[2]], type = ""corpus"", tag = ""Title"") &lt;- gsub(""^\\s+|\\s+$"","""", gsub(""Title"", """", gsub(""[[:punct:]]"", '', exc[[2]][4])))
meta(exc[[2]], type = ""corpus"", tag = ""TeamMembers"") &lt;- gsub(""^\\s+|\\s+$"","""", gsub(""Team Members"", """", gsub(""[[:punct:]]"", '', exc[[2]][6])))
meta(exc[[2]], type = ""corpus"", tag = ""ManagerName"") &lt;- gsub(""^\\s+|\\s+$"","""", gsub(""Name of your"", """", gsub(""[[:punct:]]"", '', exc[[2]][8])))
</code></pre>

<p>And have a look</p>

<pre><code># inspect
meta(exc[[2]], type = ""corpus"")
Available meta data pairs are:
  Author       : 
  DateTimeStamp: 2013-04-22 14:06:10
  Description  : 
  Heading      : 
  ID           : fake2.txt
  Language     : en
  Origin       : 
User-defined local meta data pairs are:
$Name2
[1] ""Joe Blow""

$Title
[1] ""Shift Lead""

$TeamMembers
[1] ""Melanie Baumgartner Toby Morrison""

$ManagerName
[1] ""Selma Furtgenstein""
</code></pre>

<p>If you have a large number of documents then a <code>lapply</code> function that includes these <code>meta</code> functions would be the way to go.</p>

<p>Now that we've got the custom metadata, we can subset the documents to exclude that part of the text:</p>

<pre><code># create new corpus that excludes part of doc that is now in metadata. We just use square bracket indexing to subset the lines that are the second table of the forms (slightly different for each doc type)
excBody &lt;- Corpus(VectorSource(c(paste(exc[[1]][13:length(exc[[1]])], collapse = "",""), 
                      paste(exc[[2]][9:length(exc[[2]])], collapse = "",""))))
# get rid of all the white spaces
excBody &lt;- tm_map(excBody, stripWhitespace)
</code></pre>

<p>Have a look:</p>

<pre><code>inspect(excBody)
A corpus with 2 text documents

The metadata consists of 2 tag-value pairs and a data frame
Available tags are:
  create_date creator 
Available variables in the data frame are:
  MetaID 

[[1]]
|CURRENT RESEARCH FOCUS |,| |,|Lorem ipsum dolor sit amet, consectetur adipiscing elit. |,|Donec at ipsum est, vel ullamcorper enim. |,|In vel dui massa, eget egestas libero. |,|Phasellus facilisis cursus nisi, gravida convallis velit ornare a. |,|MAIN AREAS OF EXPERTISE |,|Vestibulum aliquet faucibus tortor, sed aliquet purus elementum vel. |,|In sit amet ante non turpis elementum porttitor. |,|TECHNOLOGY PLATFORMS, INSTRUMENTATION EMPLOYED |,| Vestibulum sed turpis id nulla eleifend fermentum. |,|Nunc sit amet elit eu neque tincidunt aliquet eu at risus. |,|Cras tempor ipsum justo, ut blandit lacus. |,|INDUSTRY PARTNERS (WITHIN THE PAST FIVE YEARS) |,| Pellentesque facilisis nisl in libero scelerisque mattis eu quis odio. |,|Etiam a justo vel sapien rhoncus interdum. |,|ANTICIPATED PARTICIPATION IN PROGRAMS, EITHER APPROVED OR UNDER DEVELOPMENT |,|(Please include anticipated percentages of your time.) |,| Proin vitae ligula quis enim vulputate sagittis vitae ut ante. |,|ADDITIONAL ROLES, DISTINCTIONS, ACADEMIC QUALIFICATIONS AND NOTES |,|e.g., First Aid Responder, Other languages spoken, Degrees, Charitable Campaign |,|Canvasser (GCWCC), OSH representative, Social Committee |,|Sed nec tellus nec massa accumsan faucibus non imperdiet nibh. |,,

[[2]]
CURRENT RESEARCH FOCUS,,* Lorem ipsum dolor sit amet, consectetur adipiscing elit.,* Donec at ipsum est, vel ullamcorper enim.,* In vel dui massa, eget egestas libero.,* Phasellus facilisis cursus nisi, gravida convallis velit ornare a.,MAIN AREAS OF EXPERTISE,* Vestibulum aliquet faucibus tortor, sed aliquet purus elementum vel.,* In sit amet ante non turpis elementum porttitor. ,TECHNOLOGY PLATFORMS, INSTRUMENTATION EMPLOYED,* Vestibulum sed turpis id nulla eleifend fermentum.,* Nunc sit amet elit eu neque tincidunt aliquet eu at risus.,* Cras tempor ipsum justo, ut blandit lacus.,INDUSTRY PARTNERS (WITHIN THE PAST FIVE YEARS),* Pellentesque facilisis nisl in libero scelerisque mattis eu quis odio.,* Etiam a justo vel sapien rhoncus interdum.,ANTICIPATED PARTICIPATION IN PROGRAMS, EITHER APPROVED OR UNDER DEVELOPMENT ,(Please include anticipated percentages of your time.),* Proin vitae ligula quis enim vulputate sagittis vitae ut ante.,ADDITIONAL ROLES, DISTINCTIONS, ACADEMIC QUALIFICATIONS AND NOTES,e.g., First Aid Responder, Other languages spoken, Degrees, Charitable Campaign Canvasser (GCWCC), OSH representative, Social Committee,* Sed nec tellus nec massa accumsan faucibus non imperdiet nibh.,,
</code></pre>

<p>Now the documents are ready for text mining, with the data from the upper table moved out of the document and into the document metadata.</p>

<p>Of course all of this depends on the documents being highly regular. If there are different numbers of lines in the first table in each doc, then the simple indexing method might fail (give it a try and see what happens) and something more robust will be needed.</p>

<p><strong>UPDATE: A more robust method</strong></p>

<p>Having read the question a little more carefully, and <a href=""https://stackoverflow.com/q/16161573/1036500"">got a bit more education about regex</a>, here's a method that is more robust and doesn't depend on indexing specific lines of the documents. Instead, we use regular expressions to extract text from between two words to make the metadata and split the document</p>

<p>Here's how we make the User-defined local meta data (a method to replace the one above)</p>

<pre><code>library(gdata) # for the trim function
txt &lt;- paste0(as.character(exc[[1]]), collapse = "","")

# inspect the document to identify the words on either side of the string
# we want, so 'Name' and 'Title' are on either side of 'John Doe'
extract &lt;- regmatches(txt, gregexpr(""(?&lt;=Name).*?(?=Title)"", txt, perl=TRUE))
meta(exc[[1]], type = ""corpus"", tag = ""Name1"") &lt;- trim(gsub(""[[:punct:]]"", """", extract))

extract &lt;- regmatches(txt, gregexpr(""(?&lt;=Title).*?(?=Team)"", txt, perl=TRUE))
meta(exc[[1]], type = ""corpus"", tag = ""Title"") &lt;- trim(gsub(""[[:punct:]]"","""", extract))

extract &lt;- regmatches(txt, gregexpr(""(?&lt;=Members).*?(?=Supervised)"", txt, perl=TRUE))
meta(exc[[1]], type = ""corpus"", tag = ""TeamMembers"") &lt;- trim(gsub(""[[:punct:]]"","""", extract))

extract &lt;- regmatches(txt, gregexpr(""(?&lt;=your).*?(?=Supervisor)"", txt,  perl=TRUE))
meta(exc[[1]], type = ""corpus"", tag = ""ManagerName"") &lt;- trim(gsub(""[[:punct:]]"","""", extract))

# inspect
meta(exc[[1]], type = ""corpus"")

Available meta data pairs are:
  Author       : 
  DateTimeStamp: 2013-04-22 13:59:28
  Description  : 
  Heading      : 
  ID           : fake1.doc
  Language     : en_CA
  Origin       : 
User-defined local meta data pairs are:
$Name1
[1] ""John Doe""

$Title
[1] ""Manager""

$TeamMembers
[1] ""Elise Patton Jeffrey Barnabas""

$ManagerName
[1] ""Selma Furtgenstein""
</code></pre>

<p>Similarly we can extract the sections of your second table into separate 
vectors and then you can make them into documents and corpora or just work 
on them as vectors.</p>

<pre><code>txt &lt;- paste0(as.character(exc[[1]]), collapse = "","")
CURRENT_RESEARCH_FOCUS &lt;- trim(gsub(""[[:punct:]]"","""", regmatches(txt, gregexpr(""(?&lt;=CURRENT RESEARCH FOCUS).*?(?=MAIN AREAS OF EXPERTISE)"", txt, perl=TRUE))))
[1] ""Lorem ipsum dolor sit amet consectetur adipiscing elit                             Donec at ipsum est vel ullamcorper enim                                            In vel dui massa eget egestas libero                                               Phasellus facilisis cursus nisi gravida convallis velit ornare a""


MAIN_AREAS_OF_EXPERTISE &lt;- trim(gsub(""[[:punct:]]"","""", regmatches(txt, gregexpr(""(?&lt;=MAIN AREAS OF EXPERTISE).*?(?=TECHNOLOGY PLATFORMS, INSTRUMENTATION EMPLOYED)"", txt, perl=TRUE))))
    [1] ""Vestibulum aliquet faucibus tortor sed aliquet purus elementum vel                 In sit amet ante non turpis elementum porttitor""
</code></pre>

<p>And so on. I hope that's a bit closer to what you're after. If not, it might be best to break down your task into a set of smaller, more focused questions, and ask them separately (or wait for one of the gurus to stop by this question!). </p>
",2,2,1591,2013-04-19 13:37:32,https://stackoverflow.com/questions/16105973/extract-semi-structured-text-from-word-documents
Counting punctuation in text using Python and regex,"<p>I am trying to count the number of times punctuation characters appear in a novel. For example, I want to find the occurrences of question marks and periods along with all the other non alphanumeric characters. Then I want to insert them into a csv file. I am not sure how to do the regex because I don't have that much experience with python. Can someone help me out? </p>

<pre><code>texts=string.punctuation
counts=dict(Counter(w.lower() for w in re.findall(r""\w+"", open(cwd+""/""+book).read())))
writer = csv.writer(open(""author.csv"", 'a'))
writer.writerow([counts.get(fieldname,0) for fieldname in texts])
</code></pre>
","python, regex, text-mining","<pre><code>In [1]: from string import punctuation

In [2]: from collections import Counter

In [3]: counts = Counter(open('novel.txt').read())

In [4]: punctuation_counts = {k:v for k, v in counts.iteritems() if k in punctuation}
</code></pre>
",7,3,10664,2013-04-30 04:08:17,https://stackoverflow.com/questions/16292007/counting-punctuation-in-text-using-python-and-regex
Location mining from text,"<p>I'm working on a text mining problem: extract the place from the text. The place could be either only states, or more specific such as name of a neighborhood in Chicago, or even a specific address. But it's only in US.</p>

<p>I've been trying Yahoo Place maker api, but I can't create the api key ( the website is not responding). Is there anyway to do it, such as rapid miner, or write a comprehensive regex?</p>
","data-mining, text-mining","<p>Consider Stanford Named Entity Recognizer (NER). Online demo here:</p>

<p><a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow"">http://nlp.stanford.edu:8080/ner/process</a></p>

<p>It's a java library. License is GPL v2, though the license to distribute in a standalone app is pricey.</p>
",1,1,175,2013-05-05 22:53:36,https://stackoverflow.com/questions/16389937/location-mining-from-text
Parse JSON to objects without any third-party tools,"<p>How do I parse <a href=""http://en.wikipedia.org/wiki/JSON"" rel=""nofollow"">JSON</a> using VB.NET without any third-party tools?</p>

<p>I know that there is the JSON.NET tool which parses the JSON string to objects and <a href=""http://en.wikipedia.org/wiki/XML"" rel=""nofollow"">XML</a>. </p>

<p>What would be a method for parsing it?</p>

<p>Please don't reply saying ""Why not use JSON.NET?"". I want to create my own methods.</p>
",".net, vb.net, json, parsing, text-mining","<p>As the commenter said, <a href=""http://www.json.org/"" rel=""nofollow noreferrer"">http://www.json.org/</a> has all the control flow logic you need to do a simple implementation. Implementing your own <strong>basic</strong> version should not be too hard. I would strongly recommend using a third party one - I have had to hand crank my own one in C back in the day and it was not fun.</p>
<p>PS: An existing parser can save you lots of time and grey hairs when developing your own parser, use something like <a href=""https://jsonformatter.org/json-parser"" rel=""nofollow noreferrer"">https://jsonformatter.org/json-parser</a></p>
",1,2,691,2013-05-08 23:40:11,https://stackoverflow.com/questions/16451997/parse-json-to-objects-without-any-third-party-tools
R Text mining - how to change texts in R data frame column into several columns with bigram frequencies?,"<p>In addition to question <a href=""https://stackoverflow.com/questions/15258954/r-text-mining-how-to-change-texts-in-r-data-frame-column-into-several-columns"">R Text mining - how to change texts in R data frame column into several columns with word frequencies?</a> I am wondering how I can manage to make columns with bigrams frequencies instead of just word frequencies.
Again, many thanks in advance!</p>

<p>This is the example data frame (thanks to Tyler Rinker).</p>

<pre><code>      person sex adult                                 state code
1         sam   m     0         Computer is fun. Not too fun.   K1
2        greg   m     0               No it's not, it's dumb.   K2
3     teacher   m     1                    What should we do?   K3
4         sam   m     0                  You liar, it stinks!   K4
5        greg   m     0               I am telling the truth!   K5
6       sally   f     0                How can we be certain?   K6
7        greg   m     0                      There is no way.   K7
8         sam   m     0                       I distrust you.   K8
9       sally   f     0           What are you talking about?   K9
10 researcher   f     1         Shall we move on?  Good then.  K10
11       greg   m     0 I'm hungry.  Let's eat.  You already?  K11
</code></pre>

<p>Data set above:</p>

<pre><code>library(qdap); DATA
</code></pre>
","r, text-mining","<p>The dev version of <code>qdap</code> (should go to CRAN within the next few days) does ngrams.  For now you'll need to use the <a href=""https://github.com/trinker/qdap"" rel=""nofollow"">dev version</a>.  On the toy data set this is fast but on a larger data set such as <code>qdap</code>'s <code>mraja1</code> data set requires ~5 minutes to complete.  You could:</p>

<ol>
<li>Select the bigrams more wisely (i.e., don't use them all as there's going to be a ton)</li>
<li>Wait the time</li>
<li><a href=""http://trinkerrstuff.wordpress.com/2012/08/19/parallelization-speed-up-functions-in-a-package/"" rel=""nofollow"">Run it in parallel</a></li>
<li>Figure out another way to do this</li>
<li>Get a faster computer</li>
</ol>

<p>Here's the code to get the dev version of <code>qdap</code> and run the bigram search:</p>

<pre><code>library(devtools)
install_github(""qdap"", ""trinker"")
library(qdap)

## this gets the bigrams
bigrams &lt;- sapply(ngrams(DATA$state)[[c(""all_n"", ""n_2"")]], paste, collapse="" "")

## This searches by grouping variable for bigram use
termco(DATA$state, DATA$person, bigrams)


## To get raw values
termco(DATA$state, DATA$person, bigrams)[[""raw""]]
</code></pre>
",2,1,1251,2013-05-18 15:54:55,https://stackoverflow.com/questions/16626168/r-text-mining-how-to-change-texts-in-r-data-frame-column-into-several-columns
How to recreate same DocumentTermMatrix with new (test) data,"<p>Suppose I have text based training data and testing data. To be more specific, I have two data sets - training and testing - and both of them have one column which contains text and is of interest for the job at hand.</p>

<p>I used tm package in R to process the text column in the training data set. After removing the white spaces, punctuation, and stop words, I stemmed the corpus and finally created a document term matrix of 1 grams containing the frequency/count of the words in each document. I then took a pre-determined cut-off of, say, 50 and kept only those terms that have a count of greater than 50.  </p>

<p>Following this, I train a, say, GLMNET model using the DTM and the dependent variable (which was present in the training data). Everything runs smooth and easy till now.</p>

<p>However, how do I proceed when I want to score/predict the model on the testing data or any new data that might come in the future?</p>

<p>Specifically, what I am trying to find out is that how do I create the exact DTM on new data? </p>

<p>If the new data set does not have any of the similar words as the original training data then all the terms should have a count of zero (which is fine). But I want to be able to replicate the exact same DTM (in terms of structure) on any new corpus.</p>

<p>Any ideas/thoughts?</p>
","r, machine-learning, nlp, text-mining, tm","<p>If I understand correctly, you have made a dtm, and you want to make a new dtm from new documents that has the same columns (ie. terms) as the first dtm. If that's the case, then it should be a matter of sub-setting the second dtm by the terms in the first, perhaps something like this:</p>

<p>First set up some reproducible data...</p>

<p>This is your training data...</p>

<pre><code>library(tm)
# make corpus for text mining (data comes from package, for reproducibility) 
data(""crude"")
corpus1 &lt;- Corpus(VectorSource(crude[1:10]))    
# process text (your methods may differ)
skipWords &lt;- function(x) removeWords(x, stopwords(""english""))
funcs &lt;- list(tolower, removePunctuation, removeNumbers,
              stripWhitespace, skipWords)
crude1 &lt;- tm_map(corpus1, FUN = tm_reduce, tmFuns = funcs)
crude1.dtm &lt;- DocumentTermMatrix(crude1, control = list(wordLengths = c(3,10))) 
</code></pre>

<p>And this is your testing data...    </p>

<pre><code>corpus2 &lt;- Corpus(VectorSource(crude[15:20]))  
# process text (your methods may differ)
skipWords &lt;- function(x) removeWords(x, stopwords(""english""))
funcs &lt;- list(tolower, removePunctuation, removeNumbers,
              stripWhitespace, skipWords)
crude2 &lt;- tm_map(corpus2, FUN = tm_reduce, tmFuns = funcs)
crude2.dtm &lt;- DocumentTermMatrix(crude2, control = list(wordLengths = c(3,10))) 
</code></pre>

<p><strong>Here is the bit that does what you want:</strong>    </p>

<p>Now we keep only the terms in the testing data that are present in the training data...</p>

<pre><code># convert to matrices for subsetting
crude1.dtm.mat &lt;- as.matrix(crude1.dtm) # training
crude2.dtm.mat &lt;- as.matrix(crude2.dtm) # testing

# subset testing data by colnames (ie. terms) or training data
xx &lt;- data.frame(crude2.dtm.mat[,intersect(colnames(crude2.dtm.mat),
                                           colnames(crude1.dtm.mat))])
</code></pre>

<p>Finally add to the testing data all the empty columns for terms in the training data that are not in the testing data...</p>

<pre><code># make an empty data frame with the colnames of the training data
yy &lt;- read.table(textConnection(""""), col.names = colnames(crude1.dtm.mat),
                 colClasses = ""integer"")

# add incols of NAs for terms absent in the 
# testing data but present # in the training data
# following SchaunW's suggestion in the comments above
library(plyr)
zz &lt;- rbind.fill(xx, yy)
</code></pre>

<p>So <code>zz</code> is a data frame of the testing documents, but has the same structure as the training documents (ie. same columns, though many of them contain NA, as SchaunW notes).</p>

<p>Is that along the lines of what you want?</p>
",12,12,8921,2013-05-19 01:30:09,https://stackoverflow.com/questions/16630627/how-to-recreate-same-documenttermmatrix-with-new-test-data
Is there any Java based open source framework to find values from a text field based on key string with delimiter?,"<p>Is there any Java based open source framework to find values in a text field based on key string with delimiter? </p>

<p>Example:</p>

<p>Key: username 
Start-Delimiter: ; 
End-Delimiter: ;</p>

<p>Key: on 
Start-Delimiter: ; 
End-Delimiter: ;</p>

<p>Sample Input: A user with username ;surenraju; logged into the system on ;Thu May 2, 2013 2:30pm;</p>

<p>Results: 
username -  surenraju<br>
on - Thu May 2, 2013 2:30pm </p>

<p>Thanks in advance.</p>
","java, jakarta-ee, machine-learning, text-mining","<p>We have solved this problem with Regex. </p>

<p>Sample code:</p>

<pre><code>    ArrayList&lt;String&gt; keys = new ArrayList&lt;String&gt;();
    keys.add(""username"");
    keys.add(""on"");
    String startDelimiter = "":"";
    String endDelimiter = "":"";
    String searchStr = ""A user with username :suren: logged into the system on :22 May 2013 2:30 PM:"";
    for (String key : keys) {
        String pattern = ""(""+key+"")[ ]*?""+startDelimiter+""([^"" +endDelimiter+ ""]+)""+endDelimiter;
        Pattern p = Pattern.compile(pattern);
        Matcher m = p.matcher(searchStr);
        while (m.find()) {
            System.out.println(""Key: "" + m.group(1) + "" Value: ""
                    + m.group(2));
        }
    }
</code></pre>

<p>Tests</p>

<p>I. Key: username Start-Delimiter: ; End-Delimiter: ;</p>

<p>Need to find a value which if followed by key( in this case username ) and value is between start and end delimiters(in this case ;).</p>

<p>II. Key: on Start-Delimiter: ; End-Delimiter: ;</p>

<p>Sample Input:</p>

<p>A user with username ;suren; logged into the system on ;Thu May 2, 2013 2:30pm;</p>

<p>Results: I. username - suren II. on - Thu May 2, 2013 2:30pm</p>
",1,0,108,2013-05-20 11:02:14,https://stackoverflow.com/questions/16648062/is-there-any-java-based-open-source-framework-to-find-values-from-a-text-field-b
"Create a term frequency matrix using 2 columns from a csv file, in R?","<p>I'm new to R. I'm mining data which is present in csv file - summaries of reports in one column, date of report in another column and report's agency in the thrid column. I need to investigate how terms associated with ‘fraud’ have changed over time or vary by agency. I've filtered the rows containing the term 'fraud' and created a new csv file.</p>

<p>How can I create a term freq matrix with years as rows and terms as columns so that I can look for top freq terms and do some clustering?</p>

<p>Basically, I need to create a term frequency matrix of terms against year</p>

<pre><code>Input data: (csv)
**Year**    **Summary** (around 300 words each)    
1945             &lt;text&gt;
1985             &lt;text&gt;
2011             &lt;text&gt;

Desired 0utput : (Term frequency matrix)

       term1     term2    term3  term4 .......
1945     3         5        7       8 .....
1985     1         2        0       7  .....
2011      .            .   .    

Any help would be greatly appreciated.
</code></pre>
","r, csv, text-mining","<p><strong>In the future please provide a minimal working example.</strong>  </p>

<p>This isn't exactly using tm but qdap instead as it fits your data type better:</p>

<pre><code>library(qdap)
#create a fake data set (please do this in the future yourself) 
dat &lt;- data.frame(year=1945:(1945+10), summary=DATA$state) 

##    year                               summary
## 1  1945         Computer is fun. Not too fun.
## 2  1946               No it's not, it's dumb.
## 3  1947                    What should we do?
## 4  1948                  You liar, it stinks!
## 5  1949               I am telling the truth!
## 6  1950                How can we be certain?
## 7  1951                      There is no way.
## 8  1952                       I distrust you.
## 9  1953           What are you talking about?
## 10 1954         Shall we move on?  Good then.
## 11 1955 I'm hungry.  Let's eat.  You already?
</code></pre>

<p>Now to create the word frequency matrix (similar to a term document matrix):</p>

<pre><code>t(with(dat, wfm(summary, year)))

##      about already am are be ... you
## 1945     0       0  0   0  0       0
## 1946     0       0  0   0  0       0
## 1947     0       0  0   0  0       0
## 1948     0       0  0   0  0       1
## 1949     0       0  1   0  0       0
## 1950     0       0  0   0  1       0
## 1951     0       0  0   0  0       0
## 1952     0       0  0   0  0       1
## 1953     1       0  0   1  0       1
## 1954     0       0  0   0  0       0
## 1955     0       1  0   0  0       1
</code></pre>

<p>Or you can create a tru DocumentTermMatrix as of <a href=""http://trinkerrstuff.wordpress.com/2014/02/23/qdap-1-1-0-released-on-cran/"" rel=""nofollow"">qdap version 1.1.0</a>:</p>

<pre><code>with(dat, dtm(summary, year))

## &gt; with(dat, dtm(summary, year))
## A document-term matrix (11 documents, 41 terms)
## 
## Non-/sparse entries: 51/400
## Sparsity           : 89%
## Maximal term length: 8 
## Weighting          : term frequency (tf)
</code></pre>
",4,1,3615,2013-05-21 18:45:14,https://stackoverflow.com/questions/16677292/create-a-term-frequency-matrix-using-2-columns-from-a-csv-file-in-r
findAssocs for multiple terms in R,"<p>In R I used the <code>[tm package][1]</code> for building a term-document matrix from a corpus of documents. </p>

<p>My goal is to extract word-associations from <strong>all</strong> bigrams in the term document matrix and return for each the top three or some. Therefore I'm looking for a variable that holds all row.names from the matrix so the function <code>findAssocs()</code> can do his job.</p>

<p>This is my code so far:</p>

<pre><code>library(tm)
library(RWeka)
txtData &lt;- read.csv(""file.csv"", header = T, sep = "","")
txtCorpus &lt;- Corpus(VectorSource(txtData$text))

...further preprocessing

#Tokenizer for n-grams and passed on to the term-document matrix constructor
BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi &lt;- TermDocumentMatrix(txtCorpus, control = list(tokenize = BigramTokenizer))

#term argument holds two words since the BigramTokenizer extracted all pairs from txtCorpus
findAssocs(txtTdmBi, ""cat shop"", 0.5)
cat cabi  cat scratch  ...
    0.96         0.91
</code></pre>

<p>I tried to define a variable with all the row.names from <code>txtTdmBi</code> and feed it to the <code>findAssocs()</code> function. However, with the following result:</p>

<pre><code>allRows &lt;- c(row.names(txtTdmBi))
findAssocs(txtTdmBi, allRows, 0.5)
Error in which(x[term, ] &gt; corlimit) : subscript out of bounds
In addition: Warning message:
In term == Terms(x) :
  longer object length is not a multiple of shorter object length
</code></pre>

<p>Because extracting associations for a term spent over multiple term-document matrices is already explained <a href=""https://stackoverflow.com/questions/16695866/r-finding-the-top-10-terms-associated-with-the-term-fraud-across-documents-i/16696053#16696053"">here</a>, I guess it would be possible to find the associations for multiple terms in a single term-document matrix. Except how? </p>

<p>I hope someone can clarify me how to solve this. Thanks in advance for any support.</p>
","r, text-mining, term-document-matrix","<p>If I understand correctly, an <code>lapply</code> solution is probably the way to answer your question. This is the same approach as the answer that you link to, but here's a self-contained example that might be closer to your use case:</p>

<p>Load libraries and reproducible data (please include these in your future questions here)</p>

<pre><code>library(tm)
library(RWeka)
data(crude)
</code></pre>

<p>Your bigram tokenizer...</p>

<pre><code>#Tokenizer for n-grams and passed on to the term-document matrix constructor
BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi &lt;- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
</code></pre>

<p>Check that it worked by inspecting a random sample...</p>

<pre><code>inspect(txtTdmBi[1000:1005, 10:15])
A term-document matrix (6 terms, 6 documents)

Non-/sparse entries: 1/35
Sparsity           : 97%
Maximal term length: 18 
Weighting          : term frequency (tf)

                    Docs
Terms                248 273 349 352 353 368
  for their            0   0   0   0   0   0
  for west             0   0   0   0   0   0
  forced it            0   0   0   0   0   0
  forced to            0   0   0   0   0   0
  forces trying        1   0   0   0   0   0
  foreign investment   0   0   0   0   0   0
</code></pre>

<p><strong>Here is the answer to your question:</strong></p>

<p>Now use a <code>lapply</code> function to calculate the associated words for every item in the vector of terms in the term-document matrix. The vector of terms is most simply accessed with <code>txtTdmBi$dimnames$Terms</code>. For example <code>txtTdmBi$dimnames$Terms[[1005]]</code> is ""foreign investment"".</p>

<p>Here I've used <code>llply</code> from the <code>plyr</code> package so we can have a progress bar (comforting for big jobs), but it's basically the same as the base <code>lapply</code> function. </p>

<pre><code>library(plyr)
dat &lt;- llply(txtTdmBi$dimnames$Terms, function(i) findAssocs(txtTdmBi, i, 0.5), .progress = ""text"" )
</code></pre>

<p>The output is a list where each item in the list is a vector of named numbers where the name is the term and the number is the correlation value. For example, to see the terms associated with ""foreign investment"", we can access the list like so:</p>

<pre><code>dat[[1005]]
</code></pre>

<p>and here are the terms associated with that term (I've just pasted in the top few)</p>

<pre><code>168 million              1986 was            1987 early               300 mln                31 pct 
                 1.00                  1.00                  1.00                  1.00                  1.00 
                a bit          a crossroads             a leading           a political          a population 
                 1.00                  1.00                  1.00                  1.00                  1.00 
            a reduced              a series            a slightly            about zero    activity continues 
                 1.00                  1.00                  1.00                  1.00                  1.00 
         advisers are   agricultural sector       agriculture the              all such          also reviews 
                 1.00                  1.00                  1.00                  1.00                  1.00 
         and advisers           and attract           and imports       and liberalised             and steel 
                 1.00                  1.00                  1.00                  1.00                  1.00 
            and trade           and virtual       announced since            appears to           are equally 
                 1.00                  1.00                  1.00                  1.00                  1.00 
     are recommending             areas for              areas of                 as it              as steps 
                 1.00                  1.00                  1.00                  1.00                  1.00 
            asia with          asian member    assesses indonesia           attract new            balance of 
                 1.00                  1.00                  1.00                  1.00                  1.00 
</code></pre>

<p>Is that what you want to do?</p>

<p>Incidentally, if your term-document matrix is very large, you may want to try this version of <code>findAssocs</code>:</p>

<pre><code># u is a term document matrix
# term is your term
# corlimit is a value -1 to 1

findAssocsBig &lt;- function(u, term, corlimit){
  suppressWarnings(x.cor &lt;-  gamlr::corr(t(u[ !u$dimnames$Terms == term, ]),        
                                         as.matrix(t(u[  u$dimnames$Terms == term, ]))  ))  
  x &lt;- sort(round(x.cor[(x.cor[, term] &gt; corlimit), ], 2), decreasing = TRUE)
  return(x)
}
</code></pre>

<p>This can be used like so: </p>

<pre><code>dat1 &lt;- llply(txtTdmBi$dimnames$Terms, function(i) findAssocsBig(txtTdmBi, i, 0.5), .progress = ""text"" )
</code></pre>

<p>The advantage of this is that it uses a different method of converting the TDM to a matrix <code>tm:findAssocs</code>. This different method uses memory more efficiently and so prevents this kind of message: <code>Error: cannot allocate vector of size 1.9 Gb</code> from occurring. </p>

<p>Quick benchmarking shows that both <code>findAssocs</code> functions are about the same speed, so the main difference is in the use of memory:</p>

<pre><code>library(microbenchmark)
microbenchmark(
dat1 &lt;- llply(txtTdmBi$dimnames$Terms, function(i) findAssocsBig(txtTdmBi, i, 0.5)),
dat &lt;- llply(txtTdmBi$dimnames$Terms, function(i) findAssocs(txtTdmBi, i, 0.5)),
times = 10)

Unit: seconds
                                                                                     expr      min       lq   median
 dat1 &lt;- llply(txtTdmBi$dimnames$Terms, function(i) findAssocsBig(txtTdmBi,      i, 0.5)) 10.82369 11.03968 11.25492
     dat &lt;- llply(txtTdmBi$dimnames$Terms, function(i) findAssocs(txtTdmBi,      i, 0.5)) 10.70980 10.85640 11.14156
       uq      max neval
 11.39326 11.89754    10
 11.18877 11.97978    10
</code></pre>
",6,2,8743,2013-05-30 12:21:40,https://stackoverflow.com/questions/16836022/findassocs-for-multiple-terms-in-r
How to set term freq bound to extract a new term doc matrix?,"<p>I have a term doc matrix (16,977 terms, 29,414 documents):</p>
<pre><code>Non-/sparse entries: 355000/499006478
Sparsity           : 100%
Maximal term length: 7 
Weighting          : term frequency (tf)
</code></pre>
<p>For further analysis I got to restrict the term number to 2,425. How can I generate a new term doc matrix by including terms with freq over and above 20 for instance?</p>
<p>Since the matrix is large, traditional method <code>as.matrix</code> cannot be applied.</p>
","r, bigdata, sparse-matrix, text-mining, tm","<p>Something like this might work... Index the DTM as a simple triplet matrix using a function from the slam package, that'll save you from having to convert it to a dense matrix. </p>

<pre><code>library(slam)
library(tm)
data(crude)
dtm1 &lt;- DocumentTermMatrix(crude)


# Find the total occurances of each word in all docs
colTotals &lt;-  col_sums(dtm1)

# keep only  words that occur &gt;20 times in all docs
dtm2 &lt;- dtm1[,which(colTotals &gt; 20)]

&gt; dtm1
A document-term matrix (20 documents, 1266 terms)

Non-/sparse entries: 2255/23065
Sparsity           : 91%
Maximal term length: 17 
Weighting          : term frequency (tf)

&gt; dtm2
A document-term matrix (20 documents, 12 terms)

Non-/sparse entries: 174/66
Sparsity           : 28%
Maximal term length: 6 
Weighting          : term frequency (tf)
</code></pre>

<p>Does that work on your data and answer your question?</p>
",6,3,4152,2013-06-02 06:35:59,https://stackoverflow.com/questions/16880411/how-to-set-term-freq-bound-to-extract-a-new-term-doc-matrix
How to select stop words using tf-idf? (non english corpus),"<p>I have managed to evaluate the <a href=""http://en.wikipedia.org/wiki/Tf-idf"" rel=""noreferrer"">tf-idf function</a> for a given corpus. How can I find the stopwords and the best words for each document? I understand that a low tf-idf for a given word and document means that it is not a good word for selecting that document.</p>
","information-retrieval, text-mining, stop-words, tf-idf","<p>Stop-words are those words that appear very commonly across the documents, therefore loosing their representativeness. The best way to observe this is to measure the number of documents a term appears in and filter those that appear in more than 50% of them, or the top 500 or some type of threshold that you will have to tune.</p>

<p>The best (as in more representative) terms in a document are those with higher tf-idf because those terms are common in the document, while being rare in the collection.</p>

<p>As a quick note, as @Kevin pointed out, very common terms in the collection (i.e., stop-words) produce very low tf-idf anyway. However, they will change some computations and this would be wrong if you assume they are pure noise (which might not be true depending on the task). In addition, if they are included your algorithm would be slightly slower.</p>

<p>edit:
As @FelipeHammel says, you can directly use the IDF (remember to invert the order) as a measure which is (inversely) proportional to df. This is completely equivalent for ranking purposes, and therefore to select the top ""k"" terms. However, it is not possible to use it to select based on ratios (e.g., words that appear in more than 50% of the documents), although a simple thresholding will fix that (i.e., selecting terms with idf lower than a specific value). In general, a fix number of terms is used.</p>

<p>I hope this helps.</p>
",16,13,20978,2013-06-04 21:08:06,https://stackoverflow.com/questions/16927494/how-to-select-stop-words-using-tf-idf-non-english-corpus
How to convert a termDocumentMatrix which I have got from text mining in R into excel or CSV file?,"<p>To be more more specific. Lets say I have a character vector ""names"" with the following elements:</p>

<pre><code>Names[1]&lt;-""aaron, matt, patrick"",
Names[2]&lt;-""jiah, ron, melissa, john, patrick""
</code></pre>

<p>and so on......I have 22956 elements like this. I want to separate all the names and assign them a separate column in excel.
How do I do this? It requires text mining. But I am not sure how to do this. </p>

<p>Thank you.</p>
","r, machine-learning, text-mining","<p>I assume you have a list of strings elements separated by a comma, with different number of elements.</p>

<pre><code>Names &lt;- c(""aaron, matt, patrick"",
           ""jiah, ron, melissa, john, patrick"")

## get max number of elements
mm &lt;-  mm &lt;- max(unlist(lapply(strsplit(Names,','),length)))
## set all rows the same length
lapply(strsplit(Names,','),function(x) {length(x) &lt;- mm;x})
## create a data frame with the data welle formatted
res &lt;- do.call(rbind,lapply(strsplit(Names,','),function(x) {length(x) &lt;- mm;x}))
## save the file
write.csv(res,'output.csv')
</code></pre>

<p>I think also you can use <code>rbind.fill</code> from plyr package, but you have to coerce each row to a <code>data.frame</code>( certain cost).</p>
",1,0,3635,2013-06-07 10:11:31,https://stackoverflow.com/questions/16981599/how-to-convert-a-termdocumentmatrix-which-i-have-got-from-text-mining-in-r-into
An issue with vectorising Gsub,"<p><strong>Aim:</strong>
I am a newcomer to <code>R</code>, but I am trying to familiarise myself with programming in <code>R</code>. In a current task, I wanted to replace a number of words occurring in a <code>corpus</code> whilst keeping in tact the structure of the <code>corpus</code>.</p>

<p><code>Gsub</code> did not allow vectors to be used for patterns and corresponding replacements, so I decided to write a modified <code>Gsub</code> function. (<em>I am aware of the <code>Gsubfn</code> function, but I would like to develop some programming skills too.</em>)</p>

<p><strong>Data Generation</strong></p>

<pre><code>a&lt;- c(""this is a testOne"",""this is testTwo"",""this is testThree"",""this is testFour"")
corpus&lt;- Corpus(VectorSource(a))
pattern1&lt;- c(""testOne"",""testTwo"",""testThree"")
replacement1&lt;- c(""gameOne"",""gameTwo"",""gameThree"")
</code></pre>

<p><strong>Modified Gsub</strong></p>

<pre><code>gsub2&lt;- function(myPattern, myReplacement, myCorpus, fixed=FALSE,ignore.case=FALSE){
for (i in 1:length(myCorpus)){
    for (j in 1:length(myPattern)){
    myCorpus[[i]]&lt;- gsub(myPattern[j],myReplacement[j], myCorpus[[i]], fixed=TRUE)
    }
}
}
</code></pre>

<p><strong>Code Execution</strong></p>

<pre><code>gsub2(pattern1,replacement1,corpus,fixed=TRUE)
</code></pre>

<p>However, no change is produced in the actual corpus. I think it is because all the changes are being made within the function, and thus, are limited to within the function. And then I tried returning the corpus but <code>R</code> fails to recognise the corpus object.</p>

<p>Could someone point me in the right direction, please?
Thanks.</p>
","r, function, for-loop, text-mining, gsub","<p>What if you, as you already suggested, return the <code>corpus</code> object?</p>

<pre><code>gsub2&lt;- function(myPattern, myReplacement, myCorpus, fixed=FALSE,ignore.case=FALSE){
  for (i in 1:length(myCorpus)){
    for (j in 1:length(myPattern)){
      myCorpus[[i]]&lt;- gsub(myPattern[j],myReplacement[j], myCorpus[[i]], fixed=TRUE)
    }
  }
  return(myCorpus)
}
</code></pre>

<p>and then</p>

<pre><code>a &lt;- gsub2(pattern1,replacement1,corpus,fixed=TRUE)

 &gt; class(a)
[1] ""VCorpus"" ""Corpus""  ""list""   


&gt; for (i in 1:length(a)){print(a[[i]])}
this is a gameOne
this is gameTwo
this is gameThree
this is testFour
</code></pre>

<p>Is this not what you want?</p>
",2,4,393,2013-06-11 10:31:54,https://stackoverflow.com/questions/17041981/an-issue-with-vectorising-gsub
Output of text2people from http://www.datasciencetoolkit.org/,"<p>I just installed the <a href=""http://www.datasciencetoolkit.org/"" rel=""nofollow"">data toolkit</a> and run text2people from the command line on files and a couple of web pages.</p>

<p>As output, I get something like</p>

<pre><code>Peter Williams,Peter,Williams,,m,151431,151445,stdin
David Philippaerts,David,Philippaerts,,m,152500,152518,stdin
Da Ryse,Da,Ryse,,m,158551,158558,stdin
</code></pre>

<p>I can guess that the first fields are name, surname and sex, but I couldn't understand how to get the other info shown in the <a href=""http://www.datasciencetoolkit.org/developerdocs"" rel=""nofollow"">website</a>, like ethnicity. Should I use it via python/javascript etc.?
Help and documentation are really minimal ...</p>
",text-mining,"<p>Download and extract the python_tools.zip. If you installed the libraries to your OS, you can create your program where you want, otherwise you can just write your test program to the directory where dstk.py is located.</p>

<p>Here's a simple test program. It has a list of people whose information it fetches from the service. It will then go through their ethnicity information and prints their most likely ethnicity along with its percentage.</p>

<pre><code>import dstk
from pprint import pprint

dstk = dstk.DSTK()

# List of people you want to search for
people_names = [""Samuel L. Jackson"", ""Michelle Yeoh"", ""Danny Trejo"", ""Vanessa Minnillo"",""Naomi Campbell"",""Chuck Norris""]

# Query information for each person in the list
people = dstk.text2people("","".join(people_names))

# Print the structure of the received information
#print people

# Prints the structure of the people in more readable way
#pprint(people)

# Print name and ethnicity information of person
for person in people:

    if person['ethnicity'] == None:
        print (person['first_name'] + "" "" + person['surnames']).ljust(26), ""Unknown ethnicity""
    else:
        ethnics = ['percentage_american_indian_or_alaska_native','percentage_asian_or_pacific_islander','percentage_black','percentage_hispanic','percentage_two_or_more','percentage_white']
        highest_probability = 0
        highest_index = 0

        # Find highest percentage
        for eth_index in ethnics:
            if person['ethnicity'][eth_index] &gt; highest_probability:
                highest_probability = person['ethnicity'][eth_index]
                highest_index = eth_index
        print (person['first_name'] + "" "" + person['surnames']).ljust(20), str(person['ethnicity'][highest_index]).ljust(5), highest_index
</code></pre>

<p>The code above will print the following:</p>

<pre><code>Samuel L Jackson     53.02 percentage_black
Michelle Yeoh        87.74 percentage_asian_or_pacific_islander
Danny Trejo          94.15 percentage_hispanic
Vanessa Minnillo           Unknown ethnicity
Naomi Campbell       76.47 percentage_white
Chuck Norris         82.01 percentage_white
</code></pre>

<p>You can see the names of the variables by printing the structure you receive from the server (<code>pprint(people)</code>), and the names are quite obvious.</p>

<p>I had a hard time finding anyone who would be counted as multiraced or american indian. The database seems to insist they're white.</p>
",1,1,131,2013-06-11 12:20:16,https://stackoverflow.com/questions/17043949/output-of-text2people-from-http-www-datasciencetoolkit-org
What is the difference between Information Extraction and Text Mining?,"<p>It may be looking easy. But I am confused. </p>

<p>What is the difference between Text Mining and Information Extraction ? </p>
","nlp, information-retrieval, text-mining, information-extraction","<p><strong>Information extraction</strong> </p>

<p>(IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction.</p>

<p><strong>Text Mining</strong> </p>

<p>is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on metadata or on full-text indexing.</p>

<p>Text mining is vast area as compared to information retrieval. Typical text mining tasks include document classification, document clustering, building ontology, sentiment analysis, document summarization, Information extraction etc. 
Where as information retrieval typically deals with crawling, parsing and indexing document, retrieving documents.</p>

<p><a href=""https://cs.stackexchange.com/questions/7181/relation-and-difference-between-information-retrieval-and-information-extraction"">Source</a></p>
",6,11,8521,2013-06-22 06:10:04,https://stackoverflow.com/questions/17247874/what-is-the-difference-between-information-extraction-and-text-mining
Counting words in a single document from corpus in R and putting it in dataframe,"<p>I have got text documents, in each document I have text featuring tv series spoilers. Each of the documents is a different series. I want to compare the most used words of each series, I was thinking I could plot them using ggplot, and have 'Series 1 Terms that occur at least x times' on one axis and ' 'Series 2 Terms that occur at least x times' on another. I expect what I need is a dataframe with 3 columns 'Terms', 'Series x', 'Series Y'. With series x and y having the number of times that word occurs.</p>

<p>I have tried multiple ways to do this but failed. The closest I have got is I can read the corpus and create a dataframe with all the terms in one column like so:</p>

<pre><code>library(""tm"")

corpus &lt;-Corpus(DirSource(""series""))
corpus.p &lt;-tm_map(corpus, removeWords, stopwords(""english""))  #removes stopwords
corpus.p &lt;-tm_map(corpus.p, stripWhitespace)  #removes stopwords
corpus.p &lt;-tm_map(corpus.p, tolower)  
corpus.p &lt;-tm_map(corpus.p, removeNumbers)
corpus.p &lt;-tm_map(corpus.p, removePunctuation)
dtm &lt;-DocumentTermMatrix(corpus.p)
docTermMatrix &lt;- inspect(dtm)
termCountFrame &lt;- data.frame(Term = colnames(docTermMatrix))
</code></pre>

<p>I then know I could add a column adding up the words like this:</p>

<pre><code>termCountFrame$seriesX &lt;- colSums(docTermMatrix)
</code></pre>

<p>but that would add occurrences from both of the documents, when I only want one.</p>

<p>So my questions are:</p>

<p>1) Is it possible to use colSums on a single doc, if not is there another way to turn the doctermmatrix into a dataframe with term counts for each document</p>

<p>2) Does anybody know how I can limit this so I get the most used terms in each document</p>
","r, dataframe, text-mining, corpus","<p>If your data are in a Document Term Matrix, you'd use <code>tm::findFreqTerms</code> to get the most used terms in a document. Here's a reproducible example:</p>

<pre><code>require(tm)
data(crude)
dtm &lt;- DocumentTermMatrix(crude)
dtm
A document-term matrix (20 documents, 1266 terms)

Non-/sparse entries: 2255/23065
Sparsity           : 91%
Maximal term length: 17 
Weighting          : term frequency (tf)

# find most frequent terms in all 20 docs
findFreqTerms(dtm, 2, 100)

# find the doc names
dtm$dimnames$Docs
 [1] ""127"" ""144"" ""191"" ""194"" ""211"" ""236"" ""237"" ""242"" ""246"" ""248"" ""273"" ""349"" ""352"" ""353"" ""368"" ""489"" ""502""
[18] ""543"" ""704"" ""708""

# do freq words on one doc
findFreqTerms(dtm[dtm$dimnames$Docs == ""127""], 2, 100)
 [1] ""crude""     ""cut""       ""diamond""   ""dlrs""      ""for""       ""its""       ""oil""       ""price""    
 [9] ""prices""    ""reduction"" ""said.""     ""that""      ""the""       ""today""     ""weak""
</code></pre>

<p><strong>Here's how you'd find the most frequent words for each doc in the dtm, one document at a time:</strong></p>

<pre><code># find freq words for each doc, one by one
list_freqs &lt;- lapply(dtm$dimnames$Docs, 
              function(i) findFreqTerms(dtm[dtm$dimnames$Docs == i], 2, 100))


list_freqs
[[1]]
 [1] ""crude""     ""cut""       ""diamond""   ""dlrs""      ""for""       ""its""       ""oil""       ""price""    
 [9] ""prices""    ""reduction"" ""said.""     ""that""      ""the""       ""today""     ""weak""     

[[2]]
 [2] ""\""opec""       ""\""the""        ""15.8""         ""ability""      ""above""        ""address""      ""agreement""   
 [8] ""analysts""     ""and""          ""before""       ""bpd""          ""but""          ""buyers""       ""current""     
[15] ""demand""       ""emergency""    ""energy""       ""for""          ""has""          ""have""         ""higher""      
[22] ""hold""         ""industry""     ""its""          ""keep""         ""market""       ""may""          ""meet""        
[29] ""meeting""      ""mizrahi""      ""mln""          ""must""         ""next""         ""not""          ""now""         
[36] ""oil""          ""opec""         ""organization"" ""prices""       ""problem""      ""production""   ""said""        
[43] ""said.""        ""set""          ""that""         ""the""          ""their""        ""they""         ""this""        
[50] ""through""      ""will""        

[[3]]
[3] ""canada""   ""canadian"" ""crude""    ""for""      ""oil""      ""price""    ""texaco""   ""the""     

[[4]]
[4] ""bbl.""    ""crude""   ""dlrs""    ""for""     ""price""   ""reduced"" ""texas""   ""the""     ""west""   

[[5]]
 [5] ""and""        ""discounted"" ""estimates""  ""for""        ""mln""        ""net""        ""pct""        ""present""   
 [9] ""reserves""   ""revenues""   ""said""       ""study""      ""that""       ""the""        ""trust""      ""value""     

[[6]]
 [6] ""ability""       ""above""         ""ali""           ""and""           ""are""           ""barrel.""      
 [7] ""because""       ""below""         ""bpd""           ""bpd.""          ""but""           ""daily""        
[13] ""difficulties""  ""dlrs""          ""dollars""       ""expected""      ""for""           ""had""          
[19] ""has""           ""international"" ""its""           ""kuwait""        ""last""          ""local""        
[25] ""march""         ""markets""       ""meeting""       ""minister""      ""mln""           ""month""        
[31] ""official""      ""oil""           ""opec""          ""opec\""s""       ""prices""        ""producing""    
[37] ""pumping""       ""qatar,""        ""quota""         ""referring""     ""said""          ""said.""        
[43] ""sheikh""        ""such""          ""than""          ""that""          ""the""           ""their""        
[49] ""they""          ""this""          ""was""           ""were""          ""which""         ""will""         

[[7]]
 [7] ""\""this""        ""and""           ""appears""       ""are""           ""areas""         ""bank""         
 [7] ""bankers""       ""been""          ""but""           ""crossroads""    ""crucial""       ""economic""     
[13] ""economy""       ""embassy""       ""fall""          ""for""           ""general""       ""government""   
[19] ""growth""        ""has""           ""have""          ""indonesia\""s""  ""indonesia,""    ""international""
[25] ""its""           ""last""          ""measures""      ""nearing""       ""new""           ""oil""          
[31] ""over""          ""rate""          ""reduced""       ""report""        ""say""           ""says""         
[37] ""says.""         ""sector""        ""since""         ""the""           ""u.s.""          ""was""          
[43] ""which""         ""with""          ""world""        

[[8]]
 [8] ""after""      ""and""        ""deposits""   ""had""        ""oil""        ""opec""       ""pct""        ""quotes""    
 [9] ""riyal""      ""said""       ""the""        ""were""       ""yesterday.""

[[9]]
 [9] ""1985/86""     ""1986/87""     ""1987/88""     ""abdul-aziz""  ""about""       ""and""         ""been""       
 [8] ""billion""     ""budget""      ""deficit""     ""expenditure"" ""fiscal""      ""for""         ""government"" 
[15] ""had""         ""its""         ""last""        ""limit""       ""oil""         ""projected""   ""public""     
[22] ""qatar,""      ""revenue""     ""riyals""      ""riyals.""     ""said""        ""sheikh""      ""shortfall""  
[29] ""that""        ""the""         ""was""         ""would""       ""year""        ""year's""     

[[10]]
 [10] ""15.8""      ""about""     ""above""     ""accord""    ""agency""    ""ali""       ""among""     ""and""      
 [9] ""arabia""    ""are""       ""dlrs""      ""for""       ""free""      ""its""       ""kuwait""    ""market""   
[17] ""market,""   ""minister,"" ""mln""       ""nazer""     ""oil""       ""opec""      ""prices""    ""producing""
[25] ""quoted""    ""recent""    ""said""      ""said.""     ""saudi""     ""sheikh""    ""spa""       ""stick""    
[33] ""that""      ""the""       ""they""      ""under""     ""was""       ""which""     ""with""     

[[11]]
 [11] ""1.2""        ""and""        ""appeared""   ""arabia's""   ""average""    ""barrel.""    ""because""    ""below""     
 [9] ""bpd""        ""but""        ""corp""       ""crude""      ""december""   ""dlrs""       ""export""     ""exports""   
[17] ""february""   ""fell""       ""for""        ""four""       ""from""       ""gulf""       ""january""    ""january,""  
[25] ""last""       ""mln""        ""month""      ""month,""     ""neutral""    ""official""   ""oil""        ""opec""      
[33] ""output""     ""prices""     ""production"" ""refinery""   ""said""       ""said.""      ""saudi""      ""sell""      
[41] ""sources""    ""than""       ""the""        ""they""       ""throughput"" ""week""       ""yanbu""      ""zone""      

[[12]]
 [12] ""and""       ""arab""      ""crude""     ""emirates""  ""gulf""      ""ministers"" ""official""  ""oil""      
 [9] ""states""    ""the""       ""wam""      

[[13]]
 [13] ""accord"" ""agency"" ""and""    ""arabia"" ""its""    ""nazer""  ""oil""    ""opec""   ""prices"" ""saudi""  ""the""   
[12] ""under"" 

[[14]]
 [14] ""crude""   ""daily""   ""for""     ""its""     ""oil""     ""opec""    ""pumping"" ""that""    ""the""     ""was""    

[[15]]
 [15] ""after""   ""closed""  ""new""     ""nuclear"" ""oil""     ""plant""   ""port""    ""power""   ""said""    ""ship""   
[11] ""the""     ""was""     ""when""   

[[16]]
 [16] ""about""       ""and""         ""development"" ""exploration"" ""for""         ""from""        ""help""       
 [8] ""its""         ""mln""         ""oil""         ""one""         ""present""     ""prices""      ""research""   
[15] ""reserve""     ""said""        ""strategic""   ""the""         ""u.s.""        ""with""        ""would""      

[[17]]
 [17] ""about""       ""and""         ""benefits""    ""development"" ""exploration"" ""for""         ""from""       
 [8] ""group""       ""help""        ""its""         ""mln""         ""oil""         ""one""         ""policy""     
[15] ""present""     ""prices""      ""protect""     ""research""    ""reserve""     ""said""        ""strategic""  
[22] ""study""       ""such""        ""the""         ""u.s.""        ""with""        ""would""      

[[18]]
 [18] ""1.50""    ""company"" ""crude""   ""dlrs""    ""for""     ""its""     ""lowered"" ""oil""     ""posted""  ""prices"" 
[11] ""said""    ""said.""   ""the""     ""union""   ""west""   

[[19]]
 [19] ""according""    ""and""          ""april""        ""before""       ""can""          ""change""       ""efp""         
 [8] ""energy""       ""entering""     ""exchange""     ""for""          ""futures""      ""has""          ""hold""        
[15] ""increase""     ""into""         ""mckiernan""    ""new""          ""not""          ""nymex""        ""oil""         
[22] ""one""          ""position""     ""prices""       ""rule""         ""said""         ""spokeswoman."" ""that""        
[29] ""the""          ""traders""      ""transaction""  ""when""         ""will""        

[[20]]
 [20] ""1986,""        ""1987""         ""billion""      ""cubic""        ""fiscales""     ""january""      ""mln""         
 [8] ""pct""          ""petroliferos"" ""yacimientos""  
</code></pre>

<p>If you want this output in a dataframe, you can do this:</p>

<pre><code># from here http://stackoverflow.com/a/7196565/1036500
L &lt;- list_freqs
cfun &lt;- function(L) {
  pad.na &lt;- function(x,len) {
    c(x,rep(NA,len-length(x)))
  }
  maxlen &lt;- max(sapply(L,length))
  do.call(data.frame,lapply(L,pad.na,len=maxlen))
}
# make dataframe of words (but probably you want words as rownames and cells with counts?)
tab_freqa &lt;- cfun(L)
</code></pre>

<p>But if you want to plot 'doc 1 high freq terms vs doc 2 high freq terms', then we'll need a different approach...</p>

<pre><code># convert dtm to matrix
mat &lt;- as.matrix(dtm)

# make data frame similar to ""3 columns 'Terms', 
# 'Series x', 'Series Y'. With series x and y 
# having the number of times that word occurs""
cb &lt;- data.frame(doc1 = mat['127',], doc2 = mat['144',])

# keep only words that are in at least one doc
cb &lt;- cb[rowSums(cb)  &gt; 0, ]

# plot
require(ggplot2)
ggplot(cb, aes(doc1, doc2)) +
  geom_text(label = rownames(cb), 
           position=position_jitter())
</code></pre>

<p>Or perhaps slightly more efficiently, we can make one big dataframe of all the docs and make plots from that:</p>

<pre><code># this is the typical method to turn a 
# dtm into a df...
df &lt;- as.data.frame(as.matrix(dtm))
# and transpose for plotting
df &lt;- data.frame(t(df))
# plot
require(ggplot2)
ggplot(df, aes(X127, X144)) +
  geom_text(label = rownames(df), 
           position=position_jitter())
</code></pre>

<p>After you remove stopwords this will look better, but this is a good proof of concept. Is that what you were after?</p>

<p><img src=""https://i.sstatic.net/msc1X.png"" alt=""enter image description here""></p>
",11,6,13408,2013-06-25 10:23:03,https://stackoverflow.com/questions/17294824/counting-words-in-a-single-document-from-corpus-in-r-and-putting-it-in-dataframe
Obtaining data from PubMed using python,"<p>I have a list of PubMed entries along with the PubMed ID's. I would like to create a python script or use python which accepts a PubMed id number as an input and then fetches the abstract from the PubMed website. </p>

<p>So far I have come across NCBI Eutilities and the importurl library in Python but I don't know how I should go about writing a template.</p>

<p>Any pointers will be appreciated.</p>

<p>Thank you,</p>
","python, text-mining","<p>Wow, I was working on a similar project myself just a week or so ago!  </p>

<p><strong>Edit:</strong> I recently updated the code to take advantage of <a href=""https://www.crummy.com/software/BeautifulSoup/"" rel=""noreferrer"" title=""BeautifulSoup"">BeautifulSoup</a>.  I have my own virtualenv for it, but you can install it with pip.</p>

<p>Basically, my program takes a pubmed ID, a DOI, or a text file of lines of pubmed IDs and/or DOIs, and grabs information about the article.  It can easily be tweaked for your own needs to obtain the abstract, but here's my code:</p>

<pre><code>import re
import sys
import traceback
from bs4 import BeautifulSoup
import requests

class PubMedObject(object):
    soup = None
    url = None

    # pmid is a PubMed ID
    # url is the url of the PubMed web page
    # search_term is the string used in the search box on the PubMed website
    def __init__(self, pmid=None, url='', search_term=''):
        if pmid:
            pmid = pmid.strip()
            url = ""http://www.ncbi.nlm.nih.gov/pubmed/%s"" % pmid
        if search_term:
            url = ""http://www.ncbi.nlm.nih.gov/pubmed/?term=%s"" % search_term
        page = requests.get(url).text
        self.soup = BeautifulSoup(page, ""html.parser"")

        # set the url to be the fixed one with the PubMedID instead of the search_term
        if search_term:
            try:
                url = ""http://www.ncbi.nlm.nih.gov/pubmed/%s"" % self.soup.find(""dl"",class_=""rprtid"").find(""dd"").text
            except AttributeError as e:  # NoneType has no find method
                print(""Error on search_term=%s"" % search_term)
        self.url = url

    def get_title(self):
        return self.soup.find(class_=""abstract"").find(""h1"").text

    #auths is the string that has the list of authors to return
    def get_authors(self):
        result = []
        author_list = [a.text for a in self.soup.find(class_=""auths"").findAll(""a"")]
        for author in author_list:
            lname, remainder = author.rsplit(' ', 1)
            #add periods after each letter in the first name
            fname = ""."".join(remainder) + "".""
            result.append(lname + ', ' + fname)

        return ', '.join(result)

    def get_citation(self):
        return self.soup.find(class_=""cit"").text

    def get_external_url(self):
        url = None
        doi_string = self.soup.find(text=re.compile(""doi:""))
        if doi_string:
            doi = doi_string.split(""doi:"")[-1].strip().split("" "")[0][:-1]
            if doi:
                url = ""http://dx.doi.org/%s"" % doi
        else:
            doi_string = self.soup.find(class_=""portlet"")
            if doi_string:
                doi_string = doi_string.find(""a"")['href']
                if doi_string:
                    return doi_string

        return url or self.url

    def render(self):
        template_text = ''
        with open('template.html','r') as template_file:
            template_text = template_file.read()

        try:
            template_text = template_text.replace(""{{ external_url }}"", self.get_external_url())
            template_text = template_text.replace(""{{ citation }}"", self.get_citation())
            template_text = template_text.replace(""{{ title }}"", self.get_title())
            template_text = template_text.replace(""{{ authors }}"", self.get_authors())
            template_text = template_text.replace(""{{ error }}"", '')
        except AttributeError as e:
            template_text = template_text.replace(""{{ external_url }}"", '')
            template_text = template_text.replace(""{{ citation }}"", '')
            template_text = template_text.replace(""{{ title }}"", '')
            template_text = template_text.replace(""{{ authors }}"", '')
            template_text = template_text.replace(""{{ error }}"", '&lt;!-- Error --&gt;')

        return template_text.encode('utf8')

def start_table(f):
    f.write('\t\t\t\t\t\t\t\t\t&lt;div class=""resourcesTable""&gt;\n');
    f.write('\t\t\t\t\t\t\t\t\t\t&lt;table border=""0"" cellspacing=""0"" cellpadding=""0""&gt;\n');

def end_table(f):
    f.write('\t\t\t\t\t\t\t\t\t\t&lt;/table&gt;\n');
    f.write('\t\t\t\t\t\t\t\t\t&lt;/div&gt;\n');

def start_accordion(f):
    f.write('\t\t\t\t\t\t\t\t\t&lt;div class=""accordion""&gt;\n');

def end_accordion(f):
    f.write('\t\t\t\t\t\t\t\t\t&lt;/div&gt;\n');

def main(args):
    try:
        # program's main code here
        print(""Parsing pmids.txt..."")
        with open('result.html', 'w') as sum_file:
            sum_file.write('&lt;!--\n')
        with open('pmids.txt','r') as pmid_file:
        with open('result.html','a') as sum_file:
        for pmid in pmid_file:
            sum_file.write(pmid)
        sum_file.write('\n--&gt;\n')
        with open('pmids.txt','r') as pmid_file:
            h3 = False
            h4 = False
            table_mode = False
            accordion_mode = False
            with open('result.html', 'a') as sum_file:
                for pmid in pmid_file:
                    if pmid[:4] == ""####"":
                        if h3 and not accordion_mode:
                            start_accordion(sum_file)
                            accordion_mode = True
                        sum_file.write('\t\t\t\t\t\t\t\t\t&lt;h4&gt;&lt;a href=""#""&gt;%s&lt;/a&gt;&lt;/h4&gt;\n' % pmid[4:].strip())
                        h4 = True
                    elif pmid[:3] == ""###"":
                        if h4:
                            if table_mode:
                                end_table(sum_file)
                                table_mode = False
                            end_accordion(sum_file)
                            h4 = False
                            accordion_mode = False
                        elif h3:
                            end_table(sum_file)
                            table_mode = False
                        sum_file.write('\t\t\t\t\t\t\t\t&lt;h3&gt;&lt;a href=""#""&gt;%s&lt;/a&gt;&lt;/h3&gt;\n' % pmid[3:].strip())
                        h3 = True                        
                    elif pmid.strip():
                        if (h3 or h4) and not table_mode:
                            start_table(sum_file)
                            table_mode = True
                        if pmid[:4] == ""http"":
                            if pmid[:18] == ""http://dx.doi.org/"":
                                sum_file.write(PubMedObject(search_term=pmid[18:]).render())
                            else:
                                print(""url=%s"" % pmid)
                                p = PubMedObject(url=pmid).render()
                                sum_file.write(p)
                                print(p)
                        elif pmid.isdigit():
                            sum_file.write(PubMedObject(pmid).render())
                        else:
                            sum_file.write(PubMedObject(search_term=pmid).render())
                if h3:
                    if h4:
                        end_table(sum_file)
                        end_accordion(sum_file)
                    else:
                        end_table(sum_file)
            pmid_file.close()
        print(""Done!"")

    except BaseException as e:
        print traceback.format_exc()
        print ""Error: %s %s"" % (sys.exc_info()[0], e.args)
        return 1
    except:
        # error handling code here
        print ""Error: %s"" % sys.exc_info()[0]
        return 1  # exit on error
    else:
        raw_input(""Press enter to exit."")
        return 0  # exit errorlessly

if __name__ == '__main__':
    sys.exit(main(sys.argv))
</code></pre>

<p>It now returns a HTML file based on the information it downloaded.  Here is the template.txt:</p>

<pre><code>&lt;tr&gt;{{ error }}
    &lt;td valign=""top"" class=""resourcesICO""&gt;&lt;a href=""{{ external_url }}"" target=""_blank""&gt;&lt;img src=""/image/ico_sitelink.gif"" width=""24"" height=""24"" /&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=""{{ external_url }}""&gt;{{ title }}&lt;/a&gt;&lt;br /&gt;
    {{ authors }}&lt;br /&gt;
    &lt;em&gt;{{ citation }}&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
</code></pre>

<p>When you run it, the program will ask you for the DOI or the Pubmed ID.  If you do not provide one, it will read pmids.txt.Feel free to use the code as you see fit.</p>
",9,17,30825,2013-07-01 16:17:27,https://stackoverflow.com/questions/17409107/obtaining-data-from-pubmed-using-python
python symmetric word matrix using nltk,"<p>I'm trying to create a symmetric word matrix from a text document. </p>

<p>For example: 
text = ""Barbara is good. Barbara is friends with Benny. Benny is bad."" </p>

<p>I have tokenized the text document using nltk. Now I want to count how many times other words appear in the same sentence. From the text above, I want to create the matrix below: </p>

<pre><code>        Barbara good    friends Benny   bad
Barbara 2   1   1   1   0
good    1   1   0   0   0
friends 1   0   1   1   0
Benny   1   0   1   2   1
bad     0   0   1   1   1
</code></pre>

<p>Note the diagonals are the frequency of the word. Since Barbara appears with Barbara in a sentence as often as there are Barbaras. I hope to not overcount, but this is not a big issue if the code becomes too complicated. </p>
","python, nltk, text-mining","<p>First we tokenize the text, iterate through each sentence, and iterate through all pairwise combinations of the words in each sentence, and store out counts in a nested <code>dict</code>:</p>

<pre><code>from nltk.tokenize import word_tokenize, sent_tokenize
from collections import defaultdict
import numpy as np
text = ""Barbara is good. Barbara is friends with Benny. Benny is bad.""

sparse_matrix = defaultdict(lambda: defaultdict(lambda: 0))

for sent in sent_tokenize(text):
    words = word_tokenize(sent)
    for word1 in words:
        for word2 in words:
            sparse_matrix[word1][word2]+=1

print sparse_matrix
&gt;&gt; defaultdict(&lt;function &lt;lambda&gt; at 0x7f46bc3587d0&gt;, {
'good': defaultdict(&lt;function &lt;lambda&gt; at 0x3504320&gt;, 
    {'is': 1, 'good': 1, 'Barbara': 1, '.': 1}), 
'friends': defaultdict(&lt;function &lt;lambda&gt; at 0x3504410&gt;, 
    {'friends': 1, 'is': 1, 'Benny': 1, '.': 1, 'Barbara': 1, 'with': 1}), etc..
</code></pre>

<p>This is essentially like a matrix, in that we can index <code>sparse_matrix['good']['Barbara']</code> and get the number <code>1</code>, and index <code>sparse_matrix['bad']['Barbara']</code> and get <code>0</code>, but we actually aren't storing counts for any words that never co-occured, the <code>0</code> is just generated by the <code>defaultdict</code> only when you ask for it. This can really save a lot of memory when doing this stuff. If we need a dense matrix for some type of linear algebra or other computational reason, we can get it like this:</p>

<pre><code>lexicon_size=len(sparse_matrix)
def mod_hash(x, m):
    return hash(x) % m
dense_matrix = np.zeros((lexicon_size, lexicon_size))

for k in sparse_matrix.iterkeys():
    for k2 in sparse_matrix[k].iterkeys():
        dense_matrix[mod_hash(k, lexicon_size)][mod_hash(k2, lexicon_size)] = \
            sparse_matrix[k][k2]

print dense_matrix
&gt;&gt;
[[ 0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  1.  1.  1.  1.  0.  1.]
 [ 0.  0.  1.  1.  1.  0.  0.  1.]
 [ 0.  0.  1.  1.  1.  1.  0.  1.]
 [ 0.  0.  1.  0.  1.  2.  0.  2.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  1.  1.  1.  2.  0.  3.]]
</code></pre>

<p>I would recommend looking at <a href=""http://docs.scipy.org/doc/scipy/reference/sparse.html"" rel=""noreferrer"">http://docs.scipy.org/doc/scipy/reference/sparse.html</a> for other ways of dealing with matrix sparsity.</p>
",8,6,4109,2013-07-03 21:53:26,https://stackoverflow.com/questions/17458751/python-symmetric-word-matrix-using-nltk
Methods for extracting locations from text?,"<p>What are the recommended methods for extracting locations from free text? </p>

<p>What I can think of is to use regex rules like ""words ... in location"". But are there better approaches than this?</p>

<p>Also I can think of having a lookup hash table table with names for countries and cities and then compare every extracted token from the text to that of the hash table.</p>

<p>Does anybody know of better approaches?</p>

<p>Edit: I'm trying to extract locations from tweets text. So the issue of high number of tweets might also affect my choice for a method.</p>
","nlp, text-mining, information-extraction, named-entity-recognition, named-entity-extraction","<p>All rule-based approaches will fail (if your text is really ""free""). That includes regex, context-free grammars, any kind of lookup... Believe me, I've been there before :-)</p>

<p>This problem is called <strong>Named Entity Recognition</strong>. Location is one of the 3 most studied classes (with Person and Organization). Stanford NLP has an open source Java implementation that is extremely powerful: <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"">http://nlp.stanford.edu/software/CRF-NER.shtml</a></p>

<p>You can easily find implementations in other programming languages.</p>
",11,10,6635,2013-07-20 12:58:47,https://stackoverflow.com/questions/17762516/methods-for-extracting-locations-from-text
R text mining documents from CSV file (one row per doc),"<p>I am trying to work with the tm package in R, and have a CSV file of customer feedback with each line being a different instance of feedback. I want to import all the content of this feedback into a corpus but I want each line to be a different document within the corpus, so that I can compare the feedback in a DocTerms Matrix. There are over 10,000 rows in my data set.</p>

<p>Originally I did the following:</p>

<pre><code>fdbk_corpus &lt;-Corpus(VectorSource(fdbk), readerControl = list(language=""eng""), sep=""\t"")
</code></pre>

<p>This creates a corpus with 1 document and >10,000 rows, and I want >10,000 docs with 1 row each.</p>

<p>I imagine I could just have 10,000+ separate CSV or TXT documents within a folder and create a corpus from that... but I'm thinking there is a much simpler answer than that, reading each line as a separate document.</p>
","r, text-mining, documents, corpus, tm","<p>Here's a complete workflow to get what you want:</p>

<pre><code># change this file location to suit your machine
file_loc &lt;- ""C:\\Documents and Settings\\Administrator\\Desktop\\Book1.csv""
# change TRUE to FALSE if you have no column headings in the CSV
x &lt;- read.csv(file_loc, header = TRUE)
require(tm)
corp &lt;- Corpus(DataframeSource(x))
dtm &lt;- DocumentTermMatrix(corp)
</code></pre>

<p>In the <code>dtm</code> object each row will be a doc, or a line of your original CSV file. Each column will be a word.</p>
",18,10,21481,2013-08-01 14:50:01,https://stackoverflow.com/questions/17997364/r-text-mining-documents-from-csv-file-one-row-per-doc
R parse strings of numbers from between brackets in a character string,"<p>I have a messy file that I'm attempting to parse into numeric data in R.  The data is contained in a file that is not XML, but follows a specific format:</p>

<pre><code>""{""metrics"":{""skin_temp"":{""min"":81.5,""max"":96.8,""sum"":93480.6,
  ""summary"":{""max_skin_temp_per_minute"":null,""min_skin_temp_per_minute"":null},
  ""values"":[93.2,93.2,93.3,93.3]],""stdev"":0.9,""avg"":2.1},
  ""gsr"":{""min"":0.000149,""max"":31.5,""sum"":10300.0,
  ""summary"":{""max_gsr_per_minute"":null,""min_gsr_per_minute"":null},
  ""values"":[1.22,1.23,1.2,1.2],""stdev"":9.630000000000001,""avg"":10.1},
  ""steps"":{""min"":0,""max"":104,""sum"":4202,
  ""summary"":{""max_steps_per_minute"":null,""min_steps_per_minute"":null},
  ""values"":[0,0,0,0]],""stdev"":13.8,""avg"":4}}""
</code></pre>

<p>All I'm interested in is the code that comes in chunks after the <code>""values""</code> labels (this information is included by the website I'm pulling the data from, but I can easily compute summary statistics in R if I want them).</p>

<p>I know there is an easier way, but the code I have so far looks like this:</p>

<pre><code>raw_data      &lt;- gsub('\\""', '', raw_data)
analysis_data &lt;- c()
positioner    &lt;- 0

for (x in 1:3) {
  # find where the data starts (and add 8 more for the 'values' text)
  data_start    &lt;- regexpr(""values:["", substring(raw_data, positioner), 
                           fixed=TRUE)[[1]] + 8 + positioner    
  data_end      &lt;- regexpr(""]"", substring(raw_data, data_start), 
                           fixed=TRUE)[[1]] + data_start - 2
  data_col      &lt;- as.numeric(strsplit(substring(raw_data, data_start, 
                              data_end), "", "")[[1]])
  analysis_data &lt;- cbind(analysis_data, data_col)
  positioner    &lt;- positioner + data_end
} 
</code></pre>

<p>Sometimes this works, but sometimes the <code>positioner</code> variable gets tricked.  Is there a simpler way to pull this code?</p>
","r, text-mining","<p>The raw data you see is in a format called <code>JSON</code> (See <a href=""https://stackoverflow.com/questions/383692/what-is-json-and-why-would-i-use-it"">What Is JSON?</a> )</p>

<p>However, as @user1609452 points out in the comments, it is poorly formatted.  If what is posted in the OP is representative of the actual raw data being used, then it simply has some misplaced double square brackets and missing a closing curly brace. Both easy to fix. </p>

<h3>Fix the JSON</h3>

<pre><code># store the JSON as a single string
raw_data &lt;- '{""metrics"":{""skin_temp"":{""min"":81.5,""max"":96.8,""sum"":93480.6, ""summary"":{""max_skin_temp_per_minute"":null,""min_skin_temp_per_minute"":null}, ""values"":[93.2,93.2,93.3,93.3]],""stdev"":0.9,""avg"":2.1}, ""gsr"":{""min"":0.000149,""max"":31.5,""sum"":10300.0, ""summary"":{""max_gsr_per_minute"":null,""min_gsr_per_minute"":null}, ""values"":[1.22,1.23,1.2,1.2],""stdev"":9.630000000000001,""avg"":10.1}, ""steps"":{""min"":0,""max"":104,""sum"":4202, ""summary"":{""max_steps_per_minute"":null,""min_steps_per_minute"":null}, ""values"":[0,0,0,0]],""stdev"":13.8,""avg"":4}}'


## Clean up the JSON
raw_data &lt;- gsub(""\\]\\]"", ""\\]"", raw_data)
raw_data &lt;- paste0(raw_data, ""}"")
</code></pre>

<h3>Once your JSON is nice and clean, it is easy to parse:</h3>

<pre><code>library(rjson)
dat &lt;- fromJSON(raw_data)
lapply(dat[[""metrics""]], function(D) if (""values"" %in% names(D)) D$values else NA)

# or more succinctly: 
lapply(dat[[""metrics""]], `[[`, ""values"")
</code></pre>

<h3>Results:</h3>

<pre><code>$skin_temp
[1] 93.2 93.2 93.3 93.3

$gsr
[1] 1.22 1.23 1.20 1.20

$steps
[1] 0 0 0 0
</code></pre>
",4,4,1159,2013-08-03 13:57:33,https://stackoverflow.com/questions/18033589/r-parse-strings-of-numbers-from-between-brackets-in-a-character-string
How to merge two data frames using (parts of) text values?,"<p>I have two data frames both with columns containing texts. Now I want to merge those data frames by using (imperfect) matches between the text columns. If e.g. cell 1 of the text column of data frame 1 has a text value that contains a (part of a) word that resembles a (part of a) word in the text value of cel 2 of the text column of data frame 2, then I want the data frames be me merged using these cells. What is the best way to do this in R?</p>

<p>I am not sure if my question is clear enough, but if so, does anyone know of an R package or a function that can help me do this kind of merging?</p>

<p>Many thanks in advance!</p>
","r, reference, text-mining, matching","<p>Try the <a href=""http://cran.r-project.org/web/packages/RecordLinkage/"" rel=""nofollow""><code>RecordLinkage</code></a> package.  </p>

<p>Here is a possible solution where the merge works based on generally how ""close"" the two ""words"" match:</p>

<pre><code>library(reshape2)
library(RecordLinkage)
set.seed(16)
l &lt;- LETTERS[1:10]
ex1 &lt;- data.frame(lets = paste(l, l, l, sep = """"), nums = 1:10)
ex2 &lt;- data.frame(lets = paste(sample(l), sample(l), sample(l), sep = """"), 
                  nums = 11:20)
ex1
# lets nums
# 1   AAA    1
# 2   BBB    2
# 3   CCC    3
# 4   DDD    4
# 5   EEE    5
# 6   FFF    6
# 7   GGG    7
# 8   HHH    8
# 9   III    9
# 10  JJJ   10
ex2
# lets nums
# 1   GDJ   11
# 2   CFH   12
# 3   DBE   13
# 4   BED   14
# 5   FJB   15
# 6   JHG   16
# 7   AII   17
# 8   ICC   18
# 9   EGF   19
# 10  HAA   20
lets &lt;- melt(outer(ex1$lets, ex2$lets, FUN = ""levenshteinDist""))
lets &lt;- lets[lets$value &lt; 2, ] # adjust the ""&lt; 2"" as necessary
cbind(ex1[lets$Var1, ], ex2[lets$Var2, ])
# lets nums lets nums
# 9  III    9  AII   17
# 3  CCC    3  ICC   18
# 1  AAA    1  HAA   20
</code></pre>
",1,0,694,2013-08-09 13:19:41,https://stackoverflow.com/questions/18148212/how-to-merge-two-data-frames-using-parts-of-text-values
Open huge text file and perform regex searching,"<p>I'm trying to open a huge text file (1 GB) and perform some text mining.<br>
I'm willing to perform some regex searching.<br>
When I'm using the read() function, I'm getting the error:</p>

<pre><code>File ""C:\Python33\lib\encodings\latin_1.py"", line 26, in decode
return codecs.latin_1_decode(input,self.errors)[0]
MemoryError
</code></pre>

<p>My code is:  </p>

<pre><code>dataFile = open('data/AllData_2000001_3000000.txt', 'r', encoding=""latin-1"")
print(dataFile.read())
</code></pre>

<p>What will be the best way to open the text, in order to perform regex search?<br>
Thanks!  </p>
","python, regex, text-mining","<p>Depends on what you want to do:</p>

<p>If you really want to perform a regex search on the whole 1GB in one piece, you don't have a lot of options. Switching to 64-bit Python might be one if you're using a 32-bit version right now.</p>

<p>Is your text file organized in lines or something the like? In that case you can read one line, process it, go to the next and so on. Of course this works with any kind of 'chunk' that you can isolate.</p>
",0,2,305,2013-08-11 21:35:28,https://stackoverflow.com/questions/18176931/open-huge-text-file-and-perform-regex-searching
Huge text file to small excel files,"<p>I have a huge text file (4 GB), where each ""line"" is of the syntax:
<code>[number] [number]_[number] [Text]</code>.<br>
For example</p>

<pre><code>123 12_14 Text 1
1234 13_456 Text 2
33 12_12 Text 3
24 678_10 Text 4
</code></pre>

<p>My purpose is to have this data saved as Excel file, where each ""line"" in the text file,<br>
is a row in the excel file. According to the past example:</p>

<pre><code>[A1] 123
[B1] 12_14
[C1] Text 1
[A2] 1234
[B2] 13_456
[C2] Text 2
[A3] 33
[B3] 12_12
[C3] Text 3
[A4] 24
[B4] 678_10
[C4] Text 4
</code></pre>

<p>My plan is to iterate the text ""lines"", as advised <a href=""https://stackoverflow.com/a/18178159/1869297"">here</a>, separate the ""lines"",<br>
and save to the cells in an excel file. </p>

<p>Because of the text size issue, I thought to create many small excel files, which all together will be equal to the text file.  </p>

<p>Than I need to analyze the small excel files, mainly found terms that where mentioned in the <code>[Text]</code> cells, and count the number of apperance, related to the <code>[number]</code> cells (representing a post and ID of a post).  </p>

<p>Finally, I need to sum all this data in an excel file.</p>

<p>I'm considering the best way to create and analyze the excel files.<br>
As mentioned <a href=""https://stackoverflow.com/q/3239207/1869297"">here</a> the main libraries are <a href=""http://www.python-excel.org/"" rel=""nofollow noreferrer"">xlrd</a> and <a href=""http://docs.python.org/2/library/csv.html"" rel=""nofollow noreferrer"">csv</a>.</p>
","python, csv, export-to-excel, text-mining, xlrd","<p>""I'm pretty sure I don't have other options than small excel files, but what will be the another approach?""</p>

<p>Your huge text file is a type of database, although an inconvenient one. A bunch of small Excel files are another, even less convenient representation of the same database. I assume you are looking to make a bunch of small files because Excel has an upper limit on how many rows it can contain (65'000 or 1'000'000 depending on the version of Excel). However, as has <a href=""https://stackoverflow.com/a/16770616/282912"">been noted</a>, Excel files are truly horrible database stores.</p>

<p>Since you are already using Python, use module <a href=""http://docs.python.org/2/library/sqlite3.html"" rel=""nofollow noreferrer""><code>sqlite3</code></a>, it's already built in and it's a real database, and it can handle more than a million rows. And it's fast.</p>

<p>But I wanted to get an idea how fast it is with data on the scale that you propose so I created a 30M row database of roughly the same complexity as your dataset. The schema is simple:</p>

<pre><code>create table words
    (id integer primary key autoincrement not null,
     first text, second text, third text);
</code></pre>

<p>and populated it with random trigrams drawn from /usr/dict/words (I have a module for generating test data like this which makes entries that look like </p>

<pre><code>sqlite&gt; select * from words limit 5;
1|salvation|prorates|bird
2|fore|embellishment|empathized
3|scalier|trinity|graze
4|exes|archways|interrelationships
5|voguish|collating|partying
</code></pre>

<p>but a simple query for a row I knew was near the end took longer than I'd hoped:</p>

<pre><code>select * from words where first == ""dole"" and second == ""licked"";
29599672|dole|licked|hates
</code></pre>

<p>took about 7 seconds on a pretty average 3-year-old desktop so I added a couple of indexes</p>

<pre><code>create index first on words (first);
create index second on words (second);
</code></pre>

<p>which did double the size of the database file from 1.1GB to 2.3GB but brought the simple query time down to a rather reasonable 0.006 second. I don't think you'll do as well with Excel.</p>

<p>So parse your data however you must, but then put it in a real database.</p>
",3,1,475,2013-08-14 12:26:58,https://stackoverflow.com/questions/18231734/huge-text-file-to-small-excel-files
Naive Bayes classifier bases decision only on a-priori probabilities,"<p>I'm trying to classify tweets according to their sentiment into three categories (Buy, Hold, Sell). I'm using R and the package e1071. </p>

<p>I have two data frames: one trainingset and one set of new tweets which sentiment need to be predicted.</p>

<p>trainingset dataframe:</p>

<pre><code>   +--------------------------------------------------+

   **text | sentiment**

   *this stock is a good buy* | Buy

   *markets crash in tokyo* | Sell

   *everybody excited about new products* | Hold

   +--------------------------------------------------+
</code></pre>

<p>Now I want to train the model using the tweet text  <code>trainingset[,2]</code> and the sentiment category <code>trainingset[,4]</code>.</p>

<pre><code>classifier&lt;-naiveBayes(trainingset[,2],as.factor(trainingset[,4]), laplace=1)
</code></pre>

<p>Looking into the elements of classifier with </p>

<p><code>classifier$tables$x</code> </p>

<p>I find that the conditional probabilities are calculated..There are different probabilities for every tweet concerning Buy,Hold and Sell.So far so good.</p>

<p>However when I predict the training set with:</p>

<p><code>predict(classifier, trainingset[,2], type=""raw"")</code></p>

<p>I get a classification which is based <strong>only</strong> on the a-priori probabilities, which means every tweet is classified as Hold (because ""Hold"" had the largest share among the sentiment). So every tweet has the same probabilities for Buy, Hold, and Sell:</p>

<pre><code>      +--------------------------------------------------+

      **Id | Buy | Hold | Sell**

      1  |0.25 | 0.5  | 0.25

      2  |0.25 | 0.5  | 0.25

      3  |0.25 | 0.5  | 0.25

     ..  |..... | ....  | ...

      N  |0.25 | 0.5  | 0.25

     +--------------------------------------------------+
</code></pre>

<p>Any ideas what I'm doing wrong? 
Appreciate your help!</p>

<p>Thanks</p>
","r, machine-learning, classification, text-mining","<p>It looks like you trained the model using <strong>whole sentences</strong> as inputs, while it seems that you want to use <strong>words</strong> as your input features.</p>

<blockquote>
  <p>Usage:</p>

<pre><code>## S3 method for class 'formula'
naiveBayes(formula, data, laplace = 0, ..., subset, na.action = na.pass)
## Default S3 method:
naiveBayes(x, y, laplace = 0, ...)


## S3 method for class 'naiveBayes'
predict(object, newdata,
  type = c(""class"", ""raw""), threshold = 0.001, ...)
</code></pre>
  
  <p>Arguments:</p>

<pre><code>  x: A numeric matrix, or a data frame of categorical and/or
     numeric variables.

  y: Class vector.
</code></pre>
</blockquote>

<p>In particular, if you train <code>naiveBayes</code> this way:</p>

<pre><code>x &lt;- c(""john likes cake"", ""marry likes cats and john"")
y &lt;- as.factor(c(""good"", ""bad"")) 
bayes&lt;-naiveBayes( x,y )
</code></pre>

<p>you get a classifier able to recognize just these two sentences:</p>

<pre><code>Naive Bayes Classifier for Discrete Predictors

Call:
naiveBayes.default(x = x,y = y)

A-priori probabilities:
y
 bad good 
 0.5  0.5 

Conditional probabilities:
            x
      x
y      john likes cake marry likes cats and john
  bad                0                         1
  good               1                         0
</code></pre>

<p>to achieve a <strong>word level</strong> classifier you need to run it with words as inputs</p>

<pre><code>x &lt;-             c(""john"",""likes"",""cake"",""marry"",""likes"",""cats"",""and"",""john"")
y &lt;- as.factors( c(""good"",""good"", ""good"",""bad"",  ""bad"",  ""bad"", ""bad"",""bad"") )
bayes&lt;-naiveBayes( x,y )
</code></pre>

<p>you get</p>

<pre><code>Naive Bayes Classifier for Discrete Predictors

Call:
naiveBayes.default(x = x,y = y)

A-priori probabilities:
y
 bad good 
 0.625 0.375 

Conditional probabilities:
      x
y            and      cake      cats      john     likes     marry
  bad  0.2000000 0.0000000 0.2000000 0.2000000 0.2000000 0.2000000
  good 0.0000000 0.3333333 0.0000000 0.3333333 0.3333333 0.0000000
</code></pre>

<p>In general <code>R</code> is not well suited for processing NLP data, <code>python</code> (or at least <code>Java</code>) would be much better choice.</p>

<p>To convert a sentence to the words, you can use the <code>strsplit</code> function</p>

<pre><code>unlist(strsplit(""john likes cake"","" ""))
[1] ""john""  ""likes"" ""cake"" 
</code></pre>
",8,6,2823,2013-08-15 16:33:18,https://stackoverflow.com/questions/18257260/naive-bayes-classifier-bases-decision-only-on-a-priori-probabilities
Using Naive Bayes Classification to Identity a Twitter User&#39;s Gender,"<p>I have become part of a project at school that has been a lot of fun so far and it just got a little bit more interesting. I have roughly 600,000 tweets in my possession (each contains screen name, geo location, text, etc.) and my goal is to try to classify each user as either male or female. Now using Twitter4J I can get what the user's full name, number of friends, re-tweets, etc. So I was wondering if a combination of looking at a users name and also doing text analysis would be a possible answer. I was originally thinking I could make this like a rule based classifier where I could first look at the user's name then analyze their text and attempt to come to a conclusion of M or F. I'm guessing I would have trouble using something such as naive bayes since I don't have the real truth values?</p>

<p>Also with the names, I would be checking some kind of dictionary to interpret whether the name was male or female. I know there are cases where it's hard to tell but that's why I'd be looking at their tweet texts as well. I also forgot to mention; with these 600,000 tweets, I have at minimum two tweets per user available to me.</p>

<p>Any ideas or input on classifying a user's gender would be greatly appreciated! I don't have a ton of experience in this area and I'm looking to learn anything I can get my hands on. </p>
","twitter, machine-learning, classification, text-mining","<blockquote>
  <p>I'm guessing I would have trouble using something such as naive bayes since I don't have the real truth values?</p>
</blockquote>

<p>Any supervised learning algorithm, such as Naive Bayes, requires preparing training set. Without the actual gender for some data you cannot build such a model. On the other hand, if you come out with some rule bases system (like the one based on the users' names) you can try a semi-supervised approach. Using your rule based system, you can create some labelling of your data, lets say that your rule based classifier is <code>RC</code> and can answer ""Male"", ""Female"", ""Do not know"", you can create a labelling of your data <code>X</code> using <code>RC</code> in a natural way:</p>

<pre><code>X_m = { x in X : RC(x)=""Male"" }
X_f = { x in X : RC(x)=""Female"" }
</code></pre>

<p>Once you did it, you can create a training set for the supervised learning model using all your data <strong>except</strong> the one used for creating <code>RC</code> - so in this case - users' names (I assume, that <code>RC</code> answers ""Male"" or ""Female"" iff it is entirely ""sure"" about it). As a result, you will train a classifier, which will try to generalize concept of gender from all additional data (like words used, location etc.). Lets call it <code>SC</code>. After that, you can simply create a ""complex"" classifier:</p>

<pre><code>C(x) = ""Male"" iff RC(x)= Male"" or 
                  (RC(x)=""Do not know"" &amp;&amp; SC(x)=""Male"")
       ""Female"" iff RC(x)= Female"" or 
                    (RC(x)=""Do not know"" &amp;&amp; SC(x)=""Female"")
</code></pre>

<p>This way you can on one hand use the most valuable information (user name) in the rule based way, while in the same time exploit power of supervised learning for the ""hard cases"" while not having the ""ground truth"" in the first place.</p>
",2,1,2917,2013-08-17 16:48:20,https://stackoverflow.com/questions/18291153/using-naive-bayes-classification-to-identity-a-twitter-users-gender
How to use a regular expression inside TermDocumentMatrix for text mining?,"<p>I know that I can use the tm package to count the occurrences of specific words in a corpus using the Dictionary function:</p>

<pre><code>require(tm)
data(crude)

dic &lt;- Dictionary(""crude"")
tdm &lt;- TermDocumentMatrix(crude, control = list(dictionary = dic, removePunctuation = TRUE))
inspect(tdm)
</code></pre>

<p>I would like to know if there is a facility to instead supply a regular expression to Dictionary instead of a fixed word?</p>

<p>Sometimes stemming may not be what I want (e.g. I may want to pick up spelling mistakes ) and so I would like to do something like: </p>

<pre><code>dic &lt;- Dictionary(c(""crude"", 
                    ""\\bcrud[[:alnum:]]+""),
                    ""\\bcrud[de]"")
</code></pre>

<p>and thus continue to use the facilities of the tm package?</p>
","regex, r, text-mining, tm","<p>I'm not sure that you can put regex in the dictionary function as it only accepts a character vector or a term-document matrix. The work-around I'd suggest is using regex to subset the terms in the term-document matrix, then do word counts:</p>

<pre><code># What I would do instead
tdm &lt;- TermDocumentMatrix(crude, control = list(removePunctuation = TRUE))
# subset the tdm according to the criteria
# this is where you can use regex
crit &lt;- grep(""cru"", tdm$dimnames$Terms)
# have a look to see what you got
inspect(tdm[crit])
        A term-document matrix (2 terms, 20 documents)

    Non-/sparse entries: 10/30
    Sparsity           : 75%
    Maximal term length: 7 
    Weighting          : term frequency (tf)

             Docs
    Terms     127 144 191 194 211 236 237 242 246 248 273 349 352 353 368 489 502 543
      crucial   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0
      crude     2   0   2   3   0   2   0   0   0   0   5   2   0   2   0   0   0   2
             Docs
    Terms     704 708
      crucial   0   0
      crude     0   1
# and count the number of times that criteria is met in each doc
colSums(as.matrix(tdm[crit]))
127 144 191 194 211 236 237 242 246 248 273 349 352 353 368 489 502 543 704 708 
  2   0   2   3   0   2   2   0   0   0   5   2   0   2   0   0   0   2   0   1 
# count the total number of times in all docs
sum(colSums(as.matrix(tdm[crit])))
[1] 23
</code></pre>

<p>If this isn't what you want, go ahead and edit your question to include some example data that is properly representative of your actual use-case, and an example of your desired output.</p>
",3,1,1456,2013-08-22 14:18:21,https://stackoverflow.com/questions/18383074/how-to-use-a-regular-expression-inside-termdocumentmatrix-for-text-mining
Adding custom stopwords in R tm,"<p>I have a Corpus in R using the <code>tm</code> package. I am applying the <code>removeWords</code> function to remove stopwords</p>

<pre><code>tm_map(abs, removeWords, stopwords(""english"")) 
</code></pre>

<p>Is there a way to add my own custom stop words to this list?</p>
","r, text-mining, stop-words, corpus, tm","<p><code>stopwords</code> just provides you with a vector of words, just <code>c</code>ombine your own ones to this.</p>

<pre><code>tm_map(abs, removeWords, c(stopwords(""english""),""my"",""custom"",""words"")) 
</code></pre>
",41,17,42254,2013-08-26 14:22:05,https://stackoverflow.com/questions/18446408/adding-custom-stopwords-in-r-tm
Split with strsplit(...) textvectors into chunks with R,"<p>Please help me for my small project.</p>

<p>Have a big-list of text elements. Each element should be split into a small-list of sentences. Each small-list should be 'saved' as one element into a new column of the initial big-list at the same position ('row') like the original text element.</p>

<p>The splitting criteria are <code>""/$""</code>, <code>""und/KON""</code>, <code>""oder/KON""</code>. This should be kept at the head of the new small-list-element.</p>

<p>I've tried with regular expressions like <code>""/$|und/KON|oder/KON""</code> and manny combinations of escaping <code>""$""</code>, <code>""|""</code>, <code>""/""</code>. Also i tried to change the parameter <code>perl = TRUE</code>, <code>fixed = TRUE</code> and <code>FALSE</code>. Every time i try noting happens. Seems that the <code>|</code> is not interpreted properly. What do you recommend to solve the problem?</p>

<pre><code>library(stringr) # don't know if it's required

# Input list to be splitted at each
#      ""/$"", ""und/KON"", ""oder/KON""
#      but should keep the expression at the start of the next list element
#      
#      Would be nice but not necessary: The small-list to be named after the ID in the first column

&gt; r &lt;- list(ID=c(01, 02, 03),
            elements=c(""This should become my first small-list :/$. the first element ,/$, the second element ,/$, and the third element ./$."",
                       ""This should become my second small-list :/$. Element eins und/KON Element zwei oder/KON Element drei ./$."",
                       ""This should become my third small-list :/$. Element Alpha und/KON Element Beta oder/KON Element Gamma ./$."")

# Would look something like 
r$small_lists &lt;- sapply(r$elements ,function(x) as.list(strsplit(x,""/$|und/KON""|oder/KON"", fixed=TRUE)))
&gt; r$small_lists

$01
[1] ""This should become my first small-list ""
[2] "":/$. the first element ""
[3] "",/$, the second element ""
[4] "",/$, and the third element ""
[5] ""./$.""

$02 
[1] ""This should become my second small-list ""
[2] "":/$. Element eins ""
[3] ""und/KON Element zwei ""
[4] ""oder/KON Element drei""
[5] ""./$.""

$03
[1] ""This should become my third small-list ""
[2] "":/$. Element Alpha ""
[3] ""und/KON Element Beta ""
[4] ""oder/KON Element Gamma ""
[5] ""./$.""

&gt; class(r)
[1] ""list""
&gt; class(r$small_lists)
[1] ""list""
</code></pre>
","regex, r, text-mining, strsplit","<p>You actually have more patterns to split on than you indicate, if that's the output you desire. Note that my patterns are different from yours. All special characters have been escaped with <code>\\</code>.</p>

<p>To keep things manageable, I would create a separate vector of the patterns that you want to split on, paste them together in a master pattern, search for them and prepend them by some string you know doesn't occur in your text, and split on that.</p>

<p>Here are the ""patterns"" that I've identified:</p>

<pre><code>Pattern &lt;- c("":/\\$"", "",/\\$"", ""\\./\\$"",
             ""und/KON"", ""oder/KON"")
</code></pre>

<p>We can <code>paste</code> these patterns together to get the master pattern. <code>sep</code> on the interior <code>paste</code> is the pipe symbol for matching different patterns. The whole pattern is put within brackets (<code>(</code> and <code>)</code>) so that we can reference it later.</p>

<pre><code>Pattern &lt;- paste(""("", paste(Pattern, collapse = ""|""), "")"", sep = """")
</code></pre>

<p>We can now use <code>gsub</code> to add a ""prefix"" to the pattern (that's what the <code>\\1</code> refers to). We need that prefix because you want to retain the mentioned expression.</p>

<pre><code>## Insert some text pattern you know doesn't occur in your text
## Here, I've prepended the matched patterns with ""^&amp;*""
## You now have something on which you can split
strsplit(gsub(Pattern, ""^&amp;*\\1"", r$elements), ""^&amp;*"", fixed = TRUE)
# [[1]]
# [1] ""This should become my first small-list ""
# [2] "":/$. the first element ""                
# [3] "",/$, the second element ""               
# [4] "",/$, and the third element ""            
# [5] ""./$.""                                   
# 
# [[2]]
# [1] ""This should become my second small-list ""
# [2] "":/$. Element eins ""                      
# [3] ""und/KON Element zwei ""                   
# [4] ""oder/KON Element drei ""                  
# [5] ""./$.""                                    
# 
# [[3]]
# [1] ""This should become my third small-list ""
# [2] "":/$. Element Alpha ""                    
# [3] ""und/KON Element Beta ""                  
# [4] ""oder/KON Element Gamma ""                
# [5] ""./$."" 
</code></pre>

<hr>

<p>Continuing from above, to get the named list you describe:</p>

<pre><code>out &lt;- strsplit(gsub(Pattern, ""^&amp;*\\1"", r$elements), ""^&amp;*"", fixed = TRUE)
setNames(lapply(out, `[`, -1), lapply(out, `[`, 1))
# $`This should become my first small-list `
# [1] "":/$. the first element ""    
# [2] "",/$, the second element ""   
# [3] "",/$, and the third element ""
# [4] ""./$.""                       
# 
# $`This should become my second small-list `
# [1] "":/$. Element eins ""    
# [2] ""und/KON Element zwei "" 
# [3] ""oder/KON Element drei ""
# [4] ""./$.""                  
# 
# $`This should become my third small-list `
# [1] "":/$. Element Alpha ""    
# [2] ""und/KON Element Beta ""  
# [3] ""oder/KON Element Gamma ""
# [4] ""./$."" 
</code></pre>
",3,1,285,2013-08-28 17:32:07,https://stackoverflow.com/questions/18494530/split-with-strsplit-textvectors-into-chunks-with-r
R: split text with multiple regex patterns and exceptions,"<p>Would like to split a vector of character elements <code>text</code> in sentences. There are more then one pattern of splitting criteria (<code>""and/ERT""</code>, <code>""/$""</code>). Also there are exceptions(<code>:/$.</code>, <code>and/ERT then</code>, <code>./$. Smiley</code>) from the patterns.</p>

<p>The try: Match the cases where the split should be. Insert an unusual pattern (<code>""^&amp;*""</code>) at that place. <code>strsplit</code> the specific pattern</p>

<p>Problem: I don't know how to handle properly exceptions. There are explicit cases where the unusual pattern (<code>""^&amp;*""</code>) should be eliminated and the original text restored before running <code>strsplit</code>.</p>

<p>Code:</p>

<pre><code>text &lt;- c(""This are faulty propositions one and/ERT two ,/$, which I want to split ./$. There are cases where I explicitly want and/ERT some where I don't want to split ./$. For example :/$. when there is an and/ERT then I don't want to split ./$. This is also one case where I dont't want to split ./$. Smiley !/$. Thank you ./$!"",
""This are the same faulty propositions one and/ERT two ,/$, which I want to split ./$. There are cases where I explicitly want and/ERT some where I don't want to split ./$. For example :/$. when there is an and/ERT then I don't want to split ./$. This is also one case where I dont't want to split ./$. Smiley !/$. Thank you ./$!"",
""Like above the same faulty propositions one and/ERT two ,/$, which I want to split ./$. There are cases where I explicitly want and/ERT some where I don't want to split ./$. For example :/$. when there is an and/ERT then I don't want to split ./$. This is also one case where I dont't want to split ./$. Smiley !/$. Thank you ./$!"")

patternSplit &lt;- c(""and/ERT"", ""/\\$"") # The class of split-cases is much larger then in this example. Therefore it is not possible to adress them explicitly.
patternSplit &lt;- paste(""("", paste(patternSplit, collapse = ""|""), "")"", sep = """")

exceptionsSplit &lt;- c(""\\:/\\$\\."", ""and/ERT then"", ""\\./\\$\\. Smiley"")
exceptionsSplit &lt;- paste(""("", paste(exceptionsSplit, collapse = ""|""), "")"", sep = """")

# If you don't have exceptions, it works here. Unfortunately it splits ""*$/*"" into ""*"" and ""$/*"". Would be convenient to avoid this. See example ""ideal"" split below.
textsplitted &lt;- strsplit(gsub(patternSplit, ""^&amp;*\\1"", text), ""^&amp;*"", fixed = TRUE) # 

# Ideal split:
textsplitted
&gt; textsplitted
[[1]]
 [1] ""This are faulty propositions one and/ERT"" 
 [2] ""two ,/$,"" 
 [3] ""which I want to split ./$.""
 [4] ""There are cases where I explicitly want and/ERT"" 
 [5] ""some where I don't want to split ./$."" 
 [6] ""For example :/$. when there is an and/ERT then I don't want to split ./$.""
 [7] ""This is also one case where I dont't want to split ./$. Smiley !/$."" 
 [8] ""Thank you ./$!""

[[2]]
 [1] ""This are the same faulty propositions one and/ERT 
 [2] ""two ,/$,""
#...      

# This try doesen't work!
text &lt;- gsub(patternSplit, ""^&amp;*\\1"", text)
text &lt;- gsub(exceptionsSplit, ""[original text without ""^&amp;*""]"", text)
textsplitted &lt;- strsplit(text, ""^&amp;*"", fixed = TRUE)
</code></pre>
","regex, r, text-mining, strsplit","<p>I think you can use this expression to attain the splits you want. As <code>strsplit</code> uses up the characters it splits on you will have to split on the spaces following the things to match for/not to match for (which is what you have in the desired output in your OP):</p>

<pre><code>strsplit( text[[1]] , ""(?&lt;=and/ERT)\\s(?!then)|(?&lt;=/\\$[[:punct:]])(?&lt;!:/\\$[[:punct:]])\\s(?!Smiley)""  , perl = TRUE )
#[[1]]
#[1] ""This are faulty propositions one and/ERT""                                 
#[2] ""two ,/$,""                                                                 
#[3] ""which I want to split ./$.""                                               
#[4] ""There are cases where I explicitly want and/ERT""                          
#[5] ""some where I don't want to split ./$.""                                    
#[6] ""For example :/$. when there is an and/ERT then I don't want to split ./$.""
#[7] ""This is also one case where I dont't want to split ./$. Smiley !/$.""      
#[8] ""Thank you ./$!"" 
</code></pre>

<h3>Explanation</h3>

<ul>
<li><code>(?&lt;=and/ERT)\\s</code>  -  split on a space, <code>\\s</code> that <strong>IS</strong> preceded, <code>(?&lt;=...)</code> by <code>""and/ERT""</code></li>
<li><code>(?!then)</code>  -  <strong>BUT</strong> only if that space is <strong>NOT</strong> followed, <code>(?!...)</code> by <code>""then""</code></li>
<li><code>|</code>  -  OR operator to chain the next expression</li>
<li><code>(?&lt;=/\\$[[:punct:]])</code>  -  positive look-behind assertion for <code>""/$""</code> followed by any letter of punctuation</li>
<li><code>(?&lt;!:/\\$[[:punct:]])\\s(?!Smiley)</code> - match a space that is <strong><em>NOT</em></strong> preceded by <code>"":/$""[[:punct:]]</code> (but according to the previous point <strong>IS</strong> preceded by <code>""/$[[:punct:]]""</code> but <strong><em>NOT</em></strong> followed, <code>(?!...)</code> by <code>""Smiley""</code></li>
</ul>
",10,9,3256,2013-09-09 11:16:27,https://stackoverflow.com/questions/18697005/r-split-text-with-multiple-regex-patterns-and-exceptions
R: Extract capital letters and special characters with strsplit and perl REGEX syntax,"<p>How you would extract only the <code>/</code> with the following capital letters, and the whole <code>[[:punct:]]/$[[:punct:]]</code>.</p>

<pre><code>text &lt;- c(""This/ART ,/$; Is/NN something something/else A/VAFIN faulty/ADV text/ADV which/ADJD i/PWS propose/ADV as/APPR Example/NE ./$. So/NE It/PTKNEG makes/ADJD no/VAFIN sense/ADV at/KOUS all/PDAT ,/$, it/APPR Has/ADJA Errors/NN  ,/$; and/APPR it/APPR is/CARD senseless/NN again/ART ./$:"")

# HOW to?
textPOS &lt;- strsplit(text,""(   )|(?&lt;=[[:punct:]]/\\$[[:punct:]])"", perl=TRUE)
#                          ^^^ 
#                         extract only the ""/"" with the following capital letters
#                         and the whole ""[[:punct:]]/$[[:punct:]]""

# Expected RETURN:
&gt; textPOS
[1] ""/ART"" "",/$;"" ""/NN"" ""/VAFIN"" ""/ADV"" ""/ADV"" ""/ADJD"" ""/PWS"" ""/ADV"" ""/APPR"" ""/NE"" ""./$."" ""/NE"" ""/PTKNEG"" ""/ADJD"" ""/VAFIN"" ""/ADV"" ""/KOUS"" ""/PDAT"" "",/$,"" ""/APPR"" ""/ADJA"" ""/NN"" "",/$;"" ""/APPR"" ""/APPR"" ""/CARD"" ""/NN"" ""/ART"" ""./$:""
</code></pre>

<p>Thank you! :)</p>
","regex, r, text-mining, strsplit","<p>You can use <code>gregexpr</code> and <code>regmatches</code>:</p>

<pre><code>regmatches(text, gregexpr('[[:punct:]]*/[[:alpha:][:punct:]]*', text))
# [[1]]
#  [1] ""/ART""    ""/NN""     ""/VAFIN""  ""/ADV""    ""/ADV""    ""/ADJD""   ""/PWS""    ""/ADV""    ""/APPR""   ""/NE""     ""./$.""    ""/NE""    
# [13] ""/PTKNEG"" ""/ADJD""   ""/VAFIN""  ""/ADV""    ""/KOUS""   ""/PDAT""   "",/$,""    ""/APPR""   ""/ADJA""   ""/NN""     "",/$;""    ""/APPR""  
# [25] ""/APPR""   ""/CARD""   ""/NN""     ""/ART""    ""./$:""   
</code></pre>

<p>In words the regex says: ""find things that start with zero or more punctuation marks followed by a slash followed by one or more letters or punctuation.  If you want to include numbers switch to <code>[:alnum:]</code>.</p>

<hr>

<p>Per comments, if you want only uppercase letters the regex would become:</p>

<pre><code>regmatches(text, gregexpr('[[:punct:]]*/[[:upper:][:punct:]]*', text))
</code></pre>

<p>As @eddi suggests, <code>[A-Z]</code> and <code>[:upper:]</code> are roughly equivalent.  Again as @eddi suggests,  this regex will catch teh /LETTERS case as well as the /$punct case:</p>

<pre><code>/[A-Z]+|[[:punct:]]/\\$[[:punct:]]
</code></pre>
",7,2,2995,2013-09-13 14:25:01,https://stackoverflow.com/questions/18788726/r-extract-capital-letters-and-special-characters-with-strsplit-and-perl-regex-s
Text Mining with too much data,"<p>I am trying to play with text mining tools that the R language offers but I am facing the following problem since I am running in an old machine.</p>

<p>I want to create a Document Term Matrix using the tm package and the Corpus function.
When I create the DTM I receive an error that can allocate memory of 4GB (My machine has 2 GB of memory). How in general do you face such a problem? For example, in general applications the DTM should be much greater than my matrix. Is there a way to use an SQL database instead of using the memory? </p>

<p>//I have studied a releated post about using the sqldf library in order to create a temporary sqlite database. But in this case I can not even create the matrix.</p>
","r, matrix, text-mining","<blockquote>
  <p>How in general do you face such a problem?</p>
</blockquote>

<p>Use a <a href=""https://en.wikipedia.org/wiki/Sparse_matrix"" rel=""nofollow"">sparse matrix data structure</a>. Without that, text mining is pretty much impossible. With one, I can process 100s of 1000s of document in a few hundred MB.</p>

<p>I don't work in R myself, but it's bound to have a sparse matrix package somewhere.</p>
",4,0,253,2013-09-19 12:12:26,https://stackoverflow.com/questions/18894320/text-mining-with-too-much-data
Is it possible to provide a list of custom stopwords to RTextTools package?,"<p>With the tm package I'm able to do it like this:</p>

<pre><code>c0 &lt;- Corpus(VectorSource(text))
c0 &lt;- tm_map(c0, removeWords, c(stopwords(""english""),mystopwords))
</code></pre>

<p><code>mystopwords</code> being a vector of the additional stopwords I want to remove.</p>

<p>But I can't find an equivalent way to do it using the RTextTools package. For example:</p>

<pre><code>dtm &lt;- create_matrix(text,language=""english"",
             removePunctuation=T,
             stripWhitespace=T,
             toLower=T,
             removeStopwords=T, #no clear way to specify a custom list here!
             stemWords=T)
</code></pre>

<p>Is it possible to do this? I really like the <code>RTextTools</code> interface and it would be a pity to have to move back to <code>tm</code>.</p>
","r, text-mining, stop-words, tm","<p>There are three (or possible even more) solutions to your problem:</p>

<p>First, use the <code>tm</code> package only for removing words. Both packages deal with the same objects, therefore you can use <code>tm</code> just for removing words and than the <code>RTextTools</code> package. Even when you look inside the function <code>create_matrix</code> it uses <code>tm</code> functions.</p>

<p>Second, modify the <code>create_matrix</code> function. For example add an input parameter like <code>own_stopwords=NULL</code> and add the following lines:</p>

<pre><code># existing line
corpus &lt;- Corpus(VectorSource(trainingColumn), 
                     readerControl = list(language = language))
# after that add this new line
if(!is.null(own_stopwords)) corpus &lt;- tm_map(corpus, removeWords, 
                                          words=as.character(own_stopwords))
</code></pre>

<p>Third, write your own function, something like this:</p>

<pre><code># excluder function
remove_my_stopwords&lt;-function(own_stw, dtm){
  ind&lt;-sapply(own_stw, function(x, words){
    if(any(x==words)) return(which(x==words)) else return(NA)
  }, words=colnames(dtm))
  return(dtm[ ,-c(na.omit(ind))])  
}
</code></pre>

<p>let´s have a look if it works:</p>

<pre><code># let´s test it
data(NYTimes)
data &lt;- NYTimes[sample(1:3100, size=10,replace=FALSE),]
matrix &lt;- create_matrix(cbind(data[""Title""], data[""Subject""]))

head(colnames(matrix), 5)
# [1] ""109""         ""200th""       ""abc""         ""amid""        ""anniversary""


# let´s consider some ""own"" stopwords as words above
ostw &lt;- head(colnames(matrix), 5)

matrix2&lt;-remove_my_stopwords(own_stw=ostw, dtm=matrix)

# check if they are still there
sapply(ostw, function(x, words) any(x==words), words=colnames(matrix2))
#109       200th         abc        amid anniversary 
#FALSE       FALSE       FALSE       FALSE       FALSE 
</code></pre>

<p>HTH</p>
",3,6,6110,2013-10-08 04:37:19,https://stackoverflow.com/questions/19239190/is-it-possible-to-provide-a-list-of-custom-stopwords-to-rtexttools-package
Use scikit-learn TfIdf with gensim LDA,"<p>I've used various versions of TFIDF in scikit learn to model some text data.</p>

<pre><code>vectorizer = TfidfVectorizer(min_df=1,stop_words='english')
</code></pre>

<p>The resulting data X is in this format:</p>

<pre><code>&lt;rowsxcolumns sparse matrix of type '&lt;type 'numpy.float64'&gt;'
    with xyz stored elements in Compressed Sparse Row format&gt;
</code></pre>

<p>I wanted to experiment with LDA as a way to do reduce dimensionality of my sparse matrix.
Is there a simple way to feed the NumPy sparse matrix X into a gensim LDA model?</p>

<pre><code>lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100)
</code></pre>

<p>I can ignore scikit and go the way the gensim tutorial outlines, but I like the simplicity of the scikit vectorizers and all of its parameters.</p>
","python, scikit-learn, text-mining, lda","<p><a href=""http://radimrehurek.com/gensim/matutils.html"" rel=""noreferrer"">http://radimrehurek.com/gensim/matutils.html</a></p>

<pre><code>class gensim.matutils.Sparse2Corpus(sparse, documents_columns=True)

      Convert a matrix in scipy.sparse format into a streaming gensim corpus.
</code></pre>
",11,9,4934,2013-10-21 21:16:10,https://stackoverflow.com/questions/19504898/use-scikit-learn-tfidf-with-gensim-lda
Is there any data-mining/text-mining/machine learning techniques to find the most appropriate Tags for a given document,"<p>Say I have a huge set of documents represented in relational Table with columns</p>

<pre><code>    ID (unique identifier)
    Title (255 characters)
    Description (5000 characters)
    Category (predefined meta-data )
    Additional Notes (1000 characters )
</code></pre>

<p>I would like to add one or more Tags for each row in the document table. Here Tags refer to a word or a group of words that tells readers what a document is about.</p>

<p>Is there any data-mining/text-mining/machine learning techniques or approach that will help me to find the most appropriate Tags for a given document without human interference.  </p>
","machine-learning, nlp, data-mining, tagging, text-mining","<p>One of the simple possible approaches: for a given document calculate <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">TF-IDF</a> measure for every word and choose top-N words as tags (or cut candidates by some threshold).
Also in your case it's reasonable to use empirical boosting coefficients for words in the Title and Category fields.</p>
",1,-3,337,2013-10-27 06:33:23,https://stackoverflow.com/questions/19615318/is-there-any-data-mining-text-mining-machine-learning-techniques-to-find-the-mos
Creating a Set is giving different output than expected,"<p>I am combining the processed data of two essays into one. I want to create a set two count how many different words are used as well as other analysis.  However, when I combine them, and do set(entire), I am returned with just a set of letters.  I have the code below as well as the output I am getting. I would like for the output to be all the words being used.</p>

<pre><code>print set(entire)
set([' ', '1', '0', '3', '2', '5', '4', '6', '9', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x'])



from __future__ import division
import nltk
import csv
import re
from string import punctuation
import enchant
from enchant.checker import SpellChecker

dictionary = enchant.Dict(""en_US"")
chkr = SpellChecker(""en_US"")

with open('2012ShortAnswers.csv', 'rb') as csvfile:
    data = csv.reader(csvfile, delimiter="","")

    writer = csv.writer(open('2012output.csv', 'wb'))

    for row in data:

        row3 = row[3]
        row3 = row3.lower().replace('  ', ' ')
        row4 = row[4]
        row4 = row4.lower().replace('  ', ' ')

        row3 = row3.replace('\n', '')
        row4 = row4.replace('\n', '')

        for p in list(punctuation):
            row3 = row3.replace(p, '')
            row4 = row4.replace(p, '')

        entire = row3 + row4
        set(entire)
</code></pre>
","python, csv, text-mining","<p><code>row3</code> and <code>row4</code> are strings.  At no point do you split them into words.  When you do <code>set</code> on a string, it makes a set of the characters in the string.</p>

<p>Perhaps try <code>row3 = row3.split()</code> and likewise for row4, then do <code>set(row3+row4)</code>.</p>

<p>That won't really fix it, though, since right now you aren't doing anything with that set.  You should create some other set outside the loop and add to it on each loop iteration.  Right now you create a set on each iteration but just throw it away.</p>
",0,0,58,2013-11-01 06:21:09,https://stackoverflow.com/questions/19721814/creating-a-set-is-giving-different-output-than-expected
c++: use map as value of another map,"<p>I just wonder if I can use a ""complicated"" map as the value of another map. I have self-defined several structs as follow:</p>

<pre><code>typedef std::vector&lt;std::string&gt; pattern;
typedef std::map&lt;int, std::vector&lt;pattern&gt;&gt; dimPatternsMap;
typedef std::map&lt;int, dimPatternsMap&gt; supportDimMapMap;
</code></pre>

<p>OK let me explain these things...<code>pattern</code> is a vector of strings. For the ""smaller"" map <code>dimPatternsMap</code>, the key is an integer which is the dimension of pattern (the size of that vector containing strings) and the value is vector containing patterns (which is a vector of vectors...).</p>

<p>The ""bigger"" map <code>supportDimMapMap</code> also use an integer as the key value, but use <code>dimPatternsMap</code> as its value. The key means ""support count"".</p>

<p>Now I begin to construct this ""complicated"" map:</p>

<pre><code>supportDimMapMap currReverseMap;
pattern p = getItFromSomePlace();  //I just omit the process I got pattern and its support
int support = getItFromSomePlaceToo();

if(currReverseMap.find(support) == currReverseMap.end()) {
    dimPatternsMap newDpm;

    std::vector&lt;pattern&gt; newPatterns;
    newPatterns.push_back(currPattern);
    newDpm[dim] = newPatterns;

    currReverseMap[support] = newDpm;

} else{
    dimPatternsMap currDpm = currReverseMap[support];

    if(currDpm.find(dim) == currDpm.end()) {
        std::vector&lt;pattern&gt; currDimPatterns;
        currDimPatterns.push_back(currPattern);

        currDpm[dim] = currDimPatterns;
    } else {
        currDpm[dim].push_back(currPattern);
    }
}
</code></pre>

<p>Forgive me the code is really a mass...</p>

<p>But then as I want to traverse the map like:</p>

<pre><code>for(supportDimMapMap::iterator iter = currReverseMap.begin(); iter != currReverseMap.end(); ++iter) {
        int support = iter-&gt;first;
        dimPatternsMap dpm = iter-&gt;second;

        for(dimPatternsMap::iterator ittt = dpm.begin(); ittt != dpm.end(); ++ittt) {
            int dim = ittt-&gt;first;
            std::vector&lt;pattern&gt; patterns = ittt-&gt;second;
            int s = patterns.size();
        }
}
</code></pre>

<p>I found the value <code>s</code> is always 1, which means that for each unique support value and for each dimension of that support value, there is only one pattern! But as I debug my code in the map constructing process, I indeed found that the size is not 1 - I actually added the new patterns into the map successfully...But when it comes to traversing, all the sizes become 1 and I don't know why...</p>

<p>Any suggestions or explanations will be greatly appreciated! Thanks!!</p>
","c++, vector, dictionary, text-mining, stdmap","<pre><code>dimPatternsMap currDpm = currReverseMap[support];
</code></pre>

<p><code>currDpm</code> is a copy of <code>currReverseMap[support]</code>.  It is not the same object.  So then when you make changes to <code>currDpm</code>, nothing within <code>currReverseMap</code> changes.</p>

<p>On the other hand, if you use a reference:</p>

<pre><code>dimPatternsMap&amp; currDpm = currReverseMap[support];
</code></pre>

<p>then <code>currDpm</code> and <code>currReverseMap[support]</code> really are the same object, so later statements using <code>currDpm</code> will really be changing a value within <code>currReverseMap</code>.</p>

<p>There are a few other places where your code could benefit from references too.</p>
",2,1,113,2013-11-02 23:31:51,https://stackoverflow.com/questions/19748121/c-use-map-as-value-of-another-map
Finding most frequent term in each document of a corpus,"<p>I've been using R's <code>tm</code> package with much success on classificaiton issues.  I know how to find the most frequent terms across the entire corpus (with <code>findFreqTerms()</code>), but don't see anything within the documentation that would find the most frequent term (after I've stemmed and removed stopwords, but before I remove sparse terms) in each individual document in the corpus.  I've tried using <code>apply()</code> and the <code>max</code> command, but this gives me the maximum number of times the term in each document occurs, not the name of the term itself. </p>

<pre><code>library(tm)

data(""crude"")
corpus&lt;-tm_map(crude, removePunctuation)
corpus&lt;-tm_map(corpus, stripWhitespace)
corpus&lt;-tm_map(corpus, tolower)
corpus&lt;-tm_map(corpus, removeWords, stopwords(""English""))
corpus&lt;-tm_map(corpus, stemDocument)
dtm &lt;- DocumentTermMatrix(corpus)
maxterms&lt;-apply(dtm, 1, max)
maxterms
127 144 191 194 211 236 237 242 246 248 273 349 352 
 5  13   2   3   3  10   8   3   7   9   9   4   5 
353 368 489 502 543 704 708 
 4   4   4   5   5   9   4 
</code></pre>

<p>Thoughts?</p>
","r, apply, text-mining, tm","<p>Ben's answer gives what you've asked for but I am not sure if what you asked for is wise.  It does not account for ties.  Here is an approach and a second one using <a href=""https://github.com/trinker/qdap"" rel=""nofollow"">the qdap package</a>.  They will give you lists with the words (in qdap's case a list of data frames with words and frequencies.  You can use <code>unlist</code> to get you the rest of the way with the first option and <code>lapply</code>, indexing and <code>unlist</code> with qdap.  The qdap approach works on the raw <code>Corpus</code>:</p>

<p><strong>Option #1:</strong>    </p>

<pre><code>apply(dtm, 1, function(x) unlist(dtm[[""dimnames""]][2], 
    use.names = FALSE)[x == max(x)])
</code></pre>

<p><strong>Option #2 with qdap:</strong>     </p>

<pre><code>library(qdap)
dat &lt;- tm_corpus2df(crude)
tapply(stemmer(dat$text), dat$docs, freq_terms, top = 1, 
    stopwords = tm::stopwords(""English""))
</code></pre>

<p>Wrapping the <code>tapply</code> with <code>lapply(WRAP_HERE, ""["", 1)</code> makes the two answers identical in content and nearly in format.</p>

<p><strong>EDIT:</strong> Added an example that is a leaner use of qdap:</p>

<pre><code>FUN &lt;- function(x) freq_terms(x, top = 1, stopwords = stopwords(""English""))[, 1]
lapply(stemmer(crude), FUN)

## [[1]]
## [1] ""oil""   ""price""
## 
## [[2]]
## [1] ""opec""
## 
## [[3]]
## [1] ""canada""   ""canadian"" ""crude""    ""oil""      ""post""     ""price""    ""texaco""  
## 
## [[4]]
## [1] ""crude""
## 
## [[5]]
## [1] ""estim""  ""reserv"" ""said""   ""trust"" 
## 
## [[6]]
## [1] ""kuwait"" ""said""  
## 
## [[7]]
## [1] ""report"" ""say""   
## 
## [[8]]
## [1] ""yesterday""
## 
## [[9]]
## [1] ""billion""
## 
## [[10]]
## [1] ""market"" ""price"" 
## 
## [[11]]
## [1] ""mln""
## 
## [[12]]
## [1] ""oil""
## 
## [[13]]
## [1] ""oil""   ""price""
## 
## [[14]]
## [1] ""oil""  ""opec""
## 
## [[15]]
## [1] ""power""
## 
## [[16]]
## [1] ""oil""
## 
## [[17]]
## [1] ""oil""
## 
## [[18]]
## [1] ""dlrs""
## 
## [[19]]
## [1] ""futur""
## 
## [[20]]
## [1] ""januari""
</code></pre>
",4,3,5159,2013-11-04 02:25:26,https://stackoverflow.com/questions/19760746/finding-most-frequent-term-in-each-document-of-a-corpus
Problems with TermDocumentMatrix function in R,"<p>I'm trying to create a <em>TermDocumentMatrix</em> using <em>tm</em> package, but seem to have encountered difficulties.</p>

<p>The input:</p>

<pre><code>trainDF&lt;-as.matrix(list(""I'm going home"", ""trying to fix this"", ""when I go home""))
</code></pre>

<p>Goal - creating a TDM from the input: (not all controls parameters listed below) </p>

<pre><code>control &lt;- list(
    weight= weightTfIdf, 
    removeNumbers=TRUE, 
    removeStopwords=TRUE, 
    removePunctuation=TRUE,    
    stemWords=TRUE, 
    maxWordLength=maxWordL,
    bounds=list(local=c(minDocFreq, maxDocFreq))
)

tdm&lt;- TermDocumentMatrix(Corpus(DataframeSource(trainDF)),control = control)
</code></pre>

<p>The error I get:</p>

<pre><code>Warning message:
In is.na(x) : is.na() applied to non-(list or vector) of type 'NULL'
</code></pre>

<p>And the tdm object is empty. Any ideas?</p>
","r, text-mining","<p>The error suggests something is wrong with your choice of minimum and maximum document frequency in the bounds.  For example, the following works:</p>

<pre><code>control=list(weighting = weightTfIdf,
             removeNumbers=TRUE, 
             removeStopwords=TRUE, 
             removePunctuation=TRUE, 
             bounds=list(local=c(1,3)))
tdm&lt;- TermDocumentMatrix(Corpus(DataframeSource(trainDF)), control=control)
</code></pre>

<p>Note that in the latest versions of TM, To specify a weighting you need to use <code>weighting = weightTfIdf</code> rather than <code>weight = weightTfIdf</code>.  Similarly, you should use <code>stemming=TRUE</code> in your control list to stem words.  I'm not sure that <code>maxWordLength</code> is an option currently.  TM will silently ignore invalid options in the control list, so you won't know that something is wrong until you go back to inspect the matrix.</p>
",3,1,2162,2013-11-05 13:57:09,https://stackoverflow.com/questions/19790721/problems-with-termdocumentmatrix-function-in-r
"tm: read in data frame, keep text id&#39;s, construct DTM and join to other dataset","<p>I'm using package tm.</p>

<p>Say I have a data frame of 2 columns, 500 rows.
The first column is ID which is randomly generated and has both character and number in it: ""txF87uyK""
The second column is actual text :  ""Today's weather is good. John went jogging. blah, blah,...""</p>

<p>Now I want to create a document-term matrix from this data frame. </p>

<p>My problem is I want to keep the ID information so that after I got the document-term matrix, I can join this matrix with another matrix that has each row being other information (date, topic, sentiment) of each document and each row is identified by document ID.</p>

<p>How can I do that? </p>

<p>Question 1: How do I convert this data frame into a corpus and get to keep ID information?</p>

<p>Question 2: After getting a dtm, how can I join it with another data set by ID?</p>
","r, text-mining, tm","<p>First, some example data from <a href=""https://stackoverflow.com/a/15506875/1036500"">https://stackoverflow.com/a/15506875/1036500</a></p>

<pre><code>examp1 &lt;- ""When discussing performance with colleagues, teaching, sending a bug report or searching for guidance on mailing lists and here on SO, a reproducible example is often asked and always helpful. What are your tips for creating an excellent example? How do you paste data structures from r in a text format? What other information should you include? Are there other tricks in addition to using dput(), dump() or structure()? When should you include library() or require() statements? Which reserved words should one avoid, in addition to c, df, data, etc? How does one make a great r reproducible example?""
examp2 &lt;- ""Sometimes the problem really isn't reproducible with a smaller piece of data, no matter how hard you try, and doesn't happen with synthetic data (although it's useful to show how you produced synthetic data sets that did not reproduce the problem, because it rules out some hypotheses). Posting the data to the web somewhere and providing a URL may be necessary. If the data can't be released to the public at large but could be shared at all, then you may be able to offer to e-mail it to interested parties (although this will cut down the number of people who will bother to work on it). I haven't actually seen this done, because people who can't release their data are sensitive about releasing it any form, but it would seem plausible that in some cases one could still post data if it were sufficiently anonymized/scrambled/corrupted slightly in some way. If you can't do either of these then you probably need to hire a consultant to solve your problem"" 
examp3 &lt;- ""You are most likely to get good help with your R problem if you provide a reproducible example. A reproducible example allows someone else to recreate your problem by just copying and pasting R code. There are four things you need to include to make your example reproducible: required packages, data, code, and a description of your R environment. Packages should be loaded at the top of the script, so it's easy to see which ones the example needs. The easiest way to include data in an email is to use dput() to generate the R code to recreate it. For example, to recreate the mtcars dataset in R, I'd perform the following steps: Run dput(mtcars) in R Copy the output In my reproducible script, type mtcars &lt;- then paste. Spend a little bit of time ensuring that your code is easy for others to read: make sure you've used spaces and your variable names are concise, but informative, use comments to indicate where your problem lies, do your best to remove everything that is not related to the problem. The shorter your code is, the easier it is to understand. Include the output of sessionInfo() as a comment. This summarises your R environment and makes it easy to check if you're using an out-of-date package. You can check you have actually made a reproducible example by starting up a fresh R session and pasting your script in. Before putting all of your code in an email, consider putting it on http://gist.github.com/. It will give your code nice syntax highlighting, and you don't have to worry about anything getting mangled by the email system.""
examp4 &lt;- ""Do your homework before posting: If it is clear that you have done basic background research, you are far more likely to get an informative response. See also Further Resources further down this page. Do help.search(keyword) and apropos(keyword) with different keywords (type this at the R prompt). Do RSiteSearch(keyword) with different keywords (at the R prompt) to search R functions, contributed packages and R-Help postings. See ?RSiteSearch for further options and to restrict searches. Read the online help for relevant functions (type ?functionname, e.g., ?prod, at the R prompt) If something seems to have changed in R, look in the latest NEWS file on CRAN for information about it. Search the R-faq and the R-windows-faq if it might be relevant (http://cran.r-project.org/faqs.html) Read at least the relevant section in An Introduction to R If the function is from a package accompanying a book, e.g., the MASS package, consult the book before posting. The R Wiki has a section on finding functions and documentation""
examp5 &lt;- ""Before asking a technical question by e-mail, or in a newsgroup, or on a website chat board, do the following:  Try to find an answer by searching the archives of the forum you plan to post to. Try to find an answer by searching the Web. Try to find an answer by reading the manual. Try to find an answer by reading a FAQ. Try to find an answer by inspection or experimentation. Try to find an answer by asking a skilled friend. If you're a programmer, try to find an answer by reading the source code. When you ask your question, display the fact that you have done these things first; this will help establish that you're not being a lazy sponge and wasting people's time. Better yet, display what you have learned from doing these things. We like answering questions for people who have demonstrated they can learn from the answers. Use tactics like doing a Google search on the text of whatever error message you get (searching Google groups as well as Web pages). This might well take you straight to fix documentation or a mailing list thread answering your question. Even if it doesn't, saying “I googled on the following phrase but didn't get anything that looked promising” is a good thing to do in e-mail or news postings requesting help, if only because it records what searches won't help. It will also help to direct other people with similar problems to your thread by linking the search terms to what will hopefully be your problem and resolution thread. Take your time. Do not expect to be able to solve a complicated problem with a few seconds of Googling. Read and understand the FAQs, sit back, relax and give the problem some thought before approaching experts. Trust us, they will be able to tell from your questions how much reading and thinking you did, and will be more willing to help if you come prepared. Don't instantly fire your whole arsenal of questions just because your first search turned up no answers (or too many). Prepare your question. Think it through. Hasty-sounding questions get hasty answers, or none at all. The more you do to demonstrate that having put thought and effort into solving your problem before seeking help, the more likely you are to actually get help. Beware of asking the wrong question. If you ask one that is based on faulty assumptions, J. Random Hacker is quite likely to reply with a uselessly literal answer while thinking Stupid question..., and hoping the experience of getting what you asked for rather than what you needed will teach you a lesson.""
</code></pre>

<p>Put example data in a data frame...</p>

<pre><code>df &lt;- data.frame(ID = sapply(1:5, function(i) paste0(sample(letters, 5), collapse = """")),
                 txt = sapply(1:5, function(i) eval(parse(text=paste0(""examp"",i))))
                 )
</code></pre>

<p><strong>Here is the answer to ""Question 1: How do I convert this data frame into a corpus and get to keep ID information?""</strong></p>

<p>Use <code>DataframeSource</code> and <code>readerControl</code> to convert data frame to corpus (from <a href=""https://stackoverflow.com/a/15693766/1036500"">https://stackoverflow.com/a/15693766/1036500</a>)...</p>

<pre><code>require(tm)
m &lt;- list(ID = ""ID"", Content = ""txt"")
myReader &lt;- readTabular(mapping = m)
mycorpus &lt;- Corpus(DataframeSource(df), readerControl = list(reader = myReader))

# Manually keep ID information from https://stackoverflow.com/a/14852502/1036500
for (i in 1:length(mycorpus)) {
  attr(mycorpus[[i]], ""ID"") &lt;- df$ID[i]
}
</code></pre>

<p>Now some example data for your second question...</p>

<p>Make Document Term Matrix from <a href=""https://stackoverflow.com/a/15506875/1036500"">https://stackoverflow.com/a/15506875/1036500</a>...</p>

<pre><code>skipWords &lt;- function(x) removeWords(x, stopwords(""english""))
funcs &lt;- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords)
a &lt;- tm_map(mycorpus, FUN = tm_reduce, tmFuns = funcs)
mydtm &lt;- DocumentTermMatrix(a, control = list(wordLengths = c(3,10)))
inspect(mydtm)
</code></pre>

<p>Make another example dataset to join to...</p>

<pre><code>df2 &lt;- data.frame(ID = df$ID,
                  date =  seq(Sys.Date(), length.out=5, by=""1 week""),
                  topic =   sapply(1:5, function(i) paste0(sample(LETTERS, 3), collapse = """")) ,
                  sentiment = sample(c(""+ve"", ""-ve""), 5, replace = TRUE)
                  )
</code></pre>

<p><strong>Here is the answer to ""Question 2: After getting a dtm, how can I join it with another data set by ID?""</strong></p>

<p>Use <code>merge</code> to join the dtm to example dataset of dates, topics, sentiment...</p>

<pre><code>mydtm_df &lt;- data.frame(as.matrix(mydtm))
# merge by row.names from https://stackoverflow.com/a/7739757/1036500
merged &lt;- merge(df2, mydtm_df, by.x = ""ID"", by.y = ""row.names"" )
head(merged)

      ID     date.x topic sentiment able actually addition allows also although
1 cpjmn 2013-11-07   XRT       -ve    0        0        2      0    0        0
2 jkdaf 2013-11-28   TYJ       -ve    0        0        0      0    1        0
3 jstpa 2013-12-05   SVB       -ve    2        1        0      0    1        0
4 sfywr 2013-11-14   OMG       -ve    1        1        0      0    0        2
5 ylaqr 2013-11-21   KDY       +ve    0        1        0      1    0        0
always answer answering answers anything archives are arsenal ask asked asking
1      1      0         0       0        0        0   1       0   0     1      0
2      0      0         0       0        0        0   0       0   0     0      0
3      0      8         2       3        1        1   0       1   2     1      3
4      0      0         0       0        0        0   0       0   0     0      0
5      0      0         0       0        1        0   0       0   0     0      0
</code></pre>

<p>There, now you have:</p>

<ol>
<li>Answers to your two questions (normally this site is just one question per... question)</li>
<li>Several kinds of example data that you can use when you ask your next question (makes your question a lot more engaging for folks who might want to answer)</li>
<li>Hopefully a sense that the answers to your questions can already be found elsewhere on the stackoverflow <a href=""/questions/tagged/r"" class=""post-tag"" title=""show questions tagged &#39;r&#39;"" rel=""tag"">r</a> tag, if you can think of how to break your questions down into smaller steps.</li>
</ol>

<p>If this <em>doesn't</em> answer your questions, ask another question and include code to reproduce your use-case as exactly as you can. If it <em>does</em> answer your question, then you should <a href=""https://meta.stackexchange.com/a/5235"">mark it as accepted</a> (at least until a better one comes along, eg. Tyler might pop in with a one-liner from his impressive <a href=""https://github.com/trinker/qdap"" rel=""nofollow noreferrer"">qdap</a> package...)</p>
",16,11,13164,2013-11-08 02:38:48,https://stackoverflow.com/questions/19850638/tm-read-in-data-frame-keep-text-ids-construct-dtm-and-join-to-other-dataset
How to scrape web content and then count frequencies of words in R?,"<p>This is my code:</p>

<pre><code>library(XML)
library(RCurl)
url.link &lt;- 'http://www.jamesaltucher.com/sitemap.xml'
blog &lt;- getURL(url.link)
blog          &lt;- htmlParse(blog, encoding = ""UTF-8"")
titles  &lt;- xpathSApply (blog ,""//loc"",xmlValue)             ## titles

traverse_each_page &lt;- function(x){
  tmp &lt;- htmlParse(x)
  xpathApply(tmp, '//div[@id=""mainContent""]')
}
pages &lt;- lapply(titles[2:3], traverse_each_page)
</code></pre>

<p>Here is the pseudocode:</p>

<ol>
<li>Take a xml document: <code>http://www.jamesaltucher.com/sitemap.xml</code></li>
<li>Go to each link</li>
<li>Parse the html content of each link</li>
<li>Extract the text inside <code>div id=""mainContent""</code></li>
<li>Count the frequencies of each word that appears for all the articles, case-insensitive.</li>
</ol>

<p><strong>I have managed to complete steps 1-4. I need some help with no. 5.</strong></p>

<p>Basically if the word ""the"" appears twice in article 1 and five times in article 2. I want to know that ""the"" appears a total of seven times in 2 articles.</p>

<p><strong>Also, I do not know how to view the contents I have extracted into <code>pages</code>. I want to learn how to view the contents which will make it easier for me to debug.</strong></p>
","r, web-scraping, text-mining, tm","<p>Here you go, start to finish. I changed your code for web-scraping so it gets less non-text stuff and then down the bottom is the word counts.</p>

<p>Here's your code for downloading the URLs...</p>

<pre><code>library(XML)
library(RCurl)
url.link &lt;- 'http://www.jamesaltucher.com/sitemap.xml'
blog &lt;- getURL(url.link)
blog          &lt;- htmlParse(blog, encoding = ""UTF-8"")
titles  &lt;- xpathSApply (blog ,""//loc"",xmlValue)             ## titles
</code></pre>

<p>I've changed your function to extract the text from each page...</p>

<pre><code>traverse_each_page &lt;- function(x){
  tmp &lt;- htmlParse(getURI(x))
  xpathSApply(tmp, '//div[@id=""mainContent""]', xmlValue)
}
pages &lt;- sapply(titles[2:3], traverse_each_page)
</code></pre>

<p>Let's remove newline and other non-text characters...</p>

<pre><code>nont &lt;- c(""\n"", ""\t"", ""\r"")
pages &lt;- gsub(paste(nont,collapse=""|""), "" "", pages)
</code></pre>

<p>Regarding your second question, to inspect the contents in <code>pages</code>, just type it at the console:</p>

<pre><code>pages
</code></pre>

<p>Now let's do your step 5 'Count the frequencies of each word that appears for all the articles, case-insensitive.'</p>

<pre><code>require(tm)
# convert list into corpus 
mycorpus &lt;- Corpus(VectorSource(pages))
# prepare to remove stopwords, ie. common words like 'the'
skipWords &lt;- function(x) removeWords(x, stopwords(""english""))
# prepare to remove other bits we usually don't care about
funcs &lt;- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
# do it
a &lt;- tm_map(mycorpus, FUN = tm_reduce, tmFuns = funcs)
# make document term matrix
mydtm &lt;- DocumentTermMatrix(a, control = list(wordLengths = c(3,10)))
</code></pre>

<p>Here's where you see the count of each word per document</p>

<pre><code>inspect(mydtm)
# you can assign it to a data frame for more convenient viewing
my_df &lt;- inspect(mydtm)
my_df
</code></pre>

<p>Here's how you count the total frequencies of each word that appears for all the articles, case-insensitive...</p>

<pre><code>apply(mydtm, 2, sum)
</code></pre>

<p>Does that answer your question? I guess that you're probably really only interested in the most frequent words (as @buruzaemon's answer details), or a certain sub-set of words, but that's another question...</p>
",5,3,3672,2013-11-08 04:33:46,https://stackoverflow.com/questions/19851655/how-to-scrape-web-content-and-then-count-frequencies-of-words-in-r
naiveBayes using a word matrix and 3+ classes for prediction,"<p>I'm having difficulty understanding A) the output of naiveBayes and B) the predict() function for naiveBayes. </p>

<p>This is not my data, but here's a fun example of what I'm trying to do and the errors I am getting:</p>

<pre><code>require(RTextTools)
require(useful)

script &lt;- data.frame(lines=c(""Rufus, Brint, and Meekus were like brothers to me. And when I say brother, I don't mean, like, an actual brother, but I mean it like the way black people use it. Which is more meaningful I think"",""If there is anything that this horrible tragedy can teach us, it's that a male model's life is a precious, precious commodity. Just because we have chiseled abs and stunning features, it doesn't mean that we too can't not die in a freak gasoline fight accident"",
                         ""Why do you hate models, Matilda"",""What is this? A center for ants? How can we be expected to teach children to learn how to read... if they can't even fit inside the building?"",""Look, I think I know what this is about and I'm complimented but not interested."",
                         ""Hi Derek! My name's Little Cletus and I'm here to tell you a few things about child labor laws, ok? They're silly and outdated. Why back in the 30s, children as young as five could work as they pleased; from textile factories to iron smelts. Yippee! Hurray!"",""Todd, are you not aware that I get farty and bloated with a foamy latte?"",""Oh, I'm sorry, did my pin get in the way of your ass? Do me a favor and lose five pounds immediately or get out of my building like now!"",
                         ""It's that damn Hansel! He's so hot right now!"",""Obey my dog!"",
                         ""I hear words like beauty and handsomness and incredibly chiseled features and for me that's like a vanity of self absorption that I try to steer clear of."",""Yeah, you're cool to hide here, but first me and him got to straighten some shit out."",
                         ""I wasn't like every other kid, you know, who dreams about being an astronaut, I was always more interested in what bark was made out of on a tree. Richard Gere's a real hero of mine. Sting. Sting would be another person who's a hero. The music he's created over the years, I don't really listen to it, but the fact that he's making it, I respect that. I care desperately about what I do. Do I know what product I'm selling? No. Do I know what I'm doing today? No. But I'm here, and I'm gonna give it my best shot."",""I totally agree with you. But how do you feel about male models?"",
                         ""So I'm rappelling down Mount Vesuvius when suddenly I slip, and I start to fall. Just falling, ahh ahh, I'll never forget the terror. When suddenly I realize Holy shit, Hansel, haven't you been smoking Peyote for six straight days, and couldn't some of this maybe be in your head?""))

people &lt;- as.factor(c(""Zoolander"",""Zoolander"",""Zoolander"",""Zoolander"",""Zoolander"",
                         ""Mugatu"",""Mugatu"",""Mugatu"",""Mugatu"",""Mugatu"",
                         ""Hansel"",""Hansel"",""Hansel"",""Hansel"",""Hansel""))

script.doc.matrix &lt;- create_matrix(script$lines,language = ""english"",removeNumbers=TRUE, removeStopwords = TRUE, stemWords=FALSE)
script.matrix &lt;- as.matrix(script.doc.matrix)

nb.script &lt;- naiveBayes(script.matrix,people)

nb.predict &lt;- predict(nb.script,script$lines)
nb.predict
</code></pre>

<p><strong><em>My questions:</em></strong> </p>

<p><strong>A)  naiveBayes output:</strong> </p>

<p>When I run </p>

<pre><code>nb.script$tables
</code></pre>

<p>I get tables such as this:</p>

<pre><code>$young
           young
people      [,1]   [,2]
  Hansel     0.0 0.0000000
  Mugatu     0.2 0.4472136
  Zoolander  0.0 0.0000000
</code></pre>

<p>How am I supposed to interpret this??? I thought these were supposed to be probabilities, but I don't understand what each column, [,1] &amp; [,2] mean.  Also, aren't the probabilities presented in these tables supposed to add up to 1.0?  Why don't they?  It would make sense if there was a third column, should there be?  </p>

<p>Should I be using <code>type=raw</code> in <code>naiveBayes()</code> perhaps?? </p>

<p><strong>B)  predict() of the naiveBayes:</strong>  </p>

<p>The output gives me Hansel as the prediction for every entry.  I believe this is happening simply because it is Alphabetically the first class.  In other instances in my predictions, if Hansel was listed 4x, Mugatu 6x, and Zoolander 5x, the predict() function would end up giving me Mugatu as the prediction for EVERY entry simply because it was listed the most times in the class vector. </p>

<p>edit:  for my question... how can I get the prediction to give me an ACTUAL prediction??? </p>

<p>Output of the prediction is as follows:</p>

<blockquote>
  <p>""> nb.predict</p>
  
  <p>[1] Hansel Hansel Hansel Hansel Hansel Hansel Hansel Hansel Hansel Hansel Hansel
  [12] Hansel Hansel Hansel Hansel</p>
  
  <blockquote>
    <p>Levels: Hansel Mugatu Zoolander</p>
  </blockquote>
</blockquote>

<p>Here is a link to a similar question:  <a href=""https://stackoverflow.com/questions/18257260/r-naives-bayes-classifier-bases-decision-only-on-a-priori-probabilities/18258454#18258454"">R: Naives Bayes classifier bases decision only on a-priori probabilities</a> 
However the answer isn't really helping me out too much. </p>

<p>Thanks in advance! </p>
","r, machine-learning, classification, text-mining","<p>For the first part of your question, the columns of your matrix <code>script.matrix</code> are numeric.  <code>naiveBayes</code> interprets numeric inputs as continuous data from a Gaussian distribution.  The tables you see in your answer give the sample mean (column 1) and standard deviation (column 2) for these numeric variables across the factor categories.  </p>

<p>What you probably want is to have naiveBayes recognize that your input variables are indicators.  A simple way to do that is to convert the entire <code>script.matrix</code> to a character matrix:</p>

<pre><code># Convert columns to characters    
script.matrix &lt;- apply(as.matrix(script.doc.matrix),2,as.character)
</code></pre>

<p>With this change:</p>

<pre><code>&gt; nb.predict &lt;- predict(nb.script,script$lines)
&gt; nb.script$tables$young
           young
people        0   1
  Hansel    1.0 0.0
  Mugatu    0.8 0.2
  Zoolander 1.0 0.0
</code></pre>

<p>To see the predicted classes:</p>

<pre><code>&gt; nb.predict &lt;- predict(nb.script, script.matrix)
&gt; nb.predict
 [1] Zoolander Zoolander Zoolander Zoolander Zoolander Mugatu    Mugatu   
 [8] Mugatu    Mugatu    Mugatu    Hansel    Hansel    Hansel    Hansel   
[15] Hansel   
Levels: Hansel Mugatu Zoolander
</code></pre>

<p>To see the raw probabilities from the naiveBayes fit:</p>

<pre><code>predict(nb.script, script.matrix, type='raw')
</code></pre>
",3,1,1392,2013-11-18 13:07:48,https://stackoverflow.com/questions/20048603/naivebayes-using-a-word-matrix-and-3-classes-for-prediction
Information extraction with Python using huge list of entity names,"<p>I have a large collection of multilingual html files from which I'd like to extract structured data. I also have huge list (+5M) of entity names occurring in the corpus (multi-word: persons &amp; organisation names, places,...) that can be of help. </p>

<p>I'm looking for a Python library that can do fast tagging of text with entity names (and perhaps but not necessary do other task like POS tagging and elementary NER). The result should be searchable with simple REGEXP like expression augmented with tags. For example: "".+? [last_name] (is|was)( best)? CEO of [organisation_name]"".</p>

<p>I've tried to find this functionality in NLTK and CLIPS pattern (pattern.search is similar) but failed. The closest open source library with such functionality is GATE but it is in Java and seems like overkill for this task.</p>

<p>Thanks, </p>

<p>Davor</p>
","python, regex, text-mining, information-extraction","<p>You can try htql.RegEx from <a href=""http://htql.net"" rel=""nofollow"">http://htql.net</a>.  Here is the example from the website: </p>

<pre><code>import htql; 
address = '88-21 64th st , Rego Park , New York 11374'
states=['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 
    'Delaware', 'District Of Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 
    'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 
    'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 
    'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 
    'Oregon', 'PALAU', 'Pennsylvania', 'PUERTO RICO', 'Rhode Island', 'South Carolina', 'South Dakota', 
    'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 
    'Wyoming']; 

a=htql.RegEx(); 
a.setNameSet('states', states);

state_zip1=a.reSearchStr(address, ""&amp;[s:states][,\s]+\d{5}"", case=False)[0]; 
# state_zip1 = 'New York 11374'

state_zip2=a.reSearchList(address.split(), r""&amp;[ws:states]&lt;,&gt;?&lt;\d{5}&gt;"", case=False)[0]; 
# state_zip2 = ['New', 'York', '11374']
</code></pre>
",0,2,601,2013-11-19 21:41:05,https://stackoverflow.com/questions/20082664/information-extraction-with-python-using-huge-list-of-entity-names
Using Sklearn&#39;s TfidfVectorizer transform,"<p>I am trying to get the tf-idf vector for a single document using Sklearn's TfidfVectorizer object. I create a vocabulary based on some training documents and use fit_transform to train the TfidfVectorizer. Then, I want to find the tf-idf vectors for any given testing document. </p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

self.vocabulary = ""a list of words I want to look for in the documents"".split()
self.vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word', 
                 stop_words='english')
self.vect.fit_transform(self.vocabulary)

...

doc = ""some string I want to get tf-idf vector for""
tfidf = self.vect.transform(doc)
</code></pre>

<p>The problem is that this returns a matrix with n rows where n is the size of my doc string. I want it to return just a single vector representing the tf-idf for the entire string. How can I make this see the string as a single document, rather than each character being a document? Also, I am very new to text mining so if I am doing something wrong conceptually, that would be great to know. Any help is appreciated.</p>
","python, document, text-mining, tf-idf","<p>If you want to compute tf-idf only for a given vocabulary, use <code>vocabulary</code> argument to <code>TfidfVectorizer</code> constructor, </p>

<pre><code>vocabulary = ""a list of words I want to look for in the documents"".split()
vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word', 
           stop_words='english', vocabulary=vocabulary)
</code></pre>

<p>Then, to fit, i.e. calculate counts, with a given <code>corpus</code>, i.e. an iterable of documents, use <code>fit</code>:</p>

<pre><code>vect.fit(corpus)
</code></pre>

<p>Method <code>fit_transform</code> is a shortening for</p>

<pre><code>vect.fit(corpus)
corpus_tf_idf = vect.transform(corpus) 
</code></pre>

<p>Last, <code>transform</code> method accepts a corpus, so for a single document, you should pass it as list, or it is treated as iterable of symbols, each symbol being a document.</p>

<pre><code>doc_tfidf = vect.transform([doc])
</code></pre>
",48,37,59425,2013-11-21 21:18:43,https://stackoverflow.com/questions/20132070/using-sklearns-tfidfvectorizer-transform
Identify which keywords can be found in which files,"<h2>The Problem</h2>

<p>Suppose I have a text file containing a list of words. Each word appears on a separate line. Let's take the following as an example and we'll call it ""my_dictionary_file"":</p>

<p><strong>my_dictionary_file.txt</strong></p>

<pre><code>Bill
Henry
Martha
Sally
Alex
Paul
</code></pre>

<p>In my current directory, I have several files which contain the above names. The problem is that I do not know which files contain which names. This is what I'd like to find out; a sort of matching game. In other words, I want to match each name in my_dictionary_file.txt to the file in which the name appears.</p>

<p>As an example, let's say that the files in my working directory look like the following:</p>

<p><strong>file1.txt</strong></p>

<pre><code>There is a man called Bill. He is tall.
</code></pre>

<p><strong>file2.txt</strong></p>

<pre><code>There is a girl called Martha. She is small.
</code></pre>

<p><strong>file3.txt</strong></p>

<pre><code>Henry and Sally are a couple.
</code></pre>

<p><strong>file4.txt</strong></p>

<pre><code>Alex and Paul are two bachelors.
</code></pre>

<hr>

<h2>What I've tried</h2>

<p><strong>First.</strong> Using the <a href=""http://unixhelp.ed.ac.uk/CGI/man-cgi?fgrep"" rel=""nofollow"">fgrep</a> command with the -o and -f options, </p>

<pre><code>$ fgrep -of my_dictionary_file.txt file1.txt
Bill
</code></pre>

<p>I can identify that the name Bill can be found in file1.txt.</p>

<p><strong>Second.</strong> Using the fgrep command with the -r -l and -f options,</p>

<pre><code>$ fgrep -rlf names.txt .
./names.txt
./file1.txt
./file4.txt
./file3.txt
./file2.txt
</code></pre>

<p>I can search through all of the files in the current directory to find out if the files contain the list of names in my_dictionary_file.txt</p>

<hr>

<h2>The sought-after solution</h2>

<p>The solution that I am looking for would be along the lines of combining both of the two attempts above. To be more explicit, I'd like to know that:</p>

<pre><code>Bill belongs to file1.txt
Martha belongs to file2.txt
Henry and Sally belong to file3.txt
Alex and Paul belong to file4.txt
</code></pre>

<p>Any suggestions or pointers towards commands other than fgrep would be greatly appreciated!</p>

<hr>

<h2>Note</h2>

<p>The actual problem that I am trying to solve is a scaled up version of this simplified example. I'm hoping to base my answer on responses to this question, so bear in mind that in reality the dictionary file contains hundreds of names and that there are a hundred or more files in the current directory. </p>

<p>Typing </p>

<pre><code>$ fgrep -of my_dictionary_file.txt file1.txt
Bill

$ fgrep -of my_dictionary_file.txt file2.txt
Martha

$ fgrep -of my_dictionary_file.txt file3.txt
Henry Sally

$ fgrep -of my_dictionary_file.txt file4.txt
Alex Paul
</code></pre>

<p>does, of course, get me the results, but I'm looking for an efficient method to collect the results for me - perhaps, pipe the results to a single .txt file.</p>
","linux, grep, text-files, text-mining","<p>If you fgrep all the files at once with the -o option, fgrep should print both the file name and the text that matched:</p>

<pre><code>$ fgrep -of dict.txt file*.txt
file1.txt:Bill
file2.txt:Martha
file3.txt:Henry
file3.txt:Sally
file4.txt:Alex
file4.txt:Paul
</code></pre>
",1,0,45,2013-12-01 00:13:27,https://stackoverflow.com/questions/20307235/identify-which-keywords-can-be-found-in-which-files
How to custom tag word(s) in GATE JAPE grammar?,"<p>I have a set of documents and each document has different heading. Example if document heading says ""Psychological Evaluation"" I want to tag the document as ""Medicalrule"".</p>

<ol>
<li>I loaded the document and loaded ANNIE with defaults.</li>
<li>In Processing Resources > New > Jape Transducer 
2.1 wrote the following code in the text document and saved it as .JAPE extension</li>
</ol>

<p>CODE :</p>

<hr>

<pre><code>Phase: ConjunctionIdentifier
Input: Token Split    
Rule: Medicalrule
(
({Token.string==""Psychological""})+({Token.string == "" ""})+ ({Token.string == ""Evaluation""}):Meddoc({Token.kind==""word""})
)

--&gt; 
:Meddoc
  {
    gate.AnnotationSet matchedAnns= (gate.AnnotationSet) bindings.get(""Meddoc""); gate.FeatureMap newFeatures= Factory.newFeatureMap();newFeatures.put(""rule"",""Medicalrule"");annotations.add(matchedAnns.firstNode(),matchedAnns.lastNode(),""CC"", newFeatures);
 }
</code></pre>

<hr>

<ol>
<li>Loaded the above created  .JAPE file and reinitialized</li>
</ol>

<p>After the application is run the Annotation Set does not show the tag !</p>

<p>Am I doing wrong somewhere ?It would be great if someone could help me on this.</p>

<p>Appreciate your time.</p>

<p>Thank you</p>
","grammar, text-mining, gate","<p>There are three issues I can see here.</p>

<ul>
<li>First, as ashingel says, spaces are not represented as <code>Token</code> annotations - this is deliberate as in most cases you don't care about the spacing between words, only the words themselves.</li>
<li>Second, the trailing <code>({Token.kind==""word""})</code> means that the rule will only match when ""Psychological Evaluation"" is followed by another word before the end of the current sentence (because you've got <code>Split</code> in the Input line).</li>
<li>Third, you're only binding the <code>Meddoc</code> label to the ""Evaluation"" token, not to the whole match.</li>
</ul>

<p>I would try and simplify the LHS of the rule:</p>

<pre><code>Phase: ConjunctionIdentifier
Input: Token Split    
Rule: Medicalrule
(
  {Token.string==""Psychological""}
  {Token.string == ""Evaluation""}
):meddoc
</code></pre>

<p>and for the RHS (a) you don't need to do the explicit <code>bindings.get</code> because you've used a labelled block so you already have the bound annots available, (b) you should use <code>outputAS</code> instead of <code>annotations</code>, and (c) you should generally avoid the <code>add</code> method that takes nodes, as it isn't safe if the input and output annotation sets are different.  If you're using a recent snapshot of GATE then the <code>gate.Utils</code> static methods can help you a lot here</p>

<pre class=""lang-java prettyprint-override""><code>:meddoc {
    Utils.addAnn(outputAS, meddocAnnots,""CC"",
                 Utils.featureMap(""rule"",""Medicalrule""));
}
</code></pre>

<p>If you're using 7.1 or earlier then the <code>addAnn</code> method isn't available so it's slightly more convoluted:</p>

<pre class=""lang-java prettyprint-override""><code>:meddoc {
  try {
    outputAS.add(Utils.start(meddocAnnots), Utils.end(meddocAnnots),""CC"",
                 Utils.featureMap(""rule"",""Medicalrule""));
  } catch(InvalidOffsetException e) { // can't happen, but won't compile without
    throw new JapeException(e);
  }
}
</code></pre>

<p>Finally, just to check, you did definitely add your new JAPE Transducer PR to the end of the pipeline?</p>
",1,1,1719,2013-12-02 17:31:09,https://stackoverflow.com/questions/20334315/how-to-custom-tag-words-in-gate-jape-grammar
Getting count of keywords using tm package in R,"<p>I'm trying to get a count of the keywords in my corpus using the R ""tm"" package. This is my code so far: </p>

<pre><code># get the data strings
f&lt;-as.vector(forum[[1]])

# replace +
f&lt;-gsub(""+"", "" "", f ,fixed=TRUE)

# lower case
f&lt;-tolower(f)

# show all strings that contain mobile
mobile&lt;- f[grep(""mobile"", f, ignore.case = FALSE, perl = FALSE, value = FALSE,
     fixed = FALSE, useBytes = FALSE, invert = FALSE)]
text.corp.mobile &lt;- Corpus(VectorSource(mobile))
text.corp.mobile &lt;- tm_map(text.corp.mobile , removePunctuation) 
text.corp.mobile &lt;- tm_map(text.corp.mobile , removeWords, c(stopwords(""english""),""mobile"")) 
dtm.mobile &lt;- DocumentTermMatrix(text.corp.mobile)
dtm.mobile 
dtm.mat.mobile &lt;- as.matrix(dtm.mobile)
dtm.mat.mobile
</code></pre>

<p>This returns a table with binary results of weather a keyword appeared in one of the corpus texts or not. 
Instead of getting the final result in a binary form I would like to get a count for each keyword. For example: 
'car' appeared 5 times
'button' appeared 9 times</p>
","r, text-mining, tm, text-analysis","<p>without seeing your actual data, its a bit hard to tell but since you just called <code>DocumentTermMatrix</code> I would try something like this:</p>

<pre><code>dtm.mat.mobile &lt;- as.matrix(dtm.mobile)
word.freqs &lt;- sort(rowSums(dtm.mat.mobile), decreasing=TRUE)
</code></pre>
",1,2,844,2013-12-20 20:54:43,https://stackoverflow.com/questions/20711792/getting-count-of-keywords-using-tm-package-in-r
How to extract a list of strings out of single string by using regex?,"<p>I have a long string that is actually a set of concepts. I want to mine the string and to create a list of concepts.</p>

<p>The string begins with:</p>

<p><code>Abduction and retroduction Action research: a case study Analysis of variance (ANOVA) Attitudes Autobiography see Biographical method...</code></p>

<p>The list contains dictionary entries. In vast majority of cases the capital letters mark the beginning of new entry. I want to make a list of entries.</p>

<p>I have tried <code>re.findall(r""([A-Z].+?)\s[A-Z]"")</code>. But it filters out every second entry. Instead of [""Abduction and retroduction"", ""Action research: a case study"", ""Analysis of variance (ANOVA)""] I get: [""Abduction and retroduction"", ""Analysis of variance (ANOVA)""]</p>
","python, regex, string, list, text-mining","<p>By default you can have overlapping results, it is the reason why all second contiguous entry is skipped (since you match his first letter). A way to avoid this problem is to not match this first letter by using a lookahead assertion <code>(?=..)</code> that means ""followed by"" <i>(A lookahead is only a check and matches nothing)</i>:</p>

<pre><code>re.findall(r""(\b[A-Z].+?)(?=\s[A-Z]|\s*$)"")
</code></pre>
",1,0,90,2013-12-22 15:29:26,https://stackoverflow.com/questions/20730950/how-to-extract-a-list-of-strings-out-of-single-string-by-using-regex
R tm package and cyrillic text,"<p>I am trying to do some text mining with russian text using tm package and have some issues.</p>

<p>preprocessing speed heavily depends on encoding.</p>

<pre><code>library(tm)
rus_txt&lt;-paste(readLines('http://lib.ru/LITRA/PUSHKIN/dubrowskij.txt',encoding='cp1251'), collapse=' ')
object.size(rus_txt)
eng_txt&lt;-paste(readLines('http://www.gutenberg.org/cache/epub/1112/pg1112.txt',encoding='UTF-8'), collapse=' ')
object.size(eng_txt)
# text sizes nearly identical
rus_txt_utf8&lt;-iconv(rus_txt, to='UTF-8')
system.time(rus_txt_lower&lt;-tolower(rus_txt_utf8))
#3.17         0.00         3.19 
system.time(rus_txt_lower&lt;-tolower(eng_txt))
#0.03         0.00         0.03
system.time(rus_txt_lower&lt;-tolower(rus_txt))
#0.07         0.00         0.08
</code></pre>

<p>40 times faster! and on large corporas difference was up to 500 times!</p>

<p>Lets try to tokenize some text (this function used in TermDocumentMatrix):</p>

<pre><code>some_text&lt;-""Несколько  лет  тому  назад  в  одном  из своих  поместий жил старинный
русской барин, Кирила Петрович Троекуров. Его богатство, знатный род и связи
давали ему большой вес в губерниях, где  находилось его имение.  Соседи рады
были угождать малейшим его прихотям; губернские чиновники трепетали  при его
имени;  Кирила  Петрович принимал знаки  подобострастия как надлежащую дань;
дом его  всегда был полон  гостями, готовыми тешить  его барскую праздность,
разделяя  шумные,  а  иногда  и  буйные  его  увеселения.  Никто  не  дерзал
отказываться от его приглашения, или в известные  дни не являться  с должным
почтением в село  Покровское.""
scan_tokenizer(some_text)
#[1] ""Несколько""  ""лет""        ""тому""       ""назад""      ""в""          ""одном""      ""из""         ""своих""     
# [9] ""поместий""   ""жил""        ""старинный""  ""русской""    ""барин,""     ""Кирила""     ""Петрович""   ""Троекуров.""
#[17] ""Его""        ""богатство,"" ""знатный""    ""род""        ""и""          ""св"" 
</code></pre>

<p>oops... Seems R core function scan() see russian lower case letter 'я' as EOF. I tried diffrent encodings but I haven't answer how to fix this.</p>

<p>Ok lets try to remove punctuation:</p>

<pre><code>removePunctuation(""жил старинный русской барин, Кирила Петрович Троекуров"")
#""жил старинный русской барин Кирила Петрови Троекуров""
</code></pre>

<p>Hmm...where is letter 'ч'? Ok with UTF-8 encoding this works fine, but it took some time to found it.
 also I had issue with removeWords() function perfomance but can't reproduce it.
<strong>Main question is: How to read and tokenize texts with letter 'я'?</strong>
my locale:</p>

<pre><code>Sys.getlocale()
#[1] ""LC_COLLATE=Russian_Russia.1251;LC_CTYPE=Russian_Russia.1251;LC_MONETARY=Russian_Russia.1251;LC_NUMERIC=C;LC_TIME=Russian_Russia.1251""
</code></pre>
","r, encoding, text-mining, tm","<p><strong>1) Question</strong>: How to read and tokenize texts with letter 'я'? <strong>Answer</strong>: try to write your own tokenizer and use it. For example:</p>

<pre><code>my_tokenizer &lt;- function (x) 
{
  strsplit(iconv(x, to='UTF-8'), split='([[:space:]]|[[:punct:]])+', perl=F)[[1]]
}
 TDM &lt;- TermDocumentMatrix(corpus,control=list(tokenize=my_tokenizer, weighting=weightTf, wordLengths = c(3,10)))
</code></pre>

<p><strong>2)</strong> Performance heavily depend on... performance of <strong>tolower function</strong>. May be this is a bug, I don't know, but on every time you call it you have to convert your text into native encoding using enc2native. (of course if your text language is not english).</p>

<pre><code>doc.corpus &lt;- Corpus(VectorSource(enc2native(textVector)))
</code></pre>

<p>And moreover after all text preprocessing on your corpus you have to convert it again. (this is because TermDocumentMatrix and many other function in <strong>tm</strong> package internally use <strong>tolower</strong>)</p>

<p><code>tm_map(doc.corpus, enc2native)</code></p>

<p>So your full flow will look like something like this:</p>

<pre><code>createCorp &lt;-function(textVector)
{
  doc.corpus &lt;- Corpus(VectorSource(enc2native(textVector)))
  doc.corpus &lt;- tm_map(doc.corpus, tolower)
  doc.corpus &lt;- tm_map(doc.corpus, removePunctuation)
  doc.corpus &lt;- tm_map(doc.corpus, removeWords, stopwords(""russian""))
  doc.corpus &lt;- tm_map(doc.corpus, stemDocument, ""russian"")
  doc.corpus &lt;- tm_map(doc.corpus, stripWhitespace)
  return(tm_map(doc.corpus, enc2native))
}
my_tokenizer &lt;- function (x) 
{
  strsplit(iconv(x, to='UTF-8'), split='([[:space:]]|[[:punct:]])+', perl=F)[[1]]
}
TDM &lt;- TermDocumentMatrix(corpus,control=list(tokenize=my_tokenizer, weighting=weightTf, wordLengths = c(3,10)))
</code></pre>
",2,3,2741,2013-12-26 08:16:29,https://stackoverflow.com/questions/20781499/r-tm-package-and-cyrillic-text
JButton run main method,"<p>I'm doing some text mining application. It consists of <code>TextRazor</code> API Java Swing. How can I use <code>JButton</code> to run <code>main()</code> class? Once the button is clicked, the code in <code>main()</code> class must be triggered. Below is the code, please help me.</p>

<pre><code>private void jButton4ActionPerformed(java.awt.event.ActionEvent evt) {   
    //I want the main class to be called here** 
}
private void jButton5ActionPerformed(java.awt.event.ActionEvent evt) {         
    JOptionPane.showMessageDialog(null, ""Completed Analysis!"",""Alert"", 1);
    jButton5.setEnabled(false);
    jTextArea2.setEditable(false);
    jTextArea3.setEditable(false);
}   
    /**
     * @param args
     * @throws NetworkException 
     */
public static void main(String[] args) throws NetworkException, AnalysisException {

    // Sample request, showcasing a couple of TextRazor features
    String API_KEY = ""7d5066bec76cb47f4eb4e557c60e9b979f9a748aacbdc5a44ef9375a"";

    TextRazor client = new TextRazor(API_KEY);

    client.addExtractor(""words"");
    client.addExtractor(""entities"");
    client.addExtractor(""entailments"");
    client.addExtractor(""senses"");
    client.addExtractor(""entity_companies"");

    String rules = ""entity_companies(CompanyEntity) :- entity_type(CompanyEntity, 'Company')."";

    client.setRules(rules);

    AnalyzedText response = client.analyze(""Barclays misled shareholders and the public RBS about one of the biggest investments in the bank's history, a BBC Panorama investigation has found."");

    for (Sentence sentence : response.getResponse().getSentences()) {
        for (Word word : sentence.getWords()) {
            System.out.println(""----------------"");
            System.out.println(""Word: "" + word.getLemma());

            for (Entity entity : word.getEntities()) {
                System.out.println(""Matched Entity: "" + entity.getEntityId());
            }
            for (Sense sense: word.getSenses()) {
                System.out.println(""Word sense: "" + sense.getSynset() + "" has score: "" + sense.getScore());
            }                
        }
    }

    // Use a custom rule to match 'Company' type entities

    for (Custom custom : response.getResponse().getCustomAnnotations()) {
        for (Custom.BoundVariable variable : custom.getContents()) {
            if (null != variable.getEntityValue()) {
                for (Entity entity : variable.getEntityValue()) {
                    System.out.println(""Variable: "" + variable.getKey() + "" Value:"" + entity.getEntityId());
                }
            }
        }
    }           
}
</code></pre>
","java, swing, text-mining","<p>Main method in Class is also a normal method, which is designed to start the java application by JVM. But, you can also call it in your method</p>

<pre><code>private void jButton5ActionPerformed(java.awt.event.ActionEvent evt) {         
    JOptionPane.showMessageDialog(null, ""Completed Analysis!"",""Alert"", 1);
    jButton5.setEnabled(false);
    jTextArea2.setEditable(false);
    jTextArea3.setEditable(false);
    ClassName.main(new String[]{""arg1"",""arg2""}); 
}  
</code></pre>

<p>added dummy arguments to just invoke the main method</p>
",4,0,1480,2013-12-31 07:48:03,https://stackoverflow.com/questions/20853830/jbutton-run-main-method
"R, merge multiple rows of text data frame into one cell","<p>I have a text data frame that looks like below.</p>

<pre><code>&gt; nrow(gettext.df)
[1] 3

&gt; gettext.df
 gettext
 1 hello,
 2 Good to hear back from you.
 3 I've currently written an application and I'm happy about it
</code></pre>

<p>I wanted to merge this text data into one cell (to do sentiment analysis) as below </p>

<pre><code>&gt; gettext.df
  gettext
  1 hello, Good to hear back from you. I've currently written an application and I'm happy about it
</code></pre>

<p>so I collapsed the cell using below code   </p>

<pre><code>paste(gettext.df, collapse ="" "")
</code></pre>

<p>but it seems like it makes those text data into one chunk (as one word) so I cannot scan the sentence word by word.</p>

<p>Is there any way that I can merge those sentence as a collection of sentences, without transforming  as one big word chunk?</p>
","r, merge, text-mining","<p>You have to transform the data frame column into a character vector before using <code>paste</code>. </p>

<pre><code>paste(unlist(gettext.df), collapse ="" "")
</code></pre>

<p>This returns:</p>

<pre><code>[1] ""hello, Good to hear back from you. I've currently written an application and I'm happy about it""
</code></pre>
",25,9,23951,2013-12-31 08:59:02,https://stackoverflow.com/questions/20854615/r-merge-multiple-rows-of-text-data-frame-into-one-cell
Get minimal shared part between elements of string&#39;s vector,"<p>Having a list of vector of strings:</p>

<pre><code>xx &lt;- c(""concord wanderer basic set air snug beige"",
  ""concord wanderer basic set air snug black noir"", 
  ""concord wanderer basic set air snug blue bleu"", 
  ""concord wanderer basic set air snug brown marron"", 
  ""concord wanderer basic set air snug green vert"", 
   ""concord wanderer basic set air snug grey gris"", 
   ""concord wanderer basic set air snug red rouge"", 
   ""concord wanderer basic set air snug rose"" )
</code></pre>

<p>I tried to get minimal shared part between elements of the vector, for example, here I should get:</p>

<pre><code>""concord wanderer basic set air snug""
</code></pre>

<p>xx is a result of a previous process, so I am sure that there is a shared part between the elements. But the removed part is not always at the end of he strings.</p>

<p>Using <code>strsplit</code> and <code>`table</code> I get this partial solution, but it is a little bit tricky and I loose the original order of words:     </p>

<pre><code>table_x &lt;- table(unlist(strsplit(xx,' ')))
paste(names(table_x[table_x==max(table_x)]),collapse=' ')
[1] ""air basic concord set snug wanderer""
</code></pre>

<p>I am pretty sure that there is better solution. I tried with <code>agrep</code> or <code>adist</code> but without a lot of success.</p>
","string, r, intersection, text-mining, fuzzy-search","<p>You could use <code>intersect</code> with <code>Reduce</code> to get the output you want.</p>

<pre><code>paste(Reduce(intersect, strsplit(xx, "" "")), collapse="" "")
#[1] ""concord wanderer basic set air snug""
</code></pre>
",7,4,109,2014-01-01 19:45:32,https://stackoverflow.com/questions/20872502/get-minimal-shared-part-between-elements-of-strings-vector
Best way to detect features based on text,"<p>I have a ""simple"" problem: I have text sections and based on this it should be decided its whether ""Category A"" or ""Category B"".</p>

<p>As training data I have classified sections of text, which the algorithm can be trained.</p>

<p>The text sections look something like this:</p>

<h2>Category A</h2>

<pre><code>a blue car drives
</code></pre>

<p>or</p>

<pre><code>the blue bus stops
</code></pre>

<p>or</p>

<pre><code>the blue bike drives
</code></pre>

<h2>Category B</h2>

<pre><code> a red bike drives
</code></pre>

<p>or</p>

<pre><code> the red bus stops
</code></pre>

<p>(The section text contains up to 20 words and the vary is massive)</p>

<p>If I have trained the algorithm with this example data, it should decide if text contains ""blue"" it's Categorie A, if its contains ""red"" it's Categorie B and so on.</p>

<p>The algorithm should learn based on training data if the frequency of a word is it likely more Category A or B. </p>

<p>Whats the best way to do this and which tool should I use?</p>
","machine-learning, classification, text-mining","<p>You can try <a href=""https://www.inkling.com/read/program-collective-intelligence-toby-segaran-1st/chapter-6/the-fisher-method"" rel=""nofollow"">Fisher method</a>, in which the probability of both positive (A) and negative (B) for each feature word (red, blue) in the document are calculated. The probability that a sentence with each of the two specified word (red, blue) belongs in the specified category (A, B) is obtained, assuming there will be an equal number of items in each category. Then a combined probability is obtained.</p>

<p>Since the features are not independent, this won’t be a real probability, but it works much like the Bayesian classifier. The value returned by the Fisher method is a much better estimate of probability, which can be very useful when reporting results or deciding cutoffs.</p>
",0,0,64,2014-01-02 13:04:00,https://stackoverflow.com/questions/20883640/best-way-to-detect-features-based-on-text
Can we grouping Annotations in GATE,"<p><strong>How can we group all annotations between two annotations?</strong></p>

<p>I'm new to GATE and am trying to group annotations together , Not sure if we can do this , Please help. 
For Example In the following text :</p>

<pre><code>Page-1
Age:53 
Person: Nathan

Page-2
Treatment : Initial Evaluation
History: Yes

Page-3
..........
</code></pre>

<p>If my Gazetteer list consists of different tags, page tag for each page number, age, person, Treatment, History etc. I want to group all tags from Page-1 to Page-2 under Page-1 Annotation and all tags between Page-2 and Page-3 under Page-2.</p>

<p>Please let me know if more information required on this question.</p>

<p>Thanks in advance.</p>
","java, text-mining, gate","<p>I'm not entirely sure what you mean by ""group together"" but you can certainly create annotations that span across the content of each ""page"".  Assuming you have a <code>PageNumber</code> annotation on each ""Page-1"", ""Page-2"" etc. then you can use something like this to create annotations spanning from one <code>PageNumber</code> to the next.  I'm using a <code>control = once</code> JAPE to do this, you could equivalently use a Groovy script or a custom PR</p>

<pre><code>Imports: { import static gate.Utils.*; }
Phase: PageSpans
Input: PageNumber
Options: control = once

Rule: PageSpan
({PageNumber})
--&gt;
{
  try {
    List&lt;Annotation&gt; numbers = inDocumentOrder(inputAS.get(""PageNumber""));
    for(int i = 0; i &lt; numbers.size(); i++) {
      outputAS.add(start(numbers.get(i)), // from start of this PageNumber, to...
                   (i+1 &lt; numbers.size()
                     ? start(numbers.get(i+1)) // start of the next number, or...
                     : end(doc) // ...if no more PageNumbers then end of document
                   ),
                   ""Page"",
                   // store the text under the PageNumber as a feature of Page
                   featureMap(""id"", stringFor(doc, numbers.get(i))));
    }
  } catch(InvalidOffsetException e) {
    throw new JapeException(""Invalid offset from existing annotation"", e);
  }
}
</code></pre>

<p>In your comment you ask about moving all the annotations under each ""page"" into a separate annotation set.  This would be relatively straightforward once you have done the above, and if you have the page number as a feature on your <code>Page</code> annotations as I have done with the ""id"" feature.  Then you could define another JAPE that does something like this:</p>

<pre><code>Imports: { import static gate.Utils.*; }
Phase: SetPerPage
Input: Age X Y // and whatever other annotation types you want to copy
Options: control = all

Rule: MoveToPageSet
({Age}|{X}|{Y}):entity
--&gt;
:entity {
  try {
    for(Annotation e : entityAnnots) {
      // find the (only) Page annotation that covers this entity
      Annotation thePage = getOnlyAnn(getCoveringAnnotations(inputAS, e, ""Page""));
      // get the corresponding annotation set
      AnnotationSet pageSet = doc.getAnnotations(
              (String)thePage.getFeatures().get(""id""));
      // and copy the annotation into it
      pageSet.add(start(e), end(e), e.getType(), e.getFeatures());
    }
  } catch(InvalidOffsetException e) {
    throw new JapeException(""Invalid offset from existing annotation"", e);
  }
  // optionally remove from input set
  // inputAS.removeAll(entityAnnots);
}
</code></pre>
",0,0,332,2014-01-06 14:51:41,https://stackoverflow.com/questions/20952575/can-we-grouping-annotations-in-gate
The meaning/implication of the matrices generated by Singular Value Decomposition (SVD) for Latent Semantic Analysis (LSA),"<p>SVD is used in LSA to get the latent semantic information. I am confused about the interpretation about the SVD matrices.</p>

<p>We first build a document-term matrix. And then use SVD to decompose it into 3 matrices.</p>

<p>For example:</p>

<p>The doc-term matrix M1 is  M x N, where:</p>

<pre><code>M = the number of documents
N = the number of terms
</code></pre>

<p>And M1 was decomposed into:</p>

<pre><code>M1 = M2 * M3 * M4, where:

M2: M x k

M3: k x k

M4: k x N
</code></pre>

<p>I see the interpretation like below:</p>

<p>The k <strong>column</strong> of M2 stands for categories of similar <strong>semantics</strong>.
The k <strong>row</strong> of M4 stands for the <strong>topics</strong>.</p>

<p>My questions are:</p>

<ol>
<li><p>Why is k interpreted like above? How do we know it is similar semantics and topics?</p></li>
<li><p>Why the similar semantics equal the topics?</p></li>
<li><p>Why k is interpreted differently between M2 and M4</p></li>
<li><p>How to interpret the M3?</p></li>
</ol>

<p>I am really confused. It seems the interpretation is totally arbitrary. Is that what <strong>latent</strong> meant to be?</p>
","machine-learning, nlp, data-mining, text-mining, lda","<p>I warmly recommend reading the information retrieval chapter in the SNLP bible by Manning and Schutze.
In 5 pages it explains everything you want to know about LSI and SVD. </p>

<p>You will find paragraphs like this :</p>

<p><img src=""https://i.sstatic.net/rqMxw.png"" alt=""enter image description here""></p>
",1,0,669,2014-01-08 06:58:32,https://stackoverflow.com/questions/20988969/the-meaning-implication-of-the-matrices-generated-by-singular-value-decompositio
Information Gain Calculation for a text file?,"<p>I'm working on <strong>""text categorization using Information gain,PCA and Genetic Algorithm""</strong> But after performing <strong>Preprocessing</strong>(Stemming, stopword removal, TFIDF) on the document m confused how to move ahead for information gain part.</p>

<p>my <strong>out file</strong> contain <strong>word</strong> and there <strong>TFIDF</strong> value.</p>

<p>like
<strong>WORD - TFIDF VALUE</strong></p>

<p>together(word) - 0.235(tfidf value)</p>

<p>come(word) - 0.2548(tfidf value)</p>

<p>when using weka for information gain (""<strong>InfoGainAttributeEval.java</strong>"") it require <strong>.arff</strong> file format as input.</p>

<p>Is there any to convert <strong>text</strong> file into <strong>.arff</strong> format.
or any other way to preform Information gain other than weka?</p>

<p>Is there any other open source for Calculating information gain for document ?</p>
","java, data-mining, information-retrieval, text-mining","<p>I found my answer.
In this we have to generate <strong>arff</strong> file.</p>

<p>In .arff file</p>

<p><strong>@RELATION section</strong> will contain all words present in your whole document after <strong>preprocessing</strong> .Each word will be of type <strong>real</strong> because <strong>tfidf value</strong> is a real value.</p>

<p><strong>@data section</strong> will contain their <strong>tfidf</strong> value calculated during <strong>preprocessing</strong>.
for example first will contain <strong>tfidf value</strong> all words present in first document an at last  colunm the document categary.</p>

<pre><code>@RELATION filename
@ATTRIBUTE word1 real
@ATTRIBUTE word2 real
@ATTRIBUTE word3 real
.
.
.
.so on
@ATTRIBUTE class {cacm,cisi,cran,med}

@data
0.5545479562,0.27,0.554544479562,0.4479562,cacm
0.5545479562,0.27,0.554544479562,0.4479562,cacm
0.55454479562,0.1619617,0.579562,0.5542,cisi
0.5545479562,0.27,0.554544479562,0.4479562,cisi
0.0,0.2396113617,0.44479562,0.2,cran
0.5545479562,0.27,0.554544479562,0.4479562,carn
0.5545177444479562,0.26196113617,0.0,0.0,med
0.5545479562,0.27,0.554544479562,0.4479562,med
</code></pre>

<p>after you generate this file you can give this file as input to <code>InfoGainAttributeEval.java</code>. and this working for me.</p>
",1,2,2208,2014-01-11 14:05:16,https://stackoverflow.com/questions/21063206/information-gain-calculation-for-a-text-file
feature selection within large data set,"<p>I want to know what are the most acceptable ways to find features(special words) within large data set. When I say special words, I mean words which are most used in a specific field.</p>

<p>For example, I have two books:</p>

<ol>
<li>book1: a book about economics</li>
<li>book2: a book about art</li>
</ol>

<p>Now, I choose book1 and want to see which words are most related to it. I guess such words as  'financial', 'dollar', 'revenue' etc. will dominate the top of the most used words list. Even though the words may occur in the book2 too, frequencies will be less than book1.</p>

<p>On the other hand, choosing book2 is supposed to yield words such as 'abstract', 'renaissance', 'romanticism', 'culture' etc.</p>

<p>Of course the result depends on context(in the above example, it depends on book1 and book2).</p>

<p>It is obvious, chosen algorithm must be able to eliminate stop-words.</p>

<p>So, I am wondering which methods are being used for this problem.</p>
","nlp, data-mining, text-mining, feature-selection","<p><a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">tf-idf</a> should help since it combines</p>

<ol>
<li>number of times a word appears in a document (i.e. each book)</li>
<li>number of times a word appears in the set of documents (a.k.a. corpus)</li>
</ol>

<p>If a word appears a lot in a document but not so much in the corpus, it is likely characteristic of the document and will have a high tf-idf score. If, on the other hand, a word appears frequently in a document and also frequently in the whole corpus, it is not very characteristic of such document and this does not have a high tf-idf score. The words with the highest tf-idf measures per document are the most relevant.</p>

<p>Stop word removal may be a step you want want to perform on your data before getting tf-idf measures for your documents, but you may want to try with and without stop words to compare performance.</p>

<p>EDIT:</p>

<p>To support what I mentioned in the comment re. not having to come up with the stopwords yourself, here's NLTK's English stopwords, which you can add to or remove from according to whatever you want to implement:</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; from nltk.corpus import stopwords
&gt;&gt;&gt; stopwords.words('english')
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 
'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 
'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 
'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 
'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 
'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 
'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 
'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 
'with', 'about', 'against', 'between', 'into', 'through', 'during', 
'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 
'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 
'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 
'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 
'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 
't', 'can', 'will', 'just', 'don', 'should', 'now']
</code></pre>
",1,0,153,2014-01-16 10:08:25,https://stackoverflow.com/questions/21158757/feature-selection-within-large-data-set
Clustering: how to extract most distinguishing features?,"<p>I have a set of documents that I am trying to cluster based on their vocabulary (that is, first making a corpus and then a sparse matrix with the <code>DocumentTermMatrix</code> command and so on). To improve the clusters and to understand better what features/words make a particular document fall into a particular cluster, I would like to know what the most distinguishing features for each cluster are.</p>

<p>There is an example of this in the <em>Machine Learning with R</em> book by Lantz, if you happen to know it - he clusters teen social media profiles by the interests they have pegged, and ends up with a table like this that shows ""each cluster ... with the features that most distinguish it from the other clusters"":</p>

<pre><code>cluster 1  | cluster 2 | cluster 3 ....
swimming   | band      | sports  ... 
dance      | music     | kissed ....
</code></pre>

<p>Now, my features aren't quite as informative, but <strong>I'd still like to be able to build something like that</strong>.<br>
However, the book does not explain how the table was constructed. I have tried my best to google creatively, and perhaps the answer is some obvious calculation on the cluster means, but being a newbie to R as well as to statistics, I could not figure it out. Any help is much appreciated, including links to previous questions or other resources I may have missed!</p>

<p>Thanks.</p>
","r, cluster-analysis, text-mining","<p>I had a similar problem some time ago..</p>

<p>Here is what I did:</p>

<pre><code>require(""tm"")
require(""skmeans"")
require(""slam"")

# clus: a skmeans object
# dtm: a Document Term Matrix
# first: eg. 10 most frequent words per cluster
# unique: if FALSE all words of the DTM will be used
#         if TRUE only cluster specific words will be used 



# result: List with words and frequency of words 
#         If unique = TRUE, only cluster specific words will be considered.
#         Words which occur in more than one cluster will be ignored.



mfrq_words_per_cluster &lt;- function(clus, dtm, first = 10, unique = TRUE){
  if(!any(class(clus) == ""skmeans"")) return(""clus must be an skmeans object"")

  dtm &lt;- as.simple_triplet_matrix(dtm)
  indM &lt;- table(names(clus$cluster), clus$cluster) == 1 # generate bool matrix

  hfun &lt;- function(ind, dtm){ # help function, summing up words
    if(is.null(dtm[ind, ]))  dtm[ind, ] else  col_sums(dtm[ind, ])
  }
  frqM &lt;- apply(indM, 2, hfun, dtm = dtm)

  if(unique){
    # eliminate word which occur in several clusters
    frqM &lt;- frqM[rowSums(frqM &gt; 0) == 1, ] 
  }
  # export to list, order and take first x elements 
  res &lt;- lapply(1:ncol(frqM), function(i, mat, first)
                head(sort(mat[, i], decreasing = TRUE), first),
                mat = frqM, first = first)

  names(res) &lt;- paste0(""CLUSTER_"", 1:ncol(frqM))
  return(res)
}
</code></pre>

<p>A small example:</p>

<pre><code>data(""crude"")
dtm &lt;- DocumentTermMatrix(crude, control =
                          list(removePunctuation = TRUE,
                               removeNumbers = TRUE,
                               stopwords = TRUE))

rownames(dtm) &lt;- paste0(""Doc_"", 1:20)
clus &lt;- skmeans(dtm, 3)


mfrq_words_per_cluster(clus, dtm)
mfrq_words_per_cluster(clus, dtm, unique = FALSE)
</code></pre>

<p>HTH</p>
",3,0,1757,2014-01-17 11:53:26,https://stackoverflow.com/questions/21185336/clustering-how-to-extract-most-distinguishing-features
Notepad++ moving tagged text strings to excel,"<p>So for a side hobby I'm doing some basic meta data gathering using text mining on the Project Gutenberg version of Herodotus but I'm stuck at the point of transferring the tagged text strings into excel. Essentially what I'm trying to do is create is a master list of all People, Places and Groups/Organizations mentioned in Herodotus and how many times each is mentioned in the text. I want to then use this list to populate some data visualizations in Tableau and/or Powerview, I have both.</p>

<p>I've already run the text through the Stanford NER which did a good job of at least identifying nearly all Persons, Organizations and Locations. I then manually checked over the document in notepadd++ to fix the numerous errors the NER made when analyzing ancient Greek names and places. I also removed the footnotes from the text because I don't care about them, only the original text. If you download the attached .txt you'll see that each proper noun is marked /PERSON, /LOCATION or /ORGANIZATION.</p>

<p>Now where I'm stuck is trying to get the tagged text strings into excel so I can use the data. A simple ctr+f reveals that in just book1 there are like 880 /PERSON tagged words. Essentially what I'm trying to do is grab each and every string that precedes one of the /PERSON, /LOCATION, or /ORGANIZATION and copy them into excel. </p>

<p>I looked into Regex expressions for notepad++ to see if I could select all text strings where the string ends in /PERSON but I cannot seem to figure it out. I can get the regex to select all ""/PERSON"" but I don't understand regex well enough to get it to select all ""name/PERSON"" or ""place/LOCATION"" strings in their entirety if that makes sense.</p>

<p>EDIT: I forgot to ask about using SQL or Python to help me solve this problem. From my work I'm familiar with using SQL queries on databases. So this is a stupid question but can you even use SQL to directly query a .txt file? If so then I could pretty easily write a SQL statement to extract the tagged text strings.</p>

<p>I'm less familiar with Python but is it possible to extract the info I'm looking for via some python scripting? </p>

<p>Finally the question I should have asked in the original question. Am I going about this all wrong? I think using Notepad++ to correct the Stanford NER tags was necessary but maybe going straight from the tagged .txt to excel is the wrong approach. </p>

<p><a href=""https://www.dropbox.com/s/k5m8yag6tpae05w/HerodotusB1NER.txt"" rel=""nofollow"">https://www.dropbox.com/s/k5m8yag6tpae05w/HerodotusB1NER.txt</a></p>

<p>2ND EDIT: So I finally got around to playing with the regex expressions both of you provided and they are almost working perfectly. However, I think its trimming off some of the result set actually. </p>

<p>A perfect example is the character ""Deïokes"" who is being trimmed into just ""okes/PERSON"" after I run the regex search. I think the a-z part of the regex is ignoring special letters like the umlaut over the i in Deïokes. </p>

<p>How would I tweak the regex search to tolerate those sorts of special characters? If the regex cannot accomodate those special characters then I think it wouldn't be too manually intensive to go in and fix the special characters where they show up here and there.</p>
","regex, text, notepad++, text-mining, excel-2013","<p>I gave this another try and found an awfully more easy solution to just copy the stuff to Excel. I don't have Notepad++, but I do use <a href=""http://www.pspad.com/"" rel=""nofollow noreferrer"">PSPad</a> occasionally if my IDE is not around. It offers pretty much the same features as Notepad++. Some things it does better and others it doesn't. The regex search is pretty good, and the search dialogue has a button that says <kbd>Copy</kbd>.</p>
<p><img src=""https://i.sstatic.net/Nl0Zp.png"" alt=""Find dialogue"" /></p>
<p>I copied your file and used my regex from the other answer without the capture groups. We don't need them as it will copy the complete match. Remember the <code>\b</code> is a word boundary and not a real character that will be copied.</p>
<p><img src=""https://i.sstatic.net/O0JMF.png"" alt=""Copied search results"" /></p>
<p>And voila, here we go. A list of names with their classification that should be easy enough to copy to Excel and split into columns there.</p>
",0,1,860,2014-01-17 21:59:28,https://stackoverflow.com/questions/21196733/notepad-moving-tagged-text-strings-to-excel
collection of variant of words using special symbols,"<p>I am working on a project related to spam filtering. Many of you might be aware of this technique used by spammers:</p>

<ol>
<li>writing ""items"" as ""|tem"" (pipe instead of i)</li>
<li>$ale instead of sale</li>
<li>h0t instead of hot (zero instead of letter 'o')</li>
</ol>

<p>etc. etc.</p>

<p>I am wondering if there is a database available for all such possible variants of words using special symbols? Or does any one know about some good strategy to tackle this trick?</p>

<p>Currently what I have done is, I simply replace '@' with 'a', '|' with 'i', '$' with 's' and so on. I need your views on this issue! Please help.</p>
","java, algorithm, text, data-structures, text-mining","<p>It seems you are taking a message as a starting point and trying to transform it. </p>

<p>Another aproach could be to start by defining a list of words which are likely to be changed (sale, viagra, etc) and then generate all possible similar words. As a measure of similarity you can take a Levenshtein distance.</p>
",1,1,62,2014-01-20 15:01:56,https://stackoverflow.com/questions/21237221/collection-of-variant-of-words-using-special-symbols
weka StringToWordVector filter reversion (java),"<p>I can't deal with clustering with <strong>weka</strong> library. I have string attributes, so I use StringToWordVector filter, but how can I after clustering move back from WordVector to string representation to show 'human readable' results?
I want to revert this operation:</p>

<pre><code>StringToWordVector filter = new StringToWordVector();
filter.setInputFormat(instancesToFilter);
Instances dataFiltered = Filter.useFilter(instancesToFilter, filter);
</code></pre>

<p>Is it possibile?</p>
","java, cluster-analysis, weka, text-mining","<p>The <code>StringToWordVector</code> filter cannot be reversed. However, you have at least two possibilities:</p>

<ul>
<li>If you just want to see or show the original strings that are in each cluster, you can add an <code>ID</code> attribute, ensure it is not used during clustering (to avoid unexpected behavior), then recover the text from the original strings (<code>ARFF</code> file).</li>
<li>If you want to show some meaningful summary of the contents of each cluster, you can just output the most frequent/heavy words in each cluster. This is a rather common approach when clustering texts.</li>
</ul>
",2,1,745,2014-01-21 14:15:14,https://stackoverflow.com/questions/21260583/weka-stringtowordvector-filter-reversion-java
How to improve the performance when working with wikipedia data and huge no. of webpages?,"<p>I am supposed to extract representative terms from an organisation's website using wikipedia's article-link data dump.
To achieve this I've - </p>

<ol>
<li>Crawled &amp; downloaded organisation's webpages. (~110,000)</li>
<li>Created a dictionary of wikipedia ID and terms/title. (~40million records)</li>
</ol>

<p>Now, I'm supposed to process each of the webpages using the dictionary to recognise terms and track their term IDs &amp; frequencies.</p>

<p>For the dictionary to fit in memory, I've splitted the dictionary into smaller files. Based on my experiment with a small data-set, the processing time for the above will be around 75 days.</p>

<p>And this is just for 1 organisation. I have to do the same for more than 40 of them.</p>

<p>Implementation - </p>

<ul>
<li>HashMap for storing dictionary in memory.</li>
<li>looping through each map entry to search the term in a webpage, using Boyer-Moore search implementation.</li>
<li>Repeating the above for each webpage, and storing results in a HashMap.</li>
</ul>

<p>I've tried optimizing the code and tuning the JVM for better performance.</p>

<p>Can someone please advise on a more efficient way to implement the above, reducing the processing time to a few days.</p>

<p>Is Hadoop an option to consider?</p>
","java, hadoop, ontology, text-mining, wikimedia-dumps","<p>Based on your question:</p>

<pre><code>Number of Documents = 110000

Dictionary =&gt; List of [TermID, Title Terms] = 40million entries

Size of documents = 11000 * 1KB per document on an average =  26.9GB
(1KB per document on an average)

Size of dictionary = 40million * 256bytes = 9.5GB of raw data
(256bytes per entry on an average)
</code></pre>

<p>How did you arrive at the 75 days estimate?</p>

<p>There are number of performance targets:</p>

<ul>
<li>How are you storing the Documents?</li>
<li>How are you storing/retrieving the Dictionary? ( assuming not all of it in memory unless you can afford to)</li>
<li>How many machines are you running it on?</li>
<li>Are you performing the dictionary lookups in parallel? ( of-course assuming dictionary is immutable once you have already processed whole of wikipedia )</li>
</ul>

<p>Here is an outline of what I believe you are doing:</p>

<pre><code>dictionary = read wikipedia dictionary
document = a sequence of documents
documents.map { doc =&gt;
  var docTermFreq = Map[String, Int]()
  for(term &lt;- doc.terms.map if(dictionary.contains(term)) ) {
     docTermFreq = docTermFreq + (term -&gt; docTermFreq.getOrElse(term, 0) + 1)
  }
  // store docTermFreq map
}
</code></pre>

<p>What this is essentially doing is breaking up each document into tokens and then performing a lookup in wikipedia dictionary for its token's existence.</p>

<p>This is exactly what a <a href=""http://lucene.apache.org/core/3_5_0/api/core/org/apache/lucene/analysis/Analyzer.html"" rel=""nofollow"">Lucene Analyzer</a> does.</p>

<p>A <a href=""http://lucene.apache.org/core/3_5_0/api/core/index.html?org/apache/lucene/analysis/Tokenizer.html"" rel=""nofollow"">Lucene Tokenizer</a> will convert document into tokens. This happens before the terms are indexed into lucene. So all you have to do is implement a Analyzer which can lookup the Wikipedia Dictionary, for whether or not a token is in dictionary.</p>

<p>I would do it like this: </p>

<ul>
<li>Take every document and prepare a token stream ( using an Analyzer described above )</li>
<li>Index the document terms.</li>
<li>At this point you will have wikipedia terms only, in the Lucene Index.</li>
</ul>

<p>When you do this, you will have ready-made statistics from the Lucene Index such as:</p>

<ul>
<li><a href=""http://lucene.apache.org/core/3_5_0/api/core/org/apache/lucene/index/IndexReader.html#docFreq%28org.apache.lucene.index.Term%29"" rel=""nofollow"">Document Frequency</a> of a Term</li>
<li><a href=""http://lucene.apache.org/core/3_0_3/api/core/org/apache/lucene/index/TermFreqVector.html"" rel=""nofollow"">TermFrequencyVector</a> ( exactly what you need )</li>
<li>and a ready to use inverted index! ( for a quick introduction to <a href=""http://xapian.org/docs/intro_ir.html"" rel=""nofollow"">Inverted Index and Retrieval</a> )</li>
</ul>

<p>There are lot of things you can do to improve the performance. For example:</p>

<ul>
<li>Parallelize the document stream processing.</li>
<li>You can store the dictionary in key-value database such as <a href=""http://www.oracle.com/technetwork/database/database-technologies/berkeleydb/overview/index.html"" rel=""nofollow"">BerkeylyDB</a> or Kyoto Cabinet, or even an in-memory key-value storage such as <a href=""http://redis.io/"" rel=""nofollow"">Redis</a> or <a href=""http://memcached.org/"" rel=""nofollow"">Memcache</a>.</li>
</ul>

<p>I hope that helps.</p>
",0,0,183,2014-01-23 14:09:13,https://stackoverflow.com/questions/21310539/how-to-improve-the-performance-when-working-with-wikipedia-data-and-huge-no-of
tm.package: findAssocs vs Cosine,"<p>I'm new here and my questions is of mathematical rather than programming nature where I would like to get a second opinion on whether my approach makes sense.</p>

<p>I was trying to find associations between words in my corpus using the function <code>findAssocs</code>, from the <code>tm</code> package. Even though it appears to perform reasonably well on the data available through the package, such as New York Times and US Congress, I was disappointed with its performance on my own, less tidy dataset. It appears to be prone being distorted by rare document that contain several repetitions of the same words which seems to create a strong association between them. I've found that the cosine measure gives a better picture of how related the terms are, even though based on literature, it only tends to be used to measure the similarity of documents rather than terms. Let's use USCongress data from <code>RTextTools</code> package to demonstrate what I mean:</p>

<p>First, I'll set everything up...</p>

<pre><code>data(USCongress)

text = as.character(USCongress$text)

corp = Corpus(VectorSource(text)) 

parameters = list(minDocFreq        = 1, 
                  wordLengths       = c(2,Inf), 
                  tolower           = TRUE, 
                  stripWhitespace   = TRUE, 
                  removeNumbers     = TRUE, 
                  removePunctuation = TRUE, 
                  stemming          = TRUE, 
                  stopwords         = TRUE, 
                  tokenize          = NULL, 
                  weighting         = function(x) weightSMART(x,spec=""ltn""))

tdm = TermDocumentMatrix(corp,control=parameters)
</code></pre>

<p>Let's say we are interested to investigate the relationship between ""Government"" and ""Foreign"":</p>

<pre><code># Government: appears in 37 docs and between then it appears 43 times
length(which(text %like% "" government""))
sum(str_count(text,""government""))

# Foreign: appears in 49 document and between then it appears 56 times
length(which(text %like% ""foreign""))
sum(str_count(text,""foreign""))

length(which(text[which(text %like% ""government"")] %like% ""foreign""))
# together they appear 3 times

# looking for ""foreign"" and ""government""
head(as.data.frame(findAssocs(tdm,""foreign"",0.1)),n=10)

             findAssocs(tdm, ""foreign"", 0.1)
countri                                 0.34
lookthru                                0.30
tuberculosi                             0.26
carryforward                            0.24
cor                                     0.24
malaria                                 0.23
hivaid                                  0.20
assist                                  0.19
coo                                     0.19
corrupt                                 0.19

# they do not appear to be associated
</code></pre>

<p>Now let's add another document that contains ""foreign government"" repeated 50 times:</p>

<pre><code>text[4450] = gsub(""(.*)"",paste(rep(""\\1"",50),collapse="" ""),""foreign government"")
corp = Corpus(VectorSource(text)) 
tdm = TermDocumentMatrix(corp,control=parameters)

#running the association again:
head(as.data.frame(findAssocs(tdm,""foreign"",0.1)),n=10)

             findAssocs(tdm, ""foreign"", 0.1)
govern                                  0.30
countri                                 0.29
lookthru                                0.26
tuberculosi                             0.22
cor                                     0.21
carryforward                            0.20
malaria                                 0.19
hivaid                                  0.17
assist                                  0.16
coo                                     0.16
</code></pre>

<p>As you can see, now it's a different story and it all comes down to a single document.</p>

<p>Here I would like to do something unconventional: use the cosine to find similarity between terms sitting in the document space. This measure tend to be used to find similarity between documents rather than terms, but I see no reason why it can't be used to find similarity between words. In a conventional sense, documents are the vectors while terms are the axes and we can detect their similarity based on the angle between these documents. But a Term Document Matrix is a transpose of a Document Term Matrix, and similarly, we can project terms in the document space, i.e. let your documents be the axes and your terms the vectors between which you can measure the angle. It doesn't seem to suffer from the same drawbacks as the simple correlation:</p>

<pre><code>cosine(as.vector(tdm[""government"",]),as.vector(tdm[""foreign"",]))
     [,1]
[1,]    0
</code></pre>

<p>Other than that, the 2 measures appear to be very similar:</p>

<pre><code>tdm.reduced = removeSparseTerms(tdm,0.98)

Proximity = function(tdm){ 
  d = dim(tdm)[1] 
  r = matrix(0,d,d,dimnames=list(rownames(tdm),rownames(tdm))) 
  for(i in 1:d){ 
    s = seq(1:d)[-c(1:(i-1))] 
    for(j in 1:length(s)){ 
      r[i,s[j]] = cosine(as.vector(tdm[i,]),as.vector(tdm[s[j],])) 
      r[s[j],i] = r[i,s[j]] 
    } 
  } 
  diag(r) = 0 
  return(r) 
}

rmat = Proximity(tdm.reduced)

# findAssocs method
head(as.data.frame(sort(findAssocs(tdm.reduced,""fund"",0),decreasing=T)),n=10)

        sort(findAssocs(tdm.reduced, ""fund"", 0), decreasing = T)
use                                                         0.11
feder                                                       0.10
insur                                                       0.09
author                                                      0.07
project                                                     0.05
provid                                                      0.05
fiscal                                                      0.04
govern                                                      0.04
secur                                                       0.04
depart                                                      0.03

# cosine method
head(as.data.frame(round(sort(rmat[,""fund""],decreasing=T),2)),n=10)

       round(sort(rmat[, ""fund""], decreasing = T), 2)
use                                              0.15
feder                                            0.14
bill                                             0.14
provid                                           0.13
author                                           0.12
insur                                            0.11
state                                            0.10
secur                                            0.09
purpos                                           0.09
amend                                            0.09
</code></pre>

<p>Surprisingly, though, I haven't seen cosine being used to detect similarities between terms, which makes me wonder if I've missed something important. Perhaps this method is flawed in a way I haven't though of. So any thoughts on what I've done would be very much appreciated.</p>

<p>If you've made it that far, thanks for reading!!</p>

<p>Cheers</p>
","r, math, text-mining, tm, cosine-similarity","<p>If I understand your query (which should be on stack exchange I think).  I believe the issue is that term distances in <code>findAssocs</code> is using Euclidean measurement.  So a document that that is simply double the words becomes an outlier and considered much different in the distance measurement.<br>
Switching to cosine as a measure for documents is widely used so I suspect terms are ok too.  I like the <code>skmeans</code> package for clustering documents by cosine.  Spherical K-Means will accept a TDM directly and does cosine distance with unit length.</p>

<p>This <a href=""https://www.youtube.com/watch?v=ZEkO8QSlynY"" rel=""nofollow"">video</a> at ~11m in shows it in case you don't already know.
Hope that was a bit helpful...in the end I believe cosine is acceptable.</p>
",2,4,2232,2014-01-25 23:34:53,https://stackoverflow.com/questions/21357656/tm-package-findassocs-vs-cosine
Use R to convert PDF files to text files for text mining,"<p>I have nearly one thousand pdf journal articles in a folder. I need to text mine on all article's abstracts from the whole folder. Now I am doing the following:</p>

<pre><code>dest &lt;- ""~/A1.pdf""

# set path to pdftotxt.exe and convert pdf to text
exe &lt;- ""C:/Program Files (x86)/xpdfbin-win-3.03/bin32/pdftotext.exe""
system(paste(""\"""", exe, ""\"" \"""", dest, ""\"""", sep = """"), wait = F)

# get txt-file name and open it
filetxt &lt;- sub("".pdf"", "".txt"", dest)
shell.exec(filetxt)
</code></pre>

<p>By this, I am converting one pdf file to one .txt file and then copying the abstract in another .txt file and compile it manually. This work is troublesome. </p>

<p>How can I read all individual articles from the folder and convert them into .txt file which contain only the abstract from each article. It can be done by limiting the content between ABSTRACT and INTRODUCTION in each article; but I am not able to do so. Any help is appreciated.</p>
","r, text-mining, tm, pdftotext","<p>Yes, not really an <code>R</code> question as IShouldBuyABoat notes, but something that <code>R</code> can do with only minor contortions...</p>

<p>Use <code>R</code> to convert PDF files to txt files...</p>

<pre><code># folder with 1000s of PDFs
dest &lt;- ""C:\\Users\\Desktop""

# make a vector of PDF file names
myfiles &lt;- list.files(path = dest, pattern = ""pdf"",  full.names = TRUE)

# convert each PDF file that is named in the vector into a text file 
# text file is created in the same directory as the PDFs
# note that my pdftotext.exe is in a different location to yours
lapply(myfiles, function(i) system(paste('""C:/Program Files/xpdf/bin64/pdftotext.exe""', 
             paste0('""', i, '""')), wait = FALSE) )
</code></pre>

<p>Extract only abstracts from txt files...</p>

<pre><code># if you just want the abstracts, we can use regex to extract that part of
# each txt file, Assumes that the abstract is always between the words 'Abstract'
# and 'Introduction'
mytxtfiles &lt;- list.files(path = dest, pattern = ""txt"",  full.names = TRUE)
abstracts &lt;- lapply(mytxtfiles, function(i) {
  j &lt;- paste0(scan(i, what = character()), collapse = "" "")
  regmatches(j, gregexpr(""(?&lt;=Abstract).*?(?=Introduction)"", j, perl=TRUE))
})
</code></pre>

<p>Write abstracts into separate txt files...</p>

<pre><code># write abstracts as txt files 
# (or use them in the list for whatever you want to do next)
lapply(1:length(abstracts),  function(i) write.table(abstracts[i], file=paste(mytxtfiles[i], ""abstract"", ""txt"", sep="".""), quote = FALSE, row.names = FALSE, col.names = FALSE, eol = "" "" ))
</code></pre>

<p>And now you're ready to do some text mining on the abstracts.</p>
",22,21,38619,2014-01-30 00:33:06,https://stackoverflow.com/questions/21445659/use-r-to-convert-pdf-files-to-text-files-for-text-mining
R_ read table: separator,"<p>I want to read the text file.</p>

<p>contents of file are like below.</p>

<pre><code>2013-08-13 19:26:58  Method for modifying a piece of 3D geometry
2013-08-13 19:26:57  Method of interactively modifying a feature
...
</code></pre>

<p>I want to read this file on like this table</p>

<pre><code>dateTime Method
""2013-08-13 19:26:58"" ""Method for modifying a piece of 3D geometry""
""2013-08-13 19:26:57"" ""Method of interactively modifying a feature""
...
</code></pre>

<p>As you see, I want to separate the line with two consecutive white spaces (""\s\s"") not one white space.</p>

<p>How can i do this?</p>

<p>I tried to use read.table function, But the one character is allowed for separator.</p>

<p>Or can i read the file contents without first column?</p>

<p>like this.</p>

<pre><code>""Method for modifying a piece of 3D geometry""
""Method of interactively modifying a feature""
</code></pre>

<p>Please give me some advices. Thank you</p>
","r, text-mining, read.table","<p>Just replace the double space with any sep character first:</p>

<pre><code>txt&lt;-""2013-08-13 19:26:58  Method for modifying a piece of 3D geometry
2013-08-13 19:26:57  Method of interactively modifying a feature""


read.table(sep=""|"",text=gsub(""  "",""|"",txt), header=F)

#                   V1                                          V2
#1 2013-08-13 19:26:58 Method for modifying a piece of 3D geometry
#2 2013-08-13 19:26:57 Method of interactively modifying a feature
</code></pre>
",3,0,1210,2014-02-06 09:17:28,https://stackoverflow.com/questions/21598485/r-read-table-separator
Bytes vs Characters vs Words - which granularity for n-grams?,"<p>At least 3 types of n-grams can be considered for representing text documents: </p>

<ul>
<li>byte-level n-grams</li>
<li>character-level n-grams</li>
<li>word-level n-grams</li>
</ul>

<p>It's unclear to me which one should be used for a given task (clustering, classification, etc). I read somewhere that character-level n-grams are preferred to word-level n-grams when the text contains typos, so that ""Mary loves dogs"" remains similar to ""Mary lpves dogs"".</p>

<p>Are there other criteria to consider for choosing the ""right"" representation?</p>
","nlp, data-mining, text-mining, n-gram","<p><strong>Evaluate</strong>. The criterion for choosing the representation is <em>whatever works</em>.</p>

<p>Indeed, character level (!= bytes, unless you only care about english) probably is the most common representation, because it is robust to spelling differences (which do not need to be errors, if you look at history; spelling changes). So for spelling correction purposes, this works well.</p>

<p>On the other hand, <a href=""https://books.google.com/ngrams"" rel=""noreferrer"">Google Books n-gram</a> viewer uses word level n-grams on their books corpus. Because they don't want to analyze spelling, but term usage over time; e.g. ""child care"", where the individual words aren't as interesting as their combination. This was shown to be very useful in machine translation, often referred to as ""refrigerator magnet model"".</p>

<p>If you are not processing international language, bytes may be meaningful, too.</p>
",8,8,3786,2014-02-09 08:18:39,https://stackoverflow.com/questions/21656861/bytes-vs-characters-vs-words-which-granularity-for-n-grams
How whether a string is randomly generated or plausibly an English word?,"<p>I have a corpus of text which contains some strings. In these strings, some are English words, some are random such as VmsVKmGMY6eQE4eMI, there are no limit on the number of characters in each string. </p>

<p>Is there any way to test whether or not one string is a English word?   I am looking for some kind of algorithm that does the job.  This is in Java, and I rather not to implement an extra dictionary. </p>
","java, text, data-mining, text-mining","<p>If you mean some kind of a rule of a thumb that distinguishes english word from random text, there is none. For reasonable accuracy you will need to query an external source, whether it's the Web, dictionary, or a service. </p>

<p>If you only need to check for an existence of the word, I would suggest <a href=""http://wordnet.princeton.edu/"" rel=""nofollow"">Wordnet</a>. It is pretty simple to use and there is a nice Java API for it called <a href=""http://sourceforge.net/projects/jwordnet/"" rel=""nofollow"">JWNL</a>, that makes querying Wordnet dictionary a breeze.</p>
",1,6,4016,2014-02-11 23:21:53,https://stackoverflow.com/questions/21715354/how-whether-a-string-is-randomly-generated-or-plausibly-an-english-word
Is there any Part-Of-Speech tagger in C#?,"<p>My data pre-processing for data clustering needs <strong>part of speech (POS)</strong> tagging. I am wondering if there's some library in C# ready for this.</p>
","c#, nlp, text-mining, part-of-speech","<h1>SharpNLP</h1>

<p>The best tool for natural language processing implemented in c# is <a href=""http://sharpnlp.codeplex.com/"" rel=""noreferrer"">SharpNLP</a>. </p>

<p>SharpNLP is a C# port of the Java OpenNLP tools, plus additional code to facilitate natural language processing.</p>

<h1>Embedding IronPython and NLTK</h1>

<p>Python provides a package <a href=""http://www.nltk.org/"" rel=""noreferrer"">NLTK</a> (Natural Language Toolkit) used widely by many computational linguists, NLP researchers.</p>

<p>One can try to embed <a href=""http://ironpython.net/"" rel=""noreferrer"">IronPython</a> under C# and run NLTK from there.</p>

<p>You can check the following <a href=""http://blog.samibadawi.com/2010/03/open-source-nlp-in-c-35-using-nltk.html"" rel=""noreferrer"">link</a> on how to do it.</p>
",5,8,4409,2014-02-19 04:53:39,https://stackoverflow.com/questions/21871374/is-there-any-part-of-speech-tagger-in-c
Row sum for large term-document matrix / simple_triplet_matrix ?? {tm package},"<p>So I have a very large term-document matrix:</p>

<pre><code>&gt; class(ph.DTM)
[1] ""TermDocumentMatrix""    ""simple_triplet_matrix""

&gt; ph.DTM
A term-document matrix (109996 terms, 262811 documents)

Non-/sparse entries: 3705693/28904453063
Sparsity           : 100%
Maximal term length: 191 
Weighting          : term frequency (tf)
</code></pre>

<p>How do I get the rowSum (frequency) of each term? I tried:</p>

<pre><code>&gt; apply(ph.DTM, 1, sum)
Error in vector(typeof(x$v), nr * nc) : vector size cannot be NA
In addition: Warning message:
In nr * nc : NAs produced by integer overflow
</code></pre>

<p>Obviously, I know about <code>removeSparseTerms</code>:</p>

<pre><code>ph.DTM2 &lt;- removeSparseTerms(ph.DTM, 0.99999)
</code></pre>

<p>Which cuts down the size a bit:</p>

<pre><code>&gt; ph.DTM2
A term-document matrix (28842 terms, 262811 documents)

Non-/sparse entries: 3612620/7576382242
Sparsity           : 100%
Maximal term length: 24 
Weighting          : term frequency (tf)
</code></pre>

<p>But I still cannot apply any matrix-related functions to it:</p>

<pre><code>&gt; as.matrix(ph.DTM2)
Error in vector(typeof(x$v), nr * nc) : vector size cannot be NA
In addition: Warning message:
In nr * nc : NAs produced by integer overflow
</code></pre>

<p><strong>How can I just get a simple row sum on this object?? Thanks!!</strong></p>
","r, text-mining","<p>OK, after some more Google'ing, I came across the <code>slam</code> package, which enables:</p>

<pre><code>ph.DTM3 &lt;- rollup(ph.DTM, 2, na.rm=TRUE, FUN = sum)
</code></pre>

<p>Which works.</p>
",22,17,12933,2014-02-20 22:50:43,https://stackoverflow.com/questions/21921422/row-sum-for-large-term-document-matrix-simple-triplet-matrix-tm-package
Where to find domain-specific corpus for a text mining task?,"<p>I am working on a text mining project which focus on the computer technology documents. So there're many jargons. Tasks like part-of-speech tagging require some training data to built a pos-tagger. And I think this training data should be from the same domain with words like "".NET, COM, JAVA"" correctly tagged.</p>

<p>So where can I find such corpus? Or is there any work around? Or can we tune an existing tagger to handle domain specific task?</p>
","machine-learning, nlp, data-mining, text-mining, corpus","<p>Gathering training data (and defining features) is going to be the hardest step of this problem. I'm sure there are datasets out there. But an alternative option for you would be to identify a few journals or news sites that focus on your area of interest and crawl them and pull down the text, perhaps validating each article you pull down by searching for keywords. I've done that before to develop a corpus focused on elections.</p>
",3,0,894,2014-02-27 14:38:51,https://stackoverflow.com/questions/22071685/where-to-find-domain-specific-corpus-for-a-text-mining-task
Collating IRC archives into a corpus for text mining,"<p>Assume I want to do text mining on IRC archived, and want to parse the corpus by months and years, using an archive such as <a href=""http://donttreadonme.co.uk/rubinius/index.html"" rel=""nofollow"">this one</a> as the source.</p>

<p>In R, what would the overall strategy be for approaching this problem?</p>
","r, web-scraping, text-mining","<p>For the scraping part here's some starter code.</p>

<pre><code>library(XML)

rootUri &lt;- ""http://donttreadonme.co.uk""

doc &lt;- htmlParse(paste0(rootUri, ""/rubinius/index.html""))

links &lt;- xpathSApply(doc, ""//a/@href"")

links &lt;- grep(""rubinius/2014"", links, value = TRUE)
links &lt;- gsub("".."", """", links, fixed = TRUE)

messages &lt;- lapply(links[1:5], function(l) {
    doc &lt;- htmlParse(paste0(rootUri, l))
    readHTMLTable(doc, which = 1, header = FALSE)
})

messages &lt;- do.call(rbind, messages)

##              V1            V2
## href.1 00:33:57     travis-ci
## href.2 05:04:23     travis-ci
## href.3 05:27:44     travis-ci
## href.4 10:00:59 yorickpeterse
## href.5 13:23:36 yorickpeterse
## href.6 13:23:53 yorickpeterse
##                                                                                          V3
## href.1     [travis-ci] rubinius/rubinius/master (fcc5b8c - Brian Shirai): The build passed.
## href.2 [travis-ci] rubinius/rubinius/master (901a6bc - Brian Shirai): The build was broken.
## href.3  [travis-ci] rubinius/rubinius/master (5cffe7b - Brian Shirai): The build was fixed.
## href.4                                                                              morning
## href.5           oh RubyGems, why do you need the ext builder during runtime?
## href.6                                this better not be because I forgot --rubygems ignore
</code></pre>
",1,0,111,2014-02-28 02:01:07,https://stackoverflow.com/questions/22084721/collating-irc-archives-into-a-corpus-for-text-mining
How to avoid weird umlaute error when using data.table,"<p>I need to operate sums on a sparse dataframe considering the IDs    </p>

<pre><code>require(data.table)
sentEx = structure(list(abend = c(1, 1, 0, 0, 2), aber = c(0, 1, 0, 0, 
0), über = c(1, 0, 0, 0, 0), überall = c(0, 0, 0, 0, 0), überlegt = c(0, 
0, 0, 0, 0), ID = structure(c(1L, 1L, 2L, 2L, 2L), .Label = c(""0019"", 
""0021""), class = ""factor""), abgeandert = c(1, 1, 1, 0, 0), abgebildet = c(0, 
0, 1, 1, 0), abgelegt = c(0, 0, 0, 0, 3)), .Names = c(""abend"", 
""aber"", ""über"", ""überall"", ""überlegt"", ""ID"", ""abgeandert"", ""abgebildet"", 
""abgelegt""), row.names = c(1L, 2L, 16L, 17L, 18L), class = ""data.frame"")

sentEx  # How it looks
   abend aber über überall überlegt   ID abgeandert abgebildet abgelegt
1      1    0    1       0        0 0019          1          0        0
2      1    1    0       0        0 0019          1          0        0
16     0    0    0       0        0 0021          1          1        0
17     0    0    0       0        0 0021          0          1        0
18     2    0    0       0        0 0021          0          0        3
</code></pre>

<p>Without ""umlaute"" it works fine:</p>

<pre><code>sentEx.dt &lt;- data.table(sentEx[,-c(3,4,5)])[, lapply(.SD, sum), by=ID]
(sentExSum &lt;- as.data.frame(sentEx.dt))  # Need again as dataframe, which looks like:
    ID abend aber abgeandert abgebildet abgelegt
1 0019     2    1          2          0        0
2 0021     2    0          1          2        3 
</code></pre>

<p>But otherwise i get this error:</p>

<pre><code>sentEx.dt &lt;- data.table(sentEx)[, lapply(.SD, sum), by=ID]
# Error in gsum(`Ã¼ber`) : object 'Ã¼ber' not found
      sentExSum &lt;- as.data.frame(sentEx.dt)
</code></pre>

<p>Some Additional seesion info (since the issue seems to be system related - see comments):</p>

<pre><code>sessionInfo()
R version 3.0.2 (2013-09-25)
Platform: x86_64-w64-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252    LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
[5] LC_TIME=German_Germany.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] data.table_1.9.2

loaded via a namespace (and not attached):
[1] plyr_1.8.1     Rcpp_0.11.0    reshape2_1.2.2 stringr_0.6.2  tools_3.0.2
</code></pre>

<p>Also requested commands:</p>

<pre><code>require(data.table); test.data.table()
Running C:/Users/Krohana/Documents/R/win-library/3.0/data.table/tests/tests.Rraw 
Loading required package: reshape
Loading required package: hexbin
Loading required package: xts
Loading required package: bit64
Test 167.2 not run. If required call library(hexbin) first.
Don't know how to automatically pick scale for object of type ITime. Defaulting to continuous
Don't know how to automatically pick scale for object of type ITime. Defaulting to continuous
Tests 487 and 488 not run. If required call library(reshape) first.
Test 841 not run. If required call library(xts) first.
Tests 897-899 not run. If required call library(bit64) first.
All 1220 tests in inst/tests/tests.Rraw completed ok in 24.321sec on Sun Mar 02 17:57:26 2014 ts/tests.Rraw completed ok in 24.638sec on Sun Mar 02 17:55:45 2014
</code></pre>

<p>Requested commands2:</p>

<pre><code>&gt; Encoding(names(sentEx))
[1] ""unknown"" ""unknown"" ""UTF-8""   ""UTF-8""   ""UTF-8""   ""unknown"" ""unknown"" ""unknown"" ""unknown""
&gt; options(datatable.verbose=TRUE)
&gt; options(datatable.verbose=TRUE); options(datatable.optimize=1L);
</code></pre>
","r, data.table, text-mining, sparse-matrix","<p>I couldn't reproduce either. But, there was another ""object not found error"" that Arun found, that I'm hoping is this one too.</p>

<p>Now in v1.9.3, commit 1212. From NEWS :</p>

<blockquote>
  <p>o  An error ""object [name] not found"" could occur in some
  circumstances, particularly after a previous error. Reported with
  non-ASCII characters in a column name, a red herring we hope since
  non-ASCII characters are supported in column names in data.table. Fix
  implemented and tests added.</p>
</blockquote>

<p>If it happens again, please let us know. Your test has been added verbatim to the test suite, thanks.</p>
",4,5,765,2014-03-02 13:15:47,https://stackoverflow.com/questions/22128047/how-to-avoid-weird-umlaute-error-when-using-data-table
Text mining Clustering Analysis in R - Error :Two dimensional array,"<p>I'm trying to follow a document that has some code on text mining clustering analysis.
I'm fairly new to R and the concept of text mining/clustering so please bear with me if i sound illiterate.</p>

<p>I create a simple matrix called dtm and then run kmeans to produce 3 clusters. The code im having issues is where a function has been defined to get ""five most common words of the documents  in the cluster""</p>

<pre><code>dtm0.75 = as.matrix(dt0.75)
dim(dtm0.75)

kmeans.result = kmeans(dtm0.75, 3)

perClusterCounts = function(df, clusters, n)
{
  v = sort(colSums(df[clusters == n, ]), 
           decreasing = TRUE)
  d = data.frame(word = names(v), freq = v)
  d[1:5, ]
}
perClusterCounts(dtm0.75, kmeans.result$cluster, 1)
</code></pre>

<p>Upon running this code i get the following error:</p>

<p>Error in colSums(df[clusters == n, ]) : 
  'x' must be an array of at least two dimensions</p>

<p>Could someone help me fix this please?</p>

<p>Thank you.</p>
","arrays, r, k-means, text-mining","<p>I can't reproduce your error, it works fine for me. Update your question with a reproducible example and you might get a more useful answer. Perhaps your input data object is empty, what do you get with <code>dim(dtm0.75)</code>?</p>

<p>Here it is working fine on the data that comes with the <code>tm</code> package: </p>

<pre><code>library(tm)
data(crude)

dt0.75 &lt;- DocumentTermMatrix(crude)

dtm0.75 = as.matrix(dt0.75)
dim(dtm0.75)

kmeans.result = kmeans(dtm0.75, 3)

perClusterCounts = function(df, clusters, n)
{
  v = sort(colSums(df[clusters == n, ]), 
           decreasing = TRUE)
  d = data.frame(word = names(v), freq = v)
  d[1:5, ]
}
perClusterCounts(dtm0.75, kmeans.result$cluster, 1)

                 word freq
the               the   69
and               and   25
for               for   12
government government   11
oil               oil   10
</code></pre>
",0,1,933,2014-03-18 03:55:30,https://stackoverflow.com/questions/22470059/text-mining-clustering-analysis-in-r-error-two-dimensional-array
Calculating word frequencies in Python or R,"<p>Editing the question as directed by Tyler in the comments below.</p>

<p>As part of a larger text mining project, I have created a .csv file which has titles of books in the first column <strong></strong> and the whole contents of the book in the second column as <strong></strong>
My goal is to create a word cloud consisting of top n (n = 100 or 200 or 1000 depending on how skewed the scores are going to be) most frequently repeated words in the text for each title after removing the common stop words in English (for which the R-tm (text mining) package has a beautiful function - <strong>removeStopwords</strong>). 
Hope this explains my problem better.</p>

<p>Problem statement:</p>

<p>My input is in the below format in a csv file:</p>

<pre><code>title   text
1   &lt;huge amount of text1&gt;
2   &lt;huge amount of text2&gt;
3   &lt;huge amount of text3&gt;
</code></pre>

<p>Here's a MWE with similar data:</p>

<pre><code>library(tm)
data(acq)
dat &lt;- data.frame(title=names(acq[1:3]), text=unlist(acq[1:3]), row.names=NULL)
</code></pre>

<p>I would like to find out the top <strong>""n""</strong> terms by frequency appearing in the corresponding text for each title excluding the stop words. The ideal output would be a table in excel or csv that would look like: </p>

<pre><code>title   term    frequency
1       ..       ..
1       ..       ..
1       
1       
1       
2       
2       
2       
2       
2       
3       
3       
3       ..      ..
</code></pre>

<p>Please guide if this could be accomplished R or Python.
Anyone please?</p>
","python, r, text-mining","<p>In Python, you can use <code>Counter</code> from the collections module, and <code>re</code> to split the sentence at each word, giving you this:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; t = ""This is a sentence with many words. Some words are repeated""
&gt;&gt;&gt; Counter(re.split(r'\W', t)).most_common()
[('words', 2), ('a', 1), ('', 1), ('sentence', 1), ('This', 1), ('many', 1), ('is', 1), ('Some', 1), ('repeated', 1), ('are', 1), ('with', 1)]
</code></pre>
",3,0,1675,2014-03-21 09:19:34,https://stackoverflow.com/questions/22554639/calculating-word-frequencies-in-python-or-r
Is there any algorithm or solution for term identification?,"<p>I want to extract terminological units from a corpus of specialized documents. Is there any algorithm or out-of-box solution for this? Can nltk do this?</p>

<p>It seems this thread addressed my question.
<a href=""https://stackoverflow.com/questions/11320776/extracting-terms-with-contextual-relevance-noun-phrases-from-text-in-a-net-pr"">Extracting terms with contextual relevance (noun phrases) from text in a .NET project</a></p>
","machine-learning, nlp, text-mining","<p>The description of what you want isn't very clear. To get better help you should probably also post an example</p>

<p>It sounds like what you're looking for is called <a href=""http://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow"">Named Entity Recognition</a>. Depending exactly on what you want (and your data) there are existing systems that are very good, but the problem is definitely not solved. If this is what you want, important systems to look at are <a href=""http://en.wikipedia.org/wiki/General_Architecture_for_Text_Engineering"" rel=""nofollow"">GATE</a>, <a href=""https://opennlp.apache.org/"" rel=""nofollow"">Apache OpenNLP</a> and even <a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a>.</p>
",1,0,45,2014-03-25 10:26:56,https://stackoverflow.com/questions/22631676/is-there-any-algorithm-or-solution-for-term-identification
How to calculate TF*IDF for a single new document to be classified?,"<p>I am using document-term vectors to represent a collection of document. I use <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""noreferrer"">TF*IDF</a> to calculate the term weight for each document vector. Then I could use this matrix to train a model for document classification.</p>

<p>I am looking forward to classify new document in future. But in order to classify it, I need to turn the document into a document-term vector first, and the vector should be composed of TF*IDF values, too.</p>

<p><strong>My question is, how could I calculate the TF*IDF with just a single document?</strong></p>

<p>As far as I understand, TF can be calculated based on a single document itself, but the IDF can only be calculated with a collection of document. In my current experiment, I actually calculate the TF*IDF value for the <strong>whole</strong> collection of documents. And then I use <strong>some</strong> documents as training set and <strong>the others</strong> as test set.</p>

<p>I just suddenly realized that this seems not so applicable to real life.</p>

<h2>ADD 1</h2>

<p>So there are actually 2 subtly different scenarios for classification:</p>

<ol>
<li>to classify some documents whose content are known but label are not
known.</li>
<li>to classify some totally unseen document.</li>
</ol>

<p>For 1, we can combine <strong>all</strong> the documents, both with and without labels. And get the TF*IDF over all of them. <strong>This way, even we only use the documents <em>with labels</em> for training, the training result will still contain the influence of the documents <em>without labels</em>.</strong></p>

<p>But my scenario is 2.</p>

<p>Suppose I have the following information for term <strong><em>T</em></strong> from the summary of the <strong>training</strong> set corpus:</p>

<ul>
<li>document count for T in the training set is <strong><em>n</em></strong></li>
<li>total number of training documents is <strong><em>N</em></strong></li>
</ul>

<p>Should I calculate the IDF of t for a <strong>unseen</strong> document D as below?</p>

<p><strong>IDF(t, D)= log((N+1)/(n+1))</strong> </p>

<h2>ADD 2</h2>

<p>And what if I encounter a term in the new document <strong>which didn't show up in the training corpus before</strong>?
How should I calculate the weight for it in the doc-term vector?</p>
","machine-learning, classification, information-retrieval, text-mining, document-classification","<p>TF-IDF doesn't make sense for a single document, independent of a corpus.  It's fundamentally about emphasizing relatively rare and informative words.</p>

<p>You need to keep corpus summary information in order to compute TF-IDF weights.  In particular, you need the document count for each term and the total number of documents.</p>

<p>Whether you want to use summary information from the whole training set and test set for TF-IDF, or for just the training set is a matter of your problem formulation.  If it's the case that you only care to apply your classification system to documents whose contents you have, but whose labels you do not have (this is actually pretty common), then using TF-IDF for the entire corpus is okay.  If you want to apply your classification system to entirely unseen documents after you train, then you only want to use the TF-IDF summary information from the training set.</p>
",12,25,15506,2014-04-01 15:59:04,https://stackoverflow.com/questions/22790974/how-to-calculate-tfidf-for-a-single-new-document-to-be-classified
Why can we use entropy to measure the quality of language model?,"<p>I am reading the &lt; <a href=""http://nlp.stanford.edu/fsnlp/"" rel=""nofollow"">Foundations of Statistical Natural Language Processing</a> >. It has the following statement about the relationship between information entropy and language model:</p>

<blockquote>
  <p>...The essential point here is that if a model captures more of the
  structure of a language, then the entropy of the model should be
  lower. In other words, we can sue entropy as a measure of the quality
  of our models...</p>
</blockquote>

<p>But how about this example:</p>

<p>Suppose we have a machine that spit 2 characters, A and B, one by one. And the designer of the machine makes A and B has the equal probability.</p>

<p>I am not the designer. And I try to model it through experiment.</p>

<p>During a initial experiment, I see the machine split the following character sequence:</p>

<blockquote>
  <p>A, B, A</p>
</blockquote>

<p>So I model the machine as P(A)=2/3 and P(B)=1/3. And we can calculate entropy of this model as :</p>

<pre><code>-2/3*Log(2/3)-1/3*Log(1/3)= 0.918 bit  (the base is 2)
</code></pre>

<p>But then, the designer tell me about his design, so I refined my model with this more information. The new model looks like this:</p>

<p>P(A)=1/2 P(B)=1/2</p>

<p>And the entropy of this new model is:</p>

<pre><code>-1/2*Log(1/2)-1/2*Log(1/2) = 1 bit
</code></pre>

<p>The second model is obviously better than the first one. But the entropy increased.</p>

<p>My point is, due to the arbitrariness of the model being tried, we cannot blindly say a smaller entropy indicates a better model.</p>

<p>Could anyone shed some light on this?</p>

<h2>ADD 1</h2>

<p>(Much thanks to Rob Neuhaus!)</p>

<p>Yes, after I re-digested the mentioned NLP book. I think I can explain it now.</p>

<p>What I calculated is actually the entropy of the language model distribution. It cannot be used to evaluate the effectiveness of a language model.</p>

<p>To evaluate a language model, we should measure how much <strong><em>surprise</em></strong> it gives us for real sequences in that language. For each real word encountered, the language model will give a probability <strong>p</strong>. And we use <strong>-log(p)</strong> to quantify the surprise. And we average the total surprise over a long enough sequence. So, in case of a 1000-letter sequence with 500 A and 500 B,
the surprise given by the 1/3-2/3 model will be:</p>

<p>[-500*log(1/3) - 500*log(2/3)]/1000 = 1/2 * Log(9/2)</p>

<p>While the correct 1/2-1/2 model will give:</p>

<p>[-500*log(1/2) - 500*log(1/2)]/1000 = 1/2 * Log(8/2)</p>

<p><strong>So, we can see, the 1/3, 2/3 model gives more surprise, which indicates it is worse than the correct model.</strong></p>

<p>Only when the sequence is long enough, the average effect will mimic the expectation over the 1/2-1/2 distribution. If the sequence is short, it won't give a convincing result.</p>

<p>I didn't mention the <strong>cross-entropy</strong> here since I think this jargon is too intimidating and not much helpful to reveal the root cause.</p>
","machine-learning, nlp, data-mining, text-mining","<p>If you had a larger sample of data, it's very likely that the model that assigns 2/3 to A and 1/3 to B will do worse than the true model, which gives 1/2 to each.  The problem is that your training set is too small, so you were mislead into thinking the wrong model was better.  I encourage you to experiment, generate a random string of length 10000, where each character equally likely.  Then measure the <a href=""http://en.wikipedia.org/wiki/Cross_entropy#Estimation"" rel=""nofollow"">cross entropy</a> of the 2/3,1/3 model vs the 1/2,1/2 model on that much longer string.  I am sure you will see the latter performs better.  Here is some sample Python code demonstrating the fact.  </p>

<pre><code>from math import log
import random

def cross_entropy(prediction_probability_seq):
    probs = list(prediction_probability_seq)
    return -sum(log(p, 2) for p in probs) / len(probs)

def predictions(seq, model):
    for item in seq:
         yield model[item]

rand_char_seq = [random.choice(['a', 'b']) for _ in xrange(1000)]

def print_ent(m):
    print 'cross entropy of', str(m), \
        cross_entropy(predictions(rand_char_seq, m))

print_ent({'a': .5, 'b': .5})
print_ent({'a': 2./3, 'b': 1./3})
</code></pre>

<p>Notice that if you add an extra 'a' to the choice, then the second model (which is closer to the true distribution) gets lower cross entropy than the first.</p>

<p>However, one other thing to consider is that you really want to measure the likelihood on held out data that you didn't observe during training.  If you do not do this, more complicated models that memorize the noise in the training data will have an advantage over smaller/simpler models that don't have as much ability to memorize noise.</p>

<p>One real problem with likelihood as measuring language model quality is that it sometimes doesn't perfectly predict the actual higher level application error rate.  For example, language models are often used in speech recognition systems.  There have been improved language models (in terms of entropy) that didn't drive down the overall system's word error rate, which is what the designers really care about.  This can happen if the language model improves predictions where the entire recognition system is already confident enough to get the right answer.</p>
",2,3,1680,2014-04-08 09:49:35,https://stackoverflow.com/questions/22933412/why-can-we-use-entropy-to-measure-the-quality-of-language-model
dimension reduction in spam filtering,"<p>I'm performing an experiment in which I need to compare classification performance of several classification algorithms for spam filtering, viz. Naive Bayes, SVM, J48, k-NN, RandomForests, etc. I'm using the WEKA data mining tool. While going through the literature I came to know about various dimension reduction methods which can be broadly classified into two types-</p>

<ol>
<li>Feature Reduction: Principal Component Analysis, Latent Semantic Analysis, etc.</li>
<li>Feature Selection: Chi-Square, InfoGain, GainRatio, etc.</li>
</ol>

<p>I have also read this tutorial of WEKA by Jose Maria in his blog: <a href=""http://jmgomezhidalgo.blogspot.com.es/2013/02/text-mining-in-weka-revisited-selecting.html"" rel=""nofollow"">http://jmgomezhidalgo.blogspot.com.es/2013/02/text-mining-in-weka-revisited-selecting.html</a></p>

<p>In this blog he writes, ""A typical text classification problem in which dimensionality reduction can be a big mistake is spam filtering"". So, now I'm confused whether dimensionality reduction is of any use in case of spam filtering or not?</p>

<p>Further, I have also read in the literature about Document Frequency and TF-IDF as being one of feature reduction techniques. But I'm not sure how does it work and come into play during classification.</p>

<p>I know how to use weka, chain filters and classifiers, etc. The problem I'm facing is since I don't have enough idea about feature selection/reduction (including TF-IDF) I am unable to decide how and what feature selection techniques and classification algorithms I should combine to make my study meaningful. I also have no idea about optimal threshold value that I should use with chi-square, info gain, etc.</p>

<p>In StringToWordVector class, I have an option of IDFTransform, so does it makes sence to set it to TRUE and also use a feature selection technique, say InfoGain?</p>

<p>Please guide me and if possible please provide links to resources where I can learn about dimension reduction in detail and can plan my experiment meaningfully!</p>
","data-mining, weka, text-mining, spam-prevention, text-classification","<p>Well, Naive Bayes seems to work best for spam filtering, and it doesn't play nicely with dimensionality reduction.</p>

<p>Many dimensionality reduction methods try to identify the features of the highest variance. This of course won't help a lot with spam detection, you want discriminative features.</p>

<p>Plus, there is not only one type of spam, but many. Which is likely why naive Bayes works better than many other methods that assume there is only one type of spam.</p>
",0,0,465,2014-04-09 10:38:38,https://stackoverflow.com/questions/22960024/dimension-reduction-in-spam-filtering
How to extract support vectors from SVM classifier in Python?,"<p>I'm using LinearSVM from SciKit library in python to classify text data, and it works perfectly. 
My question is that is there a way to find the support vectors of my classifier model?
I think the support vectors for my data would be a list of words!</p>

<p>I need this because I want to find a distinguishing feature of the classifier for different text data. (How different text data are distinct)</p>

<p>Thanks in advance.</p>
","python, classification, svm, text-mining","<p>For the SVM case in scikit-learn you should be able to access the support vectors in the following way:</p>

<pre><code>&gt;&gt;&gt; # get support vectors
&gt;&gt;&gt; clf.support_vectors_
array([[ 0.,  0.],
       [ 1.,  1.]])
&gt;&gt;&gt; # get indices of support vectors
&gt;&gt;&gt; clf.support_ 
array([0, 1]...)
&gt;&gt;&gt; # get number of support vectors for each class
&gt;&gt;&gt; clf.n_support_ 
array([1, 1]...)
</code></pre>

<p>[Source: <a href=""http://scikit-learn.org/stable/modules/svm.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/svm.html</a> ]</p>
",1,2,5783,2014-04-15 17:21:38,https://stackoverflow.com/questions/23090420/how-to-extract-support-vectors-from-svm-classifier-in-python
algorithm to get topic / focus of sentence out of words in sentence,"<p>Are there any well-know or successful algorithms for obtaining the topic and / or focus of a sentence ( question ) out of the words in the sentence question?</p>

<p>If not, how would I got about getting the topic / focus of the question. It seems that the topic / focus of the questions is usually a noun or a noun-phrase.</p>

<p>So the first thing I would do is determine the nouns by Part Of Speech tagging the question. but then how do I know if I should get just the nouns or the noun(s) and a adjective before it, or the noun and the adverb before it, or the noun(s) and verb?</p>

<p>For example:</p>

<p>In ' did the quick brown fox jump over the lazy dog ', get ' quick brown fox ', ' jump ', and ' lazy dog '.</p>

<p>In ' what is the population of japan ', get ' population ' and ' japan '</p>

<p>In ' what color is milk ' get ' color ' and ' milk '</p>

<p>In ' What is the height of Mt. Everest ' get ' Mt. Everst ' and ' Height '.</p>

<p><strong>While writing these I guess the easiest way is removing stop words.</strong></p>
","algorithm, nlp, text-mining","<p>This could be thought of as a parsing problem and I personally find the <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">stanford nlp</a> tool  very effective . </p>

<p><a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow"">Here</a> is the link to the demo of the stanford parser</p>

<p>For the example , did the quick brown fox jump over the lazy dog
The output you get is </p>

<pre><code>did/VBD
the/DT
quick/JJ
brown/JJ
fox/NN
jump/VB
over/RP
the/DT
lazy/JJ
dog/NN
</code></pre>

<p>From the output you can write an extractor to extract the nouns ( adjectives and adverbs if need be) and thus obtain the topics from the sentence .</p>

<p>Moreover , the parse tree looks like </p>

<pre><code>(ROOT
  (SINV (VBD did)
    (NP (DT the) (JJ quick) (JJ brown) (NN fox))
    (VP (VB jump)
      (PRT (RP over))
      (NP (DT the) (JJ lazy) (NN dog)))))
</code></pre>

<p>If you take a closer look at the parse tree , the output you are expecting are both the NP(noun phrases) - the quick brown fox and the lazy dog . </p>

<p>I hope this helps ! </p>
",3,3,2178,2014-04-21 20:35:21,https://stackoverflow.com/questions/23205556/algorithm-to-get-topic-focus-of-sentence-out-of-words-in-sentence
Clustering one text file into groups and topics in python,"<p>I'm new in text mining and I have a very big text file where every line represents a review about an item (a sentence).</p>

<p>I would like to find both the groups and the topics that exist within the reviews.
So my question is what are the features, groups and topics of my data? Could the occurence frequency of each word be used as features? Do we have to consider every line (review) as a document itself then we have to cluster the reviews? I'm also wondering if the number of groups or topic should be known in prior since in any unsupervised algorithm the number of clusters is supposed to be a known parameter.</p>

<p>My second question is how can I edit this <a href=""http://scikit-learn.org/stable/auto_examples/document_clustering.html#example-document-clustering-py"" rel=""nofollow"">k-means clustering code</a> to find the groups and the <a href=""http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html#example-applications-topics-extraction-with-nmf-py"" rel=""nofollow"">NMF code</a> to find topics using my reviews.txt file.</p>
","python, cluster-analysis, text-mining, feature-extraction, text-analysis","<p>Firstly, as suggested in the comments, you can grab the basics from a good book on text mining or information retrieval. My suggestions is: <a href=""http://nlp.stanford.edu/IR-book/"" rel=""nofollow"">Introduction to Information Retrieval</a>.</p>

<p>Now trying to briefly answer your queries:</p>

<p>//my question is what are the features// - As in most text mining problems, features in your case could be terms (words) in every sentence. You can estimate the term frequencies and use <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">TF-IDF</a> representation,a very popular way of representing documents. </p>

<p>//groups// - Since every sentence represents an individual review, you can think of every sentence as a tiny document and use <a href=""http://en.wikipedia.org/wiki/Document_clustering"" rel=""nofollow"">document clustering</a> to identify the groups.</p>

<p>//topics of my data?// - Yes, there is something called <a href=""http://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow"">topic modelling</a>, which will help you to identify the topics in from a collection of documents. But, not sure if it applies to your problem.</p>

<p>//Do we have to consider every line (review) as a document itself then we have to cluster the reviews? // - Yes.</p>

<p>//I'm also wondering if the number of groups or topic should be known in prior since in any unsupervised algorithm the number of clusters is supposed to be a known parameter.// - This is not really the case. Many clustering algorithms do not expect prior knowledge on no. of clusters, such as <a href=""http://en.wikipedia.org/wiki/Hierarchical_clustering"" rel=""nofollow"">hierarchical clustering</a>, <a href=""http://en.wikipedia.org/wiki/Affinity_propagation"" rel=""nofollow"">affinity propagation</a>. Even for algorithms which expect the no. of clusters, there are a <a href=""http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set"" rel=""nofollow"">number of ways</a> to predict this.</p>
",2,1,2563,2014-04-25 02:18:55,https://stackoverflow.com/questions/23283115/clustering-one-text-file-into-groups-and-topics-in-python
"GATE API and JAPE code, return empty result","<p>I used GATE API with java code and tried to run one of the known JAPE rules on text of document but unfortunatly I could not get the appropriate results. My code as following:</p>

<pre><code>public void initAnnie() throws GateException, IOException {
    Out.prln(""Initialising ANNIE..."");

    // load the ANNIE application from the saved state in plugins/ANNIE
    File pluginsHome = Gate.getPluginsHome();
    File anniePlugin = new File(pluginsHome, ""ANNIE"");
    File annieGapp = new File(anniePlugin, ""ANNIE_with_defaults.gapp"");
    annieController = (CorpusController) PersistenceManager
            .loadObjectFromFile(annieGapp);

    Out.prln(""...ANNIE loaded"");
} // initAnnie()

/** Tell ANNIE's controller about the corpus you want to run on */
public void setCorpus(Corpus corpus) {
    annieController.setCorpus(corpus);
} // setCorpus

/** Run ANNIE */
public void execute() throws GateException {
    Out.prln(""Running ANNIE..."");
    annieController.execute();
    Out.prln(""...ANNIE complete"");
} // execute()

/**
 * Run from the command-line, with a list of URLs as argument.
 * &lt;P&gt;
 * &lt;B&gt;NOTE:&lt;/B&gt;&lt;BR&gt;
 * This code will run with all the documents in memory - if you want to
 * unload each from memory after use, add code to store the corpus in a
 * DataStore.
 */
public static void main(String args[]) throws GateException, IOException {
// initialise the GATE library
Out.prln(""Initialising GATE..."");
Gate.init();
Out.prln(""...GATE initialised"");
// load ANNIE plugin - you must do this before you can create tokeniser
// or JAPE transducer resources.
Gate.getCreoleRegister().registerDirectories(
new File(Gate.getPluginsHome(), ""ANNIE"").toURI().toURL());

 // Build the pipeline
  SerialAnalyserController pipeline =
 (SerialAnalyserController)Factory.createResource(
   ""gate.creole.SerialAnalyserController"");
  LanguageAnalyser tokeniser = (LanguageAnalyser)Factory.createResource(
  ""gate.creole.tokeniser.DefaultTokeniser"");
LanguageAnalyser jape = (LanguageAnalyser)Factory.createResource(
 ""gate.creole.Transducer"", gate.Utils.featureMap(
     ""grammarURL"", new     
 File(""C:path/to/univerity_rules.jape"").toURI().toURL(),
   ""encoding"", ""UTF-8"")); // ensure this matches the file
pipeline.add(tokeniser);
pipeline.add(jape);

// create document and corpus
// create a GATE corpus and add a document for each command-line
// argument
Corpus corpus = Factory.newCorpus(""JAPE corpus"");

 URL u = new URL(""file:/path/to/Document.txt"");
 FeatureMap params = Factory.newFeatureMap();
 params.put(""sourceUrl"", u);
 params.put(""preserveOriginalContent"", new Boolean(true));
 params.put(""collectRepositioningInfo"", new Boolean(true));
 Out.prln(""Creating doc for "" + u);
 Document doc = (Document)
   Factory.createResource(""gate.corpora.DocumentImpl"", params);
 corpus.add(doc);
 pipeline.setCorpus(corpus);

// run it
pipeline.execute();

// extract results
System.out.println(""Found annotations of the following types: "" +
  doc.getAnnotations().getAllTypes());


} // main

 }
</code></pre>

<p>and the JAPE rule used as follow:</p>

<pre><code>Phase:firstpass 
Input: Lookup Token 

//note that we are using Lookup and Token both inside our rules. 
Options: control = appelt


Rule: University1 
Priority: 20
(
  {Token.string == ""University""} 
  {Token.string == ""of""}
  {Lookup.minorType == city} 
):orgName 
--&gt;
:orgName.Organisation = 
  {kind = ""university"", rule = ""University1""}
</code></pre>

<p>and finally the result that I got as follow:</p>

<pre><code> Initialising GATE...
 log4j:WARN No appenders could be found for logger (gate.Gate).
 log4j:WARN Please initialize the log4j system properly.
 log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
  ...GATE initialised
 Creating doc for file:path/to/Document.txt
 Found annotations of the following types: [SpaceToken, Token]
</code></pre>

<p>Please any help</p>
","java, text-mining, gate, text-analysis","<p>The problem is that you don't have ""Lookup"" annotations you are trying to use in your JAPE program.</p>

<p>You need to add 2 additional resources:</p>

<pre><code>    LanguageAnalyser gazetter = (LanguageAnalyser)Factory.createResource(
            ""gate.creole.gazetteer.DefaultGazetteer"");
    LanguageAnalyser splitter = (LanguageAnalyser)Factory.createResource(
            ""gate.creole.splitter.SentenceSplitter"");
</code></pre>

<p>Your processing resources should run in following order:</p>

<pre><code>    pipeline.add(tokeniser);
    pipeline.add(gazetter);
    pipeline.add(splitter); 
    pipeline.add(jape);
</code></pre>

<p>Gazetterr will create ""Lookup"" annotations.</p>

<p>Sentence splitter will stop creating ""Organisation"" annotations that span over two sentences.</p>

<p>that was tested, and it works for me.</p>

<pre><code>...GATE initialised
Creating doc for file:/Users/andreyshafirin/tmp/testdoc.txt
Found annotations of the following types: [Lookup, Organisation, Token, Split, SpaceToken, Sentence]
</code></pre>

<p>PS:</p>

<p>I think there is a better approach to work with GATE from Java code.
You can create application in GATE Developer, customize it and save it to file (<a href=""http://gate.ac.uk/sale/tao/splitch3.html"" rel=""nofollow"">here you will find how</a>). Then you can load GATE application from your java code (see this <a href=""http://gate.ac.uk/wiki/code-repository/javadoc/src-html/sheffield/examples/BatchProcessApp.html"" rel=""nofollow"">example for you</a>, and more <a href=""http://gate.ac.uk/wiki/code-repository/"" rel=""nofollow"">other examples here</a> to get idea how). This way you don't have to worry about bunch of details and features related to properties of processing resources (you will define and change them in GUI).</p>

<p>Good luck with GATE.</p>
",2,1,2214,2014-04-25 05:49:35,https://stackoverflow.com/questions/23285082/gate-api-and-jape-code-return-empty-result
Finding position information of repeated characters in string,"<p>I have a DNA sequence that I'm trying to determine the quality of the assembly by visualizing the presence and frequency of n (generic nucleotides). I can do this by putting it into a specific format and loading it into some other software. </p>

<p>The output would be tsv/csv with the following column names: startposition endposition value(end-start)</p>

<p>My first thought was to create a vector of characters from the string and then loop through recording the start and stop positions of a block of n's. It works, but I'm almost certain there is a better and easier way. I'm not very good with regex, but willing to use it with some help.</p>

<pre><code>stringa&lt;-""attaaatagccgcggaagacctttttcatatgatagaatccccaacacannnnnnacgtagacaeagagaagattttcccccggagagcgcgaatagannnnnnnnnccataatatttataaattaatttat""
stringvector&lt;-strsplit(stringa, NULL)[[1]]

newdf&lt;-data.frame(start=rep(NA, 10), end=rep(NA, 10), length=rep(NA, 10))
counter=1

for (i in 1:length(stringvector)){
    if (stringvector[i]==""n""){
        if (is.na(newdf[counter,][1])){
                #record start position
                newdf[counter,][1]&lt;-i
        } else {
            #do nothing you found the start already
        }
    }
    if (stringvector[i]!=""n"" &amp; !is.na(newdf[counter,][1]) &amp; is.na(newdf[counter,][2])){
        newdf[counter,][2]&lt;-i-1
        newdf[counter,][3]&lt;-newdf[counter,][2]-newdf[counter,][1]
        counter=counter+1
    }
}
</code></pre>

<p>The sequence lengths should not be bigger than about 50 million characters and I don't need to do this for many sequences, but was hoping for something more elegant. </p>

<p>Any ideas?</p>
","string, r, text-mining","<p>Another possibility using the <code>stringi</code> package:</p>

<pre><code>library(stringi)
m &lt;- stri_locate_all(str = ""annaannn"", regex = ""n+"")[[1]]
length &lt;- m[ , ""end""] - m[ , ""start""] + 1
cbind(m, length)
#      start end length
# [1,]     2   3      2
# [2,]     6   8      3
</code></pre>

<p><strong>Edit:</strong> Added benchmark. <code>stri_locate_all</code> seems fastest. Please note that I added an ""end"" variable to @ilir's answer to make it more comparable to the other two alternatives.</p>

<pre><code>fun_stri &lt;- function(x){
  m &lt;- stri_locate_all(str = x, regex = ""n+"")[[1]]
  length &lt;- m[ , ""end""] - m[ , ""start""] + 1
  cbind(m, length)
}

fun_greg &lt;- function(x){
  result &lt;- gregexpr(pattern=""n+"", c(x))[[1]]
  data.frame(start = result,
             end = result + attr(result, ""match.length"") - 1,
             length = attr(result, ""match.length""))
}

fun_rle &lt;- function(stringa, char = ""n"") { 
  tmp &lt;- rle(strsplit(stringa, NULL)[[1]])
  start &lt;- sapply(which(tmp$values == char)-1, function(x) sum(tmp$length[1:x]))+1 
  length &lt;- tmp$length[tmp$values == char]
  return(data.frame(start, end = start + length, length))
}

# just check results on short strings
fun_stri(""annaannn"")
fun_greg(""annaannn"")
fun_rle(""annaannn"")

fun_stri(stringa)
fun_greg(stringa)
fun_rle(stringa) 

library(microbenchmark)

# size = 1e4
x &lt;- paste(sample(c(""a"", ""t"", ""c"", ""g"", ""n""), size = 1e4, replace = TRUE), collapse = """")
microbenchmark(
  fun_stri(x),
  fun_greg(x),
  fun_rle(x),
  times = 10)

# Unit: microseconds
#        expr       min        lq     median        uq       max neval
# fun_stri(x)   535.221   574.753   632.9140   684.611   711.980    10
# fun_greg(x)   709.699   748.473   776.9815   790.286   913.068    10
#  fun_rle(x) 47588.602 48281.955 50071.7875 67811.410 87781.053    10

# size = 1e5 
x &lt;- paste(sample(c(""a"", ""t"", ""c"", ""g"", ""n""), 1e5, replace = TRUE), collapse = """")
microbenchmark(
  fun_stri(x),
  fun_greg(x),
  fun_rle(x),
  times = 10)

# Unit: milliseconds
#        expr         min          lq      median          uq         max neval
# fun_stri(x)    2.871487    2.963478    3.011184    3.202578    3.272142    10
# fun_greg(x)    4.842070    4.914295    5.013888    5.368927    5.490949    10
#  fun_rle(x) 3853.887170 3868.795788 3889.699217 4228.450830 4411.025536    10

# size = 5e7 (""about 50 million characters"" in OP)
x &lt;- paste(sample(c(""a"", ""t"", ""c"", ""g"", ""n""), size = 5e7, replace = TRUE), collapse = """")
microbenchmark(
  fun_stri(x),
  fun_greg(x),
  times = 10)

# Unit: seconds
#        expr      min      lq   median       uq      max neval
# fun_stri(x) 1.479089 1.54405 1.606676 1.757292 1.974795    10
# fun_greg(x) 2.381448 2.39754 2.422554 2.476974 2.643259    10
</code></pre>
",4,1,1072,2014-04-29 09:45:15,https://stackoverflow.com/questions/23361096/finding-position-information-of-repeated-characters-in-string
Creating a Corpus with Spanish Text in R,"<p>Trying to do some text-mining and wordcloud visualization on Spanish text. I actually have 9 different .txt files, but will just post one for reproduction.</p>

<blockquote>
  <p>""Nos los representantes del pueblo de la Nación ARGENTINA, reunidos en
  Congreso General Constituyente por voluntad y elección de las
  provincias que la componen, en cumplimiento de pactos preexistentes,
  con el objeto de constituir la unión nacional, afianzar la justicia,
  consolidar la paz interior, proveer la defensa común, promover el
  bienestar general, y asegurar los beneficios de la libertad, para
  nosotros, para nuestra posteridad, y para todos los hombres del mundo
  que quieran habitar en el suelo argentino: invocando la protección de
  Dios, fuente de toda razón y justicia: ordenamos, decretamos y
  establecemos esta Constitución, para la Nación ARGENTINA.""</p>
</blockquote>

<p>The file is saved as a .txt file. Below is my naïve attempt to generate the term-document-matrix with the correct encoding. When I inspect it, I am not getting the text as it is in the original file (""constitución"" becomes ""constitucif3n,"" for example). I'm new to text-mining, and knowing that the solution probably involves a wide variety of co-dependent adjustments, I figured I'd ask here instead of searching for 4 hours. Thanks in advance.</p>

<pre><code>#Generate Term-Document-Matrix

#Convert Text to Corpus and Clean
cleanCorpus &lt;- function(corpus) {
  corpus.tmp &lt;- tm_map(corpus, removePunctuation)
  corpus.tmp &lt;- tm_map(corpus.tmp, stripWhitespace)
  corpus.tmp &lt;- tm_map(corpus.tmp, tolower)
  corpus.tmp &lt;- tm_map(corpus.tmp, removeWords, stopwords(""spanish""))
  return(corpus.tmp)
}

generateTDM &lt;- function(path) {
  cor.tmp &lt;- Corpus(DirSource(directory=path, encoding=""ISO8859-1""))
  cor.cl &lt;- cleanCorpus(cor.tmp)
  tdm.tmp &lt;- TermDocumentMatrix(cor.cl)
  tdm.s &lt;- removeSparseTerms(tdm.tmp, 0.7)
}

tdm &lt;- generateTDM(pathname)
tdm.m &lt;- as.matrix(tdm)
</code></pre>
","r, encoding, text-mining, word-cloud","<p>Answer: Make sure the original text file is UTF-8 encoded. To do this, I had to change up my Saving preferences in TextEdit on Mac. This made everything work seamlessly.</p>
",1,4,2819,2014-05-05 23:59:22,https://stackoverflow.com/questions/23483826/creating-a-corpus-with-spanish-text-in-r
RTextTools package error,"<p>I am using create_container in RTextTools package in R, but getting the error:</p>

<pre><code>container &lt;- create_container(doc_matrix, rawTags, trainSize=1:0.8*nrow(tagsSub),
                              testSize=0.8*nrow(tagsSub)+1:nrow(tagsSub),
                              virgin=FALSE)

# Error in [.simple_triplet_matrix(matrix, totalSize, ) : subscript out of bounds !
</code></pre>

<p>Can anyone suggest what is going wrong?</p>
","r, text-mining","<p>It appears that you might not be specifying the sizes correctly. The <code>:</code> operator has a very high priority. You might try </p>

<pre><code>container &lt;- create_container(doc_matrix, rawTags, 
    trainSize=1:(0.8*nrow(tagsSub)), 
    testSize=(0.8*nrow(tagsSub)+1):nrow(tagsSub), 
    virgin=FALSE
)
</code></pre>
",1,0,1488,2014-05-07 00:57:16,https://stackoverflow.com/questions/23506964/rtexttools-package-error
Term frequency matrix,"<p>I have a string like this:</p>

<p>m&lt;-""abcdabcdbcadacbddabcc...""</p>

<p>I would like to generate a matrix like this:</p>

<p><img src=""https://i.sstatic.net/mFuDC.png"" alt=""enter image description here""></p>

<p>How can I do that in r?</p>
","r, text-mining, word-frequency, term-document-matrix","<p>This gives what I believe you're after:</p>

<pre><code>m &lt;- ""abcdabcdbcadacbddabcc""

library(qdap)

chars &lt;- unique(unlist(strsplit(m, """")))
terms &lt;- paste2(expand.grid(rep(list(chars), 3)), sep="""")
t(counts(termco(m, match.list=sort(terms)))[, -c(1:2)])
</code></pre>

<p>Output:</p>

<pre><code>    1
aaa 0
aab 0
aac 0
aad 0
aba 0
.
.
.
dcc 0
dcd 0
dda 1
ddb 0
ddc 0
ddd 0
</code></pre>
",3,-3,351,2014-05-23 13:45:36,https://stackoverflow.com/questions/23830925/term-frequency-matrix
Remove chararcters in text corpus,"<p>I'm analyzing a corpus of emails. Some emails contain URLs. When I apply the <code>removePunctuation</code> function from the <strong>tm</strong> library, I get <code>httpwww</code>, and then I lose the info of a web address. What I would like to do, is to replace the <code>""://""</code> with <code>"" ""</code> across all of the corpus. I tried <code>gsub</code>, but then I the datatype of the corpus changes and I can't continue to process it with <strong>tm</strong> package.</p>

<p>Here is an example:</p>

<p>As you can see, <code>gsub</code> changes the class of the corpus to an array of characters, causing <code>tm_map</code> to fail.</p>

<pre><code>&gt; corpus
# A corpus with 4257 text documents
&gt; corpus1 &lt;- gsub(""http://"",""http "",corpus)
&gt; class(corpus1)
# [1] ""character""
&gt; class(corpus)
# [1] ""VCorpus"" ""Corpus""  ""list""   
&gt; cleanSW &lt;- tm_map(corpus1,removeWords, stopwords(""english""))
# Error in UseMethod(""tm_map"", x) : 
# no applicable method for 'tm_map' applied to an object of class ""character""
&gt; cleanSW &lt;- tm_map(corpus,removeWords, stopwords(""english""))
&gt; cleanSW
# A corpus with 4257 text documents
</code></pre>

<p>How can I bypass it? Maybe there's a way to convert it back to corpus from array of characters?</p>
","regex, r, text-mining, tm","<p>Found a solution to this problem here: <a href=""https://stackoverflow.com/questions/18153504/removing-non-english-text-from-corpus-in-r-using-tm"">Removing non-English text from Corpus in R using tm()</a>, Corpus(VectorSource(dat1)) worked for me.</p>
",2,0,2041,2014-05-28 08:21:38,https://stackoverflow.com/questions/23906279/remove-chararcters-in-text-corpus
modifying stopwords in R&#39;s TM package,"<p>I have a problem modifying the english.dat stopword file from R's TM package.
Anything I add to it is unrecognized. I tried adding at the beginning of the file, the middle, the end, still nothing works. Only the original text of the file is recognized.
I tried saving the file as ASCI, UTF, UTF-8, to no avail.</p>

<p>Any ideas?</p>

<p>thanks</p>
","r, text-mining, stop-words, tm","<p>Try adding them this way, as a concatenation to the ""english"" list:</p>

<pre><code>myStopwords &lt;- c(stopwords('english'), ""available"", ""via"") to add words
myCorpus &lt;- tm_map(myCorpus, removeWords, myStopwords)
</code></pre>
",6,1,3210,2014-06-02 15:21:56,https://stackoverflow.com/questions/23998022/modifying-stopwords-in-rs-tm-package
Input data from database and process it,"<p>as shown in title, i have a code that can tokenize, stopping, and stemming for text mining.. but the code can only process the word i give in ""String result"" line.. how can i do if i want to input the data i have in database? and then the result is saved in other database..</p>

<pre><code>class TokenizeStopStem
{
private  Set&lt;String&gt; stop_word_set;

public TokenizeStopStem(){

    BufferedReader fileStream = null;
    try {
        stop_word_set = new HashSet&lt;String&gt;();
        String path = ""E:\\stopwords.txt"";
        File file = new File(path);
        fileStream = new BufferedReader(new FileReader(file));
        String kata;
        while ( ( kata = fileStream.readLine() ) != null)
            stop_word_set.add(kata);
    } catch (Exception ex) {
        Logger.getLogger(TokenizeStopStem.class.getName()).log(Level.SEVERE, null, ex);
    }

    String result = tokenizeStopStem(""Coba kalo yang ini mau bermain gimana jadinya kegantengan"");
    System.out.println(result);

}

public static void main(String[] args) {
    new TokenizeStopStem();

}


private  String tokenizeStopStem(String input) {

    TokenStream tokenStream = new StandardTokenizer(
            Version.LUCENE_36, new StringReader(input));
    tokenStream = new StopFilter(Version.LUCENE_36, tokenStream, stop_word_set);
    tokenStream = new IndonesianStemFilter(tokenStream);
    StringBuilder sb = new StringBuilder();
    OffsetAttribute offsetAttribute = tokenStream.addAttribute(OffsetAttribute.class);
    CharTermAttribute charTermAttr = tokenStream.getAttribute(CharTermAttribute.class);
    try{
        while (tokenStream.incrementToken()) {
            if (sb.length() &gt; 0) {
                sb.append("" "");
            }
            sb.append(charTermAttr.toString());
        }
    }
    catch (IOException e){
        System.out.println(e.getMessage());
    }
    return sb.toString();
}
</code></pre>
","java, text-mining","<p>You should replace your ""Coba kalo"" string with a method that returns a string called <code>loadRawData()</code>.  <code>loadRawData</code> would probably look something like this:</p>

<pre><code>private String loadRawData(String id) throws SQLException
{
    Connection con = getSourceConnection();
    Statement stmt = null;
    String query = ""select data from table where id='"" + id + ""'"";

    String rawData = null;
    try {
        stmt = con.createStatement();
        ResultSet rs = stmt.executeQuery(query);
        if (rs.next()) {
            rawData = rs.getString(""data"");
        }
    } finally {
        if (stmt != null) { stmt.close(); }
    }
    return rawData;
}
</code></pre>

<p>It calls <code>getSourceConnection()</code> which creates a Connection instance pointing to the database where you will retrieve your data.  <a href=""http://docs.oracle.com/javase/tutorial/jdbc/basics/connecting.html"" rel=""nofollow"">See here for how to establish a connection</a>.  The ""id"" passed in is presumably a way to uniquely identify your record, and in fact the data returned if found, will be the data found in column ""data"".  If there is no error, but no record is found, it will return null, so you should prepare for that possibility.</p>

<p>You should surround the call with a try ... catch, so you can deal with unexpected problems should they arise.  </p>

<p>If everything goes well, and you have your result, you should pass that result to a method called <code>saveTokenData</code> passing the string.  Unfortunately, I cannot know how you wish to save this information, so I cannot enter details, however you'll see that the code is very similar to the code above, with the exception that you would call <code>getDestinationConnection()</code> instead of <code>getSourceConnection()</code> and you would be calling <code>executeUpdate</code> with an update query rather than <code>executeQuery</code>.</p>

<p>This will do the trick, however, you should consider using threads to do this.  This will greatly improve the efficiency of this program, however, it will also considerably complicate it as well.  My advice is to first get it running, and afterwards you can attempt to use threads.  </p>

<p>Hope that helps!</p>

<p><a href=""http://docs.oracle.com/javase/tutorial/jdbc/basics/retrieving.html"" rel=""nofollow"">Reference for retrieving data from a database using JDBC.</a>.</p>
",1,0,81,2014-06-04 08:48:15,https://stackoverflow.com/questions/24032887/input-data-from-database-and-process-it
How do I separate words in a given text in R?,"<p>For example I have a text file with content as follows:</p>

<pre><code>I wantto separate those wordswhich arejoined.
</code></pre>

<p>How do I separate the words in this text so that I get this as output.</p>

<pre><code>I want to separate those words which are joined.
</code></pre>

<p>Basically, something which can detect meaningless words from the text and make them meaningful. </p>

<p>For example, the code should detect that ""wantto"" does not make any sense and after processing it, it should be able to return ""want to"" as output. </p>

<p>It may return some other meaningful combination of words but that is fine.   </p>
","r, text-mining","<p>I am attaching a <strong>quick and dirty code</strong> that should help you to correct atleast two word spelling errors without using the aspell. The dictionary I used is the big.txt from Peter Norvig's site which should be enough for common words. You can use the <code>correctSentence</code> function to see the results</p>

<pre><code>## big.txt Taken for Peter Norvig's basic spell checker data file
words &lt;- scan(""http://norvig.com/big.txt"", what = character())

split_matches &lt;-function(word) {
num_char &lt;- nchar(word)
return_str &lt;- character()
start_pos &lt;- 0
end_pos &lt;- num_char
for(i in 1:num_char)
{
    str &lt;- substr(word,1,num_char-i+1)
    if(str %in% words)
    {
      return_str &lt;- str
      start_pos &lt;- nchar(return_str)
      break
    }

 }
 return_str &lt;- c(return_str,substr(word,start_pos+1,end_pos))
 return_str

}

correctSentence &lt;- function(sentence) {
  list_of_words &lt;- strsplit(sentence,"" "")
  list_of_words  &lt;- list_of_words[[1]]
  num_words &lt;- length(list_of_words)

  output_str &lt;- character()
  for(i in 1:num_words){
  word &lt;- list_of_words[i]
  if(word %in% words) {
      paste(output_str,word,sep="" "")
      output_str &lt;- c(output_str,word)
  }
  else {
     output_str &lt;- c(output_str,split_matches(word))
  }

}
  output_str &lt;-paste(output_str,collapse="" "")
  output_str
}
# test this with your sentence
correctSentence(""I wantto separate those wordswhich arejoined"")
</code></pre>
",2,2,2128,2014-06-05 11:08:10,https://stackoverflow.com/questions/24058618/how-do-i-separate-words-in-a-given-text-in-r
Replace all words that begin and end in a certain character in R,"<p>I'm working on a corpus of email messages, and trying to replace all html tags in the corpus with the string ''. How can I replace all html tag using the fact that they begin with >&lt; and end with > ?</p>

<p>Example:</p>

<pre><code>&lt;html&gt;
  &lt;body&gt;
  This is some random text.
 &lt;p&gt;This is some text in a paragraph.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Should be translated to:</p>

<pre><code>&lt;html&gt;
  &lt;html&gt;
  This is some random text.
    &lt;html&gt;This is some text in a paragraph.&lt;html&gt;
  &lt;html&gt;
&lt;html&gt;
</code></pre>

<p>Thanks</p>
","r, text-mining, tm","<p>You should use the power of the regex with <code>gsub</code>. If you simply want to replace any <code>&lt;markup_name&gt;</code> by <code>&lt;hml&gt;</code> then <code>gsub(""&lt;[^&gt;]+&gt;"", ""&lt;html&gt;"", email_text)</code> will do it.</p>

<p>The trick is <code>[^&gt;]\+</code> which extends (<code>+</code>) the regex until the first <code>&gt;</code> (<code>[^&gt;]</code> matches any character that is not <code>&gt;</code>).</p>
",2,0,434,2014-06-06 19:43:55,https://stackoverflow.com/questions/24089538/replace-all-words-that-begin-and-end-in-a-certain-character-in-r
How to remove all text between &lt;style&gt; tags?,"<p>I'm doing a text mining assignment on R, and I have a corpus that includes some html documents. I would like to remove the <code>&lt;style&gt;</code> tags and all the text between them, preferably with gsub function.</p>

<p>Example:</p>

<p>Turn this:</p>

<pre><code>&lt;style&gt;
.s4-tn{
border-left: 1px #0071C5 solid;
padding: 0px;
margin: 0px;
font-family: ""Intel Clear"", Verdana, verdana, san-serif;
font-size: 15px;
font-weight: lighter;
color: #0071C5; }

.s4-toplinks .s4-tn a.selected:hover{
    color:#1F497D;
    text-decoration: none;
}
&lt;/style&gt;
&lt;img id=""corner"" src=""/sites/HR_ETM/SitePages/img/bottom_bar.png""/&gt;
</code></pre>

<p>to this:</p>

<pre><code>&lt;img id=""corner"" src=""/sites/HR_ETM/SitePages/img/bottom_bar.png""/&gt;
</code></pre>
","r, text-mining, gsub","<p>I would use <code>removeNodes</code></p>

<pre><code>library(XML)
doc &lt;- htmlParse(txt,asText=TRUE)
styleNodes &lt;- getNodeSet(doc, ""//style"")
removeNodes(styleNodes)
doc

&gt; removeNodes(styleNodes)
NULL
&gt; doc
&lt;!DOCTYPE html PUBLIC ""-//W3C//DTD HTML 4.0 Transitional//EN"" ""http://www.w3.org/TR/REC-html40/loose.dtd""&gt;
&lt;html&gt;
&lt;head&gt;&lt;/head&gt;
&lt;body&gt;&lt;img id=""corner"" src=""/sites/HR_ETM/SitePages/img/bottom_bar.png""&gt;&lt;/body&gt;
&lt;/html&gt;

&gt; 
</code></pre>

<p>To save your edited <code>XML</code> you can use <code>saveXML</code>:</p>

<pre><code>&gt; saveXML(doc)
[1] ""&lt;!DOCTYPE html PUBLIC \""-//W3C//DTD HTML 4.0 Transitional//EN\"" \""http://www.w3.org/TR/REC-html40/loose.dtd\""&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;/head&gt;\n&lt;body&gt;&lt;img id=\""corner\"" src=\""/sites/HR_ETM/SitePages/img/bottom_bar.png\""&gt;&lt;/body&gt;\n&lt;/html&gt;\n""
</code></pre>

<p>To select comment nodes use:</p>

<pre><code>commentNodes &lt;- getNodeSet(doc, ""//comment()"")
</code></pre>
",6,3,731,2014-06-10 15:13:11,https://stackoverflow.com/questions/24144622/how-to-remove-all-text-between-style-tags
How to remove consecutive upper case characters from a text in r?,"<p>For example I have a text </p>

<pre><code>a &lt;- ""This IS A SAMple sentence TMP""
</code></pre>

<p>I want the output to be as:</p>

<pre><code>""This A ple sentence""
</code></pre>

<p>How do i do it? Some easier way to do this?</p>
","r, text-mining","<pre><code>  library(stringr)
  str_trim(gsub(""[A-Z]{2,}"","""",a))
  [1] ""This  A ple sentence""
</code></pre>
",7,-2,1474,2014-06-13 11:04:45,https://stackoverflow.com/questions/24203852/how-to-remove-consecutive-upper-case-characters-from-a-text-in-r
Filter rows/documents from Document-Term-Matrix in R,"<p>Using the tm-package in R I create a Document-Term-Matrix:</p>

<pre><code>dtm &lt;- DocumentTermMatrix(cor, control = list(dictionary=c(""someTerm"")))
</code></pre>

<p>Whichs results in something like this: </p>

<pre><code>A document-term matrix (291 documents, 1 terms)

Non-/sparse entries: 48/243
Sparsity           : 84%
Maximal term length: 8 
Weighting          : term frequency (tf) 

                   Terms
Docs                someTerm
doc1                       0
doc2                       0
doc3                       7
doc4                       22
doc5                       0
</code></pre>

<p>Now I would like to filter this Document-Term-Matrix according to the number of the occurrences of someTerm in the documents. E.g. filter out only the documents where someTerm appears at least once. I.e. doc3 and doc4 here.  </p>

<p>How can I achieve this? </p>
","r, matrix, text-mining, tm","<p>It's very similar to how you would subset a regular R matrix. For example, to create a document term matrix from the example Reuters dataset with only rows where the term ""would"" appears more than once:</p>

<pre><code>reut21578 &lt;- system.file(""texts"", ""crude"", package = ""tm"")

reuters &lt;- VCorpus(DirSource(reut21578),
    readerControl = list(reader = readReut21578XMLasPlain))

dtm &lt;- DocumentTermMatrix(reuters)
v &lt;- as.vector(dtm[,""would""]&gt;1)
dtm2 &lt;- dtm[v, ]

&gt; inspect(dtm2[, ""would""])
A document-term matrix (3 documents, 1 terms)

Non-/sparse entries: 3/0
Sparsity           : 0%
Maximal term length: 5 
Weighting          : term frequency (tf)

     Terms
Docs  would
  246     2
  489     2
  502     2
</code></pre>

<p>A <code>tm</code> document term matrix is a simple triplet matrix from package <code>slam</code> so the <code>slam</code> documentation helps in figuring out how to manipulate dtms.</p>
",6,6,7358,2014-06-14 21:07:02,https://stackoverflow.com/questions/24224298/filter-rows-documents-from-document-term-matrix-in-r
unsupervised semantic clustering of phrases,"<p>I have about a thousand potential survey items as a vector of strings that I want to reduce to a few hundred. Normally when we talk about data reduction, we have actual data. I administer the items to participants and use factor analysis, PCA, or some other dimension reduction method. </p>

<p>In my case, I don't have any data. Just the items (i.e., text strings). I want to reduce the set by eliminating items with similar meanings. Presumably they would be highly correlated if actually administered to participants. </p>

<p>I've been reading about clustering approaches for textual analysis. This <a href=""https://stackoverflow.com/questions/9014313/how-to-find-similar-sentences-phrases-in-r"">SO question</a> demonstrates an approach I've seen used in different examples. The OP notes that the clustering solution does not quite answer his/her question. Here's how it would be applied (unsatisfactorily) in my case:</p>

<pre><code># get data (2 columns, 152 rows)
</code></pre>

<p><a href=""https://www.dropbox.com/s/af9bsjih48dem3o/text.R"" rel=""nofollow noreferrer"">link to text.R</a> file with dput() of sample items</p>

<pre><code># clustering
library(tm)
library(Matrix)
x &lt;- TermDocumentMatrix( Corpus( VectorSource(text$item) ) )
y &lt;- sparseMatrix( i=x$i, j=x$j, x=x$v, dimnames = dimnames(x) )  
plot( hclust(dist(t(y))) )
</code></pre>

<p>The plot shows that items 145 and 149 are clustered: </p>

<p>145 ""Lets you know you are not wanted"" </p>

<p>149 ""Lets you know he loves you""</p>

<p>These items share the same stem, ""lets you know"", which probably accounts for the clustering. Semantically, they are opposites. </p>

<p>The OP had a similar challenge with his/her example. A commenter pointed to the <code>wordnet</code> package as a possible solution.</p>

<p><strong>Question</strong> (edited based on feedback)</p>

<p>How can I prevent items like 145 and 149 from clustering because they share stems?</p>

<p>A secondary question with less programmatic focus: Does anyone see better solution here? Many approaches I've come across involve supervised learning, test/training datasets, and classification. I believe what I am looking for is more semantic similarity/clustering (e.g., FAC <a href=""http://scholar.harvard.edu/bstewart/files/tad2.pdf"" rel=""nofollow noreferrer"">pdf</a>). </p>
","r, cluster-analysis, text-mining, unsupervised-learning","<p>+1 to @TylerRinker's suggestions to </p>

<ul>
<li>remove stopwords and </li>
<li>use Jockers' methods of clustering using only nouns (I have a worked example of that <a href=""https://stackoverflow.com/a/16010600/1036500"">here</a>. </li>
</ul>

<p>Another option you should try is making your term document matrix from bigrams rather than unigrams. If you're interested in phrases, bigrams are a good start. I have a worked example of that <a href=""https://stackoverflow.com/a/15977288/1036500"">here</a>.</p>

<p>Here's a worked example of <strong>combining stopword removal with bigrams</strong>. With this example you can iterate using different parameter values to get the clustering that seems most sensible to you.</p>

<p>Get the data...</p>

<pre><code>dat &lt;- text &lt;- structure(list(id = c(""GHQ1"", ""GHQ2"", ""GHQ3"", ""GHQ4"", ""GHQ5"", 
                                 ""GHQ6"", ""GHQ7"", ""GHQ8"", ""GHQ9"", ""GHQ10"", ""GHQ11"", ""GHQ12"", ""GHQ13"", 
                                 ""GHQ14"", ""GHQ15"", ""GHQ16"", ""GHQ17"", ""GHQ18"", ""GHQ19"", ""GHQ20"", 
                                 ""GHQ21"", ""GHQ22"", ""GHQ23"", ""GHQ24"", ""CGMH9"", ""GHQ25"", ""GHQ26"", 
                                 ""GHQ27"", ""GHQ28"", ""GHQ29"", ""GHQ30"", ""GHQ31"", ""PARQ01A-P"", ""PARQ02A-P"", 
                                 ""PARQ03A-P"", ""PARQ04A-P"", ""PARQ05A-P"", ""PARQ06A-P"", ""PARQ07A-P"", 
                                 ""PARQ08A-P"", ""PARQ09A-P"", ""PARQ10A-P"", ""PARQ11A-P"", ""PARQ12A-P"", 
                                 ""PARQ13A-P"", ""PARQ14A-P"", ""PARQ15A-P"", ""PARQ16A-P"", ""PARQ17A-P"", 
                                 ""PARQ18A-P"", ""PARQ19A-P"", ""PARQ20A-P"", ""PARQ21A-P"", ""PARQ22A-P"", 
                                 ""PARQ23A-P"", ""PARQ24A-P"", ""PARQ25A-P"", ""PARQ26A-P"", ""PARQ27A-P"", 
                                 ""PARQ28A-P"", ""PARQ29A-P"", ""PARQ30A-P"", ""PARQ31A-P"", ""PARQ32A-P"", 
                                 ""PARQ33A-P"", ""PARQ34A-P"", ""PARQ35A-P"", ""PARQ36A-P"", ""PARQ37A-P"", 
                                 ""PARQ38A-P"", ""PARQ39A-P"", ""PARQ40A-P"", ""PARQ41A-P"", ""PARQ42A-P"", 
                                 ""PARQ43A-P"", ""PARQ44A-P"", ""PARQ45A-P"", ""PARQ46A-P"", ""PARQ47A-P"", 
                                 ""PARQ48A-P"", ""PARQ49A-P"", ""PARQ50A-P"", ""PARQ51A-P"", ""PARQ52A-P"", 
                                 ""PARQ53A-P"", ""PARQ54A-P"", ""PARQ55A-P"", ""PARQ56A-P"", ""PARQ57A-P"", 
                                 ""PARQ58A-P"", ""PARQ59A-P"", ""PARQ60A-P"", ""PARQ01A-C"", ""PARQ02A-C"", 
                                 ""PARQ03A-C"", ""PARQ04A-C"", ""PARQ05A-C"", ""PARQ06A-C"", ""PARQ07A-C"", 
                                 ""PARQ08A-C"", ""PARQ09A-C"", ""PARQ10A-C"", ""PARQ11A-C"", ""PARQ12A-C"", 
                                 ""PARQ13A-C"", ""PARQ14A-C"", ""PARQ15A-C"", ""PARQ16A-C"", ""PARQ17A-C"", 
                                 ""PARQ18A-C"", ""PARQ19A-C"", ""PARQ20A-C"", ""PARQ21A-C"", ""PARQ22A-C"", 
                                 ""PARQ23A-C"", ""PARQ24A-C"", ""PARQ25A-C"", ""PARQ26A-C"", ""PARQ27A-C"", 
                                 ""PARQ28A-C"", ""PARQ29A-C"", ""PARQ30A-C"", ""PARQ31A-C"", ""PARQ32A-C"", 
                                 ""PARQ33A-C"", ""PARQ34A-C"", ""PARQ35A-C"", ""PARQ36A-C"", ""PARQ37A-C"", 
                                 ""PARQ38A-C"", ""PARQ39A-C"", ""PARQ40A-C"", ""PARQ41A-C"", ""PARQ42A-C"", 
                                 ""PARQ43A-C"", ""PARQ44A-C"", ""PARQ45A-C"", ""PARQ46A-C"", ""PARQ47A-C"", 
                                 ""PARQ48A-C"", ""PARQ49A-C"", ""PARQ50A-C"", ""PARQ51A-C"", ""PARQ52A-C"", 
                                 ""PARQ53A-C"", ""PARQ54A-C"", ""PARQ55A-C"", ""PARQ56A-C"", ""PARQ57A-C"", 
                                 ""PARQ58A-C"", ""PARQ59A-C"", ""PARQ60A-C""), item = c(""Been feeling unhappy or depressed"", 
                                                                                  ""Been feeling reasonably happy, all things considered"", ""Feeling edgy and bad-tempered"", 
                                                                                  ""Feel constantly under strain"", ""Found everything getting on top of you"", 
                                                                                  ""Been feeling nervous and strung-up all the time"", ""found at times you couldn't do anything because your nerves were too bad"", 
                                                                                  ""found everything getting on top of you"", ""thought of the possibility that you might make away with yourself"", 
                                                                                  ""found that the idea of taking your own life kept coming into your mind?"", 
                                                                                  ""found yourself withing you were dead and away from it all?"", 
                                                                                  ""felt that life isn't worth living"", ""felt that life was entirely hopeless?"", 
                                                                                  ""been able to enjoy your normal day-to-day activities"", ""been satisfied with the way you've carried out your task"", 
                                                                                  ""felt that you are playing a useful part in things"", ""felt on the whole you were doing things well?"", 
                                                                                  ""been feeling perfectly well and in good health"", ""been feeling in need of a good tonic"", 
                                                                                  ""been feeling run down and out of sorts?"", ""felt that you are ill"", 
                                                                                  ""been getting any pains in your head"", ""been getting a feeling of tightness or pressure in your head"", 
                                                                                  ""been having hot or cold spells"", ""Do you feel you have physical problems because of stress?"", 
                                                                                  ""Lost sleep over worry"", ""Had difficulty in staying asleep once you are off"", 
                                                                                  ""felt capable of making decisions about things"", ""been taking longer over the things that you do"", 
                                                                                  ""been managing to keep yourself busy and occupied"", ""been thinking of yourself as a worthless person"", 
                                                                                  ""been getting scared or panicky for no good reason "", ""You say nice things about your child"", 
                                                                                  ""You nag or scold your child when (s)he is bad"", ""You ignore your child"", 
                                                                                  ""You wonder if you really love your child"", ""You talk to your child about daily routines and plans, and listen to what (s)he has to say"", 
                                                                                  ""You complain about your child to others when (s)he does not listen to you"", 
                                                                                  ""You take an interest in your child"", ""You want your child to bring friends home, and you try to make things pleasant for them"", 
                                                                                  ""You call your child names and make fun of him/her"", ""You ignore your child as long as (s)he does nothing to bother you"", 
                                                                                  ""You yell at your child when you are angry"", ""You sit close with your child so that (s)he feels free to talk about important things"", 
                                                                                  ""You are harsh with your child"", ""You enjoy having your child around you"", 
                                                                                  ""You make your child feel proud when (s)he does well"", ""Your hit your child even when (s)he may not deserve it, like for small mistakes"", 
                                                                                  ""You forget things you are supposed to do for your child"", ""You see your child as an annoyance"", 
                                                                                  ""You praise your child to others"", ""You punish your child when you are angry"", 
                                                                                  ""You make sure your child has the right kind of food to eat"", 
                                                                                  ""You talk to your child in a warm and loving way"", ""You get angry easily at your child"", 
                                                                                  ""You are too busy to answer your child's questions"", ""You hate/despise your child"", 
                                                                                  ""You say nice things to your child when (s)he deserves it, such as when (s)he does well in school"", 
                                                                                  ""You are irritable with your child"", ""You care about who your child's friends are"", 
                                                                                  ""You are really interested in what your child does"", ""You say many unkind things to your child"", 
                                                                                  ""You pay no attention to your child when (s)he asks for help"", 
                                                                                  ""You think it is your child's own fault when (s)he is having trouble"", 
                                                                                  ""You make your child feel wanted and needed"", ""You tell your child (s)he annoys you"", 
                                                                                  ""You pay a lot of attention to your child"", ""You tell your child how proud you are of him/her when (s)he is good"", 
                                                                                  ""You hurt your child's feelings"", ""You forget important things your child thinks you should remember"", 
                                                                                  ""When your child misbehaves, you make him/her feel unloved"", 
                                                                                  ""You make your child feel what (s)he does is important"", ""When your child does something wrong, you frighten or threaten him/her"", 
                                                                                  ""You like to spend time with your child, for example you sit and laugh together"", 
                                                                                  ""You try to help your child when (s)he is scared or upset"", ""When your child misbehaves, you shame him/her in front of his/her friends"", 
                                                                                  ""You avoid your child's company"", ""You complain about your child"", 
                                                                                  ""You care about what your child thinks, and encourage him/her to talk about it"", 
                                                                                  ""You feel other children are better than your own child"", ""When you make plans, you take your child's thoughts into consideration"", 
                                                                                  ""You let your child do things (s)he thinks are important, even if it is hard for you"", 
                                                                                  ""When your child misbehaves, you compare him/her unfavorably with other children"", 
                                                                                  ""You want to leave your child in someone else's care (for example, a neighbor or relative)"", 
                                                                                  ""You let your child know (s)he is not wanted"", ""You are interested in the things your child does"", 
                                                                                  ""You try to make your child feel better when (s)he is hurt or sick"", 
                                                                                  ""You tell your child you are ashamed of him/her when (s)he misbehaves"", 
                                                                                  ""You let your child know you love him/her"", ""You treat your child gently and with kindness"", 
                                                                                  ""When your child misbehaves, you make him/her feel ashamed or guilty"", 
                                                                                  ""You try to make your child happy"", ""Says nice things about you"", 
                                                                                  ""Nags or scolds you when you are bad"", ""Ignores you"", ""Does not really love you"", 
                                                                                  ""Talks to you about your plans and listens to what you have to say"", 
                                                                                  ""Complains about you to others when you do not listen to him"", 
                                                                                  ""Takes an interest in you"", ""Wants you to bring your friends home, and tries to make things pleasant for them"", 
                                                                                  ""Calls you names, ridicules you, and makes fun of you"", ""Ignores you as long as you do nothing to bother him"", 
                                                                                  ""Yells at you when he is angry"", ""Sits close with you so that you feel free to talk about important things"", 
                                                                                  ""Treats you harshly"", ""Enjoys having you around him"", ""Make you feel proud when you do well"", 
                                                                                  ""Hits you even when you do not deserve it, like for small mistakes"", 
                                                                                  ""Forgets things he is supposed to do for you"", ""Sees you as an annoyance"", 
                                                                                  ""Praises you to others"", ""Punishes you severely when he is angry"", 
                                                                                  ""Makes sure you have the right kind of food to eat"", ""Talks to you in a warm and loving way"", 
                                                                                  ""Gets angry at you easily"", ""Is too busy to answer your questions"", 
                                                                                  ""Seems to hate / despise you"", ""Says nice things to you when you deserve them, such as when you do well in school"", 
                                                                                  ""Gets mad quickly and picks on you"", ""Wants to know who your friends are"", 
                                                                                  ""Is really interested in what you do"", ""Says many unkind things to you"", 
                                                                                  ""Pays no attention when you ask for help"", ""Thinks it is your own fault when you are having trouble"", 
                                                                                  ""Makes you feel wanted and needed"", ""Tells you that you annoy him"", 
                                                                                  ""Pays a lot of attention to you"", ""Tells you how proud he is of you when you are good"", 
                                                                                  ""Goes out of his way to hurt your feelings"", ""Forgets important things you think he should remember"", 
                                                                                  ""Makes you feel unloved if you misbehave"", ""Makes you feel what you do is important"", 
                                                                                  ""Frightens or threatens you when you do something wrong"", ""Likes to spend time with you, for example you sit and laugh together"", 
                                                                                  ""Tries to help you when you are scared or upset"", ""Shames you in front of your friends when you misbehave"", 
                                                                                  ""Tries to stay away from you"", ""Complains about you and talks about you behind your back"", 
                                                                                  ""Cares about what you think, and likes you to talk about it"", 
                                                                                  ""Feels other children are better than you are no matter what you do"", 
                                                                                  ""Cares about what you would like when he makes plans"", ""Lets you do things you think are important, even if it is hard for him"", 
                                                                                  ""Thinks other children behave better than you do"", ""Wants other people to take care of you (for example, a neighbor or relative)"", 
                                                                                  ""Lets you know you are not wanted"", ""Is interested in the things you do"", 
                                                                                  ""Shows concern and tries to make you feel better when you are hurt or sick"", 
                                                                                  ""Tells you how ashamed he is when you misbehave"", ""Lets you know he loves you"", 
                                                                                  ""Treats you gently and with kindness"", ""Makes you feel ashamed or guilty when you misbehave"", 
                                                                                  ""Tries to make you happy"")), .Names = c(""id"", ""item""), row.names = c(NA, 
                                                                                                                                                       152L), class = ""data.frame"")
</code></pre>

<p>Now make a tdm of bigrams, then remove bigrams containing stopwords...</p>

<pre><code>library(""RWeka"")
library(""tm"")
library(""Matrix"")    
BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
x &lt;- TermDocumentMatrix(Corpus(VectorSource(text$item)), control = list(tokenize = BigramTokenizer))
# little bit of regex to remove bigrams with stopwords in them, cf. https://stackoverflow.com/a/6947724/1036500
stpwrds &lt;- paste(stopwords(""en""), collapse = ""|"")
x$dimnames$Terms[!grepl(stpwrds, x$dimnames$Terms)]
[1] ""cold spells""  ""else s""       ""feel free""    ""feel proud""  [5] ""feel unloved"" ""feels free""   ""lost sleep"" 
</code></pre>

<p>A quick test with removing bigrams using the stock list of stopwords that comes with the <code>tm</code> package shows that it only leaves us with 8 bigrams! Clearly we need a smaller stopword list, so let's make a custom list by finding the most frequent words in this particular corpus and removing those. </p>

<pre><code># find freq words in corpus
x &lt;- TermDocumentMatrix(Corpus(VectorSource(text$item)))
# arbitrary choice of 10 occurances = hi freq
mystopwords &lt;- findFreqTerms(x, 10, Inf)
</code></pre>

<p>You should experiment with the lowfreq value, I've set it at 10 after trying just a few, but other values might be better.</p>

<pre><code># try to filter the bigrams again with custom stopword list
x &lt;- TermDocumentMatrix(Corpus(VectorSource(text$item)), control = list(tokenize = BigramTokenizer))
# little bit of regex to remove bigrams with mystopwords in them, cf. https://stackoverflow.com/a/6947724/1036500
mystpwrds &lt;- paste(mystopwords, collapse = ""|"")
# subset tdm to keep only bigrams remaining after mystopwords removed
x &lt;- x[x$dimnames$Terms[!grepl(mystpwrds, x$dimnames$Terms)],]
y &lt;- sparseMatrix( i=x$i, j=x$j, x=x$v, dimnames = dimnames(x) )  
plot( hclust(dist(t(y))) )
</code></pre>

<p><img src=""https://i.sstatic.net/O7y7X.png"" alt=""enter image description here""></p>

<p>But that's a bit hard to read, so let's print the group members out like so</p>

<pre><code>hc &lt;- hclust(dist(t(y)))
cutree(hc, k = 100)

 1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 
  1   2   3   4   5   3   6   5   3   7   8   9  10  11  12  13 
 17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32 
 14  15  16  17   3  18  19  20  21  22  23  24  25  26  27  28 
 33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48 
  3  29   3  30  31  32  33  34  35  36   3  37   3   3  38  39 
 49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 
 40  41   3   3  42  43  44  45   3  46   3   3  47  48  49  50 
 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 
  3  51  52  53   3   3   3  54  55  56  57  58   3   3   3  59 
 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96 
 60  61   3  62  63  47  64  65   3   3  66   3   3  67   3  68 
 97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 
 69  70  33  71  35  72  73  74   3  75   3  76  40  41   3  73 
113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 
 42  43  77  45  78  79  80  81  47  48  82  83   3   3  84  85 
129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 
 86  87   3  88  89  90  91  92  93   3   3  59   3  94  95  96 
145 146 147 148 149 150 151 152 
  3  97  98  99 100   3  66   3 
</code></pre>

<p>And we see that we have rows 145 and 149 in different groups. Whether or not this is a good answer is hard to tell, since you don't specify what your desired output should be. That's why you got close votes, it's difficult to infer from your question what a good answer would be (more specifically, the problem is that you ask to: ""recommend or find a tool, library or favorite off-site resource"" which lead to opinionated answers), the SO crowd seem to prefer a concrete example of the desired output. You might try your question at the new <a href=""https://datascience.stackexchange.com/"">data science stack exchange site</a>. </p>

<p>Anyway, hopefully you've now got a few more ideas and a few more knobs to twiddle in your explorations of the data. Feel free to ask another question if you come up against a specific programming problem related to this. </p>
",3,7,2329,2014-06-16 11:14:25,https://stackoverflow.com/questions/24242408/unsupervised-semantic-clustering-of-phrases
Deleting lots of characters from a list in R,"<p>I have a list of characters with sentences. I have about 10000+ lines. I want to delete 1000+ words from it. So I have a character vector with the words to be deleted. I am using the approach as follows:</p>

<pre><code>c&lt;-gsub(pattern = wordsToBeDeleted,replacement = """",x = mainList)
</code></pre>

<p>This is using only the first word. How can I get this done?</p>
","r, text-mining, gsub, tm","<p><code>gsub</code> only takes one pattern at at time, but You could combine it with <code>Reduce</code></p>

<pre><code>#sample data
sentences&lt;-c(
    ""Morbi in tempus metus, quis commodo eros"",
    ""Cum sociis natoque penatibus et magnis dis parturient montes"",
    ""Nulla diam quam, imperdiet vitae blandit eu"",
    ""Nullam nec pellentesque sapien, ac mollis mauris"")

words&lt;-c(""quis"",""eros"",""diam"",""nec"")
</code></pre>

<p>New we loop over all the words, removing them from the sentences</p>

<pre><code>Reduce(function(a,b) gsub(b,"""", a,fixed=T), words, sentences)
</code></pre>

<p>which gives us</p>

<pre><code>[1] ""Morbi in tempus metus,  commodo ""                            
[2] ""Cum sociis natoque penatibus et magnis dis parturient montes""
[3] ""Nulla  quam, imperdiet vitae blandit eu""                     
[4] ""Nullam  pellentesque sapien, ac mollis mauris"" 
</code></pre>
",1,1,55,2014-06-19 06:10:23,https://stackoverflow.com/questions/24299826/deleting-lots-of-characters-from-a-list-in-r
startProbability in hidden markov models,"<p>How can I calculate the startProbabilitise in hidden markov model for POS tagging? is that means: (# repeats of one tag)/(# repeats of all tags)? or (number of sentences beginning with one tag)/(all sentences)? if not, what?</p>
","nlp, tagging, text-mining, hidden-markov-models, pos-tagger","<p>Considering a second order HMM, Maximum Likelihood Estimate gives:</p>

<pre><code>P(SomeTag | &lt;BOS&gt;,&lt;BOS&gt;) = count(&lt;BOS&gt;,&lt;BOS&gt;,SomeTag) / count(&lt;BOS&gt;,&lt;BOS&gt;)
</code></pre>

<p>It corresponds to your second proposal: </p>

<blockquote>
  <p>(number of sentences beginning with one tag)/(all sentences)</p>
</blockquote>
",0,0,117,2014-06-23 09:53:50,https://stackoverflow.com/questions/24363013/startprobability-in-hidden-markov-models
"How do I remove verbs, prepositions, conjunctions etc from my text?","<p>Basically in my text I just want to keep nouns and remove other parts of speech. </p>

<p>I do not think there is any automated way for this. If there is please suggest.</p>

<p>If there is no automated way, I can also do it manually, but for that I would require lists of all possible say, verbs or prepositions or conjunctions or adjectives etc. Can somebody please suggest a possible source where I can get these specific lists.</p>
","python, r, text-mining","<p>You can use the <a href=""http://www.nltk.org/"" rel=""noreferrer"">NLTK</a> part-of-speech tagger to tag each word, then only keep the nouns. Here's an example of the NLTK tagger, taken from the NLTK homepage:</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; sentence = """"""At eight o'clock on Thursday morning
... Arthur didn't feel very good.""""""
&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)
&gt;&gt;&gt; tokens
['At', 'eight', ""o'clock"", 'on', 'Thursday', 'morning',
'Arthur', 'did', ""n't"", 'feel', 'very', 'good', '.']
&gt;&gt;&gt; tagged = nltk.pos_tag(tokens)
&gt;&gt;&gt; tagged[0:6]
[('At', 'IN'), ('eight', 'CD'), (""o'clock"", 'JJ'), ('on', 'IN'),
('Thursday', 'NNP'), ('morning', 'NN')]
</code></pre>

<p>In your case, you'd keep every element of the <code>tagged</code> list that have a tag starting with N, i.e. all the nouns, and throw the rest away. Check out the <a href=""http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""noreferrer"">complete list of tags</a>; you might also want to include foreign words (FW), for example.</p>

<p>NLTK is free to use, and it comes with its own data sets that are also free. You won't have to build lists of prepositions and so on yourself.</p>
",44,15,37625,2014-06-25 10:28:25,https://stackoverflow.com/questions/24406201/how-do-i-remove-verbs-prepositions-conjunctions-etc-from-my-text
"In R tm package, build corpus FROM Document-Term-Matrix","<p>It's straightforward to build a document-term matrix from a corpus with the tm package. 
I'd like to build a corpus from a document-term-matrix. </p>

<p>Let M be the number of documents in a document set. 
Let V be the number of terms in the vocabulary of that document set.Then a document-term-matrix is an M*V matrix. </p>

<p>I also have a vocabulary vector, of length V.  In the vocabulary vector are the words represented by indices in the document-term-matrix. </p>

<p>From the dtm and vocabulary vector, I'd like to construct a ""corpus"" object.  This is because I'd like to stem my document set.  I built my dtm and vocab manually - i.e. there never was a tm ""corpus"" object representing my dataset, so I can't use the function, </p>

<pre><code>tm_map(corpus, stemDocument, language=""english"")
</code></pre>

<p>I've been trying to build a workaround where I stem the vocabulary and only keep unique words, but then it gets somewhat complicated trying to maintain the correspondence between the dtm and the vocabulary vector.</p>

<p>Ideally, the end result would be that my vocabulary vector is stemmed and only contains unique entries, and the dtm indices correspond to the stemmed vocabulary vector. If you can think of some other way to do that, I would appreciate that as well.  </p>

<p>My troubles would be fixed if I could simply build a tm ""corpus"" from my dtm and vocabulary vector, stem the corpus, and then convert back to a dtm and vocabulary vector (I already know how to make those conversions). </p>

<p>Let me know if I can clarify the problem any further.  </p>
","r, text-mining, tm, corpus, lda","<p>Here's on approach providing my own <a href=""https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example"">minimal reproducible example</a> (as a new user you may not be aware that this is your responsibility) from the <code>tm</code> package:</p>

<pre><code>## Minimal Reproducible Example
library(tm)
data(""crude"")
dtm &lt;- DocumentTermMatrix(crude,
    control = list(weighting =
    function(x)
        weightTfIdf(x, normalize = FALSE),
        stopwords = TRUE))

## Convert tdm to a list of text
dtm2list &lt;- apply(dtm, 1, function(x) {
    paste(rep(names(x), x), collapse="" "")
})

## convert to a Corpus
myCorp &lt;- VCorpus(VectorSource(dtm2list))
inspect(myCorp)

## Stemming
myCorp &lt;- tm_map(myCorp, stemDocument)
inspect(myCorp)
</code></pre>
",5,4,5299,2014-06-25 21:27:53,https://stackoverflow.com/questions/24418893/in-r-tm-package-build-corpus-from-document-term-matrix
Keep document ID with R corpus,"<p>I have searched stackoverflow and the web and can only find partial solutions OR some that don't work due to changes in TM or qdap. Problem below:</p>

<p>I have a dataframe: <strong>ID</strong> and <strong>Text</strong> (Simple document <em>id/name</em> and then some <em>text</em>)</p>

<p>I have two issues:</p>

<p><strong>Part 1</strong>: How can I create a tdm or dtm and maintain the document name/id? It only shows ""character(0)"" on inspect(tdm).<br>
<strong>Part 2</strong>: I want to keep only a specific list of terms, i.e. opposite of remove custom stopwords. I want this to happen in the corpus, not the tdm/dtm.</p>

<p>For Part 2, I used a solution I got here: <a href=""https://stackoverflow.com/questions/17979104/how-to-implement-proximity-rules-in-tm-dictionary-for-counting-words"">How to implement proximity rules in tm dictionary for counting words?</a> <br><br>
This one happens on the tdm part! Is there a better solution for Part 2 where you use something like ""tm_map(my.corpus, <strong><em>keepOnlyWords</em></strong>, customlist)""?</p>

<p>Any help will be greatly appreciated.
Thanks much!</p>
","r, text, text-mining, tm, corpus","<p>In newer versions of tm this is a lot easier with the DataframeSource() function. </p>

<p>""A data frame source interprets each row of the data frame x as a document. The first column must be named ""doc_id"" and contain a unique string identifier for each document. The second column must be named ""text"" and contain a ""UTF-8"" encoded string representing the document's content. Optional additional columns are used as document level metadata.""</p>

<p>So in this case:</p>

<pre><code>dd &lt;-data.frame(
    doc_id=10:13,
    text=c(""No wonder, then, that ever gathering volume from the mere transit "",
      ""So that in many cases such a panic did he finally strike, that few "",
      ""But there were still other and more vital practical influences at work"",
      ""Not even at the present day has the original prestige of the Sperm Whale"")
    ,stringsAsFactors=F
 )

Corpus = VCorpus(DataframeSource(dd))
</code></pre>
",4,10,8576,2014-07-01 02:07:57,https://stackoverflow.com/questions/24501514/keep-document-id-with-r-corpus
dictionary from a text file,"<p>I do have a file <code>f1</code> which has words and an emotional values (values from +6 to -6)</p>

<pre><code>normal  0
sad -2
happy   4
</code></pre>

<p>I have another file <code>f2</code> which has texts (tweets) containing say average of 4 or 5 words (line by line).</p>

<p>I want to read text in <code>f2</code> line by line and for each line, for every word I have to search whether it is there in <code>f1</code>. If it is, then I have to get the value and add it. Likewise I have to sum values for every word (if it is in the list) in the sentence and print it.
So print should be like this (for example for first three lines)</p>

<pre><code>3
0
-2
</code></pre>

<p>I have a code like this.I am getting error ""value error:mixing iterating and read method will loose data"" Please correct the code or at least give a new method to do this.</p>

<pre><code>f2=open(""file2.txt"",""r"")
for line in f2:
    l=f2.readline()
    afinn = dict(map(lambda (k,v): (k,int(v)),[ line.split('\t') for line in open(""file1.txt"") ]))
    value= sum(map(lambda word: afinn.get(word, 0), l.lower().split()))
    print value
f1.close()
f2.close()
</code></pre>
","python, python-2.7, text-mining","<p>There are several problems with your code:</p>

<pre><code>for line in f2:
    l=f2.readline()
</code></pre>

<p>You're iterating over the file implicitly <em>and</em> explicitly at the same time - not a good idea. In the first iteration <code>line</code> will contain the first line of your file, and <code>l</code> will contain the second line. In the next iteration, <code>line</code> and <code>l</code> will contain the third and fourth line, respectively (and so on). Pick one - I would choose the first one and drop the <code>readline()</code> call.</p>

<p>Then, you <em>reassign</em>  line in your list comprehension that's reading <code>file1.txt</code>. That means you're overwriting <code>line</code>, and you're reading <code>file1.txt</code> again and again during each iteration - a huge waste. Read it once, store it and refer to that in your loop.</p>

<p>Furthermore, <code>dict(map(lambda(...)))</code> is rather unpythonic - we do have dict comprehensions for that. But in this case, a simpler version is probably even better:</p>

<p>This is how you could fill your <code>words</code> dictionary (you could do that as a one-liner too, but readability counts, so let's keep it simple):</p>

<pre><code>with open(""file1.txt"") as f1:
    words = {}
    for line in f1:
        word, score = line.split()
        words[word] = int(score)
</code></pre>

<p>Now you could go and read your input file:</p>

<pre><code>with open(""file2.txt"") as f2:
    for line in f2:
        contents = line.split()
        value = sum(words.get(word, 0) for word in contents)
        print value
</code></pre>
",2,0,550,2014-07-07 12:41:35,https://stackoverflow.com/questions/24610872/dictionary-from-a-text-file
tm.plugin.sentiment issue. Error: could not find function &quot;DMetaData&quot;,"<p>I have tried countless times in different ways to run the score() function in the tm.plugin.sentiment package in R but I keep getting the same error. This is a sample code:</p>

<pre><code>    library(tm.plugin.webmining)
    library(tm.plugin.sentiment)
    cor &lt;- WebCorpus(GoogleFinanceSource(""NASDAQ:MSFT""))
    tm_tag_score &lt;- tm_term_score
    corpus &lt;- score(cor)
</code></pre>

<p>This is the error I get:</p>

<pre><code>    Error in score(cor) : could not find function ""DMetaData""
</code></pre>
","r, text, text-mining, tm","<p>Looks like it's caused by the removal of the DMetaData function from the tm package. Refer to this issue on github:</p>

<p><a href=""https://github.com/mannau/tm.plugin.sentiment/issues/1"" rel=""nofollow"">https://github.com/mannau/tm.plugin.sentiment/issues/1</a></p>

<p>upgrading to the latest version of tm.plugin.sentiment from github using devtools fixed this for me.</p>

<pre><code>library(devtools)
install_github(""mannau/tm.plugin.sentiment"")
</code></pre>
",1,1,959,2014-07-07 13:45:08,https://stackoverflow.com/questions/24612080/tm-plugin-sentiment-issue-error-could-not-find-function-dmetadata
TextMining of Twitter DB in RapidMiner evaluates all comments in a only way,"<p>I'm trying to use SVMs in RapidMiner to classify tweets contained in a database (TASS 2014 corpus), however, regardless of what that says Performance module, the applied model ends always evaluating all tweets in one way (ie all positive, all negative, all neutral, or all none). Not sure if I'm taking a misconfiguration. </p>

<p><a href=""http://www.subirimagenes.net/i/140711042146793099.png"" rel=""nofollow"">After Multipliy operator</a>, roles outputs are (first roles, second name, third type): </p>

<ul>
<li>text, text, text </li>
<li>label, value, nominal </li>
<li>id, tweetid, nominal </li>
</ul>

<p><a href=""http://www.subirimagenes.net/i/140711042147212040.png"" rel=""nofollow"">After September Operator Role (2)</a>, roles outputs are (roles first, second name, third type): </p>

<ul>
<li>label, text, text </li>
<li>id, tweetid, nominal</li>
</ul>

<p>Then, I add the XML code of RapidMiner:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;process version=""5.3.015""&gt;
  &lt;context&gt;
    &lt;input/&gt;
    &lt;output/&gt;
    &lt;macros/&gt;
  &lt;/context&gt;
  &lt;operator activated=""true"" class=""process"" compatibility=""5.3.015"" expanded=""true"" name=""Process""&gt;
    &lt;process expanded=""true""&gt;
      &lt;operator activated=""false"" class=""read_database"" compatibility=""5.3.015"" expanded=""true"" height=""60"" name=""Read Database"" width=""90"" x=""45"" y=""30""&gt;
        &lt;parameter key=""connection"" value=""sqlserver2014""/&gt;
        &lt;parameter key=""query"" value=""SELECT TOP 4246 &amp;quot;content&amp;quot;, &amp;quot;tweetid&amp;quot;, &amp;quot;value&amp;quot;&amp;#10;FROM &amp;quot;dbo&amp;quot;.&amp;quot;TweetsTrainClean&amp;quot;&amp;#10;WHERE 'POS'=SUBSTRING(value,1,3)&amp;#10;UNION ALL&amp;#10;SELECT TOP 4246 &amp;quot;content&amp;quot;, &amp;quot;tweetid&amp;quot;, &amp;quot;value&amp;quot;&amp;#10;FROM &amp;quot;dbo&amp;quot;.&amp;quot;TweetsTrainClean&amp;quot;&amp;#10;WHERE 'NEU'=SUBSTRING(value,1,3)&amp;#10;UNION ALL&amp;#10;SELECT TOP 4246 &amp;quot;content&amp;quot;, &amp;quot;tweetid&amp;quot;, &amp;quot;value&amp;quot;&amp;#10;FROM &amp;quot;dbo&amp;quot;.&amp;quot;TweetsTrainClean&amp;quot;&amp;#10;WHERE 'NEG'=SUBSTRING(value,1,3)""/&gt;
        &lt;enumeration key=""parameters""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""read_database"" compatibility=""5.3.015"" expanded=""true"" height=""60"" name=""Read Database (5)"" width=""90"" x=""45"" y=""300""&gt;
        &lt;parameter key=""connection"" value=""sqlserver2014""/&gt;
        &lt;parameter key=""query"" value=""SELECT top 1000 *&amp;#10;FROM &amp;quot;dbo&amp;quot;.&amp;quot;TweetsGeneralClean&amp;quot;""/&gt;
        &lt;enumeration key=""parameters""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""false"" class=""read_database"" compatibility=""5.3.015"" expanded=""true"" height=""60"" name=""Read Database (2)"" width=""90"" x=""45"" y=""210""&gt;
        &lt;parameter key=""connection"" value=""sqlserver2014""/&gt;
        &lt;parameter key=""query"" value=""SELECT *&amp;#10;FROM (&amp;#10;     SELECT *, ROW_NUMBER() OVER (ORDER BY tweetid) AS RowNum&amp;#10;     FROM &amp;quot;dbo&amp;quot;.&amp;quot;TweetsGeneralClean&amp;quot;&amp;#10;     ) AS tabla&amp;#10;WHERE tabla.RowNum BETWEEN 30000 AND 64798""/&gt;
        &lt;enumeration key=""parameters""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""select_attributes"" compatibility=""5.3.015"" expanded=""true"" height=""76"" name=""Select Attributes (2)"" width=""90"" x=""179"" y=""255""&gt;
        &lt;parameter key=""attribute_filter_type"" value=""subset""/&gt;
        &lt;parameter key=""attributes"" value=""|tweetid|content""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text:process_document_from_data"" compatibility=""5.3.002"" expanded=""true"" height=""76"" name=""Process Documents from Data (2)"" width=""90"" x=""313"" y=""255""&gt;
        &lt;parameter key=""keep_text"" value=""true""/&gt;
        &lt;list key=""specify_weights""/&gt;
        &lt;process expanded=""true""&gt;
          &lt;connect from_port=""document"" to_port=""document 1""/&gt;
          &lt;portSpacing port=""source_document"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 1"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 2"" spacing=""0""/&gt;
        &lt;/process&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""set_role"" compatibility=""5.3.015"" expanded=""true"" height=""76"" name=""Set Role (2)"" width=""90"" x=""447"" y=""255""&gt;
        &lt;parameter key=""attribute_name"" value=""text""/&gt;
        &lt;parameter key=""target_role"" value=""label""/&gt;
        &lt;list key=""set_additional_roles""&gt;
          &lt;parameter key=""tweetid"" value=""id""/&gt;
        &lt;/list&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""read_database"" compatibility=""5.3.015"" expanded=""true"" height=""60"" name=""Read Database (3)"" width=""90"" x=""45"" y=""75""&gt;
        &lt;parameter key=""connection"" value=""sqlserver2014""/&gt;
        &lt;parameter key=""query"" value=""select &amp;quot;content&amp;quot;,&amp;quot;tweetid&amp;quot;,&amp;quot;value&amp;quot; from &amp;quot;dbo&amp;quot;.&amp;quot;TweetsTrainClean&amp;quot;&amp;#10;where 'POS'=SUBSTRING(&amp;quot;value&amp;quot;,1,3) or 'NEG'=SUBSTRING(&amp;quot;value&amp;quot;,1,3)&amp;#10;&amp;#9;or 'NEU'=SUBSTRING(&amp;quot;value&amp;quot;,1,3) or 'NON'=SUBSTRING(&amp;quot;value&amp;quot;,1,3)&amp;#10;order by rand()""/&gt;
        &lt;enumeration key=""parameters""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text_to_nominal"" compatibility=""5.3.015"" expanded=""true"" height=""76"" name=""Text to Nominal"" width=""90"" x=""112"" y=""120""&gt;
        &lt;parameter key=""attribute_filter_type"" value=""single""/&gt;
        &lt;parameter key=""attribute"" value=""value""/&gt;
        &lt;parameter key=""attributes"" value=""|type|value""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""set_role"" compatibility=""5.3.015"" expanded=""true"" height=""76"" name=""Set Role"" width=""90"" x=""179"" y=""30""&gt;
        &lt;parameter key=""attribute_name"" value=""tweetid""/&gt;
        &lt;parameter key=""target_role"" value=""id""/&gt;
        &lt;list key=""set_additional_roles""&gt;
          &lt;parameter key=""value"" value=""label""/&gt;
        &lt;/list&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""nominal_to_text"" compatibility=""5.3.015"" expanded=""true"" height=""76"" name=""Nominal to Text"" width=""90"" x=""246"" y=""120""&gt;
        &lt;parameter key=""attribute_filter_type"" value=""single""/&gt;
        &lt;parameter key=""attribute"" value=""content""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text:process_document_from_data"" compatibility=""5.3.002"" expanded=""true"" height=""76"" name=""Process Documents from Data"" width=""90"" x=""313"" y=""30""&gt;
        &lt;parameter key=""keep_text"" value=""true""/&gt;
        &lt;list key=""specify_weights""/&gt;
        &lt;process expanded=""true""&gt;
          &lt;connect from_port=""document"" to_port=""document 1""/&gt;
          &lt;portSpacing port=""source_document"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 1"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 2"" spacing=""0""/&gt;
        &lt;/process&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""multiply"" compatibility=""5.3.015"" expanded=""true"" height=""94"" name=""Multiply"" width=""90"" x=""380"" y=""120""/&gt;
      &lt;operator activated=""true"" class=""x_validation"" compatibility=""5.3.015"" expanded=""true"" height=""112"" name=""Validation"" width=""90"" x=""514"" y=""30""&gt;
        &lt;parameter key=""number_of_validations"" value=""5""/&gt;
        &lt;parameter key=""sampling_type"" value=""linear sampling""/&gt;
        &lt;process expanded=""true""&gt;
          &lt;operator activated=""true"" class=""select_attributes"" compatibility=""5.3.015"" expanded=""true"" height=""76"" name=""Select Attributes"" width=""90"" x=""45"" y=""30""&gt;
            &lt;parameter key=""attribute_filter_type"" value=""no_missing_values""/&gt;
            &lt;parameter key=""attribute"" value=""content""/&gt;
            &lt;parameter key=""attributes"" value=""tweetid|type||content""/&gt;
          &lt;/operator&gt;
          &lt;operator activated=""true"" class=""nominal_to_binominal"" compatibility=""5.3.015"" expanded=""true"" height=""94"" name=""Nominal to Binominal"" width=""90"" x=""45"" y=""120""&gt;
            &lt;parameter key=""attribute_filter_type"" value=""single""/&gt;
            &lt;parameter key=""attribute"" value=""value""/&gt;
          &lt;/operator&gt;
          &lt;operator activated=""true"" class=""polynomial_by_binomial_classification"" compatibility=""5.3.015"" expanded=""true"" height=""76"" name=""Polynominal by Binominal Classification"" width=""90"" x=""45"" y=""255""&gt;
            &lt;process expanded=""true""&gt;
              &lt;operator activated=""true"" class=""support_vector_machine_linear"" compatibility=""5.3.015"" expanded=""true"" height=""76"" name=""SVM (Linear)"" width=""90"" x=""45"" y=""255""/&gt;
              &lt;connect from_port=""training set"" to_op=""SVM (Linear)"" to_port=""training set""/&gt;
              &lt;connect from_op=""SVM (Linear)"" from_port=""model"" to_port=""model""/&gt;
              &lt;portSpacing port=""source_training set"" spacing=""0""/&gt;
              &lt;portSpacing port=""sink_model"" spacing=""0""/&gt;
            &lt;/process&gt;
          &lt;/operator&gt;
          &lt;connect from_port=""training"" to_op=""Select Attributes"" to_port=""example set input""/&gt;
          &lt;connect from_op=""Select Attributes"" from_port=""example set output"" to_op=""Nominal to Binominal"" to_port=""example set input""/&gt;
          &lt;connect from_op=""Nominal to Binominal"" from_port=""example set output"" to_op=""Polynominal by Binominal Classification"" to_port=""training set""/&gt;
          &lt;connect from_op=""Polynominal by Binominal Classification"" from_port=""model"" to_port=""model""/&gt;
          &lt;portSpacing port=""source_training"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_model"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_through 1"" spacing=""0""/&gt;
        &lt;/process&gt;
        &lt;process expanded=""true""&gt;
          &lt;operator activated=""true"" class=""apply_model"" compatibility=""5.3.015"" expanded=""true"" height=""76"" name=""Apply Model"" width=""90"" x=""45"" y=""30""&gt;
            &lt;list key=""application_parameters""/&gt;
          &lt;/operator&gt;
          &lt;operator activated=""true"" class=""performance"" compatibility=""5.3.015"" expanded=""true"" height=""76"" name=""Performance"" width=""90"" x=""179"" y=""30""/&gt;
          &lt;connect from_port=""model"" to_op=""Apply Model"" to_port=""model""/&gt;
          &lt;connect from_port=""test set"" to_op=""Apply Model"" to_port=""unlabelled data""/&gt;
          &lt;connect from_op=""Apply Model"" from_port=""labelled data"" to_op=""Performance"" to_port=""labelled data""/&gt;
          &lt;connect from_op=""Performance"" from_port=""performance"" to_port=""averagable 1""/&gt;
          &lt;portSpacing port=""source_model"" spacing=""0""/&gt;
          &lt;portSpacing port=""source_test set"" spacing=""0""/&gt;
          &lt;portSpacing port=""source_through 1"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_averagable 1"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_averagable 2"" spacing=""0""/&gt;
        &lt;/process&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""apply_model"" compatibility=""5.3.015"" expanded=""true"" height=""76"" name=""Apply Model (2)"" width=""90"" x=""648"" y=""210""&gt;
        &lt;list key=""application_parameters""/&gt;
      &lt;/operator&gt;
      &lt;connect from_op=""Read Database (5)"" from_port=""output"" to_op=""Select Attributes (2)"" to_port=""example set input""/&gt;
      &lt;connect from_op=""Select Attributes (2)"" from_port=""example set output"" to_op=""Process Documents from Data (2)"" to_port=""example set""/&gt;
      &lt;connect from_op=""Process Documents from Data (2)"" from_port=""example set"" to_op=""Set Role (2)"" to_port=""example set input""/&gt;
      &lt;connect from_op=""Set Role (2)"" from_port=""example set output"" to_op=""Apply Model (2)"" to_port=""unlabelled data""/&gt;
      &lt;connect from_op=""Read Database (3)"" from_port=""output"" to_op=""Text to Nominal"" to_port=""example set input""/&gt;
      &lt;connect from_op=""Text to Nominal"" from_port=""example set output"" to_op=""Set Role"" to_port=""example set input""/&gt;
      &lt;connect from_op=""Set Role"" from_port=""example set output"" to_op=""Nominal to Text"" to_port=""example set input""/&gt;
      &lt;connect from_op=""Nominal to Text"" from_port=""example set output"" to_op=""Process Documents from Data"" to_port=""example set""/&gt;
      &lt;connect from_op=""Process Documents from Data"" from_port=""example set"" to_op=""Multiply"" to_port=""input""/&gt;
      &lt;connect from_op=""Multiply"" from_port=""output 1"" to_op=""Validation"" to_port=""training""/&gt;
      &lt;connect from_op=""Multiply"" from_port=""output 2"" to_port=""result 1""/&gt;
      &lt;connect from_op=""Validation"" from_port=""model"" to_op=""Apply Model (2)"" to_port=""model""/&gt;
      &lt;connect from_op=""Validation"" from_port=""averagable 1"" to_port=""result 3""/&gt;
      &lt;connect from_op=""Apply Model (2)"" from_port=""labelled data"" to_port=""result 2""/&gt;
      &lt;portSpacing port=""source_input 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 2"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 3"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 4"" spacing=""0""/&gt;
    &lt;/process&gt;
  &lt;/operator&gt;
&lt;/process&gt;
</code></pre>
","twitter, svm, text-mining, rapidminer","<p>I don't have access to the data so I can't be sure for certain but I assume one of the attributes contains the complete tweet text as a few words.</p>

<p>With that assumption, the <code>Process Documents</code> operator needs to contain operators to split the text into tokens. Use the <code>Tokenize</code> operator for this and simply place it inside each of the <code>Process Documents</code> operators.</p>

<p>In addition, the word list output generated by the <code>Process Documents</code> for the training operation should be connected to the <code>Process Documents</code> operator which builds the test set. This is important as it eliminates any additional attributes from appearing in the test set that correspond to words that have not been seen during the training and therefore ensures the model will be applied correctly.</p>
",0,0,364,2014-07-11 02:35:28,https://stackoverflow.com/questions/24689265/textmining-of-twitter-db-in-rapidminer-evaluates-all-comments-in-a-only-way
R webcorpus attribute extraction,"<p>I am using the <strong>tm.plugin.webmining</strong> to get latest news about a company say microsoft using the following command</p>

<pre><code>corpus&lt;-WebCorpus(GoogleBlogSearchSource(stock))
</code></pre>

<p>When I run <strong>meta(corpus[[1]])</strong> i get</p>

<blockquote>
  <p>Metadata:</p>
  
  <blockquote>
    <p>author       : character(0)
    datetimestamp: 2014-07-17 20:28:10
    description  : Microsoft Layoffs ÃƒÂƒÃ‚Â¢ÃƒÂ‚Ã‚Â€ÃƒÂ‚Ã‚Â“ What it Means for MSFT StockInvestorplace.comWhile the layoffs are obviously
    going to be hardest on the workers, as investors we still have to take
    a rational and objective look at the corporation to see what it means
    for MSFT ÃƒÂƒÃ‚Â¢ÃƒÂ‚Ã‚Â€ÃƒÂ‚Ã‚Â“ particularly if you are personally a
    Microsoft stock holder ...Why Microsoft (MSFT) Stock Is Up
    TodayTheStreet.comEarnings Preview: Microsoft Corporation (MSFT),
    Apple Inc (AAPL), Facebook ...International Business TimesWhat Do
    Microsoft's Layoff Plans Tell Us About Satya Nadella's Vision?Motley
    FoolTech InsiderÂ -Insider Monkey (blog)all 2,176 news articlesÂ Â»
    heading      : Microsoft Layoffs ÃƒÂ¢Ã‚Â€Ã‚Â“ What it Means for MSFT Stock - Investorplace.com
    id           : tag:news.google.com,2005:cluster=<a href=""http://investorplace.com/2014/07/microsoft-layoffs-means-msft-stock/"" rel=""nofollow"">http://investorplace.com/2014/07/microsoft-layoffs-means-msft-stock/</a>
    language     : character(0)
    origin       : <a href=""http://news.google.com/news/url?sa=t&amp;fd=R&amp;ct2=us&amp;usg=AFQjCNEadqFvThyxvJU3O5uHa6wiyoWNEw&amp;clid=c3a7d30bb8a4878e06b80cf16b898331&amp;cid=52778559643673&amp;ei=Cr3LU8jGNMnNkwX_lYCICQ&amp;url=http://investorplace.com/2014/07/microsoft-layoffs-means-msft-stock/"" rel=""nofollow"">http://news.google.com/news/url?sa=t&amp;fd=R&amp;ct2=us&amp;usg=AFQjCNEadqFvThyxvJU3O5uHa6wiyoWNEw&amp;clid=c3a7d30bb8a4878e06b80cf16b898331&amp;cid=52778559643673&amp;ei=Cr3LU8jGNMnNkwX_lYCICQ&amp;url=http://investorplace.com/2014/07/microsoft-layoffs-means-msft-stock/</a></p>
  </blockquote>
</blockquote>

<p>So here I see that the different attributes are here but when I run</p>

<pre><code>Headers&lt;-sapply(meta(corpus,FUN=function(x){attr(x,""heading"")})
</code></pre>

<p>Headers is a list of 100 items with null values. I am pretty sure this particular code was running a few days back. What changed in between was I reinstalled the packages on the new system and also updated R to 3.1.1 instead of R 3.1.0(earlier)</p>

<p>What can I do to get separate lists of headers, descriptions timestamp, etc, which I later want to convert into a 100X3 data frame.</p>
","r, attributes, text-mining, tm, corpus","<p>With the newest R, Please try the following code:</p>

<p>Code : </p>

<pre><code>headers&lt;-meta(corpus,tag=""heading"")
</code></pre>
",0,1,506,2014-07-20 13:14:28,https://stackoverflow.com/questions/24850856/r-webcorpus-attribute-extraction
How to avoid &quot;too many redirects&quot; error when using readLines(url) in R?,"<p>I am trying to mine news articles from various sources by doing</p>

<pre><code>site = readLines(link)
</code></pre>

<p>link being the url of the site I am trying to download. Most of the time this works but with some specific sources I get the error:</p>

<pre><code>Error in file(con, ""r"") : cannot open the connection
In addition: Warning message:
In file(con, ""r"") : too many redirects, aborting ...
</code></pre>

<p>Which I'd like to avoid but so far I had no success in doing so.</p>

<p>Replicating this is quite easy as virtually none of the New York Times links work</p>

<p>e.g. <a href=""http://www.nytimes.com/2014/08/01/us/politics/african-leaders-coming-to-talk-business-may-also-be-pressed-on-rights.html"" rel=""nofollow"">http://www.nytimes.com/2014/08/01/us/politics/african-leaders-coming-to-talk-business-may-also-be-pressed-on-rights.html</a></p>
","r, url, text-mining","<p>It seems like the NYT site forces redirects for cookie and tracking purposes. Looks like the built-in URL reader isn't able to deal with them correctly (not sure if it supports cookies which is probably the problem).</p>

<p>Anyway, you might consider using the <code>RCurl</code> package to access the file instead. Try</p>

<pre><code>library(RCurl)
link  = ""http://www.nytimes.com/2014/08/01/us/politics/african-leaders-coming-to-talk-business-may-also-be-pressed-on-rights.html?_r=0""

site &lt;- getURL(link, .opts = curlOptions(
    cookiejar="""",  useragent = ""Mozilla/5.0"", followlocation = TRUE
))
</code></pre>
",2,2,735,2014-08-01 18:10:47,https://stackoverflow.com/questions/25086185/how-to-avoid-too-many-redirects-error-when-using-readlinesurl-in-r
Regular expression for mining content of a file,"<p>I have a text file like below:</p>

<pre><code>&lt;Author&gt;Marilyn1949
&lt;Content&gt;great way of doing things.
can you provide more info.blah blah blah..  
&lt;Date&gt;Dec 1, 2008...
(file content continues in similar fashion for other authors)""
</code></pre>

<p>I am trying to extract the content section using the code below. Can you help me figure out what am i missing as my file is just an getting genrated as an array of []. </p>

<pre><code>text_file = open(""output/out.txt"", ""w"")
for file in os.listdir(""./""):
    if glob.fnmatch.fnmatch(file, '*.txt'):
        with open(file, ""r"") as source:
            L= source.read()                
            pattern =  re.compile(r'&lt;Content&gt;*&lt;Date&gt;')              
            for match in L:
                result = re.findall(r'&lt;Content&gt;.*&lt;Date&gt;', match)
                text_file.write(str(result))
                text_file.write('\n')
</code></pre>
","python, regex, text-mining","<p>The dot character matches anything except a newline. Use the <a href=""https://docs.python.org/2/library/re.html#contents-of-module-re"" rel=""nofollow""><code>re.DOTALL</code> flag</a> to make it match newlines as well:</p>

<pre><code>result = re.findall(r'&lt;Content&gt;.*&lt;Date&gt;', match, flags=re.DOTALL)
</code></pre>

<p>Also, you might not want to capture the tags:</p>

<pre><code>result = re.findall(r'&lt;Content&gt;(.*)&lt;Date&gt;', match, flags=re.DOTALL)
</code></pre>

<p>And cleaning up your example a bit:</p>

<pre><code>with open(file, ""r"") as source:
    results = re.findall(r'&lt;Content&gt;(.*?)&lt;Date&gt;', source.read(), flags=re.DOTALL)
    text_file.write('\n'.join(results))
</code></pre>
",0,-1,120,2014-08-07 07:54:11,https://stackoverflow.com/questions/25177036/regular-expression-for-mining-content-of-a-file
Use tm&#39;s Corpus function with big data in R,"<p>I'm trying to do text mining on big data in R with <code>tm</code>. </p>

<p>I run into memory issues frequently (such as <code>can not allocation vector of size....</code> ) and use the established methods of troubleshooting those issues, such as </p>

<ul>
<li>using 64-bit R </li>
<li>trying different OS's (Windows, Linux, Solaris, etc)</li>
<li>setting <code>memory.limit()</code> to its maximum</li>
<li>making sure that sufficient RAM and compute is available on the server (which there is)</li>
<li>making liberal use of <code>gc()</code></li>
<li>profiling the code for bottlenecks</li>
<li>breaking up big operations into multiple smaller operations</li>
</ul>

<p>However, when trying to run <code>Corpus</code> on a vector of a million or so text fields, I encounter a slightly different memory error than usual and I'm not sure how to work-around the problem. The error is:</p>

<pre><code>&gt; ds &lt;- Corpus(DataframeSource(dfs))
Error: memory exhausted (limit reached?)
</code></pre>

<p>Can (and should) I run <code>Corpus</code> incrementally on blocks of rows from that source dataframe then combine the results? Is there a more efficient way to run this? </p>

<p>The size of the data that will produce this error depends on the computer running it, but if you take the built-in <code>crude</code> dataset and replicate the documents until it's large enough, then you can replicate the error. </p>

<p><strong>UPDATE</strong></p>

<p>I've been experimenting with trying to combine smaller corpa, i.e.</p>

<pre><code>test1 &lt;- dfs[1:10000,]
test2 &lt;- dfs[10001:20000,]

ds.1 &lt;- Corpus(DataframeSource(test1))
ds.2 &lt;- Corpus(DataframeSource(test2))
</code></pre>

<p>and while I haven't been successful, I did discover <code>tm_combine</code> which is <a href=""https://www.rdocumentation.org/packages/tm/topics/tm_combine"" rel=""nofollow noreferrer"">supposed to solve this exact problem</a>. The only catch is that for some reason, my 64-bit build of R 3.1.1 with the newest version of <code>tm</code> can't find the function <code>tm_combine</code>. Perhaps it was removed from the package for some reason? I'm investigating...</p>

<pre><code>&gt; require(tm)
&gt; ds.12 &lt;- tm_combine(ds.1,ds.2)
Error: could not find function ""tm_combine""
</code></pre>
","r, bigdata, text-mining, tm","<p>I don't know if <code>tm_combine</code> became deprecated or why it's not found in the <code>tm</code> namespace, but I did find a solution through using <code>Corpus</code> on smaller chunks of the dataframe then combining them.</p>

<p><a href=""https://stackoverflow.com/questions/6616805/r-text-mining-package-allowing-to-incorporate-new-documents-into-an-existing-co/6617011#6617011"">This</a> StackOverflow post had a simple way to do that without <code>tm_combine</code>:</p>

<pre><code>test1 &lt;- dfs[1:100000,]
test2 &lt;- dfs[100001:200000,]

ds.1 &lt;- Corpus(DataframeSource(test1))
ds.2 &lt;- Corpus(DataframeSource(test2))

#ds.12 &lt;- tm_combine(ds.1,ds.2) ##Error: could not find function ""tm_combine""
ds.12 &lt;- c(ds.1,ds.2)
</code></pre>

<p>which gives you:</p>

<blockquote>
  <p>ds.12</p>
</blockquote>

<pre><code>&lt;&lt;VCorpus (documents: 200000, metadata (corpus/indexed): 0/0)&gt;&gt;
</code></pre>

<p>Sorry not to figure this out on my own before asking. I tried and failed with other ways of combining objects.</p>
",2,6,3539,2014-08-27 17:33:51,https://stackoverflow.com/questions/25533594/use-tms-corpus-function-with-big-data-in-r
TermDocumentMatrix errors in R,"<p>I have been working through numerous online examples of the {tm} package in R, attempting to create a TermDocumentMatrix.  Creating and cleaning a corpus has been pretty straightforward, but I consistently encounter an error when I attempt to create a matrix.  The error is: </p>

<blockquote>
  <p>Error in UseMethod(""meta"", x) : 
    no applicable method for 'meta' applied to an object of class ""character""
  In addition: Warning message:
  In mclapply(unname(content(x)), termFreq, control) :
    all scheduled cores encountered errors in user code</p>
</blockquote>

<p>For example, here is code from Jon Starkweather's text mining <a href=""http://www.unt.edu/rss/class/Jon/Benchmarks/TextMining_L_JDS_Jan2014.pdf"">example</a>.  Apologies in advance for such long code, but this does produce a reproducible example.  Please note that the error comes at the end with the {tdm} function.  </p>

<pre><code>#Read in data
policy.HTML.page &lt;- readLines(""http://policy.unt.edu/policy/3-5"")

#Obtain text and remove mark-up
policy.HTML.page[186:202]
id.1 &lt;- 3 + which(policy.HTML.page == ""                    TOTAL UNIVERSITY        &lt;/div&gt;"")
id.2 &lt;- id.1 + 5
text.data &lt;- policy.HTML.page[id.1:id.2]
td.1 &lt;- gsub(pattern = ""&lt;p&gt;"", replacement = """", x = text.data, 
     ignore.case = TRUE, perl = FALSE, fixed = FALSE, useBytes = FALSE)

td.2 &lt;- gsub(pattern = ""&lt;/p&gt;"", replacement = """", x = td.1, ignore.case = TRUE,
     perl = FALSE, fixed = FALSE, useBytes = FALSE)

text.d &lt;- td.2; rm(text.data, td.1, td.2)

#Create corpus and clean 
library(tm)
library(SnowballC)
txt &lt;- VectorSource(text.d); rm(text.d)
txt.corpus &lt;- Corpus(txt)
txt.corpus &lt;- tm_map(txt.corpus, tolower)
txt.corpus &lt;- tm_map(txt.corpus, removeNumbers)
txt.corpus &lt;- tm_map(txt.corpus, removePunctuation)
txt.corpus &lt;- tm_map(txt.corpus, removeWords, stopwords(""english""))
txt.corpus &lt;- tm_map(txt.corpus, stripWhitespace); #inspect(docs[1])
txt.corpus &lt;- tm_map(txt.corpus, stemDocument)

# NOTE ERROR WHEN CREATING TDM
tdm &lt;- TermDocumentMatrix(txt.corpus)
</code></pre>
","r, text-mining, tm, corpus, term-document-matrix","<p>The link provided by jazzurro points to the solution.  The following line of code</p>

<pre><code> txt.corpus &lt;- tm_map(txt.corpus, tolower)
</code></pre>

<p>must be changed to</p>

<pre><code> txt.corpus &lt;- tm_map(txt.corpus, content_transformer(tolower))
</code></pre>
",26,12,15662,2014-08-28 14:36:26,https://stackoverflow.com/questions/25551514/termdocumentmatrix-errors-in-r
Issues with Dates in Apache OpenNLP,"<p>For a recent project to aid me learning NLP I am working on a number of documents, each of which contain a date. What I would like to be able to do is read the unstructured data and identify the date or dates within, converting it into a numeric format and possibly setting it to the documents metadata. (Note: Since the documents being used is all pseudo information, the actual meta data of the files being read in are false).</p>

<p>Recently I have been attempting to use OpenNLP in conjunction with Lucene to do so and it works to a given degree.
However if the date is written as ""13 January 1990"" or ""2010/01/05"", OpenNLP only identifies ""January 1990"" and ""2010"" respectively, but not the entire date.  Other date formats may have issues as well, I have yet to try them all.  While I recognise that OpenNLP works upon a statistical basis rather than a format basis, I can't help but get the feeling I'm making an elementary mistake.</p>

<p>Am I making a mistake?  If not is there an easy manner in which to rectify this?</p>

<p>I understand that I may be able to construct my own trained model based on a training data set.  Is the Apache OpenNLP one freely available, so I may extend it? Are there any others that are freely available?</p>

<p>Is there a better way to do this?  I've heard of Apache UIMA, the main reason why I went for OpenNLP is due to its mention in Taming Text by Manning.  I should note that the extraction of dates is the first stage of the project and other data will be extracted later as well.</p>

<p>Many thanks for any response.</p>
","apache, text-mining, opennlp, data-extraction","<p>I am not an expert in OpenNLP but I know that the problem you are trying to solve is called Temporal Expression Extraction (because I do research in this field :P). Nowadays, there are some systems which can greatly help you in extracting and unambiguously representing the temporal meaning of such expressions.</p>

<p>Here are some references:</p>

<ol>
<li>ManTIME, <a href=""http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/"" rel=""nofollow"">online demo</a>, <a href=""https://github.com/filannim/ManTIME/"" rel=""nofollow"">software</a></li>
<li>HeidelTime, <a href=""http://heideltime.ifi.uni-heidelberg.de/heideltime/"" rel=""nofollow"">online demo</a>, <a href=""https://github.com/HeidelTime/heideltime/releases"" rel=""nofollow"">software</a></li>
<li>SUTime, <a href=""http://nlp.stanford.edu:8080/sutime/"" rel=""nofollow"">online demo</a>, <a href=""http://nlp.stanford.edu/software/sutime.shtml"" rel=""nofollow"">software</a></li>
</ol>

<p>If you want a broader overview of the field, please have a look at the results of the last temporal information extraction challenge (<a href=""http://derczynski.com/sheffield/papers/tempeval-3.pdf"" rel=""nofollow"">TempEval-3, Task A</a>).</p>

<p>I hope this helps. :)</p>
",2,1,670,2014-09-01 10:48:03,https://stackoverflow.com/questions/25604121/issues-with-dates-in-apache-opennlp
Better understanding of cosine similarity,"<p>I am doing a little research on text mining and data mining. I need more help in understanding cosine similarity. I have read about it and notice that all of the given examples on the internet is using tf-idf before computing it through cosine-similarity. </p>

<p>My question </p>

<p>Is it possible to calculate cosine similarity just by using highest frequency distribution from a text file which will be the dataset. Most of the videos and tutorials that i go through has tf-idf ran before inputting it's data into cosine similarity, if no, what other types of equation/algorithm that can be input into cosine similarity? </p>

<p>2.Why is normalization used with tf-idf to compute cosine similarity? (can i do it without normalization?) Cosine similarity are computed from normalization of tf-idf output. Why is normalization needed? </p>

<p>3.What cosine similarity actually does to the weights of tf-idf?</p>
","data-mining, text-mining, cosine-similarity","<p>I do not understand question 1.</p>

<ol start=""2"">
<li><p>TF-IDF weighting is a weighting scheme that worked well for lots of people on real data (think Lucene search). But the theoretical foundations of it are a bit weak. In particular, everybody seems to be using a slightly different version of it... and yes, it is weights + cosine similarity. In practise, you may want to try e.g. Okapi BM25 weighting instead, though.</p></li>
<li><p>I do not undestand this question either. Angular similarity is beneficial because the <em>length</em> of the text has less influence than with other distances. Furthermore, sparsity can be nicely exploitet. As for the weights, IDF is a heuristic with only loose statistical arguments: frequent words are more likely to occur at random, and thus should have less weight.</p></li>
</ol>

<p>Maybe you can try to rephrase your questions so I can fully understand them. Also search for related questions such as these: <a href=""https://stackoverflow.com/questions/6255835/cosine-similarity-and-tf-idf?rq=1"">Cosine similarity and tf-idf</a> and 
<a href=""https://stackoverflow.com/questions/17537722/better-text-documents-clustering-than-tf-idf-and-cosine-similarity?rq=1"">Better text documents clustering than tf/idf and cosine similarity?</a></p>
",0,0,803,2014-09-01 20:38:38,https://stackoverflow.com/questions/25612631/better-understanding-of-cosine-similarity
arabic text mining using R,"<p>I am a new user and I just want to get help with my work on R. i am doing Arabic text mining and I would love to have some help anyone have experience in this fields. So far I felt to normalize the Arabic text and even R doesn't print the Arabic characters in the console. I am stuck now and I don’t know is it right to change the language like doing the mining in Weka or any other way. Can anyone advise me if anyone achieved anything in mining Arabic text using R?<br>
By the way I am working on Arabic tweets data set analysis. It took my one month to fetch the data. And I don’t know how long will take me to pre-processing the text.</p>
","r, text, arabic, text-mining, tweets","<p>I don't have much experience in this area, but I do not have problems with Arabic characters when I try this:</p>

<pre><code>require(tm)
require(tm.plugin.webmining)
require(SnowballC)

corpus &lt;- WebCorpus(GoogleNewsSource(""سلام""))
corpus
inspect(corpus)

tdm &lt;- TermDocumentMatrix(corpus)
</code></pre>

<p>Make sure to install the proper fonts on your OS and IDE.</p>

<pre><code>```{r}
y &lt;&lt;- dget(""file"") # get the file ext rated from MongoDB with rmongodb package
a &lt;&lt;- y$tweet_text # extract only the text of the tweets in the dataset
text_df &lt;&lt;- data.frame(a, stringsAsFactors = FALSE) # Save as a data frame
myCorpus_df &lt;&lt;- Corpus(DataframeSource(text_df_2)) # Compute a Corpus from the data frame
```
</code></pre>

<p>In OS X Arabic characters are properly represented :</p>

<pre><code>```{r}
str(myCorpus_df[1:2])
```

List of 2
 $ 1:List of 2
  ..$ content: chr ""The CHRONICLE EYE  Ahrar al#Sham is clearly fighting #ISIS where its men storm some #Manbij buildings #Aleppo ""
  ..$ meta   :List of 7
  .. ..$ author       : chr(0) 
  .. ..$ datetimestamp: POSIXlt[1:1], format: ""2014-07-03 22:42:18""
  .. ..$ description  : chr(0) 
  .. ..$ heading      : chr(0) 
  .. ..$ id           : chr ""1""
  .. ..$ language     : chr ""en""
  .. ..$ origin       : chr(0) 
  .. ..- attr(*, ""class"")= chr ""TextDocumentMeta""
  ..- attr(*, ""class"")= chr [1:2] ""PlainTextDocument"" ""TextDocument""


 $ 2:List of 2
  ..$ content: chr ""RT @######## جبهة النصرة مهاجرينها وأنصارها  مقراتها مكان آمن لكل من يخشى على نفسه الآذى ""
  ..$ meta   :List of 7
  .. ..$ author       : chr(0) 
  .. ..$ datetimestamp: POSIXlt[1:1], format: ""2014-07-03 22:42:18""
  .. ..$ description  : chr(0) 
  .. ..$ heading      : chr(0) 
  .. ..$ id           : chr ""2""
  .. ..$ language     : chr ""en""
  .. ..$ origin       : chr(0) 
  .. ..- attr(*, ""class"")= chr ""TextDocumentMeta""
  ..- attr(*, ""class"")= chr [1:2] ""PlainTextDocument"" ""TextDocument""
 - attr(*, ""class"")= chr [1:2] ""VCorpus"" ""Corpus""
</code></pre>

<p>When I check the encoding of an Arabic word on the both OS (OS X and Win 7), it seems to be well coded :</p>

<pre><code>```{r}
Encoding(""لمياه_و_الإصحا"")
```

[1] ""UTF-8""
</code></pre>

<p>This may also be helpful: 
<a href=""https://stackoverflow.com/questions/21238631/reading-arabic-data-text-in-r-and-plot"">Reading arabic data text in R and plot()</a></p>
",2,4,3308,2014-09-03 23:01:28,https://stackoverflow.com/questions/25654921/arabic-text-mining-using-r
Optimize NLTK Code To Make Predictions From Text,"<p>I am trying to build a model to predict if the salary of a job description is above or below the 75th percentile (above 1, below 0) My data has about 250,000 rows and its very hard to tokenize all the text from the job descriptions. My code seems to work fine, but it takes insane amounts of time to do it above 100 rows. I need to find a way to make it more efficient so that I can include more rows to my prediction. </p>

<pre><code>import random
import nltk
import pandas
import csv
import numpy as np

io = pandas.read_csv('Train_rev1.csv',sep=',',usecols=(2,10), nrows=501)
#converted = df.apply(lambda io : int(io[0]))
data = [np.array(x) for x in io.values]

random.shuffle(data)
size = int(len(data) * 0.6)
test_set, train_set = data[size:], data[:size]
train_set = np.array(train_set)
test_set = np.array(test_set)
x = train_set[:,1]
Sal75=np.percentile(x,75)
y = test_set[:,1]
Test75=np.percentile(y,75)

for i in range(len(train_set[:,1])):
    if train_set[i,1]&gt;=Sal75:
        train_set[i,1]=1
    else:
        train_set[i,1]=0

for i in range(len(test_set[:,1])):
    if test_set[i,1]&gt;=Test75:
        test_set[i,1]=1
    else:
        test_set[i,1]=0

train_setT = [tuple(x) for x in train_set]
test_setT = [tuple(x) for x in test_set]



from nltk.tokenize import word_tokenize
all_words = set(word.lower() for passage in train_setT for word in word_tokenize(passage[0]))
t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in train_setT]

classifier = nltk.NaiveBayesClassifier.train(t)

all_words2 = set(word.lower() for passage in test_setT for word in word_tokenize(passage[0]))
tt = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in test_setT]


print nltk.classify.accuracy(classifier, tt)
classifier.show_most_informative_features(20)
testres = []
predres = []
for i in range(len(tt)):
    testres.append(tt[i][1])
for i in range(len(tt)):
    z = classifier.classify(tt[i][0])
    predres.append(z)
from nltk.metrics import ConfusionMatrix
cm = nltk.ConfusionMatrix(testres, predres)
print(cm)
</code></pre>

<p>The csv file was extracted from Kaggle.<a href=""http://www.kaggle.com/c/job-salary-prediction/data"" rel=""nofollow"">Use Train_rev1</a> </p>
","python, performance, nltk, tokenize, text-mining","<p>After you have split the data into 60% and 40% You can do the following. This will require new tools, and perhaps not NLTK.  </p>

<pre><code>import random
import nltk
import pandas
import csv
import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics
from operator import itemgetter
from sklearn.metrics import classification_report
train_setT = [tuple(x) for x in train_set]
test_setT = [tuple(x) for x in test_set]


train_set = np.array([''.join(el[0]) for el in train_setT])
test_set = np.array([''.join(el[0]) for el in test_setT])

y_train = np.array([el[1] for el in train_setT])
y_test = np.array([el[1] for el in test_setT])

vectorizer = TfidfVectorizer(min_df=2,ngram_range=(1, 2), strip_accents='unicode', norm='l2')

X_train = vectorizer.fit_transform(train_set)
X_test = vectorizer.transform(test_set)

nb_classifier = MultinomialNB().fit(X_train, y_train)

y_nb_predicted = nb_classifier.predict(X_test)


print metrics.confusion_matrix(y_test, y_nb_predicted)
print classification_report(y_test, y_nb_predicted)
</code></pre>
",1,1,1421,2014-09-12 21:32:44,https://stackoverflow.com/questions/25817177/optimize-nltk-code-to-make-predictions-from-text
Framework for Implement Text Mining Pipeline with Java,"<p>I need to develop text document processing pipeline with several processing stages. So I'm looking for a good Java based framework to handle the pipeline with multithreaded processing as I want to focus more on business logic in each processing stage.</p>

<p>After searching few hours, I found following frameworks which goes with some of my requirements. But all of them have their own drawbacks</p>

<p><a href=""http://commons.apache.org/sandbox/commons-pipeline/"" rel=""nofollow"">Apache Commons Pipeline</a> - Seems this is not an active project anymore and doesn't have good documentation</p>

<p><a href=""https://github.com/tinkerpop/pipes/wiki"" rel=""nofollow"">TinkerPop Pipes</a> -  Doesn't support multithreaded execution</p>

<p><a href=""http://projects.spring.io/spring-batch/"" rel=""nofollow"">Spring Batch</a> - Doesn't support several processing stages</p>

<p>Can anyone suggest me any other good lightweight framework for this purpose?</p>
","java, machine-learning, pipeline, text-mining","<p>You can take a look at <a href=""https://github.com/j-easy/easy-batch"" rel=""nofollow noreferrer"">easy batch</a>. It allows you to easily develop pipelines with Java. It also supports parallelism. It was designed to address drawbacks of the frameworks you mentioned with a lightweight and easy to use alternative.</p>
<p>Hope it helps</p>
",0,0,972,2014-09-14 22:00:07,https://stackoverflow.com/questions/25838426/framework-for-implement-text-mining-pipeline-with-java
parsing complex text-line and save each entry to a variable - java,"<p>I have auto-generated lines like the following:</p>

<pre><code>  40   1655.28   -------   212.61     2.01   ( 40.31%) (  9)STRE C=C (aminoA - part A) -&gt; 1
</code></pre>

<p>is their any rational  and ""easy"" way to split the previous line other that <code>line.split(""\\s+"");</code>? and get something like the following:</p>

<pre><code>a0=40
a1=1655.28
</code></pre>

<p>...</p>

<pre><code>ai=40.31
aii=9
aiii=STRE C=C
aiv=aminoA - part A
</code></pre>

<p>The problem with <code>line.split(""\\s+"");</code> is that I have round bracket without spaces between two entries; e.g.:<code>(  9)STRE C=C</code> or <code>(aminoA - part A)</code>, which gives results like: <code>ax1=(</code>,<code>ax2=9)STRE</code> and <code>ay1=(aminoA</code>. This of course could be caught, but I need in this case to check each entry by multiple regex in - for me - complex-nested if-conditions.</p>
","java, parsing, split, text-mining","<p>So, without fixed columns then I would use regex. I would also use a class to hold your columns for ease of lookup. </p>

<pre><code>class Result {

    private static Pattern resultRegex = Pattern.compile(
        ""(\\d+)"" // 40
        + ""\\s+""
        + ""(\\d+\\.\\d+)"" // 1655.28
        + ""\\s+\\-+\\s+""
        + ""(\\d+\\.\\d+)"" // 212.61
        + ""\\s+""
        + ""(\\d+\\.\\d+)"" // 2.01
        + ""\\s+\\(\\s*"" // (
        + ""(\\d+\\.\\d+)"" // 40.31
        + ""%\\)\\s+\\(\\s*"" // ) (
        + ""(\\d)"" // 9
        + ""\\)"" // )
        + ""(.*)"" // STRE C=C
        + ""\\("" // (
        + ""(.*)"" // aminoA - part A
        + ""\\)""); // )

    private final int col0;
    private final double col1;
    private final double col2;
    private final double col3;
    private final double col4;
    private final int col5;
    private final String col6;
    private final String col7;

    Result(String data) {

        Matcher matcher = resultRegex.matcher(data);
        matcher.find();
        col0 = Integer.parseInt(matcher.group(1));
        col1 = Double.parseDouble(matcher.group(2));
        col2 = Double.parseDouble(matcher.group(3));
        col3 = Double.parseDouble(matcher.group(4));
        col4 = Double.parseDouble(matcher.group(5));
        col5 = Integer.parseInt(matcher.group(6));
        col6 = matcher.group(7);
        col7 = matcher.group(8);
    }

    @Override
    public String toString() {
        return new StringBuilder()
            .append(""col0="").append(col0)
            .append("",col1="").append(col1)
            .append("",col2="").append(col2)
            .append("",col3="").append(col3)
            .append("",col4="").append(col4)
            .append("",col5="").append(col5)
            .append("",col6="").append(col6)
            .append("",col7="").append(col7).toString();
    }
}
</code></pre>

<p>Then you can read each line from your file/stream/whatever and do this</p>

<pre><code>String data = ""40   1655.28   -------   212.61     2.01   ( 40.31%) (  9)STRE C=C (aminoA - part A) -&gt; 1"";
Result result = new Result(data);
System.out.println(result);
</code></pre>

<p>Obviously, I haven't added any validation but hopefully that's a good enough example to get started.</p>
",1,0,106,2014-09-17 08:04:59,https://stackoverflow.com/questions/25885350/parsing-complex-text-line-and-save-each-entry-to-a-variable-java
Removing overly common words (occur in more than 80% of the documents) in R,"<p>I am working with the 'tm' package in to create a corpus. I have done most of the preprocessing steps.
The remaining thing is to remove overly common words (terms that occur in more than 80% of the documents). Can anybody help me with this?</p>

<pre><code>dsc &lt;- Corpus(dd)
dsc &lt;- tm_map(dsc, stripWhitespace)
dsc &lt;- tm_map(dsc, removePunctuation)
dsc &lt;- tm_map(dsc, removeNumbers)
dsc &lt;- tm_map(dsc, removeWords, otherWords1)
dsc &lt;- tm_map(dsc, removeWords, otherWords2)
dsc &lt;- tm_map(dsc, removeWords, otherWords3)
dsc &lt;- tm_map(dsc, removeWords, javaKeywords)
dsc &lt;- tm_map(dsc, removeWords, stopwords(""english""))
dsc = tm_map(dsc, stemDocument)
dtm&lt;- DocumentTermMatrix(dsc, control = list(weighting = weightTf, 
                         stopwords = FALSE))

dtm = removeSparseTerms(dtm, 0.99) 
# ^-  Removes overly rare words (occur in less than 2% of the documents)
</code></pre>
","r, text-mining, tm","<p>What if you made a <code>removeCommonTerms</code> function</p>

<pre><code>removeCommonTerms &lt;- function (x, pct) 
{
    stopifnot(inherits(x, c(""DocumentTermMatrix"", ""TermDocumentMatrix"")), 
        is.numeric(pct), pct &gt; 0, pct &lt; 1)
    m &lt;- if (inherits(x, ""DocumentTermMatrix"")) 
        t(x)
    else x
    t &lt;- table(m$i) &lt; m$ncol * (pct)
    termIndex &lt;- as.numeric(names(t[t]))
    if (inherits(x, ""DocumentTermMatrix"")) 
        x[, termIndex]
    else x[termIndex, ]
}
</code></pre>

<p>Then if you wanted to remove terms that in are >=80% of the documents, you could do</p>

<pre><code>data(""crude"")
dtm &lt;- DocumentTermMatrix(crude)
dtm
# &lt;&lt;DocumentTermMatrix (documents: 20, terms: 1266)&gt;&gt;
# Non-/sparse entries: 2255/23065
# Sparsity           : 91%
# Maximal term length: 17
# Weighting          : term frequency (tf)

removeCommonTerms(dtm ,.8)
# &lt;&lt;DocumentTermMatrix (documents: 20, terms: 1259)&gt;&gt;
# Non-/sparse entries: 2129/23051
# Sparsity           : 92%
# Maximal term length: 17
# Weighting          : term frequency (tf)
</code></pre>
",13,5,12859,2014-09-18 05:55:57,https://stackoverflow.com/questions/25905144/removing-overly-common-words-occur-in-more-than-80-of-the-documents-in-r
Split Identifier and Method Names in Creating Source Code Corpus,"<p>I am trying to create a corpus from Java source code. <br>
I am following the preprocessing steps in this paper <a href=""http://cs.queensu.ca/~sthomas/data/Thomas_2011_MSR.pdf"" rel=""nofollow"">http://cs.queensu.ca/~sthomas/data/Thomas_2011_MSR.pdf</a> <br><br>
Based on the section [2.1] the following thing should be removed: <br>
- characters related to the syntax of the programming language [already done by removePunctuation] <br>
- programming language keywords [already done by tm_map(dsc, removeWords, javaKeywords)] <br>
- common English-language stopwords [already done by tm_map(dsc, removeWords, stopwords(""english""))] <br>
- word stemming [already done by tm_map(dsc, stemDocument)] <br></p>

<p>The remaining part is to split identifier and method names into multiple parts based on common naming conventions. <br><br>For example 'firstName' should be split into 'first' and 'name'.
<br><br>Another example 'calculateAge' should be split into 'calculate' and 'age'.
<br>Can anybody help me with this?</p>

<pre><code>    library(tm)
    dd = DirSource(pattern = "".java"", recursive = TRUE)
    javaKeywords = c(""abstract"",""continue"",""for"",""new"",""switch"",""assert"",""the"",""default"",""package"",""synchronized"",""boolean"",""do"",""if"",""private"",""this"",""break"",""double"",""implements"",""protected"",""throw"",""byte"",""else"",""the"",""null"",""NULL"",""TRUE"",""FALSE"",""true"",""false"",""import"",""public"",""throws"",""case"",""enum"", ""instanceof"",""return"",""transient"",""catch"",""extends"",""int"",""short"",""try"",""char"",""final"",""interface"",""static"",""void"",""class"",""finally"",""long"",""volatile"",""const"",""float"",""native"",""super"",""while"")
    dsc &lt;- Corpus(dd)
    dsc &lt;- tm_map(dsc, stripWhitespace)
    dsc &lt;- tm_map(dsc, removePunctuation)
    dsc &lt;- tm_map(dsc, removeNumbers)
    dsc &lt;- tm_map(dsc, removeWords, stopwords(""english""))
    dsc &lt;- tm_map(dsc, removeWords, javaKeywords)
    dsc = tm_map(dsc, stemDocument)
    dtm&lt;- DocumentTermMatrix(dsc, control = list(weighting = weightTf, stopwords = FALSE))
</code></pre>
","r, text-mining, tm","<p>I've written a tool in Perl to do all kinds of source code preprocessing, including identifier splitting:</p>

<p><a href=""https://github.com/stepthom/lscp"" rel=""nofollow"">https://github.com/stepthom/lscp</a></p>

<p>The relevant piece of code there is:</p>

<pre><code>=head2 tokenize
 Title    : tokenize
 Usage    : tokenize($wordsIn)
 Function : Splits words based on camelCase, under_scores, and dot.notation.
          : Leaves other words alone.
 Returns  : $wordsOut =&gt; string, the tokenized words
 Args     : named arguments:
          : $wordsIn =&gt; string, the white-space delimited words to process
=cut
sub tokenize{
    my $wordsIn  = shift;
    my $wordsOut = """";

    for my $w (split /\s+/, $wordsIn) {
        # Split up camel case: aaA ==&gt; aa A
        $w =~ s/([a-z]+)([A-Z])/$1 $2/g;

        # Split up camel case: AAa ==&gt; A Aa
        # Split up camel case: AAAAa ==&gt; AAA Aa
        $w =~ s/([A-Z]{1,100})([A-Z])([a-z]+)/$1 $2$3/g;

        # Split up underscores 
        $w =~ s/_/ /g;

        # Split up dots
        $w =~ s/([a-zA-Z0-9])\.+([a-zA-Z0-9])/$1 $2/g;

        $wordsOut = ""$wordsOut $w"";
    }

    return removeDuplicateSpaces($wordsOut);
}
</code></pre>

<p>The above hacks are based on my own experience with preprocessing source code for textual analysis. Feel free to steal and modify.</p>
",1,1,357,2014-09-20 21:21:58,https://stackoverflow.com/questions/25953426/split-identifier-and-method-names-in-creating-source-code-corpus
Text Mining - What is the best way to mine descriptive excel sheet data,"<p>I have university placement data pulled from databases in excel sheet. I need to text mine the job description offered by companies, which is a descriptive field for all the rows and then come up with the analysis of profiles in demand. 
Here is a snapshot of the data
<img src=""https://i.sstatic.net/xXzzf.png"" alt=""enter image description here""></p>

<p>Could anyone help me to kick start this activity?</p>

<p>Thanks
Saurabh</p>
","excel, text-mining, data-analysis, vba","<p>I am not a data expert but I have some data mining experience. I would try following these steps for starters:</p>

<ol>
<li><p>Excel is not a good for such an analysis. Find some tool dedicated to data mining e.g. RStudio. R has many useful out-of-the-box algorithms for data mining.</p></li>
<li><p>Cleanse the data e.g. all texts to lower case, remove stop words, remove punctuation, remove additional white spaces. </p></li>
<li><p>Tokenize the data e.g. 1 word tokens - ""finance"", ""bachelor""</p></li>
<li><p>Decide on how you will assert if a certain profile is in demand or not? If by profile you mean that you need the information on the frequency of certain tokens appearing in the data more often then others e.g. ""finance"", ""bachelor"" etc. then simply create a frequency matrix. R allows you to create a visualisation of this - Word Clouds. </p></li>
</ol>

<p>This is to start you off :). I am sure there is much more to be suggested in this matter.</p>
",1,0,4051,2014-09-25 10:02:43,https://stackoverflow.com/questions/26035773/text-mining-what-is-the-best-way-to-mine-descriptive-excel-sheet-data
Regular expression to parse sequence IDs,"<p>I'm having a bit of trouble with using regular expressions to extract information from flat files
(just text). The files are structured as such: </p>

<h3>#</h3>

<p>ID (e.g. >YAL001C)</p>

<p>Annotations/metadata (short phrases describing origin of ID)</p>

<p>Sequence (very long string of characters, e.g. KRHDE .... ~500 letters on average)</p>

<h3>#</h3>

<p>I am trying to extract only IDs and sequences (skip all the metadata). Unfortunately, list 
operations alone don't suffice, e.g.</p>

<pre><code>with open(""composition.in"",""rb"") as all_info:
    all_info=all_info.read() 
    all_info=all_info.split(""&gt;"")[1:]
</code></pre>

<p>because the metadata/annotation part of the text is littered with '>' characters that 
cause the list that is generated to be incorrectly structured. List comprehensions get very
ugly after a certain point, so I am trying the following:</p>

<pre><code>with open(""composition.in"",""rb"") as yeast_all:
yeast_all=yeast_all.read() # convert file to string

## Regular expression to clean up rogue ""&gt;"" characters
## i.e. ""&lt;i&gt;"", ""&lt;sub&gt;"", etc which screw up
## the structure of the eveuntual list
import re
id_delimeter = r'^&gt;{1}+\w{7,10}+\s' 
match=re.search(id_delimeter, yeast_all)
if match:
    print 'found', match.group()
else:
    print 'did not find'        
yeast_all=yeast_all.split(id_delimeter)[1:]
</code></pre>

<p>I get only an error message saying ""error: multiple repeat""</p>

<p>The IDs are of type:</p>

<blockquote>
  <p>YAL001C</p>
  
  <p>YGR103W  </p>
  
  <p>YKL068W-A</p>
</blockquote>

<p>The first character is always "">"", followed by capital letters and numbers and sometimes dashes
(-). I would like a RE that could be used to find all such occurrences and split the text 
using the RE as a delimeter in order to get IDs and sequences and leave out metadata. I am new to regular expressions so have limited knowledge of the topic!</p>

<p>Note: Only a single newline between each of the three fields (ID, metadata, sequence)       </p>
","python, regex, text-mining","<p>Try</p>

<pre><code>&gt;(?P&lt;id&gt;[\w-]+)\s.*\n(?P&lt;sequence&gt;[\w\n]+)
</code></pre>

<p>You'll find the ID in the group <code>id</code>and the sequence in the group <code>sequence</code>.</p>

<p><a href=""http://regex101.com/r/wC3kO3/4"" rel=""nofollow"">Demo.</a></p>

<p>Explanation:</p>

<pre><code>&gt; # start with a ""&gt;"" character
(?P&lt;id&gt; # capture the ID in group ""id""
    [\w-]+ # this matches any number (&gt;1) of word characters (A to Z, a to z, digits, and _) or dashes ""-""
)
\s+ # after the ID, there must be at least one whitespace character
.* # consume the metadata part, we have no interest in this
\n # up to a newline
(?P&lt;sequence&gt; # finally, capture the sequence data in group ""sequence""
    [\w\n]+ # this matches any number (&gt;1) of word characters and newlines.
)
</code></pre>

<p>As python code:</p>

<pre><code>text= '''&gt;YKL068W-A
foo
ABCD

&gt;XYZ1234
&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&gt;&lt;&lt;&lt;&gt;
LMNOP'''

pattern= '&gt;(?P&lt;id&gt;[\w-]+)\n.*\n(?P&lt;sequence&gt;\w+)'

for id, sequence in re.findall(pattern, text):
    print((id, sequence))
</code></pre>
",0,0,388,2014-10-02 19:12:45,https://stackoverflow.com/questions/26167898/regular-expression-to-parse-sequence-ids
How can I create a bag of words for latex strings?,"<p>I have a set of input paragraphs in latex formats. I want to create a bag of words from them.</p>

<p>Taking a set of guys that look like these: </p>

<pre><code>""Some guy did something with \emph{ yikes } $ \epsilon $""
</code></pre>

<p>I want to out put a dictionary: </p>

<pre><code>{ 
  ""Some"": 40,
   ...
   ""yikes"": 10
   ""epsilon (or unicode for it)"": 3
} 
</code></pre>

<p>That is I need a dictionary where the set of keys are the set of words/symbols/equations (I'll call all of these words for brevity) across all paragraphs and a count of their occurrences across all paragraphs as well. </p>

<p>From there given k-ordered-tuple of words, I need a k-array for each paragraph where the ith element in the array represents the count of the word in the ith tuple in that paragraph. </p>

<p>so say <code>(Some, dunk, yikes, epsilon)</code> will give me 
<code>[1, 0, 1, 1]</code> for the stated example. </p>

<p>I've tried this by using a lexer to get the tokens out and processing the tokens directly. This is difficult and error prone not to mention slow. Is there a better strategy or tool that can do this? </p>

<p>There are some corner cases to consider with special characters: </p>

<pre><code>G\""""odel =&gt; Gödel 
</code></pre>

<p>for example. I'd like to preserve these.</p>

<p>Also, I'd like to drop equations all together or keep them as one word. Equations occur in between $ ... $ signs. </p>
","machine-learning, scikit-learn, text-mining, feature-extraction","<p>If I understand correctly, you are trying to do the following:</p>

<ol>
<li><p>Split the sentence into words:</p>

<pre class=""lang-py prettyprint-override""><code>s = ""Some guy did something with \emph{ yikes } \epsilon""
words = s.split()
print words
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>['Some', 'guy', 'did', 'something', 'with', '\\emph{', 'yikes', '}', '\\epsilon']
</code></pre></li>
<li><p>Count the number of occurrences:</p>

<pre class=""lang-py prettyprint-override""><code>from collections import Counter
dictionary = Counter(words)
print dictionary
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>Counter({'did': 1, '}': 1, '\\epsilon': 1, 'Some': 1, 'yikes': 1, 'something': 1, 'guy': 1, 'with': 1, '\\emph{': 1})
</code></pre></li>
<li><p>Access words and their corresponding numbers as separate lists:</p>

<pre class=""lang-py prettyprint-override""><code>print dictionary.keys()
print dictionary.values()
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>['did', '}', '\\epsilon', 'Some', 'yikes', 'something', 'guy', 'with', '\\emph{']
[1, 1, 1, 1, 1, 1, 1, 1, 1]
</code></pre></li>
</ol>

<p>Note that I didn't process any word, yet. You might want to strip brackets or backslashes. But this can be easily done by traversing the dictionary (or lists) with a for-loop and handling each entry individually.</p>

<hr>

<p><strong>To convert LaTeX umlauts</strong> to unicode characters is somehow a whole new problem. There are several stackoverflow questions and answers on this topic. Maybe you just need to find/replace them in the initial string:</p>

<pre class=""lang-py prettyprint-override""><code>s = s.replace('\\""o', unichr(252))
</code></pre>

<p>(Note that depending on your command line encoding you might not see umlauts with <code>print s</code>. But they are not lost, as can be shown using <code>print repr(s)</code>.)</p>

<p><strong>To preserve equations</strong> you can split the string using a regular expression rather than <code>split</code>:</p>

<pre><code>import re
print re.findall('\$.+\$|[\w]+', s)
</code></pre>

<p>Output:</p>

<pre><code>['Some', 'guy', 'did', 'something', 'with', 'emph', 'yikes', '$ \\epsilon $']
</code></pre>

<p>Please see <a href=""https://stackoverflow.com/a/25828094/3419103"">my answer to another question</a> for a similar example and a more detailed explanation.</p>
",2,0,230,2014-10-04 18:10:37,https://stackoverflow.com/questions/26195681/how-can-i-create-a-bag-of-words-for-latex-strings
R - Text Mining - Importing a Corpus and keeping the file names in document term matrix,"<p>Up until recently (1 month ago) the code shown below allowed me to import a series of .txt documents stored in a local folder into R, to create a Corpus, pre-process it and finally to convert it into a Document Term Matrix. The issue I am having is that the document names are not being imported, instead each document is listed as 'character(0)'. </p>

<p>One of my aims is to conduct topic modelling on the corpus and so it is important that I can relate the document names to the topics that the model produces. </p>

<p>Does anyone have any suggestions as to what has changed? Or how I can fix this?</p>

<pre><code>library(""tm"")
library(""SnowballC"")

setwd(""C:/Users/Documents/Dataset/"")
corpus &lt;-Corpus(DirSource(""blog""))


#pre_processing
myStopwords &lt;- c(stopwords(""english""))
your_corpus &lt;- tm_map(corpus, tolower)
your_corpus &lt;- tm_map(your_corpus, removeNumbers)
your_corpus &lt;- tm_map(your_corpus, removeWords, myStopwords) 
your_corpus &lt;- tm_map(your_corpus, stripWhitespace)
your_corpus &lt;- tm_map(your_corpus, removePunctuation)
your_corpus &lt;- tm_map(your_corpus, stemDocument)
your_corpus &lt;- tm_map(your_corpus, PlainTextDocument)

#creating a doucment term matrix
myDtm &lt;- DocumentTermMatrix(your_corpus, control=list(wordLengths=c(3,Inf)))

dim(myDtm)
inspect(myDtm)
</code></pre>
","r, import, text-mining, corpus","<p>Here's a debugging session to identify / correct the loss of file name.  The tolower line was modified, and the plaintext line was commented-out since these lines remove the file information.  Also, if you check ds$reader, you can see the baseline reader creates a plain text document.</p>

<pre><code>library(""tm"")
library(""SnowballC"")

# corpus &lt;-Corpus(DirSource(""blog""))

sf&lt;-system.file(""texts"", ""txt"", package = ""tm"")
ds &lt;-DirSource(sf)
your_corpus &lt;-Corpus(ds)

# Check status with the following line
meta(your_corpus[[1]])

#pre_processing
myStopwords &lt;- c(stopwords(""english""))
# your_corpus &lt;- tm_map(your_corpus, tolower)
your_corpus &lt;- tm_map(your_corpus, content_transformer(tolower))
meta(your_corpus[[1]])
your_corpus &lt;- tm_map(your_corpus, removeNumbers)
meta(your_corpus[[1]])
your_corpus &lt;- tm_map(your_corpus, removeWords, myStopwords) 
meta(your_corpus[[1]])
your_corpus &lt;- tm_map(your_corpus, stripWhitespace)
meta(your_corpus[[1]])
your_corpus &lt;- tm_map(your_corpus, removePunctuation)
meta(your_corpus[[1]])
your_corpus &lt;- tm_map(your_corpus, stemDocument)
meta(your_corpus[[1]])
#your_corpus &lt;- tm_map(your_corpus, PlainTextDocument)
#meta(your_corpus[[1]])

#creating a doucment term matrix
myDtm &lt;- DocumentTermMatrix(your_corpus, control=list(wordLengths=c(3,Inf)))

dim(myDtm)
inspect(myDtm)
</code></pre>
",2,2,6911,2014-10-08 13:19:44,https://stackoverflow.com/questions/26257951/r-text-mining-importing-a-corpus-and-keeping-the-file-names-in-document-term
R build TermDocumentMatrix with removeSparseTerms parameter,"<p>Am I able to remove sparse terms WHILE creating a <code>tm::TermDocumentMatrix</code> object? </p>

<p>I tried: </p>

<pre><code>TermDocumentMatrix(file.corp, control = list(removeSparseTerms=0.998))
</code></pre>

<p>but it does not work. </p>
","r, text-mining, tm, term-document-matrix","<p>No, you cannot remove sparse terms like that with the <code>TermDocumentMatrix</code> function. If you check the help for that function with <code>?TermDocumentMatrix</code> you'll see that the options for <code>control</code> are listed in the help for  <code>termFreq</code>, and when you look at the help for that function with <code>?termFreq</code>, you'll see that <code>removeSparseTerms</code> is not listed there. Although you have <code>bounds</code> which can do a related job.</p>

<p>If you just want a one-liner that combines <code>TermDocumentMatrix</code> and <code>removeSparseTerms</code>, you simply flip your line inside-out and that will work fine:</p>

<pre><code>removeSparseTerms(TermDocumentMatrix(file.corp), 0.998)
</code></pre>

<p>I recommend you have a careful look at the documentation for the <code>tm</code> package, it's one of better examples of a well-documented contributed package. It might save you time waiting for someone to answer your questions here!</p>
",1,1,757,2014-10-21 11:05:02,https://stackoverflow.com/questions/26485174/r-build-termdocumentmatrix-with-removesparseterms-parameter
How to remove the numbers in the file in java,"<p>This is my code..Help me remove the numbers before comma and print both the words</p>

<pre><code>for(int y=0;y&lt;nso.length;y++)
        {
                File e=new File(nso[y]);
                BufferedReader be=new BufferedReader(new FileReader(e));
            String st,t;
            while((st=be.readLine())!=null)
            {
                if(st.contains(""ns""))
                {
                    int offset=st.indexOf(""("");
                  st=st.substring(offset+1, st.length()-3);  
                   System.out.println(st);
                  ap.write(st);
                  ap.println(""\n"");
                }
            }
            ap.close();
            be.close();
</code></pre>

<p>The file contains</p>

<pre><code> phone-6, This
    know-8, I
    looking-5, you
    phone-14, this-
    graphics-2, has-
</code></pre>
","java, text-mining","<p>You can use <code>st.replaceAll(regex,"""")</code> here to replace all numbers with <code>""""</code>.
For example 
<code>
String result=st.replaceAll(""[0-9]+"","""");
System.out.println(result);</code></p>
",0,0,1903,2014-10-28 09:18:47,https://stackoverflow.com/questions/26604601/how-to-remove-the-numbers-in-the-file-in-java
Counter ngram with tm package in R,"<p>I created a script for the frequency of words in a document using the object and a dictionary documentTermMatrix in R. The script works on individual words and not on the compound word
es. ""foo"" ""bar"" ""foo bar""</p>

<p>This is the code</p>

<pre><code>require(tm)
my.docs &lt;- c(""foo bar word1 word2"")
myCorpus &lt;- Corpus(VectorSource(my.docs))
inspect(DocumentTermMatrix(myCorpus,list(dictionary = c(""foo"",""bar"",""foo bar""))))
</code></pre>

<p>But the result is </p>

<pre><code>Terms

Docs bar foo  foo bar

   1   1   1        0
</code></pre>

<p>I would have to find one ""foo bar"" = 1 </p>

<p>How can I fix this?</p>
","r, dictionary, frequency, text-mining, tm","<p>The problem is that <code>DocummentTermMatrix(...)</code> is tokenizing at word breaks be default. You need at least bigrams.</p>

<p>Credit to <a href=""https://stackoverflow.com/questions/16836022/findassocs-for-multiple-terms-in-r"">this post</a> for the basic approach.</p>

<pre><code>library(tm)
library(RWeka)
my.docs &lt;- c(""foo bar word1 word2"")
myCorpus &lt;- Corpus(VectorSource(my.docs))
myDict   &lt;- c(""foo"",""bar"",""foo bar"")
BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))
inspect(DocumentTermMatrix(myCorpus, control=list(tokenize=BigramTokenizer,
                                                  dictionary=myDict)))
# &lt;&lt;DocumentTermMatrix (documents: 1, terms: 3)&gt;&gt;
# ...
#     Terms
# Docs bar foo foo bar
#    1   1   1       1
</code></pre>
",4,4,1215,2014-11-05 18:13:21,https://stackoverflow.com/questions/26764187/counter-ngram-with-tm-package-in-r
Big Text Corpus breaks tm_map,"<p>I have been breaking my head over this one over the last few days. I searched all the SO archives and tried the suggested solutions but just can't seem to get this to work. I have sets of txt documents in folders such as 2000 06, 1995 -99 etc, and want to run some basic text mining operations such as creating document term matrix and term document matrix and doing some operations based co-locations of words. My script works on a smaller corpus, however, when I try it with the bigger corpus, it fails me. I have pasted in the code for one such folder operation.</p>

<pre><code>library(tm) # Framework for text mining.
library(SnowballC) # Provides wordStem() for stemming.
library(RColorBrewer) # Generate palette of colours for plots.
library(ggplot2) # Plot word frequencies.
library(magrittr)
library(Rgraphviz)
library(directlabels)

setwd(""/ConvertedText"")
txt &lt;- file.path(""2000 -06"")

docs&lt;-VCorpus(DirSource(txt, encoding = ""UTF-8""),readerControl = list(language = ""UTF-8""))
docs &lt;- tm_map(docs, content_transformer(tolower), mc.cores=1)
docs &lt;- tm_map(docs, removeNumbers, mc.cores=1)
docs &lt;- tm_map(docs, removePunctuation, mc.cores=1)
docs &lt;- tm_map(docs, stripWhitespace, mc.cores=1)
docs &lt;- tm_map(docs, removeWords, stopwords(""SMART""), mc.cores=1)
docs &lt;- tm_map(docs, removeWords, stopwords(""en""), mc.cores=1)
#corpus creation complete

setwd(""/ConvertedText/output"")
dtm&lt;-DocumentTermMatrix(docs)
tdm&lt;-TermDocumentMatrix(docs)
m&lt;-as.matrix(dtm)
write.csv(m, file=""dtm.csv"")
dtms&lt;-removeSparseTerms(dtm, 0.2)
m1&lt;-as.matrix(dtms)
write.csv(m1, file=""dtms.csv"")
# matrix creation/storage complete

freq &lt;- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf &lt;- data.frame(word=names(freq), freq=freq)
freq[1:50]
#adjust freq score in next line
p &lt;- ggplot(subset(wf, freq&gt;100), aes(word, freq))+ geom_bar(stat=""identity"")+ theme(axis.text.x=element_text(angle=45, hjust=1))
ggsave(""frequency2000-06.png"", height=12,width=17, dpi=72)
# frequency graph generated


x&lt;-as.matrix(findFreqTerms(dtm, lowfreq=1000))
write.csv(x, file=""freqterms00-06.csv"")
png(""correlation2000-06.png"", width=12, height=12, units=""in"", res=900)
graph.par(list(edges=list(col=""lightblue"", lty=""solid"", lwd=0.3)))
graph.par(list(nodes=list(col=""darkgreen"", lty=""dotted"", lwd=2, fontsize=50)))
plot(dtm, terms=findFreqTerms(dtm, lowfreq=1000)[1:50],corThreshold=0.7)
dev.off()
</code></pre>

<p>When I use the mc.cores=1 argument in tm_map, the operation continues indefinitely. However, if I use the lazy=TRUE argument in tm_map, it seemingly goes well, but subsequent operations give this error. </p>

<pre><code>Error in UseMethod(""meta"", x) : 
  no applicable method for 'meta' applied to an object of class ""try-error""
In addition: Warning messages:
1: In mclapply(x$content[i], function(d) tm_reduce(d, x$lazy$maps)) :
  all scheduled cores encountered errors in user code
2: In mclapply(unname(content(x)), termFreq, control) :
  all scheduled cores encountered errors in user code
</code></pre>

<p>I have been looking all over for a solution but have failed consistently. Any help would be greatly appreciated!</p>

<p>Best!
k</p>
","r, text-mining, tm, text-analysis, term-document-matrix","<p>I found a solution that works.</p>

<p><strong>Background/Debugging Steps</strong></p>

<p>I tried several things that did not work:</p>

<ul>
<li>Adding ""content_transformer"" to some tm_map, to all, to one(totower)</li>
<li>Adding ""lazy = T"" to tm_map</li>
<li>Tried some parallel computing packages</li>
</ul>

<p>While it isn't working for 2 of my scripts, it works every time for a third script. But the code of all three scripts is the same only the size of the .rda file I am loading is different. The data structure is also identical for all three.</p>

<ul>
<li>Dataset 1: Size - 493.3KB = error </li>
<li>Dataset 2: Size - 630.6KB = error</li>
<li>Dataset 3: Size - 300.2KB = works!</li>
</ul>

<p>Just weird.</p>

<p>My <code>sessionInfo()</code> output:</p>

<pre><code>R version 3.1.2 (2014-10-31)
Platform: x86_64-apple-darwin13.4.0 (64-bit)

locale:
[1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] snowfall_1.84-6    snow_0.3-13        Snowball_0.0-11    RWekajars_3.7.11-1 rJava_0.9-6              RWeka_0.4-23      
[7] slam_0.1-32        SnowballC_0.5.1    tm_0.6             NLP_0.1-5          twitteR_1.1.8      devtools_1.6      

loaded via a namespace (and not attached):
[1] bit_1.1-12     bit64_0.9-4    grid_3.1.2     httr_0.5       parallel_3.1.2 RCurl_1.95-4.3    rjson_0.2.14   stringr_0.6.2 
[9] tools_3.1.2
</code></pre>

<p><strong>Solution</strong></p>

<p>I just added this line after loading the data and everything works now:</p>

<pre><code>MyCorpus &lt;- tm_map(MyCorpus,
                     content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
                     mc.cores=1)
</code></pre>

<p>Found the hint here: <a href=""http://davetang.org/muse/2013/04/06/using-the-r_twitter-package/"">http://davetang.org/muse/2013/04/06/using-the-r_twitter-package/</a> (The author has updated his code due to the error on November 26, 2014.)</p>
",13,6,6807,2014-11-09 23:30:15,https://stackoverflow.com/questions/26834576/big-text-corpus-breaks-tm-map
Self-conflicting stopwords in R tm text mining,"<p>I'm in the process of cleaning up data for text mining. This involves removing numbers, punctuation, and stopwords (common words that would just be noise in the data mining), and later doing word stemming.</p>

<p>Using the <code>tm</code> package in <code>R</code>, you can remove stopwords, for example using <code>tm_map(myCorpus, removeWords, stopwords('english'))</code>.  The <code>tm</code> manual itself demonstrates using <code>stopwords(""english""))</code>. This word list contains contractions such as ""I'd"" and ""I'll"", as well as the very common word ""I"":</p>

<pre><code>&gt; library(tm)
&gt; which(stopwords('english') == ""i"")
[1] 1
&gt; which(stopwords('english') == ""i'd"")
[1] 69
</code></pre>

<p>(Text is assumed to be lowercase before removing stopwords.)</p>

<p>But (presumably) because ""i"" comes first in the list, the contractions are never removed:</p>

<pre><code>&gt; removeWords(""i'd like a soda, please"", stopwords('english'))
[1] ""'d like  soda, please""
</code></pre>

<p>A quick hack is to reverse the wordlist:</p>

<pre><code>&gt; removeWords(""i'd like a soda, please"", rev.default(stopwords('english')))
[1] "" like  soda, please""
</code></pre>

<p>Another solution is to find/make a better wordlist.</p>

<p>Is there a better/correct way to use stopwords('english')?</p>
","r, data-mining, text-mining, tm","<p>The problem here comes from the underdetermined work flow made possible by the tools you are using.  Simply put, removing stop words means filtering tokens, but the text you are removing the stop words from has not yet been tokenized.</p>
<p>Specifically, the <code>i</code> is removed from <code>i'm</code> because the tokeniser splits on the apostrophe.  In the text analysis package <strong>quanteda</strong>, you are required to tokenise the text first and only then remove features based on token matches.  For instance:</p>
<pre><code>require(quanteda)
removeFeatures(tokenize(&quot;i'd like a soda, please&quot;), c(&quot;i'd&quot;, &quot;a&quot;))
# tokenizedText object from 1 document.
# Component 1 :
# [1] &quot;like&quot;   &quot;soda&quot;   &quot;,&quot;      &quot;please&quot;
</code></pre>
<p><strong>quanteda</strong> also has a built-in list of the most common stopwords, so this works too (and here, we have also removed punctuation):</p>
<pre><code>removeFeatures(tokenize(&quot;i'd like a soda, please&quot;, removePunct = TRUE),
               stopwords(&quot;english&quot;))
# tokenizedText object from 1 document.
# Component 1 :
# [1] &quot;like&quot;   &quot;soda&quot;   &quot;please&quot;
</code></pre>
<p>In my opinion (biased, admittedly, since I designed <strong>quanteda</strong>) this is a better way to remove stopwords in English and most other languages.</p>
<h3>Update Jan 2021, for a more modern version of quanteda</h3>
<pre class=""lang-r prettyprint-override""><code>require(&quot;quanteda&quot;)
## Loading required package: quanteda
## Package version: 2.1.2

tokens(&quot;i'd like a soda, please&quot;) %&gt;%
  tokens_remove(c(&quot;i'd&quot;, &quot;a&quot;))
## Tokens consisting of 1 document.
## text1 :
## [1] &quot;like&quot;   &quot;soda&quot;   &quot;,&quot;      &quot;please&quot;

# or using the stopwords list and removing punctuation
tokens(&quot;i'd like a soda, please&quot;, remove_punct = TRUE) %&gt;%
  tokens_remove(stopwords(&quot;en&quot;))
## Tokens consisting of 1 document.
## text1 :
## [1] &quot;like&quot;   &quot;soda&quot;   &quot;please&quot;
</code></pre>
<p><sup>Created on 2021-02-01 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v1.0.0)</sup></p>
",3,4,1517,2014-11-13 01:32:55,https://stackoverflow.com/questions/26899857/self-conflicting-stopwords-in-r-tm-text-mining
creating a cluster from XML file,"<p>I am working on text analytics. I am using carrot2 api for Java through which I am reading an XML file and trying to generate the cluster. But I am unable to find any simple code snippet for the same. Can someone help me for the same.</p>

<p>Thanks in advance</p>
","java, text-mining, carrot2, lingo","<p><a href=""http://download.carrot2.org/stable/javadoc/overview-summary.html#overview_description"" rel=""nofollow"">Carrot2 JavaDoc overview</a> has a number of code examples. Furthermore, the <a href=""http://project.carrot2.org/download-java-api.html"" rel=""nofollow"">Java API package</a> comes with more self-contained Java code examples.</p>
",0,1,150,2014-11-13 14:14:14,https://stackoverflow.com/questions/26910893/creating-a-cluster-from-xml-file
Sentiment Analysis java Library,"<p>I have some unlabeled microblogging posts and I want to create a sentiment analysis module. </p>

<p>To do this I have try <a href=""http://nlp.stanford.edu/sentiment/"">Stanford library</a> and <a href=""http://www.alchemyapi.com/api/sentiment-analysis/"">Alchemy Api</a> web service but the result it is not very good.  For now I don't want  training my classifier. </p>

<p>So I would like to suggest me some libraries or some web services about that. I would prefer a tested Library.  The language of this posts is English. Also the preprocessing has been done.</p>

<p>P.S.</p>

<p>The programing language that I use is Java EE</p>
","java, machine-learning, data-mining, text-mining, sentiment-analysis","<p>If you want a good sentiment analysis service and you don't want to train your own classifier, you have to pay for it. However, it's worth mentioning that don't exist perfect tools in this field. There aren't tools that guarantee 100% of accuracy in their analysis. </p>

<p>Having said that, a couple of months ago I played around with <a href=""https://semantria.com/support/developer/"" rel=""nofollow"">Semantria/Lexalytics</a>. They have a straightforward Java SDK and a good accuracy on their sentiment analysis results.</p>
",4,7,24330,2014-11-15 18:32:03,https://stackoverflow.com/questions/26949249/sentiment-analysis-java-library
Parse GATE Document to get Co-Reference Text,"<p>I'm creating a GATE app which used to find co-reference text. It works fine and I have created zipped file of the app by export option provided in GATE. </p>

<p>Now I'm trying to use the same in my Java code. </p>

<pre><code>    Gate.runInSandbox(true);
    Gate.setGateHome(new File(gateHome));
    Gate.setPluginsHome(new File(gateHome, ""plugins""));
    Gate.init();
    URL applicationURL = new URL(""file:"" + new Path(gateHome, ""application.xgapp"").toString());

    application = (CorpusController) PersistenceManager.loadObjectFromUrl(applicationURL);
    corpus = Factory.newCorpus(""Megaki Corpus"");
    application.setCorpus(corpus);

    Document document = Factory.newDocument(text);

    corpus.add(document);
    application.execute();
    corpus.clear();
</code></pre>

<p>Now how can I parse this document and get co-reference text?</p>
","java, reference, annotations, text-mining, gate","<p>I do not know about yours, but co-references created manually using the Co-reference Editor are stored in a <strong>document feature</strong>. The feature name seems to be <code>""MatchesAnnots""</code> and the type <code>Map&lt;String, List&lt;List&lt;Integer&gt;&gt;&gt;</code>.</p>

<p>In my case, following code prints <code>as name: null</code> (the default annotation set) followed by all co-reference chains present in it.</p>

<pre class=""lang-java prettyprint-override""><code>Object obj = document.getFeatures().get(""MatchesAnnots"");

@SuppressWarnings(""unchecked"")
Map&lt;String, List&lt;List&lt;Integer&gt;&gt;&gt; map = (Map&lt;String, List&lt;List&lt;Integer&gt;&gt;&gt;) obj;

for (Entry&lt;String, List&lt;List&lt;Integer&gt;&gt;&gt; e : map.entrySet()) {
    System.err.println(""as name: ""+  e.getKey());
    for (List&lt;Integer&gt; chain : e.getValue()) {
        System.err.println(""chain : ""+  chain);         
    }
}
</code></pre>
",3,2,291,2014-11-20 09:11:27,https://stackoverflow.com/questions/27035648/parse-gate-document-to-get-co-reference-text
Meaning of Stanford Spanish POS Tagger tags,"<p>I am tagging Spanish text with the Stanford POS Tagger (via NLTK in Python).</p>

<p>Here is my code:</p>

<pre><code>import nltk
from nltk.tag.stanford import POSTagger
spanish_postagger = POSTagger('models/spanish.tagger', 'stanford-postagger.jar')
spanish_postagger.tag('esta es una oracion de prueba'.split())
</code></pre>

<p>The result is:</p>

<pre><code>[(u'esta', u'pd000000'),
(u'es', u'vsip000'),
(u'una', u'di0000'),
(u'oracion', u'nc0s000'),
(u'de', u'sp000'),
(u'prueba', u'nc0s000')]
</code></pre>

<p>I want to know where can I found what exactly means pd000000, vsip000, di0000, nc0s000, sp000?</p>
","python, stanford-nlp, text-mining","<p>This is a simplified version of the tagset used in the <strong><a href=""http://clic.ub.edu/corpus/en"" rel=""nofollow noreferrer"">AnCora treebank</a></strong>. You can find their tagset documentation here: <a href=""https://web.archive.org/web/20160325024315/http://nlp.lsi.upc.edu/freeling/doc/tagsets/tagset-es.html"" rel=""nofollow noreferrer"">https://web.archive.org/web/20160325024315/http://nlp.lsi.upc.edu/freeling/doc/tagsets/tagset-es.html</a></p>

<p>The ""simplification"" consists of nulling out many of the final fields which don't strictly belong in a part-of-speech tag. For example, our part-of-speech tagger will always give you null (<code>0</code>) values for the NER field of the original tagset (see <a href=""https://web.archive.org/web/20160325024315/http://nlp.lsi.upc.edu/freeling/doc/tagsets/tagset-es.html#nombres"" rel=""nofollow noreferrer"">EAGLES noun documentation</a>).</p>

<p>In short: <strong>the fields in the POS tags produced by our tagger correspond exactly to AnCora POS fields, but a lot of those fields will be null</strong>. For most practical purposes you'll only need to look at the first 2–4 characters of the tag. The first character always indicates the broad POS category, and the second character indicates some kind of subtype.</p>

<hr>

<p>We're in the process of writing some introductory documentation for using Spanish with CoreNLP (that means understanding these tags, and much else) right now. For the moment, you can find more information on the first page of our <a href=""https://docs.google.com/document/d/1lI-ie4-GGx2IA6RJNc0PMb3CHDoNQMUa0gj0eQEDYQ0/edit?usp=sharing"" rel=""nofollow noreferrer"">technical documentation</a>.</p>
",10,6,3795,2014-11-20 19:01:39,https://stackoverflow.com/questions/27047450/meaning-of-stanford-spanish-pos-tagger-tags
Using R to loop through vector and copy some sequences to data.frame,"<p>I want to search through a vector for the sequence of strings ""hello"" ""world"". When I find this sequence, I want to copy it, including the 10 elements before and after, as a row in a data.frame to which I'll apply further analysis.</p>

<p>My problem: I get an error ""new column would leave holes after existing columns"". I'm new to coding, so I'm not sure how to manipulate data.frames. Maybe I need to create rows in the loop?</p>

<p>This is what I have:</p>

<pre><code>df = data.frame()
i &lt;- 1
for(n in 1:length(v))
{

  if(v[n] == 'hello' &amp; v[n+1] == 'world')
  {
    df[i,n-11:n+11] &lt;- v[n-10:n+11]
    i &lt;- i+1
  }
}
</code></pre>

<p>Thanks!</p>
","r, text-mining","<p>May be this helps</p>

<pre><code>indx &lt;- which(v1[-length(v1)]=='hello'&amp; v1[-1]=='world')
lst &lt;- Map(function(x,y) {s1 &lt;- seq(x,y)
                   v1[s1[s1&gt;0 &amp; s1 &lt; length(v1)]]}, indx-10, indx+11)

len &lt;- max(sapply(lst, length))

d1 &lt;- as.data.frame(do.call(rbind,lapply(lst, `length&lt;-`, len)))
</code></pre>

<h3>data</h3>

<pre><code>set.seed(496)
v1 &lt;- sample(c(letters[1:3], 'hello', 'world'), 100, replace=TRUE)
</code></pre>
",0,0,206,2014-11-22 17:52:02,https://stackoverflow.com/questions/27080557/using-r-to-loop-through-vector-and-copy-some-sequences-to-data-frame
Regular Expression in Vim,"<p>I am using regular expression to select a specific pattern between two semicolon "";""</p>

<p>The string that I am testing on is:</p>

<pre><code>; DNA catabolic process ; adf sdfc sdfwefsdf ;
</code></pre>

<p>And I am using the following regex in vim to select the ""DNA"" pattern and the remaining two words before the semicolon "";"" so that the remaining pattern after the semicolon ""i.e adf sdfc ..."" will not be selected.  I am not able to figure out a way to select the remaining pattern after ""DNA"" and before the semicolon.  Do you have any idea to solve this?  </p>

<p>Here is the regex that I came up with:</p>

<pre><code>:%s/\(\;\)\(\s\)\(\DNA\)/\3\4/cg
</code></pre>
","regex, vim, expression, text-mining","<p>You could try the below regex to print the string <code>DNA</code> plus the following two words.</p>

<pre><code>:%s/; \+\(DNA \+[^;]*\) \+;.*/\1/cg
</code></pre>

<p>Output:</p>

<pre><code>DNA catabolic process
</code></pre>

<ul>
<li><code>&lt;space&gt;\+</code> Matches one or more spaces</li>
<li><code>[^;]*</code> Negated character class which matches any character but not of <code>;</code> zero or more times.</li>
</ul>
",4,0,314,2014-11-24 04:44:57,https://stackoverflow.com/questions/27098007/regular-expression-in-vim
How to Text Mining Specific Data,"<p>I have a list of ID with lengthy descriptions separated with semicolons.  The following is an example of one ID with its description.  </p>

<pre><code>  ID      Description 
O95831    activation of cysteine-type endopeptidase activity involved in apoptotic process; apoptotic DNA fragmentation; apoptotic process; cell redox homeostasis; chromosome condensation; DNA catabolic process; intrinsic apoptotic signaling pathway in response to endoplasmic reticulum stress; mitochondrial respiratory chain complex I assembly; NAD(P)H oxidase activity; neuron apoptotic process; neuron differentiation; oxidoreductase activity, acting on NAD(P)H; positive regulation of apoptotic process; regulation of apoptotic DNA fragmentation
</code></pre>

<p><strong>Problem:</strong> Figure out a way to text mining the description in which the expression ""mitochondria"" or ""mitochondrial"" or ""mitochondrion"" is mentioned.  Would regex be useful to solve this problem? or what other ways that might be useful?</p>

<p><strong>Expected Result:</strong> extraction of the description which the the phrase ""mitochondrial"" is mentioned</p>

<pre><code>O95831    ;mitochondrial respiratory chain complex I assembly;
</code></pre>

<p>Your help is appreciated,</p>
","regex, text, text-mining, extract","<p>You can use a regex like</p>

<pre><code>(\d+).*(.\s(?:mitochondria|mitochondrial|mitochondrion)[^;]+;)
</code></pre>

<p>The capture groups 1 and 2 will contain</p>

<pre><code>O95831    ;mitochondrial respiratory chain complex I assembly;
</code></pre>

<p>Example : <a href=""http://regex101.com/r/mR8xA7/1"" rel=""nofollow"">http://regex101.com/r/mR8xA7/1</a></p>

<p>Python code would be like</p>

<pre><code>&gt;&gt;&gt; re.findall(r""""""(\d+).*(.\s(?:mitochondria|mitochondrial|mitochondrion)[^;]+;)"""""", str)
[('095831', '; mitochondrial respiratory chain complex I assembly;')]
</code></pre>
",1,0,556,2014-11-24 16:48:59,https://stackoverflow.com/questions/27109864/how-to-text-mining-specific-data
build word co-occurence edge list in R,"<p>I have a chunk of sentences and I want to build the undirected edge list of word co-occurrence and see the frequency of every edge. I took a look at the <code>tm</code> package but didn't find similar functions. Is there some package/script I can use? Thanks a lot!</p>

<p>Note: A word doesn't co-occur with itself. A word which appears twice or more co-occurs with other words for only once in the same sentence.</p>

<p>DF:</p>

<pre><code>sentence_id text
1           a b c d e
2           a b b e
3           b c d
4           a e
5           a
6           a a a
</code></pre>

<p>OUTPUT</p>

<pre><code>word1 word2 freq
a     b     2
a     c     1
a     d     1
a     e     3
b     c     2
b     d     2
b     e     2
c     d     2
c     e     1
d     e     1
</code></pre>
","r, text-mining, network-analysis","<p>It's convoluted so there's got to be a better approach:</p>

<pre><code>dat &lt;- read.csv(text=""sentence_id, text
1,           a b c d e
2,           a b b e
3,           b c d
4,           a e"", header=TRUE)


library(qdapTools); library(tidyr)
x &lt;- t(mtabulate(with(dat, by(text, sentence_id, bag_o_words))) &gt; 0)
out &lt;- x %*% t(x)
out[upper.tri(out, diag=TRUE)] &lt;- NA

out2 &lt;- matrix2df(out, ""word1"") %&gt;%
    gather(word2, freq, -word1) %&gt;%
    na.omit() 

rownames(out2) &lt;- NULL
out2

##    word1 word2 freq
## 1      b     a    2
## 2      c     a    1
## 3      d     a    1
## 4      e     a    3
## 5      c     b    2
## 6      d     b    2
## 7      e     b    2
## 8      d     c    2
## 9      e     c    1
## 10     e     d    1
</code></pre>

<p><strong>Base only solution</strong></p>

<pre><code>out &lt;- lapply(with(dat, split(text, sentence_id)), function(x) {
    strsplit(gsub(""^\\s+|\\s+$"", """", as.character(x)), ""\\s+"")[[1]]
})

nms &lt;- sort(unique(unlist(out)))

out2 &lt;- lapply(out, function(x) {
    as.data.frame(table(x), stringsAsFactors = FALSE)
})

dat2 &lt;- data.frame(x = nms)

for(i in seq_along(out2)) {
    m &lt;- merge(dat2, out2[[i]], all.x = TRUE)
    names(m)[i + 1] &lt;- dat[[""sentence_id""]][i]
    dat2 &lt;- m
}

dat2[is.na(dat2)] &lt;- 0
x &lt;- as.matrix(dat2[, -1]) &gt; 0

out3 &lt;- x %*% t(x)
out3[upper.tri(out3, diag=TRUE)] &lt;- NA
dimnames(out3) &lt;- list(dat2[[1]], dat2[[1]])

out4 &lt;- na.omit(data.frame( 
        word1 = rep(rownames(out3), ncol(out3)),  
        word2 = rep(colnames(out3), each = nrow(out3)),
        freq = c(unlist(out3)),
        stringsAsFactors = FALSE)
)

row.names(out4) &lt;- NULL

out4
</code></pre>
",2,5,2685,2014-11-26 15:46:29,https://stackoverflow.com/questions/27153320/build-word-co-occurence-edge-list-in-r
Load documents(represented by sentences) from a single text file in R,"<p>I have file with multiple lines, each one representing a document for my scenario. I searched how to create a corpus from it and found about R tm package function <code>readPlain</code> but that will load the whole text file as one document. I also found the way to load documents at <a href=""https://stackoverflow.com/questions/7927367/r-text-file-and-text-mining-how-to-load-data"">R text file and text mining...how to load data</a> but that specified the method which takes a folder path, and for each of the file in it, it creates a document.</p>

<p>How can I form different documents for each of the sentences.</p>
","r, text-mining, tm","<p>Try readLines(""/path/to/yourfile.txt"")
Each line will be a different element in a text vector NLines long where Nlines is the number of lines in your document. 
Otherwise, see scan().
Both have a skip option if you need it, and an nlines option if you want to read it in chunks. </p>
",0,0,727,2014-12-05 14:10:25,https://stackoverflow.com/questions/27317746/load-documentsrepresented-by-sentences-from-a-single-text-file-in-r
loading the data to Corpus from 2 directories in R,"<p>I am using tm package in R to perform Text Mining over a data set whose structure is as follows:</p>

<p>There is a directory group_Data which contains 2 different directories with name B and C. Now directory B contains documents and directory C also contains documents.</p>

<p>I know the way to load the data of directory B and C individually by creating 2 Corpus:</p>

<pre><code>library(tm)
pathToB = ""group_Data/B""
pathToC = ""group_Data/C""

bCorpus = Corpus(DirSource(pathToB), 
                readerControl = list(reader = readPlain))
cCorpus = Corpus(DirSource(pathToC), 
                readerControl = list(reader = readPlain))
length(bCorpus)
length(cCorpus)
</code></pre>

<p>But I need to load the data from B and C into a single Corpus. 
This is what I tried:</p>

<pre><code>pathToBAndC = ""group_Data""
corpusBC = Corpus(DirSource(pathToBAndC), 
                readerControl = list(reader = readPlain))
</code></pre>

<p>On running the length command as mentioned below, it gives 0</p>

<pre><code>length(corpusBC)
</code></pre>

<p>Can someone point out if I am missing out an option inside Corpus method that could do this for me?</p>
","r, text-mining, tm","<p>You can either combine corpuses with</p>

<pre><code>corpusBC &lt;- c(bCorpus, cCorpus)
</code></pre>

<p>Or according to the documentation for <code>?DirSource</code>you can pass in a vector of paths</p>

<pre><code>corpusBC &lt;- Corpus(DirSource(c(pathToB, pathToC)), 
    readerControl = list(reader = readPlain))
</code></pre>
",0,0,791,2014-12-07 05:44:55,https://stackoverflow.com/questions/27340008/loading-the-data-to-corpus-from-2-directories-in-r
What is CoNLL data format?,"<p>I am using a open source jar (Mate Parser) which outputs in the CoNLL 2009 format after dependency parsing. I want to use the dependency parsing results for Information Extraction, however, I only understand part of the output in the CoNLL data format.</p>
<p>Can someone explain the CoNLL data format?</p>
","nlp, text-parsing, text-mining, information-extraction","<p>There are many different <a href=""http://ifarm.nl/signll/conll/"" rel=""noreferrer"">CoNLL</a> formats since CoNLL is a different shared task each year. The format for CoNLL 2009 is described <a href=""http://ufal.mff.cuni.cz/conll2009-st/task-description.html"" rel=""noreferrer"">here</a>. Each line represents a single word with a series of tab-separated fields. <code>_</code>s indicate empty values. <a href=""https://mate-tools.googlecode.com/files/shortmanual.pdf"" rel=""noreferrer"">Mate-Parser's manual</a> says that it uses the first 12 columns of CoNLL 2009:</p>
<pre><code>ID FORM LEMMA PLEMMA POS PPOS FEAT PFEAT HEAD PHEAD DEPREL PDEPREL
</code></pre>
<p>The definition of some of these columns come from earlier shared tasks (the <a href=""https://web.archive.org/web/20160814191537/http://ilk.uvt.nl/conll/#dataformat"" rel=""noreferrer"">CoNLL-X format</a> used in 2006 and 2007):</p>
<ul>
<li><code>ID</code> (index in sentence, starting at 1)</li>
<li><code>FORM</code> (word form itself)</li>
<li><code>LEMMA</code> (word's lemma or stem)</li>
<li><code>POS</code> (part of speech)</li>
<li><code>FEAT</code> (list of morphological features separated by |)</li>
<li><code>HEAD</code> (index of syntactic parent, 0 for <code>ROOT</code>)</li>
<li><code>DEPREL</code> (syntactic relationship between <code>HEAD</code> and this word)</li>
</ul>
<p>There are variants of those columns (e.g., <code>PPOS</code> but not <code>POS</code>) that start with <code>P</code> indicate that the value was automatically predicted rather a gold standard value.</p>
<p><strong>Update:</strong> There is now a <a href=""http://universaldependencies.github.io/docs/format.html"" rel=""noreferrer"">CoNLL-U</a> data format as well which extends the CoNLL-X format.</p>
",72,67,55416,2014-12-11 05:45:51,https://stackoverflow.com/questions/27416164/what-is-conll-data-format
cleaning text files in python 2 : TypeError: coercing to Unicode:,"<p>I am trying to clean up so text files in python. I want to take out stop words, digits and the new line character. But I keep getting <strong>coercing to Unicode python text</strong> . Here is my code:</p>

<pre><code>import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
import string
from string import digits

 def cleanupDoc(s):
     s = s.translate(None,digits)
     s = s.rstrip('\n')  
     stopset = set(stopwords.words('english'))
     tokens = nltk.word_tokenize(s)
     cleanup = "" "".join(filter(lambda word: word not in stopset, s.split()))
     return cleanup

flist=glob.glob('/home/uiucinfo/Desktop/*txt')
mylist=[]
for fname in flist:
    tfile = open(fname, 'r+')
    line = tfile.readlines()
    #line = cleanupDoc(line)
    mylist.append(line)

for fdoc in mylist:
    doc = open(fdoc)
    newDoc = cleanupDoc(doc)
    doc.close()
</code></pre>

<p>My Error  </p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 3, in &lt;module&gt;
TypeError: coercing to Unicode: need string or buffer, list found
</code></pre>
","python, unicode, nltk, text-mining","<pre><code>import nltk
form nltk import word_tokenize
from nltk.corpus import stopwords
#nltk.download() 
import string
from string import digits
import glob
import re


def cleanupDoc(s):    
     #s = s.translate(None,digits)
     #s = s.rstrip('\n')  
     stopset = set(stopwords.words('english'))
     tokens = nltk.word_tokenize(s)
     cleanup = "" "".join(filter(lambda word: word not in stopset, s.split()))
     return cleanup

flist=glob.glob('/home/uiucinfo/Desktop/*txt')
mylist=[]
for fname in flist:
    tfile = open(fname, 'r+')
    line = tfile.readlines()
    #line = cleanupDoc(line)
    mylist.append(line)

for fdoc in mylist:
    # remove \n or digit from fdoc
    fdoc = [re.sub(r'[\""\n]|\d', '', x) for x in fdoc]
    # convert list to string 
    fdoc = ''.join(fdoc)
    print fdoc
    newDoc = cleanupDoc(fdoc)
    print "" newDoc: "" , newDoc
</code></pre>
",0,0,360,2014-12-14 16:09:31,https://stackoverflow.com/questions/27471177/cleaning-text-files-in-python-2-typeerror-coercing-to-unicode
Error in enc2utf8(x) : argumemt is not a character vector,"<p><code>Error in enc2utf8(x) : argumemt is not a character vector</code> is the error I get when I try to run the code below in R 3.1.2 . can anyone please help me understand if I am missing something here ? </p>

<p>OS used is Windows </p>

<pre><code>#Text Cleaning: tm Code
  clean&lt;-function(text){
  library(NLP)
  library(tm)
  sample&lt;- Corpus(VectorSource(text),readerControl=list(language=""english""))
  sample&lt;- tm_map(sample, function(x) iconv(enc2utf8(x), sub = ""bytes""))
  sample&lt;-tm_map(sample,removePunctuation)
  sample &lt;- tm_map(sample, stripWhitespace)
  sample&lt;-tm_map(sample,removeNumbers)
  sample&lt;-tm_map(sample,removeWords,stopwords('smart'))
  sample &lt;- tm_map(sample, stripWhitespace)
  sample &lt;- tm_map(sample, stripWhitespace)
  dtm &lt;- DocumentTermt(sample[1:3])Matrix(sample)
  return(list(sample,dtm))
  }
 fileName &lt;- 'input.txt'
 test = readChar(fileName, file.info(fileName)$size)
 clean (test)
</code></pre>
","r, text-mining","<p>You have to refer to the <code>content</code> of the corpus, i.e., the character vector in <code>sample$content</code>:</p>

<pre><code>tm_map(sample, function(x) iconv(enc2utf8(x$content), sub = ""bytes""))
</code></pre>

<p>Here, I replaced <code>enc2utf8(x)</code> with <code>enc2utf8(x$content)</code>.</p>
",3,3,6910,2014-12-15 05:54:35,https://stackoverflow.com/questions/27478161/error-in-enc2utf8x-argumemt-is-not-a-character-vector
Searching R corpus for all words ending in &quot;esque&quot;,"<p>I am using R's <code>tm</code> package to get word frequencies using the dictionary method. I want to find all words that end with ""esque"" whether they are spelled ""abcd-esque"", ""abcdesque"" or ""abcd esque"" (since all different spellings exist in my corpus). How can I create a regular expression for this? This is what I have so far. Any help/tips would be greatly appreciated.</p>

<pre><code>text &lt;- Corpus(DirSource(""txt/""))
text &lt;- tm_map(text,tolower) 
text &lt;- tm_map(text,stripWhitespace) 
dtm.text &lt;- DocumentTermMatrix(text)
list&lt;-inspect(
    DocumentTermMatrix(text,list(dictionary = c(""rose"", ""green"", ""esque"")))
)
</code></pre>
","regex, r, dictionary, text-mining, tm","<pre><code>inspect(dtm.text[, grepl(""esque$"", dtm.text$dimnames$Terms)])
</code></pre>

<p>As a side note <code>tolower</code> won't work with a current version of <code>tm</code>. You should use <code>contetn_transformer</code> instead:</p>

<pre><code>tm_map(text, content_transformer(tolower))
</code></pre>
",5,3,2306,2014-12-19 03:24:24,https://stackoverflow.com/questions/27559290/searching-r-corpus-for-all-words-ending-in-esque
Unable to process accented words using NLTK tokeniser,"<p>I'm trying to compute the frequencies of words in an utf-8 encoded text file with the following code. Having successfully tokenized the file content and then looping through the words, my program is not able to read the accented characters.</p>

<pre><code>import csv
import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords

print ""computing word frequency...""
if lang == ""fr"":
    stop = stopwords.words(""french"")
    stop = [word.encode(""utf-8"") for word in stop]
    stop.append(""les"")
    stop.append(""a"")
elif lang == ""en"":
    stop = stopwords.words(""english"")


rb = csv.reader(open(path+file_name))
wb = csv.writer(open('results/all_words_'+file_name,'wb'))

tokenizer = RegexpTokenizer(r'\w+')

word_dict = {}

i = 0

for row in rb:
    i += 1
    if i == 5:
        break
    text = tokenizer.tokenize(row[0].lower())
    text = [j for j in text if j not in stop]
    #print text
    for doc in text:
        try:

            try:
                word_dict[doc] += 1

            except:

                word_dict[doc] = 1
        except:
            print row[0]
            print "" "".join(text)




word_dict2 = sorted(word_dict.iteritems(), key=operator.itemgetter(1), reverse=True)

if lang == ""English"":
    for item in word_dict2:
        wb.writerow([item[0],stem(item[0]),item[1]])
else:
    for item in word_dict2:
        wb.writerow([item[0],item[1]])

print ""Finished""
</code></pre>

<p>Input text file:</p>

<pre><code>rt annesorose envie crêpes
envoyé jerrylee bonjour monde dimanche crepes dimanche
The output written in a file is destroying certain words.
bonnes crepes tour nouveau vélo
aime crepe soleil ça fera bien recharger batteries vu jours hard annoncent
</code></pre>

<p>Results output:</p>

<pre><code>crepes,2
dimanche,2
rt,1
nouveau,1
envie,1
v�,1 
jerrylee,1
cleantext,1
lo,1
bonnes,1
tour,1
crêpes,1
monde,1
bonjour,1
annesorose,1
envoy�,1
</code></pre>

<p>envoy� is envoyé in the actual file.</p>

<p>How can I correct this problem with accented characters?</p>
","python, nltk, text-mining","<p>If you're using py2.x, reset default encoding to 'utf8': </p>

<pre><code>import sys
reload(sys)
sys.setdefaultencoding('utf8')
</code></pre>

<p>Alternatively, you can use a <code>ucsv</code> module, see see <a href=""https://stackoverflow.com/questions/1846135/python-csv-library-with-unicode-utf-8-support-that-just-works/9347871#9347871"">General Unicode/UTF-8 support for csv files in Python 2.6</a></p>

<p>or use <code>io.open()</code>:</p>

<pre><code>$ echo """"""rt annesorose envie crêpes
&gt; envoyé jerrylee bonjour monde dimanche crepes dimanche
&gt; The output written in a file is destroying certain words.
&gt; bonnes crepes tour nouveau vélo
&gt; aime crepe soleil ça fera bien recharger batteries vu jours hard annoncent"""""" &gt; someutf8.txt
$ python
&gt;&gt;&gt; import io, csv
&gt;&gt;&gt; text = io.open('someutf8.txt', 'r', encoding='utf8').read().split('\n')
&gt;&gt;&gt; for row in text:
...     print row
... 
rt annesorose envie crêpes
envoyé jerrylee bonjour monde dimanche crepes dimanche
The output written in a file is destroying certain words.
bonnes crepes tour nouveau vélo
aime crepe soleil ça fera bien recharger batteries vu jours hard annoncent
</code></pre>

<p>Lastly, rather than using such a complex reading and counting module, simply use <code>FreqDist</code> in NLTK, see section 3.1 from <a href=""http://www.nltk.org/book/ch01.html"" rel=""nofollow noreferrer"">http://www.nltk.org/book/ch01.html</a></p>

<p>Or personally, i prefer collections.Counter:</p>

<pre><code>$ python
&gt;&gt;&gt; import io
&gt;&gt;&gt; text = io.open('someutf8.txt', 'r', encoding='utf8').read()
&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; Counter(word_tokenize(text))
Counter({u'crepes': 2, u'dimanche': 2, u'fera': 1, u'certain': 1, u'is': 1, u'bonnes': 1, u'v\xe9lo': 1, u'batteries': 1, u'envoy\xe9': 1, u'vu': 1, u'file': 1, u'in': 1, u'The': 1, u'rt': 1, u'jerrylee': 1, u'destroying': 1, u'bien': 1, u'jours': 1, u'.': 1, u'written': 1, u'annesorose': 1, u'annoncent': 1, u'nouveau': 1, u'envie': 1, u'hard': 1, u'cr\xeapes': 1, u'\xe7a': 1, u'monde': 1, u'words': 1, u'bonjour': 1, u'a': 1, u'crepe': 1, u'soleil': 1, u'tour': 1, u'aime': 1, u'output': 1, u'recharger': 1})
&gt;&gt;&gt; myFreqDist = Counter(word_tokenize(text))
&gt;&gt;&gt; for word, freq in myFreqDist.items():
...     print word, freq
... 
fera 1
crepes 2
certain 1
is 1
bonnes 1
vélo 1
batteries 1
envoyé 1
vu 1
file 1
in 1
The 1
rt 1
jerrylee 1
destroying 1
bien 1
jours 1
. 1
written 1
dimanche 2
annesorose 1
annoncent 1
nouveau 1
envie 1
hard 1
crêpes 1
ça 1
monde 1
words 1
bonjour 1
a 1
crepe 1
soleil 1
tour 1
aime 1
output 1
recharger 1
</code></pre>
",2,1,1591,2014-12-26 17:11:42,https://stackoverflow.com/questions/27659861/unable-to-process-accented-words-using-nltk-tokeniser
How many singular values to keep in the R package lsa,"<p>I used the function lsa in the R package lsa to get the semantic space. The input is a term-document matrix. The problem is that the dimcalc_share() function used by lsa by default seems to be wrong. The help page of the function says the function ""finds the first position in the descending sequence of singular values where their sum meets or exceeds the specified share."" I understand the words as the function keeps the nth largest singular values such that the sum of these values exceeds a certain percentage of the sum of all singular values. 
The function's source code is</p>

<pre><code>function(share=0.5)
{
    function(x){
        if(any(which(cumsum(s/sum(s))&lt;=share))){
            d=max(which(cumsum(s/sum(s))&lt;=share))+1
        }
        else{
            d=length(s)
        }
        return(d)
    }
}
</code></pre>

<p>I have two questions with the source code:
1. why plus 1 to the d?
2. if the fraction of the first singular value is larger than share, the function will keep all singular values, while I suppose the function should just keep the first one.</p>
","r, text-mining, lsa","<p><strong>Your first question is ""why the <code>+ 1</code>?""</strong></p>

<p>Let's look at how these functions work:</p>

<pre><code># create some files
td = tempfile()
dir.create(td)
write( c(""dog"", ""cat"", ""mouse""), file=paste(td, ""D1"", sep=""/"") )
write( c(""ham"", ""mouse"", ""sushi""), file=paste(td, ""D2"", sep=""/"") )
write( c(""dog"", ""pet"", ""pet""), file=paste(td, ""D3"", sep=""/"") )

# LSA
data(stopwords_en)
myMatrix = textmatrix(td, stopwords=stopwords_en)
myMatrix = lw_logtf(myMatrix) * gw_idf(myMatrix)
myLSAspace = lsa(myMatrix, dims=dimcalc_share())
as.textmatrix(myLSAspace)

             D1         D2         D3
cat   0.3616693  0.6075489  0.3848429
dog   0.4577219  0.2722711  1.2710784
mouse 0.5942734  1.3128719  0.1357196
ham   0.6075489  1.5336529 -0.1634938
sushi 0.6075489  1.5336529 -0.1634938
pet   0.6099616 -0.2591316  2.6757285
</code></pre>

<p>So, <code>lsa</code> gets dimensions from <code>dimcalc_share()</code> based on the input matrix and a given share (.5 by default) and runs a Singular Value Decomposition to map the original TDM to a new <code>LSAspace</code>.</p>

<p>Those dimensions are the number of singular values for the dimensionality reduction in LSA. 
<code>dimcalc_share()</code> finds the first position in the descending sequence of singular values s where their sum (divided by the sum of all values) meets or exceeds the specified share.</p>

<p>The function is written such that it <code>d</code> is equal to the <code>max()</code> position <code>&lt;= share</code>:</p>

<pre><code>&gt; # Break it apart
&gt; s &lt;- myMatrix
&gt; share &lt;- .5
&gt; 
&gt; any(which(cumsum(s/sum(s)) &lt;= share)) #TRUE
[1] TRUE
&gt; cumsum(s/sum(s)) &lt;= share
 [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
&gt; d = max(which(cumsum(s/sum(s)) &lt;= share)) + 1
&gt; d
[1] 10
</code></pre>

<p>If you only used <code>d -1</code>, which would give you 9 instead of 10, then you'd instead have a position where the <code>cumsum</code> is still <code>&lt;=</code> to <code>share</code>. That wouldn't work:</p>

<pre><code>&gt; myMatrix = lw_logtf(myMatrix) * gw_idf(myMatrix)
&gt; myLSAspace2 = lsa(myMatrix, dims=d-1)
Error in SVD$u[, 1:dims] : subscript out of bounds
</code></pre>

<p>Equivalently</p>

<pre><code>&gt; dims = 9
&gt; myLSAspace = lsa(myMatrix, dims)
Error in SVD$u[, 1:dims] : subscript out of bounds
</code></pre>

<p>So the function <code>dimshare_calc()</code> is correct in using <code>+ 1</code>.</p>

<p><strong>Your 2nd question, modified for this example, is ""would dimcalc_share() = 18 instead of = 1 if the first value was > share?""</strong></p>

<p>If the first value were <code>&gt; share</code> then the first <code>if</code> condition would return false and, as you hypothesized, would instead use <code>length(s)</code> which is 18. </p>

<p>You might follow up with a question on CrossValidated to confirm that your intuition that it should <code>= 1</code> is correct (though that makes sense to me). If so, it would be simple to re-write the function with <code>d = 1</code> as the <code>else</code>.</p>
",1,1,1674,2015-01-02 14:25:36,https://stackoverflow.com/questions/27743674/how-many-singular-values-to-keep-in-the-r-package-lsa
Compute similarity between elements from a csv file in R,"<p>Firstly, I am new to R</p>

<p>I am having a dataframe as :</p>

<p>df&lt;- </p>

<pre><code>column-1  column-2 column-3 column-4

vf34       bn56     qw34    mn569
vf34       cv34             mn569
           bn56     qw34    asder45
nght       cv34             asder45
vf34       cv34             mn569
</code></pre>

<p>Now i want to compute the similarity matrix as :</p>

<pre><code>Output1:
          vf34  nght  bn56  cv34  qw34   mn569  asder45     
vf34      0     0     1     2     1      3      0
nght      0     0     0     1     0      0      1
bn56      1     0     0     0     2      1      1
cv34      2     1     0     0     0      2      1
qw34      1     0     2     0     0      1      1
mn569     3     0     1     2     1      0      0
asder45   0     1     1     1     1      0      0 
</code></pre>

<p>So, basically it should find all the possible pairs from the dataframe (or csv file) and form a matrix with the number of occurrences.</p>

<p>For ex: first row, sixth column is 3. So that says in the entire data vf34 and mn569 combination has 
occurred 3 times.</p>

<p>Blank values in the data means the data is missing in the orginal data itself.</p>

<p>I am able to do this in python using countvectorizer and then multiplying the obtained matrix with its transpose. However i am new to R. Could any one help me out with this? </p>

<pre><code> and Output2 that i need is:

1  1 3 2 1 0
 and so on for 5 rows.

 This 1; 1; 3; 2; 1; 0 means: 
 (vf34 and bn56); (vf34 and qw34); (vf34 and mn569); (bn56 and qw34); (bn56 and mn569); 
 (qw34 and mn569) combinations that have occurred.
 These values can be obtained from output1 that is given above.
</code></pre>

<p>I need these values for all the five rows. How to do this?</p>
","r, csv, text-mining","<p>Here is a way to get the expected result.  The work flow is:</p>

<ol>
<li>Get the unique elements from the ""dataset"" (<code>unique(unlist(df))</code>)</li>
<li>Remove the empty strings (<code>''</code>)</li>
<li>Create a pairwise combination (""indx"") of columns (<code>combn(1:..)</code>)</li>
<li><code>split</code> the ""indx"" by the columns of ""indx""</li>
<li>Subset the ""df"" (<code>df[x]</code>)</li>
<li>Remove the empty strings</li>
<li>Change the ""character""  columns to ""factor"" class with levels as ""Un1""</li>
<li>Get the frequency using <code>table</code> and sum (<code>+</code>) the list elements.</li>
<li><p>The result (<code>res</code>) and transpose of the result is summed again to get the lower and upper diagonal elements same.</p>

<pre><code>Un &lt;- unique(unlist(df))
Un1 &lt;- Un[Un!='']
indx &lt;- combn(1:ncol(df),2)
res &lt;- Reduce(`+`,lapply(split(indx, col(indx)), function(x) {
            x1 &lt;- df[x]
            x2 &lt;- x1[!(x1[,1]==''|x1[,2]==''),]
            x2[] &lt;- lapply(x2, factor, levels=Un1)
            tbl &lt;- table(x2)}))

 res1 &lt;- res+t(res)
res1
#           column.2
#column.1  vf34 nght bn56 cv34 qw34 mn569 asder45
# vf34       0    0    1    2    1     3       0
# nght       0    0    0    1    0     0       1
# bn56       1    0    0    0    2     1       1
# cv34       2    1    0    0    0     2       1
# qw34       1    0    2    0    0     1       1
# mn569      3    0    1    2    1     0       0
# asder45    0    1    1    1    1     0       0
</code></pre></li>
</ol>

<h3>Update</h3>

<p>Regarding the ""output2"", it is not very clear as the values are not matching with your expected results (possible typo?)</p>

<pre><code>lapply(seq_len(nrow(df)), function(i) {x1 &lt;- unlist(df[i,])
                        x2 &lt;- x1[x1!='']
                        i1 &lt;- combn(x2,2)
                   diag(res1[i1[1,], i1[2,]])})
#[[1]]
#[1] 1 1 3 2 1 1

#[[2]]
#[1] 2 3 2

#[[3]]
#[1] 2 1 1

#[[4]]
#[1] 1 1 1

#[[5]]
#[1] 2 3 2
</code></pre>

<h3>data</h3>

<pre><code>df &lt;- structure(list(column.1 = c(""vf34"", ""vf34"", """", ""nght"", ""vf34""
), column.2 = c(""bn56"", ""cv34"", ""bn56"", ""cv34"", ""cv34""), column.3 = c(""qw34"", 
"""", ""qw34"", """", """"), column.4 = c(""mn569"", ""mn569"", ""asder45"", 
""asder45"", ""mn569"")), .Names = c(""column.1"", ""column.2"", ""column.3"", 
""column.4""), class = ""data.frame"", row.names = c(NA, -5L))
</code></pre>
",3,0,314,2015-01-12 17:21:46,https://stackoverflow.com/questions/27907429/compute-similarity-between-elements-from-a-csv-file-in-r
R text mining - intersection between text fields,"<p>I was wondering if there's a quick way to find directed intersection between 2 text strings e.g. </p>

<pre><code> t1 &lt;- ""I have achieved my goals over the past 20 years and look forward for my next chalanges""
 t2 &lt;- "" have achieved goals and look my chalanges some other words bla bla""
</code></pre>

<p>t1 isContainedIn t2 would return 7 because 7 words that apeared in t1 also apeared in t2 .
Also, t1 and t2 are 2 columns in a data frame, so I would need to apply that function on the entire data frame and attached the result column to my original data frame.
This is how my data frame 'data.selected' looks like:</p>

<pre><code>        keywords                                         title
1  Samsung UN48H6350 48"" Samsung UN48H6350 48"" Full 1080p Smart HDTV 120Hz with Wi-Fi +$50 Visa Gift Card
2  Samsung UN48H6350 48""     Samsung UN48H6350 48"" Full HD Smart LED TV -Bundle- (See Below for Contents)
3  Samsung UN48H6350 48""      Samsung UN48H6350 48"" Class Full HD Smart LED TV -BUNDLE- See below Details
4  Samsung UN48H6350 48""     Samsung UN48H6350 48"" Full HD Smart LED TV With BD-H5100 Blu-ray Disc Player
5  Samsung UN48H6350 48""                 Samsung UN48H6350 48"" Smart 1080p Clear Motion Rate 240 LED HDTV
6  Samsung UN48H6350 48""            Samsung UN48H6350 - 48-Inch Full HD 1080p Smart HDTV 120Hz with Wi-Fi
7  Samsung UN48H6350 48""               Samsung 6350 Series UN48H6350 48"" 1080p HD LED LCD Internet TV NEW
8  Samsung UN48H6350 48""  Samsung Un48h6350af 75"" 1080p Led-lcd Tv - 16:9 - Hdtv 1080p - (un75h6350afxza)
9  Samsung UN48H6350 48""                         Samsung UN48H6350 - 48"" HD 1080p Smart HDTV 120Hz Bundle
10 Samsung UN48H6350 48""   Samsung UN48H6350 - 48-Inch Full HD 1080p Smart HDTV 120Hz with Wi-Fi, (R#416)
</code></pre>
","r, nlp, intersection, text-mining","<p>I guess another similar way would by just to use a simple <code>match</code></p>

<pre><code>string &lt;- strsplit(c(t1, t2), ""\\s+"") # similar to @Richard
length(na.omit(match(string[[2]], string[[1]])))
## [1] 7
</code></pre>

<p>Or maybe <code>lapply</code></p>

<pre><code>length(unlist(lapply(string[[2]], intersect, string[[1]])))
## [1] 7
</code></pre>
",4,3,572,2015-01-13 19:33:46,https://stackoverflow.com/questions/27930097/r-text-mining-intersection-between-text-fields
Detecting users emotions by analyzing the text,"<p>im doing a project which will detect users emotions through text? Im new to this area im still finding the best algorithm to detect the emotions from text.suggest me a good method to do this?</p>
","data-mining, text-mining","<p>You can use in a simple way:</p>

<ol>
<li>Build a list of lets say 300 quotes with respective emotion.</li>
</ol>

<blockquote>
  <p>I'm done of this kind of game   => Bored<br> 
  Size doesn't mean everything    => Defense<br>
  I will bring you all pain       => Angry<br>
  Fortune doesn't favor fools     => advise<br></p>
</blockquote>

<ol start=""2"">
<li><p>Now make a filter to select only important words from each phrase, removing words that will not impact in the meaning</p></li>
<li><p>Create a range from 0 to 100 and lets create for example 5 emotions that will fit 0-20 21-40 41-60 61-80 81-100. Each emotion will be filled in a range. For example a good range would be:
<code>coolness -&gt; happiness -&gt; normal -&gt; bored -&gt; sad</code></p></li>
<li><p>Now for each important word from step 2 you will have to assign a number in the range of 0-100 that most describe the emotion (For example: very happy is 1 little happy is 20 little sad is 81 very sad is 100)</p></li>
<li><p>now you will have quotes like numbers and you can put emotions in array of numbers</p></li>
<li><p>Now you can use a Neural Network (you will train the NN with your arrays and for a new entry it will try to 'say' what is the emotion that is seems to be.</p></li>
</ol>

<p>Lets suppose you find a tool for training and testing NN as below.</p>

<p><img src=""https://i.sstatic.net/ogSym.png"" alt=""nn image""></p>

<p>Each of the inputs will be the numbers of your ""emotion array"" and the output will be a binary number. if you want represent a 0-4 (cool to sad) range, so you will need 3 outputs (3 bits = 8 possible numbers represented since 2 bit we can only 4 numbers)</p>

<p>the training file for your neural network will look like below:</p>

<blockquote>
  <p>i1 i2 i3 i4 output<br>
  21 27 01 07 1<br>
  01 07 02 91 5<br>
  15 27 31 40 3<br>
  16 01 07 55 4<br></p>
</blockquote>

<p>By words it means: for this set of inputs representing these emotions i will have to show 1 (happy) and the same thinking to the other lines</p>

<p>THis file will be lots of lines of traning (300 i said above) and the result of the training will be the WEIGHTS of the neural networks be calibrated to your ""artificial inteligence""</p>

<p>Of course the output depends of the quality of your data and your filter.</p>

<p>Also you will have to study how many inputs your Neural network will have, how many layers and so on.</p>
",1,-2,218,2015-01-16 14:55:16,https://stackoverflow.com/questions/27986677/detecting-users-emotions-by-analyzing-the-text
"How to convert a sparse or simple_triplet_matrix into a tm-package Document Term Matrix without going through Corpus/VCorpus, in R?","<p>I have a sparseMatrix (library Matrix) or a simple_triplet_matrix (library slam) of docs x terms, such as:</p>

<pre><code>library(Matrix)
mat &lt;- sparseMatrix(i = c(1,2,4,5,3), j = c(2,3,4,1,5), x = c(3,2,3,4,1))
rownames(mat) &lt;- paste0(""doc"", 1:5)
colnames(mat) &lt;- paste0(""word"", 1:5)

5 x 5 sparse Matrix of class ""dgCMatrix""
     word1 word2 word3 word4 word5
doc1     .     3     .     .     .
doc2     .     .     2     .     .
doc3     .     .     .     .     1
doc4     .     .     .     3     .
doc5     4     .     .     .     .
</code></pre>

<p>or:</p>

<pre><code>library(slam)
mat2 &lt;- simple_triplet_matrix(c(1,2,4,5,3), j = c(2,3,4,1,5), v = c(3,2,3,4,1),
                          dimnames = list(paste0(""doc"", 1:5), paste0(""word"", 1:5)))
</code></pre>

<p>And I wish to turn either of these matrices into a tm::Document-Term-Matrix, without going through a Corpus/VCorpus creation.</p>

<p>This works only for small matrices:
<a href=""https://stackoverflow.com/questions/24418893/in-r-tm-package-build-corpus-from-document-term-matrix"">In R tm package, build corpus FROM Document-Term-Matrix</a></p>

<p>My matrix is quite big, ~16K x ~53K, so the list suggested there is too large for a reasonable RAM, and besides I don't see why I should go through Corpus creation where the tm package manual explicitly says a Document Term Matrix is a sparse matrix.</p>

<p>Any suggestions on how to convert a already sparse matrix into tm's Document Term Matrix?</p>

<p>Thank you.</p>
","r, sparse-matrix, text-mining, tm","<p>The documentation is admittedly a little tricky here. You can use the coercing function <code>as.DocumentTermMatrix</code> but not the direct constructor <code>DocumentTermMatrix</code> on a <code>simple_triplet_matrix</code>.</p>

<pre><code>library(slam)
library(Matrix)
mat2 = simple_triplet_matrix(c(1,2,4,5,3), j = c(2,3,4,1,5), v = c(3,2,3,4,1),
                              dimnames = list(paste0(""doc"", 1:5), paste0(""word"", 1:5)))
mat2 = as.DocumentTermMatrix(mat2, weighting = weightTfIdf)
</code></pre>

<p>You can check:</p>

<pre><code>&gt; class(mat2)
[1] ""DocumentTermMatrix""    ""simple_triplet_matrix""
</code></pre>
",7,3,1580,2015-01-18 11:54:39,https://stackoverflow.com/questions/28009371/how-to-convert-a-sparse-or-simple-triplet-matrix-into-a-tm-package-document-term
How to efficiently check neighbor elements for characteristic,"<p>I have a text here for example Lorem ipsum lets say I am looking for car with a diesel engine. Realtext is about 11000 words. I am using Python3 and looked into nltk, but didnt find the right idea.</p>

<p>Exampletext:</p>

<p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam     nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor <strong>my old car has a nice diesel engine</strong> sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.</p>

<p>Question:</p>

<p>How would i do this efficiently?
Can you tell me some text-mining algorithms for further research, for example if I want to search for more than one keyword.</p>

<p><strong>Update1 Begin</strong></p>

<p>I want to find the distance ( other words ) between two words in a text. 
In my example the distance is 4 (3 words between car and diesel)</p>

<p><strong>Update1 End</strong></p>

<p>My idea so far is to iterate over the list of words and check if the word is a car then I check if 5 words before and 5 after the current word are the same as diesel. In my real code i make some fuzzymatching so you can ignore special cases like ""car.""</p>

<pre><code>near = 5
textLines = text.splitlines()
words = []
for line in textLines:
    wordlist = line.split(' ')
    for word in wordlist:    
            words.append(word)

for idx, val in enumerate(words):
    if word is 'car': 
        print (idx, val)
        print (""near words"")
        for x in range(1,near+1):
            print(words[idx-x])
            # check for diesel
        print(""after"")
        for x in range(1,near+1):
            print(words[idx+x])
            # check for diesel    
</code></pre>
","python, nltk, text-mining","<pre><code>from collections import defaultdict
from nltk import word_tokenize
import math

text = """"""Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor my old car has a nice diesel engine sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.""""""

word_index = defaultdict(list)

for i,word in enumerate(word_tokenize(text)):
    word_index[word].append(i)

# First occurence of 'car'
car = word_index['car'][0]
# Indices of 'diesel'
diesel = word_index['diesel']

nearest_diesel = min(diesel, key=lambda x:abs(x-car))

distance = math.fabs(car - nearest_diesel)

print distance
</code></pre>
",0,0,130,2015-01-18 23:04:25,https://stackoverflow.com/questions/28015829/how-to-efficiently-check-neighbor-elements-for-characteristic
Extracting textual content from XML documents using XSLT,"<p>How it is possible to extract textual content of an XML document preferably using XSLT. </p>

<p>For such fragment,</p>

<pre><code>&lt;record&gt;
    &lt;tag1&gt;textual content&lt;/tag1&gt;
    &lt;tag2&gt;textual content&lt;/tag2&gt;
    &lt;tag2&gt;textual content&lt;/tag2&gt;
&lt;/record&gt;
</code></pre>

<p>the desired result is : </p>

<p>textual content, textual content, textual content</p>

<p>What's the best format for output (table, CSV, etc,) in which the content be processable for further operation, such as text mining?</p>

<p>Thanks</p>

<p><strong>Update</strong></p>

<p>To extend the question, how it’s possible to extract content of each record separately. For example, for the below XML:</p>

<pre><code>&lt;Records&gt;
&lt;record id=""1""&gt;
    &lt;tag1&gt;textual co&lt;/tag1&gt;
    &lt;tag2&gt;textual con&lt;/tag2&gt;
    &lt;tag2&gt;textual cont&lt;/tag2&gt;
&lt;/record&gt;
&lt;record id=""2""&gt;
    &lt;tag1&gt;some text&lt;/tag1&gt;
    &lt;tag2&gt;some tex&lt;/tag2&gt;
    &lt;tag2&gt;some te&lt;/tag2&gt;
&lt;/record&gt;
&lt;/Records&gt;
</code></pre>

<p>The desired result should be such as:</p>

<pre><code>(textual co, textual con, textual cont) , (some text, some tex, some te)
</code></pre>

<p>or in better format for further processing operations.</p>
","xml, xslt, text-mining","<p>You can use the following XSLT:</p>

<pre><code>&lt;xsl:transform version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform""&gt;
&lt;xsl:output method=""text"" indent=""yes""/&gt;
&lt;xsl:strip-space elements=""*""/&gt;
&lt;xsl:template match=""/""&gt;
    &lt;xsl:apply-templates select=""//text()""/&gt;
&lt;/xsl:template&gt;
&lt;xsl:template match=""text()""&gt;
    &lt;xsl:value-of select="".""/&gt;
    &lt;xsl:if test=""position() != last()""&gt;, &lt;/xsl:if&gt;
&lt;/xsl:template&gt;
&lt;/xsl:transform&gt;
</code></pre>

<p>And for the update in the question, you can use the following XSLT:</p>

<pre><code>&lt;xsl:transform version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform""&gt;
&lt;xsl:output method=""text"" indent=""yes""/&gt;
&lt;xsl:strip-space elements=""*""/&gt;
&lt;xsl:template match=""/*""&gt;
    &lt;xsl:apply-templates/&gt;
&lt;/xsl:template&gt;
&lt;xsl:template match=""*""&gt;(&lt;xsl:apply-templates select="".//text()""/&gt;)&lt;xsl:if test=""position() != last()""&gt;, &lt;/xsl:if&gt;
&lt;/xsl:template&gt;
&lt;xsl:template match=""text()""&gt;
    &lt;xsl:value-of select="".""/&gt;
    &lt;xsl:if test=""position() != last()""&gt;, &lt;/xsl:if&gt;
&lt;/xsl:template&gt;
&lt;/xsl:transform&gt;
</code></pre>
",1,-2,287,2015-01-19 20:18:15,https://stackoverflow.com/questions/28032853/extracting-textual-content-from-xml-documents-using-xslt
"Given a dictionary of word and frequency pairs, how to proceed with text mining in scikit","<p>I already have word frequencies and categories like this:</p>

<pre><code>y = ['animals', 'restaurants', 'sports']
x = [{'cat':1, 'dog':2}, {'food':4, 'drink':2}, {'baseball':4, 'basketball':5}]
</code></pre>

<p>How should I proceed with building the pipeline per the tutorial as follows:</p>

<pre><code>&gt;&gt;&gt; from sklearn.pipeline import Pipeline
&gt;&gt;&gt; text_clf = Pipeline([('vect', CountVectorizer()),
...                      ('tfidf', TfidfTransformer()),
...                      ('clf', MultinomialNB()),
... ])

&gt;&gt;&gt; text_clf = text_clf.fit(twenty_train.data, twenty_train.target)
</code></pre>

<p>CountVectorizer is expecting a string... I guess I could create a string from the dictionary and repeat each word the number of times it occurs? </p>

<p><a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></p>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html</a></p>
","python, scikit-learn, text-mining","<p>If you already have word frequencies then use a <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html"" rel=""nofollow"">DictVectorizer</a>:</p>

<pre><code>from sklearn.feature_extraction import DictVectorizer

pipeline = Pipeline([('dvect', DictVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', MultinomialNB())])
model = pipeline.fit(x, y)
</code></pre>

<p>Then you can do:</p>

<pre><code>&gt;&gt;&gt; model.predict([{'cat':1}])[0]
'animals'
</code></pre>
",1,1,594,2015-01-28 15:10:53,https://stackoverflow.com/questions/28195652/given-a-dictionary-of-word-and-frequency-pairs-how-to-proceed-with-text-mining
My DocumentTermMatrix reduces to Zero columns,"<pre><code>train &lt;- read.delim('train.tsv', header= T, fileEncoding= ""windows-1252"",stringsAsFactors=F)
</code></pre>

<p>Train.tsv contains 1,56,060 lines of text with 4 column names Phrase, PhraseID, SentenceID and Sentiment(on scale of 0 to 4).Phrase column has the text lines. (Tm package already loaded)
<strong>R Version: 3.1.2 ; OS: Windows 7, 64 bit, 4 GB RAM.</strong>  </p>

<pre><code>&gt; dput(head(train,6)) 
structure(list(PhraseId = 1:6, SentenceId = c(1L, 1L, 1L, 1L, 
1L, 1L), Phrase = c(""A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story ."", 
""A series of escapades demonstrating the adage that what is good for the goose"", 
""A series"", ""A"", ""series"", ""of escapades demonstrating the adage that what is good for the goose""
), Sentiment = c(1L, 2L, 2L, 2L, 2L, 2L)), .Names = c(""PhraseId"", 
""SentenceId"", ""Phrase"", ""Sentiment""), row.names = c(NA, 6L), class = ""data.frame"")
</code></pre>

<p><strong>This is the top 6 rows of train document.</strong></p>

<pre><code>clean_corpus &lt;- function(corpus)
  {
   mycorpus &lt;- tm_map(corpus, removeWords,stopwords(""english""))  
   mycorpus &lt;- tm_map(mycorpus, removeWords,c(""movie"",""actor"",""actress""))  
   mycorpus &lt;- tm_map(mycorpus, stripWhitespace)  
   mycorpus &lt;- tm_map(mycorpus, tolower)  
   mycorpus &lt;- tm_map(mycorpus, removeNumbers)
   mycorpus &lt;- tm_map(mycorpus, removePunctuation)
   mycorpus &lt;- tm_map(mycorpus, PlainTextDocument ) 
   return(mycorpus) 
}

# Build DTM
generateDTM &lt;- function(df)
{
   m &lt;- list(Sentiment = ""Sentiment"", Phrase = ""Phrase"")
   myReader &lt;- readTabular(mapping = m)
   mycorpus &lt;- Corpus(DataframeSource(df), readerControl = list(reader = myReader))

#Code to attach sentiment label with every text line
    for (i in 1:length(mycorpus)) 
     {
     attr(mycorpus[[i]], ""Sentiment"") &lt;- df$Sentiment[i]
   }
   mycorpus &lt;- clean_corpus(mycorpus)
   dtm &lt;- DocumentTermMatrix(mycorpus)
   return(dtm)
}

dtm1 &lt;- generateDTM(train) 
</code></pre>

<p>Here I have made two functions. One to clean the corpus and other to make DTM (Document Term Matrix). I have also linked each sentiment value with every line of text. Now when i use dimensions of dtm1; it shows 156060 rows but 0 columns. </p>

<p>So, how can i generate a DTM with sentiment labels attached? </p>
","r, text-mining, tm, term-document-matrix","<p>When you set up your reader, you want to map something to the ""content"" of the document, otherwise it doesn't know what text to use to make the corpus. Othe rvalues are stored as metadata. Try changing the code to</p>

<pre><code>m &lt;- list(Sentiment = ""Sentiment"", content = ""Phrase"")
myReader &lt;- readTabular(mapping = m)
mycorpus &lt;- Corpus(DataframeSource(df), readerControl = list(reader = myReader))
</code></pre>
",1,0,167,2015-01-31 05:35:35,https://stackoverflow.com/questions/28248549/my-documenttermmatrix-reduces-to-zero-columns
big document term matrix - error when counting the number of characters of documents,"<p>I have built a big document-term matrix with the package <code>RTextTools</code>.</p>

<p>Now I am trying to count the number of characters in the matrix rows so that I can remove empty documents before performing topic modeling.</p>

<p>My code gives no errors when I apply it to a sample of my corpus, obtaining a smaller matrix, but when I try to count the row length of the documents in the matrix produced from my entire corpus (~75000 tweets) I get the following error message:</p>

<pre><code>Error in vector(typeof(x$v), nr * nc) : 
  the dimension of the vector no cannot be NA
And: Warning message:
In nr * nc : NA produced by integer overflow
</code></pre>

<p>This is my code:</p>

<pre><code>matrix &lt;- create_matrix(data$clean_text, language=""french"", stemWords=TRUE, removeStopwords=TRUE, removeNumbers=TRUE, stripWhitespace=TRUE, toLower=TRUE, removePunctuation=TRUE, minWordLength=3)

rowTotals &lt;- apply(matrix, 1, sum)
</code></pre>

<p>If I try with a matrix of 25000 documents I get the following error:</p>

<pre><code>message: rowTotals &lt;- apply(matrix, 1, sum) 
Errore: cannot allocate vector of size 7.1 Gb
</code></pre>
","r, matrix, text-mining, tm","<p>You might be able to work around this if you keep your data in the dtm, which uses a sparse matrix representation that is much more memory efficient than a regular matrix. </p>

<p>The reason why the <code>apply</code> function gives an error is because it converts the sparse matrix into a regular matrix (the <code>matrix</code> object in your Q - btw it's poor style to give data objects names that are also names of functions, especially base functions). This means that R has to allocate memory for all the zeros in the dtm (which are typically mostly zeros, so that's a lot of memory with zeros in it). With a sparse matrix R doesn't need to store any of the zeros. </p>

<p>Here's the first few lines of the source for <code>apply</code>, see the last line here for the conversion to regular matrix:</p>

<pre><code>apply
function (X, MARGIN, FUN, ...) 
{
    FUN &lt;- match.fun(FUN)
    dl &lt;- length(dim(X))
    if (!dl) 
        stop(""dim(X) must have a positive length"")
    if (is.object(X)) 
        X &lt;- if (dl == 2L) 
            as.matrix(X) # this is where your memory gets filled with zeros
</code></pre>

<p>So how to avoid that conversion? Here's one way to loop over the rows to get their sums while keeping the sparse matrix format: </p>

<pre><code>sapply(seq(nrow(matrix)), function(i) sum(matrix[i,]))
[1] 2 1 2 2 1
</code></pre>

<p>Subsetting this way preserves the sparse format and does not convert the object to the more memory expensive common matrix representation. We can check the representation:</p>

<pre><code>str(matrix[1,])
List of 6
 $ i       : int [1:2] 1 1
 $ j       : int [1:2] 1 3
 $ v       : num [1:2] 1 1
 $ nrow    : int 1
 $ ncol    : int 6
 $ dimnames:List of 2
  ..$ Docs : chr ""1""
  ..$ Terms: chr [1:6] ""document"" ""file"" ""first"" ""second"" ...
 - attr(*, ""class"")= chr [1:2] ""DocumentTermMatrix"" ""simple_triplet_matrix""
 - attr(*, ""weighting"")= chr [1:2] ""term frequency"" ""tf""
</code></pre>

<p>So in the <code>sapply</code> function we are always working on a sparse matrix. And even if <code>sum</code> (or whatever function you use there) does some kind of conversion, it's only going to be converting one row of the dtm, rather than the entire thing. </p>

<p>The general principle when working with largish text data in R is to keep your dtm as a sparse matrix and then you should be able to keep within memory limits. </p>
",1,1,1740,2015-01-31 11:16:53,https://stackoverflow.com/questions/28250961/big-document-term-matrix-error-when-counting-the-number-of-characters-of-docum
Stats properties among documents R,"<p>I have a table, such as:</p>

<pre><code>test_data &lt;- data.frame(
  doc = c(1,1,2,2,2,3,3),
  word = c(""person"", ""grand"", ""person"", ""moment"", ""bout"", ""person"", ""moment""),
   frenq= c(9,8,5,4,3,5,3))
</code></pre>

<p>I would like to calculate mean and std for each ""word"" and create a new table such as.</p>

<pre><code>    word   freq (number of docs)  mean    std 
 personn     19                3  6.33  2.309
  moment      7                2  2.33  2.081
</code></pre>

<p>And the main problem is the sdt, for example, for the word ""person"" is sd(c(9,5,5)) but the for word ""moment"" is sd(c(0,4,3)). Zero is the first number because this word is not in the doc 1.</p>
","r, statistics, std, text-mining","<p>You can try <code>dplyr</code>.  Create a new dataset (""d1"") by the unique combinations of ""doc"", and ""word"" columns of ""test_data"" (<code>expand.grid(..)</code>).  Join ""d1"" with ""test_data"" (<code>left_join</code>), replace the <code>NA</code> values in ""frenq"" by ""0"" (<code>replace(frenq,..)</code>), get the summary statistics using <code>mutate_each</code> after grouping by ""word"".</p>

<pre><code>library(dplyr)
d1 &lt;- expand.grid(doc=unique(test_data$doc), word=unique(test_data$word))
res &lt;- left_join(d1, test_data) %&gt;%
                   mutate(frenq=replace(frenq, is.na(frenq), 0)) %&gt;%
                   group_by(word) %&gt;% 
                   summarise_each(funs(freq=sum,NumberOfdocs= sum(.!=0),
                         mean, std=sd), frenq)
  res
  #    word freq Numberofdocs     mean      std
  #1   bout    3            1 1.000000 1.732051
  #2  grand    8            1 2.666667 4.618802
  #3 moment    7            2 2.333333 2.081666
  #4 person   19            3 6.333333 2.309401
</code></pre>

<p>Or using a similar approach using methods in <code>data.table</code>.  Convert ""data.frame"" to ""data.table"" (<code>setDT</code>), set ""doc"", ""word"" as the key columns (<code>setkey</code>), crossjoin unique elements of ""doc"" and ""word"" (<code>CJ(doc=...,)</code>), assign '0' for <code>NA</code> elements in ""frenq"" (<code>is.na(frenq), frenq:=0</code>), and get the summary statistics (<code>list(freq=..)</code>) grouped by ""word"".  </p>

<pre><code>  library(data.table)
  setkey(setDT(test_data), doc, word)[CJ(doc=unique(doc), 
        word=unique(word))][is.na(frenq), frenq:=0][,
           list(freq=sum(frenq), Numberofdocs=sum(frenq!=0), 
                  mean=mean(frenq), std=sd(frenq)) , by = word]
   #    word freq Numberofdocs     mean      std
   #1:   bout    3            1 1.000000 1.732051
   #2:  grand    8            1 2.666667 4.618802
   #3: moment    7            2 2.333333 2.081666
   #4: person   19            3 6.333333 2.309401
</code></pre>
",2,2,66,2015-02-03 03:01:14,https://stackoverflow.com/questions/28290644/stats-properties-among-documents-r
Can&#39;t Inspect Text Corpus in R,"<p>I am trying to create Corpus for further analysis, the code I am showing You suddenly stopped working and I cannot find solution for this error. I execute this:</p>

<pre><code>library(""tm"")
library(""SnowballC"")
library(""wordcloud"")
library(""arules"")
library(""arulesViz"")
#library(""e1071"")

#WCZYTAJ_DANE######################################################################

setwd(""D:/Dysk Google/Shared/SGGW/MGR_R2/Metody Eksploracji Danych/_PROJEKT"")
smSPAM &lt;- read.table(""smSPAM.txt"", sep=""\t"", quote="""", stringsAsFactors = F)
dim(smSPAM)
colnames(smSPAM) &lt;- c(""class"", 'text')
head(smSPAM,50)

#zamienia spam ham na 1 0
smSPAM$class=ifelse(smSPAM$class==""ham"", ""0"", ""1"")
head(smSPAM$text,50)
#View(smSPAM[smSPAM$class==""1"",])

#STWORZ_KORPUS#####################################################################

#tworze korpus na potrzeby documenttermmatrix
smSPAM.corp &lt;- Corpus(VectorSource(smSPAM$text))
inspect(smSPAM.corp)
</code></pre>

<p>But I get this error in log:</p>

<pre><code>Error in (function (classes, fdef, mtable):
unable to find an inherited method for function ‘inspect’ for signature ‘""VCorpus""’
</code></pre>

<p>However I can still perform stemming, removing white spaces etc. on this Corpus, only inspect doesn't work. </p>
","r, text, text-mining, tm","<p>Ok I found what my problem was - both tm and arules packages containt inspect functions do I had to detach arulesViz and arules (in that order cause latter is needed by former) and It's working again. </p>
",2,0,1992,2015-02-03 17:14:55,https://stackoverflow.com/questions/28304853/cant-inspect-text-corpus-in-r
R: tm: TextDocument level metadata setting looks to be very inefficient,"<p>I'm loading text documents from the database, then I create corpus from them, and finally I set prefixed id of the document (I need to use prefix, since I've got documents of several types).</p>

<pre><code>rs &lt;- dbSendQuery(con,""SELECT id::TEXT, content FROM entry"")
entry.d = data.table(fetch(rs,n=-1))
entry.vs = VectorSource(entry.d$content)
entry.vc = VCorpus(entry.vs, readerControl = list(language = ""pl""))
meta(entry.vc, tag = 'id', type = 'local') = paste0(""e:"",entry.d$id)
</code></pre>

<p>This works very slow. It takes 6 minutes, when</p>

<pre><code>tm_map(entry.vc, tm_reduce, tmFuns = funs, mc.cores=1)
</code></pre>

<p>where <code>funs</code> is the list of 6 functions, needs only 2 minutes more. </p>

<p>Is there any way to do it faster?</p>
","r, metadata, text-mining, tm, corpus","<p>I've changed my code to set IDs during initialization of the VCorpus.</p>

<pre><code>rs &lt;- dbSendQuery(con,""SELECT ('e:'||id) AS id, content, 'pl'::TEXT AS language FROM entry"")
entry.d = data.table(fetch(rs,n=-1))
entry.dfs = DataframeSource(entry.d)
reader &lt;- readTabular(mapping=list(content=""content"", id=""id"", language='language'))
entry.vc = VCorpus(entry.dfs, readerControl = list(reader = reader))
</code></pre>

<p>And now it takes only 2.5 minute to generate VCorpus with custom IDs. </p>
",2,1,99,2015-02-10 02:02:04,https://stackoverflow.com/questions/28422877/r-tm-textdocument-level-metadata-setting-looks-to-be-very-inefficient
Extract text around the reference number in research document,"<p>I want to extract the text surrounding the reference number.<br>
for example:<br>
text is :  </p>

<blockquote>
  <p>The sociological assumption is a constraint on the trust in the underlying social graph: the graph needs to have strong trust as evidenced,
      for example, by face to face interaction demonstrating social nodes
      knowledge of each other [10, 11]. While the first assumption has
      been questioned recently in [8], where it is shown that even the honest subgraph may have some cuts that disrupt the algorithmic property on which Sybil defenses are based, the trust, though being a crucial requirement for these designs, was not considered carefully. Even worse, these defense [10, 11, 2, 4] — when verified against real-world networks — have considered samples of online social graphs, which are known to possess weaker value of  rust.</p>
</blockquote>

<p>here i want to extract the cited text for reference number [8], and same for [10],[11] [2] and [4]. </p>
","java, regex, extract, text-mining","<p>You haven't actually given an example of the output you wish to collect. Neither have you provided any code that you have used to attempt this.</p>

<p>Anyway. If I assume you want all of the text prior to the citation then your regular expression would be something like:</p>

<pre><code>(.*?)\[(.*?)\]
</code></pre>

<p>This captures two groups, the first being the text and the second being the citation. You can apply this to the text with the following code:</p>

<pre><code>private static final Pattern pattern = Pattern.compile(""(.*?)\\[(.*?)\\]"");

public static void extract(String input) {
    Matcher matcher = pattern.matcher(input);

    while (matcher.find()) {
        String text = matcher.group(1);
        String citation = matcher.group(2);

        System.out.println(""The text: '"" + text + ""'\n\thas citation(s): "" + citation);
    }
}
</code></pre>

<p>For your provided input this collects the following:</p>

<pre><code>The text: 'The sociological assumption is a constraint on the trust in the underlying social graph: the graph needs to have strong trust as evidenced, for example, by face to face interaction demonstrating social nodes knowledge of each other '
    has citation(s): 10, 11
The text: '. While the first assumption has been questioned recently in '
    has citation(s): 8
</code></pre>

<p>After reading your comments, it seems that you want to find the citations that may appear in any given sentence. Since sentences are finished with fullstops, and may contain more than one citation, you need to approach that in a two stage process:</p>

<pre><code>public static void main(String[] args) {
    String input = ""..."";

    List&lt;CitedSentence&gt; citations = new ArrayList&lt;CitedSentence&gt;();
    for (String sentence : convertToSentences(input)) {
        citations.addAll(findCitations(sentence));
    }

    for (CitedSentence citation : citations) {
        System.out.println(citation);
    }
}

public static String[] convertToSentences(String input) {
    return input.split(""\\s*\\.\\s*"");
}

private static final Pattern pattern = Pattern.compile(""\\[(.*?)\\]"");
public static List&lt;CitedSentence&gt; findCitations(String sentence) {
    Matcher matcher = pattern.matcher(sentence);
    List&lt;CitedSentence&gt; result = new ArrayList&lt;CitedSentence&gt;();

    while (matcher.find()) {
        String citation = matcher.group(1);

        for (String currentCitation : citation.split("","")) {
            result.add(new CitedSentence(sentence, currentCitation.trim()));
        }
    }

    return result;
}

static class CitedSentence {
    String sentence, citation;

    public CitedSentence(String sentence, String citation) {
        this.sentence = sentence;
        this.citation = citation;
    }

    public String toString() {
        return ""["" + citation + ""]: "" + sentence;
    }
}
</code></pre>

<p>When I run this, it produces the following:</p>

<pre><code>[10]: The sociological assumption is a constraint on the trust in the underlying social graph: the graph needs to have strong trust as evidenced, for example, by face to face interaction demonstrating social nodes knowledge of each other [10, 11]
[11]: The sociological assumption is a constraint on the trust in the underlying social graph: the graph needs to have strong trust as evidenced, for example, by face to face interaction demonstrating social nodes knowledge of each other [10, 11]
[8]: While the first assumption has been questioned recently in [8], where it is shown that even the Copyright is held by the author/owner(s)
</code></pre>

<p>I only used part of the sample text.</p>
",0,-3,86,2015-02-10 14:37:32,https://stackoverflow.com/questions/28434347/extract-text-around-the-reference-number-in-research-document
Favorite tool for word/phrase counting,"<p>I am looking for a tool that performs counting of words and, more importantly, phrases, in large amounts of open-ended text responses.  I need the ability to exclude certain words (a, the, and, etc.) as well.  </p>

<p>I am aware of a few tools that do this:</p>

<pre><code> - http://www.mywritertools.com/default.asp
 - http://www.hermetic.ch/wfca/wfca.htm
</code></pre>

<p>As well as some lists of available text mining software</p>

<pre><code> - http://en.wikipedia.org/wiki/List_of_text_mining_software
 - http://academic.csuohio.edu/kneuendorf/content/cpuca/qtap.htm
 - http://www.predictiveanalyticstoday.com/top-30-software-for-text-analysis-text-mining-text-analytics/
</code></pre>

<p>Most of these either a) cost money, or b) provide much more/different functionality than I need.  I am not opposed to paying a moderate amount (&lt; $100) for a decent tool, but am hoping to get some input first to avoid buying something that doesn't meet my needs.</p>

<p><strong>Data specifics:</strong><br />
1) currently resides in a SQL database, but can transformed into whatever format needed (text file, excel, whatever)<br />
2) contains  an opened ended response, and a category id relating to a specific product or type of product (eg. either ""soda"" or ""pepsi"")</p>

<p><strong>Needs</strong><br />
1) Ability to count common words and phrases<br />
2) Ability to exclude a list of words (a, the, and, etc.) such that ""wash car"" and ""wash <strong>the</strong> car"" would count as the same phrase</p>

<p><strong>Would be nice to have</strong><br />
1) Ability to match based on root word so that ""<strong>wash</strong> the car"", ""<strong>washed</strong> the car"" and ""<strong>washes</strong> the car"" all match<br />
2) Ability to see what words appear near each other so that I can get a count of the times that ""wash car"", ""wash the car"" and ""car wash"" appear as a single count.</p>

<p><strong>Icing on the cake</strong><br />
1) Ability to do counts based on categories.  Not a big deal as the number of categories is relatively low and I can run each individually, but this may change in the future.</p>

<p>Please share any advice/experience/suggestions!  Also, I am not opposed to writing my own tool, but don't want to reinvent the wheel.  In the absence of a specific tool, any libraries that may assist in doing this (especially for root word matching), would also be appreciated.</p>
","full-text-search, text-mining, data-analysis, word-count, text-analysis","<p>So it doesn't look like this is something anyone else really needs, but just in case, here's how I solved my problem.</p>

<p>I used 2 different tools:</p>

<ul>
<li>Hermetic Word Frequency Advanced (<a href=""http://www.hermetic.ch/wfca/wfca.htm"" rel=""nofollow"">http://www.hermetic.ch/wfca/wfca.htm</a>)</li>
<li>RapidMiner Studio (<a href=""https://rapidminer.com/"" rel=""nofollow"">https://rapidminer.com/</a>) with the Text Processing extension added through RapidMiner Marketplace</li>
</ul>

<p>The RapidMiner text processing tools worked great for extracting json, segmenting data, extracting relevant data, and then tokenizing/normalizing and removing common words prior to the actual processing I needed to do.  It also allowed the creation of n-grams, and then word frequency analysis including n-grams.  Very cool tool that has a lot more possibilities.</p>

<p>Since the requirements for this specific project wanted ONLY a list of phrases with their frequencies (Could have extracted this from the output of the RapidMiner word frequency analysis but would have required some manual work), I also employed the use of the Hermetic Word Frequency Advanced (HWFA) tool, which allowed for the counting of only phrases.  </p>

<p>After pre-processing the text using RapidMiner (tokenizing, making all lowercase, removing common words, stemming), I took that output and ran it through (HWFA) to get exactly what I wanted.  I was surprised with as powerful as RapidMiner is that it wasn't possible to get only phrases back with their frequencies, but if it is possible (which it might be - the tool is pretty powerful), my 3 days of toying with it didn't uncover how.</p>

<p>RapidMiner Studio Basic and the Text Mining extension are both free.  HWFA was $60 (overpriced if you ask me, but did the trick in a pinch).</p>

<p>Hope this may be able to help someone else some day!</p>
",0,0,666,2015-02-13 19:22:18,https://stackoverflow.com/questions/28507148/favorite-tool-for-word-phrase-counting
Features Vectors to build classifier to detect subjectivity,"<p>I am trying to build a classifier to detect subjectivity. I have text files tagged with subjective and objective . I am little lost with the concept of features creation from this data. I have found the lexicon of the subjective and objective tag. One thing that I can do is to create a feature of having words present in respective dictionary. Maybe the count of words present in subjective and objective dictionary. After that I intend to use naive bayes or SVM to develop the model</p>

<p>My problem is as follow</p>

<ol>
<li>Is my approach correct ?</li>
<li>Can I create more features ? If possible suggest some or point me to some paper or link</li>
<li>Can I do some test like chi -sq etc to identify effective words from the dictionary ?</li>
</ol>
","nlp, text-mining, sentiment-analysis","<p>You are basically on the right track. I would try and apply classifier with features you already have and see how well it will work, before doing anything else.  </p>

<p>Actually best way to improve your work is to google for subjectivity classification papers and read them (there are a quite a <a href=""https://scholar.google.ru/scholar?as_ylo=2011&amp;q=subjectivity%20classification&amp;hl=en&amp;as_sdt=0,5"" rel=""nofollow"">number of them</a>). For example <a href=""http://arxiv.org/ftp/arxiv/papers/1312/1312.6962.pdf"" rel=""nofollow"">this one</a> lists typical features for this task.</p>

<p>And yes Chi-squared can be used to construct dictionaries for text classification (other commonly used methods are TD*IDF, pointwise mutal information and LDA)</p>

<p>Also, recently new neural network-based methods for text classification such as <a href=""http://arxiv.org/pdf/1405.4053v2.pdf"" rel=""nofollow"">paragraph vector</a> and <a href=""http://arxiv.org/pdf/1406.3830v1.pdf"" rel=""nofollow"">dynamic convolutional neural networks with k-max pooling</a> demonstrated state-of-the-art results on sentiment analysis, thus they should probably be good for subjectivity classification as well.</p>
",2,0,144,2015-02-16 05:36:56,https://stackoverflow.com/questions/28535136/features-vectors-to-build-classifier-to-detect-subjectivity
How to find term frequency within a DTM in R?,"<p>I've been using the tm package to create a DocumentTerm Matrix as follows:</p>

<pre><code>library(tm)
library(RWeka)
library(SnowballC)
src &lt;- DataframeSource(data.frame(data3$JobTitle))

# create a corpus and transform data
# Sets the default number of threads to use
options(mc.cores=1)
c_copy &lt;- c &lt;- Corpus(src)
c &lt;- tm_map(c, content_transformer(tolower), mc.cores=1)
c &lt;- tm_map(c,content_transformer(removeNumbers), mc.cores=1)
c &lt;- tm_map(c,removeWords, stopwords(""english""), mc.cores=1)
c &lt;- tm_map(c,content_transformer(stripWhitespace), mc.cores=1)

#make DTM
dtm &lt;- DocumentTermMatrix(c, control = list(tokenize = BigramTokenizer))
</code></pre>

<p>Now, the DTM comes out fine - what I want to do is get the frequencies of the frequent terms within the DTM. Obviously, I can use findFreqTerms to get the terms themselves, but not the actual frequencies. termFreq only works on TextDocument, not a DTM or TDM - any ideas?</p>

<p>Output from str - the frequent terms are in $ Terms:</p>

<pre><code>&gt; str(dtm)
List of 6
 $ i       : int [1:190] 1 2 3 4 5 6 7 8 9 10 ...
 $ j       : int [1:190] 1 2 3 4 5 6 7 8 9 10 ...
 $ v       : num [1:190] 1 1 1 1 1 1 1 1 1 1 ...
 $ nrow    : int 119
 $ ncol    : int 146
 $ dimnames:List of 2
  ..$ Docs : chr [1:119] ""1"" ""2"" ""3"" ""4"" ...
  ..$ Terms: chr [1:146] ""account administrator"" ""account assistant"" ""account director"" ""account executive"" ...
 - attr(*, ""class"")= chr [1:2] ""DocumentTermMatrix"" ""simple_triplet_matrix""
 - attr(*, ""weighting"")= chr [1:2] ""term frequency"" ""tf""
</code></pre>
","r, text-mining","<p>Thanks to NicE for the advice - it works well. Adding in the weighting argument allows me to get out the term frequencies when I inspect the DTM. Simple matter then of summing up per column.</p>

<pre><code>dtm &lt;- DocumentTermMatrix(c, control = list(tokenize = BigramTokenizer, weighting=weightTf))
freqs &lt;- as.data.frame(inspect(dtm))
colSums(freqs)
</code></pre>
",3,3,7268,2015-02-18 09:50:48,https://stackoverflow.com/questions/28580460/how-to-find-term-frequency-within-a-dtm-in-r
R grouping by name and perform stats (t-test),"<p>I have two data.frames:</p>

<pre><code>word1=c(""a"",""a"",""a"",""a"",""b"",""b"",""b"")    
word2=c(""a"",""a"",""a"",""a"",""c"",""c"",""c"")
values1 = c(1,2,3,4,5,6,7)
values2 = c(3,3,0,1,2,3,4)
df1 = data.frame(word1,values1)
df2 = data.frame(word2,values2)
</code></pre>

<p>df1: </p>

<pre><code>  word1  values1
1   a      1
2   a      2
3   a      3
4   a      4
5   b      5
6   b      6
7   b      7
</code></pre>

<p>df2:</p>

<pre><code> word2  values2
1   a     3
2   a     3
3   a     0
4   a     1
5   c     2
6   c     3
7   c     4
</code></pre>

<p>I would like to split these dataframes by <code>word*</code>, and perform two sample <code>t.test</code>s in R.</p>

<p>For example, the word ""a"" is in both data.frames. What's the <code>t.test</code> between the data.frames for the word ""a""? And do this for all the words that are in both data.frames.</p>

<p>The result is a data.frame(result):</p>

<pre><code>   word  tvalues
1   a    0.4778035
</code></pre>

<p>Thanks</p>
","r, statistics, dataframe, text-mining","<p>Find the words common to both dataframes, then loop over these words, subsetting both dataframes and performing the <code>t.test</code> on the subsets.</p>

<p>E.g.:</p>

<pre><code>df1 &lt;- data.frame(word=sample(letters[1:5], 30, replace=TRUE),
                  x=rnorm(30))

df2 &lt;- data.frame(word=sample(letters[1:5], 30, replace=TRUE),
                  x=rnorm(30))

common_words &lt;- sort(intersect(df1$word, df2$word))

setNames(lapply(common_words, function(w) {
  t.test(subset(df1, word==w, x), subset(df2, word==w, x))
}), common_words)
</code></pre>

<p>This returns a list, where each element is the output of the <code>t.test</code> for one of the common words. <code>setNames</code> just names the list elements so you can see which words they correspond to.</p>

<p>Note I've created new example data here since your example data only have one word in common (<code>a</code>) and so don't really resemble your true problem.</p>

<hr>

<p>If you just want a matrix of statistics, you can do something like:</p>

<pre><code>t(sapply(common_words, function(w) {
  test &lt;- t.test(subset(df1, word==w, x), subset(df2, word==w, x))
  c(test$statistic, test$parameter, p=test$p.value, 
    `2.5%`=test$conf.int[1], `97.5%`=test$conf.int[2])
}))

##            t        df          p       2.5%      97.5%
## a  0.9141839  8.912307 0.38468553 -0.4808054  1.1313220
## b -0.2182582  7.589109 0.83298193 -1.1536056  0.9558315
## c -0.2927253  8.947689 0.77640684 -1.5340097  1.1827691
## d -2.7244728 12.389709 0.01800568 -2.5016301 -0.2826952
## e -0.3683153  7.872407 0.72234501 -1.9404345  1.4072499
</code></pre>
",0,-1,56,2015-02-20 21:15:33,https://stackoverflow.com/questions/28638343/r-grouping-by-name-and-perform-stats-t-test
Text Documents Clustering - Non Uniform Clusters,"<p>I have been trying to cluster a set of text documents. I have a sparse TFIDF matrix with around 10k documents (subset of a large dataset), and I try to run the scikit-learn k-means algorithm with different sizes of clusters (10,50,100). Rest all the parameters are default values.</p>

<p>I get a very strange behavior that no matter how many clusters I specify or even if I change the number of iterations, there would be 1 cluster in the lot which would contain most of the documents in itself and there will be many clusters which would have just 1 document in them. This is highly non-uniform behavior </p>

<p>Does anyone know what kind of problem am I running into? </p>
","scikit-learn, cluster-analysis, k-means, text-mining, tf-idf","<p>I noticed with the help of above answers and comments that there was a problem with outliers and noise in original space. For this, we should use a dimensionality reduction method which eliminates the unwanted noise in the data. I tried random projections first but it failed to work with text data, simply because the problem was still not solved.
Then using Truncated Singular Value Decomposition, I was able to get perfect uniform clusters. Hence, the <a href=""https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD"" rel=""nofollow"">Truncated SVD</a> is the way to go with textual data in my opinion.</p>
",0,0,250,2015-02-25 18:12:46,https://stackoverflow.com/questions/28726548/text-documents-clustering-non-uniform-clusters
scraping text file in R,"<p>I am trying to extract some information from the following text file using R. I need the line <code>""Processor: SPARC T5""</code> which is under heading ""Hardware Systems"" and then under ""Java EE AppServer &amp; Database Server HW (SUT hardware)"". I tried the following which matches the expression and gives me all the Processor information. I have 50 different text files like this and need to extract this information from all of them. How do I extract the Processor information just under the ""Hardware Systems"" and ""Java EE AppServer &amp; Database Server HW (SUT hardware)"" heading. </p>

<pre><code>a &lt;-readLines(""http://spec.org/jEnterprise2010/results/res2013q3/jEnterprise2010-20130904-00045.txt"")
b &lt;- grep(""Processor:"",a) 
c &lt;- a[b]
c[1] ""  Processor:         SPARC T5""         ""  Processor:         Intel Xeon X5670""
</code></pre>
","r, web-scraping, text-mining","<p>Well, you can narrow down the section by looking for the section header, then seeing where the section starts by seeing where the indentation stops and only search those lines. For example</p>

<pre><code>sectionmarker &lt;- ""Java EE AppServer &amp; Database Server HW (SUT hardware)""
s&lt;-grep(sectionmarker, a, fixed=TRUE)
e&lt;-grep(""^\\S"", a[-(1:s)])[1]
grep(""Processor"", a[(s+1):(s+e-1)], fixed=T, value=T)[1]
# [1] ""  Processor:         SPARC T5""
</code></pre>
",2,0,895,2015-02-26 04:47:31,https://stackoverflow.com/questions/28734651/scraping-text-file-in-r
R text mining : Grouping of similar patterns from a dataframe.,"<p>I have applied various cleaning functions from the <code>tm</code> package, like removing punctuation, numbers, special chars, common English words etc. and got a data-frame as shown below. Remember, I don't have a primary-key, like cust_id or account_number to rely on</p>

<pre><code>sno        names
001        SIRIS BLACK
002        JOHN DOE
003        STEPHEN HRYY
004        SIRIUS BLACK
005        SIRUS BLACK
006        JON DOE
007        STEPHEN HARRY
008        STIPHEN HURRY
009        JHN DOE 
</code></pre>

<p>Looking at the above data, I can really feel that there is a similarity of patterns and that those names are close to one another. How do I calculate the percentage of equality of patterns using the available text-mining functions of R, so that I can finally get a data-frame with all unique names? </p>

<p>Assumptions and Shortcomings :</p>

<ol>
<li><p>Just bluntly assuming that unique names, might be the ones with maximum characters, because the raw data that I have has loads of typos on the names. (logical assumption, perhaps will reduce the number of typos)</p></li>
<li><p>The <code>agrep()</code> function searches for approximate matches to pattern within a large string, and problem here is I actually don't know what the patterns are.</p></li>
</ol>

<p>Group the similar strings like this :</p>

<pre><code>sno        names
001        SIRIS BLACK          
002        SIRIUS BLACK
003        SIRUS BLACK
004        JHN DOE
005        JOHN DOE
006        JON DOE
007        STEPHEN HARRY
008        STIPHEN HURRY
009        STEPHEN HRYY
</code></pre>

<p>And finally get this :</p>

<pre><code>001     JOHN DOE
002     STEPHEN HARRY
003     STIPHEN HURRY
004     SIRIUS BLACK
</code></pre>
","r, dataframe, text-mining, tm, names","<p>For the <code>agrep</code> part, here's one way - you can play with the parameters to tune your results:</p>

<pre><code>sim &lt;- setNames(lapply(1:nrow(df), function(i) agrep(df$names[i], df$names, max.distance = list(all=2, insertions=2, deletions=2, substitutions=0))), df$names)
sim &lt;- lapply(sim, function(x) unique(df$names[x]))
df$names2 &lt;- sapply(sim, ""["", 1)
df[!duplicated(df$names2), ]
#   sno         names        names2
# 1   1   SIRIS BLACK   SIRIS BLACK
# 2   2      JOHN DOE      JOHN DOE
# 3   3  STEPHEN HRYY  STEPHEN HRYY
# 8   8 STIPHEN HURRY STIPHEN HURRY
</code></pre>
",4,4,1755,2015-03-02 09:07:16,https://stackoverflow.com/questions/28805989/r-text-mining-grouping-of-similar-patterns-from-a-dataframe
Read HTML code into R for data &amp; text mining,"<p>I am trying to read the information located on this site into R for data and text analysis:</p>

<pre><code>http://www.nhl.com/scores/htmlreports/20142015/PL020916.HTM
</code></pre>

<p>I have tried to read the source code into R using the following packages and code:</p>

<pre><code>library(XML)
theurl &lt;- ""http://www.nhl.com/scores/htmlreports/20142015/PL020916.HTM""
tables &lt;- readHTMLTable(theurl)

con = url(""http://www.nhl.com/scores/htmlreports/20142015/PL020916.HTM"")
htmlCode=readLines(con)
close(con)
htmlCode
</code></pre>

<p>I am looking for an output that is a flat file of the information provided.</p>
","html, r, data-mining, text-mining","<p>I'm not sure what info you are looking for from the page you've provided, but here's how you can read it in using rvest...</p>

<pre><code>url &lt;- ""http://www.nhl.com/scores/htmlreports/20142015/PL020916.HTM""
library(""rvest"")
url %&gt;% html()
</code></pre>
",0,1,2310,2015-03-02 21:24:51,https://stackoverflow.com/questions/28819716/read-html-code-into-r-for-data-text-mining
Algorithm for multiple extended string matching,"<p>I need to implement an algorithm for multiple extended string matching in text.</p>

<p><strong>Extended</strong> means the presence of wildcards (any number of characters instead of a star), for example:</p>

<pre><code>abc*def //matches abcdef, abcpppppdef etc.
</code></pre>

<p><strong>Multiple</strong> means that the search is going on simultaneously for multiple string patterns (not a separate search for each pattern), for example:</p>

<pre><code>abc*def
abc
whatever
some*string
</code></pre>

<p><strong>QUESTION:</strong></p>

<p>What is the fast algorithm that can do multiple extended string matching?</p>

<p>Preferably, optimized for SIMD instructions and multicore implementation. Open source implementation (C/C++/Python) would be great as well.</p>

<p>Thank you</p>
","algorithm, text-mining","<p>I think that it might make sense to start by reading the following  Wikipedia article's section: <a href=""http://en.wikipedia.org/wiki/Regular_expression#Implementations_and_running_times"" rel=""nofollow"">http://en.wikipedia.org/wiki/Regular_expression#Implementations_and_running_times</a>. You can then perform a literature review on algorithms, implementing <em>regular expression pattern matching</em>.</p>

<p>In terms of practical <strong>implementation</strong>, there is a large variety of <em>regular expression (regex)</em> engines in a form of libraries, focused on one or more programming languages. Most likely, the best and most popular option is the C/C++ <a href=""http://www.pcre.org"" rel=""nofollow"">PCRE library</a>, with its newest version PCRE2, released in 2015. Another C++ regex library, which is quite popular at Google, is <a href=""https://code.google.com/p/re2"" rel=""nofollow"">RE2</a>. I recommend you to read <a href=""http://swtch.com/~rsc/regexp/regexp3.html"" rel=""nofollow"">this paper</a>, along with the two other, linked within the article, for details on algorithms, implementation and benchmarks. Just recently, Google has released <a href=""https://github.com/google/re2j"" rel=""nofollow"">RE2/J</a> - a linear time version of RE2 for Java: see <a href=""http://google-opensource.blogspot.com/2015/02/re2j-linear-time-regular-expression.html"" rel=""nofollow"">this blog post</a> for details. Finally, I ran across an interesting pure C regex library <a href=""https://github.com/laurikari/tre"" rel=""nofollow"">TRE</a>, which offers way too many cool features to list here. However, you can read about them all on <a href=""http://laurikari.net/tre/about"" rel=""nofollow"">this page</a>.</p>

<p><strong>P.S.</strong> If the above is not enough for you, feel free to visit <a href=""http://en.wikipedia.org/wiki/Comparison_of_regular_expression_engines"" rel=""nofollow"">this Wikipedia page</a> for details of many more regex engines/libraries and their comparison across several criteria. Hope my answer helps.</p>
",2,1,241,2015-03-10 07:26:13,https://stackoverflow.com/questions/28962097/algorithm-for-multiple-extended-string-matching
what is getText function in text-mining? Where does it come from? [r],"<p>I am following a text-mining example from Social Media Mining with R by Nathan Dannerman &amp; Richard Heimann : <a href=""https://rads.stackoverflow.com/amzn/click/com/1783281774"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"" title=""The book"">The Book</a> . After pulling tweets using <code>searchTwitter</code> function, the author uses <code>sapply</code> on the <code>list</code> to extract the text portion like this: </p>

<p><code>rstats &lt;- searchTwitter(""#rstats"", n = 1000)</code></p>

<p><code>rstats_list &lt;- sapply(rstats, function (x) x$getText())</code></p>

<p>This neatly makes a character vector with only the text portion from the tweets. What is getText() ? I can't find anything on it anywhere - the author has not sufficiently explained. I looked at the <code>tm</code> package documentation - nothing! I looked for questions related to this on SO - <a href=""https://stackoverflow.com/questions/24829982/getting-text-from-tweets/24832483#24832483"">Getting Text From Tweets</a> The answerer is asking the same question as I. On Inside-R, I found this: <a href=""http://www.inside-r.org/r-doc/base/gettext"" rel=""nofollow noreferrer"">http://www.inside-r.org/r-doc/base/gettext</a> but it's not the same as <code>getText</code> . Can someone explain what I am missing? </p>
","r, twitter, text-mining, tm","<p>getText is an accessor method for the 'status' class, as described here: <a href=""http://www.inside-r.org/howto/mining-twitter-airline-consumer-sentiment"" rel=""noreferrer"">http://www.inside-r.org/howto/mining-twitter-airline-consumer-sentiment</a>.</p>

<p>Sorry for not clarifying in the text,
Nathan Danneman</p>
",6,4,2930,2015-03-11 14:49:37,https://stackoverflow.com/questions/28989810/what-is-gettext-function-in-text-mining-where-does-it-come-from-r
How to visualize the findAssocs() result from tm,"<p>I've extracted some tweets and put them into a term document matrix. Next I started looking for word associations - words which most frequently occur together.</p>

<p><code>tweets_tdm &lt;- TermDocumentMatrix(tweets_corpus)</code></p>

<p><code>findAssocs(tweets_tdm, 'stackoverflow', 0.20)</code></p>

<p>I get results which look like:</p>

<p><code>programming               0.33
java                       0.27
moderator                  0.27</code></p>

<p>How can I visualize these results apart from doing a bar chart / pie chart ? I would like to do a visualization which has the search word ""stackoverflow"" as the axis / hub and the associated words as the node or spokes.</p>
","r, data-visualization, text-mining, tm","<p>Here is a perspective from using the <code>igraph</code> package and one version of the possible output.  There are more choices for formatting, of course. </p>

<pre><code>terms &lt;- c(""programming"", ""java"", ""moderator"", ""extraword"")
probs &lt;- c(0.33, 0.27, 0.27, .55)
df &lt;- data.frame(terms = terms, probs = probs)
g &lt;- graph.data.frame(df, directed = TRUE)
plot(g) 
</code></pre>

<p><img src=""https://i.sstatic.net/WZ0Av.png"" alt=""enter image description here""></p>
",4,2,1383,2015-03-12 18:30:26,https://stackoverflow.com/questions/29017608/how-to-visualize-the-findassocs-result-from-tm
Document term matrix in R,"<p>I have the following code:</p>

<pre><code>rm(list=ls(all=TRUE)) #clear data
setwd(""~/UCSB/14 Win 15/Issy/text.fwt"") #set working directory
files &lt;- list.files(); head(files) #load &amp; check working directory

fw1 &lt;- scan(what=""c"", sep=""\n"",file=""fw_chp01.fwt"")

library(tm) 
corpus2&lt;-Corpus(VectorSource(c(fw1)))
skipWords&lt;-(function(x) removeWords(x, stopwords(""english"")))

#remove punc, numbers, stopwords, etc
funcs&lt;-list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords)
corpus2.proc&lt;-tm_map(corpus2, FUN = tm_reduce, tmFuns = funcs)

corpus2a.dtm &lt;- DocumentTermMatrix(corpus2.proc, control = list(wordLengths = c(1,110))) #create document term matrix
</code></pre>

<p>I'm trying use some of the operations detailed in the tm reference manual (<a href=""http://cran.r-project.org/web/packages/tm/tm.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/tm/tm.pdf</a>) with little success. For example, when I try to use the findFreqTerms, I get the following error: </p>

<pre><code>Error: inherits(x, c(""DocumentTermMatrix"", ""TermDocumentMatrix"")) is not TRUE
</code></pre>

<p>Can anyone clue me in as to why this isn't working and what I can do to fix it?</p>

<p>Edited for @lawyeR:</p>

<p>head(fw1) produces the first six lines of the text (Episode 1 of Finnegans Wake by James Joyce): </p>

<pre><code>[1] ""003.01    riverrun, past Eve and Adam's, from swerve of shore to bend""      
[2] ""003.02  of bay, brings us by a commodius vicus of recirculation back to""    
[3] ""003.03  Howth Castle and Environs.""                                         
[4] ""003.04    Sir Tristram, violer d'amores, fr'over the short sea, had passen-""
[5] ""003.05  core rearrived from North Armorica on this side the scraggy""        
[6] ""003.06  isthmus of Europe Minor to wielderfight his penisolate war: nor""  
</code></pre>

<p>inspect(corpus2) outputs each line of the text in the following format (this is the final line of the text): </p>

<pre><code>[[960]]
&lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;
029.36  borough. #this part differs by line of course
</code></pre>

<p>inspect(corpus2a.dtm) returns a table of all the types (there are 4163 in total( in the text in the following format: </p>

<pre><code>Docs  youths yoxen yu yurap yutah zee zephiroth zine zingzang zmorde zoom
  1        0     0  0     0     0   0         0    0        0      0    0
  2        0     0  0     0     0   0         0    0        0      0    0
</code></pre>
","r, matrix, text-mining, tm, corpus","<p>Here is a simplified form of what you provided and did, and <code>tm</code> does its job.  It may be that one or more of your cleaning steps caused a problem.</p>

<pre><code>&gt; library(tm) 
&gt; fw1 &lt;- c(""riverrun, past Eve and Adam's, from swerve of shore to bend      
+                                  of bay, brings us by a commodius vicus of recirculation back to
+                                  Howth Castle and Environs.      
+                                  Sir Tristram, violer d'amores, fr'over the short sea, had passen-
+                                  core rearrived from North Armorica on this side the scraggy    
+                                  isthmus of Europe Minor to wielderfight his penisolate war: nor"")
&gt; 
&gt; corpus&lt;-Corpus(VectorSource(c(fw1)))
&gt; inspect(corpus)
&lt;&lt;VCorpus (documents: 1, metadata (corpus/indexed): 0/0)&gt;&gt;

[[1]]
&lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;
riverrun, past Eve and Adam's, from swerve of shore to bend      
                                 of bay, brings us by a commodius vicus of recirculation back to
                                 Howth Castle and Environs.      
                                 Sir Tristram, violer d'amores, fr'over the short sea, had passen-
                                 core rearrived from North Armorica on this side the scraggy    
                                 isthmus of Europe Minor to wielderfight his penisolate war: nor

&gt; dtm &lt;- DocumentTermMatrix(corpus)
&gt; findFreqTerms(dtm)
 [1] ""adam's,""       ""and""           ""armorica""      ""back""          ""bay,""          ""bend""         
 [7] ""brings""        ""castle""        ""commodius""     ""core""          ""d'amores,""     ""environs.""    
[13] ""europe""        ""eve""           ""fr'over""       ""from""          ""had""           ""his""          
[19] ""howth""         ""isthmus""       ""minor""         ""nor""           ""north""         ""passen-""      
[25] ""past""          ""penisolate""    ""rearrived""     ""recirculation"" ""riverrun,""     ""scraggy""      
[31] ""sea,""          ""shore""         ""short""         ""side""          ""sir""           ""swerve""       
[37] ""the""           ""this""          ""tristram,""     ""vicus""         ""violer""        ""war:""         
[43] ""wielderfight"" 
</code></pre>

<p>As another point, I find it useful at the start to load a few other complementary packages to <code>tm</code>.</p>

<pre><code>library(SnowballC); library(RWeka); library(rJava); library(RWekajars)
</code></pre>

<p>For what its worth, as compared to your somewhat complicated cleaning steps, I usually trudge along like this (replace comments$comment with your text vector):</p>

<pre><code>comments$comment &lt;- tolower(comments$comment)
comments$comment &lt;- removeNumbers(comments$comment)
comments$comment &lt;- stripWhitespace(comments$comment) 
comments$comment &lt;- str_replace_all(comments$comment, ""  "", "" "") 
# replace all double spaces internally with single space   
# better to remove punctuation with str_ because the tm function doesn't insert a space
library(stringr)
comments$comment &lt;- str_replace_all(comments$comment, pattern = ""[[:punct:]]"", "" "") 
comments$comment &lt;- removeWords(comments$comment, stopwords(kind = ""english""))
</code></pre>
",0,1,4927,2015-03-17 06:58:44,https://stackoverflow.com/questions/29093057/document-term-matrix-in-r
read multiple text files from multiple folders,"<p>I'm trying to read all the '*.txt' files in the subfolders, but it seems like there is a problem in the loop. Basically, folders are structured as following:</p>

<pre><code>branch1    branch 2     txt.file    result I want
1 -------- 2002----------a---------------a
  ---------2003----------b---------------b+c
               ----------c
2 ---------2004----------d---------------d
  ---------2005----------e---------------e+f
               ----------f
</code></pre>

<p>So, I've been listing directories into the list, like below:</p>

<pre><code>setwd(""C:/Users/J/Desktop/research/DATA/test"")
parent.folder&lt;-""C:/Users/J/Desktop/research/DATA/test""
sub.folders1 &lt;- list.dirs(parent.folder, recursive=TRUE)[-1]
sub.folders2 &lt;- list.dirs(sub.folders1, recursive=FALSE)
r.scripts &lt;- file.path(sub.folders2)
</code></pre>

<p>Then I tried to find <code>.txt</code> files in each folder, then read them all through so that individual folders can contain a single text file. Like I pictured above at ""results I want"". (I do not want to read many files into a single data frame!) </p>

<p>So from here I tried to read <code>.txt</code> files in the same folder using a for loop, but seems like there is a problem in the code I've written.</p>

<pre><code>for (k in 1:length(r.scripts)){
  file.name.v &lt;- list.files(r.scripts[k], pattern=""*.txt"")
    for (f in 1:length(file.name.v)){
      file.read.v &lt;- scan(paste(r.scripts,file.name.v[f], sep=""/""),
                          what =""character"",sep=""\n"")
     }
   }
</code></pre>
","r, file, text-mining","<p>Your forgot to give a problem description, but this is something that will work:</p>

<pre><code>parent.folder&lt;-""C:/Users/J/Desktop/research/DATA/test""
setwd(parent.folder)

sub.folders1 &lt;- list.dirs(parent.folder, recursive=TRUE)[-1]
sub.folders2 &lt;- list.dirs(sub.folders1, recursive=FALSE)
r.scripts &lt;- file.path(sub.folders2)

for (k in r.scripts){
  file.name.v &lt;- list.files(k, pattern=""*.txt"")
  for (f in file.name.v){
    file.read.v &lt;- scan(paste(k, f, sep=""/""),
                        what =""character"",sep=""\n"")
  }
}
</code></pre>

<p><strong>Edit</strong></p>

<pre><code># create list
l &lt;- list()
for (k in r.scripts){
  file.name.v &lt;- list.files(k, pattern=""*.txt"")
  for (f in file.name.v){
    l[[k]] &lt;- c(l[[k]], scan(paste(k, f, sep=""/""),
                             what =""character"",sep=""\n""))
  }
}
</code></pre>
",1,1,3168,2015-03-18 12:57:33,https://stackoverflow.com/questions/29122723/read-multiple-text-files-from-multiple-folders
Error while instaling Open GRM thrax,"<p>I have already installed Open Fst in Ubuntu and its working fine. Now i'm trying to install Open GRM thrax. I have tried installing with 2 different versions of thrax.</p>

<p>Thrax version 1.1.0:</p>

<pre><code>thraxOpenGrm/thrax-1.1.0$ ./configure
</code></pre>

<p>below is the error that i get.</p>

<pre><code>checking how to hardcode library paths into programs... immediate
checking for bison... no
checking for byacc... no
checking for std::tr1::hash&lt;long long unsigned&gt;... yes
checking for __gnu_cxx::slist&lt;int&gt;... yes
checking fst/fst.h usability... yes
checking fst/fst.h presence... no
configure: WARNING: fst/fst.h: accepted by the compiler, rejected by the preprocessor!
configure: WARNING: fst/fst.h: proceeding with the compiler's result
checking for fst/fst.h... yes
checking fst/extensions/far/far.h usability... yes
checking fst/extensions/far/far.h presence... no
configure: WARNING: fst/extensions/far/far.h: accepted by the compiler, rejected by the preprocessor!
configure: WARNING: fst/extensions/far/far.h: proceeding with the compiler's result
checking for fst/extensions/far/far.h... yes
checking fst/extensions/pdt/pdt.h usability... no
checking fst/extensions/pdt/pdt.h presence... no
checking for fst/extensions/pdt/pdt.h... no
configure: error: fst/extensions/pdt/pdt.h header not found
</code></pre>

<p>Thrax version 0.1.0:</p>

<pre><code>thraxOpenGrm/thrax-0.1.0$ ./configure
</code></pre>

<p>below is the error that i get.</p>

<pre><code>checking how to hardcode library paths into programs... immediate
checking for bison... no
checking for byacc... no
checking for std::tr1::hash&lt;long long unsigned&gt;... yes
checking for __gnu_cxx::slist&lt;int&gt;... yes
checking fst/fst.h usability... no
checking fst/fst.h presence... no
checking for fst/fst.h... no
configure: error: fst/fst.h header not found
</code></pre>

<p>It throws different errors with different thrax versions. I read a solution in this forum.</p>

<p><a href=""http://www.openfst.org/twiki/bin/view/Forum/GrmThraxForum"" rel=""nofollow"">http://www.openfst.org/twiki/bin/view/Forum/GrmThraxForum</a></p>

<p>It says openfst must be 'built' with <code>./configure --enable-far=true</code> . i uninstalled openfst and installed it using <code>./configure --enable-far=true</code> and also with <code>./configure --enable-far</code>. The error still persists. </p>
","c++, ubuntu-14.04, text-mining, text-analysis, openfst","<p>During installation of openfst you have to type:</p>

<pre><code>./configure --enable-far=true --enable-pdt=true --enable-mpdt=true
</code></pre>

<p>Then you should install thrax and while on it, type in terminal:</p>

<pre><code>export LD_LIBRARY_PATH=/usr/local/lib
</code></pre>

<p>Worked for me for openfst-1.5.4 and thrax-1.2.2.</p>

<p>When I got:</p>

<pre><code>checking fst/extensions/pdt/pdt.h usability... no
</code></pre>

<p>I added: </p>

<pre><code>--enable-pdt=true
</code></pre>

<p>to <code>./configure</code> for openfst and I did the same for mpdt error. If you get other errors, you can try doing the same.</p>
",9,4,2929,2015-03-25 11:05:38,https://stackoverflow.com/questions/29253913/error-while-instaling-open-grm-thrax
How to create a feature that detect age in text in different languages?,"<p>I have a text classification task in several languages. What aproach should use if I would like to create a feature that extract age from text if this are the possible classes: <code>18-24</code>,<code>25-34</code>,<code>35-49</code> and <code>50-xx""</code> and I have only tweets as a corpus. I all ready tried using all the tweets but with very low performance(0.66) any idea of how to aproach this task?. Thanks in advance.</p>
","machine-learning, nlp, artificial-intelligence, text-mining","<p>Since it is still a research task, I suggest several links to scientific papers (links and the following summary are mostly taken from 'related work' section of <a href=""https://web.archive.org/web/20191220181718/https://cyberleninka.ru/article/n/opredelenie-demograficheskih-atributov-polzovateley-mikroblogov"" rel=""nofollow noreferrer"">our paper</a> - unfortunately, in Russian, so I edited Google translation a little).</p>
<p>So, take a look at these works (marked by year): <a href=""https://web.archive.org/web/20160617034418/http://www.aaai.org/ocs/index.php/ICWSM/09/paper/viewPDFInterstitial/208/537"" rel=""nofollow noreferrer"">2009</a>, <a href=""https://web.archive.org/web/20160101182151/https://csc-869-mlog.googlecode.com/files/p37-rao.pdf"" rel=""nofollow noreferrer"">2010</a>, <a href=""https://www.researchgate.net/profile/Claudia-Peersman/publication/221615645_Predicting_age_and_gender_in_online_social_networks/links/09e4150cb2d78bda25000000/Predicting-age-and-gender-in-online-social-networks.pdf"" rel=""nofollow noreferrer"">2011</a>, <a href=""https://web.archive.org/web/20150412051417/http://depot.knaw.nl:80/13345/1/nguyen-icwsm2013.pdf"" rel=""nofollow noreferrer"">2013</a>, <a href=""https://ieeexplore.ieee.org/document/7051893"" rel=""nofollow noreferrer"">2014</a>.</p>
<p>In summary: you should find or create tagged corpora and use supervised machine learning with the following features:</p>
<ol>
<li>text features: n-grams over words and characters,</li>
<li>stylistic features: parts of speech, slang, the average sentence length, punctuation, acronyms, emoticons, etc.</li>
<li>social network features: the number of friends a user, the number of posts displayed on the page of the user, the total number of posts, the average number of comments for a post of the user.</li>
</ol>
",3,0,420,2015-03-25 17:59:47,https://stackoverflow.com/questions/29263097/how-to-create-a-feature-that-detect-age-in-text-in-different-languages
How can I measure string similarity between sentences?,"<p>I have the following task.</p>

<p>Given is a list of strings like so:</p>

<pre><code>        var strings = [
            'Steve jobs created the iPod when he was at Apple',
            'I really like the new Macbook by Apple',
            'Jony Ive was concerned being fired by Steve Jobs after his return to Apple',
            'The new Macbook has just one USB-C type connector',
            'I like bananas',
            'The brezels I can buy in my local store are much better than the ones in the supermarket',
            'the',
            'foo',
            'Steve'
        ];
</code></pre>

<p>I now want to compare each string with each other and for each comparison I want to find out how similar they are to each other on a scale from 0-1 (or 0%-100%).</p>

<p>So, I googled around a little and found this: <a href=""https://stackoverflow.com/questions/955110/similarity-string-comparison-in-java#answer-16018452"">Similarity String Comparison in Java</a></p>

<p>So, I followed the instruction there, and ported the method <code>similarity(String s1, String s2)</code> to JavaScript:</p>

<pre><code>        function similarity(s1, s2) {
            var longer = s1;
            var shorter = s2;
            if (s1.length &lt; s2.length) {
                longer = s2;
                shorter = s1;
            }
            var longerLength = longer.length;
            if (longerLength == 0) {
                return 1.0;
            }
            return (longerLength - longer.LevenshteinDistance(shorter)) / longerLength;
        }
</code></pre>

<p>As comparison algorithm I used Levenshtein:</p>

<pre><code>        String.prototype.LevenshteinDistance = function (s2) {
            var array = new Array(this.length + 1);
            for (var i = 0; i &lt; this.length + 1; i++)
                array[i] = new Array(s2.length + 1);

            for (var i = 0; i &lt; this.length + 1; i++)
                array[i][0] = i;
            for (var j = 0; j &lt; s2.length + 1; j++)
                array[0][j] = j;

            for (var i = 1; i &lt; this.length + 1; i++) {
                for (var j = 1; j &lt; s2.length + 1; j++) {
                    if (this[i - 1] == s2[j - 1]) array[i][j] = array[i - 1][j - 1];
                    else {
                        array[i][j] = Math.min(array[i][j - 1] + 1, array[i - 1][j] + 1);
                        array[i][j] = Math.min(array[i][j], array[i - 1][j - 1] + 1);
                    }
                }
            }
            return array[this.length][s2.length];
        };
</code></pre>

<p>So, as a test I ran a full loop comparing each string with each other and printing the result like this:</p>

<pre><code>            for (var i in strings){
                var s = strings[i];
                print('Checking string: ""' + s + '""');
                for (var j in strings){
                    print('-----');
                    var s2 = strings[j];
                    print('vs ""' + s2 + '""');
                    var sim = similarity(s, s2);
                    print('Similarity: ' + Math.round(sim*100) + '%');
                }
                print('&lt;br&gt;////// NEXT /////////////////////////////////////////////////&lt;br&gt;');
            }
</code></pre>

<p>Ok, now this is the result: <a href=""https://jsfiddle.net/wxksfa4w/"" rel=""nofollow noreferrer"">https://jsfiddle.net/wxksfa4w/</a></p>

<p>Now, looking at the results I get some good matches but also some that are completely unrelated to each other, for example:</p>

<p>""Steve jobs created the iPod when he was at Apple"" and ""I like bananas"" match for 13%?</p>

<p>""Steve jobs created the iPod when he was at Apple"" and just ""Steve"" match for just 10% although the exact same word ""Steve"" is used as in the first sentence?</p>

<p>How can I get better semantic results? Is Levenshtein the wrong algorithm? From what I understood is that Levenshtein counts the numbers of steps of how to change sentence 1 to sentence 2. So, the length of a string seems to have a major impact on the result even if there is semantic similarity.</p>

<p>Any advice?</p>
","javascript, text-mining, levenshtein-distance","<p>You should probably be using the presence of words in both sentences as a high hint of similarity. A simple way of doing it is using each sentence as a bag of words and using <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">tf-idf</a></p>
",1,6,3366,2015-03-29 02:19:14,https://stackoverflow.com/questions/29324937/how-can-i-measure-string-similarity-between-sentences
Parse text by uppercase in R,"<p>I have many large text files with the following basic composition:</p>

<pre><code>text&lt;-""this is a speech text. FIRST PERSON: hi all, thank you for coming. SECOND PERSON: thank you for inviting us""
</code></pre>

<p>As you can see, it is composed of: 1) Random text, 2) Person in uppercase, 3) Speech.</p>

<p>I've managed to separate in a list all the words using: </p>

<pre><code>textw&lt;-unlist(strsplit(text,"" ""))
</code></pre>

<p>I then find all the position of the words which are uppercase:</p>

<pre><code>grep(pattern = ""^[[:upper:]]*$"",x = textw)
</code></pre>

<p>And I have separated the names of persons into a vector;</p>

<pre><code>upperv&lt;-textw[grep(pattern = ""^[[:upper:]]*$"",x = textw)]
</code></pre>

<p>The desired outcome would be a data frame or table like this: </p>

<pre><code>Result&lt;-data.frame(person=c("" "",""FIRST PERSON"",""SECOND PERSON""),
         message=c(""this is a speech test."",""hi all, thank you for coming."",""thank you for inviting us""))

Result
         person                       message
1                      this is a speech test.
2  FIRST PERSON hi all, thank you for coming.
3 SECOND PERSON     thank you for inviting us
</code></pre>

<p>I'm having trouble ""linking"" each message to it's author. </p>

<p>Also be noted: there are uppercase words which are NOT an author, for example ""I"". How could I specify a separation only where 2 or more uppercase words are next to each other? </p>

<p>In other words, if position 2 and 3 are upper case, then place as message everything from position 4 until next occurrence of double uppercases. </p>

<p>Any help appreciated. </p>
","r, text, text-mining, uppercase","<p>Here's one approach using the <strong>stringi</strong> package:</p>

<pre><code>text &lt;- ""this is a speech text. FIRST PERSON: hi all, thank you for coming. SECOND PERSON: thank you for inviting us""

library(stringi)
txt &lt;- unlist(stri_split_regex(text, ""(?&lt;![A-Z]{2,1000})\\s+(?=[A-Z]{2,1000})""))

data.frame(
    person = stri_extract_first_regex(txt, ""[A-Z ]+(?=(:\\s))""),
    message = stri_replace_first_regex(txt, ""[A-Z ]+:\\s+"", """")
)


##          person                       message
## 1          &lt;NA&gt;        this is a speech text.
## 2  FIRST PERSON hi all, thank you for coming.
## 3 SECOND PERSON     thank you for inviting us
</code></pre>
",8,3,238,2015-03-31 00:56:49,https://stackoverflow.com/questions/29358823/parse-text-by-uppercase-in-r
Matching a list of phrases to a corpus of documents and returning phrase frequency,"<p>I have a list of phrases and a corpus of documents.There are 100k+ phrases and 60k+ documents in the corpus. The phrases are might/might not present in the corpus. I'm looking forward to find the term frequency of each phrase present in the corpus.</p>

<p>An example dataset:</p>

<pre><code>Phrases &lt;- c(""just starting"", ""several kilometers"", ""brief stroll"", ""gradually boost"", ""5 miles"", ""dark night"", ""cold morning"")
Doc1 &lt;- ""If you're just starting with workout, begin slow.""
Doc2 &lt;- ""Don't jump in brain initial and then try to operate several kilometers without the need of worked out well before.""
Doc3 &lt;- ""It is possible to end up injuring on your own and carrying out more damage than good.""
Doc4 &lt;- ""Instead start with a brief stroll and gradually boost the duration along with the speed.""
Doc5 &lt;- ""Before you know it you'll be working 5 miles without any problems.""
</code></pre>

<p>I am new to text analytics in R and have approached this problem on the lines of Tyler Rinker's solution to this <a href=""https://stackoverflow.com/questions/8996513/r-text-mining-counting-the-number-of-times-a-specific-word-appears-in-a-corpus/29392168#29392168"">R Text Mining: Counting the number of times a specific word appears in a corpus?</a>.</p>

<p>Here's my approach so far:</p>

<pre><code>library(tm)
library(qdap)
Docs &lt;- c(Doc1, Doc2, Doc3, Doc4, Doc5)
text &lt;- removeWords(Docs, stopwords(""english""))
text &lt;- removePunctuation(text)
text &lt;- tolower(text)
corp &lt;- Corpus(VectorSource(text))
Phrases &lt;- tolower(Phrases)
word.freq &lt;- apply_as_df(corp, termco_d, match.string=Phrases)
mcsv_w(word.freq, dir = NULL, open = T, sep = "", "", dataframes = NULL,
        pos = 1, envir = as.environment(pos))
</code></pre>

<p>When I'm exporting the results in csv, it is only giving me whether phrase 1 is present in any of the docs or not.</p>

<p>I'm expecting an output as below (excluding the non-matching phrases):</p>

<pre><code>Docs      Phrase1     Phrase2    Phrase3    Phrase4    Phrase5
1         0           1          2          0          0
2         1           0          0          1          0
</code></pre>
","r, text-mining, tm, word-frequency, qdap","<p>I tried with your approach and can't replicate:</p>

<p>Using:</p>

<pre><code>library(tm)
library(qdap)
Docs &lt;- c(Doc1, Doc2, Doc3, Doc4, Doc5)
text &lt;- removeWords(Docs, stopwords(""english""))
text &lt;- removePunctuation(text)
text &lt;- tolower(text)
corp &lt;- Corpus(VectorSource(text))
Phrases &lt;- tolower(Phrases)
word.freq &lt;- apply_as_df(corp, termco_d, match.string = Phrases)
mcsv_w(word.freq, dir = NULL, open = T, sep = "", "", dataframes = NULL,
        pos = 1, envir = as.environment(pos))
</code></pre>

<p>I get the following csv:</p>

<pre><code>docs    word.count  term(just starting) term(several kilometers)    term(brief stroll)  term(gradually boost)   term(5 miles)   term(dark night)    term(cold morning)
1   7   1   0   0   0   0   0   0
2   12  0   1   0   0   0   0   0
3   7   0   0   0   0   0   0   0
4   9   0   0   1   1   0   0   0
5   7   0   0   0   0   0   0   0
</code></pre>
",0,2,1821,2015-04-01 17:35:15,https://stackoverflow.com/questions/29397235/matching-a-list-of-phrases-to-a-corpus-of-documents-and-returning-phrase-frequen
R - slowly working lapply with sort on ordered factor,"<p>Based on the question <a href=""https://stackoverflow.com/questions/25330753/more-efficient-means-of-creating-a-corpus-and-dtm/25333858"">More efficient means of creating a corpus and DTM</a> I've prepared my own method for building a Term Document Matrix from a large corpus which (I hope) do not require Terms x Documents memory.</p>

<pre><code>sparseTDM &lt;- function(vc){
  id = unlist(lapply(vc, function(x){x$meta$id}))
  content = unlist(lapply(vc, function(x){x$content}))
  out = strsplit(content, ""\\s"", perl = T)
  names(out) = id
  lev.terms = sort(unique(unlist(out)))
  lev.docs = id

  v1 = lapply(
    out,
    function(x, lev) {
      sort(as.integer(factor(x, levels = lev, ordered = TRUE)))
    },
    lev = lev.terms
  )

  v2 = lapply(
    seq_along(v1),
    function(i, x, n){
      rep(i,length(x[[i]]))
    },
    x = v1,
    n = names(v1)
  )

  stm = data.frame(i = unlist(v1), j = unlist(v2)) %&gt;%
    group_by(i, j) %&gt;%
    tally() %&gt;%
    ungroup()

  tmp = simple_triplet_matrix(
    i = stm$i,
    j = stm$j,
    v = stm$n,
    nrow = length(lev.terms),
    ncol = length(lev.docs),
    dimnames = list(Terms = lev.terms, Docs = lev.docs)
  )

  as.TermDocumentMatrix(tmp, weighting = weightTf)
}
</code></pre>

<p>It slows down at calculation of <code>v1</code>. It was running for 30 minutes and I stopped it.</p>

<p>I've prepared a small example:</p>

<pre><code>b = paste0(""string"", 1:200000)
a = sample(b,80)
microbenchmark(
  lapply(
    list(a=a),
    function(x, lev) {
      sort(as.integer(factor(x, levels = lev, ordered = TRUE)))
    },
    lev = b
  )
)
</code></pre>

<p>Results are:</p>

<pre><code>Unit: milliseconds
expr      min       lq      mean   median       uq      max neval
...  25.80961 28.79981  31.59974 30.79836 33.02461 98.02512   100
</code></pre>

<p>Id and content has 126522 elements, Lev.terms has 155591 elements, so it looks that I've stopped processing too early. Since ultimately I'll be working on ~6M documents I need to ask... Is there any way to speed up this fragment of code? </p>
","r, text-mining, lapply, corpus, term-document-matrix","<p>For now I've speeded it up replacing </p>

<pre><code>sort(as.integer(factor(x, levels = lev, ordered = TRUE)))
</code></pre>

<p>with</p>

<pre><code>ind = which(lev %in% x)
cnt = as.integer(factor(x, levels = lev[ind], ordered = TRUE))
sort(ind[cnt])
</code></pre>

<p>Now timings are:</p>

<pre><code>expr      min       lq     mean   median       uq      max neval
...  5.248479 6.202161 6.892609 6.501382 7.313061 10.17205   100
</code></pre>
",1,3,370,2015-04-05 23:37:09,https://stackoverflow.com/questions/29463464/r-slowly-working-lapply-with-sort-on-ordered-factor
Configure default chain in config.toml in meta-toolkit,"<p>I want to configure a config.toml file in <a href=""https://meta-toolkit.github.io/meta/"" rel=""nofollow"">meta-toolkit</a> with default filter chain but specifying each of the filters. I plan to make modifications to some of the filters so I want to have a baseline filter chain.</p>

<p>I have tried the following attributes:</p>

<pre><code>[[analyzers]]
method = ""ngram-word""
ngram = 1
    [[analyzers.filter]]
    type = ""icu-tokenizer""

    [[analyzers.filter]]
    type = ""lowercase""

    [[analyzers.filter]]
    type = ""alpha""

    [[analyzers.filter]]
    type = ""length""  
    min = 2
    max = 35

    [[analyzers.filter]]
    type = ""list""
    filename = ""../data/lemur-stopwords.txt""

    [[analyzers.filter]]
    type = ""porter2-stemmer""

    [[analyzers.filter]]
    type = ""empty-sentence""
</code></pre>

<p>I'm getting:</p>

<blockquote>
  <p>token_stream_exception: what(): file required for list_filter config</p>
</blockquote>
","c++, search-engine, text-mining, meta-toolkit","<p>The attribute should be file instead of filename.</p>

<pre><code>[[analyzers.filter]]
type = ""list""
file = ""../data/lemur-stopwords.txt""
</code></pre>
",1,0,269,2015-04-14 10:08:46,https://stackoverflow.com/questions/29624601/configure-default-chain-in-config-toml-in-meta-toolkit
Fatal Error in tm (text mining) document term matrix creation,"<p><code>tm</code> is throwing an error when I try to create a document term matrix</p>

<pre><code>library(tm)
data(crude)

#control parameters
dtm.control &lt;- list(
    tolower           = TRUE, 
    removePunctuation = TRUE,
    removeNumbers     = TRUE,
    stopWords         = stopwords(""english""),
    stemming          = TRUE, # false for sentiment
    wordLengths       = c(3, ""inf""))

dtm &lt;- DocumentTermMatrix(corp, control = dtm.control)
</code></pre>

<p>Error:</p>

<blockquote>
  <p>Error in simple_triplet_matrix(i = i, j = j, v = as.numeric(v), nrow = length(allTerms),  : 
    'i, j, v' different lengths
  In addition: Warning messages:
  1: In mclapply(unname(content(x)), termFreq, control) :
    all scheduled cores encountered errors in user code
  2: In simple_triplet_matrix(i = i, j = j, v = as.numeric(v), nrow = length(allTerms),  :
    NAs introduced by coercion</p>
</blockquote>

<p>What am I doing wrong?
Also:</p>

<p>I am using these tutorials:</p>

<ul>
<li><a href=""http://begriffs.com/posts/2015-02-25-text-mining-in-r.html"" rel=""nofollow"">Basic Text Mining</a> </li>
<li><a href=""http://begriffs.com/posts/2015-02-25-text-mining-in-r.html"" rel=""nofollow"">Text Mining in R</a></li>
</ul>

<p>Are there better/ more recent walkthroughs?</p>
","r, text-mining, tm","<p>You might consider a few changes in your code, especially removeStopWords and creating a corpus.  Below worked for me:</p>

<pre><code>library(tm)
data(""crude"")

#control parameters
dtm.control &lt;- list(
  tolower           = TRUE, 
  removePunctuation = TRUE,
  removeNumbers     = TRUE,
  removestopWords   = TRUE,
  stemming          = TRUE, # false for sentiment
  wordLengths       = c(3, ""inf""))

corp &lt;- Corpus(VectorSource(crude))

dtm &lt;- DocumentTermMatrix(corp, control = dtm.control)

&gt; inspect(dtm)
&lt;&lt;DocumentTermMatrix (documents: 20, terms: 848)&gt;&gt;
Non-/sparse entries: 1877/15083
Sparsity           : 89%
Maximal term length: 16
Weighting          : term frequency (tf)
</code></pre>
",0,0,1432,2015-04-21 16:00:16,https://stackoverflow.com/questions/29777415/fatal-error-in-tm-text-mining-document-term-matrix-creation
Modifying corpus by inserting codewords using Python,"<p>I have about a corpus (30,000 customer reviews) in a csv file (or a txt file). This means each customer review is a line in the text file. Some examples are:</p>

<ul>
<li>This bike is amazing, but the brake is very poor</li>
<li>This ice maker works great, the price is very reasonable, some bad
smell from the ice maker</li>
<li>The food was awesome, but the water was very rude</li>
</ul>

<p>I want to change these texts to the following:</p>

<ul>
<li>This bike is amazing POSITIVE, but the brake is very poor NEGATIVE</li>
<li>This ice maker works great POSITIVE and the price is very reasonable
POSITIVE, some bad NEGATIVE smell from the ice maker</li>
<li>The food was awesome POSITIVE, but the water was very rude NEGATIVE</li>
</ul>

<p>I have two separate lists (lexicons) of positive words and negative words. For example, a text file contains such positive words as:</p>

<ul>
<li>amazing</li>
<li>great</li>
<li>awesome</li>
<li>very cool</li>
<li>reasonable</li>
<li>pretty</li>
<li>fast</li>
<li>tasty</li>
<li>kind</li>
</ul>

<p>And, a text file contains such negative words as:</p>

<ul>
<li>rude</li>
<li>poor</li>
<li>worst</li>
<li>dirty</li>
<li>slow</li>
<li>bad</li>
</ul>

<p>So, I want the Python script that reads the customer review: when any of the positive words is found, then insert ""POSITIVE"" after the positive word; when any of the negative words is found, then insert ""NEGATIVE"" after the positive word.</p>

<p>Here is the code I have tested so far. This works (see my comments in the codes below), but it needs improvement to meet my needs described above.</p>

<p>Specifically, <code>my_escaper</code> works (this code finds such words as cheap and good and replace them with cheap POSITIVE and good POSITIVE), but the problem is that I have two files (lexicons), each containing about thousand positive/negative words. So what I want is that the codes read those word lists from the lexicons, search them in the corpus, and replace those words in the corpus (for example, from ""good"" to ""good POSITIVE"", from ""bad"" to ""bad NEGATIVE"").</p>

<pre><code>#adapted from http://stackoverflow.com/questions/6116978/python-replace-multiple-strings

import re

def multiple_replacer(*key_values):
    replace_dict = dict(key_values)
    replacement_function = lambda match: replace_dict[match.group(0)]
    pattern = re.compile(""|"".join([re.escape(k) for k, v in key_values]), re.M)
    return lambda string: pattern.sub(replacement_function, string)

def multiple_replace(string, *key_values):
    return multiple_replacer(*key_values)(string)

#this my_escaper works (this code finds such words as cheap and good and replace them with cheap POSITIVE and good POSITIVE), but the problem is that I have two files (lexicons), each containing about thousand positive/negative words. So what I want is that the codes read those word lists from the lexicons, search them in the corpus, and replace those words in the corpus (for example, from ""good"" to ""good POSITIVE"", from ""bad"" to ""bad NEGATIVE"")      

my_escaper = multiple_replacer(('cheap','cheap POSITIVE'), ('good', 'good POSITIVE'), ('avoid', 'avoid NEGATIVE'))

d = []
with open(""review.txt"",""r"") as file:
    for line in file:
        review = line.strip()
        d.append(review) 

for line in d:
    print my_escaper(line) 
</code></pre>
","python, text-mining, corpus","<p>A straightforward way to code this would be to load your positive and negative words from your lexicons into separate sets.  Then, for each review, split the sentence into a list of words and look-up each word in the sentiment sets.  Checking set membership is <a href=""https://wiki.python.org/moin/TimeComplexity"" rel=""nofollow"">O(1) in the average case</a>.  Insert the sentiment label (if any) into the word list and then join to build the final string.</p>

<p>Example:</p>

<pre><code>import re

reviews = [
    ""This bike is amazing, but the brake is very poor"",
    ""This ice maker works great, the price is very reasonable, some bad smell from the ice maker"",
    ""The food was awesome, but the water was very rude""
    ]

positive_words = set(['amazing', 'great', 'awesome', 'reasonable'])
negative_words = set(['poor', 'bad', 'rude'])

for sentence in reviews:
    tagged = []
    for word in re.split('\W+', sentence):
        tagged.append(word)
        if word.lower() in positive_words:
            tagged.append(""POSITIVE"")
        elif word.lower() in negative_words:
            tagged.append(""NEGATIVE"")
    print ' '.join(tagged)
</code></pre>

<p>While this approach is straightforward, there is a downside: you lose the punctuation due to the use of <code>re.split()</code>.</p>
",1,1,217,2015-04-22 18:52:09,https://stackoverflow.com/questions/29806462/modifying-corpus-by-inserting-codewords-using-python
How to extract only person A&#39;s statements in a conversation between two persons A and B,"<p>I have a record of conversations between two arbitrary persons A and B.</p>

<pre><code>c1 &lt;- ""Person A: blabla...something Person B: blabla something else Person A: OK blabla""
c2 &lt;- ""Person A: again blabla Person B: blabla something else Person A: thanks blabla""
</code></pre>

<p>The data frame looks like this:</p>

<pre><code>df &lt;- data.frame(id = rbind(123, 345), conversation = rbind(c1, c2))

df

    id                                                                     conversation
c1 123 Person A: blabla...something Person B: blabla something else Person A: OK blabla
c2 345   Person A: again blabla Person B: blabla something else Person A: thanks blabla
</code></pre>

<p>Now I would like to extract only the part of person A and put it in a data frame. The result should be:</p>

<pre><code>   id                     person_A
1 123 blabla...something OK blabla
2 345   again blabla thanks blabla
</code></pre>
","regex, r, dataframe, text-mining, text-extraction","<p>I'm a big fan of solving this sort of problem in a way that gives you access to all the data (that includes Person B's discourse as well).  I love <strong>tidyr</strong>'s <code>extract</code> for this sort of column splitting.  I used to use a <code>do.call(rbind, strsplit()))</code> approach but love how clean the <code>extract</code> approach is.</p>

<pre><code>c1 &lt;- ""Person A: blabla...something Person B: blabla something else Person A: OK blabla""
c2 &lt;- ""Person A: again blabla Person B: blabla something else Person A: thanks blabla""
c3 &lt;- ""Person A: again blabla Person B: blabla something else""
df &lt;- data.frame(id = rbind(123, 345, 567), conversation = rbind(c1, c2, c3))


if (!require(""pacman"")) install.packages(""pacman"")
pacman::p_load(dplyr, tidyr)

conv &lt;- strsplit(as.character(df[[""conversation""]]), ""\\s+(?=Person\\s)"", perl=TRUE)

df2 &lt;- df[rep(1:nrow(df), sapply(conv, length)), ,drop=FALSE]
rownames(df2) &lt;- NULL
df2[[""conversation""]] &lt;- unlist(conv)

df2 %&gt;%
    extract(conversation, c(""Person"", ""Conversation""), ""([^:]+):\\s+(.+)"")

##    id   Person          Conversation
## 1 123 Person A    blabla...something
## 2 123 Person B blabla something else
## 3 123 Person A             OK blabla
## 4 345 Person A          again blabla
## 5 345 Person B blabla something else
## 6 345 Person A         thanks blabla
## 7 567 Person A          again blabla
## 8 567 Person B blabla something else


df2 %&gt;%
    extract(conversation, c(""Person"", ""Conversation""), ""([^:]+):\\s+(.+)"") %&gt;%
    filter(Person == ""Person A"")    

##    id   Person       Conversation
## 1 123 Person A blabla...something
## 2 123 Person A          OK blabla
## 3 345 Person A       again blabla
## 4 345 Person A      thanks blabla
## 5 567 Person A       again blabla
</code></pre>

<p>Or collapse them as you show in the desired output:</p>

<pre><code>df2 %&gt;%
    extract(conversation, c(""Person"", ""Conversation""), ""([^:]+):\\s+(.+)"") %&gt;%
    filter(Person == ""Person A"") %&gt;%
    group_by(id) %&gt;%
    select(-Person) %&gt;%
    summarise(Person_A =paste(Conversation, collapse="" ""))

##    id                     Person_A
## 1 123 blabla...something OK blabla
## 2 345   again blabla thanks blabla
## 3 567                 again blabla
</code></pre>

<p><strong>Edit</strong>:  In reality I suspect your data has real names like ""john Smith"" vs. ""Person A"".  If this is the case this initial regex split will capture a first and last name that uses caps followed by a colon:</p>

<pre><code>c1 &lt;- ""Greg Smith: blabla...something Sue Williams: blabla something else Greg Smith: OK blabla""
c2 &lt;- ""Greg Smith: again blabla Sue Williams: blabla something else Greg Smith: thanks blabla""
c3 &lt;- ""Greg Smith: again blabla Sue Williams: blabla something else""
df &lt;- data.frame(id = rbind(123, 345, 567), conversation = rbind(c1, c2, c3))r


conv &lt;- strsplit(as.character(df[[""conversation""]]), ""\\s+(?=([A-Z][a-z]+\\s+[A-Z][a-z]+:))"", perl=TRUE)

df2 &lt;- df[rep(1:nrow(df), sapply(conv, length)), ,drop=FALSE]
rownames(df2) &lt;- NULL
df2[[""conversation""]] &lt;- unlist(conv)

df2 %&gt;%
    extract(conversation, c(""Person"", ""Conversation""), ""([^:]+):\\s+(.+)"")

##    id       Person          Conversation
## 1 123   Greg Smith    blabla...something
## 2 123 Sue Williams blabla something else
## 3 123   Greg Smith             OK blabla
## 4 345   Greg Smith          again blabla
## 5 345 Sue Williams blabla something else
## 6 345   Greg Smith         thanks blabla
## 7 567   Greg Smith          again blabla
## 8 567 Sue Williams blabla something else
</code></pre>
",4,8,353,2015-04-23 08:34:58,https://stackoverflow.com/questions/29817928/how-to-extract-only-person-as-statements-in-a-conversation-between-two-persons
Text Preprocessing in Spark-Scala,"<p>I want to apply preprocessing phase on a large amount of text data in Spark-Scala such as <strong>Lemmatization - Remove Stop Words(using Tf-Idf) - POS tagging</strong> , there is any way to implement them in Spark - Scala ? </p>

<p>for example here is one sample of my data: </p>

<pre><code>The perfect fit for my iPod photo. Great sound for a great price. I use it everywhere. it is very usefulness for me.
</code></pre>

<p>after preprocessing:</p>

<pre><code>perfect fit iPod photo great sound great price use everywhere very useful
</code></pre>

<p>and they have POS tags e.g <code>(iPod,NN) (photo,NN)</code></p>

<p>there is a POS tagging (sister.arizona) is it applicable in Spark?  </p>
","scala, text, apache-spark, preprocessor, text-mining","<p>Anything is possible. The question is what YOUR preferred way of doing this would be. </p>

<p>For example, do you have a stop word dictionary that works for you (it could just simply be a Set), or would you want to run TF-IDF to automatically pick the stop words (note that this would require some supervision, such as picking the threshold at which the word would be considered a stop word). You can provide the dictionary, and Spark's MLLib <a href=""https://spark.apache.org/docs/1.3.0/mllib-feature-extraction.html#tf-idf"">already comes with TF-IDF</a>.</p>

<p>The POS tags step is tricky. Most NLP libraries on the JVM (e.g. Stanford CoreNLP) don't implement java.io.Serializable, but you can perform the map step using them, e.g.</p>

<pre><code>myRdd.map(functionToEmitPOSTags)
</code></pre>

<p>On the other hand, don't emit an RDD that contains non-serializable classes from that NLP library, since steps such as collect(), saveAsNewAPIHadoopFile, etc. will fail. Also to reduce headaches with serialization, use Kryo instead of the default Java serialization. There are numerous posts about this issue if you google around, but see <a href=""https://spark.apache.org/docs/1.2.0/tuning.html"">here</a> and <a href=""https://ogirardot.wordpress.com/2015/01/09/changing-sparks-default-java-serialization-to-kryo/"">here</a>.</p>

<p>Once you figure out the serialization issues, you need to figure out which NLP library to use to generate the POS tags. There are plenty of those, e.g. <a href=""https://github.com/stanfordnlp/CoreNLP"">Stanford CoreNLP</a>, <a href=""http://alias-i.com/lingpipe/"">LingPipe</a> and <a href=""http://mallet.cs.umass.edu/"">Mallet</a> for Java, <a href=""https://github.com/dlwh/epic"">Epic</a> for Scala, etc. Note that you can of course use the Java NLP libraries with Scala, including with wrappers such as the University of Arizona's <a href=""https://github.com/sistanlp/processors"">Sista wrapper</a> around Stanford CoreNLP, etc.</p>

<p>Also, why didn't your example lower-case the processed text? That's pretty much the first thing I would do. If you have special cases such as iPod, you could apply the lower-casing except in those cases. In general, though, I would lower-case everything. If you're removing punctuation, you should probably first split the text into sentences (split on the period using regex, etc.). If you're removing punctuation in general, that can of course be done using regex.</p>

<p>How deeply do you want to stem? For example, the Porter stemmer (there are implementations in every NLP library) stems so deeply that ""universe"" and ""university"" become the same resulting stem. Do you really want that? There are less aggressive stemmers out there, depending on your use case. Also, why use stemming if you can use lemmatization, i.e. splitting the word into the grammatical prefix, root and suffix (e.g. walked = walk (root) + ed (suffix)). The roots would then give you better results than stems in most cases. Most NLP libraries that I mentioned above do that.</p>

<p>Also, what's your distinction between a stop word and a non-useful word? For example, you removed the pronoun in the subject form ""I"" and the possessive form ""my,"" but not the object form ""me."" I recommend picking up an NLP textbook like ""Speech and Language Processing"" by Jurafsky and Martin (for the ambitious), or just reading the one of the engineering-centered books about NLP tools such as <a href=""https://www.packtpub.com/application-development/natural-language-processing-java"">LingPipe</a> for Java, <a href=""https://www.packtpub.com/application-development/python-3-text-processing-nltk-3-cookbook"">NLTK</a> for Python, etc., to get a good overview of the terminology, the steps in an NLP pipeline, etc.</p>
",12,-2,5834,2015-04-28 12:39:22,https://stackoverflow.com/questions/29919779/text-preprocessing-in-spark-scala
Authorship Attribution using Machine Learning,"<p>I am working on a practical machine learning problem as an exercise. I just need help formulating my problem.</p>

<p>I have text from 20 books of a famous old Author. there are 5 more books that has been debated throughout history if the belong to the same author or not. </p>

<p>I am thinking about the best way to represent this problem. I am thinking of using a bag-of-words appoach to find the most significant words used by the author. </p>

<p>Should I treat it as a Naive Bayes (Spam/Ham) problem, or should I use KNN classification (Author/non-author) to detect the class of each document. Is there another way of doing it?</p>
","machine-learning, classification, text-mining, text-classification, document-classification","<p>I think Naive Bayes can give you insights. One more way can be , find out features which separate such books ex<br>
1. Complexity of words , some writers are easy to understand and use common words , i am hinting towards IDF (Inverse document frequency)<br>
2. Some words may not not even exist at his time like ""selfie"" , ""mobile"" etc.  </p>

<p>Try to find a lot of features like that and can also train a discriminative classifier. </p>
",0,-2,119,2015-04-28 15:47:09,https://stackoverflow.com/questions/29923983/authorship-attribution-using-machine-learning
what methods are there to classify documents?,"<p>I am trying to do document classification. But I am really confused between feature selections and tf-idf. Are they the same or two different ways of doing classification?</p>

<p>Hope somebody can tell me? I am not really sure that my question will make sense to you guys.</p>
","machine-learning, classification, text-mining, tf-idf, feature-selection","<p>Yes, you are confusion a lot of things.</p>

<ul>
<li><p>Feature selection is the abstract term for <em>choosing</em> features (0 or 1). Stopword removal can be seen as feature selection.</p></li>
<li><p>TF is <em>one</em> method of <em>extracting</em> features from text: counting words.</p></li>
<li><p>IDF is <em>one</em> method of assigning weights to features.</p></li>
</ul>

<p>Neither of them is classification... they are <em>popular</em> for text classification, but they are even more popular for information retrieval, which is not classification...</p>

<p>However, many classifiers work on numeric data, so the common process is to 1. Extract features (e.g.: TF) 2. Select features (e.g. remove stopwords) 3. Weight features (e.g. IDF) 4. Train a classifier on the resulting numerical vectors. 5. Predict the classes of new/unlabeled documents.</p>
",2,-3,271,2015-05-05 15:44:39,https://stackoverflow.com/questions/30057545/what-methods-are-there-to-classify-documents
Text mining and NLP: from R to Python,"<p>First of all, saying that I am new to python. At the moment, I am ""translating"" a lot of R code into python and learning along the way. This question relates to this one <a href=""https://stackoverflow.com/questions/22797393/exactly-replicating-r-text-preprocessing-in-python"">replicating R in Python</a> (in there they actually suggest to wrap it up using <code>rpy2</code>, which I would like to avoid for learning purposes). </p>

<p>In my case, rather than exactly replicating R in python, I would actually like to learn a ""pythonian"" way of doing what I am describing here:</p>

<p>I have a long vector (40000 elements) in which each element is a piece of text, for example:</p>

<pre><code>&gt; descr
[1] ""dress Silver Grey Printed Jersey Dress 100% cotton""
[2] ""dress Printed Silk Dress 100% Silk Effortless style.""                                                                                                                                                                                    
[3] ""dress Rust Belted Kimono Dress 100% Silk relaxed silhouette, mini length"" 
</code></pre>

<p>I then preprocess it as, for example: </p>

<pre><code># customized function to remove patterns in strings. used later within tm_map
rmRepeatPatterns &lt;- function(str) gsub('\\b(\\S+?)\\1\\S*\\b', '', str,
                                   perl = TRUE)

# process the corpus
pCorp &lt;- Corpus(VectorSource(descr))
pCorp &lt;- tm_map(pCorp, content_transformer(tolower))
pCorp &lt;- tm_map(pCorp, rmRepeatPatterns)
pCorp &lt;- tm_map(pCorp, removeStopWords)
pCorp &lt;- tm_map(pCorp, removePunctuation)
pCorp &lt;- tm_map(pCorp, removeNumbers)
pCorp &lt;- tm_map(pCorp, stripWhitespace)
pCorp &lt;- tm_map(pCorp, PlainTextDocument)

# create a term document matrix (control functions can also be passed here) and a table: word - freq
Tdm1 &lt;- TermDocumentMatrix(pCorp)
freq1 &lt;- rowSums(as.matrix(Tdm1))
dt &lt;- data.table(terms=names(freq1), freq=freq1)

# and perhaps even calculate a distance matrix (transpose because Dist operates on a row basis)
D &lt;- Dist(t(as.matrix(Tdm1)))
</code></pre>

<p>Overall, I would like to know an adequate way of doing this in python, mainly the text processing. </p>

<p>For example, I could remove stopwords and numbers as they describe here <a href=""https://stackoverflow.com/questions/5541745/get-rid-of-stopwords-and-punctuation"">get rid of StopWords and Numbers</a> (although seems a lot of work for such a simple task). But all the options I see imply processing the text itself rather than mapping the whole corpus. In other words, they imply ""looping"" through the <code>descr</code> vector. </p>

<p>Anyway, any help would be really appreciated. Also, I have a bunch of customised functions like <code>rmRepeatPatterns</code>, so learning how to map these would be extremely useful. </p>

<p>thanks in advance for your time. </p>
","python, r, nltk, text-mining, tm","<p>Looks like ""doing this"" involves making some regexp substitutions to a list of strings. Python offers a lot more power than R in this domain. Here's how I'd apply your <code>rmRepeatedPatterns</code> substitution, using a <em>list comprehension</em>:</p>

<pre><code>pCorp = [ re.sub(r'\b(\S+?)\1\S*\b', '', line) for line in pCorp ]
</code></pre>

<p>If you wish to wrap this in a function:</p>

<pre><code>def rmRepeatedPatterns(line):
    return re.sub(r'\b(\S+?)\1\S*\b', '', line)

pCorp = [ rmRepeatedPatterns(line) for line in pCorp ]
</code></pre>

<p>Python also has a <code>map</code> operator that you could use with your function:</p>

<pre><code>pCorp = map(rmRepeatedPatterns, pCorp)
</code></pre>

<p>But list comprehensions are more powerful, expressive and flexible; as you see you can apply simple substitutions without burying them in a function. </p>

<p>Additional notes:</p>

<ol>
<li><p>If your datasets are large, you can also learn about using <em>generators</em> instead of list comprehensions; essentially they let you generate your elements on demand, instead of creating a lot of intermediate lists.</p></li>
<li><p>Python has some operators like <code>map</code>, but if you'll be doing a lot of matrix manipulations you should read about <code>numpy</code>, which offers a more R-like experience.</p></li>
</ol>

<p><strong>Edit:</strong> Having looked again at your sample R script, here's how I'd do the rest of the clean-up, ie. take your list of lines, convert to lower case, drop punctuation and digits (specifically: everything that's not an English letter), and remove stopwords. </p>

<pre><code># Lower-case, split into words, discard everything that's not a letter
tok_lines = [ re.split(r""[^a-z]+"", line.lower()) for line in pCorp ]
# tok_lines is now a list of lists of words

stopwordlist = nltk.corpus.stopwords.words(""english"") # or any other list
stopwords = set(w.lower() for w in stopwordlist)
cleantoks = [ [ t for t in line if t not in stopwords ] 
                                        for line in tok_lines ]
</code></pre>

<p>I wouldn't advise using either of the proposed solutions in the <a href=""https://stackoverflow.com/questions/5541745/get-rid-of-stopwords-and-punctuation"">question you link to</a>. Looking up things in a set is <em>a lot</em> faster than looking them up in a large list, and I would use a comprehension instead of <code>filter()</code>.</p>
",1,2,1431,2015-05-06 18:36:14,https://stackoverflow.com/questions/30084933/text-mining-and-nlp-from-r-to-python
Efficient way to Split text data in R,"<p>I am working on text mining,
Lets say my data set has the column having the Text data posted in twitter.
e.g
@john Its a fantastic work@lita checkout this is amazing @Amy great App</p>

<p>I want to check to split this to   @john Its a fantastic work, @lita checkout this is amazing, @Amy great App</p>

<p>then i want to see who has posted originally and who has re-posted. </p>

<p>P.S: I am facing another problem while installing the 'sna' package in R, as it showing no such package. </p>
","regex, r, twitter, split, text-mining","<p>You can try</p>

<pre><code> strsplit(str1, '(?&lt;=[^@]) ?(?=@)', perl=TRUE)[[1]]
#[1] ""@john Its a fantastic work""     ""@lita checkout this is amazing""
#[3] ""@Amy great App""        
</code></pre>

<h3>data</h3>

<pre><code>str1 &lt;-  ""@john Its a fantastic work@lita checkout this is amazing @Amy great App""
</code></pre>
",4,1,102,2015-05-07 08:24:03,https://stackoverflow.com/questions/30095618/efficient-way-to-split-text-data-in-r
R Text Mining and Random Forest,"<p>I am working on a data set that has a bunch of raw text that I am vectorizing and using in my matrix for a random forest regression. My question is, should I be treating each word as a .factor or a .numeric if it is a sparse matrix? Which one speed up the computation time?</p>
","r, text-mining","<p>My understanding is that R matrices coerce factors to characters, so you're better off using numeric.</p>

<p>I'm not terribly familiar with RandomForest -- I have a general idea of what it does, but I'm not sure about the guts of its R implementation. If you need to give it a design matrix (for instance, how ANOVAs or GLMs work when you implement them by hand), you can try using the <code>model.matrix</code> function.</p>
",0,0,685,2015-05-09 15:11:29,https://stackoverflow.com/questions/30141582/r-text-mining-and-random-forest
Text analysis : What after term-document matrix?,"<p>I am trying to build predictive models from text data. I built document-term matrix from the text data (unigram and bigram) and built different types of models on that (like svm, random forest, nearest neighbor etc). All the techniques gave decent results, but I want to improve the results. I tried tuning the models by changing parameters, but that doesn't seem to improve the performance much. What are the possible next steps for me?</p>
","r, machine-learning, nlp, svm, text-mining","<p>This isn't really a programming question, but anyway:</p>

<p>If your goal is prediction, as opposed to text classification, usual methods are backoff models (<a href=""http://www.usna.edu/Users/cs/nchamber/courses/nlp/f12/slides/set4-smoothing.pdf"" rel=""nofollow"">Katz Backoff</a>) and interpolation/smoothing, e.g. <a href=""http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf"" rel=""nofollow"">Kneser-Ney smoothing</a>. </p>

<p>More complicated models like Random Forests are AFAIK not absolutely necessary and may pose problems if you need to make predictions quickly. If you are using an interpolation model, you can still tune the model parameters (lambda) using a held out portion of the data.</p>

<p>Finally, I agree with NEO on the reading part and would recommend ""Speech and Language Processing"" by Jurafsky and Martin.</p>
",1,0,469,2015-05-14 11:41:15,https://stackoverflow.com/questions/30236472/text-analysis-what-after-term-document-matrix
What is the meaning of &#39;cut-off&#39; and &#39;iteration&#39; for trainings in OpenNLP?,"<p>what is the meaning of <code>cut-off</code> and <code>iteration</code> for training in OpenNLP? or for that matter natural language processing. I need just a layman explanation of these terms. As far as I think, iteration is the number of times the algorithm is repeated and cut off is a value such that if a text has value above this cut off for some specific category it will get mapped to that category. Am I right?</p>
","text-mining, opennlp","<p>Correct, the term <em>iteration</em> refers to the general notion of <em>iterative algorithms</em>, where one sets out to solve a problem by successively producing (hopefully increasingly more accurate) approximations of some ""ideal"" solution. Generally speaking, the more iterations, the more accurate (""better"") the result will be, but of course the more computational steps have to be carried out.</p>

<p>The term <em>cutoff</em> (aka <em>cutoff frequency</em>) is used to designate a method of reducing the size of <em>n-gram language models</em> (as used by OpenNLP, e.g. its part-of-speech tagger). Consider the following example:</p>

<pre><code>Sentence 1 = ""The cat likes mice.""
Sentence 2 = ""The cat likes fish.""
Bigram model = {""the cat"" : 2, ""cat likes"" : 2, ""likes mice"" : 1, ""likes fish"" : 1}
</code></pre>

<p>If you set the cutoff frequency to 1 for this example, the n-gram model would be reduced to</p>

<pre><code>Bigram model = {""the cat"" : 2, ""cat likes"" : 2}
</code></pre>

<p>That is, the cutoff method removes from the language model  those n-grams that occur infrequently in the training data. Reducing the size of n-gram language models is sometimes necessary, as the number of even bigrams (let alone trigrams, 4-grams, etc.) explodes for larger corpora. The remaning information (n-gram counts) can then be used to statistically estimate the probability  of  a  word (or its POS tag) given  the
(n-1)
previous
words (or POS tags). </p>
",15,9,3068,2015-05-14 12:57:52,https://stackoverflow.com/questions/30238014/what-is-the-meaning-of-cut-off-and-iteration-for-trainings-in-opennlp
Text Mining Cleanup with Ruby &amp; Regex,"<p>I have a word count hash, something as the following:</p>

<pre><code>words = {
  ""love""   =&gt; 10,
  ""hate""   =&gt; 12,
  ""lovely"" =&gt; 3,
  ""loving"" =&gt; 2,
  ""loved""  =&gt; 1, 
  ""peace""  =&gt; 14,
  ""thanks"" =&gt; 3,
  ""wonderful"" =&gt; 10,
  ""grateful"" =&gt; 10
  # there are more but you get the idea
}
</code></pre>

<p>I want to make sure that ""love"", ""loved"" &amp; ""loving"" are all counted as ""love"". So I am adding all their counts together to be the count for ""love"", and removing the rest of the variation of ""love"". However, at the same time, I don't want ""lovely"" to be counted as ""love"", so I am preserving it as it is. </p>

<p>So I'll get something like this in the end.</p>

<pre><code>words = [
  ""love""   =&gt; 13,
  ""hate""   =&gt; 12,
  ""lovely"" =&gt; 3,
  ""peace""  =&gt; 14,
  ""thanks"" =&gt; 3,
  ""wonderful"" =&gt; 10,
  ""grateful"" =&gt; 10
  # there are more but you get the idea
]
</code></pre>

<p>I have some code that sort of works, but I think the logic of the last line is really wrong. I wonder if you can help me fix this or suggest a better way of doing this. </p>

<pre><code>words.select { |k| /\Alov[a-z]*/.match(k) }
words[""love""] = purgedWordCount.select { |k| /\Alov[a-z]*/.match(k) }.map(&amp;:last).reduce(:+) - 1 # that 1 is for the 1 for ""lovely""; I tried not to hard code it by using words[""lovely""], but it messed things up completely, so I had to do this. 
words.delete_if { |k| /\Alov[a-z]*/.match(k) &amp;&amp; k != ""love"" &amp;&amp; k != ""lovely"" }
</code></pre>

<p>Thanks!</p>
","ruby-on-rails, ruby, regex, text, text-mining","<pre><code>words = {
  ""love""   =&gt; 10,
  ""hate""   =&gt; 12,
  ""lovely"" =&gt; 3,
  ""loving"" =&gt; 2,
  ""loved""  =&gt; 1,
  ""peace""  =&gt; 14,
  ""thanks"" =&gt; 3,
  ""wonderful"" =&gt; 10,
  ""grateful"" =&gt; 10
  # there are more but you get the idea
}

aggregated_words = words.inject({}) do |memo, (word, count)|
  key = word =~ /\Alov.+/ &amp;&amp; word != ""lovely"" ? ""love"" : word
  memo[key] = memo[key].to_i + count
  memo
end

&gt; {""love""=&gt;13, ""hate""=&gt;12, ""lovely""=&gt;3, ""peace""=&gt;14, ""thanks""=&gt;3, ""wonderful""=&gt;10, ""grateful""=&gt;10}
</code></pre>
",0,0,198,2015-05-14 15:58:19,https://stackoverflow.com/questions/30241836/text-mining-cleanup-with-ruby-regex
compare TypedDependencies from Stanford NLP dependency parser tree,"<p>I am trying a semantic match between two sentences by comparing the dependencies.<br>
I am getting two Stanford dependency trees from two different sentences. I want to compare and get a score for the semantic match between the sentences.<br></p>

<pre><code>for(TypedDependency td1 : dependencyList1)
    {
        for(TypedDependency td2 : dependencyList2)
        {
            score = td1.compareTo(td2);
        }
    }
</code></pre>

<p><code>dependencyList1</code> and <code>dependencyList2</code> are the list of all dependencies from sentences1 and sentence 2 respectively.
  I am using a <code>compareTo</code> function which gives out scores of <code>-1,0,1</code>.<br>
I then average out the scores to come up with a final score.<br>
I don't know how these scores are calculated.<br> Is there a better way to compare and identify similar dependencies.<br>
Any help would be appreciated.</p>
","nlp, stanford-nlp, text-mining","<p><code>compareTo()</code> gives you an ordering between dependencies, e.g., for sorting (see <a href=""https://docs.oracle.com/javase/7/docs/api/java/lang/Comparable.html"" rel=""nofollow"">https://docs.oracle.com/javase/7/docs/api/java/lang/Comparable.html</a>). To find similar dependencies, you first need to formalize exactly what you mean by ""similar"", and then make a custom scoring function.</p>

<p>A natural metric, beyond simple equality, is collapsing things like <code>*subj</code> (nsubj, nsubjpass, csubj, csubjpass) and <code>*obj</code> (dobj, iobj). If you care about the endpoints of the arcs, checking for lemma match rather than word match is maybe a good start. Similarity in vector space (e.g., with word2vec or GloVE) is also quite effective.</p>

<p>The list of dependencies, for reference, can be found at: <a href=""http://universaldependencies.github.io/docs/u/dep/index.html"" rel=""nofollow"">http://universaldependencies.github.io/docs/u/dep/index.html</a></p>
",4,0,673,2015-05-28 05:19:30,https://stackoverflow.com/questions/30497786/compare-typeddependencies-from-stanford-nlp-dependency-parser-tree
How to convert raw input into index value in R,"<p>Thanks for help first.
I have a raw input file ""<code>foo.txt</code>"", and another <code>dictionary</code> file ""<code>dic.csv</code>"", where each row in dic file is a <code>key-value</code> pair, the key is one character and the value is another character.</p>

<p>maybe like this, <strong>foo.txt</strong>:</p>

<pre><code>abcd
dcba
aaaa
</code></pre>

<p>and <strong>dic.csv</strong>:</p>

<pre><code>a 1
b 2
c 3
d 4
</code></pre>

<p>I want to convert the ""foo.txt"" to the values in the dic file according to the key they matched, such that in result of:</p>

<pre><code>1234
4321
1111
</code></pre>

<p>Are there any efficient way to do this?</p>

<p>Thanks!</p>
","r, dictionary, text-mining","<p>Try</p>

<pre><code>chartr(paste(d1$Col1, collapse=''), paste(d1$Col2, collapse=""""), v1)
#[1] ""1234"" ""4321"" ""1111""
</code></pre>

<p>Or instead of using two <code>paste</code>, we can loop over with <code>sapply</code></p>

<pre><code>v2 &lt;- sapply(d1, paste, collapse='')
chartr(v2[1], v2[2], v1)
#[1] ""1234"" ""4321"" ""1111""
</code></pre>

<p>Or using <code>mgsub</code> from <code>qdap</code></p>

<pre><code>library(qdap)
mgsub(d1$Col1, d1$Col2, v1)
#[1] ""1234"" ""4321"" ""1111""
</code></pre>

<p>Or</p>

<pre><code>library(gsubfn)
gsubfn('[abcd]', list(a=1, b=2, c=3, d=4), v1)
#[1] ""1234"" ""4321"" ""1111""
</code></pre>

<h3>data</h3>

<pre><code> v1 &lt;- c('abcd', 'dcba', 'aaaa')
 d1 &lt;- data.frame(Col1= letters[1:4], Col2=1:4, stringsAsFactors=FALSE)
</code></pre>
",4,3,417,2015-05-29 06:35:20,https://stackoverflow.com/questions/30522687/how-to-convert-raw-input-into-index-value-in-r
Text Mining - removePunctuation not removing quotes and dashes,"<p>I have been doing some text mining. I created the DTM matrix using the 
following steps. </p>

<pre><code>corpus1&lt;-VCorpus(VectorSource(resume1$Dat1)) 

corpus1&lt;-tm_map(corpus1,content_transformer(tolower)) 
corpus1&lt;-tm_map(corpus1,content_transformer(trimWhiteSpace))

dtm&lt;-DocumentTermMatrix(corpus1, 
                           control = list(removePunctuation = TRUE, 
                                          removeNumbers = TRUE, 
                                          removeSparseTerms=TRUE, 
                                            stopwords = TRUE)) 
</code></pre>

<p>​After all the run I am still getting words like -quotation, ""fun, model""​ 
, etc in dtm.Also getting blanks like ""        "" in the data</p>

<p>What can I do about it? I do not need this dahses and extra quotations. </p>
","r, text-mining, tm","<p>I'm not sure why DocumentTermMatrix isn't working for you, but you could try using tm_map to pre-process the corpus before transforming it into a dtm. This works for me (Note that I reorder the default stoplist because otherwise it removes the stems of apostrophe words before the entire word, leaving stranded 's'):</p>

<pre><code>corpus1 &lt;- VCorpus(VectorSource(resume1$dat))

reorder.stoplist &lt;- c(grep(""[']"", stopwords('english'), value = TRUE), 
                      stopwords('english')[!(1:length(stopwords('english')) %in% grep(""[']"", stopwords('english')))])

corpus1 &lt;- tm_map(corpus1, content_transformer(tolower))
corpus1 &lt;- tm_map(corpus1, removeWords, reorder.stoplist)
corpus1 &lt;- tm_map(corpus1, removePunctuation)
corpus1 &lt;- tm_map(corpus1, removeNumbers)
corpus1 &lt;- tm_map(corpus1, stripWhitespace)

corpus1 &lt;- DocumentTermMatrix(corpus1)
</code></pre>
",2,3,1721,2015-06-08 07:19:10,https://stackoverflow.com/questions/30703215/text-mining-removepunctuation-not-removing-quotes-and-dashes
LDA with tm package in R using bigrams,"<p>I have a csv with every row as a document. I need to perform LDA upon this. I have the following code :</p>

<pre><code>library(tm)
library(SnowballC)
library(topicmodels)
library(RWeka)

X = read.csv('doc.csv',sep="","",quote=""\"""",stringsAsFactors=FALSE)

corpus &lt;- Corpus(VectorSource(X))
corpus &lt;- tm_map(tm_map(tm_map(corpus, stripWhitespace), tolower), stemDocument)
corpus &lt;- tm_map(corpus, PlainTextDocument)
BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
dtm &lt;- DocumentTermMatrix(corpus, control = list(tokenize=BigramTokenizer,weighting=weightTfIdf))
</code></pre>

<p>At this point checking the dtm object gives</p>

<pre><code>&lt;&lt;DocumentTermMatrix (documents: 52, terms: 477)&gt;&gt;
Non-/sparse entries: 492/24312
Sparsity           : 98%
Maximal term length: 20
Weighting          : term frequency - inverse document frequency (normalized) (tf-idf)
</code></pre>

<p>Now I proceed to perform LDA upon this</p>

<pre><code>rowTotals &lt;- apply(dtm , 1, sum) 
dtm.new   &lt;- dtm[rowTotals&gt; 0, ]
g = LDA(dtm.new,10,method = 'VEM',control=NULL,model=NULL)
</code></pre>

<p>I get the following error</p>

<pre><code>Error in LDA(dtm.new, 10, method = ""VEM"", control = NULL, model = NULL) : 
  The DocumentTermMatrix needs to have a term frequency weighting
</code></pre>

<p>The Document Term matrix was clearly weighted. What am I doing wrong ?</p>

<p>Kindly Help.</p>
","r, text-mining, tm, tf-idf, lda","<p>The Document Term matrix needs to have a term frequency weighting:</p>

<pre><code>DocumentTermMatrix(corpus, 
                   control = list(tokenize = BigramTokenizer, 
                             weighting = weightTf))
</code></pre>
",1,0,1804,2015-06-11 06:24:34,https://stackoverflow.com/questions/30773168/lda-with-tm-package-in-r-using-bigrams
Save and reuse TfidfVectorizer in scikit learn,"<p>I am using TfidfVectorizer in scikit learn to create a matrix from text data. Now I need to save this object to reuse it later. I tried to use pickle, but it gave the following error.</p>
<pre class=""lang-none prettyprint-override""><code>loc=open('vectorizer.obj','w')
pickle.dump(self.vectorizer,loc)
*** TypeError: can't pickle instancemethod objects
</code></pre>
<p>I tried using <code>joblib</code> in sklearn.externals, which again gave similar error. Is there any way to save this object so that I can reuse it later?</p>
<p>Here is my full object:</p>
<pre class=""lang-py prettyprint-override""><code>class changeToMatrix(object):
    def __init__(self,ngram_range=(1,1),tokenizer=StemTokenizer()):
        from sklearn.feature_extraction.text import TfidfVectorizer
        self.vectorizer = TfidfVectorizer(ngram_range=ngram_range,analyzer='word',lowercase=True,
                                          token_pattern='[a-zA-Z0-9]+',strip_accents='unicode',
                                          tokenizer=tokenizer)

    def load_ref_text(self,text_file):
        textfile = open(text_file,'r')
        lines = textfile.readlines()
        textfile.close()
        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
        sentences = [item.strip().strip('.') for item in sent_tokenizer.tokenize(' '.join(lines).strip())]
        #vectorizer is transformed in this step
        chk2 = pd.DataFrame(self.vectorizer.fit_transform(sentences1).toarray())
        return sentences, [chk2]

    def get_processed_data(self,data_loc):
        ref_sentences,ref_dataframes=self.load_ref_text(data_loc)
        loc = open(&quot;indexedData/vectorizer.obj&quot;,&quot;w&quot;)
        pickle.dump(self.vectorizer,loc) #getting error here
        loc.close()
        return ref_sentences, ref_dataframes
</code></pre>
","python, nlp, scikit-learn, pickle, text-mining","<p>Firstly, it's better to leave the import at the top of your code instead of within your class:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
class changeToMatrix(object):
  def __init__(self,ngram_range=(1,1),tokenizer=StemTokenizer()):
    ...
</code></pre>

<p>Next <code>StemTokenizer</code> don't seem to be a canonical class. Possibly you've got it from <a href=""http://sahandsaba.com/visualizing-philosophers-and-scientists-by-the-words-they-used-with-d3js-and-python.html"" rel=""noreferrer"">http://sahandsaba.com/visualizing-philosophers-and-scientists-by-the-words-they-used-with-d3js-and-python.html</a> or maybe somewhere else so <strong><em>we'll assume it returns a list of strings</em></strong>.</p>

<pre><code>class StemTokenizer(object):
    def __init__(self):
        self.ignore_set = {'footnote', 'nietzsche', 'plato', 'mr.'}

    def __call__(self, doc):
        words = []
        for word in word_tokenize(doc):
            word = word.lower()
            w = wn.morphy(word)
            if w and len(w) &gt; 1 and w not in self.ignore_set:
                words.append(w)
        return words
</code></pre>

<p>Now to answer your actual question, it's possible that you need to open a file in byte mode before dumping a pickle, i.e.:</p>

<pre><code>&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; import cPickle as pickle
&gt;&gt;&gt; vectorizer = TfidfVectorizer(ngram_range=(0,2),analyzer='word',lowercase=True, token_pattern='[a-zA-Z0-9]+',strip_accents='unicode',tokenizer=word_tokenize)
&gt;&gt;&gt; vectorizer
TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',
        dtype=&lt;type 'numpy.int64'&gt;, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(0, 2), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents='unicode', sublinear_tf=False,
        token_pattern='[a-zA-Z0-9]+',
        tokenizer=&lt;function word_tokenize at 0x7f5ea68e88c0&gt;, use_idf=True,
        vocabulary=None)
&gt;&gt;&gt; with open('vectorizer.pk', 'wb') as fin:
...     pickle.dump(vectorizer, fin)
... 
&gt;&gt;&gt; exit()
alvas@ubi:~$ ls -lah vectorizer.pk 
-rw-rw-r-- 1 alvas alvas 763 Jun 15 14:18 vectorizer.pk
</code></pre>

<p><strong>Note</strong>: Using the <code>with</code> idiom for i/o file access automatically closes the file once you get out of the <code>with</code> scope.</p>

<p>Regarding the issue with <code>SnowballStemmer()</code>, note that <code>SnowballStemmer('english')</code> is an object while the stemming function is <code>SnowballStemmer('english').stem</code>. </p>

<p><strong>IMPORTANT</strong>:</p>

<ul>
<li><code>TfidfVectorizer</code>'s tokenizer parameter expects to take a string and return a list of string</li>
<li>But Snowball stemmer does not take a string as input and return a list of string.</li>
</ul>

<p>So you will need to do this:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import SnowballStemmer
&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; stemmer = SnowballStemmer('english').stem
&gt;&gt;&gt; def stem_tokenize(text):
...     return [stemmer(i) for i in word_tokenize(text)]
... 
&gt;&gt;&gt; vectorizer = TfidfVectorizer(ngram_range=(0,2),analyzer='word',lowercase=True, token_pattern='[a-zA-Z0-9]+',strip_accents='unicode',tokenizer=stem_tokenize)
&gt;&gt;&gt; with open('vectorizer.pk', 'wb') as fin:
...     pickle.dump(vectorizer, fin)
...
&gt;&gt;&gt; exit()
alvas@ubi:~$ ls -lah vectorizer.pk 
-rw-rw-r-- 1 alvas alvas 758 Jun 15 15:55 vectorizer.pk
</code></pre>
",17,28,43918,2015-06-15 10:35:38,https://stackoverflow.com/questions/30843011/save-and-reuse-tfidfvectorizer-in-scikit-learn
How do I extract certain words in my document into a dataframe in R?,"<p>EDIT: Reproducible example (I hope I'm doing it right):</p>

<p>I went ahead and used <code>as.character(docs[1])</code> to create a string, for reproducibility: </p>

<p><code>""list(list(content = c(\""Name: Birthdate (MM/DD): Print Date: Student ID: Institution ID: Page:\"", \""\"", \""MyName MyBirthday 06/16/2015 N1111111 002785 1 of 1\"", \""\"", \""A string I don't want\"", \""\"", \""More stuff I don't want\"", \""Don't want\"", \""\"", \""Names of Classes\"", \n\""\"", \""Class numbers and sections that I don't want\"", \""\"", \""Current Cumulative\"", \""\"", \""AHRS (don't want)\"", \""12.0 12.0 (no)\"", \""\"", \""EHRS (WANT THIS)\"", \""12.0 12.0\"", \""\"", \""QHRS (no)\"", \""12.0 12.0\"", \""\"", \""QPTS (no) \"", \"" (no) 45.900 45.900\"", \""\"", \""GPA\"", \""3.825 3.825\"", \""\"", \""Spring 2015\"", ""etc"", \""\"",  \""End of Graduate Record\"", \""\"", \""\\f\""), meta = list(author = NULL, datetimestamp = NULL, description = NULL, heading = NULL, id = \""Unofficial June 2015 copy 2.pdf\"", language = \""en\"", origin = NULL)))""</code></p>

<p>All I want out of this mess is the ID Number (which is N1111111 in this example), the semester (Fall 2014 and Spring 2015), the numbers following EHRS (12.0 12.0, each in its own column), and the numbers following GPA (3.825 3.825, each in its own column).</p>

<hr>

<p>I have text data from academic transcripts that needs to put into a dataframe for analysis. I have converted the transcript pdf into text, but now I need certain information in a dataframe. Specifically, I need data in the following columns:</p>

<p>Student ID, Fall 1 Current Hours, Fall 1 Cumulative Hours, Fall 1 Current GPA, Spring 1 Current Hours, Spring 1 Cumulative Hours, Spring 1 Current GPA, Spring 1 Cumulative GPA, Summer 1 Current Hours, Summer 1 Cumulative Hours, Summer 1 Current GPA, Summer 1 Cumulative GPA</p>

<p>etc, for every semester the student remains at the university.</p>

<p>The number of hours comes from EHRS, and cases where no summer courses are listed are treated as 0 current hours, 0 current gpa, and cumulative hours and gpa are the same as for the spring immediately preceding it.</p>

<p>So far, I've converted the pdf to text using the tm library and have the following example transcript:</p>

<p><code>docs &lt;- Corpus(DirSource(cname), readerControl=list(reader=readPDF()))</code></p>

<p><code>inspect(docs[1])</code></p>

<p>Student Name MM/YY 06/16/2015 N11111111 002785 1 of 1</p>

<p>Name of University Beginning of Graduate Record</p>

<p>Fall 2014 Name of School
Master of Science Major: Major</p>

<p>Name of Class 1 Name of Class 2 Name of Class 3 Name of Class 4</p>

<p>COURSE+SECTION 3.0 B+ COURSE+SECTION 3.0 A COURSE+SECTION 3.0 A COURSE+SECTION 3.0 A</p>

<p>Current Cumulative</p>

<p>AHRS
12.0 12.0</p>

<p>EHRS
12.0 12.0</p>

<p>QHRS
12.0 12.0</p>

<p>QPTS
45.900 45.900</p>

<p>GPA
3.825 3.825</p>

<p>Spring 2015</p>

<p>Name of School
Master of Science Major: Major</p>

<p>Name of Class 1 Name of Class 2 Name of Class 3</p>

<p>COURSE+SECTION 2.0 A COURSE+SECTION 2.0 A COURSE+SECTION 2.0 A-</p>

<p>Name of Class 4 COURSE+SECTION 2.0 A</p>

<p>Name of Class 5</p>

<p>COURSE+SECTION 2.0 A-</p>

<p>Name of Class 6 COURSE+SECTION 4.0 A</p>

<p>Name of Class 7</p>

<p>COURSE+SECTION 3.0 B+</p>

<p>Name of Class 8</p>

<p>COURSE+SECTION</p>

<p>3.0 A</p>

<p>Current Cumulative</p>

<p>AHRS
20.0 32.0</p>

<p>EHRS
20.0 32.0</p>

<p>QHRS
20.0 32.0</p>

<p>QPTS
76.700 122.600</p>

<p>GPA
3.835 3.831</p>

<p>End of Graduate Record</p>
","r, text-mining","<p>This is a strategy I use when documents look alike. If documents are EXACTLY the same. You can skip most of the grep() and use direct references (ie, txt[1]) to the location of the information you want to parse. </p>

<p>Extraction Tactics: </p>

<ul>
<li>Use <code>grep</code> to identify target row. Using anchors <code>^</code> or <code>$</code> work well. </li>
<li>Once target row is identified, use <code>strsplit</code> to break into elements needed. Repeat last step.</li>
<li>Use direct references (<code>txt[1]</code>) or regex (<code>txt[grep(""GPA"",txt)]</code>) where possible. </li>
<li>Parse and reformat in whichever manner you please. </li>
</ul>

<p>readLines</p>

<pre><code>txt &lt;- readLines(con=textConnection(
'Student Name MM/YY 06/16/2015 N11111111 002785 1 of 1

Name of University Beginning of Graduate Record

Fall 2014 Name of School Master of Science Major: Major

Name of Class 1 Name of Class 2 Name of Class 3 Name of Class 4

COURSE+SECTION 3.0 B+ COURSE+SECTION 3.0 A COURSE+SECTION 3.0 A COURSE+SECTION 3.0 A

Current Cumulative

AHRS 12.0 12.0

EHRS 12.0 12.0

QHRS 12.0 12.0

QPTS 45.900 45.900

GPA 3.825 3.825

Spring 2015

Name of School Master of Science Major: Major

Name of Class 1 Name of Class 2 Name of Class 3

COURSE+SECTION 2.0 A COURSE+SECTION 2.0 A COURSE+SECTION 2.0 A-

Name of Class 4 COURSE+SECTION 2.0 A

Name of Class 5

COURSE+SECTION 2.0 A-

Name of Class 6 COURSE+SECTION 4.0 A

Name of Class 7

COURSE+SECTION 3.0 B+

Name of Class 8

COURSE+SECTION

3.0 A

Current Cumulative

AHRS 20.0 32.0

EHRS 20.0 32.0

QHRS 20.0 32.0

QPTS 76.700 122.600

GPA 3.835 3.831

End of Graduate Record'))
</code></pre>

<p>Cleaning</p>

<pre><code># trim of http://stackoverflow.com/questions/2261079/whitespace-in-r
trim &lt;- function (x) gsub(""^\\s+|\\s+$"", """", x)
txt &lt;- trim(txt)
# Drop empties
txt &lt;- txt[txt != """"]
</code></pre>

<p>Search and Parse: ID Number</p>

<pre><code>id &lt;- strsplit(txt[1], "" "")
id &lt;- id[grep(""^[N][0-9]"",id)] # Starts with N followed by 0-9
</code></pre>

<p>Search and Parse: GPA</p>

<pre><code>gpa &lt;- txt[grep(""GPA"",txt)]
gpa &lt;- strsplit(gpa, "" "")
gpa &lt;- matrix(
  as.numeric(
    t(
      as.data.frame(gpa)
      )[1:2, 2:3]
    ),ncol = 2)
</code></pre>

<p>... and so on. </p>
",0,2,648,2015-06-16 16:39:31,https://stackoverflow.com/questions/30873402/how-do-i-extract-certain-words-in-my-document-into-a-dataframe-in-r
Extract relevant attributes from postal addresses data in order to do PCA on those Data (using R),"<p>I have big file which contains string information : postal addresses.
Address example : ""1780 wemmel  rue hendrik de mol 59/7""</p>

<p>I need to do a PCA analysis on that Data in order to identify on the individuals graph the clusters that represent the physicals delivery posts (building, companies, ...). To do that I need to extract numeric (or not numeric) relevant information from the strings and make it my attributes, then I can analyze it using PCA.</p>

<p>I started with creating 36 attributes (A-Z and 0-9) that represent the occurrence of each alpha character and digit. But the PCA doesn't give a good result yet, I need to extract more attributes that can characterize the Data.</p>

<p>I need your ideas about what I can extract from the Data to have a good representation of the clusters on the individual graph. I'm using R.</p>

<p>Thank you.</p>
","r, data-mining, text-mining, pca, text-extraction","<p>I think that task is not for PCA. I would first try to introduce some kind of distance measure between 2 addresses. You can <strong>either</strong> use entire address as a single feature - then there're plenty of general-purpose string similarity measures, for example Levenshtein distance. There's a <a href=""https://stat.ethz.ch/R-manual/R-devel/library/utils/html/adist.html"" rel=""nofollow noreferrer"">method</a> in <code>utils</code> package. <strong>Or</strong> introduce more features, like number of building, postal code, etc. and use combination of Euclidean distance and text-similarity distance. Your 36 variables seem too much for the task. Anyway, your distance measure should give small value for 'close' addresses and large value for irrelevant addresses in your domain.</p>

<p>After deciding on distance measure and choosing features, apply <a href=""https://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow noreferrer"">k-means clustering</a> with custom distance function to your data. You can use <a href=""http://cran.r-project.org/web/packages/flexclust/index.html"" rel=""nofollow noreferrer"">flexclust</a> package for that. Nice suggestions for determining number of clusters can be found <a href=""https://stackoverflow.com/questions/15376075/cluster-analysis-in-r-determine-the-optimal-number-of-clusters"">here</a>. </p>

<p>With that you'll likely find your clusters. Good luck.</p>
",2,0,75,2015-06-17 09:51:06,https://stackoverflow.com/questions/30888087/extract-relevant-attributes-from-postal-addresses-data-in-order-to-do-pca-on-tho
Quotes and hyphens not removed by tm package functions while cleaning corpus,"<p>I'm trying to clean the corpus and I've used the typical steps, like the code below:</p>

<pre><code>docs&lt;-Corpus(DirSource(path))
docs&lt;-tm_map(docs,content_transformer(tolower))
docs&lt;-tm_map(docs,content_transformer(removeNumbers))
docs&lt;-tm_map(docs,content_transformer(removePunctuation))
docs&lt;-tm_map(docs,removeWords,stopwords('en'))
docs&lt;-tm_map(docs,stripWhitespace)
docs&lt;-tm_map(docs,stemDocument)
dtm&lt;-DocumentTermMatrix(docs)
</code></pre>

<p>Yet when I inspect the matrix there are few words that come with quotes, such as:
""we"" 
""company""
""code
guidelines""
-known
-accelerated</p>

<p>It seems that the words themselves are inside the quotes but when I try to run removePunctuation code again it doesn't work. Also there are some words with bullets in front of that I also can't remove. </p>

<p>Any help would be greatly appreciated.</p>
","r, text-mining, tm","<p><code>removePunctuation</code> uses <code>gsub('[[:punct:]]','',x)</code> i.e. removes symbols: <code>!""#$%&amp;'()*+, \-./:;&lt;=&gt;?@[\\\]^_</code>{|}~`. To remove other symbols, like typographic quotes or bullet signs (or any other), declare your own transformation function:</p>

<pre><code>removeSpecialChars &lt;- function(x) gsub(""“•”"","""",x)
docs &lt;- tm_map(docs, removeSpecialChars)
</code></pre>

<p>Or you can go further and remove everything that is not alphanumerical symbol or space:</p>

<pre><code>removeSpecialChars &lt;- function(x) gsub(""[^a-zA-Z0-9 ]"","""",x)
docs &lt;- tm_map(docs, removeSpecialChars)
</code></pre>
",11,8,5755,2015-06-23 05:04:02,https://stackoverflow.com/questions/30994194/quotes-and-hyphens-not-removed-by-tm-package-functions-while-cleaning-corpus
How to use OpenNLP to get POS tags in R?,"<p>Here is the R Code:</p>

<pre><code>library(NLP) 
library(openNLP)
tagPOS &lt;-  function(x, ...) {
s &lt;- as.String(x)
word_token_annotator &lt;- Maxent_Word_Token_Annotator()
a2 &lt;- Annotation(1L, ""sentence"", 1L, nchar(s))
a2 &lt;- annotate(s, word_token_annotator, a2)
a3 &lt;- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w &lt;- a3[a3$type == ""word""]
POStags &lt;- unlist(lapply(a3w$features, `[[`, ""POS""))
POStagged &lt;- paste(sprintf(""%s/%s"", s[a3w], POStags), collapse = "" "")
list(POStagged = POStagged, POStags = POStags)}
str &lt;- ""this is a the first sentence.""
tagged_str &lt;-  tagPOS(str)
</code></pre>

<p>Output is :</p>

<blockquote>
  <p>tagged_str
      $POStagged
      [1]""this/DT is/VBZ a/DT the/DT first/JJ sentence/NN ./.""</p>
</blockquote>

<p>Now I want to extract only NN word i.e sentence from the above sentence and want to store it into a variable .Can anyone help me out with this .</p>
","r, nlp, text-mining, opennlp, pos-tagger","<p>There might be more elegant ways to obtain the result, but this one should work:</p>

<pre><code>q &lt;- strsplit(unlist(tagged_str[1]),'/NN')
q &lt;- tail(strsplit(unlist(q[1]),"" "")[[1]],1)
#&gt; q
#[1] ""sentence""
</code></pre>

<p>Hope this helps.</p>
",2,7,13351,2015-06-23 06:19:58,https://stackoverflow.com/questions/30995232/how-to-use-opennlp-to-get-pos-tags-in-r
R Text Mining with quanteda,"<p>I have a data set (Facebook posts) (via netvizz) and I use the quanteda package in R. Here is my R code.</p>

<pre><code># Load the relevant dictionary (relevant for analysis)
liwcdict &lt;- dictionary(file = ""D:/LIWC2001_English.dic"", format = ""LIWC"")

# Read File
# Facebooks posts could be generated by  FB Netvizz 
# https://apps.facebook.com/netvizz
# Load FB posts as .csv-file from .zip-file 
fbpost &lt;- read.csv(""D:/FB-com.csv"", sep="";"")

# Define the relevant column(s)
fb_test &lt;-as.character(FB_com$comment_message) #one column with 2700 entries
# Define as corpus
fb_corp &lt;-corpus(fb_test)
class(fb_corp)

# LIWC Application
fb_liwc&lt;-dfm(fb_corp, dictionary=liwcdict)
View(fb_liwc)
</code></pre>

<p>Everything works until:</p>

<pre><code>&gt; fb_liwc&lt;-dfm(fb_corp, dictionary=liwcdict)
Creating a dfm from a corpus ...
   ... indexing 2,760 documents
   ... tokenizing texts, found 77,923 total tokens
   ... cleaning the tokens, 1584 removed entirely
   ... applying a dictionary consisting of 68 key entries
Error in `dimnames&lt;-.data.frame`(`*tmp*`, value = list(docs = c(""text1"",  : 
  invalid 'dimnames' given for data frame
</code></pre>

<p>How would you interpret the error message? Are there any suggestions to solve the problem?</p>
","r, text-mining, text-analysis, quanteda","<p>There was a bug in quanteda version 0.7.2 that caused <code>dfm()</code> to fail when using a dictionary when one of the documents contains no features.  Your example fails because in the cleaning stage, some of the Facebook post ""documents"" end up having all of their features removed through the cleaning steps.</p>

<p>This is not only fixed in 0.8.0, but also we changed the underlying implementation of dictionaries in <code>dfm()</code>, resulting in a significant speed improvement.  (The LIWC is still a large and complicated dictionary, and the regular expressions still mean that it is much slower to use than simply indexing tokens.  We will work on optimising this further.)</p>

<pre><code>devtools::install_github(""kbenoit/quanteda"")
liwcdict &lt;- dictionary(file = ""LIWC2001_English.dic"", format = ""LIWC"")
mydfm &lt;- dfm(inaugTexts, dictionary = liwcdict)
## Creating a dfm from a character vector ...
##    ... indexing 57 documents
##    ... lowercasing
##    ... tokenizing
##    ... shaping tokens into data.table, found 134,024 total tokens
##    ... applying a dictionary consisting of 68 key entries
##    ... summing dictionary-matched features by document
##    ... indexing 68 feature types
##    ... building sparse matrix
##    ... created a 57 x 68 sparse dfm
##    ... complete. Elapsed time: 14.005 seconds.
topfeatures(mydfm, decreasing=FALSE)
## Fillers   Nonfl   Swear      TV  Eating   Sleep   Groom   Death  Sports  Sexual 
##       0       0       0      42      47      49      53      76      81     100 
</code></pre>

<p>It will also work if a document contains zero features after tokenization and cleaning, which is probably what is breaking the older <code>dfm</code> you are using with your Facebook texts.</p>

<pre><code>mytexts &lt;- inaugTexts
mytexts[3] &lt;- """"
mydfm &lt;- dfm(mytexts, dictionary = liwcdict, verbose = FALSE)
which(rowSums(mydfm)==0)
## 1797-Adams 
##          3 
</code></pre>
",2,0,3664,2015-06-24 14:37:56,https://stackoverflow.com/questions/31029582/r-text-mining-with-quanteda
How can I extract 2-4 words on each side of a specific term in R?,"<p>How can I extract 2-4 words on each side of a specific term from a string/corpus in R?</p>

<p>Here is an example:</p>

<p>I would like to extract 2 words around 'converse'. </p>

<pre><code>txt &lt;- ""Socially when people meet they should converse to present their
       views and listen to other people's opinions to enhance their perspective"" 
</code></pre>

<p>Output should be like: </p>

<pre><code>""they should converse to present""
</code></pre>
","regex, r, text-mining, sentiment-analysis","<p>I guess this solves your problem:</p>

<pre><code>/((?:\S+\s){2}converse(?:\s\S+){2})/
</code></pre>

<p>Demo: <a href=""https://regex101.com/r/tS9kB0/1"" rel=""nofollow"">https://regex101.com/r/tS9kB0/1</a></p>

<p>If you need other weights on either side, I guess you can see what to change.</p>
",4,-3,332,2015-06-24 15:46:25,https://stackoverflow.com/questions/31031129/how-can-i-extract-2-4-words-on-each-side-of-a-specific-term-in-r
How do I tag a document if a word is not present in it?,"<p>I am performing text mining on text data having 2500 documents and looking for a specific word in the document. </p>

<p>I want to tag the document if a word say 'laceration' is not present in it and get the output as list of documents not having that word. And would also like to save the output in a text file.</p>

<p>I am using the following code</p>

<pre><code>library(qdapRegex)

grab2 &lt;- rm_(pattern=S(""@around_"", 1, ""laceration"", 1), extract=TRUE)

grab2(l$Text)
</code></pre>

<p>Sample output I am getting</p>

<pre><code>[[2164]]
[1] NA

[[2165]]
[1] NA

[[2166]]
[1] ""laceration""

[[2167]]
[1] NA

[[2168]]
[1] NA
</code></pre>

<p>I want the code which will return only the documents without the word 'laceration'. And want to write the output in a file.</p>
","r, text-mining","<p>While you could do this in R, it would be much more efficient to do this at the command line (using a Linux-like OS or CygWin if on Windows):</p>

<pre><code>grep -v ""\blaceration\b"" *.txt &gt;ListOfNoLac
</code></pre>

<p>In R, you could do this:</p>

<pre><code>fileList &lt;- list.files(""."", ""\\.txt$"")
hasLac &lt;- sapply(fileList, function(x) length(grep(""\\blaceration\\b"", readLines(x))) &gt; 0)
fileList[!hasLac]
</code></pre>
",2,-3,92,2015-06-24 19:34:15,https://stackoverflow.com/questions/31035454/how-do-i-tag-a-document-if-a-word-is-not-present-in-it
Skipping an unknown number of words in python,"<p>So I usually am just extracting phrases and print them out in a pre specified format after I run a script over a document.</p>

<p>I use this code to split up my setences</p>

<pre><code>def iterphrases(text):
    return re.split(r'\.\s', re.sub(r'\.\s*$', '', text))
</code></pre>

<p>Then I read the file and if the word is in the file I append the sentence into a dictionary. </p>

<pre><code>def find_keywords(OutputFile, keys):
    phrase_combos= keys + [x.upper() for x in keys] + [x.lower() for x in keys] + [x.capitalize() for x in keys] 
   keys = list(set(phrase_combos))
    cwd = os.getcwd()
print 'Working in current directory : ', cwd
cwdfiles = os.listdir(cwd)

    filenames = []
    for item in cwdfiles:
        if item[-4:] == '.txt':
        filenames.append(item)

    out = defaultdict(list) 
    for filename in filenames:
        for phrase in iterphrases(open(filename).read()):
             for keyword in keys:

                if phrase.lower().index('no') &lt; phrase.index(keyword): 
                    out[keyword].append((filename, phrase))
    my_dict= dict(**out)
</code></pre>

<p>I do some stuff with this and it has worked great for a while but now I need to find things that are NOT something. I could find many phrases but some skip words and would not match exactly for example if my phrase was the word foo.</p>

<p>No foo. Not foo. Not foo or bar. No foo and no bar. Are all in my dictionary but I also need: </p>

<pre><code>Not bar or foo. Not bar or foo or banana. Not bar or banana or foo. Not bar, banana, or foo. Not bar, foo, or banana. 
</code></pre>

<p>To all appear as a results as well. Right now it can not match on it because bar foo is not right next to a negating word. Is there a way I can say 'Match if negative words appear regardless of how many other words are between the word/phrase of interest as long as you are in the same sentence' ?</p>

<p>Creating something like this for example.</p>

<pre><code>This is a group of Text. There is no foo. There is no bar. There is no foo 
or bar. There is no bar or foo. I have coffee. I have a bar. No bar for you. 
</code></pre>

<p>Ought to return :
     {'bar' : There is no bar. , There is no bar or foo. , There is no foo or bar., No bar for you.}</p>
","python, regex, text-mining","<p>Try searching with a regular expression. You can search a list of keywords and negate them with a list of negations.
The trick is to compile a regular expression which searches inside your sentences for 'a negation word in somewhere before my keyword'. This means:</p>

<pre><code>re.compile(r'\b{!s}\b.+\b{!s}\b'.format(neg, keyword), re.I)
</code></pre>

<p>Where <code>\b</code> means 'word boundary'. So it is a word, followed by gibberish (<code>.+</code>) followed by a word. And with the <code>format</code> we set the words to the negation word and the keyword. <code>re.I</code> sets the ignore-cases-flag.</p>

<p>Now with all your examples and some examples I think you don't want to match like 'Nonono this is not the right foo' or 'Anonymus foo...' I came up with the following, which should give you a starting point:</p>

<pre class=""lang-py prettyprint-override""><code>import re
text = 'Not foo. Not No foo. Not foo or bar. No foo and no bar. Not bar or foo. Not bar or foo or banana. Not bar or banana or foo. Not bar, banana, or foo. Not bar, foo, or banana. This is a group of Text. There is no foo. There is no bar. There is no foo or bar. There is no bar or foo. I have coffee. I have a bar. No bar for you. Nonono, this is the wrong foo. Nono this is also a wrong foo. Anonymous foo.'
keywords = ['foo']
negated = ['no', 'not']

phraselist = re.split(r'\.\s', text)

out = {}

for phrase in phraselist:
    for keyword in keywords:
        for neg in negated:
            regex = re.compile(r'\b{!s}\b.+\b{!s}\b'.format(neg, keyword), re.I)
            if regex.search(phrase.lower()):
                try:
                    if not phrase in out[keyword]:
                        out[keyword].append(phrase) 
                except KeyError:
                    out[keyword] = [phrase]

print(out)

expected = 'Not foo. Not No foo. Not foo or bar. No foo and no bar. Not bar or foo. Not bar or foo or banana. Not bar or banana or foo. Not bar, banana, or foo. Not bar, foo or banana. There is no foo. There is no foor or bar. There is no bar or foo.'
print(expected)
</code></pre>

<p>The output is:</p>

<pre class=""lang-none prettyprint-override""><code>{'foo': ['Not foo', 'Not No foo', 'Not foo or bar', 'No foo and no bar', 'Not ba
r or foo', 'Not bar or foo or banana', 'Not bar or banana or foo', 'Not bar, ban
ana, or foo', 'Not bar, foo, or banana', 'There is no foo', 'There is no foo or
bar', 'There is no bar or foo']}
Not foo. Not No foo. Not foo or bar. No foo and no bar. Not bar or foo. Not bar
or foo or banana. Not bar or banana or foo. Not bar, banana, or foo. Not bar, fo
o or banana. There is no foo. There is no foor or bar. There is no bar or foo.
</code></pre>
",1,-1,532,2015-06-29 13:19:40,https://stackoverflow.com/questions/31116818/skipping-an-unknown-number-of-words-in-python
WordCloud of transaction activities in R,"<p>I am trying to generate a wordcloud from some transaction activities in order to show where people spend the most money. The transaction activities look like the following:</p>

<pre><code>Description       Amount
Albertson         20
Albertson         30
Albertson         35
CVS               10
CVS               40
Walmart           15
Walmart           44
...
</code></pre>

<p>I can generate wordcloud easily by Description's frequency. But how can I get wordcloud which sorted by sum(amount) of each category? Thanks!</p>

<p>BTW here is my code</p>

<pre><code>require(tm)
require(wordcloud)
require(RColorBrewer)

data_corpus &lt;- Corpus(VectorSource(data))

data_corpus &lt;- tm_map(data_corpus, content_transformer(tolower), mc.cores=1)
data_corpus &lt;- tm_map(data_corpus, removePunctuation, mc.cores=1)
data_corpus &lt;- tm_map(data_corpus, function(x)removeWords(x,stopwords()), mc.cores=1)
data_corpus &lt;- tm_map(data_corpus, removeNumbers, mc.cores=1)

pal2 &lt;- brewer.pal(8,""Dark2"")
png(""25-34.png"", width=1280,height=800)
wordcloud(data_corpus, scale=c(6,.2),min.freq=50,max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
</code></pre>
","r, text-mining","<p>I loaded your mini table into a dataframe called data. Then ran the following code:</p>

<pre><code>require(wordcloud)
require(RColorBrewer)
library(dplyr)
# group by Description and sum the Amounts
data &lt;- data %&gt;% group_by(Description) %&gt;% summarise(Amount = sum(Amount))

pal2 &lt;- brewer.pal(8,""Dark2"")
wordcloud(data$Description, freq = data$Amount, scale=c(6,.2),min.freq=50,max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
</code></pre>

<p>No need for the tm package. Just specify your description in the words section and the Amount in the frequency section. </p>
",0,1,76,2015-07-01 19:40:04,https://stackoverflow.com/questions/31170234/wordcloud-of-transaction-activities-in-r
Sentiment analysis: more than 3 sentiments,"<p>My app needs sentiment analysis functionality. I've found plenty of services and libraries which can help with this task. But most of them have ""three-dimensional"" output: the text may be classified as ""positive"", ""negative"" or ""neutral. </p>

<p>But what if I need larger variety of options? For example: ""confident/doubtful"", ""calm/alerted"", ""kind/aggressive"" or something like that.</p>

<p>Is it even possible to perform such classification? May be there are already some services/frameworks/libraries available?</p>
","java, nlp, text-mining, sentiment-analysis","<p>You should try <a href=""http://wndomains.fbk.eu/wnaffect.html"" rel=""nofollow"">WordNet-Affect</a>. This ressource provides a tree of emotions. As it is a quite old ressource, you will have to manually parsed it and to map the IDs with <a href=""http://wordnet.princeton.edu/wordnet/download/old-versions/"" rel=""nofollow"">WordNet 1.6</a> synsets (I did this work in Python <a href=""https://github.com/clemtoy/WNAffect"" rel=""nofollow"">here</a>).</p>
",1,3,619,2015-07-02 18:19:56,https://stackoverflow.com/questions/31191865/sentiment-analysis-more-than-3-sentiments
“CFINDER” output data importation and calculation of mean of pairs,"<p>I face a problem: I have to import a text file (<code>from_soft.txt</code>, which is output from ""CFINDER"" software). It looks like this:</p>

<pre><code>from_soft:
# text-text-text-text-text-text-text-text-text-text-
# text-text-text-text-text-text-text-text-text-text-
# text-text-text-text-text-text-text-text-text-text-
# text-text-text-text-text-text-text-text-text-text-

1: SpeciesA SpeciesB SpeciesC SpeciesD SpeciesE
2: SpeciesA SpeciesC SpeciesE SpeciesD SpeciesF SpeciesG SpeciesH
3: SpeciesB SpeciesC SpeciesF
4: SpeciesB SpeciesC SpeciesD SpeciesF SpeciesH
[...]
</code></pre>

<p>I manage to import it into R with <code>readLines</code>:</p>

<pre><code>cliques&lt;-readLines(""from_soft.txt"",n=-1,ok=T,warn=T,encoding=""unknow"",skipNul=F)
</code></pre>

<p>And I delete first rows to:</p>

<pre><code>from_soft&lt;-as.data.frame(from_soft)[-(1:5),]
</code></pre>

<p>The table <code>from_soft</code> is like this:</p>

<pre><code>head(from_soft)
[1] 0: SpeciesA SpeciesB SpeciesC SpeciesD SpeciesE   1: SpeciesA SpeciesB SpeciesC SpeciesD SpeciesE   2: SpeciesA SpeciesB SpeciesC SpeciesD SpeciesE  
[4] 3: SpeciesB SpeciesC SpeciesD SpeciesF SpeciesH [...]
</code></pre>

<p>On another side I have a table <code>ref</code> that indicates a 'value' for each pair of species. It looks like this:</p>

<pre><code>print(ref)
3324   SpeciesA  SpeciesB       1
3325   SpeciesA  SpeciesC       2
3326   SpeciesA  SpeciesD      12
3327   SpeciesA  SpeciesE       1
3328   SpeciesA  SpeciesF      71
3329   SpeciesA  SpeciesG       6
3330   SpeciesA  SpeciesH      15
3331   SpeciesB  SpeciesC       2
3332   SpeciesB  SpeciesF       4
3333   SpeciesB  SpeciesD      17
[...]
</code></pre>

<p>Each row of <code>from_soft</code> correspond to a 'clique' in a graph. It means that each species are interconnected to each other. I would like to calculate for each row the 'mean connection'.</p>

<p>As an example for row <code>1:</code> here are all the existing pairs:</p>

<pre><code>1: SpeciesASpeciesB|SpeciesASpeciesC|SpeciesASpeciesD|SpeciesASpeciesE|SpeciesBSpeciesC|SpeciesBSpeciesD|SpeciesBSpeciesE|SpeciesCSpeciesD|SpeciesCSpeciesE|SpeciesDSpeciesE|
</code></pre>

<p>Each existing pair has a value given in <code>ref</code>. The output file I would like to have is a table like this:</p>

<pre><code>1: 3.5 (= mean of all pairs in the clique '1:')
2: 4.2
3: 1.5
4: 6
[...]
</code></pre>

<p>Any idea to perform that?</p>
","r, text-mining","<p>The solution :</p>

<pre><code>paires &lt;- interaction(ref[,2:3] , drop = T)
rl &lt;- readLines(""from_soft.txt"")
rl &lt;- rl[-(1:5)]
l &lt;- strsplit(rl , "" "")
l &lt;- lapply(l , tail , -1)
comb &lt;- lapply(l , function(x) apply(combn(x , 2) , 2 , paste , collapse = "".""))
sapply(comb , function(x) mean(ref$V4[match(x , paires)]))`
</code></pre>

<p>Cheers,</p>

<p>R.</p>
",1,0,74,2015-07-03 12:54:19,https://stackoverflow.com/questions/31249261/cfinder-output-data-importation-and-calculation-of-mean-of-pairs
Maximum occurrence of any set of words in text in R,"<p>Given a set of lines, I have to find maximum occurrence of words(need not be single word, can be set of words also.)</p>

<p>say, I have a text like,</p>

<pre><code>string &lt;- ""He is john beck. john beck is working as an chemical engineer. Most of the chemical engineers are john beck's friend""
</code></pre>

<p>I want output to be,</p>

<pre><code>john beck - 3
chemical engineer - 2
</code></pre>

<p>Is there any function or package which does this?</p>
","r, string, text-mining","<p>Try this:</p>

<pre><code>string &lt;- ""He is john beck. john beck is working as an chemical engineer. Most of the chemical engineers are john beck's friend""
library(tau)
library(tm)
tokens &lt;- MC_tokenizer(string) 
tokens &lt;- tokens[tokens != """"]
string_ &lt;- paste(stemCompletion(stemDocument(tokens), tokens), collapse = "" "")

## if you want only bi-grams: 
tab &lt;- sort(textcnt(string_, method = ""string"", n = 2), decreasing = TRUE)
data.frame(Freq = tab[tab &gt; 1])
#                   Freq
# john beck            3
# chemical engineer    2

## if you want uni-, bi- and tri-grams: 
nmin &lt;- 1; nmax &lt;- 3
tab &lt;- sort(do.call(c, lapply(nmin:nmax, function(x) textcnt(string_, method = ""string"", n = x) )), decreasing = TRUE)
data.frame(Freq = tab[tab &gt; 1])
#                   Freq
# beck                 3
# john                 3
# john beck            3
# chemical             2
# engineer             2
# is                   2
# chemical engineer    2
</code></pre>
",3,1,125,2015-07-06 11:50:03,https://stackoverflow.com/questions/31245105/maximum-occurrence-of-any-set-of-words-in-text-in-r
"How to find instances in an unlabeled dataset, that are most promising to be informative when building a classifier?","<p>My problem is that I have a large unlabeled dataset, but over time I want it to become labeled and build a confident classifier. </p>

<p>This can be done by active learning, but active learning needs an initial classifier to be built for it to then estimate and rank the remaining unlabeled instances by how informative they are expected to be to the classifier.</p>

<p>To build the initial classifier, I need to label some examples by hand. my questions is: Are there methods to find likely informative examples in the initial unlabeled dataset, without the help of an initial classifier?</p>

<p>I thought about just using k-means with some number of clusters, run it and label one example from each cluster, then train the classifier on these.
Is there a better way?</p>
","machine-learning, dataset, data-mining, text-mining","<p>I have to disagree with Edward Raff.</p>

<p>k-means may turn out to be useful here (if your data is continuous).</p>

<p>Just use a rather large value of k.</p>

<p>The idea is to avoid picking too similar objects, but get a sample that covers the data reasonably well. k-means may fail to ""cluster"" complex data, but it works reasonably well for <em>quantization</em>. So it will return a ""less random, more representative"" sample from your data.</p>

<p>But beware: <em>k-means centers do not correspond to data points</em>. You could either use a medoid based algorithm, or just find the closes instance to each center.</p>

<p>Some alternatives:</p>

<ul>
<li>if you can afford to label ""a"" objects, run k-means with k=a</li>
<li>run k-means with k=5*a, and select 20% of the centers (maybe preferring those with highest density)</li>
<li>choose 0.5*a by k-means, 0.5*a randomly</li>
<li>do either, but choose only 0.5*a objects to label. Train a classifier, find the 0.5*a unlabeled objects that the classifier had the lowest confidence on</li>
</ul>
",0,0,282,2015-07-06 17:30:36,https://stackoverflow.com/questions/31252072/how-to-find-instances-in-an-unlabeled-dataset-that-are-most-promising-to-be-inf
R dynamic stop word list with terms of frequency one,"<p>I am working on a text mining assignment and am stuck at the moment. The following is based on <code>Zhaos Text Mining</code> with Twitter. I cannot get it to work, maybe one of you has a good idea?</p>

<p><strong>Goal:</strong> I would like to remove all terms from the corpus with a word count of one instead of using a stopword list.</p>

<p><strong>What I did so far:</strong> I have downloaded the tweets and converted them into a data frame.</p>

<pre><code>tf1 &lt;- Corpus(VectorSource(tweets.df$text))


tf1 &lt;- tm_map(tf1, content_transformer(tolower))


removeUser &lt;- function(x) gsub(""@[[:alnum:]]*"", """", x)
tf1 &lt;- tm_map(tf1, content_transformer(removeUser))


removeNumPunct &lt;- function(x) gsub(""[^[:alpha:][:space:]]*"", """", x)
tf1 &lt;- tm_map(tf1, content_transformer(removeNumPunct))


removeURL &lt;- function(x) gsub(""http[[:alnum:]]*"", """", x)
tf1 &lt;- tm_map(tf1, content_transformer(removeURL))

tf1 &lt;- tm_map(tf1, stripWhitespace)


#Using TermDocMatrix in order to find terms with count 1, dont know any other way
tdmtf1 &lt;- TermDocumentMatrix(tf1, control = list(wordLengths = c(1, Inf)))

ones &lt;- findFreqTerms(tdmtf1, lowfreq = 1, highfreq = 1)

tf1Copy &lt;- tf1

tf1List &lt;- setdiff(tf1Copy, ones)


tf1CList &lt;- paste(unlist(tf1List),sep="""", collapse="" "")

tf1Copy &lt;- tm_map(tf1Copy, removeWords, tf1CList)

tdmtf1Test &lt;- TermDocumentMatrix(tf1Copy, control = list(wordLengths = c(1, Inf)))

#Just to test success...
ones2 &lt;- findFreqTerms(tdmtf1Test, lowfreq = 1, highfreq = 1)
(ones2)
</code></pre>

<p><strong>The Error:</strong></p>

<blockquote>
  <p>Error in gsub(sprintf(""(*UCP)\b(%s)\b"", paste(sort(words, decreasing = TRUE),  :   invalid regular expression '(*UCP)\b(senior data scientist global strategy firm <br>
  25.0010230541229 48 17 6 6 115 1 186 0 1 en kdnuggets poll primary programming language for analytics data mining data scienc <br>
  25.0020229816437 48 17 6 6 115 1 186 0 2 en iapa canberra seminar mining the internet of everything official statistics in the information age anu june <br> 25.0020229816437 48 17 6 6 115 1 186 0 3 en handling and processing strings in r an ebook in pdf format pages<br>
  25.0020229816437 48 17 6 6 115 1 186 0 4 en webinar getting your data into r by hadley wickham am edt june th <br>
  25.0020229816437 48 17 6 6 115 1 186 0 5 en  before loading the rdmtweets dataset please run librarytwitter to load required package <br>
  25.0020229816437 48 17 6 6 115 1 186 0 6 en an infographic on sas vs r vs python datascience via  <br>
  25.0020229816437 48 17 6 6 115 1 186 0 7 en r is again the kdnuggets poll on top analytics data mining science software <br>
  25.0020229816437 48 17 6 6 115 1 186 0 8 en i will run  </p>
</blockquote>

<p><strong>In Addition:</strong></p>

<blockquote>
  <p>Warning message: In gsub(sprintf(""(*UCP)\b(%s)\b"", paste(sort(words, decreasing = TRUE),  :   PCRE pattern compilation error
          'regular expression is too large'
          at ''</p>
</blockquote>

<p>PS sorry for the bad format at the end could not get it fixed.</p>
","r, text-mining, tm, stop-words","<p>Here's a way how to remove all terms from the corpus with a word count of one:</p>

<pre><code>library(tm)
mytweets &lt;- c(""This is a doc"", ""This is another doc"")

corp &lt;- Corpus(VectorSource(mytweets))
inspect(corp)
# [[1]]
# &lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;
# This is a doc
# 
# [[2]]
# &lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;
#   This is another doc
##            ^^^ 

dtm &lt;- DocumentTermMatrix(corp)
inspect(dtm)
# Terms
# Docs another doc this
# 1       0   1    1
# 2       1   1    1

(stopwords &lt;- findFreqTerms(dtm, 1, 1))
# [1] ""another""

corp &lt;- tm_map(corp, removeWords, stopwords)
inspect(corp)
# [[1]]
# &lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;
# This is a doc
# 
# [[2]]
# &lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;
# This is  doc
##        ^ 'another' is gone
</code></pre>

<p><em>(As a side note: The token 'a' from 'This is a...' is gone, too, because <code>DocumentTermMatrix</code> cuts out tokens with a length &lt; 3 by default.)</em></p>
",1,0,1372,2015-07-06 18:00:25,https://stackoverflow.com/questions/31252594/r-dynamic-stop-word-list-with-terms-of-frequency-one
R: dtm with ngram tokenizer plus dictionary broken in Ubuntu?,"<p>I am creating a document term matrix, with a dictionary and ngram tokenization.  It works on my Windows 7 laptop, but not on a similarly configured Ubuntu 14.04.2 server.  <em>UPDATE:  It also works on a Centos server.</em></p>

<pre><code>library(tm)
library(RWeka)
library((SnowballC))

newBigramTokenizer = function(x) {
  tokenizer1 = RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 2))
  if (length(tokenizer1) != 0L) { return(tokenizer1)
  } else return(WordTokenizer(x))
}

textvect &lt;- c(""this is a story about a girl"", 
              ""this is a story about a boy"", 
              ""a boy and a girl went to the store"",
              ""a store is a place to buy things"",
              ""you can also buy things from a boy or a girl"",
              ""the word store can also be a verb meaning to position something for later use"")

textvect &lt;- iconv(textvect, to = ""utf-8"")
textsource &lt;- VectorSource(textvect)
textcorp &lt;- Corpus(textsource)

textdict &lt;- c(""boy"", ""girl"", ""store"", ""story about"")
textdict &lt;- iconv(textdict, to = ""utf-8"")

# OK
dtm &lt;- DocumentTermMatrix(textcorp, control=list(dictionary=textdict))

# OK on Windows laptop
# freezes or generates error on Ubuntu server
dtm &lt;- DocumentTermMatrix(textcorp, control=list(tokenize=newBigramTokenizer,
                                             dictionary=textdict))
</code></pre>

<p>Error from the Ubuntu server (at the last line in the source example):</p>

<pre><code>/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/rt.jar: invalid LOC header (bad signature)
Error in simple_triplet_matrix(i = i, j = j, v = as.numeric(v), nrow = length(allTerms),  :
  'i, j' invalid
In addition: Warning messages:
1: In mclapply(unname(content(x)), termFreq, control) :
  scheduled core 1 encountered error in user code, all values of the job will be affected
2: In simple_triplet_matrix(i = i, j = j, v = as.numeric(v), nrow = length(allTerms),  :
  NAs introduced by coercion
</code></pre>

<p>I have already tried some of the suggestions in <a href=""https://stackoverflow.com/questions/18504559/twitter-data-analysis-error-in-term-document-matrix"">Twitter Data Analysis - Error in Term Document Matrix</a> and 
<a href=""https://stackoverflow.com/questions/20577040/error-in-simple-triplet-matrix-unable-to-use-rweka-to-count-phrases"">Error in simple_triplet_matrix -- unable to use RWeka to count Phrases</a></p>

<p>I had thought my problem could be attributed to one of these, but now the script is running on a Centos server with the same locales and JVM as the problematic Ubuntu server.</p>

<ul>
<li>the locales</li>
<li>the minor difference in JVMs</li>
<li>the parallel library?  mclapply is mentioned in the error message, and parallel is listed in the session info (for all systems, though.)</li>
</ul>

<p>Here are the two environments:</p>

<p><strong>R version 3.1.2 (2014-10-31)
Platform: x86_64-w64-mingw32/x64 (64-bit)</strong></p>

<pre><code>PS C:\&gt; java -version
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
java version ""1.7.0_72""
Java(TM) SE Runtime Environment (build 1.7.0_72-b14)
Java HotSpot(TM) 64-Bit Server VM (build 24.72-b04, mixed mode)

locale: 
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] RWeka_0.4-23 tm_0.6       NLP_0.1-5   

loaded via a namespace (and not attached):
[1] grid_3.1.2         parallel_3.1.2     rJava_0.9-6        RWekajars_3.7.11-1 slam_0.1-32       
[6] tools_3.1.2         
</code></pre>

<p><strong>R version 3.1.2 (2014-10-31)
Platform: x86_64-pc-linux-gnu (64-bit)</strong></p>

<pre><code>$ java -version
java version ""1.7.0_79""
OpenJDK Runtime Environment (IcedTea 2.5.5) (7u79-2.5.5-0ubuntu0.14.04.2)
OpenJDK 64-Bit Server VM (build 24.79-b02, mixed mode)

locale:
[1] LC_CTYPE=en_US.UTF-8          LC_NUMERIC=C                  LC_TIME=en_US.UTF-8          
[4] LC_COLLATE=en_US.UTF-8        LC_MONETARY=en_US.UTF-8       LC_MESSAGES=en_US.UTF-8      
[7] LC_PAPER=en_US.UTF-8          LC_NAME=en_US.UTF-8           LC_ADDRESS=en_US.UTF-8       
[10] LC_TELEPHONE=en_US.UTF-8      LC_MEASUREMENT=en_US.UTF-8    LC_IDENTIFICATION=en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] RWeka_0.4-23 tm_0.6       NLP_0.1-5   

loaded via a namespace (and not attached):
[1] grid_3.1.2         parallel_3.1.2     rJava_0.9-6        RWekajars_3.7.11-1 slam_0.1-32       
[6] tools_3.1.2     
</code></pre>

<p><strong>R version 3.2.0 (2015-04-16)
Platform: x86_64-redhat-linux-gnu (64-bit)
Running under: CentOS Linux 7 (Core)</strong></p>

<pre><code>$ java -version
java version ""1.7.0_79""
OpenJDK Runtime Environment (rhel-2.5.5.1.el7_1-x86_64 u79-b14)
OpenJDK 64-Bit Server VM (build 24.79-b02, mixed mode)


locale:
 [1] LC_CTYPE=en_US.UTF-8          LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8           LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8       LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8          LC_NAME=en_US.UTF-8
 [9] LC_ADDRESS=en_US.UTF-8        LC_TELEPHONE=en_US.UTF-8
[11] LC_MEASUREMENT=en_US.UTF-8    LC_IDENTIFICATION=en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] RWeka_0.4-24 tm_0.6-2     NLP_0.1-8

loaded via a namespace (and not attached):
[1] parallel_3.2.0     tools_3.2.0        slam_0.1-32        grid_3.2.0
[5] rJava_0.9-6        RWekajars_3.7.12-1
</code></pre>
","r, ubuntu, text-mining, quanteda","<p>If you prefer something simpler but no less flexible or powerful, how about trying out the <a href=""http://github.com/kbenoit/quanteda"" rel=""nofollow noreferrer"">quanteda</a> package?  It can make quick work of your dictionary and bigram task in three lines:</p>

<pre><code># or: devtools::install_github(""kbenoit/quanteda"")
require(quanteda)

# use dictionary() to construct dictionary from named list
textdict &lt;- dictionary(list(mydict = c(""boy"", ""girl"", ""store"", ""story about"")))

# convert to document-feature matrix, with 1grams + 2grams, apply dictionary
dfm(textvect, dictionary = textdict, ngrams = 1:2, concatenator = "" "")
## Document-feature matrix of: 6 documents, 1 feature.
## 6 x 1 sparse Matrix of class ""dfmSparse""
##        features
## docs    mydict
##   text1      2
##   text2      2
##   text3      3
##   text4      1
##   text5      2
##   text6      1

# alternative is to consider the dictionary as a thesaurus of synonyms, 
# not exclusive in feature selection as is a dictionary 
dfm.all &lt;- dfm(textvect, thesaurus = textdict,
               ngrams = 1:2, concatenator = "" "", verbose = FALSE)
topfeatures(dfm.all)
##      a  MYDICT   a boy  a girl      is    is a      to a story   about about a 
##     11      11       3       3       3       3       3       2       2       2 

dfm_sort(dfm.all)[1:6, 1:12]
## Document-feature matrix of: 6 documents, 12 features.
## 6 x 12 sparse Matrix of class ""dfmSparse""
##        features
## docs    a MYDICT a boy a girl is is a to a story about about a also buy
##   text1 2      2     0      1  1    1  0       1     1       1    0   0
##   text2 2      2     1      0  1    1  0       1     1       1    0   0
##   text3 2      3     1      1  0    0  1       0     0       0    0   0
##   text4 2      1     0      0  1    1  1       0     0       0    0   1
##   text5 2      2     1      1  0    0  0       0     0       0    1   1
##   text6 1      1     0      0  0    0  1       0     0       0    1   0
</code></pre>
",2,1,513,2015-07-07 21:11:54,https://stackoverflow.com/questions/31279347/r-dtm-with-ngram-tokenizer-plus-dictionary-broken-in-ubuntu
Convert Large CSV DTM to tm package DTM,"<p>I have a large csv file (3.8 Gb) with data in column (term), row (document) format. I would like to convert this to a dtm from the tm package.</p>

<p>I am skipping the <code>read.csv</code> step here, but you get the idea.</p>

<pre><code>dtm &lt;- structure(list(the = c(2L, 1L), apple = c(0L, 2L), dumb = c(1L, 0L)), .Names = c(""the"", ""apple"", ""dumb""), class = ""data.frame"", row.names = c(NA, -2L))
</code></pre>

<p>I then don't know how to convert this to a formal tm package dtm:</p>

<pre><code>c &lt;- Corpus(DataframeSource(dtm))
</code></pre>

<p>That's wrong, obviously.</p>

<p>Thanks for any direction.</p>
","r, csv, text-mining, tm","<p>This will do it:</p>

<pre><code>tmDTM &lt;- tm::as.DocumentTermMatrix(slam::as.simple_triplet_matrix(dtm),
                                   weighting = tm::weightTf)
</code></pre>

<p>The <a href=""http://github.com/kbenoit/quanteda"" rel=""nofollow"">quanteda</a> package has some nice implementations to this functionality as well.</p>
",1,0,468,2015-07-08 17:34:06,https://stackoverflow.com/questions/31299724/convert-large-csv-dtm-to-tm-package-dtm
How do I clean twitter data in R?,"<p>I extracted tweets from twitter using the twitteR package and saved them into a text file. </p>

<p>I have carried out the following on the corpus </p>

<pre><code>xx&lt;-tm_map(xx,removeNumbers, lazy=TRUE, 'mc.cores=1')
xx&lt;-tm_map(xx,stripWhitespace, lazy=TRUE, 'mc.cores=1')
xx&lt;-tm_map(xx,removePunctuation, lazy=TRUE, 'mc.cores=1')
xx&lt;-tm_map(xx,strip_retweets, lazy=TRUE, 'mc.cores=1')
xx&lt;-tm_map(xx,removeWords,stopwords(english), lazy=TRUE, 'mc.cores=1')
</code></pre>

<p>(using mc.cores=1 and lazy=True as otherwise R on mac is running into errors)</p>

<pre><code>tdm&lt;-TermDocumentMatrix(xx)
</code></pre>

<p>But this term document matrix has a lot of strange symbols, meaningless words and the like.
If a tweet is </p>

<pre><code> RT @Foxtel: One man stands between us and annihilation: @IanZiering.
 Sharknado‚Äã 3: OH HELL NO! - July 23 on Foxtel @SyfyAU
</code></pre>

<p>After cleaning the tweet I want only proper complete english words to be left , i.e a sentence/phrase void of everything else (user names, shortened words, urls)</p>

<p>example: </p>

<pre><code>One man stands between us and annihilation oh hell no on 
</code></pre>

<p>(Note: The transformation commands in the tm package are only able to remove stop words, punctuation whitespaces and also conversion to lowercase)</p>
","r, twitter, text-mining, data-cleaning","<p>Using gsub and </p>

<blockquote>
  <p>stringr package </p>
</blockquote>

<p>I have figured out part of the solution for removing retweets, references to screen names, hashtags, spaces, numbers, punctuations, urls . </p>

<pre><code>  clean_tweet = gsub(""&amp;amp"", """", unclean_tweet)
  clean_tweet = gsub(""(RT|via)((?:\\b\\W*@\\w+)+)"", """", clean_tweet)
  clean_tweet = gsub(""@\\w+"", """", clean_tweet)
  clean_tweet = gsub(""[[:punct:]]"", """", clean_tweet)
  clean_tweet = gsub(""[[:digit:]]"", """", clean_tweet)
  clean_tweet = gsub(""http\\w+"", """", clean_tweet)
  clean_tweet = gsub(""[ \t]{2,}"", """", clean_tweet)
  clean_tweet = gsub(""^\\s+|\\s+$"", """", clean_tweet) 
</code></pre>

<p>ref: ( Hicks , 2014)
After the above 
I did the below.</p>

<pre><code> #get rid of unnecessary spaces
clean_tweet &lt;- str_replace_all(clean_tweet,"" "","" "")
# Get rid of URLs
clean_tweet &lt;- str_replace_all(clean_tweet, ""http://t.co/[a-z,A-Z,0-9]*{8}"","""")
# Take out retweet header, there is only one
clean_tweet &lt;- str_replace(clean_tweet,""RT @[a-z,A-Z]*: "","""")
# Get rid of hashtags
clean_tweet &lt;- str_replace_all(clean_tweet,""#[a-z,A-Z]*"","""")
# Get rid of references to other screennames
clean_tweet &lt;- str_replace_all(clean_tweet,""@[a-z,A-Z]*"","""")   
</code></pre>

<p>ref: (Stanton 2013)</p>

<p>Before doing any of the above I collapsed the whole string into a single long character using the below.</p>

<p><code>paste(mytweets, collapse="" "")</code></p>

<p>This cleaning process has worked for me quite well as opposed to the tm_map transforms.</p>

<p>All that I am left with now is a set of proper words and a very few improper words.
Now, I only have to figure out how to remove the non proper english words. 
Probably i will have to subtract my set of words from a dictionary of words. </p>
",18,14,25216,2015-07-10 19:04:57,https://stackoverflow.com/questions/31348453/how-do-i-clean-twitter-data-in-r
Text Mining sql schema files,"<p>I have a collection of big sql files. From those Files I want to keep only the ""CREATE TABLE"" and ""Alter table add constraint Foreign Key"" statements. Is There a toll I can use to mine those two regular expressions? I know that I could use grep but I don't have linux</p>
","java, sql, text-mining","<p>You can build a small Java program to obtain only such sentences. e.g.:</p>

<pre><code>String input = new String(Files.readAllBytes(Paths.get(""file.sql"")), ""UTF-8"");
String regex = ""(?i)((create table|alter table add constraint foreign key)[^;]+;)""
        .replace("" "", ""\\s+"");
Pattern pattern = Pattern.compile(regex);
Matcher matcher = pattern.matcher(input);
while (matcher.find()) {
    System.out.println(matcher.group());
}
</code></pre>
",2,0,69,2015-07-14 10:49:22,https://stackoverflow.com/questions/31404498/text-mining-sql-schema-files
CPU-and-memory efficient NGram extraction with R,"<p>I wrote an algorithm which extract NGrams (bigrams, trigrams, ... till 5-grams) from a list of 50000 street addresses. My goal is to have for each address a boolean vector representing whether the NGrams are present or not in the address. Therefor each address will be characterized by a vector of attributes, and then I can carry out a clustering on the addresses. 
The algo works that way : 
I start with the bi-grams, I calculate all the combinations of (a-z and 0-9 and / and tabulation) : for example : aa,ab,ac,...,a8,a9,a/,a ,ba,bb,...
Then I carry out a loop for each address and extract for all the bigrams the information 0 or 1 (bi-gram not present or present). 
Afterward, I calculate for the bigrams that occur the most the trigrams. 
And so on ...
My problem is the time that the algo takes to run. Another problem : R reach its maximal capacity when there are more than 10000 NGrams. It's obvious because a 50000*10000 matrice is huge.
I need your ideas to optimize the algo or to change it. Thank you.</p>
","r, performance, text-mining, n-gram","<p>Try the <code>quanteda</code> package, using this method.  If you just want tokenized texts, replace the <code>dfm(</code> with <code>tokenize(</code>.</p>

<p>I'd be <strong>very</strong> interested to know how it works on your 50,000 street addresses.  We've put a lot of effort into making <code>dfm()</code> very fast and robust.</p>

<pre><code>myDfm &lt;- dfm(c(""1780 wemmel"", ""2015 schlemmel""), what = ""character"", 
             ngram = 1:5, concatenator = """", 
             removePunct = FALSE, removeNumbers = FALSE, 
             removeSeparators = FALSE, verbose = FALSE)
t(myDfm) # for easier viewing
#         docs
# features text1 text2
#           1     1
# s         0     1
# sc        0     1
# sch       0     1
# schl      0     1
# w         1     0
# we        1     0
# wem       1     0
# wemm      1     0
# 0         1     1
# 0         1     0
# 0 w       1     0
# 0 we      1     0
# 0 wem     1     0
# 01        0     1
# 015       0     1
# 015       0     1
# 015 s     0     1
# 1         1     1
# 15        0     1
# 15        0     1
# 15 s      0     1
# 15 sc     0     1
# 17        1     0
# 178       1     0
# 1780      1     0
# 1780      1     0
# 2         0     1
# 20        0     1
# 201       0     1
# 2015      0     1
# 2015      0     1
# 5         0     1
# 5         0     1
# 5 s       0     1
# 5 sc      0     1
# 5 sch     0     1
# 7         1     0
# 78        1     0
# 780       1     0
# 780       1     0
# 780 w     1     0
# 8         1     0
# 80        1     0
# 80        1     0
# 80 w      1     0
# 80 we     1     0
# c         0     1
# ch        0     1
# chl       0     1
# chle      0     1
# chlem     0     1
# e         2     2
# el        1     1
# em        1     1
# emm       1     1
# emme      1     1
# emmel     1     1
# h         0     1
# hl        0     1
# hle       0     1
# hlem      0     1
# hlemm     0     1
# l         1     2
# le        0     1
# lem       0     1
# lemm      0     1
# lemme     0     1
# m         2     2
# me        1     1
# mel       1     1
# mm        1     1
# mme       1     1
# mmel      1     1
# s         0     1
# sc        0     1
# sch       0     1
# schl      0     1
# schle     0     1
# w         1     0
# we        1     0
# wem       1     0
# wemm      1     0
# wemme     1     0
</code></pre>
",3,2,869,2015-07-15 08:00:27,https://stackoverflow.com/questions/31424687/cpu-and-memory-efficient-ngram-extraction-with-r
Why isn&#39;t stemDocument stemming?,"<p>I am using the 'tm' package in R to create a term document matrix using stemmed terms. The process is completing, but the resulting matrix includes terms that don't appear to have been stemmed, and I'm trying to understand why that is and how to fix it.</p>

<p>Here is the script for the process, which uses a couple of online news stories as the sandbox:</p>

<pre><code>library(boilerpipeR)
library(RCurl)
library(tm)

# Pull the relevant parts of the news stories using 'boilerpipeR' and 'RCurl'
url &lt;- ""http://blogs.wsj.com/digits/2015/07/14/google-mozilla-disable-flash-over-security-concerns/""
extract &lt;- LargestContentExtractor(getURL(url))
url2 &lt;- ""http://www.cnet.com/news/startup-lands-100-million-to-challenge-smartphone-superpowers-apple-and-google/""
extract2 &lt;- LargestContentExtractor(getURL(url2))

# Now put those text vectors in a corpus and create a tdm
news.corpus &lt;- VCorpus(VectorSource(c(extract, extract2)))
news.tdm &lt;- TermDocumentMatrix(news.corpus,
  control = list(removePunctuation = TRUE,
                 stopwords = TRUE,
                 stripWhitespace = TRUE,
                 stemDocument = TRUE))

# Now inspect the result
findFreqTerms(news, 4)
</code></pre>

<p>Here is the output that last line produces:</p>

<pre><code>[1] ""acadine""       ""adobe""         ""android""       ""browser""       ""challenge""     ""companies""     ""company""       ""devices""       ""firefox""       ""flash""        
[11] ""funding""       ""gong""          ""hackers""       ""international"" ""ios""           ""like""          ""million""       ""mobile""        ""mozilla""       ""mozillas""     
[21] ""new""           ""online""        ""operating""     ""said""          ""security""      ""smartphones""   ""software""      ""startup""       ""system""        ""systems""      
[31] ""tsinghua""      ""unigroup""      ""used""          ""users""         ""videos""        ""web""           ""will""  
</code></pre>

<p>In line 1, for example, we see ""companies"" and ""company"", and we see ""devices"". I thought stemming would reduce ""companies"" and ""company"" to the same stem (""compani""?), and I thought it would trim the ""s"" off plurals like ""devices"". Am I wrong about that? If not, why isn't this code producing the desired result here?</p>
","r, nlp, text-mining, tm","<p>Use <code>stemming = TRUE</code> or <code>stemming = stemDocument</code> instead of <code>stemDocument = TRUE</code>. (<code>?termFreq</code> shows that <code>stemDocument</code> is no valid control parameter.) </p>
",2,1,1603,2015-07-15 18:50:31,https://stackoverflow.com/questions/31438688/why-isnt-stemdocument-stemming
Splitting strings in R,"<p>I have a following line</p>

<pre><code>    x&lt;-""CUST_Id_8Name:Mr.Praveen KumarDOB:Mother's Name:Contact Num:Email address:Owns Car:Products held with Bank:Company Name:Salary per. month:Background:""
</code></pre>

<p>I want to extract ""CUST_Id_8"", ""Mr. Praveen Kumar"" and anything written after DOB: Mother's name: Contact Num: and so on stored in variables like Customer Id, Name, DOB and so on.</p>

<p>Please help.</p>

<p>I used </p>

<pre><code>    strsplit(x, "":"")
</code></pre>

<p>But the result is a list containing the texts. But I need blanks if there is nothing after the variable name.</p>

<p>Can any1 tell how to extract the string between two words. Like if I want to extract ""Mr. Praveen Kumar"" between Name: and DOB  </p>
","r, rstudio, text-mining, text-analysis","<p>You can use <code>regexec</code> and <code>regmatches</code> to pull out the various data items as substrings. Here's a worked example:</p>

<p>Sample data</p>

<pre><code>txt &lt;- c(""CUST_Id_8Name:Mr.Praveen KumarDOB:Mother's Name:Contact Num:Email address:Owns Car:Products held with Bank:Company Name:Salary per. month:Background:"",
         ""CUST_Id_15Name:Mr.Joe JohnsonDOB:01/02/1973Mother's Name:BarbaraContact Num:0123 456789Email address:joe@joesville.comOwns Car:YesProducts held with Bank:Savings, CurrentCompany Name:Joes villeSalary per. month:$100000Background:shady"")
</code></pre>

<p>Pattern to match:</p>

<pre><code>pattern &lt;- ""CUST_Id_(.*)Name:(.*)DOB:(.*)Mother's Name:(.*)Contact Num:(.*)Email address:(.*)Owns Car:(.*)Products held with Bank:(.*)Company Name:(.*)Salary per. month:(.*)Background:(.*)""
var_names &lt;- strsplit(pattern, ""[:_]\\(\\.\\*\\)"")[[1]]
</code></pre>

<p>Run the match:</p>

<pre><code>data &lt;- as.data.frame(do.call(""rbind"", regmatches(txt, regexec(pattern, txt))))[, -1]
colnames(data) &lt;- var_names
</code></pre>

<p>Output:</p>

<pre><code>#  CUST_Id             Name        DOB Mother's Name Contact Num
#1       8 Mr.Praveen Kumar                                     
#2      15   Mr.Joe Johnson 01/02/1973       Barbara 0123 456789
#      Email address Owns Car Products held with Bank Company Name
#1                                                                
#2 joe@joesville.com      Yes        Savings, Current   Joes ville
#  Salary per. month Background
#1                             
#2           $100000      shady
</code></pre>
",3,0,262,2015-07-20 09:57:26,https://stackoverflow.com/questions/31513552/splitting-strings-in-r
Really fast word ngram vectorization in R,"<p>edit: The new package text2vec is excellent, and solves this problem (and many others) really well.</p>

<p><a href=""https://cran.r-project.org/web/packages/text2vec/index.html"" rel=""noreferrer"">text2vec on CRAN</a>
<a href=""https://github.com/dselivanov/text2vec"" rel=""noreferrer"">text2vec on github</a>
<a href=""https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html"" rel=""noreferrer"">vignette that illustrates ngram tokenization</a></p>

<p>I have a pretty large text dataset in R, which I've imported as a character vector:</p>

<pre><code>#Takes about 15 seconds
system.time({
  set.seed(1)
  samplefun &lt;- function(n, x, collapse){
    paste(sample(x, n, replace=TRUE), collapse=collapse)
  }
  words &lt;- sapply(rpois(10000, 3) + 1, samplefun, letters, '')
  sents1 &lt;- sapply(rpois(1000000, 5) + 1, samplefun, words, ' ')
})
</code></pre>

<p>I can convert this character data to a bag-of-words representation as follows:</p>

<pre><code>library(stringi)
library(Matrix)
tokens &lt;- stri_split_fixed(sents1, ' ')
token_vector &lt;- unlist(tokens)
bagofwords &lt;- unique(token_vector)
n.ids &lt;- sapply(tokens, length)
i &lt;- rep(seq_along(n.ids), n.ids)
j &lt;- match(token_vector, bagofwords)
M &lt;- sparseMatrix(i=i, j=j, x=1L)
colnames(M) &lt;- bagofwords
</code></pre>

<p>So R can vectorize 1,000,000 million short sentences into a bag-of-words representation in about 3 seconds (not bad!):</p>

<pre><code>&gt; M[1:3, 1:7]
10 x 7 sparse Matrix of class ""dgCMatrix""
      fqt hqhkl sls lzo xrnh zkuqc mqh
 [1,]   1     1   1   1    .     .   .
 [2,]   .     .   .   .    1     1   1
 [3,]   .     .   .   .    .     .   .
</code></pre>

<p>I can throw this sparse matrix into <a href=""https://cran.r-project.org/web/packages/glmnet/index.html"" rel=""noreferrer"">glmnet</a> or <a href=""https://cran.r-project.org/web/packages/irlba/index.html"" rel=""noreferrer"">irlba</a> and do some pretty awesome quantitative analysis of textual data.  Hooray!</p>

<p>Now I'd like to extend this analysis to a bag-of-ngrams matrix, rather than a bag-of-words matrix.  So far, the fastest way I've found to do this is as follows (all of the ngram functions I could find on CRAN choked on this dataset, so <a href=""https://stackoverflow.com/questions/16489748/converting-a-list-of-tokens-to-n-grams"">I got a little help from SO</a>):</p>

<pre><code>find_ngrams &lt;- function(dat, n, verbose=FALSE){
  library(pbapply)
  stopifnot(is.list(dat))
  stopifnot(is.numeric(n))
  stopifnot(n&gt;0)
  if(n == 1) return(dat)
  pblapply(dat, function(y) {
    if(length(y)&lt;=1) return(y)
    c(y, unlist(lapply(2:n, function(n_i) {
      if(n_i &gt; length(y)) return(NULL)
      do.call(paste, unname(as.data.frame(embed(rev(y), n_i), stringsAsFactors=FALSE)), quote=FALSE)
    })))
  })
}

text_to_ngrams &lt;- function(sents, n=2){
  library(stringi)
  library(Matrix)
  tokens &lt;- stri_split_fixed(sents, ' ')
  tokens &lt;- find_ngrams(tokens, n=n, verbose=TRUE)
  token_vector &lt;- unlist(tokens)
  bagofwords &lt;- unique(token_vector)
  n.ids &lt;- sapply(tokens, length)
  i &lt;- rep(seq_along(n.ids), n.ids)
  j &lt;- match(token_vector, bagofwords)
  M &lt;- sparseMatrix(i=i, j=j, x=1L)
  colnames(M) &lt;- bagofwords
  return(M)
}

test1 &lt;- text_to_ngrams(sents1)
</code></pre>

<p>This takes about 150 seconds (not bad for a pure r function), but I'd like to go faster and extend to bigger datasets.</p>

<p>Are there any <strong>really fast</strong> functions in R for n-gram vectorization of text?  Ideally I'm looking for an <a href=""https://cran.r-project.org/web/packages/Rcpp/index.html"" rel=""noreferrer"">Rcpp</a> function that takes a character vector as input, and returns a sparse matrix of documents x ngrams as output, but would also be happy to have some guidance writing the Rcpp function myself.</p>

<p>Even a faster version of the <code>find_ngrams</code> function would be helpful, as that's the main bottleneck.  R is surprisingly fast at tokenization.</p>

<p><strong>Edit 1</strong>
Here's another example dataset:</p>

<pre><code>sents2 &lt;- sapply(rpois(100000, 500) + 1, samplefun, words, ' ')
</code></pre>

<p>In this case, my functions for creating a bag-of-words matrix take about 30 seconds and my functions for creating a bag-of-ngrams matrix take about 500 seconds.  Again, existing n-gram vectorizers in R seem to choke on this dataset (though I'd love to be proven wrong!)</p>

<p><strong>Edit 2</strong>
Timings vs tau:</p>

<pre><code>zach_t1 &lt;- system.time(zach_ng1 &lt;- text_to_ngrams(sents1))
tau_t1 &lt;- system.time(tau_ng1 &lt;- tau::textcnt(as.list(sents1), n = 2L, method = ""string"", recursive = TRUE))
tau_t1 / zach_t1 #1.598655

zach_t2 &lt;- system.time(zach_ng2 &lt;- text_to_ngrams(sents2))
tau_t2 &lt;- system.time(tau_ng2 &lt;- tau::textcnt(as.list(sents2), n = 2L, method = ""string"", recursive = TRUE))
tau_t2 / zach_t2 #1.9295619
</code></pre>
","r, vectorization, text-mining, n-gram, text2vec","<p>This is a really interesting problem, and one that I have spent a lot of time grappling with in the <strong>quanteda</strong> package.  It involves three aspects that I will comment on, although it's only the third that really addresses your question.  But the first two points explain why I have only focused on the ngram creation function, since -- as you point out -- that is where the speed improvement can be made. </p>

<ol>
<li><p><em>Tokenization.</em>  Here you are using <code>string::str_split_fixed()</code> on the space character, which is the fastest, but not the best method for tokenizing.  We  implemented this almost exactly the same was in <code>quanteda::tokenize(x, what = ""fastest word"")</code>.  It's not the best because <strong>stringi</strong> can do much smarter implementations of whitespace delimiters.  (Even the character class <code>\\s</code> is smarter, but slightly slower -- this is implemented as <code>what = ""fasterword""</code>).  Your question was not about tokenization though, so this point is just context.</p></li>
<li><p><em>Tabulating the document-feature matrix</em>.  Here we also use the <strong>Matrix</strong> package, and index the documents and features (I call them features, not terms), and create a sparse matrix directly as you do in the code above.  But your use of <code>match()</code> is a lot faster than the match/merge methods we were using through <strong>data.table</strong>.  I am going to recode the <code>quanteda::dfm()</code> function since your method is more elegant and faster.  Really, really glad I saw this!</p></li>
<li><p><em>ngram creation</em>.  Here I think I can actually help in terms of performance.  We implement this in <strong>quanteda</strong> through an argument to <code>quanteda::tokenize()</code>, called <code>grams = c(1)</code> where the value can be any integer set.  Our match for unigrams and bigrams would be <code>ngrams = 1:2</code>, for instance.  You can examine the code at <a href=""https://github.com/kbenoit/quanteda/blob/master/R/tokenize.R"">https://github.com/kbenoit/quanteda/blob/master/R/tokenize.R</a>, see the internal function <code>ngram()</code>.  I've reproduced this below and made a wrapper so that we can directly compare it to your <code>find_ngrams()</code> function.</p></li>
</ol>

<p>Code:</p>

<pre><code># wrapper
find_ngrams2 &lt;- function(x, ngrams = 1, concatenator = "" "") { 
    if (sum(1:length(ngrams)) == sum(ngrams)) {
        result &lt;- lapply(x, ngram, n = length(ngrams), concatenator = concatenator, include.all = TRUE)
    } else {
        result &lt;- lapply(x, function(x) {
            xnew &lt;- c()
            for (n in ngrams) 
                xnew &lt;- c(xnew, ngram(x, n, concatenator = concatenator, include.all = FALSE))
            xnew
        })
    }
    result
}

# does the work
ngram &lt;- function(tokens, n = 2, concatenator = ""_"", include.all = FALSE) {

    if (length(tokens) &lt; n) 
        return(NULL)

    # start with lower ngrams, or just the specified size if include.all = FALSE
    start &lt;- ifelse(include.all, 
                    1, 
                    ifelse(length(tokens) &lt; n, 1, n))

    # set max size of ngram at max length of tokens
    end &lt;- ifelse(length(tokens) &lt; n, length(tokens), n)

    all_ngrams &lt;- c()
    # outer loop for all ngrams down to 1
    for (width in start:end) {
        new_ngrams &lt;- tokens[1:(length(tokens) - width + 1)]
        # inner loop for ngrams of width &gt; 1
        if (width &gt; 1) {
            for (i in 1:(width - 1)) 
                new_ngrams &lt;- paste(new_ngrams, 
                                    tokens[(i + 1):(length(tokens) - width + 1 + i)], 
                                    sep = concatenator)
        }
        # paste onto previous results and continue
        all_ngrams &lt;- c(all_ngrams, new_ngrams)
    }

    all_ngrams
}
</code></pre>

<p>Here is the comparison for a simple text:</p>

<pre><code>txt &lt;- c(""The quick brown fox named Seamus jumps over the lazy dog."", 
         ""The dog brings a newspaper from a boy named Seamus."")
tokens &lt;- tokenize(toLower(txt), removePunct = TRUE)
tokens
# [[1]]
# [1] ""the""    ""quick""  ""brown""  ""fox""    ""named""  ""seamus"" ""jumps""  ""over""   ""the""    ""lazy""   ""dog""   
# 
# [[2]]
# [1] ""the""       ""dog""       ""brings""    ""a""         ""newspaper"" ""from""      ""a""         ""boy""       ""named""     ""seamus""   
# 
# attr(,""class"")
# [1] ""tokenizedTexts"" ""list""     

microbenchmark::microbenchmark(zach_ng &lt;- find_ngrams(tokens, 2),
                               ken_ng &lt;- find_ngrams2(tokens, 1:2))
# Unit: microseconds
#                                expr     min       lq     mean   median       uq     max neval
#   zach_ng &lt;- find_ngrams(tokens, 2) 288.823 326.0925 433.5831 360.1815 542.9585 897.469   100
# ken_ng &lt;- find_ngrams2(tokens, 1:2)  74.216  87.5150 130.0471 100.4610 146.3005 464.794   100

str(zach_ng)
# List of 2
# $ : chr [1:21] ""the"" ""quick"" ""brown"" ""fox"" ...
# $ : chr [1:19] ""the"" ""dog"" ""brings"" ""a"" ...
str(ken_ng)
# List of 2
# $ : chr [1:21] ""the"" ""quick"" ""brown"" ""fox"" ...
# $ : chr [1:19] ""the"" ""dog"" ""brings"" ""a"" ...
</code></pre>

<p>For your really large, simulated text, here is the comparison:</p>

<pre><code>tokens &lt;- stri_split_fixed(sents1, ' ')
zach_ng1_t1 &lt;- system.time(zach_ng1 &lt;- find_ngrams(tokens, 2))
ken_ng1_t1 &lt;- system.time(ken_ng1 &lt;- find_ngrams2(tokens, 1:2))
zach_ng1_t1
#    user  system elapsed 
# 230.176   5.243 246.389 
ken_ng1_t1
#   user  system elapsed 
# 58.264   1.405  62.889 
</code></pre>

<p>Already an improvement, I'd be delighted if this could be improved further.  I also should be able to implement the faster <code>dfm()</code> method into <strong>quanteda</strong> so that you can get what you want simply through:</p>

<pre><code>dfm(sents1, ngrams = 1:2, what = ""fastestword"",
    toLower = FALSE, removePunct = FALSE, removeNumbers = FALSE, removeTwitter = TRUE)) 
</code></pre>

<p>(That already works but is slower than your overall result, because the way you create the final sparse matrix object is faster - but I will change this soon.)</p>
",10,16,4998,2015-07-22 17:50:11,https://stackoverflow.com/questions/31570437/really-fast-word-ngram-vectorization-in-r
all possible wordform completions of a (biomedical) word&#39;s stem,"<p>I'm familiar with word stemming and completion from the tm package in R.  </p>

<p>I'm trying to come up with a quick and dirty method for finding all variants of a given word (within some corpus.)  For example, I'd like to get ""leukocytes"" and ""leuckocytic"" if my input is ""leukocyte"".</p>

<p>If I had to do it right now, I would probably just go with something like:</p>

<pre><code>library(tm)
library(RWeka)
dictionary &lt;- unique(unlist(lapply(crude, words)))
grep(pattern = LovinsStemmer(""company""), 
    ignore.case = T, x = dictionary, value = T)
</code></pre>

<p>I used Lovins because Snowball's Porter doesn't seem to be aggressive enough.</p>

<p>I'm open to suggestions for other stemmers, scripting languages (Python?), or entirely different approaches.</p>
","python, r, nlp, bioinformatics, text-mining","<p>This solution requires preprocessing your corpus. But once that is done it is a very quick dictionary lookup. </p>

<pre><code>from collections import defaultdict
from stemming.porter2 import stem

with open('/usr/share/dict/words') as f:
    words = f.read().splitlines()

stems = defaultdict(list)

for word in words:
    word_stem = stem(word)
    stems[word_stem].append(word)

if __name__ == '__main__':
    word = 'leukocyte'
    word_stem = stem(word)
    print(stems[word_stem])
</code></pre>

<p>For the <code>/usr/share/dict/words</code> corpus, this produces the result </p>

<pre><code>['leukocyte', ""leukocyte's"", 'leukocytes']
</code></pre>

<p>It uses the <a href=""https://pypi.python.org/pypi/stemming/1.0"" rel=""nofollow noreferrer""><code>stemming</code></a> module that can be installed with </p>

<pre><code>pip install stemming
</code></pre>
",1,3,329,2015-07-23 19:30:27,https://stackoverflow.com/questions/31596476/all-possible-wordform-completions-of-a-biomedical-words-stem
Looking for an optimized way of replacing list patterns in long documents,"<p>Using tm package, I have a corpus of 10,900 documents (<em>docs</em>).</p>

<pre><code>docs = Corpus(VectorSource(abstracts$abstract))
</code></pre>

<p>And I also have a list of terms (<em>termslist</em>) and all their synonyms and different spellings. I use it to transform each of synonyms or spellings into one term.</p>

<pre><code>Term, Synonyms
term1, synonym1
term1, synonym2
term1, synonym3
term2, synonym1
... etc
</code></pre>

<p>The way I'm doing it right now is to loop through all documents, and another nester loop through all terms to find and replace.</p>

<pre><code>for (s in 1:length(docs)){
  for (i in 1:nrow(termslist)){
    docs[[s]]$content&lt;-gsub(termslist[i,2], termslist[i,1], docs[[s]])
  }
  print(s)
}
</code></pre>

<p>Currently this takes a second for a document (having around 1000 row in <em>termslist</em>), which means 10,900 seconds which is roughly 6 hours!</p>

<p>Is there a more optimized way of doing this within tm package or within R generally?</p>

<p><strong>UPDATE:</strong> </p>

<p>mathematical.coffee's answer was actually helpful. I had to re-create a table with unique terms as rows and the second column would be the synonyms separated by '|' , then just loop over them. Now it takes significantly less time than before.</p>

<p>**[The messy] code of creating the new table:</p>

<pre><code>newtermslist&lt;-list()
authname&lt;-unique(termslist[,1])
newtermslist&lt;- cbind(newtermslist,authname)
syns&lt;-list()
for (i in seq(authname)){
  syns&lt;- rbind(syns,
                   paste0('(', 
                          paste(termslist[which(termslist[,1]==authname[i]),2],collapse='|')
                          , ')')
  )
}
newtermslist&lt;-cbind(newtermslist,syns)
newtermslist&lt;-cbind(unlist(newtermslist[,1]),unlist(newtermslist[,2]))
</code></pre>
","r, text-mining, tm","<p>I think when you wish to perform many replacements, this may be the only way to do it (i.e. sequentially, saving the replaced output as the input for the next replacement).</p>

<p>However, you might gain some speed trying (you will have to do some benchmarking to compare):</p>

<ul>
<li>use <code>fixed=T</code> (since your synonyms are not regexes but literal spellings), <code>useBytes=T</code> (**see <code>?gsub</code> - if you have multibyte locale this may or may not be a good idea). Or</li>
<li>compress your termslist - if <code>blue</code> has synonyms <code>cerulean</code>, <code>cobalt</code> and <code>sky</code>, then your regex could be <code>(cerulean|cobalt|sky)</code> with replacement <code>blue</code>, so that all the synonyms for <code>blue</code> are replaced in one iteration rather than in 3 separate ones. To do this, you'd preprocess your termslist - e.g. <code>newtermslist &lt;- ddply(terms, .(term), summarize, regex=paste0('(', paste(synonym, collapse='|'), ')'))</code> and then do your current loop over this. You will have <code>fixed=F</code> (the default, i.e. use regex).</li>
<li>see also <code>?tm_map</code> and <code>?content_transformer</code>. I'm not sure if these will speed things up at all, but you could try.</li>
</ul>

<p>(Re benchmarking - try <code>library(rbenchmark); benchmark(expression1, expression2, ...)</code>, or good ol' <code>system.time</code> for timing, <code>Rprof</code> for profiling)</p>
",1,3,91,2015-07-25 00:28:59,https://stackoverflow.com/questions/31621620/looking-for-an-optimized-way-of-replacing-list-patterns-in-long-documents
Regular expression taking too much time to compile in R,"<p>I read a file in ratingsFile using     </p>

<pre><code>ratingsFile &lt;- readLines(""~/ratings.list"",encoding = ""UTF-8"")
</code></pre>

<p>The file's first few lines looks like</p>

<pre><code>  0000000125  1478759   9.2  The Shawshank Redemption (1994)
  0000000125  1014575   9.2  The Godfather (1972)
  0000000124  683611   9.0  The Godfather: Part II (1974)
  0000000124  1451861   8.9  The Dark Knight (2008)
  0000000124  1150611   8.9  Pulp Fiction (1994)
  0000000133  750978   8.9  Schindler's List (1993)
</code></pre>

<p>Using regular expression I extracted </p>

<pre><code>  match &lt;- gregexpr(""[0-9A-Za-z;'$%&amp;?@./]+"",ratingsFile)
  match &lt;- regmatches(ratingsFile,match)


  next_match &lt;- gregexpr(""[0-9][.][0-9]+"",ratingsFile)
  next_match &lt;- regmatches(ratingsFile,next_match)
</code></pre>

<p>The sample output of match looks like </p>

<pre><code>  ""0000000125"" ""1014575""    ""9.2""        ""The""        ""Godfather""  ""1972""  
</code></pre>

<p>For cleaning that data and changing to the form i need I did</p>

<pre><code>  movies_name &lt;- character(0)
  rating &lt;- character(0)
  for(i in 1:length(match)){

      match[[i]]&lt;-match[[i]][-1:-3] #for removing not need cols 
      len &lt;- length(match[[i]])
      match[[i]]&lt;-match[[i]][-len]#removing last column also not needed
      movies_name&lt;-append(movies_name,paste(match[[i]],collapse ="" ""))
      #appending movies name
      rating &lt;- append(rating,next_match[[i]]) 
      #appending rating
}
</code></pre>

<p>Now this final block of code is taking too long to execute.I have left he compilation process for hours but still it is not completed as the file is  636497 lines long.</p>

<p>How can i reduce the compilation time in this case?</p>
","regex, r, time-complexity, text-mining","<p>Try this:</p>

<pre><code>ratingsFile &lt;- readLines(n = 6)
0000000125  1478759   9.2  The Shawshank Redemption (1994)
0000000125  1014575   9.2  The Godfather (1972)
0000000124  683611   9.0  The Godfather: Part II (1974)
0000000124  1451861   8.9  The Dark Knight (2008)
0000000124  1150611   8.9  Pulp Fiction (1994)
0000000133  750978   8.9  Schindler's List (1993)
setNames(as.data.frame(t(sapply(regmatches(ratingsFile, regexec(""\\d{10}\\s+\\d+\\s+([0-9.]+)\\s+(.*?)\\s\\(\\d{4}\\)"", ratingsFile)), ""["", -1))), c(""rating"", ""movie_name""))
#   rating               movie_name
# 1    9.2 The Shawshank Redemption
# 2    9.2            The Godfather
# 3    9.0   The Godfather: Part II
# 4    8.9          The Dark Knight
# 5    8.9             Pulp Fiction
# 6    8.9         Schindler's List
</code></pre>
",2,1,124,2015-07-28 09:32:34,https://stackoverflow.com/questions/31672478/regular-expression-taking-too-much-time-to-compile-in-r
Text mining - feature with a lot of spelling probs and differentations,"<p>I'd like to make sense of the feature ""color"". The problem is that it has more than 15.000 specifications with a lot of spelling problems (e.g brwon &lt;-> brown, oliv &lt;-> olive), but also differentations (lightblue &lt;-> blue) in it.</p>

<p>How is it possible to make sense of such a feature? Are there any resources, R packages or python modules?</p>
","r, nlp, text-mining, knime","<p>R can use aspell (command is available). But you need to install aspell on your machine and maybe even hunspell. Hunspell is used as a spellcheck in chrome / firefox and Rstudio for example. </p>

<p>Read this <a href=""http://journal.r-project.org/archive/2011-2/RJournal_2011-2_Hornik+Murdoch.pdf"" rel=""nofollow"">journal</a> for more information about aspell and hunspell in R. </p>

<p>But this will only take care of spelling errors. You could use regex to look for main colors. </p>
",1,-4,457,2015-07-29 10:19:44,https://stackoverflow.com/questions/31697680/text-mining-feature-with-a-lot-of-spelling-probs-and-differentations
"text mining with tm package in R ,remove words starting from [http] or any other specifc word","<p>I am new to R and text mining. I had made a word cloud out of twitter feed related to some term. The problem that I'm facing is that in the wordcloud it shows http:... or htt...
How do I deal about this issue
I tried using metacharacter * but I still doubt if I'm applying it right</p>

<blockquote>
  <p>tw.text = removeWords(tw.text,c(stopwords(""en""),""rt"",""http\\*""))</p>
</blockquote>

<p>somebody into text-minning please help me with this.</p>
","r, text-mining, tm, word-cloud, metacharacters","<p>If you are looking to remove URLs from your string, you may use:</p>

<pre><code>gsub(""(f|ht)tp(s?)://(.*)[.][a-z]+"", """", x)
</code></pre>

<p>Where <code>x</code> would be:</p>

<pre><code>x &lt;- c(""some text http://idontwantthis.com"", 
         ""same problem again http://pleaseremoveme.com"")
</code></pre>

<hr>

<p>It would be easier to provide you with a specific answer if you could post sample of your data but the following example would give you a clean text with no URLs:</p>

<pre><code>&gt; clean_x &lt;- gsub(""(f|ht)tp(s?)://(.*)[.][a-z]+"", """", x)
&gt; clean_x
[1] ""some text ""          ""same problem again ""
</code></pre>

<p><em>As a side point, I would suggest that it may be worth searching for the existing methods to clean text before mining. For example the <code>clean</code> function discussed <a href=""http://rpackages.ianhowson.com/cran/quanteda/man/clean.html"" rel=""nofollow"">here</a> would enable you to do this automatically. On similar lines, there are function to clean your text from tweets (<code>#</code>,<code>@</code>), punctuation and other undesirable entries.</em></p>
",3,1,7339,2015-07-29 13:52:21,https://stackoverflow.com/questions/31702488/text-mining-with-tm-package-in-r-remove-words-starting-from-http-or-any-other
algorithm to identify book titles in tweets,"<p>I am trying to make a program that will analyse a large set of tweets and generate a report about the top 10 most popular books that people have tweeted about. The problem is I don't have any idea how to identify the book titles in tweets. It would be great if someone could name the algorithm for such type of work or at least guide me in the right direction.</p>
","algorithm, twitter, nlp, nltk, text-mining","<p>If you don't have the list of book titles, <a href=""https://en.wikipedia.org/wiki/Lists_of_books"" rel=""nofollow"">Wikipedia provides some lists</a>. But if your goal is to compare new books (the best sellers of the summer) I guess they won't be in these lists. You can find other lists on Internet... Anyway (for reliability) you need a list.</p>

<p>Then, as @Adam_G told ealier:</p>

<blockquote>
  <p>There is no way to automatically extract ""book titles"". For instance, how could you differentiate between someone talking about ""the Martian"" they saw in their backyard last night and the book by Andy Weir?</p>
</blockquote>

<p>Let's imagine two books ""The cat"" and ""The fear of green sharks"".
There are many tweets containing the sequence ""the cat"" but in most cases these tweets are not about the book. Conversely, all the tweets containing ""the fear of the green sharks"" are about the book obviously.</p>

<p>So you should assign for each sequence of tokens a ""probability"" <code>P</code> to be the title of the book. In the previous example, ""the cat"" has a very low <code>P</code> and ""the fear of the green sharks"" a very high <code>P</code>. You can compute the value of <code>P</code> from the number of words in the title and the frequency of these words (and 2-gram, 3-gram,...) in the whole language (which can be computed using a big corpus) </p>

<p>Now consider this tweet:</p>

<blockquote>
  <p>I read ""The Cat"" and I loved it!!</p>
</blockquote>

<p>As human we understand that ""The Cat"" is a book because the tweet contains ""read"".
So, for titles with a low <code>P</code> (e.g. ""the cat"") you can try to use a machine learning algorithm to know whether the tweet is really about the book. The idea is to compute the probability for a tweet containing a title to really be about the book given the words in the tweet. Learn more about machine learning algorithms, for example Naive Bayes classifier.</p>
",1,-2,216,2015-08-01 21:45:53,https://stackoverflow.com/questions/31766550/algorithm-to-identify-book-titles-in-tweets
Count how many times specific words are used,"<p>I want to perform textmining on several bank account descriptions. My first step would be get a ranking of the words that are used the most in the description.</p>

<p>So lets say i have a dataframe that looks like this:</p>

<pre><code>    a                       b
    1 1          House expenses
    2 2 Office furniture bought
    3 3 Office supplies ordered
</code></pre>

<p>Then I want to create a ranking of the use of the words. Like this:</p>

<pre><code>    Name      Times
    1. Office   2
    2. Furniture 1
</code></pre>

<p>Etc...</p>

<p>Any thoughts on how I can quickly get an overview of the words that are used most in the description?</p>
","r, text-mining","<p>Another way around this is using the tm package. 
You can create a corpus:</p>

<pre><code>     require(tm)
     corpus &lt;- Corpus(DataframeSource(data))
     dtm&lt;-DocumentTermMatrix(corpus)
     dtmDataFrame &lt;- as.data.frame(inspect(dtm))
</code></pre>

<p>by default it makes term frequencies tf using ""weightTf"". I converted the Document Term Matrix into a Dataframe. 
 Now what you have is a row per document, a column for each term and the value is the term frequency for every term, you can just create the rankings in a straightforward way, adding all values for each column. </p>

<pre><code>colSums(dtmDataFrame)
</code></pre>

<p>You can sort it too after, whatever. The good point of using tm is that you can filter easily words out, process them with bunch of things like stop words, remove punctuations, stemming, remove sparse words in case you need it.</p>
",2,2,901,2015-08-06 11:41:13,https://stackoverflow.com/questions/31854843/count-how-many-times-specific-words-are-used
Best Features for Term Level Clustering,"<p>At the moment, I am working on a project that related to mining Twitter data. The aim of the project is to find the themes that can be used to represent the set of tweets. To help us finding the themes, we came up with an idea to do term level clustering. The terms are some important concepts that already extracted using some TextMining tools.
Well, my main question is, what are the best features to define term similarity? In this project, due to an insufficient amount of data, I am doing the unsupervised learning, which is clustering using the k-means algorithm.
I do have some extracted features. As I understand, one way to know the semantic (not actually) meaning of a term is by seeing the context of which the term is mentioned. Therefore, what I have at the moment are preceding and following WORD and POS of the term. For instance:</p>

<pre><code>I drink a cup of XYZ
She had a spoon of ABC yesterday.
</code></pre>

<p>By seeing the preceding word and POS - cup/NN and of/IN for XYZ and spoon/NN and of/IN for ABC - I knew that XYZ and ABC might be a liquid material or component. Well, it sounds very naive, in fact, I don't get the good clusters. In addition to the previous features, I have some named entity types that I considered as features. For instance, entity type like Person, Location, Problem (in medical), MEDTERM etc.</p>

<p>So, what are the common features for term level clustering? Any comments and suggestions would be appreciated. I do open to any guidance, such as paper, link etc. Thanks </p>

<p>EDIT: In addition to those features, I've extracted the head nouns of each term and considered it as one of my features. I am thinking of using a head noun in the case for multi-word terms.</p>
","twitter, cluster-analysis, k-means, text-mining, feature-extraction","<p>Well, let me see if I understood correctly what you need. You already extracted/found the terms you want as centres of your clusters, and now you want to find all terms which are similar to them so they get grouped in the proper cluster?. </p>

<p>In general you need to define a similarity measure (distance) and here is the main point, what you want that similarity distance to measure or determine. If you are looking for term to term similarity, just letters then you can try things like Levenshtein distance for example, but if what you want to find are contextual similar terms, even they are written in a very different way but could mean the same thing, thats different from Levenshtein pretty much harder to do.</p>

<p>What is important to keep in mind is that you need a measure of similarity to find the similar terms. What I see you call features some named entity types, normally k-means is bad when dealing with non continuos data. </p>
",1,0,57,2015-08-07 20:00:06,https://stackoverflow.com/questions/31885716/best-features-for-term-level-clustering
Splitting columns in a dataframe,"<p>I have a dataframe named a with 1 variable having the following structure</p>

<pre><code>    Name                Pid Pri Thd  Hnd   Priv        CPU Time    Elapsed Time 
    Idle                  0   0   4    0      0    13:26:52.515     4:18:08.670
    System                4   8 148 1199   1616     0:10:14.750     4:18:08.670
    smss                388  11   2   49    336     0:00:00.109     4:18:08.597 
</code></pre>

<p>Now I want to split the dataframe into the columns with colnames Name, Pid, Pro, Thd, Hnd and so on.</p>

<p>I tried strsplit:</p>

<pre><code>    df&lt;-strsplit(a,"" "")
</code></pre>

<p>but the results were like they were splitted into all those characters having a space. 
Please help</p>

<pre><code>&gt; dput(a)
c(""Name                Pid Pri Thd  Hnd   Priv        CPU Time    Elapsed Time "", 
""Idle                  0   0   4    0      0    13:26:52.515     4:18:08.670"", 
""System                4   8 148 1199   1616     0:10:14.750     4:18:08.670"", 
""smss                388  11   2   49    336     0:00:00.109    4:18:08.597"" )
</code></pre>
","r, dataframe, text-mining","<p>We can use <code>read.table</code></p>

<pre><code>  df1 &lt;- read.table(text=a, sep='', header=TRUE, stringsAsFactors=FALSE)
  df1 
  #    Name Pid Pri Thd  Hnd
  #1   Idle   0   0   4    0
  #2 System   4   8 148 1199
  #3   smss 388  11   2   49

  str(df1)
  #'data.frame':    3 obs. of  5 variables:
  #$ Name: chr  ""Idle"" ""System"" ""smss""
  #$ Pid : int  0 4 388
  #$ Pri : int  0 8 11
  #$ Thd : int  4 148 2
  #$ Hnd : int  0 1199 49
</code></pre>

<p>Suppose the new object is 'a1', we read the lines (<code>read.table</code>) without the header line, and then set the column names of the new dataset ('df2') after creating some quotes for ""CPU Time"" and ""Elapsed Time"" using <code>gsub</code> (to be read as a single string) with <code>lookarounds</code> and get the <code>vector</code> of words with <code>scan</code></p>

<pre><code>  df2 &lt;- read.table(text=a1[-1], sep='', header=FALSE, 
                 stringsAsFactors=FALSE)
  colnames(df2) &lt;-  scan(text=gsub('(?&lt;=Time)\\s|\\s(?=Elapsed|CPU)',
                ""'"", a1[1], perl=TRUE), what='', quiet=TRUE)
  df2
  #    Name Pid Pri Thd  Hnd Priv     CPU Time Elapsed Time
  #1   Idle   0   0   4    0    0 13:26:52.515  4:18:08.670
  #2 System   4   8 148 1199 1616  0:10:14.750  4:18:08.670
  #3   smss 388  11   2   49  336  0:00:00.109  4:18:08.597
</code></pre>

<h3>data</h3>

<pre><code>  a &lt;- c(""Name                Pid Pri Thd  Hnd"",
   ""Idle                  0   0   4    0"", 
   ""System                4   8 148 1199   "", 
  ""smss                388  11   2   49    ""
  )
</code></pre>
",1,0,68,2015-08-10 09:56:51,https://stackoverflow.com/questions/31916570/splitting-columns-in-a-dataframe
PHP extracting data from text,"<p>I have an old windows 95 program that exports data without account numbers, seasonal accounts, and if accounts contains a sub account.</p>

<p>I am, however, able to print customer information and notes that has the above information to a pdf file and copy that text to notepad; which I would like to extract the data.</p>

<p>The order the data: 1) page headers (I do not need this data.)</p>

<p><strong>Company Name</strong></p>

<p><strong>Customer Information and Notes</strong></p>

<p><strong>Computed Monday, August 10 2015 Page 1</strong></p>

<p>2) standard titles and 3) the data after titles:</p>

<p><strong>Ser Name:</strong> Block, Sunny <strong>Route:</strong> 1</p>

<p><strong>Address:</strong> 3354 ASPEN RD. <strong>Frequency:</strong> Monthly</p>

<p><strong>Address:</strong> ST PETE, GA 33333 <strong>Week/Day:</strong> First Monday</p>

<p><strong>City State Zip:</strong> data <strong>Sched Time (HH:MM):</strong> 10:00A</p>

<p><strong>Ser Phone:</strong> 555-1212 <strong>Service:</strong> BASIC SERVICE</p>

<p><strong>Bill to:</strong> BLOCK,SUNNY <strong>Rate ($):</strong> 24.00</p>

<p><strong>Company Name</strong></p>

<p><strong>Customer Information and Notes</strong></p>

<p><strong>Computed Monday, August 10 2015 Page 2</strong></p>

<p><strong>Address:</strong> 1123 Sligh <strong>Terms:</strong> CASH</p>

<p><strong>Address:</strong> Apt B</p>

<p><strong>notes:</strong> Sunny has a mean dog</p>

<p>Do not enter unless dog is put up</p>

<p>Then it loops to next customers data and so on.</p>

<p>The main titles never change, such as, ser name, route, address, notes, phone. There is a set number of titles in order; however, the title <strong>notes:</strong> can take 1 -16 lines; and the header is random throughout the data. and although the titles are in order, address is titled 4 times for both service- line 1 and line 2 and billing addresses- line 1 and line 2.</p>

<p>I would like to set variables to these titles and only take what's after them; the extraction part through PHP. Is there anyway to do this?</p>
","php, mysql, text-mining","<p><strong>I don't think it's possible for a perfect solution, but FWIW, maybe this is good enough for you.</strong></p>

<p>Without a known / reliable delimiter between clients, I can't think of any good way you can get the notes without having the header stuff for the next company included, unless you can do something involving a big lookup table of all client names. </p>

<p>I do have (an ugly) regex that may reliably help as far as the other stuff though:</p>

<pre><code>$content='[the contents of your file]';
preg_match_all('~(Ser Name|Route|Address|Frequency|Week/Day|City State Zip|Sched Time \(HH:MM\)|Ser Phone|Service|Bill to|Rate \(\$\)|Terms|notes):\s*((?:(?!Ser Name|Route|Address|Frequency|Week/Day|City State Zip|Sched Time \(HH:MM\)|Ser Phone|Service|Bill to|Rate \(\$\)|Terms|notes).)+)~is',$content,$matches);
</code></pre>

<p>So this basically looks for the ""header"" and puts into first captured group, and then matches up to the next ""header"" and puts that into 2nd captured group. </p>

<p>Perhaps this is good enough for you, but TBH I can't think of anything better you can do, unless you can improve your extraction to a better format. </p>

<p>So your example data would output:</p>

<pre><code>Array
(
    [0] =&gt; Array
        (
            [0] =&gt; Ser Name: Block, Sunny 
            [1] =&gt; Route: 1


            [2] =&gt; Address: 3354 ASPEN RD. 
            [3] =&gt; Frequency: Monthly


            [4] =&gt; Address: ST PETE, GA 33333 
            [5] =&gt; Week/Day: First Monday


            [6] =&gt; City State Zip: data 
            [7] =&gt; Sched Time (HH:MM): 10:00A


            [8] =&gt; Ser Phone: 555-1212 
            [9] =&gt; Service: BASIC SERVICE


            [10] =&gt; Bill to: BLOCK,SUNNY 
            [11] =&gt; Rate ($): 24.00

Company Name

Customer Information and Notes

Computed Monday, August 10 2015 Page 2


            [12] =&gt; Address: 1123 Sligh 
            [13] =&gt; Terms: CASH


            [14] =&gt; Address: Apt B


            [15] =&gt; notes: Sunny has a mean dog
        )

    [1] =&gt; Array
        (
            [0] =&gt; Ser Name
            [1] =&gt; Route
            [2] =&gt; Address
            [3] =&gt; Frequency
            [4] =&gt; Address
            [5] =&gt; Week/Day
            [6] =&gt; City State Zip
            [7] =&gt; Sched Time (HH:MM)
            [8] =&gt; Ser Phone
            [9] =&gt; Service
            [10] =&gt; Bill to
            [11] =&gt; Rate ($)
            [12] =&gt; Address
            [13] =&gt; Terms
            [14] =&gt; Address
            [15] =&gt; notes
        )

    [2] =&gt; Array
        (
            [0] =&gt; Block, Sunny 
            [1] =&gt; 1


            [2] =&gt; 3354 ASPEN RD. 
            [3] =&gt; Monthly


            [4] =&gt; ST PETE, GA 33333 
            [5] =&gt; First Monday


            [6] =&gt; data 
            [7] =&gt; 10:00A


            [8] =&gt; 555-1212 
            [9] =&gt; BASIC SERVICE


            [10] =&gt; BLOCK,SUNNY 
            [11] =&gt; 24.00

Company Name

Customer Information and Notes

Computed Monday, August 10 2015 Page 2


            [12] =&gt; 1123 Sligh 
            [13] =&gt; CASH


            [14] =&gt; Apt B


            [15] =&gt; Sunny has a mean dog
        )

)
</code></pre>
",2,1,94,2015-08-10 23:36:56,https://stackoverflow.com/questions/31930869/php-extracting-data-from-text
Exporting relevant words TF-IDF TextBlob python,"<p>I followed this <a href=""http://blog.christianperone.com/?p=1589"" rel=""nofollow"">tutorial</a> to search the relevant words in my documents. My code:</p>

<pre><code>&gt;&gt;&gt; for i, blob in enumerate(bloblist):
print i+1
scores = {word: tfidf(word, blob, bloblist) for word in blob.words}
sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)
for word, score in sorted_words[:10]:
    print(""\t{}, score {}"".format(word, round(score, 5)))

1
 k555ld-xx1014h, score 0.19706
 fuera, score 0.03111
 dentro, score 0.01258
 i5, score 0.0051
 1tb, score 0.00438
 sorprende, score 0.00358
 8gb, score 0.0031
 asus, score 0.00228
 ordenador, score 0.00171
 duro, score 0.00157 
2
 frentes, score 0.07007
 write, score 0.05733
 acceleration, score 0.05255
 aprovechando, score 0.05255
 . . . 
</code></pre>

<p>Here's my problem, I would like to export a data frame with the following information: index, 10 top words (separated with commas). Something that i can save with pandas dataframe. 
Example:</p>

<pre><code>TOPWORDS = pd.DataFrame(topwords.items(), columns=['ID', 'TAGS'])
</code></pre>

<p>Thank you all in advance. </p>
","python, text-mining, tf-idf, textblob","<p>Solved!</p>

<p>Here's my solution, perhaps not the best but it works. </p>

<pre><code>tags = {}
for i, blob in enumerate(bloblist):
      scores = {word: tfidf(word, blob, bloblist) for word in blob.words}
      sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)
      a =""""
      for word, score in sorted_words[:10]:
           a= a + ' '+ word
      tags[i+1] = a
</code></pre>
",1,1,592,2015-08-12 08:16:15,https://stackoverflow.com/questions/31959668/exporting-relevant-words-tf-idf-textblob-python
Using R to filter comments for text mining,"<p>I am using R and relatively new to programming so any help will be appreciated. </p>

<p>I am text mining for a survey and would like to filter comments with a combination of words. The data set has been read from a csv file.  </p>

<p>I want to filter the comments that contain the words ""abroad"" and ""charges""</p>

<p>I am using the grepl function to recognise the pattern with in the comments.
I have managed to filter the data in the Comment section which has the words
""abroad"" and ""charges"" by using the following code:</p>

<pre><code>ac &lt;- filter(data, grepl(""abroad|charges"", Comment))

  ac$Comment
</code></pre>

<p>This returns comments with words ""abroad"" and ""charges"" but it returns comments which can either have ""abroad"" or ""charges"". I would like a combination of both words. I tried replacing | with &amp; but this does not work.</p>

<p>I have also tried subset:</p>

<pre><code>ac &lt;- subset(data, Comment %in% c(""abroad"", ""charges""))

ac$Comment
</code></pre>

<p>None of these return the desired results. Am  I missing something obvious? How can I view comments that contain only certain words in them. So if I further wanted to explore my text I could try to find the combination of ""abroad"" and ""charges"" and ""expensive."" </p>

<p>Thanks any help would be great. </p>
","r, text-mining","<p>We can use a double <code>grep</code> with <code>&amp;</code> operator inside the <code>filter</code> and it should only be <code>TRUE</code> for words that contain both 'abroad' and 'charges' in the <code>string</code>.</p>

<pre><code> filter(data, grepl(""abroad"", Comment) &amp; grepl('charges', Comment))
</code></pre>
",1,2,977,2015-08-12 10:38:23,https://stackoverflow.com/questions/31962798/using-r-to-filter-comments-for-text-mining
How to create a good NER training model in OpenNLP?,"<p>I just have started with OpenNLP. I need to create a simple training model to recognize name entities. </p>

<p>Reading the doc here <a href=""https://opennlp.apache.org/docs/1.8.0/apidocs/opennlp-tools/opennlp/tools/namefind"" rel=""nofollow noreferrer"">https://opennlp.apache.org/docs/1.8.0/apidocs/opennlp-tools/opennlp/tools/namefind</a> I see this simple text to train the model:</p>

<pre><code>&lt;START:person&gt; Pierre Vinken &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .
Mr . &lt;START:person&gt; Vinken &lt;END&gt; is chairman of Elsevier N.V. , the Dutch publishing group .
&lt;START:person&gt; Rudolph Agnew &lt;END&gt; , 55 years old and former chairman of Consolidated Gold Fields PLC ,
    was named a director of this British industrial conglomerate .
</code></pre>

<p>The questions are two:</p>

<ul>
<li><p>Why should i have to put the names of the persons in a text (phrase) context ? Why not write person's name one for each line? like:</p>

<pre><code>&lt;START:person&gt; Robert &lt;END&gt;

&lt;START:person&gt; Maria &lt;END&gt;

&lt;START:person&gt; John &lt;END&gt;
</code></pre></li>
<li><p>How can I also add extra information to that name?
For example I would like to save the information Male/Female for each name.</p></li>
</ul>

<p>(I know there are systems that try to understand it reading the last letter, like the ""a"" for <strong>Female</strong> etc but i would like to add it myself)</p>

<p>Thanks.</p>
","java, nlp, text-mining, opennlp, named-entity-recognition","<p>The answer to your first question is that the algorithm works on surrounding context(tokens) within a sentence; it's not just a simple lookup mechanism. OpenNLP uses maximum entropy, which is a form of multinomial logistic regression to build its model. The reason for this is to reduce ""word sense ambiguity,"" and find entities in context. For instance, if my name is April, I can easily get confused with the month of April, and if my name is May, then I would get confused with the month of May as well as the verb may. For your second part of the first question, you could make a list of names that are known, and use those names in a program that looks at your sentences and automatically annotates them to help you create a training set, however making a list of names alone without context will not train the model sufficiently or at all. In fact, there is an OpenNLP addon called the ""modelbuilder addon"" designed for this: you give it a file of names, and it uses the names and some of your data (sentences) to train a model. If you are looking for particular names of generally non ambiguous entities, you may be better off just using a list and something like regex to discover names rather than NER.</p>

<p>As for your second question there are a few options, but in general, I don't think NER is a great tool for delineating something like gender, however with enough training sentences you may get decent results. Since NER uses a model based on surrounding tokens in your sentence training set to establish the existence of a named entity, it can't do much in terms of identifying gender. You may be better off finding all person names, then referencing an index of names that you know are male or female to get a match. Also, some names, like Pat, are both male and female, and in most textual data there will be no indication of which it is to neither human nor machine. That being said, you could create a male and female model separately, or you could create different entity types within the same model. You could use an annotation like this (using different entity type names of male.person and female.person). I've never tried this but it might do ok, you'd have to test it on your data.</p>

<pre><code>&lt;START:male.person&gt; Pierre Vinken &lt;END&gt; , 61 years old , will join the board as a nonexecutive director Nov. 29 .
Mrs . &lt;START:female.person&gt; Maria &lt;END&gt; is chairman of Elsevier N.V. , the Dutch publishing group
</code></pre>

<p>NER= Named Entity Recognition</p>

<p>HTH</p>
",20,18,7753,2015-08-14 13:43:46,https://stackoverflow.com/questions/32011615/how-to-create-a-good-ner-training-model-in-opennlp
R tm TermDocumentMatrix based on a sparse matrix,"<p>I have a collection of books in txt format and want to apply some procedures of the <code>tm</code> R library to them. However, I prefer to clean the texts in bash rather than in R because it is much faster.</p>

<p>Suppose I am able to get from bash a data.frame such as:</p>

<pre><code>book term frequency
--------------------
1     the      10
1     zoo      2
2     animal   2
2     car      3
2     the      20
</code></pre>

<p>I know that TermDocumentMatrices are actually sparse matrices with metadata. In fact, I can create a sparse matrix from the TDM using the TDM's i, j and v entries for the i, j and x ones of the sparseMatrix function. Please help me if you know how to do the inverse, or in this case, how to construct a TDM by using the three columns in the data.frame above. Thanks!</p>
","r, bash, text-mining, tm","<p>You could try</p>

<pre><code>library(tm)
library(reshape2)
txt &lt;- readLines(n = 7)
book term frequency
--------------------
1     the      10
1     zoo      2
2     animal   2
2     car      3
2     the      20
df &lt;- read.table(header=T, text=txt[-2])
dfwide &lt;- dcast(data = df, book ~ term, value.var = ""frequency"", fill = 0)
mat &lt;- as.matrix(dfwide[, -1]) 
dimnames(mat) &lt;- setNames(dimnames(dfwide[-1]), names(df[, 1:2]))
(tdm &lt;- as.TermDocumentMatrix(t(mat), weighting = weightTf))
# &lt;&lt;TermDocumentMatrix (terms: 4, documents: 2)&gt;&gt;
#   Non-/sparse entries: 5/3
# Sparsity           : 38%
# Maximal term length: 6
# Weighting          : term frequency (tf)

as.matrix(tdm)
#        Docs
# Terms     1  2
# animal    0  2
# car       0  3
# the      10 20
# zoo       2  0
</code></pre>
",2,-1,169,2015-08-20 18:38:26,https://stackoverflow.com/questions/32125871/r-tm-termdocumentmatrix-based-on-a-sparse-matrix
R LDAvis defining documents for each topic,"<p>This is a question about LDA and the application LDAvis in R. As this is my first time using this package I would appreciate any help that could help my research.</p>

<p>I want to be able to view the documents that have been defined by each topic based on probability. I am using survey data and I am looking at the comment section and have defined each of them as documents. </p>

<p>I am going to use the example ""A topic model for movie reviews"" by cpsievert, as this is very similar to my code. The full code can be found in the following link: Visit</p>

<p><a href=""http://cpsievert.github.io/LDAvis/reviews/reviews.html"" rel=""nofollow"">http://cpsievert.github.io/LDAvis/reviews/reviews.html</a></p>

<p>I have got to the stage of fitting the model using the LDA model based on the following code:</p>

<pre><code>set.seed(123)
fit &lt;- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                               num.iterations = G, alpha = alpha, 
                               eta = eta, initial = NULL, burnin = 0,
                               compute.log.likelihood = TRUE)
</code></pre>

<p>And then used the LDAvis to create the interactive html, with the following code:</p>

<pre><code>json &lt;- createJSON(phi = MovieReviews$phi, 
               theta = MovieReviews$theta, 
               doc.length = MovieReviews$doc.length, 
               vocab = MovieReviews$vocab, 
               term.frequency = MovieReviews$term.frequency)
</code></pre>

<p>Now based on the interactive html I have defined the topic based on the frequency terms. There is an example of this on Movie Reviews, which can be found with following link:</p>

<p><a href=""http://cpsievert.github.io/LDAvis/reviews/vis/#topic=7&amp;lambda=0.6&amp;term="" rel=""nofollow"">http://cpsievert.github.io/LDAvis/reviews/vis/#topic=7&amp;lambda=0.6&amp;term=</a></p>

<p>This topic can be defined as comedies for Movie Reviews. </p>

<p>So if in this example ""topic7"" is known as comedies, how can you view the reviews, which are most probable to be defined by this topic?</p>

<p>I would like to know, how would I define documents based on topic7 and then be able to view them, say using:</p>

<pre><code>View(MovieReviwes$Topic7)
</code></pre>

<p>I apologise if this question is broad and long, but if someone could answer it by using the example I have given in the link, this would help greatly. Thanks in advance.</p>
","r, text-mining, lda, topic-modeling","<p>I think you may not fully understand what lda's do and how they work. The lda model will generate a list of <em>k</em> topics, and then tell you which words were assigned to which topics and their respective probabilities for being assigned to each of the various topics. It sounds like what you're really trying to do is perform document/topic classification rather than word/topic classification, and if that's the case then the <code>lda</code> package isn't going to suit your needs. </p>

<p>If you wanted a <em>really</em> dirty method of document classification based on the <code>lda</code> object I guess you could just return the name of the topic with the greatest number of words assigned to it for each document, though I'd imagine that you'd run into issues if there were ties (the probability of ties increases as <em>k</em> increases and the number of documents increases). </p>

<p>EDIT: The quick and dirty way as requested: </p>

<pre><code>sums &lt;- fit$document_sums
sums &lt;- t(sums)
sums &lt;- as.data.frame(sums)
topics &lt;- colnames(sums)[max.col(sums,ties.method=""first"")]
topics &lt;- t(topics)
sums$topics &lt;- topics
</code></pre>
",1,1,1708,2015-08-24 13:58:48,https://stackoverflow.com/questions/32184192/r-ldavis-defining-documents-for-each-topic
R tm removeWords function not removing words,"<p>I am trying to remove some words from a corpus I have built but it doesn't seem to be working. I first run through everything and create a dataframe that lists my words in order of their frequency. I use this list to identify words I am not interested in and then try to create a new list with the words removed. However, the words remain in my dataset. I am wondering what I am doing wrong and why the words aren't being removed? I have included the full code below:</p>

<pre><code>install.packages(""rvest"")
install.packages(""tm"")
install.packages(""SnowballC"")
install.packages(""stringr"")
library(stringr) 
library(tm) 
library(SnowballC) 
library(rvest)

# Pull in the data I have been using. 
paperList &lt;- html(""http://journals.plos.org/plosone/search?q=nutrigenomics&amp;sortOrder=RELEVANCE&amp;filterJournals=PLoSONE&amp;resultsPerPage=192"")
paperURLs &lt;- paperList %&gt;%
  html_nodes(xpath=""//*[@class='search-results-title']/a"") %&gt;%
  html_attr(""href"")
paperURLs &lt;- paste(""http://journals.plos.org"", paperURLs, sep = """")
paper_html &lt;- sapply(1:length(paperURLs), function(x) html(paperURLs[x]))

paperText &lt;- sapply(1:length(paper_html), function(x) paper_html[[1]] %&gt;%
                      html_nodes(xpath=""//*[@class='article-content']"") %&gt;%
                      html_text() %&gt;%
                      str_trim(.))
# Create corpus
paperCorp &lt;- Corpus(VectorSource(paperText))
for(j in seq(paperCorp))
{
  paperCorp[[j]] &lt;- gsub("":"", "" "", paperCorp[[j]])
  paperCorp[[j]] &lt;- gsub(""\n"", "" "", paperCorp[[j]])
  paperCorp[[j]] &lt;- gsub(""-"", "" "", paperCorp[[j]])
}

paperCorp &lt;- tm_map(paperCorp, removePunctuation)
paperCorp &lt;- tm_map(paperCorp, removeNumbers)

paperCorp &lt;- tm_map(paperCorp, removeWords, stopwords(""english""))

paperCorp &lt;- tm_map(paperCorp, stemDocument)

paperCorp &lt;- tm_map(paperCorp, stripWhitespace)
paperCorpPTD &lt;- tm_map(paperCorp, PlainTextDocument)

dtm &lt;- DocumentTermMatrix(paperCorpPTD)

termFreq &lt;- colSums(as.matrix(dtm))
head(termFreq)

tf &lt;- data.frame(term = names(termFreq), freq = termFreq)
tf &lt;- tf[order(-tf[,2]),]
head(tf)

# After having identified words I am not interested in
# create new corpus with these words removed.
paperCorp1 &lt;- tm_map(paperCorp, removeWords, c(""also"", ""article"", ""Article"", 
                                              ""download"", ""google"", ""figure"",
                                              ""fig"", ""groups"",""Google"", ""however"",
                                              ""high"", ""human"", ""levels"",
                                              ""larger"", ""may"", ""number"",
                                              ""shown"", ""study"", ""studies"", ""this"",
                                              ""using"", ""two"", ""the"", ""Scholar"",
                                              ""pubmedncbi"", ""PubMedNCBI"",
                                              ""view"", ""View"", ""the"", ""biol"",
                                              ""via"", ""image"", ""doi"", ""one"", 
                                              ""analysis""))

paperCorp1 &lt;- tm_map(paperCorp1, stripWhitespace)
paperCorpPTD1 &lt;- tm_map(paperCorp1, PlainTextDocument)
dtm1 &lt;- DocumentTermMatrix(paperCorpPTD1)
termFreq1 &lt;- colSums(as.matrix(dtm1))
tf1 &lt;- data.frame(term = names(termFreq1), freq = termFreq1)
tf1 &lt;- tf1[order(-tf1[,2]),]
head(tf1, 100)
</code></pre>

<p>If you look through <code>tf1</code> you will notice that plenty of the words that were specified to be removed have not actually been removed.</p>

<p>Just wondering what I am doing wrong, and how I might remove these words from my data?</p>

<p>NOTE: <code>removeWords</code> is doing something because the output from <code>head(tm, 100)</code> and <code>head(tm1, 100)</code> are not exactly the same. So <code>removeWords</code> seems to removing some instances of the words I am trying to get rid of, but not all instances.</p>
","r, text, text-mining, tm, corpus","<p>I switched some code around and added tolower. The stopwords are all in lowercase, so you need to do that first before you remove stopwords.</p>

<pre><code>paperCorp &lt;- tm_map(paperCorp, removePunctuation)
paperCorp &lt;- tm_map(paperCorp, removeNumbers)
# added tolower
paperCorp &lt;- tm_map(paperCorp, tolower)
paperCorp &lt;- tm_map(paperCorp, removeWords, stopwords(""english""))
# moved stripWhitespace
paperCorp &lt;- tm_map(paperCorp, stripWhitespace)

paperCorp &lt;- tm_map(paperCorp, stemDocument)
</code></pre>

<p>Upper case words no longer needed, since we set everything to lower case. You can remove these. </p>

<pre><code>paperCorp &lt;- tm_map(paperCorp, removeWords, c(""also"", ""article"", ""Article"", 
                                               ""download"", ""google"", ""figure"",
                                               ""fig"", ""groups"",""Google"", ""however"",
                                               ""high"", ""human"", ""levels"",
                                               ""larger"", ""may"", ""number"",
                                               ""shown"", ""study"", ""studies"", ""this"",
                                               ""using"", ""two"", ""the"", ""Scholar"",
                                               ""pubmedncbi"", ""PubMedNCBI"",
                                               ""view"", ""View"", ""the"", ""biol"",
                                               ""via"", ""image"", ""doi"", ""one"", 
                                               ""analysis""))

paperCorpPTD &lt;- tm_map(paperCorp, PlainTextDocument)

dtm &lt;- DocumentTermMatrix(paperCorpPTD)

termFreq &lt;- colSums(as.matrix(dtm))
head(termFreq)

tf &lt;- data.frame(term = names(termFreq), freq = termFreq)
tf &lt;- tf[order(-tf[,2]),]
head(tf)

           term  freq
fatty     fatty 29568
pparα     ppara 23232
acids     acids 22848
gene       gene 15360
dietary dietary 12864
scholar scholar 11904

tf[tf$term == ""study""]


data frame with 0 columns and 1659 rows
</code></pre>

<p>And as you can see, the outcome is that study is no longer in the corpus. The rest of the words are also gone</p>
",16,9,36907,2015-08-26 11:44:45,https://stackoverflow.com/questions/32225770/r-tm-removewords-function-not-removing-words
Replace words into special string except nouns and adjectives in Python,"<p>I want to replace words (e.g., verbs, adverbs...) into some special string (e.g., ""NIL"") except adjectives and nouns. </p>

<p>That is to say, for a text:</p>

<blockquote>
  <p>anarchism originated as a term of abuse first used against early working class radicals</p>
</blockquote>

<p>I first do POS tagging (universal format), resulting in a tagged format:</p>

<blockquote>
  <p>anarchism/NOUN originated/VERB as/ADP a/DET term/NOUN of/ADP abuse/NOUN first/ADV used/VERB against/ADP early/ADJ working/NOUN class/NOUN radicals/NOUN</p>
</blockquote>

<p>and I want to obtain the text like this:</p>

<blockquote>
  <p>anarchism/NOUN NIL NIL NIL term/NOUN NIL abuse/NOUN NIL NIL NIL NIL working/NOUN class/NOUN radicals/NOUN</p>
</blockquote>

<p>which preserve the nouns and adjectives while replace the other words with special string (like ""NIL"").</p>

<p>Is there some efficient way to do this in Python, my corpus size could be 10G+.</p>

<p>Thanks a lot!</p>
","python, nltk, text-mining, pos-tagger","<p>Try splitting the string into each word, and check what type of word it is:</p>

<pre><code>string = 'anarchism/NOUN originated/VERB as/ADP a/DET term/NOUN of/ADP abuse/NOUN first/ADV used/VERB against/ADP early/ADJ working/NOUN class/NOUN radicals/NOUN'
string = string.split(' ')
temp = ''
for a in string:
    if '/NOUN' in a:
        temp += a + ' '
    else:
        temp += 'NIL '
string = temp
print(string)
</code></pre>
",1,0,966,2015-08-28 06:10:36,https://stackoverflow.com/questions/32264398/replace-words-into-special-string-except-nouns-and-adjectives-in-python
Calculate distance between words in a window in Python,"<p>I want to build a distance matrix (sparse matrix), which is similar with co-occurrence matrix. In a window of size 7, I want to compute the distance between two specific words and move the window by turn.</p>

<p>Take this for example:</p>

<blockquote>
  <p>anarchism/NOUN NIL NIL NIL term/NOUN NIL abuse/NOUN NIL NIL NIL NIL working/NOUN class/NOUN radicals/NOUN</p>
</blockquote>

<p>This is what I have noun, I replace non-related words with ""NIL"" (just indicates the distance) and preserve the nouns and adjectives.</p>

<p>I want to compute the distance between (term, abuse), (working, class), (working, radicals), (class, radicals). Other distances are not computed because they are out of window size 7.</p>

<p>And I want to get a matrix-record for sparseMatrix such like this:</p>

<pre><code>    i       j     dis
1 term    abuse    2
2 working class    1
3 working radicals 2
4 class   radicals 1
</code></pre>

<hr>

<p>Is there some efficient way to do this in Python, my corpus size could be 10G+.</p>

<p>Thanks a lot!</p>
","python, matrix, distance, text-mining","<p>So basically, you want to get distances of 7 or less. You can do this easily by this process (pseudocode):</p>

<pre><code>for each word index:
  skip if word[index] is NIL
  for offset from 1 to 7:
    stop if index+offset is past last element
    skip if word[index+offset] is NIL
    make triplet (word[index], word[index+offset], offset)
</code></pre>

<p>It should be easy stuffing it into <a href=""http://docs.scipy.org/doc/scipy/reference/sparse.html"" rel=""nofollow"">scipy sparse matrix</a> (you will also need to assign each discrete word an ID, as scipy matrix indices must be integers).</p>

<p>EDIT: No idea why I put <code>6</code> in there... Should be <code>7</code>, of course.</p>
",1,0,705,2015-08-28 07:45:06,https://stackoverflow.com/questions/32265855/calculate-distance-between-words-in-a-window-in-python
Avoid stemming of Acronyms?,"<p>I am using the <code>pattern_capture</code> filter to preserve all the acronyms</p>

<pre><code>PUT test_index/_settings
{
  ""index.analysis.filter"": {
    ""acronym_en_EN"": {
      ""type"": ""pattern_capture"",
      ""patterns"": [
        ""(?:[a-zA-Z]\\.)+"", 
        ""((?:[a-zA-Z]\\.)+[a-zA-Z])"",
        ""((?:[a-zA-Z]\\.)+[s]$)"",
        ""((?:[a-zA-Z]\\.)+[s][\\.]$)""
        ],
      ""preserve_original"": true
    }
  }
}
</code></pre>

<p>But i noticed that acronyms that end with <code>s</code> or <code>s.</code> are stemmed as there is one stemmer filter also attached to the analyzer. The regular expressions in the filter above for handling <code>s</code> are also not working.</p>

<p>I test the output using this</p>

<pre><code>GET test_index/_analyze?tokenizer=standard&amp;filters=lowercase,acronym_en_EN,apostrophe,porter_stemmer_en_EN&amp;text=u.s.a. u.s. s.w.a.t u.t. 
</code></pre>

<p>this gives me </p>

<pre><code>{
   ""tokens"": [
      {
         ""token"": ""u.s.a"",
         ""start_offset"": 0,
         ""end_offset"": 5,
         ""type"": ""&lt;ALPHANUM&gt;"",
         ""position"": 1
      },
      {
         ""token"": ""u."",
         ""start_offset"": 7,
         ""end_offset"": 10,
         ""type"": ""&lt;ALPHANUM&gt;"",
         ""position"": 2
      },
      {
         ""token"": ""u."",
         ""start_offset"": 7,
         ""end_offset"": 10,
         ""type"": ""&lt;ALPHANUM&gt;"",
         ""position"": 2
      },
      {
         ""token"": ""s.w.a.t"",
         ""start_offset"": 12,
         ""end_offset"": 19,
         ""type"": ""&lt;ALPHANUM&gt;"",
         ""position"": 3
      },
      {
         ""token"": ""u.t"",
         ""start_offset"": 20,
         ""end_offset"": 23,
         ""type"": ""&lt;ALPHANUM&gt;"",
         ""position"": 4
      }
   ]
}
</code></pre>

<p>Is there any way I can preserve the acronyms ending with <code>s</code> so that for <code>u.s.</code> or <code>u.s</code> I don't get <code>u.</code>?</p>
","elasticsearch, dsl, text-mining","<p>I don't think this is supported out of the box. I believe the way to do this is to teach the <code>pattern_capture</code> filter how to mark its captures as <code>keyword</code> tokens ala the <code>keyword_marker</code> filter.</p>

<p>Honestly you could probably hack something together with two <code>pattern_replace</code> token filters - one on either side of the stemmer. Just slap a <code>$</code> or something on the front of the acronyms and rip it off on the other side.</p>
",1,0,579,2015-09-01 22:08:06,https://stackoverflow.com/questions/32341633/avoid-stemming-of-acronyms
Best method to confirm an entity,"<p>I would like to understand the best approach to the following problem.</p>

<p>I have documents really similar to resume/cv and I have to extract entities (Name, Surname, Birthday, Cities, zipcode etc).</p>

<p>To extract those entities I am combining different finders (Regex, Dictionary etc)</p>

<p>There are no problems with those finders, but, I am looking for a method / algorithm or something like that to confirm the entities.</p>

<p>With ""confirm"" I mean that I have to find specific term (or entities) in proximities (closer to the entities I have found).</p>

<p>Example:</p>

<pre><code>My name is &lt;name&gt;
Name: &lt;name&gt;
Name and Surname: &lt;name&gt;
</code></pre>

<p>I can confirm the entity <code>&lt;name&gt;</code> because it is closer to specific term that let me understand the ""context"". If i have ""name"" or ""surname"" words near the entity  so i can say that i have found the <code>&lt;name&gt;</code> with a good probability.</p>

<p>So the goal is write those kind of rules to confirm entities. Another example should be:</p>

<blockquote>
  <p>My address is ......, 00143 Rome</p>
</blockquote>

<p>Italian zipcodes are 5 digits long (numeric only), it is easy to find a 5 digits number inside my document (i use regex as i wrote above), and i also check it by querying a database to understand if the number exists. The problem here is that i need one more check to confirm (definitely) it.</p>

<p>I must see if that number is near the entity <code>&lt;city&gt;</code>, if yes, ok... I have good probabilities.</p>

<p>I also tried to train a model but i do not really have a ""context"" (sentences).
Training the model with:</p>

<pre><code>My name is: &lt;name&gt;John&lt;/name&gt;
Name: &lt;name&gt;John&lt;/name&gt;
Name/Surname: &lt;name&gt;John&lt;/name&gt;
&lt;name&gt;John&lt;/name&gt; is my name
</code></pre>

<p>does not sound good to me because:</p>

<ol>
<li>I have read we need many sentences to train a good model</li>
<li>Those are not ""sentences"" i do not have a ""context"" (remember where I said the document is similar to resume/cv)</li>
<li>Maybe those phrases are too short</li>
</ol>

<p>I do not know how many different ways i could find to say the exact thing, but surely I can not find 15000 ways :)</p>

<p>What method should I use to try to confirm my entities?</p>

<p>Thank you so much!</p>
","java, nlp, text-mining, opennlp, named-entity-recognition","<h2>Problem statement</h2>

<p>First of all, I don't think that your decomposition of the task into 2 steps (extract and confirm) is the best, if only I don't miss some specifics in the problem. If I understand correctly, you goal is to extract structured info like Name/City/etc from the set of docs with maximum precision and recall; either metric can be more important, but usually they are considered with equal weights - e.g. by using F1-measure.</p>

<h2>Evaluate first</h2>

<p>'You can't control what you can't measure' <a href=""https://en.wikiquote.org/wiki/Tom_DeMarco"" rel=""nofollow noreferrer"">Tom DeMarco</a> </p>

<p>I'd propose to firstly prepare evaluation system and marked up dataset: for each document find correct Name/City/etc - it can be done fully manually (which is more 'true', but more hard way) or semi-automatically, e.g. by applying some method, including that under development, and correcting its errors if any.
Evaluation system should be able to compute Precision and Recall (see <a href=""https://en.wikipedia.org/wiki/Confusion_matrix"" rel=""nofollow noreferrer"">Confusion matrix</a> in order to easily implement them by yourself).</p>

<p>As for its size, I wouldn't be so afraid by necessity of preparing too big dataset: sure, more is better, but it is crucial for the case with complex (significantly non-linear) tasks and a lot of features. I believe 100-200 docs to be enough for start in your case - and it would take several hours to prepare.</p>

<p>Then you can evaluate your simple extractors based on RegExps and Dictionaries - best if different aspects (Name or City) would have separate metrics. Depending on results, your actions may differ.</p>

<h2>Low precision - add more specific features</h2>

<p>If the method shows too low precision, i.e. extract too many wrong items, you should add specificity, or specific features; I'd search for them in scientific papers devoted to Information extraction concerning to those aiming at the specific information type, be it Name/Surname, or Address, or something more vague like skills if you're interested in such info. For instance, many papers (like [<a href=""https://en.wikipedia.org/wiki/Confusion_matrix"" rel=""nofollow noreferrer"">2</a>] and [<a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow noreferrer"">3</a>]) devoted to Resume parsing note that Name/Surname are usually placed at the very beginning of the text; or that Cities are usually preceded by 'at'. 
I don't know specifics of you documents, but I doubt they violate such patterns.</p>

<p>Also it may be useful and easy to treat output of Named Entity Recognizer, e.g. <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow noreferrer"">Standord NLP</a>, as a feature (see also <a href=""https://stackoverflow.com/questions/30664399/name-extraction-cv-resume-stanford-ner-opennlp"">relevant question</a>)</p>

<p>Again, harder but better is to analyze approaches used by NERC and to adapt them to specifics of your task and docs.</p>

<p>These features can be aggregated by any Supervised machine learning (start with Logistic Regression and Random Forest if you have not much experience): you know positive and negative (all but not positive) answers from your evaluation dataset, just transform them into feature space and feed to some ML lib like <a href=""http://www.cs.waikato.ac.nz/ml/weka/"" rel=""nofollow noreferrer"">Weka</a>.</p>

<h2>Low recall - extract more candidates</h2>

<p>If the method shows too low recall, i.e. misses a lot of items, then you should extend set of candidates - for example, develop less restrictive patterns, add fuzzy matching (look at <a href=""https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance"" rel=""nofollow noreferrer"">Jaro-Winkler</a> or <a href=""https://en.wikipedia.org/wiki/Soundex"" rel=""nofollow noreferrer"">Soundex</a> string metric) to Dictionary lookup. </p>

<p>Another option is to apply Part-of-Speech tagging and take each noun as a candidate - maybe each Proper noun for some info items, or take noun bigrams, or add other weak restrictions. In this case, most probably, your precision will degrade, so the paragraph above would have to be considered.</p>

<p>NB: If your data comes from Web (e.g. profiles from LinkedIn), try to search by keywords 'Web data extraction' or take a look at import.io</p>

<h2>Literature</h2>

<p><em>just a few random, try to search at <a href=""https://scholar.google.ru/scholar?q=information%20extraction%20survey%20&amp;btnG=&amp;hl=ru&amp;as_sdt=0%2C5"" rel=""nofollow noreferrer"">Google scholar</a>, preferably start from surveys</em></p>

<ol>
<li><p>Renuka S. Anami, Gauri R. Rao. Automated Profile Extraction and
Classification with Stanford Algorithm. International Journal of
Innovative Technology and Exploring Engineering (IJITEE) ISSN:
2278-3075, Volume-4 Issue-7, December 2014 (<a href=""http://www.ijitee.org/attachments/File/v4i7/G1916124714.pdf"" rel=""nofollow noreferrer"">link</a>)</p></li>
<li><p>Swapnil Sonar. Resume Parsing with Named Entity Clustering
Algorithm. 2015 (<a href=""http://www.slideshare.net/swapnilmsonar/resume-parsing-with-named-entity-clustering-algorithm"" rel=""nofollow noreferrer"">link</a>)</p></li>
</ol>
",6,9,793,2015-09-04 14:16:05,https://stackoverflow.com/questions/32400289/best-method-to-confirm-an-entity
data mining algorithm that suggest for this situation,"<p>This is not a directly programming related question, but it's about selecting the right data mining algorithm.</p>

<p>i have some folders suppose 100 folders , contents of these folders are images and text documents , i have excel tables ( 100 tables ) for these folders which mean for each folder there are particular table , this excel table content as follow : </p>

<p>in header ( columns header ) include the content of this folder and rows include the files i would be check ( my test files ) the values in this table are o's and 1's if the file is found in that folder then value 1 otherwise o , these test files names are same for all folders , </p>

<p>Q: what are the best data mining algorithm can work on excel file tables , and can cluster these folders based on test file content for example cluster 1 include folders which contain the files 1 and file 20 and file 25 .. and so on .. consider i use matlab language ? </p>

<p>thanks ...  </p>
","excel, algorithm, data-mining, text-mining","<p>The english here is a little confusing so I'll interpret the question best as I can. What you want to do here doesn't seem to require any complicated algorithm. Go ahead take your excel data and export it as CSV so that you can work in Matlab. </p>

<p>Right now you have data as follows:</p>

<p><code>Folder -&gt; [ Files ]</code></p>

<p>You probably want to build an index this way:</p>

<p><code>File -&gt; [ Folders ]</code></p>

<p>This way, you when you ask the question: ""What folders contain files 1, 20 and 25"", you can look up (in constant time) 3 things:</p>

<ol>
<li>Folders that contain file 1</li>
<li>Folders that contain file 20</li>
<li>Folders that contain file 25</li>
</ol>

<p>And then take the intersection of those sets. </p>

<p>===================================================</p>

<p>The other thing you might be interested in doing is ""clustering"". For that, go ahead and take your Folder descriptors (the ones and zeroes) and treat that as a feature/vector. Then go ahead and run any clustering algorithm on it. K-means clustering is an easy one to implement in Matlab. </p>

<p>[1] <a href=""https://en.wikipedia.org/wiki/Cluster_analysis"" rel=""nofollow"">https://en.wikipedia.org/wiki/Cluster_analysis</a></p>
",1,-1,84,2015-09-06 22:15:54,https://stackoverflow.com/questions/32428824/data-mining-algorithm-that-suggest-for-this-situation
How to abstract bigram topics instead of unigrams using Latent Dirichlet Allocation (LDA) in python- gensim?,"<h1>LDA Original Output</h1>

<ul>
<li><p>Uni-grams </p>

<ul>
<li><p>topic1 -scuba,water,vapor,diving</p></li>
<li><p>topic2 -dioxide,plants,green,carbon</p></li>
</ul></li>
</ul>

<h1>Required Output</h1>

<ul>
<li><p>Bi-gram topics</p>

<ul>
<li><p>topic1 -scuba diving,water vapor</p></li>
<li><p>topic2 -green plants,carbon dioxide</p></li>
</ul></li>
</ul>

<p>Any idea?</p>
","nlp, text-mining, lda, gensim","<p>You can use word2vec to get most similar terms from the top n topics abstracted using LDA.</p>

<p>LDA Output</p>

<p>Create a dictionary of bi-grams using topics abstracted  (for ex:-san_francisco)</p>

<p>check <a href=""http://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/"" rel=""nofollow"">http://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/</a></p>

<p>Then, do word2vec to get most similar words (uni-grams,bi-grams etc)  </p>

<p><strong>Word</strong>      and           <strong>Cosine distance</strong>  </p>

<p>los_angeles               (0.666175)<br>
golden_gate               (0.571522)<br>
oakland                   (0.557521)  </p>

<p>check <a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">https://code.google.com/p/word2vec/</a>  (From words to phrases and beyond)</p>
",3,5,13427,2015-09-09 09:51:02,https://stackoverflow.com/questions/32476336/how-to-abstract-bigram-topics-instead-of-unigrams-using-latent-dirichlet-allocat
Font issue on Ubuntu machine in parsing PDF File,"<p>I have an application on my Ubuntu 14.04.x Machine. This application does text mining on PDF files. I suspect that it is using Apache Tika etc...</p>

<p>The problem is that, during its reading process, I get the following warning: </p>

<pre><code>2015-09-10 14:15:35 [WARN] FontManager Font not found: CourierNewPSMT
2015-09-10 14:15:36 [WARN] FontManager Font not found: CourierNewPSMT
2015-09-10 14:19:33 [WARN] FontManager Font not found: Helvetica
2015-09-10 14:19:34 [WARN] FontManager Font not found: ESQWSF+Helvetica
2015-09-10 14:19:34 [WARN] FontManager Font not found: ESQWSF+Helvetica
2015-09-10 14:19:34 [WARN] FontManager Font not found: ESQWSF+Helvetica
......
</code></pre>

<p>How can I get those fonts on my machine? Or is it some java lib that I am missing for fonts?</p>
","java, ubuntu-14.04, text-mining, apache-tika","<p>I would do a three step approach to fix this issue.</p>

<ol>
<li>Analyse what files are searched for and not found using strace</li>
<li>Use apt-file to search for the package providing these files</li>
<li>Install the missing package</li>
</ol>

<p>1.) Install strace if it's not already installed <code>sudo apt-get install strace</code></p>

<p>Check what files are used by your app:</p>

<p><code>$&gt; strace &lt;your app&gt; 2&gt;&amp;1 | grep open</code></p>

<p>you can further filter this for ENOENT errors:</p>

<p><code>$&gt; strace &lt;your app&gt; 2&gt;&amp;1 | grep open | grep ENOENT</code></p>

<p>Now you should know what files are missing.</p>

<p>2.) Check what package is providing this file.
(dpkg -S  only works for already installed packages)</p>

<pre><code>su
apt-get install apt-file
apt-file update
apt-file search &lt;filename&gt;
</code></pre>

<p>3.) install that package using <code>apt-get install &lt;package&gt;</code></p>

<p>I've no Ubuntu here, but the MS fonts are normally available in a package called ""mscorefont"" or similar.</p>
",4,9,1025,2015-09-10 18:24:21,https://stackoverflow.com/questions/32509140/font-issue-on-ubuntu-machine-in-parsing-pdf-file
&quot;RTextTools&quot; create_matrix got an error,"<p>I was running RTextTools package to build a text classification model.</p>

<p>And when I prepare the prediction dataset and tried to transform it in to matrix. I got error as:</p>

<pre><code>Error in if (attr(weighting, ""Acronym"") == ""tf-idf"") weight &lt;- 1e-09 : 
  argument is of length zero
</code></pre>

<p>My code is as below:</p>

<pre><code>table&lt;-read.csv(""traintest.csv"",header = TRUE)
dtMatrix &lt;- create_matrix(table[""COMMENTS""])
container &lt;- create_container(dtMatrix, 
                              table$LIKELIHOOD_TO_RECOMMEND, 
                              trainSize=1:5000,testSize=5001:10000, 
                              virgin=FALSE)
model &lt;- train_model(container, ""SVM"", kernel=""linear"", cost=1)

predictionData&lt;-read.csv(""rest.csv"",header = TRUE)
**predMatrix &lt;- create_matrix(predictionData[""COMMENTS""],originalMatrix=dtMatrix)**
Error in if (attr(weighting, ""Acronym"") == ""tf-idf"") weight &lt;- 1e-09 : 
      argument is of length zero
</code></pre>

<p>The error was given by the last code (bold)
I tried search on google but didn't see one clear solution.</p>

<p>Thanks</p>
","r, classification, text-mining","<p>Run this:</p>

<pre><code>trace(""create_matrix"",edit=T)
</code></pre>

<p>In the source code box that pops up, line 42 will have a misspelling of the word ""acronym"".  Change the ""A"" to an ""a"" and hit ""Save"" - it should work fine after that.</p>
",36,12,4011,2015-09-11 00:09:17,https://stackoverflow.com/questions/32513513/rtexttools-create-matrix-got-an-error
re.sub() in Python does not always work while replacing currency values in string,"<p>I have built a ""Currency Tagger"" in Python which identifies all currency expressions and replaces them with a tagged string.</p>

<p>Example,<br>
replace <code>""I have $20 in my pocket""</code><br>
with <code>""I have &lt;Currency&gt;$20&lt;/Currency&gt; in my pocket""</code></p>

<p>One of the tasks requires me to substitute the string identified as Currency with the tagged string. I am using <code>re.sub()</code> to do this.</p>

<p>It works perfectly for every form of string except of the form ""$4.4B"" or ""$4.4M"". </p>

<p>I tried running simple example in my python console and found that <code>re.sub()</code> works inconsistently with patterns which have a mixed dollar pattern.</p>

<p>For example,</p>

<pre><code>&gt;&gt;&gt; text = ""I have #20 in my pocket""
&gt;&gt;&gt; re.sub(""#20"", ""$20"", text)
'I have $20 in my pocket'
&gt;&gt;&gt; text = ""I have $20 in my pocket""
&gt;&gt;&gt; re.sub(""$20"", ""#20"", text)
'I have $20 in my pocket'
</code></pre>

<p>In the above example you see that when I am trying to replace ""$20"" with ""#20"" it does not work (in the second case).</p>

<p>Any help would be greatly appreciated of course. A very silly bug has cropped up and is stalling major work because of this.</p>
","python, regex, python-2.7, text-mining","<p><code>$</code> is a <code>special character</code> .So if you want to replace it use</p>

<pre><code> re.sub(r""\$20"", ""#20"", text)

          ^^
</code></pre>

<p>You will have to <code>escape</code> it.Also use <code>r</code> mode to avoid <code>escaping</code> problems.</p>

<p><code>$</code> means end of string.So your regex was being ineffective.</p>
",6,3,1017,2015-09-11 05:06:42,https://stackoverflow.com/questions/32515853/re-sub-in-python-does-not-always-work-while-replacing-currency-values-in-strin
Inspect the corresponding terms (vocab? in English) of document-term matrix using tm,"<p>Hi this must be super basic:</p>

<p>I am using tm package to create a document term matrix from a corpus, so the column names of my matrix are the indices of the terms in my corpus. Could anyone be so nice to tell me how to inspect the original words in my corpus that correspond to these indices in the matrix? Thank you so much!!</p>
","r, text-mining, tm","<p>I slightly suspect you are trying to connect the words back to the original corpus and get the corpus indices.  If so you can do this using @jlhoward's example:</p>

<pre><code>library(tm)
docs &lt;- c(""This is a text. And me too."", 
          ""This another one."", 
          ""This is some more."")

cor  &lt;- Corpus(VectorSource(docs))
tdm  &lt;- TermDocumentMatrix(cor, control=list(tolower=TRUE, removePunctuation=TRUE))
as.matrix(tdm)

txt &lt;- sapply(cor, function(x) x[[1]])


setNames(lapply(rownames(tdm), function(x){
   grep(x, txt, ignore.case=TRUE)
}), rownames(tdm))

## $another
## [1] 2
## 
## $more
## [1] 3
## 
## $one
## [1] 2
## 
## $some
## [1] 3
## 
## $text
## [1] 1
## 
## $this
## [1] 1 2 3
</code></pre>
",0,1,104,2015-09-12 14:46:12,https://stackoverflow.com/questions/32540099/inspect-the-corresponding-terms-vocab-in-english-of-document-term-matrix-usin
R text mining filtering string from text,"<p>I was wondering if there's an existing R function that given a text and a list of strings as input, will filter out the matching strings in the list that are found within the text?</p>

<p>For example,</p>

<pre><code>x &lt;- ""This is a new way of doing things.""
mywords &lt;- c(""This is"", ""new"", ""not"", ""maybe"", ""things."")
filtered_words &lt;- Rfunc(x, mywords)
</code></pre>

<p>Then filtered_words will contain ""This is"", ""new"" and ""things."".</p>

<p>Is there any such function?</p>
","r, text-mining","<p>We can use <code>str_extract_all</code> from <code>library(stringr)</code>.  The output will be a <code>list</code>, which can be <code>unlist</code>ed to convert it to a <code>vector</code>.</p>

<pre><code>library(stringr)
unlist(str_extract_all(x, mywords))
#[1] ""This is"" ""new""     ""things.""
</code></pre>
",1,0,792,2015-09-19 03:33:24,https://stackoverflow.com/questions/32663903/r-text-mining-filtering-string-from-text
How to find the closest word to a vector using word2vec,"<p>I have just started using Word2vec and I was wondering how can we find the closest word to a vector suppose.
 I have this vector which is the average vector for a set of vectors:</p>

<pre><code>array([-0.00449447, -0.00310097, 0.02421786, ...], dtype=float32)
</code></pre>

<p>Is there a straight forward way to find the most similar word in my training data to this vector?</p>

<p>Or the only solution is to calculate the cosine similarity between this vector and the vectors of each word in my training data, then select the closest one?</p>

<p>Thanks.</p>
","python, text-mining, data-analysis, word2vec","<p>For <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">gensim</a> implementation of word2vec there is <code>most_similar()</code> function that lets you find words semantically close to a given word:</p>

<pre><code>&gt;&gt;&gt; model.most_similar(positive=['woman', 'king'], negative=['man'])
[('queen', 0.50882536), ...]
</code></pre>

<p>or to it's vector representation:</p>

<pre><code>&gt;&gt;&gt; your_word_vector = array([-0.00449447, -0.00310097, 0.02421786, ...], dtype=float32)
&gt;&gt;&gt; model.most_similar(positive=[your_word_vector], topn=1))
</code></pre>

<p>where <code>topn</code> defines the desired number of returned results.</p>

<p>However, my gut feeling is that function does exactly the same that you proposed, i.e. calculates cosine similarity for the given vector and each other vector in the dictionary (which is quite inefficient...)</p>
",52,33,48360,2015-09-24 11:03:04,https://stackoverflow.com/questions/32759712/how-to-find-the-closest-word-to-a-vector-using-word2vec
How to &quot;split&quot; a text document or string of text in R so that each word is its own row in a dataframe?,"<pre><code>documents &lt;- c(""This is document number one"", ""document two is the second element of the vector"")
</code></pre>

<p>the dataframe I'm trying to create is: </p>

<pre><code>idealdf &lt;- c(""this"", ""is"", ""document"", ""number"", ""one"", ""document"", ""two"", ""is"", ""the"", ""second"", ""element"", ""of"", ""the"", ""vector"") 
</code></pre>

<p>I've been using the tm package to convert my document to a corpus and get arid of punctuation, convert to lowercase, etc. through functions like: </p>

<pre><code>#create a corpus:
myCorpus &lt;- Corpus(VectorSource(documents))

#convert to lowercase:
myCorpus &lt;- tm_map(myCorpus, content_transformer(tolower))

#remove punctuation:
myCorpus &lt;- tm_map(myCorpus, removePunctuation)
</code></pre>

<p>...but I'm having trouble trying to get it in a df where each word has it's own row (I prefer that every word have its own row- even if the same word shows up as multiple rows). </p>

<p>Thanks. </p>
","r, text-mining, tm, corpus","<p>How about</p>

<pre><code>library(stringi)
data.frame(words = unlist(stri_extract_all_words(stri_trans_tolower(documents))))
#       words
# 1      this
# 2        is
# 3  document
# 4    number
# 5       one
# 6  document
# 7       two
# 8        is
# 9       the
# 10   second
# 11  element
# 12       of
# 13      the
# 14   vector
</code></pre>
",5,0,3351,2015-09-29 01:05:31,https://stackoverflow.com/questions/32834067/how-to-split-a-text-document-or-string-of-text-in-r-so-that-each-word-is-its-o
How to combine multiple feature sets in bag of words,"<p>I have text classification data with predictions depending on categories, 'descriptions' and 'components'.  I could do the classification using bag of words in python with scikit on 'descriptions'. But  I want to get predictions using both categories in bag of words with weights to individual feature sets
x = descriptions + 2* components
How should  I proceed?</p>
","python-2.7, machine-learning, scikit-learn, text-mining, text-classification","<p>You can train individual classifiers for descriptions and merchants, and obtain a final score using <code>score = w1 * predictions + w2 * components.</code></p>

<p>The values of <code>w1</code> and <code>w2</code> should be obtained using cross validation.</p>

<p>Alternatively, you can train a single multiclass classifier by combining the training dataset. </p>

<p>You will now have 4 classes: </p>

<ol>
<li>Neither 'predictions' nor 'components'</li>
<li>'predictions' but not 'components'</li>
<li>not 'predictions' but 'components'</li>
<li>'predictions' and 'components'</li>
</ol>

<p>And you can go ahead and train as usual.</p>
",0,0,807,2015-09-30 06:41:41,https://stackoverflow.com/questions/32859460/how-to-combine-multiple-feature-sets-in-bag-of-words
Read an irregular text data file into R,"<p>I am trying to ""import"" data from a non-data.frame shape text file with multiple precipitation rates reports. The reports are all equal, a sample of one is the following: </p>

<pre><code>  I D E A M  -  INSTITUTO DE HIDROLOGIA, METEOROLOGIA Y ESTUDIOS AMBIENTALES
                                                                                                          INFORMATION SYSTEM
                                  PRECIPITATION TOTAL VALUES (mms)                              NATIONAL ENVIRONMENTAL 

DATE OF PROCESS :  2015/09/15                    YEAR  1980                              STATION ID : 11010010  VUELTA LA

LAT    0527 N               TIPO EST    PM                   STATE      CHOCO                   INSTALLATION DATE   1943-ENE
LON   7632 W               ENT     01  IDEAM            CITY  LLORO                   FECHA-SUSPENSION
ELE   100 m.s.n.m         REGIONAL    01  ANTIOQUIA        CORRIENTE  ANDAGUEDA


      DAY       JAN *  FEB *  MAR *  APR *  MAY  *  JUN *  JUL *  AGO *  SEP *  OCT *  NOV *  DEC *


       01                 30.0       .0       .0      3.0     80.0       .0      3.0       .0     35.0     88.0      1.0
       02                   .0      1.0       .0      1.0    100.0       .0       .0      6.0      1.0     65.0     69.0
       03                 35.0    100.0       .0     10.0       .0       .0       .0     70.0     40.0     42.0     16.0
       04                   .0       .0     80.0      3.0    140.0      8.0       .0    135.0     20.0     48.0     15.0
       05                   .0       .0       .0      8.0      3.0     20.0      4.0     19.0     80.0       .0     20.0
       06                   .0       .0    100.0    138.0       .0      6.0       .0      4.0     20.0       .0     10.0
       07                 31.0     10.0       .0     30.0     15.0     50.0      6.0       .0      4.0       .0       .0
       08                   .0     44.0       .0     10.0     40.0       .0       .0       .0      7.0       .0      4.0
       09                 35.0      3.0     23.0       .0     20.0    140.0       .0      6.0       .0     32.0     16.0
       10                   .0     75.0       .0       .0     60.0       .0       .0     23.0      3.0      1.0      5.0
       11                   .0     17.0       .0     15.0     80.0       .0       .0     80.0       .0       .0      3.0
       12                   .0     75.0       .0      8.0       .0     63.0     10.0       .0       .0     17.0     10.0
       13                   .0     20.0       .0     60.0       .0       .0       .0    110.0     50.0      3.0     25.0
       14                 55.0       .0     26.0     12.0       .0      3.0    140.0      4.0     74.0       .0     38.0
       15                   .0       .0      3.0      7.0     10.0       .0      6.0       .0     35.0     12.0     27.0
       16                   .0      4.0     89.0     20.0      3.0       .0       .0     10.0       .0       .0       .0
       17                 45.0       .0      9.0       .0     30.0       .0      2.0       .0     60.0    103.0       .0
       18                 30.0       .0       .0       .0     21.0       .0     20.0     15.0       .0       .0       .0
       19                   .0    130.0       .0     10.0     12.0      8.0       .0      3.0     20.0     49.0     40.0
       20                 45.0       .0     25.0    190.0       .0     38.0      8.0       .0      8.0      3.0      1.0
       21                  1.0       .0     45.0     50.0       .0     35.0       .0      2.0     13.0      1.0      4.0
       22                   .0       .0     20.0       .0       .0       .0       .0     16.0     10.0     12.0     50.0
       23                 40.0       .0     40.0     16.0       .0     30.0       .0     13.0      2.0    106.0     10.0
       24                   .0       .0     45.0     60.0       .0      3.0       .0     25.0       .0     16.0       .0
       25                   .0       .0       .0       .0     18.0     10.0       .0      3.0       .0     50.0     20.0
       26                 10.0       .0       .0       .0      9.0      6.0     20.0     20.0      6.0     15.0      3.0
       27                   .0    135.0     60.0     40.0     80.0     15.0       .0     18.0     10.0     77.0       .0
       28                 10.0       .0      9.0     15.0       .0       .0       .0      6.0     72.0    102.0       .0
       29                 23.0      6.0       .0       .0       .0       .0       .0     23.0       .0     34.0       .0
       30                            .0     10.0       .0     20.0      3.0       .0     64.0     14.0    111.0       .0
       31                            .0              31.0              10.0       .0                .0                .0


                                  ***  ANNUAL VALUES  ***

                                 TOTAL                  6954.0
                                 No DE RAIN DAYS         210
                                 MAX 24 Hrs        190.0
</code></pre>

<p>The text file includes one report after the other, all with the same header <code>""I D E A M  -  INSTITUTO DE HIDROLOGIA, METEOROLOGIA Y ESTUDIOS AMBIENTALES""</code>. I have already ""read"" the text file using the <code>readLines()</code>function and I was hoping to create a data frame with the information of each report, something like this: </p>

<pre><code>DATE        STATION_ID  LAT    LON    ELE CITY STATE PRECIPITATION
01/JAN/1980 11010010    0527 N 7632 W 100 LLORO CHOCO 0
</code></pre>

<p>I have been trying split each report and then start to parse each line. Unfortunately is a slow process. I understand this page looks for delimited questions, but I am kind of stuck.</p>

<p>Thanks in advance. </p>
","r, text, dataframe, text-mining, plaintext","<p>Here's one way to do it.</p>

<ol>
<li>Use <code>readLines()</code> to read in the full page, 56 lines.</li>
<li>Determine the information from the header by knowing the line numbers and positions in the line for latitude, longitude, elevation, city, state, and year. Use <code>substr()</code></li>
<li>Using the year obtained there, write out all the dates of that year. <code>cbind</code> that with the header information.</li>
<li>Use a function that takes the day of month and month number, and locates the corresponding precipitation on the page. Line number is <code>14 + dayOfMonth</code>, horizontal offset can be a vector with 12 numbers, one for each month. Add that column to your page.</li>
</ol>

<p>If you <code>rbind</code> each page as you go through, you will end up with a long (!) tidy dataset. [edit] You will also spend an eternity as memory is managed if your dataset is large. Instead you can create a list of dataframes and bind them all at the end. See <a href=""https://stackoverflow.com/questions/11486369/growing-a-data-frame-in-a-memory-efficient-manner"">this question</a> and <a href=""https://stackoverflow.com/questions/2851327/converting-a-list-of-data-frames-into-one-data-frame-in-r"">this question</a> for more information.</p>

<p>Here is some code I came up with: you can test it on a short extract first.</p>

<pre><code>library(""lubridate"")
raw2page &lt;- function(rawdata) {
# Takes a vector of chars, one page of data, returns a tidy dataframe
# Template for the page header
yearbound &lt;- c(5,60,63)
stationbound &lt;- c(5,105,112)
latbound &lt;- c(7,16,19)
longbound &lt;- c(8,16,19)
deptobound &lt;- c(7,81,101)
municipiobound &lt;- c(8,81,101)

framebounds &lt;- rbind(yearbound,stationbound,latbound,longbound,deptobound,municipiobound)
colnames(framebounds) &lt;- c(""line"",""start"",""end"")
framebounds &lt;- as.data.frame(framebounds)

framedata &lt;- data.frame()
framedata &lt;- as.data.frame(rbind(with(framebounds, substr(rawdata[line],start,end))))
colnames(framedata) &lt;- c(""year"",""station"",""latitude"",""longitude"",""depto"",""municipio"")
trim &lt;- function (x) gsub(""^\\s+|\\s+$"", """", x)
framedata$depto &lt;- trim(framedata$depto)
framedata$municipio &lt;- trim(framedata$municipio)

# Make a column listing all dates of the year
st &lt;- as.Date(paste(framedata[1]$year,""-01-01"",sep=""""))
en &lt;- as.Date(paste(framedata[1]$year,""-12-31"",sep=""""))
date &lt;- seq(as.Date(st),as.Date(en), by=1)
pagedata &lt;- cbind(framedata,date)

# horizontal offsets for the last digit of each month (the last digit is aligned)
mboundaries&lt;-c(25,34,43,52,61,70,79,88,97,106,115,124)
# now we can take the dates we generated before and use these coordinates to read the rainfall amount into a vector
rainfall &lt;- as.numeric(substr(rawdata[14+mday(pagedata$date)],mboundaries[month(pagedata$date)]-6,mboundaries[month(pagedata$date)] ))
# and bind the vector to the page data to make a tidy data set 
page &lt;- cbind(pagedata,rainfall)
page
}

raw &lt;- readLines(""area1.txt"") # read in all the data

# Get all the page header line numbers
headers &lt;- as.data.frame(grep(""HIDROLOGIA"", raw))
colnames(headers) &lt;- c(""linenum"")

listOfDataFrames &lt;- vector(mode = ""list"", length = nrow(headers))

# page by page, append onto the list
output &lt;- data.frame()
for (i in 1:nrow(headers)) {
  start &lt;- headers[i,]
  end &lt;- start + 56
  listOfDataFrames[[i]] &lt;- raw2page(raw[start:end])
      }
library(""plyr"")
output &lt;- rbind.fill(listOfDataFrames)
print(summary(output))
</code></pre>
",3,1,2158,2015-10-01 21:16:27,https://stackoverflow.com/questions/32896997/read-an-irregular-text-data-file-into-r
Error in substring arguments when calling mapply with regmatches as argument,"<p>[![enter image description here][1]][1]The following code compiles a data frame of reviews of businesses and aims to extract the business name from each row.  There are no matches with the regular expression in the first 14 rows, I've noticed, and there's no problem there.  As soon as I include the first row with a match (15 in this case), I receive the following error.  </p>

<pre><code>Error in substring(x[ind], so, eo) : invalid substring arguments
</code></pre>

<p>It seems like the problem is between regexpr sending the results to index.list, and regmatches being unable to use the index.list as the proper argument.  </p>

<p>Note that when I run just the 15th row without the mapply, I obtain the proper result.  If there is a more efficient way to get the results of regmatches into a new column in lieu of mapply, please let me know.  I cannot use stringr or other packages (school assignment).  </p>

<pre><code>require(""tm"")
reviews &lt;- VCorpus(DirSource(""C:/...../reviews""))

all.reviews &lt;-   data.frame(text=unlist(sapply(reviews,'[',""content"")),stringsAsFactors=F)

data &lt;- all.reviews[10:15,]

index.list &lt;- mapply(regexpr, ""(?&lt;=Review of )(\\w+ )+(?=-\\s*   [A-Z])"", data, perl=T))
rest.names &lt;- mapply(regmatches, data, index.list)
</code></pre>

<p>I am unable to attach an image, so here is a sample from the 15th row of all.reviews data.frame (which has dim 90 X 1):</p>

<p><em>Via S.'s Review of Good To Go -Bronx (4/5) on Yelp. Good To Go 22 reviews Rating Details Categories: Restaurants American (Traditional)Restaurants Italian American (Traditional); Italian [Edit] 1894 Eastchester Rd Bronx; NY 10461 (718) 829-2222 <a href=""http://www.good2gorestaurant.com"" rel=""nofollow"">http://www.good2gorestaurant.com</a> Explore the menu Add Photos Hours: Mon-Thu; Sun 10 am - 10pm Fri-Sat 10 am - 11 pm Good for Kids: Yes Accepts Credit Cards: Yes Parking: Valet; Garage; Street;Private Lot Attire: Casual Good for Groups: Yes Price Range: $ Takes Reservations: Yes</em> </p>

<p>The expected output for the function is a vector with 90 elements, where each element is the restaurant name (or a blank if nothing was extracted from the string, since there are other review formats that I will address with additional regexpr statements).  We would expect to see ""Good to Go"" in the 15th index.  </p>
","regex, r, text-mining, mapply","<p>A classmate of mine came up with an answer.  Turns out that the last argument of mapply should be a list, not a dataframe, so lapply does the trick:  </p>

<pre><code>data.mult &lt;- all.reviews[1:15,]    

index.list &lt;- lapply(data.mult,function(x) {
regexpr(""(?&lt;=Review of )([A-z-'\\s*]+)+\\w+(?=\\s*-\\s*[A-Z])"",x,perl=T)})

rest.names &lt;- mapply(regmatches, data.mult, index.list)
</code></pre>
",0,0,701,2015-10-04 02:09:31,https://stackoverflow.com/questions/32929351/error-in-substring-arguments-when-calling-mapply-with-regmatches-as-argument
Extract words from continous strings,"<p>I have inputs:</p>

<pre><code>callme
senditnow
runningcar
</code></pre>

<p>how can i extract words like call me,send it now, running car.Is there any library in python to do that using some dictionary.</p>
","python, algorithm, nltk, text-mining","<p>I don't know about proper ways to do it, but there are ways to cheat ! </p>

<p>It was an algorithm exercise problem I solved at university, where you have a string with no spaces (e.g. <code>thesearethereasons</code>) and you're trying to get back the words.</p>

<p>The trick was trying to turn the problem into a graph (Directed Acyclic Graph) : 
You need a function to check if a word exists in dictionnary (I used  <code>/usr/share/dict/words</code> parsed with <code>grep</code> at the time), and then try all combination of words that way. Store the words and start/end indices.</p>

<pre><code>These (0,4)
The (0,2)
Sea (3,5)
[...]
</code></pre>

<p>You then just need to turn these into a graph by making links between end of one word and beginning of other :</p>

<pre><code>*--+The----Sea-------(no more words there)
   |
   +-These---Are+----The+-------Reason (not end)
                |       +----Reasons [String end]   &lt;== Solution
                |
                +----There---A---Sons [String end]  &lt;== False Positive
</code></pre>

<p>Now you have a word graph, just follow it (DFS) to the end. Any path that ends with the string's end represent words =)</p>

<p>As you can imagine, several word combinations can do the trick, giving you back a series of ""plausible sentences"". Not a perfect solution then</p>
",5,2,249,2015-10-05 09:20:04,https://stackoverflow.com/questions/32945053/extract-words-from-continous-strings
Find dates in text,"<p>I want to find Dates in a document.</p>

<p>And return this Dates in an array.</p>

<p><b>Lets suppose I have this text:</b></p>

<pre><code>On the 03/09/2015 I am swiming in a pool, that was build on the 27-03-1994
</code></pre>

<p>Now my code should return <code>['03/09/2015','27-03-1994']</code> or simply two Date objects in an array.</p>

<p>My idea was to solve this problem with regex, but the method <code>search()</code> only returns one result and with <code>test()</code> I only can test a string!</p>

<p><b>How would you try to solve it?</b> Espacially when you dont know the exact format of the Date? Thanks</p>
","javascript, regex, node.js, text-mining","<p>You can use <strong><a href=""https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/String/match"" rel=""noreferrer""><code>match()</code></a></strong> with regex <strong><a href=""https://regex101.com/r/vJ7sH0/3"" rel=""noreferrer""><code>/\d{2}([\/.-])\d{2}\1\d{4}/g</code></a></strong></p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>var str = 'On the 03/09/2015 I am swiming in a pool, that was build on the 27-03-1994';

var res = str.match(/\d{2}([\/.-])\d{2}\1\d{4}/g);

document.getElementById('out').value = res;</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;input id=""out""&gt;</code></pre>
</div>
</div>
</p>

<p><img src=""https://www.debuggex.com/i/HPXh-wn3jvD4B0Z2.png"" alt=""Regular expression visualization""></p>

<p>Or you can do something like this with help of capturing group</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>var str = 'On the 03/09/2015 I am swiming in a pool, that was build on the 27-03-1994';

var res = str.match(/\d{2}(\D)\d{2}\1\d{4}/g);

document.getElementById('out').value = res;</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;input id=""out""&gt;</code></pre>
</div>
</div>
</p>

<p><img src=""https://www.debuggex.com/i/ftFhAhAJbwPMvLXW.png"" alt=""Regular expression visualization""></p>
",9,1,10883,2015-10-05 13:17:25,https://stackoverflow.com/questions/32949649/find-dates-in-text
Extracting Keywords from text in R,"<p>I want to extract Insurance services related keywords from text in R. I created keywords list and used common function from <code>qdap</code> library.</p>

<pre><code>   bag &lt;- bag_o_words(corpus) 
   b &lt;- common(bag,keywords,overlap=""all"")
</code></pre>

<p>But the results are just the common words with more than 1 frequency.
I have also used <code>RKEA</code> library.</p>

<pre><code>keywords &lt;- c(""directasia"", ""directasia.com"", ""Frank"", ""frank"", ""OCBC"", ""NTUC"",
              ""NTUC Income"", ""Frank by OCBC"", ""customer service"", ""atm"",
              ""insurance"", ""claim"", ""agent"", ""premium"", ""policy"", ""customer care"",
              ""customer"", ""draft"", ""account"", ""credit"", ""savings"",""debit"",""ivr"",
              ""offer"", ""transacation"", ""banking"", ""website"", ""mobile"", ""i-safe"",
               ""customer"", ""demat"", ""network"", ""phone"", ""interest"", ""loan"",
               ""transfer"", ""deposit"",  ""otp"", ""rewards"", ""redemption"")
   tmpdir &lt;- tempfile()
   dir.create(tmpdir)
   model &lt;- file.path(tmpdir, ""crudeModel"")
   createModel(corpus,keywords,model)
   extractKeywords(corpus, model)
</code></pre>

<p>However I am getting the following errors</p>

<blockquote>
  <p>Error in createModel(corpus, keywords, model) : number of documents and keywords does not match</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>Error in .jcall(ke, ""V"", ""extractKeyphrases"", .jcall(ke,Ljava/util/Hashtable;"",  :  java.io.FileNotFoundException: C:\Users\Bitanshu\AppData\Local\Temp\RtmpEHu9uA\file14c4160f41c2\crudeModel (The system cannot find the file specified)</p>
</blockquote>

<p>The second error is I think because <code>createModel</code> is not successful.</p>

<p>Can anyone suggest how to rectify this or an alternative approach?
The text data has been extracted from twitter.</p>
","r, nlp, text-mining, text-analysis","<p>You can try the <strong>quanteda</strong> package.  I'd suggest getting the GitHub version instead of the CRAN release, since just two days ago I overhauled the <code>kwic()</code> function.  Example:</p>

<pre><code>&gt; require(quanteda)
&gt; kwic(inaugTexts, ""asia"")
                                           contextPre keyword                       contextPost
 [1841-Harrison, 8599]        or Egypt and the lesser    Asia would furnish the larger dividend
     [1909-Taft, 1872]     our shores from Europe and    Asia of course reduces the necessity  
 [1925-Coolidge, 2215] differences in both Europe and    Asia . But there is a                 
[1953-Eisenhower, 325]           the earth. Masses of    Asia have awakened to strike off      
    [2013-Obama, 1514] We will support democracy from    Asia to Africa, from the   
</code></pre>
",2,3,5002,2015-10-07 07:39:42,https://stackoverflow.com/questions/32986417/extracting-keywords-from-text-in-r
Is it a good idea to use GridSearch on small dataset and apply results on big one?,"<p>I have a Pipeline with TfidVectorizer and OneVsRestClassifier(SGDClassifier). This is the parameters of the gridSearch I want to perform:</p>

<pre><code>parameters = {'tfidf-vect__ngram_range': ((1, 1), (1, 3)),
              'tfidf-vect__stop_words': (None,'english'),
              'tfidf-vect__min_df': (1e-3,1e-6),
              'tfidf-vect__max_features': (1e7,1e4),
              'tfidf-vect__norm': ('l1','l2',None),
              'tfidf-vect__use_idf': (True, False),
              'tfidf-vect__sublinear_tf': (True, False),
              'clf__estimator__alpha': (1e-5, 1e-7),
              'clf__estimator__loss':('hinge', 'log', 'modified_huber'),
              'clf__estimator__penalty':(None, 'l2', 'l1','elasticnet'),
              'clf__estimator__class_weight':(""auto"", None),
              'clf__estimator__warm_start':(True,False),
              'clf__estimator__average':(True,False,4,8,16)
}
</code></pre>

<p><strong>Problem:</strong> I want to know which is the best combination of parameters but I can't run a gridSearch like this on 100k instances with my computer.</p>

<p><strong>Question:</strong> How similar would the results of this kind of gridSearch be (maybe with smaller parameter set) on 100k instances dataset and a subset of lets say 10-20k samples?</p>

<p>As you may already know I am dealing with multi-label classification of texts problem.</p>

<p>Thank you :)</p>
","machine-learning, scikit-learn, text-mining, grid-search, hyperparameters","<p>Yes, that is a decent strategy. You aren't guaranteed in any way to get the best - but they should still be reasonably good. You do have to be careful though that you don't overfit to the smaller dataset with your parameter search. </p>
",1,0,454,2015-10-07 15:44:23,https://stackoverflow.com/questions/32996704/is-it-a-good-idea-to-use-gridsearch-on-small-dataset-and-apply-results-on-big-on
Get all the indices of unique elements,"<p>I have a dataset with 500 000 entries. Each entry in it has a userId and a productId. I want to get all userIds corresponding to each distinct productIds. But the list is to huge that none of the following method works for me, it's going very slow. Is there any faster solution.</p>

<p><strong>Using <code>lapply</code>: (Problem: Traversing the whole rpid list for each uniqPids elements)</strong></p>

<pre><code>orderedIndx &lt;- lapply(uniqPids, function(x){
    which(rpid %in% x)
})
names(orderedIndx) &lt;- uniqPids
#Looking for indices with each unique productIds
</code></pre>

<p><strong>Using <code>For</code> loop:</strong></p>

<pre><code>  orderedIndx &lt;- list()
  for(j in 1:length(rpid)){
    existing &lt;- length(orderedIndx[rpid[j]])
    orderedIndx[rpid[j]][existing + 1] &lt;- j
  }
</code></pre>

<p><strong>Sample Data:</strong></p>

<pre><code>ruid[1:10]
# [1] ""a3sgxh7auhu8gw"" ""a1d87f6zcve5nk"" ""abxlmwjixxain""  ""a395borc6fgvxv"" ""a1uqrsclf8gw1t"" ""adt0srk1mgoeu"" 
 [7] ""a1sp2kvkfxxru1"" ""a3jrgqveqn31iq"" ""a1mzyo9tzk0bbi"" ""a21bt40vzccyt4""

rpid[1:10]
# [1] ""b001e4kfg0"" ""b001e4kfg0"" ""b000lqoch0"" ""b000ua0qiq"" ""b006k2zz7k"" ""b006k2zz7k"" ""b006k2zz7k"" ""b006k2zz7k""
 [9] ""b000e7l2r4"" ""b00171apva""
</code></pre>

<p>Output should be like: </p>

<pre><code>b001e4kfg0 -&gt; a3sgxh7auhu8gw, a1d87f6zcve5nk
b000lqoch0 -&gt; abxlmwjixxain
b000ua0qiq -&gt; a395borc6fgvxv
b006k2zz7k -&gt; a1uqrsclf8gw1t, adt0srk1mgoeu, a1sp2kvkfxxru1, a3jrgqveqn31iq
b000e7l2r4 -&gt; a1mzyo9tzk0bbi
b00171apva -&gt; a21bt40vzccyt4
</code></pre>
","r, text-mining, data-processing","<p>Not exactly sure what type of output you want, or how many rows you have in your dataset, but I'd suggest 3 versions and you can chose the one you like. First version uses <code>dplyr</code> and character values for your variables. I expect this to be slow if you have millions of rows. Second version uses <code>dplyr</code> but factor variables. I expect this to be faster than the previous one. Third version uses <code>data.table</code>. I expect this to be equally fast, or faster than the second version.</p>

<pre><code>library(dplyr)

ruid = 
c(""a3sgxh7auhu8gw"", ""a1d87f6zcve5nk"", ""abxlmwjixxain"",  ""a395borc6fgvxv"",
  ""a1uqrsclf8gw1t"", ""adt0srk1mgoeu"", ""a1sp2kvkfxxru1"", ""a3jrgqveqn31iq"",
  ""a1mzyo9tzk0bbi"", ""a21bt40vzccyt4"")

rpid =
c(""b001e4kfg0"", ""b001e4kfg0"", ""b000lqoch0"", ""b000ua0qiq"", ""b006k2zz7k"",
  ""b006k2zz7k"", ""b006k2zz7k"", ""b006k2zz7k"", ""b000e7l2r4"", ""b00171apva"")

### using dplyr and character values
dt = data.frame(rpid, ruid, stringsAsFactors = F)

dt %&gt;%
  group_by(rpid) %&gt;%
  do(data.frame(list_ruids = paste(c(.$ruid), collapse="", ""))) %&gt;%
  ungroup

#         rpid                                                    list_ruids
#        (chr)                                                         (chr)
# 1 b000e7l2r4                                                a1mzyo9tzk0bbi
# 2 b000lqoch0                                                 abxlmwjixxain
# 3 b000ua0qiq                                                a395borc6fgvxv
# 4 b00171apva                                                a21bt40vzccyt4
# 5 b001e4kfg0                                a3sgxh7auhu8gw, a1d87f6zcve5nk
# 6 b006k2zz7k a1uqrsclf8gw1t, adt0srk1mgoeu, a1sp2kvkfxxru1, a3jrgqveqn31iq


# ----------------------------------

### using dplyr and factor values
dt = data.frame(rpid, ruid, stringsAsFactors = T)

dt %&gt;%
  group_by(rpid) %&gt;%
  do(data.frame(list_ruids = paste(c(levels(dt$ruid)[.$ruid]), collapse="", ""))) %&gt;%
  ungroup

#         rpid                                                    list_ruids
#       (fctr)                                                         (chr)
# 1 b000e7l2r4                                                a1mzyo9tzk0bbi
# 2 b000lqoch0                                                 abxlmwjixxain
# 3 b000ua0qiq                                                a395borc6fgvxv
# 4 b00171apva                                                a21bt40vzccyt4
# 5 b001e4kfg0                                a3sgxh7auhu8gw, a1d87f6zcve5nk
# 6 b006k2zz7k a1uqrsclf8gw1t, adt0srk1mgoeu, a1sp2kvkfxxru1, a3jrgqveqn31iq


# -------------------------------------

library(data.table)

### using data.table
dt = data.table(rpid, ruid)

dt[, list(list_ruids = paste(c(ruid), collapse="", "")), by = rpid]

#          rpid                                                    list_ruids
# 1: b001e4kfg0                                a3sgxh7auhu8gw, a1d87f6zcve5nk
# 2: b000lqoch0                                                 abxlmwjixxain
# 3: b000ua0qiq                                                a395borc6fgvxv
# 4: b006k2zz7k a1uqrsclf8gw1t, adt0srk1mgoeu, a1sp2kvkfxxru1, a3jrgqveqn31iq
# 5: b000e7l2r4                                                a1mzyo9tzk0bbi
# 6: b00171apva                                                a21bt40vzccyt4
</code></pre>
",1,0,216,2015-10-07 21:01:17,https://stackoverflow.com/questions/33002343/get-all-the-indices-of-unique-elements
How to get term-document matrix from multiple documents with Spark?,"<p>I'm trying to generete a term-document matrix from multiple documents. I could run LDA Model from a already created matrix, now I need this step back.
Ive tried to implement a simple term-doc matrix, but now I'm stucked. What I did was:</p>

<pre><code>//GETS ALL FILES FROM INPUT PATH
JavaPairRDD&lt;String, String&gt; doc_words = context.wholeTextFiles(input_path);

//SPLIT BY "" ""
JavaPairRDD&lt;String, String&gt; tokenized = doc_words.flatMapValues(Preprocessing_DocumentTermMatrix.WORDS_EXTRACTOR);

//SEE METHOD WORDS_MAPPER.
JavaRDD&lt;Tuple2&lt;Tuple2&lt;String, String&gt;, Integer&gt;&gt; rdd = tokenized.flatMap(WORDS_MAPPER);


//METHOD WORDS_MAPPER
public static final FlatMapFunction&lt;Tuple2&lt;String, String&gt;, Tuple2&lt;Tuple2&lt;String, String&gt;, Integer&gt;&gt; WORDS_MAPPER = new FlatMapFunction&lt;Tuple2&lt;String, String&gt;, Tuple2&lt;Tuple2&lt;String, String&gt;, Integer&gt;&gt;() {

    public Iterable&lt;Tuple2&lt;Tuple2&lt;String, String&gt;, Integer&gt;&gt; call(Tuple2&lt;String, String&gt; stringIntegerTuple2) throws Exception {
        return Arrays.asList(new Tuple2&lt;Tuple2&lt;String, String&gt;, Integer&gt;(new Tuple2&lt;String,String&gt;(stringIntegerTuple2._1(), stringIntegerTuple2._2()), 1)); 
    } 
};
</code></pre>

<p>So, this function give me a result like this:</p>

<pre><code>((DOC_0, TERM0), 1)
((DOC_0, TERM0), 1)
((DOC_0, TERM1), 1)
((DOC_1, TERM0), 1)
((DOC_1, TERM2), 1)
</code></pre>

<p>I guess this is allright, but now I need to reduce it and extract an output like this:</p>

<pre><code>(DOC_0, (TERM0, 2), (TERM1, 1))
(DOC_1, (TERM0, 1), (TERM2, 1))
</code></pre>

<p>Ive tried a lot of things and could not get it... Some one can help me?</p>
","java, apache-spark, text-mining, apache-spark-mllib, term-document-matrix","<p>Here is solution :</p>

<pre><code>JavaPairRDD&lt;String, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; newrdd = JavaPairRDD.fromJavaRDD(rdd).reduceByKey((a, b) -&gt; a + b)
                .mapToPair(t -&gt; new Tuple2&lt;&gt;(t._1._1, new Tuple2&lt;&gt;(t._1._2, t._2))).groupByKey();
</code></pre>
",2,2,611,2015-10-07 21:05:52,https://stackoverflow.com/questions/33002407/how-to-get-term-document-matrix-from-multiple-documents-with-spark
Removal of Phrase using wildcards,"<p>I'm searching on how to use wildcard characters as part of the removal criteria for a section of a corpus. I was unable to find anything on SO or google related to this issue.</p>
<p>Purpose: Analyzing large dataset of standardized notes where employee input is broken into sections of the text.</p>
<p><strong>Example data:</strong></p>
<p><code>***Date; Area: asdfwerqw Detail: xxxxx Requested Action: xxxxxx Assigned to: John Doe</code></p>
<p><strong>Portion to extract for analysis:</strong></p>
<p><code>Detail:xxxxx  Requested Action:xxxxxx</code></p>
<p>Number of items before Detail may be more. Also, Assigned to: may not appear.</p>
","r, text-mining, tm","<p>It's hard to tell without more examples and details, but you're probably going to want to use regular expressions with positive lookahead and optional items:</p>

<pre><code>library(stringr)

text &lt;- c(""***Date; Area: asdfwerqw Detail: xxxxx Requested Action: xxxxxx Assigned to: John Doe"")

str_extract_all(text, c(""Detail(.*?)(?=Requested Action:)"", ""Requested Action:((.*?)(?=Assigned to:))?""))

# [[1]]
# [1] ""Detail: xxxxx ""
# 
# [[2]]
# [1] ""Requested Action: xxxxxx ""
</code></pre>
",0,-1,94,2015-10-08 19:04:46,https://stackoverflow.com/questions/33023974/removal-of-phrase-using-wildcards
SVD in a term document matrix do not give me values I want,"<p>I am trying to replicate an example in a paper called ""An introduction to LSA"": 
<a href=""http://lsa.colorado.edu/papers/dp1.LSAintro.pdf"" rel=""noreferrer"">An introduction to LSA</a></p>

<p>In the example they have the following term-document matrix:</p>

<p><a href=""https://i.sstatic.net/2pBpD.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/2pBpD.png"" alt=""enter image description here""></a></p>

<p>And then they apply SVD and get the following:</p>

<p><a href=""https://i.sstatic.net/HaOBJ.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/HaOBJ.png"" alt=""enter image description here""></a></p>

<p>Trying to replicate this, I wrote the following R code:</p>

<pre><code>library(lsa); library(tm)

d1 = ""Human machine interface for ABC computer applications""
d2 = ""A survey of user opinion of computer system response time""
d3 = ""The EPS user interface management system""
d4 = ""System and human system engineering testing of EPS""
d5 &lt;- ""Relation of user perceived response time to error measurement""
d6 &lt;- ""The generation of random, binary, ordered trees""
d7 &lt;- ""The intersection graph of paths in trees""
d8 &lt;- ""Graph minors IV: Widths of trees and well-quasi-ordering""
d9 &lt;- ""Graph minors: A survey""

# Words that appear in at least two of the titles
D &lt;- c(d1, d2, d3, d4, d5, d6, d7, d8, d9)

corpus &lt;- Corpus(VectorSource(D))

# Remove Punctuation
corpus &lt;- tm_map(corpus, removePunctuation)

# tolower
corpus &lt;- tm_map(corpus, content_transformer(tolower))

# Stopword Removal
corpus &lt;- tm_map(corpus, function(x) removeWords(x, stopwords(""english"")))

# term document Matrix
myMatrix &lt;- TermDocumentMatrix(corpus)

# Delete terms that only appear in a document
rowTotals &lt;- apply(myMatrix, 1, sum)
myMatrix.new &lt;- myMatrix[rowTotals &gt; 1, ]

# Correlation Matrix of terms
cor(t(as.matrix(myMatrix.new)))

# lsaSpace &lt;- lsa(myMatrix.new)
# myMatrix.reduced &lt;- lsaSpace$tk %*% diag(lsaSpace$sk) %*% t(lsaSpace$dk)

mySVD &lt;- svd(myMatrix.new)
</code></pre>

<p>I got the same term-document matrix, and actually obtained the same correlations:</p>

<pre><code>&gt; inspect(myMatrix.new)
&lt;&lt;TermDocumentMatrix (terms: 12, documents: 9)&gt;&gt;
Non-/sparse entries: 28/80
Sparsity           : 74%
Maximal term length: 9
Weighting          : term frequency (tf)

           Docs
Terms       1 2 3 4 5 6 7 8 9
  computer  1 1 0 0 0 0 0 0 0
  eps       0 0 1 1 0 0 0 0 0
  graph     0 0 0 0 0 0 1 1 1
  human     1 0 0 1 0 0 0 0 0
  interface 1 0 1 0 0 0 0 0 0
  minors    0 0 0 0 0 0 0 1 1
  response  0 1 0 0 1 0 0 0 0
  survey    0 1 0 0 0 0 0 0 1
  system    0 1 1 2 0 0 0 0 0
  time      0 1 0 0 1 0 0 0 0
  trees     0 0 0 0 0 1 1 1 0
  user      0 1 1 0 1 0 0 0 0
&gt; cor(as.matrix(t(myMatrix.new)))
            computer        eps      graph      human  interface     minors
computer   1.0000000 -0.2857143 -0.3779645  0.3571429  0.3571429 -0.2857143
eps       -0.2857143  1.0000000 -0.3779645  0.3571429  0.3571429 -0.2857143
graph     -0.3779645 -0.3779645  1.0000000 -0.3779645 -0.3779645  0.7559289
human      0.3571429  0.3571429 -0.3779645  1.0000000  0.3571429 -0.2857143
interface  0.3571429  0.3571429 -0.3779645  0.3571429  1.0000000 -0.2857143
minors    -0.2857143 -0.2857143  0.7559289 -0.2857143 -0.2857143  1.0000000
response   0.3571429 -0.2857143 -0.3779645 -0.2857143 -0.2857143 -0.2857143
survey     0.3571429 -0.2857143  0.1889822 -0.2857143 -0.2857143  0.3571429
system     0.0433555  0.8237545 -0.4588315  0.4335550  0.0433555 -0.3468440
time       0.3571429 -0.2857143 -0.3779645 -0.2857143 -0.2857143 -0.2857143
trees     -0.3779645 -0.3779645  0.5000000 -0.3779645 -0.3779645  0.1889822
user       0.1889822  0.1889822 -0.5000000 -0.3779645  0.1889822 -0.3779645
            response     survey     system       time      trees       user
computer   0.3571429  0.3571429  0.0433555  0.3571429 -0.3779645  0.1889822
eps       -0.2857143 -0.2857143  0.8237545 -0.2857143 -0.3779645  0.1889822
graph     -0.3779645  0.1889822 -0.4588315 -0.3779645  0.5000000 -0.5000000
human     -0.2857143 -0.2857143  0.4335550 -0.2857143 -0.3779645 -0.3779645
interface -0.2857143 -0.2857143  0.0433555 -0.2857143 -0.3779645  0.1889822
minors    -0.2857143  0.3571429 -0.3468440 -0.2857143  0.1889822 -0.3779645
response   1.0000000  0.3571429  0.0433555  1.0000000 -0.3779645  0.7559289
survey     0.3571429  1.0000000  0.0433555  0.3571429 -0.3779645  0.1889822
system     0.0433555  0.0433555  1.0000000  0.0433555 -0.4588315  0.2294157
time       1.0000000  0.3571429  0.0433555  1.0000000 -0.3779645  0.7559289
trees     -0.3779645 -0.3779645 -0.4588315 -0.3779645  1.0000000 -0.5000000
user       0.7559289  0.1889822  0.2294157  0.7559289 -0.5000000  1.0000000
</code></pre>

<p>However I tried to apply SVD to the matrix, and the only values that are equal are the eigenvalues, I cannot get what they got in the paper.</p>

<pre><code>&gt; mySVD
$d
[1] 3.3408838 2.5417010 2.3539435 1.6445323 1.5048316 1.3063820 0.8459031
[8] 0.5601344 0.3636768

$u
             [,1]        [,2]       [,3]          [,4]        [,5]        [,6]
 [1,] -0.24047023 -0.04315195  0.1644291  0.5949618181 -0.10675529 -0.25495513
 [2,] -0.30082816  0.14127047 -0.3303084 -0.1880919179  0.11478462  0.27215528
 [3,] -0.03613585 -0.62278523 -0.2230864 -0.0007000721 -0.06825294  0.11490895
 [4,] -0.22135078  0.11317962 -0.2889582  0.4147507404 -0.10627512 -0.34098332
 [5,] -0.19764540  0.07208778 -0.1350396  0.5522395837  0.28176894  0.49587801
 [6,] -0.03175633 -0.45050892 -0.1411152  0.0087294706 -0.30049511  0.27734340
 [7,] -0.26503747 -0.10715957  0.4259985 -0.0738121922  0.08031938 -0.16967639
 [8,] -0.20591786 -0.27364743  0.1775970  0.0323519366 -0.53715000  0.08094398
 [9,] -0.64448115  0.16730121 -0.3611482 -0.3334616013 -0.15895498 -0.20652259
[10,] -0.26503747 -0.10715957  0.4259985 -0.0738121922  0.08031938 -0.16967639
[11,] -0.01274618 -0.49016179 -0.2311202 -0.0248019985  0.59416952 -0.39212506
[12,] -0.40359886 -0.05707026  0.3378035 -0.0991137295  0.33173372  0.38483192
              [,7]          [,8]        [,9]
 [1,] -0.302240236  0.0623280150 -0.49244436
 [2,]  0.032994110 -0.0189980144  0.16533917
 [3,]  0.159575477 -0.6811254380 -0.23196123
 [4,]  0.522657771 -0.0604501376  0.40667751
 [5,] -0.070423441 -0.0099400372  0.10893027
 [6,]  0.339495286  0.6784178789 -0.18253498
 [7,]  0.282915727 -0.0161465472  0.05387469
 [8,] -0.466897525 -0.0362988295  0.57942611
 [9,] -0.165828575  0.0342720233 -0.27069629
[10,]  0.282915727 -0.0161465472  0.05387469
[11,] -0.288317461  0.2545679452  0.22542407
[12,]  0.002872175 -0.0003905042 -0.01232935

$v
              [,1]        [,2]        [,3]        [,4]        [,5]          [,6]
 [1,] -0.197392802  0.05591352 -0.11026973  0.94978502  0.04567856 -7.659356e-02
 [2,] -0.605990269 -0.16559288  0.49732649  0.02864890 -0.20632728 -2.564752e-01
 [3,] -0.462917508  0.12731206 -0.20760595 -0.04160920  0.37833623  7.243996e-01
 [4,] -0.542114417  0.23175523 -0.56992145 -0.26771404 -0.20560471 -3.688609e-01
 [5,] -0.279469108 -0.10677472  0.50544991 -0.15003543  0.32719441  3.481305e-02
 [6,] -0.003815213 -0.19284794 -0.09818424 -0.01508149  0.39484121 -3.001611e-01
 [7,] -0.014631468 -0.43787488 -0.19295557 -0.01550719  0.34948535 -2.122014e-01
 [8,] -0.024136835 -0.61512190 -0.25290398 -0.01019901  0.14979847  9.743417e-05
 [9,] -0.081957368 -0.52993707 -0.07927315  0.02455491 -0.60199299  3.622190e-01
             [,7]         [,8]        [,9]
 [1,]  0.17731830 -0.014393259  0.06369229
 [2,] -0.43298424  0.049305326 -0.24278290
 [3,] -0.23688970  0.008825502 -0.02407687
 [4,]  0.26479952 -0.019466944  0.08420690
 [5,]  0.67230353 -0.058349563  0.26237588
 [6,] -0.34083983  0.454476523  0.61984719
 [7,] -0.15219472 -0.761527011 -0.01797518
 [8,]  0.24914592  0.449642757 -0.51989050
 [9,]  0.03803419 -0.069637550  0.45350675
</code></pre>

<p>Am I missing something?</p>

<p>Best Regards</p>

<p>EDIT:</p>

<p>It is supposed in the example, that the dimension is reduced and they deleted the less eigenvalues. My problem is that the correlations I get after SVD are different than that of the example:</p>

<p><a href=""https://i.sstatic.net/y5Ixf.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/y5Ixf.png"" alt=""enter image description here""></a></p>
","r, matrix, text-mining, svd, lsa","<p>I managed to find my mistake. When I was reconstructing the matrix, the transpose of the M = U D V' was not computed correctly. Now it works, sorry, it was my mistake... Also, I was computing cor between documents, when what I wanted was between terms.</p>

<p>I added the following lines:</p>

<pre><code>mySVD &lt;- svd(myMatrix.new)

Mp &lt;- mySVD$u[, c(1,2)] %*% diag(mySVD$d)[c(1, 2), c(1, 2)] %*% t(mySVD$v[, c(1, 2)])

rownames(Mp) &lt;- rownames(myMatrix.new)
cor(t(Mp))
</code></pre>
",3,6,1671,2015-10-13 21:20:09,https://stackoverflow.com/questions/33112832/svd-in-a-term-document-matrix-do-not-give-me-values-i-want
Difference between text mining and topic modeling,"<p>I am bit confused when people talk about text mining and topic modelling randomly in the discussions. Can anyone please explain the exact meaning of these two and their differences.</p>
","machine-learning, nlp, data-mining, text-mining, topic-modeling","<p><a href=""https://en.wikipedia.org/wiki/Text_mining"" rel=""nofollow"">Text mining</a> is a broad topic.</p>

<p><a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow"">Topic modeling</a> is one possible subtopic of text mining.</p>

<p>For further details, please see Wikipedia.</p>
",0,0,397,2015-10-14 09:50:58,https://stackoverflow.com/questions/33122092/difference-between-text-mining-and-topic-modeling
How to use lexicon dictionary in c#,"<p>i am working on sentiment analysis in c#, I have done preprocessing, and the next part is lexicon based analysis, for which I have found English Lexicon of about 6800 word <a href=""https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon"" rel=""nofollow"">lexicon by Professor Bing Liu </a> which contains two text files, one for positive and other is for negative. </p>

<p>I was thinking that I have to just find the each word(sentiment word) from these files that either the particular word is positive or negative. but the problem is, these files contains words without any space , without any format (means individual word can not be recognized from file ). </p>

<p>So how can I find the word in file? Or is there any other way through which I can work easily with this?</p>
","c#, dictionary, nlp, text-mining, sentiment-analysis","<p>The file uses <code>\n</code> as a line separator (unlike standard Windows <code>\r\n</code>).
So, just not open it with <strong>NotePad</strong> or alike, do it with <strong>WordPad</strong>.</p>

<p>To load the file into a collection (let it be <code>HashSet&lt;String&gt;</code> - you, probably, want to test if a word is <em>within the positive words</em> or not), you can use <em>Linq</em>:</p>

<pre><code>  HashSet&lt;String&gt; positives = new HashSet&lt;String&gt;(File
    .ReadLines(@""C:\positive-words.txt"")
    .Where(item =&gt; !String.isNullOrEmpty(item) &amp;&amp; !item.StartsWith("";"")));

  ....

  String testWord = ...

  if (positives.Contains(testWord)) {
    ...
  }
</code></pre>

<p>Actual file's content is</p>

<pre><code>;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
; 
; Opinion Lexicon: Positive
...
;       frequently in social media content. 
;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

a+
abound
abounds
abundance
...
zenith
zest
zippy
</code></pre>
",1,2,1396,2015-10-16 14:33:46,https://stackoverflow.com/questions/33173056/how-to-use-lexicon-dictionary-in-c
Term frequency table to DocumentTermMatrix in tm R package,"<p>I am using the tm package in R to do some text mining. I have a matrix of term frequencies where every row is a document, every column is a word and every cell is the frequency of the word. I am trying to convert that to a <code>DocumentTermTermMatrix</code> object. I can't seem to find a function that deals with that. Looks like the sources are usually the documents. </p>

<p>I've tried <code>as.DocumentTermTermMatrix()</code> but it asks for an argument ""weighting"" giving the following error:</p>

<blockquote>
  <p>Error in .TermDocumentMatrix(t(x), weighting) : <br/>
        argument ""weighting"" is missing, with no default</p>
</blockquote>

<p>here is the code for a simple reproducible example </p>

<pre><code>docs = matrix(sample(1:10, 50, replace=T), byrow = TRUE, ncol = 5, nrow=10) 
rownames(docs) = paste0(""doc"", 1:10)
colnames(docs) = c(""grad"", ""school"", ""is"", ""sleep"", ""deprivation"")
</code></pre>

<p>so I would need to coerce the matrix docs into a <code>DocumentTermMatrix</code>.</p>
","r, text-mining, tm, word-frequency","<p>Using your code example, you can use the following:</p>

<pre><code>docs = matrix(sample(1:10, 50, replace=T), byrow = TRUE, ncol = 5, nrow=10) 
rownames(docs) = paste0(""doc"", 1:10)
colnames(docs) = c(""grad"", ""school"", ""is"", ""sleep"", ""deprivation"")

dtm &lt;- as.DocumentTermMatrix(docs, weighting = weightTfIdf)
</code></pre>

<p>If you read the help DocumentTermMatrix you see the following under arguments</p>

<blockquote>
  <p><strong>weighting:</strong> A weighting function capable of handling a TermDocumentMatrix. It defaults to weightTf for term frequency
  weighting. Available weighting functions shipped with the tm package
  are weightTf, weightTfIdf, weightBin, and weightSMART.</p>
</blockquote>

<p>Depending on your need you have to specify the weighting formula to use with your document term matrix. Or create one yourself.</p>
",1,1,1731,2015-10-17 15:36:00,https://stackoverflow.com/questions/33188390/term-frequency-table-to-documenttermmatrix-in-tm-r-package
Unable to convert a Corpus to Data Frame in R,"<p>I've looked at the other similar questions that have been posted here (like <a href=""https://stackoverflow.com/questions/24703920/r-tm-package-vcorpus-error-in-converting-corpus-to-data-frame"">this</a>), but the problem persists.</p>

<p>I have a dataframe of textual data, which I need to stem. So I'm converting it into a corpus, stemming it, then completing the words from the stems, and then trying to get a dataframe of text as output.</p>

<pre><code>myCorpus &lt;- Corpus(VectorSource(textDf$text))
myCorpus &lt;- tm_map(myCorpus, removeWords, stopwords('english'))
myCorpus &lt;- tm_map(myCorpus, content_transformer(tolower))
myCorpus &lt;- tm_map(myCorpus, removePunctuation)
dictCorpus &lt;- myCorpus
myCorpus &lt;- tm_map(myCorpus, stemDocument)
myCorpus &lt;- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
</code></pre>

<p>Now I'm trying to get a dataframe back from this corpus so I've tried these following commands.</p>

<p><code>dataframe&lt;-data.frame(text=unlist(sapply(myCorpus, '[', ""content"")), 
                      stringsAsFactors=F)</code></p>

<p>and </p>

<p><code>dataframe&lt;-data.frame(text=unlist(sapply(myCorpus,</code>[<code>)), stringsAsFactors=F)</code></p>

<p>and also</p>

<pre><code>dataframe &lt;- 
    data.frame(id=sapply(corpus, meta, ""id""),
               text=unlist(lapply(sapply(corpus, '[', ""content""),paste,collapse=""\n"")),
               stringsAsFactors=FALSE)
</code></pre>

<p>from <a href=""https://stackoverflow.com/questions/32036391/how-to-convert-corpus-to-data-frame-with-meta-data-in-r"">this</a> link</p>

<p>All of them produce the following error:</p>

<pre><code>Error in UseMethod(""meta"", x) : 
  no applicable method for 'meta' applied to an object of class ""character""
</code></pre>

<p>Any help would be greatly appreciated.</p>
","r, text-mining, tm, corpus","<p>This ought to do it:</p>

<pre><code>data.frame(text = sapply(myCorpus, as.character), stringsAsFactors = FALSE)
</code></pre>

<p><strong>edited with working solution</strong>, using <code>crude</code> as example</p>

<p>The problem here is that you cannot apply <code>stemCompletion</code> as a transformation.  </p>

<pre><code>getTransformations()
## [1] ""removeNumbers""     ""removePunctuation"" ""removeWords""       ""stemDocument""      ""stripWhitespace""  
</code></pre>

<p>does not include <code>stemCompletion</code>, which takes a vector of stemmed tokens as input.</p>

<p>So this should do it: first you extract the transformed texts and tokenise them, then complete the stems, then paste back together.  Here I have illustrated the solution using the built-in <code>crude</code> corpus.</p>

<pre><code>data(crude)
myCorpus &lt;- crude 
myCorpus &lt;- tm_map(myCorpus, removeWords, stopwords('english'))
myCorpus &lt;- tm_map(myCorpus, content_transformer(tolower))
myCorpus &lt;- tm_map(myCorpus, removePunctuation)
dictCorpus &lt;- myCorpus
myCorpus &lt;- tm_map(myCorpus, stemDocument)
# tokenize the corpus
myCorpusTokenized &lt;- lapply(myCorpus, scan_tokenizer)
# stem complete each token vector
myTokensStemCompleted &lt;- lapply(myCorpusTokenized, stemCompletion, dictCorpus)
# concatenate tokens by document, create data frame
myDf &lt;- data.frame(text = sapply(myTokensStemCompleted, paste, collapse = "" ""), stringsAsFactors = FALSE)
</code></pre>
",21,10,20355,2015-10-18 00:42:42,https://stackoverflow.com/questions/33193152/unable-to-convert-a-corpus-to-data-frame-in-r
Generating text from vector with counts,"<p>Here is my situation, let's suppose I have this data:</p>

<pre><code>        freq
hello    1
bye      2
</code></pre>

<p>I want to create, from this data a string of this structure:</p>

<pre><code>""hello bye bye""
</code></pre>

<p>I can do it using for loops and inelegant approaches. However my data can have up to 10000 rows, and is not only 1 vector I want to process. Is there an efficient way to do this?</p>
","r, string, vector, text-mining","<p>This line should work for tables, matrices, and data frames.</p>

<pre><code>paste(rep(rownames(data), data[,1]), collapse = "" "")
</code></pre>

<p>If you have other columns and <code>freq</code> may not be the first, you can use <code>data[, ""freq""]</code> (or <code>data$freq</code> or <code>data[[""freq""]]</code> for data frames and <em>dplyr</em> ""tbl"" objects) in place of <code>data[,1]</code> to be more explicit.</p>
",3,0,50,2015-10-18 15:47:06,https://stackoverflow.com/questions/33199913/generating-text-from-vector-with-counts
R Build an xml corpus out of multiple XML files,"<p>I am trying to process a corpus of xml files for text mining purposes. Is there a way to import multiple files into one xml object/database that could be worked with XPath later?</p>

<p>Is that a smart thing to do? I found a similar reports that import xml files into other data formats, such as dataframes or tm Corpus objects <a href=""https://stackoverflow.com/questions/22676706/parsing-multiple-xml-files-to-a-single-dateframe-in-r"">Parsing multiple xml files to a Single Dateframe in R</a>, however keeping them in an XML format should keep them tidy, maintain access to context as annotated corpora can have deep trees and make processing simpler because of the nice query language?</p>

<p>Many thanks for the consultation.</p>
","xml, r, xpath, text-mining, corpus","<p>I found this small program called <a href=""https://code.google.com/p/mergex/downloads/detail?name=mergex.exe&amp;can=4&amp;q="" rel=""nofollow"">mergex.exe</a> useful. It merges several XML files into a single file at the command line, really intuitive and simple.</p>
",1,0,744,2015-11-02 15:06:29,https://stackoverflow.com/questions/33480697/r-build-an-xml-corpus-out-of-multiple-xml-files
Spark - Reduce operation taking too long,"<p>I'm making an application with Spark that will run some topic extration algorithms. For that, first I need to make some preprocessing, extracting the document-term matrix by the end. Ive could done that, but for a (not that much) big collection of documents (only 2 thousand, 5MB), this proccess is taking forever.</p>

<p>So, debugging, Ive found where the program kinda stucks, and it's in a reduce operation. What I'm doing in this part of the code is counting how many times each term occurs on the collection, so first I done a ""map"", couting it for each rdd, and them I ""reduce"" it, saving the result inside a hashmap. The map operation is very fast, but in the reduce, its splitting the operation in 40 blocks, and each block takes 5~10 minutes to proccess.</p>

<p>So I'm trying to figure out what I'm doing wrong, or if reduce operations are that much costly.</p>

<p>SparkConf: Standalone mode, using local[2]. I've tried to use it as ""spark://master:7077"", and it worked, but still the same slowness.</p>

<p>Code:</p>

<p><em>""filesIn"" is a JavaPairRDD where the key is the file path and the value is the content of the file.
So, first the map, where I take this ""filesIn"", split the words, and count their frequency (in that case doesn't matter what document is)
And then the reduce, where I create a HashMap (term, freq).</em></p>

<pre><code>JavaRDD&lt;HashMap&lt;String, Integer&gt;&gt; termDF_ = filesIn.map(new Function&lt;Tuple2&lt;String, String&gt;, HashMap&lt;String, Integer&gt;&gt;() {

        @Override
        public HashMap&lt;String, Integer&gt; call(Tuple2&lt;String, String&gt; t) throws Exception {
            String[] allWords = t._2.split("" "");

            HashMap&lt;String, Double&gt; hashTermFreq = new HashMap&lt;String, Double&gt;();
            ArrayList&lt;String&gt; words = new ArrayList&lt;String&gt;();
            ArrayList&lt;String&gt; terms = new ArrayList&lt;String&gt;();
            HashMap&lt;String, Integer&gt; termDF = new HashMap&lt;String, Integer&gt;();

            for (String term : allWords) {

                if (hashTermFreq.containsKey(term)) {
                    Double freq = hashTermFreq.get(term);
                    hashTermFreq.put(term, freq + 1);
                } else {
                    if (term.length() &gt; 1) {
                        hashTermFreq.put(term, 1.0);
                        if (!terms.contains(term)) {
                            terms.add(term);
                        }
                        if (!words.contains(term)) {
                            words.add(term);
                            if (termDF.containsKey(term)) {
                                int value = termDF.get(term);
                                value++;
                                termDF.put(term, value);
                            } else {
                                termDF.put(term, 1);
                            }
                        }
                    }
                }
            }
            return termDF;
        }
    });

 HashMap&lt;String, Integer&gt; termDF = termDF_.reduce(new Function2&lt;HashMap&lt;String, Integer&gt;, HashMap&lt;String, Integer&gt;, HashMap&lt;String, Integer&gt;&gt;() {

        @Override
        public HashMap&lt;String, Integer&gt; call(HashMap&lt;String, Integer&gt; t1, HashMap&lt;String, Integer&gt; t2) throws Exception {
            HashMap&lt;String, Integer&gt; result = new HashMap&lt;String, Integer&gt;();

            Iterator iterator = t1.keySet().iterator();

            while (iterator.hasNext()) {
                String key = (String) iterator.next();
                if (result.containsKey(key) == false) {
                    result.put(key, t1.get(key));
                } else {
                    result.put(key, result.get(key) + 1);
                }

            }

            iterator = t2.keySet().iterator();

            while (iterator.hasNext()) {
                String key = (String) iterator.next();
                if (result.containsKey(key) == false) {
                    result.put(key, t2.get(key));
                } else {
                    result.put(key, result.get(key) + 1);
                }

            }

            return result;
        }
    });
</code></pre>

<p>Thanks!</p>
","mapreduce, apache-spark, text-mining, tf-idf","<p>OK, so just off the top of my head:</p>

<ul>
<li>Spark transformations are lazy. It means that <code>map</code> is not executed until you call subsequent <code>reduce</code> action so what you describe as slow <code>reduce</code> is most likely slow <code>map</code> + <code>reduce</code></li>
<li><code>ArrayList.contains</code> is O(N) so all these <code>words.contains</code> and <code>terms.contains</code> are extremely inefficient</li>
<li><code>map</code> logic smells fishy. In particular:

<ul>
<li>if term has been already seen you never get into <code>else</code> branch</li>
<li>at first glance <code>words</code> and <code>terms</code> should have exactly the same content and should be equivalent to the <code>hashTermFreq</code> keys or <code>termDF</code> keys.</li>
<li>it looks like values in <code>termDF</code> can only take value 1. If this is what you want and you ignore frequencies what is the point of creating <code>hashTermFreq</code>?</li>
</ul></li>
<li><code>reduce</code> phase as implemented here means an inefficient linear scan with growing object over the data while you what you really want is <code>reduceByKey</code>. </li>
</ul>

<p>Using Scala as a pseudocode your whole code can be efficiently expressed as follows:</p>

<pre class=""lang-scala prettyprint-override""><code>val termDF = filesIn.flatMap{
  case (_, text) =&gt; 
    text.split("" "") // Split
    .toSet // Take unique terms 
    .filter(_.size &gt; 1) // Remove single characters
    .map(term =&gt; (term, 1))} // map to pairs
  .reduceByKey(_ + _) // Reduce by key

termDF.collectAsMap // Optionally
</code></pre>

<p>Finally it looks like you're reinventing the wheel. At least some tools you need are already implemented in <a href=""https://spark.apache.org/docs/latest/mllib-feature-extraction.htm"" rel=""nofollow""><code>mllib.feature</code></a> or <a href=""https://spark.apache.org/docs/latest/ml-features.html"" rel=""nofollow""><code>ml.feature</code></a></p>
",2,2,2621,2015-11-06 02:42:39,https://stackoverflow.com/questions/33558593/spark-reduce-operation-taking-too-long
R Write in a txt File all column names that meet certain criteria,"<p>Let suppose I have the following data:</p>

<pre><code>A
      1 2
term1 0 1
term2 2 3
</code></pre>

<p>I want a txt file like this one:</p>

<pre><code>term1 2
term2 1 2
</code></pre>

<p>In other words, I write the rowname, and the column names in which the values are greater than 0.</p>

<p>I tried the following:</p>

<pre><code>filename &lt;- ""palabra_documento/mapeo.txt""
fileConn&lt;-file(filename)
rowNames &lt;- rownames(AA.matrix)
for(i in 1:nrow(AA.matrix))
{
    line &lt;- names(AA.matrix[i, AA.matrix[i, ] &gt; 0])
    line &lt;- c(rowNames[i], line, ""\n"")
    writeLines(paste(line, collapse = "" ""), fileConn)

}
close(fileConn)
</code></pre>

<p>However it does not work for two reasons:</p>

<ol>
<li>Rewrites the file everytime (I can only put one line in the txt file, my mistake)</li>
<li>When there is only one column that meet the criteria, it is a scalar value, so, no names for its column.</li>
</ol>
","r, file, loops, matrix, text-mining","<pre><code>(A &lt;- matrix(c(0, 2, 1, 3), ncol = 2, dimnames = list(c(""term1"", ""term2""), 1:2)))
#       1 2
# term1 0 1
# term2 2 3
l &lt;- apply(A, 1, function(x) which(x &gt; 0, arr.ind=TRUE, useNames = FALSE))
sink(""output.txt"")
for (i in 1:length(l))
    cat(names(l[i]), paste(l[[i]], collapse = "" ""), ""\n"")
sink()

file.show(""output.txt"")
# term1 2 
# term2 1 2 
</code></pre>
",1,1,146,2015-11-06 16:27:50,https://stackoverflow.com/questions/33571289/r-write-in-a-txt-file-all-column-names-that-meet-certain-criteria
how to know which keywords matched in elasticsaearch,"<p>Say that I query:</p>

<pre><code>POST /story/story/_search
{  
   ""query"":{  
      ""bool"":{  
         ""should"":[  
            {  
               ""match"":{  
                  ""termVariations"":{  
                     ""query"":""not driving"",
                     ""type"":""boolean"",
                     ""operator"":""AND""
                  }
               }
            },
            {  
               ""match"":{  
                  ""termVariations"":{  
                     ""query"":""driving"",
                     ""type"":""boolean"",
                     ""operator"":""AND""
                  }
               }
            }
         ]
      }
   }
}
</code></pre>

<p>This query returned by one analyzer or another 3 documents. 
How do I tell which should clause was matched? Can Elasticsearch return the matched phrase along with the result?</p>

<p>Thanks!</p>
","elasticsearch, lucene, text-mining, nosql","<p>The best option here would be <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/1.4/search-request-named-queries-and-filters.html"" rel=""nofollow"">named queries</a>.
You can name your query and the name of the queries that matched would be provided per document.</p>

<pre><code>{
  ""query"": {
    ""bool"": {
      ""should"": [
        {
          ""match"": {
            ""name.first"": {
              ""query"": ""qbox"",
              ""_name"": ""first""
            }
          }
        },
        {
          ""match"": {
            ""name.last"": {
              ""query"": ""search"",
              ""_name"": ""last""
            }
          }
        }
      ]
    }
  }
}
</code></pre>
",2,2,66,2015-11-12 15:14:53,https://stackoverflow.com/questions/33674787/how-to-know-which-keywords-matched-in-elasticsaearch
Getting specific numbers from a text file containing a mix of alphanumeric characters in python,"<p>I have a <code>.txt</code> file that looks like this but much longer:</p>

<pre><code>Image0001_01.tif[1] &lt;- Image0035_01.tif[1]: (410.0, -362.0) correlation (R)=0.05516124 (176 ms)
Image0001_01.tif[1] &lt;- Image0002_01.tif[1]: (489.0, -495.0) correlation (R)=0.047715914 (287 ms)
Image0002_01.tif[1] &lt;- Image0003_01.tif[1]: (647.0, 0.0) correlation (R)=0.8842946 (295 ms)
Image0001_01.tif[1] &lt;- Image0036_01.tif[1]: (265.0, -363.0) correlation (R)=0.039207384 (365 ms)
Image0002_01.tif[1] &lt;- Image0034_01.tif[1]: (626.0, -626.0) correlation (R)=0.60634625 (124 ms)
...........
</code></pre>

<p>I'd like to turn this into a comma separated file (csv) so that I can look at the correlations (R-values) but running into problems because of the weird formatting of this file. Is there a way I can do this in Python?</p>
","python, csv, text-mining","<p>Use <strong>re</strong> and <strong>csv</strong> in python to parse your file and convert it to a csv file:</p>

<pre><code>import re
import csv

re_expression = '^(.*?) &lt;- (.*?): \((.*?), (.*?)\) correlation \(R\)=(.*?) \((.*?) ms\)$'

with open('output.csv', 'w', newline='') as csvfile:
    outfile = csv.writer(csvfile)
    with open('input.txt') as f:
        while True:
            line = f.readline()
            if not line: break
            m = re.split(re_expression, line)
            outfile.writerow(m[1:-1])
</code></pre>
",1,0,137,2015-11-16 19:45:08,https://stackoverflow.com/questions/33743436/getting-specific-numbers-from-a-text-file-containing-a-mix-of-alphanumeric-chara
Getting term weights out of an LDA model in R,"<p>I was wondering if anyone knows of a way to extract term weights / probabilities out of a topic model constructed in R, using the <code>topicmodels</code> package.</p>

<p>Following the example in the following <a href=""https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf"" rel=""nofollow"">link</a> I created a topic model like so:</p>

<pre><code>Gibbs = LDA(JSS_dtm, k = 4, 
            method = ""Gibbs"",
            control = list(seed = 1, burnin = 1000, thin = 100, iter = 1000))
</code></pre>

<p>we can then get the topics using <code>topics(Gibbs,1)</code>, terms using <code>terms(Gibbs,10)</code> and even the topic probabilities using <code>Gibbs@gamma</code>, but after looking at <code>str(Gibbs)</code> it appears that there is no way to get term probabilities within each topic. This would be useful because topic 1 could be 50% term A and 50% term B, while topic 2 can be 90% Term C and 10% term D. I'm aware that tools like MALLET and Python's NLTK module offer this capability, but I was also hoping that a similar solution may exist in R.</p>

<p>If anyone know how this can be achieved, please let us know. </p>

<p>Many thanks!</p>

<p><strong>EDIT:</strong></p>

<p>For the benefit of the others, I thought I'd share my current workaround. If I knew term probabilities, I'd be able to visualise them and give the viewer a better understanding of what each topic means, but without the probabilities, I'm simply breaking down my data by each topic and creating a word cloud for each topic using binary weights. While these values are not probabilities, they give an indication of what each topic focuses on.</p>

<p>See the below code:</p>

<pre><code>JSS_text   &lt;- sapply(1:length(JSS_papers[,""description""]), function(x) unlist(JSS_papers[x,""description""]))
jss_df     &lt;- data.frame(text=JSS_text,topic=topics(Gibbs, 1))
jss_dec_df &lt;- data.frame()

for(i in unique(topics(Gibbs, 1))){
  jss_dec_df &lt;- rbind(jss_dec_df,data.frame(topic = i, 
                                            text = paste(jss_df[jss_df$topic==i,""text""],collapse="" "")))
}

corpus &lt;- Corpus(VectorSource(jss_dec_df$text))
JSS_dtm &lt;- TermDocumentMatrix(corpus,control = list(stemming = TRUE, 
                                                    stopwords = TRUE, 
                                                    minWordLength = 3,
                                                    removeNumbers = TRUE, 
                                                    removePunctuation = TRUE,
                                                    function(x)weightSMART(x,spec=""bnc"")))

(JSS_dtm  = removeSparseTerms(JSS_dtm,0.1)) # not the sparsity parameter 

library(wordcloud)
comparison.cloud(as.matrix(JSS_dtm),random.order=F,max.words=100, 
                 scale=c(6,0.6),colours=4,title.size=2)
</code></pre>
","r, text-mining, topic-modeling","<p>Figured it out -- to get the term weights, use <code>posterior(lda_object)$terms</code>. Turned out to be much easier than I thought!</p>
",4,3,1196,2015-11-21 00:37:40,https://stackoverflow.com/questions/33838329/getting-term-weights-out-of-an-lda-model-in-r
classification of data where attribute values are strings,"<p>I have a labeled data set with 7 attributes and about 80,000 rows. However, 3 of these attributes contain more than 50% missing data. I filtered the data to ignore rows with any null values which left me with about 30,000 rows of complete data. The format of the values of each attribute are strings as in ""this is the value of an instance of attribute i."" The desired output (labels) are binary (0 or 1) and there is a label associated with every instance. I want to train a classifier to predict the desired output on a test set. I am using Python and sklearn, and am stuck on how to extract features from this dataset. Any recommendations would be much appreciated. Thanks</p>
","python, scikit-learn, classification, text-mining, feature-extraction","<p>Scikit-learn has several tools explicitly designed to extract features from text inputs; see the <a href=""http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"" rel=""nofollow"">Text Feature Extraction</a> section of the docs.</p>

<p>Here's an example of a classifier built from a list of strings:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

data = [['this is about dogs', 'dogs are really great'],
        ['this is about cats', 'cats are evil']]
labels = ['dogs',
          'cats']

vec = CountVectorizer()  # count word occurrences
X = vec.fit_transform([' '.join(row) for row in data])

clf = MultinomialNB()  # very simple model for word counts
clf.fit(X, labels)

new_data = ['this is about cats too', 'I think cats are awesome']
new_X = vec.transform([' '.join(new_data)])

print(clf.predict(new_X))
# ['cats']
</code></pre>
",1,0,1288,2015-11-23 21:37:39,https://stackoverflow.com/questions/33881183/classification-of-data-where-attribute-values-are-strings
How to scrape all subreddit posts in a given time period,"<p>I have a function to scrape all the posts in the Bitcoin subreddit between 2014-11-01 and 2015-10-31.  </p>

<p>However, I'm only able to extract about 990 posts that go back only to October 25.  I don't understand what's happening.  I included a Sys.sleep of 15 seconds between each extract after referring to <a href=""https://github.com/reddit/reddit/wiki/API"">https://github.com/reddit/reddit/wiki/API</a>, to no avail.</p>

<p>Also, I experimented with scraping from another subreddit (fitness), but it also returned around 900 posts.  </p>

<pre><code>require(jsonlite)
require(dplyr)

getAllPosts &lt;- function() {
    url &lt;- ""https://www.reddit.com/r/bitcoin/search.json?q=timestamp%3A1414800000..1446335999&amp;sort=new&amp;restrict_sr=on&amp;rank=title&amp;syntax=cloudsearch&amp;limit=100""
    extract &lt;- fromJSON(url)
    posts &lt;- extract$data$children$data %&gt;% dplyr::select(name, author,   num_comments, created_utc,
                                             title, selftext)  
    after &lt;- posts[nrow(posts),1]
    url.next &lt;- paste0(""https://www.reddit.com/r/bitcoin/search.json?q=timestamp%3A1414800000..1446335999&amp;sort=new&amp;restrict_sr=on&amp;rank=title&amp;syntax=cloudsearch&amp;after="",after,""&amp;limit=100"")
    extract.next &lt;- fromJSON(url.next)
    posts.next &lt;- extract.next$data$children$data

    # execute while loop as long as there are any rows in the data frame
    while (!is.null(nrow(posts.next))) {
        posts.next &lt;- posts.next %&gt;% dplyr::select(name, author, num_comments, created_utc, 
                                    title, selftext)
        posts &lt;- rbind(posts, posts.next)
        after &lt;- posts[nrow(posts),1]
        url.next &lt;- paste0(""https://www.reddit.com/r/bitcoin/search.json?q=timestamp%3A1414800000..1446335999&amp;sort=new&amp;restrict_sr=on&amp;rank=title&amp;syntax=cloudsearch&amp;after="",after,""&amp;limit=100"")
        Sys.sleep(15)
        extract &lt;- fromJSON(url.next)
        posts.next &lt;- extract$data$children$data
    }
    posts$created_utc &lt;- as.POSIXct(posts$created_utc, origin=""1970-01-01"")
    return(posts)
}

posts &lt;- getAllPosts()
</code></pre>

<p>Does reddit have some kind of limit that I'm hitting?</p>
","r, web-scraping, text-mining, reddit","<p>Yes, all reddit listings (posts, comments, etc.) are capped at 1000 items; they're essentially just cached lists, rather than queries, for performance reasons.</p>

<p>To get around this, you'll need to do some clever searching <a href=""https://www.reddit.com/wiki/search#wiki_cloudsearch_syntax"" rel=""nofollow"">based on timestamps</a>.</p>
",4,6,4943,2015-11-24 19:07:41,https://stackoverflow.com/questions/33901832/how-to-scrape-all-subreddit-posts-in-a-given-time-period
How to chain together multiple qdap transformations for text mining / sentiment (polarity) analysis in R,"<p>I have a <code>data.frame</code> that has week numbers, <code>week</code>, and text reviews, <code>text</code>. I would like to treat the <code>week</code> variable as my grouping variable and run some basic text analysis on it (e.g. <code>qdap::polarity</code>). Some of the review text have multiple sentences; however, I only care about the week's polarity ""on-the-whole"". </p>

<p>How can I chain together multiple text transformations before running <code>qdap::polarity</code> and adhere to its warning messages? I am able to chain together transformations with the <code>tm::tm_map</code> and <code>tm::tm_reduce</code> -- is there something comparable in <code>qdap</code>? What is the proper way to pre-treat/transform this text prior to running <code>qdap::polarity</code> and/or <code>qdap::sentSplit</code>?</p>

<p>More details in the following code / reproducible example:</p>

<pre><code>library(qdap)
library(tm)

df &lt;- data.frame(week = c(1, 1, 1, 2, 2, 3, 4),
                 text = c(""This is some text. It was bad. Not good."",
                          ""Another review that was bad!"",
                          ""Great job, very helpful; more stuff here, but can't quite get it."",
                          ""Short, poor, not good Dr. Jay, but just so-so. And some more text here."",
                          ""Awesome job! This was a great review. Very helpful and thorough."",
                          ""Not so great."",
                          ""The 1st time Mr. Smith helped me was not good.""),
                 stringsAsFactors = FALSE)

docs &lt;- as.Corpus(df$text, df$week)

funs &lt;- list(stripWhitespace,
             tolower,
             replace_ordinal,
             replace_number,
             replace_abbreviation)

# Is there a qdap function that does something similar to the next line?
# Or is there a way to pass this VCorpus / Corpus directly to qdap::polarity?
docs &lt;- tm_map(docs, FUN = tm_reduce, tmFuns = funs)


# At the end of the day, I would like to get this type of output, but adhere to
# the warning message about running sentSplit. How should I pre-treat / cleanse
# these sentences, but keep the ""week"" grouping?
pol &lt;- polarity(df$text, df$week)

## Not run:
# check_text(df$text)
</code></pre>
","r, text-mining, sentiment-analysis, tm, qdap","<p>You could run <code>sentSplit</code> as suggested in the warning as follows:</p>

<pre><code>df_split &lt;- sentSplit(df, ""text"")
with(df_split, polarity(text, week))

##   week total.sentences total.words ave.polarity sd.polarity stan.mean.polarity
## 1    1               5          26       -0.138       0.710             -0.195
## 2    2               6          26        0.342       0.402              0.852
## 3    3               1           3       -0.577          NA                 NA
## 4    4               2          10        0.000       0.000                NaN
</code></pre>

<p>Note that I have a breakout sentiment package <a href=""https://github.com/trinker/sentimentr"" rel=""nofollow""><strong>sentimentr</strong></a> available on github that is an improvment in speed, functionality, and documentation over the <strong>qdap</strong> version.  This does the sentence splitting internally in the <code>sentiment_by</code> function.  The script below allows you to install the package and use it:</p>

<pre><code>if (!require(""pacman"")) install.packages(""pacman"")
p_load_gh(""trinker/sentimentr"")

with(df, sentiment_by(text, week))

##    week word_count        sd ave_sentiment
## 1:    2         25 0.7562542    0.21086408
## 2:    1         26 1.1291541    0.05781106
## 3:    4         10        NA    0.00000000
## 4:    3          3        NA   -0.57735027
</code></pre>
",1,3,286,2015-12-01 14:49:23,https://stackoverflow.com/questions/34023200/how-to-chain-together-multiple-qdap-transformations-for-text-mining-sentiment
path error for tree tagger with koRpus R package,"<p>I try to use treeTagger that I installed from <a href=""http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/#Windows"" rel=""nofollow"">here</a> in R with the package koRpus. </p>

<pre><code>library(koRpus)
tagged.results &lt;- treetag(as.factor(""salut ça va""), treetagger=""manual"", lang=""fr"", TT.options=list(path=""C:\\TreeTagger\\bin\\tree-tagger.exe""))
</code></pre>

<p>generates the following error : </p>

<pre><code>Erreur dans path.expand(path) : argument 'path' incorrect
</code></pre>

<p>Which I don't understand because I can see all the files in this path, which are : tree-tagger and tree-tagger-flush (application files), tag-french and chunk-french which are windows command file. </p>

<p>I also tried :</p>

<pre><code>set.kRp.env(TT.cmd=""C:\\TreeTagger\\bin\\tree-tagger.exe"", lang=""fr"")
tagged.text &lt;- treetag(as.factor(""salut ça va""),lang=""fr"")
</code></pre>

<p>The second generates the same error </p>
","r, windows, path, text-mining, pos-tagger","<p>There are several issues here. First the as.factor(""salut ca va"") should be a file with that text in it. You are also missing a preset value inside of TT.options. You will want to put preset=""fr"" after the path argument. Finally the path itself should point to the root directory.</p>

<p>The documentation <a href=""http://www.inside-r.org/packages/cran/koRpus/docs/treetag"" rel=""nofollow"">here</a> states ""TT.options
A list of options to configure how TreeTagger is called. You have two basic choices: Either you choose one of the pre-defined presets or you give a full set of valid options:
path Mandatory: The absolute path to the TreeTagger root directory. That is where its subfolders bin, cmd and lib are located.""</p>

<p>You are pointing the path variable inside of the bin directory to the .exe file. Run the following code to point to the root directory where the bin directory is located as follows:</p>

<pre><code>library(koRpus)
tagged.results &lt;- treetag(""test.txt"", treetagger=""manual"", lang=""fr"", TT.options=list(path=""C:\\TreeTagger"", preset=""fr""))
</code></pre>
",1,0,1943,2015-12-10 20:53:53,https://stackoverflow.com/questions/34211537/path-error-for-tree-tagger-with-korpus-r-package
How fast is Stanford&#39;s CoreNLP sentiment analysis tool?,"<ol>
<li>I'm trying to find out whether it's feasible for me to use the the CoreNLP sentiment analysis tool (<a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/code.html</a>) on a dataset equivalent in size to about 1 million IMDB reviews.</li>
</ol>

<p>I could not find any absolute metrics anywhere online about average times. I would appreciate if someone could point me to any place about these statistics regarding the speed.</p>

<ol start=""2"">
<li><p>Also, this is what I'm trying - to see if it's possible to estimate a movie rating by using the text alone i.e. by summing up scores for each sentence in a review. Does anything in my idea or in the code snippet below look stupid (should be done better)? I get the feeling that I might be using this tool for something that it's not suited for or I'm doing it the wrong way.</p>

<pre><code>public static double getTextSentimentScore(String text){
Annotation annotation = pipeline.process(text);
double sum = 0;
List&lt;CoreMap&gt; sentences = (List&lt;CoreMap&gt;) annotation.get(CoreAnnotations.SentencesAnnotation.class);
int i = 0;
for (CoreMap sentence : sentences) {
    String sentiment = sentence.get(SentimentCoreAnnotations.SentimentClass.class);
    int sentimentScore = 0;
    if (sentiment.equals(""Very positive""))
        sentimentScore = 5;
    if (sentiment.equals(""Positive""))
        sentimentScore = 4;
    if (sentiment.equals(""Neutral""))
        sentimentScore = 3;
    if (sentiment.equals(""Negative""))
        sentimentScore = 2;
    if (sentiment.equals(""Very negative""))
        sentimentScore = 1;
    sum += sentimentScore;
    System.out.println(sentiment + ""\t"" + sentimentScore);
}
return (sum/sentences.size());
</code></pre>

<p>}</p></li>
</ol>
","stanford-nlp, text-mining, sentiment-analysis","<p>If you run this command:</p>

<pre><code>java -Xmx5g -cp ""stanford-corenlp-full-2015-12-09/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,parse,sentiment -filelist list-of-sample-docs.txt
</code></pre>

<p>the final output will give you timing information</p>

<p>So all you have to do is:</p>

<ol>
<li><p>take 100 IMDB reviews, put them in files named imdb_review_1, imdb_review_2, etc...</p></li>
<li><p>put each filename one file per line in list-of-sample-docs.txts</p></li>
<li><p>run that command and the final output will show total time for each annotator and total time elapsed</p></li>
</ol>
",2,0,414,2015-12-12 07:08:59,https://stackoverflow.com/questions/34237295/how-fast-is-stanfords-corenlp-sentiment-analysis-tool
How to remove a column of words in a document term matrix?,"<p>I trained my machine learning model using training dataset via a document term matrix.
I am trying to predict my test dataset but unfortunately it contains words that the training dataset does not come with.</p>

<p>My question is how do I actually remove those words in my test dataset that is not found in the training dataset.</p>

<p>I am using tm package and I created a DocumentTermMatrix.</p>
","text-mining, tm","<p>An easy way to do this is with the <strong>quanteda</strong> text analysis package.  Once you construct a document-feature matrix, you can select its features from a second ""dfm"".  This allows you to construct a dfm for a training set, and then easily select those features from a test set that are common with those in the training set.</p>

<p>Here is an illustration, from the <code>?selectFeatures</code> help page:</p>

<pre><code>require(quanteda)
textVec1 &lt;- c(""This is text one."", ""This, the second text."", ""Here: the third text."")
textVec2 &lt;- c(""Here are new words."", ""New words in this text."")
features(dfm1 &lt;- dfm(textVec1))
#
#   ... lowercasing
#   ... tokenizing
#   ... indexing documents: 3 documents
#   ... indexing features: 8 feature types
#   ... created a 3 x 8 sparse dfm
#   ... complete. 
# Elapsed time: 0.077 seconds.
# [1] ""this""   ""is""     ""text""   ""one""    ""the""    ""second"" ""here""   ""third"" 

features(dfm2a &lt;- dfm(textVec2))
#
#   ... lowercasing
#   ... tokenizing
#   ... indexing documents: 2 documents
#   ... indexing features: 7 feature types
#   ... created a 2 x 7 sparse dfm
#   ... complete. 
# Elapsed time: 0.006 seconds.
# [1] ""here""  ""are""   ""new""   ""words"" ""in""    ""this""  ""text"" 

(dfm2b &lt;- selectFeatures(dfm2a, dfm1))
# found 3 features from 8 supplied types in a dfm, padding 0s for another 5 
# Document-feature matrix of: 2 documents, 8 features.
# 2 x 8 sparse Matrix of class ""dfmSparse""
#       this is text one the second here third
# text1    0  0    0   0   0      0    1     0
# text2    1  0    1   0   0      0    0     0
identical(features(dfm1), features(dfm2b))
# [1] TRUE
</code></pre>
",0,0,1203,2015-12-13 02:59:58,https://stackoverflow.com/questions/34247537/how-to-remove-a-column-of-words-in-a-document-term-matrix
Skip or delete lines between two patterns,"<p>I have text-based data that I can read using <code>readLines</code>. I would like to use R to eliminate the lines that started with Hello World to !Hello World.</p>

<pre><code>abc
adb
exy
Hello World
123
abc
adb
aex
!Hello World
bfd
exy
uyt
</code></pre>
","r, text-mining","<p>You can use standard vector indexing to grab the values that do not fall between the first incidence of <code>""Hello World""</code> and the last incidence of <code>""!Hello World""</code>:</p>

<pre><code>d[!seq_along(d) %in% min(which(d == ""Hello World"")):max(which(d == ""!Hello World""))]
# [1] ""abc"" ""adb"" ""exp"" ""bfd"" ""exy"" ""uyt""
</code></pre>

<p>Data:</p>

<pre><code>d = c(""abc"", ""adb"", ""exp"", ""Hello World"", ""123"", ""abc"", ""adb"", ""aex"", ""!Hello World"", ""bfd"", ""exy"", ""uyt"")
</code></pre>
",4,2,64,2015-12-15 00:17:00,https://stackoverflow.com/questions/34278952/skip-or-delete-lines-between-two-patterns
"How can I find compound words, removing spaces between them and replace them in my corpus?","<p>I have many compound terms such as hello World, good Morning, good Night,... which I want to find them in my corpus and then replace them with their equivalent as helloWorld, goodMorning, goodNight. So in this way I can preserve their concept.
I can do it one by one, however its very tedious as there are many compound terms. I need to do this in R language. </p>
","r, text-mining, corpus","<p>If all your compound terms are separated only by blanks, you can use <code>gsub</code>:</p>

<pre><code>&gt; x = c(""hello World"", ""good Morning"", ""good Night"")
&gt; y = gsub(pattern = "" "", replacement = """", x = x)
&gt; print(y)
[1] ""helloWorld""  ""goodMorning"" ""goodNight""  
</code></pre>

<p>You can always add more patterns to <code>pattern</code> argument. Read more about regular expression in R <a href=""https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html"" rel=""nofollow"">here</a> and <a href=""http://www.regular-expressions.info/rlanguage.html"" rel=""nofollow"">here</a>.</p>

<p><strong>Edit</strong></p>

<blockquote>
  <p>@user4241750: True, but I only want to do this for particular compound
  terms(There are many) not all the terms in the corpus since there are
  many other terms in the corpus</p>
</blockquote>

<p>If you know all particular compound terms you want to change, you can specify it on <code>docs[[j]]</code>. Say the only terms you want to change are ""simple parts"" and ""good morning"":</p>

<pre><code>terms.to.change = c(""simple parts"",""good morning"")
for (j in seq(corpus)) {
  positions.to.change = which(docs[[j]] %in% terms.to.change)
  docs[[j]][positions.to.change] &lt;- gsub("" "", """", docs[[j]][positions.to.change])
}
</code></pre>
",0,-5,553,2015-12-17 07:31:02,https://stackoverflow.com/questions/34328960/how-can-i-find-compound-words-removing-spaces-between-them-and-replace-them-in
What&#39;s the difference between indicative summarization and informative summarization?,"<p>I have trouble in distinguishing between indicative summarization and informative summarization. Can you give me a clear example to show the difference between them?</p>

<p>Thanks in advance!</p>
","nlp, text-mining, text-processing, summarization","<p>Following <a href=""http://research.microsoft.com/en-us/people/cyl/tipster-proc-hovy-lin-final.pdf"" rel=""nofollow"">hovy&amp;lin1998</a>:</p>

<blockquote>
  <p>An indicative
  summary provides merely an indication of the
  principal subject matter or domain of the input text(s)
  without including its contents. After reading an
  informative summary, one can explain what the input
  text was about, but not necessarily what was
  contained in it. An informative summary reflects
  (some of) the content, and allows one to describe
  (parts of) what was in the input text. </p>
</blockquote>

<p><strong>Informative summary:</strong> It contains the informative part of the original text. After reading it, you can tell what are the main ideas in the original document. you can find informative summaries in reasearch articles where the author tries to present the essentiel of its reasearch.</p>

<p><strong>Indicative summary:</strong> It doesn't contain informative content; it contains only metadata (a description of the document). It is used to inform the reader about the scope of the document and such to help help them decide whether or not to consult the original document. You can find such type of summaries, for example, on the verso of the title page of books and reports.</p>

<p>Following <a href=""http://www.newworldencyclopedia.org/entry/Abstract_(summary)"" rel=""nofollow"">newworldencyclopedia</a>:</p>

<p><strong>Informative content:</strong> are material contents of the document, which includes conclusions, suggestions, and recommendations. </p>

<p><strong>Metadata:</strong> is a description of what kind of information it is, which includes the purpose, scope, and research methodology.</p>
",4,5,3152,2015-12-17 09:24:46,https://stackoverflow.com/questions/34330922/whats-the-difference-between-indicative-summarization-and-informative-summariza
Error using &quot;TermDocumentMatrix&quot; and &quot;Dist&quot; functions in R,"<p>I have been trying to replicate the example <a href=""http://www.rexamine.com/2014/06/text-mining-in-r-automatic-categorization-of-wikipedia-articles/"" rel=""nofollow"">here</a>: but I have had some problems along the way.</p>

<p>Everything worked fine until here:</p>

<pre><code>docsTDM &lt;- TermDocumentMatrix(docs8)
</code></pre>

<blockquote>
  <p>Error in UseMethod(""meta"", x) : 
            no applicable method for 'meta' applied to an object of class ""character""<br>
            In addition: Warning message:<br>
            In mclapply(unname(content(x)), termFreq, control) :<br>
              all scheduled cores encountered errors in user code</p>
</blockquote>

<p>So I was able to fix that error modifying this previous step by changing this:</p>

<pre><code>docs8 &lt;- tm_map(docs7, tolower)
</code></pre>

<p>To this:</p>

<pre><code>docs8 &lt;- tm_map(docs7, content_transformer(tolower))
</code></pre>

<p>But then I got in trouble again with:</p>

<pre><code>docsdissim &lt;- dissimilarity(docsTDM, method = ""cosine"")
</code></pre>

<blockquote>
  <p>Error: could not find function ""dissimilarity""</p>
</blockquote>

<p>Then I learned that the ""dissimilarity"" function was replaced by the <code>dist</code> function, so I did:</p>

<pre><code>docsdissim &lt;- dist(docsTDM, method = ""cosine"")
</code></pre>

<blockquote>
  <p>Error in crossprod(x, y)/sqrt(crossprod(x) * crossprod(y)) : 
            non-conformable arrays</p>
</blockquote>

<p>And there is where I'm stuck.</p>

<p>By the way, my R version is : </p>

<blockquote>
  <p>R version 3.2.2 (2015-08-14) running on CentOS 7</p>
</blockquote>
","r, text-mining, text-classification, text-analysis","<p>change </p>

<pre><code>docsdissim &lt;- proxy::dist(docsTDM, method = ""cosine"")
</code></pre>

<p>to </p>

<pre><code>docsdissim &lt;- dist(as.matrix(docsTDM), method = ""cosine"")
</code></pre>

<p><code>dist</code> requires as input a numeric matrix, data frame or ""dist"" object and event though a termdocumentmatrix is a matrix, it needs to be transformed here. </p>
",3,3,1629,2015-12-19 15:08:55,https://stackoverflow.com/questions/34372166/error-using-termdocumentmatrix-and-dist-functions-in-r
How to filter meta data by user-defined statements in R?,"<p>There is a function called <code>sFilter</code> in R to filter meta data. However, <a href=""http://www.inside-r.org/packages/cran/tm/docs/sFilter"" rel=""nofollow"">the function</a> is an old (Version: 0.5-10) tm package. Is there any function instead of it in a new version?</p>

<p>My code block is;</p>

<pre><code>query &lt;- ""LEWISSPLIT == 'TRAIN'""
trainData &lt;- tm_filter(Corpus, FUN = sFilter, query)
</code></pre>

<p>It means, get documents which have ""TRAIN"" value in their LEWISSPLIT attribute. </p>

<pre><code>&lt;REUTERS TOPICS=?? LEWISSPLIT=?? CGISPLIT=?? OLDID=?? NEWID=??&gt;
</code></pre>
","r, filter, metadata, text-mining, tm","<p>Just write your own filtering function:</p>

<pre><code>trainData &lt;- tm_filter(Corpus, FUN = function(x, qry) any(meta(x)[""lewissplit""] == qry), ""TRAIN"")
</code></pre>

<p>This was adapted from <code>example(tm_filter)</code>. There is an example using <code>grep()</code> for more flexible search.</p>
",2,1,639,2015-12-21 15:26:06,https://stackoverflow.com/questions/34399093/how-to-filter-meta-data-by-user-defined-statements-in-r
Dataset for training text classifier,"<p>I'm new to data mining and I am trying to build a classifier that is able to classify student theses abstracts into a predefined set of categories under the area of Computer Science, e.g. Machine learning, Image Processing...etc.
I do not have enough classified abstracts to be used as a training dataset so would you please direct me to a dataset that can be used for this particular purpose. </p>
","text-mining, text-classification","<p>You can use the DBLP data (downloadable from <a href=""http://dblp.uni-trier.de/xml/"" rel=""nofollow"">http://dblp.uni-trier.de/xml/</a>) to generate a list of publications. Based on conferences/journal you can generate your classes e.g. MLJR is alway Machine Learning.</p>

<p>The abstracts you can acquire using:
<a href=""https://github.com/arc12/Text-Mining-Weak-Signals/blob/master/Abstract%20Acquisition%20Scripts/DBLP%20XML%20fetch%20abstracts%20.pl"" rel=""nofollow"">https://github.com/arc12/Text-Mining-Weak-Signals/blob/master/Abstract%20Acquisition%20Scripts/DBLP%20XML%20fetch%20abstracts%20.pl</a></p>
",0,-3,82,2015-12-24 14:11:36,https://stackoverflow.com/questions/34454065/dataset-for-training-text-classifier
Text Mining in a string using R,"<p>I recently started using R and a newbie for data analysis.</p>

<p>Is it possible in R to find the <strong>number of repetitions in a single main string</strong> of data when a string of data is used for searching through it?</p>

<p>Example:<br>
Main string:  '<strong>abcdefghikllabcdefgllabcd'</strong><br>
and search string: '<strong>lla</strong>' </p>

<p>Desired output: '<strong>abcdefghik lla bcdefg lla bcd</strong>'</p>

<p>[I tried using <strong>grep()</strong> function of R, but It is not working in the desired way and only gives the number of repetitions of search string in multiple main strings.]</p>

<p>Thank you in advance.</p>
","r, text-mining","<p>This works too using regex capture groups:</p>

<pre><code>gsub(""(lla)"","" \\1 "",""abcdefghikllabcdefgllabcd"")
</code></pre>
",2,2,402,2016-01-01 05:58:36,https://stackoverflow.com/questions/34553913/text-mining-in-a-string-using-r
How to scrape text from websites using Python,"<p>I wrote a code in python using 'requests' and 'beautifulSoup' api to scrape text data from first 100 sites return by google.
Well it works good on most of sites but it is giving errors on those which are responding later or not responding at all 
I am getting this error</p>

<p><em>raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='www.lfpress.com', port=80): Max retries exceeded with url: /2015/11/06/fair-with-a-flare-samosas-made-easy (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))</em></p>

<p>Am I supposed to change code written inside requests API? Or I need to use some proxy? How can I leave that site and move on to next one? As error is stopping my execution. </p>
","python, web-scraping, beautifulsoup, python-requests, text-mining","<p>Add a ""try except"" block around your call to catch that exception and continue if you don't care about the error like:</p>

<pre><code>import requests
try:
    requests.get('http://stackoverflow.com/')
except requests.packages.urllib3.exceptions.MaxRetryError as e:
    print repr(e)
</code></pre>
",2,2,1653,2016-01-02 22:08:59,https://stackoverflow.com/questions/34571229/how-to-scrape-text-from-websites-using-python
Java - Implementing Machine Learning methods on text mining,"<p>I have some texts and i would like to mine these by implementing <strong>Machine Learning</strong> methods in Java using Weka libraries. For that purpose, i've already did something so far but since whole code is too long i just want to show some key methods and get an idea about how to train and test my dataset, and interpret results etc.</p>

<p>FYI, i am processing tweets with Twitter4J.</p>

<p>First, i fetched the tweets and saved in text file(of course in ARFF format). Then I manually labeled them regarding their sentiments(positive,neutral,negative). Based on selected classifier, i created test set from my training set thanks to <strong>cross-validation</strong>. Finally i classified them and print the summary and confusion matrix.  </p>

<p>Here is one of my classifiers:  Naive Bayes code:</p>

<pre><code>public static void ApplyNaiveBayes(Instances data) throws Exception {

    System.out.println(""Applying Naive Bayes \n"");
    data.setClassIndex(data.numAttributes() - 1); 
    StringToWordVector swv = new StringToWordVector();
    swv.setInputFormat(data);
    Instances dataFiltered = Filter.useFilter(data, swv);
    //System.out.println(""Filtered data "" +dataFiltered.toString());

    System.out.println(""\n\nFiltered data:\n\n"" + dataFiltered);

    Instances[][] split = crossValidationSplit(dataFiltered, 10);
    Instances[] trainingSets = split[0];
    Instances[] testingSets = split[1];


    NaiveBayes classifier = new NaiveBayes(); 

    FastVector predictions = new FastVector();


    classifier.buildClassifier(dataFiltered);
    System.out.println(""\n\nClassifier model:\n\n"" + classifier);     

    // Test the model
    for (int i = 0; i &lt; trainingSets.length; i++) {
        classifier.buildClassifier(trainingSets[i]);
        // Test the model         
        Evaluation eTest = new Evaluation(trainingSets[i]);
        eTest.evaluateModel(classifier, testingSets[i]);

        // Print the result to the Weka explorer:
        String strSummary = eTest.toSummaryString();
        System.out.println(strSummary);

        // Get the confusion matrix
        double[][] cmMatrix = eTest.confusionMatrix();
        for(int row_i=0; row_i&lt;cmMatrix.length; row_i++){
            for(int col_i=0; col_i&lt;cmMatrix.length; col_i++){
                System.out.print(cmMatrix[row_i][col_i]);
                System.out.print(""|"");
            }
            System.out.println();
        }
    }
}
</code></pre>

<p>And FYI, crossValidationSplit method is here:</p>

<pre><code>    public static Instances[][] crossValidationSplit(Instances data, int     
    numberOfFolds) {
        Instances[][] split = new Instances[2][numberOfFolds];

        for (int i = 0; i &lt; numberOfFolds; i++) {
            split[0][i] = data.trainCV(numberOfFolds, i);
            split[1][i] = data.testCV(numberOfFolds, i);
        }

        return split;
    }
</code></pre>

<p>In the end, I've got 10 different results(because k=10). One of them is:</p>

<pre><code>  Correctly Classified Instances           4               36.3636 %
  Incorrectly Classified Instances         7               63.6364 %
  Kappa statistic                          0.0723
  Mean absolute error                      0.427 
  Root mean squared error                  0.5922
  Relative absolute error                 93.4946 %
  Root relative squared error            116.5458 %
  Total Number of Instances               11     

  2.0|0.0|1.0|
  1.0|1.0|2.0|
  3.0|0.0|1.0|
</code></pre>

<p>So, how i can i interpret the results? Do you think i'm doing right about training and test sets? 
I want to obtain given text file's sentiment percent (positive,neutral,negative). How to infer my demand from these results?
Thanks for reading...  </p>
","java, machine-learning, weka, text-mining, sentiment-analysis","<p>Unfortunately your code is a bit confused. </p>

<p>First of all, you train your model on the full set of your set:</p>

<pre><code>classifier.buildClassifier(dataFiltered);
</code></pre>

<p>then you retrain your model inside your for loop:</p>

<pre><code>for (int i = 0; i &lt; trainingSets.length; i++) {
    classifier.buildClassifier(trainingSets[i]);
    ...
 }
</code></pre>

<p>than you calculate the confusion mtx too. I think it is unnecessary. </p>

<p>In my opinion you need to apply <code>Evaluation.crossValidateModel()</code> method as the follows:
    <code>
    //set the class index
    dataFiltered.setClassIndex(dataFiltered.numAttributes() - 1);
    //build a model -- choose a classifier as you want
    classifier.buildClassifier(dataFiltered);
    Evaluation eval = new Evaluation(dataFiltered);
    eval.crossValidateModel(classifier, dataFiltered, 10, new Random(1));
    //print stats -- do not require to calculate confusion mtx, weka do it!
    System.out.println(classifier);
    System.out.println(eval.toSummaryString());
    System.out.println(eval.toMatrixString());
    System.out.println(eval.toClassDetailsString());
</code></p>
",3,4,1118,2016-01-03 11:26:23,https://stackoverflow.com/questions/34575976/java-implementing-machine-learning-methods-on-text-mining
Is there a more efficient way to append lines from a large file to a numpy array? - MemoryError,"<p>I'm trying to use this <a href=""https://pypi.python.org/pypi/lda"" rel=""nofollow"" title=""lda"">lda</a> package to process a term-document matrix csv file with 39568 rows and 27519 columns containing counting/natural numbers only.</p>

<p>Problem: I'm getting a MemoryError with my approach to read the file and store it to a numpy array.</p>

<p>Goal: Get the numbers from the TDM csv file and convert it to numpy array so I can use the numpy array as input for the lda.</p>

<pre><code>with open(""Results/TDM - Matrix Only.csv"", 'r') as matrix_file:
    matrix = np.array([[int(value) for value in line.strip().split(',')] for line in matrix_file])
</code></pre>

<p>I've also tried using the numpy append, vstack and concatenate and I still get the MemoryError.</p>

<p>Is there a way to avoid the MemoryError?</p>

<p><strong>Edit:</strong></p>

<p>I've tried using dtype <em>int32</em> and <em>int</em> and it gives me:</p>

<blockquote>
  <p>WindowsError: [Error 8] Not enough storage is available to process this command</p>
</blockquote>

<p>I've also tried using dtype <em>float64</em> and it gives me:</p>

<blockquote>
  <p>OverflowError: cannot fit 'long' into an index-sized integer</p>
</blockquote>

<p>With these codes:</p>

<pre><code>fp = np.memmap(""Results/TDM-memmap.txt"", dtype='float64', mode='w+', shape=(len(documents), len(vocabulary)))
matrix = np.genfromtxt(""Results/TDM.csv"", dtype='float64', delimiter=',', skip_header=1)
fp[:] = matrix[:]
</code></pre>

<p>and</p>

<pre><code>with open(""Results/TDM.csv"", 'r') as tdm_file:
    vocabulary = [value for value in tdm_file.readline().strip().split(',')]
    fp = np.memmap(""Results/TDM-memmap.txt"", dtype='float64', mode='w+', shape=(len(documents), len(vocabulary)))
    for idx, line in enumerate(tdm_file):
        fp[idx] = np.array(line.strip().split(','))
</code></pre>

<p>Other info that might matter</p>

<ul>
<li>Win10 64bit</li>
<li>8GB RAM (7.9 usable) | peaks at 5.5GB from more or less 3GB (around 2GB used) before it reports MemoryError</li>
<li>Python 2.7.10 [MSC v.1500 32 bit (Intel)]</li>
<li>Using PyCharm Community Edition 5.0.3</li>
</ul>
","python, csv, numpy, out-of-memory, text-mining","<p>Since your word counts will be almost all zeros, it would be much more efficient to store them in a <code>scipy.sparse</code> matrix. For example:</p>

<pre><code>from scipy import sparse
import textmining
import lda

# a small example matrix
tdm = textmining.TermDocumentMatrix()
tdm.add_doc(""here's a bunch of words in a sentence"")
tdm.add_doc(""here's some more words"")
tdm.add_doc(""and another sentence"")
tdm.add_doc(""have some more words"")

# tdm.sparse is a list of dicts, where each dict contains {word:count} for a single
# document
ndocs = len(tdm.sparse)
nwords = len(tdm.doc_count)
words = tdm.doc_count.keys()

# initialize output sparse matrix
X = sparse.lil_matrix((ndocs, nwords),dtype=int)

# iterate over documents, fill in rows of X
for ii, doc in enumerate(tdm.sparse):
    for word, count in doc.iteritems():
        jj = words.index(word)
        X[ii, jj] = count
</code></pre>

<p><code>X</code> is now an (ndocs, nwords) <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html#scipy.sparse.lil_matrix"" rel=""nofollow""><code>scipy.sparse.lil_matrix</code></a>, and <code>words</code> is a list corresponding to the columns of <code>X</code>:</p>

<pre><code>print(words)
# ['a', 'and', 'another', 'sentence', 'have', 'of', 'some', 'here', 's', 'words', 'in', 'more', 'bunch']

print(X.todense())
# [[2 0 0 1 0 1 0 1 1 1 1 0 1]
#  [0 0 0 0 0 0 1 1 1 1 0 1 0]
#  [0 1 1 1 0 0 0 0 0 0 0 0 0]
#  [0 0 0 0 1 0 1 0 0 1 0 1 0]]
</code></pre>

<p>You could pass <code>X</code> directly to <a href=""http://pythonhosted.org/lda/api.html#lda.lda.LDA.fit"" rel=""nofollow""><code>lda.LDA.fit</code></a>, although it will probably be faster to convert it to a <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix"" rel=""nofollow""><code>scipy.sparse.csr_matrix</code></a> first:</p>

<pre><code>X = X.tocsr()
model = lda.LDA(n_topics=2, random_state=0, n_iter=100)
model.fit(X)
# INFO:lda:n_documents: 4
# INFO:lda:vocab_size: 13
# INFO:lda:n_words: 21
# INFO:lda:n_topics: 2
# INFO:lda:n_iter: 100
# INFO:lda:&lt;0&gt; log likelihood: -126
# INFO:lda:&lt;10&gt; log likelihood: -102
# INFO:lda:&lt;20&gt; log likelihood: -99
# INFO:lda:&lt;30&gt; log likelihood: -97
# INFO:lda:&lt;40&gt; log likelihood: -100
# INFO:lda:&lt;50&gt; log likelihood: -100
# INFO:lda:&lt;60&gt; log likelihood: -104
# INFO:lda:&lt;70&gt; log likelihood: -108
# INFO:lda:&lt;80&gt; log likelihood: -98
# INFO:lda:&lt;90&gt; log likelihood: -98
# INFO:lda:&lt;99&gt; log likelihood: -99
</code></pre>
",1,3,978,2016-01-03 20:04:26,https://stackoverflow.com/questions/34581021/is-there-a-more-efficient-way-to-append-lines-from-a-large-file-to-a-numpy-array
Unwanted removal of some stopwords in scikitlearn,"<p>I want to retain single character in my vector. In scikit-learn <code>CountVectorizer</code> even, i keep <code>stop_word</code> parameter as <code>None</code> internal implementation is removing some characters from the newly created vectors. How can it be handled?</p>
","python, scikit-learn, text-mining, text-analysis","<p>That is because the <code>token_pattern</code> parameter defaults to <code>'(?u)\\b\\w\\w+\\b'</code>, which filters all words (provided the parameter <code>analyzer</code> is set to <code>'word'</code>, which is the default) that only consist of a single character (e.g. 'a' or 'i'). If you set <code>token_pattern</code> to a different regex, e.g. <code>'(?u)\\b\\w+\\b'</code> single character words should be retained.</p>

<p>Example:</p>

<pre><code>In [71]: from sklearn.feature_extraction.text import CountVectorizer
In [72]: corpus = ['I like my coffee with a shot of rum.']

In [73]: vec = CountVectorizer()
In [74]: vec.fit(corpus)
In [75]: vec.vocabulary_

Out[75]: {'coffee': 0, 'like': 1, 'my': 2, 'of': 3, 'rum': 4, 'shot': 5, 'with': 6}

In [76]: vec = CountVectorizer(token_pattern='(?u)\\b\\w+\\b')
In [77]: vec.fit(corpus) 
In [78]: vec.vocabulary_
Out[78]: {'a': 0, 'coffee': 1, 'i': 2, 'like': 3, 'my': 4, 'of': 5, 'rum': 6, 'shot': 7, 'with': 8}
</code></pre>
",1,-1,441,2016-01-08 11:44:34,https://stackoverflow.com/questions/34676340/unwanted-removal-of-some-stopwords-in-scikitlearn
How do I check whether a given string is a valid geographical location or not?,"<p>I have a list of strings (noun phrases) and I want to filter out all valid geographical locations from them. Most of these (unwanted location names) are country or city or state names. What would be a way to do this? Is there any open-source lookup table available which contains all country, states, cities of the world?</p>

<p>Example desired output:
<em>TREC4</em>: false,   <em>Vienna</em>: <strong>true</strong>,   <em>Ministry</em>: false,   <em>IBM</em>: false,   <em>Montreal</em>: <strong>true</strong>,   <em>Singapore</em>: <strong>true</strong></p>

<p>Unlike this post: <a href=""https://stackoverflow.com/questions/12042452/verify-user-input-location-string-is-a-valid-geographic-location"">Verify user input location string is a valid geographic location?</a>
I have a high number of strings like these (~0.7 million) so <em>google geolocation API</em> is probably not an option for me.</p>
","geolocation, nlp, gis, text-mining, data-science","<p>You can use geoplanet data by Yahoo, or geonames data by geonames.org.
Here is a link to geoplanet TSV file containing 5 million geographical places of the world :
<a href=""https://developer.yahoo.com/geo/geoplanet/data/"" rel=""nofollow"">https://developer.yahoo.com/geo/geoplanet/data/</a></p>

<p>Moreover, geoplanet data will provide you type ( city,country,suburb etc) of the geographical place, along with a unique id.
<a href=""https://developer.yahoo.com/geo/geoplanet/guide/concepts.html"" rel=""nofollow"">https://developer.yahoo.com/geo/geoplanet/guide/concepts.html</a></p>

<p>You can do a lowercase, sanitized ( e.g. remove special characters and other anomalies) match of your needle string to the names present in this data.
If you do not want full file scans, first processing this data to store it in a fast lookup database like mongodb or redis will be beneficial.</p>
",3,3,3053,2016-01-08 17:37:23,https://stackoverflow.com/questions/34682869/how-do-i-check-whether-a-given-string-is-a-valid-geographical-location-or-not
How to correctly Load .txt files into Vcorpus in R?,"<p>All.
I want to analyze the content of several .txt files in R. I'm having trouble when importing them.
Here is my code (there are 238 .txt files in the data/txt/2012/ directory):</p>

<pre><code>library(tm)   
cname &lt;- file.path(""../data"", ""txt"", ""2012"")
docs &lt;- Corpus(DirSource(cname), readerControl=list(reader=readPlain))
</code></pre>

<p>Now, if I take a look in docs its a Vcorpus with 238 documents as expected:</p>

<pre><code>&gt; docs
    &lt;&lt;VCorpus&gt;&gt;
    Metadata:  corpus specific: 0, document level (indexed): 0
    Content:  documents: 238
</code></pre>

<p>Here is where I get trouble understanding what is happening:</p>

<pre><code>&gt; docs[1]
    &lt;&lt;VCorpus&gt;&gt;
    Metadata:  corpus specific: 0, document level (indexed): 0
    Content:  documents: 1

&gt; docs[[1]]
   &lt;&lt;PlainTextDocument&gt;&gt;
   Metadata:  7
   Content:  chars: 2156
</code></pre>

<p>The way I see it, there are two levels of Vcorpus, The first contains all 238 documents, the second has one document each. I want to have only one Vcorpus that has 238 documents and then the PlainTextDocument, expected output would be (notice I use only [1] and not [[1]] to get the PlainTextDocument):</p>

<pre><code>&gt; docs[1]
   &lt;&lt;PlainTextDocument&gt;&gt;
   Metadata:  7
   Content:  chars: 2156
</code></pre>

<p>Is there a way I can load the .txt files into a Vcorpus with the desired format?
Or I should work with the way that is being loaded now?</p>

<p>Thanks a lot.
Cheers.</p>
","r, text-mining, tm, corpus","<p>It seems to me that you have correctly loaded the corpus.</p>

<p>The introduction document to the tm package says that you can use,say, <code>writeLines(as.character(docs[[4]])</code>) to get a textual representation of document 4.</p>

<p>You can also use <code>content(docs[[4]])</code>.</p>
",1,1,659,2016-01-13 14:43:30,https://stackoverflow.com/questions/34769538/how-to-correctly-load-txt-files-into-vcorpus-in-r
Convert text dataset to .arff file,"<p>I have this data set <a href=""https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences"" rel=""nofollow"">https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences</a></p>

<p>and I need to convert it from .txt to .arff file to make classification with weka program</p>
","converters, weka, text-mining, text-classification, arff","<p>Use a programming language.</p>

<p>It's not hard to write a program that</p>

<ol>
<li>reads the input format line by line</li>
<li>outputs the arff header</li>
<li>outputs the data in arff sparse vector format</li>
</ol>
",1,-2,1050,2016-01-13 20:22:30,https://stackoverflow.com/questions/34776325/convert-text-dataset-to-arff-file
Word Frequency Feature Normalization,"<p>I am extracting the features for a document. One of the features is <code>the frequency of the word in the document</code>. The problem is that the number of sentences in the training set and test set is not necessarily the same. So, I need to normalized it in some way. One possibility (that came to my mind) was to divide the frequency of the word by the number of sentences in the document. By my supervisor told me that it's better to normalize it in a logarithmic way. I have no idea what does that mean. Can anyone help me?</p>

<p>Thanks in advance,</p>

<p>PS: I also saw <a href=""https://stackoverflow.com/questions/12155550/how-to-normalize-word-frequencies-of-document-in-weka"">this</a> topic, but it didn't help me.</p>
","machine-learning, normalization, text-mining, feature-extraction, word-frequency","<p>The first question to ask is, what algorithm you are using subsequently? For many algorithms it is sufficient to normalize the bag of words vector, such that it sums up to one or that some other norm is one.</p>

<p>Instead of normalizing by the number of sentence you should, however, normalize by the total number of words in the document. Your test corpus might have longer sentences, for example. </p>

<p>I assume the recommendation of your supervisor means that you do not report the counts of the words but the logarithm of the counts. In addition I would suggest to look into the TF/IDF measure in general. this is imho more common in Textmining</p>
",2,2,5616,2016-01-15 09:08:34,https://stackoverflow.com/questions/34807621/word-frequency-feature-normalization
R: extract and paste keyword matches,"<p>I am new to R and have been struggling with this one. I want to create a new column, that checks if a set of any of words (""foo"", ""x"", ""y"") exist in column 'text', then write that value in new column.</p>

<p>I have a data frame that looks like this: a-></p>

<pre><code> id     text        time   username
 1     ""hello x""     10     ""me""
 2     ""foo and y""   5      ""you""
 3     ""nothing""     15     ""everyone""
 4     ""x,y,foo""     0      ""know""
</code></pre>

<p>The correct output should be:</p>

<p>a2 -></p>

<pre><code>id     text        time   username        keywordtag  
 1     ""hello x""     10     ""me""          x
 2     ""foo and y""   5      ""you""         foo,y
 3     ""nothing""     15     ""everyone""    0 
 4     ""x,y,foo""     0      ""know""        x,y,foo
</code></pre>

<p>I have this:</p>

<pre><code>df1 &lt;- data.frame(text = c(""hello x"", ""foo and y"", ""nothing"", ""x,y,foo""))
terms &lt;- c('foo', 'x', 'y')
df1$keywordtag &lt;- apply(sapply(terms, grepl, df1$text), 1, function(x) paste(terms[x], collapse=','))
</code></pre>

<p>Which works, but crashes R when my needleList contains 12k words and my text has 155k rows. Is there a way to do this that won't crash R?</p>
","r, apply, text-mining, grepl","<p>This is a variation on what you have done, and what was suggested in the comments.  This uses <code>dplyr</code> and <code>stringr</code>.  There may be a more efficient way but this may not crash your R session.</p>

<pre><code>library(dplyr)
library(stringr)

terms      &lt;- c('foo', 'x', 'y')
term_regex &lt;- paste0('(', paste(terms, collapse = '|'), ')')

### Solution: this uses dplyr::mutate and stringr::str_extract_all
df1 %&gt;%
    mutate(keywordtag = sapply(str_extract_all(text, term_regex), function(x) paste(x, collapse=',')))
#       text keywordtag
#1   hello x          x
#2 foo and y      foo,y
#3   nothing           
#4   x,y,foo    x,y,foo
</code></pre>
",2,1,1189,2016-01-22 17:10:18,https://stackoverflow.com/questions/34952321/r-extract-and-paste-keyword-matches
How do you extract keywords based on surrounding characters,"<p>Trying to extract keywords from a string in R but so far unable to find a solution</p>

<p>Giving the string</p>

<pre><code>""erj\""abc\""ejwojeowje \""123\""fjoejfoejf"" 
</code></pre>

<p>How to extract the keywords <code>""abc""</code> and <code>""123""</code> into a character vector?
I tried using <code>strsplit</code> based on <code>\""</code> but it would also return other unnecessary keywords.</p>
","r, text-mining","<p>We can try <code>sub</code></p>

<pre><code>scan(text=sub('[^""]+""([^""]+)""[^""]+""(\\d+).*',
                           ""\\1 \\2"", str1), what='')
#[1] ""abc"" ""123""
</code></pre>

<h3>data</h3>

<pre><code>str1 &lt;- ""erj\""abc\""ejwojeowje \""123\""fjoejfoejf"" 
</code></pre>
",1,1,75,2016-01-27 05:05:59,https://stackoverflow.com/questions/35029279/how-do-you-extract-keywords-based-on-surrounding-characters
How to predict correct country name for user provided country name?,"<p>I am planning to do some data tuning on my data.</p>

<p><strong>Situation</strong>-I have a data which has a field <code>country</code>. It contains user input country names( It might contain spelling mistakes or different country names for same country like US/U.S.A/United States for USA). I have a list of correct country names.</p>

<p><strong>What I want</strong>- To predict which closest country it is referring to. For example- If <code>U.S.</code> is given then it will change to <code>USA</code>(correct country name in our list).</p>

<p>Is there any way I can do it using Java or opennlp or any other method?</p>
","java, nlp, string-matching, text-mining, opennlp","<p>You can use <a href=""http://developers.gettyimages.com/api/docs/v3/countries/get/"" rel=""nofollow"">Getty API</a> . It will give you abbreviations of country name. Just play on this API.</p>

<p>OR</p>

<p>You can also use <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow"">Levenshtein Distance</a> to get most closest country name.</p>

<p>Try this out. Will help you.</p>
",3,7,891,2016-01-27 06:06:41,https://stackoverflow.com/questions/35029952/how-to-predict-correct-country-name-for-user-provided-country-name
R - tm package: Reduce the number of term matrix for the creation of a term-term adjacency visualization,"<p>I am having problems to make a reproducible term-term adjacency visualization of my corpus, which has about 800K words.</p>

<p>I am following <a href=""https://rdatamining.wordpress.com/2012/05/17/an-example-of-social-network-analysis-with-r-using-package-igraph/"" rel=""nofollow noreferrer"">a tutorial</a> whose term matrix contains just 20 terms, and therefore, the result is optimal:</p>

<p><a href=""https://i.sstatic.net/ZPfuw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZPfuw.png"" alt=""Term-Term Adjacency Graph""></a> </p>

<p>I figure, that my problem is that I am not able to reduce my term matrix to, lets say, the 50 most relevant terms of my corpus. I have found a comment from a site outside SO that could help, but I am not able to adapt it to my needs. In this comment, there is said, that I should have to play with my bounds when I create the Term matrix, so I ended with this code:</p>

<pre><code>dtm2 &lt;- DocumentTermMatrix(ds4.1g, control=list(wordLengths=c(1, Inf), +
bounds=list(global=c(floor(length(ds4.1g)*0.3), floor(length(ds4.1g)*0.6)))))


tdm92.1g2 &lt;- removeSparseTerms(dtm2, 0.99)

tdm2.1g2 &lt;- tdm92.1g2

# Creates a Boolean matrix (counts # docs w/terms, not raw # terms)
tdm3.1g &lt;- inspect(tdm2.1g2)
tdm3.1g[tdm3.1g&gt;=1] &lt;- 1 

# Transform into a term-term adjacency matrix
termMatrix.1gram &lt;- tdm3.1g %*% t(tdm3.1g)
</code></pre>

<p>So, if I am understanding this correctly, I can make the Term-Matrix get only those terms that appears at least in 30% of my documents, but not in more than 60 % of them.</p>

<p>However, no matter how I define this bounds, my term matrix <code>termMatrix.1gram</code> have always 115K elements, whose make impossible a visualization as I want. Is there a way to limit this elements to, let say, just 50 elements?</p>

<p><strong><em>How do I get my corpus?</em></strong></p>

<p>Just for clarification, I write down here the code I use to generate my corpus with the tm package:</p>

<pre><code>#specify where is the directory of the files.
folderdir &lt;- paste0(dirname(myFile),""/"", project, ""/"")

#load the corpus.
corpus &lt;- Corpus(DirSource(folderdir, encoding = ""UTF-8""), readerControl=list(reader=readPlain,language=""de""))
#cleanse the corpus.
ds0.1g &lt;- tm_map(corpus, content_transformer(tolower))
ds1.1g &lt;- tm_map(ds0.1g, content_transformer(removeWords), stopwords(""german""))
ds2.1g &lt;- tm_map(ds1.1g, stripWhitespace)
ds3.1g &lt;- tm_map(ds2.1g, removePunctuation)
ds4.1g &lt;- tm_map(ds3.1g, stemDocument)
ds4.1g &lt;- tm_map(ds4.1g, removeNumbers)
ds5.1g   &lt;- tm_map(ds4.1g, content_transformer(removeWords), c(""a"", ""b"", ""c"", ""d"", ""e"", ""f"",""g"",""h"",""i"",""j"",""k"",""l"",
                                                               ""m"",""n"",""o"",""p"",""q"",""r"",""s"",""t"",""u"",""v"",""w"",""x"",""y"",""z""))
#create matrixes.
tdm.1g &lt;- TermDocumentMatrix(ds4.1g)
dtm.1g &lt;- DocumentTermMatrix(ds4.1g)
#reduce the sparcity.
tdm89.1g &lt;- removeSparseTerms(tdm.1g, 0.89)
tdm9.1g  &lt;- removeSparseTerms(tdm.1g, 0.9)
tdm91.1g &lt;- removeSparseTerms(tdm.1g, 0.91)
tdm92.1g &lt;- removeSparseTerms(tdm.1g, 0.92)

tdm2.1g &lt;- tdm92.1g
</code></pre>

<p>As you can see, is the traditional way to get it using the tm package. The text are originally saved individually in different txt documents in a folder in my computer.</p>
","r, text-mining, tm, adjacency-matrix","<blockquote>
  <p>my problem is that I am not able to reduce my term matrix to, lets
  say, the 50 most relevant terms</p>
</blockquote>

<p>If ""relevancy"" means frequency, you could do it like this:</p>

<pre><code>library(tm)
data(""crude"")
tdm &lt;- TermDocumentMatrix(crude)
dtm &lt;- DocumentTermMatrix(crude)
head(as.matrix(tdm))
tdm &lt;- tdm[names(tail(sort(rowSums(as.matrix(tdm))), 50)), ]
tdm
# &lt;&lt;TermDocumentMatrix (terms: 50, documents: 20)&gt;&gt;
# ...
dtm &lt;- dtm[, names(tail(sort(colSums(as.matrix(dtm))), 50))]
inspect(dtm)
# &lt;&lt;DocumentTermMatrix (documents: 20, terms: 50)&gt;&gt;
# ...
</code></pre>
",4,1,3348,2016-01-28 13:13:06,https://stackoverflow.com/questions/35062510/r-tm-package-reduce-the-number-of-term-matrix-for-the-creation-of-a-term-term
Count number of times a word-wildcard appears in text (in R),"<p>I have a vector of either regular words (""activated"") or wildcard words (""activat*""). I want to:</p>

<p>1) Count the number of times each word appears in a given text (i.e., if ""activated"" appears in text, ""activated"" frequency would be 1).</p>

<p>2) Count the number of times each word wildcard appears in a text (i.e., if ""activated"" and ""activation"" appear in text, ""activat*"" frequency would be 2).</p>

<p>I'm able to achieve (1), but not (2). Can anyone please help? thanks.</p>

<pre><code>library(tm)
library(qdap)
text &lt;- ""activation has begun. system activated""
text &lt;- Corpus(VectorSource(text))
words &lt;- c(""activation"", ""activated"", ""activat*"")

# Using termco to search for the words in the text
apply_as_df(text, termco, match.list=words)

# Result:
#      docs    word.count    activation    activated    activat*
# 1   doc 1             5     1(20.00%)    1(20.00%)           0
</code></pre>
","r, wildcard, text-mining, word-frequency, qdap","<p>Is it possible that this might have to do something with the versions? I ran the exact same code (see below) and got what you expected</p>

<pre><code>    &gt; text &lt;- ""activation has begunm system activated""
    &gt; text &lt;- Corpus(VectorSource(text))
    &gt; words &lt;- c(""activation"", ""activated"", ""activat"")
    &gt; apply_as_df(text, termco, match.list=words)
       docs word.count activation activated   activat
    1 doc 1          5  1(20.00%) 1(20.00%) 2(40.00%)
</code></pre>

<p>Below is the output when I run <code>R.version()</code>. I am running this in RStudio Version 0.99.491 on Windows 10. </p>

<pre><code>    &gt; R.Version()

    $platform
    [1] ""x86_64-w64-mingw32""

    $arch
    [1] ""x86_64""

    $os
    [1] ""mingw32""

    $system
    [1] ""x86_64, mingw32""

    $status
    [1] """"

    $major
    [1] ""3""

    $minor
    [1] ""2.3""

    $year
    [1] ""2015""

    $month
    [1] ""12""

    $day
    [1] ""10""

    $`svn rev`
    [1] ""69752""

    $language
    [1] ""R""

    $version.string
    [1] ""R version 3.2.3 (2015-12-10)""

    $nickname
    [1] ""Wooden Christmas-Tree""
</code></pre>

<p>Hope this helps</p>
",1,1,476,2016-01-28 16:52:28,https://stackoverflow.com/questions/35067324/count-number-of-times-a-word-wildcard-appears-in-text-in-r
Classification documents based on topic frequency,"<p>I need a way to clarify The dominant topics for the following data set , the following data set produced after pre-processing all docs ,<br>
the following selected topics frequencies are follow : </p>

<pre><code>                                         TOPICS 
id   Doc-name   total words     Politics    sport    food   animals  
1       doc1        1000          300         250     100     350
2       doc2        2000          1000        400     200     400
3       doc3        4000          500         300     2000    200
etc... 
</code></pre>

<p>question are :
is there any classification method for this kind of data set ? 
if I consider doc1 is animals is this true ?
is there any way to calculate probability of each topic in that document to find doc dominant topic ? 
any suggestion please ?  </p>
","text, text-mining, text-analysis","<p>This method of classification is only good when the type of document should be determine in relation to a given topic. In no way this type of analysis can give an idea of the real context it blogs to.</p>

<p>What is the context of the sentence if I say ""<em>The athlete is certainly faster than any cat, dog, cow or a sheep</em>""? Does it speak about animals? </p>

<p>The only conclusion you can make about the context of the sentence through this type of analysis is that ""<em>The sentence has factors leading to describe sports and animals. The participation of those factors are 4 to 2</em>"".  </p>

<p>You can go on calculating the probability using standard methods. But the relevance of the numbers to the real context can be distant.   </p>
",0,1,136,2016-02-09 09:36:59,https://stackoverflow.com/questions/35288429/classification-documents-based-on-topic-frequency
Can I insert variables in Regular Expression?,"<p>I want to use regex so as to obtain specific information from the text and I give an example with a semi-pseudocode ~ you can also reply me with semi-pseudocode:</p>

<pre><code>list=[""orange"",""green"",""grey""]
text= ""The Orange is orange""
for word in list:
     if word == re.compile(r'word, text):
           capture Orange in order to have the noun
</code></pre>

<p>Beware! My question focuses whether there is a possibility to use <strong>variables</strong> (as <strong>word</strong> up above) so as to make a loop and see if there are equal words in an text based on a list.</p>

<p>Do not focus on how to capture the Orange.</p>
","regex, list, text-mining","<p>I think Biffen has the right idea, you're in a world of pain if you're using this for POS tagging. Anyway, this allows you to match words in your <code>text</code> variable</p>

<pre><code>for word in list:
    if word in text:
        # Do what you want with word
</code></pre>

<p>If you wanted to use regex then you can build patterns from strings, use parentheses to capture. Then use <code>group()</code> to access captured patterns</p>

<pre><code>for word in list:

    pattern = re.compile("".*("" + word + "").*"")
    m = re.match(pattern, text)

    if m:
        print(m.group(1))
</code></pre>
",0,0,112,2016-02-09 14:27:52,https://stackoverflow.com/questions/35294542/can-i-insert-variables-in-regular-expression
Why are Cosine Similarity and TF-IDF used together?,"<blockquote>
  <p><code>TF-IDF</code> and <code>Cosine Similarity</code> is a commonly used combination for
  text clustering. Each document is represented by vectors of TF-IDF
  weights.</p>
</blockquote>

<p>This is what my text book says.</p>

<p>With Cosine Similarity you can then compute the similarities between those documents.</p>

<p>But why are exactly those techniques used together?<br>
What is the advantage?</p>

<p>Could for example Jaccard Similarity also be used?</p>

<p>I know, <em>how</em> it works, but I want to know, <em>why</em> exactly these techniques.</p>
","data-mining, text-mining, tf-idf, cosine-similarity, linguistics","<p>TF-IDF is the <em>weighting</em> used.</p>

<p>Cosine is the <em>measure</em> used.</p>

<p>You could use cosine without weighting, but results then usually are worse. Jaccard works on sets - it's not obvious how to use weights without turning it into something else without making it the same as Cosine.</p>
",4,5,837,2016-02-09 20:27:52,https://stackoverflow.com/questions/35301534/why-are-cosine-similarity-and-tf-idf-used-together
"Removing Punctuation Marks except smiles - R, tm package","<p>I am using tm package, in R. I want to remove all punctuation marks from this text, except the smiles.</p>

<pre><code>data &lt;- c(""conflict need resolved :&lt;. turned conversation exchange ideas richer environment one tricky concepts :D , �conflict� always top business agendas :&gt;. maybe different ideas/opinions different :) "" )
</code></pre>

<p>I have tried </p>

<p><code>library(tm)
 data &lt;- gsub(""[^a-z]"", "" "", data, ignore.case = TRUE)</code></p>

<p>that is removing all punctuation, include smiles, as output </p>

<pre><code>data &lt;- conflict need resolved turned conversation exchange ideas richer environment one tricky concepts conflict always top business agendas maybe different ideas opinions different
</code></pre>

<p>when I need,</p>

<pre><code>data &lt;- conflict need resolved :&lt; turned conversation exchange ideas richer environment one tricky concepts :D conflict always top business agendas :&gt; maybe different ideas opinions different :) 
</code></pre>

<p>Suggestion pls.</p>
","r, text-mining, tm, punctuation","<p>I would write a dictionary of smiley, replace them all with text, remove the punctuation, and then replace them back.</p>

<pre><code># Make the dictionary. You need to make sure the strings are not in the text, which can be tested with something like stri_match(str=data,regex = smiles$r)
smiles &lt;- data.frame(s=c("":&lt;"","":&gt;"","":)"","":("","";)"","":D""),
                     r=c(""unhappyBracket"",""happyBracket"",""happyParen"",""unhappyParen"",""winkSmiley"",""DSmiley""))

library(stringi)
## replace smiley with text
data &lt;- stri_replace_all_fixed(data,pattern = smiles$s,replacement = smiles$r,vectorize_all = FALSE)
## remove punctuation
data &lt;- gsub(""[^a-z]"", "" "", data, ignore.case = TRUE)
## replace text-smiley with punctuation smiley
data &lt;- stri_replace_all_fixed(data,pattern = smiles$r,replacement = smiles$s,vectorize_all = FALSE)
</code></pre>

<p>Note that if the smiley are important to your analysis, you should leave them as words, since they will be easier to manipulate this way. Also, you may want to look into <code>tm::removePunctuation()</code> and <code>tm::tm_map</code> to handle the punctuation removal step.</p>
",3,0,754,2016-02-15 17:53:43,https://stackoverflow.com/questions/35415914/removing-punctuation-marks-except-smiles-r-tm-package
"how to delete all English words, except special punctuation, in R","<p>I have a data file in R,</p>

<pre><code>data &lt;- ""conflict need resolved :&lt;  turned conversation exchange ideas richer environment one tricky concepts :D    conflict  always top business agendas :&gt;  maybe different ideas opinions different :)"" 
</code></pre>

<p>from this I want to remove all words, only the smiles will be there, and the output I am expecting,</p>

<pre><code>"":&lt; :D :&gt; :)""
</code></pre>

<p>Is there any library or method in R for doing this task easily?</p>
","regex, r, text-mining, data-cleaning","<p>You can use <code>[[:alnum:]]</code> as a regexp pattern for all numeric and alphanumeric characters of a string</p>

<pre><code>s &lt;- gsub(""[[:alnum:]]*"", """", ""conflict need resolved :&lt;  turned conversation exchange ideas richer environment one tricky concepts :D    conflict  always top business agendas :&gt;  maybe different ideas opinions different :) "")
gsub("" +"", "" "", s)

[1] "" :&lt; : :&gt; :) ""
</code></pre>
",1,1,229,2016-02-15 19:43:26,https://stackoverflow.com/questions/35417759/how-to-delete-all-english-words-except-special-punctuation-in-r
How to scrape &lt;br&gt;-delimited content in a webpage with rvest?,"<p>Good day, StackOverflowers.</p>

<p>I have this piece of content:</p>

<p><a href=""https://i.sstatic.net/TVA1e.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/TVA1e.png"" alt=""A &lt;div&gt; with a &lt;p&gt; with its content separated by &lt;br&gt;""></a></p>

<p>EDIT: Here it is in text form:</p>

<pre><code>&lt;p&gt;&lt;b&gt;Tu dinero /  Acciones que acaban con tu quincena&lt;/b&gt;&lt;br&gt;&lt;br&gt;EVITA SOBREENDEUDARTE&lt;br&gt;&lt;br&gt;POR SONIA SOTO&lt;br&gt;&lt;br&gt;El día de pago llegó, pero tú simplemente no pareces emocionarte como todos los mortales, no sólo de México, sino del mundo. Si esto te pasa, sólo puede existir una razón y es que has entrado en un círculo vicioso en el cual trabajas para pagar deudas, y pides más préstamos para sobrevivir la siguiente quincena, porque esta tampoco te alcanzará.&lt;br&gt;&lt;br&gt;Si ya intentaste reducir al máximo tus gastos innecesarios, pero ni así la libras, sería Interesante que pusieras atención en tus hábitos, estamos seguros que ahí puedes encontrar varios porqués a tu situación. Piggo, la plataforma de inversión y ahorro, ha detectado algunos que pueden ser la causa de que el dinero se te vaya de las manos, chécalos y modifica alguna de estas conductas.&lt;br&gt;&lt;br&gt;30 POR CIENTO de tus ingresos es lo máximo que puedes comprometerán deudas, recomienda Condusef&lt;/p&gt;
</code></pre>

<p>I am able to select this tag using the following:</p>

<pre><code>html_node(read_html(x), ""div#readMoreText &gt; p"")
</code></pre>

<p>But if I run <code>html_text()</code> what I get is the entire text without any separation between lines, like this:</p>

<pre><code>[[1]]
[1] ""Tu dinero /  Acciones que acaban con tu quincenaEVITASOBREENDEUDARTEPOR SONIA SOTOEl día de pago llegó, pero tú simplemente no pareces emocionarte como todos los mortales, no sólo de México, sino del mundo. Si esto te pasa, sólo puede existir una razón y es que has entrado en un círculo vicioso en el cual trabajas para pagar deudas, y pides más préstamos para sobrevivir la siguiente quincena, porque esta tampoco te alcanzará.Si ya intentaste reducir al máximo tus gastos innecesarios, pero ni así la libras, sería Interesante que pusieras atención en tus hábitos, estamos seguros que ahí puedes encontrar varios porqués a tu situación. Piggo, la plataforma de inversión y ahorro, ha detectado algunos que pueden ser la causa de que el dinero se te vaya de las manos, chécalos y modifica alguna de estas conductas.30 POR CIENTO de tus ingresos es lo máximo que puedes comprometerán deudas, recomienda Condusef""
</code></pre>

<p>Is there a way with <code>rvest</code> to extract this piece of text and have it replace tags with whitespaces instead of just removing them and making the text illegible?</p>

<p>Thanks.</p>

<p>J.</p>
","html, r, web-scraping, text-mining, rvest","<p>Here is the 'piping' replication which I find more intuitive:</p>

<pre><code>library(rvest)
read_html(""Somewebpage.html"") %&gt;%
 html_nodes(., ""div#readMoreText &gt; p"") %&gt;%
 gsub(pattern = '&lt;.*?&gt;', replacement = ""|""., )
</code></pre>

<p>Splitting the nodes by an appropriate pipe (|) delimiter will help you when there are a lot things you are extracting</p>
",4,4,1206,2016-03-02 21:53:22,https://stackoverflow.com/questions/35758855/how-to-scrape-br-delimited-content-in-a-webpage-with-rvest
Is there any feature of TextBlob to obtain neutral classification?,"<p>I am interested in building a text classifier using textBlob but from my research does not look like after you train the classifier to return neutral tags. Does anyone know a way to implement this ? Or is there a similar library which provides neutral classification ? Thank you.</p>
","python, text-mining, textblob","<p>I am using If else Statement for this : 
like</p>

<pre><code>from textblob import TextBlob
from textblob.sentiments import NaiveBayesAnalyzer

blob = TextBlob(message, analyzer=NaiveBayesAnalyzer())
a = (blob.sentiment)
if a.__getattribute__(""p_pos"") &gt; a.__getattribute__(""p_neg""):
    tag = str(a.__getattribute__(""classification""))
    sentiment = str(a.__getattribute__(""p_pos""))
elif a.__getattribute__(""p_pos"") &lt; a.__getattribute__(""p_neg""):
    tag = str(a.__getattribute__(""classification""))
    sentiment = str(a.__getattribute__(""p_neg""))
else:
    tag = ""N""
    sentiment = ""0""
</code></pre>

<p>If their is neutral sentence, p_pos and p_neg will be equal!</p>

<p>This works for me!!!
Hope it works for you</p>
",1,0,651,2016-03-02 22:42:16,https://stackoverflow.com/questions/35759673/is-there-any-feature-of-textblob-to-obtain-neutral-classification
How to select part of a text in R,"<p>I have an HTML file which consists of 5 different articles and I would like to extract each one of these articles separately in R and run some analysis per article. Each article starts with <code>&lt; doc&gt;</code> and ends with <code>&lt; /doc&gt;</code> and also has a document number.Example:</p>

<pre><code>&lt;doc&gt;
&lt;docno&gt; NA123455-0001 &lt;/docno&gt;
&lt;docid&gt; 1 &lt;/docid&gt;
&lt;p&gt;
NASA one-year astronaut Scott Kelly speaks after coming home to Houston on  
March 3, 2016. Behind Kelly, 
from left to right: U.S. Second Lady Jill Biden; Kelly's identical in      
brother, Mark; 
John Holdren, Assistant to the President for Science and ...
&lt;/p&gt;
&lt;/doc&gt;
&lt;doc&gt;
&lt;docno&gt; KA25637-1215 &lt;/docno&gt;
&lt;docid&gt; 65 &lt;/docid&gt;
&lt;date&gt;
&lt;p&gt;
February 1, 2014, Sunday 
&lt;/p&gt;
&lt;/date&gt;
&lt;section&gt;
&lt;p&gt;
WASHINGTON -- Former Republican presidential nominee Mitt Romney 
is charging into the increasingly divisive 2016 GOP 
White House sweepstakes Thursday with a harsh takedown of front-runner 
Donald Trump, calling him a ""phony"" and exhorting fellow 
&lt;/p&gt;
&lt;/type&gt;
&lt;/doc&gt;
&lt;doc&gt;
&lt;docno&gt; JN1234567-1225 &lt;/docno&gt;
&lt;docid&gt; 67 &lt;/docid&gt;
&lt;date&gt;
&lt;p&gt;
March 5, 2003
&lt;/p&gt;
&lt;/date&gt;
&lt;section&gt;
&lt;p&gt;
SEOUL—New U.S.-led efforts to cut funding for North Korea's nuclearweapons
program through targeted 
sanctions risk faltering because of Pyongyang's willingness to divert all
available resources to its 
military, even at the risk of economic collapse ...
&lt;/p&gt;
&lt;/doc&gt;
</code></pre>

<p>I have uploaded the url by using <code>readLines()</code> function and combined all lines together by using  </p>

<pre><code> articles&lt;- paste(articles, collapse="" "")
</code></pre>

<p>I would like to select first article which is between <code>&lt; doc&gt;..&lt; /doc&gt;</code> and assign it to <code>article1</code>, and second one to <code>article2</code> and so on. </p>

<p>Could you please advise how to construct the function in order to select each one of these articles separately?</p>
","r, text-mining","<p>You could use <code>strsplit</code>, which splits strings on whatever text or regex you give it. It will give you a list with one item for each part of the string between the splitting string, which you can then subset into different variables, if you like. (You could use other regex functions, as well, if you prefer.)</p>

<pre><code>splitArticles &lt;- strsplit(articles, '&lt;doc&gt;')
</code></pre>

<p>You'll still need to chop out the <code>&lt;/doc&gt;</code> tags (plus a lot of other cruft, if you just want the text), but it's a start.</p>

<p>A more typical way to do the same thing would be to use a package for html scraping/parsing. Using the <code>rvest</code> package, you'd need something like</p>

<pre><code>library(rvest)
read_html(articles) %&gt;% html_nodes('doc') %&gt;% html_text()
</code></pre>

<p>which will give you a character vector of the contents of <code>&lt;doc&gt;</code> tags. It may take more cleaning, especially if there are whitespace characters that you need to clean. Picking your selector carefully for <code>html_nodes</code> may help you avoid some of this; it looks like if you used <code>p</code> instead of <code>doc</code>, you're more likely to just get the text.</p>
",1,0,1996,2016-03-03 13:54:12,https://stackoverflow.com/questions/35774115/how-to-select-part-of-a-text-in-r
Sentence to Word Table with R,"<p>I have some sentences, from the sentences I want to separate the words to get row vector each. But the words are repeating to match with the largest sentence's row vector that I do not want. I want no matter how large the sentence is, the row vector of each of the sentences will only be the words one time. </p>

<pre><code>sentence &lt;- c(""case sweden"", ""meeting minutes ht board meeting st march now also attachment added agenda today s board meeting"", ""draft meeting minutes board meeting final meeting minutes ht board meeting rd april"")
sentence &lt;- cbind(sentence)
word_table &lt;- do.call(rbind, strsplit(as.character(sentence), "" ""))
test &lt;- cbind(sentence, word_table)
</code></pre>

<p>This is what I get now, <a href=""https://i.sstatic.net/ZaBoi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZaBoi.png"" alt=""enter image description here""></a></p>

<p>And this is what I want, 
<a href=""https://i.sstatic.net/eZmXJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eZmXJ.png"" alt=""enter image description here""></a></p>

<p>I mean <strong>no-repeating</strong>.</p>
","r, data-mining, text-mining, cpu-word, sentencecase","<p>The Solution from <strong>rawr</strong>,</p>

<pre><code>sentence &lt;- c(""case sweden"", ""meeting minutes ht board meeting st march now also attachment added agenda today s board meeting"", ""draft meeting minutes board meeting final meeting minutes ht board meeting rd april"")
dd &lt;- read.table(text = paste(sentence, collapse = '\n'), fill = TRUE)
test &lt;- cbind(sentence, dd)
</code></pre>

<h2>Or,</h2>

<pre><code>cc &lt;- read.table(text = paste(gsub('\n', '', sentence), collapse = '\n'), fill = TRUE)
test1 &lt;- cbind(sentence, cc)
</code></pre>

<p>Thanks.</p>
",2,2,361,2016-03-07 22:51:00,https://stackoverflow.com/questions/35855735/sentence-to-word-table-with-r
R: Row numbers unmatched for Sentence to word-table,"<p>From my previous <a href=""https://stackoverflow.com/questions/35855735/sentence-to-word-table-with-r"">problem</a>, I have some <code>texts</code> in different rows, and from the text I am trying to generating <code>word-table</code> for each of the words. But problem is occurring when the row number of the text column, and row number of the <code>word-table</code> unlike. It has been found for some text, two or more rows are being created. So finally I cannot <code>cbind</code> these two together. Code is here. I just want the outcome will be exactly same row number of the text that I can bind them together to show for which text is which <code>word-table</code>.</p>

<pre><code>texts &lt;- c(""concratulations    successfully signed   company  please find attached quick guide   can  t figure   immediately "", "" conversation   laughing services  sweden"", ""p please find attached drafted budget   p "", ""p please finad attached  agenda  today s board meeting  p "", ""p hi   nbsp   p    p please find attached  darft meeting minutes  today s meeting   p "", ""p please find attached  final version   minutes  updated action log  please let  know  actions   done   ll update  excel  nbsp   p "", ""p hi    p    p please find attached  draft meeting minutes   action log  please provide comments   end  next week   p    p   nice spring party  saturday    p    p   tuija  p "", "" p welcome team priority   hope   enjoy yo  p "", ""p please find attached  flyer   can study  share   p "", ""p attached new version  voice   receiver   p    p minor change request  invitation code       mentioned    invitation code may       tell  check  code  invitation email    end    alarm bell  example telling  new comments   "", ""comment  etc     front page  now  seemed  end without warning    p "", ""p memo attached  actions    p "", ""p please find attached  updated board roles  responsibilities    made  changes   red   document   please review  especially   role  relevant contact info   prepare  comment   meeting  wednesday  nbsp   p "", ""p attached documents  review  please comment  soonest   p "")
texts &lt;- cbind(texts)
## to remove multi-white spaces
MyDf &lt;- gsub(""\\s+"","" "",texts)
MyDf &lt;- gsub(""\r?\n|\r"", "" "", MyDf)
MyDf &lt;- cbind(MyDf)
colnames(MyDf) &lt;- c(""Introduction"")

## this way, extra rows are being generated
word_table &lt;- read.table(text = paste(gsub('\n', ' ', MyDf), collapse = '\n'), fill = TRUE)

## this way, the words are being repeated to match with the largest text
word_table &lt;- do.call(rbind, strsplit(as.character(MyDf), "" ""))
</code></pre>

<p>More details: the texts had multiple whitespaces, or tab. Initial assumption was, may be that additional spaces creating the problem, but after removing the additional white spaces, still it is in the same problem.</p>

<p><strong>Please Help</strong></p>
","r, data-mining, text-mining, sentence-similarity","<p><strong>Solution</strong>: honorable mention for <code>breaker</code> and <code>cSplit</code> function.</p>

<pre><code>texts &lt;- c(""concratulations    successfully signed   company  please find attached quick guide   can  t figure   immediately "", "" conversation   laughing services  sweden"", ""p please find attached drafted budget   p "", ""p please finad attached  agenda  today s board meeting  p "", ""p hi   nbsp   p    p please find attached  darft meeting minutes  today s meeting   p "", ""p please find attached  final version   minutes  updated action log  please let  know  actions   done   ll update  excel  nbsp   p "", ""p hi    p    p please find attached  draft meeting minutes   action log  please provide comments   end  next week   p    p   nice spring party  saturday    p    p   tuija  p "", "" p welcome team priority   hope   enjoy yo  p "", ""p please find attached  flyer   can study  share   p "", ""p attached new version  voice   receiver   p    p minor change request  invitation code       mentioned    invitation code may       tell  check  code  invitation email    end    alarm bell  example telling  new comments   "", ""comment  etc     front page  now  seemed  end without warning    p "", ""p memo attached  actions    p "", ""p please find attached  updated board roles  responsibilities    made  changes   red   document   please review  especially   role  relevant contact info   prepare  comment   meeting  wednesday  nbsp   p "", ""p attached documents  review  please comment  soonest   p "")
texts &lt;- cbind(texts)
## to remove multi-white spaces
MyDf &lt;- gsub(""\\s+"","" "",texts)
MyDf &lt;- gsub(""\r?\n|\r"", "" "", MyDf)
MyDf &lt;- cbind(MyDf)
colnames(MyDf) &lt;- c(""Introduction"")
n &lt;- matrix(texts[ ,1 ], nrow = nrow(texts), ncol = ncol(texts))
library(splitstackshape)
library(data.table)
breaker &lt;- function(X) {
        strsplit(X, ""[[:space:]]|(?=[.!?])"", perl=TRUE)
}
aaa &lt;- breaker(n)
aaa &lt;- cbind(aaa)
#############################################################################################
cSplit &lt;- function(indt, splitCols, sep = "","", direction = ""wide"", 
                   makeEqual = NULL, fixed = TRUE, drop = TRUE, 
                   stripWhite = FALSE) {
        message(""`cSplit` is now part of the 'splitstackshape' package (V1.4.0)"")
        ## requires data.table &gt;= 1.8.11
        require(data.table)
        if (!is.data.table(indt)) setDT(indt)
        if (is.numeric(splitCols)) splitCols &lt;- names(indt)[splitCols]
        if (any(!vapply(indt[, splitCols, with = FALSE],
                        is.character, logical(1L)))) {
                indt[, eval(splitCols) := lapply(.SD, as.character),
                     .SDcols = splitCols]
        }

        if (length(sep) == 1) 
                sep &lt;- rep(sep, length(splitCols))
        if (length(sep) != length(splitCols)) {
                stop(""Verify you have entered the correct number of sep"")
        }

        if (isTRUE(stripWhite)) {
                indt[, eval(splitCols) := mapply(function(x, y) 
                        gsub(sprintf(""\\s+%s\\s+|\\s+%s|%s\\s+"", 
                                     x, x, x), x, y), 
                        sep, indt[, splitCols, with = FALSE], 
                        SIMPLIFY = FALSE)]
        }  

        X &lt;- lapply(seq_along(splitCols), function(x) {
                strsplit(indt[[splitCols[x]]], split = sep[x], fixed = fixed)
        })

        if (direction == ""long"") {
                if (is.null(makeEqual)) {
                        IV &lt;- function(x,y) if (identical(x,y)) TRUE else FALSE
                        makeEqual &lt;- ifelse(Reduce(IV, rapply(X, length, how = ""list"")),
                                            FALSE, TRUE)
                }
        } else if (direction == ""wide"") {
                if (!is.null(makeEqual)) {
                        if (!isTRUE(makeEqual)) {
                                message(""makeEqual specified as FALSE but set to TRUE"")
                                makeEqual &lt;- TRUE
                        }
                        makeEqual &lt;- TRUE
                } else {
                        makeEqual &lt;- TRUE
                }
        }
        if (isTRUE(makeEqual)) {
                SetUp &lt;- lapply(seq_along(X), function(y) {
                        A &lt;- vapply(X[[y]], length, 1L)
                        list(Mat = cbind(rep(seq_along(A), A), sequence(A)),
                             Val = unlist(X[[y]]))
                })    
                Ncol &lt;- max(unlist(lapply(SetUp, function(y) y[[""Mat""]][, 2]), 
                                   use.names = FALSE))
                X &lt;- lapply(seq_along(SetUp), function(y) {
                        M &lt;- matrix(NA_character_, nrow = nrow(indt), ncol = Ncol)
                        M[SetUp[[y]][[""Mat""]]] &lt;- SetUp[[y]][[""Val""]]
                        M
                })
                if (direction == ""wide"") {
                        X &lt;- lapply(seq_along(X), function(x) {
                                colnames(X[[x]]) &lt;- paste(splitCols[x], 
                                                          sequence(ncol(X[[x]])), 
                                                          sep = ""_"")
                                X[[x]]
                        })
                        if (isTRUE(drop)) {
                                cbind(indt, do.call(cbind, X))[, eval(splitCols) := NULL][]
                        } else {
                                cbind(indt, do.call(cbind, X))
                        }
                } else {
                        indt &lt;- indt[rep(sequence(nrow(indt)), each = Ncol)]
                        X &lt;- lapply(X, function(y) as.vector(t(y)))
                        indt[, eval(splitCols) := lapply(X, unlist, use.names = FALSE)][]
                }  
        } else {
                Rep &lt;- vapply(X[[1]], length, integer(1L))
                indt &lt;- indt[rep(sequence(nrow(indt)), Rep)]
                indt[, eval(splitCols) := lapply(X, unlist, use.names = FALSE)][]
        }
}

df &lt;- cSplit(as.data.frame(aaa), ""aaa"", "","")
df &lt;- data.frame(cbind(texts, df))

######################################################################################
## Heading
Heading &lt;- df[ ,1]

## Word Table
df &lt;- df[ ,2:ncol(df)]

## first column
aaa_first &lt;- df[,1]
aaa_first &lt;- cbind(aaa_first)
c &lt;- substring(aaa_first, 3)

## last column
aaa_end &lt;- df[ ,ncol(df)]
aaa_end &lt;- cbind(aaa_end)
e &lt;- substr(aaa_end, 1, nchar(aaa_end)-1)

## Middole columns
d &lt;- df[ ,3:ncol(df)-1]

cc &lt;- cbind(Heading, c, d, e )
## cc &lt;- cbind( c, d, e )

cc &lt;- data.frame(lapply(cc, as.character), stringsAsFactors = FALSE)

df2 &lt;- as.data.frame(sapply(cc,gsub,pattern= "")"",replacement=""""))
# df2 &lt;- as.data.frame(sapply(df2,gsub,pattern=""("",replacement=""""))
df3 &lt;- as.data.frame(sapply(df2, function(x) gsub(""\"""", """", x)))
</code></pre>
",0,0,70,2016-03-09 16:23:57,https://stackoverflow.com/questions/35897093/r-row-numbers-unmatched-for-sentence-to-word-table
Getting repeated terms after Latent Dirichlet allocation,"<p>I was trying this for Latent Dirichlet allocation implemenation but getting repeated terms.How can I unique terms from LDA?</p>
<blockquote>
<p>library(tm)<br />
Loading required package: NLP<br />
myCorpus &lt;- Corpus(VectorSource(tweets$text))<br />
myCorpus &lt;- tm_map(myCorpus, content_transformer(tolower))<br />
removeURL &lt;- function(x) gsub(&quot;http[^[:space:]]<em>&quot;, &quot;&quot;, x)<br />
myCorpus &lt;- tm_map(myCorpus, content_transformer(removeURL))<br />
removeNumPunct &lt;- function(x) gsub(&quot;[^[:alpha:][:space:]]</em>&quot;, &quot;&quot;, x)<br />
myCorpus &lt;- tm_map(myCorpus, content_transformer(removeNumPunct))<br />
myStopwords &lt;- c(stopwords('english'), &quot;available&quot;, &quot;via&quot;)<br />
myStopwords &lt;- setdiff(myStopwords, c(&quot;r&quot;, &quot;big&quot;))<br />
myCorpus &lt;- tm_map(myCorpus, removeWords, myStopwords)<br />
myCorpus &lt;- tm_map(myCorpus, stripWhitespace)<br />
myCorpusCopy &lt;- myCorpus<br />
myCorpus &lt;- tm_map(myCorpus, stemDocument)<br />
library('SnowballC')<br />
myCorpus &lt;- tm_map(myCorpus, stemDocument)<br />
dtm&lt;-DocumentTermMatrix(myCorpus)<br />
library(&quot;RTextTools&quot;, lib.loc=&quot;~/R/win-library/3.2&quot;)<br />
library(&quot;topicmodels&quot;, lib.loc=&quot;~/R/win-library/3.2&quot;)<br />
om1&lt;-LDA(dtm,30)<br />
terms(om1)</p>
</blockquote>
<p><img src=""https://i.sstatic.net/KzecE.jpg"" alt=""This is the output"" /></p>
","r, text-mining","<p>According to <a href=""https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a> In LDA, each document is viewed as a mixture of various topics. That is for each document (tweet) we get the probability of the tweet belonging to each topic. The probability sums to 1.</p>

<p>Similarly each topic is be viewed as a mixture of various terms(words). That is for each topic we get the probability of each word belonging to the topic. The probability sums to 1. 
Hence for every word topic combination there is a probability assigned. The code <code>terms(om1)</code> gets the word with the highest probability for each topic.</p>

<p>So in your case you are finding same word having the highest probability in multiple topics. This is not an error.</p>

<p>The below code will create <strong>TopicTermdf</strong> dataset which has the distribution of all the words for each topic. Looking at the dataset, will  help you understand better.</p>

<p>The below code is based on the following <a href=""https://stackoverflow.com/questions/14875493/lda-with-topicmodels-how-can-i-see-which-topics-different-documents-belong-to"">LDA with topicmodels, how can I see which topics different documents belong to?</a> post.</p>

<p><strong>Code:</strong></p>

<pre><code># Reproducible data - From Coursera.org John Hopkins Data Science Specialization Capstone project, SwiftKey Challange dataset

tweets &lt;- c(""How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long."",
           ""When you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason."",
           ""they've decided its more fun if I don't."",
           ""So Tired D; Played Lazer Tag &amp; Ran A LOT D; Ughh Going To Sleep Like In 5 Minutes ;)"",
           ""Words from a complete stranger! Made my birthday even better :)"",
           ""First Cubs game ever! Wrigley field is gorgeous. This is perfect. Go Cubs Go!"",
           ""i no! i get another day off from skool due to the wonderful snow (: and THIS wakes me up...damn thing"",
           ""I'm coo... Jus at work hella tired r u ever in cali"",
           ""The new sundrop commercial ...hehe love at first sight"",
           ""we need to reconnect THIS WEEK"")


library(tm)
myCorpus &lt;- Corpus(VectorSource(tweets))
myCorpus &lt;- tm_map(myCorpus, content_transformer(tolower))
removeURL &lt;- function(x) gsub(""http[^[:space:]]"", """", x)
myCorpus &lt;- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct &lt;- function(x) gsub(""[^[:alpha:][:space:]]"", """", x)
myCorpus &lt;- tm_map(myCorpus, content_transformer(removeNumPunct))
myStopwords &lt;- c(stopwords('english'), ""available"", ""via"")
myStopwords &lt;- setdiff(myStopwords, c(""r"", ""big""))
myCorpus &lt;- tm_map(myCorpus, removeWords, myStopwords)
myCorpus &lt;- tm_map(myCorpus, stripWhitespace)
myCorpusCopy &lt;- myCorpus
myCorpus &lt;- tm_map(myCorpus, stemDocument)
library('SnowballC')
myCorpus &lt;- tm_map(myCorpus, stemDocument)
dtm&lt;-DocumentTermMatrix(myCorpus)

library(RTextTools)
library(topicmodels)
om1&lt;-LDA(dtm,3)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>&gt; # Get the top word for each topic 
&gt; terms(om1) 
Topic 1 Topic 2 Topic 3 
""youll""   ""cub"" ""anoth"" 
&gt; 
&gt; #Top word for each topic
&gt; colnames(TopicTermdf)[apply(TopicTermdf,1,which.max)]
[1] ""youll"" ""cub""   ""anoth""

&gt; 
</code></pre>
",4,2,2531,2016-03-10 16:14:09,https://stackoverflow.com/questions/35921727/getting-repeated-terms-after-latent-dirichlet-allocation
Python: Encoding characters but still work with the list,"<p>So for a text mining assignment we try to collect tweets (especially the texts) and run the <strong>stanford NER tagger</strong> to find out if there are persons or locations mentioned. This could also be done by checking the hashtags, but the idea is to use some text mining tools.</p>

<p>so let's say that we have data loaded from a cPickle file which is saved, loaded and split on white space.</p>

<pre><code>hil_text = [[u'Man', u'is', u'not', u'a', u'issue', u'cah', u'me', u'pum', u'pum', u'tun', u'up', u'#InternationalWomensDay', u'#cham', u'#empowerment', u'#Clinton2016', u'#PiDay2016'], [u'Shonda', u'Land', u'came', u'out', u'with', u'a', u'great', u'ad', u'for', u'Clinton:https://t.co/Vfg9lAKNaH#Clinton2016'], [u'RT', u'@BeaverforBernie:', u'Trump', u'and', u'the', u""#Clinton's"", u'are', u'the', u'same.', u'They', u'worship', u'$$$$$.', u'https://t.co/yUXoJaL6mJ'], [u'.@GloriaLaRiva', u'on', u'#Clinton,', u'Reagans', u'&amp;amp;', u'#AIDS:', u'\u201cClinton', u'just', u're-wrote', u'history\u201d', u'https://t.co/L3YuIyFjxo', u'Clinton', u'incapable', u'of', u'telling', u'truth.'], [u'#KKK', u'Leader', u'Gets', u'Behind', u'This', u'Democratic', u'Candidate', u'https://t.co/p9yTQ2sXmV', u'How', u'fitting!', u'#Hillary2016', u'#HillaryClinton', u'#Hillary', u'#Killary', u'#tcot'], [u'#KKK', u'Leader', u'Gets', u'Behind', u'This', u'Democratic', u'Candidate', u'https://t.co/p9yTQ2sXmV', u'How', u'fitting!', u'#Hillary2016', u'#HillaryClinton', u'#Hillary', u'#Killary', u'#tcot'], [u'RT', u'@jvlibrarylady:', u'President', u'Clinton', u'at', u'rally', u'for', u'Hillary', u'at', u'Teamsters', u'Local', u'245', u'in', u'Springfield,', u'Mo.', u'#HillaryClintonForPresident', u'https://t.\u2026'], [u'RT', u'@jvlibrarylady:', u'President', u'Clinton', u'at', u'rally', u'for', u'Hillary', u'at', u'Teamsters', u'Local', u'245', u'in', u'Springfield,', u'Mo.', u'#HillaryClintonForPresident', u'https://t.\u2026']]
</code></pre>

<p>The tagger doesn't accept the unicode, so in trying to get it to work we tried to do the following.</p>

<pre><code>for word in hil_text:
    for x in word:
        print x.encode('utf-8',errors='ignore')
        print tagger.tag(x.encode('utf-8',errors='ignore')
</code></pre>

<p>This results in x being the word printed, but the tagger tagging each letter separately.</p>

<p>Is there a way to encode it and send it through the tagger as a word? Or in other words to encode parts of a list but still keep that part in a list?</p>

<p>And why does the tagger tag each letter and not just the whole x?</p>
","python, unicode, text-mining, named-entity-recognition","<p>It looks like <code>tagger.tag</code> is expecting a <em>sequence</em> of strings. But you are passing in a <em>single string</em>, which python will treats as <em>sequence of characters</em>. To fix that, try this:</p>

<pre><code>for section in hil_text:
    # encode each word in the section, and put them in a new list
    words = [word.encode('utf-8') for word in section]
    # pass the list of encoded words to the tagger
    print tagger.tag(words)
</code></pre>
",0,-2,43,2016-03-14 22:00:14,https://stackoverflow.com/questions/35998911/python-encoding-characters-but-still-work-with-the-list
Text summarization in R language,"<p>I have long text file using help of <code>R language</code> I want to summarize text in at least 10 to 20 line or in small sentences.
How to summarize text in at least 10 line with <code>R language</code> ?</p>
","r, text, text-mining, summarization","<p>You may try this (from the <a href=""https://cran.r-project.org/web/packages/LSAfun/index.html"" rel=""noreferrer"" title=""LSAfun"">LSAfun</a> package):</p>

<pre><code>genericSummary(D,k=1)
</code></pre>

<p>whereby 'D' specifies your text document and 'k' the number of sentences to be used in the summary. (Further modifications are shown in the package documentation).</p>

<p>For more information:
<a href=""http://search.r-project.org/library/LSAfun/html/genericSummary.html"" rel=""noreferrer"">http://search.r-project.org/library/LSAfun/html/genericSummary.html</a></p>
",5,4,10430,2016-03-17 15:26:37,https://stackoverflow.com/questions/36064946/text-summarization-in-r-language
R textmining build a corpus structured with subsets of a dataframe,"<p>I'm new in formulas with R and I'm struggling quite a bit to change some repetitive code into something more compact. As suggested in the comment of MrFlick, I posted the working solution I've already found in the answers section.</p>

<p>So my problem is to create various corpuses for a comparative wordcloud using several different categorisations, as you can see here in <a href=""https://wikispiral.org/tiki-index.php?page=CG%20Facilitators%20-%20Semantic_exploration&amp;CGname=Braine%20l%27Alleud"" rel=""nofollow"">wikispiral.org</a>. For this I need to create a list of character vectors based on subsets of the original dataframe (given categories present in the dataframe). See the following example:</p>

<pre><code>library(wordcloud)
library(tm)

element &lt;- c(""Adams Pearmain "", ""Aia Ilu "", ""Airlie Red Flesh"", ""Akane "", ""Åkerö "", ""Alkmene"", ""Allington Pippin "", ""Ambrosia "", ""Anna "", ""Annurca "", ""Antonovka "", ""Apollo "", ""Ariane "", ""Arkansas Black "", ""Arthur Turner"")
qty &lt;- c(2, 1, 4, 3, 6, 2, 1, 4, 3, 6, 2, 1, 4, 3, 6)
category1 &lt;- c(""Red"", ""Green"", ""Red"", ""Green"", ""Yellow"", ""Orange"", ""Red"", ""Red"", ""Green"", ""Red"", ""Green"", ""Yellow"",  ""Green"", ""Yellow"", ""Orange"")
category2 &lt;- c(""small"", ""big"", ""big"", ""small"", ""small"", ""medium"", ""medium"", ""medium"", big"", ""big"", ""small"", ""medium"", ""big"", ""very big"", ""medium"")
d &lt;- data.frame(element=element, qty=qty, category1=category1, category2=category2)
</code></pre>

<p>Which gives this dataframe :</p>

<pre><code>    element             qty category1   category2
1   Adams Pearmain      2   Red         small
2   Aia Ilu             1   Green       big
3   Airlie Red Flesh    4   Red         small
4   Akane               3   Green       big
5   Åkerö               6   Yellow      small
6   Alkmene             2   Orange      big
7   Allington Pippin    1   Red         small
8   Ambrosia            4   Red         big
9   Anna                3   Green       small
10  Annurca             6   Red         big
11  Antonovka           2   Green       small
12  Apollo              1   Yellow      big
13  Ariane              4   Green       small
14  Arkansas Black      3   Yellow      big
15  Arthur Turner       6   Orange      big
</code></pre>

<p>I am currently creating my list of vectors for wordcloud  as such:</p>

<pre><code>## Subsetting two dataframes to category2 values
wordBig &lt;- d[d$category2 == ""big"",]
wordSmall &lt;- d[d$category2 == ""small"",]

## Extracting the vectors in the category1 columns
wordSmall &lt;- as.vector(wordSmall$category1)
wordBig &lt;- as.vector(wordBig$category1)

## Building the list for the corpus
wordALL &lt;- list(wordBig, wordSmall) # Without list() it doesn' t work
</code></pre>

<p>And finally:</p>

<p>corpus &lt;- Corpus(VectorSource(wordALL), readerControl = list(language = ""fr""))</p>

<p>In my real life example in <a href=""https://wikispiral.org/tiki-index.php?page=CG%20Facilitators%20-%20Semantic_exploration&amp;CGname=Braine%20l%27Alleud"" rel=""nofollow"">wikispiral.org</a>, there is a dynamic array of dimensions - not only the categories ""big"" or ""small"" (and some of those categories are defined by the users of the website, and are quite unpredictable). Even for the fixed categories the code was getting repetitive and ugly, and each dimension's existence had to be tested in order to avoid an error <code>comparative.wordcloud()</code> produces if there is NAs in a category (like no data in the ""big"" category).</p>

<p>So my question is: how to transform the precedent example in a more compact code, which is able to:
1 - detect the categories in the classification column
2 - build the list of character vectors
3 - Perhaps do this avoiding loops...</p>

<p>I' ve already found an answer that I have put in the answers section following MrFlick's advice.</p>
","r, dataframe, text-mining, tm, word-cloud","<p>OK, following 42's suggestion in his comment, it is indeed quicker and cleaner...</p>

<pre><code>listtest &lt;- split(d,d$category2)
listtest &lt;- lapply(listtest, droplevels.data.frame)
wordALL &lt;- lapply(listtest, ""[["", ""element"")
</code></pre>

<p>And there it is, a list of factors, from a dataframe, ready to build a corpus:</p>

<pre><code>corpus &lt;- Corpus(VectorSource(wordALL), readerControl = list(language = ""fr""))
</code></pre>
",0,0,328,2016-03-18 17:15:14,https://stackoverflow.com/questions/36090462/r-textmining-build-a-corpus-structured-with-subsets-of-a-dataframe
How to prepare feature vectors for text classification when the words in the text is not frequently repeating?,"<p>I need to perform the text classification on set of emails. But all the words in my text are thinly sparse i.e frequency of each word with respect to all the documents are very less. words are not that much frequently repeating. Since to train the classifiers I think document term matrix with frequency as weightage is not suitable. Can you please suggest me what kind of other methods I need to use . </p>

<p>Thanks</p>
","machine-learning, nlp, text-mining, information-retrieval, stemming","<p>The real problem will be, that if your words are that sparse a learned classifier will not generalise to the real world data. However, there are several solutions to it</p>

<p>1.) Use more data. This is kind-of a no-brainer. However, you can not only add labeled data you can also use unlabelled data in a <strong>semi-supervised learning</strong></p>

<p>2.) Use more data (part b). You can look into the <strong>transfer learning</strong> setting. There you build a classifier on a large data set with similar characteristics. This might be twitter streams and then adapt this classifier to your domain</p>

<p>3.) Get your processing pipeline right. Your problem might origin from a suboptimal processing pipeline. Are you doing <strong>stemming</strong>? In the email the word <em>steming</em> should be mapped onto <em>stem</em>. This can be pushed even further by using synonym matching with a dictionary.</p>
",0,0,188,2016-03-21 06:17:21,https://stackoverflow.com/questions/36124329/how-to-prepare-feature-vectors-for-text-classification-when-the-words-in-the-tex
How to transform text data into numerical data considering difference of distance between text before clustering,"<p>For example let's say I have these values: &quot;BMW&quot;, &quot;MERCEDES&quot; and &quot;FIAT&quot;.
A normal transformation would be to give them numbers 1, 2 and 3:</p>
<p>If I want to measure the distance between these values it would be 1 between &quot;BMW&quot; and &quot;MERCEDES&quot; and also 1 between &quot;BMW&quot; and &quot;FIAT&quot; while this result is not the one needed because (for example) distance between &quot;MERCEDES&quot; and &quot;BMW&quot; should be way smaller than the one between &quot;BMW&quot; and &quot;FIAT&quot; because they're belonging to the same pricing category while fiat is cheaper.</p>
<p>Classifying them and giving them weights would be easy if it was a small range of exemplars but what to do when you have thousands of car brands (for example) knowing that there is no specific attribute or field associated with each brand to give a hint about price (or class or anything for that matter) for weighing automation.</p>
","normalization, transformation, text-mining","<p>You can use e.g. MDS to project the data to a low-dimensional vector space approximation that yields the desired point distances.</p>

<p>The real problem is how to get a meaningful distance matrix in the first place.</p>
",0,-1,46,2016-03-21 23:18:31,https://stackoverflow.com/questions/36143429/how-to-transform-text-data-into-numerical-data-considering-difference-of-distanc
Efficient jaccard similarity DocumentTermMatrix,"<p>I want a way to efficiently calculate Jaccard similarity between documents of a <code>tm::DocumentTermMatrix</code>.  I can do something similar for cosine similarity via the <strong>slam</strong> package as shown in <a href=""https://stackoverflow.com/a/29755756/1000343"">this answer.</a> I came across <a href=""https://stats.stackexchange.com/a/89947/7482"">another question and response</a> on CrossValidated that was R specific but about matrix algebra not necessarily the most efficient route.  I tried to implement that solution with more efficient <strong>slam</strong> functions but do not get the same solution as when I use a less efficient approach of coercing the DTM to a matrix and using <code>proxy::dist</code>.</p>

<p>How can I efficiently calculate Jaccard similarity between documents of a large DocumentTermMatrix in R?</p>

<p><strong>#Data &amp; Pacages</strong></p>

<pre><code>library(Matrix);library(proxy);library(tm);library(slam);library(Matrix)

mat &lt;- structure(list(i = c(1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 3L, 1L, 
    2L, 3L, 3L, 3L, 4L, 4L, 4L, 4L), j = c(1L, 1L, 2L, 2L, 3L, 3L, 
    4L, 4L, 4L, 5L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L), v = c(1, 
    1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1), nrow = 4L, 
        ncol = 12L, dimnames = structure(list(Docs = c(""1"", ""2"", 
        ""3"", ""4""), Terms = c(""computer"", ""is"", ""fun"", ""not"", ""too"", 
        ""no"", ""it's"", ""dumb"", ""what"", ""should"", ""we"", ""do"")), .Names = c(""Docs"", 
        ""Terms""))), .Names = c(""i"", ""j"", ""v"", ""nrow"", ""ncol"", ""dimnames""
    ), class = c(""DocumentTermMatrix"", ""simple_triplet_matrix""), weighting = c(""term frequency"", 
    ""tf""))
</code></pre>

<p><strong>#Inefficient Calculation (expected output)</strong></p>

<pre><code>proxy::dist(as.matrix(mat), method = 'jaccard')

##       1     2     3
## 2 0.000            
## 3 0.875 0.875      
## 4 1.000 1.000 1.000
</code></pre>

<p><strong>#My Attempt</strong></p>

<pre><code>A &lt;- slam::tcrossprod_simple_triplet_matrix(mat)
im &lt;- which(A &gt; 0, arr.ind=TRUE)
b &lt;- slam::row_sums(mat)
Aim &lt;- A[im]

stats::as.dist(Matrix::sparseMatrix(
      i = im[,1],
      j = im[,2],
      x = Aim / (b[im[,1]] + b[im[,2]] - Aim),
      dims = dim(A)
))

##     1   2   3
## 2 2.0        
## 3 0.1 0.1    
## 4 0.0 0.0 0.0
</code></pre>

<p>Outputs do not match.</p>

<p>FYI Here is the original text:</p>

<pre><code>c(""Computer is fun. Not too fun."", ""Computer is fun. Not too fun."", 
    ""No it's not, it's dumb."", ""What should we do?"")
</code></pre>

<p>I'd expect elements 1 &amp; 2 to be 0 distance and element 3 to be closer to  element 1 than element 1 and 4 (I'd expect furthest distance as no words are shared) as seen in the <code>proxy::dist</code> solution.</p>

<p><strong>EDIT</strong></p>

<p>Note that even on a medium sized DTM the matrix becomes huge.  Here's an example with the <strong>vegan</strong> package.  Note 4 minutes to solve where as the cosine similarity is ~5 seconds.</p>

<pre><code>library(qdap); library(quanteda);library(vegan);library(slam)
x &lt;- quanteda::convert(quanteda::dfm(rep(pres_debates2012$dialogue), stem = FALSE, 
        verbose = FALSE, removeNumbers = FALSE), to = 'tm')


## &lt;&lt;DocumentTermMatrix (documents: 2912, terms: 3368)&gt;&gt;
## Non-/sparse entries: 37836/9769780
## Sparsity           : 100%
## Maximal term length: 16
## Weighting          : term frequency (tf)

tic &lt;- Sys.time()
jaccard_dist_mat &lt;- vegan::vegdist(as.matrix(x), method = 'jaccard')
Sys.time() - tic #Time difference of 4.01837 mins

tic &lt;- Sys.time()
tdm &lt;- t(x)
cosine_dist_mat &lt;- 1 - crossprod_simple_triplet_matrix(tdm)/(sqrt(col_sums(tdm^2) %*% t(col_sums(tdm^2))))
Sys.time() - tic #Time difference of 5.024992 secs
</code></pre>
","r, text-mining, tm, slam","<p>Jaccard measure is a measure between <strong>SETS</strong> and input matrix should be <strong>binary</strong>. The <a href=""https://stats.stackexchange.com/questions/49453/calculating-jaccard-or-other-association-coefficient-for-binary-data-using-matri/89947#89947"">very first line</a> says:</p>

<pre><code>## common values:
A = tcrossprod(m)
</code></pre>

<p>In case of bag-of-words <code>DTM</code> this is not the number of common values!</p>

<pre><code>library(text2vec)
library(magrittr)
library(Matrix)

jaccard_similarity &lt;- function(m) {
  A &lt;- tcrossprod(m)
  im &lt;- which(A &gt; 0, arr.ind=TRUE, useNames = F)
  b &lt;- rowSums(m)
  Aim &lt;- A[im]
  sparseMatrix(
    i = im[,1],
    j = im[,2],
    x = Aim / (b[im[,1]] + b[im[,2]] - Aim),
    dims = dim(A)
  )
}

jaccard_distance &lt;- function(m) {
  1 - jaccard_similarity(m)
}

cosine &lt;- function(m) {
  m_normalized &lt;- m / sqrt(rowSums(m ^ 2))
  tcrossprod(m_normalized)
}
</code></pre>

<p><strong>Benchmarks:</strong></p>

<pre><code>data(""movie_review"")
tokens &lt;- movie_review$review %&gt;% tolower %&gt;% word_tokenizer

dtm &lt;- create_dtm(itoken(tokens), hash_vectorizer(hash_size = 2**16))
dim(dtm)
# 5000 65536

system.time(dmt_cos &lt;- cosine(dtm))
# user  system elapsed 
#  2.524   0.169   2.693 

system.time( {
  dtm_binary &lt;- transform_binary(dtm)
  # or simply
  # dtm_binary &lt;- sign(dtm)
  dtm_jac &lt;- jaccard_similarity(dtm_binary)  
})
#   user  system elapsed 
# 11.398   1.599  12.996
max(dtm_jac)
# 1
dim(dtm_jac)
# 5000 5000
</code></pre>

<p><strong>EDIT 2016-07-01:</strong></p>

<p>See even faster version from <a href=""https://github.com/dselivanov/text2vec/blob/0.4/R/distance.R#L2-L26"" rel=""nofollow noreferrer"">text2vec 0.4</a> (<strong>~2.85x</strong> when not need to convert from <code>dgCMatrix</code> to <code>dgTMatrix</code> and <strong>~1.75x</strong> when need column major <code>dgCMatrix</code>)</p>

<pre><code>jaccard_dist_text2vec_04 &lt;- function(x, y = NULL, format = 'dgCMatrix') {
  if (!inherits(x, 'sparseMatrix'))
    stop(""at the moment jaccard distance defined only for sparse matrices"")
  # union x
  rs_x = rowSums(x)
  if (is.null(y)) {
    # intersect x
    RESULT = tcrossprod(x)
    rs_y = rs_x
  } else {
    if (!inherits(y, 'sparseMatrix'))
      stop(""at the moment jaccard distance defined only for sparse matrices"")
    # intersect x y
    RESULT = tcrossprod(x, y)
    # union y
    rs_y = rowSums(y)
  }
  RESULT = as(RESULT, 'dgTMatrix')
  # add 1 to indices because of zero-based indices in sparse matrices
  # 1 - (...) because we calculate distance, not similarity
  RESULT@x &lt;- 1 - RESULT@x / (rs_x[RESULT@i + 1L] + rs_y[RESULT@j + 1L] - RESULT@x)
  if (!inherits(RESULT, format))
    RESULT = as(RESULT, format)
  RESULT
}
system.time( {
   dtm_binary &lt;- transform_binary(dtm)
   dtm_jac &lt;-jaccard_dist(dtm_binary, format = 'dgTMatrix')
 })
 #  user  system elapsed 
 # 4.075   0.517   4.593  
system.time( {
   dtm_binary &lt;- transform_binary(dtm)
   dtm_jac &lt;-jaccard_dist(dtm_binary, format = 'dgCMatrix')
 })
 #  user  system elapsed 
 # 6.571   0.939   7.516
</code></pre>
",4,9,3469,2016-03-25 13:10:33,https://stackoverflow.com/questions/36220585/efficient-jaccard-similarity-documenttermmatrix
write a CSV file with Python after scraping text from website,"<p>I'm trying to write a CSV file after scraping text from website. I already found this answer: <a href=""https://stackoverflow.com/questions/34580286/how-can-write-scraped-content-to-a-csv-file"">How can write scraped content to a CSV file?</a> but it doesn't solve my problem.</p>

<p>The problem is that I got only one row after writing the CSV file.</p>

<p>This is my code so far (I used <code>bs4</code> and <code>mechanize</code>):</p>

<pre><code>from bs4 import BeautifulSoup
import mechanize

url = ""https://www.contratos.gov.co/consultas/detalleProceso.do?numConstancia=15-12-3634534""
br = mechanize.Browser()
search = br.open(url)

# Browser options
br.set_handle_equiv(True)
br.set_handle_redirect(True)
br.set_handle_referer(True)
br.set_handle_robots(False)
br.addheaders = [('User-agent', 'Firefox')]

response = search.read()

soup = BeautifulSoup(response, 'lxml')
text = soup.findAll('td', {'class': 'tablaslistOdd'})
for t in text:
    result = t.text.replace(""\t"", """").replace(""\r"", """").replace('\n', '')
    newResult = result.encode('utf-8')
    #print newResult
    for line in newResult:
        output = open('data/myCSVfile.csv', 'w')
        output.write(newResult)
        output.close()
</code></pre>

<p>What I'm looking for is to write for each line of text a CSV column. 
Is this possible?
Any suggestions to solve the problem are appreciated!</p>
","python, csv, web-scraping, beautifulsoup, text-mining","<p>The problem is that each time you close the file and open it, it rewrites what is in there. So you need not to close it. For example, your code to write the csv may look like this:</p>

<pre><code>with open('data/myCSVfile.csv', 'w') as f:
    for line in newResult:
        f.write(newResult + '\n')
</code></pre>

<hr>

<p>By the way, <code>with</code> statement allows you not to care about closing the file - it closes it for you. It's a good practice to use it, because it makes your code cleaner and closes the file right after you don't need it.</p>

<hr>

<p>To get the format you want try this:</p>

<pre><code>import csv

lines = []
for t in soup.findAll('td', {'class': 'tablaslistOdd'}):
    text = t.text.replace(""\t"", """").replace(""\r"", """").replace('\n', '').encode('utf-8')
    lines.append(text)

with open('myCSVfile.csv', 'w') as f:
    writer = csv.writer(f, delimiter=',')
    writer.writerow(lines)
</code></pre>
",2,0,1945,2016-03-28 23:14:13,https://stackoverflow.com/questions/36272772/write-a-csv-file-with-python-after-scraping-text-from-website
What is the structure of ATIS (Airline Travel Information System) dataset,"<p>When I use ATIS (Airline Travel Information System) dataset(<a href=""http://lisaweb.iro.umontreal.ca/transfert/lisa/users/mesnilgr/atis/"" rel=""nofollow"">http://lisaweb.iro.umontreal.ca/transfert/lisa/users/mesnilgr/atis/</a>) to do research in recurrent-neural-network. I am confused with it's structure.</p>

<p>For example, after using <code>data = pickle.load(open(""./dataset/atis.fold0.pkl"",  ""rb""),encoding='iso-8859-1')</code> to load the atis.fold0.pkl, I use <code>print (np.shape(data_train))</code> to get <code>(4,)</code>. I think the data[0] is the training set, the data[1] is the valid set, the data[2] is the test set and the data[3] is the dict. </p>

<p>But when I use <code>print(np.shape(data[0]))</code>, I get <code>(3, 3983)</code>. I wonder why I get 3 rows in it? what's the difference between these three rows. </p>

<p>And what's the difference between file atis.fold0.pkl, atis.fold1.pkl, atis.fold2.pkl, atis.fold3.pkl, atis.fold4.pkl?</p>
","python, dataset, text-mining, recurrent-neural-network","<pre><code>f = gzip.open(filename, 'rb')
try:
    train_set, valid_set, test_set, dicts = pickle.load(f, encoding='latin1')
except:
    train_set, valid_set, test_set, dicts = pickle.load(f)


print np.shape(train_set)
</code></pre>

<p>Each of the pickle can be divided into training, validation, testing, and its dictionary. When you see the dictionary elements, they contain the<br> words2idx <br>
tables2idx<br>
labels2idx<br></p>

<p>Now test the following code
<code>
for i in train_set:
&#09;print len(i[0])
</code></p>

<p>It will return same length file.
So the First element is the words.Second is the tables2idx and third is final result of slot filling (labels2idx)</p>

<p>Use the dict to decry pt the ids, you will get the meaning. </p>
",2,2,3562,2016-03-30 04:11:14,https://stackoverflow.com/questions/36299544/what-is-the-structure-of-atis-airline-travel-information-system-dataset
How to keep the beginning and end of sentence markers with quanteda,"<p>I'm trying to create 3-grams using R's <code>quanteda</code> package.</p>

<p>I'm struggling to find a way to keep in the n-grams beginning and end of sentence markers, the <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> as in the code below.</p>

<p>I thought that using the <code>keptFeatures</code> with a regular expression that matched those should maintain them but the chevron markers are always removed.</p>

<p>How can I keep the chevron markers from being removed or what is the best way to delimit beginning and end of sentence with <code>quanteda</code>?</p>

<p>As a bonus question what is the advantage of <code>docfreq(mydfm)</code> over <code>colSums(mydfm)</code>, the result of str(colSums(mydfm)) and str(docfreq(mydfm)) is almost identical (<code>Named num [1:n]</code> the former, <code>Named int [1:n]</code> the latter)? </p>

<pre><code>library(quanteda)
text &lt;- ""&lt;s&gt;I'm a sentence and I'd better be formatted properly!&lt;/s&gt;&lt;s&gt;I'm a second sentence&lt;/s&gt;""

qc &lt;- corpus(text)

mydfm  &lt;- dfm(qc, ngram=3, removeNumbers = F, stem=T, keptFeatures=""\\&lt;/?s\\&gt;"")

names(colSums(mydfm))

# Output:
# [1] ""s_i'm_a""    ""i'm_a_sentenc""    ""a_sentenc_and""    ""sentenc_and_i'd""
# [2] ""and_i'd_better""   ""i'd_better_be""    ""better_be_format""   
# [3] ""be_format_proper"" ""format_proper_s""  ""proper_s_s""   ""s_s_i'm""    
# [4] ""i'm_a_second""   ""a_second_sentenc""   ""second_sentenc_s""
</code></pre>

<p><strong>EDIT:</strong></p>

<p>Corrected keepFeatures to keptFeatures in code snippet.</p>
","r, nlp, text-mining, tm, quanteda","<p>To return a simple vector, just unlist the <code>tokenizedText"" object returned from</code>tokenize()<code>(which is a specially classed list, with additional attributes).  Here I used the</code>what = ""fasterword""<code>which splits on ""\\s"" -- it's a tiny bit smarter than</code>what = ""fastestword""<code>which splits on</code>"" ""`.</p>

<pre><code># how to not remove the &lt;s&gt;, and return a vector 
unlist(toks &lt;- tokenize(text, ngrams = 3, what = ""fasterword""))
## [1] ""&lt;s&gt;I'm_a_sentence""                ""a_sentence_and""                  
## [3] ""sentence_and_I'd""                 ""and_I'd_better""                  
## [5] ""I'd_better_be""                    ""better_be_formatted""             
## [7] ""be_formatted_properly!&lt;/s&gt;&lt;s&gt;I'm"" ""formatted_properly!&lt;/s&gt;&lt;s&gt;I'm_a"" 
## [9] ""properly!&lt;/s&gt;&lt;s&gt;I'm_a_second""     ""a_second_sentence&lt;/s&gt;"" 
</code></pre>

<p>To keep it within sentence, tokenise the object twice, the first time by sentence, the second time by <code>fasterword</code>.</p>

<pre><code># keep it within sentence
(sents &lt;- unlist(tokenize(text, what = ""sentence"")))
## [1] ""&lt;s&gt;I'm a sentence and I'd better be formatted properly!""
## [2] ""&lt;/s&gt;&lt;s&gt;I'm a second sentence&lt;/s&gt;"" 
tokenize(sents, ngrams = 3, what = ""fasterword"")
## tokenizedText object from 2 documents.
## Component 1 :
## [1] ""&lt;s&gt;I'm_a_sentence""      ""a_sentence_and""         ""sentence_and_I'd""       ""and_I'd_better""        
## [5] ""I'd_better_be""          ""better_be_formatted""    ""be_formatted_properly!""
## 
## Component 2 :
## [1] ""&lt;/s&gt;&lt;s&gt;I'm_a_second""   ""a_second_sentence&lt;/s&gt;""
</code></pre>

<p>To preserve the chevron markers in a dfm, you can pass through the same options that you used above in the <code>tokenize()</code> call, since <code>dfm()</code> calls <code>tokenize()</code> but with different defaults -- it uses the ones most users will probably want, whereas <code>tokenize()</code> is much more conservative.</p>

<pre><code># Bonus questions:
myDfm &lt;- dfm(text, verbose = FALSE, what = ""fasterword"", removePunct = FALSE)
# ""chevron"" markers are not removed
features(myDfm)
## [1] ""&lt;s&gt;i'm""              ""a""                   ""sentence""            ""and""                 ""i'd""                
## [6] ""better""              ""be""                  ""formatted""           ""properly!&lt;/s&gt;&lt;s&gt;i'm"" ""second""             
## [11] ""sentence&lt;/s&gt;"" 
</code></pre>

<p>Final part of the bonus question was the difference between <code>docfreq()</code> and <code>colSums()</code>.  The former returns the count of documents in which a term occurs, the latter sums the columns to get a total term frequency across documents.  See below how different these are for the term <code>""representatives""</code>.</p>

<pre><code># Difference between docfreq() and colSums():
myDfm2 &lt;- dfm(inaugTexts[1:4], verbose = FALSE)
myDfm2[, ""representatives""]
docfreq(myDfm2)[""representatives""]
colSums(myDfm2)[""representatives""]
## Document-feature matrix of: 4 documents, 1 feature.
## 4 x 1 sparse Matrix of class ""dfmSparse""
##                  features
## docs              representatives
##   1789-Washington               2
##   1793-Washington               0
##   1797-Adams                    2
##   1801-Jefferson                0
docfreq(myDfm2)[""representatives""]
## representatives 
##               2 
colSums(myDfm2)[""representatives""]
## representatives 
##               4 
</code></pre>

<p><strong>Update: Some commands and behaviours have changed in quanteda v0.9.9:</strong></p>

<p>Return a simple vector, retaining chevrons:</p>

<pre><code>as.character(toks &lt;- tokens(text, ngrams = 3, what = ""fasterword""))
#  [1] ""&lt;s&gt;I'm_a_sentence""                ""a_sentence_and""                   ""sentence_and_I'd""                
#  [4] ""and_I'd_better""                   ""I'd_better_be""                    ""better_be_formatted""             
#  [7] ""be_formatted_properly!&lt;/s&gt;&lt;s&gt;I'm"" ""formatted_properly!&lt;/s&gt;&lt;s&gt;I'm_a""  ""properly!&lt;/s&gt;&lt;s&gt;I'm_a_second""    
# [10] ""a_second_sentence&lt;/s&gt;"" 
</code></pre>

<p>Keeping within sentence:</p>

<pre><code>(sents &lt;- as.character(tokens(text, what = ""sentence"")))
# [1] ""&lt;s&gt;I'm a sentence and I'd better be formatted properly!"" ""&lt;/s&gt;&lt;s&gt;I'm a second sentence&lt;/s&gt;""                       
tokens(sents, ngrams = 3, what = ""fasterword"")
# tokens from 2 documents.
# Component 1 :
# [1] ""&lt;s&gt;I'm_a_sentence""      ""a_sentence_and""         ""sentence_and_I'd""       ""and_I'd_better""         ""I'd_better_be""         
# [6] ""better_be_formatted""    ""be_formatted_properly!""
# 
# Component 2 :
# [1] ""&lt;/s&gt;&lt;s&gt;I'm_a_second""   ""a_second_sentence&lt;/s&gt;""
</code></pre>

<p>Bonus question part 1:</p>

<pre><code>featnames(dfm(text, verbose = FALSE, what = ""fasterword"", removePunct = FALSE))
#  [1] ""&lt;s&gt;i'm""              ""a""                   ""sentence""            ""and""                 ""i'd""                
#  [6] ""better""              ""be""                  ""formatted""           ""properly!&lt;/s&gt;&lt;s&gt;i'm"" ""second""             
# [11] ""sentence&lt;/s&gt;""
</code></pre>

<p>Bonus question part 2 is unchanged.</p>
",2,2,851,2016-03-30 23:33:53,https://stackoverflow.com/questions/36321619/how-to-keep-the-beginning-and-end-of-sentence-markers-with-quanteda
Convert topicmodels output to JSON,"<p>I use the following function to convert the topicmodels output to JSON output to use in ldavis. </p>

<pre><code>topicmodels_json_ldavis &lt;- function(fitted, corpus, doc_term){
     ## Required packages
     library(topicmodels)
     library(dplyr)
     library(stringi)
     library(tm)
     library(LDAvis)

     ## Find required quantities
     phi &lt;- posterior(fitted)$terms %&gt;% as.matrix
     theta &lt;- posterior(fitted)$topics %&gt;% as.matrix
     vocab &lt;- colnames(phi)
     doc_length &lt;- vector()
     for (i in 1:length(corpus)) {
          temp &lt;- paste(corpus[[i]]$content, collapse = ' ')
          doc_length &lt;- c(doc_length, stri_count(temp, regex = '\\S+'))
     }
     temp_frequency &lt;- inspect(doc_term)
     freq_matrix &lt;- data.frame(ST = colnames(temp_frequency),
                               Freq = colSums(temp_frequency))
     rm(temp_frequency)

     ## Convert to json
     json_lda &lt;- LDAvis::createJSON(phi = phi, theta = theta,
                                    vocab = vocab,
                                    doc.length = doc_length,
                                    term.frequency = freq_matrix$Freq)

     return(json_lda)
}
</code></pre>

<p>but I receive the following error</p>

<p><strong>Error in LDAvis::createJSON(phi = phi, theta = theta, vocab = vocab, doc.length = doc_length,  :    Length of doc.length not equal 
      to the number of rows in theta; both should be equal to the number of 
      documents in the data.</strong></p>

<p>Here is my complete code:</p>

<pre><code>data &lt;- read.csv(""textmining.csv"")


corpus &lt;- Corpus(DataframeSource(data.frame(data$reasonforleaving))) 

# Remove punctuations and numbers because they are generally uninformative.
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, removeNumbers)
# Convert all words to lowercase.
corpus &lt;- tm_map(corpus, content_transformer(tolower))
# Remove stopwords such as ""a"", ""the"", etc.
corpus &lt;- tm_map(corpus, removeWords, stopwords(""english""))
# Use the SnowballC package to do stemming.
library(SnowballC)
corpus &lt;- tm_map(corpus, stemDocument)


# remove extra words
toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, "" "", x))
corpus &lt;- tm_map(corpus, toSpace, ""still"")
corpus &lt;- tm_map(corpus, toSpace, ""also"")

# Remove excess white spaces between words.

corpus &lt;- tm_map(corpus, stripWhitespace)
# Inspect the first document to see what it looks like.
corpus[[1]]$content 

dtm &lt;- DocumentTermMatrix(corpus)

# remove empty documents
library(slam)
dtm = dtm[row_sums(dtm)&gt;0,]

# Use topicmodels package to conduct LDA analysis.

burnin &lt;- 500
iter &lt;- 1000
keep &lt;- 30
k &lt;- 5

result55 &lt;- LDA(dtm, 5)
ldaoutput = topicmodels_json_ldavis(result55,corpus, dtm)
</code></pre>

<p>Do you know why I receive the error?</p>

<p>Thanks</p>
","text-mining, lda, topic-modeling","<h3>Problem</h3>

<p>Your problem is in <code>for (i in 1:length(corpus))</code> in</p>

<pre><code> doc_length &lt;- vector()
     for (i in 1:length(corpus)) {
          temp &lt;- paste(corpus[[i]]$content, collapse = ' ')
          doc_length &lt;- c(doc_length, stri_count(temp, regex = '\\S+'))
     }
</code></pre>

<p>Remember, you have removed some ""empty"" documents from your DocumentTermMatrix in <code>dtm = dtm[row_sums(dtm)&gt;0,]</code>, 
so your vector length here is going to be too big.</p>

<h3>Suggestion</h3>

<p>You may want to keep a vector of the empty docs around as it will help you not only to generate the JSON but also to go back and forth between your empty and full document sets.<br>
<code>doc.length = colSums( as.matrix(tdm) &gt; 0 )[!empty.docs]</code></p>

<p><em>My suggestion assumes you have the full <code>tdm</code> with empty docs in place</em></p>
",0,0,2393,2016-03-31 20:07:13,https://stackoverflow.com/questions/36342754/convert-topicmodels-output-to-json
Hashingvectorizer and Multinomial naive bayes are not working together,"<p>I am trying to write a twitter sentiment analysis program with <code>Scikit-learn</code> in python 2.7. OS is Linux Ubuntu 14.04. </p>

<p>In Vectorizing step, I want to use <code>Hashingvectorizer()</code>. To test the classifier accuracy it works fine with <code>LinearSVC</code>, <code>NuSVC</code>, <code>GaussianNB</code>, <code>BernoulliNB</code> and <code>LogisticRegression</code> classifiers, but for <code>MultinomialNB</code>, it returns this error</p>

<pre><code>Traceback (most recent call last):
  File ""/media/test.py"", line 310, in &lt;module&gt;
    classifier_rbf.fit(train_vectors, y_trainTweets)
  File ""/home/.local/lib/python2.7/site-packages/sklearn/naive_bayes.py"", line 552, in fit
    self._count(X, Y)
  File ""/home/.local/lib/python2.7/site-packages/sklearn/naive_bayes.py"", line 655, in _count
    raise ValueError(""Input X must be non-negative"")
ValueError: Input X must be non-negative
[Finished in 16.4s with exit code 1] 
</code></pre>

<p>Here is the block code related to this error</p>

<pre><code>vectorizer = HashingVectorizer()
train_vectors = vectorizer.fit_transform(x_trainTweets)
test_vectors = vectorizer.transform(x_testTweets)

classifier_rbf = MultinomialNB()
classifier_rbf.fit(train_vectors, y_trainTweets)
prediction_rbf = classifier_rbf.predict(test_vectors)
</code></pre>

<p>Why it is happening and how can I solve it? </p>
","python, python-2.7, scikit-learn, text-mining","<p>You need to set <code>non_negative</code> argument to <code>True</code>, when initialising your vectorizer</p>

<pre><code>vectorizer = HashingVectorizer(non_negative=True)
</code></pre>
",5,6,3418,2016-04-06 16:07:08,https://stackoverflow.com/questions/36456518/hashingvectorizer-and-multinomial-naive-bayes-are-not-working-together
"An easy tutorial for a tool that supports text classification, clustering and topic modeling","<p>What is a text mining tool with some easy tutorials and active community? I found some popular but not sure which one to start with.  </p>
","weka, text-mining, gensim, topic-modeling, mallet","<p>I suggest <a href=""https://de.dariah.eu/tatom-intro"" rel=""nofollow"">TAToM</a> by Allan Riddell.</p>

<p>And there is a portal to more tutorials called  <a href=""https://fedora.clarin-d.uni-saarland.de/hub/kwsearch/topic%20model/0/"" rel=""nofollow"">TeLeMaCo</a> at the CLARIN centre at Saarland University.</p>
",1,1,241,2016-04-08 03:35:44,https://stackoverflow.com/questions/36491071/an-easy-tutorial-for-a-tool-that-supports-text-classification-clustering-and-to
How to find ngram frequency of a column in a pandas dataframe?,"<p>Below is the input pandas dataframe I have.</p>

<p><a href=""https://i.sstatic.net/ltSrD.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/ltSrD.png"" alt=""enter image description here""></a></p>

<p>I want to find the frequency of unigrams &amp; bigrams. A sample of what I am expecting is shown below<a href=""https://i.sstatic.net/7NOKk.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/7NOKk.png"" alt=""enter image description here""></a></p>

<p>How to do this using nltk or scikit learn?</p>

<p>I wrote the below code which takes a string as input. How to extend it to series/dataframe?</p>

<pre><code>from nltk.collocations import *
desc='john is a guy person you him guy person you him'
tokens = nltk.word_tokenize(desc)
bigram_measures = nltk.collocations.BigramAssocMeasures()
finder = BigramCollocationFinder.from_words(tokens)
finder.ngram_fd.viewitems()
</code></pre>
","pandas, nlp, scikit-learn, nltk, text-mining","<p>If your data is like</p>
<pre><code>import pandas as pd
df = pd.DataFrame([
    'must watch. Good acting',
    'average movie. Bad acting',
    'good movie. Good acting',
    'pathetic. Avoid',
    'avoid'], columns=['description'])
</code></pre>
<p>You could use the <code>CountVectorizer</code> of the package <code>sklearn</code>:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
word_vectorizer = CountVectorizer(ngram_range=(1,2), analyzer='word')
sparse_matrix = word_vectorizer.fit_transform(df['description'])
frequencies = sum(sparse_matrix).toarray()[0]
pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])
</code></pre>
<p>Which gives you :</p>
<pre><code>                frequency
good            3
pathetic        1
average movie   1
movie bad       2
watch           1
good movie      1
watch good      3
good acting     2
must            1
movie good      2
pathetic avoid  1
bad acting      1
average         1
must watch      1
acting          1
bad             1
movie           1
avoid           1
</code></pre>
<p><strong>EDIT</strong></p>
<p><code>fit</code> will just &quot;train&quot; your vectorizer : it will split the words of your corpus and create a vocabulary with it. Then <code>transform</code> can take a new document and create vector of frequency based on the vectorizer vocabulary.</p>
<p>Here your training set is your output set, so you can do both at the same time (<code>fit_transform</code>). Because you have 5 documents, it will create 5 vectors as a matrix. You want a global vector, so you have to make a <code>sum</code>.</p>
<p><strong>EDIT 2</strong></p>
<p>For big dataframes, you can speed up the frequencies computation by using:</p>
<pre><code>frequencies = sum(sparse_matrix).data
</code></pre>
<p>or</p>
<pre><code>frequencies = sparse_matrix.sum(axis=0).T
</code></pre>
",32,14,16958,2016-04-12 11:39:29,https://stackoverflow.com/questions/36572221/how-to-find-ngram-frequency-of-a-column-in-a-pandas-dataframe
Split multiple joined words with upper and lower case,"<p>I found a few questions related with this subject. However, I haven't found a solution that brings a specific idea about how to split joined words (in Spanish)  with upper and lower case, using regex for example.</p>

<p>I'm using PyPDF2 in order to extract text from several pdfs. The information is always at the same order.</p>

<p>After run a PyPDF2 code I get items like these:</p>

<pre><code>'MASCULINOFecha de NacimientoLugar de Nacimiento'
'CASADONivel Educativo'
</code></pre>

<p>In both cases, items are key words from pdf content. The output I'm trying to get should be like this (using the examples before):</p>

<pre><code>'MASCULINO'
'Fecha de Nacimiento'
'Lugar de Nacimiento'
'CASADO'
'Nivel Educativo'
</code></pre>

<p>I tried regex module to split specific patterns. This is my code so far:</p>

<pre><code>pdfFile = open('example.pdf', 'rb')
pdfReader = PyPDF2.PdfFileReader(pdfFile)
for page in range(0, pdfReader.getNumPages()):
    text = pdfReader.getPage(page).extractText()
    for line in text.split(':'):
        pattern = re.compile(r'([A-Z][a-z]+(?=\s[A-Z])(?:\s[A-Z][a-z]+)+)')
        result = re.findall(pattern, line)
        print result
</code></pre>

<p>It splits a few items, but no all of them.</p>

<p>Is there a better regex pattern to split those kind of words?</p>

<p>Any suggestions to solve the problem are appreacite. Thanks</p>
","python, regex, python-2.7, pdf, text-mining","<p>Try with <code>(?&lt;=[A-Za-z])(?=[A-Z][a-z])</code> and replace with <code>\n</code> or split.</p>
<p>This will detect the <code>zero-width</code> between upper or lower case AND upper or lower case. That seems to be logical separator here.</p>
<p><strong>Input</strong></p>
<pre><code>MASCULINO|Fecha de Nacimiento|Lugar de Nacimiento
CASADO|Nivel Educativo
</code></pre>
<p><code>|</code> denotes matched zero width.</p>
<p><strong>Output</strong></p>
<pre><code>MASCULINO
Fecha de Nacimiento
Lugar de Nacimiento
CASADO
Nivel Educativo
</code></pre>
<p><strong><a href=""https://regex101.com/r/zC7eC8/1"" rel=""nofollow noreferrer"">Regex101 Demo</a></strong></p>
<p>As Wiktor mentioned in comment</p>
<blockquote>
<p>You cannot use re.split with an empty string matching regex. Use the PyPi regex module if you need split.</p>
<p>There is no bug of this kind in re.sub, it is used as a workaround: you insert unused characters into the string with re.sub, and then re.split with this character. Just choose some char that is sure to be absent from the input (usually a control character, or a character from the unused Unicode range).</p>
</blockquote>
<p>Substituting <code>~</code> in matched zero width and splitting on <code>~</code> will give you array of results.</p>
<p><strong>Python Code:</strong></p>
<pre><code>import re
line='MASCULINOFecha de NacimientoLugar de Nacimiento CASADONivel Educativo'
result = re.sub('(?&lt;=[A-Za-z])(?=[A-Z][a-z])', '~', line,)
result = re.split('~', result)
print result
</code></pre>
<p><strong><a href=""https://ideone.com/nJQjIH"" rel=""nofollow noreferrer"">Ideone Demo</a></strong></p>
",1,0,1115,2016-04-13 05:59:15,https://stackoverflow.com/questions/36589797/split-multiple-joined-words-with-upper-and-lower-case
How to extract rtf tables,"<p>I have an rtf file. It has lots of tables in it. I have been trying to use java (POI and tika) to extract the tables. This is easy enough in a .doc where the tables are defined as such. However in a rtf file there doesn't seem to be any 'this is a table' tag as part of the meta data. Does anyone know what the best strategy is for extracting a table from such a file? Would converting it to another file format help. Any clues for me to look up?</p>
","java, text, rtf, text-mining, text-extraction","<p>Thanks hexin for your answer. In the end I was able to use Tika by using the TXTParser and then putting all the segments between bold tags(which is how my tables are separated) into an arraylist. I had to use the tab seperators to define tables from there.
Here is the code without the bit to extract the tables based on tabs (still working on it):</p>

<pre><code>import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.html.HtmlParser;
import org.apache.tika.parser.rtf.RTFParser;
import org.apache.tika.parser.txt.TXTParser;
import org.apache.tika.sax.BodyContentHandler;
import org.xml.sax.SAXException;


public class TextParser {
public static void main(final String[] args) throws IOException,TikaException{
 //detecting the file type
 BodyContentHandler handler = new BodyContentHandler(-1);
 Metadata metadata = new Metadata();

 FileInputStream inputstream = new FileInputStream(new File(""/Users/mydoc.rtf""));
 ParseContext pcontext = new ParseContext();

 //Text document parser
 TXTParser TXTParser = new TXTParser();
 try {
     TXTParser.parse(inputstream, handler, metadata,pcontext);

} catch (SAXException e) {

    e.printStackTrace();
} 
 String s=handler.toString();

Pattern pattern = Pattern.compile(""(\\\\b\\\\f1\\\\fs24.+?\\\\par .+?)\\\\b\\\\f1\\\\fs24.*?\\{\\\\"",Pattern.DOTALL);

Matcher matcher = pattern.matcher(s);
ArrayList&lt;String&gt; arr= new ArrayList&lt;String&gt;();

while (matcher.find()) {
       arr.add(matcher.group(1));
     }

 for(String name : arr){
     System.out.println(""The array number is: ""+arr.indexOf(name)+"" \n\n ""+name);
 }

 }
}
</code></pre>
",2,3,2460,2016-04-16 09:16:30,https://stackoverflow.com/questions/36662323/how-to-extract-rtf-tables
F-Regression Feature Selection Using Scipy Sparse Arrays,"<p>I'm trying to do some feature selection on text features for a regression problem. Currently, the training set has ~200K features - way too many. I want to use some of the feature selection tools within scikit-learn, but I'm having issues working with scipy sparse matrices, particularly when trying to pass in the <code>f_regression</code> scoring function to the <code>SelectKBest</code> transformer.</p>

<p>It appears that the <code>f_regression</code> scoring function takes as arguments an <code>X</code> feature matrix, a <code>y</code> response vector, and an optional <code>center</code> argument, which is set to True by default. I believe what would solve the problem is if I could pass <code>f_regression</code> with <code>center=False</code> to the <code>SelectKBest</code> transformer, however if I try something like:</p>

<pre><code>f_regressor = f_regression(X, y, center=False)
feature_selector = SelectKBest(f_regressor, k=k)
selected_features = feature_selector.fit_transform(X, y)
</code></pre>

<p>I get an error stating that the scoring function is not callable. I'm assuming this is because when I initialize it as <code>f_regress</code>, it immediately returns p-values and f-scores for the features.</p>

<p>Further, in the source code for the <code>SelectKBest</code> transformer, it doesn't look like the fit function does any checking for this <code>center</code> argument, so I don't see any straightforward way to pass this scoring function with <code>center=False</code> to the transformer:</p>

<pre><code># Abbreviated from the sklearn source
def fit(self, X, y):
    X, y = check_X_y(X, y, ['csr', 'csc'])

    # Error I've been getting when instantiating the f_regressor - not callable
    if not callable(self.score_func):
        raise TypeError(""The score function should be a callable, %s (%s) ""
                        ""was passed.""
                        % (self.score_func, type(self.score_func)))

    self._check_params(X, y)

    """"""Score func gets called here - only on X and y, assuming center=True.
    Maybe some argument checking could happen here in the future?
    Not sure if `center` argument could be passed as attribute via
    the constructor?
    """"""

    score_func_ret = self.score_func(X, y)
    if isinstance(score_func_ret, (list, tuple)):
        self.scores_, self.pvalues_ = score_func_ret
        self.pvalues_ = np.asarray(self.pvalues_)
    else:
        self.scores_ = score_func_ret
        self.pvalues_ = None

    self.scores_ = np.asarray(self.scores_)

    return self
</code></pre>

<p>If anyone has a workaround for this in the near future, that would be greatly appreciated. Thanks in advance for reading.</p>
","python, scipy, scikit-learn, sparse-matrix, text-mining","<p>See comment from @hpaul above. Using <code>functools</code> library and using the <code>.partial()</code> method to override the default arguments works great. Something like:</p>

<pre><code>f_regress = functools.partial(f_regression, center=False)
feature_selector = SelectKBest(f_regress, k=k)
</code></pre>

<p>and then use as normal.</p>
",0,0,463,2016-04-17 16:21:28,https://stackoverflow.com/questions/36679050/f-regression-feature-selection-using-scipy-sparse-arrays
How to preserve the original structure of a word while textmining,"<p>I would like to create a list of words that appear at least two times based on a specific web page.
I succeeded to get the data and to get a list with count per each word but 
I need to retain the words that have  an Upper case to stay this way.Right now the code produces the words list with lower case only.
For example, the word ""Miami"" turns into ""miami"" while I need it as ""Miami"".</p>

<p>How can I get the words in their original structure ? </p>

<p>Attached is the code :</p>

<pre><code>library(XML)
web_page &lt;- htmlTreeParse(""http://www.larryslist.com/artmarket/the-talks/dennis-scholls-multiple-roles-from-collecting-art-to-winning-emmy-awards/""
                          ,useInternal = TRUE)

doctext = unlist(xpathApply(web_page, '//p', xmlValue))
doctext = gsub('\\n', ' ', doctext)
doctext = paste(doctext, collapse = ' ')

library(tm)
SampCrps&lt;- Corpus(VectorSource(doctext))
corp &lt;- tm_map(SampCrps, PlainTextDocument)

oz &lt;- tm_map(corp, removePunctuation, preserve_intra_word_dashes = FALSE) # remove punctuation
oz &lt;- tm_map(corp, removeWords, stopwords(""english"")) # remove stopwords
dtm &lt;-DocumentTermMatrix(oz)

findFreqTerms(dtm,2) # words that apear at least 2 times
dtmMatrix &lt;- as.matrix(dtm) 
wordsFreq &lt;- colSums(dtmMatrix)
wordsFreq &lt;- sort(wordsFreq, decreasing=TRUE)
head(wordsFreq)
wordsFreq &lt;-  as.data.frame(wordsFreq)
wordsFreq &lt;- data.frame(word = rownames(wordsFreq), count = wordsFreq, row.names = NULL)
head(wordsFreq,50)
</code></pre>

<p>The same problem occurs when I use this lines of code to get a three word ngram:</p>

<pre><code>library(RWeka)
BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm &lt;- TermDocumentMatrix(oz, control = list(tokenize = BigramTokenizer))
inspect(tdm)
</code></pre>
","r, text-mining, tm","<p>The problem is that by default, there is an option in <code>DocumentTermMatrix()</code> that lowercases your terms.  Turn this off and you will preserve case.</p>

<pre><code>dtm &lt;- DocumentTermMatrix(oz, control = list(tolower = FALSE))
colnames(dtm)[grep("".iami"", colnames(dtm))]
## [1] ""Miami""   ""Miami,""  ""Miami.""  ""Miami’s""
</code></pre>

<p>Here's another way to do it using the <strong>quanteda</strong> package, might be more straightforward:</p>

<pre><code>require(quanteda)
# straight from text to the matrix
dfmMatrix &lt;- dfm(doctext, removeHyphens = TRUE, toLower = FALSE, 
                 ignoredFeatures = stopwords(""english""), verbose = FALSE)
# gets frequency counts, sorted in descending order of total term frequency
termfreqs &lt;- topfeatures(dfmMatrix, n = nfeature(dfmMatrix))
# remove those with frequency &lt; 2
termfreqs &lt;- termfreqs[termfreqs &gt;= 2]
head(termfreqs, 20)
##      art            I      artists   collecting         work           We   collection   collectors 
##       35           29           19           17           15           14           13           12 
##     What contemporary          The        world           us           It        Miami          one 
##       11           10           10           10           10            9            9            8 
##   always         many         make          Art 
##        8            8            8            7 
</code></pre>

<p>We can see that the case for ""Miami"" (e.g.) is preserved:</p>

<pre><code>termfreqs[grep("".iami"", names(termfreqs))]
## Miami Miami’s 
##     9       2 
</code></pre>
",2,1,98,2016-04-19 14:17:27,https://stackoverflow.com/questions/36721064/how-to-preserve-the-original-structure-of-a-word-while-textmining
Longest line in text dataset,"<p>I am looking for a way to find the length of the longest line in a text file. </p>

<p>E.g. consider a simple dataset from the <code>tm</code> package.</p>

<pre><code>install.packages(""tm"")
library(tm)
txt &lt;- system.file(""texts"", ""txt"", package = ""tm"") 

ovid &lt;- VCorpus(DirSource(txt, encoding = ""UTF-8""), readerControl = 
list(language = ""lat""))

length(ovid)
[1] 5
</code></pre>

<p><code>ovid</code> is composed by five documents each one composed by a character vector of <code>n</code> elements (from 16 to 18), between which I would like to identify the longest. 
I found documentation for <a href=""https://stackoverflow.com/questions/1292630/how-to-open-a-file-and-find-the-longest-length-of-a-line-and-then-print-it-out"">python</a>, <a href=""https://stackoverflow.com/questions/33657892/deleting-longest-line-from-txt"">C#</a> and for <a href=""https://stackoverflow.com/questions/1655372/longest-line-in-a-file"">bash shell</a> but, surprisingly, I did not find anything with R. Because of that, my attempts were quite naive, with:</p>

<pre><code>max(nchar(ovid))
[1] 5410
max(length(ovid))
[1] 5
</code></pre>
","r, text-mining, tm","<p>Actually it's the <em>fourth text</em> which is the longest, once we remove the padding from whitespace.  Here's how.  Note that a lot of this comes from the difficulty of getting texts out of a <strong>tm</strong> (V)Corpus object, which has been asked (several times) before, for instance <a href=""https://stackoverflow.com/questions/30435054/how-to-show-corpus-text-in-r-tm-package"">here</a>.</p>

<p>Note that I am interpreting your question about ""lines"" as referring to the five documents, which are more than five lines each, but consist of multiple lines (between 16 and 18 length character vectors each).  I hope I have interpreted this correctly.</p>

<pre><code>texts &lt;- sapply(ovid$content, ""[["", ""content"")
str(texts)
## List of 5
## $ : chr [1:16] ""    Si quis in hoc artem populo non novit amandi,"" ""         hoc legat et lecto carmine doctus amet."" ""    arte citae veloque rates remoque moventur,"" ""         arte leves currus: arte regendus amor."" ...
## $ : chr [1:17] ""    quas Hector sensurus erat, poscente magistro"" ""         verberibus iussas praebuit ille manus."" ""    Aeacidae Chiron, ego sum praeceptor Amoris:"" ""         saevus uterque puer, natus uterque dea."" ...
## $ : chr [1:17] ""    vera canam: coeptis, mater Amoris, ades!"" ""    este procul, vittae tenues, insigne pudoris,"" ""         quaeque tegis medios, instita longa, pedes."" ""    nos venerem tutam concessaque furta canemus,"" ...
## $ : chr [1:17] ""    scit bene venator, cervis ubi retia tendat,"" ""         scit bene, qua frendens valle moretur aper;"" ""    aucupibus noti frutices; qui sustinet hamos,"" ""         novit quae multo pisce natentur aquae:"" ...
## $ : chr [1:18] ""    mater in Aeneae constitit urbe sui."" ""    seu caperis primis et adhuc crescentibus annis,"" ""         ante oculos veniet vera puella tuos:"" ""    sive cupis iuvenem, iuvenes tibi mille placebunt."" ...
</code></pre>

<p>So here we have extracted the texts, but they are on multiple lines represented by one element each of the character vectors that each ""document"" comprises, and because they are verses, there is variable white space padding at the beginning and end of some of these elements.  Let's trim these and just leave the text, using <strong>stringi</strong>'s <code>stri_trim_both</code> function.</p>

<pre><code># need to trim leading and trailing whitespace
texts &lt;- lapply(texts, stringi::stri_trim_both)
## texts[1]
## [[1]]
## [1] ""Si quis in hoc artem populo non novit amandi,""     ""hoc legat et lecto carmine doctus amet.""          
## [3] ""arte citae veloque rates remoque moventur,""        ""arte leves currus: arte regendus amor.""           
## [5] """"                                                  ""curribus Automedon lentisque erat aptus habenis,"" 
## [7] ""Tiphys in Haemonia puppe magister erat:""           ""me Venus artificem tenero praefecit Amori;""       
## [9] ""Tiphys et Automedon dicar Amoris ego.""             ""ille quidem ferus est et qui mihi saepe repugnet:""
## [11] """"                                                  ""sed puer est, aetas mollis et apta regi.""         
## [13] ""Phillyrides puerum cithara perfecit Achillem,""     ""atque animos placida contudit arte feros.""        
## [15] ""qui totiens socios, totiens exterruit hostes,""     ""creditur annosum pertimuisse senem.""              

# now paste them together to make a single character vector of the five documents
texts &lt;- sapply(texts, paste, collapse = ""\n"")
str(texts)
##  chr [1:5] ""Si quis in hoc artem populo non novit amandi,\nhoc legat et lecto carmine doctus amet.\narte citae veloque rates remoque movent""| __truncated__ ...
cat(texts[1])
## Si quis in hoc artem populo non novit amandi,
## hoc legat et lecto carmine doctus amet.
## arte citae veloque rates remoque moventur,
## arte leves currus: arte regendus amor.
## 
## curribus Automedon lentisque erat aptus habenis,
## Tiphys in Haemonia puppe magister erat:
## me Venus artificem tenero praefecit Amori;
## Tiphys et Automedon dicar Amoris ego.
## ille quidem ferus est et qui mihi saepe repugnet:
##     
## sed puer est, aetas mollis et apta regi.
## Phillyrides puerum cithara perfecit Achillem,
## atque animos placida contudit arte feros.
## qui totiens socios, totiens exterruit hostes,
## creditur annosum pertimuisse senem.
</code></pre>

<p>That's looking more like it.  Now we can figure out which was longest.</p>

<pre><code>nchar(texts)
## [1] 600 621 644 668 622
which.max(nchar(texts))
## [1] 4
</code></pre>
",1,2,1379,2016-04-20 08:38:21,https://stackoverflow.com/questions/36738246/longest-line-in-text-dataset
non-standard characters cause program to end,"<p>I am learning python for data mining and I have a text file that contains a list of world cities and their coordinates. With my code, I am trying to find the coordinates of a list of cities. Everything works well until there is a city name with non-standard characters. I expect the program will skip that name and move to the next, but it terminates. How will I make the program skip names it cannot find and continue to the next?</p>

<pre><code>lst = ['Paris', 'London', 'Helsinki', 'Amsterdam', 'Sant Julià de Lòria',
       'New York', 'Dublin']
source = 'world.txt'
fh = open(source)
n = 0
for line in fh:
    line.rstrip()

    if lst[n] not in line:
        continue
    else:
        co = line.split(',')
        print lst[n], 'Lat: ', co[5], 'Long: ', co[6]

        if n &lt; (len(lst)-1):
            n = n + 1
        else:
            break
</code></pre>

<p>The outcome of this run is:</p>

<pre class=""lang-none prettyprint-override""><code>&gt;&gt;&gt;
Paris Lat:  33.180704 Long:  67.470836

London Lat:  -11.758217 Long:  17.084013

Helsinki Lat:  60.175556 Long:  24.934167

Amsterdam Lat:  6.25 Long:  -57.5166667

&gt;&gt;&gt;
</code></pre>
","python, python-2.7, text-mining","<p>Your code has a number of issues. The following fixes most, if not all, of them — and should never terminate when a city isn't found.</p>

<pre><code># -*- coding: iso-8859-1 -*-
from __future__ import print_function

cities = ['Paris', 'London', 'Helsinki', 'Amsterdam', 'Sant Julià de Lòria', 'New York',
          'Dublin']
SOURCE = 'world.txt'

for city in cities:
    with open(SOURCE) as fh:
        for line in fh:
            if city in line:
                fields = line.split(',')
                print(fields[0], 'Lat: ', fields[5], 'Long: ', fields[6])
                break
</code></pre>
",0,-2,46,2016-04-23 15:56:53,https://stackoverflow.com/questions/36813047/non-standard-characters-cause-program-to-end
Assigning weights to different features in R,"<p>Is it possible to assign weights to different features before formulating a DFM in R?</p>

<p>Consider this example in R</p>

<p><code>str=""apple is better than banana""
mydfm=dfm(str, ignoredFeatures = stopwords(""english""), verbose = FALSE)</code></p>

<p>DFM mydfm looks like:</p>

<pre><code>docs apple better banana
text1  1      1     1
</code></pre>

<p>But, I want to assign weights(apple:5, banana:3) beforehand, so that DFM mydfm looks like:</p>

<pre><code>docs apple better banana
text1  5      1     3
</code></pre>
","r, text-mining, tm, quanteda","<p>I don't think so, however you can easily do it afterwards: </p>

<pre><code>library(quanteda)
str &lt;- ""apple is better than banana""
mydfm &lt;- dfm(str, ignoredFeatures = stopwords(""english""), verbose = FALSE)
idx &lt;- which(names(weights) %in% colnames(mydfm))
mydfm[, names(weights)[idx]] &lt;-  mydfm[, names(weights)[idx]] %*% diag(weights[idx])
mydfm
# 1 x 3 sparse Matrix of class ""dgCMatrix""
#        features
# docs    apple better banana
#   text1     5      1      3
</code></pre>
",1,3,806,2016-04-23 20:17:46,https://stackoverflow.com/questions/36815926/assigning-weights-to-different-features-in-r
Uploading many files in Shiny,"<p>I am developing an app that helps to organize and visualize many PDF documents by topic/theme. I can upload and read a single PDF but I have difficulty in reading multiple PDF documents.</p>

<p>For single PDF document:</p>

<p>ui.R</p>

<pre><code>  ---
  fileInput('file1', 'Choose PDF File', accept=c('.pdf'))

 ---
</code></pre>

<p>server.R</p>

<pre><code>   --------

   library(pdftools)

   -------


 mypdf&lt;-reactive({

   inFile &lt;- input$file1

   if (is.null(inFile)){
  return(NULL)
  }else{
  pdf_text(inFile$datapath)

   }

  })
</code></pre>

<hr>

<p>To upload multiple PDF files, I have to use multiple = TRUE in the ui.R portion of the code, but how can I read in all the uploaded files?</p>
","r, pdf, shiny, visualization, text-mining","<p>The uploaded files can be read in a for loop like this </p>

<pre><code>for(i in 1:length(input$files[,1])){
  lst[[i]] &lt;- read.csv(input$files[[i, 'datapath']])
}
</code></pre>

<p>This is an example for CSV files but you can do the same for pdf files.</p>
",6,5,12188,2016-04-25 20:08:54,https://stackoverflow.com/questions/36850114/uploading-many-files-in-shiny
How detect foreign words in Corpus?,"<p>Suppose I am parsing an english corpus with the <code>tm</code> package, and I do the usual cleaning steps.</p>

<pre><code>library(tm)
data(""crude"")
corpus &lt;- Corpus(crude)

corpus &lt;- tm_map(corpus, content_transformer(tolower))
corpus &lt;- tm_map(corpus, content_transformer(removeWords)) stopwords(""english""))
corpus &lt;- tm_map(corpus, stripWhitespace)
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, stemDocument)
corpus &lt;- tm_map(corpus, PlainTextDocument)

# text matrices
tdm &lt;- TermDocumentMatrix(corpus)
dtm&lt;- DocumentTermMatrix(corpus)
</code></pre>

<p>How do I identify the words written in a different language than the one of the corpus? A similar problem is faced with <a href=""https://stackoverflow.com/questions/27191457/detecting-foreign-words"">Python here</a>, but my research did not produces interesting results. </p>
","r, text-mining","<p>This is not a complete solution, but I feel like it might help. I recently had to do something similar where I had to remove words from a corpus with Chinese characters. I ended up using a custom transformation with a regex to remove anything with a non a-z 0-9 character in it.</p>

<pre><code>corpus &lt;- tm_map(corpus, content_transformer(function(s){
  gsub(pattern = '[^a-zA-Z0-9\\s]+',
       x = s,
       replacement = "" "",
       ignore.case = TRUE,
       perl = TRUE)
}))
</code></pre>

<p>For example, if there is a Chinese word in there, it gets removed.</p>

<pre><code>gsub(pattern = '[^a-zA-Z0-9\\s]+',
     x = 'English 象形字 Chinese',
     replacement = """",
     ignore.case = TRUE,
     perl = TRUE)
</code></pre>

<p>Output: ""English  Chinese""</p>

<p>It's trickier if you are trying to remove words from a language like Spanish because some letters have an accent while others don't. For example, this doesn't work completely, but maybe it's a start.</p>

<pre><code>gsub(pattern = '[a-zA-Z0-9]+[^a-zA-Z0-9\\s]+[a-zA-Z0-9]+',
     x = 'El jalapeño es caliente',
     replacement = """",
     ignore.case = TRUE,
     perl = TRUE)
</code></pre>

<p>Output: ""El  es caliente""</p>

<p>Hope this helps!</p>
",1,0,714,2016-04-29 12:44:24,https://stackoverflow.com/questions/36938703/how-detect-foreign-words-in-corpus
Create new column if column contains on or more of multiple strings from a vector,"<p>I am trying to classify hospitality data.</p>

<p>My dataset looks something like the dataset below and is approx. 400000 rows long.</p>

<pre><code>dataset&lt;-data.frame(id=c(1001:1005), Role_title = c(""Head Chef"",""Nurse"",
        ""Latin America Travel Sales Consultants \xfc\xbe\x8c\xb6\x84\xbcK OTE \xfc\xbe\x8c\xb6\x84\xbcK"",""Cooks Wanted"",""Calling all waiters""))
</code></pre>

<p>The terms I am looking for are stems as well as full strings and the vector is approx 100 rows long.</p>

<pre><code>terms=c(""chef"",""cook"",""wait"")
</code></pre>

<p>I would like to create a new column 'Contains terms' which places a 1 into 'Contains terms' if the one or more strings from the vector is matched or partially matched  Row 'Role_title', and a 0 if it does not, so that the dataset will look like the below.</p>

<pre><code>dataset&lt;-data.frame(id=c(1001:1005), Role_title = c(""Head Chef"",""Nurse"",
      ""Acting Director Sales"",""Cooks Wanted"",""Calling all waiters""),
        Contains_terms=c(1,0,0,1,1))
terms=c(""chef"",""cook"",""wait"")
</code></pre>
","r, string, text, dplyr, text-mining","<p>We can use <code>stri_detect</code> from <code>stringi</code> to return a logical vector after <code>paste</code>ing the 'terms' vector to create the <code>pattern</code>, convert the logical <code>vector</code> to binary by wrapping with <code>as.integer</code></p>

<pre><code>library(stringi)
transform(dataset, Contains_terms = as.integer(stri_detect(toupper(Role_title), 
         regex=paste(toupper(terms), collapse=""|""))))
#    id                Role_title Contains_terms
#1 1001                 Head Chef              1
#2 1002                     Nurse              0
#3 1003 Acting     Director Sales              0
#4 1004              Cooks Wanted              1
#5 1005       Calling all waiters              1
</code></pre>

<hr>

<p>Or another option with <code>grep</code> (also if there are many elements in 'terms'</p>

<pre><code>as.integer(Reduce(`|`, lapply(toupper(terms), `grepl`, 
                 x=toupper(dataset$Role_title))))
#[1] 1 0 0 1 1
</code></pre>

<hr>

<p>If we want to do processing for elements in ""Role_title"" that have only ""ASCII"" characters (based on the updated dataset in the OP's post)</p>

<pre><code>i1 &lt;- stri_enc_mark(dataset$Role_title)==""ASCII""
i1
#[1]  TRUE  TRUE FALSE  TRUE  TRUE
dataset$Contains_terms[i1] &lt;-  as.integer(Reduce(`|`, lapply(toupper(terms), `grepl`, 
                 x=toupper(dataset$Role_title[i1]))))
dataset$Contains_terms
#[1]  1  0 NA  1  1
</code></pre>
",1,1,107,2016-05-01 05:49:45,https://stackoverflow.com/questions/36963515/create-new-column-if-column-contains-on-or-more-of-multiple-strings-from-a-vecto
Stemming words using tm package in R does not work properly?,"<p>I am doing some text mining (PCA, HC, K-Means) and so far I have managed to code everything right. However, there is a small flaw I'd like to fix.</p>

<p>When I try to stem my Corpus it does not work properly as there are different words with the same radical which aren't identified in the correct way. 
These are the words I am particularly interested in (it's in Spanish and they mean ""kids"" or related):</p>

<pre><code>niñera, niños, niñas, niña, niño
</code></pre>

<p>But when I run the code I get that these words are still the same except for </p>

<pre><code>niña, niño --&gt; niñ 
</code></pre>

<p>But the other remain the same so I end up only stemming for niña/niño but not for the others.</p>

<p>This is my code for creating the corpus:</p>

<pre><code>corp &lt;- Corpus(DataframeSource(data.frame(x$service_name)))
docs &lt;- tm_map(corp, removePunctuation)
docs &lt;- tm_map(docs, removeNumbers) 
docs &lt;- tm_map(docs, tolower)
docs &lt;- tm_map(docs, removeWords, stopwords(""spanish""))
docs &lt;- tm_map(docs, stemDocument, language = ""spanish"") 
docs &lt;- tm_map(docs, PlainTextDocument) 
dtm &lt;- DocumentTermMatrix(docs)   
dtm  
</code></pre>

<p>I'd really appreciate some suggestions!
Thank you</p>
","r, text-mining, corpus","<p>It seems that the stemming transform can only be applied to PlainTextDocument types.  See <code>? stemDocument</code>. </p>

<pre><code>sp.corpus = Corpus(VectorSource(c(""la niñera. los niños. las niñas. la niña. el niño."")))
docs &lt;- tm_map(sp.corpus, removePunctuation)
docs &lt;- tm_map(docs, removeNumbers) 
docs &lt;- tm_map(docs, tolower)
docs &lt;- tm_map(docs, removeWords, stopwords(""spanish""))
docs &lt;- tm_map(docs, PlainTextDocument)  # needs to come before stemming
docs &lt;- tm_map(docs, stemDocument, ""spanish"")
print(docs[[1]]$content)

# "" niñer  niñ  niñ  niñ  niñ""
</code></pre>

<p>versus</p>

<pre><code># WRONG
sp.corpus = Corpus(VectorSource(c(""la niñera. los niños. las niñas. la niña. el niño."")))
docs &lt;- tm_map(sp.corpus, removePunctuation)
docs &lt;- tm_map(docs, removeNumbers) 
docs &lt;- tm_map(docs, tolower)
docs &lt;- tm_map(docs, removeWords, stopwords(""spanish""))
docs &lt;- tm_map(docs, stemDocument, ""spanish"")  # WRONG: apply PlainTextDocument first
docs &lt;- tm_map(docs, PlainTextDocument)  
print(docs[[1]]$content)

# "" niñera  niños  niñas  niña  niñ""
</code></pre>

<p>In my opinion, this detail is not obvious and it'd be nice to get at least a warning when stemDocument is invoked on a non-PlainTextDocument.</p>
",18,6,7232,2016-05-01 14:08:24,https://stackoverflow.com/questions/36967573/stemming-words-using-tm-package-in-r-does-not-work-properly
Alternatives for wget giving &#39;ERROR 403: Forbidden&#39;,"<p>I'm trying to get text from multiple Pubmed papers using wget, but seems NCBI website don't allow this. Any alternatives?</p>

<pre><code>Bernardos-MacBook-Pro:pangenome_papers_pubmed_result bernardo$ wget -i ./url.txt
--2016-05-04 10:49:34--  http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4560400/
Resolving www.ncbi.nlm.nih.gov... 130.14.29.110, 2607:f220:41e:4290::110
Connecting to www.ncbi.nlm.nih.gov|130.14.29.110|:80... connected.
HTTP request sent, awaiting response... 403 Forbidden
2016-05-04 10:49:34 ERROR 403: Forbidden.

--2016-05-04 10:49:34--  http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4547177/
Reusing existing connection to www.ncbi.nlm.nih.gov:80.
HTTP request sent, awaiting response... 403 Forbidden
2016-05-04 10:49:34 ERROR 403: Forbidden.
</code></pre>
","web-scraping, wget, text-mining","<p>Set custom User Agent like this:</p>

<pre><code>wget --user-agent=""Mozilla"" http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4560400/
</code></pre>
",36,15,61026,2016-05-04 07:59:27,https://stackoverflow.com/questions/37021799/alternatives-for-wget-giving-error-403-forbidden
Extract text after a symbol in R,"<pre><code>sample1 = read.csv(""pirate.csv"")
sample1[,7] 
[1] &gt;&gt;xyz&gt;&gt;hello&gt;&gt;mate 1
[2] &gt;&gt;xyz&gt;&gt;hello&gt;&gt;mate 2
[3] &gt;&gt;xyz&gt;&gt;mate 3
[4] &gt;&gt;xyz&gt;&gt;mate 4
[5] &gt;&gt;xyz&gt;&gt;hello&gt;&gt;mate 5
[6] &gt;&gt;xyz&gt;&gt;hello&gt;&gt;mate 6
</code></pre>

<p>I have to extract and create an array which contains all the words after last <code>&gt;&gt;</code>.</p>

<p>How to do this?</p>

<p>Also, How can I extract (a) o qwerty, (b) mate1 and (c) pirate1 in different variables from the following string</p>

<pre><code>p= '&gt;&gt;xyz- o qwerty&gt;&gt;hello&gt;&gt;mate1&gt;&gt;sole pirate1'
</code></pre>

<p>Thanks</p>
","regex, r, text-mining, extract","<pre><code>x &lt;- c('&gt;&gt;xyz&gt;&gt;hello&gt;&gt;mate 1', '&gt;&gt;xyz&gt;&gt;hello&gt;&gt;mate 2', '&gt;&gt;xyz&gt;&gt;mate 3', ' &gt;&gt;xyz&gt;&gt;mate 4' ,'&gt;&gt;xyz&gt;&gt;hello&gt;&gt;mate 5')
sub('.*&gt;&gt;', '', x)
#[1] ""mate 1"" ""mate 2"" ""mate 3"" ""mate 4"" ""mate 5""
</code></pre>
",34,22,52935,2016-05-05 12:59:32,https://stackoverflow.com/questions/37051288/extract-text-after-a-symbol-in-r
powershell why can&#39;t I export this object to csv,"<p>I'm trying to count the number of words per PDF file in a source folder and export the name and wordcount to a csv. But my output csv seems to count the number of PDFs (123) although the content of my object seems right.</p>
<h1>Snippet</h1>
<pre><code>$source = 'C:\Data\SCRIPTS\R\TextMining\PDFs'
$results= @{} 
Get-ChildItem -Path $source -Filter *.pdf -Recurse | ForEach-Object{
    $count = Get-Content $_.FullName | Measure-Object -Word
    $results.Add($_.FullName, $count.Words)}
$results 
Export-Csv C:\Data\SCRIPTS\R\TextMining\PageClustering\PDFs\PGs\PGs_WC.csv -InputObject $results -notypeinformation
</code></pre>
<p>I can display the filename and wordcount to the console but the pipe to csv comes out with errors.</p>
<h1>Output</h1>
<pre><code>IsReadOnly  IsFixedSize IsSynchronized          Keys                                                               Values                                       SyncRoot       Count
FALSE          FALSE        FALSE System.Collections.Hashtable+KeyCollection            System.Collections.Hashtable+ValueCollection    System.Object   123
</code></pre>
<p>I'm learning to use PS - what am I doing wrong?</p>
","powershell, export-to-csv, text-mining","<p>Please try following:</p>

<pre><code>$source = 'C:\Data\SCRIPTS\R\TextMining\PDFs'
$results= @()
Get-ChildItem -Path $source -Filter *.pdf -Recurse | ForEach-Object{
    $count = Get-Content $_.FullName | Measure-Object -Word
    $results += New-Object PSObject -Property @{
            'Name' = $_.FullName
            'Wert' = $count.Words
            }
   }
$results 
$results  | Export-Csv C:\Data\SCRIPTS\R\TextMining\PageClustering\PDFs\PGs\PGs_WC.csv  -notype
</code></pre>
",2,1,3206,2016-05-06 09:11:52,https://stackoverflow.com/questions/37068522/powershell-why-cant-i-export-this-object-to-csv
R: partial match dictionary terms using grep and tm package,"<p>Hi: I have a dictionary of negative terms that has been prepared by others.  I am not sure how they have gone about doing the stemming, but it looks like they have used something other than the Porter Stemer.   The dictionary has a wildcard character (*) that I think is supposed to enable a stemming to happen. But I don't know how to make use of that with grep() or the tm package in the R context, so I stripped it out hoping to find a way to grep the partial match. 
So the original dictionary looks like this</p>

<pre><code>#load libraries
library(tm)
#sample dictionary terms for polarize and outlaw
negative&lt;-c('polariz*', 'outlaw*')
#strip out wildcard
negative&lt;-gsub('*', '', negative)
#test corpus
test&lt;-c('polarize', 'polarizing', 'polarized', 'polarizes', 'outlaw', 'outlawed', 'outlaws')
#Here is how R's porter stemmer stems the text
stemDocument(test)
</code></pre>

<p>So, if I stemmed my corpus with R's stemmer, terms like 'outlaw' would be found in the dictionary, but it wouldn't match terms like 'polarized' and such because they would be stemmed differently than what is found in the dictionary. </p>

<p>So, what I would like to have is some way to have the tm package match only exact parts of each word.  So, without stemming my documents, I would like it to be able to pick out 'outlaw' in the term 'outlawing' and 'outlaws' and to pick out 'polariz' in 'polarized', 'polarizing and 'polarizes'.  Is this possible?</p>

<pre><code>#Define corpus
test.corp&lt;-Corpus(VectorSource(test))  
#make Document Term Matrix
dtm&lt;-documentTermMatrix(test.corp, control=list(dictionary=negative))
#inspect
inspect(dtm)
</code></pre>
","r, dictionary, text-mining, tm","<p>I haven't seen any <strong>tm</strong> answers, so here's one using the <strong>quanteda</strong> package as an alternative.  It allows you to use ""<a href=""https://en.wikipedia.org/wiki/Glob_(programming)"" rel=""nofollow"">glob</a>"" wildcard values in your dictionary entries, which is the default <code>valuetype</code> for <strong>quanteda's</strong> dictionary functions.  (See <code>?dictionary</code>.)  With this approach, you do not need to stem your text.</p>

<pre><code>library(quanteda)
packageVersion(""quanteda"")
## [1] ‘0.9.6.2’

# create a quanteda dictionary, essentially a named list
negative &lt;- dictionary(list(polariz = 'polariz*', outlaw = 'outlaw*'))
negative
## Dictionary object with 2 key entries.
##  - polariz: polariz*
##  - outlaw: outlaw*

test &lt;- c('polarize', 'polarizing', 'polarized', 'polarizes', 'outlaw', 'outlawed', 'outlaws')

dfm(test, dictionary = negative, valuetype = ""glob"", verbose = FALSE)
## Document-feature matrix of: 7 documents, 2 features.
## 7 x 2 sparse Matrix of class ""dfmSparse""
##        features
## docs    polariz outlaw
##   text1       1      0
##   text3       1      0
##   text2       1      0
##   text4       1      0
##   text5       0      1
##   text6       0      1
##   text7       0      1
</code></pre>
",1,1,923,2016-05-06 15:18:20,https://stackoverflow.com/questions/37075952/r-partial-match-dictionary-terms-using-grep-and-tm-package
Sentiment analysis Lexicon,"<p>i have created a corpus  and processed it using tm package, a snippet below </p>

<pre><code>cleanCorpus&lt;-function(corpus){

corpus.tmp &lt;- tm_map(corpus, content_transformer(tolower))
corpus.tmp &lt;- tm_map(corpus.tmp, removePunctuation)
corpus.tmp &lt;- tm_map(corpus.tmp, removeNumbers)
corpus.tmp &lt;- tm_map(corpus.tmp, removeWords,stopwords(""english""))
corpus.tmp &lt;- tm_map(corpus.tmp, stemDocument)
corpus.tmp &lt;- tm_map(corpus.tmp, stripWhitespace)

return(corpus.tmp)
}

myCorpus &lt;-Corpus(VectorSource(Data$body),readerControl =  list(reader=readPlain))

cln.corpus&lt;-cleanCorpus(myCorpus)
</code></pre>

<p>Now i am using the mpqa lexicon to get the total number of positive words and negative words in each document of the corpus. </p>

<p>so i have the list with me as </p>

<pre><code>pos.words &lt;- lexicon$word[lexicon$Polarity==""positive""]
neg.words &lt;- lexicon$word[lexicon$Polarity==""negative""] 
</code></pre>

<p>How should i go about comparing the content of each document with the positive and negative list and get the counts of both per document?
 i checked other posts on tm dictionaries but looks like the feature is withdrawn. </p>
","r, text-mining","<p>For example</p>

<pre><code>library(tm)
data(""crude"")
myCorpus &lt;- crude[1:2]
pos.words &lt;- c(""advantag"", ""easy"", ""cut"")
neg.words &lt;- c(""problem"", ""weak"", ""uncertain"")
weightSenti &lt;- structure(function (m) {
    m$v &lt;- rep(1, length(m$v))
    m$v[rownames(m) %in% neg.words] &lt;- m$v[rownames(m) %in% neg.words] * -1
    attr(m, ""weighting"") &lt;- c(""binarySenti"", ""binSenti"")
    m
}, class = c(""WeightFunction"", ""function""), name = ""binarySenti"", acronym = ""binSenti"")
tdm &lt;- TermDocumentMatrix(cln.corpus, control=list(weighting=weightSenti, dictionary=c(pos.words, neg.words)))
colSums(as.matrix(tdm))
# 127 144 
#   2  -2
</code></pre>
",1,0,923,2016-05-09 12:44:35,https://stackoverflow.com/questions/37116180/sentiment-analysis-lexicon
Remove special apostrophe in R,"<p>I am doing some text mining and I would like to remove the apostrophe "" from my text (delete it). I tried to use gsub as follow but it does not work </p>

<pre><code>text &lt;- ""\""branch""

removeSpecialChars &lt;- function(x){
     result &lt;- gsub('""',x)
     return(result)
}

without &lt;- removeSpecialChars(text)
</code></pre>

<p>The desired Output would be branch and not ""branch. Thanks for your help</p>

<p>EDIT to go further (i am trying to clean a text).</p>

<p>The Input is a list conatining a lot of different string. For example </p>

<pre><code>Input &lt;- list(c(""e"",""b"", ""stackoverflow"", ""\""branch""))

cleanCorpus &lt;- function(corpus){
  corpus.tmp &lt;- tm_map(corpus, removePunctuation,preserve_intra_word_dashes = TRUE)

  removeSpecialChars &lt;- function(x){
    result &lt;- gsub('""', """",x)
    return(result)
  }
  corpus.tmp &lt;- removeSpecialChars(corpus.tmp)

  corpus.tmp &lt;- tm_map(corpus.tmp, stripWhitespace)
  corpus.tmp &lt;- tm_map(corpus.tmp, content_transformer(tolower))
  corpus.tmp &lt;- tm_map(corpus.tmp, removeWords, stopwords(""english""))
  return(corpus.tmp)
}
result &lt;- cleanCorpus(Input)
</code></pre>
","r, text-mining, apostrophe","<p>We need to use the <code>replacement</code></p>
<pre><code>gsub('&quot;', &quot;&quot;, text)
#[1] &quot;branch&quot;
</code></pre>
<h3>data</h3>
<pre><code>text &lt;- &quot;\&quot;branch&quot;
</code></pre>
",3,2,3857,2016-05-12 09:19:35,https://stackoverflow.com/questions/37182513/remove-special-apostrophe-in-r
nltk sentence tokenizer gives AttributeError,"<p>I am very new to python and NLTK. 
One issue is baffling me: </p>

<p>When I do</p>

<pre><code>tokenized = custom_sent_tokenizer.tokenize(""some long text"")
</code></pre>

<p>It gives me perfect result. But when I change this hard coded string to a variable containing huge text, it gives me the error mentioned in subject viz: </p>

<pre><code>tokenized = custom_sent_tokenizer.tokenize(text)
...
AttributeError: 'list' object has no attribute 'abbrev_types'
</code></pre>

<p>Below is my full code: </p>

<pre><code>from __future__ import division
import urllib.request
import csv
import nltk
from string import punctuation
from nltk.corpus import stopwords
from nltk.tokenize import PunktSentenceTokenizer

comments = open(""CNPS_Comments.txt"").read()
comments_list = comments.split('\n')

custom_sent_tokenizer = PunktSentenceTokenizer(comments_list[:300])
##tokenized = custom_sent_tokenizer.tokenize(""some long text"")
text=""""
for comment in comments_list:
   text += comment

tokenized = custom_sent_tokenizer.tokenize(text)
def process_content():
  try:
    for i in tokenized[:5]:
        words = nltk.word_tokenize(i)
        tagged = nltk.pos_tag(words)
        print(tagged)

except Exception as e:
    print(str(e))


process_content()
</code></pre>

<p>I started with python today and there could be many things I am not doing effectively here.     </p>
","python, python-3.x, nltk, tokenize, text-mining","<p>The line that's giving you trouble is correct: That's how you're supposed to use the sentence tokenizer, with a single string as its argument. You're getting an error because you have created a monster :-)</p>

<p>The Punkt sentence tokenizer is based on an unsupervised algorithm: You give it a long text, and it figures out where the sentence boundaries must lie. But you have trained your tokenizer with a <em>list</em> of sentences (the first 300 elements in <code>comments_list</code>), which is incorrect. Somehow the tokenizer doesn't notice, and gives you something that errors out when you try to use it properly.</p>

<p>To fix the problem, train your tokenizer with a single string. You can best join a list of strings into one like this:</p>

<pre><code>tokenizer = PunktSentenceTokenizer("" "".join(comments_list[:300]))
</code></pre>

<p>PS. You must be wrong about it working successfully when you tokenized a literal string. Certainly there were other differences between the code that worked, and the code in your question.</p>
",4,0,1056,2016-05-12 12:52:34,https://stackoverflow.com/questions/37187500/nltk-sentence-tokenizer-gives-attributeerror
How can I create a term matrix that sums numeric values associated to each document?,"<p>I'm a bit new to R and tm so struggling with this exercise!</p>

<p>I have one <strong>description</strong> column with messy unstructured data containing words about the name, city and country of a customer. And another column with the amount of <strong>sold items</strong>.</p>

<pre><code>**Description   Sold Items**
Mrs White London UK 10
Mr Wolf London UK   20
Tania Maier Berlin Germany  10
Thomas Germany  30
Nick Forest Leeds UK    20
Silvio Verdi Italy Torino   10
Tom Cardiff UK  10
Mary House London   5
</code></pre>

<p>Using the tm package and documenttermmatrix, I'm able to break down each row into terms and get the frequency of each word (i.e. the number of customers with that word).</p>

<pre><code>         UK London  Germany …   Mary
Frequency   4   3   2   …   1
</code></pre>

<p>However, I would also like to sum the total amount of sold items.</p>

<p>The desired output should be:</p>

<pre><code>         UK London  Germany …   Mary
Frequency   4   3   2   …   1
Sum of Sold Items   60  35  40  …   5
</code></pre>

<p>How can I get to this result?</p>
","r, text-mining, tm, corpus","<p>Assuming you can get to the stage where you have the <code>Frequency</code> table:</p>

<pre><code>           UK London  Germany …   Mary
Frequency   4   3   2   …   1
</code></pre>

<p>and you can extract the words you can use an apply function with a <code>grep</code>. Here I will create a vector which represents your dictionary you extract from your <code>frequency</code> table:</p>

<pre><code>S_data&lt;-read.csv(""data.csv"",stringsAsFactors = F)

Words&lt;-c(""UK"",""London"",""Germany"",""Mary"")
</code></pre>

<p>Then use this in an apply as follows. This could be more efficiently done. But you will get the idea:</p>

<pre><code>string_rows&lt;-sapply(Words, function(x) grep(x,S_data$Description))

string_sum&lt;-unlist(lapply(string_rows, function(x) sum(S_data$Items[x])))
&gt; string_sum
     UK  London Germany    Mary 
     60      35      40       5 
</code></pre>

<p>Just bind this onto your <code>frequency</code> table</p>
",1,3,74,2016-05-13 12:37:57,https://stackoverflow.com/questions/37210268/how-can-i-create-a-term-matrix-that-sums-numeric-values-associated-to-each-docum
How to use stanford nlp library in java?,"<p>Does anyone know how to use a stanford nlp library for lemmatization. It is giving a maven framework style. However, I only just wanted to use in normal library. I have imported the nlp libraries. However, it is giving me a ClassNotFoundException. </p>

<blockquote>
  <p>java.lang.ClassNotFoundException: org.slf4j.LoggerFactory</p>
</blockquote>

<p>Any idea on what minimum libraries are required to add for this lemmatizer?</p>
","java, text-mining, lemmatization","<p>Just create java project using Maven.
And add following dependency:</p>

<p>For SBT:<code>""edu.stanford.nlp"" % ""stanford-corenlp"" % ""3.6.0"",
""edu.stanford.nlp"" % ""stanford-corenlp"" % ""3.6.0"" classifier ""models"",
""edu.stanford.nlp"" % ""stanford-parser"" % ""3.6.0""</code></p>

<p>Note: In your case use Maven. Above 3 dependency enough for core NLP.<br>
But for your Exception use :
Maven org.slf4j dependency or include org.slf4j jar in your project.</p>
",0,0,673,2016-05-16 05:00:04,https://stackoverflow.com/questions/37246970/how-to-use-stanford-nlp-library-in-java
set encoding for reading text files into tm Corpora,"<p>loading a bunch of documents using tm Corpus i need to specify encoding.</p>

<p>All documents are UTF-8 encoded. If openend via text editor content is ok but corpus contents is full of strange symbols (indicioâ., ‘sœs....)
Source text is in spanish. ES_es</p>

<pre><code>library(tm)
cname &lt;- file.path(""C:"", ""Users"", ""john"", ""Documents"", ""texts"")
docs &lt;- Corpus(DirSource(cname), encoding =""UTF-8"")

&gt; Error in Corpus(DirSource(cname), encoding = ""UTF-8"") : 
  unused argument (encoding = ""UTF-8"")
</code></pre>

<p>EDITED:</p>

<p>Getting  str(documents[1])  from corpus I've noticed:</p>

<p>.. ..$ language     : chr ""en""</p>

<p>How can I specify, for instance ""UTF-8"", ""Latin1"" or any other encoding to avoid strange symbols?</p>

<p>Regards</p>
","text, encoding, text-mining, tm, corpus","<p>From the ""C:"" it's clear you are using Windows, which assumes a Windows-1252 encoding (on most systems) rather than UTF-8.  You could try reading the files in as character and then setting <code>Encoding(myCharVector) &lt;- ""UTF-8""</code>.  If the input encoding was UTF-8 this should cause your system to recognise and display the UTF-8 characters properly.</p>

<p>Alternatively this will work, although it also makes <strong>tm</strong> unnecessary:</p>

<pre><code>require(quanteda)
docs &lt;- corpus(textfile(""C:/Users/john/Documents/texts/*.txt"", encoding = ""UTF-8""))
</code></pre>

<p>Then you can see the texts using for example:</p>

<pre><code>cat(texts(docs)[1:2])
</code></pre>

<p>They should have the encoding bit set and display properly.  Then if you prefer, you can get these into <strong>tm</strong> using:</p>

<pre><code>docsTM &lt;- Corpus(VectorSource(texts(docs)))
</code></pre>
",0,0,1411,2016-05-17 14:00:36,https://stackoverflow.com/questions/37278333/set-encoding-for-reading-text-files-into-tm-corpora
Word Association In R,"<p>I am searching for a solution/library or any function that finds the most frequent word associations within a paragraph. For example: </p>

<blockquote>
  <p>This tree gives red apple. Bananas are yellow. The apple I ate was red. </p>
</blockquote>

<p>In the above text, we should be able to get Association of each word with all other words in the sentence (after removing stop words and stemming). So lets say the above text gives association as:</p>

<p>tree - red      : 0.41
tree - apple    : 0.46
bananas - yellow: 0.30
apple - red     : 0.8</p>

<p>The most frequent two words repeated in the text are ""apple - red"" combination since both of the words occur in two sentences. </p>

<p>The two solutions I have tried are :</p>

<ol>
<li><p>findAssoc() of tm library: </p>

<pre><code>      Word AssociatedWord Association  
1    apple            red           1
2    apple            ate         0.5
3    apple           tree         0.5
4      red          apple           1
5      red            ate         0.5
6      red           tree         0.5
7      ate          apple         0.5 
8      ate            red         0.5  
9  bananas         yellow           1  
10    tree          apple         0.5 
11    tree            red         0.5
12  yellow        bananas           1
</code></pre>

<p>The result shown above is the output of text given above. Sentences are entered individually as it does not find association on a single line text.</p></li>
<li><p>A customized solution using most frequent n-grams: this is not feasible since it only checks consecutively occurring words. </p></li>
</ol>

<p>I am just looking for a solution that gives the most frequent word association. I can't break the text into multiple lines so would there be any solution of such kind?
Any help would be appreciated.</p>
","r, nlp, text-mining, tm","<p>It is not clear at all what you want. What do you mean by frequent words association on a single line of text? Association values require a metric, in findAssc() the metric is reflecting how many times 2 words appear in the same text. </p>

<p>When you have something like ""This tree gives red apple"" in a document the information you have is that tree-apple are both in the same document, that's it, and maybe that they are separated by 2 words, or something like that, what do you want as metric here? define one.</p>
",1,2,2589,2016-05-18 13:31:02,https://stackoverflow.com/questions/37301172/word-association-in-r
NLTK Find occurrences of a word within 5 words (left/right) of context words in a corpus,"<p>I used scrapy to crawl a website to get thousands of .txt files, each containing a text in natural language (description of a drug-induced experience). The name of each of these files is a unique number.
I also have a .csv file with metadata associated with each of these unique numbers (i.e. I have a column for text_number, and other columns for the metadata corresponding to this particular number). One of the category of metadata is a dosage number (in mg).</p>

<p>Here is what I'm trying to do:</p>

<ol>
<li><p>Find which .txt files contain an occurrence of a specific word ('self') within 5 words (left and right) of one of 100 specific context words (I have a precise list).</p></li>
<li><p>Get the average dosage number (from the metadata) of .txt files singled out in the first step, in order to compare it to the average dosage number of all .txt files.</p></li>
</ol>

<p>I really don't know how to proceed...</p>
","python, nltk, text-mining","<p>I think a regular expression might be a good solution to this.
They're fast, and you have a lot of data.
Not sure what the best way to do this would be, but here's one solution.</p>

<p>Say your target word ('self') and your list of context words looks like this:</p>

<pre><code>target_word = 'self'
context_words = ['one', 'hundred', 'context', 'words']
#mine is much shorter than yours! ;)
</code></pre>

<p>Then you can create a regular expression that expects words to be separated by spaces.
I used one pattern for when the context word is before and one for when the context word is after, then combined them with an or ('|').
Not sure if that's necessary, just couldn't easily think of another way.</p>

<pre><code>import re
matches_up_to_4_words = '( [^ ]*){0,4} ?'
matches_context_word = '(' + '|'.join(context_words) + ')'
matches_target_word = target_word
context_before = matches_context_word + matches_up_to_4_words + matches_target_word
context_after = matches_target_word + matches_up_to_4_words + matches_context_word
pattern = re.compile('(' + context_before + '|' + context_after + ')')

matching_metadata = []
for filename in filenames:
    filestring = open(filename, 'rb').read()
    ## you can tokenize here for better word segmentation
    ## http://www.nltk.org/api/nltk.tokenize.html 
    if re.search(pattern, filestring):
        print ""the target word appeared near a context word""
        ## get the metadata
        metadata = get_the_metadata(filename, filestring)
        matching_metadata.append(metadata)
</code></pre>

<p>Then you have the metadata stored that you can work with.</p>
",0,2,3679,2016-05-19 18:47:21,https://stackoverflow.com/questions/37331708/nltk-find-occurrences-of-a-word-within-5-words-left-right-of-context-words-in
R wordstem chopping words too much,"<p>I'll show by example:</p>

<pre><code>library(data.table)
dt &lt;- data.table(words = c(""finance"", ""financial"", ""business""),
                  freq = c(123, 5, 4589))
dt &lt;- dt[, words := SnowballC::wordStem(words, language = ""english"")]
View(dt)

words    freq
financ    123
financi    5
busi     4589
</code></pre>

<p>I thought word stemming would give me finance, finance and business. 
    I would at least expect finance and financial to have the same base word. 
    Im trying to group similar words, it works for some words like have and having both become 
    have, but for some like the above it doesnt seem to work, unless Im misunderstanding?</p>
","r, data.table, text-mining, cpu-word, stemming","<p>It seems like your result is what the Porter stemmer algorithm is so supposed to do.</p>
<p><a href=""http://snowball.tartarus.org/algorithms/porter/stemmer.html"" rel=""nofollow noreferrer"">Documentation</a> (Step 4) shows examples of stemming with the suffixes used in your example:</p>
<blockquote>
<p>(m&gt;1) AL       -&gt;                          revival         -&gt;      reviv</p>
<p>(m&gt;1) ANCE         -&gt;                          allowance       -&gt;      allow</p>
</blockquote>
<p>If you want to group your words then you might want to trim them before running <code>wordStem</code> or use string matching functions after stemming (e.g. <code>agrep</code>).</p>
",1,2,201,2016-05-23 08:14:20,https://stackoverflow.com/questions/37385880/r-wordstem-chopping-words-too-much
What is the optimal topic-modelling workflow with MALLET?,"<h2>Introduction</h2>

<p>I'd like to know what other topic modellers consider to be an optimal topic-modelling workflow all the way from pre-processing to maintenance. While this question consists of a number of sub-questions (which I will specify below), I believe this thread would be useful for myself and others who are interested to learn about best practices of end-to-end process.</p>

<h2>Proposed Solution Specifications</h2>

<p>I'd like the proposed solution to preferably rely on <strong>R</strong> for text processing (but <strong>Python</strong> is fine also) and topic-modelling itself to be done in <strong>MALLET</strong> (although if you believe other solutions work better, please let us know). I tend to use the <code>topicmodels</code> package in <code>R</code>, however I would like to switch to <code>MALLET</code> as it offers many benefits over <code>topicmodels</code>. It can handle a lot of data, it does not rely on specific text pre-processing tools and it appears to be widely used for this purpose. However some of the issues outline below are also relevant for <code>topicmodels</code> too. I'd like to know how others approach topic modelling and which of the below steps could be improved. Any useful piece of advice is welcome.</p>

<h2>Outline</h2>

<p>Here is how it's going to work: I'm going to go through the workflow which in my opinion works reasonably well, and I'm going to outline problems at each step.</p>

<h2>Proposed Workflow</h2>

<h2>1. Clean text</h2>

<p>This involves removing punctuation marks, digits, stop words, stemming words and other text-processing tasks. Many of these can be done either as part of term-document matrix decomposition through functions such as for example <code>TermDocumentMatrix</code> from <code>R</code>'s package <code>tm</code>.</p>

<p><strong>Problem:</strong> This however may need to be performed on the text strings directly, using functions such as <code>gsub</code> in order for MALLET to consume these strings. Performing in on the strings directly is not as efficient as it involves repetition (e.g. the same word would have to be stemmed several times)</p>

<h2>2. Construct features</h2>

<p>In this step we construct a term-document matrix (TDM), followed by the filtering of terms based on frequency, and TF-IDF values. It is preferable to limit your bag of features to about 1000 or so. Next go through the terms and identify what requires to be <strong>(1) dropped</strong> (some stop words will make it through), <strong>(2) renamed</strong> or <strong>(3) merged</strong> with existing entries. While I'm familiar with the concept of stem-completion, I find that it rarely works well. </p>

<p><strong>Problem:</strong> <strong>(1)</strong> Unfortunately <code>MALLET</code> does not work with TDM constructs and to make use of your TDM, you would need to find the difference between the original TDM -- with no features removed -- and the TDM that you are happy with. This difference would become stop words for MALLET. <strong>(2)</strong> On that note I'd also like to point out that feature selection does require a substantial amount of manual work and if anyone has ideas on how to minimise it, please share your thoughts.</p>

<p><strong>Side note:</strong> If you decide to stick with <code>R</code> alone, then I can recommend the <code>quanteda</code> package which has a function <code>dfm</code> that accepts a <code>thesaurus</code> as one of the parameters. This thesaurus allows to to capture <em>patterns</em> (usually regex) as opposed to words themselves, so for example you could have a pattern <code>\\bsign\\w*.?ups?</code> that would match <code>sign-up</code>, <code>signed up</code> and so on. </p>

<h2>3. Find optimal parameters</h2>

<p>This is a hard one. I tend to break data into test-train sets and run cross-validation fitting a model of <code>k</code> topics and testing the fit using held-out data. Log likelihood is recorded and compared for different resolutions of topics.</p>

<p><strong>Problem:</strong> Log likelihood does help to understand how good is the fit, but <strong>(1)</strong> it often tends to suggest that I need more topics than it is practically sensible and <strong>(2)</strong> given how long it generally takes to fit a model, it is virtually impossible to find or test a grid of optimal values such as iterations, alpha, burn-in and so on.</p>

<p><strong>Side note:</strong> When selecting the optimal number of topics, I generally select a range of topics incrementing by 5 or so as incrementing a range by 1 generally takes too long to compute.</p>

<h2>4. Maintenance</h2>

<p>It is easy to classify new data into a set existing topics. However if you are running it over time, you would naturally expect that some of your topics may cease to be relevant, while new topics may appear. Furthermore, it might be of interest to study the lifecycle of topics. This is difficult to account for as you are dealing with a problem that requires an unsupervised solution and yet for it to be tracked over time, you need to approach it in a supervised way.</p>

<p><strong>Problem:</strong> To overcome the above issue, you would need to <strong>(1)</strong> fit new data into an old set of topics, <strong>(2)</strong> construct a new topic model based on new data <strong>(3)</strong> monitor log likelihood values over time and devise a threshold when to switch from old to new; and <strong>(4)</strong> merge old and new solutions somehow so that the evolution of topics would be revealed to a lay observer.</p>

<h1>Recap of Problems</h1>

<ul>
<li>String cleaning for <code>MALLET</code> to consume the data is inefficient.</li>
<li>Feature selection requires manual work.</li>
<li>Optimal number of topics selection based on LL does not account for what is practically sensible</li>
<li>Computational complexity does not give the opportunity to find an optimal grid of parameters (other than the number of topics)</li>
<li>Maintenance of topics over time poses challenging issues as you have to retain history but also reflect what is currently relevant.</li>
</ul>

<p>If you've read that far, I'd like to thank you, this is a rather long post. If you are interested in the suggest, feel free to either add more questions in the comments that you think are relevant or offer your thoughts on how to overcome some of these problems.</p>

<p>Cheers</p>
","python, r, text-mining, lda, mallet","<p>Thank you for this thorough summary!</p>

<p>As an alternative to <code>topicmodels</code> try the package <code>mallet</code> in R. It runs Mallet in a JVM directly from R and allows you to pull out results as R tables. I expect to release a new version soon, and compatibility with <code>tm</code> constructs is something others have requested.</p>

<p>To clarify, it's a good idea for <em>documents</em> to be at most around 1000 tokens long (<em>not</em> vocabulary). Any more and you start to lose useful information. The assumption of the model is that the position of a token within a given document doesn't tell you anything about that token's topic. That's rarely true for longer documents, so it helps to break them up.</p>

<p>Another point I would add is that documents that are too short can also be a problem. Tweets, for example, don't seem to provide enough contextual information about word co-occurrence, so the model often devolves into a one-topic-per-doc clustering algorithm. Combining multiple related short documents can make a big difference.</p>

<p>Vocabulary curation is in practice the most challenging part of a topic modeling workflow. Replacing selected multi-word terms with single tokens (for example by swapping spaces for underscores) before tokenizing is a very good idea. Stemming is almost never useful, at least for English. Automated methods can help vocabulary curation, but this step has a profound impact on results (much more than the number of topics) and I am reluctant to encourage people to fully trust any system.</p>

<p>Parameters: I do not believe that there is a right number of topics. I recommend using a number of topics that provides the granularity that suits your application. Likelihood can often detect when you have too few topics, but after a threshold it doesn't provide much useful information. Using hyperparameter optimization makes models much less sensitive to this setting as well, which might reduce the number of parameters that you need to search over.</p>

<p>Topic drift: This is not a well understood problem. More examples of real-world corpus change would be useful. Looking for changes in vocabulary (e.g. proportion of out-of-vocabulary words) is a quick proxy for how well a model will fit.</p>
",4,4,1625,2016-05-23 08:52:33,https://stackoverflow.com/questions/37386595/what-is-the-optimal-topic-modelling-workflow-with-mallet
Python Pandas - How to format and split a text in column ?,"<p>I have a set of strings in a dataframe like below</p>

<pre><code>ID TextColumn
1 This is line number one
2 I love pandas, they are so puffy
3 [This $tring is with specia| characters, yes it is!]
</code></pre>

<p>A. I want to format this string to eliminate all the special characters
B. Once formatted,  I'd like to get a list of unique words (space being the only split) </p>

<p>Here is the code I have written:</p>

<p>get_df_by_id dataframe has one selected frame, say ID 3.</p>

<pre><code>#replace all special characters
formatted_title = get_df_by_id['title'].str.replace(r'[\-\!\@\#\$\%\^\&amp;\*\(\)\_\+\[\]\;\'\.\,\/\{\}\:\""\&lt;\&gt;\?]' , '')
# then split the words
results = set()
get_df_by_id['title'].str.lower().str.split().apply(results.update)
print results
</code></pre>

<p>But when I check output, I could see that special characters are still in the list.</p>

<pre><code>Output

set([u'[this', u'is', u'it', u'specia|', u'$tring', u'is!]', u'characters,', u'yes', u'with'])
</code></pre>

<p>Intended output should be like below:</p>

<pre><code>set([u'this', u'is', u'it', u'specia', u'tring', u'is', u'characters,', u'yes', u'with'])
</code></pre>

<p>Why does formatted dataframe still retain the special characters?</p>
","python, pandas, text-mining, data-analysis","<p>I think you can first <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.replace.html"" rel=""nofollow""><code>replace</code></a> special characters (I add <code>\|</code> to the end), then <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.lower.html"" rel=""nofollow""><code>lower</code></a> text, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html"" rel=""nofollow""><code>split</code></a> by <code>\s+</code> (arbitrary wtitespaces). Output is DataFrame. So you can <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html"" rel=""nofollow""><code>stack</code></a> it to <code>Series</code>, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.drop_duplicates.html"" rel=""nofollow""><code>drop_duplicates</code></a> and last <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.tolist.html"" rel=""nofollow""><code>tolist</code></a>:</p>

<pre><code>print (df['title'].str
                  .replace(r'[\-\!\@\#\$\%\^\&amp;\*\(\)\_\+\[\]\;\'\.\,\/\{\}\:\""\&lt;\&gt;\?\|]','')
                  .str
                  .lower()
                  .str
                  .split('\s+', expand=True)
                  .stack()
                  .drop_duplicates()
                  .tolist())

['this', 'is', 'line', 'number', 'one', 'i', 'love', 'pandas', 'they', 'are', 
'so', 'puffy', 'tring', 'with', 'specia', 'characters', 'yes', 'it']
</code></pre>
",2,2,3185,2016-05-25 06:25:20,https://stackoverflow.com/questions/37429296/python-pandas-how-to-format-and-split-a-text-in-column
Change the length of ContextPre and ContextPost in Quanteda KWIC,"<p>Is there a way to increase the number of words appearing before and after the keyword in Quanteda kwic function? </p>

<p>I've tried by changing the numeric value in: </p>

<pre><code>options(width = 200)
</code></pre>

<p>but it didn't work. 
@KenBenoit</p>
","r, text-mining, quanteda","<p><code>options(width)</code> affects the number of text columns displayed by the R interpreter. You want the <code>window</code> argument to <code>kwic()</code>:</p>

<pre><code>&gt;  kwic(data_corpus_inaugural, ""war against"")
                                               contextPre     keyword                         contextPost
 [1857-Buchanan, 2933:2934] advantage of the fortune of [ war against ] a sister republic, we
 [1901-McKinley, 2284:2285]         . We are not waging [ war against ] the inhabitants of the Philippine
 [1901-McKinley, 2299:2300]  portion of them are making [ war against ] the United States. By
 [1901-McKinley, 2413:2414]    used when those who make [ war against ] us shall make it no
[1933-Roosevelt, 1851:1852]   Executive power to wage a [ war against ] the emergency, as great
&gt;  kwic(data_corpus_inaugural, ""war against"", window=7)
                                                       contextPre     keyword                                  contextPost
 [1857-Buchanan, 2933:2934] to take advantage of the fortune of [ war against ] a sister republic, we purchased these
 [1901-McKinley, 2284:2285]      be deceived. We are not waging [ war against ] the inhabitants of the Philippine Islands.
 [1901-McKinley, 2299:2300]      . A portion of them are making [ war against ] the United States. By far the
 [1901-McKinley, 2413:2414]  needed or used when those who make [ war against ] us shall make it no more.
[1933-Roosevelt, 1851:1852]   - broad Executive power to wage a [ war against ] the emergency, as great as the
</code></pre>
",3,2,98,2016-05-25 11:08:45,https://stackoverflow.com/questions/37435338/change-the-length-of-contextpre-and-contextpost-in-quanteda-kwic
"Regex works, but not on strings in my vector","<p>So I am attempting to use grep to find pattern and replace values within my single column data frame. I basically want grep that says ""delete everything after the comma until the end of the string"". 
I wrote the expression, and it works on my dummy vector:</p>

<pre><code>&gt; library(stringr)
&gt; pretendvector &lt;- c(""Hi"",""Hi,there"",""Hi there, how are you"")
&gt;str_replace(pretendvector, regex(',.*$'),'')
[1] ""Hi""       ""Hi""       ""Hi there""
</code></pre>

<p>However, when apply the same expression to my vector (since its for stringr I vectorized the column of the dataframe), it returns every value in the column, and does not apply the expression. Does anyone have any idea why this might be? </p>
","regex, r, text-mining, stringr","<p>I guess the OP didn't assign the output from <code>str_replace</code> to a new object or update the original vector.  In that case,</p>

<pre><code>newvector &lt;- str_replace(pretendvector, regex(',.*$'),'') 
</code></pre>

<p>We can also do this using <code>sub</code> from <code>base R</code> </p>

<pre><code>newvector &lt;- sub("",.*"", """", pretendvector)
</code></pre>
",0,1,67,2016-05-26 02:43:40,https://stackoverflow.com/questions/37450687/regex-works-but-not-on-strings-in-my-vector
remove all words from a character vector that are NOT certain words,"<p>I have a character list that looks like this</p>

<pre><code>[70] ""CSF  5896-6133""                                                           
[71] ""CRT  16""                                                                  
[72] ""SEEF  54-55""                                                              
[73] ""CIF  190-195""                                                             
[74] ""DE &amp; /ON CIF  196-222""                                                    
[75] "" CRT  17 ""                                                                
[76] "" SEEF  56-57""                                                             
[77] ""DE &amp; /ON CSF  6134-6725 ""                                                 
[78] "" SEEF  58-60""                                                             
[79] ""CRT 18""                                                                   
[80] "" CSF 6726-6837""                                                           
[81] ""SEEF 61""                                                                  
[82] "" CSF 6840-6926""                                                           
[83] "" CIF 223-226""                                                             
[84] ""SEEF 62-63""                                                               
[85] "" CSF 6927-7065""                                                           
[86] "" CIF 226-228""                                                             
[87] ""CSF 7066-7185""                                                            
[88] ""CSF 7186-7311""                                                            
[89] "" CIF 229""                                                                 
[90] "" SEEF 66""                                                                 
[91] ""CSF 7312-7561""                                                            
[92] "" CRT 19""                                                                  
[93] "" SEEF 67-68""                                                              
[94] ""Final data QAQC done on CSF  1-7561""                                      
[95] "" CIF  1-229""                                                              
[96] "" SEEF  1-68 ""                                                             
[97] "" CRT  1-19""                                                               
[98] ""082015-HOBA-G17-1 changed to offPlot based on GIS review of searched     area""
</code></pre>

<p>As you can see this is only part of it.</p>

<p>I want to remove all words that are NOT either a number or</p>

<pre><code>CSF, CIF, SEEF, CRT
</code></pre>

<p>So that for example the section from 94-98 would look like</p>

<pre><code>[94] ""CSF  1-7561""                                      
[95] "" CIF  1-229""                                                              
[96] "" SEEF  1-68 ""                                                             
[97] "" CRT  1-19""                                                               
</code></pre>

<p>As you can see line 98 would be deleted completely because it had none of the keywords I wanted it to have. Line 94 also got stripped of some words. </p>
","r, character, text-mining","<p>Consider the following vector:</p>

<pre><code>v &lt;- c(""Final data QAQC done on CSF  1-7561"", 
       ""CIF  1-229"", 
       ""SEEF  1-68"", 
       ""CRT  1-19"",
       ""082015-HOBA-G17-1 changed to offPlot based on GIS review of searched     area"")
</code></pre>

<p>You could do:</p>

<pre><code>## vector with words to match
cond &lt;- c(""CSF"", ""CIF"", ""SEEF"", ""CRT"")
## regex that captures digits and tolerates dashes (-) 
reg  &lt;- ""(\\d+-?)+$""
## pattern to match either words or regex 
pattern &lt;- paste(c(cond, reg), collapse = ""|"")
</code></pre>

<p>Then use <code>stri_extract_all()</code> from the <code>stringi</code> package:</p>

<pre><code>library(stringi)
stri_extract_all_regex(v, pattern)
</code></pre>

<p>Which gives:</p>

<pre><code>#[[1]]
#[1] ""CSF""    ""1-7561""
#
#[[2]]
#[1] ""CIF""   ""1-229""
#
#[[3]]
#[1] ""SEEF"" ""1-68""
#
#[[4]]
#[1] ""CRT""  ""1-19""
#
#[[5]]
#[1] NA
</code></pre>

<hr>

<p>As per mentionned by @akrun, you could also do:</p>

<pre><code>regmatches(v, gregexpr(pattern, v))
</code></pre>

<p>Which gives: </p>

<pre><code>#[[1]]
#[1] ""CSF""    ""1-7561""
#
#[[2]]
#[1] ""CIF""   ""1-229""
#
#[[3]]
#[1] ""SEEF"" ""1-68""
#
#[[4]]
#[1] ""CRT""  ""1-19""
#
#[[5]]
#character(0)
</code></pre>
",3,2,909,2016-05-26 16:34:38,https://stackoverflow.com/questions/37466512/remove-all-words-from-a-character-vector-that-are-not-certain-words
tolower function of the corpus package throws an error,"<p>I trying to do some text mining using twitter data. I do the following:</p>

<pre><code>#connect to twitter API
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

#set radius and amount of requests
N=200  # tweets to request from each query
S=200  # radius in miles

lats=c(38.9,40.7)
lons=c(-77,-74)

roger=do.call(rbind,lapply(1:length(lats), function(i) searchTwitter('Roger+Federer',
                                                                  lang=""en"",n=N,resultType=""recent"",
                                                                  geocode=paste(lats[i],lons[i],paste0(S,""mi""),sep="",""))))
</code></pre>

<p>This all works fine but when I want to use the tolower function of the corpus package like this:</p>

<pre><code>data=as.data.frame(cbind(tweet=rogertext))
corpus=Corpus(VectorSource(data$tweet))
corpus=tm_map(corpus,tolower)
</code></pre>

<p>It trows this error:</p>

<pre><code>&gt; corpus=tm_map(corpus,tolower)
Error in FUN(X[[i]], ...) : 
invalid input 'RT @Federerism: Roger Federer reaches  5 million followers   on twitter  Love You Roger í ½í¸˜ í ½í¸ í ½í¸˜ í ½í¸ #Roger #Federer #   Federerism #Maestro https:/â€¦' in 'utf8towcs'
</code></pre>

<p>Any thought on what goes wrong?</p>
","r, text-mining, tm, corpus","<p><code>base::tolower</code> chokes on special characters. This is often a problem when mining tweets. You could try catching errors or just use stringi's tolower pendant: </p>

<pre><code># tw &lt;- searchTwitter('Roger Federer reaches  5 million followers   on twitter  Love You Roger', n=1) 
download.file(""https://www.dropbox.com/s/33ilhcu2v82nwuq/twitter_tolower.rda?dl=1"", tf &lt;- tempfile(fileext = "".rda""), mode=""wb"")
load(tf) 

tw[[1]]$getText()
# [1] ""RT @Federerism: Roger Federer reaches  5 million followers on twitter  Love You Roger \xed��\xed�\u0098 \xed��\xed�\u008d \xed��\xed�\u0098 \xed��\xed�\u008d #Roger #Federer # Federerism #Maestro https:/…""

## Does not work:
tolower(tw[[1]]$getText())
# Error in tolower(tw[[1]]$getText()) : 
#   invalid input 'RT @Federerism: Roger Federer reaches  5 million followers on twitter  Love You Roger í ½í¸˜ í ½í¸ í ½í¸˜ í ½í¸ #Roger #Federer # Federerism #Maestro https:/â€¦' in 'utf8towcs'

## Works:
stringi::stri_trans_tolower(tw[[1]]$getText())
# [1] ""rt @federerism: roger federer reaches  5 million followers on twitter  love you roger \xed��\xed�\u0098 \xed��\xed�\u008d \xed��\xed�\u0098 \xed��\xed�\u008d #roger #federer # federerism #maestro https:/…""

## Works, too:
library(tm)
corp &lt;- Corpus(VectorSource(tw[[1]]$getText()))
corp &lt;- tm_map(corp, content_transformer(stringi::stri_trans_tolower))
content(corp[[1]])
# [1] ""rt @federerism: roger federer reaches  5 million followers on twitter  love you roger \xed��\xed�\u0098 \xed��\xed�\u008d \xed��\xed�\u0098 \xed��\xed�\u008d #roger #federer # federerism #maestro https:/…""
</code></pre>
",2,2,2158,2016-05-28 08:54:24,https://stackoverflow.com/questions/37497070/tolower-function-of-the-corpus-package-throws-an-error
Cant get lat and longitude values of tweets,"<p>I collected some twitter data doing this: </p>

<pre><code>#connect to twitter API
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

#set radius and amount of requests
N=200  # tweets to request from each query
S=200  # radius in miles

lats=c(38.9,40.7)
lons=c(-77,-74)

roger=do.call(rbind,lapply(1:length(lats), function(i) searchTwitter('Roger+Federer',
                                                                lang=""en"",n=N,resultType=""recent"",
                                                              geocode=paste  (lats[i],lons[i],paste0(S,""mi""),sep="",""))))
</code></pre>

<p>After this I've done:</p>

<pre><code>rogerlat=sapply(roger, function(x) as.numeric(x$getLatitude()))
rogerlat=sapply(rogerlat, function(z) ifelse(length(z)==0,NA,z))  

rogerlon=sapply(roger, function(x) as.numeric(x$getLongitude()))
rogerlon=sapply(rogerlon, function(z) ifelse(length(z)==0,NA,z))  

data=as.data.frame(cbind(lat=rogerlat,lon=rogerlon))
</code></pre>

<p>And now I would like to get all the tweets that have long and lat values:</p>

<pre><code>data=filter(data, !is.na(lat),!is.na(lon))
lonlat=select(data,lon,lat)
</code></pre>

<p>But now I only get NA values.... Any thoughts on what goes wrong here?</p>
","r, text-mining, tm","<p>As <a href=""https://stackoverflow.com/users/4266453/chris"">Chris</a> mentioned, <code>searchTwitter</code> does not return the lat-long of a tweet. You can see this by going to the <a href=""https://cran.r-project.org/web/packages/twitteR/twitteR.pdf"" rel=""nofollow noreferrer"">twitteR</a> documentation, which tells us that it returns a <code>status</code> object.</p>

<p><strong>Status Objects</strong></p>

<p>Scrolling down to the status object, you can see that 11 pieces of information are included, but lat-long is not one of them. However, we are not completely lost, because the user's screen name is returned.</p>

<p>If we look at the user object, we see that a user's object at least includes a location.</p>

<p>So I can think of at least two possible solutions, depending on what your use case is.</p>

<p><strong>Solution 1: Extracting a User's Location</strong></p>

<pre><code># Search for recent Trump tweets #
tweets &lt;- searchTwitter('Trump', lang=""en"",n=N,resultType=""recent"",
              geocode='38.9,-77,50mi')

# If you want, convert tweets to a data frame #
tweets.df &lt;- twListToDF(tweets)

# Look up the users #
users &lt;- lookupUsers(tweets.df$screenName)

# Convert users to a dataframe, look at their location#
users_df &lt;- twListToDF(users)

table(users_df[1:10, 'location'])

                                       ❤ Texas  ❤ ALT.SEATTLE.INTERNET.UR.FACE 
                   2                            1                            1 
               Japan             Land of the Free                  New Orleans 
                   1                            1                            1 
  Springfield OR USA                United States                          USA 
                   1                            1                            1 

# Note that these will be the users' self-reported locations,
# so potentially they are not that useful
</code></pre>

<p><strong>Solution 2: Multiple searches with limited radius</strong></p>

<p>The other solution would be to conduct a series of repeated searches, increment your latitude and longitude with a small radius. That way you can be relatively sure that the user is close to your specified location.</p>
",6,3,1883,2016-05-30 08:05:04,https://stackoverflow.com/questions/37520596/cant-get-lat-and-longitude-values-of-tweets
Getting data from twitter with R?,"<p>I am using R 3.1.3 on Platform: x86_64-apple-darwin13.4.0 (64-bit) and tm_0.6-2 version of package</p>

<p>Here is my codes following: </p>

<pre><code>install.packages(c(""twitterR"",""ROAuth"",""RCurl"",""tm"",""wordcloud"",""SnowballC""))
library(SnowballC)
library(twitteR)
library(ROAuth)
library(RCurl)
library(tm)
library(wordcloud)
#twitter authentication
consumerKey &lt;- "" ""
consumerSecret &lt;- "" ""
accessToken &lt;- "" ""
accessTokenSecret &lt;- "" ""

twitteR::setup_twitter_oauth(consumerKey,consumerSecret,accessToken,accessTokenSecret)

#retrive tweets from twitter
tweets=searchTwitter(""euro2016+france"",lang = ""en"",n=500,resultType = ""recent"")
class(tweets)
head(tweets)
#converting list to vector
tweets_text=sapply(tweets,function(x) x$getText())
str(tweets_text)
#creates corpus from vector of tweets
tweets_corpus=Corpus(VectorSource(tweets_text))
inspect(tweets_corpus[100])

#cleaning
tweets_clean=tm_map(tweets_corpus,removePunctuation,lazy= T)
tweets_clean=tm_map(tweets_clean,content_transformer(tolower),lazy = T)
tweets_clean=tm_map(tweets_clean,removeWords,stopwords(""english""),lazy = T)
tweets_clean=tm_map(tweets_clean,removeNumbers,lazy = T)
tweets_clean=tm_map(tweets_clean,stripWhitespace,lazy = T)
tweets_clean=tm_map(tweets_clean,removeWords,c(""euro2016"",""france""),lazy = T)
#wordcloud play with parameters
wordcloud(tweets_clean)
</code></pre>

<p>When I run the final line, I got:</p>

<blockquote>
  <p>Error in UseMethod(""meta"", x) :    no applicable method for 'meta'
  applied to an object of class ""try-error"" In addition: Warning
  messages: 1: In mclapply(x$content[i], function(d) tm_reduce(d,
  x$lazy$maps)) :   all scheduled cores encountered errors in user code
  2: In mclapply(unname(content(x)), termFreq, control) :   all
  scheduled cores encountered errors in user code</p>
</blockquote>

<p>Does anyone know solution a for this?</p>
","r, text-mining, tm, twitter-r","<p>Somehow there seems to be a encoding problem with the <code>removeWords</code> function when it is used together with the <code>tm_map</code> function (see also <a href=""https://stackoverflow.com/questions/9637278/r-tm-package-invalid-input-in-utf8towcs"">here</a>).</p>

<p>A work around could be using the function earlier, at the point where you load the text into the corpus:</p>

<pre><code>#converting list to vector
tweets_text=sapply(tweets,function(x) x$getText())
str(tweets_text)

# removing words
tweets_text&lt;- sapply(tweets_text, function(x) removeWords(x, c(""euro2016"",""france"")))
tweets_text&lt;- sapply(tweets_text, function(x) removeWords(x, stopwords(""english"")))


#creates corpus from vector of tweets
tweets_corpus=Corpus(VectorSource(tweets_text))
inspect(tweets_corpus[100])

#cleaning
tweets_clean=tm_map(tweets_corpus,removePunctuation)
tweets_clean=tm_map(tweets_clean,content_transformer(tolower))
#tweets_clean=tm_map(tweets_clean,removeWords,stopwords(""english""))
tweets_clean=tm_map(tweets_clean,removeNumbers,lazy = T)
tweets_clean=tm_map(tweets_clean,stripWhitespace,lazy = T)
#tweets_clean=tm_map(tweets_clean,removeWords,c(""euro2016"",""france""),lazy = T)
wordcloud(tweets_clean)
</code></pre>
",1,2,406,2016-06-03 20:23:13,https://stackoverflow.com/questions/37622774/getting-data-from-twitter-with-r
R function converting text file into document term matrix,"<p>I have text file with three columns which are document id, term id and term frequency. Is there an R function that converts this data to document term matrix? </p>
","r, text-mining","<p>For example</p>

<pre><code>df &lt;- read.table(header=T, text='""doc"" ""term"" ""freq""
1 ""foo"" 1
1 ""bar"" 2
2 ""hello"" 1
2 ""world"" 2')
library(tm)
dtm &lt;- as.DocumentTermMatrix(xtabs(freq~doc+term, df), weighting=weightTf)
as.matrix(dtm)
#     Terms
# Docs bar foo hello world
#    1   2   1     0     0
#    2   0   0     1     2
</code></pre>
",2,0,604,2016-06-06 15:47:51,https://stackoverflow.com/questions/37661668/r-function-converting-text-file-into-document-term-matrix
Delete hyphen (special chars) while processing text in RapidMiner,"<p>I would like to delete all hyphens in the text document which I analyze in Rapidminer. For that I use operator ""Process documents from files"" to analyze large PDF-files. Each file contains a lot of hyphens which I would like to delete before I'll tokenize the text into pieces (non letters). I've used operator ""Replace token"". With it I can replace hyphens with other symbols, but I cannot replace them with nothing or empty string("" ""). I've tried also to use my own customized dictionary of stopwords(non-letters, -). This operator does no work at all. I've saved my dictionary containing the chars and words I want to delete as a text file (each in the new line). Can anybody help on this issue?      </p>
","replace, text-mining, rapidminer","<p>You can use <code>Replace Tokens</code> with the following parameters. </p>

<p><code>replace what</code> <code>()[-]</code> </p>

<p><code>replace by</code> <code>$1</code></p>

<p>It's a bit of a hack but it works because the first capturing group between the brackets will always be empty and the whole regular expression will match a single hyphen. The <code>$1</code> is the result of the first capture group and it's always empty.</p>

<p>Here's an example process that shows this working.</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;process version=""7.0.000""&gt;
  &lt;context&gt;
    &lt;input/&gt;
    &lt;output/&gt;
    &lt;macros/&gt;
  &lt;/context&gt;
  &lt;operator activated=""true"" class=""process"" compatibility=""7.0.000"" expanded=""true"" name=""Process""&gt;
    &lt;process expanded=""true""&gt;
      &lt;operator activated=""true"" class=""text:create_document"" compatibility=""7.0.000"" expanded=""true"" height=""68"" name=""Create Document"" width=""90"" x=""246"" y=""187""&gt;
        &lt;parameter key=""text"" value=""some text &amp;#10;with&amp;#10;some-text-with-hyphens-in&amp;#10;hyphens in&amp;#10;""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text:replace_tokens"" compatibility=""7.0.000"" expanded=""true"" height=""68"" name=""Replace Tokens"" width=""90"" x=""447"" y=""187""&gt;
        &lt;list key=""replace_dictionary""&gt;
          &lt;parameter key=""()[-]"" value=""$1""/&gt;
        &lt;/list&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text:process_documents"" compatibility=""7.0.000"" expanded=""true"" height=""103"" name=""Process Documents"" width=""90"" x=""648"" y=""187""&gt;
        &lt;parameter key=""vector_creation"" value=""Term Occurrences""/&gt;
        &lt;process expanded=""true""&gt;
          &lt;operator activated=""true"" class=""text:tokenize"" compatibility=""7.0.000"" expanded=""true"" height=""68"" name=""Tokenize"" width=""90"" x=""179"" y=""85""/&gt;
          &lt;connect from_port=""document"" to_op=""Tokenize"" to_port=""document""/&gt;
          &lt;connect from_op=""Tokenize"" from_port=""document"" to_port=""document 1""/&gt;
          &lt;portSpacing port=""source_document"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 1"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 2"" spacing=""0""/&gt;
        &lt;/process&gt;
      &lt;/operator&gt;
      &lt;connect from_op=""Create Document"" from_port=""output"" to_op=""Replace Tokens"" to_port=""document""/&gt;
      &lt;connect from_op=""Replace Tokens"" from_port=""document"" to_op=""Process Documents"" to_port=""documents 1""/&gt;
      &lt;connect from_op=""Process Documents"" from_port=""example set"" to_port=""result 1""/&gt;
      &lt;portSpacing port=""source_input 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 2"" spacing=""0""/&gt;
    &lt;/process&gt;
  &lt;/operator&gt;
&lt;/process&gt;
</code></pre>

<p>Hope that helps as a basis.</p>
",1,1,757,2016-06-08 11:06:31,https://stackoverflow.com/questions/37700765/delete-hyphen-special-chars-while-processing-text-in-rapidminer
how to extract listed names (placenames) from a text?,"<p>I would like to extract placenames from a text and geolocate them on a map - automatically with R. The first step would be to extract the placenames.</p>

<p>I downloaded a list of placenames (from <a href=""http://download.geonames.org/export/dump/"" rel=""nofollow"">geonames</a>); but how do I match the words from the geonames-placename-list within the text?</p>

<p>The possibility with <code>intersect()</code> does work only, when I convert the text to a vector - and therefore need to split the text into words, which causes the match-operator to only find one-word-placenames like ""berlin"" but not ""new york"" etc.</p>

<p>Does a function to compare a list with text (as string) exist?</p>

<p>MWE:</p>

<pre><code>list = c(""Wien"", ""London"", ""New York"")
text = ""Er sah den Stadtplan von Wien in New York.""
words = unlist(strsplit(text, ""\\W""))
intersect(list, words)
</code></pre>

<p>results in only:</p>

<pre><code>&gt; [1] ""Wien""
</code></pre>
","r, list, compare, geocoding, text-mining","<p>you could use something like that</p>

<pre><code>library(stringr)
    list = c(""Wien"", ""London"", ""New York"")
    text = ""Er sah den Stadtplan von Wien in New York.""
    words=as.character()

    for (i in 1:length(list)){

        if (is.na(str_extract(text,list[i]))) next

        x&lt;-str_extract(text,list[i])
        words&lt;-c(words,x)
    }


    &gt; words
    [1] ""Wien""     ""New York""
</code></pre>
",0,1,93,2016-06-09 12:49:57,https://stackoverflow.com/questions/37726501/how-to-extract-listed-names-placenames-from-a-text
How to implement a backup tokenizer switch in RWeka?,"<p>I am using R-tm-Rweka packages to do some text mining. Instead of building a tf-tdm on single words, which is not enough for my purposes, i have to extract ngrams. I used <a href=""https://stackoverflow.com/questions/19615181/finding-ngrams-in-r-and-comparing-ngrams-across-corpora?rq=1"">@Ben</a> function <code>TrigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
tdm &lt;- TermDocumentMatrix(a, control = list(tokenize = TrigramTokenizer))
</code><br>
to extract trigrams. The output has an apparent error, see below. It picks up 4-, 3- and 2-word phrases. Ideally, it should have ONLY picked up the 4-word noun phrase and dropped the (3- and 2-word)rest. How do I force this solution, like Python NLTK has a backup tokenizer option? </p>

<p>abstract strategy             <code>-&gt;this is incorrect</code>><br>
  abstract strategy board        <code>-&gt;incorrect</code><br>
  abstract strategy board game   <code>-&gt; this should be the correct output</code></p>

<p>accenture executive<br>
  accenture executive simple<br>
  accenture executive simple comment   </p>

<p>Many thanks.</p>
","r, text-mining, tm, rweka","<p>I think you were very close with the attempt that you made. Except that you have to understand that what you were telling <code>Weka</code> to do was to capture 2-gram and 3-gram tokens; that's just how <code>Weka_control</code> was specified.</p>

<p>Instead I'd recommend to use the different token sizes in different tokenizers and select or merge the results according to your preference or decision rule.</p>

<p>I think it would be worth checking out this <a href=""http://hack-r.com/n-gram-wordclouds-in-r/"" rel=""nofollow noreferrer"">great tutorial</a> on n-gram wordclouds.</p>

<p>A solid code snippet for n-gram text mining is:</p>

<pre><code># QuadgramTokenizer ####
QuadgramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4)
</code></pre>

<p>for 4-grams,</p>

<pre><code># TrigramTokenizer ####
TrigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3)
</code></pre>

<p>For 3-grams, and of course</p>

<pre><code># BigramTokenizer ####
BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)
</code></pre>

<p>for 2-grams.</p>

<p>You might be able to avoid your earlier problem by running the different gram sizes separately like this instead of setting <code>Weka_control</code> to a range.</p>

<p>You can apply the tokenizer like this:</p>

<pre><code>tdm.ng &lt;- TermDocumentMatrix(ds5.1g, control = list(tokenize = BigramTokenizer))
dtm.ng &lt;- DocumentTermMatrix(ds5.1g, control = list(tokenize = BigramTokenizer))
</code></pre>

<p>If you still have problems please just provide a <a href=""https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example"">reproducible</a> example and I'll follow up.</p>
",0,1,191,2016-06-10 14:37:22,https://stackoverflow.com/questions/37750951/how-to-implement-a-backup-tokenizer-switch-in-rweka
How to download full article text from Pubmed?,"<p>I am working on a project that requires to work with Genia corpus. According to the literature Genia Corpus is made from articles extracted by searching 3 Mesh terms : “transcription factor”, “blood cell” and “human” on Medline/Pubmed. I want to extract full text article(which are freely available) for the articles in Genia corpus from Pubmed. I have tried many approaches but I am not able to find a way to download full text in text or XML or Pdf format. </p>

<p>Using Entrez utils provided by NCBI : </p>

<ol>
<li><p>I have tried using the approach mentioned here - 
<a href=""http://www.hpa-bioinformatics.org.uk/bioruby-api/classes/Bio/NCBI/REST/EFetch/Methods.html#M002197"" rel=""nofollow"">http://www.hpa-bioinformatics.org.uk/bioruby-api/classes/Bio/NCBI/REST/EFetch/Methods.html#M002197</a></p>

<p>which uses the Ruby gem Bio like this to get the information for a given PubMed ID - 
Bio::NCBI::REST::EFetch.pubmed(15496913)</p>

<p>But, it doesn't return the full text for the PMID.</p></li>
<li><p>Internally, it makes a call like this - 
<a href=""http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&amp;id=1372388&amp;retmode=text&amp;rettype=medline"" rel=""nofollow"">http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&amp;id=1372388&amp;retmode=text&amp;rettype=medline</a></p>

<p>But, both the Ruby gem and the above call don't return the full text.</p></li>
<li><p>On further Internet search, I found that the allowed values for PubMed for rettype and retmode don't have an option to get the full text, as mentioned in the table here - 
<a href=""http://www.ncbi.nlm.nih.gov/books/NBK25499/table/chapter4.T._valid_values_of__retmode_and/?report=objectonly"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/books/NBK25499/table/chapter4.T._valid_values_of__retmode_and/?report=objectonly</a></p></li>
<li><p>All the examples and other scripts I have seen on the Internet are only about extracting abstracts. authors etc. and none of them discuss extracting the full text.</p></li>
<li><p>Here is another link that I found that uses Python package Bio, but only accesses the information about authors - 
<a href=""https://www.biostars.org/p/172296/"" rel=""nofollow"">https://www.biostars.org/p/172296/</a></p></li>
</ol>

<p>How can I download full text of the article in text or XML or Pdf format using Entrez utils provided by NCBI? Or are there already available scripts or web crawlers that I can use? </p>
","ruby, bioinformatics, text-mining, biopython, pubmed","<p>You can use <code>biopython</code> to get articles which are on PubMedCentral and then get PDF from it. For all articles which are hosted somewhere else, it is difficult to get a generic solution to get the PDF.</p>

<p>It seems that PubMedCentral does not want you to download articles in bulk. Requests via <code>urllib</code> are blocked, but the same URL works from a browser.</p>

<pre><code>from Bio import Entrez

Entrez.email = ""Your.Name.Here@example.org""


#id is a string list with pubmed IDs
#two of have a public PMC article, one does not
handle = Entrez.efetch(""pubmed"", id=""19304878,19088134"", retmode=""xml"")

records = Entrez.parse(handle)
#checks for all records if they have a PMC identifier
#prints the URL for downloading the PDF
for record in records:
    if record.get('MedlineCitation'):
        if record['MedlineCitation'].get('OtherID'):
           for other_id in record['MedlineCitation']['OtherID']:
               if other_id.title().startswith('Pmc'):
                   print('http://www.ncbi.nlm.nih.gov/pmc/articles/%s/pdf/' % (other_id.title().upper()))
</code></pre>
",3,0,14335,2016-06-14 06:28:24,https://stackoverflow.com/questions/37804479/how-to-download-full-article-text-from-pubmed
How can I analzye only specific sentences in R?,"<p>I have a data set of 4,500 long texts that I analyze using R packages. I want to analyze only sentences that contain specific words, how can I do it? and can I use R to create a second set of all the sentences (which will be independent of the original set?)
thanks</p>
","r, text-mining, text-analysis","<p>Data:</p>

<pre><code>lorem &lt;- ""\nLorem ipsum dolor sit amet, consectetur adipisicing elit,\nsed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nUt enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi\nut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit\nin voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur\nsint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit\nanim id est laborum.\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque\nlaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi\narchitecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas\nsit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione\nvoluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet,\nconsectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et\ndolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum\nexercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi\nconsequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam\nnihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n""
</code></pre>

<p>Write it as a single text file ""lorem_ipsum.txt""</p>

<pre><code>cat(lorem, file=""lorem_ipsum.txt"")
lorem &lt;- readLines(""lorem_ipsum.txt"")
</code></pre>

<p>To return only lines that contain the word ""lit""</p>

<pre><code>output&lt;-grep(""lit"", lorem, value=T)
output

[1] ""Lorem ipsum dolor sit amet, consectetur adipisicing elit,""                                
[2] ""in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur""                
[3] ""sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit""          
[4] ""consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et""
[5] ""consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam""   
</code></pre>

<p>Delete the ""lorem_ipsum.txt"" file</p>

<pre><code>unlink(""lorem_ipsum.txt"")
</code></pre>
",0,-4,39,2016-06-14 22:20:42,https://stackoverflow.com/questions/37823076/how-can-i-analzye-only-specific-sentences-in-r
"Number of vowels, consonants and syllables in a text document with Rapidminer","<p>I have a very short question: how to calculate the number of vowels, consonants and syllables in a text document with RapidMiner?</p>
","text-mining, rapidminer","<p>Replace vowels with blank and calculate lengths before and after. Same for consonants. Syllables more challenging.</p>
",0,0,54,2016-06-16 14:39:14,https://stackoverflow.com/questions/37862350/number-of-vowels-consonants-and-syllables-in-a-text-document-with-rapidminer
How to extract word frequency from document-term matrix?,"<p>I am doing LDA analysis with Python. And I used the following code to create a document-term matrix</p>

<pre><code>corpus = [dictionary.doc2bow(text) for text in texts].
</code></pre>

<p>Is there any easy ways to count the word frequency over the whole corpus. Since I do have the dictionary which is a term-id list, I think I can match the word frequency with term-id.</p>
","python, dictionary, text-mining","<p>You can use <code>nltk</code> in order to count word frequency in string <code>texts</code></p>

<pre><code>from nltk import FreqDist
import nltk
texts = 'hi there hello there'
words = nltk.tokenize.word_tokenize(texts)
fdist = FreqDist(words)
</code></pre>

<p><code>fdist</code> will give you word frequency of given string <code>texts</code>. </p>

<p>However, you have a list of text. One way to count frequency is to use <code>CountVectorizer</code> from <code>scikit-learn</code> for list of strings.</p>

<pre><code>import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
texts = ['hi there', 'hello there', 'hello here you are']
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)
freq = np.ravel(X.sum(axis=0)) # sum each columns to get total counts for each word
</code></pre>

<p>this <code>freq</code> will correspond to value in dictionary <code>vectorizer.vocabulary_</code></p>

<pre><code>import operator
# get vocabulary keys, sorted by value
vocab = [v[0] for v in sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))]
fdist = dict(zip(vocab, freq)) # return same format as nltk
</code></pre>
",6,0,8604,2016-06-16 18:09:14,https://stackoverflow.com/questions/37866450/how-to-extract-word-frequency-from-document-term-matrix
"How to Calculate Values from Tables, Based On the RowNames in Matlab","<p>In Matlab, I have 2 tables, 1 table contains all of other tables' values. First table is named T1</p>

<pre><code>freq = [2;3;4;5;6;54;3;4];
words = {'finn';'jake';'iceking';'marceline';'shelby';'bmo';'naptr';'simon'};

T1 = table(freq,...
      'RowNames',words)
</code></pre>

<p>Table 2 is</p>

<pre><code>freq = [10;3;6;3]
words = {'finn';'jake';'simon';'shelby'}
T2 = table(freq,...
      'RowNames',words)
</code></pre>

<p>How do I use values from T2 into T1 and print like this:</p>

<pre><code>T3=
                                                                                   freq2
finn      %is scanned from T2, words that arent contain in T2, is ignored     2/10    %(2 is taken from T2)     
jake                                                                          3/3  %(3 is taken from T2)   
iceking                                                                       4 or 0 or etc   %(as long as this name is ignored)
marceline                                                                     5 or 0 or etc %(as long as this name is ignored)
shelby                                                                        6/3 %(as long as this name is ignored)
bmo                                                                           54 or 0 or etc  %(as long as this name is ignored)
naptr                                                                         3 or 0 or etc  %(as long as this name is ignored)
simon                                                                         4/6  %(6 is taken from T2)
</code></pre>
","matlab, text, text-mining, mining","<p>This should do it. </p>

<pre><code>%copy T1 to be T3
T3=T1; 
%find where the elements in each table are
[~,T1Ind,T2Ind] = intersect(T1.Properties.RowNames,T2.Properties.RowNames);

%modify the values
T3{T1Ind,1}=T3{T1Ind,1}./T2{T2Ind,1};

%modify the others if you want to 
T3{~ismember(T1.Properties.RowNames,T2.Properties.RowNames),1}=0; %or etc
</code></pre>

<p>(Just keeping the preedited if you ever need it that way around)</p>

<pre><code>%If it should be based on the smaller table
T4 = table(T2.freq./T1{T2.Properties.RowNames,1},'RowNames',T2.Properties.RowNames)
</code></pre>

<p>if you are going to work more with tables you should read ""Access Data in a Table"" in the matlab documentation, its really good for learning the different ways to extract data from a table</p>
",0,0,91,2016-06-20 07:37:32,https://stackoverflow.com/questions/37916929/how-to-calculate-values-from-tables-based-on-the-rownames-in-matlab
Datasets in Biodomain like Word similarity datasets used in word2vec and Glove,"<p>I am training word2vec on biomedical texts. In order to perform word similarity and word analogy tests I want to have pairs of biomedical terms having same relationships(could be any), just like we have a comprehensive list of City-State data in word2vec. I tried searching the web but since I am new to the domain I am finding it confusing.  </p>

<p>So, where can I find the list relevant to Drug-gene or Protein-action, etc? Or how can I mine this data. Please suggest publicly available such datasets. Also, please suggest any additional interesting relationships which I can also query. </p>

<p>Another way would be to use available ontologies as they include relations between concepts such as has-part, is-a-way-of-doing, is-a-cause-of, is-a-symptom-of etc. Can I use ontologies to extract such pairs? If yes, then what ontologies and how? </p>

<p>Are there any gold standard datasets already available that can serve my purpose? </p>
","nlp, bioinformatics, text-mining, biopython","<blockquote>
  <p>So, where can I find the list relevant to Drug-gene or Protein-action,
  etc?</p>
</blockquote>

<p>Have a look at ChEMBL, e.g. <a href=""https://www.ebi.ac.uk/chembl/compound/inspect/CHEMBL25"" rel=""nofollow"">aspirin</a> is linked to its target <a href=""https://www.ebi.ac.uk/chembl/target/inspect/CHEMBL2094253"" rel=""nofollow"">cyclooxygenase</a></p>

<blockquote>
  <p>Another way would be to use available ontologies as they include
  relations between concepts such as has-part, is-a-way-of-doing,
  is-a-cause-of, is-a-symptom-of etc. Can I use ontologies to extract
  such pairs? If yes, then what ontologies and how?</p>
</blockquote>

<p>A good start is the <a href=""http://www.ebi.ac.uk/chebi/init.do"" rel=""nofollow"">ChEBI ontology</a>.</p>
",1,1,77,2016-06-21 17:28:14,https://stackoverflow.com/questions/37950871/datasets-in-biodomain-like-word-similarity-datasets-used-in-word2vec-and-glove
R dictionary: create a many-to-one mapping,"<p>Consider the following MWE in a text mining exercise, using R{tm}:
Toyota has several SUV models in the US.<code>models&lt;-c(""highlander"",""land cruiser"",""rav4"",""sequoia"",""4runner"")</code>. The general media refers to these not as ""toyota rav4"" (corpus already transformed to lower case) but as ""rav4"". To get a single column of toyota suvs in a DocumentTermMatrix, i need to convert all these brands into one generic ""toyota_suv"". What I am doing now is to repeat <code>mycorpus&lt;-tm_map(mycorpus, gsub, pattern=""rav4"", replacement=""toyota_suv"")</code> for length(models). A hack would be to set up <code>model_names&lt;-rep(""toyota_suv"",length(models))</code> and get on with life. How can I set up a dictionary with many-to-one mapping, so that all <code>models</code> are replaced with 'toyota_suv' in one expression? Many thanks.</p>
","r, text-mining, tm","<p>You can use a vectorized substitution function.  The <code>stringi</code> package offers such a function with the <code>stri_replace_all</code> family of functions.  Here, I'm using <code>stri_replace_all_fixed</code>, but adjust case sensitivity and other options as needed.</p>

<pre><code>library(tm)
library(stringi)

toyota_suvs &lt;- c(""highlander"",""land cruiser"",""rav4"",""sequoia"",""4runner"")

tm_map(toyCorp, stri_replace_all_fixed,
    pattern = toyota_suvs, replacement = ""toyota_suv"",
    vectorize_all = FALSE)
</code></pre>

<p>data:</p>

<pre><code>toyExample &lt;- c(""you don't know about the rav4, John Snow"",
    ""the highlander is a great car"",
    ""I want a land cruiser"")

toyCorp &lt;- Corpus(VectorSource(toyExample))
</code></pre>
",4,1,159,2016-06-30 04:50:07,https://stackoverflow.com/questions/38114190/r-dictionary-create-a-many-to-one-mapping
Best way to clean free text then turn into a transaction dataset,"<p>I have survey information that contains free text that I would like to clean then put into a transaction dataset to run in the arules R package. Right now the text looks like this.</p>

<pre><code>id | Answers    
1  | John thinks that the product is not worth the price
2  | Amy believes that the functionality is well above expectations 
</code></pre>

<p>Here's what I'm trying to do:</p>

<pre><code>1 | John | thinks   | Product       | Not   | Worth | Price    
1 | Amy  | Believes | Functionality | Above | Expectations
</code></pre>

<p>Right now I have been able to clean the data using <code>tm</code> package but I don't know what is the best way to convert it to a transaction dataset. I've turned the information into all lowercase and removed the stop words.</p>

<p>Let's just say my data is in data frame called ""Questions"". I am unable to convert the corpus into a transaction dataset after I have cleaned it.</p>
","r, text-mining, market-basket-analysis","<p>You can try:</p>

<pre><code>library(stringr)
str_split(data$Answers, "" "")
</code></pre>

<p>The output is a list: </p>

<pre><code>[[1]]
 [1] ""John""    ""thinks""  ""that""    ""the""     ""product"" ""is""      ""not""     ""worth""   ""the""     ""price""  

[[2]]
[1] ""Amy""           ""believes""      ""that""          ""the""           ""functionality"" ""is""           
[7] ""well""          ""above""         ""expectations"" 
</code></pre>

<h1>Edit:</h1>

<p>Removing duplicates using the <code>unique</code> function:</p>

<pre><code>my_list &lt;- str_split(data$Answers, "" "")
lapply(my_list , unique)

[[1]]
[1] ""John""    ""thinks""  ""that""    ""the""     ""product"" ""is""      ""not""     ""worth""   ""price""  

[[2]]
[1] ""Amy""           ""believes""      ""that""          ""the""           ""functionality"" ""is""           
[7] ""well""          ""above""         ""expectations"" 
</code></pre>
",0,1,214,2016-07-01 02:21:56,https://stackoverflow.com/questions/38136051/best-way-to-clean-free-text-then-turn-into-a-transaction-dataset
text mining sparse/Non-sparse meaning,"<p>Can somebody tell me, meaning for below code and on outputs? I did create Corpus here</p>

<pre><code>frequencies = DocumentTermMatrix(corpus)
frequencies
</code></pre>

<p>output is </p>

<pre><code>&lt;&lt;DocumentTermMatrix (documents: 299, terms: 1297)&gt;&gt;
Non-/sparse entries: 6242/381561
Sparsity           : 98%
Maximal term length: 19
Weighting          : term frequency (tf)
</code></pre>

<p>And code for sparse is here.</p>

<pre><code>sparse = removeSparseTerms(frequencies, 0.97)
sparse
</code></pre>

<p>output is </p>

<pre><code>&gt; sparse
&lt;&lt;DocumentTermMatrix (documents: 299, terms: 166)&gt;&gt;
Non-/sparse entries: 3773/45861
Sparsity           : 92%
Maximal term length: 10
Weighting          : term frequency (tf)
</code></pre>

<p>What is happening over here , What does Non-/sparse entries and Sparsity mean? Can somebody help me in understanding these. </p>

<p>Thank you.</p>
","r, text-mining","<p>By this code you have created a document term matrix of the corpus</p>

<pre><code>frequencies = DocumentTermMatrix(corpus)
</code></pre>

<p>Document Term Matrix (DTM) lists all occurrences of words in the corpus, by document. In the DTM, the documents are represented by rows and the terms (or words) by columns.  If a word occurs in a particular document, then the matrix entry for corresponding to that row and column is 1, else it is 0 (multiple occurrences within a document are recorded – that is, if a word occurs twice in a document, it is recorded as “2” in the relevant matrix entry).</p>

<p>As an example consider corpus of having two documents.</p>

<p>Doc1: bananas are good</p>

<p>Doc2: bananas are yellow</p>

<p>DTM for the above corpus would look like</p>

<pre><code>              banana          are        yellow       good
Doc1            1               1          1            0

Doc2            1               1          0            1
</code></pre>

<p>The output</p>

<pre><code>&lt;&lt;DocumentTermMatrix (documents: 299, terms: 1297)&gt;&gt;
Non-/sparse entries: 6242/381561
Sparsity           : 98%
Maximal term length: 19
Weighting          : term frequency (tf)
</code></pre>

<p>The output signifies that DTM has 299 entries which has over 1297 terms which have appeared at least once. </p>

<pre><code>sparse = removeSparseTerms(frequencies, 0.97)
</code></pre>

<p>Now you are removing those terms which don't appear too often in your data. We will remove any element that doesn't appear in atleast 3% of the entries (or documents). Relating to the above created DTM we are basically removing those columns whose entries are 1 in least number of documents.</p>

<p>Now if you look at the output </p>

<pre><code>&gt; sparse
&lt;&lt;DocumentTermMatrix (documents: 299, terms: 166)&gt;&gt;
Non-/sparse entries: 3773/45861
Sparsity           : 92%
Maximal term length: 10
Weighting          : term frequency (tf)
</code></pre>

<p>The number of entries (documents) are still the same i.e 299 but number of terms terms which have appeared at least once has changed to 166.</p>
",8,5,12793,2016-07-01 09:22:51,https://stackoverflow.com/questions/38141711/text-mining-sparse-non-sparse-meaning
"When running kmeans, is there a rule of thumb on whether tdm or dtm?","<p>My particular corpus contains approx 20k documents and ~9k terms once processed and stemmed.</p>

<p>This is due to the nature of data collection - user submitted online surveys were they tend to leave very short 1 sentence or even 1 or 2 word responses.</p>

<p>If I run <code>kmeans()</code> on the tdm and then the dtm the results are different if I look at e.g. within cluster sum of squares. I know that a tdm is just a transposed dtm and vice versa.</p>

<p>After discussing both tdm and dtm, <a href=""http://www.r-bloggers.com/text-mining-the-complete-works-of-william-shakespeare/"" rel=""nofollow"">this</a> post on r bloggers said:</p>

<blockquote>
  <p>Which of these proves to be most convenient will depend on the
  relative number of documents and terms in your data.</p>
</blockquote>

<p>With so many terms and documents I found plotting a cusplot very difficult. So I removed some sparcity (.96) which left me with 33 terms. But still a very large number of documents. Presumably most text mining scenarios are the reverse, with a higher number of ters relative to documents.</p>

<p>Based on my description would I run kmeans on the tdm or dtm? I'm seeking to group terms together to aim to find out generalizations about why people are submitting these forms.</p>

<p>Sample code block I have been playing with, what exactly is the difference between kfit and kfit1?</p>

<pre><code>library(tm) # for text mining

## make a example corpus
# make a df of documents a to i

# try making some docs mostly about pets
a &lt;- ""dog bunny dog cat hamster""
b &lt;- ""cat cat bunny dog hamster""
c &lt;- ""cat fish dog""
d &lt;- ""cat dog bunny hamster fish""

# try making the remaining docs about fruits
 e &lt;- ""apple mango orange carrot""
f &lt;- ""cabbage apple dog""
g &lt;- ""orange mango cat apple""
h &lt;- ""apple apple orange""
i &lt;- ""apple orange carrot""
j &lt;- c(a,b,c,d,e,f,g,h,i)
x &lt;- data.frame(j)

# turn x into a document term matrix (dtm)
docs &lt;- Corpus(DataframeSource(x))
tdm &lt;- TermDocumentMatrix(docs)
dtm &lt;- DocumentTermMatrix(docs)

# kmeans clustering
set.seed(123)
kfit &lt;- kmeans(tdm, 2)
kfit1 &lt;- kmeans(dtm, 2)
#plot – need library cluster
library(cluster)
clusplot(m, kfit$cluster, color=T, shade=T, labels=2, lines=0)

# t(table(kfit$cluster, 1:dtm$nrow)) for docs based analysis
table(tdm$dimnames$Terms, kfit$cluster) # for term based analysis
</code></pre>
","r, cluster-analysis, k-means, text-mining","<ol>
<li>It depends on the <em>implementation</em>.</li>
</ol>

<p>Usually, implementations expect instances in <em>rows</em>.</p>

<ol start=""2"">
<li>It depends on your task.</li>
</ol>

<p>If you want to cluster documents, then documents should be the instances. Running on the transposed matrix will cluster terms, by the documents they appear in.</p>

<p>Similar to computing row averages vs. column averages, they both are mathematically the same, but have a very different semantic. Doing the wrong thing because it is ""more convenient"" (?!?) sounds like a very bad idea.</p>
",2,-1,735,2016-07-03 21:02:32,https://stackoverflow.com/questions/38174301/when-running-kmeans-is-there-a-rule-of-thumb-on-whether-tdm-or-dtm
How can I use some keywords to find which articles contain these keywords?,"<p>I am a new programmer for R. And I have some articles(.txt) saved in a folder.
Now I can import articles in R. I have two methods and I don't know which one is much better.</p>

<p>Here is my code:</p>

<pre><code># 1
library(tm)   
cname &lt;- file.path(""D:/magazine_pass"")
docs &lt;- Corpus(DirSource(cname), readerControl=list(reader=readPlain))

# 2
dir.list &lt;- list.files(""D:/magazine_pass"" , full.name = TRUE)
for(i in 1:length(dir.list)){
      file0 &lt;- dir.list[i]
      s &lt;- readLines(file0,encoding=""ASCII"")
      s &lt;- sapply(s,function(row) iconv(row, ""ASCII"", ""ASCII"", sub=""""))
   }
</code></pre>

<p>And I am also trying to use some <code>biokeywords(ex.clean energy,wearable device)</code> to find which articles contain these keywords.
How can I do with that?</p>

<p>Please show me the code and simply describe it. Thanks a lot.</p>
","r, text-mining, tm","<p><code>label1 = subset(docs, grepl(paste(c(""clean energy"",""wearable device""), collapse = ""|""), docs))</code></p>

<p>This should look through your corpus and pull out any entries that contain the words inside the grepl function. The basic grep function searches files for a string pattern that matches the pattern provided. grepl returns a logical vector of TRUE/FALSE for whether patterns are matched within the function.</p>
",0,2,131,2016-07-11 16:31:23,https://stackoverflow.com/questions/38311880/how-can-i-use-some-keywords-to-find-which-articles-contain-these-keywords
Stopwords eliminating and vector making,"<p>In text2vec, the only function I could find about Stopwords is “create_vocabulary”. But in text mining mission, we usually need to eliminate the stopwords in the resource document and then build corpus or other further processes. How do we use “stopword” to tackle the documents in building corpus, dtm and tcm using text2vec? </p>

<p>I’ve used tm for text mining before. It has a function for analyzing PDF document, but it reads one paper as several vectors(one line, one vector), not read each of the document as a vector as my expect. Furthermore, the format exchange function in tm have mess code problem in Chinese. If use text2vec to read documents, could it read one paper into a vector?(aka. Is the volume of vector large enough for one paper published on journals?) Otherwise, the corpus and vector built in text2vec compatible with which built in tm?</p>
","r, text-mining, stop-words","<p>There are two ways to create document-term matrix:</p>

<ol>
<li>Using feature hashing</li>
<li>Using vocabulary</li>
</ol>

<p>See <a href=""https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html"" rel=""nofollow"">text-vectorization vignette</a> for details.</p>

<p>You are interesting in 2 choice. This mean you should build vocabulary - set of words/ngrams which will be used in all downstream tasks. <code>create_vocabulary</code> creates vocabulary object and only terms from this object will be used in further steps. So if you will provide <code>stopwords</code> to <code>create_vocabulary</code>, it will remove them from the set of all observed words in the corpus.As you can see you should provide stopwords only once. All the dowstream tasks will work with vocabulary. </p>

<p>Answer on second question. </p>

<p><code>text2vec</code> doesn't provide high-level functions for reading PDF documents. However it allows user to provide custom reader function. All you need is to read full articles with some function and reshape them to character vector where each element corresponds to desired unit of information (full article, paragraph, etc). For example you can easily combine lines into single element with <code>paste()</code> function. For example:</p>

<pre><code>article = c(""sentence 1."", ""sentence 2"")
full_article = paste(article, collapse = ' ')
#  ""sentence 1. sentence 2""
</code></pre>

<p>Hope this helps.</p>
",1,1,614,2016-07-12 07:36:51,https://stackoverflow.com/questions/38322675/stopwords-eliminating-and-vector-making
How to filter documents in a tm corpus in R based on metadata?,"<p>I am using the R tm package and I am trying to select certain documents by their index and their metadata:</p>

<pre><code>orbit_corpus&lt;-Corpus( tm_corpus, readerControl = list(reader=myReader))

meta(my_corpus[[1]])

author  : a8
origin  : Department 
heading : WhiB
id      : 1
year    : 2013
</code></pre>

<p>I would like to get find all documents that within the first hundred documents of my corpus that have been published in 2013.
This works to identify whether the metadata 'year' for document 1 are 2013.      </p>

<pre><code>meta(my_corpus[[1]],""year"") == 2013
[1] TRUE
</code></pre>

<p>I need something that gives me the option to find among the first 100 all indexes, which  meet the criterion.
I would imagine something similar to this (but it does not work and unfortunately would probably also not generate a list of the documents).</p>

<pre><code>meta(orbit_corpus[[1:100]],""year"") == 2013
Error in x$content[[i]] : recursive indexing failed at level 4
</code></pre>

<p>Many thanks for the help!</p>
","r, metadata, text-mining, tm, corpus","<p>You could use <code>tm_filter</code> on the first 100 documents of your corpus (<code>orbit_corpus[1:100]</code>)</p>

<pre><code>tm_filter(orbit_corpus[1:100], FUN = function(x) meta(x)[[""year""]] == ""2013"")
</code></pre>

<hr>

<p><strong>From the documentation</strong></p>

<blockquote>
  <p><code>tm_filter</code> returns a corpus containing documents where <code>FUN</code> matches</p>
</blockquote>
",4,3,2675,2016-07-12 13:08:43,https://stackoverflow.com/questions/38329847/how-to-filter-documents-in-a-tm-corpus-in-r-based-on-metadata
R Parses incomplete text from webpages (HTML),"<p>I am trying to parse the plain text from multiple scientific articles for subsequent text analysis. So far I use a <a href=""https://github.com/tonybreyal/Blog-Reference-Functions/blob/master/R/htmlToText/htmlToText.R"" rel=""nofollow"">R script by Tony Breyal</a> based on the packages <strong>RCurl</strong> and <strong>XML</strong>. This works fine for all targeted journals, except for those published by <em><a href=""http://www.sciencedirect.com"" rel=""nofollow"">http://www.sciencedirect.com</a></em>. When I try to parse the articles from SD (and this is consistent for all tested journals I need to access from SD), the text object in R just stores the first part of the whole document in it. Unfortunately, I am not too familiar with html, but I think the problem should be in the SD html code, since it works in all other cases.
I am aware that some journals are not open accessible, but I have access authorisations and the problems also occur in open access articles (check the example).
This is the code from Github:</p>

<pre><code> htmlToText &lt;- function(input, ...) {
###---PACKAGES ---###
 require(RCurl)
 require(XML)


###--- LOCAL FUNCTIONS ---###
# Determine how to grab html for a single input element
 evaluate_input &lt;- function(input) {    
# if input is a .html file
if(file.exists(input)) {
  char.vec &lt;- readLines(input, warn = FALSE)
  return(paste(char.vec, collapse = """"))
}

# if input is html text
if(grepl(""&lt;/html&gt;"", input, fixed = TRUE)) return(input)

# if input is a URL, probably should use a regex here instead?
if(!grepl("" "", input)) {
  # downolad SSL certificate in case of https problem
  if(!file.exists(""cacert.perm"")) download.file(url=""http://curl.haxx.se/ca/cacert.pem"", destfile=""cacert.perm"")
  return(getURL(input, followlocation = TRUE, cainfo = ""cacert.perm""))
}

# return NULL if none of the conditions above apply
return(NULL)
}

# convert HTML to plain text
convert_html_to_text &lt;- function(html) {
doc &lt;- htmlParse(html, asText = TRUE)
text &lt;- xpathSApply(doc, ""//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]"", xmlValue)
return(text)
}

# format text vector into one character string
collapse_text &lt;- function(txt) {
return(paste(txt, collapse = "" ""))
 }

###--- MAIN ---###
# STEP 1: Evaluate input
html.list &lt;- lapply(input, evaluate_input)

# STEP 2: Extract text from HTML
text.list &lt;- lapply(html.list, convert_html_to_text)

# STEP 3: Return text
text.vector &lt;- sapply(text.list, collapse_text)
return(text.vector)
}
</code></pre>

<p>This is now my code and an example article:</p>

<pre><code>target &lt;- ""http://www.sciencedirect.com/science/article/pii/S1754504816300319""
temp.text &lt;- htmlToText(target)
</code></pre>

<p>The unformatted text stops somewhere in the Method section: </p>

<blockquote>
  <p><em>DNA was extracted using the MasterPure™ Yeast DNA Purification Kit
  (Epicentre, Madison, Wisconsin, USA) following the manufacturer's
  instructions.</em></p>
</blockquote>

<p>Any suggestions/ideas?</p>

<p>P.S. I also tried <strong><code>html_text</code></strong> based on <strong><code>rvest</code></strong> with the same outcome.</p>
","html, r, xml, text-mining, rvest","<p>You can prbly use your existing code and just add <code>?np=y</code> to the end of the URL, but this is a bit more compact:</p>

<pre><code>library(rvest)
library(stringi)

target &lt;- ""http://www.sciencedirect.com/science/article/pii/S1754504816300319?np=y""

pg &lt;- read_html(target)
pg %&gt;%
  html_nodes(xpath="".//div[@id='centerContent']//child::node()/text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]"") %&gt;% 
  stri_trim() %&gt;% 
  paste0(collapse="" "") %&gt;% 
  write(file=""output.txt"")
</code></pre>

<p>A bit of the output (total for that article was >80K): </p>

<pre><code> Fungal Ecology Volume 22 , August 2016, Pages 61–72        175394|| Species richness 
 influences wine ecosystem function through a dominant species Primrose J. Boynton a , , , 
 Duncan Greig a , b a  Max Planck Institute for Evolutionary Biology, Plön, 24306, Germany 
 b  The Galton Laboratory, Department of Genetics, Evolution, and Environment, University 
 College London, London, WC1E 6BT, UK Received 9 November 2015, Revised 27 March 2016, 
 Accepted 15 April 2016, Available online 1 June 2016 Corresponding editor: Marie Louise
 Davey Abstract Increased species richness does not always cause increased ecosystem function. 
 Instead, richness can influence individual species with positive or negative ecosystem effects. 
 We investigated richness and function in fermenting wine, and found that richness indirectly 
 affects ecosystem function by altering the ecological dominance of Saccharomyces cerevisiae . 
 While S. cerevisiae generally dominates fermentations, it cannot dominate extremely species-rich 
 communities, probably because antagonistic species prevent it from growing. It is also diluted 
 from species-poor communities, 
</code></pre>
",1,2,210,2016-07-13 09:37:49,https://stackoverflow.com/questions/38347902/r-parses-incomplete-text-from-webpages-html
Remove stopwords and tolower function slow on a Corpus in R,"<p>I  have corpus roughly with 75 MB data. I am trying to use the following command </p>

<pre><code>tm_map(doc.corpus, removeWords, stopwords(""english""))
tm_map(doc.corpus, tolower)
</code></pre>

<p>This two alone functions are taking at least 40 mins to run. I am looking for speeding up the process as I am using <code>tdm</code> matrix for my model. </p>

<p>I have tried commands like <code>gc()</code> and <code>memory.limit(10000000)</code> very frequently but I am not able to speed up my process speed. </p>

<p>I have a system with <code>4GB</code> RAM and running a local database to read the input data. </p>

<p>Hoping for suggestions to speed up! </p>
","r, performance, text-mining, tm","<p>Maybe you can give quanteda a try</p>

<pre><code>library(stringi)
library(tm)
library(quanteda)

txt &lt;- stri_rand_lipsum(100000L)
print(object.size(txt), units = ""Mb"")
# 63.4 Mb

system.time(
  dfm &lt;- dfm(txt, toLower = TRUE, ignoredFeatures = stopwords(""en"")) 
)
# Elapsed time: 12.3 seconds.
#        User      System verstrichen 
#       11.61        0.36       12.30 

system.time(
  dtm &lt;- DocumentTermMatrix(
    Corpus(VectorSource(txt)), 
    control = list(tolower = TRUE, stopwords = stopwords(""en""))
  )
)
#  User      System verstrichen 
# 157.16        0.38      158.69 
</code></pre>
",2,1,1867,2016-07-14 14:53:58,https://stackoverflow.com/questions/38377483/remove-stopwords-and-tolower-function-slow-on-a-corpus-in-r
R - Package tm - Which terms correspond to each common root after stemming?,"<p>Corpus created, stopwords defined, cleansing done (removePunctuation, removeNumbers, tolower...).</p>

<p>The corpus is now ready to be stemmed. The function is executed correctly and all works as it should, but...</p>

<p><strong>I need to know which words are being stemmed to each common root. Is that possible using the tm package? Or any other package?</strong></p>

<p>For example, <em>TermA1, TermA2, TermB1, TermB2, TermB3</em>, all of them are stemmed to <em>Term</em> and my new Corpus reflect only <em>Term</em>. However, I need also to know which words are associated with each root word, and therefore an optimal output should be:</p>

<pre><code>Term     Stemm
TermA1   Term
TermA2   Term
TermB1   Term
TermB2   Term
TermB3   Term
...
WordA1   Word
WordB1   Word
WordB2   Word
WordB3   Word
WordC1   Word
</code></pre>
","r, nlp, text-mining, tm","<p>In the tm package there is the function <strong>stemCompletion</strong> that allows you to complete each stemmed word given a specific dictionary.</p>

<p>To obtain your output do as follows:</p>

<pre><code>library(tm)
data(""crude"")
words &lt;- stemCompletion(c(""compan"", ""entit"", ""suppl""), crude)
stemmed &lt;-  names(words)
stemcomp &lt;- unname(words)
data.table(stemmed, stemcomp)
</code></pre>

<p>References: <a href=""http://www.inside-r.org/packages/cran/tm/docs/stemCompletion"" rel=""nofollow"">stemCompletion {tm}</a></p>

<p>[UPDATE: more german words]</p>

<p>I tried this to verify the behavior with german vowels:</p>

<pre><code>library(SnowballC)
library(tm)
library(data.table)

text &lt;- c(""für"", ""aktuelle"", ""Nachrichten"", ""und"", ""Themen"", ""Bilder"",
       ""und"", ""Videos"", ""aus"", ""den"", ""Bereichen"", ""News"", ""Wirtschaft"",""Politik"",""können"", ""Fremdschämen"", ""Lebensmüde"", ""Erklärungsnot"")

stem &lt;- stemmed &lt;- wordStem(text, language = ""porter"")
completed &lt;- stemCompletion(stemmed, text)
comparison &lt;- data.table(text, stemmed, completed)
</code></pre>

<p>In the table comparison you can see that the original words with the german vowels are not being stemmed but, if you try to complete a certain given stem like ""f"" with <code>stemCompletion(""f"", text)</code> you will obtain the correct word ""für"". 
This is strange, maybe you can follow from here and try to find some work around.</p>
",2,0,311,2016-07-21 18:17:58,https://stackoverflow.com/questions/38511611/r-package-tm-which-terms-correspond-to-each-common-root-after-stemming
Feature selection in document-feature matrix by using chi-squared test,"<p>I am doing texting mining using natural language processing. I used <code>quanteda</code> package to generate a document-feature matrix (dfm). Now I want to do feature selection using a chi-square test.
I know there were already a lot of people asked this question. However, I couldn't find the relevant code for that. (The answers just gave a brief concept, like this: <a href=""https://stats.stackexchange.com/questions/93101/how-can-i-perform-a-chi-square-test-to-do-feature-selection-in-r"">https://stats.stackexchange.com/questions/93101/how-can-i-perform-a-chi-square-test-to-do-feature-selection-in-r</a>)</p>

<p>I learned that I could use <code>chi.squared</code> in <code>FSelector</code> package but I don't know how to apply this function to a dfm class object (<code>trainingtfidf</code> below). (Shows in the manual, it applies to the predictor variable)</p>

<p>Could anyone give me a hint? I appreciate it!</p>

<p>Example code:</p>

<pre><code>description &lt;- c(""From month 2 the AST and total bilirubine were not measured."", ""16:OTHER - COMMENT REQUIRED IN COMMENT COLUMN;07/02/2004/GENOTYPING;SF- genotyping consent not offered until T4."",  ""M6 is 13 days out of the visit window"")
code &lt;- c(4,3,6)
example &lt;- data.frame(description, code)

library(quanteda)
trainingcorpus &lt;- corpus(example$description)

trainingdfm &lt;- dfm(trainingcorpus, verbose = TRUE, stem=TRUE, toLower=TRUE, removePunct= TRUE, removeSeparators=TRUE, language=""english"", ignoredFeatures = stopwords(""english""), removeNumbers=TRUE, ngrams = 2)

# tf-idf
trainingtfidf &lt;- tfidf(trainingdfm, normalize=TRUE)

sessionInfo()
R version 3.3.0 (2016-05-03)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows &gt;= 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    
</code></pre>
","r, text-mining, feature-selection, quanteda, fselector","<p>Here's a general method for computing Chi-squared values for features.  It requires that you have some variable against which to form the associations, which here could be some classification variable you are using for training your classifier.</p>

<p>Note that I am showing how to do this in the <strong>quanteda</strong> package, but the results should be general enough to work for other text package matrix objects.  Here, I am using the data from the auxiliary <a href=""https://github.com/quanteda/quanteda.corpora"" rel=""nofollow noreferrer""><strong>quantedaData</strong></a> package that has all of the State of the Union addresses of US presidents.</p>

<pre><code>data(data_corpus_sotu, package = ""quanteda.corpora"")
table(docvars(data_corpus_sotu, ""party""))
## Democratic Democratic-Republican            Federalist           Independent 
##         90                    28                     4                     8 
## Republican                  Whig 
##         9                     8 
sotuDemRep &lt;- corpus_subset(data_corpus_sotu, party %in% c(""Democratic"", ""Republican""))

# make the document-feature matrix for just Reps and Dems
sotuDfm &lt;- dfm(sotuDemRep, remove = stopwords(""english""))

# compute chi-squared values for each feature
chi2vals &lt;- apply(sotuDfm, 2, function(x) { 
    chisq.test(as.numeric(x), docvars(sotuDemRep, ""party""))$statistic
})

head(sort(chi2vals, decreasing = TRUE), 10)
## government       will     united     states       year     public   congress       upon 
##   85.19783   74.55845   68.62642   66.57434   64.30859   63.19322   59.49949   57.83603 
##        war     people 
##   57.43142   57.38697 
</code></pre>

<p>These can now be selected using the <code>dfm_select()</code> command.  (Note that column indexing by name would also work.)</p>

<pre><code># select just 100 top Chi^2 vals from dfm
dfmTop100cs &lt;- dfm_select(sotuDfm, names(head(sort(chi2vals, decreasing = TRUE), 100)))
## kept 100 features, from 100 supplied (glob) feature types

head(dfmTop100cs)
## Document-feature matrix of: 182 documents, 100 features.
## (showing first 6 documents and first 6 features)
##               features
## docs           citizens government upon duties constitution present
##   Jackson-1830       14         68   67     12           17      23
##   Jackson-1831       21         26   13      7            5      22
##   Jackson-1832       17         36   23     11           11      18
##   Jackson-1829       17         58   37     16            7      17
##   Jackson-1833       14         43   27     18            1      17
##   Jackson-1834       24         74   67     11           11      29
</code></pre>

<p><strong>Added: With >= v0.9.9 this can be done using the <code>textstat_keyness()</code> function.</strong></p>

<pre><code># to avoid empty factors
docvars(data_corpus_sotu, ""party"") &lt;- as.character(docvars(data_corpus_sotu, ""party""))

# make the document-feature matrix for just Reps and Dems
sotuDfm &lt;- data_corpus_sotu %&gt;%
    corpus_subset(party %in% c(""Democratic"", ""Republican"")) %&gt;%
    dfm(remove = stopwords(""english""))

chi2vals &lt;- dfm_group(sotuDfm, ""party"") %&gt;%
    textstat_keyness(measure = ""chi2"")
head(chi2vals)
#   feature     chi2 p n_target n_reference
# 1       - 221.6249 0     2418        1645
# 2  mexico 181.0586 0      505         182
# 3    bank 164.9412 0      283          60
# 4       "" 148.6333 0     1265         800
# 5 million 132.3267 0      366         131
# 6   texas 101.1991 0      174          37
</code></pre>

<p>This information can then be used to select the most discriminating features, after the sign of the chi^2 score is removed.</p>

<pre><code># remove sign
chi2vals$chi2 &lt;- abs(chi2vals$chi2)
# sort
chi2vals &lt;- chi2vals[order(chi2vals$chi2, decreasing = TRUE), ]
head(chi2vals)
#          feature     chi2 p n_target n_reference
# 1              - 221.6249 0     2418        1645
# 29044 commission 190.3010 0      175         588
# 2         mexico 181.0586 0      505         182
# 3           bank 164.9412 0      283          60
# 4              "" 148.6333 0     1265         800
# 29043        law 137.8330 0      607        1178


dfmTop100cs &lt;- dfm_select(sotuDfm, chi2vals$feature)
## kept 100 features, from 100 supplied (glob) feature types

head(dfmTop100cs, nf = 6)
Document-feature matrix of: 6 documents, 6 features (0% sparse).
6 x 6 sparse Matrix of class ""dfm""
              features
docs           fellow citizens senate house representatives :
  Jackson-1829      5       17      2     3               5 1
  Jackson-1830      6       14      4     6               9 3
  Jackson-1831      9       21      3     1               4 1
  Jackson-1832      6       17      4     1               2 1
  Jackson-1833      2       14      7     4               6 1
  Jackson-1834      3       24      5     1               3 5
</code></pre>
",3,5,2187,2016-07-23 06:19:14,https://stackoverflow.com/questions/38538821/feature-selection-in-document-feature-matrix-by-using-chi-squared-test
Initializing a matrix in R,"<p>I want to initialise a matrix with randomly generated numbers such that the sum of numbers in a row/column is 1 in 1 go.Both do not need to be 1 simultaneously i.e. either row sum is 1 or column sum is 1 </p>
","r, matrix, text-mining","<p>For sum of rows = 1 you could try something like:</p>

<pre><code>num_rows &lt;- 5
num_cols &lt;- 5

random_uniform_matrix &lt;- matrix(runif(num_rows * num_cols), nrow = num_rows, ncol = num_cols)

random_uniform_matrix_normalised &lt;- random_uniform_matrix / rowSums(random_uniform_matrix)

random_uniform_matrix_normalised
# [,1]       [,2]       [,3]       [,4]       [,5]
# [1,] 0.23587728 0.09577532 0.28102271 0.03763127 0.34969342
# [2,] 0.07252286 0.42979916 0.19738456 0.19545165 0.10484177
# [3,] 0.12868304 0.30537875 0.08245634 0.26911364 0.21436823
# [4,] 0.31938540 0.37610285 0.18834984 0.10297283 0.01318908
# [5,] 0.10775810 0.09167090 0.54077248 0.16717661 0.09262190
</code></pre>
",1,0,822,2016-07-26 14:43:31,https://stackoverflow.com/questions/38593030/initializing-a-matrix-in-r
R tm substitute words in Corpus using gsub,"<p>I have a large document corpus with more than 200 documents. As you can expect from such a large corpus, some of the words are misspelled, used in different formats, and so on and so forth. I have done the standard text processing such as convert to lower case, remove punctuation, word stemming. I am trying to substitute some words to correct spelling and standardize them before moving on to analysis. I have done more that 100 substitution using the same syntax as below and for most of the substitutions, it is working as expected. However, some (about 5%) are not working. For example the following substitutions seem to have only limited effect:</p>

<pre><code>docs &lt;- tm_map(docs, content_transformer(gsub), pattern = ""medecin|medicil|medicin|medicinee"", replacement = ""medicine"")
docs &lt;- tm_map(docs, content_transformer(gsub), pattern = ""eephant|eleph|elephabnt|elleph|elephanyt|elephantant|elephantant"", replacement = ""elephant"")
docs &lt;- tm_map(docs, content_transformer(gsub), pattern = ""firehood|firewod|firewoo|firewoodloc|firewoog|firewoodd|firewoodd"", replacement = ""firewood"") 
</code></pre>

<p>By limited effect I mean that even though some substitutions are working, some are not. For example, despite trying to replace ""<em>elephantant</em>"", ""<em>medicinee</em>"", ""<em>firewoodd</em>"", they still exist when I create the DTM (document term matrix).</p>

<p>I have no idea why this mixed effect is happening.</p>

<p>Also the following line is replacing every word in the corpus with some combination of collect:</p>

<pre><code>docs &lt;- tm_map(docs, content_transformer(gsub), pattern = ""colect|colleci|collectin|collectiong|collectng|colllect|"", replacement = ""collect"")
</code></pre>

<p>Just for reference, when I substitute just a single word, I am using the syntax (notice the <em>fixed=TRUE</em>):</p>

<pre><code>docs &lt;- tm_map(docs, content_transformer(gsub), pattern = ""charcola"", replacement = ""charcoal"", fixed=TRUE)
</code></pre>

<p>The one that is a single substitution and failing is:</p>

<pre><code>docs &lt;- tm_map(docs, content_transformer(gsub), pattern = ""dogmonkeycat"", replacement = ""dog monkey cat"", fixed=TRUE)
</code></pre>
","regex, r, text-mining, tm","<p>The issue you have is that the alternations in your patterns are not anchored, and thus only the first one matched ""wins"", i.e. used, and the rest is not considered.</p>

<p>You should either use some ""anchors"" (say, word boundaries) around the alternations:</p>

<pre><code>pattern = ""\\b(medecin|medicil|medicin|medicinee)\\b""
</code></pre>

<p>or just put the longer alternatives <em>before</em> shorter ones:</p>

<pre><code>pattern = ""medicinee|medecin|medicil|medicin""
</code></pre>

<p>Note that you can make the pattern faster by using character classes for commonly mistyped vowels (see <code>[ei]</code>) and groups:</p>

<pre><code>pattern = ""med[ie]ci(?:n(?:ee)?|l)""
</code></pre>
",7,5,7309,2016-07-27 07:00:51,https://stackoverflow.com/questions/38605890/r-tm-substitute-words-in-corpus-using-gsub
R: find ngram using dfm when there are multiple sentences in one document,"<p>I have a big dataset (>1 million rows) and each row is a multi-sentence  text. For example following is a sample of 2 rows:</p>

<pre><code>mydat &lt;- data.frame(text=c('I like apple. Me too','One two. Thank you'),stringsAsFactors = F)
</code></pre>

<p>What I was trying to do is extracting the bigram terms in each row (the ""."" will be able to separate ngram terms). If I simply use the dfm function:</p>

<pre><code>mydfm  = dfm(mydat$text,toLower = T,removePunct = F,ngrams=2)
dtm = as.DocumentTermMatrix(mydfm)
txt_data = as.data.frame(as.matrix(dtm))
</code></pre>

<p>These are the terms I got:</p>

<pre><code>""i_like""     ""like_apple"" ""apple_.""    ""._me""       ""me_too""     ""one_two""    ""two_.""      ""._thank""    ""thank_you"" 
</code></pre>

<p>These are What I expect, basically ""."" is skipped and used to separate the terms:</p>

<pre><code>""i_like""     ""like_apple""  ""me_too""     ""one_two""    ""thank_you"" 
</code></pre>

<p>Believe writing slow loops can solve this as well but given it is a huge dataset I would prefer efficient ways similar to the dfm() in quanteda to solve this. Any suggestions would be appreciated!</p>
","r, nlp, text-mining, quanteda","<p>If your goal is just to extract those bigrams, then you could use <code>tokens</code> twice.  Once to tokenize to sentences, then again to make the ngrams for each sentence.</p>

<pre><code>library(""quanteda"")
mydat$text %&gt;% 
    tokens(mydat$text, what = ""sentence"") %&gt;% 
    as.character() %&gt;%
    tokens(ngrams = 2, remove_punct = TRUE) %&gt;%
    as.character()
#[1] ""I_like""     ""like_apple"" ""Me_too""     ""One_two""    ""Thank_you""
</code></pre>

<p>Insert a <code>tokens_tolower()</code> after the first <code>tokens()</code> call if you like, or use <code>char_tolower()</code> at the end.</p>
",1,1,1269,2016-07-31 02:46:45,https://stackoverflow.com/questions/38680309/r-find-ngram-using-dfm-when-there-are-multiple-sentences-in-one-document
Working with text classification and big sparse matrices in R,"<p>I'm working on a text multi-class classification project and I need to build the document / term matrices and train and test in R language.</p>

<p>I already have datasets that don't fit in the limited dimensionality of the base matrix class in R and would need to build big sparse matrices to be able to classify for example, 100k tweets. I am using the <strong>quanteda</strong> package, as it has been for now more useful and reliable than the package <strong>tm</strong>, where creating a DocumentTermMatrix with a dictionary, makes the process incredibly memory hungry with small datasets. Currently, as I said, I use <strong>quanteda</strong> to build the equivalent Document Term Matrix container that later on I transform into a data.frame to perform the training.</p>

<p>I want to know if there is a way to build such big matrices. I have been reading about the <strong>bigmemory</strong> package that allows this kind of container but I am not sure it will work with <strong>caret</strong> for the later classification. Overall I want to understand the problem and build a workaround to be able to work with bigger datasets, as the RAM is not a (big) problem (32GB) but I'm trying to find a way to do it and I feel completely lost about it.</p>
","r, classification, text-mining, r-caret, quanteda","<p>At what moment did you reach ram constraints? </p>

<p><code>quanteda</code> is good package to work with NLP on medium datasets. But also I suggest to try my <a href=""https://github.com/dselivanov/text2vec"" rel=""noreferrer"">text2vec</a> package. Generally it is considerably memory friendly and doesn't require to load all the raw text into the RAM (for example it can create DTM for wikipedia dump on a 16gb laptop). </p>

<p>Second point is that I strongly don't recommend to convert data into <code>data.frame</code>. Try to work with <code>sparseMatrix</code> objects directly. </p>

<p>Following method will work good for text classification:</p>

<ol>
<li>logistic regression with L1 penalty (see <code>glmnet</code> package)</li>
<li>Linear SVM (see <code>LiblineaR</code>, but worth to serach for alternatives)</li>
<li>Also worth to try `xgboost. I would prefer linear models. So you can try linear booster.</li>
</ol>
",7,3,1703,2016-08-03 23:28:06,https://stackoverflow.com/questions/38755207/working-with-text-classification-and-big-sparse-matrices-in-r
reducing whitespace to 1 space between words,"<p>I have a list of facebook posts from which I have removed symbology.  Now I am left with gaps between the text - 2 or more spaces, which I would like to condense.  How can I remove the extra whitespace so that there is only one space between words?  Also, how could one remove all capital letters that stand by themselves in the text? </p>

<pre><code>&gt; head(posts)
[1] ""Syntel Recruitment Drive in this week for FRESHERS   New Registration Link 2016 for 2013 2014 2015 Passout Graduates Qualification   Any Graduate B E B Tech MCA M E M Tech Syntel Registration Link""            
[2] ""Dont Miss This Opportunity to be get placed in one of the best MNC companies in the world   eBay freshers this week of January 2016  Qualification   Any Graduate Can Apply   eBay Registration Link""            
[3] ""Recent Pass Outs with 55  or More are eligible to Apply in  Wipro   Go to the Updated Link for  LastDay Reference Drive  Jan 2016  Apply Link for  Fresher  Referral  Apply Link""                                
[4] ""Robert Bosch Recruitment Drive in this week for FRESHERS   New Registration Link 2016 for 2013 2014 2015 Passout Graduates Qualification   Any Graduate B E B Tech MCA M E M Tech Robert Bosch Registration Link""
[5] ""Mega  JOB  OPENINGS  OF  THE  YEAR  Mphasis Recruitment for FRESHERS January 2016 Qualification   BE  B Tech  B Sc  BCA  Any Graduates  MCA  MBA  ME  M Tech  Post Graduates  Mphasis Registration Link""         
[6] ""TRIGENT Recruitment Drive in this week for FRESHERS   New Registration Link 2016 for 2013 2014 2015 Passout Graduates Qualification   Any Graduate B E B Tech MCA M E M Tech Trigent Registration Link""  


&gt; dput(head(posts))
c(""Syntel Recruitment Drive in this week for FRESHERS   New Registration Link 2016 for 2013 2014 2015 Passout Graduates Qualification   Any Graduate B E B Tech MCA M E M Tech Syntel Registration Link"", 
""Dont Miss This Opportunity to be get placed in one of the best MNC companies in the world   eBay freshers this week of January 2016  Qualification   Any Graduate Can Apply   eBay Registration Link"", 
""Recent Pass Outs with 55  or More are eligible to Apply in  Wipro   Go to the Updated Link for  LastDay Reference Drive  Jan 2016  Apply Link for  Fresher  Referral  Apply Link"", 
""Robert Bosch Recruitment Drive in this week for FRESHERS   New Registration Link 2016 for 2013 2014 2015 Passout Graduates Qualification   Any Graduate B E B Tech MCA M E M Tech Robert Bosch Registration Link"", 
""Mega  JOB  OPENINGS  OF  THE  YEAR  Mphasis Recruitment for FRESHERS January 2016 Qualification   BE  B Tech  B Sc  BCA  Any Graduates  MCA  MBA  ME  M Tech  Post Graduates  Mphasis Registration Link"", 
""TRIGENT Recruitment Drive in this week for FRESHERS   New Registration Link 2016 for 2013 2014 2015 Passout Graduates Qualification   Any Graduate B E B Tech MCA M E M Tech Trigent Registration Link""
)
</code></pre>
","r, text, text-mining, gsub","<p>using <code>gsub</code>, you could try</p>

<pre><code>posts &lt;- gsub("" +"", "" "", posts)
</code></pre>

<p>This will replace every set of adjacent spaces with a single space.</p>
",1,1,40,2016-08-04 14:35:15,https://stackoverflow.com/questions/38770362/reducing-whitespace-to-1-space-between-words
Distance matrix calculation taking too long in R,"<p>I have a term document matrix (tdm) in R (created from a corpus of around 16,000 texts) and I'm trying to create a distance matrix, but it's not loading and I'm not sure how long its supposed to take(it's already been over 20 minutes).  I also tried creating a distance matrix using the document term matrix format, but it still does not load.  Is there anything I can do to speed up the process.  For the tdm, the rows are the text documents and the columns are the possible words, so the entries in the cells of the matrix are counts of each given word per document.
this is what my code looks like:</p>

<pre><code>library(tm)
library(slam)
library(dplyr)
library(XLConnect)
wb &lt;- loadWorkbook(""Descriptions.xlsx"")
df &lt;- readWorksheet(wb, sheet=1) 
docs &lt;- Corpus(VectorSource(df$Long_Descriptions))
docs &lt;- tm_map(docs, removePunctuation) %&gt;%
  tm_map(removeNumbers) %&gt;%
  tm_map(content_transformer(tolower), lazy = TRUE) %&gt;%
  tm_map(removeWords, stopwords(""english""), lazy = TRUE) %&gt;%
  tm_map(stemDocument, language = c(""english""), lazy = TRUE) 
dtm &lt;- DocumentTermMatrix(docs)
tdm &lt;- TermDocumentMatrix(docs, control = list(removePunctuation = TRUE, stopwords = TRUE))
z&lt;-as.matrix(dist(t(tdm), method = ""cosine""))
</code></pre>

<p>(I know my code should be reproducible, but I'm not sure how I can share my data.  The excel document has one column entitle Long_Descriptions, and example of row values are separated by commas as followed: I like cats, I am a dog person, I have three bunnies, I am a cat person but I want a pet rabbit)</p>
","r, text-mining","<p>Cosine distance is a simple dot product of two matrices with L2 normalization. In your case it even simpler - product of L2 normalized dtm on dtm transposed. Here is reproducible example using <code>Matrix</code> and <code>text2vec</code> packages:</p>

<pre><code>library(text2vec)
library(Matrix)

cosine &lt;- function(m) {
  m_normalized &lt;- m / sqrt(rowSums(m ^ 2))
  tcrossprod(m_normalized)
}

data(""movie_review"")
data = rep(movie_review$review, 3)
it = itoken(data, tolower, word_tokenizer)
v = create_vocabulary(it) %&gt;% 
  prune_vocabulary(term_count_min = 5)
vectorizer = vocab_vectorizer(v)
it = itoken(data, tolower, word_tokenizer)
dtm = create_dtm(it, vectorizer)
dim(dtm)
# 15000 24548

system.time( dtm_cos &lt;- cosine(dtm) )
# user  system elapsed 
# 41.914   6.963  50.761 
dim(dtm)
# 15000 15000
</code></pre>

<p>EDIT:
For <code>tm</code> package see this question: <a href=""https://stackoverflow.com/questions/29750519/r-calculate-cosine-distance-from-a-term-document-matrix-with-tm-and-proxy"">R: Calculate cosine distance from a term-document matrix with tm and proxy</a></p>
",0,0,948,2016-08-06 02:24:06,https://stackoverflow.com/questions/38800009/distance-matrix-calculation-taking-too-long-in-r
"text mining in R, correlation of terms plot with the values","<p>I make a plot about correlation of terms in text mining.</p>
<p>And I would like to put the correlation value besied the line like the image bellow.</p>
<p><a href=""https://i.sstatic.net/mMsrx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mMsrx.png"" alt=""enter image description here"" /></a></p>
<p>What should I add next to plot()? text()? or is there some other option to do it?</p>
<h3>R code; correlation of terms</h3>
<pre><code>freq.terms&lt;-findFreqTerms(dtm, lowfreq=500)[1:25]
plot(dtm,term=freq.terms,corThreshold=0.25,weighting=T)
</code></pre>
","r, correlation, text-mining","<p>Here's where I'm at.  The main idea is to make a list of edge attributes that we can pass into <code>plot</code>.</p>

<pre><code>library(tm)
library(graph)
library(igraph)

# Install Rgraphviz
source(""http://bioconductor.org/biocLite.R"")
biocLite(""Rgraphviz"")

data(""acq"")
dtm &lt;- DocumentTermMatrix(acq,
  control = list(weighting = function(x) weightTfIdf(x, normalize=FALSE),
  stopwords = TRUE))
freq.terms &lt;- findFreqTerms(dtm, lowfreq=10)[1:25]
assocs &lt;- findAssocs(dtm, term=freq.terms, corlimit=0.25)

# Recreate edges, using code from plot.DocumentTermMatrix
m &lt;- dtm
corThreshold &lt;- 0.25
m &lt;- as.matrix(m[, freq.terms])
c &lt;- cor(m)
c[c &lt; corThreshold] &lt;- 0
c[is.na(c)] &lt;- 0
diag(c) &lt;- 0
ig &lt;- graph.adjacency(c, mode=""undirected"", weighted=TRUE)
g1 &lt;- as_graphnel(ig)

# Make edge labels
ew &lt;- as.character(unlist(edgeWeights(g1)))
ew &lt;- ew[setdiff(seq(along=ew), Rgraphviz::removedEdges(g1))]
names(ew) &lt;- edgeNames(g1)
eAttrs &lt;- list()
elabs &lt;- paste(""        "", round(as.numeric(ew), 2)) # so it doesn't print on top of the edge
names(elabs) &lt;- names(ew)
eAttrs$label &lt;- elabs
fontsizes &lt;- rep(7, length(elabs))
names(fontsizes) &lt;- names(ew)
eAttrs$fontsize &lt;- fontsizes

plot(dtm, term=freq.terms, corThreshold=0.25, weighting=T, 
  edgeAttrs=eAttrs)
</code></pre>

<p>The main remaining problem is that the plot prints the edge labels twice: once using default settings, apparently, and another time using the fontsize that we specified in <code>eAttrs</code>.
<a href=""https://i.sstatic.net/1f6RO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1f6RO.png"" alt=""Correlation plot""></a></p>

<p><strong>Edit.</strong> It seems that in order to get the labels to render correctly, we can't use <code>plot</code> directly.  Using <code>renderGraph</code> (which <code>plot</code> calls) seems to work.  We make a numeric vector for the edge weights, and pass this into <code>renderEdgeInfo</code> as the <code>lwd</code> argument.  You'll have to change the manual offset for the labels (elabs &lt;- <code>paste(""   "",...)</code>) so that the labels are the right distance away from the edges.</p>

<pre><code>weights &lt;- as.numeric(ew)
names(weights) &lt;- names(ew)

edgeRenderInfo(g1) &lt;- list(label=elabs, fontsize=fontsizes, lwd=weights*5)
nodeRenderInfo(g1) &lt;- list(shape=""box"", fontsize=20)
g1 &lt;- layoutGraph(g1)
renderGraph(g1)
</code></pre>

<p><a href=""https://i.sstatic.net/K6PlQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/K6PlQ.png"" alt=""enter image description here""></a></p>
",4,3,2467,2016-08-06 13:24:05,https://stackoverflow.com/questions/38804749/text-mining-in-r-correlation-of-terms-plot-with-the-values
In which scenario do you use chunking instead of full parsing?,"<p>Chunking or shallow parsing segments a sentence into a sequence of syntactic constituents or chunks, i.e. sequences of adjacent words grouped on the basis of linguistic properties. It is often referred as efficient and robust approach to parsing natural language and a popular alternative to the full parsing but in which scenario chunking would be the more appropriate technique
over full parsing.</p>
",text-mining,"<p>This is nothing more than my own personal bias, but if for some reason you only need to detect noun and/or verb phrases, you are often might be better off with chunking. E.g., for document clustering, topic tagging, or simply identifying keywords, NP or VP chunking can be more than sufficient. Also, if you need to work with a language for which no tree-banks exist, you might want to fall back to chunking.</p>

<p>Chunking typically has the advantage of being orders of magnitude faster than deep parsing, but modern (perceptron/neural) parsers are much faster than deep parsers used to be five or ten years ago. However, even to date, deep parsing can choke on long sentences. And, obviously, annotating tree-banks to train a deep parser is more costly than annotating the NP/VP phrases or even just building a rule-based chunker - particularly if you need to detect phrases in non-English texts.</p>
",1,0,427,2016-08-09 06:20:19,https://stackoverflow.com/questions/38843440/in-which-scenario-do-you-use-chunking-instead-of-full-parsing
Cleaning Street Addresses in Text Mining,"<p>Looking for a way to remove street addresses from the text I currently have. Is there a regular expression that can detect text within range of numbers? What I'm thinking is that I have a zip code and usually a number at the start of the address.</p>

<p>1234 Parks St., Los Angeles, CA 90001</p>

<p>My main issue is that I want to remove the street name from my dataset while I do my other cleaning and look for other words within my set.</p>

<p>I am using Rstudio to do the cleaning.</p>
","regex, r, text-mining","<p>This returns a character vector. Read the regex as breaking it into three capture-groups with the parens: the first is any count of consecutive digits, followed by any number of non-digits, followed by 5 digits. Return only the first and the third with a space in-between (if there is a match) and make no change if no match;</p>

<pre><code>&gt; gsub(""([0-9]*)(\\D*)(\\d{5})"", ""\\1 \\3"", test)
[1] ""1234 90001"" ""9876 94501""
</code></pre>

<p>It would need further parsing to return a set of numeric vectors</p>

<pre><code>&gt; scan( text=gsub(""([0-9]*)(\\D*)(\\d{5})"", ""\\1 \\3"", test), what=list("""", """") )
Read 2 records
[[1]]
[1] ""1234"" ""9876""

[[2]]
[1] ""90001"" ""94501""
</code></pre>

<p>Probably better to read in zips as character (because you will want to preserve leading zeros), but could convert the street numbers to numeric by changing the <code>what</code> list types:</p>

<pre><code>&gt; scan( text=gsub(""([0-9]*)(\\D*)(\\d{5})"", ""\\1 \\3"", test), what=list( numeric(), """") )
Read 2 records
[[1]]
[1] 1234 9876

[[2]]
[1] ""90001"" ""94501""
</code></pre>

<p>To make this more useful:</p>

<pre><code>&gt; setNames( data.frame( scan( text=gsub(""([0-9]*)(\\D*)(\\d{5})"", ""\\1 \\3"", test), 
                              what=list( numeric(), """") ) , 
                       stringsAsFactors=FALSE), 
            c( ""StrtNumber"", ""ZIP"") )
Read 2 records
  StrtNumber   ZIP
1       1234 90001
2       9876 94501
</code></pre>
",1,-1,931,2016-08-15 21:10:52,https://stackoverflow.com/questions/38963131/cleaning-street-addresses-in-text-mining
Replace part of a string (text mining),"<p>I would like to replace ""Replace"" part within strings from df$x to the first word of df$y column. I have a df like this: </p>

<pre><code>x                 y
ABC-Replace-YUI   M46 Hello
CBD-Replace-TYU   MD5 Hello
DBE-Replace-RTY   M6 Hello
EBF-Replace-ERT   M79 Hello
FBG-Replace-WER   MMM8 Hello
</code></pre>

<p>And I would like to get the following data: </p>

<pre><code>x               y
ABC-M46-YUI     M46 Hello
CBD-MD5-TYU     MD5 Hello
DBE-M6-RTY      M6 Hello
EBF-M79-ERT     M79 Hello
FBG-MMM8-WER    MMM8 Hello
</code></pre>

<p>Unfortunately, I have no experience in text mining and I need the most efficient way to do that as I have a huge dataset with similar substitutions for each row. Thank you.</p>
","r, text-mining","<p>We can use <code>str_replace</code> to replace the 'Replace' with the first word of each string in 'y' column (extracted with <code>word</code>)</p>

<pre><code>library(stringr)
df1$x &lt;- str_replace(df1$x, ""Replace"", word(df1$y,1))
df1$x
#[1] ""ABC-M46-YUI""  ""CBD-MD5-TYU""  ""DBE-M6-RTY""   ""EBF-M79-ERT""  ""FBG-MMM8-WER""
</code></pre>

<h3>data</h3>

<pre><code>df1 &lt;- structure(list(x = c(""ABC-Replace-YUI"", ""CBD-Replace-TYU"", ""DBE-Replace-RTY"", 
""EBF-Replace-ERT"", ""FBG-Replace-WER""), y = c(""M46 Hello"", ""MD5 Hello"", 
""M6 Hello"", ""M79 Hello"", ""MMM8 Hello"")), .Names = c(""x"", ""y""), 
class = ""data.frame"", row.names = c(NA, -5L))
</code></pre>
",2,0,189,2016-08-23 12:45:59,https://stackoverflow.com/questions/39101622/replace-part-of-a-string-text-mining
text2vec in R- Transform new data?,"<p>There is documentation on creating a DTM (document term matrix) for the text2vec package, for example the following where a TFIDF weighting is applied after building the matrix:</p>

<pre><code>data(""movie_review"")
N &lt;- 1000
it &lt;- itoken(movie_review$review[1:N], preprocess_function = tolower,
tokenizer = word_tokenizer)
v &lt;- create_vocabulary(it)
vectorizer &lt;- vocab_vectorizer(v)
it &lt;- itoken(movie_review$review[1:N], preprocess_function = tolower,
tokenizer = word_tokenizer)
dtm &lt;- create_dtm(it, vectorizer)
# get tf-idf matrix from bag-of-words matrix
dtm_tfidf &lt;- transformer_tfidf(dtm)
</code></pre>

<p>It is common practice to create a DTM based on a training dataset and use that dataset as input to a model. Then, when new data is encountered (a test set) one needs to create the same DTM on the new data (meaning all the same terms that were used in the training set). Is there anyway in the package to transform a new data set in this manner (in scikit we have a transform method for just this type of instance).</p>
","r, text-mining, text2vec","<p>Actually when I started <code>text2vec</code> I kept that pipeline at the first place. Now we are preparing new release with updated <a href=""https://github.com/lmullen/text2vec/blob/c33984cd8f70f715d7f0bb1df593ddabe6e4d257/vignettes/text-vectorization.Rmd"" rel=""nofollow"">documentation</a>.</p>

<p>For v0.3 following should work:</p>

<pre><code>data(""movie_review"")
train_rows = 1:1000
prepr = tolower
tok = word_tokenizer

it &lt;- itoken(movie_review$review[train_rows], prepr, tok, ids = movie_review$id[train_rows])
v &lt;- create_vocabulary(it) %&gt;% 
  prune_vocabulary(term_count_min = 5)

vectorizer &lt;- vocab_vectorizer(v)
it &lt;- itoken(movie_review$review[train_rows], prepr, tok)
dtm_train &lt;- create_dtm(it, vectorizer)
# get idf scaling from train data
idf = get_idf(dtm_train)
# create tf-idf
dtm_train_tfidf &lt;- transform_tfidf(dtm_train, idf)

test_rows = 1001:2000
# create iterator
it &lt;- itoken(movie_review$review[test_rows], prepr, tok, ids = movie_review$id[test_rows])
# create dtm using same vectorizer, but new iterator
dtm_test_tfidf &lt;- create_dtm(it, vectorizer) %&gt;% 
  # transform  tf-idf using idf from train data
  transform_tfidf(idf)
</code></pre>
",4,1,1342,2016-08-26 20:45:17,https://stackoverflow.com/questions/39174394/text2vec-in-r-transform-new-data
R Tidytext and unnest_tokens error,"<p>Very new to R and have started to use the tidytext package.</p>

<p>I'm trying to use arguments to feed into the <code>unnest_tokens</code> function so I can do multiple column analysis. So instead of this</p>

<pre><code>library(janeaustenr)
library(tidytext)
library(dplyr)
library(stringr)

original_books &lt;- austen_books() %&gt;%
  group_by(book) %&gt;%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex(""^chapter [\\divxlc]"",
                                                 ignore_case = TRUE)))) %&gt;%
  ungroup()

original_books

tidy_books &lt;- original_books %&gt;%
              unnest_tokens(word, text)
</code></pre>

<p>The last line of code would be:</p>

<pre><code>output&lt;- 'word'
input&lt;- 'text'

tidy_books &lt;- original_books %&gt;%
              unnest_tokens(output, input)
</code></pre>

<p>But I'm getting this:</p>

<blockquote>
  <p>Error in check_input(x) : 
   Input must be a character vector of any length or a list of character
   vectors, each of which has a length of 1.</p>
</blockquote>

<p>I've tried using <code>as.character()</code> without much luck. </p>

<p>Any ideas on how this would work?</p>
","r, text-mining","<p>Try</p>

<pre><code>tidy_books &lt;- original_books %&gt;% 
              unnest_tokens_(output, input)
</code></pre>

<p>with the underscore in <code>unnest_tokens_</code>.</p>

<p><code>unnest_tokens_</code> is the ""standard evaluation"" version of <code>unnest_tokens</code>, and allows you to pass in variable names as strings.  See <a href=""https://cran.r-project.org/web/packages/dplyr/vignettes/nse.html"" rel=""noreferrer"">Non-standard evaluation</a> for a discussion of standard vs non-standard evaluation.</p>
",5,4,11191,2016-08-30 02:13:27,https://stackoverflow.com/questions/39217789/r-tidytext-and-unnest-tokens-error
writeLines is not returning text,"<p>I want to display the entire (or partial) textual content of 400+ documents I have in a Corpus. To do so I've used the function <code>writeLines</code> but it doesn't return the actual text contained in the document, instead it returns this:</p>

<blockquote>
  <p>list(list(content = c("""", """"), meta = list(author = character(0),   atetimestamp = list(sec = 33.0082728862762, min = 22, hour = 12, mday = 5, mon   = 8, year = 116, wday = 1, yday = 248, isdst = 0), description = character(0), heading...... </p>
</blockquote>

<p>This is how I've coded:</p>

<pre><code>library(tm)
library(SnowballC)

#Partition each cell in Excel into separate document
textdata &lt;- read.csv(""C:/Users/biat/Documents/survey/openanswers.csv"", header = FALSE)
require(tm)

doc &lt;- Corpus(DataframeSource(textdata), readerControl = list(language=""swedish""))

writeLines(as.character(doc))
</code></pre>

<p>Does the problem lie in the R-code or in the CSV file? When I've used <code>writeLines</code> together with <code>DirSource</code> it returns the text. Anyone know how to suppress the info it returns above and how to retrieve only the text in the document? </p>
","r, text-mining, tm, data-science","<p>try the following to have the text printed to your console, this is what you ask for if I understand well? </p>

<pre><code>library(tm)
data(""crude"") # example set from tm
output &lt;- sapply( crude, function(x) x$content) #get the content from your object
cat(output) # have your text outputted
</code></pre>

<p>ps: try and supply a reproducible example for your questions</p>
",0,0,340,2016-09-05 12:34:15,https://stackoverflow.com/questions/39330736/writelines-is-not-returning-text
How to find trailing and leading words of a word using R?,"<p>I have a text document which has a million words. Now, I need to know how to find trailing and leading words of a word using R.</p>

<p>For example, If I want to find out the words that are coming before and after the word ""error"". It could be anything like following with leading words</p>

<pre><code>""typo error""
""manual error""
""system error""
</code></pre>

<p>and with trailing words like</p>

<pre><code>""error corrected""
""error found""
""error occurred""
</code></pre>

<p>Any idea how to do this? Thanks in advance for your inputs.</p>
","r, text-mining","<p>For words coming before error:</p>

<pre><code>x &lt;- ""no error and no error and some error"" # input

library(gsubfn)
rx &lt;- ""(\\w+) error""
table(strapplyc(x, rx)[[1]])
</code></pre>

<p>giving:</p>

<pre><code>  no some 
   2    1
</code></pre>

<p>Replace <code>rx</code> with the following for words after error:</p>

<pre><code>rx &lt;- ""error (\\w+)""
</code></pre>
",3,0,96,2016-09-11 13:33:00,https://stackoverflow.com/questions/39436634/how-to-find-trailing-and-leading-words-of-a-word-using-r
"How to break conversation data into pairs of (Context , Response)","<p>I'm using Gensim Doc2Vec model, trying to cluster portions of a customer support conversations. My goal is to give the support team an auto response suggestions.</p>

<p><strong>Figure 1:</strong> shows a sample conversations where the user question is answered in the next conversation line, making it easy to extract the data:</p>

<p><a href=""https://i.sstatic.net/N4ri4.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/N4ri4.gif"" alt=""Figure 1""></a> </p>

<p><sup>during the conversation <strong>""hello""</strong> and <strong>""Our offices are located in NYC""</strong> should be suggested</sup></p>

<hr>

<p><strong>Figure 2:</strong> describes a conversation where the questions and answers are not in sync</p>

<p><a href=""https://i.sstatic.net/oHUQu.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oHUQu.gif"" alt=""Figure 2""></a></p>

<p><sup>during the conversation <strong>""hello""</strong> and <strong>""Our offices are located in NYC""</strong> should be suggested</sup></p>

<hr>

<p><strong>Figure 3:</strong> describes a conversation where the context for the answer is built over time, and for classification purpose (I'm assuming) some of the lines are redundant.</p>

<p><a href=""https://i.sstatic.net/muf6Y.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/muf6Y.gif"" alt=""Figure 3""></a></p>

<p><sup>during the conversation <strong>""here is a link for the free trial account""</strong> should be suggested</sup></p>

<hr>

<p>I have the following data per conversation line (simplified):<br>
who wrote the line (user or agent), text, time stamp</p>

<p>I'm using the following code to train my model:</p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedLineDocument
import datetime

print('Creating documents',datetime.datetime.now().time())
context = TaggedLineDocument('./test_data/context.csv')

print('Building model',datetime.datetime.now().time())

model = Doc2Vec(context,size = 200, window = 10, min_count = 10, workers=4)
print('Training...',datetime.datetime.now().time())

for epoch in range(10):
    print('Run number :',epoch)
    model.train(context)

model.save('./test_data/model')
</code></pre>

<p><strong>Q</strong>: How should I structure my training data and what heuristics could be applied in order to extract it from the raw data?</p>
","python, text-mining, doc2vec, gensym","<p>To train a model I would start by concatenating consecutive sequences of messages. What I would do is, using the timestamps, concatenate the messages without any message in between from the other entity.</p>

<p>For instance:</p>

<pre><code>Hello
I have a problem
I cannot install software X
                                       Hi
                                       What error do you get?
</code></pre>

<p>would be:</p>

<pre><code>Hello I have a problem I cannot install software X
                                       Hi What error do you get?
</code></pre>

<p>Then I would train a model with sentences in that format. I would do that because I am assuming that the conversations have a ""single topic"" all the time between interactions from the entities. And in that scenario suggesting a single message <code>Hi What error do you get?</code> would be totally fine.</p>

<p>Also, take a look at the data. If the questions from the users are usually single-sentenced (as in the examples) sentence detection could help a lot. In that case I would apply sentence detection on the concatenated strings (<code>nltk</code> could be an option) and use only single-sentenced questions for training. That way you can avoid the out-of-sync problem when training the model at the price of reducing the size of the dataset.</p>

<p>On the other hand, I would <em>really</em> consider to start with a very simple method. For example you could score questions by tf-idf and, to get a suggestion, you can take the most similar question in your dataset wrt some metric (e.g. cosine similarity) and suggest the answer for that question. That will perform very bad in sentences with context information (e.g. <code>how do you do it?</code>) but can perform well in sentences like <code>where are you based?</code>.</p>

<p>My last suggestion is because <a href=""https://arxiv.org/abs/1509.01626"" rel=""nofollow"">traditional methods perform even better than complex NN methods when the dataset is small</a>. How big is your dataset?</p>

<p><em>How</em> you train a NN method is also crucial, there are a lot of hyper-parameters, and tuning them properly can be difficult, that's why having a baseline with a simple method can help you a lot to check how well you are doing. In this other <a href=""http://arxiv.org/abs/1607.05368"" rel=""nofollow"">paper</a> they compare the different hyper-parameters for doc2vec, maybe you find it useful.</p>

<p><strong>Edit:</strong> a completely different option would be to train a model to ""link"" questions with answers. But for that you should manually tag each question with the corresponding answer and then train a supervised learning model on that data. That could potentially generalize better but with the added effort of manually labelling the sentences and still it doesn't look like an easy problem to me.</p>
",6,13,1230,2016-09-14 12:00:31,https://stackoverflow.com/questions/39489933/how-to-break-conversation-data-into-pairs-of-context-response
GATE: comparing Token.string to Macro,"<p>What I want, is comparing a 'Token.string' with a defined Macro.</p>

<p>What I tried:</p>

<pre><code>Macro: ADDRESSING_NOUN
({Token.kind == word, Token.string ==~ ""(?i)(sir|madam)""})

Rule: Name
(
  {Token.kind == word, Token.string !=~ ADDRESSING_NOUN}
)
</code></pre>

<p>Sadly this doesn't work. </p>

<p>So is there a way to compare them?</p>
","java, text-mining, gate","<p>My question was answered by <code>Ian Roberts</code> via <code>Gate mailing list</code>.</p>

<p>his answer:  </p>

<pre><code>Template: addressing = ""(?i)(sir|madam)""

Rule: Name
({Token.kind == word, Token.string !=~ [addressing]})
</code></pre>
",1,0,86,2016-09-15 09:06:49,https://stackoverflow.com/questions/39507105/gate-comparing-token-string-to-macro
Replacing `Zero Width No-Break Space` with `space` in R,"<p>I want to replace <code>Zero Width No-Break Space</code> with <code>space</code> in a <code>Persian text</code> using <code>R</code>.
I used the code like this according to <a href=""http://www.fileformat.info/info/unicode/char/feff/index.htm"" rel=""nofollow noreferrer"">this link</a>.</p>
<pre><code>testAdrs&lt;-&quot;خيابان‌ مولوي‌نرسيده‌به‌قيام‌&quot;
testAdrs&lt;-gsub('\xef\xbb\xbf',' ',testAdrs)
</code></pre>
<p>I want my <code>testAdrs</code> to be like <code>&quot;خيابان‌ مولوي‌ نرسيده‌ به‌ قيام‌&quot;</code>, however
there is no change in my string.</p>
<p>what is the problem?</p>
","regex, r, text-mining","<p>As I examined your text <code>'خيابان‌ مولوي‌نرسيده‌به‌قيام‌'</code> on my terminal, I got: </p>

<pre><code>&gt;&gt;&gt; خيابان\U+200C مولوي\U+200Cنرسيده\U+200Cبه\U+200Cقيام\U+
</code></pre>

<p>and converted all these chars to hex in python shell, I got:</p>

<pre><code>&gt;&gt;&gt; binascii.unhexlify(binascii.hexlify(u""خيابان\U+200C مولوي\U+200C نرسيده\U+200C به\U+200C قيام\U+200C"".encode('utf-16'))).decode('utf-16')
u'\u062e\u064a\u0627\u0628\u0627\u0646\u200c \u0645\u0648\u0644\u0648\u064a\u200c \u0646\u0631\u0633\u064a\u062f\u0647\u200c \u0628\u0647\u200c \u0642\u064a\u0627\u0645\u200c'
</code></pre>

<p>You will see that there is no <code>\ufeff</code>(""ZERO WIDTH NO-BREAK SPACE"") in the output of the program above. An another proof is <a href=""https://regex101.com/r/lB8pZ1/1"" rel=""nofollow"">here</a> you will see that <code>ǎ</code> easily be matched but non of <code>\x{feff}</code> is existed.</p>

<p>Thus, the problem of yours is no ""ZERO WIDTH NO-BREAK SPACE"" in your string. I guess kind of space that you want to replace might be <a href=""http://www.fileformat.info/info/unicode/char/200c/index.htm"" rel=""nofollow"">this one</a> <code>\u200C</code>(""ZERO WIDTH NON-JOINER"").</p>
",3,1,707,2016-09-17 11:41:07,https://stackoverflow.com/questions/39546411/replacing-zero-width-no-break-space-with-space-in-r
Quanteda - Extracting identified dictionary words,"<p>I am trying to extract the identified dictionary words from a Quanteda dfm, but have been unable to find a solution. </p>

<p>Does someone have a solution for this?</p>

<p>Sample input: </p>

<pre><code>dict &lt;- dictionary(list(season = c(""spring"", ""summer"", ""fall"", ""winter"")))
dfm  &lt;- dfm(""summer is great"", dictionary  = dict)
</code></pre>

<p>Output:</p>

<pre><code> &gt; dfm
 Document-feature matrix of: 1 document, 1 feature.
 1 x 1 sparse Matrix of class ""dfmSparse""

   features
docs    season
text1      1
</code></pre>

<p>I now know that a seasonality dict word has been identified in the sentence, but I would also like to know which word it was. </p>

<p>This should preferably be extracted in the table format:</p>

<pre><code>docs    dict     dictWord
text1   season   summer
</code></pre>
","r, text-mining, quanteda","<p>You can create a second dfm using the <code>keptFeatures</code> argument, and then <code>cbind()</code> it to the first, dictionaried dfm.</p>

<pre><code>dict &lt;- dictionary(list(season = c(""spring"", ""summer"", ""fall"", ""winter"")))
txt &lt;- ""summer is great""
season_dfm  &lt;- dfm(txt, dictionary  = dict, verbose = FALSE)
dict_dfm &lt;- dfm(txt, select = dict, verbose = FALSE)

cbind(season_dfm, dict_dfm)
## Document-feature matrix of: 1 document, 2 features.
## 1 x 2 sparse Matrix of class ""dfmSparse""
##       season summer
## text1      1      1
</code></pre>

<p>If you want the output as a table, it would be:</p>

<pre><code>dict_df &lt;- as.data.frame(combined_dfm)
names(dict_df)[2] &lt;- ""dictWord""
dict_df
##       season dictWord
## text1      1        1
</code></pre>

<p>but that only works if you have a single dictionary value per text -- otherwise the <code>dict_dfm</code> will have multiple feature columns.</p>
",2,0,1177,2016-09-28 11:38:31,https://stackoverflow.com/questions/39746362/quanteda-extracting-identified-dictionary-words
R- Iteratively combine consecutive elements of a character vector until an empty string element is reached,"<p>I have a character vector made up of long strings (alphanumeric + special characters) such as the one described below.  </p>

<pre><code>txt &lt;- c(
         ""Spicy jalapeno bacon ipsum dolor amet"", 
         ""tenderloin. pariatur quis"",
         """",
         ""consequat pancetta jerky"", 
         ""porchetta non chuck exercitation"",
         ""laborum labore ball tip."",
         """",
         """",
         ""Duis swine turkey kielbasa. Strip "",
         ""steak ribeye laboris,""
        )
</code></pre>

<p>Output needed is  </p>

<pre><code>&gt; txt
[1] ""Spicy jalapeno bacon ipsum dolor amet tenderloin. pariatur quis""
[2] ""consequat pancetta jerky porchetta non chuck exercitation laborum labore ball tip.""
[3] ""Duis swine turkey kielbasa. Strip steak ribeye laboris,""
</code></pre>

<p>Things to consider:<br>
1. The empty string element/s act as linebreakers. They could be more than one consecutively.<br>
2. On joining two elements together, a space needs to be added in between.</p>
","r, text, text-mining","<p>One of a plethora of ways to do this:</p>

<pre><code>library(dplyr)
library(purrr)

data_frame(txt=txt, grp=cumsum(txt=="""")) %&gt;% 
  group_by(grp) %&gt;% 
  do(data_frame(joined=paste0(.$txt, collapse="" ""))) %&gt;% 
  mutate(joined=trimws(joined)) %&gt;% 
  filter(joined != """") %&gt;% 
  ungroup() %&gt;% 
  select(joined) %&gt;% 
  flatten_chr()
## [1] ""Spicy jalapeno bacon ipsum dolor amet tenderloin. pariatur quis""                   
## [2] ""consequat pancetta jerky porchetta non chuck exercitation laborum labore ball tip.""
## [3] ""Duis swine turkey kielbasa. Strip  steak ribeye laboris,""                    
</code></pre>
",2,-1,37,2016-10-04 14:40:54,https://stackoverflow.com/questions/39855188/r-iteratively-combine-consecutive-elements-of-a-character-vector-until-an-empty
R tm package (version),"<p>I am starting a project of text mining using R and almost every resource I've found needs package <code>tm</code>, problem is, this package won't load because it imports package <code>slam</code> which is unavailable for R version 3.3.0.</p>

<ul>
<li><p>Does anyone know a good package for text mining other than this?</p></li>
<li><p>Would it be suitable to downgrade my R version?</p></li>
</ul>
","r, version, text-mining","<p>I simply updated to 3.3.1. This worked for me when I ran into the same thing last week on an OSX system running 3.3.1.   I used the method found here to save my packages, but to my delight I did not have to restore them (maybe bc it's only a minor version?): <a href=""https://www.datascienceriot.com/how-to-upgrade-r-without-losing-your-packages/kris/"" rel=""nofollow"">https://www.datascienceriot.com/how-to-upgrade-r-without-losing-your-packages/kris/</a></p>

<p>Once on 3.3.1, slam installed with no problems.</p>

<p><a href=""https://cran.r-project.org/web/packages/slam/slam.pdf"" rel=""nofollow"">Package Slam</a></p>

<p>Simply download the latest R installer and run it, and you should be good to go. </p>
",1,1,399,2016-10-04 20:19:49,https://stackoverflow.com/questions/39861070/r-tm-package-version
extracting a line of numerical values from text file in R,"<p><strong>THE PROBLEM:</strong> 
I need to extract numerical values from a text file and have the strings transformed to numeric values.</p>

<p>For example, in my text file:</p>

<p>Yada yada yada...</p>

<p>Base frequencies: 0.247 0.355 0.158 0.261</p>

<p>blah blah ...</p>

<p>alpha[0]: 0.466477 rates[0] ac ag at cg ct gt: 0.0987 2.4837 0.4734 0.4902 0.2713 1.0000</p>

<p>more words...
End of text file.</p>

<p>i need to pull out:
 base (a vector, which should be (0.247,0.355,0.158,0.261))
 alpha (a variable which should equal 0.466477)
 rates (a vector which should equal (0.0987, 2.4837, 0.4734, 0.4902))</p>

<p><strong>WHAT I HAVE DONE</strong></p>

<pre><code>library(tm)
#Read in text file
myfile &lt;- ""RAxML_info.gtr1""
mdata &lt;- readLines(my file)
cline &lt;- grep(""Base frequencies:"",mdata,value=TRUE)
as.vector(gsub(""Base frequencies: "", """", cline))
</code></pre>

<blockquote>
  <blockquote>
    <p>[1] ""0.247 0.335 0.158 0.261 ""</p>
  </blockquote>
</blockquote>

<p>this is just treating as one string and I cannot get it to be a vector of numeric values.  </p>

<p>Using RStudio and R version 3.3.1</p>
","r, character, text-mining","<p>As per @HubertL's comment, you can use <code>strsplit</code> to get from where you are to where you want to be:</p>

<pre><code>line &lt;- ""0.247 0.335 0.158 0.261 ""
line &lt;- strsplit( line, split = "" "" )[[1]]
line &lt;- as.numeric( line )

line
[1] 0.247 0.335 0.158 0.261
</code></pre>
",0,-2,88,2016-10-04 23:09:10,https://stackoverflow.com/questions/39863109/extracting-a-line-of-numerical-values-from-text-file-in-r
making a text document a numeric list,"<p>I am trying to automatically make a big corpus into a numeric list. One number per line. For example I have the following data:</p>

<pre><code>Df.txt = 

In the years thereafter, most of the Oil fields and platforms were named after pagan “gods”.
We love you Mr. Brown.
Chad has been awesome with the kids and holding down the fort while I work later than usual! The kids have been busy together playing Skylander on the XBox together, after Kyan cashed in his $$$ from his piggy bank. He wanted that game so bad and used his gift card from his birthday he has been saving and the money to get it (he never taps into that thing either, that is how we know he wanted it so bad). We made him count all of his money to make sure that he had enough! It was very cute to watch his reaction when he realized he did! He also does a very good job of letting Lola feel like she is playing too, by letting her switch out the characters! She loves it almost as much as him.
so anyways, i am going to share some home decor inspiration that i have been storing in my folder on the puter. i have all these amazing images stored away ready to come to life when we get our home.
With graduation season right around the corner, Nancy has whipped up a fun set to help you out with not only your graduation cards and gifts, but any occasion that brings on a change in one's life. I stamped the images in Memento Tuxedo Black and cut them out with circle Nestabilities. I embossed the kraft and red cardstock with TE's new Stars Impressions Plate, which is double sided and gives you 2 fantastic patterns. You can see how to use the Impressions Plates in this tutorial Taylor created. Just one pass through your die cut machine using the Embossing Pad Kit is all you need to do - super easy!
If you have an alternative argument, let's hear it! :)
</code></pre>

<p>First I read the text using the command <code>readLines</code>:</p>

<pre><code>text &lt;- readLines(""Df.txt"", encoding = ""UTF-8"")
</code></pre>

<p>Secondly I get all the text into lower letters and I remove unnecessary spacing:</p>

<pre><code>## Lower cases input:
lower_text &lt;- tolower(text)
## removing leading and trailing spaces:
Spaces_remove &lt;- str_trim(lower_text)
</code></pre>

<p>From here on, I will like to assign each line a number e.g.:</p>

<pre><code>""In the years thereafter, most of the Oil fields and platforms were named after pagan “gods”."" = 1
""We love you Mr. Brown."" = 2
...
""If you have an alternative argument, let's hear it! :)"" = 6
</code></pre>

<p>Any ideas?</p>
","r, text, text-mining","<p>You already do kinda have numeric line # associations with the vector (it's indexed numerically), but…</p>

<pre><code>text_input  &lt;- 'In the years thereafter, most of the Oil fields and platforms were named after pagan “gods”.
We love you Mr. Brown.
Chad has been awesome with the kids and holding down the fort while I work later than usual! The kids have been busy together playing Skylander on the XBox together, after Kyan cashed in his $$$ from his piggy bank. He wanted that game so bad and used his gift card from his birthday he has been saving and the money to get it (he never taps into that thing either, that is how we know he wanted it so bad). We made him count all of his money to make sure that he had enough! It was very cute to watch his reaction when he realized he did! He also does a very good job of letting Lola feel like she is playing too, by letting her switch out the characters! She loves it almost as much as him.
so anyways, i am going to share some home decor inspiration that i have been storing in my folder on the puter. i have all these amazing images stored away ready to come to life when we get our home.
With graduation season right around the corner, Nancy has whipped up a fun set to help you out with not only your graduation cards and gifts, but any occasion that brings on a change in one\'s life. I stamped the images in Memento Tuxedo Black and cut them out with circle Nestabilities. I embossed the kraft and red cardstock with TE\'s new Stars Impressions Plate, which is double sided and gives you 2 fantastic patterns. You can see how to use the Impressions Plates in this tutorial Taylor created. Just one pass through your die cut machine using the Embossing Pad Kit is all you need to do - super easy!
If you have an alternative argument, let\'s hear it! :)'

library(dplyr)
library(purrr)
library(stringi)

textConnection(text_input) %&gt;% 
  readLines(encoding=""UTF-8"") %&gt;% 
  stri_trans_tolower() %&gt;% 
  stri_trim() -&gt; corpus

# data frame with explicit line # column
df &lt;- data_frame(line_number=1:length(corpus), text=corpus)

# list with an explicit line number field
lst &lt;- map(1:length(corpus), ~list(line_number=., text=corpus[.]))

# implicit list numeric ids
as.list(corpus)

# explicit list numeric id's (but they're really string keys)
setNames(as.list(corpus), 1:length(corpus))

# named vector
set_names(corpus, 1:length(corpus))
</code></pre>

<p>There are a <em>plethora</em> of R packages that significantly ease the burden of text processing/NLP ops. Doing this work outside of them is likely to be reinventing the wheel. The <a href=""https://cran.r-project.org/web/views/NaturalLanguageProcessing.html"" rel=""nofollow"">CRAN NLP Task View</a> lists many of them.</p>
",1,0,45,2016-10-09 12:43:08,https://stackoverflow.com/questions/39943770/making-a-text-document-a-numeric-list
Coreference Resolution by CoreNLP CorefChainAnnotation.class not working,"<p>I am using core nlp library to find coreference in my text </p>

<blockquote>
  <p>Tyson lives in New York City with his wife and their two children.</p>
</blockquote>

<p>when I am running this on <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow"">Stanford CoreNLP Online demo</a> it's giving me correct <a href=""https://i.sstatic.net/1Yjdy.png"" rel=""nofollow"">output</a></p>

<p>but when I run this text on my machine it's returning null on this line of code</p>

<blockquote>
  <p>Map graph = document.get(CorefChainAnnotation.class);</p>
</blockquote>

<p>Thank you </p>
","parsing, semantic-web, text-mining, stanford-nlp","<p>Look into this complete example - <a href=""http://blog.pengyifan.com/resolve-coreference-using-stanford-corenlp/"" rel=""nofollow noreferrer"">http://blog.pengyifan.com/resolve-coreference-using-stanford-corenlp/</a>. I guess you are missing something as i am unable to understand the exact reason from the code you provided.</p>
",0,0,139,2016-10-18 13:07:30,https://stackoverflow.com/questions/40109181/coreference-resolution-by-corenlp-corefchainannotation-class-not-working
How to classify new documents with tf-idf?,"<p>If I use the <code>TfidfVectorizer</code> from <code>sklearn</code> to generate feature vectors as:</p>

<p><code>features = TfidfVectorizer(min_df=0.2, ngram_range=(1,3)).fit_transform(myDocuments)</code></p>

<p>How would I then generate feature vectors to classify a new document? Since you cant calculate the tf-idf for a single document. </p>

<p>Would it be a correct approach, to extract the feature names with:</p>

<p><code>feature_names = TfidfVectorizer.get_feature_names()</code></p>

<p>and then count the term frequency for the new document according to the <code>feature_names</code>?</p>

<p>But then I won't get the weights that have the information of a words importance.</p>
","python, scikit-learn, text-mining, tf-idf, text-analysis","<p>You need to save the instance of the TfidfVectorizer, it will remember the term frequencies and vocabulary that was used to fit it. It may make things clearer sense if rather than using <code>fit_transform</code>, you use <code>fit</code> and <code>transform</code> separately:</p>

<pre><code>vec = TfidfVectorizer(min_df=0.2, ngram_range=(1,3))
vec.fit(myDocuments)
features = vec.transform(myDocuments)
new_features = fec.transform(myNewDocuments)
</code></pre>
",11,9,5197,2016-10-18 15:32:44,https://stackoverflow.com/questions/40112373/how-to-classify-new-documents-with-tf-idf
tf-idf implementation,"<p>I,am making Plagiarism Detection for 2 strings and for that I am using ""Levenshtein Distance Algorithm"" to find percentage of plagiarism and ""tf idf"" to find keywords. But now i am having problem for highlighting the text similar text, I am thinking of using keywords as a seed to form clusters and highlight that cluster, but it seems to be alot of work. Can anyone guide me to how to do it, or any other way. Please help me its my college project.</p>
","text-mining, tf-idf","<p>i used LCS to get common substring (i know its not perfect) and used @Mithgroth to highlight those substrings</p>
",0,0,250,2016-10-22 16:46:42,https://stackoverflow.com/questions/40194603/tf-idf-implementation
agrep function of R is not working for text matching,"<p>I am trying to match string using <code>agrep</code> function of <code>R</code>. I do not understand, why it's not returning any value. I am looking a solution which will give  closed match of the given text. In the given example it should show <code>""ms sharda stone crusher prop rupa""</code></p>

<p>I would appreciate any kind of help.
Thanks in advance.</p>

<pre><code>x= as.vector(c(""sharda stone crusher prop roopa"",""sharda stone crusher prop rupa""))
agrep(""ms sharda stone crusher prop rupa devi"",x,ignore.case=T,value=T,max.distance = 0.1, useBytes = FALSE)
character(0)
</code></pre>
","r, text-mining, agrep","<p>It is because of your <code>max.distance</code> parameter. see <code>?agrep</code>.</p>

<p>for instance:</p>

<pre><code>agrep(""ms sharda stone crusher prop rupa devi"",x,ignore.case=T,value=T,max.distance = 0.2, useBytes = FALSE)
""sharda stone crusher prop rupa""
agrep(""ms sharda stone crusher prop rupa devi"",x,ignore.case=T,value=T,max.distance = 0.25, useBytes = FALSE)
""sharda stone crusher prop roopa"" ""sharda stone crusher prop rupa"" 
agrep(""ms sharda stone crusher prop rupa devi"",x,ignore.case=T,value=T,max.distance = 9, useBytes = FALSE)
""sharda stone crusher prop rupa""
agrep(""ms sharda stone crusher prop rupa devi"",x,ignore.case=T,value=T,max.distance = 10, useBytes = FALSE)
""sharda stone crusher prop roopa"" ""sharda stone crusher prop rupa"" 
</code></pre>

<p>If you want only the closest match see:
<a href=""https://stackoverflow.com/questions/5721883/agrep-only-return-best-matches"">best match</a></p>
",0,0,766,2016-10-25 10:02:40,https://stackoverflow.com/questions/40236989/agrep-function-of-r-is-not-working-for-text-matching
Extracting unknown dates from txt/HTML files using R,"<p>I want to extract Dates from txt(or HTML) documents using a Pattern which I identified in the text using the R tm package. I have newspaper articles on my PC in the folders data_X_txt and data_X (in HTML). Each of the folders contains documents named after a company which contains all newspaper articles in one txt or html document. I downloaded these documents in HTML from Lexis Nexis. </p>

<p>For each document I want to know the Upload dates from the contained articles. I identified that the Uploaddate is given for each article following the word UPDATE:.</p>

<p>So I found this question which is similar to my problem 
<a href=""https://stackoverflow.com/questions/37766260/extract-unknown-words-from-a-recurrent-pattern?noredirect=1&amp;lq=1"">Extract unknown words from a recurrent pattern</a></p>

<p>But I have several problems getting to the solution.<br>
First off, I don't know how to correctly upload my Data from the single documents into R for further processing with a regex formula.</p>

<p>Secondly I have problems with understanding and applying the sub formula myself. See this formula, which I found:</p>

<pre><code>sub(""^(?:https?:\\/\\/)?[^\\/]+\\/([^\\/]+).*$"", ""\\1"", tmp[,5])
</code></pre>

<p>I have difficulties adapting the pattern part of sub (the first part I assume) to my problem.
Also I don't know what the second part means. For the third part I know that this is the source of the text but I don't know what [,5] means.</p>

<p>Here the code in full:</p>

<pre><code>tmp &lt;- read.csv(""LaVanguardia_facebook_statuses.csv"")
sub(""^(?:https?:\\/\\/)?[^\\/]+\\/([^\\/]+).*$"", ""\\1"", tmp[,5])
</code></pre>

<p>also a txt file I use:
<a href=""https://www.dropbox.com/s/e24ywni8z3s8wqk/SolarWorldAG_25.03.2008_1.HTML.txt?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/e24ywni8z3s8wqk/SolarWorldAG_25.03.2008_1.HTML.txt?dl=0</a></p>

<p>My knowledge of R is currently Swirl courses and specifically on text mining <a href=""https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html"" rel=""nofollow noreferrer"">https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html</a></p>
","r, regex, text-mining, tm","<p>The text mining package will not help much if all you need are the dates, but the regular expression capabilities of R are pretty useful.</p>

<p>To achieve specifically what you asked for, try <code>gregexpr</code> w/ <code>regmatches</code>:</p>

<pre><code>fileName &lt;- ""~/Downloads/SolarWorldAG_25.03.2008_1.HTML.txt""
mytxt &lt;- readChar(fileName, file.info(fileName)$size)
regmatches(mytxt, regexec(""UPDATE:"",mytxt))

regmatches(mytxt, gregexpr(
""UPDATE: [A-Za-z]{0,10} ?[0-9]{1,2}\\. [A-Z]{1}[a-z|ä]{2,8} [0-9]{4}"", 
mytxt))
</code></pre>

<p>It says, in English: look for the literal <code>UPDATE:</code> followed by a space, followed by an optional set of 0 to 10 characters corresponding to the (optional) day of the week in german, an optional space, a 1 to 2 digit number, a period (escaped by a <code>\\</code>, because reasons) a capital letter, all lowercase letters of the english alphabet and ä, in a sequence of 2 to 8 letters, followed by a space, followed by a 4 digit number. </p>

<p>You get:  </p>

<pre><code>[1] ""UPDATE: 18. März 2008""      ""UPDATE: 14. März 2008""     
[3] ""UPDATE: 13. März 2008""      ""UPDATE: 14. März 2008""     
[5] ""UPDATE: 28. Februar 2008""   ""UPDATE: 20. Februar 2008"" 
...
[189] ""UPDATE: 31. Dezember 2004""      ""UPDATE: 3. Januar 2005""        
[191] ""UPDATE: 9. Dezember 2004""       ""UPDATE: 23. November 2004""      
</code></pre>
",0,1,112,2016-11-01 13:35:17,https://stackoverflow.com/questions/40361176/extracting-unknown-dates-from-txt-html-files-using-r
Text mining and Machine learning,"<p>I have a DataSet of words and texts and I want to make clusters (by K-means )or any other unsupervised/supervised learning method to distinguish words for example , the word 'John' will be classified as a name(and will be clustered with other person names) , 'brazil' as a place and etc...
Is there a model that I can use to solve the problem.
I have Heard of N-grams but I dont know how to plot the Ngrams probability on a x,y plot or such<br>
P.S if you have any examples that will be wonderful  </p>
","machine-learning, nlp, cluster-analysis, text-mining, named-entity-recognition","<p>How about word2vec and embeddings?<br>
<a href=""https://deeplearning4j.org/word2vec"" rel=""nofollow noreferrer"">https://deeplearning4j.org/word2vec</a></p>
",0,-5,216,2016-11-06 18:40:06,https://stackoverflow.com/questions/40453077/text-mining-and-machine-learning
Breaking a paragraph into a vector of sentences in R,"<p>I have the following paragraph:</p>

<blockquote>
  <p>Well, um...such a personal topic. No wonder I am the first to write a review. Suffice to say this stuff does just what they claim and tastes pleasant. And I had, well, major problems in this area and now I don't. 'Nuff said. :-)</p>
</blockquote>

<p>for the purpose of applying the <code>calculate_total_presence_sentiment</code> command from the<code>RSentiment</code> package I would like to break this paragraph into a vector of sentences as follows:</p>

<pre><code>[1] ""Well, um...such a personal topic.""                                       
[2] ""No wonder I am the first to write a review.""                             
[3] ""Suffice to say this stuff does just what they claim and tastes pleasant.""
[4] ""And I had, well, major problems in this area and now I don't.""           
[5] ""'Nuff said.""                                                             
[6] "":-)""
</code></pre>

<p>Would appreciate your help on this.</p>
","r, text-mining","<p><code>qdap</code> has a convenient function for this:</p>

<blockquote>
  <p>sent_detect_nlp - Detect and split sentences on endmark boundaries
  using <strong>openNLP</strong> &amp; <strong>NLP</strong> utilities which matches the onld version of the
  <strong>openNLP</strong> package's now removed <code>sentDetect</code> function.</p>
</blockquote>

<pre><code>library(qdap)

txt &lt;- ""Well, um...such a personal topic. No wonder I am the first to write a review. Suffice to say this stuff does just what they claim and tastes pleasant. And I had, well, major problems in this area and now I don't. 'Nuff said. :-)""

sent_detect_nlp(txt)
#[1] ""Well, um...such a personal topic.""                                       
#[2] ""No wonder I am the first to write a review.""                             
#[3] ""Suffice to say this stuff does just what they claim and tastes pleasant.""
#[4] ""And I had, well, major problems in this area and now I don't.""           
#[5] ""'Nuff said.""                                                             
#[6] "":-)""
</code></pre>
",1,1,1333,2016-11-08 05:18:41,https://stackoverflow.com/questions/40479496/breaking-a-paragraph-into-a-vector-of-sentences-in-r
Make all words uppercase in Wordcloud in R,"<p>When creating Wordclouds it is most common to make all the words lowercase. However, I want the wordclouds to display the words uppercase. After forcing the words to be uppercase the wordcloud still display lowercase words. Any ideas why?</p>

<p>Reproducable code:</p>

<pre><code>    library(tm)
    library(wordcloud)

data &lt;- data.frame(text = c(""Creativity is the art of being ‘productive’ by using
          the available resources in a skillful manner. 
          Scientifically speaking, creativity is part of
          our consciousness and we can be creative –
          if we know – ’what goes on in our mind during
          the process of creation’.
          Let us now look at 6 examples of creativity which blows the mind.""))

text &lt;- paste(data$text, collapse = "" "")

# I am using toupper() to force the words to become uppercase.
text &lt;- toupper(text)

source &lt;- VectorSource(text)
corpus &lt;- VCorpus(source, list(language = ""en""))

# This is my function for cleaning the text                  
clean_corpus &lt;- function(corpus){
             corpus &lt;- tm_map(corpus, removePunctuation)
             corpus &lt;- tm_map(corpus, removeNumbers)
             corpus &lt;- tm_map(corpus, stripWhitespace)
             corpus &lt;- tm_map(corpus, removeWords, c(stopwords(""en"")))
             return(corpus)
}   

clean_corp &lt;- clean_corpus(corpus)
data_tdm &lt;- TermDocumentMatrix(clean_corp)
data_m &lt;- as.matrix(data_tdm)

commonality.cloud(data_m, colors = c(""#224768"", ""#ffc000""), max.words = 50)
</code></pre>

<p>This produces to following output</p>

<p><a href=""https://i.sstatic.net/BZP6w.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BZP6w.png"" alt=""enter image description here""></a></p>
","r, text-mining, tm, word-cloud","<p>It's because behind the scenes <code>TermDocumentMatrix(clean_corp)</code> is doing <code>TermDocumentMatrix(clean_corp, control = list(tolower = TRUE))</code>. If you set it to <code>TermDocumentMatrix(clean_corp, control = list(tolower = FALSE))</code>, then the words stay uppercase. Alternatively, you can also adjust the row names of your matrix afterwards: <code>rownames(data_m) &lt;- toupper(rownames(data_m))</code>.  </p>
",5,3,1687,2016-11-17 11:28:19,https://stackoverflow.com/questions/40653728/make-all-words-uppercase-in-wordcloud-in-r
scikit learn: How to include others features after performed fit and transform of TFIDFVectorizer?,"<p>Just a brief idea of my situation:
I have 4 columns of input: <strong>id</strong>, <strong>text</strong>, <strong>category</strong>, <strong>label</strong>.</p>

<p>I used <strong>TFIDFVectorizer</strong> on the <strong>text</strong> which gives me a list of instances with word tokens of TFIDF score.</p>

<p>Now I'd like to include the <strong>category</strong> (no need to pass TFIDF) as another feature in the data outputed by the vectorizer.</p>

<p>Also note that prior to the vectorization, the data have passed <strong>train_test_split</strong>.</p>

<p>How could I achieve this?</p>

<p>Initial code:</p>

<pre><code>#initialization
import pandas as pd
path = 'data\data.csv'
rappler= pd.read_csv(path)
X = rappler.text
y = rappler.label
#rappler.category - contains category for each instance

#split train test data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

#feature extraction
from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer()
X_train_dtm = vect.fit_transform(X_train)
#after or even prior to perform fit_transform, how can I properly add category as a feature?
X_test_dtm = vect.transform(X_test)

#actual classfication
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(X_train_dtm, y_train)
y_pred_class = nb.predict(X_test_dtm)

#display result
from sklearn import metrics
print(metrics.accuracy_score(y_test,y_pred_class))
</code></pre>
","machine-learning, scikit-learn, text-mining, feature-extraction","<p>I would suggest doing your train test split after feature extraction.</p>

<p>Once you have the TF-IDF feature lists just add the other feature for each sample.</p>

<p>You will have to encode the category feature, a good choice would be <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow noreferrer"">sklearn's LabelEncoder</a>. Then you should have two sets of numpy arrays that can be joined.</p>

<p>Here is a toy example:</p>

<pre><code>X_tfidf = np.array([[0.1, 0.4, 0.2], [0.5, 0.4, 0.6]])
X_category = np.array([[1], [2]])
X = np.concatenate((X_tfidf, X_category), axis=1)
</code></pre>

<p>At this point you would continue as you were, starting with the train test split.</p>
",0,0,1452,2016-11-18 14:46:42,https://stackoverflow.com/questions/40679883/scikit-learn-how-to-include-others-features-after-performed-fit-and-transform-o
How to extract all string matches from a column using a input corpus/list in pandas?,"<p>For example I have the below list of strings as input corpus (actually its a big list with 100 values).
action=['jump','fly','run','swim']</p>

<p>Data contains a column called action_description. How can I extract all the string matches in the action_description using action list as input corpus? </p>

<p>Note: I have already done lemmitization description_action, so if the column have words like jumping or jumped its already converted to jump.</p>

<p><strong><em>Sample input &amp; output</em></strong></p>

<pre><code>""I love to run and while my friend prefer to swim"" --&gt; ""run swim""
""Allan excels at high jump but he is not a good at running"" --&gt; ""jump run""
</code></pre>

<p>Note: I found the below pandas function but its not well documentated so couldnt figure out how to use it. </p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extractall.html"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extractall.html</a></p>

<p>Please recommend a optimal solution since by input dataframe have 200K rows.</p>

<p>EDIT
Words like jumper &amp; runway should be ignore by the algorithm i.e. should not be classified as jump &amp; run.</p>
","python, regex, pandas, nltk, text-mining","<p><strong><em>Steps:</em></strong></p>

<ol>
<li>We perform lemmatization only on verbs by supplying <code>pos='v'</code> and let the nouns remain as they were before by iterating thorugh each word in that list got by <code>str.split</code> operation.</li>
<li>Then, take all the matches of words present in the lookup list and the lemmatized list using <code>set</code>. </li>
<li>Finally, join them to return string as the output.</li>
</ol>

<hr>

<pre><code>from nltk.stem.wordnet import WordNetLemmatizer

action = ['jump','fly','run','swim']     # lookup list
lem = WordNetLemmatizer() 
fcn = lambda x: "" "".join(set([lem.lemmatize(w, 'v') for w in x]).intersection(set(action)))
df['action_description'] = df['action_description'].str.split().apply(fcn)
df
</code></pre>

<p><a href=""https://i.sstatic.net/utD8K.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/utD8K.png"" alt=""enter image description here""></a></p>

<hr>

<p>Starting <code>DF</code> used:</p>

<pre><code>df = pd.DataFrame(dict(action_description=[""I love to run and while my friend prefer to swim"", 
                                           ""Allan excels at high jump but he is not a good at running""]))
</code></pre>

<hr>

<p>To generate binary flags (0/1), we can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.get_dummies.html"" rel=""nofollow noreferrer""><code>str.get_dummies</code></a> method by splitting strings on whitespace and computing it's indicator variables as shown:</p>

<pre><code>bin_flag = df['action_description'].str.get_dummies(sep=' ').add_suffix('_flag')
pd.concat([df['action_description'], bin_flag], axis=1)
</code></pre>

<p><a href=""https://i.sstatic.net/6jkKE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6jkKE.png"" alt=""enter image description here""></a></p>
",2,1,1551,2016-11-21 08:45:41,https://stackoverflow.com/questions/40715950/how-to-extract-all-string-matches-from-a-column-using-a-input-corpus-list-in-pan
converting a text corpus to a text document with vocabulary_id and respective tfidf score,"<p>I have a text corpus with say 5 documents, every document is separated with each other by /n. I want to provide an id to every word in the document and calculate its respective <code>tfidf</code> score.
for example, suppose we have a text corpus named ""corpus.txt"" as follows:-</p>

<p>""Stack
over flow 
text vectorization scikit
python scipy sparse csr""
while calculating the tfidf using</p>

<pre><code>mylist =list(""corpus.text"")
vectorizer= CountVectorizer
x_counts = vectorizer_train.fit_transform(mylist) 
tfidf_transformer = TfidfTransformer()
x_tfidf = tfidf_transformer.fit_transform(x_counts)
</code></pre>

<p>the output is </p>

<pre><code>(0,12) 0.1234 #for 1st document
(1,8) 0.3456  #for 2nd  document
(1,4) 0.8976
(2,15) 0.6754 #for third document
(2,14) 0.2389
(2,3) 0.7823
(3,11) 0.9897 #for fourth document
(3,13) 0.8213
(3,5) 0.7722
(3,6) 0.2211
(4,7) 0.1100 # for fifth document
(4,10) 0.6690
(4,2) 0.0912
(4,9) 0.2345
(4,1) 0.1234
</code></pre>

<p>I converted this <code>scipy.sparse.csr</code> matrix into a list of lists to remove the document id, and keeping only the vocabulary_id and its respective <code>tfidf</code> score using:</p>

<pre><code>m = x_tfidf.tocoo()
mydata = {k: v for k, v in zip(m.col, m.data)} 
key_val_pairs = [str(k) + "":"" + str(v) for k, v in mydata.items()] 
</code></pre>

<p>but the problem is that I am getting an output where the vocabulary_id and its respective <code>tfidf</code> score is arranged in ascending order and without any reference to document.</p>

<p>For example, for the above given corpus my current output(I have dumped into a text file using json) looks like:</p>

<pre><code>1:0.1234
2:0.0912
3:0.7823
4:0.8976
5:0.7722
6:0.2211
7:0.1100
8:0.3456
9:0.2345
10:0.6690
11:0.9897
12:0.1234
13:0.8213
14:0.2389
15:0.6754
</code></pre>

<p>whereas I would have want my text file to be like as follows:</p>

<pre><code>12:0.1234
8:0.3456 4:0.8976
15:0.1234 14:0.2389 3:0.7823
11:0.9897 13:0.8213 5:0.7722 6:0.2211
7:0.1100 10:0.6690 2:0.0912 9:0.2345 1:0.1234
</code></pre>

<p>any idea how to get it done ?</p>
","python, machine-learning, text-mining, tf-idf","<p>I guess this is what you need. Here <code>corpus</code> is a collection of documents.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
corpus = [""stack over flow stack over flow text vectorization scikit"", ""stack over flow""]

vectorizer = TfidfVectorizer()
x = vectorizer.fit_transform(corpus) # corpus is a collection of documents

print(vectorizer.vocabulary_) # vocabulary terms and their index
print(x) # tf-idf weights for each terms belong to a particular document
</code></pre>

<p>This prints:</p>

<pre><code>{'vectorization': 5, 'text': 4, 'over': 1, 'flow': 0, 'stack': 3, 'scikit': 2}
  (0, 2)    0.33195438857 # first document, word = scikit
  (0, 5)    0.33195438857 # word = vectorization
  (0, 4)    0.33195438857 # word = text
  (0, 0)    0.472376562969 # word = flow
  (0, 1)    0.472376562969 # word = over
  (0, 3)    0.472376562969 # word = stack
  (1, 0)    0.57735026919 # second document
  (1, 1)    0.57735026919
  (1, 3)    0.57735026919
</code></pre>

<p>From this information, you can represent the documents in your desired way as following:</p>

<pre><code>cx = x.tocoo()
doc_id = -1
for i,j,v in zip(cx.row, cx.col, cx.data):
    if doc_id == -1:
        print(str(j) + ':' + ""{:.4f}"".format(v), end=' ')
    else:
        if doc_id != i:
            print()
        print(str(j) + ':' + ""{:.4f}"".format(v), end=' ')
    doc_id = i
</code></pre>

<p>This prints:</p>

<pre><code>2:0.3320 5:0.3320 4:0.3320 0:0.4724 1:0.4724 3:0.4724 
0:0.5774 1:0.5774 3:0.5774
</code></pre>
",1,0,692,2016-11-22 12:39:10,https://stackoverflow.com/questions/40742105/converting-a-text-corpus-to-a-text-document-with-vocabulary-id-and-respective-tf
File extension renaming in R,"<p>I am just trying to change the filename extensions to .doc. I'm trying the code below but it does not work. How come? I'm using instructions from <a href=""https://www.r-bloggers.com/programatically-rename-files-or-do-other-stuff-to-them-in-r/"" rel=""noreferrer"">here</a></p>

<pre><code>startingDir&lt;-""C:/Data/SCRIPTS/R/TextMining/myData""

filez&lt;-list.files(startingDir)

sapply(filez,FUN=function(eachPath){
  file.rename(from=eachPath,to=sub(pattern ="".LOG"",replacement="".DOC"",eachPath))
})
</code></pre>

<p>The output I get is:</p>

<pre><code>DD17-01.LOG DD17-02.LOG DD17-03.LOG  DD17-4.LOG  DD17-5.LOG DD37-01.LOG DD37-02.LOG DD39-01.LOG DD39-02.LOG DD39-03.LOG 
      FALSE       FALSE       FALSE       FALSE       FALSE       FALSE       FALSE       FALSE       FALSE       FALSE 
</code></pre>
","r, text-mining, file-rename","<p>It is even easier. Here we start by creating 10 files (in the shell):</p>

<pre><code>$ for i in 0 1 2 3 4 5 6 7 8 9; do touch file${i}.log; done
</code></pre>

<p>Then in R it is really just three <em>vectorized</em> operations:</p>

<pre><code>files &lt;- list.files(pattern=""*.log"")
newfiles &lt;- gsub("".log$"", "".doc"", files)
file.rename(files, newfiles)
</code></pre>

<p>We read the filenames, do the transformation on all of them at once (replacing the trailing <code>.log</code> with <code>.doc</code>) and renamed <em>all</em> files at once from the old names to the new names.  </p>

<p>This will echo a <code>TRUE</code> for each implicit renaming:</p>

<pre><code>edd@max:/tmp/filerename$ Rscript renameFiles.R 
 [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
edd@max:/tmp/filerename$ ls
file0.doc  file1.doc  file2.doc  file3.doc  file4.doc  file5.doc  
file6.doc  file7.doc  file8.doc  file9.doc  renameFiles.R
edd@max:/tmp/filerename$ 
</code></pre>

<p><strong>Edit:</strong> Here is an even more explicit walkthrough doing EVERYTHING in R:</p>

<pre><code>edd@max:/tmp/filerename/new$ ls                    ## no files here
renameFiles.R
edd@max:/tmp/filerename/new$ cat renameFiles.R     ## code we will run

options(width=50)
ignored &lt;- sapply(1:10, function(n) write.csv(n, file=paste0(""file"", n, "".log"")))
files &lt;- list.files(pattern=""*.log"")
print(files)

newfiles &lt;- gsub("".log$"", "".doc"", files)
file.rename(files, newfiles)

files &lt;- list.files(pattern=""*.doc"")
print(files)
edd@max:/tmp/filerename/new$ Rscript renameFiles.R  ## running it
 [1] ""file10.log"" ""file1.log""  ""file2.log"" 
 [4] ""file3.log""  ""file4.log""  ""file5.log"" 
 [7] ""file6.log""  ""file7.log""  ""file8.log"" 
[10] ""file9.log"" 
 [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
[10] TRUE
 [1] ""file10.doc"" ""file1.doc""  ""file2.doc"" 
 [4] ""file3.doc""  ""file4.doc""  ""file5.doc"" 
 [7] ""file6.doc""  ""file7.doc""  ""file8.doc"" 
[10] ""file9.doc"" 
edd@max:/tmp/filerename/new$ 
</code></pre>
",13,8,6113,2016-11-23 01:41:50,https://stackoverflow.com/questions/40754689/file-extension-renaming-in-r
Custom words in Package &#39;qdap&#39; in R,"<p>I am using the qdap package in R to do a spell check. I run the below code and gives an output like this</p>

<pre><code>which_misspelled(""I use a 50Gb broadband connection"") 

&gt; 4           5 
&gt;""gb"" ""broadband""
</code></pre>

<p>The words make sense but the corrections for these are irrelevant.Is there any option where we could give our custom words list for this function to not filter on ?</p>
","r, text-mining, qdap","<p>The function <code>which_misspelled()</code> contains the argument <code>dictionary =</code> which defaults to <a href=""https://rdrr.io/cran/qdapDictionaries/man/GradyAugmented.html"" rel=""nofollow"">qdapDictionaries::GradyAugmented</a>. If your input of words isn't present in there, it will be considered <em>misspelled</em>. </p>

<p>If you want for example the word <code>""gb""</code> to be recognized as correct spelling, you should define a new <code>dictionary</code> :</p>

<pre><code>library(qdap)
dict &lt;- c(qdapDictionaries::GradyAugmented, ""gb"")
which_misspelled(""I use a 50Gb broadband connection"", dictionary = dict)
#          5 
#""broadband"" 
</code></pre>
",3,1,505,2016-11-28 15:15:40,https://stackoverflow.com/questions/40847661/custom-words-in-package-qdap-in-r
Keep EXACT words from R corpus,"<p>From answer posted on: Keep document ID with R corpus by @MrFlick </p>

<p>I am trying to slightly modify what is a great example. </p>

<p><strong>Question:</strong> How do I modify the <em><code>content_transformer</code> function</em> to keep only <strong>exact</strong> words? You can see in the inspect output that wonderful is counted as wonder and ratio is counted as rationale. I do not have a strong understanding of <code>gregexpr</code> and <code>regmatches</code>. </p>

<p>Create data frame:</p>

<pre><code>dd &lt;- data.frame(
  id = 10:13,
  text = c(""No wonderful, then, that ever"",
           ""So that in many cases such a "",
           ""But there were still other and"",
           ""Not even at the rationale"")
  , stringsAsFactors = F
)
</code></pre>

<p>Now, in order to read special attributes from a data.frame, we will use the <code>readTabular</code> function to make our own custom data.frame reader</p>

<pre><code>library(tm)
myReader &lt;- readTabular(mapping = list(content = ""text"", id = ""id""))
</code></pre>

<p>specify the column to use for the contents and the id in the data.frame. Now we read it in with <code>DataframeSource</code> but use our custom reader.</p>

<pre><code>tm &lt;- VCorpus(DataframeSource(dd), readerControl = list(reader = myReader))
</code></pre>

<p>Now if we want to only keep a certain set of words, we can create our own content_transformer function. One way to do this is</p>

<pre><code>  keepOnlyWords &lt;- content_transformer(function(x, words) {
        regmatches(x, 
            gregexpr(paste0(""\\b("",  paste(words, collapse = ""|""), ""\\b)""), x)
        , invert = T) &lt;- "" ""
        x
    })
</code></pre>

<p>This will replace everything that's not in the word list with a space. Note that you probably want to run <code>stripWhitespace</code> after this. Thus our transformations would look like</p>

<pre><code>keep &lt;- c(""wonder"", ""then"", ""that"", ""the"")

tm &lt;- tm_map(tm, content_transformer(tolower))
tm &lt;- tm_map(tm, keepOnlyWords, keep)
tm &lt;- tm_map(tm, stripWhitespace)
</code></pre>

<p>Inspect dtm matrix:</p>

<pre><code>&gt; inspect(dtm)
&lt;&lt;DocumentTermMatrix (documents: 4, terms: 4)&gt;&gt;
Non-/sparse entries: 7/9
Sparsity           : 56%
Maximal term length: 6
Weighting          : term frequency (tf)

    Terms
Docs ratio that the wonder
  10     0    1   1      1
  11     0    1   0      0
  12     0    0   1      0
  13     1    0   1      0
</code></pre>
","r, regex, text-mining, corpus","<p>Switching grammars to <code>tidytext</code>, your current transformation would be</p>

<pre><code>library(tidyverse)
library(tidytext)
library(stringr)

dd %&gt;% unnest_tokens(word, text) %&gt;% 
    mutate(word = str_replace_all(word, setNames(keep, paste0('.*', keep, '.*')))) %&gt;% 
    inner_join(data_frame(word = keep))

##   id   word
## 1 10 wonder
## 2 10    the
## 3 10   that
## 4 11   that
## 5 12    the
## 6 12    the
## 7 13    the
</code></pre>

<p>Keeping exact matches is easier, as you can use joins (which use <code>==</code>) instead of regex:</p>

<pre><code>dd %&gt;% unnest_tokens(word, text) %&gt;% 
    inner_join(data_frame(word = keep))

##   id word
## 1 10 then
## 2 10 that
## 3 11 that
## 4 13  the
</code></pre>

<p>To take it back to a document-term matrix,</p>

<pre><code>library(tm)

dd %&gt;% mutate(id = factor(id)) %&gt;%    # to keep empty rows of DTM
    unnest_tokens(word, text) %&gt;% 
    inner_join(data_frame(word = keep)) %&gt;% 
    mutate(i = 1) %&gt;% 
    cast_dtm(id, word, i) %&gt;% 
    inspect()

## &lt;&lt;DocumentTermMatrix (documents: 4, terms: 3)&gt;&gt;
## Non-/sparse entries: 4/8
## Sparsity           : 67%
## Maximal term length: 4
## Weighting          : term frequency (tf)
## 
##     Terms
## Docs then that the
##   10    1    1   0
##   11    0    1   0
##   12    0    0   0
##   13    0    0   1
</code></pre>

<hr>

<p>Currently, your function is matching <code>words</code> with a boundary before <em>or</em> after. To change it to before <em>and</em> after, change the <code>collapse</code> parameter to include boundaries:</p>

<pre><code>tm &lt;- VCorpus(DataframeSource(dd), readerControl = list(reader = myReader))

keepOnlyWords&lt;-content_transformer(function(x,words) {
        regmatches(x, 
            gregexpr(paste0(""(\\b"",  paste(words, collapse = ""\\b|\\b""), ""\\b)""), x)
        , invert = T) &lt;- "" ""
        x
    })

tm &lt;- tm_map(tm, content_transformer(tolower))
tm &lt;- tm_map(tm, keepOnlyWords, keep)
tm &lt;- tm_map(tm, stripWhitespace)

inspect(DocumentTermMatrix(tm))

## &lt;&lt;DocumentTermMatrix (documents: 4, terms: 3)&gt;&gt;
## Non-/sparse entries: 4/8
## Sparsity           : 67%
## Maximal term length: 4
## Weighting          : term frequency (tf)
## 
##     Terms
## Docs that the then
##   10    1   0    1
##   11    1   0    0
##   12    0   0    0
##   13    0   1    0
</code></pre>
",2,0,1807,2016-12-02 14:46:35,https://stackoverflow.com/questions/40934879/keep-exact-words-from-r-corpus
Data mining: Representing data in transactional/data matrix form,"<p>I am working on the Enron dataset to classify emails and using Python 3. I have pre-processed the data (tokenizing, removing stop words, stemming) and currently working on representing the data in transactional and data-matrix format. This is my understanding of the process:</p>

<ol>
<li>Find tf-idf for every word in every document.</li>
<li>Sort the words based on tfidf scores. </li>
<li>Get top ""k"" words based on score.</li>
<li>Iterate through corpus and find intersection of top ""k"" words with words in every document. Print list of top ""k"" words in every document to get data in transactional form.</li>
<li>Representing the presence/absence (1/0) of top ""k"" words in each document represents data in data matrix form.</li>
</ol>

<h2>Consider following 3 documents:</h2>

<ul>
<li>doc1: The quick fox jumped over the quick dog;  </li>
<li>doc2: The quick fox jumped;  </li>
<li>doc3: The dog was lazy;</li>
</ul>

<h2>tfidf calculation:</h2>

<pre><code>tf(""quick"", doc1) = 2; 
tf(""quick"", doc2) = 1; 
idf(""quick"") = log(3/2) = 0.176; 
tfidf(""quick"", doc1) = 2*0.176 = 0.352; 
tfidf(""quick"", doc2) = 1*0.176 = 0.176; 

tf(""lazy"", doc3) = 1;
idf(""lazy"") = log(3/1) = 0.477;
tfidf(""lazy"", doc3) = 1*0.477 = 0.477;

tf(""fox"", doc1) = 1; 
tf(""fox"", doc2) = 1; 
idf(""fox"") = log(3/2) = 0.176; 
tfidf(""fox"", doc1) = 1*0.176 = 0.176; 
tfidf(""fox"", doc2) = 1*0.176 = 0.176; 

tf(""dog"", doc1) = 1; 
tf(""dog"", doc3) = 1; 
idf(""dog"") = log(3/2) = 0.176; 
tfidf(""dog"", doc1) = 1*0.176 = 0.176; 
tfidf(""dog"", doc3) = 1*0.176 = 0.176; 
</code></pre>

<p>So, if the above words were to be sorted, their rank would be as follows: </p>

<pre><code>lazy (0.477), quick (0.352), quick (0.176), fox(0.176), fox(0.176), dog(0.176), dog(0.176).
</code></pre>

<h2>Questions:</h2>

<ol>
<li>Based on above calculation, what are the top 4 words? Is it for the
overall corpus, or the top word in every document?</li>
<li>Is the sorting of the words correct?</li>
<li>Suppose the top 4 words are: lazy, quick, quick, fox;  </li>
</ol>

<blockquote>
<pre><code>transactional form is:  
doc1: quick, fox, quick 
doc2: quick, fox
doc3: lazy

data-matrix form is: 
doc1: 1,1,0,0,1,0 (quick, fox, jump, over, quick, dog) 
doc2: 1,1,0 (quick, fox, jump) 
doc3: 0,1 (dog, lazy)
</code></pre>
</blockquote>

<p>Above forms will change if the top 4 words were to be: lazy, quick, fox, dog. Is my understanding correct?</p>
","python, data-mining, text-mining, tf-idf","<blockquote>
  <ol>
  <li>Based on above calculation, what are the top 4 words? Is it for the overall corpus, or the top word in every document?</li>
  </ol>
</blockquote>

<p>When you are selecting top <code>k</code> words, it becomes the controlled vocabulary (text mining term) for your corpus. I encourage you to go through this <a href=""http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/"" rel=""nofollow noreferrer"">tutorial</a>. Few important points:</p>

<ul>
<li>When you are selecting top <code>k</code> words from the entire corpus, you are actually considering <code>ttf-idf</code> where <code>ttf</code> mean total term frequency. When you consider one single document and compute a term's frequency, we call it TF. When we do the same for the whole corpus, it becomes TTF.</li>
</ul>

<p><strong>For your example:</strong></p>

<p>Unique words are: <code>The, quick, fox, jumped, over, the, dog, was, lazy</code></p>

<p>I encourage you before pre-process your data, convert them to either upper or lower case. Then <code>The</code> and <code>the</code> will be same!</p>

<p>If you do that, then unique words are: <code>The, quick, fox, jumped, over, dog, was, lazy</code></p>

<p>Total unique words: 8</p>

<p>Term frequencies for each unique words are: </p>

<pre><code>The = 2,1,1 | quick = 2,1,0 | fox = 1,1,0 | jumped = 1,1,0
over = 1,0,0 | dog = 1,0,1 | was = 0,0,1 | lazy = 0,0,1
</code></pre>

<p>Total words in the corpus: <code>8 + 4 + 4 = 16</code></p>

<p>Total term frequency (TTF) and document frequency (DF) for unique words are:</p>

<pre><code>The = 4, 3 | quick = 3, 2 | fox = 2, 2 | jumped = 2, 2
over = 1, 1 | dog = 2, 2 | was = 1, 1 | lazy = 1, 1
</code></pre>

<p>If we just follow a simple definition of inverted document frequency (IDF) as <code>IDF = Log(total documents in corpus / DF)</code>, then TTF-IDF weight (we actually call them TF as well) of each words become:</p>

<pre><code>The = 4 * log(3/3) = 4 * 0 = 0
quick = 3 * log(3/2) = 3 * 0.18 = 0.54
fox = 2 * log(3/2) = 2 * 0.18 = 0.36
jumped = 2 * log(3/2) = 2 * 0.18 = 0.36
over = 1 * log(3/1) = 1 * 0.48 = 0.48
dog = 2 * log(3/2) = 2 * 0.18 = 0.36
was = 1 * log(3/1) = 1 * 0.48 = 0.48
lazy = 1 * log(3/1) = 1 * 0.48 = 0.48
</code></pre>

<p>So, the top 4 words should be: <code>qucik, over, was, lazy</code>. During computing tf-idf weight, you can give different weight to tf or idf. <strong>keep this in mind, you are not selecting top 4 words for each document but from entire corpus.</strong> That's why total term frequency is used instead of term frequency. By the way, when you are consider a whole corpus, <code>term freqeucny</code> and <code>total term frequency</code> terms are used interchangeably. </p>

<blockquote>
  <ol start=""2"">
  <li>Is the sorting of the words correct?</li>
  </ol>
</blockquote>

<p>Sorting is correct. Once you compute tf-idf weight score for each unique terms (we call it dictionary terms in text mining), just <strong>sort them in descending order</strong> in pick top <code>k</code>. You should pick the words with higher tf-idf weight. If your idea is not clear about TF and IDF, i encourage you to read this Wikipedia <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">article</a>.</p>

<blockquote>
  <ol start=""3"">
  <li>Suppose the top 4 words are: lazy, quick, quick, fox; Above forms will change if the top 4 words were to be: lazy, quick, fox, dog. Is my understanding correct?</li>
  </ol>
</blockquote>

<p>Answer of your question is <strong>Yes</strong> because your controlled vocabulary is changed, so as your document respresentation will be. Once you select the top <code>k</code> words, assign them an index value. Then you need to put <code>1</code> if a particular word from vocabulary appears in a document, otherwise <code>0</code>. You can also use the <code>Term-Frequency</code> instead of just putting <code>1</code>.</p>

<p><strong>Note that</strong>, your data matrix is wrong as you have selected top 4 words as controlled vocabulary, the length of each document representation should be 4 as well. So, for example if our controlled vocabulary is: <code>qucik, over, was, lazy</code>, then the document representation should look like as below.</p>

<pre><code>doc1: 1 1 0 0 ['was', 'lazy' missing]
doc2: 1 0 0 0 ['over', 'was', 'lazy' missing]
doc3: 0 0 1 1 ['was', 'lazy' missing]
</code></pre>

<p>You can generate the same using <code>Term-Frequency</code>. Just put term-frequency (respect to individual documents) instead of <code>1</code>. For example, representation for document 1 will look like: <code>2, 1, 0, 0 ['quick' appears twice]</code>. </p>

<p><strong>Remember to follow a particular sequence of the controlled vocabulary terms</strong>. Thats why i said, give an index number to each controlled vocabulary terms. For example, in the examples i provided, i used: <code>quick = 0, over = 1, was = 2, lazy = 3</code>.</p>

<p>One more thing, I want to inform you that the way you are following to represent a document, is called <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow noreferrer"">Bag-of-Words</a> representation. Its very interesting and i encourage you to read documentation on it.</p>

<p>Hopefully, my answer will help you.</p>
",1,0,209,2016-12-09 14:21:05,https://stackoverflow.com/questions/41062502/data-mining-representing-data-in-transactional-data-matrix-form
Lucene scoring: get cosine similarity as scores,"<p>I'm trying to solve nearest neighbor search problem.
Here is my code:</p>

<pre><code>// Indexing
val analyzer = new StandardAnalyzer()
val directory = new RAMDirectory()
val config = new IndexWriterConfig(analyzer)
val iwriter = new IndexWriter(directory, config)

val queryField = ""fieldname""
stringData.foreach { str =&gt;
  val doc = new Document()
  doc.add(new TextField(queryField, str, Field.Store.YES))
  iwriter.addDocument(doc)
}
iwriter.close()

// Searching
val ireader = DirectoryReader.open(directory)
val isearcher = new IndexSearcher(ireader)

val parser = new QueryParser(queryField, analyzer)
val query = parser.parse(""Some text for testing"")

val hits = isearcher.search(query, 10).scoreDocs
</code></pre>

<p>When I look on the value hits I see scores more then 1.</p>

<p>As far as I know, lucene scoring formula is: </p>

<pre><code>score(q,d) = coord-factor(q,d) · query-boost(q) · cosSim(q,d) · doc-len-norm(d) · doc-boost(d)
</code></pre>

<p>But I want to get only cosine similarity in range[0,1] between query and document instead of  coord-factor, doc-len-norm and so on. 
What is a possible way to achieve it?</p>
","elasticsearch, solr, lucene, full-text-search, text-mining","<p>If you have gone through this official <a href=""https://lucene.apache.org/core/4_0_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html"" rel=""nofollow noreferrer"">documentation</a>, you would realize that the rest of the terms in the <code>score</code> expression is important and makes the scoring process more logical and coherent.</p>

<p>But still if you want to achieve a scoring process using only Cosine Similaity, then you can write your custom similarity class. I have used different types of similarity method for document retrieval in my <a href=""http://www.cs.virginia.edu/~hw5x/Course/IR2015/_site/mps/2015/11/12/mp3/"" rel=""nofollow noreferrer"">class assignment</a>. So, in short you can write your own similarity method and assign it to the Lucene's <code>index searcher</code>. I am giving an example here which you modify to accomplish what you want.</p>

<p>Write your custom class (you just need to override one method in your class).</p>

<pre><code>import org.apache.lucene.search.similarities.BasicStats;
import org.apache.lucene.search.similarities.SimilarityBase;

public class MySimilarity extends SimilarityBase {

    @Override
    protected float score(BasicStats stats, float termFreq, float docLength) {
        double tf = 1 + (Math.log(termFreq) / Math.log(2));
        double idf = Math.log((stats.getNumberOfDocuments() + 1) / stats.getDocFreq()) / Math.log(2);
        float dotProduct = (float) (tf * idf);
        return dotProduct;
    }

}
</code></pre>

<p>Then assign your implemented method to <code>index searcher</code> for relevance calculation as below.</p>

<pre><code>IndexReader reader = DirectoryReader.open(FSDirectory.open(new File(indexPath)));
IndexSearcher indexSearcher = new IndexSearcher(reader);
indexSearcher.setSimilarity(new MySimilarity());
</code></pre>

<p>Here, i am using tf-idf dot product to compute similarity between query and documents. Formula is,</p>

<p><a href=""https://i.sstatic.net/r80UA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/r80UA.png"" alt=""enter image description here""></a></p>

<p>Two things need to be mentioned here are:</p>

<ul>
<li><strong>stats.getNumberOfDocuments()</strong> returns total number documents in the index.</li>
<li><strong>stats.getDocFreq()</strong> returns document frequency for a term appeared in both query and document.</li>
</ul>

<p>Lucene will now call the <code>score()</code> method that you have implemented to compute relevance score for each of the matched terms; terms that appeare both in query and documents.</p>

<p>This is not an straight forward answer to your question i know but you can use the approach i mentioned above in anyway you want. I implemented 6 different scoring technique in my homework assignment. I hope it will help you too.</p>
",3,0,1677,2016-12-11 20:39:25,https://stackoverflow.com/questions/41090904/lucene-scoring-get-cosine-similarity-as-scores
Split Speaker and Dialogue in RStudio,"<p>I have documents such as : </p>

<blockquote>
  <p>President Dr. Norbert Lammert: I declare the session open.</p>
  
  <p>I will now give the floor to Bundesminister Alexander Dobrindt. </p>
  
  <p>(Applause of CDU/CSU and delegates of the SPD)   </p>
  
  <p>Alexander Dobrindt, Minister for Transport and Digital Infrastructure: </p>
  
  <p>Ladies and Gentleman. We will today start the biggest investment in infrastructure that ever existed, with over 270 billion Euro, over 1 000 projects and a clear financing perspective. </p>
  
  <p>(Volker Kauder [CDU/CSU]: Genau!)</p>
  
  <p>(Applause of the CDU/CSU and the SPD)                                                                                 </p>
</blockquote>

<p>And when I read those .txt documents I would like to create a second column indicating the speaker name.</p>

<p>So what I tried was to first create a list of all possible names and replace them..   </p>

<pre><code>library(qdap)

members &lt;- c(""Alexander Dobrindt, Minister for Transport and Digital Infrastructure:"",""President Dr. Norbert Lammert:"")
members_r &lt;- c(""@Alexander Dobrindt, Minister for Transport and Digital Infrastructure:"",""@President Dr. Norbert Lammert:"")

prok &lt;- scan("".txt"", what = ""character"", sep = ""\n"")
prok &lt;- mgsub(members,members_r,prok)

prok &lt;- as.data.frame(prok)
prok$speaker &lt;- grepl(""@[^\\@:]*:"",prok$prok, ignore.case = T)
</code></pre>

<p>My plan was to then get the name between @ and : via regex if speaker == true and apply it downwards until there is a different name (and remove all applause/shout brackets obviously), but that is also where I am not sure how I could do that.</p>
","r, text-mining","<p>Here is an approach leaning heavily on <code>dplyr</code>.</p>

<p>First, I added a sentence to your sample text to illustrate why we can't just use a colon to identify speaker names.</p>

<pre><code>sampleText &lt;-
""President Dr. Norbert Lammert: I declare the session open.

I will now give the floor to Bundesminister Alexander Dobrindt.

(Applause of CDU/CSU and delegates of the SPD)

Alexander Dobrindt, Minister for Transport and Digital Infrastructure:

Ladies and Gentleman. We will today start the biggest investment in infrastructure that ever existed, with over 270 billion Euro, over 1 000 projects and a clear financing perspective.

(Volker Kauder [CDU/CSU]: Genau!)

(Applause of the CDU/CSU and the SPD)

This sentence right here: it is an example of a problem""
</code></pre>

<p>I then split the text to simulate the format that it appears you are reading it in (which also puts each speech in a part of a list).</p>

<pre><code>splitText &lt;- strsplit(sampleText, ""\n"")
</code></pre>

<p>Then, I am pulling out all of the potential speakers (anything that precedes a colon) to </p>

<pre><code>allSpeakers &lt;- lapply(splitText, function(thisText){
  grep("":"", thisText, value = TRUE) %&gt;%
    gsub("":.*"", """", .) %&gt;%
    gsub(""\\("", """", .)
}) %&gt;%
  unlist() %&gt;%
  unique()
</code></pre>

<p>Which gives us:</p>

<pre><code>[1] ""President Dr. Norbert Lammert""                                        
[2] ""Alexander Dobrindt, Minister for Transport and Digital Infrastructure""
[3] ""Volker Kauder [CDU/CSU]""                                              
[4] ""This sentence right here"" 
</code></pre>

<p>Obviously, the last one is not a legitimate name, so should be excluded from our list of speakers:</p>

<pre><code>legitSpeakers &lt;-
  allSpeakers[-4]
</code></pre>

<p>Now, we are ready to work through the speech. I have included stepwise comments below, instead of describing in text here</p>

<pre><code>speechText &lt;- lapply(splitText, function(thisText){

  # Remove applause and interjections (things in parentheses)
  # along with any blank lines; though you could leave blanks if you want
  cleanText &lt;-
    grep(""(^\\(.*\\)$)|(^$)"", thisText
         , value = TRUE, invert = TRUE)

  # Split each line by a semicolor
  strsplit(cleanText, "":"") %&gt;%
    lapply(function(x){
      # Check if the first element is a legit speaker
      if(x[1] %in% legitSpeakers){
        # If so, set the speaker, and put the statement in a separate portion
        # taking care to re-collapse any breaks caused by additional colons
        out &lt;- data.frame(speaker = x[1]
                          , text = paste(x[-1], collapse = "":""))
      } else{
        # If not a legit speaker, set speaker to NA and reset text as above
        out &lt;- data.frame(speaker = NA
                          , text = paste(x, collapse = "":""))
      }
      # Return whichever version we made above
      return(out)
    }) %&gt;%
    # Bind all of the rows together
    bind_rows %&gt;%
    # Identify clusters of speech that go with a single speaker
    mutate(speakingGroup = cumsum(!is.na(speaker))) %&gt;%
    # Group by those clusters
    group_by(speakingGroup) %&gt;%
    # Collapse that speaking down into a single row
    summarise(speaker = speaker[1]
              , fullText = paste(text, collapse = ""\n""))
})
</code></pre>

<p>This yields</p>

<pre><code>[[1]]

speakingGroup  speaker                                                                fullText                                                                                                                                                                                                                                        

            1  President Dr. Norbert Lammert                                          I declare the session open.\nI will now give the floor to Bundesminister Alexander Dobrindt.                                                                                                                                                     
            2  Alexander Dobrindt, Minister for Transport and Digital Infrastructure  Ladies and Gentleman. We will today start the biggest investment in infrastructure that ever existed, with over 270 billion Euro, over 1 000 projects and a clear financing perspective.\nThis sentence right here: it is an example of a problem
</code></pre>

<p>If you prefer to have each line of text separately, replace the <code>summarise</code> at the end with <code>mutate(speaker = speaker[1])</code> and you will get one line for each line of the speech, like this:</p>

<pre><code>speaker                                                                text                                                                                                                                                                                      speakingGroup
President Dr. Norbert Lammert                                          I declare the session open.                                                                                                                                                                           1
President Dr. Norbert Lammert                                          I will now give the floor to Bundesminister Alexander Dobrindt.                                                                                                                                       1
Alexander Dobrindt, Minister for Transport and Digital Infrastructure                                                                                                                                                                                                        2
Alexander Dobrindt, Minister for Transport and Digital Infrastructure  Ladies and Gentleman. We will today start the biggest investment in infrastructure that ever existed, with over 270 billion Euro, over 1 000 projects and a clear financing perspective.              2
Alexander Dobrindt, Minister for Transport and Digital Infrastructure  This sentence right here: it is an example of a problem                                                                                                                                               2
</code></pre>
",1,6,398,2016-12-12 12:02:56,https://stackoverflow.com/questions/41100482/split-speaker-and-dialogue-in-rstudio
Lucene: Filter query by doc ID,"<p>I want to have in the search response only documents with specified doc id. In stackoverflow I found this question (<a href=""https://stackoverflow.com/questions/10627655/lucene-filter-with-docids"">Lucene filter with docIds</a>) but as far as I understand there is created the additional field in the document and then doing search by this field. Is there another way to deal with it?</p>
","elasticsearch, solr, lucene, full-text-search, text-mining","<p>Lucene's docids are intended only to be internal keys. You should not be using them as search keys, or storing them for later use. Those ids are subject to change without warning. They will be changed when updating or reindexing documents, and can change at other times, such as segment merges, as well.</p>

<p>If you want your documents to have a unique identifier, you should generate that key separate from the docId, and index it as a field in your document.</p>
",4,0,988,2016-12-15 18:16:30,https://stackoverflow.com/questions/41170794/lucene-filter-query-by-doc-id
Aspect based sentiment analysis libraries,"<p>I'm working on a project where I have to perform aspect-based sentiment analysis on verbal comments. Can anybody suggest some good existing libraries or examples?</p>
","python, text-mining, data-analysis, sentiment-analysis, data-science","<p>Something similar to your project is the Twitter sentiment analysis projects.</p>

<p>Follow some examples:</p>

<ol>
<li><a href=""http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/"" rel=""nofollow noreferrer"">First example</a></li>
<li><a href=""http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/"" rel=""nofollow noreferrer"">Second example</a></li>
<li><a href=""http://www.sananalytics.com/lab/twitter-sentiment/"" rel=""nofollow noreferrer"">Third example</a></li>
</ol>

<p>I suggest you to use <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">NLTK</a> library. Check also the <a href=""http://www.nltk.org/howto/"" rel=""nofollow noreferrer"">'How To Section'</a> for examples.</p>

<p>I hope that these information are usefull.</p>

<p>If these are not in your interest, please add some details to your questio in order to answer better.</p>
",1,0,3410,2016-12-29 17:31:44,https://stackoverflow.com/questions/41384529/aspect-based-sentiment-analysis-libraries
Calculate cosine similarity of all possible text pairs retrieved from 4 mysql tables,"<p>I have 4 tables with schema (app, text_id, title, text). Now I'd like to compute the cosine similarity between all possible text pairs (title &amp; text concatenated) and store them eventually in a csv file with fields (app1, app2, text_id1, text1, text_id2, text2, cosine_similarity).</p>

<p>Since there are a lot of possible combinations it should run quite efficient. What is the most common approach here? I'd appreciate any pointers.</p>

<p>Edit:
Although the provided reference might touch my problem, I still cant figure out how to approach this. Could someone provide more details on the strategy to accomplish this task? Next to the calculated cosine similarity I need also the corresponding text pairs as an output.</p>
","python, numpy, scikit-learn, text-mining, cosine-similarity","<p>The following is a minimal example to calculate the pairwise cosine similarities between a set of documents (assuming you have successfully retrieved the title and text from your database).</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Assume thats the data we have (4 short documents)
data = [
    'I like beer and pizza',
    'I love pizza and pasta',
    'I prefer wine over beer',
    'Thou shalt not pass'
]

# Vectorise the data
vec = TfidfVectorizer()
X = vec.fit_transform(data) # `X` will now be a TF-IDF representation of the data, the first row of `X` corresponds to the first sentence in `data`

# Calculate the pairwise cosine similarities (depending on the amount of data that you are going to have this could take a while)
S = cosine_similarity(X)

'''
S looks as follows:
array([[ 1.        ,  0.4078538 ,  0.19297924,  0.        ],
       [ 0.4078538 ,  1.        ,  0.        ,  0.        ],
       [ 0.19297924,  0.        ,  1.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ,  1.        ]])

The first row of `S` contains the cosine similarities to every other element in `X`. 
For example the cosine similarity of the first sentence to the third sentence is ~0.193. 
Obviously the similarity of every sentence/document to itself is 1 (hence the diagonal of the sim matrix will be all ones). 
Given that all indices are consistent it is straightforward to extract the corresponding sentences to the similarities.
'''
</code></pre>
",7,1,5364,2017-01-06 11:12:43,https://stackoverflow.com/questions/41504454/calculate-cosine-similarity-of-all-possible-text-pairs-retrieved-from-4-mysql-ta
Translation and mapping of emoticons encoded as UTF-8 code in text,"<p>I am working with text which includes emoticons. I need to be able to find these and replace them with tags which can be analysed. How to do this?</p>

<pre><code>&gt; main$text[[4]]
[1] ""Spread d wrd\xf0\u009f\u0098\u008e""
&gt; grepl(""\xf0"", main$text[[4]])
[1] FALSE
</code></pre>

<p>I tried the above. <strong>Why did it not work?</strong> I also tried <code>iconv</code> into ASCII, then the byte encoding I got, could be searched with grepl.</p>

<pre><code>&gt; abc&lt;-iconv(main$text[[4]], ""UTF-8"", ""ASCII"", ""byte"")
&gt; abc
[1] ""Spread d wrd&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;8e&gt;""
&gt; grepl(""&lt;f0&gt;"", abc)
[1] TRUE
</code></pre>

<p><strong>I really do not understand what I did here and what happened.</strong> I also do not understand how the above conversion introduced <code>\n</code> characters into the text.</p>

<p>I also did not know how to encode these, once they were searcheable. I found a list <a href=""https://github.com/today-is-a-good-day/Emoticons/blob/master/emDict.csv"" rel=""nofollow noreferrer"">here</a>, but it fell short (for example, <code>""U+E00E"" - &lt;ee&gt;&lt;80&gt;&lt;8e&gt;</code> was not in the list). Is there a comprehensive list for such a mapping?</p>

<p><strong><em>ADDENDUM</em></strong></p>

<p>After a lot of trial and error, here is what I realised. There are two kinds of encodings for the emojis in the data. One is in the form of bytes, which is searchable by <code>grepl(""\x9f"", ...., useBytes=T)</code>, like the <code>main$text[[4]]</code>, and another (<code>main$text[[6]]</code>) which is searchable as the unicode character without <code>useBytes=T</code>, i.e. <code>grepl(""\ue00e"",....)</code>. Even the way they are displayed in <code>View()</code> and when called on the console is different. <strong>I am absolutely confused as to what is going on here</strong>.</p>

<pre><code> main$text[[4]]
[1] ""Spread d wrd\xf0\u009f\u0098\u008e""
 main[4,]
            timestamp fromMe              remoteResource remoteResourceDisplayName type
b 2014-08-30 02:58:58  FALSE 112233@s.whatsapp.net                ABC text
                                      text   date
b Spread d wrd&lt;f0&gt;&lt;U+009F&gt;&lt;U+0098&gt;&lt;U+008E&gt; 307114
 main$text[[6]]
[1] """"
 main[6,]
            timestamp fromMe              remoteResource remoteResourceDisplayName type     text
b 2014-08-30 02:59:17  FALSE 12345@s.whatsapp.net           XYZ text &lt;U+E00E&gt;
    date
b 307114
 grepl(""\ue00e"", main$text[[6]])
[1] TRUE
 grepl(""&lt;U+E00E&gt;"", main$text[[6]])
[1] FALSE
 grepl(""\u009f"", main$text[[4]])
[1] FALSE
 grepl(""\x9f"", main$text[[4]])
[1] FALSE
 grepl(""\x9f"", main$text[[4]], fixed=T)
[1] FALSE
 grepl(""\x9f"", main$text[[4]], useBytes=T)
[1] TRUE
</code></pre>

<p>The maps I have are also different. The one for the bytes case works well. But the other one doesnot, since I am unable to create the <code>""\ue00e""</code> required to search. Here is the sample of the other map, corresponding to the Softbank <code>&lt;U+E238&gt;</code>.</p>

<pre><code> emmm[11]
[1] ""E238""
</code></pre>
","r, encoding, utf-8, ascii, text-mining","<p>Searching for a single byte of a multi-byte UTF-8 encoded character only works if done with <code>useBytes = TRUE</code>. The fact that <code>""\xf0""</code> here is a part of a multi-byte character is obscured by the less than perfect Unicode support of R on Windows (used in the original example, I presume). How to match by bytes:</p>

<pre><code>foo &lt;- ""\xf0\x9f\x98\x8e"" # U+1F60E SMILING FACE WITH SUNGLASSES
Encoding(foo) &lt;- ""UTF-8""
grepl(""\xf0"", foo, useBytes = TRUE)
</code></pre>

<p>I don't see much use for matching one byte, though. Searching for the whole character would then be:</p>

<pre><code>grepl(foo, paste0(""Smiley: "", foo, "" and more""), useBytes = TRUE)
</code></pre>

<p>Valid ASCII codes correspond to integers 0–127. The <code>iconv()</code> conversion to ASCII in the example replaces any invalid byte 0xYZ (corresponding to integers 128–255) with the literal text <code>&lt;yz&gt;</code> where <code>y</code> and <code>z</code> are hexadecimal digits. As far as I can see, it should not introduce any newlines (<code>""\n""</code>).</p>

<p>Using the character list linked to in the question, here is some example code which performs one kind of ""emoji tagging"" to input strings, namely replacing the emoji with its (slightly formatted) name.</p>

<pre><code>emoji_table &lt;- read.csv2(""https://github.com/today-is-a-good-day/Emoticons/raw/master/emDict.csv"",
                         stringsAsFactors = FALSE)

emoji_names &lt;- emoji_table[, 1]
text_bytes_to_raw &lt;- function(x) {
    loc &lt;- gregexpr(""\\x"", x, fixed = TRUE)[[1]] + 2
    as.raw(paste0(""0x"", substring(x, loc, loc + 1)))
}
emoji_raw &lt;- lapply(emoji_table[, 3], text_bytes_to_raw)
emoji_utf8 &lt;- vapply(emoji_raw, rawToChar, """")
Encoding(emoji_utf8) &lt;- ""UTF-8""

gsub_many &lt;- function(x, patterns, replacements) {
    stopifnot(length(patterns) == length(replacements))
    x2 &lt;- x
    for (k in seq_along(patterns)) {
        x2 &lt;- gsub(patterns[k], replacements[k], x2, useBytes = TRUE)
    }
    x2
}

tag_emojis &lt;- function(x, codes, names) {
    gsub_many(x, codes, paste0(""&lt;"", gsub(""[[:space:]]+"", ""_"", names), ""&gt;""))
}

each_tagged &lt;- tag_emojis(emoji_utf8, emoji_utf8, emoji_names)

all_in_one &lt;- tag_emojis(paste0(emoji_utf8, collapse = """"),
                         emoji_utf8, emoji_names)

stopifnot(identical(paste0(each_tagged, collapse = """"), all_in_one))
</code></pre>

<p>As to why <code>U+E00E</code> is not on that emoji list, I don't think it should be. This code point is in a <a href=""https://en.wikipedia.org/wiki/Private_Use_Areas"" rel=""nofollow noreferrer"">Private Use Area</a>, where character mappings are not standardized. For comprehensive Unicode character lists, you cannot find a better authority than the Unicode Consortium, e.g. <a href=""http://unicode.org/emoji/"" rel=""nofollow noreferrer"">Unicode Emoji</a>. Additionally, see <a href=""https://stackoverflow.com/q/39847816/1023771"">convert utf8 code point strings like &lt;U+0161&gt; to utf8</a> .</p>

<p><strong>Edit after addendum</strong></p>

<p>When there is a string of exactly four hexadecimal digits representing a Unicode code point (let's say <code>""E238""</code>), the following code will convert the string to the corresponding UTF-8 representation, the occurrence of which can be checked with the <code>grep()</code> family of functions. This answers the question of how to ""automatically"" generate the character that can be manually created by typing <code>""\uE238""</code>.</p>

<pre><code>library(stringi)

hex4_to_utf8 &lt;- function(x) {
    stopifnot(grepl(""^[[:xdigit:]]{4}$"", x))
    stringi::stri_enc_toutf8(stringi::stri_unescape_unicode(paste0(""\\u"", x)))
}

foo &lt;- ""E238""
foo_utf8 &lt;- hex4_to_utf8(foo)
</code></pre>

<p>The value of the <code>useBytes</code> option should not matter in the following <code>grep()</code> call. In the previous code example, I used <code>useBytes = TRUE</code> as a precaution, as I'm not sure how well R on Windows handles Unicode code points <code>U+10000</code> and larger (five or six digits). Clearly it cannot properly print such codepoints (as shown by the <code>U+1F60E</code> example), and input with the <code>\U</code> + 8 digits method is <a href=""https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Literal-constants"" rel=""nofollow noreferrer"">not possible</a>.</p>

<p>The example in the question shows that R (on Windows) may print Unicode characters with the <code>&lt;U+E238&gt;</code> notation rather than as <code>\ue238</code>. The reason seems to be <code>format()</code>, also used in <code>print.data.frame()</code>. For example (R for Windows running on Wine):</p>

<pre><code>&gt; format(""\ue238"")
[1] ""&lt;U+E238&gt;""
</code></pre>

<p>When tested in an 8-bit locale on Linux, the same notation is already used by the default print method. One must note that in this case, this is only a printed representation, which is different from how the character is originally stored.</p>
",1,2,2510,2017-01-09 04:43:39,https://stackoverflow.com/questions/41541138/translation-and-mapping-of-emoticons-encoded-as-utf-8-code-in-text
Count the number of elements in a string separated by comma,"<p>I am dealing with text strings such as the following:
<code>LN1 2DW, DN21 5BJ, DN21 5BL, ...</code></p>

<p>In Python, how can I count the number of elements between commas? Each element can be made of 6, 7, or 8 characters, and in my example there are 3 elements shown. The separator is always a comma.</p>

<p>I have never done anything related to text mining so this would be a start for me.</p>
","python, text, text-mining","<p>You can count the number of commas:</p>

<pre><code>text.count("","") + 1
# 3
</code></pre>
",31,13,55653,2017-01-10 14:48:20,https://stackoverflow.com/questions/41571591/count-the-number-of-elements-in-a-string-separated-by-comma
Text Mining in R - Remove Rows from Text File Starting With Keywords,"<p>I am reading a text file into R like the following:</p>

<pre><code>test&lt;-readLines(""D:/AAPL MSFT Earnings Calls/Test/Test.txt"")
</code></pre>

<p>This file was converted from a PDF and retains some header data that I want to get rid of. They will start with words like ""Page,"" ""Market Cap,"" and so forth. </p>

<p>How do I delete out all rows beginning these keywords in my TXT file? This is as opposed to deleting rows containing that word.  </p>

<hr>

<p>Using one of the answers below I modified a bit to read in a </p>

<pre><code>setwd(""C:/Users/George/Google Drive/PhD/Strategic agility/Source Data/Peripherals Earnings Calls 2016"")
text1&lt;-readLines(""test.txt"")
text

library(purrr)
library(stringr)
text1 &lt;- ""foo
Page, bar
baz
Market Cap, qux""
text1 &lt;- readLines(con = textConnection(file))
ignore_patterns &lt;- c(""^Page,"", ""^Market\\s+Cap,"")
text1 %&gt;% discard(~ any(str_detect(.x, ignore_patterns)))

text1
</code></pre>

<p>Here is the output I get:</p>

<pre><code>&gt; text1
[1] ""foo""             ""Page, bar""       ""baz""             ""Market Cap, qux""
</code></pre>

<p>What are the foo/baz/qux characters? Thank you </p>
","r, pdf, text-mining","<pre><code># once you have read and stored in a data.frame
# perform below subsetting :
x = grepl(""^(Page|Market Cap)"", df$id) # where df is you data.frame and 'id' is your 
                                       # column name that has those unwanted keywords
df &lt;- df[!x,]  # does the job!
</code></pre>

<p><code>^</code> helps to check the start. So if row starts with either <code>Page</code> or(<code>|</code>)<code>Market Cap</code> then <code>grepl</code> return <code>TRUE</code></p>
",1,0,1285,2017-01-10 18:03:13,https://stackoverflow.com/questions/41575419/text-mining-in-r-remove-rows-from-text-file-starting-with-keywords
"Use R to do web Crawler and it can not capture content I need(text mining)(Taiwanese BBS, ptt)","<p>this is Joe from National Taipei University of Business, Taiwan. I'm  currently doing a research on online games and E-sports by text mining in the social media. I chose to get the data from the most popular BBS, ""PTT"",in Taiwan, but it seems my code can only capture the article titles but cannot reach the contents.</p>

<p>I tried to get the texts from www.ptt.cc/bbs/LoL/index6402.html to index6391, and the code I used is here in <a href=""https://www.dropbox.com/s/wde1dejrqcssv0j/R_try.R?dl=0"" rel=""nofollow noreferrer"">my R code data</a> or <a href=""https://www.dropbox.com/s/b5p7ixjhepp68ow/try.txt?dl=0"" rel=""nofollow noreferrer"">R code txt file</a> or following.</p>

<pre><code>install.packages(""httr"")
install.packages(""XML"")
install.packages(""RCurl"")
install.packages(""xml2"")

library(httr)
library(XML)
library(RCurl)
library(xml2)


data &lt;- list()

for( i in 6391:6402) {
  tmp &lt;- paste(i, '.html', sep='')
  url &lt;- paste('https://www.ptt.cc/bbs/LoL/index', tmp, sep='')
  tmp &lt;- read_html(url)
  html &lt;- htmlParse(getURL(url))
  url.list &lt;- xml_find_all(tmp, ""//div[@class='title']/a[@href]"")
  data &lt;- rbind(data, as.matrix(paste('https://www.ptt.cc', url.list, sep='')))
}
data &lt;- unlist(data)

getdoc &lt;- function(line){
  start &lt;- regexpr('https://www', line)[1]
  end &lt;- regexpr('html', line)[1]

  if(start != -1 &amp; end != -1){
    url &lt;- substr(line, start, end+3)
    html &lt;- htmlParse(getURL(url), encoding='UTF-8')
    doc &lt;- xpathSApply(html, ""//div[@id='main-content']"", xmlValue)
    name &lt;- strsplit(url, '/')[[1]][4]
    write(doc, gsub('html', 'txt', name))
  }      
}
setwd(""E:/data"")
sapply(data, getdoc)
</code></pre>

<p>But this code can only capture the titles and my txt files are empty. I'm not sure which part goes wrong and thus I need some advice from you at stackoverflow. </p>

<p>Any advice will be very much appreciated and anyone helping me with this will be on the list of acknowledgement in my thesis, and, if you're curious about it, I will inform you of the research result after it is done. :)</p>
","r, text, text-mining","<p>Something like: </p>

<pre><code>library(tidyverse)
library(rvest)

# change the end number
pages &lt;- map(6391:6392, ~read_html(sprintf(""https://www.ptt.cc/bbs/LoL/index%d.html"", .)))

map(pages, ~xml_find_all(., ""//div[@class='title']/a[@href]"")) %&gt;% 
  map(xml_attr, ""href"") %&gt;% 
  flatten_chr() %&gt;% 
  map_df(function(x) {
    URL &lt;- sprintf(""https://www.ptt.cc%s"", x)
    pg &lt;- read_html(URL)
    data_frame(
      url=URL,
      text=html_nodes(pg, xpath=""//div[@id='main-content']"") %&gt;% html_text()
    )
  }) -&gt; df

glimpse(df)
## Observations: 40
## Variables: 2
## $ url  &lt;chr&gt; ""https://www.ptt.cc/bbs/LoL/M.1481947445.A.17B.html"", ""https://www.ptt.cc/b...
## $ text &lt;chr&gt; ""作者rainnawind看板LoL標題[公告] LoL 板 開始舉辦樂透!時間Sat Dec 17 12:04:03 2016\nIMT KDM 勝...
</code></pre>

<p>to make a data frame or sub out the last part with:</p>

<pre><code>dir.create(""pttdocs"")

map(pages, ~xml_find_all(., ""//div[@class='title']/a[@href]"")) %&gt;% 
  map(xml_attr, ""href"") %&gt;% 
  flatten_chr() %&gt;% 
  walk(function(x) {

    URL &lt;- sprintf(""https://www.ptt.cc%s"", x)

    basename(x) %&gt;% 
      tools::file_path_sans_ext() %&gt;% 
      sprintf(fmt=""%s.txt"") %&gt;% 
      file.path(""pttdocs"", .) -&gt; fil

    pg &lt;- read_html(URL)

    html_nodes(pg, xpath=""//div[@id='main-content']"") %&gt;% 
      html_text() %&gt;% 
      writeLines(fil)

  }) 
</code></pre>

<p>to write files to a directory.</p>
",1,0,211,2017-01-17 11:31:49,https://stackoverflow.com/questions/41696033/use-r-to-do-web-crawler-and-it-can-not-capture-content-i-needtext-miningtaiwa
Create vocabulary dictionary for text mining,"<p>I have the following code:</p>

<pre><code>train_set = (""The sky is blue."", ""The sun is bright."")
test_set = (""The sun in the sky is bright."",
    ""We can see the shining sun, the bright sun."")
</code></pre>

<p>Now Im trying to calculate the word frequency like this:</p>

<pre><code>    from sklearn.feature_extraction.text import CountVectorizer
    vectorizer = CountVectorizer()
</code></pre>

<p>Next I would like to print the voculabary. Therefore I do:</p>

<pre><code>vectorizer.fit_transform(train_set)
print vectorizer.vocabulary
</code></pre>

<p>Right now I get the ouput none. While I expect something like:</p>

<pre><code>{'blue': 0, 'sun': 1, 'bright': 2, 'sky': 3}
</code></pre>

<p>Any thoughts where this goes wrong?</p>
","python, nlp, text-mining","<p>I think you can try this:</p>

<pre><code>print vectorizer.vocabulary_
</code></pre>
",5,3,10868,2017-01-17 13:59:19,https://stackoverflow.com/questions/41699065/create-vocabulary-dictionary-for-text-mining
Cosine similarity of 2 DTMs in R,"<p>I have 2 Document term matrices: </p>

<ol>
<li>DTM 1 has say 1000 vectors(1000 docs) and </li>
<li>DTM2 has 20 vectors (20 docs)</li>
</ol>

<p>So basically I want to compare each document of DTM1 against DTM2 and would want to see which DTM1 docs are closest to which DTM2 docs using the cosine function. Any pointers would help! </p>

<p>I have created a cosine matrix using the ""slam"" package. </p>

<pre><code>Docs   –glyma –ie   –initi –stafford ‘bureaucratic’ ‘empti ‘holi ‘incontrovert
  1  0.000000   0 0.000000  0.000000       0.000000      0     0             0
  2  0.000000   0 0.000000  0.000000       0.000000      0     0             0
  3  0.000000   0 0.000000  0.000000       0.000000      0     0             0
  4  0.000000   0 0.000000  0.000000       0.000000      0     0             0
  5  0.000000   0 0.000000  0.000000       0.000000      0     0             0
  6  0.000000   0 0.000000  0.000000       4.906891      0     0             0
  7  0.000000   0 0.000000  4.906891       0.000000      0     0             0
  8  0.000000   0 0.000000  0.000000       0.000000      0     0             0
  9  0.000000   0 4.906891  0.000000       0.000000      0     0             0
  10 4.906891   0 0.000000  0.000000       0.000000      0     0             0
</code></pre>

<p>The cosine function results are:</p>

<p><a href=""https://i.sstatic.net/UHvGc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UHvGc.png"" alt=""enter image description here""></a></p>

<p>However, this matrix compares the docs of DTM1 with one another. I want these vectors to be compared with the vectors of DTM2 and then find the closest DTM2 document for a given DTM1 document. </p>
","r, text-mining, trigonometry","<p>Here is a way to calculate the cosine distance between two matrices. The use of tm is just for data purposes... </p>

<pre><code>library(slam)
library(tm)
data(""acq"")
data(""crude"")

dtm &lt;- DocumentTermMatrix(c(acq, crude))

index &lt;- sample(1:70, size = 10)

dtm1 &lt;- dtm[index, ]
dtm2 &lt;- dtm[-index, ]

cosine_sim &lt;- tcrossprod_simple_triplet_matrix(dtm1, dtm2)/sqrt(row_sums(dtm1^2) %*% t(row_sums(dtm2^2)))
</code></pre>

<p>The cosine function was adapted from this SO post: <a href=""https://stackoverflow.com/questions/29750519/r-calculate-cosine-distance-from-a-term-document-matrix-with-tm-and-proxy"">R: Calculate cosine distance from a term-document matrix with tm and proxy</a></p>
",3,2,2862,2017-01-18 14:00:45,https://stackoverflow.com/questions/41721431/cosine-similarity-of-2-dtms-in-r
What does &quot;document&quot; mean in a NLP context?,"<p>As I was reading about <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency"" rel=""nofollow noreferrer"">tf–idf</a> on Wiki, I was confused by what it means by the word ""document"". Does it mean paragraph?</p>

<p>""The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.""</p>
","text, nlp, text-mining, tf-idf, data-science","<p><code>Document</code> in the <code>tf-idf</code> context can typically be thought of as a <code>bag of words</code>. In a <code>vector space model</code> each word is a dimension in a very high-dimensional space, where the magnitude of an word vector is the number of occurrences of the word (term) in the document. A <code>Document-Term</code> matrix represents a matrix where the rows represent documents and the columns represent the terms, with each cell in the matrix representing # occurrences of the word in the document. Hope it's clear.</p>
",3,3,3094,2017-01-19 18:48:56,https://stackoverflow.com/questions/41749471/what-does-document-mean-in-a-nlp-context
Problems with non english letters using wordcloud by twitter mined text,"<p>I'm new to Stackoverflow and I've been doing my best to follow the guidelines. If there's however something I've missed, please let me know.</p>
<p>Lately I've been playing around with text mining in R; something I'm a novice towards. I've been using the packages you can find in the code nested below to do this. However, problem occurs when the wordcloud displays the Swedish letters å, ä and ö. As you can see in the attached picture the dots gets positioned a bit weird.</p>
<p><a href=""https://i.sstatic.net/OVqo1.png"" rel=""nofollow noreferrer"">Wordcloud image</a></p>
<p>I've been trying as best as I could solving this by myself, but whatever I've been trying, I can't seem to get it to work.</p>
<h3>What I've tried to do:</h3>
<ol>
<li>Use <code>Encoding(tweets) &lt;- &quot;UTF-8&quot;</code> in an attempt to set <code>tweets</code> to UTF-8</li>
<li>Use <code>iconv(tweets, from = &quot;UTF-8&quot;, to = &quot;UTF-8&quot;, sub = &quot;&quot;)</code></li>
</ol>
<p>Furthermore, the last part of the code after defining the corpus vecotr was copied from the author of the tm-package. He listed this as the solution after other people mentioning problems with the wordcloud function with the corpus vector as input. Without it I get an error message when trying to create the wordcloud.</p>
<pre><code>    #Get and load necessary packages:
    install.packages(&quot;twitteR&quot;)
    install.packages(&quot;ROAuth&quot;)
    install.packages(&quot;wordcloud&quot;)
    install.packages(&quot;tm&quot;)
    library(&quot;tm&quot;)
    library(&quot;wordcloud&quot;)
    library(&quot;twitteR&quot;)
    library(&quot;ROAuth&quot;) 

    #Authentication:
    api_key &lt;- &quot;XXX&quot;
    api_secret &lt;- &quot;XXX&quot;
    access_token &lt;- &quot;XXX&quot;
    access_token_secret &lt;- &quot;XXX&quot;
    cred &lt;- setup_twitter_oauth(api_key,api_secret,access_token,
                access_token_secret)

    #Extract tweets:
    search.string &lt;- &quot;#svpol&quot;
    no.of.tweets &lt;- 3200
    tweets &lt;- searchTwitter(search.string, n=no.of.tweets, since = &quot;2017-01-01&quot;)
    tweets.text &lt;- sapply(tweets, function(x){x$getText()})

    #Remove tweets that starts with &quot;RT&quot; (retweets):
    tweets.text &lt;- gsub(&quot;^\bRT&quot;, &quot;&quot;, tweets.text)
    #Remove tabs:
    tweets.text &lt;- gsub(&quot;[ |\t]{2,}&quot;, &quot;&quot;, tweets.text)
    #Remove usernames:
    tweets.text &lt;- gsub(&quot;@\\w+&quot;, &quot;&quot;, tweets.text)
    tweets.text &lt;- (tweets.text[!is.na(tweets.text)])
    tweets.text &lt;- gsub(&quot;\n&quot;, &quot; &quot;, tweets.text)
    #Remove links:
    tweets.text &lt;- gsub(&quot;http[^[:space:]]*&quot;, &quot;&quot;, tweets.text)
    #Remove stopwords:
    stopwords_swe &lt;- c(&quot;är&quot;, &quot;från&quot;, &quot;än&quot;)
    #Just a short example above, the real one is very large
    tweets.text &lt;- removeWords(tweets.text,stopwords_swe)

    #Create corpus:
    tweets.text.corpus &lt;- Corpus(VectorSource(tweets.text))
    #See notes in the longer text about the corpus vector
    tweets.text.corpus &lt;- tm_map(tweets.text.corpus,
                          content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')), mc.cores=1)
    tweets.text.corpus &lt;- tm_map(tweets.text.corpus, content_transformer(tolower), mc.cores=1)
    tweets.text.corpus &lt;- tm_map(tweets.text.corpus, removePunctuation, mc.cores=1)
    tweets.text.corpus &lt;- tm_map(tweets.text.corpus, function(x)removeWords(x,stopwords(kind = &quot;en&quot;)), mc.cores=1)

    wordcloud &lt;- wordcloud(tweets.text.corpus, min.freq = 10,
                   max.words=300, random.order=FALSE, rot.per=0.35, 
                   colors=brewer.pal(8, &quot;Set2&quot;))
    wordcloud
</code></pre>
<p>Would be super happy receiving help with this!</p>
","r, text-mining, word-cloud","<p>Managed to solve it by first encoding the vector to <code>UTF-8-MAC</code> (since I'm on OSX), then using the <code>gsub()</code> function in order to manually change the hex codes for å,ä,ö (the letters I had problems with) to the actual letters. For example <code>gsub(""0xc3 0x85"", ""å"", x)</code>, <code>gsub(""0xc3 0xa5"", ""å"", x)</code> (since case sensitivity).</p>

<p>Lastly changing the argument for the <code>tm_map()</code> function from <code>UTF-8-MAC</code> to <code>latin1</code>. That did the trick for me, hopefully someone else will find this useful in the future.</p>
",1,2,1510,2017-01-25 22:17:27,https://stackoverflow.com/questions/41862690/problems-with-non-english-letters-using-wordcloud-by-twitter-mined-text
read multiple text files into r for text mining purposes,"<p>I have a batch of text files that I need to read into r to do text mining.</p>

<p>So far, I have tried to use read.table, read.line, lapply, mcsv_r from qdap package to no avail. I have tried to write a loop to read the files, but I have to specify the name of the file, which changes in every iteration.</p>

<p>Here is what I have tried:</p>

<pre><code># Relative path points to the local folder
folder.path=""../data/InauguralSpeeches/""

# get the list of file names
speeches=list.files(path = folder.path, pattern = ""*.txt"")

for(i in 1:length(speeches))
  {

    text_df &lt;- do.call(rbind,lapply(speeches[i],read.csv))

}
</code></pre>

<p>Moreover, I have tried the following:</p>

<pre><code>library(data.table)  
files &lt;- list.files(path = folder.path,pattern = "".csv"")
temp &lt;- lapply(files, fread, sep="","")
data &lt;- rbindlist( temp )
</code></pre>

<p>And it is giving me this error when inaugAbrahamLincoln-1.csv clearly exists in the folder:</p>

<pre><code>files &lt;- list.files(path = folder.path,pattern = "".csv"")
&gt; temp &lt;- lapply(files, fread, sep="","")
Error in FUN(X[[i]], ...) : 
  File 'inaugAbrahamLincoln-1.csv' does not exist. Include one or more spaces to consider the input a system command.
&gt; data &lt;- rbindlist( temp )
Error in rbindlist(temp) : object 'temp' not found
&gt; 
</code></pre>

<p>But it only works on .csv files, not on .txt files.</p>

<p>Is there a simpler way to do text mining from multiple sources files? If so how?</p>

<p>Thanks</p>
","r, text-mining","<p>I often have this same problem.  The <strong>textreadr</strong> package that I maintain is designed to make reading .csv, .pdf, .doc, and .docx documents and directories of these documents easy.  It would reduce what you're doing to:</p>

<pre><code>textreadr::read_dir(""../data/InauguralSpeeches/"")
</code></pre>

<p>Your example is not reproducible so I do it below (please make your example reproducible in the future).</p>

<pre><code>library(textreadr)

## Minimal working example
dir.create('delete_me')
file.copy(dir(system.file(""docs/Maas2011/pos"", package = ""textreadr""), full.names=TRUE), 'delete_me', recursive=TRUE)
write.csv(mtcars, 'delete_me/mtcars.csv')
write.csv(CO2, 'delete_me/CO2.csv')
cat('test\n\ntesting\n\ntester', file='delete_me/00_00.txt')

## the read in of a directory
read_dir('delete_me') 
</code></pre>

<h2>output</h2>

<p>The output below shows the tibble output with each document registered in the <code>document</code> column.  For every line in the document there is one row for that document.  Depending on what's in the csv files this may not be fine grained enough.</p>

<pre><code>##    document                                  content
## 1       0_9 Bromwell High is a cartoon comedy. It ra
## 2     00_00                                     test
## 3     00_00                                         
## 4     00_00                                  testing
## 5     00_00                                         
## 6     00_00                                   tester
## 7       1_7 If you like adult comedy cartoons, like 
## 8      10_9 I'm a male, not given to women's movies,
## 9      11_9 Liked Stanley &amp; Iris very much. Acting w
## 10     12_9 Liked Stanley &amp; Iris very much. Acting w
## ..      ...                                      ... 
## 141   mtcars ""Ferrari Dino"",19.7,6,145,175,3.62,2.77,
## 142   mtcars ""Maserati Bora"",15,8,301,335,3.54,3.57,1
## 143   mtcars ""Volvo 142E"",21.4,4,121,109,4.11,2.78,18
</code></pre>
",3,1,5723,2017-01-27 04:08:11,https://stackoverflow.com/questions/41886815/read-multiple-text-files-into-r-for-text-mining-purposes
data frame of tfidf with Python,"<p>I have to classify some sentiments my data frame is like this</p>
<pre><code>Phrase                      Sentiment    
is it  good movie          positive    
wooow is it very goode      positive    
bad movie                  negative
</code></pre>
<p>I did some preprocessing as tokenisation stop words stemming etc ...  and I get</p>
<pre><code>Phrase                      Sentiment    
[ good , movie  ]        positive    
[wooow ,is , it ,very, good  ]   positive 
[bad , movie ]            negative
</code></pre>
<p>I need finally to get a dataframe in which the line are the text which the value is the tf_idf and the columns are the words like that</p>
<pre><code>good     movie   wooow    very      bad                Sentiment
tf idf    tfidf_  tfidf    tf_idf    tf_idf               positive
(same thing for the 2 remaining lines)
</code></pre>
","python, pandas, dataframe, text-mining, tf-idf","<p>I'd use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">sklearn.feature_extraction.text.TfidfVectorizer</a>, which is specifically designed for such tasks:</p>

<p><strong>Demo:</strong></p>

<pre><code>In [63]: df
Out[63]:
                   Phrase Sentiment
0       is it  good movie  positive
1  wooow is it very goode  positive
2               bad movie  negative
</code></pre>

<p>Solution:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word', stop_words='english')

X = vect.fit_transform(df.pop('Phrase')).toarray()

r = df[['Sentiment']].copy()

del df

df = pd.DataFrame(X, columns=vect.get_feature_names())

del X
del vect

r.join(df)
</code></pre>

<p>Result:</p>

<pre><code>In [31]: r.join(df)
Out[31]:
  Sentiment  bad  good     goode     wooow
0  positive  0.0   1.0  0.000000  0.000000
1  positive  0.0   0.0  0.707107  0.707107
2  negative  1.0   0.0  0.000000  0.000000
</code></pre>

<p><strong>UPDATE:</strong> memory saving solution:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word', stop_words='english')

X = vect.fit_transform(df.pop('Phrase')).toarray()

for i, col in enumerate(vect.get_feature_names()):
    df[col] = X[:, i]
</code></pre>

<p><strong>UPDATE2:</strong> <a href=""https://stackoverflow.com/questions/41916560/pandas-dataframe-memory-python"">related question where the memory issue was finally solved</a></p>
",7,9,4952,2017-01-27 22:40:36,https://stackoverflow.com/questions/41904197/data-frame-of-tfidf-with-python
Create a stemmer to reduce words to a base form,"<p>I am dealing with a case now for which I would like to create my own stemming algorithm. I know that there are some excellent libraries for this but this does not work for this use case.</p>

<p>In essence I would like to import a dictionary so I can loop through words in a sentence and if a word is present in a list, reduce it to its base form.</p>

<p>So in case, fe reduce 'banker' to bank. Im have produced this but this is not scalable.</p>

<pre><code>list_bank = ('banking', 'banker' )
sentence = (""There's a banker"")
banker_tags = []

for word in sentence.split():
 print(word)
</code></pre>

<p>So in case, fe reduce 'banker' to bank
     if word in list_bank:
        #replace word</p>

<p>Any suggestion on how I can get this working?</p>
","python, text-mining","<p>Put the words and their stems in a <a href=""https://docs.python.org/3.6/tutorial/datastructures.html#dictionaries"" rel=""nofollow noreferrer"">dictionary</a> and then use that to look up the stemmed form:</p>

<pre><code>dictionary = { 'banker' : 'bank', 'banking': 'bank' } # Add the rest of your words and stems
sentence = ""There's a banker""
for word in sentence.split():
    if word in dictionary:
        word = dictionary[word]
    print(word)
</code></pre>

<pre>
There's
a
bank
</pre>
",2,0,758,2017-01-28 13:53:20,https://stackoverflow.com/questions/41910572/create-a-stemmer-to-reduce-words-to-a-base-form
tm_map(gsub...) fails to replace words,"<pre><code># Loading required libraries


# Set up logistics such as reading in data and setting up corpus

```{r}

# Relative path points to the local folder
folder.path=""../data/InauguralSpeeches/""

# get the list of file names
speeches=list.files(path = folder.path, pattern = ""*.txt"")

# Truncate file names so it is only showing ""FirstLast-Term""
prez.out=substr(speeches, 6, nchar(speeches)-4)

# Create a vector NA's equal to the length of the number of speeches
length.speeches=rep(NA, length(speeches))

# Create a corpus
ff.all&lt;-Corpus(DirSource(folder.path))
```

# Clean the data

```{r}

# Use tm_map to strip all white spaces to a single space, to lower case case, remove stop words, empty strings and punctuation.
ff.all&lt;-tm_map(ff.all, stripWhitespace)
ff.all&lt;-tm_map(ff.all, content_transformer(tolower))
ff.all&lt;-tm_map(ff.all, removeWords, stopwords(""english""))
ff.all&lt;-tm_map(ff.all, removeWords, c(""can"", ""may"", ""upon"", ""shall"", ""will"",     ""must"", """"))
</code></pre>

<h1>The problem line</h1>

<p>ff.all&lt;-tm_map(ff.all, gsub, pattern = ""free"", replacement = ""freedom"")</p>

<pre><code>ff.all&lt;-tm_map(ff.all, removeWords, character(0))
ff.all&lt;-tm_map(ff.all, removePunctuation)

# tdm.all =  a Term Document Matrix
tdm.all&lt;-TermDocumentMatrix(ff.all)
</code></pre>

<p>So I am trying to replace words that are similar by one root word. For example, replacing ""free"" by ""freedom"" in a text mining project.</p>

<p>Then I learned this line from a Youtube tutorial: ff.all&lt;-tm_map(ff.all, gsub, pattern = ""free"", replacement = ""freedom"").
Without this line, the code runs.</p>

<p>With this line added, R Studio gives this error ""<strong>Error: inherits(doc, ""TextDocument"") is not TRUE</strong>"" on the execution of this line: ""<em>tdm.all&lt;-TermDocumentMatrix(ff.all)</em>""</p>

<p>I think this should be a relatively simple issue, however I could not find a solution on stackoverflow. </p>
","r, text-mining, term-document-matrix","<p>Using the <code>tm</code>'s builtin <code>crude</code> data I was able to fix your problem by wrapping <code>gsub</code> in a <code>content_transformer</code> call like so.</p>

<pre><code>ff.all&lt;-tm_map(ff.all, content_transformer(function(x) gsub(x, pattern = ""free"", replacement = ""freedom"")))
</code></pre>

<p>It has been my experience that <code>tm_map</code> does wierd things to the returned object for custom functions. So while your original line worked <code>tm_map</code> doesn't quite return a true ""Corpus"" that is what causes the errors. </p>

<p>As a side note: </p>

<p>This line seems to do nothing
    ff.all&lt;-tm_map(ff.all, removeWords, character(0))</p>

<p>Same with the <code>""""</code> in 
    ff.all&lt;-tm_map(ff.all, removeWords, c(""can"", ""may"", ""upon"", ""shall"", ""will"",     ""must"", """"))</p>

<h3>My full example</h3>

<pre><code>library(tm)
data(crude)
ff.all &lt;- crude

ff.all&lt;-tm_map(ff.all, stripWhitespace)
ff.all&lt;-tm_map(ff.all, content_transformer(tolower))
ff.all&lt;-tm_map(ff.all, removeWords, stopwords(""english""))
ff.all&lt;-tm_map(ff.all, removeWords, c(""can"", ""may"", ""upon"", ""shall"", ""will"",     ""must"", """"))

ff.all&lt;-tm_map(ff.all, content_transformer(function(x) gsub(x, pattern = ""free"", replacement = ""freedom"")))

ff.all&lt;-tm_map(ff.all, removeWords, character(0))
ff.all&lt;-tm_map(ff.all, removePunctuation)

# tdm.all =  a Term Document Matrix
tdm.all&lt;-TermDocumentMatrix(ff.all)
</code></pre>
",1,2,3863,2017-01-29 23:55:28,https://stackoverflow.com/questions/41927317/tm-mapgsub-fails-to-replace-words
What are FP-growth allowed input data type?,"<p>I would like to mine frequent pattern from my data. My dataset is however very large. First I need to transform it into transaction database format. The thing is that In my database (of 500000 records), there are 402 unique repeating items. Everywhere I see fp-growth examples with dataset of max 8 characters (A,B,C,D,E,F,G,H), however I need to represent 402 unique items. Can I use combination of alphabetical characters for this algorithm? Or are there any other approaches?
Thank you guys </p>
","c#, database, sequence, data-mining, text-mining","<p>Depends on your implementation.</p>

<p>But it's fairly standard to allow items such as 'milk' and not only one letter items.</p>
",1,-1,156,2017-01-30 21:15:45,https://stackoverflow.com/questions/41945528/what-are-fp-growth-allowed-input-data-type
Wordcloud with a specific shape,"<p>Suppose, I have a dataframe which contains some words with their frequencies. I want to create a wordcloud in R with the words inside the shape of a logo, for example, the twitter logo just like this:
<a href=""https://i.sstatic.net/TNj0R.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TNj0R.jpg"" alt=""enter image description here"" /></a></p>
<p>For the wordcloud, there is a package named <code>wordcloud2</code> and running the demo only gives a square image of the words. How would I add a specific image of my choice to be the shape of the wordcloud?</p>
<p><code>wordcloud2(demoFreq)</code> only ouputs a square.</p>
","r, text-mining, word-cloud","<p>You can use <code>wordcloud2</code> package for that. It allows you to use any image as the mask. Just put in the working directory and link to it using <code>figpath</code>. Below is the code I used to make the wordcloud. Below that is the wordcloud. <a href=""https://i.sstatic.net/M2jeo.jpg"" rel=""noreferrer"">Here</a> is the image I used as the mask.</p>

<pre><code>library(wordcloud2)
wordcloud2(demoFreq, figPath = ""twitter.jpg"")
</code></pre>

<p><a href=""https://i.sstatic.net/SjrSY.jpg"" rel=""noreferrer""><img src=""https://i.sstatic.net/SjrSY.jpg"" alt=""enter image description here""></a></p>
",17,8,15821,2017-02-03 16:13:13,https://stackoverflow.com/questions/42028462/wordcloud-with-a-specific-shape
Wordcloud in R from list of values (not from text documents),"<p>I have ranked the tokens in my texts according so a criterion and they all have a value. My list looks like this:</p>

<pre><code>value,token
3,tok1
2.84123,tok2
1.5,tok3
1.5,tok4
1.01,tok5
0.9,tok6
0.9,tok7
0.9,tok8
0.81,tok9
0.73,tok10
0.72,tok11
0.65,tok12
0.65,tok13
0.6451231,tok14
0.6,tok15
0.5,tok16
0.4,tok17
0.3001,tok18
0.3,tok19
0.2,tok20
0.2,tok21
0.1,tok22
0.05,tok23
0.04123,tok24
0.03,tok25
0.02,tok26
0.01,tok27
0.01,tok28
0.01,tok29
0.007,tok30
</code></pre>

<p>I then try to produce wordcloud with the following code:</p>

<pre><code>library(tm)
library(wordcloud)

tokList = read.table(""tokens.txt"", header = TRUE, sep = ',') 

# Create corpus
corp &lt;- Corpus(DataframeSource(tokList))
corpPTD &lt;- tm_map(corp, PlainTextDocument)

wordcloud(corpPTD, max.words = 50, random.order=FALSE)
</code></pre>

<p>Which produces:</p>

<p><a href=""https://i.sstatic.net/eQJfw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eQJfw.jpg"" alt=""enter image description here""></a></p>

<p><strong>But that is not what I want.</strong> I would like a wordcloud, where I visualize the tokens (so ""tok1"", ""tok2"", ...) according to the value that's in the table. So if the first token has a 3 then I want that word to be three times bigger than the next element in the list.</p>

<p>Can somebody maybe help?</p>
","r, dataframe, text-mining, tm, word-cloud","<p>Simply this will also work (assuming that your minimum value is not zero, if zero then filter out the corresponding tokens):</p>

<pre><code>library(RColorBrewer)
wordcloud(tokList$token, tokList$value/min(tokList$value), max.words = 50, min.freq = 1, 
                    random.order=FALSE, colors=brewer.pal(6,""Dark2""), random.color=TRUE)
</code></pre>

<p><a href=""https://i.sstatic.net/rhhuy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rhhuy.png"" alt=""enter image description here""></a></p>
",1,4,731,2017-02-05 16:29:40,https://stackoverflow.com/questions/42054467/wordcloud-in-r-from-list-of-values-not-from-text-documents
Split Text String in R,"<p>I have large list in <code>R</code> with more than 5000 elements. The elements are of the form:</p>

<pre><code>$`/home/ricardo/MultiClass/data//Execucao_PUBLICACAO_DECISAO_INTERLOCUTORIA_DETERMINACAO_DE_PAGAMENTO/1117.txt.V1 

[1] DATA DE DISPONIBILIZACAO DA PUBLICACAO PELA FONTE OFICIAL: 16/11/2016 Pag 4279 Decisao Processo N RTOrd-0122200-90.2006.5.15.0087  &lt;truncated&gt;`
</code></pre>

<p>I would like to transform this in a two columns <code>dataframe</code> where:</p>

<pre><code>c1       
The contents between $ and [1]

c2    
rest of the text
</code></pre>

<p>How can I do this split? Important to note that the numberof strings between <code>$</code> and <code>[1]</code> can change, and the strings <code>$</code>, <code>[ e ]</code> can appear in the rest of the text.</p>

<p>Thanks in advance,
Ricardo.</p>
","r, string, split, text-mining","<pre><code>library(stringr)

string &lt;- '$/home/ricardo/MultiClass/data//Execucao_PUBLICACAO_DECISAO_INTERLOCUTORIA_DETERMINACAO_DE_PAGAMENTO/1117.txt.V1 [1] DATA DE DISPONIBILIZACAO DA PUBLICACAO PELA FONTE OFICIAL: 16/11/2016 Pag 4279 Decisao Processo N RTOrd-0122200-90.2006.5.15.0087'

c1 &lt;- str_match(string = string, pattern = ""^\\$(.*) \\[1\\] (.*)"")[,2]
c2 &lt;- str_match(string = string, pattern = ""^\\$(.*) \\[1\\] (.*)"")[,3]
</code></pre>
",1,0,119,2017-02-06 21:12:27,https://stackoverflow.com/questions/42077338/split-text-string-in-r
How to check if a paragraph is part of a text in R,"<p>I have one paragrah of text (a vector of words) and I would like to see if it is ""part"" of a long text (a vector of words). However, I am know that this paragraph does not appear in the text <em>in its exact form</em>, but with slight changes: a few words could miss, the order could be slightly different, some words could be inserted as parenthetical elements etc.</p>

<p>I am currently implementing solutions ""by hand"", such as looking if most of the words of the paragraph are in the text, looking the distance between these words, their order, etc... 
I was however wondering if there is no built-in method to do that? </p>

<p>I already checked the <code>tm</code> package, but it does not seem to do that...</p>

<p>Any idea?</p>
","r, text-mining","<p>I fear that you are stuck with hand-writing an approach, e.g. <code>grep</code>-ing some word groups and having some kind of matching threshold. </p>
",1,0,153,2017-02-08 13:15:44,https://stackoverflow.com/questions/42114220/how-to-check-if-a-paragraph-is-part-of-a-text-in-r
textmining graph sentences in python,"<p>I'm trying to solve a text mining problem in python which consist on:</p>

<p><strong>Target:</strong> Create a graph composed of nodes(sentences) by tokenizing a paragraph into sentences, their edges would be their similarity.</p>

<p>This isn't new at all, but the crux of the question is not very treated on the Internet. So, after getting the sentences from a paragraph the interesting point would be to compute a matrix of similarity between sentences(all combinations) to draw the graph. </p>

<p>Is there any package to perform the similairty between several vectors in an easy way?, even with a given list of strings make a graph of similarity...</p>

<p>A reproducible example:</p>

<pre><code># tokenize into sentences
&gt;&gt;&gt; from nltk import tokenize
&gt;&gt;&gt; p = ""Help my code works. The graph isn't still connected. The code computes the relationship in a graph. ""
&gt;&gt;&gt; sentences=tokenize.sent_tokenize(p)
  ['Help my code works.', ""The graph isn't still connected."", 'The code computes the relationship in a graph.']
&gt;&gt;&gt; len (sentences)
3

# compute similarity with dice coeffcient.
&gt;&gt;&gt;def dice_coefficient(a, b):
    """"""dice coefficient 2nt/na + nb.""""""
    a_bigrams = set(a)
    b_bigrams = set(b)
    overlap = len(a_bigrams &amp; b_bigrams)
    return overlap * 2.0/(len(a_bigrams) + len(b_bigrams)
&gt;&gt;&gt;dice_coefficient(sentences[1],sentences[2])
0.918918918918919
</code></pre>

<p>So, with this function I can do it manually and later make the graph with the nodes and the edges. But always a global solution (with n sentences) is the best one.</p>

<p>Any suggestion?</p>
","python, graph, nlp, tokenize, text-mining","<p>The following list comprehension creates a list of tuples where the first two elements are indexes and the last one is the similarity:</p>

<pre><code>edges = [(i,j,dice_coefficient(x,y)) 
         for i,x in enumerate(sentences) 
         for j,y in enumerate(sentences) if i &lt; j]
</code></pre>

<p>You can now remove the edges that are under a certain threshold, and convert the remaining edges into a graph with <code>networkx</code>:</p>

<pre><code>import networkx as nx
G = nx.Graph()
G.add_edges_from((i,j) for i,j,sim in edges if sim &gt;= THRESHOLD)
</code></pre>
",2,3,1454,2017-02-08 17:33:26,https://stackoverflow.com/questions/42119884/textmining-graph-sentences-in-python
Find the similar texts across the python dataframe,"<p>Suppose I have a python dataframe as follows,</p>

<pre><code>data['text']

abc.google.com
d-2667808233512566908.ampproject.net
d-27973032622323999654.ampproject.net
def.google.com
d-28678547673442325000.ampproject.net
i1-j4-20-1-1-13960-2081004232-s.init.cedexis-radar.net
d-29763453703185417167.ampproject.net
poi.google.com
d-3064948553577027059.ampproject.net
i1-io-0-4-1-20431-1341659986-s.init.cedexis-radar.net
d-2914631797784843280.ampproject.net
i1-j1-18-24-1-11326-1053733564-s.init.cedexis-radar.net
</code></pre>

<p>I want to find the similar common texts and group it. for example, abc.google.com, def.google.com, poi.google.com will point to google.com and etc. </p>

<p>The required output is,</p>

<pre><code>google.com
ampproject.net
ampproject.net
google.com
ampproject.net
s.init.cedexis-radar.net
ampproject.net
google.com
ampproject.net
s.init.cedexis-radar.net
ampproject.net
s.init.cedexis-radar.net
</code></pre>

<p>It's more like a data cleaning exercise where I can clean the unwanted parts. One way is to manually inspect and code for every possible group. But I would be having millions of text. So is there a way / package in python to do this?</p>

<p>Sorry for asking this without trying anything. I've tried to research on this without much success. Not sure how I should start. If anybody can let me know the approach that needs to be taken also, it would be helpful for me.</p>

<p>Thanks</p>
","python, python-2.7, nlp, text-mining","<p>For cleaning, you can use regular expression if you are sure what will be specific format of the text segments in your dataset.</p>

<p>Another approach is trying to match common patterns. For example, in many text segments, you have <code>google.com</code>. You can use this information while pre-processing.</p>

<p>Example</p>

<pre><code>lines = ['abc.google.com',
         'd-2667808233512566908.ampproject.net',
         'd-27973032622323999654.ampproject.net',
         'def.google.com',
         'd-28678547673442325000.ampproject.net',
         'i1-j4-20-1-1-13960-2081004232-s.init.cedexis-radar.net',
         'd-29763453703185417167.ampproject.net',
         'poi.google.com',
         'd-3064948553577027059.ampproject.net',
         'i1-io-0-4-1-20431-1341659986-s.init.cedexis-radar.net',
         'd-2914631797784843280.ampproject.net',
         'i1-j1-18-24-1-11326-1053733564-s.init.cedexis-radar.net']


def commonSubstringFinder(string1, string2):
    common_substring = """"
    split1 = string1.split('.')
    split2 = string2.split('.')
    index1 = len(split1) - 1
    index2 = len(split2) - 1
    size = 0
    while index1 &gt;= 0 &amp; index2 &gt;= 0:
        if split1[index1] == split2[index2]:
            if common_substring:
                common_substring = split1[index1] + '.' + common_substring
            else:
                common_substring += split1[index1]
            size += 1
        else:
            ind1 = len(split1[index1]) - 1
            ind2 = len(split2[index2]) - 1
            if split1[index1][ind1] == split2[index2][ind2]:
                common_substring = '.' + common_substring
            while ind1 &gt;= 0 &amp; ind2 &gt;= 0:
                if split1[index1][ind1] == split2[index2][ind2] and split1[index1][ind1].isalpha():
                    if common_substring:
                        common_substring = split1[index1][ind1] + common_substring
                    else:
                        common_substring += split1[index1][ind1]
                else:
                    break
                ind1 -= 1
                ind2 -= 1

            break
        index1 -= 1
        index2 -= 1

    if size &gt; 1:
        return common_substring
    else:
        return """"

output = []
for line in lines:
    flag = True
    for i in range(len(output)):
        result = commonSubstringFinder(output[i], line)
        if len(result) &gt; 0:
            output[i] = result
            output.append(result)
            flag = False
            break
    if flag:
        output.append(line)

for item in output:
    print(item)
</code></pre>

<p>This outputs:</p>

<pre><code>google.com
ampproject.net
ampproject.net
google.com
ampproject.net
s.init.cedexis-radar.net
ampproject.net
google.com
ampproject.net
s.init.cedexis-radar.net
ampproject.net
s.init.cedexis-radar.net
</code></pre>
",1,0,154,2017-02-11 00:29:40,https://stackoverflow.com/questions/42170809/find-the-similar-texts-across-the-python-dataframe
How to count frequency of pair words keeping the order sequence in text using R?,"<p>So I've working on a way that I can get the frequency of pair words that happen in a sequence of events separated by a character. Example:</p>

<pre><code> Input:
 ""Start&gt;Press1&gt;Press2&gt;PressQR&gt;Exit""
 ""Start&gt;PressA&gt;Press2&gt;PressQR&gt;QuitL&gt;Exit""
 ""Start&gt;Press1&gt;Press2&gt;Press3&gt;Exit""`

 Output:
 Start&gt;Press1    2
 Press1&gt;Press2   2
 Press2&gt;PressQR  2
 PressQR&gt;Exit    1
 Start&gt;PressA    1
 PressA&gt;Press2   2
 Press2&gt;PressQR  1
 PressQR&gt;QuitL   1
 QuitL&gt;Exit      1
 Press2&gt;Press3   1
 Press3&gt;Exit     1
</code></pre>

<p>Thank you.</p>
","r, text, text-mining, word-frequency","<p>Make a directed edgelist and then aggregate:</p>

<pre><code>edgelist &lt;- do.call(rbind, lapply(strsplit(x,""&gt;""), function(x) cbind(head(x,-1), x[-1]) ))
aggregate(count ~ ., data.frame(edgelist,count=1), FUN=sum)

#        X1      X2 count
#1   Press3    Exit     1
#2  PressQR    Exit     1
#3    QuitL    Exit     1
#4    Start  Press1     2
#5   Press1  Press2     2
#6   PressA  Press2     1
#7   Press2  Press3     1
#8    Start  PressA     1
#9   Press2 PressQR     2
#10 PressQR   QuitL     1
</code></pre>
",3,0,564,2017-02-14 05:22:29,https://stackoverflow.com/questions/42218462/how-to-count-frequency-of-pair-words-keeping-the-order-sequence-in-text-using-r
keep words after keyword in string R,"<p><strong>Problem:</strong> I am working with a tokenizer for text mining and would like to limit the length of strings in my input data. The following code keeps the 
entire string if the word is included. </p>

<pre><code>    #create data frame with data 
    dd &lt;- data.frame(
    text = c(""hello how are you doing thank 
              you for helping me with this 
              problem"",""junk"",""junk""), stringsAsFactors = F)

   #keep string that only include term ""how""
   dd &lt;- filter(dd, grepl('how', text))
</code></pre>

<p><strong>Question:</strong> How can I modify the code to just keep N words after the key words. </p>

<p>e.g. </p>

<p>if N =1 then dd would include: how are </p>

<p>if N =2 then dd would include: how are you</p>

<p>if N =3 then dd would include: how are you doing</p>

<p><strong>...</strong></p>

<p>I need code that would work if I also included additional words in the keep:</p>

<pre><code>   #keep string that only include terms ""how"" and ""with""
   dd &lt;- filter(dd, grepl('how|with', text))
</code></pre>
","r, regex, text-mining","<p>Here is a possible approach wih tidy text mining packages.:
(so check dependencies...-</p>

<pre><code>library(tidytext) # install.packages(""tidytext"")
library(tidyr)    # install.packages(""tidyr"")
library(dplyr)    # install.packages(""dplyr"")

dd &lt;- data.frame(
  text = c(""hello how are you doing thank 
              you for helping me with this 
              problem"",""junk"",""junk""), stringsAsFactors = F)
</code></pre>

<p>I mention as <code>scope</code> your parameter about words horizon; easy to turn following code into a function:</p>

<pre><code>scope=2
dd %&gt;%
  unnest_tokens(ngram, text, token = ""ngrams"", n = 1+scope) %&gt;% 
  separate(ngram, paste(""word"",1:(scope+1),sep=""""), sep = "" "") %&gt;% 
  filter(word1 %in% c(""how"",""me"")) 

# A tibble: 2 × 3
  word1 word2 word3
  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;
1   how   are   you
2    me  with  this
</code></pre>

<p>In case you want to end up with strings, you have to collapse back ngrams, cf for instance this second example:</p>

<pre><code>scope=3
dd %&gt;%
unnest_tokens(ngram, text, token = ""ngrams"", n = 1+scope) %&gt;% 
  separate(ngram, paste(""word"",1:(scope+1),sep=""""), sep = "" "") %&gt;% 
  filter(word1 %in% c(""how""))  %&gt;% apply(.,1,paste, collapse= "" "")

[1] ""how are you doing""
</code></pre>

<p>Regarding your comment:
Now if you want to work chunk (string) per chunk (string), you must explicitely perform this group by processing.
Here is a way for instance:</p>

<pre><code>scope=2
subsets &lt;- 
    dd %&gt;% 
    mutate(id=1:length(text)) %&gt;%
    split(., .$id) 

unlist(lapply(subsets, function(dd) {
  dd %&gt;%
  unnest_tokens(ngram, text, token = ""ngrams"", n = 1+scope) %&gt;% 
  separate(ngram, paste(""word"",1:(scope+1),sep=""""), sep = "" "")  %&gt;%
  filter(word1 %in% c(""how"",""problem"")) %&gt;%
  apply(.,1,FUN=function(vec) paste(vec[-1],collapse="" ""))
}))

           1 
""how are you"" 
</code></pre>
",1,2,626,2017-02-14 16:11:25,https://stackoverflow.com/questions/42230991/keep-words-after-keyword-in-string-r
Extract total frequency of words from vector in R,"<p>This is the vector I have:</p>
<pre><code> posts = c(&quot;originally by: cearainmy only concern with csm is they seem a bit insulated from players.  they have private message boards where it appears most of their work goes on.  i would bet they are posting more there than in jita speakers corner.  i think that is unfortunate because its hard to know who to vote for if you never really see what positions they hold.  its sort of like ccp used to post here on the forums then they stopped.  so they got a csm to represent players and use jita park forum to interact.  now the csm no longer posts there as they have their internal forums where they hash things out.  perhaps we need a csm to the csm to find out what they are up to.i don't think you need to worry too much. the csm has had an internal forum for over 2 years, although it is getting used a lot more now than it was. a lot of what goes on in there is nda stuff that we couldn't discuss anyway.i am quite happy to give my opinion on any topic, to the extent that the nda allows, and i&quot; , &quot;fot those of you bleating about imagined nda scandals as you attempt to cast yourselves as the julian assange of eve, here's a quote from the winter summit thread:originally by: sokrateszday 3post dominion 0.0 (3hrs!)if i had to fly to iceland only for this session i would have done it. we had gathered a list of items and prepared it a bit. important things we went over were supercaps, force projection, empire building, profitability of 0.0, objectives for small gangs and of course sovereingty.the csm spent 3 hours talking to ccp about how dominion had changed 0.0, and the first thing on sokratesz's list is supercaps. its not hard to figure out the nature of the discussion.on the other hand, maybe you're right, and the csm's priority for this discussion was to talk about how underpowered and useless supercarriers are and how they needed triple the ehp and dps from their current levels?(it wasn't)&quot;)
</code></pre>
<p>I want a <code>data frame</code> as a result, that would contain words and the frequecy of times they occur.</p>
<p>So result should look something like:</p>
<pre><code>word   count
a        300
and      260
be       200
...      ...
...      ...
</code></pre>
<p>What I tried to do, was use <code>tm</code></p>
<pre><code>corpus &lt;- VCorpus(VectorSource(posts))
corpus &lt;-tm_map(corpus, removeNumbers)
corpus &lt;-tm_map(corpus, removePunctuation)
m &lt;- DocumentTermMatrix(corpus)
</code></pre>
<p>Running <code>findFreqTerms(m, lowfreq =0, highfreq =Inf )</code> just gives me the words, so I understand its a sparse matrix, how do I extract the words and their frequency?</p>
<p>Is there a easier way to do this, maybe by not using <code>tm</code> at all?</p>
","r, text-mining","<pre><code>posts = c(""originally by: cearainmy only concern with csm is they seem a bit insulated from players.  they have private message boards where it appears most of their work goes on.  i would bet they are posting more there than in jita speakers corner.  i think that is unfortunate because its hard to know who to vote for if you never really see what positions they hold.  its sort of like ccp used to post here on the forums then they stopped.  so they got a csm to represent players and use jita park forum to interact.  now the csm no longer posts there as they have their internal forums where they hash things out.  perhaps we need a csm to the csm to find out what they are up to.i don't think you need to worry too much. the csm has had an internal forum for over 2 years, although it is getting used a lot more now than it was. a lot of what goes on in there is nda stuff that we couldn't discuss anyway.i am quite happy to give my opinion on any topic, to the extent that the nda allows, and i"" , ""fot those of you bleating about imagined nda scandals as you attempt to cast yourselves as the julian assange of eve, here's a quote from the winter summit thread:originally by: sokrateszday 3post dominion 0.0 (3hrs!)if i had to fly to iceland only for this session i would have done it. we had gathered a list of items and prepared it a bit. important things we went over were supercaps, force projection, empire building, profitability of 0.0, objectives for small gangs and of course sovereingty.the csm spent 3 hours talking to ccp about how dominion had changed 0.0, and the first thing on sokratesz's list is supercaps. its not hard to figure out the nature of the discussion.on the other hand, maybe you're right, and the csm's priority for this discussion was to talk about how underpowered and useless supercarriers are and how they needed triple the ehp and dps from their current levels?(it wasn't)"")
posts &lt;- gsub(""[[:punct:]]"", '', posts)  # remove punctuations
posts &lt;- gsub(""[[:digit:]]"", '', posts)  # remove numbers
word_counts &lt;- as.data.frame(table(unlist( strsplit(posts, ""\ "") )))  # split vector by space
word_counts &lt;- with(word_counts, word_counts[ Var1 != """", ] )  # remove empty characters
head(word_counts)
#       Var1 Freq
# 2        a    8
# 3    about    3
# 4   allows    1
# 5 although    1
# 6       am    1
# 7       an    1
</code></pre>
",7,4,5498,2017-02-14 21:34:53,https://stackoverflow.com/questions/42236677/extract-total-frequency-of-words-from-vector-in-r
phrases extraction with R,"<p>I am trying to extract sentiment polarity for film review, so for this from a tagged text (using treetagger) I would like to extract all the bi-grams which tags are (ADV - VER:pper) or (VER:pres - ADJ). For example in the example below the list of extracted phrases is : bien suivi, est efficace.</p>

<p>Can you give some help please?</p>

<p>thank you in advance</p>

<pre><code>Database &lt;- read.table(""exp.txt"", header = FALSE)
Database



          V1       V2         V3

1     Toujours      ADV   toujours
2         bien      ADV       bien
3        suivi VER:pper     suivre
4          par      PRP        par
5          mon  DET:POS        mon
6   conseiller      NOM conseiller
7          Bon      NAM  &lt;unknown&gt;
8      accueil      NOM    accueil
9            ,      PUN          ,
10          ma  DET:POS        mon
11 conseillère      NOM conseiller
12         est VER:pres       être
13    efficace      ADJ   efficace
14          et      KON         et
15           à      PRP          à
16          l'  DET:ART         le
17      écoute      NOM     écoute
18           .                 
</code></pre>
","r, nlp, text-mining","<p>We can use <code>dplyr</code> to accomplish what you need:</p>

<pre><code>library(dplyr)

Database %&gt;% 
    mutate(NV1 = lead(V1), NV2 = lead(V2)) %&gt;% 
    filter((V2 == 'ADV' &amp; NV2 == 'VER:pper') | (V2 == 'VER:pres' &amp; NV2 == 'ADJ')) %&gt;%
    transmute(result = paste(V1, NV1))

#       result
#   bien suivi
# est efficace
</code></pre>

<p>Note that this is not very scalable, as you have to type any condition you need, but is something to start and may suits your need</p>
",1,0,385,2017-02-15 14:30:56,https://stackoverflow.com/questions/42252162/phrases-extraction-with-r
re.search() in python goes into an infinite loop,"<p>I'm trying to extract file paths (Windows/Ubuntu, relative/absolute) from a text document.</p>

<p>The regular expression code below is used check if a word is a file path or not. </p>

<p>It works for most of the cases but fails for one case, where it goes into an infinite loop. Any explanation for this?</p>

<pre><code>import re
path_regex = re.compile(r'^([\.]*)([/]+)(((?![&lt;&gt;:""/\\|?*]).)+((?&lt;![ .])(\\|/))?)*$' , re.I)
text = '/var/lib/jenkins/jobs/abcd-deploy-test-environment-oneccc/workspace/../workspace/abcd-deploy-test-environment.sh'
path_regex.search(text)
</code></pre>
","python, regex, text-mining, regular-language","<p>Indeed there is a problem.<br>
You have overlayed subexpressions mixed with spurious quantifiers.  </p>

<p><em><strong>modified for required parts between slashes</strong></em><br>
It is easily fixed using this <code>^([\.]*)([/]+)((?:[^&lt;&gt;:""/\\|?*.\r\n]|\.(?![\\/]))[\\/]?)*$</code>    </p>

<p>The idea is to see just what your guarding against.<br>
The guard is that you'd allow forward or back slash if not preceeded by a dot.  </p>

<p>So, you have to include the <em>dot</em> in the exclusion class with the  \ and /<br>
then qualify them in a separate alternation.  </p>

<p>If you do it this way, it will always pass.   </p>

<pre><code> ^ 
 ( [\.]* )                     # (1)
 ( [/]+ )                      # (2)
 (                             # (3 start)
      (?:                           # Group start (required between slashes)
           [^&lt;&gt;:""/\\|?*.\r\n]            # Any character, but exclude these
        |                              # or,
           \.                            # The dot, if not followed by forward or back slash
           (?! [\\/] )
      )                             # Group end
      [\\/]?                        # Optional forward or back shash
 )*                            # (3 end)
 $
</code></pre>
",4,3,1374,2017-02-21 17:53:35,https://stackoverflow.com/questions/42374498/re-search-in-python-goes-into-an-infinite-loop
NLP: Way to efficiently compare and identify trends between text,"<p>Are there algorithms or methods in which common trends/themes between text items could be evaluated?</p>

<p>For example let's say that there are four data points (text entries):</p>

<ul>
<li>""I found school very stressful today""</li>
<li>""The test in physics was pretty easy.""</li>
<li>""My physics test wasn't challenging at all""</li>
<li>""Everyone left early because the physics test was straight-forward and we finished it early.""</li>
</ul>

<p>Based on those four entries the first one is an outlier and has no relation to the rest but the other three mention how the ""physics test"" was easy (more generally, the other three express a positive sentiment around the ""physics test"").</p>

<p>Are there methods to extract the common thread between related sentences? These sentences are totally open ended and aren't restricted to simply expressing sentiment about an object - they could be talking about anything.</p>

<p>I understand this is a fairly broad question but I thought I'd ask it so see if people know of existing solutions or ways people have tackled this problem in the past.</p>
","nlp, text-mining","<p>One of the possible solution can be - generate sentence representation (Sent2Vec) first and then compare those representations. </p>

<p>There are many approaches to generate a sentence embedding of a English sentence. One of the popular approach is <a href=""https://github.com/ryankiros/skip-thoughts"" rel=""noreferrer"">skip-though vectors</a>. Just convert the sentence to vector and then use cosine similarity to compare sentences.</p>

<p>You can also use those sentence embeddings to train a neural network to accomplish your target task.</p>
",3,2,328,2017-02-22 05:32:46,https://stackoverflow.com/questions/42383436/nlp-way-to-efficiently-compare-and-identify-trends-between-text
Replace words in text with words generated using all_words,"<p>Being pretty new to <code>qdap</code>, I am not sure whether this functionality is present, but it would be great to have something as mentioned below.</p>

<p>My initial dataset.</p>

<pre><code>ID         Keywords
1          112 mills, open heart surgery, great, great job
2          Ausie, open, heart out
3          opened, heartily, 56mg)_job, orders12
4          order, macD
</code></pre>

<p>On using <code>all_words()</code> I end up with the following data.</p>

<pre><code>   WORD     FREQ
1  great       2
2  heart       2
3  open        2
4  ausie       1
5  heartily    1
6  job         1
7  macd        1
8  mgjob       1
9  mills       1
10 opened       1
11 order        1
12 orders       1
13 out          1
14 surgery      1
</code></pre>

<p>Is there a way in which the main dataset can be replaced by the exact words that are appearing through <code>all_words()</code>?</p>

<p>edit1:
So the list that comes from using all_words() should replace the original words in the dataframe, i.e 112 mills should become mills, 56mg)_job should become mgjob.</p>
","r, text-mining, qdap","<p>It is a bit more manual and I do not know how your data are formatted, but with some tinkering should do the work:</p>

<p><strong>Edit: and it is not using <code>qdap</code>, but I have assumed this is not a crucial part of the question.</strong></p>

<p><strong>2nd edit: I forgot about the substitution, corrected code below.</strong></p>

<pre><code>library(data.table)
library(tm)  # Functions with tm:: below
library(magrittr)

dt &lt;- data.table(
  ID = 1L:4L,
  Keywords = c(
    paste('112 mills', 'open heart', 'surgery', 'great', 'great job', sep = ' '),
    paste('Ausie', 'open', 'heart out', sep = ' '),
    paste('opened', 'heartily', '56mg)_job', 'orders12', sep = ' '),
    paste('order', 'macD', sep = ' ')))

# dt_2 &lt;- data.table(Tokens = tm::scan_tokenizer(dt[, Keywords]))
dt_2 &lt;- dt[, .(Tokens = unlist(strsplit(Keywords, split = ' '))), by = ID]

dt_2[, Words := tm::scan_tokenizer(Tokens) %&gt;%
       tm::removePunctuation() %&gt;%
       tm::removeNumbers()
     ]
dt_2[, Stems := tm::stemDocument(Words)]

dt_2
#     ID    Tokens    Words    Stems
#  1:  1       112                  
#  2:  1     mills    mills     mill
#  3:  1      open     open     open
#  4:  1     heart    heart    heart
#  5:  1   surgery  surgery  surgeri
#  6:  1     great    great    great
#  7:  1     great    great    great
#  8:  1       job      job      job
#  9:  2     Ausie    Ausie     Ausi
# 10:  2      open     open     open
# 11:  2     heart    heart    heart
# 12:  2       out      out      out
# 13:  3    opened   opened     open
# 14:  3  heartily heartily heartili
# 15:  3 56mg)_job    mgjob    mgjob
# 16:  3  orders12   orders    order
# 17:  4     order    order    order
# 18:  4      macD     macD     macD

# Frequencies
dt_2[, .N, by = Words]
#        Words N
#  1:          1
#  2:    mills 1
#  3:     open 2
#  4:    heart 2
#  5:  surgery 1
#  6:    great 2
#  7:      job 1
#  8:    Ausie 1
#  9:      out 1
# 10:   opened 1
# 11: heartily 1
# 12:    mgjob 1
# 13:   orders 1
# 14:    order 1
# 15:     macD 1
</code></pre>

<p><strong>2nd edit here:</strong></p>

<pre><code>res &lt;- dt_2[, .(Keywords = paste(Words, collapse = ' ')), by = ID]
res
#    ID                                  Keywords
# 1:  1  mills open heart surgery great great job
# 2:  2                      Ausie open heart out
# 3:  3              opened heartily mgjob orders
# 4:  4                                order macD
</code></pre>

<p><strong>3rd edit, in case your keywords come as lists and you would like to keep them that way.</strong></p>

<pre><code>library(data.table)
library(tm)  # Functions with tm:: below
library(magrittr)

dt &lt;- data.table(
  ID = 1L:4L,
  Keywords = list(
    c('112 mills', 'open heart', 'surgery', 'great', 'great job'),
    c('Ausie', 'open', 'heart out'),
    c('opened', 'heartily', '56mg)_job', 'orders12'),
    c('order', 'macD')))

dt_2 &lt;- dt[, .(Keywords = unlist(Keywords)), by = ID]
dt_2[, ID_temp := .I]

dt_3 &lt;- dt_2[, .(ID, Tokens = unlist(strsplit(unlist(Keywords), split = ' '))), by = ID_temp]

dt_3[, Words := tm::scan_tokenizer(Tokens) %&gt;%
       tm::removePunctuation() %&gt;%
       tm::removeNumbers() %&gt;%
       stringr::str_to_lower()
     ]
dt_3[, Stems := tm::stemDocument(Words)]
dt_3

res &lt;- dt_3[, .(
  ID = first(ID),
  Keywords = paste(Words, collapse = ' ') %&gt;% stringr::str_trim()),
  by = ID_temp]
res &lt;- res[, .(Keywords = list(Keywords)), by = ID]

# Confirm format (a list of keywords in every element)
dt[1, Keywords] %T&gt;% {print(class(.))} %T&gt;% {print(length(.[[1]]))}
res[1, Keywords] %T&gt;% {print(class(.))} %T&gt;% {print(length(.[[1]]))}
</code></pre>
",1,0,72,2017-02-28 10:00:40,https://stackoverflow.com/questions/42505576/replace-words-in-text-with-words-generated-using-all-words
Categorizing words in paragraphs into groups and assigning weights to them based on listed order,"<p>I have a paragraph which contains names of at most 18 different industries. These names are separated by a semicolon. Their order of occurrence is also significant in determining their magnitude. Hence it has to be assigned as weights to the name. The list can be divided into 3 categories:</p>

<ol>
<li>Industries that report growth. 2. Industries that report contraction. 3. Industries that report no change.</li>
</ol>

<p><em>Of the 18 manufacturing industries, 12 reported growth in January in the following order: Plastics &amp; Rubber Products; Miscellaneous Manufacturing; Apparel, Leather &amp; Allied Products; Paper Products; Chemical Products; Transportation Equipment; Food, Beverage &amp; Tobacco Products; Machinery; Petroleum &amp; Coal Products; Primary Metals; Fabricated Metal Products; and Computer &amp; Electronic Products. The five industries reporting contraction in January are: Nonmetallic Mineral Products; Wood Products; Furniture &amp; Related Products; Electrical Equipment, Appliances &amp; Components; and Printing &amp; Related Support Activities.</em></p>

<p>The above paragraph is a sample. What is the best way to separate the text into 3 categories (2 in this case) and assigning values to them based on the listed order? A pattern occurs in the text. The names begin after ':' and end at '.'
Sometimes the names of Industries that report contraction are listed first followed by Industries reporting growth. How to overcome this while automating? </p>

<p>The value assignment would depend on the count of Industries in each category. The Industries reporting growth have positive values which decrease by 1 all the way to 1. The Industries without change have 0 as default value and the Industries with contraction have negative values whose magnitude decreases by 1 all the way to -1. These categories are then to be put together and sorted to get a list (+ve, 0, -ve) in decreasing order. Still in the early stage of programming. Please bear with me. Even suggestions of strategy to solve would help me go a long way. </p>
","python, string, python-2.7, text-mining","<p>Here is code that works for the example you gave, but I can't guarantee it will work on all samples you have (especially since you did not give an example with nochanges). The main idea is to specifically look for the terms 'growth', 'no change', and 'contraction' using regular expressions (<code>import re</code>) and then get the list of companies after each. Next each of the three categories is put through a list comprehension to get associated scores so that each list entry becomes a tuple of <code>(company, value)</code>. Finally the three categories are combined into one list, sorted by value (the 1st index), and printed out. Note that if the exact word 'growth' is not used, for example 'increase' is used in its place, this will not work.</p>

<p>CODE:</p>

<pre><code>import re

sample = 'Of the 18 manufacturing industries, 12 reported growth in January in the following order: Plastics &amp; Rubber Products; Miscellaneous Manufacturing; Apparel, Leather &amp; Allied Products; Paper Products; Chemical Products; Transportation Equipment; Food, Beverage &amp; Tobacco Products; Machinery; Petroleum &amp; Coal Products; Primary Metals; Fabricated Metal Products; and Computer &amp; Electronic Products. The five industries reporting contraction in January are: Nonmetallic Mineral Products; Wood Products; Furniture &amp; Related Products; Electrical Equipment, Appliances &amp; Components; and Printing &amp; Related Support Activities.'

#Find the growth industries
growth_pattern = 'growth.*?:(.*?)\.'
growths = re.findall(growth_pattern,sample)
growths = growths[0].strip().split(';') if len(growths) == 1 else []

#Find the no change industries
nochange_pattern = 'no change.*?:(.*?)\.'
nochanges = re.findall(nochange_pattern,sample)
nochanges = nochanges[0].strip().split(';') if len(nochanges) == 1 else []

#Find the contraction industries
contraction_pattern = 'contraction.*?:(.*?)\.'
contractions = re.findall(contraction_pattern,sample)
contractions = contractions[0].strip().split(';') if len(contractions) == 1 else []

#Give numbers to each of the industries
growths = [(g.strip().replace('and ',''),len(growths)-i) for i,g in enumerate(growths)]
nochanges = [(nc.strip().replace('and ',''),0) for i,nc in enumerate(nochanges)]
contractions = [(c.strip().replace('and ',''),-(len(contractions)-i)) for i,c in enumerate(contractions)]

#Print them out to check (commented out for now)
#print('growths:'+str(growths))
#print('nochanges:'+str(nochanges))
#print('contractions:'+str(contractions))

#Combine them all together, sort by value, and print out
all_together = growths+nochanges+contractions
all_together = sorted(all_together,key=lambda x: -x[1])
print all_together
</code></pre>

<p>OUTPUT:</p>

<pre><code>[('Plastics &amp; Rubber Products', 12), ('Miscellaneous Manufacturing', 11), ('Apparel, Leather &amp; Allied Products', 10), ('Paper Products', 9), ('Chemical Products', 8), ('Transportation Equipment', 7), ('Food, Beverage &amp; Tobacco Products', 6), ('Machinery', 5), ('Petroleum &amp; Coal Products', 4), ('Primary Metals', 3), ('Fabricated Metal Products', 2), ('Computer &amp; Electronic Products', 1), ('Printing &amp; Related Support Activities', -1), ('Electrical Equipment, Appliances &amp; Components', -2), ('Furniture &amp; Related Products', -3), ('Wood Products', -4), ('Nonmetallic Mineral Products', -5)]
</code></pre>
",1,0,232,2017-03-01 16:39:59,https://stackoverflow.com/questions/42537246/categorizing-words-in-paragraphs-into-groups-and-assigning-weights-to-them-based
Extracting specific data from text column in R,"<p>I have a data set of medicine names in a column. I am trying to extract the name ,strength and unit of each medicine from this data. The term MG and ML are the qualifiers of strength in the setup. For example, let us consider the following given data set for the names of the medicines.</p>

<pre><code> Medicine name
----------------------
 FALCAN 150 MG tab
 AUGMENTIN 500MG tab
 PRE-13 0.5 ML PFS inj
 NS.9%w/v 250 ML, Glass Bottle
</code></pre>

<p>I want to extract the following information columns from this data set,</p>

<pre><code>Name     | Strength |Unit
---------| ---------|------
FALCAN   | 150      |MG
AUGMENTIN| 500      |MG
PRE-13   | 0.5      |ML
NS.9%w/v | 250      |ML
</code></pre>

<p>I have tried <code>grepl</code> etc command and could not find a good solution. I have around >12000 data to identify. Data does not follow a fixed pattern, and at few places MG and strength does not have a space in between such as 300MG. ,</p>
","r, extract, text-mining","<p>You can achieve this with multiple regular expressions. All thought I am not a regex champion I use it for the same purpose as you present here.</p>

<pre><code>meds &lt;- c('FALCAN 150 MG tab',
'AUGMENTIN 500MG tab',
'PRE-13 0.5 ML PFS inj',
'NS.9%w/v 250 ML, Glass Bottle')

library(stringr)

#Name
trimws(str_extract(str_extract(meds, '.* [0-9.]{3}'),'.* '))

#Strength
str_extract(str_extract(meds, '[0-9.]{3}( M|M)[GL]'),'[0-9.]*')

#Unit
str_extract(str_extract(meds, '( M|[0-9]M)[GL]'), 'M[GL]')
</code></pre>

<p>I know that a lot of these medicine notations can be quite different, thus I prefer to extract each item by regular expressions, in contrast to the solution presented by G. Grothendieck, who expects a certain structure in the data (3 columns). 
That way I am able to tweak each item, by inspecting all the strings that generate <code>NA</code> values.</p>
",0,0,2918,2017-03-08 15:50:07,https://stackoverflow.com/questions/42675865/extracting-specific-data-from-text-column-in-r
NLP - identifying which adjective describes which noun in a sentence,"<p>I need a method / algorithm for identifying which adjective is related to which noun in a sentence.</p>

<p>Sample input:</p>

<pre><code>""The product itself is good however this company has a terrible service""
</code></pre>

<p>As an output i'd like to get something like:</p>

<pre><code>[product, good]
[service, terrible]
</code></pre>

<p>Could you please point me some algorithms / libraries that would help with such task? </p>
","nlp, text-mining, data-science","<p>You can use <a href=""http://nlp.stanford.edu/software/stanford-dependencies.shtml"" rel=""noreferrer"">Stanford dependency parser</a>, also this <a href=""http://nlp.stanford.edu/~sebschu/pubs/schuster-manning-lrec2016.pdf"" rel=""noreferrer"">relevant paper</a>. You can check their online tool as well. For example, for your sentence, you can get the following from Stanford parser. </p>

<p>Your query</p>

<pre><code>The product itself is good however this company has a terrible service.
</code></pre>

<p>Tagging</p>

<pre><code>The/DT product/NN itself/PRP is/VBZ good/JJ however/RB this/DT company/NN has/VBZ a/DT terrible/JJ service/NN ./.
</code></pre>

<p>Parse</p>

<pre><code>(ROOT
  (S
    (NP (DT The) (NN product))
    (ADVP (PRP itself))
    (VP (VBZ is)
      (ADJP (JJ good))
      (SBAR
        (WHADVP (RB however))
        (S
          (NP (DT this) (NN company))
          (VP (VBZ has)
            (NP (DT a) (JJ terrible) (NN service))))))
    (. .)))
</code></pre>

<p>Universal dependencies</p>

<pre><code>det(product-2, The-1)
nsubj(good-5, product-2)
advmod(good-5, itself-3)
cop(good-5, is-4)
root(ROOT-0, good-5)
advmod(has-9, however-6)
det(company-8, this-7)
nsubj(has-9, company-8)
dep(good-5, has-9)
det(service-12, a-10)
amod(service-12, terrible-11)
dobj(has-9, service-12)
</code></pre>
",3,1,1377,2017-03-11 23:04:50,https://stackoverflow.com/questions/42741729/nlp-identifying-which-adjective-describes-which-noun-in-a-sentence
Not able to see single digit/letter as a term in after creating TermDocument Matrix,"<p>I used <code>TermDocument</code> Matrix in R, and documents(strings) include single letter words also. After using <code>TermDocument</code> Matrix, the terms do not include those single letter words, please suggest which control should I include as an input argument in order to include single letter word in my term document matrix.`</p>
","r, string, text-mining, term-document-matrix","<p>By default the <code>min wordlength</code> is <code>3</code>. you need to specify the parameter as <code>control</code> to override the default, check out the following code.</p>

<pre><code>library(tm)
docs &lt;- c(""This is a text"",""When Will u start"", ""1 12 123"")
corpus &lt;- Corpus(VectorSource(docs))

as.matrix(DocumentTermMatrix(corpus)) #words with length &lt; 3 ('a','u','1','12') are excluded
#    Terms
#Docs 123 start text this when will
#   1   0     0    1    1    0    0
#   2   0     1    0    0    1    1
#   3   1     0    0    0    0    0

as.matrix(DocumentTermMatrix(corpus, control = list(wordLengths=c(1,Inf))))
#    Terms
#Docs 1 12 123 a is start text this u when will
#   1 0  0   0 1  1     0    1    1 0    0    0
#   2 0  0   0 0  0     1    0    0 1    1    1
#   3 1  1   1 0  0     0    0    0 0    0    0
</code></pre>
",0,1,52,2017-03-12 06:53:19,https://stackoverflow.com/questions/42744648/not-able-to-see-single-digit-letter-as-a-term-in-after-creating-termdocument-mat
Document Term Matrix will not maintain decimal places of numbers,"<p>Before I updated my version of RStudio, everything worked great. With the update something has changed with Document Term Matrix in the 'tm' package. I want to create a dtm, but with numbers. For instance if I have a .csv with one column as shown below:</p>

<pre><code>x
1.01
11.21
123.35
212.11
</code></pre>

<p>I want the column names in my term matrix to look like this:</p>

<pre><code>1.01 11.21 123.35 212.11
1    0     0      0
0    1     0      0
0    0     1      0
0    0     0      1
</code></pre>

<p>But instead it looks like this:</p>

<pre><code>123 212
0   0
0   0
1   0
0   1
</code></pre>

<p>Here's the code that used to work:</p>

<p><code>corpus = Corpus(VectorSource(x))
dtm = DocumentTermMatrix(corpus)
dtm_df = as.data.frame(as.matrix(dtm))</code></p>

<p>Thanks in advance</p>
","r, text-mining, corpus, term-document-matrix","<p>From the 'tm' package maintainer Ingo Feinerer:</p>

<p>Here's the code that used to work:</p>

<p>corpus = Corpus(VectorSource(x))</p>

<blockquote>
  <p>Try VCorpus() instead of Corpus().</p>
</blockquote>

<p>dtm = DocumentTermMatrix(corpus)
dtm_df = as.data.frame(as.matrix(dtm))</p>

<blockquote>
  <p>That is highly inefficient (since as.matrix() generates a dense representation from the sparse term-document matrix).</p>
</blockquote>

<p>Best regards,
Ingo</p>
",1,1,123,2017-03-14 01:21:59,https://stackoverflow.com/questions/42776253/document-term-matrix-will-not-maintain-decimal-places-of-numbers
R: RSentiment::calculate_score() returns &quot;Error: arguments imply differing number of rows&quot;,"<p>I'm trying to apply RSentiment::calculate_score() to a set of sentences stored in a data.frame. Here's how I get my data:</p>

<pre><code>install.packages(""pacman"")
pacman::p_load(XML, dplyr, tidyr, stringr, rvest, audio, xml2, purrr, tidytext, ggplot2)

sapiens_code = ""1846558239""
deus_ex_code = ""1910701874""

function_product &lt;- function(prod_code){
  url &lt;- paste0(""https://www.amazon.co.uk/dp/"",prod_code)
  doc &lt;- xml2::read_html(url)
  prod &lt;- html_nodes(doc,""#productTitle"") %&gt;% html_text() %&gt;%
    gsub(""\n"","""",.) %&gt;%
    gsub(""^\\s+|\\s+$"", """", .) #Remove all white space
  prod
}

sapiens &lt;- function_product(sapiens_code)
deus_ex &lt;- function_product(deus_ex_code)

#Source function to Parse Amazon html pages for data 
source(""https://raw.githubusercontent.com/rjsaito/Just-R-Things/master/Text%20Mining/amazonscraper.R"")

# extracting reviews 
pages &lt;- 13

function_page &lt;- function(page_num, prod_code){
  url2 &lt;- paste0(""http://www.amazon.co.uk/product-reviews/"",prod_code,""/?pageNumber="", page_num)
  doc2 &lt;- read_html(url2)

  reviews &lt;- amazon_scraper(doc2, reviewer = F, delay = 2)
  reviews
}

sapiens_reviews &lt;- map2(1:pages, sapiens_code, function_page) %&gt;%   bind_rows()

deusex_reviews &lt;- map2(1:pages, deus_ex_code, function_page) %&gt;% bind_rows()

sapiens_reviews$comments &lt;- gsub(""\\."", ""\\. "",   sapiens_reviews$comments)
deusex_reviews$comments &lt;- gsub(""\\."", ""\\. "", deusex_reviews$comments)

sentence_function &lt;- function(df){
  df_sentence &lt;- df %&gt;% 
    select(comments, format, stars, helpful) %&gt;% 
    unnest_tokens(sentence, comments, token = ""sentences"")
  df_sentence
}

sapiens_sentence &lt;- sentence_function(sapiens_reviews)
deusex_sentence &lt;- sentence_function(deusex_reviews)
</code></pre>

<p>But when I try to assign a score to them, I receive an error:</p>

<pre><code> deusex_sentence &lt;- deusex_sentence %&gt;% 
   mutate(sentence_score &lt;- unname(calculate_score(sentence)))
</code></pre>

<blockquote>
  <p>Error: arguments imply differing number of rows: 34, 33</p>
</blockquote>

<p>I can't see anything fundamentally wrong with the format of my input and the output for randomly picked sentences seems fine, e.g.</p>

<pre><code>unname(calculate_score(sapiens_sentence[1, 4]))
[1] -1
</code></pre>

<p>Any ideas how to get around this? Big thanks for your help!</p>
","r, text-mining, sentiment-analysis","<p>It turns out that the problem was caused by special characters in the sentence. After removing them, I could successfully run the sentiment analysis (I incorporated the data-cleaning step in the function):</p>

<pre><code>sentence_function &lt;- function(df){
  df_sentence &lt;- df %&gt;% 
    select(comments, format, stars, helpful) %&gt;% 
    unnest_tokens(sentence, comments, token = ""sentences"") %&gt;%
    mutate(sentence2 = str_replace_all(sentence, ""[^[:alnum:]]"", "" "")) #removing all special characters

  df_sentence &lt;- df_sentence  %&gt;%
    mutate(sentence_score = unname(calculate_score(sentence2))) 

  df_sentence
}

# go and get a hot drink while this is running 
sapiens_sentence &lt;- sentence_function(sapiens_reviews)
deusex_sentence &lt;- sentence_function(deusex_reviews) 
</code></pre>
",1,1,425,2017-03-15 23:11:22,https://stackoverflow.com/questions/42822442/r-rsentimentcalculate-score-returns-error-arguments-imply-differing-numbe
Word stemming in R,"<p>I am working on a text mining project and trying to clean the text - words in singular/plural forms, verbs in different tenses and misspelling words. My sample looks like this:</p>

<pre><code>test &lt;- c(""apple"",""apples"",""wife"",""wives"",""win"",""won"",""winning"",""winner"",""orange"",""oranges"",""orenge"")
</code></pre>

<p>I tried to use the wordStem function in SnowballC package. However the results are wrong: </p>

<pre><code>""appl""   ""appl""   ""wife""   ""wive""   ""win""    ""won""    ""win""    ""winner"" ""orang""  ""orang""  ""oreng"" 
</code></pre>

<p>What I would like to see is:</p>

<pre><code>""apple""   ""apple""   ""wife""   ""wife""   ""win""    ""win""    ""win""    ""winner"" ""orange""  ""orange""  ""orange""
</code></pre>
","r, text-mining, stemming","<p>That is just how the Porter Stemmer works. The reason for this is that it allows fairly simple rules to create the stems without having to store a large English vocabulary.   For example, I think that you would not like that both <code>change</code> and <code>changing</code> go to <code>chang</code>. It seems more natural that they should both stem to <code>change</code>. So would you make a rule that if you take <code>ing</code> off the end of a word, you should add back <code>e</code> to get the stem? Then what would happen with <code>clang</code> and <code>clanging</code>? The Porter Stemmer gives <code>clang</code>. Adding <code>e</code> would give the non-word <code>clange</code>. Either you use simple processing rules that sometimes create stems that are not words, <em>or</em> you must include a large vocabulary and have more complex rules that depend on what the words are. The Porter Stemmer uses the simple rules method. </p>
",3,2,1958,2017-03-20 16:17:31,https://stackoverflow.com/questions/42908720/word-stemming-in-r
How to match similar documents in R,"<p>I have created two corpuses: one containing tweet texts and another containing company names. What I'm trying to do is find which companies are mentioned in tweets.</p>

<p>Example document of a tweet:</p>

<pre><code>&gt; writeLines(as.character(tweet_corp[[175]]))
general motor send mexican made model chevi cruze us car dealer tax free across border make usaor pay big border tax
</code></pre>

<p>Example document of a company:</p>

<pre><code>&gt; writeLines(as.character(company_corp[[1397]]))
general motor
</code></pre>

<p>I would like an output that matches tweet_corp[[175]] with company_corp[[1397]]. Is there any way to do this?</p>
","r, twitter, text-mining","<p>You could use the <code>stringr</code> package to check whether a company name occurs in a tweet, e.g.</p>

<pre><code>library(stringr)

company_name &lt;- ""general motor""

tweet &lt;- ""general motor send mexican made model chevi cruze us car dealer tax free across border make usaor pay big border tax""

# check whether a company name occurs in a string
str_detect(
  string = tweet,
  pattern = coll(company_name)
)
</code></pre>
",0,0,44,2017-03-25 23:16:10,https://stackoverflow.com/questions/43023066/how-to-match-similar-documents-in-r
How can I lemmatize english words (example: &#39;run&#39; and &#39;ran&#39;) using R to bring them all to the same tense?,"<p>I want to lemmatize english words such that all of them get converted to the same tense. For example:</p>

<pre><code>c(""ran"",""run"",""running"") 
</code></pre>

<p>should become <code>c(""run"",""run"",""run"")</code>.</p>

<p>I have already explored R packages such as tm, wordnet, RTextTools, and Snowball C; but all of them result in the output <code>c(""ran"",""run"",""run"")</code>. As you can see, they do not convert ""ran"" to ""run"". </p>
","r, nlp, text-mining, string-matching, lemmatization","<p>Have a look at the <a href=""https://github.com/trinker/textstem#lemmatizing"" rel=""noreferrer""><strong>textstem</strong> package</a> I maintain:</p>

<pre><code>if (!require(""pacman"")) install.packages(""pacman"")
pacman::p_load(textstem)

lemmatize_words(c(""ran"",""run"",""running""))
###[1] ""run"" ""run"" ""run""
</code></pre>

<p>Note that if you actually have strings and not word vectors you may want the <code>lemmatize_strings</code> function instead.</p>
",6,1,986,2017-03-26 07:18:09,https://stackoverflow.com/questions/43026129/how-can-i-lemmatize-english-words-example-run-and-ran-using-r-to-bring-th
How word2vec retrieves result from binary files?,"<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('google_news.bin', binary=True)
print(model['the']) # this prints the 300D vector for the word 'the'
</code></pre>

<p>the code loads the google_news binary file to model. 
my question is, how the line 3 computes the output from a binary file ( Since Binary files contains 0's and 1's).</p>
","neural-network, nlp, semantics, text-mining, word2vec","<p>I'm not sure exactly what the question is here, but I assume you're asking how to load the binary into your Python app? You can use <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim</a> for example which has built-in tools to decode the binary:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('google_news.bin', binary=True)
print(model['the']) # this prints the 300D vector for the word 'the'
</code></pre>

<p><strong>EDIT</strong></p>

<p>I feel your question is more about binary files in general? This does not seem related to word2vec specifically. Anyways, in a word2vec binary file each line is a pair of word and weights in binary format. First the word is decoded into a string by looping the characters until it meets the binary character for ""space"". Then the rest is decoded from binary into floats. We know the number of floats since word2vec binary files have a header, such as ""3000000 300"", which tells us there are 3m words, each word is a 300D vector.</p>

<p>A binary file is organized as a series of bytes, each 8 bits. Read more about binary on the <a href=""https://en.wikipedia.org/wiki/Binary_file"" rel=""nofollow noreferrer"">wiki page</a>. </p>

<p>The number 0.0056 in decimal format, becomes in binary:</p>

<pre><code>00111011 10110111 10000000 00110100
</code></pre>

<p>So here there are 4 bytes that make up a float. How do we know this? Because we assume the binary encodes 32 bit float.</p>

<p>What if the binary file represents 64 bit precision floats? Then the decimal 0.0056 in binary becomes:</p>

<pre><code>00111111 01110110 11110000 00000110 10001101 10111000 10111010 11000111
</code></pre>

<p>Yes, twice the length because twice the precision. So when we decode the word2vec file, if the weights are 300d, and 64 bit encoding, then there should be 8 bytes to represent each number. So a word embedding would have 300*64=19,200 binary digits in each line of the file. Get it?</p>

<p>You can google ""how binary digits"" work, millions of examples.</p>
",1,0,956,2017-03-28 16:25:57,https://stackoverflow.com/questions/43074949/how-word2vec-retrieves-result-from-binary-files
How to count word frequency from a Pandas Dataframe- Python,"<p>I currently created a Pandas Dataframe from a dictionary.
The Dataframe looks something like:</p>

<pre><code>      URL         TITLE
0   /xxxx.xx   Hi this is word count
1   /xxxx.xx   Hi this is Stack Overflow
2   /xxxx.xx   Stack Overflow Questions
</code></pre>

<p>I want to add a new column to this table, which lists the number of frequency the word ""Stack Overflow"" appears. So for example, it would be like:</p>

<pre><code>      URL         TITLE                          COUNT
0   /xxxx.xx   Hi this is word count               0
1   /xxxx.xx   Hi this is Stack Overflow           1
2   /xxxx.xx   Stack Overflow Questions            1
</code></pre>

<p>The <code>count</code> function does not seem to work for dictionaries, but only for strings.  Would there bean easy way to do this?</p>
","python, pandas, dictionary, dataframe, text-mining","<p>Assuming this is actually a <code>pandas dataframe</code>, you could do:</p>

<pre><code>import pandas as pd

table = {   'URL': ['/xxxx.xx', '/xxxx.xx', '/xxxx.xx'], 
            'TITLE': ['Hi this is word count', 'Hi this is Stack Overflow', 'Stack Overflow Questions']}

df = pd.DataFrame(table)
df['COUNT'] = df.TITLE.str.count('Stack Overflow')
print(df)
</code></pre>

<p>This yields:</p>

<pre><code>                       TITLE       URL  COUNT
0      Hi this is word count  /xxxx.xx      0
1  Hi this is Stack Overflow  /xxxx.xx      1
2   Stack Overflow Questions  /xxxx.xx      1
</code></pre>
",3,2,5642,2017-03-29 07:46:53,https://stackoverflow.com/questions/43087420/how-to-count-word-frequency-from-a-pandas-dataframe-python
StandPOSTagger in Python &quot;Could not find or load main class&quot;,"<p>I recently try to learn nltk package through <a href=""http://textminingonline.com/dive-into-nltk-part-v-using-stanford-text-analysis-tools-in-python"" rel=""nofollow noreferrer"">http://textminingonline.com/dive-into-nltk-part-v-using-stanford-text-analysis-tools-in-python</a>.
But I faced a question about performing JAVA code in Python:</p>

<pre><code>import os
java_path = ""C:\Program Files (x86)\Java\jre1.8.0_121\\bin\java.exe""
os.environ['JAVAHOME'] = java_path
os.environ['JAVAHOME']
</code></pre>

<p>It turned out:</p>

<pre><code>'C:\\Program Files (x86)\\Java\\jre1.8.0_121\\bin\\java.exe'
</code></pre>

<p>Then I run nltk code:</p>

<pre><code>import nltk
from nltk.tag.stanford import StanfordPOSTagger
english_postagger=StanfordPOSTagger('models/english-bidirectional-distsim.tagger','stanford-postagger.jar')
english_postagger.tag('hi')
</code></pre>

<p>However:</p>

<pre><code>`Error: Could not find or load main class`edu.stanford.nlp.tagger.maxent.MaxentTagger
</code></pre>

<p>I reviewed the documents in 'stanford-postagger.jar', the MaxentTagger file was there:
<a href=""https://i.sstatic.net/uAmHy.jpg"" rel=""nofollow noreferrer"">path to Maxent Tagger</a></p>

<p>May I know how I could set right class path? or other way to solve this problem.
P.S. : I don't have experience in Java, but Python. </p>
","java, python, stanford-nlp, text-mining","<p>The issue is you don't have access to the jars, so this is a CLASSPATH issue.  I'm not positive this will work with <code>nltk</code>, but I've seen previous answers where setting <code>os.environ[""CLASSPATH""]= ""/path/to/stanford-corenlp-full-2016-10-31""</code> solves this.</p>

<p>You can download Stanford CoreNLP 3.7.0 from here: </p>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/download.html"" rel=""nofollow noreferrer"">http://stanfordnlp.github.io/CoreNLP/download.html</a></p>

<p>If you want to use our tools in Python, I would recommend using the Stanford CoreNLP 3.7.0 server and making small server requests (or using the <code>stanza</code> library).</p>

<p>If you use <code>nltk</code> what I believe happens is Python just calls our Java code with <code>subprocess</code> and this can actually be very inefficient since distinct calls reload all of the models.</p>

<p>Here is a previous answer I gave which describes this more thoroughly:</p>

<p><a href=""https://stackoverflow.com/questions/42896027/cannot-use-pycorenlp-for-python3-5-through-terminal/42915635#42915635"">cannot use pycorenlp for python3.5 through terminal</a></p>
",0,0,1329,2017-04-04 15:04:55,https://stackoverflow.com/questions/43210972/standpostagger-in-python-could-not-find-or-load-main-class
tm_map merging lines on condition,"<p>I extracted the text from pdf files and created a corpus object.</p>

<p>Within the texts, I have lines ending with "","" or ""-"" and I would like to append to them the following line, because it belongs to the same sentence.</p>

<p>For instance I have</p>

<pre><code>[1566] ""this and other southeastern states (Eukerria saltensis,""      
[1567] ""Sparganophilus helenae, Sp. tennesseensis). In the"" 
</code></pre>

<p>And I would like to have instead</p>

<pre><code>[1566] ""this and other southeastern states (Eukerria saltensis, Sparganophilus helenae, Sp. tennesseensis). In the"" 
</code></pre>

<p>I tried things like replacing line breaks, but with no success :</p>

<pre><code>tm_map(myCorpus, content_transformer(gsub), pattern ="",$\n"",replacement = """")
</code></pre>

<p>Any idea on how I can do this in R?</p>
","r, text-mining, tm","<p>Thanks, it does work!</p>

<p>I had to put it in a function to make it work with tm_map, though:</p>

<pre><code>clean.X &lt;- function(X){

  X2 &lt;- paste0(X,collapse=""\n"")
  X2 &lt;- gsub("",\\n"","", "",X2)
  X2 &lt;- gsub(""\\-\\n"",""-"",X2)
  X2 &lt;- unlist(strsplit(X2,""\\n""))
  return(X2)

 }

txt2 &lt;- tm_map(txt, content_transformer(clean.X))
</code></pre>
",0,0,67,2017-04-05 21:16:17,https://stackoverflow.com/questions/43241776/tm-map-merging-lines-on-condition
Apriori in WEKA,"<p>I'm new to all these Data mining, WEKA Tool etc.,</p>

<p>In my academic project I have to deal with bug reports. I have them in my SQL Server. I took the Bug summary attribute and applied tokenization,stop words removal and stemming techniques.  </p>

<p>All the stemmed words in the summary are stored in database ; separated. Now I have to apply <strong>Frequent pattern mining algorithm</strong> and find out <strong>frequent item sets</strong> by using WEKA tool. I have my arff file like this.</p>

<pre><code>@relation ItemSets

@attribute bugid integer
@attribute summary string

@data
755113,enhanc;keep;log;recommend;share
759414,access;review;social
763806,allow;intrus;less;provid;shrunken;sidebar;social;specifi
767221,datacloneerror;deeper;dig;framework;jsm
771353,document;integr;provid;secur;social
785540,avail;determin;featur;method;provid;social;whether
785591,chat;dock;horizont;nest;overlap;scrollbar
787767,abus;api;implement;perform;runtim;warn;worker
</code></pre>

<p>After opening it in Weka, under the <strong>Associate</strong> tab of WEKA Explorer I'm unable to start the process(Start button is disabled) with Apriori selected.</p>

<p>Now please suggest me how to find frequent itemsets on the summary attribute using WEKA. I.m in need of serious help. Help will be appreciated. Thanks in advance!</p>
","weka, text-mining, apriori","<p>The reason why Apriori is not available using your file in Weka is that Apriori only allows nominal attribute values. What sort of rules are you trying to find? Could you give an example of rules you want to obtain?</p>

<pre><code>values_you_want_to_be_the_antecedent_part_of_your_rule ==&gt; values_you_want_to_be_the_consequent_part_of_your_rule
</code></pre>

<p>Changing your attributes to nominal like this</p>

<pre><code>@relation ItemSets

@attribute bugid {755113, 759414, 763806}
@attribute summary {'enhanc;keep;log;recommend;share', 'access;review;social', 'allow;intrus;less;provid;shrunken;sidebar;social;specifi'}

@data
755113,'enhanc;keep;log;recommend;share'
759414,'access;review;social'
763806,'allow;intrus;less;provid;shrunken;sidebar;social;specifi'
</code></pre>

<p>will only give you rules like</p>

<pre><code>bugid=755113 1 ==&gt; summary=enhanc;keep;log;recommend;share 1    &lt;conf:(1)&gt; lift:(3) lev:(0.22)
</code></pre>

<p>If you're looking for frequent itemsets among the summary words, the bugid is irrelevant and you can remove it from your file. Apriori is used to obtain association rules e.g. <code>enhanc, keep</code> gives <code>log</code> with support X and confidence Y. To find frequent itemsets, you need to restructure your data so that each summary word is an attribute with values true/false or true/missing, see <a href=""https://stackoverflow.com/questions/35741464/how-to-find-frequent-itemset-irrespective-of-attribute-name"">this</a> question. </p>

<p>Try the following file in Weka. Select Associate, choose Apriori, double-click on the white input field next to the Choose button. There, set <code>outputItemSets</code> to true. In the console output, you will see all frequent itemsets and all obatined rules with sufficient support. </p>

<pre><code>@relation ItemSets

@attribute enhanc {true}
@attribute keep {true}
@attribute log {true}
@attribute recommend {true}
@attribute share {true}
@attribute access {true}
@attribute review {true}
@attribute social {true}
@attribute allow {true}
@attribute intrus {true}
@attribute less {true}
@attribute provid {true}
@attribute shrunken {true}
@attribute sidebar {true}
@attribute specifi {true}


@data
true,true,true,true,true,?,?,?,?,?,?,?,?,?,?
?,?,?,?,?,true,true,true,?,?,?,?,?,?,?
?,?,?,?,?,?,?,true,true,true,true,true,true,true,true
</code></pre>

<p>The questionmarks <code>?</code> represent a missing value.</p>
",1,0,2313,2017-04-06 18:21:47,https://stackoverflow.com/questions/43262966/apriori-in-weka
"Cosine Similarity, Two different vectors but the result is aproximately 1?","<p>I have two vectors</p>

<p><strong>Vector1(1,2,3,4,5,6)</strong></p>

<p><strong>Vector2(12,13,14,15,16,17)</strong></p>

<p>Two vectors is completely different. But i used Cosine Similarity formula and the result is <strong>0.943843313096</strong>.</p>

<p>I think result is about 0,1 or 0,2... but not. Why? please explain for me, thanks so much.</p>
","machine-learning, data-mining, text-mining, bigdata","<p>If you reduce the vectors to 2 dimensions you can understand it better.</p>

<p>Vector1(1,2) and Vector2(12,13)</p>

<p>The position vector1 will make an angle of 63.5 Degrees and position vector2 will make an angle of 47.31 degrees.  The difference is only 16.1 degrees.  Cos(16.1) = 0.96081</p>

<ul>
<li>The usual way to look at the cosine distance is Cos(theta) =
A.B/(Norm(A)*Norm(B)) for the above vectors, A.B = 38 Norm(A) = 2.23,
Norm(B) = 17.69 So, Cos(theta) = 38/(2.23*17.69) = 0.96056</li>
</ul>

<p>If you find theta from the above, theta will be equal to 16.1 degrees !!!</p>
",3,-2,801,2017-04-09 16:58:28,https://stackoverflow.com/questions/43309431/cosine-similarity-two-different-vectors-but-the-result-is-aproximately-1
Word substitution within tidy text format,"<p>Hi i'm working with a tidy_text format and i am trying to substitute the strings ""emails"" and ""emailing"" into ""email"". </p>

<pre><code>set.seed(123)
terms &lt;- c(""emails are nice"", ""emailing is fun"", ""computer freaks"", ""broken modem"")
df &lt;- data.frame(sentence = sample(terms, 100, replace = TRUE))
df
str(df)
df$sentence &lt;- as.character(df$sentence)
tidy_df &lt;- df %&gt;% 
unnest_tokens(word, sentence)

tidy_df %&gt;% 
count(word, sort = TRUE) %&gt;% 
filter( n &gt; 20) %&gt;% 
mutate(word = reorder(word, n)) %&gt;% 
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) + 
coord_flip()
</code></pre>

<p>this works fine, but when i use:</p>

<pre><code> tidy_df &lt;- gsub(""emailing"", ""email"", tidy_df)
</code></pre>

<p>to substitute words and run the bar chart again i get the following error message:</p>

<p>Error in UseMethod(""group_by_"") : 
  no applicable method for 'group_by_' applied to an object of class ""character""</p>

<p>Does any one know how to easily substitute words within tidy text formats without changing structure/class of the tidy_text?</p>
","r, text-mining, tidytext","<p>Removing the ends of words like that is called <strong>stemming</strong> and there are a couple of packages in R that will do that for you, if you'd like. One is the <a href=""https://github.com/ropensci/hunspell"" rel=""noreferrer"">hunspell package from rOpenSci</a>, and another option is the SnowballC package which implements Porter algorithm stemming. You would implement that like so:

<br/></p>

<pre class=""lang-r prettyprint-override""><code>library(dplyr)
library(tidytext)
library(SnowballC)

terms &lt;- c(""emails are nice"", ""emailing is fun"", ""computer freaks"", ""broken modem"")

set.seed(123)
data_frame(txt = sample(terms, 100, replace = TRUE)) %&gt;%
        unnest_tokens(word, txt) %&gt;%
        mutate(word = wordStem(word))
#&gt; # A tibble: 253 × 1
#&gt;      word
#&gt;     &lt;chr&gt;
#&gt; 1   email
#&gt; 2       i
#&gt; 3     fun
#&gt; 4  broken
#&gt; 5   modem
#&gt; 6   email
#&gt; 7       i
#&gt; 8     fun
#&gt; 9  broken
#&gt; 10  modem
#&gt; # ... with 243 more rows
</code></pre>

<p>Notice that it is stemming <em>all</em> your text and that some of the words don't look like real words anymore; you may or may not care about that.</p>

<p>If you don't want to stem all your text using a stemmer like SnowballC or hunspell, you can use dplyr's <code>if_else</code> within <code>mutate()</code> to replace just specific words.

<br/></p>

<pre class=""lang-r prettyprint-override""><code>set.seed(123)
data_frame(txt = sample(terms, 100, replace = TRUE)) %&gt;%
        unnest_tokens(word, txt) %&gt;%
        mutate(word = if_else(word %in% c(""emailing"", ""emails""), ""email"", word))
#&gt; # A tibble: 253 × 1
#&gt;      word
#&gt;     &lt;chr&gt;
#&gt; 1   email
#&gt; 2      is
#&gt; 3     fun
#&gt; 4  broken
#&gt; 5   modem
#&gt; 6   email
#&gt; 7      is
#&gt; 8     fun
#&gt; 9  broken
#&gt; 10  modem
#&gt; # ... with 243 more rows
</code></pre>

<p>Or it might make more sense for you to use <code>str_replace</code> from the stringr package.

<br/></p>

<pre class=""lang-r prettyprint-override""><code>library(stringr)
set.seed(123)
data_frame(txt = sample(terms, 100, replace = TRUE)) %&gt;%
        unnest_tokens(word, txt) %&gt;%
        mutate(word = str_replace(word, ""email(s|ing)"", ""email""))
#&gt; # A tibble: 253 × 1
#&gt;      word
#&gt;     &lt;chr&gt;
#&gt; 1   email
#&gt; 2      is
#&gt; 3     fun
#&gt; 4  broken
#&gt; 5   modem
#&gt; 6   email
#&gt; 7      is
#&gt; 8     fun
#&gt; 9  broken
#&gt; 10  modem
#&gt; # ... with 243 more rows
</code></pre>
",15,4,4054,2017-04-11 11:02:56,https://stackoverflow.com/questions/43344108/word-substitution-within-tidy-text-format
text mining within same sentence in R,"<p>I have a text file </p>

<blockquote>
  <p>""I am writing today. Today I am thinking of writing. Today is great day""</p>
</blockquote>

<p>I am trying to find number of instances within a sentence where ""writing today"" was mentioned.It can happen that ""writing today"" is not together but still part of same sentence (eg: 2nd sentence), need to capture that as well.</p>

<p>So in the above example, my count will be 2.</p>

<p>Any idea how to do it in R?
TIA</p>
","r, text-mining","<p>There are lots of ways to do this, but with tidytext,</p>



<pre class=""lang-r prettyprint-override""><code>library(tidyverse)
library(tidytext)

data_frame(text = ""I am writing today. Today I am thinking of writing. Today is great day"") %&gt;%
    unnest_tokens(sentence, text, 'sentences', to_lower = FALSE) %&gt;%
    mutate(sentence_number = row_number()) %&gt;%
    unnest_tokens(word, sentence, 'words', drop = FALSE) %&gt;%
    group_by(sentence_number) %&gt;% 
    filter('today' %in% word, 'writing' %in% word) %&gt;% 
    select(-word) %&gt;% distinct() %&gt;% ungroup() %&gt;%
    mutate(count = n())

#&gt; # A tibble: 2 × 3
#&gt;                          sentence sentence_number count
#&gt;                             &lt;chr&gt;           &lt;int&gt; &lt;int&gt;
#&gt; 1             I am writing today.               1     2
#&gt; 2 Today I am thinking of writing.               2     2
</code></pre>
",2,0,174,2017-04-17 16:38:54,https://stackoverflow.com/questions/43455540/text-mining-within-same-sentence-in-r
Text Mining - Count Frequencies of Phrases (more than one word),"<p>I am familiar with using the tm library to create a tdm and count frequencies of terms. </p>

<p>But these terms are all single-word. </p>

<p>How can do count the # of times a multi-word phrase occurs in a document and/or corpus? </p>

<p>EDIT:</p>

<p>I am adding the code I have now to improve/clarify my post.</p>

<p>This is pretty standard code to build a term-document matrix:</p>

<pre><code>library(tm)


cname &lt;- (""C:/Users/George/Google Drive/R Templates/Gospels corpus"")   

corpus &lt;- Corpus(DirSource(cname))

#Cleaning
corpus &lt;- tm_map(corpus, tolower)
corpus &lt;- tm_map(corpus, removeNumbers)
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, stripWhitespace)
corpus &lt;- tm_map(corpus, removeWords, c(""a"",""the"",""an"",""that"",""and""))

#convert to a plain text file
corpus &lt;- tm_map(corpus, PlainTextDocument)

#Create a term document matrix
tdm1 &lt;- TermDocumentMatrix(corpus)

m1 &lt;- as.matrix(tdm1)
word.freq &lt;- sort(rowSums(m1), decreasing=T)
word.freq&lt;-word.freq[1:100]
</code></pre>

<p>The problem is that this returns a matrix of <em>single word</em> terms, example:</p>

<pre><code>  all      into      have      from      were       one      came       say       out 
  397       390       385       383       350       348       345       332       321
</code></pre>

<p>I want to be able to search for multi-word terms in the corpus instead. So for example ""came from"" instead of just ""came"" and ""from"" separately.</p>

<p>Thank you.</p>
","r, nlp, text-mining, n-gram","<p>I created following function for obtaining word n-grams and their corresponding frequencies</p>

<pre><code>library(tau) 
library(data.table)
# given a string vector and size of ngrams this function returns     word ngrams with corresponding frequencies
createNgram &lt;-function(stringVector, ngramSize){

  ngram &lt;- data.table()

  ng &lt;- textcnt(stringVector, method = ""string"", n=ngramSize, tolower = FALSE)

  if(ngramSize==1){
    ngram &lt;- data.table(w1 = names(ng), freq = unclass(ng), length=nchar(names(ng)))  
  }
  else {
    ngram &lt;- data.table(w1w2 = names(ng), freq = unclass(ng), length=nchar(names(ng)))
  }
  return(ngram)
}
</code></pre>

<p>Given a string like </p>

<pre><code>text &lt;- ""This is my little R text example and I want to count the frequency of some pattern (and - is - my - of). This is my little R text example and I want to count the frequency of some patter.""
</code></pre>

<p>Here is how to call the function for a pair of words, for phrases of length 3 pass 3 as argument</p>

<pre><code>res &lt;- createNgram(text, 2)
</code></pre>

<p>printing <code>res</code> outputs</p>

<pre><code>           w1w2      freq   length
 1:        I want    2      6
 2:        R text    2      6
 3:       This is    2      7
 4:         and I    2      5
 5:        and is    1      6
 6:     count the    2      9
 7:   example and    2     11
 8:  frequency of    2     12
 9:         is my    3      5
10:      little R    2      8
11:     my little    2      9
12:         my of    1      5
13:       of This    1      7
14:       of some    2      7
15:   pattern and    1     11
16:   some patter    1     11
17:  some pattern    1     12
18:  text example    2     12
19: the frequency    2     13
20:      to count    2      8
21:       want to    2      7
</code></pre>
",0,0,3213,2017-04-19 12:53:26,https://stackoverflow.com/questions/43496348/text-mining-count-frequencies-of-phrases-more-than-one-word
Keeping punctuation in R Document Term Matrix,"<p>I'm trying to make a <code>DocumentTermMatrix</code> in <code>R</code>, using the parameter <code>control = list()</code> to limit the terms to a pre-defined list of text-based emojis (:D, :), :(, etc.). However, dtm doesn't pick up certain emojis (like <code>"":D""</code> or <code>"":)""</code>), but some other works fine (<code>"":))""</code>) . My code: </p>

<pre><code>text = c("":D"", "":))"" ) 
corpus &lt;- Corpus(VectorSource(text)
corpus = tm_map(corpus, PlainTextDocument)
dtm = DocumentTermMatrix(corpus, list(dictionary = c("":D"" , "":))"" )))
emojidf &lt;- as.data.frame(as.matrix(dtm))

  :D :))
1  0   0
2  0   1
</code></pre>

<p>To fix this, I could use <code>content_transformer</code> and <code>gsub</code> to change the problematic emojis to words. However, I'd like to know how <code>DocumentTermMatrix</code> or even <code>Corpus</code> treat punctuation as words. </p>
","r, text-mining, tm","<p>Two issues (see <code>?DocumentTermMatrix</code> and <code>?termFreq</code>): The wordLengths filter by default demands a minimum word length of 3 characters. And tolower by default turns <code>:D</code> into <code>:d</code>. So try:   </p>

<pre><code>library(tm)
text &lt;- c("":D"", "":))"" ) 
corpus &lt;- Corpus(VectorSource(text))
dtm &lt;- DocumentTermMatrix(
  corpus, 
  control = list(
    dictionary = c("":D"" , "":))""), 
    wordLengths=c(-Inf,Inf), 
    tolower=FALSE
  )
)
as.matrix(dtm)
#     Terms
# Docs :)) :D
#    1   0  1
#    2   1  0
</code></pre>
",0,0,481,2017-04-21 08:09:21,https://stackoverflow.com/questions/43537278/keeping-punctuation-in-r-document-term-matrix
"Python: a bytes-like object is required, not &#39;str&#39; while printing","<p><strong>Problem / what I tried</strong><br></p>

<p>I downloaded the <code>textmining 1.0</code> library which I tried to run, however this gave me some import errors (because this is a python 2 lib) so I searched on stackoverflow and found out that I had to use <code>2to3.py</code> and now everything is working. However when I do this:</p>

<pre><code>def buildMatrix(self,document_list):
        print(""building matrix..."")
        tdm = textmining.TermDocumentMatrix()
        for doc in document_list:
             tdm.add_doc(doc)
        tdm.write_csv(r'path\matrix.csv', cutoff=2)
</code></pre>

<p>(document_list is just a list of <code>strings</code>)
I get the following error:</p>

<pre><code>  File ""C:\Users\RICK\Anaconda\lib\site-packages\textmining\__init__.py"", line 335, in write_csv
    f.writerow(row)

TypeError: a bytes-like object is required, not 'str'
</code></pre>

<p>I'm pretty sure the row should be a <code>string</code> while inspecting the code of <code>textmining 1.0</code>. So I wanted to print this row by editing the source code:</p>

<pre><code>f = csv.writer(open(filename, 'wb'))
        for row in self.rows(cutoff=cutoff):
            print(row)
            f.writerow(row)
</code></pre>

<p>However even now I get the same <code>TypeError</code>:</p>

<pre><code>  File ""C:\Users\RICK\Anaconda\lib\site-packages\textmining\__init__.py"", line 335, in write_csv
    print(row)

TypeError: a bytes-like object is required, not 'str'
</code></pre>

<p>I searched on stack overflow to solve this by replacing the <code>'wb'</code> by <code>'w'</code>, however this still gives me the <code>TypeError.</code><br><br>
<strong>Questions</strong></p>

<ul>
<li>How can I fix the code to make it able to write the row.</li>
<li>Why does even the print statement cause a <code>TypeError</code></li>
</ul>

<hr>

<p><strong>Edit based on comment(s)</strong>:<br>
Suggestion of Claudio still gave me the <code>TypeError</code>:</p>

<pre><code>  File ""C:\Users\RICK\Anaconda\lib\site-packages\textmining\__init__.py"", line 335, in write_csv
    f.write(row)

TypeError: a bytes-like object is required, not 'str'
</code></pre>

<p>Suggestion of Tony:<br>
Code inspection:</p>

<pre><code>for article in articles:
        abstract = searcher.getArticleAbstract(article)
        print(type(abstract)) #--&gt; returns &lt;class 'str'&gt;
        all_abstracts.append(abstract)
    txtSearcher.buildMatrix(all_abstracts)
</code></pre>

<p>I now have these <code>open</code> lines:</p>

<pre><code>f = open(os.path.join(data_dir, 'stopwords.txt'),""r"")
f = open(os.path.join(data_dir, 'dictionary.txt'),""r"")
f = csv.writer(open(filename, 'w'))
</code></pre>

<h1> Some strange things are going on </h1>

<p><a href=""https://i.sstatic.net/JqGdx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JqGdx.png"" alt=""enter image description here""></a>
<strong>This will take me to:</strong></p>

<pre><code>def write_csv(self, filename, cutoff=2):
        print(""This really makes me sad!"")

        """"""
        Write term-document matrix to a CSV file.

        filename is the name of the output file (e.g. 'mymatrix.csv').
        cutoff is an integer that specifies only words which appear in
        'cutoff' or more documents should be written out as columns in
        the matrix.

        """"""
        print(self.rows)
        f = csv.writer(open(filename, 'w'))
        for row in self.rows(cutoff=cutoff):
            f.writerow(row)
</code></pre>

<h2>It does print the ""building matrix.."" (so the function is called) however it doesn't print the <code>print(""This really makes me sad!"")</code></h2>
","python, csv, export-to-csv, text-mining","<p>To my current knowledge the actual cause of the in the question described weird behavior of the program was the fact that the question I have asked in the comments:    </p>

<p><strong><code>Are you sure that you are getting the error from the code you are editing?</code></strong></p>

<blockquote>
  <p><strong>Was not considered as relevant and the only right answer explaining all of the observed issues.</strong> </p>
</blockquote>

<p>All of the other detected issues like e.g. </p>

<p><code>**RENAME** def write_csv(...) to for example def my_write_csv(...)</code> </p>

<p>including the provided explanations and hints like: </p>

<p>If you define an own function with the same name as the function in a library you run into trouble with local/global scopes and are quite lost to know WHICH ONE was actually executed? This one from the library or this one you have defined ... The fact that the <code>print(""This really makes me sad!"")</code> inserted by you wasn't printed indicates that not THIS function was executed but the library one instead ... </p>

<p>Check the entire code including the file to read or excerpts able to reproduce the error - there is sure a very simple explanation for this weird behavior. </p>

<p>Look for a not closed parenthesis or string quotation or list <code>]</code>, etc. in the code preceding the line in which the error is indicated. </p>

<p><strong>couldn't under this circumstances lead to a success ...</strong></p>
",0,3,2859,2017-04-24 08:10:48,https://stackoverflow.com/questions/43582925/python-a-bytes-like-object-is-required-not-str-while-printing
Extracting data from a pattern in R,"<p>I'm interested in converting data from this text file into a format I could load into a MySQL Workbench database. </p>

<p><a href=""https://sbir.nasa.gov/SBIR/abstracts/17-1.html"" rel=""nofollow noreferrer"">https://sbir.nasa.gov/SBIR/abstracts/17-1.html</a></p>

<p>I want to run some R code that will give me the name of the business after each line titled </p>

<p>""SMALL BUSINESS CONCERN: (Firm Name, Mail Address, City/State/ZIP, Phone)""</p>

<p>For example, I'm looking for an output that looks something like this:</p>

<p>Transition45 Technologies, Inc. 
ATSP Innovations</p>

<p>etc. That I could load into a database column.</p>

<p>Hope that makes sense, I'm relatively new to this. Thanks.</p>
","mysql, r, text-mining, data-extraction","<p>You problem/question is not clear. </p>

<p>If I am correct, you want to extract address detail that written next after line <strong>""SMALL BUSINESS CONCERN:</strong> (Firm Name, Mail Address, City/State/ZIP, Phone)"", right?. If so, then</p>

<pre><code>url &lt;- ""https://sbir.nasa.gov/SBIR/abstracts/17-1.html""

abstracts_page &lt;- readLines(url)
abstracts_page &lt;- gsub(""&lt;.*?&gt;"", """", abstracts_page)
abstracts_page &lt;- gsub(""\\t+"", """", abstracts_page)

address_header_index &lt;- grep(""SMALL BUSINESS CONCERN:"", abstracts_page)

address_list &lt;- lapply(address_header_index, function(i) {
  return(abstracts_page[(i + 2):(i + 6)])
})

address_list &lt;- data.frame(do.call(""rbind"", address_list))

head(address_list)

#                                          X1                                   X2                   X3
# 1          Transition45 Technologies, Inc.                1739 North Case Street      Orange,&amp;nbsp;CA
# 2                         ATSP Innovations                    60 Hazelwood Drive   Champaign,&amp;nbsp;IL
# 3         Cornerstone Research Group, Inc.               2750 Indian Ripple Road      Dayton,&amp;nbsp;OH
# 4 Interdisciplinary Consulting Corporation      5745 Southwest 75th Street, #364 Gainesville,&amp;nbsp;FL
# 5                 CFD Research Corporation  701 McMillian Way Northwest, Suite D  Huntsville,&amp;nbsp;AL
# 6           LaunchPoint Technologies, Inc.        5735 Hollister Avenue, Suite B      Goleta,&amp;nbsp;CA

#            X4             X5
# 1 92865-4211  (714) 283-2118
# 2 61820-7460  (217) 417-2374
# 3 45440-3638  (937) 320-1877
# 4 32608-5504  (352) 283-8110
# 5 35806-2923  (256) 726-4800
# 6 93117-6410  (805) 683-9659
</code></pre>
",0,-1,45,2017-04-25 00:55:58,https://stackoverflow.com/questions/43599856/extracting-data-from-a-pattern-in-r
How to add custom stop word list to StopWordsRemover,"<p>I am using pyspark.ml.feature.StopWordsRemover class on my pyspark dataframe. It has ID and Text column. In addition to default stop word list provided, I would like to add my own custom list to remove all numeric values from string. </p>

<p>I can see there is a method provided to add setStopWords for this class. I think I'm struggling with the proper syntax to use this method. </p>

<pre><code>from pyspark.sql.functions import *
from pyspark.ml.feature import * 

a = StopWordsRemover(inputCol=""words"", outputCol=""filtered"")
b = a.transform(df)
</code></pre>

<p>The above code gives me expected results in the filtered column but it only removes / stops standard words. I'm looking for a method to add my own custom list which would have more words and numeric values that I wish to filter.</p>
","python, pyspark, apache-spark-sql, text-mining, stop-words","<p>You can specify it with this : </p>

<pre><code>stopwordList = [""word1"",""word2"",""word3""]

StopWordsRemover(inputCol=""words"", outputCol=""filtered"" ,stopWords=stopwordList)
</code></pre>

<h2>A small Note:</h2>

<p>The above solution replaces the original list of stop words with the list we supplied.<br/>
If you want to add your own stopwords in addition to the existing/predefined stopwords, then we need to append the list with the original list before passing into StopWordsRemover() as a parameter. We transform to set to remove any duplicate.</p>

<blockquote>
  <p><code>stopwordList = [""word1"",""word2"",""word3""]</code>
  <code>stopwordList.extend(StopWordsRemover().getStopWords())</code><br>
  <code>stopwordList = list(set(stopwordList))#optionnal</code><br> 
  <code>StopWordsRemover(inputCol=""words"", outputCol=""filtered"" ,stopWords=stopwordList)</code></p>
</blockquote>
",12,9,6745,2017-04-26 01:15:44,https://stackoverflow.com/questions/43623400/how-to-add-custom-stop-word-list-to-stopwordsremover
how to read text document in R?,"<p>I want to read text document in R based on following condition -
based on certain keywords it will read the sentences and  whenever it will find the keywords and sentence ended with full stop (.), just stores only those statement in a list.</p>

<p>output- list contain only those statement which have particular keyword.</p>

<p>I tried with scan function like this-</p>

<pre><code>b&lt;-scan(""cbt14-Short Stories For Children.txt"",what = ""char"",sep = '.', nlines = 50)
</code></pre>

<p>as scan function have so many parameter, which I, am unable to understand right now. </p>

<p>can we achieve above output using scan function???</p>

<p>keyword = ""ship""</p>

<p>input--</p>

<p>this article u can read from ""www.google.com/ship"".
Illustrated by Subir Roy and Geeta Verma Man Overboard
I stood on the deck of S.S. Rajula. As she slowly moved out of Madras harbour, I waved to my grandparents till I could see them no more. I was thrilled to be on board a ship. It was a new experience for me.
""Are you travelling alone?"" asked the person standing next to me.
""Yes, Uncle, I'm going back to my parents in Singapore,"" I replied.
""What's your name?"" he asked. ""Vasantha,"" I replied. I spent the day exploring the ship. It looked just like a big house. There were furnished rooms, a swimming pool, a room for indoor games, and a library. Yet, there was plenty of room to 11111 around. The next morning the passengers were seated in the dining hall, having breakfast. The loudspeaker spluttered noisily and then the captain's voice came loud and clear. ""Friends we have just received a message that a storm is brewing in the Indian Ocean. I request all of you to keep calm. Do not panic. Those who are inclined to sea-
3</p>

<p>output list--</p>

<p>[1]this article u can read from ""www.google.com/ship"".</p>

<p>[2]I was thrilled to be on board a ship.</p>

<p>[3] I spent the day exploring the ship.</p>
","r, text-mining","<p>The difficult part of this problem is properly separating the sentences.  In this case I am using the period followed by a space "". "" to define a sentence.  In this sample it does produce a sentence with a single word - ""Rajula""  but this may be acceptable depending on your final application.</p>

<pre><code>#split the text into sentences using a "". ""
sentences&lt;-strsplit(b, ""\\. "")
#find the sentences with the word ship in the answer
finallist&lt;-sentences[[1]][grepl(""ship"", sentences[[1]] )]
</code></pre>

<p>The above code uses base R.  Looking into the stringi or stringr library, there maybe a function to better handle the string splitting on a defined sentence.</p>
",2,3,447,2017-04-26 21:47:33,https://stackoverflow.com/questions/43645084/how-to-read-text-document-in-r
Machine Learning Libraries For Android,"<p>I am trying build a small text mining tool for my android app. I am checking for a machine learning library that will allow me to cluster, classify etc.</p>

<p>Are there any machine learning libraries for android? I came across tensorflow but I need a bit more access to common ML functions.</p>
","android, machine-learning, nlp, artificial-intelligence, text-mining","<p>You can try these ports of weka for android (they do not use the latest weka version, but it may be sufficient for your needs):</p>

<p><a href=""https://github.com/rjmarsan/Weka-for-Android"" rel=""nofollow noreferrer"">https://github.com/rjmarsan/Weka-for-Android</a></p>

<p><a href=""https://github.com/andrecamara/weka-android"" rel=""nofollow noreferrer"">https://github.com/andrecamara/weka-android</a></p>
",4,10,5478,2017-04-27 05:33:52,https://stackoverflow.com/questions/43649359/machine-learning-libraries-for-android
How to read .docx file in R or convert it into .txt file?,"<p>I'm trying to read CV. But can't find a way to read .docx file as plain text or convert it into text file.</p>
","r, text-mining","<p>read_docx from qdapTools package does the job</p>

<p><a href=""https://www.rdocumentation.org/packages/qdapTools/versions/1.3.3/topics/read_docx"" rel=""nofollow noreferrer"">https://www.rdocumentation.org/packages/qdapTools/versions/1.3.3/topics/read_docx</a></p>
",1,1,5585,2017-04-27 15:46:10,https://stackoverflow.com/questions/43662374/how-to-read-docx-file-in-r-or-convert-it-into-txt-file
text-mining: Determine to which noun a verb refers?,"<p>I'm trying to find a package/library which enables me to determine to which noun a verb refers (in Python):<br></p>

<blockquote>
  <p>The man was walking down the street</p>
</blockquote>

<p>That this will give me a result like walking <em>refers to</em> man. <br>
I tried to use <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">nltk</a> however as far as I could find out I can only tag the wordst as nouns/verbs etc. but cannot infer any references from this. </p>

<p><strong>Question</strong><br>
Are there any packages which are capable to do the above?</p>
","python, text, nlp, analytics, text-mining","<p>I think what you want to do is explore the syntactic dependencies among words. For that, you need to parse your text with a syntactic parser.</p>

<p>Since you want to know the references between nouns and verbs you will need to do two things.</p>

<p>You need to get the part-of-speech tags associated to each words (i.e. the morphology associated to each word, wether it's a ADJ, DET, VERB, NOUN, etc.) then you want to select the ones tagged as verbs and nouns.</p>

<p>Then, you want to look at which other words they connect with, I think mostly you will want to explore the 'nsubj' dependency.</p>

<p><a href=""https://spacy.io/"" rel=""nofollow noreferrer"">spaCy</a> is a NLP library for Python that performs syntactic parsing, and also has a on-line demo, if you want to try it out, check: </p>

<p><a href=""https://demos.explosion.ai/displacy/"" rel=""nofollow noreferrer"">https://demos.explosion.ai/displacy/</a></p>

<p>Here is the output for the example you gave:</p>

<p><a href=""https://i.sstatic.net/28p0h.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/28p0h.png"" alt=""enter image description here""></a></p>
",3,2,793,2017-04-28 13:14:41,https://stackoverflow.com/questions/43681137/text-mining-determine-to-which-noun-a-verb-refers
How to convert Euclidean distance to range 0 and 1 like Cosine Similarity?,"<p>Want to map Euclidean distance to the range [0, 1], somewhat like the cosine similarity of vectors.</p>

<p>For instance</p>

<pre><code>input  output
  0      1.0
  1      0.9  approximate
  2      0.8 to 0.9 somewhere
 inf     0.0
</code></pre>

<p>I tried the formula <code>1/(1+d)</code>, but that falls away from 1.0 too quickly.</p>
","machine-learning, data-mining, text-mining, bigdata","<p>It seems that you want the fraction's denominator to grow more slowly (the denominator is the bottom part, which you have as (d+1) so far).  There are various ways to handle this.  For instance, try a lower power for <strong>d</strong>, such as</p>

<pre><code>1 / (1 + d**(0.25))
</code></pre>

<p>... or an exponential decay in the denominator, such as</p>

<pre><code>1 / (1.1 ** d)
</code></pre>

<p>... or using a trig function to temper your mapping, such as</p>

<pre><code>1 - tanh(d)
</code></pre>

<p>Would something in one of these families work for you?</p>
",6,5,4337,2017-04-28 20:24:05,https://stackoverflow.com/questions/43688349/how-to-convert-euclidean-distance-to-range-0-and-1-like-cosine-similarity
"R, after utf8 filtering still weird characters","<p>I'm verry new to R but i need to do some text mining on tweets. I'm trying to clean the corpus so that there are only UTF8 characters. I use the function below to filter out the non UTF characters. </p>

<pre><code>#setup with own twitter key's and access tokens
library(twitteR)
library(tm)

setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_secret)
keyword = ""#circulatieplan""
sinceDate = ""2017-3-1""
tweets = searchTwitter(keyword,n = 300,lang = 'nl',since = sinceDate)
tweets_df = twListToDF(tweets)
tweets_df
View(tweets_df)

text = tweets_df$text
corpus = Corpus(VectorSource(text))
corpus &lt;- tm_map(corpus, content_transformer(function(x) iconv(enc2utf8(x), sub = ""byte"")))

corpus_clean &lt;- tm_map(corpus, tolower)
</code></pre>

<p>Afterwards i try to make it all lowercase but then i get some input errors.</p>

<p><code>Error in FUN(content(x), ...) : invalid input 'Elke Sleurs gehoord op de radio. Dan viel Siegi precies nog mee. #schizo ������������' in 'utf</code>8towcs'</p>

<p>My guess is that the filtering was not done perfectly and that the function can't change '�' to lowercase.</p>

<p>I'm not totaly shure how the utf filtering works and what it all means. Is there a better function for it or how do i solve this error.</p>

<p>EDIT: <br>
After looking at the raw data i found that there where some tweets that contained utf characters longer than 2 bytes. <br>
tweetid of a tweet that contains tis problem: 858280532039397379 <br>
data:</p>

<pre><code>""Elke Sleurs gehoord op de radio. Dan viel Siegi precies nog mee. #schizo \xed\xa0\xbd\xed\xb8\xb3\xed\xa0\xbd\xed\xb9\x84 #gent #circulatieplan"",
</code></pre>

<p>I then unsuccesfolly tried to remove al of them with regex. Is the regex wrong or can't u use regex on a corpus object?</p>

<pre><code>corpus &lt;- tm_map(corpus, content_transformer(function(x) gsub(x, pattern = ""(\\)\\w+"", replacement = """")))
</code></pre>
","r, twitter, utf-8, text-mining","<p>I found a way to filter the emoticons. After a lot of searching i found that there was a function that converts a character vector between encodings. <a href=""https://stat.ethz.ch/R-manual/R-devel/library/base/html/iconv.html"" rel=""nofollow noreferrer"">iconv documentation</a></p>

<pre><code>...
text = tweets_df$text    
# remove emoticons
text &lt;- sapply(text,function(row) iconv(row, ""latin1"", ""ASCII"", sub=""""))
corpus = Corpus(VectorSource(text))
...
</code></pre>
",1,0,730,2017-04-29 09:50:31,https://stackoverflow.com/questions/43694143/r-after-utf8-filtering-still-weird-characters
unrecognized option: --diagnostics-file in Mallet,"<p>I am using LDA in mallet to explore my data. I do not have training and test data. I just use it for clustering my data. 
I would like to use a number of useful diagnostic measures provided by Mallet. but when I use this query:</p>

<pre><code>bin\mallet train-topics --input doc500.mallet --num-topics 40 --num-top-words  50 --optimize-interval 10 --output-state doc500topic40-state.gz --output-topic-keys doc500topic40-keys.txt --output-doc-topics  doc500topic40-composition.txt --topic-word-weights-file  doc500topic40-word-weights.txt --diagnostics-file doc500topic40_diagnostics.xml
</code></pre>

<p>then I get this Error:</p>

<pre><code>Exception in thread ""main"" java.lang.IllegalArgumentException: 
Unrecognized option 16: --diagnostics-file
</code></pre>
","text-mining, lda, topic-modeling, diagnostics, mallet","<p>The diagnostics file option is relatively new. Make sure that you are using version 2.0.8.</p>
",3,1,198,2017-05-03 15:33:31,https://stackoverflow.com/questions/43764396/unrecognized-option-diagnostics-file-in-mallet
Removing extra space between the word and apostrophe,"<p>I have a list of strings which contain contraction from of verbs.
My list is something like this:</p>

<pre><code>[""What 's your name?"", ""Isn 't it beautiful?"",...]
</code></pre>

<p>I want to remove the space between the word and the apostrophe, so the new list would be:</p>

<pre><code>[""What's your name?"", ""Isn't it beautiful?"",...]
</code></pre>

<p>I used <code>replace()</code>but the list contains 5500 strings and there are different forms of contractions in it. The following code just replaces one form of contraction.</p>

<pre><code>s = s.replace(""'s"",""is"")
</code></pre>

<p>What should I do to remove the extra space between the word and the apostrophe?</p>
","python, string, text-mining","<pre><code>(?&lt;=[a-zA-Z])\s+(?=[a-z]*'\s*[a-z])
</code></pre>

<p>You can try this:</p>

<p><a href=""https://regex101.com/r/18GHqw/1"" rel=""nofollow noreferrer"">https://regex101.com/r/18GHqw/1</a></p>

<pre><code>import re

regex = r""(?&lt;=[a-zA-Z])\s+(?=[a-z]*'\s*[a-z])""

test_str = (""'What 's your name?','Isn 't it beautiful?'\n\n""
""Jesus ' cross\""\n""
""do n't\""\n""
""sdsda   sdsd'  sdsd"")

matches = re.finditer(regex, test_str)

for matchNum, match in enumerate(matches):
    matchNum = matchNum + 1

    print (""Match {matchNum} was found at {start}-{end}: {match}"".format(matchNum = matchNum, start = match.start(), end = match.end(), match = match.group()))

    for groupNum in range(0, len(match.groups())):
        groupNum = groupNum + 1

        print (""Group {groupNum} found at {start}-{end}: {group}"".format(groupNum = groupNum, start = match.start(groupNum), end = match.end(groupNum), group = match.group(groupNum)))
</code></pre>

<p>Note: for Python 2.7 compatibility, use <code>ur""""</code> to prefix the regex and <code>u""""</code> to prefix the test string and substitution.</p>
",0,0,2280,2017-05-04 06:11:42,https://stackoverflow.com/questions/43775138/removing-extra-space-between-the-word-and-apostrophe
word association - findAssocs and numeric (0),"<p>I'm just getting to grips with the <code>tm</code> package in R.</p>

<p>Probably a simple question, but trying to use the <code>findAssocs</code> function to get an idea for word associations in my customer enquiries insight document and I can't seem to get <code>findAssocs</code> to work correctly.</p>

<p>When I use the following:</p>

<pre><code>findAssocs(dtm, words, corlimit = 0.30)
 $population
  numeric(0)

 $migration
 numeric(0)
</code></pre>

<p>What does this mean? <code>Words</code> is a character vector of 667 words - surely there must be some correlative relationships?</p>
","r, text-mining, tm","<p>Consider the following example:</p>

<pre><code>library(tm)
corp &lt;- VCorpus(VectorSource(
          c(""hello world"", ""hello another World "", ""and hello yet another world"")))
tdm &lt;- TermDocumentMatrix(corp)
inspect(tdm)
#          Docs
# Terms     1 2 3
#   and     0 0 1
#   another 0 1 1
#   hello   1 1 1
#   world   1 1 1
#   yet     0 0 1
</code></pre>

<p>Now consider</p>

<pre><code>findAssocs(x=tdm, terms=c(""hello"", ""yet""), corlimit=.4)
# $hello
# numeric(0)
# 
# $yet
#     and another 
#     1.0     0.5 
</code></pre>

<p>From what I understand, <code>findAssocs</code> looks at the correlations of <code>hello</code> with everything but <code>hello</code> and <code>yet</code>, as well as <code>yet</code> with everything except <code>hello</code> and <code>yet</code>. <code>yet</code> and <code>and</code> have a correlation coefficient of <code>1.0</code>, which is above the lower limit of <code>0.4</code>. <code>yet</code> is also in 50% of all documents containing <code>another</code> - that's also above our 0.4 limit.  </p>

<p>Here's another example showcasing this:</p>

<pre><code>findAssocs(x=tdm, terms=c(""yet"", ""another""), corlimit=0)
# $yet
# and 
#   1 
# 
# $another
# and 
# 0.5 
</code></pre>

<p>Note that <code>hello</code> (and <code>world</code>) don't yield any results because they are in every document. This means the term frequency has zero variance and <code>cor</code> under the hood yields <code>NA</code> (like <code>cor(rep(1,3), 1:3)</code>, which gives <code>NA</code> plus a zero-standard-deviation-warning). </p>
",3,0,2204,2017-05-04 15:15:49,https://stackoverflow.com/questions/43786833/word-association-findassocs-and-numeric-0
R/python: build model from training sentences,"<p><strong>What I'm trying to achieve:</strong><br>
I have been looking for an approach for a long while now but I'm not able to find an (effective) way to this:</p>

<ul>
<li>build a model from example sentences while taking word order and synonyms into account. </li>
<li>map a sentence against this model and get a similarity score (thus a score indicating how much this sentence fits the model, in other words fits the sentences which were used to train the model)</li>
</ul>

<p><strong>What I tried</strong>:<br></p>

<ul>
<li><p><code>Python:</code> <code>nltk</code> in combination with <code>gensim</code> (as far as I could code and read it was only capable to use word similarity (but not taking order into 
account). </p></li>
<li><p><code>R:</code> used <code>tm</code> to build a <code>TermDocumentMatrix</code> which looked really promising but was not able to map anything to this matrix. Further this <code>TermDocumentMatrix</code> seems to take the order into account but misses the synonyms (I think). <br>
<a href=""https://i.sstatic.net/Yjeu4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Yjeu4.jpg"" alt=""enter image description here""></a><br>
<em>I know the lemmatization didn't go that well hahah :)</em></p></li>
</ul>

<p><strong>Question:</strong><br>
Is there any way to do achieve the steps described above using either R or Python? A simple sample code would be great (or references to a good tutorial)</p>
","python, r, nltk, text-mining, tm","<p>There are many ways to do what you described above, and it will of course take lots of testing to find an optimized solution.  But here is some helpful functionality to help solve this using python/nltk.</p>

<blockquote>
  <p>build a model from example sentences while taking word order and
  synonyms into account.</p>
</blockquote>

<p><strong>1.  Tokenization</strong></p>

<p>In this step you will want to break down individual sentences into a list of words.</p>

<p>Sample code:</p>

<pre><code>import nltk
tokenized_sentence = nltk.word_tokenize('this is my test sentence')

print(tokenized_sentence)

['this', 'is', 'my', 'test', 'sentence']  
</code></pre>

<p><strong>2.  Finding synonyms for each word.</strong>  </p>

<p>Sample code:</p>

<pre><code>from nltk.corpus import wordnet as wn
synset_list = wn.synsets('motorcar')

print(synset_list)

[Synset('car.n.01')]
</code></pre>

<p>Feel free to research synsets if you are unfamiliar, but for now just know the above returns a list, so multiple synsets are possibly returned.</p>

<p>From the synset you can get a list of synonyms.  </p>

<p>Sample code:</p>

<pre><code> print( wn.synset('car.n.01').lemma_names() )

 ['car', 'auto', 'automobile', 'machine', 'motorcar']
</code></pre>

<p>Great, now you are able to convert your sentence into a list of words, and you're able to find synonyms for all words in your sentences (while retaining the order of your sentence). Also, you may want to consider removing stopwords and stemming your tokens, so feel free to look up those concepts if you think it would be helpful.   </p>

<p>You will of course need to write the code to do this for all sentences, and store the data in some data structure, but that is probably outside the scope of this question.   </p>

<blockquote>
  <p>map a sentence against this model and get a similarity score (thus a
  score indicating how much this sentence fits the model, in other words
  fits the sentences which were used to train the model)</p>
</blockquote>

<p>This is difficult to answer since the possibilities to do this are endless, but here are a few examples of how you could approach it. </p>

<p>If you're interested in binary classification you could do something as simple as, Have I seen this sentence of variation of this sentence before (variation being same sentence but words replaced by their synonyms)?  If so, score is 1, else score is 0.  This would work, but may not be what you want. </p>

<p>Another example, store each sentence along with synonyms in a python dictionary and calculate score depending on how far down the dictionary you can align the new sentence.</p>

<p><strong>Example:</strong></p>

<p>training_sentence1 = 'This is my awesome sentence'</p>

<p>training_sentence2 = 'This is not awesome'</p>

<p>And here is a sample data structure on how you would store those 2 sentences:</p>

<pre><code>my_dictionary = {
    'this': {
        'is':{
            'my':{
                'awesome': {
                    'sentence':{}
                }
            },
            'not':{
                'awesome':{}
            }
        }
    }
}
</code></pre>

<p>Then you could write a function that traverses that data structure for each new sentence, and depending how deep it gets, give it a higher score.  </p>

<p><strong>Conclusion:</strong></p>

<p>The above two examples are just some possible ways to approach the similarity problem.  There are countless articles/whitepapers about computing semantic similarity between text, so my advice would be just explore many options.  </p>

<p>I purposely excluded supervised classification models, since you never mentioned having access to labelled training data, but of course that route is possible if you do have a gold standard data source.  </p>
",2,0,796,2017-05-05 12:24:14,https://stackoverflow.com/questions/43804884/r-python-build-model-from-training-sentences
Extract ngrams with R,"<p>I am trying to extract <code>3-grams</code> from nirvana text by using the <code>ngramrr</code> package.</p>

<pre><code>require(ngramrr)
require(tm)
require(magrittr)

nirvana &lt;- c(""hello hello hello how low"", ""hello hello hello how low"",
             ""hello hello hello how low"", ""hello hello hello"",
             ""with the lights out"", ""it's less dangerous"", ""here we are now"",
             ""entertain us"", ""i feel stupid"", ""and contagious"", ""here we are now"", 
             ""entertain us"", ""a mulatto"", ""an albino"", ""a mosquito"", ""my libido"",
             ""yeah"", ""hey yay"")

ngramrr(nirvana[1], ngmax = 3)

Corpus(VectorSource(nirvana))
</code></pre>

<p>I get this result : </p>

<pre><code>[1] ""hello""      ""hello""    ""hello""              ""how""  ""low""       ""hello hello""  ""hello hello""      
[8] ""hello how""  ""how low""  ""hello hello hello""  ""hello hello how""  ""hello how low""  
</code></pre>

<p>I would like to know what can I do to construct <code>TermDocumentMatrix</code> where terms are the <code>tri-grams</code> list.</p>

<p>Thank you</p>
","r, text-mining","<p>My comment above is almost complete but it is like this:</p>

<pre><code>nirvana %&gt;% tokens(ngrams = 1:3) %&gt;% # generate tokens
  dfm %&gt;% # generate dfm
  convert(to = ""tm"") %&gt;% # convert to tm's document-term-matrix
  t # transpose it to term-document-matrix
</code></pre>
",1,3,472,2017-05-05 14:24:49,https://stackoverflow.com/questions/43807448/extract-ngrams-with-r
get index of splited sentences from a string list,"<p>The desired result is either a function or a way to find where is a sentence within a list of strings.</p>

<pre><code>sentence = 'The cat went to the pool yesterday'

structure = ['The cat went,', 'to the pool yesterday.','I wonder if you realize the effect you are having on me. It hurts. A lot.']
</code></pre>

<p>for example</p>

<pre><code>def findsentence(sentence, list of strings):
      # do something to get the output, vec of positions to find the sentence in hte string list
return output

findsentence(sentence, structure)
&gt; (0,1) # beacuse the phrase is splitted in the list...
</code></pre>

<h1>Caution!!</h1>

<p>The challenge it is not to find exactly the sentence. Look at the example, this sentence is part of sentence position 0 and part in structure postition 1.</p>

<p>So this is not a simple, string manipulation problem.</p>
","python, string, text-mining","<p>Use the following :</p>

<pre><code>sentence = ""foo sam bar go""
structure = [""rq"", ""foo sam"", ""bar go"", ""ca"", ""da""]

def findsentencelist(sentence, list_of_strings):
    l = []
    for item in list_of_strings:
        if item in sentence:
            l.append(list_of_strings.index(item))
    return l

print str(findsentencelist(sentence, structure))
</code></pre>

<p>Hopefully this will help you, Yahli.</p>

<p>EDIT : </p>

<p>There is a problem with your variables. 
Your sentence MUST be a string - not a list.
Edit your variables and try this function again :)</p>

<p>SECOND EDIT:
I think I've finally understood what you're trying to do. Let me know if this one works better.</p>

<p>THIRD EDIT:
Jesus, Hopefully this one would solve your problem. Let me know if it did the trick :)</p>
",3,1,92,2017-05-09 11:08:14,https://stackoverflow.com/questions/43868203/get-index-of-splited-sentences-from-a-string-list
Alphabet conversion - Cyrillic to Latin,"<p>I have a list of names and surnames written on Cyrillic. </p>

<pre><code>head(text, n = 20)
   unique(clients$RODITEL)
1                     &lt;NA&gt;
2                    ЃОРЃИ
3               ALEKSANDAR
4             000000000000
5                  ТР4АЈЧЕ
6                        0
7                  HHHHHHH
8                  0000000
9                    TASKO
10    --------------------
11                   ДРАГИ
12                  СЛАВЧО
13                     ACO
14                  НИКОЛА
15                    САШО
16                  НАУМЧЕ
17                    ОРЦЕ
18                  САНДРА
19                  МИРСАД
20                   ОКТАЈ
</code></pre>

<p>What I need to do is to convert the names written on Cyrlic, such as the last 10 rows into Latin. </p>

<p>So the output would be: </p>

<pre><code>1                     &lt;NA&gt;
2                    GJORGJI
3               ALEKSANDAR
4             000000000000
5                  TRAJCHE
6                        0
7                  HHHHHHH
8                  0000000
9                    TASKO
10    --------------------
11                   DRAGI
12                  SLAVCHO
13                     ACO
14                  NIKOLA
15                    SASHO
16                  NAUMCHE
17                    ORCE
18                  SANDRA
19                  MIRSAD
20                   OKTAJ
</code></pre>

<p>The particular, Cyrlic alphabet is Macedonian. </p>

<p>I am not sure if there is any <code>R</code> package that deals with such conversion? </p>
","r, text-mining, stringr","<p>You can use functions from the package <code>stringi</code>, for example:</p>

<pre><code>&gt; stri_trans_general('ДРАГИ', 'latin')
[1] ""DRAGI""
</code></pre>
",3,1,1043,2017-05-09 12:28:26,https://stackoverflow.com/questions/43869869/alphabet-conversion-cyrillic-to-latin
Wordcloud showing colour based on continuous metadata in R,"<p>I'm creating a wordcloud in which the size of the words is based on frequency, but i want the colour of the words to be mapped to a third variable (stress, which is the amount of stress associated with each word, a numerical or continuous variable). </p>

<p>I tried the following, which gave me only two different colours (yellow and purple) while i want something more smooth. I would like some color range like a palette that goes from green to red for example.</p>

<pre><code>df = data.frame(word = c(""calling"", ""meeting"", ""conference"", ""contract"", ""negotiation"", ""email""),
n = c(20, 12, 4, 8, 10, 43),
stress = c(23, 30, 15, 40, 35, 15))
df = tbl_df(df) 
wordcloud(words = df$word, freq = df$n, col = df$stress)
</code></pre>

<p>Does anyone know how to deal with this continous metadata and get some smoothly changing colour for the words when stress goes up? Thanks!</p>
","r, colors, text-mining, word-cloud, color-palette","<p>Here is a potential solution. You want to use the <code>wordcloud2</code> package for your task. Then, you can solve your issue, I suppose. Since I do not know your real data, I created a sample data to demonstrate a prototype.</p>

<p>If you have many words, I am not sure if adding colors with a continuous variable (stress) is a good idea. One thing you could do is to create a new group variable using <code>cut()</code>. In this way, you can reduce the numbers of colors you would use in your graphics. Here, I created a new column called <code>color</code> with five colors from the viridis package.</p>

<p>When you use <code>wordcloud2()</code>, you have only two things to supply. One is data and the other is color. Font size reflects frequency of the words without specifying it.</p>

<pre><code>mydf = data.frame(word = c(""calling"", ""meeting"", ""conference"", ""contract"", ""negotiation"",
                           ""email"", ""friends"", ""chat"", ""text"", ""deal"",
                           ""business"", ""promotion"", ""discount"", ""users"", ""family""),
                  n = c(20, 12, 4, 8, 10, 43, 33, 5, 47, 28, 12, 9, 50, 31, 22),
                  stress = c(23, 30, 15, 40, 35, 15, 30, 18, 10, 5, 29, 38, 45, 8, 3))


          word  n stress
1      calling 20     23
2      meeting 12     30
3   conference  4     15
4     contract  8     40
5  negotiation 10     35
6        email 43     15
7      friends 33     30
8         chat  5     18
9         text 47     10
10        deal 28      5
11    business 12     29
12   promotion  9     38
13    discount 50     45
14       users 31      8
15      family 22      3

library(dplyr)
library(wordcloud2)
library(viridis)

mutate(mydf, color = cut(stress, breaks = c(0, 10, 20, 30, 40, Inf),
             labels = c(""#FDE725FF"", ""#73D055FF"", ""#1F968BFF"",
                        ""#2D708EFF"", ""#481567FF""),
             include.lowest = TRUE)) -&gt; temp

wordcloud2(data = temp, color = temp$color)
</code></pre>

<p><a href=""https://i.sstatic.net/eGmoN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eGmoN.png"" alt=""enter image description here""></a></p>
",3,3,3215,2017-05-10 13:48:18,https://stackoverflow.com/questions/43894416/wordcloud-showing-colour-based-on-continuous-metadata-in-r
Removing all punctuation marks in python,"<p>I need to remove all punctuation marks in the string, as a part of bigger program. 
It is working when I write it serepatly for each mark like this:</p>

<pre><code>words = [word.replace(""."", """") for word in words]
</code></pre>

<p>But when I am trying to do it in the loop, it is not working. </p>

<pre><code>line = ""I was going to leave her, but in the very last moment I had changed 
my mind. Interesting thing, many nice ways to use.""
words = line.lower().split()
for punc in [""."","",""]:
    if punc in words:
        words = [word.replace(punc, """") for word in words]
print words
</code></pre>

<p>Can you please tell me, what I am doing wrong?</p>
","python, python-2.7, text-mining, text-analysis","<p><code>translate</code> will work for you:</p>

<pre><code>&gt;&gt;line = '''I was going to leave her, but in the very last moment I had changed 
my mind. Interesting thing, many nice ways to use.'''
&gt;&gt;line = line.translate(None, ',.')
I was going to leave her but in the very last moment I had changed 
my mind Interesting thing many nice ways to use
</code></pre>
",4,2,2083,2017-05-12 07:45:44,https://stackoverflow.com/questions/43932182/removing-all-punctuation-marks-in-python
How to apply grepl for data frame,"<p>I want to use <code>grepl</code> for multiple patterns defined as a data frame. 
<code>df_sen</code> is presented as</p>

<pre><code>sentence
""She would like to go there""
""I had it few days ago""
""We have spent few millions""
</code></pre>

<p><code>df_triggers</code> is presented as follows:</p>

<pre><code>trigger
few days
few millions
</code></pre>

<p>And I want to create a matrix where sentence <code>x</code> triggers and on the intersection to see <code>1</code> if trigger was found in a sentence and <code>0</code> if it was not.</p>

<p>I have tried to do it like this:</p>

<pre><code>matrix &lt;- grepl(df_triggers$trigger, df_sen$sentence)
</code></pre>

<p>But I see the error message that I have more than 1 pattern in <code>grepl()</code>. </p>

<p>The desired output is:</p>

<pre><code>                                 few days    few millions
""She would like to go there""        0              0
""I had it few days ago""             1              0
""We have spent few millions         0              1
</code></pre>
","r, nlp, text-mining","<pre><code>sapply(df_triggers$trigger, grepl, df_sen$sentence)
</code></pre>

<p>from @docendodiscimus worked.</p>
",2,0,5077,2017-05-12 11:12:49,https://stackoverflow.com/questions/43936428/how-to-apply-grepl-for-data-frame
Tokenization and dtMatrix in python with nltk,"<p>I have a csv file with 2 columns — sentence and label.
I want to make a document-term matrix for these sentences. I am new in Python and so far I could reach this:</p>

<pre><code>import nltk
import csv
import numpy
from nltk import sent_tokenize, word_tokenize, pos_tag
reader = csv.reader(open('my_file.csv', 'rU'), delimiter= "";"",quotechar = '""')
for line in reader:
for field in line:
    tokens = word_tokenize(field)
</code></pre>

<p>But I don't get how to take only one column for tokenization and create such matrix. </p>

<p>I have read few topics on stackoverflow regarding the same issue but in all examples I could find, csv-file contained only 1 column or they hardcoded texts. </p>

<p>I would really appreciate any answer. Thank you in advance!</p>
","python, nlp, nltk, text-mining, text-analysis","<p>Suppose you have the file <code>example.csv</code> like the following:</p>

<pre><code>label;sentence
""class1"";""This is an example sentence.""
""class1"";""This is another example sentence.""
""class2"";""The third one is random.""
</code></pre>

<p><strong>Read the file</strong> using <code>DictReader</code> instead of <code>reader</code> so that it gives you each line as a dictionary</p>

<pre><code>import csv
reader = csv.DictReader(open('example.csv', 'r'), delimiter= "";"",quotechar = '""')
lines = list(reader) # this is a list each is dictionary
sentences = [l['sentence'] for l in lines] # get only 
</code></pre>

<p><strong>document-term matrix using scikit-learn</strong></p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer(lowercase=True) 
X_count = count_vect.fit_transform(sentences)
</code></pre>

<p>The dictionary (word to index dictionary) can be accessed using <code>count_vect.vocabulary_</code> and <code>X_count</code> is your document-term matrix, </p>

<pre><code>X_count.toarray()
# [[1 0 1 1 0 0 1 0 0 1]
#  [0 1 1 1 0 0 1 0 0 1]
#  [0 0 0 1 1 1 0 1 1 0]]
</code></pre>

<p><strong>document-term matrix using nltk</strong> this is kind of the same as scikit-learn but you can build the dictionary yourself and transform sentences to document-term matrix</p>

<pre><code>from nltk import word_tokenize
from itertools import chain, groupby
import scipy.sparse as sp

word_tokenize_matrix = [word_tokenize(sent) for sent in sentences]
vocab = set(chain.from_iterable(word_tokenize_matrix))
vocabulary = dict(zip(vocab, range(len(vocab)))) # dictionary of vocabulary to index

words_index = []
for r, words in enumerate(word_tokenize_matrix):
    for word in sorted(words):
        words_index.append((r, vocabulary.get(word), 1))
</code></pre>

<p>After getting row/column/value of each sentences, you can apply groupby and count words that appear more that one time.</p>

<pre><code>rows, cols, data = [], [], []
for gid, g in groupby(words_index, lambda x: (x[0], x[1])):
    rows.append(gid[0])
    cols.append(gid[1])
    data.append(len(list(g)))
X_count = sp.csr_matrix((data, (rows, cols)))
</code></pre>

<p>and here, you can build your own document-term matrix!</p>
",1,0,884,2017-05-14 09:27:36,https://stackoverflow.com/questions/43962344/tokenization-and-dtmatrix-in-python-with-nltk
Compute chi square value between ngrams and documents with Quanteda,"<p>I use Quanteda R package in order to extract ngrams (here 1grams and 2grams) from text Data_clean$Review, but I am looking for a way with R to compte Chi-square between document and the extracted ngrams :</p>

<p>Here the R code that I did to clean Up text (revoiew) and generate the n-grams.</p>

<p>Any idea please?</p>

<p>thank you </p>

<pre><code>#delete rows with empty value columns
Data_clean &lt;- Data[Data$Note!="""" &amp; Data$Review!="""",]


Data_clean$id &lt;- seq.int(nrow(Data_clean))

train.index &lt;- 1:50000
test.index &lt;- 50001:nrow(Data_clean)


#clean up
# remove grammar/punctuation
Data_clean$Review.clean &lt;- tolower(gsub('[[:punct:]0-9]', ' ', Data_clean$Review))


train &lt;- Data_clean[train.index, ]
test &lt;- Data_clean[test.index, ]

temp.tf &lt;- Data_clean$Raison.Reco.clean %&gt;% tokens(ngrams = 1:2) %&gt;% # generate tokens
      dfm  # generate dfm
</code></pre>
","r, text-mining, quanteda","<p>You would not use <code>ngrams</code> for this, but rather a function called <code>textstat_collocations()</code>.</p>

<p>It's a bit hard to follow your exact example since none of those objects are explained or supplied, but let's try it with some of <strong>quanteda</strong>'s built-in data.  I'll get the texts from the inaugural corpus and apply some filters similar to what you have above.</p>

<p>So to score bigrams for chi^2, you would use:</p>

<pre><code># create the corpus, subset on some conditions (could be Note != """" for instance)
corp_example &lt;- data_corpus_inaugural
corp_example &lt;- corpus_subset(corp_example, Year &gt; 1960)

# this will remove punctuation and numbers
toks_example &lt;- tokens(corp_example, remove_punct = TRUE, remove_numbers = TRUE)

# find and score chi^2 bigrams
coll2 &lt;- textstat_collocations(toks_example, method = ""chi2"", max_size = 2)
head(coll2, 10)
#             collocation count       X2
# 1       reverend clergy     2 28614.00
# 2       Majority Leader     2 28614.00
# 3       Information Age     2 28614.00
# 4      Founding Fathers     3 28614.00
# 5  distinguished guests     3 28614.00
# 6       Social Security     3 28614.00
# 7         Chief Justice     9 23409.82
# 8          middle class     4 22890.40
# 9       Abraham Lincoln     2 19075.33
# 10       society's ills     2 19075.33
</code></pre>

<p><strong>Added</strong>:</p>

<pre><code># needs to be a list of the collocations as separate character elements
coll2a &lt;- sapply(coll2$collocation, strsplit, "" "", USE.NAMES = FALSE)

# compound the tokens using top 100 collocations
toks_example_comp &lt;- tokens_compound(toks_example, coll2a[1:100])
toks_example_comp[[1]][1:20]
# [1] ""Vice_President""  ""Johnson""         ""Mr_Speaker""      ""Mr_Chief""        ""Chief_Justice""  
# [6] ""President""       ""Eisenhower""      ""Vice_President""  ""Nixon""           ""President""      
# [11] ""Truman""          ""reverend_clergy"" ""fellow_citizens"" ""we""              ""observe""        
# [16] ""today""           ""not""             ""a""               ""victory""         ""of""             
</code></pre>
",1,0,568,2017-05-17 16:21:13,https://stackoverflow.com/questions/44030170/compute-chi-square-value-between-ngrams-and-documents-with-quanteda
Gensim Word2Vec: poor training performance.,"<p>that might actually be a dumb question but I just can't figure out why my script with gensim.models.word2vec is not working. Here is the thing, I'm using the stanford sentiment analysis databank dataset (~11000 reviews), and i'm trying to build word2vec using gensim, this is my script: </p>

<pre><code>import gensim as gs 
import sys 

# open the datas
sentences = gs.models.word2vec.LineSentence('../processedWords.txt')
print(""size in RAM of the sentences: {}"".format(sys.getsizeof(sentences)))

# transform them
# bigram_transformer = gs.models.Phrases(sentences)

model = gs.models.word2vec.Word2Vec(sentences, min_count=10, size=100, window=5)
model.save('firstModel')
print(model.similarity('film', 'test'))
print(model.similarity('film', 'movie'))
</code></pre>

<p>Now, my problem is that the script runs in 2s, and gives only huge similarity between every pair of words. In addition, some words which are in the sentences are not in the built vocabulary. </p>

<p>I must be doing something obviously wrong, but can't figure what. </p>

<p>Thank you for your help. </p>
","python-3.x, dataset, text-mining, gensim, word2vec","<p>I'm almost certain that this is because you haven't specified a number of training iterations; I think <code>iter</code> defaults to 1, which is basically useless for training a neural net. Add the <code>iter=&lt;int&gt;</code> flag to your model declaration, e.g. <code>model = gs.models.word2vec.Word2Vec(sentences, min_count=10, size=100, window=5, iter=1000)</code>.
Kind of a face-palmer but I did the same exact thing.</p>
",3,3,1355,2017-05-20 20:26:35,https://stackoverflow.com/questions/44090503/gensim-word2vec-poor-training-performance
delete stop words in R,"<p>I Have a dataframe which has this structure:</p>
<pre><code>Note.Reco Review Review.clean.lower
10 Good Products  good products
9 Nice film      nice film
....         ....
</code></pre>
<p>The first column is the rank of the film, then the second column is the customer's review then the 3rd column is the review with lowercase letters.</p>
<p>I try now to delete stop words with this:</p>
<pre><code>Data_clean$Raison.Reco.clean1 &lt;- Corpus(VectorSource(Data_clean$Review.clean.lower))
Data_clean$Review.clean.lower1 &lt;- tm_map(Data_clean$Review.clean.lower1, removeWords, stopwords(&quot;english&quot;))
</code></pre>
<p>But R studio crashes</p>
<p>Can you help me to resolve this problem please?</p>
<p>Thank you</p>
<p><strong>EDIT:</strong></p>
<pre><code>#clean up
# remove grammar/punctuation
Data_clean$Review.clean.lower &lt;- tolower(gsub('[[:punct:]0-9]', ' ', Data_clean$Review))

Data_corpus &lt;- Corpus(VectorSource(Data_clean$Review.clean.lower))

Data_clean &lt;- tm_map(Data_corpus,  removeWords, stopwords(&quot;french&quot;))

train &lt;- Data_clean[train.index, ]
test &lt;- Data_clean[test.index, ]
</code></pre>
<p>So I get error when I run the 2 last instructions.</p>
","r, text-mining","<p>Try the below . You can do cleaning on the corpus and not column directly.  </p>

<pre><code>Data_corpus &lt;-
  Corpus(VectorSource(Data_clean$Review.clean.lower))

  Data_clean &lt;- tm_map(Data_corpus,  removeWords, stopwords(""english""))
</code></pre>

<p>EDIT:
As mentioned by you, you want to be able to access the output after removing stop words, try the below instead of the above:</p>

<pre><code>library(tm)

stopWords &lt;- stopwords(""en"")

Data_clean$Review.clean.lower&lt;- as.character(Data_clean$Review.clean.lower)
 '%nin%' &lt;- Negate('%in%')
 Data_clean$Review.clean.lower1&lt;-lapply(Data_clean$Review.clean.lower, function(x) {
  chk &lt;- unlist(strsplit(x,"" ""))
  p &lt;- chk[chk %nin% stopWords]
  paste(p,collapse = "" "")
})
</code></pre>

<p>Sample Output of above code:</p>

<pre><code>&gt;  print(Data_clean)
&gt;       note Note.Reco.Review Review.clean.lower Review.clean.lower1
&gt;     1   10    Good Products      good products       good products
&gt;     2    9        Nice film     is a nice film           nice film
</code></pre>

<p>Also check the below:
<a href=""https://stackoverflow.com/questions/15253798/r-remove-stopwords-from-a-character-vector-using-in"">R remove stopwords from a character vector using %in%</a></p>
",1,0,6542,2017-05-23 11:27:01,https://stackoverflow.com/questions/44133509/delete-stop-words-in-r
Classifying PDF Text Documents based on the presence/absence of specific words in R,"<p>I would like to be able to import PDF documents into R and classify them as either:</p>

<ul>
<li>Relevant (contains a specific string, for example, ""tacos"", <strong>within the first 100 words</strong>)</li>
<li>Irrelevant (<strong>DOES NOT</strong> contain ""tacos"" within the first 100 words)</li>
</ul>

<p>To be more specific, I would like to address the following questions:</p>

<ol>
<li>Does a package(s) exist in R to perform this basic classification?</li>
<li>If so, is it possible to generate a dataset that would look something like this in R if I had 2 PDF documents with Paper1 containing at least one instance of the string, ""tacos"", in the first 100 words and Paper2 that DOES NOT contain at least one instance of the string, ""tacos"":</li>
</ol>

<p><a href=""https://i.sstatic.net/Rn1Ch.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Rn1Ch.png"" alt=""enter image description here""></a></p>

<p>Any references to documentation/R packages/sample R code or mock examples related to this type of classification <strong>using R</strong> would be greatly appreciated! Thanks!</p>
","r, text-mining","<p>You can use the <a href=""https://cran.r-project.org/web/packages/pdftools/index.html"" rel=""nofollow noreferrer""><code>pdftools</code></a> library and do something like this:</p>

<p>First, load the library and grab some pdf file names:</p>

<pre><code>library(pdftools)
fns &lt;- list.files(""~/Documents"", pattern = ""\\.pdf$"", full = TRUE)
fns &lt;- sample(fns, 5) # sample of 5 pdf filenames... 
</code></pre>

<p>Then define a function that reads a PDF file in as text and looks up the first <code>n</code> words. (It might be useful to check for errros, like <em>unknown password</em> or things like that - my ex. function returns <code>NA</code> for such cases.)</p>

<pre><code>isRelevant &lt;- function(fn, needle, n = 100L, ...) {
  res &lt;- try({
    txt &lt;- pdf_text(fn)
    txt &lt;- scan(text = txt, what = ""character"", quote = """", quiet = TRUE) 
    any(grepl(needle, txt[1:n], ...))
  }, silent = TRUE)
  if (inherits(res, ""try-error"")) NA else res
}
res &lt;- sapply(fns, isRelevant, needle = ""mail"", ignore.case=TRUE)
</code></pre>

<p>Finally, wrap it up and put it into a data frame:</p>

<pre><code>data.frame(
  Document = basename(fns), 
  Classification = dplyr::if_else(res, ""relevant"", ""not relevant"", ""unknown"")
)
#   Document  Classification
# 1    a.pdf        relevant
# 2    b.pdf    not relevant
# 3    c.pdf        relevant
# 4    d.pdf    not relevant
# 5    e.pdf        relevant
</code></pre>
",4,0,916,2017-06-05 13:40:52,https://stackoverflow.com/questions/44370333/classifying-pdf-text-documents-based-on-the-presence-absence-of-specific-words-i
Retrieval of multiple author affiliations using RISmed from Medline object,"<p>In using the RISmed- R-package for automating data(abstract/author/affiliation etc.) retrieval from Medline, I can't retrieve multiple affiliations using the Affiliation() method. Only a first author's affiliation is retrieved  even through multiple are available. From the <a href=""https://www.nlm.nih.gov/bsd/mms/medlineelements.html#ad"" rel=""nofollow noreferrer"">https://www.nlm.nih.gov/bsd/mms/medlineelements.html#ad</a>
it appears that after Dec 2014 multiple affiliations are included in the affiliation field. Analogously the Author() method retrieves a list that contains multiple data frames accounting for all author data. Does anyone know if the Affiliation() method can do the same?</p>

<p>For example:
In retrieving affiliations for : <a href=""https://www.ncbi.nlm.nih.gov/pubmed/28578058"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/pubmed/28578058</a>
note from the link that there are 4 different affiliations. When executing the code below only first one is returned:</p>

<pre><code>      library(RISmed)
      RetrievePub &lt;- EUtilsGet(28578058)

         #Retrieve authorship
         AData &lt;- data.frame(Author(RetrievePub))
         Results_Authors = paste(paste(AData$ForeName, AData$LastName), 
          collapse = "" | "") #flatten data frame into string
         print(Results_Authors)

         #Retrieve affiliation
         Results_PubAffiliation = Affiliation(RetrievePub)
         print(Results_PubAffiliation)
</code></pre>
","r, text-mining, pubmed","<p>RISmed Version 2.1.6 only returned the Affiliation for the first author. The latest version (2.1.7) now provides the list of all affiliations in the same order as the author list. This is now available at <a href=""https://github.com/skoval/RISmed"" rel=""nofollow noreferrer"">https://github.com/skoval/RISmed</a> and can be installed with devtools. It should be available from CRAN in the next day or so.</p>
",1,0,728,2017-06-05 13:49:15,https://stackoverflow.com/questions/44370492/retrieval-of-multiple-author-affiliations-using-rismed-from-medline-object
probabilities returned by gensim&#39;s get_document_topics method doesn&#39;t add up to one,"<p>Sometimes it returns probabilities for all topics and all is fine, but  sometimes it returns probabilities for just a few topics and they don't add up to one, it seems it depends on the document. Generally when it returns few topics, the probabilities add up to more or less 80%, so is it returning just the most relevant topics? Is there a way to force it to return all probabilities?</p>

<p>Maybe I'm missing something but I can't find any documentation of the method's parameters.</p>
","text-mining, gensim, lda, topic-modeling","<p>I had the same problem and solved it by including the argument <code>minimum_probability=0</code> when calling the <code>get_document_topics</code> method of <code>gensim.models.ldamodel.LdaModel</code> objects. </p>

<pre class=""lang-py prettyprint-override""><code>    topic_assignments = lda.get_document_topics(corpus,minimum_probability=0)
</code></pre>

<p>By default, <strong>gensim doesn't output probabilities below 0.01</strong>, so for any document in particular, if there are any topics assigned probabilities under this threshold the sum of topic probabilities for that document will not add up to one.</p>

<p>Here's an example:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import common_texts
from gensim.corpora.dictionary import Dictionary
from gensim.models.ldamodel import LdaModel

# Create a corpus from a list of texts
common_dictionary = Dictionary(common_texts)
common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]

# Train the model on the corpus.
lda = LdaModel(common_corpus, num_topics=100)

# Try values of minimum_probability argument of None (default) and 0
for minimum_probability in (None, 0):
    # Get topic probabilites for each document
    topic_assignments = lda.get_document_topics(common_corpus,minimum_probability=minimum_probability)
    probabilities = [ [entry[1] for entry in doc] for doc in topic_assignments ]
    # Print output
    print(f""Calculating topic probabilities with minimum_probability argument = {str(minimum_probability)}"")
    print(f""Sum of probabilites:"")
    for i, P in enumerate(probabilities):
        sum_P = sum(P)
        print(f""\tdoc {i} = {sum_P}"")
</code></pre>

<p>And the output would be:</p>

<pre><code>Calculating topic probabilities with minimum_probability argument = None
Sum of probabilities:
    doc 0 = 0.6733324527740479
    doc 1 = 0.8585712909698486
    doc 2 = 0.7549994885921478
    doc 3 = 0.8019999265670776
    doc 4 = 0.7524996995925903
    doc 5 = 0
    doc 6 = 0
    doc 7 = 0
    doc 8 = 0.5049992203712463
Calculating topic probabilities with minimum_probability argument = 0
Sum of probabilites:
    doc 0 = 1.0000000400468707
    doc 1 = 1.0000000337604433
    doc 2 = 1.0000000079162419
    doc 3 = 1.0000000284053385
    doc 4 = 0.9999999937135726
    doc 5 = 0.9999999776482582
    doc 6 = 0.9999999776482582
    doc 7 = 0.9999999776482582
    doc 8 = 0.9999999930150807
</code></pre>

<p>This default behaviour is not very clearly stated in the documentation. The default value for <code>minimum_probability</code> for the <code>get_document_topics</code> method is <code>None</code>, however this does not set the probability to zero. Instead the value of <code>minimum_probability</code> is set to the value of <code>minimum_probability</code> of the <code>gensim.models.ldamodel.LdaModel</code> object, which by default is 0.01 as you can see in the <a href=""https://github.com/RaRe-Technologies/gensim/blob/996801bb3fb8c4e10a84eefa70f5e2ac738dd47b/gensim/models/ldamodel.py#L347"" rel=""nofollow noreferrer"">source code</a>:</p>

<pre class=""lang-py prettyprint-override""><code>def __init__(self, corpus=None, num_topics=100, id2word=None,
             distributed=False, chunksize=2000, passes=1, update_every=1,
             alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10,
             iterations=50, gamma_threshold=0.001, minimum_probability=0.01,
             random_state=None, ns_conf=None, minimum_phi_value=0.01,
             per_word_topics=False, callbacks=None, dtype=np.float32):
</code></pre>
",5,5,2549,2017-06-15 15:36:38,https://stackoverflow.com/questions/44571617/probabilities-returned-by-gensims-get-document-topics-method-doesnt-add-up-to
create heat map from PCA coordinates in R,"<p>I'd like to create a heat map on one variable against itself. However, I don't have it in matrix format. I have the PCA1 and PCA2 coordinates of each item and I'd like to know how I can create a heat map out of this. This is what my data looks like (where cluster is a k-means cluster classification)</p>

<pre><code>ID                     PCA1             PCA2          cluster
echocardiography       -0.88            0.87          9
infarction             -0.18            0.57          7
carotid                1.13             -0.80         2
aorta                  -0.03            -0.06         5
myocardial             -0.72            -0.02         3
hemorrhage             0.23             -0.67         5
</code></pre>

<p>so basically I want a heat map between the IDs that shows (by possibly using PCA coordinate distance) how correlated each ID is.</p>

<p>note: the heat map should look something like this (vs a density heat plot):
<a href=""https://i.sstatic.net/6ZviK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6ZviK.jpg"" alt=""enter image description here""></a></p>
","r, matrix, heatmap, text-mining, pca","<p>Here is a possibile solution. Hope it can help you.</p>

<pre><code>df &lt;- structure(list(ID = structure(c(3L, 5L, 2L, 1L, 6L, 4L), .Label = c(""aorta"", 
""carotid"", ""echocardiography"", ""hemorrhage"", ""infarction"", ""myocardial""
), class = ""factor""), PCA1 = c(-0.88, -0.18, 1.13, -0.03, -0.72, 
0.23), PCA2 = c(0.87, 0.57, -0.8, -0.06, -0.02, -0.67), cluster = c(9L, 
7L, 2L, 5L, 3L, 5L)), .Names = c(""ID"", ""PCA1"", ""PCA2"", ""cluster""
), class = ""data.frame"", row.names = c(NA, -6L))

# Define a distance function based on euclidean norm
# calculated between PCA values of the i-th and j-th items
dst &lt;- Vectorize(function(i,j,dtset) sqrt(sum((dtset[i,2:3]-dtset[j,2:3])^2)), vectorize.args=c(""i"",""j""))

# Here is the distance between echocardiography and infarction
dst(1,2,df)
# [1] 0.7615773
# This value is given by
sqrt(sum((df[1,2:3] - df[2,2:3])^2))

# Calculate the distance matrix
nr &lt;- nrow(df)
mtx &lt;- outer(1:nr, 1:nr, ""dst"", dtset=df)
colnames(mtx) &lt;- rownames(mtx) &lt;- df[,1]

# Plot the heatmap using ggplot2
library(reshape2)
library(ggplot2)
mtx.long &lt;- melt(mtx)
ggplot(mtx.long, aes(x = Var1, y = Var2, fill = value)) + geom_tile()+xlab("""")+ylab("""")
</code></pre>

<p><a href=""https://i.sstatic.net/ien74.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ien74.png"" alt=""enter image description here""></a></p>
",1,0,1512,2017-06-24 00:01:12,https://stackoverflow.com/questions/44731442/create-heat-map-from-pca-coordinates-in-r
How to reduce text dimensions in RapidMiner,"<p>I am having a challenge with using Rapid Miner to reduce the feature dimensions for text mining. at this point i am processing the text by word tokens and it is resulting in a very big dimension set that is not ideal for modeling and prediction. 
how can i improve the process to use other methods to clean the data and only take on relevant words? </p>

<p>i have tried applying tfidf but it removes the target variable and i am not able to see what it does before the model stage. </p>

<p>Thanks</p>
","data-mining, text-mining, rapidminer","<p>The <code>Process Documents</code> operator has a pruning option where, with some careful setting of parameters, you can remove common and rare attributes. </p>

<p>Here's a toy example to show it working.</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;&lt;process version=""7.5.000""&gt;
  &lt;context&gt;
    &lt;input/&gt;
    &lt;output/&gt;
    &lt;macros/&gt;
  &lt;/context&gt;
  &lt;operator activated=""true"" class=""process"" compatibility=""7.5.000"" expanded=""true"" name=""Process""&gt;
    &lt;process expanded=""true""&gt;
      &lt;operator activated=""true"" class=""text:create_document"" compatibility=""7.4.001"" expanded=""true"" height=""68"" name=""Create Document"" width=""90"" x=""179"" y=""187""&gt;
        &lt;parameter key=""text"" value=""the cat sat on the mat&amp;#10;the dog barked at the man&amp;#10;the cow ate the grass&amp;#10;the man sat on the grass""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text:create_document"" compatibility=""7.4.001"" expanded=""true"" height=""68"" name=""Create Document (2)"" width=""90"" x=""179"" y=""289""&gt;
        &lt;parameter key=""text"" value=""the cat sat on the mat&amp;#10;the man sat on the grass&amp;#10;the rain in spain falls mainly on the plain""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text:create_document"" compatibility=""7.4.001"" expanded=""true"" height=""68"" name=""Create Document (3)"" width=""90"" x=""179"" y=""391""&gt;
        &lt;parameter key=""text"" value=""the world is round""/&gt;
      &lt;/operator&gt;
      &lt;operator activated=""true"" class=""text:process_documents"" compatibility=""7.4.001"" expanded=""true"" height=""145"" name=""Process Documents"" width=""90"" x=""447"" y=""187""&gt;
        &lt;parameter key=""vector_creation"" value=""Term Occurrences""/&gt;
        &lt;parameter key=""prune_method"" value=""absolute""/&gt;
        &lt;parameter key=""prune_above_percent"" value=""40.0""/&gt;
        &lt;parameter key=""prune_below_absolute"" value=""2""/&gt;
        &lt;parameter key=""prune_above_absolute"" value=""5""/&gt;
        &lt;process expanded=""true""&gt;
          &lt;operator activated=""true"" class=""text:tokenize"" compatibility=""7.4.001"" expanded=""true"" height=""68"" name=""Tokenize"" width=""90"" x=""246"" y=""34""/&gt;
          &lt;connect from_port=""document"" to_op=""Tokenize"" to_port=""document""/&gt;
          &lt;connect from_op=""Tokenize"" from_port=""document"" to_port=""document 1""/&gt;
          &lt;portSpacing port=""source_document"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 1"" spacing=""0""/&gt;
          &lt;portSpacing port=""sink_document 2"" spacing=""0""/&gt;
        &lt;/process&gt;
      &lt;/operator&gt;
      &lt;connect from_op=""Create Document"" from_port=""output"" to_op=""Process Documents"" to_port=""documents 1""/&gt;
      &lt;connect from_op=""Create Document (2)"" from_port=""output"" to_op=""Process Documents"" to_port=""documents 2""/&gt;
      &lt;connect from_op=""Create Document (3)"" from_port=""output"" to_op=""Process Documents"" to_port=""documents 3""/&gt;
      &lt;connect from_op=""Process Documents"" from_port=""example set"" to_port=""result 1""/&gt;
      &lt;portSpacing port=""source_input 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 1"" spacing=""0""/&gt;
      &lt;portSpacing port=""sink_result 2"" spacing=""0""/&gt;
    &lt;/process&gt;
  &lt;/operator&gt;
&lt;/process&gt;
</code></pre>

<p>It requires some care to get it just right but hopefully this will get you started.</p>
",0,0,308,2017-06-26 19:35:21,https://stackoverflow.com/questions/44767398/how-to-reduce-text-dimensions-in-rapidminer
Determining canonical classes with text data,"<p>I have a unique problem and I'm not aware of any algorithm that can help me. Maybe someone on here does.</p>

<p>I have a dataset compiled from many different sources (teams). One field in particular is called ""type"". Here are some example values for type:</p>

<blockquote>
  <p>aple, apples, appls, ornge, fruits, orange, orange z, pear,
  cauliflower, colifower, brocli, brocoli, leeks, veg, vegetables.</p>
</blockquote>

<p>What I would like to be able to do is to group them together into e.g. fruits, vegetables, etc.</p>

<p>Put another way I have multiple spellings of various permutations of a parent level variable (fruits or vegetables in this example) and I need to be able to group them as best I can.</p>

<p>The only other potentially relevant feature of the data is the team that entered it, assuming some consistency in the way each team enters their data.</p>

<p>So, I have several million records of multiple spellings and short spellings (e.g. apple, appls) and I want to group them together in some way. In this example by fruits and vegetables.</p>

<p>Clustering would be challenging since each entry is most often 1 or two words, making it tricky to calculate a distance between terms.</p>

<p>Short of creating a massive lookup table created by a human (not likely with millions of rows), is there any approach I can take with this problem?</p>
","cluster-analysis, text-mining, word2vec","<p>You will need to first solve the spelling problem, unless you have Google scale data that could allow you to learn fixing spelling with Google scale statistics.</p>

<p>Then you will still have the problem that ""Apple"" could be a fruit or a computer. Apple and ""Granny Smith"" will be completely different. You best guess at this second stage is something like word2vec trained on <em>massive</em> data. Then you get high dimensional word vectors, and can finally try to solve the clustering challenge, if you ever get that far with decent results. Good luck.</p>
",1,-1,28,2017-06-27 06:33:47,https://stackoverflow.com/questions/45300866/determining-canonical-classes-with-text-data
a list of multiple lists of 2 for synonyms,"<p>I want to read the synonyms from a csv file , where the first word is the ""main"" word and the rest of the words in the same record are its synonyms
<a href=""https://i.sstatic.net/7xqML.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7xqML.png"" alt=""enter image description here""></a></p>

<p>now i basically want to create a list like i would have in R , </p>

<pre><code>**synonyms &lt;- list(
  list(word=""ss"", syns=c(""yy"",""yyss"")),
  list(word=""ser"", syns=c(""sert"",""sertyy"",""serty""))
)**
</code></pre>

<p>This gives me a list as </p>

<pre><code>synonyms
[[1]]
[[1]]$word
[1] ""ss""

[[1]]$syns
[1] ""yy""   ""yyss""


[[2]]
[[2]]$word
[1] ""ser""

[[2]]$syns
[1] ""sert""   ""sertyy"" ""serty""
</code></pre>

<p>which is essentially a list of lists of ""word"" and ""syns"". 
how do i go about creating the similar list while reading the word and synonyms from a csv file</p>

<p>any pointers would help !! Thanks </p>
","r, text-mining, tm","<p>This process should return what you want.</p>

<pre><code># read in data using readLines
myStuff &lt;- readLines(textConnection(temp))
</code></pre>

<p>This will return a character vector with one element per line in the file. Note that <code>textConnection</code> is not necessary for reading in files. Just supply the file path. Now, split each vector element into a vectors using <code>strsplit</code> and return a list.</p>

<pre><code>myList &lt;- strsplit(myStuff, split="" "")
</code></pre>

<p>Now, separate the first element from the remaining element for each vector within the list.</p>

<pre><code>result &lt;- lapply(myList, function(x) list(word=x[1], synonyms=x[-1]))
</code></pre>

<p>This returns the desired result. We use <code>lapply</code> to move through the list items. For each list item, we return a named list where the first element, named word, corresponds to the first element of the vector that is the list item and the remaining elements of this vector are placed in a second list element called synonyms.</p>

<pre><code>result
[[1]]
[[1]]$word
[1] ""ss""

[[1]]$synonyms
[1] ""yy""   ""yyss""


[[2]]
[[2]]$word
[1] ""ser""

[[2]]$synonyms
[1] ""sert""   ""sertyy"" ""serty"" 


[[3]]
[[3]]$word
[1] ""at""

[[3]]$synonyms
[1] ""ate""  ""ater"" ""ates""


[[4]]
[[4]]$word
[1] ""late""

[[4]]$synonyms
[1] ""lated"" ""lates"" ""latee""
</code></pre>

<p><strong>data</strong></p>

<pre><code>temp &lt;- 
""ss yy yyss
ser sert sertyy serty
at ate ater ates
late lated lates latee""
</code></pre>
",1,0,160,2017-06-28 14:05:57,https://stackoverflow.com/questions/44804828/a-list-of-multiple-lists-of-2-for-synonyms
How do I remove non-English words from a file?,"<p>I am trying to process a file with 2 columns of text and categories. From the text column, I need to remove non-English words. I am new to Python so would appreciate if there are any suggestions on how to do this. My file has 60,000 rows of instances.</p>

<p>And I can get to this point below but need help on how to move forward</p>
","python, python-3.x, pandas, text-mining","<p>This code should do the trick.</p>

<pre><code>import pandas
import requests
import string

# The following link contains a text file with the 20,000
# most frequent words in english, one in each line.
DICTIONARY_URL = 'https://raw.githubusercontent.com/first20hours/' \
                 'google-10000-english/master/20k.txt'
PATH = r""C:\path\to\file.csv""
FILTER_COLUMN_NAME = 'username'
PRINTABLES_SET = set(string.printable)

def is_english_printable(word):
    return PRINTABLES_SET &gt;= set(word)

def prepare_dictionary(url):
    return set(requests.get(url).text.splitlines())

DICTIONARY = prepare_dictionary(DICTIONARY_URL)
df = pandas.read_csv(PATH, encoding='ISO-8859-1')
df = df[df[FILTER_COLUMN_NAME].map(is_english_printable) &amp;
        df[FILTER_COLUMN_NAME].map(str.lower).isin(DICTIONARY)]
</code></pre>
",0,0,5936,2017-07-01 12:48:22,https://stackoverflow.com/questions/44861120/how-do-i-remove-non-english-words-from-a-file
"Word Embedding, LookupTable, Word Embedding Visualizations","<p>I need to ask few questions regarding word embeddings.....could be basic.</p>
<ol>
<li>When we convert a one-hot vector of a word for instance king <code>[0 0 0 1 0]</code> into an embedded vector <code>E = [0.2, 0.4, 0.2, 0.2]</code>.... is there any importance for each index in resultant word vector? For instance <code>E[1]</code> which is 0.2.... what specifically <code>E[1]</code> defines (although I know its basically a transformation into another space).... or word vector collectively defines context but not individually...</li>
<li>How the dimension (reduced or increased) of a word vector matters as compared to the original one-hot vector?</li>
<li>How can we define a lookup table in terms of an embedding layer?</li>
<li>is lookup table a kind of random generated table or it already been trained separately with respect to data instance in data and we just use it later on in Neural Network operations?</li>
<li>Is there any method to visualize an embedded vector at Hidden Layer (as we do have in Image based Neural Network Processing)?</li>
</ol>
<p>Thanks in advance</p>
","deep-learning, text-mining, word2vec, word-embedding","<p>1: Each element (or a group of element) in embedding vector have some meaning, but mostly unknown for human. Depend on what algorithm you use, a word embedding vector may have different meaning, but usually useful. 
For example, <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">Glove</a>, similar word 'frog', 'toad' stay near each other in vector space. King - man result in vector similar to Queen. </p>

<ol start=""3"">
<li><p>Turn vocab into index. For example, you have a vocabulary list:
[dog, cat, mouse, feed, play, with]
Then the sentences: Dog play with cat => 0, 4, 5, 1
While, you have embedding matrix as follow</p>

<p>[0.1, 0.1, 0]  # comment: this is dog <br>
[0.2, 0.5, 0.1] # this is cat <br>
[...] <br>
[...] <br>
[...] <br>
[...] <br></p></li>
</ol>

<p>where first row is embedding vector of dog, second row is cat, then so on
Then, you use the index (0, 4, 5, 1) after lookup would become a matrix [[0.1, 0.1, 0][...][...][0.2, 0.5, 0.1]]</p>

<ol start=""4"">
<li>either or both

<ul>
<li>You can randomly init embedding vector and training it with gradient descent</li>
<li>You can take pretrained word vector and keep it fixed (i.e: read-only, no change). 
You can train your word vector in model and use it in another model. Our you can download pretrained word vector online. Example Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip on <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">Glove</a></li>
<li>You can init with pretrained word vector and train with your model by  gradient descent</li>
</ul></li>
</ol>

<p>Update:
<strong>One-hot vector</strong> does not contain any information. You can think that one-hot vector is index of that vector in vocabulary. 
For example, Dog =>  [1, 0, 0, 0, 0, 0] and cat =>  [0, 1, 0, 0, 0, 0]. There are some different between one-hot vs index: </p>

<ul>
<li><p>if you input a list of index: [0, 4, 5, 1] to your multi-layer perceptron, it cannot learn anything (I tried...).But if you input a matrix of one-hot vector [[...1][1...][...][...]], it learn something. But it costly in term of RAM and CPU. </p></li>
<li><p>One-hot cost a lot of memory to store zeros. Thus, I suggest randomly init embedding matrix if you don't have one. Store dataset as index, and use index to look up embedding vector</p></li>
</ul>

<blockquote>
  <p>""its mean that lookup table is just a matrix of embedded vectors
  (already been trained seperately via word2vec or...) for each word in
  the vocabulary. and while in the process of neural network either we
  can use an Embedding Layer or we can just refer to embedded vector in
  lookup table for that particular embedded vector against particular
  one-hot vector.""</p>
</blockquote>

<p>Use the ""INDEX"" to look-up in lookup table. Turn dog into 0, cat into 1. One-hot vector and index contain same information, but one-hot cost more memory to store. Moreover, a lot of deeplearning framework accept index as input to embedding layer (which, output is a vector represent for a word in that index.)</p>

<blockquote>
  <p>"". How we get this embedding vector...""</p>
</blockquote>

<p>=> read paper. Here is paper about <a href=""https://arxiv.org/abs/1301.3781"" rel=""noreferrer"">Word2vec</a> and <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">Glove</a>. Ask your lecturers for more detail, they are willing to help you.  </p>
",7,7,9803,2017-07-03 09:24:22,https://stackoverflow.com/questions/44881999/word-embedding-lookuptable-word-embedding-visualizations
checking if word exist in english dictionary r,"<p>I'm performing some text analysis on  mutliple <strong>resume</strong> to generate a <code>wordcloud</code> using <code>wordcloud</code> package along with <code>tm</code> package for preprocessing the corpus of document in R.</p>

<p>The problems i'm facing are :</p>

<ol>
<li><p>Checking whether the word in corpus have some meaning ie. it belongs to english dictionary.</p></li>
<li><p>How to mine / process multiple resumes together.</p></li>
<li><p>Checking for <strong>tech</strong> terms like r,java,eclipse etc. </p></li>
</ol>

<p>Appreciate the help.</p>
","r, shiny, text-mining","<p>I've faced some issues before, so sharing solutions to your problems :</p>

<p><strong>1.</strong> There is a package <code>qdapDictionaries</code> which is a collection of dictionaries and word lists for use with the 'qdap' package.</p>

<pre><code>library(qdapDictionaries)

#create custom function
is.word  &lt;- function(x) x %in% GradyAugmented # or use any dataset from package

#use this function to filter words, df = dataframe from corpus
df &lt;- df[which(is.word(df$terms)),]
</code></pre>

<p><strong>2.</strong> Using <code>VCorpus(DirSource(...))</code> to create your corpus from directory containing all resumes</p>

<pre><code>resumeDir &lt;- ""path/all_resumes/""
myCorpus &lt;- VCorpus(DirSource(resumeDir))
</code></pre>

<p><strong>3.</strong> Create your custom dictionary file like <strong>my_dict.csv</strong> containing <code>tech</code> terms.</p>

<pre><code>#read custom dictionary
tech_dict &lt;- read.csv(""path/to/my_dict.csv"", stringsAsFactors = FALSE)
#create tech function
is.tech &lt;- function(x) x %in% tech_dict
#filter
tech_df &lt;- df[which(is.tech(df$terms)),]
</code></pre>

<p>Hope this helps.</p>
",8,9,4683,2017-07-07 05:43:22,https://stackoverflow.com/questions/44963305/checking-if-word-exist-in-english-dictionary-r
Facebook comments analyzing,"<p>Hi I am trying to analyse Facebook comments using r so when I am browsing through the codes I came across one code can anyone explain me clearly what is it </p>

<p>I got the comments after that to clean the comments the code was written as below </p>

<pre><code>sapply(comments, function(x) iconv(enc2utf8(x),sub=""byte""))
</code></pre>

<p>Can anyone explain me what is function doing ?</p>
","r, text-mining","<p><code>sapply</code> is designed to recursively apply the function in 2nd position to the element in first position. </p>

<p>This is a shortcut of the apply function. To know more about the apply family, check <a href=""https://stat.ethz.ch/R-manual/R-devel/library/base/html/apply.html"" rel=""nofollow noreferrer"">https://stat.ethz.ch/R-manual/R-devel/library/base/html/apply.html</a> </p>

<p><code>comment</code> is the element on which the function is applied. </p>

<p><code>function(x) iconv(enc2utf8(x),sub=""byte"")</code> is a function that converts characters between encoding. </p>

<p><code>iconv(enc2utf8(x),sub=""byte"")</code> does the encoding converting. </p>

<p><code>enc2utf8(x)</code> sets the encoding. </p>

<p>In this case, this is an anonymous function, meaning that you call it ""on the fly"", without having to name it. </p>

<p>You could also have written this code : </p>

<pre><code>a_function &lt;- function(x) {
    iconv(enc2utf8(x),sub=""byte"")
}

sapply(comments, a_function)
</code></pre>

<p>Best, </p>

<p>Colin</p>
",0,-1,68,2017-07-07 06:55:36,https://stackoverflow.com/questions/44964368/facebook-comments-analyzing
R Text Mining - Converting Term Document Matrix,"<p>I created a list of bigrams using:</p>

<pre><code>BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm_a.bigram = TermDocumentMatrix(docs_a,
                                control = list(tokenize = BigramTokenizer))
</code></pre>

<p>I am trying to get a count of documents each bigram is appearing in. If I understand correctly Term Document Matrix will give how many times each bigram occurs within a document. But I just need 
'1'-present in a document and 
'0'-not there.</p>

<p>How do I convert Term Document Matrix into dataframe or matrix to be able to get such count?</p>
","r, text-mining, tm, term-document-matrix, rweka","<p>A TDM is a simple_triplet_matrix from the <code>slam</code> package. Which has some fucntions for common operations line row/colSums.</p>

<p><code>slam::row_sums(tdm_a.bigram&gt;=1)</code></p>

<p>This should tell you how many documents contained each bigram.</p>
",0,0,131,2017-07-07 15:23:57,https://stackoverflow.com/questions/44974479/r-text-mining-converting-term-document-matrix
LDA Topic Model Issue,"<p>I doing some Text Mining exercises with twitter data. The original dataframe has 1280 rows. In order to avoid:</p>

<blockquote>
  <p>Error in LDA(dtm_cea, k = 8) : 
    Each row of the input matrix needs to contain at least one non-zero entry</p>
</blockquote>

<p>I eliminate in the sparse matrix any row without entries:</p>

<pre><code>rowTotals &lt;- apply(dtm , 1, sum) 
dtm.new   &lt;- dtm[rowTotals_cea&gt; 0, ] 
lda &lt;- LDA(dtm.new, k = 8)
topic &lt;- topics(lda, 1)
</code></pre>

<p>Consequently my dtm.new lost a few rows; in fact the number of rows decreases to 1273. </p>

<p>The fact is that now I need to retrieve another column from the original dataframe (1280 rows) and rbind with topic (1273 rows), to make a chart. How can I identify, in the original data, which lines should be eliminated, due to the change made to the DTM?</p>
","r, text, text-mining","<p>You need to keep track of which ones you're removing, or reconstruct the index used to remove them.</p>

<pre><code>rowstokeep &lt;- rowTotals &gt; 0
newdataframe &lt;- originaldataframe[rowstokeep, ]
</code></pre>
",0,0,160,2017-07-11 19:44:19,https://stackoverflow.com/questions/45043296/lda-topic-model-issue
Range for ngram in R,"<p>Using a command like the following in r it is possible to give the level of ngram you expect to executed:</p>

<pre><code>myDfm &lt;- dfm(txt, ngrams = 2, ignoredFeatures = stopwords(""english""))
</code></pre>

<p>Is it possible to give a range for ngram like from (1,3)?</p>

<p>Example code in python:</p>

<pre><code>vectorizer = TfidVectorizer(stop_words=stop_words, use_idf=True, ngram=range(1, 3))
</code></pre>
","r, text-mining","<pre><code>myDfm &lt;- dfm(txt, ngrams = c(1,3), ignoredFeatures = stopwords(""english""))
</code></pre>
",1,2,125,2017-07-18 12:58:05,https://stackoverflow.com/questions/45167507/range-for-ngram-in-r
Number of features in text mining,"<p>I am trying to make a predictive model based on text mining. I am confused how many features should I set up in my model. I have 1000 document in my analysis (so corpus will take around 700). Number of terms in corpus is around 20 000, so it exceeds number of documents (P >> N). Having so much features has any sense?</p>

<p>Number of features in HashingTF method should be higher than total numbers of terms in the corpus? Or should I make it smaller (like 512 features?)</p>

<p>I am a little bit confused.</p>
","machine-learning, text-mining, text-classification","<p>Assuming you are talking about using just unigrams as features, you are right that we want p &lt; n. (Not citing sources here since you seem to know what this means.)</p>

<p>Finally, to achieve p &lt; n, you could either </p>

<ol>
<li><p>select features with count>=k. Measure performance for various k and select the best k, or-</p></li>
<li><p>use all features but with L1 regularization. </p></li>
</ol>

<p>If you use hashing like you mentioned, you should set number of features less than even 512 because -</p>

<ol>
<li>n=700 and p=512 is still too skewed. </li>
<li>Typically, there are a very small number of important words. It might even be less than 50 in your case. You could try number of hash buckets = {10, 20, 50, 100, 500, 1000} and pick the best one.</li>
</ol>

<p>Good luck! </p>
",0,0,477,2017-07-19 13:24:18,https://stackoverflow.com/questions/45191993/number-of-features-in-text-mining
I see &#39;fffd&#39; in my wordcloud created using tweets of celebrities,"<p>I was trying to understand what topics do some of the celebritites about. I established a twitter API connection and got tweets of a few personalities from their verified handles.</p>

<p>I processed the tweets by following -</p>

<ol>
<li>Replaced graphic characters by blank<br>
<code>AmitText=str_replace_all(tweets.df$text,""[^[:graph:]]"", "" "")</code></li>
<li>Converted all characters to lower case</li>
<li>Removed punctuations, hyperlinks, tabs, Keyword ""rt"" and blankspaces at the begining and end of tweets</li>
<li>Created corpus, removed stopwords and created a wordcloud<br>
<code>AmitText.corpus &lt;- Corpus(VectorSource(AmitText))</code><br>
<code>AmitText.corpus &lt;- tm_map(AmitText.corpus, removeWords, stopwords(""en""))</code><br>
<code>wordcloud(AmitText.corpus,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, ""Dark2""),random.color= FALSE, random.order = FALSE, max.words = 150)</code></li>
</ol>

<p>This creates a decent wordcloud, but the problem is, I get a big 'fffd' in the middle of the wordcloud, suggesting that this is the word tweeted by the celeb the most. In fact, this is the pattern I see for all the 7 celebrities. Although I was sure this cannot be the case, I checked their raw tweets too, and found no such word as fffd in their tweets. From what I understand, this is some graphic character that isn't getting read correctly. I am not sure what is the reason and google isn't of much help</p>
","r, character-encoding, text-mining","<p>Let's try this in the beginning of your data pre-processing.</p>

<pre><code>iconv(tweet$text, from=""UTF-8"", to=""ASCII"", sub="""")
</code></pre>

<p>Hope this helps!</p>

<p><br>
<em>Don't forget to let us know if it solved your problem :)</em></p>
",1,0,70,2017-07-22 09:27:57,https://stackoverflow.com/questions/45252910/i-see-fffd-in-my-wordcloud-created-using-tweets-of-celebrities
Remove chars from string using Regular Expression,"<p>Given an array of strings which contains alphanumeric characters but also punctuations that have to be deleted. For instance the string x=""0-001"" is converted into x=""0001"".   </p>

<p>For this purpose I have:</p>

<pre><code>punctuations = list(string.punctuation)
</code></pre>

<p>Which contain all the characters that have to be removed from the strings. I'm trying to solve this using regular expressions in python, any suggestion on how to proceed using regular expressions? </p>

<pre><code>import string
punctuations = list(string.punctuation)
test = ""0000.1111""
for i, char in enumerate(test): 
    if char in punctuations:
       test = test[:i] + test[i+ 1:]
</code></pre>
","python, text-mining","<p>If all you want to do is remove non-alphanumeric characters from a string, you can do it simply with <a href=""https://docs.python.org/3/library/re.html#re.sub"" rel=""nofollow noreferrer""><code>re.sub</code></a>:</p>

<pre><code>&gt;&gt;&gt; re.sub('\W', '', '0-001')
'0001'
</code></pre>

<p>Note, the <code>\W</code> will match any character which is not a Unicode word character. This is the opposite of <code>\w</code>. For ASCII strings it's equivalent to <code>[^a-zA-Z0-9_]</code>.</p>
",3,1,2295,2017-07-23 15:12:10,https://stackoverflow.com/questions/45266640/remove-chars-from-string-using-regular-expression
Quanteda: how to remove my own list of words,"<p>Since there is no ready implementation of stopwords for Polish in quanteda, I would like to use my own list. I have it in a text file as a list separated by spaces. If need be, I can also prepare a list separated by new lines.</p>

<p>How can I remove the custom long list of stopwords from my corpus? 
How can I do that after stemming?</p>

<p>I have tried creating various formats, converting to string vectors like</p>

<pre><code>stopwordsPL &lt;- as.character(readtext(""polish.stopwords.txt"",encoding = ""UTF-8""))
stopwordsPL &lt;- read.txt(""polish.stopwords.txt"",encoding = ""UTF-8"",stringsAsFactors = F))
stopwordsPL &lt;- dictionary(stopwordsPL)
</code></pre>

<p>I have also tried to use such vectors of words in syntax</p>

<pre><code>myStemMat &lt;-
  dfm(
    mycorpus,
    remove = as.vector(stopwordsPL),
    stem = FALSE,
    remove_punct = TRUE,
    ngrams=c(1,3)
  )

dfm_trim(myStemMat, sparsity = stopwordsPL)
</code></pre>

<p>or</p>

<pre><code>myStemMat &lt;- dfm_remove(myStemMat,features = as.data.frame(stopwordsPL))
</code></pre>

<p>Nothing works. My stopwords show up in the corpus and in the analysis. What should be the proper way/syntax to apply custom stop words?</p>
","r, text-mining, quanteda","<p>Assuming your <code>polish.stopwords.txt</code> are like <a href=""https://github.com/bieli/stopwords/blob/master/polish.stopwords.txt"" rel=""noreferrer"">this</a> then you should be able to remove them from your corpus easily this way:</p>

<pre><code>stopwordsPL &lt;- readLines(""polish.stopwords.txt"", encoding = ""UTF-8"")

dfm(mycorpus,
    remove = stopwordsPL,
    stem = FALSE,
    remove_punct = TRUE,
    ngrams=c(1,3))
</code></pre>

<p>The solution using <strong>readtext</strong> is not working because it reads in the entire file as one document.  To get the individual words, you would need to tokenise it and to coerce the tokens to character.  Probably <code>readLines()</code> is easier.</p>

<p>No need to create a dictionary from <code>stopwordsPL</code> either, since <code>remove</code> should take a character vector. Also, there is no Polish stemmer implemented yet, I am afraid.</p>

<p>Currently (v0.9.9-65) the feature removal in <code>dfm()</code> does not get rid of stop words that form bigrams.  To override this, try:</p>

<pre><code># form the tokens, removing punctuation
mytoks &lt;- tokens(mycorpus, remove_punct = TRUE)
# remove the Polish stopwords, leave pads
mytoks &lt;- tokens_remove(mytoks, stopwordsPL, padding = TRUE)
## can't do this next one since no Polish stemmer in 
## SnowballC::getStemLanguages()
# mytoks &lt;- tokens_wordstem(mytoks, language = ""polish"")
# form the ngrams
mytoks &lt;- tokens_ngrams(mytoks, n = c(1, 3))
# construct the dfm
dfm(mytoks)
</code></pre>
",10,7,6068,2017-07-26 12:51:37,https://stackoverflow.com/questions/45327556/quanteda-how-to-remove-my-own-list-of-words
Create Corpus by combining words in r,"<p>I am trying to create corpus, but in that I wants to combine 2 consecutive words in document, I didn't want corpus of single words.</p>

<p>I am using below script. Is there a way in which I can create corpus ""docs"" which will be inclusion of combined 2 consecutive words in each document? Please advise.</p>

<pre><code>library(plyr)
library(tm)
library(e1071)

setwd(""C:/Assignment/Assignment-Group-Prediction/IPM"")

training&lt;- read.csv(""Data.csv"",header=T,na.strings=c(""""))

Res_Desc_Train &lt;- subset(training,select=c(""Group"",""Description""))

##Step 1 : Create Document Matrix 

docs &lt;- Corpus(VectorSource(Res_Desc_Train$Description))
docs &lt;-tm_map(docs,content_transformer(tolower))

#remove potentially problematic symbols
toSpace &lt;- content_transformer(function(x, pattern) { return (gsub(pattern, "" "", x))})
removeSpecialChars &lt;- function(x) gsub(""[^a-zA-Z0-9 ]"","""",x)
docs &lt;- tm_map(docs, toSpace, ""/"")
docs &lt;- tm_map(docs, toSpace, ""-"")
docs &lt;- tm_map(docs, toSpace, "":"")
docs &lt;- tm_map(docs, toSpace, "";"")
docs &lt;- tm_map(docs, toSpace, ""@"")
docs &lt;- tm_map(docs, toSpace, ""\\("" )
docs &lt;- tm_map(docs, toSpace, "")"")
docs &lt;- tm_map(docs, toSpace, "","")
docs &lt;- tm_map(docs, toSpace, ""_"")
docs &lt;- tm_map(docs, content_transformer(removeSpecialChars))
docs &lt;- tm_map(docs, content_transformer(tolower))
docs &lt;- tm_map(docs, removeWords, stopwords(""en""))
docs &lt;- tm_map(docs, removePunctuation)
docs &lt;- tm_map(docs, stripWhitespace)
docs &lt;- tm_map(docs, removeNumbers)
</code></pre>
","r, nlp, text-mining","<p>The <a href=""http://tm.r-forge.r-project.org/faq.html#Bigrams"" rel=""nofollow noreferrer"">FAQ</a> of the <code>tm</code> package answers your question directly:</p>
<blockquote>
<p>Can I use bigrams instead of single tokens in a term-document matrix?</p>
<p>Yes. Package NLP provides functionality to compute n-grams which can be used to construct a corresponding tokenizer. E.g.:</p>
</blockquote>
<pre><code>library(&quot;tm&quot;)
data(&quot;crude&quot;)

BigramTokenizer &lt;-
function(x)
  unlist(lapply(ngrams(words(x), 2), paste, collapse = &quot; &quot;), use.names = FALSE)

tdm &lt;- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
inspect(removeSparseTerms(tdm[, 1:10], 0.7))
</code></pre>
",1,0,613,2017-07-26 13:09:58,https://stackoverflow.com/questions/45328025/create-corpus-by-combining-words-in-r
Quanteda: how to create identically-featured dfms from a list of words,"<p>I run a randomforest on n-gram matrix of articles, because I would like to classify it to 2 categories. As a result of RF I received a list of important variables.</p>

<p>Now I would like to run random forest only on the selected first n features and then use the same features for predicting new classification. For that 
I need to create dfm only for most important variables (from RF). 
How can I create a dictionary from a list of those important variables?</p>

<p>The relevant part of the code... after creating a dictionary I have only one entry in it. How to create it properly?</p>

<pre><code>forestModel &lt;-
  randomForest(x =  as.matrix(myStemMat),y=as.factor(classVect),
               ntree = 1000 )

impVariables &lt;-
  data.frame(important = as.matrix(importance(forestModel)))

impVariables &lt;-
  impVariables %&gt;% mutate(impVar = row.names(impVariables)) %&gt;% 
  arrange(desc(MeanDecreaseGini)) %&gt;% 
  top_n(1000, wt = MeanDecreaseGini) %&gt;% 
  select(impVar) %&gt;% as.list() %&gt;% dictionary()

myStemMat &lt;-
  dfm(
    mycorpus,
    dictionary=impVariables,
    #   remove = stopwordsPL,
    stem = TRUE,
    remove_punct = TRUE,
    ngrams=c(1,2)
  )
</code></pre>

<p>In brief, when I have a list of strings, of words, n-grams, how can I create a dictionary so that I can use it in the <code>dfm()</code> function to generate term matrix?</p>

<p>Here is a link to complete code ""reproducible example"" and data it uses. <a href=""https://www.dropbox.com/s/3oe1tcfcauer0wf/text_data.zip?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/3oe1tcfcauer0wf/text_data.zip?dl=0</a></p>
","r, dictionary, text-mining, corpus, quanteda","<p>You should read the <code>?dictionary</code> carefully, since this not designed to be a set for feature selection (although it can be), but rather to create equivalence classes among values assigned to dictionary keys.</p>

<p>If your <code>impVariables</code> is a character vector of features, then you should be able to use these commands to perform the selection you want:</p>

<pre><code>toks &lt;- 
    tokens(mycorpus, remove_punct = TRUE) %&gt;%
    tokens_select(impVariables, padding = TRUE) %&gt;%
    tokens_wordstem() %&gt;%
    tokens_ngrams(n = 1:2)

dfm(toks)
</code></pre>

<p>where the last command produces a document-feature matrix of just the stemmed, ngram features that were selected in the top features from your random forest model.  Note that the <code>padding = TRUE</code> will prevent ngrams from forming that were never adjacent in your original text.  If you don't care about that, set it to <code>FALSE</code> (the default).</p>

<p><strong>ADDED:</strong></p>

<p>To select the columns of the dfm from a character vector of selection words, here's two methods we can use.</p>

<p>We will work with these sample objects:</p>

<pre><code># two sample texts and their dfm representations
txt1 &lt;- c(d1 = ""a b c f g h"",
          d2 = ""a a c c d f f f"")
txt2 &lt;- c(d1 = ""c c d f g h"",
          d2 = ""b b d i j"")
(dfm1 &lt;- dfm(txt1))
# Document-feature matrix of: 2 documents, 7 features (28.6% sparse).
# 2 x 7 sparse Matrix of class ""dfmSparse""
#     features
# docs a b c f g h d
#   d1 1 1 1 1 1 1 0
#   d2 2 0 2 3 0 0 1

(dfm2 &lt;- dfm(txt2))
# Document-feature matrix of: 2 documents, 8 features (43.8% sparse).
# 2 x 8 sparse Matrix of class ""dfmSparse""
#     features
# docs c d f g h b i j
#   d1 2 1 1 1 1 0 0 0
#   d2 0 1 0 0 0 2 1 1

impVariables &lt;- c(""a"", ""c"", ""e"", ""z"")
</code></pre>

<p><strong>First Method: Create a dfm and select on that using <code>dfm_select()</code></strong></p>

<p>Here, we are creating a dfm from the character vector of your features, just so that we register them as features, because of the way that <code>dfm_select()</code> works when the selection object is a dfm.</p>

<pre><code>impVariablesDfm &lt;- dfm(paste(impVariables, collapse = "" ""))
dfm_select(dfm1, impVariablesDfm)
# Document-feature matrix of: 2 documents, 4 features (50% sparse).
# 2 x 4 sparse Matrix of class ""dfmSparse""
#     features
# docs a c e z
#   d1 1 1 0 0
#   d2 2 2 0 0

dfm_select(dfm2, impVariablesDfm)
# Document-feature matrix of: 2 documents, 4 features (87.5% sparse).
# 2 x 4 sparse Matrix of class ""dfmSparse""
#     features
# docs a c e z
#   d1 0 2 0 0
#   d2 0 0 0 0
</code></pre>

<p><strong>Second Method: Create a dictionary and select on that using <code>dfm_lookup()</code></strong></p>

<p>Let's create a helper function to create a dictionary from a character vector:</p>

<pre><code># make a dictionary where each key = its value
char2dictionary &lt;- function(x) {
    result &lt;- as.list(x)  # make the vector into a list
    names(result) &lt;- x
    dictionary(result)
}
</code></pre>

<p>Now using dfm lookup, we get only the keys, even ones that were not observed:</p>

<pre><code>dfm_lookup(dfm1, dictionary = char2dictionary(impVariables))
# Document-feature matrix of: 2 documents, 4 features (50% sparse).
# 2 x 4 sparse Matrix of class ""dfmSparse""
#     features
# docs a c e z
#   d1 1 1 0 0
#   d2 2 2 0 0

dfm_lookup(dfm2, dictionary = char2dictionary(impVariables))
# Document-feature matrix of: 2 documents, 4 features (87.5% sparse).
# 2 x 4 sparse Matrix of class ""dfmSparse""
#     features
# docs a c e z
#   d1 0 2 0 0
#   d2 0 0 0 0
</code></pre>

<p>Note: (but the first one at least will work with v0.9.9.65):</p>

<pre><code>packageVersion(""quanteda"")
# [1] ‘0.9.9.85’
</code></pre>
",2,1,316,2017-07-26 17:19:48,https://stackoverflow.com/questions/45333597/quanteda-how-to-create-identically-featured-dfms-from-a-list-of-words
Find all unique strings in R,"<p>I am relatively new to R. I have a dataframe <code>df</code> that looks like this (one character variable only...my actual df spans 100k+ rows, but for simplicity, let's look at 5 rows only):</p>

<pre><code>V1
oximetry, hydrogen peroxide adverse effects, epoprostenol adverse effects
angioedema chemically induced, angioedema chemically induced, oximetry
abo blood group system, imipramine poisoning, adverse effects
isoenzymes, myocardial infarction drug therapy, thrombosis drug therapy
thrombosis drug therapy
</code></pre>

<p>I want to be able to output every single unique string so that it looks like this:</p>

<pre><code>V1
oximetry
hydrogen peroxide adverse effects
epoprostenol adverse effects
angioedema chemically induced
abo blood group system
imipramine poisoning
adverse effects
isoenzymes
myocardial infarction drug therapy
thrombosis drug therapy
</code></pre>

<p>Do I use the <code>tm</code> package? I tried using <code>dtm</code> but my code was inefficient since it would convert <code>dtm</code> to matrix which would require a lot of memory from 100k+ rows.</p>

<p>Please advise. Thanks!</p>
","r, string, dataframe, unique, text-mining","<p>try this:</p>

<pre><code>library(stringr)
library(tidyverse)

df &lt;- data.frame(variable = c(
'oximetry, hydrogen peroxide adverse effects, epoprostenol adverse effects',
'angioedema chemically induced, angioedema chemically induced, oximetry',
'abo blood group system, imipramine poisoning, adverse effects',
'isoenzymes, myocardial infarction drug therapy, thrombosis drug therapy',
'thrombosis drug therapy'), stringsAsFactors=FALSE)

mutate(df, variable = str_split(variable, ', ')) %&gt;%
  unnest() %&gt;% distinct()
</code></pre>
",4,1,5561,2017-07-26 19:49:28,https://stackoverflow.com/questions/45336182/find-all-unique-strings-in-r
Sklearn - feature extraction from text - normalize text features by merging plural and singular forms,"<p>I am doing some text classification right now using sklearn.</p>

<p>As first step I obviously need to use vectorizer - either CountVectorizer or TfIdfVectorizer. The issue which I want to tackle is that in my documents often times I have singular and plural forms of same word. When performing vectorization I want to 'merge' singular and plural forms and treat them as a same text feature.</p>

<p>Obviously I can manually pre-process texts and just replace all plural word forms with singular word forms when I know which words have this issue. But maybe there is some way to do it in a more automated way, when words which are extremely similar to each other are merged into same feature?</p>

<p><strong>UPDATE.</strong></p>

<p>Based on the answer provided earlier, I needed to perform a stemming. Below is a sample code which stems all words in 'review' column of a dataframe DF, which I then use in vectorization and classification. Just in case anyone finds it useful.</p>

<pre><code>from nltk.stem.snowball import SnowballStemmer

stemmer = SnowballStemmer(""english"")


df['review_token']=df['review'].apply(lambda x : filter(None,x.split("" "")))

df['review_stemmed']=df['review_token'].apply(lambda x : [stemmer.stem(y) for y in x])

df['review_stemmed_sentence']=df['review_stemmed'].apply(lambda x : "" "".join(x))
</code></pre>
","python, scikit-learn, text-mining, feature-extraction, text-classification","<p>I think what you need is stemming, namely removing the endings of words that have a common root, and it's one of the basic operations in preprocessing text data.</p>

<p>Here's some rules for stemming and lemmatization explained: <a href=""https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html</a></p>
",2,1,1180,2017-07-26 20:10:12,https://stackoverflow.com/questions/45336491/sklearn-feature-extraction-from-text-normalize-text-features-by-merging-plur
How to produce document term matrix in text2vector only from stored list of words,"<p>What is the syntax in text2vec to vectorize texts and achieve dtm with only the indicated list of words? </p>

<p>How to vectorize and produce document term matrix only on indicated features? And if the features do not appear in the text the variable should stay empty.</p>

<p>I need to produce term document matrices with exactly the same columns as in the dtm I run the modelling on, otherwise I cannot use random forest model on new documents. </p>
","r, text-mining, text2vec","<p>You can create document term matrix only from specific set of features:</p>

<pre><code>v = create_vocabulary(c(""word1"", ""word2""))
vectorizer = vocab_vectorizer(v)
dtm_test = create_dtm(it, vectorizer)
</code></pre>

<p>However I don't recommend to 1) use random forest on such sparse data - it won't work good 2) perform feature selection way you described - you will likely overfit. </p>
",2,0,464,2017-07-28 12:34:22,https://stackoverflow.com/questions/45373699/how-to-produce-document-term-matrix-in-text2vector-only-from-stored-list-of-word
How to include rows from a Pandas Dataframe that contain strings from a list?,"<p>I have a Pandas Dataframe.  One of the columns is a string.  It contains a typed report.</p>

<p>The Dataframe is <code>DF_Check</code> and the column in question is <code>DF_Check['COM']</code></p>

<p>I would like to return the rows that contain the words from <code>list1</code> in <code>DF_Check['COM']</code>.  I have a list:</p>

<pre><code>list1=['stopped','broken','failure','damaged']
</code></pre>

<p>Any my current code is:</p>

<p><code>DF_Priority=DF_Check[(DF_Check['COM'].str.contains('|'.join(priority))==True)]</code>.</p>

<p>I have also tried</p>

<p><code>DF_Priority=DF_Check[(DF_Check['COM'].str.contains('|'.join(priority)))]</code></p>

<p>They both find the rows that I am looking for, but I am also getting rows that do not match my desired criteria.</p>

<p>Any suggestions?</p>
","python, pandas, dataframe, text-mining","<pre><code>DF_Priority = DF_Check[DF_Check['COM'].str.contains('|'.join(list1))]
</code></pre>

<p>This should work... if it doesn't then you need to provide data to verify what you are doing.</p>
",1,-1,80,2017-07-31 23:44:23,https://stackoverflow.com/questions/45426875/how-to-include-rows-from-a-pandas-dataframe-that-contain-strings-from-a-list
tf-idf document term matrix and LDA: Error messages in R,"<p>Can we input tf-idf document term matrix into Latent Dirichlet Allocation (LDA)? if yes, how?</p>

<p>It does not work in my case and the LDA function requires the 'term-frequency' document term matrix.</p>

<p>Thank you</p>

<p>(I make a question as concise as possible. So, if you need more details, I can add </p>

<pre><code>##########################################################################
                           TF-IDF Document matrix construction
##########################################################################    

&gt; DTM_tfidf &lt;-DocumentTermMatrix(corpora,control = list(weighting = 
function(x)+   weightTfIdf(x, normalize = FALSE)))
&gt; str(DTM_tfidf)
List of 6
$ i       : int [1:4466] 1 1 1 1 1 1 1 1 1 1 ...
$ j       : int [1:4466] 6 10 22 26 28 36 39 41 47 48 ...
$ v       : num [1:4466] 6 2.09 1.05 3.19 2.19 ...
$ nrow    : int 64
$ ncol    : int 297
$ dimnames:List of 2
  ..$ Docs : chr [1:64] ""1"" ""2"" ""3"" ""4"" ...
  ..$ Terms: chr [1:297] ""accommod"" ""account"" ""achiev"" ""act"" ...
- attr(*, ""class"")= chr [1:2] ""DocumentTermMatrix"" ""simple_triplet_matrix""
- attr(*, ""weighting"")= chr [1:2] ""term frequency - inverse document 
frequency"" ""tf-idf""

##########################################################################
                           LDA section
##########################################################################

&gt; LDA_results &lt;-LDA(DTM_tfidf,k, method=""Gibbs"", control=list(nstart=nstart,
  +                                seed = seed, best=best, 
  +                                burnin = burnin, iter = iter, thin=thin))

##########################################################################
                           Error messages
##########################################################################
  Error in LDA(DTM_tfidf, k, method = ""Gibbs"", control = list(nstart = 
  nstart,  : 
  The DocumentTermMatrix needs to have a term frequency weighting
</code></pre>
","r, matrix, text-mining, lda, tidytext","<p>If you explore the documentation for LDA topic modeling using the topicmodels package, for example by typing <code>?LDA</code> in the R console, you'll see that this modeling procedure is expecting a frequency-weighted document-term matrix, not tf-idf-weighted. </p>

<pre><code>""Object of class ""DocumentTermMatrix"" with term-frequency weighting or an object coercible...""
</code></pre>

<p>So the answer is no, you cannot use a tf-idf-weighted DTM directly in this function. If you <em>have</em> a tf-idf-weighted DTM already, you can convert it using <code>tm::weightTf()</code> to get to the necessary weighting. If you are building a document-term matrix from scratch, then don't weight it by tf-idf.</p>
",1,2,1623,2017-08-08 09:55:35,https://stackoverflow.com/questions/45565164/tf-idf-document-term-matrix-and-lda-error-messages-in-r
How to use Traceback and debug to fix broken R code?,"<p>I'm trying to write a script that simplifies the process of producing a clean corpus from a vector or data frame for text mining and NLP.  However, my script produces an error when I run it.  My script is as follows: </p>

<pre><code>  quick_clean &lt;- function(data, Vector = TRUE, removeNumbers = TRUE, removePunctuation = TRUE, 
                     stop.words = NULL, ...) {
  if(Vector == TRUE) {
    source &lt;- VectorSource(data)
  } else {
    source &lt;- DataframeSource(data)
  }
  corp &lt;- VCorpus(source)
  corp &lt;- tm_map(corp, stripWhitespace)

  if(removePunctuation == TRUE) {
    corp &lt;- tm_map(corp, removePunctuation)
  }
  if(removeNumbers == TRUE) {
    corp &lt;- tm_map(corp, removeNumbers)
  }
  if(is.null(stop.words)) {
   return(corp)
  } else {
    corp &lt;- tm_map(corp, removeWords, c(stopwords(""en""), stop.words))
  }
  corp
}
</code></pre>

<p>When I run it, I get the following error: </p>

<pre><code>Error in get(as.character(FUN), mode = ""function"", envir = envir) : 
object 'FUN' of mode 'function' was not found 
</code></pre>

<p>I ran the traceback, but I'm not really sure how to use this information: </p>

<pre><code>7. get(as.character(FUN), mode = ""function"", envir = envir) 
6. match.fun(FUN) 
5. lapply(X, FUN, ...) 
4. tm_parLapply(content(x), FUN, ...) 
3. tm_map.VCorpus(corp, removePunctuation) 
2. tm_map(corp, removePunctuation) 
1. quick_clean(swift_vec)
</code></pre>

<p>I also ran Debug and got the following...again, I'm not sure how to use this info: </p>

<pre><code>Error in get(as.character(FUN), mode = ""function"", envir = envir) : 
  object 'FUN' of mode 'function' was not found
Called from: get(as.character(FUN), mode = ""function"", envir = envir)
Browse[1]&gt; 
</code></pre>

<p>What am I doing wrong here?  </p>
","r, function, debugging, text-mining, traceback","<p>Let's examine the <code>traceback</code> pile from the bottom:</p>

<ol>
<li>your error is in <code>quick_clean</code></li>
<li>it's on the <code>corp &lt;- tm_map(corp, removePunctuation)</code> line, luckily you only have one</li>
<li>inside <code>tm_map</code> the function itself is calling the method <code>tm_map.VCorpus</code>, as your corp object is of class <code>Vcorpus</code> and tm_map is a wrapper for different methods</li>
<li>This function itself is calling <code>tm_parLapply</code> etc...</li>
</ol>

<p>From the time you hit a reliable function in <code>traceback</code> it's usually not so useful to go much further, it means that the input you gave to the functions isn't good.</p>

<p>We learnt that you gave a <code>Vcorpus</code> object as a first parameter, so this one seems to be ok, though we may check later if its format is not problematic.</p>

<p>But let's check the other parameter, <code>removePunctuation</code>, the doc (<code>?tm_map</code>) says it requires a function, if you use <code>debug</code>, <code>debugonce</code> or <code>browser</code> (look them up). you'll see that their boolean at the time you execute the line.</p>

<p>And they're boolean because you named your function parameters just like those functions.</p>

<p>So rename your function parameters and hopefully it will run fine :).</p>

<p><strong><em>here's how you may use <code>browser</code>:</em></strong></p>

<p>define this function (spot the added line)</p>

<pre><code>quick_clean &lt;- function(data, Vector = TRUE, removeNumbers = TRUE, removePunctuation = TRUE, 
                     stop.words = NULL, ...) {
  if(Vector == TRUE) {
    source &lt;- VectorSource(data)
  } else {
    source &lt;- DataframeSource(data)
  }
  corp &lt;- VCorpus(source)
  corp &lt;- tm_map(corp, stripWhitespace)

  if(removePunctuation == TRUE) {
    browser() # &lt;----------------------------------------- here !
    corp &lt;- tm_map(corp, removePunctuation)
  }
  if(removeNumbers == TRUE) {
    corp &lt;- tm_map(corp, removeNumbers)
  }
  if(is.null(stop.words)) {
   return(corp)
  } else {
    corp &lt;- tm_map(corp, removeWords, c(stopwords(""en""), stop.words))
  }
  corp
}
</code></pre>

<p>Execute the line that triggered the error
type <code>class(corp)</code> to confirm what we already know
type <code>class(removePunctuation)</code>
Ooops, it's a boolean.
Type <code>Q</code> or the escape key to get out of the browser.</p>

<p><code>debug</code> is like <code>browser</code>, but starts at the first line of the function.</p>
",0,0,779,2017-08-09 17:35:29,https://stackoverflow.com/questions/45597235/how-to-use-traceback-and-debug-to-fix-broken-r-code
how to extract word frequency for a subset of words in R?,"<p>I have a dataframe with about 10,000 words in one column and their corresponding frequencies in another. I also have a vector with about 600 words. Each of the 600 words is a word in the data frame. How do I look up the frequencies for the 600-word vector from the 10,000 word data frame?</p>
","r, dataframe, text-mining, word-frequency","<p>use <code>dplyr</code>'s join functions.</p>

<pre><code># make the 600 vector into a dataframe
600_df &lt;- as.data.frame(600_vec)

# left join the two dataframes
df &lt;- left_join(x = 600_df, y = 10000_df, by = ""word"")
</code></pre>

<p>where the ""word"" is the variable name constant between the two dataframes</p>
",0,0,237,2017-08-10 19:18:19,https://stackoverflow.com/questions/45621904/how-to-extract-word-frequency-for-a-subset-of-words-in-r
Why can not I use &quot;TermDocumentMatrix&quot;?,"<p>Why can not I use ""TermDocumentMatrix""?</p>

<p>I used the following command to unify plural words in singular form, but I get an error.</p>

<pre><code>crudeCorp &lt;- tm_map(crudeCorp, gsub, pattern = ""smells"", replacement = ""smell"")
crudeCorp &lt;- tm_map(crudeCorp, gsub, pattern = ""feels"", replacement = ""feel"")
crudeDtm &lt;- TermDocumentMatrix(crudeCorp, control=list(removePunctuation=T))
Error in UseMethod(""meta"", x) : 
  no applicable method for 'meta' applied to an object of class ""character""
</code></pre>

<p>How should I solve it?
1. Is there a command to change from singular to cleaning?
2. Is this command I used wrong?</p>

<p>I will attach the following code to the sentence processing and matrix.</p>

<pre><code>library(tm)
library(XML)

crudeCorp&lt;-VCorpus(VectorSource(readLines(file.choose())))

#(Eliminating Extra Whitespace) 
crudeCorp &lt;- tm_map(crudeCorp, stripWhitespace)

#(Convert to Lower Case)

crudeCorp&lt;-tm_map(crudeCorp, content_transformer(tolower))


# remove stopwords from corpus

crudeCorp&lt;-tm_map(crudeCorp, removeWords, stopwords(""english""))
myStopwords &lt;- c(stopwords(""english""), ""can"", ""will"",""got"",""also"",""goes"",""get"",""much"",""since"",""way"",""even"")
myStopwords &lt;- setdiff(myStopwords, c(""will"",""can""))
crudeCorp &lt;- tm_map(crudeCorp, removeWords, myStopwords)

crudeCorp&lt;-tm_map(crudeCorp,removeNumbers)

crudeCorp &lt;- tm_map(crudeCorp, gsub, pattern = ""smells"", replacement = ""smell"")
crudeCorp &lt;- tm_map(crudeCorp, gsub, pattern = ""feels"", replacement = ""feel"")

#-(Creating Term-Document Matrices)
crudeDtm &lt;- TermDocumentMatrix(crudeCorp, control=list(removePunctuation=T))
</code></pre>

<p>example : my data</p>

<pre><code>1. I'M HAPPY
2. how are you?
3. This apple is good
(skip)
</code></pre>
","r, matrix, text-mining, tm","<p>Whyn't use below code for stemming &amp; Punctuation removal?</p>

<pre><code>crudeCorp &lt;- tm_map(crudeCorp, removePunctuation)
crudeCorp &lt;- tm_map(crudeCorp, stemDocument, language = ""english"")  
crudeDtm  &lt;- DocumentTermMatrix(crudeCorp)
</code></pre>

<p>Hope this helps!</p>
",0,0,44,2017-08-16 10:38:17,https://stackoverflow.com/questions/45711313/why-can-not-i-use-termdocumentmatrix
"When using &quot;TermDocumentMatrix&quot;, no applicable method for &#39;meta&#39; applied to an object of class &quot;character&quot;","<p>Until I used this phrase, ""TermDocumentMatrix"" was good.</p>

<pre><code>doc &lt;- tm_map(doc, gsub, pattern = ""buy"", replacement = ""bought"")
</code></pre>

<p>However, after using this phrase, ""TermDocumentMatrix"" will generate an error.</p>

<pre><code>Error in UseMethod(""meta"", x) : 
no applicable method for 'meta' applied to an object of class ""character""
</code></pre>

<p>I need a word replacement.
So I have used this phrase.</p>

<p>My doc structure is as follows.</p>

<pre><code>1. so I bought it.
2. I bought the EH AC line in November 2014 
3. 3rd product bought from AC and all no good.
(skip)
</code></pre>

<p>How can I use ""TermDocumentMatrix""?</p>

<pre><code>library(tm)
library(XML)
library(SnowballC)

doc&lt;-VCorpus(VectorSource(readLines(file.choose())))

doc &lt;- tm_map(doc, stripWhitespace)

doc &lt;- tm_map(doc, stemDocument)

doc&lt;-tm_map(doc, content_transformer(tolower))

doc&lt;-tm_map(doc, removeWords, stopwords(""english""))

myStopwords &lt;- c(stopwords(""english""), ""can"", ""will"")
myStopwords &lt;- setdiff(myStopwords, c(""will"",""can""))
doc &lt;- tm_map(doc, removeWords, myStopwords)

doc&lt;-tm_map(doc,removeNumbers)


#If you omit this step, the error will not appear in ""TermDocumentMatrix"".
doc &lt;- tm_map(doc, gsub, pattern = ""buy"", replacement = ""bought"")

doc &lt;- TermDocumentMatrix(doc, control=list(removePunctuation=T))
</code></pre>
","r, matrix, character, text-mining, tm","<p>You need to pass a proper content transformer to <code>tm_map</code>, not an arbitrary character manipulating function</p>

<pre><code>doc &lt;- tm_map(doc, content_transformer(function(x) 
    gsub(x, pattern = ""buy"", replacement = ""bought"")))
</code></pre>
",1,1,5690,2017-08-17 14:35:49,https://stackoverflow.com/questions/45738060/when-using-termdocumentmatrix-no-applicable-method-for-meta-applied-to-an-o
How to do large-scale replacement/tokenization in R tm_map gsub from a list?,"<p>Has anyone managed to create a massive find/replace function/working code snippet that exchanges out known bigrams in a dataframe?</p>

<p>Here's an example. I'm able to don onesie-twosie replacements but I really want to leverage a known lexicon of about 800 terms I want to find-replace to turn them into word units prior to DTM generation. For example, I want to turn ""Google Analytics"" into ""google-analytics"".</p>

<p>I know it's theoretically possible; essentially, a custom stopwords list functionally does almost the same thing, except without the replacement. And it seems stupid to just have 800 gsubs.</p>

<p>Here's my current code. Any help/pointers/URLs/RTFMs would be greatly appreciated.</p>

<pre><code>mystopwords &lt;- read.csv(stopwords.file, header = FALSE)
mystopwords &lt;- as.character(mystopwords$V1)
mystopwords &lt;- c(mystopwords, stopwords())

# load the file

df &lt;- readLines(file.name)

# transform to corpus

doc.vec &lt;- VectorSource(df)
doc.corpus &lt;- Corpus(doc.vec)
# summary(doc.corpus)

## Hit known phrases

docs &lt;- tm_map(doc.corpus, content_transformer(gsub), pattern = ""Google Analytics"", replacement = ""google-analytics"")

## Clean up and fix text - note, no stemming

doc.corpus &lt;- tm_map(doc.corpus, content_transformer(tolower))
doc.corpus &lt;- tm_map(doc.corpus, removePunctuation,preserve_intra_word_dashes = TRUE)
doc.corpus &lt;- tm_map(doc.corpus, removeNumbers)
doc.corpus &lt;- tm_map(doc.corpus, removeWords, c(stopwords(""english""),mystopwords))
doc.corpus &lt;- tm_map(doc.corpus, stripWhitespace)
</code></pre>
","r, nlp, text-mining, tm, tidyr","<p>The <strong>corpus</strong> library allows you to combine multi-word phrases into single tokens. When there are multiple matches, it chooses the longest one:</p>

<pre><code>library(corpus)
text_tokens(""I live in New York City, New York"",
            combine = c(""new york city"", ""new york""))

# [[1]]
# [1] ""i""             ""live""          ""in""            ""new_york_city""
# [5] "",""             ""new_york""
</code></pre>

<p>By default, the connector is the underscore character (<code>_</code>), but you can specify an alternative connector using the <code>connector</code> argument`.</p>

<p>In your example, you could do the following to get a document-by-term matrix:</p>

<pre><code>mycombine &lt;- c(""google analytics"", ""amazon echo"") # etc.
term_matrix(doc.corpus, combine = mycombine,
            drop_punct = TRUE, drop_number = TRUE,
            drop = c(stopwords_en, mystopwords))
</code></pre>

<p>Note also that <strong>corpus</strong> keeps intra-word hyphens, so there's no need for a <code>preserve_intra_word_dashes</code> option.</p>

<p>It can be a hassle to specify the preprocessing options in every function call. If you'd like, you can convert your corpus to a <em>corpus_frame</em> (a data.frame with a special <em>text</em> column), then set the preprocessing options (the <em>text_filter</em>):</p>

<pre><code>corpus &lt;- as_corpus_frame(doc.corpus)
text_filter(corpus) &lt;- text_filter(combine = mycombine,
                                   drop_punct = TRUE,
                                   drop_number = TRUE,
                                   drop = c(stopwords_en, mystopwords))
</code></pre>

<p>After that, you can just call</p>

<pre><code>term_matrix(corpus)
</code></pre>

<p>There's a lot more information about <strong>corpus</strong>, including an introductory vignette, at <a href=""http://corpustext.com"" rel=""nofollow noreferrer"">http://corpustext.com</a></p>
",1,1,424,2017-08-18 11:27:16,https://stackoverflow.com/questions/45755538/how-to-do-large-scale-replacement-tokenization-in-r-tm-map-gsub-from-a-list
Lemmatization using txt file with lemmes in R,"<p>I would like to use external txt file with Polish lemmas structured as follows:
(source for lemmas for many other languages <a href=""http://www.lexiconista.com/datasets/lemmatization/"" rel=""nofollow noreferrer"">http://www.lexiconista.com/datasets/lemmatization/</a>)</p>

<pre><code>Abadan  Abadanem
Abadan  Abadanie
Abadan  Abadanowi
Abadan  Abadanu
abadańczyk  abadańczycy
abadańczyk  abadańczyka
abadańczyk  abadańczykach
abadańczyk  abadańczykami
abadańczyk  abadańczyki
abadańczyk  abadańczykiem
abadańczyk  abadańczykom
abadańczyk  abadańczyków
abadańczyk  abadańczykowi
abadańczyk  abadańczyku
abadanka    abadance
abadanka    abadanek
abadanka    abadanką
abadanka    abadankach
abadanka    abadankami
</code></pre>

<p>What packages and with what syntax, would allow me use such txt database to lemmatize my bag of words. I realize, for English there is Wordnet, but there is no luck for those who would like to use this functionality for rare languages.</p>

<p>If not, can this database be converted to be useful with any package that provides lemmatization? Perhaps by converting it to a wide form? For instance, the form used by free AntConc concordancer, (<a href=""http://www.laurenceanthony.net/software/antconc/"" rel=""nofollow noreferrer"">http://www.laurenceanthony.net/software/antconc/</a>)</p>

<pre><code>Abadan -&gt; Abadanem, Abadanie, Abadanowi, Abadanu
abadańczyk -&gt; abadańczycy, abadańczyka, abadańczykach 
etc.
</code></pre>

<p>In brief: How can lemmatization with lemmas in txt file be done in any of the known CRAN R text mining packages ? If so, how to format such txt file?</p>

<p>UPDATE: Dear @DmitriySelivanov I got rid of all diacritical marks, now I would like to apply it on tm corpus ""docs""</p>

<pre><code>docs &lt;- tm_map(docs, function(x) lemma_tokenizer(x, lemma_hashmap=""lemma_hm"")) 
</code></pre>

<p>and I tried it as tokenizer</p>

<pre><code>LemmaTokenizer &lt;- function(x) lemma_tokenizer(x, lemma_hashmap=""lemma_hm"")

docsTDM &lt;-
  DocumentTermMatrix(docs, control = list(wordLengths = c(4, 25), tokenize=LemmaTokenizer)) 
</code></pre>

<p>It throws at me an error: </p>

<pre><code> Error in lemma_hashmap[[tokens]] : 
  attempt to select more than one element in vectorIndex 
</code></pre>

<p>The function works with a vector of texts as charm though.</p>
","r, text-mining, tm, quanteda, text2vec","<p>My guess is that here is nothing to do with text-mining packages for this task. You need just to replace word in a second column by word in a first column. You can do it with creating hashmap (for example <a href=""https://github.com/nathan-russell/hashmap"" rel=""nofollow noreferrer"">https://github.com/nathan-russell/hashmap</a>).</p>

<p>Below is example of how you can create ""lemmatizing"" tokenizer which you can easily use in <strong>text2vec</strong> (and I guess <strong>quanteda</strong> as well).</p>

<p><strong>Contributions</strong> in order to create such ""lemmatizing"" package are very welcome - will be very useful.</p>

<pre><code>library(hashmap)
library(data.table)
txt = 
  ""Abadan  Abadanem
  Abadan  Abadanie
  Abadan  Abadanowi
  Abadan  Abadanu
  abadańczyk  abadańczycy
  abadańczyk  abadańczykach
  abadańczyk  abadańczykami
  ""
dt = fread(txt, header = F, col.names = c(""lemma"", ""word""))
lemma_hm = hashmap(dt$word, dt$lemma)

lemma_hm[[""Abadanu""]]
#""Abadan""


lemma_tokenizer = function(x, lemma_hashmap, 
                           tokenizer = text2vec::word_tokenizer) {
  tokens_list = tokenizer(x)
  for(i in seq_along(tokens_list)) {
    tokens = tokens_list[[i]]
    replacements = lemma_hashmap[[tokens]]
    ind = !is.na(replacements)
    tokens_list[[i]][ind] = replacements[ind]
  }
  tokens_list
}
texts = c(""Abadanowi abadańczykach OutOfVocabulary"", 
          ""abadańczyk Abadan OutOfVocabulary"")
lemma_tokenizer(texts, lemma_hm)

#[[1]]
#[1] ""Abadan""          ""abadańczyk""      ""OutOfVocabulary""
#[[2]]
#[1] ""abadańczyk""      ""Abadan""          ""OutOfVocabulary""
</code></pre>
",3,1,1051,2017-08-18 18:02:46,https://stackoverflow.com/questions/45762559/lemmatization-using-txt-file-with-lemmes-in-r
Compare the bag of words in two document and find the matching word and their frequency in second document,"<p>I have calculated the bag of words for 'yelp.csv', 'yelpp.csv', 'yelpn.csv' and created the matrix of individuals dataset's word frequency. Now, I want to compare the bag of words of yelp with yelpn and check how many words in yelp appears in yelpn and their frequency and store it in a variable as matrix, then same for yelpp. The yelp contains both the positive and negative. yelpp, only the positive and yelpn, only the negative. can anyone complete the code? i donno whether this code is relevant,i hope so.</p>

<pre><code>getwd()
setwd(""/Users/ash/RProjects/exc"")
getwd()
df &lt;- read.csv(""yelp.CSV"",header = TRUE,quote=""\"""",stringsAsFactors= TRUE,
           strip.white = TRUE)
df
dfd&lt;-as.character(df[,2])
dfd
df2&lt;-as.character(df[,1])
df2
words &lt;- readLines(system.file(""stopwords"", ""english.dat"",
                           package = ""tm""))
s&lt;-remove_stopwords(dfd, words, lines = TRUE)
s
print(paste(""****Stopwords are removed successfully****""))
n&lt;-removeNumbers(s)
n
t&lt;-removePunctuation(n, preserve_intra_word_dashes = FALSE)
t

#pos
dfp &lt;- read.csv(""yelpp.CSV"",header = TRUE,quote=""\"""",stringsAsFactors= TRUE,
           strip.white = TRUE)
dfp
dfdp&lt;-as.character(dfp[,2])
dfdp
df2p&lt;-as.character(dfp[,1])
df2p
wordsp &lt;- readLines(system.file(""stopwords"", ""english.dat"",
                           package = ""tm""))
sp&lt;-remove_stopwords(dfdp, words, lines = TRUE)
sp
print(paste(""****Stopwords are removed successfully****""))
np&lt;-removeNumbers(sp)
np
tp&lt;-removePunctuation(np, preserve_intra_word_dashes = FALSE)
tp

#neg
dfn &lt;- read.csv(""yelpn.CSV"",header = TRUE,quote=""\"""",stringsAsFactors=   TRUE,
           strip.white = TRUE)
dfn
dfdn&lt;-as.character(dfn[,2])
dfdn
df2n&lt;-as.character(dfn[,1])
df2n
wordsn &lt;- readLines(system.file(""stopwords"", ""english.dat"",
                           package = ""tm""))
sn&lt;-remove_stopwords(dfdn, words, lines = TRUE)
sn
print(paste(""****Stopwords are removed successfully****""))
nn&lt;-removeNumbers(sn)
nn
tn&lt;-removePunctuation(nn, preserve_intra_word_dashes = FALSE)
tn



#bag
b&lt;-bag_o_words(t, apostrophe.remove = TRUE)
b
b.mat = as.matrix(b)
b.mat
bp&lt;-bag_o_words(tp, apostrophe.remove = TRUE)
bp
bp.mat = as.matrix(bp)
bp.mat
bn&lt;-bag_o_words(tn, apostrophe.remove = TRUE)
bn
bn.mat = as.matrix(bn)
bn.mat

#frequent terms
frequent_terms &lt;- freq_terms(b.mat, 2000)
frequent_terms
frequent_termsp &lt;- freq_terms(tp, 2000)
frequent_termsp
frequent_termsn &lt;- freq_terms(tn, 2000)
frequent_termsn
</code></pre>
","r, text-mining, tm, qdap, sentimentr","<p>I'm taking text for example corpuses from <a href=""https://en.wikipedia.org/wiki/Text_mining"" rel=""nofollow noreferrer"">wiki Text mining</a>. Using <code>tm</code> package and <code>findFreqTerms</code>,<code>agrep</code> function are main points in this approach.</p>

<p><code>agrep</code></p>

<blockquote>
  <p>Searches for approximate matches to pattern (the first argument) within each element of the string x (the second argument) using the generalized Levenshtein edit distance (the minimal possibly weighted number of insertions, deletions and substitutions needed to transform one string into another).</p>
</blockquote>

<p><strong>Approach Steps :</strong></p>

<p>texts -> corpuses -> data cleaning -> findfreqterms -> compare with other term doc matrix</p>

<pre><code>library(tm)

c1 &lt;- Corpus(VectorSource(""Text mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning""))

c2 &lt;- Corpus(VectorSource(""Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output""))

c3 &lt;- Corpus(VectorSource(""Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities)""))

# Data Cleaning and transformation
c1 &lt;- tm_map(c1, content_transformer(tolower))
c2 &lt;- tm_map(c2, content_transformer(tolower))
c3 &lt;- tm_map(c3, content_transformer(tolower))

c1 &lt;- tm_map(c1, removePunctuation)
c1 &lt;- tm_map(c1, removeNumbers)
c1 &lt;- tm_map(c1, removeWords, stopwords(""english""))
c1 &lt;- tm_map(c1, stripWhitespace)

c2 &lt;- tm_map(c2, removePunctuation)
c2 &lt;- tm_map(c2, removeNumbers)
c2 &lt;- tm_map(c2, removeWords, stopwords(""english""))
c2 &lt;- tm_map(c2, stripWhitespace)

c3 &lt;- tm_map(c3, removePunctuation)
c3 &lt;- tm_map(c3, removeNumbers)
c3 &lt;- tm_map(c3, removeWords, stopwords(""english""))
c3 &lt;- tm_map(c3, stripWhitespace)

dtm1 &lt;- DocumentTermMatrix(c1, control = list(weighting = weightTfIdf, stopwords = TRUE))
dtm2 &lt;- DocumentTermMatrix(c2, control = list(weighting = weightTfIdf, stopwords = TRUE))
dtm3 &lt;- DocumentTermMatrix(c3, control = list(weighting = weightTfIdf, stopwords = TRUE))

ft1 &lt;- findFreqTerms(dtm1)
ft2 &lt;- findFreqTerms(dtm2)
ft3 &lt;- findFreqTerms(dtm3)

#similarity between c1 and c2
common.c1c2 &lt;- data.frame(term = character(0), freq = integer(0))
for(t in ft1){
  find &lt;- agrep(t, ft2)
  if(length(find) != 0){
    common.c1c2 &lt;- rbind(common.c1c2, data.frame(term = t, freq = length(find)))
  }
}
# Note : this for loop can be substituted by apply family functions if taking time for large text
</code></pre>

<p><code>common.c1c2</code> contains common words between corpus1 and corpus2 with frequency</p>

<pre><code>&gt; common.c1c2
      term freq
1     also    1
2     data    2
3  derived    1
4 deriving    1
5   mining    1
6  pattern    1
7 patterns    1
8  process    1
9     text    1

&gt; ft1
 [1] ""also""        ""analytics""   ""data""        ""derived""     ""deriving""    ""devising""    ""equivalent"" 
 [8] ""highquality"" ""information"" ""learning""    ""means""       ""mining""      ""pattern""     ""patterns""   
[15] ""process""     ""referred""    ""roughly""     ""statistical"" ""text""        ""trends""      ""typically""  

&gt; ft2
 [1] ""addition""       ""along""          ""data""           ""database""       ""derived""        ""deriving""      
 [7] ""evaluation""     ""features""       ""finally""        ""input""          ""insertion""      ""interpretation""
[13] ""involves""       ""linguistic""     ""mining""         ""others""         ""output""         ""parsing""       
[19] ""patterns""       ""process""        ""removal""        ""structured""     ""structuring""    ""subsequent""    
[25] ""text""           ""usually""        ""within""        
</code></pre>

<p>This solution is not the most efficient one but hope it helps.</p>
",2,1,2859,2017-08-21 06:47:32,https://stackoverflow.com/questions/45790815/compare-the-bag-of-words-in-two-document-and-find-the-matching-word-and-their-fr
Extract text from search result URLs using R,"<p>I know R a bit, but not a pro. I am working on a text-mining project using R. </p>

<p>I searched Federal Reserve website with a keyword, say ‘inflation’. The second page of the search result has the URL: (<a href=""https://search.newyorkfed.org/board_public/search?start=10&amp;Search=&amp;number=10&amp;text=inflation"" rel=""nofollow noreferrer"">https://search.newyorkfed.org/board_public/search?start=10&amp;Search=&amp;number=10&amp;text=inflation</a>). </p>

<p>This page has 10 search results (10 URLs). I want to write a code in R which will ‘read’ the page corresponding to each of those 10 URLs and extract the texts from those web pages to .txt files. My only input is the above mentioned URL. </p>

<p>I appreciate your help. If there is any similar older post, please refer me that too. Thank you.  </p>
","r, web-scraping, nlp, text-mining","<p>This is a basic idea of how to go about scrapping this pages. Though it might be slow in r if there are many pages to be scrapped.
Now your question is a bit ambiguous. You want the end results to be <strong>.txt</strong> files. What of the webpages that has pdf??? Okay. you can still use this code and change the file extension to pdf for the webpages that have pdfs.</p>

<pre><code> library(xml2)
 library(rvest)

 urll=""https://search.newyorkfed.org/board_public/search?start=10&amp;Search=&amp;number=10&amp;text=inflation""

  urll%&gt;%read_html()%&gt;%html_nodes(""div#results a"")%&gt;%html_attr(""href"")%&gt;%
       .[!duplicated(.)]%&gt;%lapply(function(x) read_html(x)%&gt;%html_nodes(""body""))%&gt;%  
         Map(function(x,y) write_html(x,tempfile(y,fileext="".txt""),options=""format""),.,
           c(paste(""tmp"",1:length(.))))
</code></pre>

<p>This is the breakdown of the code above:
The <em>url</em> you want to scrap from:</p>

<pre><code> urll=""https://search.newyorkfed.org/board_public/search?start=10&amp;Search=&amp;number=10&amp;text=inflation""
</code></pre>

<p>Get all the <em>url's</em> that you need:</p>

<pre><code>  allurls &lt;- urll%&gt;%read_html()%&gt;%html_nodes(""div#results a"")%&gt;%html_attr(""href"")%&gt;%.[!duplicated(.)]
</code></pre>

<p>Where do you want to save your texts?? Create the temp files:</p>

<pre><code> tmps &lt;- tempfile(c(paste(""tmp"",1:length(allurls))),fileext="".txt"")
</code></pre>

<p>as per now. Your <code>allurls</code> is in class character. You have to change that to xml in order to be able to scrap them. Then finally write them into the tmp files created above:</p>

<pre><code>  allurls%&gt;%lapply(function(x) read_html(x)%&gt;%html_nodes(""body""))%&gt;%  
         Map(function(x,y) write_html(x,y,options=""format""),.,tmps)
</code></pre>

<p>Please do not leave anything out. For example after <code>...""format""),</code> there is a period. Take that into consideration. 
Now your files have been written in the <strong>tempdir</strong>. To determine where they are, just type the command <code>tempdir()</code> on the console and it should give you the location of your files. At the same time, you can change the location of the files on scrapping within the <code>tempfile</code> command. </p>

<p>Hope this helps.</p>
",0,0,2160,2017-08-27 20:19:58,https://stackoverflow.com/questions/45908989/extract-text-from-search-result-urls-using-r
c a Corpus using rep or replicate or similar,"<p>I have a small corpus e.g.</p>

<pre><code>myvec &lt;- c(""n417"", ""disturbance"", ""grand theft auto"", ""assault"", ""burglary"", 
""vandalism"", ""atmt to locate"", ""drug arrest"", ""traffic stop"", 
""larceny"", ""graffiti complaint / reporting"")

corpus &lt;- VCorpus(VectorSource(myvec))
</code></pre>

<p>If I wanted to make corpus 10 times bigger, how would I do that so that the resulting variable is a VCorpus and not a list?</p>

<p>Tried:</p>

<pre><code>corpus &lt;- replicate(10, corpus) # returns a list
corpus &lt;- VCorpus(replicate(10, corpus)) # Error: inherits(x, ""Source"") is not TRUE
corpus &lt;- c(corpus, corpus, corpus, corpus, corpus, corpus, corpus) # works, returns a corpus 7 times bigger but involves lots of typing)
</code></pre>

<p>If I have a small corpus and I want to make it ten times larger for example purposes, how could I do that?</p>
","r, text-mining, corpus","<p>We can use <code>do.call</code> with <code>c</code> after replicating</p>

<pre><code>library(tm)
do.call(c, rep(list(corpus), 7))
# &lt;&lt;VCorpus&gt;&gt;
#Metadata:  corpus specific: 0, document level (indexed): 0
#Content:  documents: 77
</code></pre>

<p>Similarly for <code>replicate</code></p>

<pre><code>do.call(c, replicate(7, corpus, simplify = FALSE))
#&lt;&lt;VCorpus&gt;&gt;
#Metadata:  corpus specific: 0, document level (indexed): 0
#Content:  documents: 77
</code></pre>

<p>The <code>simplify = FALSE</code> is not needed here with <code>replicate</code></p>

<pre><code>do.call(c, replicate(7, corpus))
#&lt;&lt;VCorpus&gt;&gt;
#Metadata:  corpus specific: 0, document level (indexed): 0
#Content:  documents: 77
</code></pre>
",2,2,38,2017-08-28 06:55:27,https://stackoverflow.com/questions/45913283/c-a-corpus-using-rep-or-replicate-or-similar
hierarchical clustering with character n-grams in R,"<p>I have seven texts, and I want to see how (dis)similar they are on the basis of character trigrams. </p>

<p>I have extracted all the character trigrams from each text. So I have seven vectors like this:</p>

<pre><code>text1 &lt;- (""aaa"", ""abc"", ""bce"", ""cef"", ""efg"", ...)
text2 &lt;- (""aaa"", ""abc"", ""dce"", ""lmm"", ...)
</code></pre>

<p>etc.</p>

<p>How do I compare the percentage of shared trigrams among each text?</p>

<p>Is there a standard way of creating a distance matrix for character trigrams? </p>
","r, text-mining, hierarchical-clustering, n-gram","<h3>Reproducible data</h3>

<pre><code>t1 &lt;- letters[1:10]
t2 &lt;- letters[1:9]
t3 &lt;- letters[1:8]
t4 &lt;- letters[1:7]
</code></pre>

<h3>base R solution</h3>

<pre><code>maxval &lt;- 4   # number of trigram vectors
all.combs &lt;- expand.grid(1:maxval, 1:maxval) %&gt;% setNames(c(""A"",""B""))  # makes all combinations, including self &lt;-&gt; self comparison
</code></pre>

<p>The following calculates length of <code>intersect</code> between vectors and divides by <code>max(length(v1), length(v2))</code> for all pairwise combination of vectors</p>

<pre><code>P &lt;- sapply(1:nrow(all.combs), function(x) length(intersect(get(paste0(""t"", all.combs$A[x])), get(paste0(""t"", all.combs$B[x])))) / max(length(get(paste0(""t"", all.combs$A[x]))), length(get(paste0(""t"", all.combs$B[x])))))  
</code></pre>

<p>Convert to matrix</p>

<pre><code>M &lt;- matrix(P, ncol=maxval)
</code></pre>

<h3>Output</h3>

<pre><code>     [,1]      [,2]      [,3]      [,4]
[1,]  1.0 0.9000000 0.8000000 0.7000000
[2,]  0.9 1.0000000 0.8888889 0.7777778
[3,]  0.8 0.8888889 1.0000000 0.8750000
[4,]  0.7 0.7777778 0.8750000 1.0000000
</code></pre>
",3,0,392,2017-08-28 21:00:40,https://stackoverflow.com/questions/45927297/hierarchical-clustering-with-character-n-grams-in-r
"LDA$new model constructor text2vec R package error: Error in .subset2(public_bind_env, &quot;initialize&quot;)(...) : unused argument (...)","<p>The error is:</p>

<pre><code>&gt; lda_model = LDA$new(n_topics = 3, vocabulary = vocab, doc_topic_prior = 0.1, topic_word_prior = 0.01)
Error in .subset2(public_bind_env, ""initialize"")(...) : 
  unused argument (vocabulary = list(term = c(""normal"", ""bobo"", ""lixo"", ""sozinho"", ""triste"", ""santo"", ""dificil"", ""homem"", ""querido"", ""doido"", ""puta"", ""namorado"", ""viciado"", ""grosso"", ""anjo"", ""maravilhoso"", ""otario"", ""ciumento"", ""feio"", ""pessimo"", ""idiota"", ""bonito"", ""capaz"", ""otimo"", ""pior"", ""serio"", ""foda"", ""ruim"", ""fofo"", ""merda"", ""lerdo"", ""novo"", ""velho"", ""mal"", ""chato"", ""legal"", ""feliz"", ""burro"", ""unico"", ""trouxa"", ""boa"", ""ninguem"", ""lindo"", ""melhor"", ""amigo"", ""louco"", ""apaixonado""), term_count = c(205, 
215, 219, 222, 223, 232, 235, 241, 251, 261, 263, 264, 274, 276, 280, 280, 282, 284, 305, 311, 323, 333, 352, 354, 355, 363, 369, 380, 397, 405, 411, 421, 434, 458, 544, 577, 589, 628, 638, 690, 796, 826, 896, 936, 1177, 1251, 1344), doc_count = c(191, 187, 166, 212, 196, 214, 218, 219, 231, 205, 239, 230, 249, 235, 242, 253, 258, 256, 242, 278, 296, 275, 310, 314, 332, 319, 324, 345, 315, 341, 339, 356, 365, 409, 466, 480, 500, 525, 577, 557, 670, 707, 702, 785, 972, 981, 
&gt; 
</code></pre>

<p>I'm getting the error when running <code>LDA$new</code> after vocabulary, tokenizer and dtm were successfully created. The full code is:</p>

<pre><code>current_dir_files = list.files(path = ""."", full.names = TRUE)
files_iterator = ifiles(current_dir_files, reader = read_file)
it_tokens &lt;- itoken(files_iterator, preprocess_function = tolower, 
                   tokenizer = word_tokenizer, progressbar = TRUE)
vocab &lt;- create_vocabulary(it_tokens, stopwords = words.remove) %&gt;%
    prune_vocabulary(term_count_min = 200, doc_proportion_max = 0.1) 

vec &lt;- vocab_vectorizer(vocabulary = vocab)

dtm &lt;- create_dtm(it = it_tokens, vectorizer = vec)

lda_model = LDA$new(n_topics = 3, vocabulary = vocab, doc_topic_prior = 0.1, topic_word_prior = 0.01)
</code></pre>

<p>I'm using ‘text2vec’ version 0.5.0, R 3.4.1 64bits, RStudio 1.0.153.</p>
","r, nlp, text-mining, lda, text2vec","<p>Please check documentation - <code>?LDA</code> and <a href=""http://text2vec.org/topic_modeling.html#latent_dirichlet_allocation"" rel=""nofollow noreferrer"">http://text2vec.org/topic_modeling.html#latent_dirichlet_allocation</a>. Signature of function was changed since text2vec 0.4, now there should not be <code>vocabulary</code> argument.</p>
",1,1,585,2017-08-29 01:10:09,https://stackoverflow.com/questions/45929291/ldanew-model-constructor-text2vec-r-package-error-error-in-subset2public-bin
How to apply a custom function to a quanteda corpus,"<p>I'm trying to migrate a script from using tm to quanteda. Reading the quanteda documentation there is a philosophy about applying changes ""downstream"" so that the original corpus is unchanged. OK.</p>

<p>I previously wrote a script to find spelling mistakes in our tm corpus and had support from our team to create a manual lookup. So, I have a csv file with 2 columns, the first column is the misspelt term and the second column is the correct version of that term. </p>

<p>Using tm package previously I did this:</p>

<pre><code># Write a custom function to pass to tm_map
# ""Spellingdoc"" is the 2 column csv
library(stringr)
library(stringi)
library(tm)
stringi_spelling_update &lt;- content_transformer(function(x, lut = spellingdoc) stri_replace_all_regex(str = x, pattern = paste0(""\\b"", lut[,1], ""\\b""), replacement = lut[,2], vectorize_all = FALSE))
</code></pre>

<p>Then within my tm corpus transformations I did this:</p>

<pre><code>mycorpus &lt;- tm_map(mycorpus, function(i) stringi_spelling_update(i, spellingdoc))
</code></pre>

<p>What is the equivilent way to apply this custom function to my quanteda corpus?</p>
","r, text-mining, quanteda","<p>Impossible to know if that will work from your example, which leaves some parts out, but generally:</p>

<p>If you want to access texts in a <strong>quanteda</strong> corpus, you can use <code>texts()</code>, and to replace those texts, <code>texts()&lt;-</code>.</p>

<p>So in your case, assuming that <code>mycorpus</code> is a <strong>tm</strong> corpus, you could do this:</p>

<pre><code>library(""quanteda"")
stringi_spelling_update2 &lt;- function(x, lut = spellingdoc) {
    stringi::stri_replace_all_regex(str = x, 
                                    pattern = paste0(""\\b"", lut[,1], ""\\b""), 
                                    replacement = lut[,2], 
                                    vectorize_all = FALSE)
}

myquantedacorpus &lt;- corpus(mycorpus)
texts(mycorpus) &lt;- stringi_spelling_update2(texts(mycorpus), spellingdoc)
</code></pre>
",1,1,422,2017-08-30 06:10:40,https://stackoverflow.com/questions/45953130/how-to-apply-a-custom-function-to-a-quanteda-corpus
Parsing names in mixed formats using R,"<p>I have a list of names in mixed formats, which I would like to separate into columns containing first and last names in R.  An example dataset:</p>

<pre><code>Names &lt;- c(""Mary Smith"",""Hernandez, Maria"",""Bonds, Ed"",""Michael Jones"")
</code></pre>

<p>The goal is to assemble a dataframe that contains names in a format like this:</p>

<pre><code>FirstNames &lt;- c(""Mary"",""Maria"",""Ed"",""Michael"")
LastNames &lt;- c(""Smith"",""Hernandez"",""Bonds"",""Jones"")
FinalData &lt;- data.frame (FirstNames, LastNames)
</code></pre>

<p>I tried a few approaches to select either the First or Last name based on whether the names are separated by a space only versus comma-space.  For instance I wanted to use regular expressions in gsub to copy first names from rows in which a comma-space separates the names:</p>

<pre><code>FirstNames2 &lt;- gsub ("".*,\\s"","""",Names)
</code></pre>

<p>This worked for rows that contained names in the <em>LastName, FirstName</em> format, but gsub collected the entire contents in rows with names in <em>FirstName LastName</em> format.</p>

<p>So my request for help is to please advise: How would you tackle this problem?  Thanks to all in advance!</p>
","r, regex, text-mining","<p>Here is a one-liner. The pattern tries <em>Firstname lastname</em> first and if that fails it tries <em>lastname, firstname</em>. No packages are used.</p>

<pre><code>read.table(text = sub(""(\\w+) (\\w+)|(\\w+), (\\w+)"", ""\\1\\4 \\2\\3"", Names), as.is=TRUE)
</code></pre>

<p>giving:</p>

<pre><code>       V1        V2
1    Mary     Smith
2   Maria Hernandez
3      Ed     Bonds
4 Michael     Jones
</code></pre>
",4,1,384,2017-08-30 14:35:13,https://stackoverflow.com/questions/45963250/parsing-names-in-mixed-formats-using-r
Extracting dates that are in different formats using regex and sorting them - pandas,"<p>I am new to text mining and I need to extract the dates from a *.txt file and sort them. The dates are in between the sentences ( each line) and their format can potentially be as follows: </p>

<pre><code>04/20/2009; 04/20/09; 4/20/09; 4/3/09
Mar-20-2009; Mar 20, 2009; March 20, 2009; Mar. 20, 2009; Mar 20 2009;
20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009
Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009
Feb 2009; Sep 2009; Oct 2010
6/2008; 12/2009
2009; 2010
</code></pre>

<p>If the day is missing consider the 1st and if the month is missing consider January. </p>

<p>My idea is to extract all dates and convert that into mm/dd/yyyy format. However I am a bit doubtful on how to find and replace paterns. This is what i have done :</p>

<pre><code>import pandas as pd

doc = []
with open('dates.txt') as file:
    for line in file:
        doc.append(line)

df = pd.Series(doc)

df2 = pd.DataFrame(df,columns=['text'])

def myfunc(x):
    if len(x)==4:
        x = '01/01/'+x
    else:
        if not re.search('/',x):
            example = re.sub('[-]','/',x)
            terms = re.split('/',x)
            if (len(terms)==2):
                if len(terms[-1])==2:
                    x = '01/'+terms[0]+'/19'+terms[-1]
                else:
                    x = '01/'+terms[0]+'/'+terms[-1] 
            elif len(terms[-1])==2:
                x = terms[0].zfill(2)+'/'+terms[1].zfill(2)+'/19'+terms[-1]
    return x

df2['text'] = df2.text.str.replace(r'(((?:\d+[/-])?\d+[/-]\d+)|\d{4})', lambda x: myfunc(x.groups('Date')[0]))
</code></pre>

<p>I have done it only for the numerical dates format. But I am a bit confused how to do it with the alfanumerical dates.</p>

<p>I know is a rough code but this is just what I got.</p>
","python, pandas, date, dataframe, text-mining","<p>I think this is one of the coursera text mining assignment. Well you can use regex and extract to get the solution. <a href=""https://www.dropbox.com/s/1w0yrc6g7yulofe/dates.txt?dl=0"" rel=""noreferrer"">dates.txt</a> i.e </p>

<pre><code>doc = []
with open('dates.txt') as file:
    for line in file:
        doc.append(line)

df = pd.Series(doc)

def date_sorter():
    # Get the dates in the form of words
    one = df.str.extract(r'((?:\d{,2}\s)?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*(?:-|\.|\s|,)\s?\d{,2}[a-z]*(?:-|,|\s)?\s?\d{2,4})')
    # Get the dates in the form of numbers
    two = df.str.extract(r'((?:\d{1,2})(?:(?:\/|-)\d{1,2})(?:(?:\/|-)\d{2,4}))')
    # Get the dates where there is no days i.e only month and year  
    three = df.str.extract(r'((?:\d{1,2}(?:-|\/))?\d{4})')
    #Convert the dates to datatime and by filling the nans in two and three. Replace month name because of spelling mistake in the text file.
    dates = pd.to_datetime(one.fillna(two).fillna(three).replace('Decemeber','December',regex=True).replace('Janaury','January',regex=True))
return pd.Series(dates.sort_values())

date_sorter()
</code></pre>

<p>Output:</p>

<pre>
9     1971-04-10
84    1971-05-18
2     1971-07-08
53    1971-07-11
28    1971-09-12
474   1972-01-01
153   1972-01-13
13    1972-01-26
129   1972-05-06
98    1972-05-13
111   1972-06-10
225   1972-06-15
31    1972-07-20
171   1972-10-04
191   1972-11-30
486   1973-01-01
335   1973-02-01
415   1973-02-01
36    1973-02-14
405   1973-03-01
323   1973-03-01
422   1973-04-01
375   1973-06-01
380   1973-07-01
345   1973-10-01
57    1973-12-01
481   1974-01-01
436   1974-02-01
104   1974-02-24
299   1974-03-01
</pre>

<p>If you want to return only the index then <code>return pd.Series(dates.sort_values().index)</code></p>

<p>Parsing of first regex </p>

<pre>
 #?: Non-capturing group 

((?:\d{,2}\s)? # The two digits group. `?` refers to preceding token or group. Here the digits of 2 or 1 and space occurring once or less.  

 (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* # The words in group ending with any letters `[]` occuring any number of times (`*`). 

 (?:-|\.|\s|,) # Pattern matching -,.,space 

 \s? #(`?` here it implies only to space i.e the preceding token)

 \d{,2}[a-z]* # less than or equal to two digits having any number of letters at the end (`*`). (Eg: may be 1st, 13th , 22nd , Jan , December etc ) . 

 (?:-|,|\s)?# The characters -/,/space may occur once and may not occur because of `?` at the end

 \s? # space may occur or may not occur at all (maximum is 1) (`?` here it refers only to space)

 \d{2,4}) # Match digit which is 2 or 4   
</pre>

<p>Hope it helps. </p>
",18,2,7713,2017-09-05 22:26:23,https://stackoverflow.com/questions/46064162/extracting-dates-that-are-in-different-formats-using-regex-and-sorting-them-pa
python string index out of range (a function works on one data won&#39;t work on another,"<pre><code>def start_with_num(data,col):
    """"""
    This function is to get the information of whether the title starts with numbers
    """"""

    num=[]
    for item in data[col]:
        if item[0].isdigit() == True:
            num.append(1)
        else:
            num.append(0)

    return num
</code></pre>

<p>This is my code, it works perfectly fine on 2500 lines of train data</p>

<p>However, when I implemented it to the test data which contains 20,000 it 
gives me an error saying the string index is out of range</p>

<p>The function is to count how many strings start with a number by the way.</p>

<p>Here is the sample data on the column</p>

<pre><code>0    UK’s response to modern slavery leaving victim...
1                                         this is good
2    The ""forgotten"" Trump roast: Relive his brutal...
3                 Meet the happiest #dog in the world!
4    Tokyo's subway is shut down amid fears over an...
5               Ban lifted on Madrid doping laboratory
6    Despite the ‘Yuck Factor,’ Leeches Are Big in ...
7    #China and #Pakistan have cemented their polit...
8    Malls are dying, but it's hard to profit from ...
9    Filipino troops kill notorious Abu Sayyaf kidn...
</code></pre>
","python, nlp, nltk, text-mining, data-manipulation","<p>It seems there is an empty string in your data. You have to look for the string legnth to:</p>

<pre><code>def start_with_num(data,col):
""""""
This function is to get the information of whether the title starts with 
numbers
""""""


num=[]
for item in data[col]:
    if len(item) &gt; 0 and item[0].isdigit() == True:
        num.append(1)
    else:
        num.append(0)

return num
</code></pre>
",0,0,82,2017-09-06 07:01:41,https://stackoverflow.com/questions/46068802/python-string-index-out-of-range-a-function-works-on-one-data-wont-work-on-ano
Strategy for computing PMI from counts: dataframes or matrices,"<p>I need to compute PMI scores for co-occurrences of bio-entities e.g. <code>Gene A - Gene B</code>, or <code>Gene C - Disease A</code>. Co-occurrences have been extracted from <a href=""https://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/PubTator/curator_identifier.cgi?page=1&amp;Species_display=1&amp;Chemical_display=1&amp;Gene_display=1&amp;Disease_display=1&amp;Mutation_display=1&amp;pmid=15803373"" rel=""nofollow noreferrer"">Pubtator</a>. I use Python 3.</p>

<p>For a set of documents, I have extracted the individual counts <code>freq(x)</code> and <code>freq(y)</code> of all entities by co-occurrence category e.g. <code>Gene-Gene</code> or <code>Gene-Disease</code>, and I have the co-occurrence counts of entity pairs <code>freq(x,y)</code>. All counts are stored in a <code>Dict</code>. </p>

<p>What would be the best approach for computing Pointwise Mutual Information (PMI) scores from the raw counts: </p>

<ul>
<li>Create two data frames (one for individual counts and one for co-occurrence counts) </li>
<li>Create two matrices? (same as above)</li>
<li>another approach? </li>
</ul>

<p>Consider that one set of data has 3 columns: <code>entity, category, count</code> and the other set has 4 columns: <code>entity_a, category, entity_b, count</code> where category represents the co-occurrence category. I need the category for the individual entity counts because if I were to use their overall total counts it would distort the results for a given co-occurrence type.</p>

<p>I have attempted the data frame approach but can't figure out a way how to create a new PMI column that computes the result using two different data frames (DFs) hence I thought maybe a matrix approach may work better? If so, why?</p>

<p>Examples of the data when transformed into DFs:</p>

<p>df1.head():</p>

<p><code>
       ent        rel count
177   5197  Gene_Gene     2
176  56744  Gene_Gene     2
175  12766  Gene_Gene     2
174   3091  Gene_Gene     2
173   3162  Gene_Gene     2
</code></p>

<p>df2.head():</p>

<p><code>
     ent_a        rel  ent_b count
247   5197  Gene_Gene  56744     1
246  12766  Gene_Gene   5197     1
245  12766  Gene_Gene  56744     1
244   3091  Gene_Gene   3162     1
243   3091  Gene_Gene  54583     1
</code></p>

<p>The PMI formula :  </p>

<p><a href=""https://i.sstatic.net/jeN4o.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jeN4o.gif"" alt=""enter image description here""></a></p>
","python, nlp, text-mining","<p>The strategy I found to work for me consists of using both DataFrames (DFs) and numpy arrays. </p>

<p>In a first step, DFs were useful for looking up and populating <code>df2</code> with the respective individual counts of the co-occurring entities. Then, in a second step, numpy arrays were used to efficiently compute the (approximated) <code>PMI</code> score for each of the co-occurrences. </p>

<p><strong>Step 1: Lookup Individual Counts</strong></p>

<ul>
<li><p>first split the co-occurring entities and add each to a new column e.g.:</p>

<p><code>df_ab['ent_a'] = df_ab.ent_ab.apply(lambda x: x.split('-')[0])</code></p></li>
<li><p>then call a lookup function to get the individual counts e.g.:</p>

<p><code>df_ab['ent_a_count'] = df_ab.ent_a.apply(get_ent, args=(df_a, 'ent_a', ))</code></p>

<p>where the lookup function looks like this:</p>

<p><code>
def get_ent(ent_df_ab, df_a, colname_df_ab):
    row_df_a = df_a[df_a[colname_df_ab] == ent_df_ab]
    i = row_df_a.iloc[0]['count']
    return i
</code></p></li>
</ul>

<p><code>df2</code> now looks like this</p>

<pre><code>```
        ent_ab  count_ab       type  ent_a  ent_b  ent_a_count  ent_b_count
0   5197-56744         2  Gene_Gene   5197  56744            2            2
1   12766-5197         1  Gene_Gene  12766   5197            2            1
2  12766-56744         1  Gene_Gene  12766  56744            2            2
3    3091-3162         4  Gene_Gene   3091   3162            6            1
4   3091-54583         2  Gene_Gene   3091  54583            6            1
```
</code></pre>

<p><strong>Step 2: Vectorized PMI Calculation</strong></p>

<ul>
<li><p>numpy array-based function for computing scores </p>

<p><code>
def compute_pmi(df):
    count_ab = np.array(df[['count_ab']])
    ent_a_count = np.array(df[['ent_a_count']])
    ent_b_count = np.array(df[['ent_b_count']])
    pmi = np.round(count_ab / (ent_a_count * ent_b_count), 3)
    df['pmi'] = pmi
    return df
</code></p></li>
</ul>
",2,1,1789,2017-09-06 09:14:58,https://stackoverflow.com/questions/46071270/strategy-for-computing-pmi-from-counts-dataframes-or-matrices
A lemmatizing function using a hash dictionary does not work with tm package in R,"<p>I would like to lemmatize Polish text using a large external dictionary (format like in txt variable below). I am not lucky, to have an option Polish with popular text mining packages. The answer <a href=""https://stackoverflow.com/a/45790325/3480717"">https://stackoverflow.com/a/45790325/3480717</a> by @DmitriySelivanov works well with simple vector of texts. (I have also removed Polish diacritics from both the dictionary and corpus.) The function works well with a vector of texts.</p>

<p>Unfortunately it does not work with the corpus format generated by tm. Let me paste Dmitriy's code:</p>

<pre><code>library(hashmap)
library(data.table)
txt = 
  ""Abadan  Abadanem
  Abadan  Abadanie
  Abadan  Abadanowi
  Abadan  Abadanu
  abadańczyk  abadańczycy
  abadańczyk  abadańczykach
  abadańczyk  abadańczykami
  ""
dt = fread(txt, header = F, col.names = c(""lemma"", ""word""))
lemma_hm = hashmap(dt$word, dt$lemma)

lemma_hm[[""Abadanu""]]
#""Abadan""


lemma_tokenizer = function(x, lemma_hashmap, 
                           tokenizer = text2vec::word_tokenizer) {
  tokens_list = tokenizer(x)
  for(i in seq_along(tokens_list)) {
    tokens = tokens_list[[i]]
    replacements = lemma_hashmap[[tokens]]
    ind = !is.na(replacements)
    tokens_list[[i]][ind] = replacements[ind]
  }
  tokens_list
}
texts = c(""Abadanowi abadańczykach OutOfVocabulary"", 
          ""abadańczyk Abadan OutOfVocabulary"")
lemma_tokenizer(texts, lemma_hm)

#[[1]]
#[1] ""Abadan""          ""abadańczyk""      ""OutOfVocabulary""
#[[2]]
#[1] ""abadańczyk""      ""Abadan""          ""OutOfVocabulary""
</code></pre>

<p><strong>now I would like to apply it on tm corpus ""docs""</strong> here is an example syntax I would use with tm package, on tm generated corpus.</p>

<pre><code>docs &lt;- tm_map(docs, function(x) lemma_tokenizer(x, lemma_hashmap=""lemma_hm""))
</code></pre>

<p>another syntax that I tried:</p>

<pre><code>LemmaTokenizer &lt;- function(x) lemma_tokenizer(x, lemma_hashmap=""lemma_hm"")

docsTDM &lt;-
  DocumentTermMatrix(docs, control = list(wordLengths = c(4, 25), tokenize=LemmaTokenizer))
</code></pre>

<p>It throws at me an error:</p>

<pre><code> Error in lemma_hashmap[[tokens]] : 
  attempt to select more than one element in vectorIndex 
</code></pre>

<p>The function works with a vector of texts but it will not work with tm corpus. Thanks in advance for suggestions (even use of this function with other text mining package if it will not work with tm).</p>
","r, text-mining, tm, quanteda, text2vec","<p>I see two problems here. 1) your custom function returns a list, while it should return a vector of strings; and 2) you are passing a wrong lemma_hashmap argument.</p>

<p>A quick workaround to fix the first problem is to use paste() and sapply() before returning the function result.</p>

<pre><code>lemma_tokenizer = function(x, lemma_hashmap, 
                           tokenizer = text2vec::word_tokenizer) {
  tokens_list = tokenizer(x)
  for(i in seq_along(tokens_list)) {
    tokens = tokens_list[[i]]
    replacements = lemma_hashmap[[tokens]]
    ind = !is.na(replacements)
    tokens_list[[i]][ind] = replacements[ind]
  }

  # paste together, return a vector
  sapply(tokens_list, (function(i){paste(i, collapse = "" "")}))
}
</code></pre>

<p>We can run the same example of your post.</p>

<pre><code>texts = c(""Abadanowi abadańczykach OutOfVocabulary"", 
          ""abadańczyk Abadan OutOfVocabulary"")
lemma_tokenizer(texts, lemma_hm)
[1] ""Abadan abadańczyk OutOfVocabulary"" ""abadańczyk Abadan OutOfVocabulary""
</code></pre>

<p>Now, we can use tm_map. Just make sure to use lemma_hm (i.e., the variable) and not ""lemma_hm"" (a string) as argument.</p>

<pre><code>docs &lt;- SimpleCorpus(VectorSource(texts))
out &lt;- tm_map(docs, (function(x) {lemma_tokenizer(x, lemma_hashmap=lemma_hm)}))
out[[1]]$content
[1] ""Abadan abadańczyk OutOfVocabulary""
</code></pre>
",2,2,1353,2017-09-08 18:30:19,https://stackoverflow.com/questions/46122591/a-lemmatizing-function-using-a-hash-dictionary-does-not-work-with-tm-package-in
Error in extracting phrases using Gensim,"<p>I am trying to get the bigrams in the sentences using Phrases in Gensim as follows.</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser
documents = [""the mayor of new york was there"", ""machine learning can be useful sometimes"",""new york mayor was present""]

sentence_stream = [doc.split("" "") for doc in documents]
#print(sentence_stream)
bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
bigram_phraser = Phraser(bigram)

for sent in sentence_stream:
    tokens_ = bigram_phraser[sent]
    print(tokens_)
</code></pre>

<p>Even though it catches ""new"", ""york"" as ""new york"", it does not catch ""machine"", learning as ""machine learning""</p>

<p>However, in the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">example shown in Gensim Website</a> they were able to catch the words ""machine"", ""learning"" as ""machine learning"".</p>

<p>Please let me know how to get ""machine learning"" as a bigram in the above example</p>
","python, data-mining, text-mining, word2vec, gensim","<p>The technique used by gensim <code>Phrases</code> is purely based on statistics of co-occurrences: how often words appear together, versus alone, in a formula also affected by <code>min_count</code> and compared against the <code>threshold</code> value. </p>

<p>It is only because your training set has 'new' and 'york' occur alongside each other twice, while other words (like 'machine' and 'learning') only occur alongside each other once, that 'new_york' becomes a bigram, and other pairings do not. What's more, even if you did find a combination of <code>min_count</code> and <code>threshold</code> that would promote 'machine_learning' to a bigram, it would <em>also</em> pair together every other bigram-that-appears-once – which is probably not what you want.</p>

<p>Really, to get good results from these statistical techniques, you need lots of varied, realistic data. (Toy-sized examples may superficially succeed, or fail, for superficial toy-sized reasons.) </p>

<p>Even then, they will tend to miss combinations a person would consider reasonable, and make combinations a person wouldn't. Why? Because our minds have much more sophisticated ways (including grammar and real-world knowledge) for deciding when clumps of words represent a single concept. </p>

<p>So even with more better data, be prepared for nonsensical n-grams. Tune or judge the model on whether it is overall improving on your goal, not any single point or ad-hoc check of matching your own sensibility.</p>

<p>(Regarding the referenced gensim documentation comment, I'm pretty sure that if you try <code>Phrases</code> on just the two sentences listed there, it won't find any of the desired phrases – not 'new_york' or 'machine_learning'. As a figurative example, the ellipses <code>...</code> imply the training set is larger, and the results indicate that the extra unshown texts are important. It's just because of the 3rd sentence you've added to your code that 'new_york' is detected. If you added similar examples to make 'machine_learning' look more like a statistically-outlying pairing, your code could promote 'machine_learning', too.)</p>
",6,3,1828,2017-09-10 05:27:14,https://stackoverflow.com/questions/46137572/error-in-extracting-phrases-using-gensim
Error getting trigrams using gensim&#39;s Phrases,"<p>I want to extract all bigrams and trigrams of the given sentences. </p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""Human Computer Interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]

sentence_stream = [doc.split("" "") for doc in documents]
bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
trigram = Phrases(bigram(sentence_stream, min_count=1, threshold=2, delimiter=b' '))

for sent in sentence_stream:
    #print(sent)
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>The code works fine for bigrams and capture 'new york' and 'machine learning' ad bigrams.</p>

<p>However, I get the following error when I try to insert trigrams.</p>

<pre><code>TypeError: 'Phrases' object is not callable
</code></pre>

<p>Please let me know, how to correct my code.</p>

<p>I am following the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">example documentation</a> of gensim.</p>
","python, nlp, data-mining, text-mining, gensim","<p>According to the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">docs</a>, you can do:</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser 

phrases = Phrases(sentence_stream)
bigram = Phraser(phrases)
trigram = Phrases(bigram[sentence_stream])
</code></pre>

<p><code>bigram</code>, being a <code>Phrases</code> object, cannot be called again, as you are doing so.</p>
",1,0,1833,2017-09-11 01:14:35,https://stackoverflow.com/questions/46147013/error-getting-trigrams-using-gensims-phrases
Issues in getting trigrams using Gensim,"<p>I want to get bigrams and trigrams from the example sentences I have mentioned.</p>

<p>My code works fine for bigrams. However, it does not capture trigrams in the data (e.g., human computer interaction, which is mentioned in 5 places of my sentences)</p>

<p><strong>Approach 1</strong> Mentioned below is my code using Phrases in Gensim.</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=1, delimiter=b' ')
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p><strong>Approach 2</strong> I even tried to use Phraser and Phrases both, but it didn't work.</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
bigram_phraser = Phraser(bigram)
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>Please help me to fix this issue of getting trigrams.</p>

<p>I am following the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""noreferrer"">example documentation</a> of Gensim.</p>
","python, data-mining, text-mining, word2vec, gensim","<p>I was able to get bigrams and trigrams with a few modifications to your code:
</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')
trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')

for sent in sentence_stream:
    bigrams_ = [b for b in bigram[sent] if b.count(' ') == 1]
    trigrams_ = [t for t in trigram[bigram[sent]] if t.count(' ') == 2]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>I removed the <code>threshold = 1</code> parameter from the bigram <code>Phrases</code> because otherwise it seems to form weird digrams that allow the construction of weird trigrams (notice that <code>bigram</code> is used to build the trigram <code>Phrases</code>); this parameter would probably come useful when you have more data. For trigrams, the <code>min_count</code> parameter also needs to be specified because it defaults to 5 if not provided.</p>

<p>In order to retrieve the bigrams and trigrams of each document, you can use this list comprehension trick to filter elements that aren't formed by two or three words, respectively. </p>

<hr>

<p><strong>Edit</strong> - a few details about the <code>threshold</code> parameter:</p>

<p>This parameter is used by the estimator to determine if two words <em>a</em> and <em>b</em> form a phrase, and that is only if:
</p>

<pre><code>(count(a followed by b) - min_count) * N/(count(a) * count(b)) &gt; threshold
</code></pre>

<p>where <em>N</em> is the total vocabulary size. By default the parameter value is 10 (see <a href=""https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases"" rel=""noreferrer"">docs</a>). So, the higher the <code>threshold</code>, the harder the constraints for words to form phrases.</p>

<p>For example, in your first approach you were trying to use <code>threshold = 1</code>, so you would get <code>['human computer','interaction is']</code> as digrams of 3 out of your 5 sentences that begin with ""human computer interaction""; that weird second digram is a result of the more relaxed threshold. </p>

<p>Then, when you try to get trigrams with default <code>threshold = 10</code> you only get <code>['human computer interaction is']</code> for those 3 sentences, and nothing for the remaining two (filtered by threshold); and because that was a 4-gram instead of a trigram it would also be filtered by <code>if t.count(' ') == 2</code>. In case that, for example, you lower the trigram threshold to 1, you can get ['human computer interaction'] as trigram for the two remaining sentences. It doesn't seem easy to get a good combination of parameters, <a href=""https://stackoverflow.com/a/46143595/8507311"">here's</a> more about it. </p>

<p>I'm not an expert, so take this conclusion with a grain of salt: I think it's better to firstly get good digram results (not like 'interaction is') before moving on, as weird digrams can add confusion to further trigrams, 4-gram... </p>
",20,14,9081,2017-09-11 04:28:12,https://stackoverflow.com/questions/46148182/issues-in-getting-trigrams-using-gensim
Importing a Term Document Matrix in CSV format into R,"<p>So I already have a TDM but it was on excel. So I saved it as CSV. Now I want to do some analysis but I can´t load it as a TDM using tm package. My CSV looks something like this:</p>

<pre><code>           item01    item02    item03     item04


red         0          1         1           0
circle      1          0         0           1
fame        1          0         0           0
yellow      0          0         1           1 
square      1          0         1           0 
</code></pre>

<p>So I haven't been able to load that file as a TDM, the best I've tried so far is this :</p>

<pre><code>myDTM &lt;- as.DocumentTermMatrix(df, weighting = weightBin)
</code></pre>

<p>But it loads 1's on all cells </p>

<pre><code>&lt;&lt;DocumentTermMatrix (documents: 2529, terms: 1952)&gt;&gt;
Non-/sparse entries: 4936608/0
Sparsity           : 0%
Maximal term length: 27
Weighting          : binary (bin)
Sample             :

             Terms
Docs            item01 item02 item03 item04
      Red        1        1     1       1                
      Circle     1        1     1       1          
      fame       1        1     1       1   
</code></pre>

<p>I've tried converting first to Corpus and other things but if i try to use any function like inspect(tdm) it returns an error, like this or similar.</p>

<pre><code>Error in `[.simple_triplet_matrix`(x, docs, terms) :
</code></pre>

<p>I really don´t believe there isn't a way to import it in the right format, any suggestion? Thanks in advance.</p>
","r, csv, text-mining, text-analysis","<p>Try first converting the CSV to a sparse matrix.  My CSV is different from yours because I typed it myself, but it's the same idea. </p>

<pre><code>&gt; library(tm)
&gt; library(Matrix)
&gt; myDF &lt;- read.csv(""my.csv"",row.names=1,colClasses=c('character',rep('integer',4)))
&gt; mySM &lt;- Matrix(as.matrix(myDF),sparse=TRUE)
&gt; myDTM &lt;- as.DocumentTermMatrix(mySM,weighting = weightBin)
&gt; inspect(myDTM)

&lt;&lt;DocumentTermMatrix (documents: 5, terms: 4)&gt;&gt;
Non-/sparse entries: 7/13
Sparsity           : 65%
Maximal term length: 6
Weighting          : binary (bin)
Sample             :
        Terms
Docs     item01 item02 item03 item04
  circle      1      1      0      0
  fame        1      0      0      0
  red         0      0      0      0
  square      1      0      1      0
  yellow      0      0      1      1 
&gt; 
</code></pre>
",0,2,876,2017-09-13 17:52:12,https://stackoverflow.com/questions/46203939/importing-a-term-document-matrix-in-csv-format-into-r
Regular Expression in R for twitter username,"<p>I am looking to handle and text between <strong>@</strong> and <strong>:</strong> with out any space for example 
@rstat:.
I would like to form a regular expression to handle this.
I have tried <strong>^@.[A-z0-9_].:$</strong> but its not working.</p>

<p>Kindly help me here.</p>
","r, regex, twitter, rstudio, text-mining","<p>The <code>^@.[A-z0-9_].:$</code> pattern matches the start of string (<code>^</code>), then a <code>@</code>, then any char (with <code>.</code>), then letters, digits, <code>_</code>, <code>`</code>, <code>[</code>, <code>\</code>, <code>]</code>, <code>^</code>, then any char again, a <code>:</code> and end of string (<code>$</code>). So, it can match, say, a <a href=""https://regex101.com/r/Dk06Pq/2"" rel=""nofollow noreferrer""><code>@§`‘:</code></a> string.</p>

<p>You may use <em>stringr</em> <code>str_extract_all</code> like this</p>

<pre><code>str_extract_all(x, ""(?&lt;=@)[^\\s:]+"")
</code></pre>

<p>If you must check for the <code>:</code> presence, add a lookahead check:</p>

<pre><code>str_extract_all(x, ""(?&lt;=@)[^\\s:]+(?=:)"")
                                  ^^^^^
</code></pre>

<p>See the <a href=""https://regex101.com/r/Dk06Pq/1"" rel=""nofollow noreferrer"">regex demo</a>.</p>

<p><strong>Details</strong></p>

<ul>
<li><code>(?&lt;=@)</code> - a location in string that is immediately preceded with <code>@</code> symbol</li>
<li><code>[^\\s:]+</code> - 1 or more (due to <code>+</code>) chars other than whitespace and <code>:</code></li>
<li><code>(?=:)</code> - a positive lookahead that requires the presence of <code>:</code> immediately to the right of the current location.</li>
</ul>
",3,0,374,2017-09-14 16:29:33,https://stackoverflow.com/questions/46224205/regular-expression-in-r-for-twitter-username
Highlight single terms within a word cloud?,"<p>Is it possible to highlight single words within a word cloud using 'wordcloud' or 'wordcloud2'?  Does one have to add another column to the data frame as ordering factor?  </p>

<p>I couldn't find any simple solution.</p>

<p>Here's what I've done:</p>

<pre><code>wordcloud(text_process$words[1:n.words],
          text_process$frequency[1:n.words],
          scale = c(18, 0.5),
          colors = c(""#666666"", ""#3E6AA0"") [factor(text_process$matches[1:n.words])],
          use.r.layout = FALSE,
          rot.per = 0.2,
          random.order = FALSE, ordered.colors=TRUE)
</code></pre>

<p>I had to introduce a criterion (called 'matches') in the data frame 'text_process' that indicates the color. I was wondering whether there's a simpler way of highlighting specific words...</p>
","r, text-mining, word-cloud","<pre><code># Not Tested
library(randomcoloR)

cols&lt;-randomColor(length(unique(test_process$words[1:n.words])), luminosity = ""dark"")

match_value&lt;-match(""HighlightThisWord"", test_process$words[1:n.words])

cols[match_value]&lt;-""orange""

wordcloud(text_process$words[1:n.words],
      text_process$frequency[1:n.words],
      scale = c(18, 0.5),
      colors = cols,
      use.r.layout = FALSE,
      rot.per = 0.2,
      random.order = FALSE, ordered.colors=TRUE)
</code></pre>
",1,1,829,2017-09-20 14:25:08,https://stackoverflow.com/questions/46324710/highlight-single-terms-within-a-word-cloud
Text Mining PDFs - Convert List of Character Vectors (Strings) to Dataframe,"<p>I'm using text mining packages to read a group of PDF documents into plaintext, and I want to export this plaintext to a dataframe/CSV/text files 
(to facilitate further analysis with RTextTools)</p>

<p>First, I pulled PDF documents into a VCorpus using the <em>tm</em> package. The tm package's VCorpus object stores lists containing a ""PlainTextDocument"" and ""TextDocument"" object for metadata and plaintext. I.e. ""Metadata: DocumentName1""...  and the content, ""The terms of X are..."".</p>

<pre><code>   library(tm)

    docs &lt;- VCorpus(DirSource(getwd()),readerControl = list(reader = readPDF))
    # Creates large VCorpus containing ~700 PlainTextDocuments 
    # (which contain strings/character vectors)
</code></pre>

<p>Unclear how to process this into a dataframe, so I managed to hunt down a package with a utility function to convert it into a list of strings.</p>

<pre><code>   library(textreg)
   strings &lt;- convert.tm.to.character(docs)
   # Converts VCorpus to large list of strings with document content
</code></pre>

<p>From either the VCorpus or this list of strings, I'd like to create a data frame of just one row, each containing a document's text, with column names corresponding to their original filename. </p>

<p>First I looked at this page, <a href=""https://stackoverflow.com/questions/27594541/export-a-list-into-a-csv-or-txt-file-in-r"">Export a list into a CSV or TXT file in R</a>, and tried using <em>sapply</em>:</p>

<pre><code>df &lt;- data.frame(text = sapply(docs, as.character), stringsAsFactors = FALSE)
    ^Error during wrapup: arguments imply differing number of rows: 1, 5, 3, 3889, 3366
</code></pre>

<p>I've also found related threads (<a href=""https://stackoverflow.com/questions/24703920/r-tm-package-vcorpus-error-in-converting-corpus-to-data-frame"">R tm package vcorpus: Error in converting corpus to data frame</a>), but found them difficult since they tend to use simpler Corpus objects.</p>

<p>Is there a simpler way I can transform my list of strings or VCorpus to a dataframe, say using dplyr/tidyr/purrr? </p>

<p>Any suggestions on improving my hacked-together solution much appreciated.</p>

<h1>Edit: Sample of data</h1>

<p>Each element of my list contains a string(/chr vector) with a full document in text. For example, </p>

<pre><code> strings[3] 
</code></pre>

<p>yields this output</p>

<p>[16] ""Table of Contents""<br>
  [17] ""Page""<br>
  [18] """"<br>
  [19] ""Contracting Parties""<br>
  [20] """"<br>
  [21] ""5""<br>
.
.
.</p>

<p>[379] ""â€œAffiliateâ€ means:""<br>
 [380] ""(a)""<br>
 [381] """"<br>
 [382] ""a company or any other entity in which any of the Parties holds, either directly or indirectly, the absolute""<br>
 [383] ""majority of the votes in the shareholdersâ€™ meeting or is the holder of more than fifty percent (50%) of the rights""<br>
 [384] ""and interests which confer the power of management on that company or entity, or has the power of""<br>
 [385] ""management and control over such company or entity;""                                                          </p>
","r, dataframe, text-mining","<p>This should do the trick:</p>

<pre><code>#dummy data generation: file names and a list of strings (your corpus)    
files &lt;- paste(""file"", 1:6)

strings &lt;- list(""a"",""b"",""c"", ""d"",""e"",""f"")
names(strings) &lt;-files
t(as.data.frame(unlist(strings)))

#             file 1 file 2 file 3 file 4 file 5 file 6
# unlist(strings) ""a""    ""b""    ""c""    ""d""    ""e""    ""f""  
</code></pre>

<h2>Edit based on data structure edit</h2>

<pre><code>files &lt;- paste(""file"", 1:6)

strings &lt;- list(c(""a"",""b""),c(""c"", ""d""),c(""e"",""f""),
                c(""g"",""h""), c(""i"",""j""), c(""k"", ""l""))

names(strings) &lt;-files
t(data.frame(Doc=sapply(strings, paste0, collapse = "" ""))) 

#     file 1 file 2 file 3 file 4 file 5 file 6
# Doc ""a b""  ""c d""  ""e f""  ""g h""  ""i j""  ""k l""  
</code></pre>
",0,1,1161,2017-09-22 15:45:47,https://stackoverflow.com/questions/46368540/text-mining-pdfs-convert-list-of-character-vectors-strings-to-dataframe
How to implement synonyms for use in a search engine?,"<p>I am working on a pet search engine (SE). </p>

<p>What I have right now is boolean keyword SE, as a library that is split in two parts:</p>

<ul>
<li><p>index: this is a inverted index ie. it associate terms with the original document where it appears</p></li>
<li><p>query: which is supplied by the user and can be arbitrarily complex boolean expression that looks like <code>(mobile OR android OR iphone) AND game</code></p></li>
</ul>

<p>I'd like to improve the search engine, in a way that does automatically extend simple queries to boolean queries so that it includes search terms that do no appear in the original query ie. I'd like to support synonyms.</p>

<p>I need some help to build the synonyms graph.</p>

<p>How can I compute list of words that appears in similar context?</p>

<p>Here is example of list of synonyms I'd like to compute:</p>

<ul>
<li>psql, pgsql, postgres, postgresql</li>
<li>mobile, iphone, android</li>
</ul>

<p>and also synonyms that includes ngrams like:</p>

<ul>
<li>rdbms, relational database management systems, ...</li>
</ul>

<p>The algorithm doesn't have to be perfect, I can post-process by hand the result, but at least I need to have a clue about what terms are similar to what other terms. </p>
","nlp, full-text-search, artificial-intelligence, text-mining, word2vec","<p>In the standard Information Retrieval (IR) literature, this enrichment of a query with additional terms (that don't appear in the initial/original query) is known as <strong>query expansion</strong>.</p>

<p>There're a plenty of standard approaches which, generally speaking, are based on the idea of <em>scoring</em> terms based on some factors and then selecting a number of terms (say K, a parameter) that have the highest scores.</p>

<p>To compute the term selection score, it is assumed that the top (M) ranked documents retrieved after initial retrieval are relevant, this being called <strong>pseudo-relevance feedback</strong>.</p>

<p>The factors on which the term selection function generally depend are:</p>

<ol>
<li>The term frequency of a term in a top ranked document - higher the better.</li>
<li>The number of documents (out of top M) in which the term occurs in - higher the better.</li>
<li>How many times does an additional term <em>co-occur</em> with a query term - the higher the better.</li>
</ol>

<p>The co-occurrence factor is the most important and would be give you terms such as 'pgsql' if the original query contains 'psql'.</p>

<p>Note that if documents are too short, this method would not work well and you have to use other methods that are necessarily semantics based such as i) <a href=""http://dl.acm.org/citation.cfm?doid=2766462.2767780"" rel=""nofollow noreferrer"">word-vector based expansion</a> or ii) wordnet-based expansion.</p>
",4,1,1236,2017-09-23 18:42:18,https://stackoverflow.com/questions/46383076/how-to-implement-synonyms-for-use-in-a-search-engine
How to replace tokens (words) with stemmed versions of words from my own table?,"<p>I got data like this (simplified):</p>

<pre><code>library(quanteda)
</code></pre>

<p>sample data</p>

<pre><code>myText &lt;- c(""ala ma kotka"", ""kasia ma pieska"")  
myDF &lt;- data.frame(myText)
myDF$myText &lt;- as.character(myDF$myText)
</code></pre>

<p>tokenization</p>

<pre><code>tokens &lt;- tokens(myDF$myText, what = ""word"",  
             remove_numbers = TRUE, remove_punct = TRUE,
             remove_symbols = TRUE, remove_hyphens = TRUE)
</code></pre>

<p>stemming with my own data
sample dictionary</p>

<pre><code>Origin &lt;- c(""kot"", ""pies"")
Word &lt;- c(""kotek"",""piesek"")

myDict &lt;- data.frame(Origin, Word)

myDict$Origin &lt;- as.character(myDict$Origin)
myDict$Word &lt;- as.character(myDict$Word)
</code></pre>

<p>what i got</p>

<pre><code>tokens[1]
[1] ""Ala""   ""ma""    ""kotka""
</code></pre>

<p>what i would like to get</p>

<pre><code>tokens[1]
[1] ""Ala""   ""ma""    ""kot""
tokens[2]
[1] ""Kasia""   ""ma""    ""pies""
</code></pre>
","r, nlp, text-mining, stemming, quanteda","<p>A similar question has been answered <a href=""https://stackoverflow.com/questions/46122591/a-lemmatizing-function-using-a-hash-dictionary-does-not-work-with-tm-package-in/46124729#46124729"">here</a>, but since that question's title (and accepted answer) do not make the obvious link, I will show you how this applies to your question specifically.  I'll also provide additional detail below to implement your own basic stemmer using wildcards for the suffixes.</p>

<h3>Manually mapping stems to inflected forms</h3>

<p>The simplest way to do this is by using a custom dictionary where the keys are your stems, and the values are the inflected forms.  You can then use <code>tokens_lookup()</code> with the <code>exclusive = FALSE, capkeys = FALSE</code> options to convert the inflected terms into their stems.</p>

<p>Note that I have modified your example a little to simplify it, and to correct what I think were mistakes.</p>

<pre><code>library(""quanteda"")
packageVersion(""quanteda"")
[1] ‘0.99.9’

# no need for the data.frame() call
myText &lt;- c(""ala ma kotka"", ""kasia ma pieska"")  
toks &lt;- tokens(myText, 
               remove_numbers = TRUE, remove_punct = TRUE,
               remove_symbols = TRUE, remove_hyphens = TRUE)

Origin &lt;- c(""kot"", ""kot"", ""pies"", ""pies"")
Word &lt;- c(""kotek"", ""kotka"", ""piesek"", ""pieska"")
</code></pre>

<p>Then we create the dictionary, as follows.  As of quanteda v0.99.9, values with the same keys are merged, so you could have a list mapping multiple, different inflected forms to the same keys.  Here, I had to add new values since the inflected forms in your original <code>Word</code> vector were not found in the <code>myText</code> example.</p>

<pre><code>temp_list &lt;- as.list(Word) 
names(temp_list) &lt;- Origin
(stem_dict &lt;- dictionary(temp_list))
## Dictionary object with 2 key entries.
## - [kot]:
##   - kotek, kotka
## - [pies]:
##   - piesek, pieska    
</code></pre>

<p>Then <code>tokens_lookup()</code> does its magic.</p>

<pre><code>tokens_lookup(toks, dictionary = stem_dict, exclusive = FALSE, capkeys = FALSE)
## tokens from 2 documents.
## text1 :
## [1] ""ala"" ""ma""  ""kot""
## 
## text2 :
## [1] ""kasia"" ""ma""    ""pies"" 
</code></pre>

<h3>Wildcarding all stems from common roots</h3>

<p>An alternative is to implement your own stemmer using the ""glob"" wildcarding to represent all suffixes for your <code>Origin</code> vector, which (here, at least) produces the same results:</p>

<pre><code>temp_list &lt;- lapply(unique(Origin), paste0, ""*"")
names(temp_list) &lt;- unique(Origin)
(stem_dict2 &lt;- dictionary(temp_list))
# Dictionary object with 2 key entries.
# - [kot]:
#   - kot*
# - [pies]:
#   - pies*

tokens_lookup(toks, dictionary = stem_dict, exclusive = FALSE, capkeys = FALSE)
## tokens from 2 documents.
## text1 :
## [1] ""ala"" ""ma""  ""kot""
## 
## text2 :
## [1] ""kasia"" ""ma""    ""pies"" 
</code></pre>
",4,0,574,2017-09-27 13:23:25,https://stackoverflow.com/questions/46449047/how-to-replace-tokens-words-with-stemmed-versions-of-words-from-my-own-table
python kernel dead when performing SVD on a sparse symmetrical matrix,"<p>I would like to reproduce the SVD method mentioned in a standford lecture on my own dataset. The slide of the lecture is as following</p>

<p><a href=""https://i.sstatic.net/qCYZl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qCYZl.png"" alt=""stanford lecture""></a></p>

<p>My dataset is of the same type, which is a word co-occurrence matrix M with a size of</p>

<pre><code>&lt;13840x13840 sparse matrix of type '&lt;type 'numpy.int64'&gt;' 
with 597828 stored elements in Compressed Sparse Column format&gt;
</code></pre>

<p>generated and processed from CountVectorizer(), note that this is a symmetric matrix.</p>

<p>However, when I tried to extract features from SVD, however, none of the following code works,</p>

<h1>1st try:</h1>

<pre><code>scipy.linalg.svd(M)
</code></pre>

<p>I have tried the matrix from sparse csr todense() and toarray(), my computer taken quite a few minutes, and it displays kernel stops. I also played around with different parameter settings</p>

<h1>2nd try:</h1>

<pre><code>scipy.sparse.linalg.svds(M)
</code></pre>

<p>I have also tried to change the matrix type from int64 to float64, however, the kernel dead after 30 seconds or so.</p>

<p>Anyone could suggest me a way to conduct SVD on this matrix in any way?</p>

<p>Thank you so much</p>
","python, sparse-matrix, text-mining, word-count, svd","<p>Seems that the matrix is to stressful for the memory. You have several options:</p>

<ol>
<li>Perform an adaptive SVD,</li>
<li>Use <a href=""https://pypi.python.org/pypi/modred"" rel=""nofollow noreferrer"">modred</a>,</li>
<li>Use the SVD from <a href=""http://matthewrocklin.com/blog/work/2015/06/26/Complex-Graphs"" rel=""nofollow noreferrer"">dask</a>.</li>
</ol>

<p>The latter two should work out of the box.
All these options will load only what the memory can.</p>
",1,0,1340,2017-10-02 05:08:54,https://stackoverflow.com/questions/46519797/python-kernel-dead-when-performing-svd-on-a-sparse-symmetrical-matrix
trying to create a value of strings using the | or operator,"<p>I am trying to scrape a website link. So far I downloaded the text and set it as a dataframe. I have the folllowing;</p>

<pre><code>keywords &lt;- c(credit | model)

text_df &lt;- as.data.frame.table(text_df)
text_df %&gt;%
  filter(str_detect(text, keywords))
</code></pre>

<p>where credit and model are two values I want to search the website, i.e. return row with the word credit or model in.</p>

<p>I get the following error</p>

<blockquote>
  <p>Error in filter_impl(.data, dots) : object 'credit' not found</p>
</blockquote>

<p>The code only returns the results with the word ""model"" in and ignores the word ""credit"".</p>

<p>How can I go about returning all results with either the word ""credit"" or ""model"" in.</p>

<p>My plan is to have <code>keywords &lt;- c(credit | model | more_key_words | something_else | many values)</code></p>

<p>Thanks in advance.</p>

<p>EDIT: </p>

<pre><code>text_df:
    Var 1    text
    1        Here is some credit information
    2        Some text which does not expalin any keywords but messy &lt;li&gt; text9182edj &lt;/i&gt;
    3        This line may contain the keyword model
    4        another line which contains nothing of use
</code></pre>

<p>So I am trying to extract just rows 1 and 3.</p>
","r, text-mining, stringr","<p>Ok I have checked it and I think it will not work you way, as you must use the or | operator inside <code>filter()</code> not inside <code>str_detect()</code></p>

<p>So it would work this way:</p>

<pre><code>keywords &lt;- c(""virg"", ""tos"")

 library(dplyr)
 library(stringr)

 iris %&gt;%
      filter(str_detect(Species, keywords[1]) | str_detect(Species, keywords[2]))
</code></pre>

<p>as a <code>keywords[1]</code> etc you have to specify each ""keyword"" from this variable</p>
",0,0,62,2017-10-05 19:01:45,https://stackoverflow.com/questions/46593000/trying-to-create-a-value-of-strings-using-the-or-operator
Quanteda: Document Feature Matrix with predefined set of features,"<p>I am using quanteda to build two document feature matrices:</p>

<pre><code>library(quanteda)
DFM1 &lt;- dfm(""this is a rock"")
#        features
# docs    this is a rock
#   text1    1  1 1    1
DFM2 &lt;- dfm(""this is music"")
#        features
# docs    this is music
#   text1    1  1     1
</code></pre>

<p>However, I want DFM2 to have a specific set of features, namely the ones from DFM1:</p>

<pre><code>DFM2 &lt;- dfm(""this is music"", *magicargument* = featnames(DFM1))
#        features
# docs    this is a rock
#   text1    1  1 0    0
</code></pre>

<p>Is there a magicargument that I am missing? Or is there another efficient way to archieve it for large bags of words? </p>
","r, text-mining, quanteda","<p>The magic argument is <code>pattern</code>, where you supply a dfm whose features will be matched (including zeroes for features not in the target dfm):</p>

<pre><code>dfm_select(DFM2, pattern = DFM1)
# Document-feature matrix of: 1 document, 4 features (50% sparse).
# 1 x 4 sparse Matrix of class ""dfmSparse""
#        features
# docs    this is a rock
#   text1    1  1 0    0
</code></pre>
",2,1,200,2017-10-05 21:33:52,https://stackoverflow.com/questions/46595124/quanteda-document-feature-matrix-with-predefined-set-of-features
How to divide text (string) by a certain character using r,"<p>How to classify strings using r</p>

<p>My text file is such a structure.</p>

<pre><code>        &gt;cell_c2&lt; 8/30/2017    This location has been closed for a few months. Recently I passed by and attracted by their street sign Teriyaki Grill Open. I gave a try. The cashier was friendly and recommended me to try their most popular Teriyaki chicken box. It came with mixed vege and steamed rice. They have an open kitchen with SS equipment. I could see the chef make grill after my order was placed. I love the teriyaki chicken with white rice. The full box costs $8 after tax. I think it's pretty reasonable for what you get near the University area. The chicken is tasty, tender, fresh, and not-to-sweet. I like this little new place, clean and friendly. Next time I will try their noodle.,         5/5/2014    1 check-in        After avoiding Sono express for five years I thought I'd give it a try to see if it had improved. ??Alas, no better than in 2008. ??The food is edible but for an Asian restaurant, I would place it on a very low tier of quality. ??Paper plates and plastic knives and forks are the norm. ??A 3 item combo seemed overpriced for the quality at $8.50. ??The egg roll was like one at the deli of a supermarket while the mixed vegetable was overlooked cabbage and broccoli with a chicken stock sauce. ??The teriyaki chicken was in small chunks and the meat tasted old. ??Sorry if my review mimics my tip, but, I felt I should review this place to dissuade others from coming here. ??I wish it were good, a quality Asian place would benefit the university neighborhood.Sono Express facadeSono Express facade,         11/10/2015            Updated reviewI regularly stopped by to have Taiwanese Pork Chop rice, and I'm a little bit sad that I found out the business has a new owner now.,         10/31/2015    Convenient location, friendly staff.I think I got some chicken with my mirin, but I'm not sure. The sauces were so vinegary I couldn't tell. Did the 3-entree special, cost $8.55 after tax for 3 very small portions of this vinegary meat and a plentiful portion of white rice. I've come to expect value to be an unheard of concept near the U, but this one takes the cake. If I had paid $6 for the meal I'd still be unsatisfied due to the lack of flavor.It's a shame, because this is a prime location for a hole-in-the-wall Asian restaurant.,         10/29/2012    1 check-in        I like this place for a few reasons.It's around the corner from the law school, is cheap and all the breaded/fried meats and fish are pan fried when you order them rather than hours before. ??All the bentos come with a big plate of rice that's always perfectly cooked (fluffy, warm, and almost buttery in texture), some sauteed veggies on the side (usually cabbage, carrots, etc.), and a main attraction--a breaded pork cutlet, a piece of fried fish, chicken drumsticks, etc. ??I can't say enough good things about the fried fish. ??The breading is light, crispy and well-seasoned. ??The fish inside is a garden variety white fish, and it's always piping hot and cooked thoroughly yet still moist and tender. ??I had the beef noodle soup today which is a new offering. ??It was really filling with a ton of noodles and a super rich beefy broth. ??It was dominated by egg noodles, which was fine by me, because I'm a pig for noodles. ??The beef was the star of the dish--it reminded me of a pot roast where it literally melts in your mouth. ??The broth was also impressive. ??My only criticisms are that the bok choy seemed like an afterthought and there wasn't enough broth. ??I wonder if the noodles soaked up quite a bit of broth the longer the soup sat in front of me. ??I've never tried any of the Americanized items. ??I usually stick with the fish bento or the Tonkatsu. ??They used to do a Katsu bowl which had fluffy, almost whipped eggs mixed in with the breaded pork and rice. ??It was my favorite, but they've not offered it the last few times I was in.Probably the thing I appreciate most about this place is that they really don't give a damn about impressing you. ??Take its paper plates and styrofoam cups or leave it. ??They have enough of a consistent Asian student clientele that they don't need to advertise or use actual dishes or provide an especially warm dining experience. ??I come here when I want to be anonymous. ??Most patrons dining in are Asian, while most others pop in for to-go orders.This is not a destination spot by any means, but it's a nice option to have around campus for when you're in the mood for something that's cheap and tastes relatively homemade, if a little boring in variety.Fish bento.Fish bento.Beef noodle soup.  I think this is seasonal during the fall/winter.Beef noodle soup.  I think this is seasonal during the fall/winter.,         10/15/2011    7 check-ins        This hole-in-the-wall place seemed sketchy at first, but alas, I found a nice old Asian couple making simple Asian food for a living here.The food is decent and tastes better than what they serve at the Union over at the University of Utah. The only thing I really recommend here is their two or three combo plates for approximately six to eight bucks. Other than that, there's nothing much here.The seating inside of Sono Express.The seating inside of Sono Express.The windows of Sono Express.The windows of Sono Express.,         3/19/2013    Clean and small restaurant. Great food on Chinese menu!Added an egg and a chicken leg. Delicious!Added an egg and a chicken leg. Delicious!,         8/10/2011    This place is good chice for a hungary student who is looking for sth sweet and like chiness. Its very quite and interestingly there is no music.  I suggest its two and three choices plate. The quality is acceptable but not that good.,         10/8/2009    This place is inexpensive and not that bad. Sure, it's nothing fancy, sure the plates are paper, but hey, the food is decent (not earth-shattering) and cheap!I would recommend this place to anyone who wants standard American-Chinese fair and does not want to spend much!,         5/8/2012    This is probably the worst place I have ever been to. It is expensive if you consider the quality of food they serve and ambience even if you compare it to the regular university hang-outs.The only sauce they have is teriyaki.They serve food in paper plates which still might have been acceptable had the owner/waitress kept her fingers off my food.There are no refills for the drinks.Since they don't appreciate you throwing your leftover drinks in the food garbage bin, you will find a small bucket right next to it, in which you will have to pour your drink. I found that pretty disgusting and was wondering whether they had ever been inspected.I will never go there again in my life. I absolutely hated everything.....in spite of it being right across from the university and being supposedly cheap.,         9/19/2007    First to Review                    Listed in Wanugee 100! Japanese, Wanugee! Salt Lake CitySono Express is your typical Collegetown food place. Cheap and across from campus. In this case, U of U campus, home of the Utes! By cheap, think paper plates and cups! The highest quality thing in here are the disposable chopsticks...they use the better pointed barrel Japanese type that separate at one end, instead of the cheaper flat square ones that are joined the entire length and never split evenly! Such a luxury!Japanese and Chinese Cuisine is their moniker. All of those words are a stretch of the truth. Japanese? There is tonkatsu, tempura, etc, but every dish is served with teriyaki sauce. What does this tell you? Chinese owners! No self-respecting Japanese would run a Teriyaki only place, unless there were no asians for 50 miles to appreciate real asian food. You know all of those Teriyaki places in OR and WA? (you know what I am talking about Portland and Seattle! They are everywhere!) Those are Chinese, Koreans or Vietnamese passing off cheap teriyaki dishes as Japanese food to ignorant non-asians whose only known Japanese food is Teriyaki something. So this abomination has spread to Utah! Actually, the owners are Chinese, I heard them speaking Cantonese, and they didn't know what Donburi was. Half the menu is Chinese which consists of the normal Low Fan oriented dishes you will find at Panda Express at your nearest suburban mall. (Actually, P.E. is a lot better quality!). Hey, but as a starving student, you don't care about quality food, you want your food budget to stretch a long ways so you have more for your beer budget! At least I did, when I was at Uni. (actually, I would bum off my buddies. They liked me so much or felt sorry for me enough that they would invite me to join them for a bite or a brew and help pay!)I digress. Nothing over $10 on this menu. The Shrimp Tempura was served on a paper plate, the diet coke in a paper cup, and the total was under $10. There were 5 shrimp pieces plus veggies, with a batter between real tempura and Jack in the Box onion rings. And the dipping sauce was...you guessed it....Teriyaki!,         8/19/2011    Every time we visit my Alma mater, my family comes to Sono Express for lunch. ??This year we found they added some traditional Chinese dishes. ??They are written in Chinese on the menu board, consisting of Pork barbecue, Lu Eggs, and roasted pork feet. ??We love them, reminds me of the lunch box on-board Taiwan's trains! ??We also love their fried tempura vegetables and sushi rolls. ??The price is very reasonable and the food is fast.
        &gt;cell_c3&lt;1/15/2016    1 check-in        First to Review        This is a little food stand right outside of Wells Fargo. ??It's adjacent to The Curryer which has been there for years and is delicious and loved by many. ??They opened up recently so I decided to brave the cold weather and go try it out for the first time. I ordered the Bibimbap with Fire Chicken. ??The basic description of Bibimbap is that it comes with some rice, veggies, meat, sauce, and a fried egg. ??You mix it all together and then enjoy. Buldak (the name of the stand) translates to Fire Chicken. I expected the fire chicken to be spicy - it wasn't spicy at all. ??Instead it more tasted like it was cooked in a fire, it had a charred/smoky taste to it. ??I wish that it had been spicier.My deduction for stars is because the meal ended up not being that enjoyable. ??It wasn't my favorite Korean food I've had and it was cold/lukewarm. ??When I walked up to the stand at 11:30 it didn't look like they were quite ready, but I asked if they were open. They said yes, took my order, and then started to prepare my dish. ??I saw the egg and chicken get cooked right in front of me, so it was peculiar that my meal was so cold. ??I think it was because the rice and veggies were cold so when it was all mixed up the temps changed to a colder dish. ??It was also a really cold day outside, but I got my hot meal and walked a couple hundred of feet to work.If you like Korean food, I'd recommend giving this place a try. ??Everything is under $10 and the portions are big. ??Plus there aren't a lot of Korean food options within walking distance nearby.Fire chicken bibimbapFire chicken bibimbap,         4/12/2016    I tried the chicken bibimbop- my egg was cooked all the way through so kind of gross and the chicken wasn't reheated all the way through. Great concept, but fell way too short in execution. I've tried 4x and keep feeling disappointed.
        &gt;cell_c4&lt;        4/1/2017    Soooo good. We had been actively avoiding this place because of its sketchy exterior and now I hugely regret that decision. It's been right down the street and I've been missing out for HOW LONG??!! The interior has been revamped into what is actually a very pleasant atmosphere.We got the spicy seafood bibimbop, beautifully crusty rice and delicious veg and seafood out of a sizzling clay bowl. It could have been spicier but a bit of gochujang brought out by the lovely waiter solved that problem.However, the true star of the show was the LA short rib bulgogi! I am still drooling after the dish is long gone. Perfectly marinated and marbled. So yum. So so yum.Short rib bulgogi and seafood bibimbopShort rib bulgogi and seafood bibimbop,         4/21/2017            Updated review8 check-ins        Another great meal! I got a message that this place will be closing soon. That is too bad cause the food and service was excellent!Pot stickersPot stickersGambitang awesomeGambitang awesomeShort ribShort rib                        See all photos from John C. for Seoul Garden                    ,         2/8/2017    Seoul Garden should absolutely be your Korean BBQ go-to.My boyfriend and I have practically become regulars at this charming family-owned restaurant, thanks to the good food, super friendly service, and well-priced BBQ options. Yun, who has been our server every time we've gone, is so sweet and always makes us feel welcome. She has an awesome upbeat attitude, especially on top of going to school full-time. We pretty much want her to be our best friend.One of the greatest things about Seoul Garden is their a la carte tabletop grill BBQ option that I haven't seen at any other Korean restaurants in Salt Lake. Basically, you can either choose the 2-person Korean BBQ meals that come with rice and side dishes, or you can pick from 6 a la carte meats that don't come with any other sides, but are cheaper (and you get more bbq variety). With the second option, you can also get yummy DIY hot pot with add-on options of veggies, udon noodles, seafood, and more. Our favorite BBQ picks are the marinated short ribs (kalbi), pork belly (samgyeopsal), and marinated beef (bulgogi). Their marinade is delicious! The first time we went we were a little confused about the BBQ options, but Yun explained it all to us, and they've since updated the menu with a chart that goes over the details.Of the other dishes we've tried there, the hot stone bibimbap, kimchi jjigae, and dduk mandu guk were our favorites!Definitely check out this gem, and support a local, family-owned business!Dduk mandu guk, or rice cake dumpling soup. Yummy broth, described by the eater as a great hearty soup.Dduk mandu guk, or rice cake dumpling soup. Yummy broth, described by the eater as a great hearty soup.Side dishes!Side dishes!A la carte tabletop BBQ - 2 orders of short ribs, 1 marinated beefA la carte tabletop BBQ - 2 orders of short ribs, 1 marinated beef                        See all photos from Erin F. for Seoul Garden                    ,         12/9/2016    COMPLETELY CLUELESS is the only way to describe this place. ??I asked what kind of noodles came with soups, where it was not listed on the menu. ??The server had no idea. ??The menu isn't very extensive, how could that be? ??He went to check and, still very unsure, mumbled something about them coming with ramen, so I asked if I could substitute rice noodles. ??I explained that I have a food intolerance and could not have wheat. ??The server then proceeded to recommend vegetarian dishes, which was completely baffling. ??I am not vegetarian nor did I ask anything about vegetarian options. ??When I asked if that was the only gluten free option, he said that he needed to check again. This time he came back saying that I can get a side of rice noodles with any of the soups. ??I pointed out that I still cannot have a soup with wheat noodles in it. ??He asks if I am a vegetarian, which again makes no sense. ??The whole time I have been asking about beef and pork dishes. ??At this point, it is still completely unclear to both me and the server what ingredients are in any of the dishes. ??I ask which dishes are gluten free or can be made gluten free. ?? Anyway, he goes to check again. ??For the same thing. ??And he is incredibly slow moving, so we've now been sitting here for 20 minutes while he figures out how to locomote to the other side of the building.This time he says he can't make any substitutions... ??Why didn't he just say this in the first place?!? ??He also still didn't know what noodles come with anything. ??Unfortunately, we just had to leave at this point due to the complete lack of comprehension on behalf of this simpleminded individual and whoever was serving as the manager on duty at the time. ??It is quite unfortunate that people in hospitality could be so completely ignorant about the food that they are serving.,         10/8/2016    1 check-in        I really wanted to like this place because of the reliable reviews of my fellow yelpers, but I was a little disappointed and here's why. Don't get me wrong, the service and food were good. I just wanted my usual Korean BBQ experience with a little bit of homemade touch. In regards to the Korean BBQ experience, I'm talking about the servers firing up the grill and we cook the meat at our own pace. ??Tongs were provided to pick up our protein and lay them on the grill. ??We ordered the pork belly and the prime beef, both not marinated. ??My friend and I planned to cook a few pieces here and there so it wouldn't overcook on the grill or get cold after a few minutes. ??Well that plan went downhill when the chef from the came to our table and asked us if she can help us cook. ??I wasn't sure what to expect so I said, SURE!. She took our plate and distributed it all over the grill. (Sad face). As I attempt to add some pork belly, she recommended not to. We didn't want to be rude if this was their usually routine, but I didn't really like it. It also felt like they took over our cooking experience and then standing over us was a little uncomfortable at times only because I wanted to catch up with my buddy. I was also expecting a lot more side dishes and wanted some of the items to taste more homemade. ??The broccoli was just boiled with no added flavors. The Kimchee tasted like the ones I've had in the jar. The BBQ sauce provided tasted like the ones I buy for my baked beans. If you need extra lettuce, it's $1.50 more and they give you 6 pieces to start with. The bean sprouts are awesome though.....yay! Like other similar Korean BBQ restaurants in SLC, I think its overpriced for the amount of food you get. I know my review sounds like it should be a 2 star rating, but the restaurant was clean, food was good and staff are friendly. ??I just wish for homemade taste and more of my own hands on cooking experience.Vegetable tempura with soy sauceVegetable tempura with soy saucePrime beefPrime beefIf you need more lettuce, they charge $1.50If you need more lettuce, they charge $1.50                        See all photos from Mylan D. for Seoul Garden                    ,         12/6/2016    1 check-in        Had high expectations coming in hearing great reviews. ??It was good. ??A solid 3.5 stars at best. I thought the servings could be bigger especially at these prices. I could mention a few other Korean joints in town that are similar to prices but serve a lot more food. The ribs and bulgogi were good. Banchans were tasty. ??Service was Great! Overall pretty good. ??I just felt there was something missing. ??And I can't quite figure it out. ??Karaoke maybe?Seafood pancakeSeafood pancakeSpicy goodnessSpicy goodnessKorean red wine (chilled)Korean red wine (chilled),         4/25/2017    1 check-in        Great food!! Meat was marinated to perfectly! Plum wine was perfect. Great service. Good place eat hang out and eat.Plum winePlum wine,         1/24/2016            Updated review4 check-ins        I threatened our server if that she did not use my card for payment over my friends that I would leave a bad review. I was kidding, kind of, no really I was definitely kidding. Even if she did not use my card, I would have left a good review! This establishment deserved a bump up from their previous rating because of their black bean noodles (usually only offered during lunch Monday-Thursday but they made it for me on a Saturday evening), their delicious kim chi, consistency in grilled mackerel (no surprise here), excellent service, and seriously how cute the yellow pickled radish was (usually comes with black bean noodles). The only downfall this time around was the sweet and sour chicken. I didn't really enjoy the sauce, it was too sweet and not tangy enough. But overall, this was a pretty solid visit.-Sailor outSweet and sour chickenSweet and sour chickenSpicy pork hot stone potSpicy pork hot stone potBlack bean noodlesBlack bean noodles,         2/11/2017    1 check-in        Great food &amp;amp; amazing service!! We started w/the fried dumplings, then got the marinated beef, spicy pork, spicy octopus &amp;amp; a broth w/their seafood medley! Wow! So yum! All of these dishes were phenomenal &amp;amp; our server Jackson was wonderful! Try it out if you haven't yet! You won't be disappointed!Great food &amp;amp; Amazing Service!  All of these dishes were phenomenal &amp;amp; our server Jackson was wonderful! Try it out if you haven't yet!Great food &amp;amp; Amazing Service!  All of these dishes were phenomenal &amp;amp; our server Jackson was wonderful! Try it out if you haven't yet!We had the fried dumplings, marinated beef to grill, spicy pork, spicy octopus &amp;amp; seafood medley broth!We had the fried dumplings, marinated beef to grill, spicy pork, spicy octopus &amp;amp; seafood medley broth!,         3/30/2017    My husband and I can here for dinner on a Sunday night. We were hungry early so decided to go right when they opened. There were already at lest five parties here before they unlocked the door! My guess is this place gets busy fast. We had the Korean BBQ that we cooked ourselves as well as the bipbimbop bowl. Both amazing. The service was also great and every person that helped us did so with a smile on their face. Definitely a place we will add into our weekend rotations.,         1/4/2017    2 check-ins        I think I've found a new Korean restaurant for me to frequent, because this place was delightful! Our server was very friendly and quick, the food was absolutely delicious, and the place was very open and clean. Plenty of options on the menu, though for those that aren't familiar with Korean fare, more pictures and/or descriptions may be helpful.One thing to note: if you want to order any bbq/grill type of foods, make sure you tell them before they seat you. I saw a table have to get up and move to another side because one guy wanted bbq.,         3/9/2016    1 check-in        ROTD 8/22/2016        I think I'm in love with this place. My husband I stopped in here for dinner for the first time a couple of weeks ago. I was initially skeptical because we were the only customers (granted, it was about 830pm on a Thursday), however that skepticism didn't last long. We started with a bit of soju, which was a very nice treat. It's very difficult to find non-Tyku soju anywhere around (even in the liquor store), but they have a couple of different varieties here. We were overwhelmed with all the tasty sounding choices, but finally settled on the tableside BBQ with spicy squid and pork belly. I'm often worried about overcooking or undercooking my food in this context, but our server made sure everything was cooked properly, and it was absolutely delicious! The sauce was very flavorful (but not devilishly spicy for us, we used the whole thing without a problem). The service was absolutely fantastic. Our server was very helpful with food and drink recommendations and is very enthusiastic about all the stuff they serve. Given the amount of customers there on that particular night, I'm a little worried about this place surviving, so please visit! I want to try all the things on the menu (which is going to take many visits). That won't happen if you don't come eat here too, so please help.,         1/1/2017    Service was...well, great as far as filling up water and getting our food went. ??But was very unfriendly and seemed...annoyed we were there. Yeah, we all felt kind of unwelcome. ??The food was ok. We were a party of 6 and they brought out the tiniest amount of side dishes. We are used to K town in LA where the side dishes are always refilled and plentiful and it just seemed really stingy. ??I guess you have to pay for more?They only have some items on the menu 2 days a week...which sucks cause we came for the black bean noodles and they don't serve those on Sunday. The tofu stew was good. ??The bulgogi was decent, but not the best I've had. ??The decor and atmosphere is a little cafeteria like.,         2/27/2016    1 check-in        I ended up order the Japchae (which was a first for me), and pleasantly surprised. The glassy noodles were tasty along with the sweet potato, mushroom, and whatever else seasoned in this dish. It all came together really well!The staff was very accommodated and even asked us their thoughts on remodeling ideas. The service was certainly topnotch!,        
</code></pre>

<p>I want to distinguish day / month / year.</p>

<p>I know there are commands. ""strsplit"" ,""str_split""</p>

<p>But I do not know how to write day / month / year. </p>

<p>If you categorize it, the data is not what I want. </p>

<p>What should I do? I would appreciate it if you could answer me.</p>

<pre><code>b&lt;-readLines(file.choose()) #My document is a .txt file.
B&lt;-strsplit (crudeCorp,""/"")
b&lt;-data.frame(B)
write.csv(B,""tqtqtq.csv"")
</code></pre>
","r, string, text-mining","<p>You need to specify a pattern for dates more completely. </p>

<pre><code>## Your sample data
b = ""10/1/2017 This was the first restaurant. 9/30/2015 i'm happy. i'm ~~. 6/20/2016  Prices were reasonable..""

Messages = strsplit(b, ""\\d{1,2}/\\d{1,2}/\\d{4}"")[[1]]
m &lt;- gregexpr(""\\d{1,2}/\\d{1,2}/\\d{4}"", b)
Dates = regmatches(b, m)[[1]] 
if(length(Messages) &gt; length(Dates)) { Messages = Messages[-1] }
as.data.frame(cbind(Dates, Messages))
</code></pre>

<blockquote>
<pre><code>      Dates                         Messages
1 10/1/2017  This was the first restaurant. 
2 9/30/2015              i'm happy. i'm ~~. 
3 6/20/2016         Prices were reasonable..
</code></pre>
</blockquote>
",1,1,198,2017-10-07 17:12:50,https://stackoverflow.com/questions/46622949/how-to-divide-text-string-by-a-certain-character-using-r
how to convert multiple sentences into bigram in python,"<p>I'm fairly new to python and I would like to convert an array of sentences to bigrams, is there a way to do this? for example</p>

<pre><code>X = ['I like u', 'u like me', ...]
</code></pre>

<p>If ngram = 2 I'm expecting the vocabulary has something like</p>

<pre><code>[0: 'I ',
 1: ' l',
 2: 'li',
 3: 'ik',
 4: 'ke',
 5: 'e ',
 6: ' u',
 7: 'u ',
 8: ' m',
 9: 'me'...]
</code></pre>

<p>so X can be converted to </p>

<pre><code> X_conv = [ '0, 1, 2, 3, 4, 5, 6',
            '7, 1, 2, 3, 4, 5, 8, 9',....]
</code></pre>

<p>Is there an functionI can do with countvectorizer?</p>
","python, text-mining, n-gram, countvectorizer","<p>Say, you have the function <code>ngrams</code>:</p>

<pre><code>def ngrams(text, n=2):
    return [text[i:i+n] for i in range(len(text)-n+1)]
</code></pre>

<p>now applying this to all elements to a list is rather easy:</p>

<pre><code>&gt;&gt;&gt; sentences = ['I like u', 'u like me']
&gt;&gt;&gt; processed = [ngrams(sentence, n=2) for sentence in sentences]
&gt;&gt;&gt; processed
[['I ', ' l', 'li', 'ik', 'ke', 'e ', ' u'], 
 ['u ', ' l', 'li', 'ik', 'ke', 'e ', ' m', 'me']]
</code></pre>

<p>So that is rather easy. To number the ngrams, you could build nested for loops, but it wouldn't look nice. </p>

<p>Instead we can use a trick: <code>collections.defaultdict</code>, which will create a new item if it doesn't exist when it is first accessed. We couple this with <code>itertools.count()</code> which returns a iterable counter. The <code>__next__</code> magic method is a callable that when called the first time returns the first number, then the second and so forth. <code>defaultdict</code> will call this method once per each new item</p>

<pre><code>from collections import defaultdict
from itertools import count

reverse_vocabulary = defaultdict(count().__next__)
numbered = [[reverse_vocabulary[ngram] for ngram in sentence]
            for sentence in processed]
print(numbered)
# [[0, 1, 2, 3, 4, 5, 6], [7, 1, 2, 3, 4, 5, 8, 9]]
</code></pre>

<p>Now the reverse vocabulary is the opposite of what you'd want:</p>

<pre><code>defaultdict(&lt;...&gt;, {' m': 8, ' u': 6, 'I ': 0, 'li': 2, 'u ': 7, 'e ': 5, 'ke': 4, 'ik': 3, 
                    ' l': 1, 'me': 9})
</code></pre>

<p>We make an ordinary dictionary of it <a href=""https://stackoverflow.com/questions/483666/python-reverse-invert-a-mapping"">by inverting the mapping</a>:</p>

<pre><code>vocabulary = {number: ngram for ngram, number in reverse_vocabulary.items()}
</code></pre>

<p>which results in vocabulary being an ordinary dictionary</p>

<pre><code>{0: 'I ', 1: ' l', 2: 'li', 3: 'ik', 4: 'ke', 5: 'e ', 6: ' u', 7: 'u ', 8: ' m', 9: 'me'}
</code></pre>
",1,0,635,2017-10-08 06:48:20,https://stackoverflow.com/questions/46628276/how-to-convert-multiple-sentences-into-bigram-in-python
Tag text using grep and paste in r,"<p>I have two data frames. The first one: </p>

<pre><code>keyword &lt;- c(""apple"",""peach"",""grape"",""berry"",""kiwi fruit"")
keyword &lt;- data.frame(keyword)
</code></pre>

<p><a href=""https://i.sstatic.net/DImYC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DImYC.png"" alt=""enter image description here""></a></p>

<p>The second one:</p>

<pre><code>sentence &lt;- c(""I like apple"",""I hate apple"",""grape is good"")
url &lt;- c(""url1"",""url2"",""url3"")
sentence &lt;- data.frame(sentence,url)
</code></pre>

<p><a href=""https://i.sstatic.net/6Kqf6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6Kqf6.png"" alt=""enter image description here""></a></p>

<p>What I need to is: if keyword is contained in sentence, paste url to the text. If multiple sentences contain the keyword, paste all url. The final result is like: </p>

<p><a href=""https://i.sstatic.net/XQykD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XQykD.png"" alt=""enter image description here""></a></p>

<p>I tried to use the code as bellow, but it did not work out as expected.</p>

<pre><code>keyword$Label &lt;- character(length(keyword$keyword))

for (i in 1:length(keyword$keyword)) {
keyword$Label[grep(keyword$keyword[i],sentence$sentence)] &lt;- sentence$url
}
</code></pre>
","r, dataframe, text, text-mining, data-manipulation","<p>A solution with <code>stringr</code> + <code>dplyr</code> + <code>tidyr</code>:</p>

<pre><code>library(stringr)
library(dplyr)
library(tidyr)

sentence %&gt;%
  mutate(sentence = str_extract(sentence, paste0(keyword$keyword, collapse = ""|""))) %&gt;%
  right_join(keyword, by = c(""sentence"" = ""keyword"")) %&gt;%
  group_by(sentence) %&gt;%
  mutate(URL = 1:n()) %&gt;%
  spread(URL, url, sep = """") %&gt;%
  rename(keyword = sentence)
</code></pre>

<p><strong>Result:</strong></p>

<pre><code># A tibble: 5 x 3
# Groups:   keyword [5]
     keyword  URL1  URL2
*      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;
1      apple  url1  url2
2      berry  &lt;NA&gt;  &lt;NA&gt;
3      grape  url3  &lt;NA&gt;
4 kiwi fruit  &lt;NA&gt;  &lt;NA&gt;
5      peach  &lt;NA&gt;  &lt;NA&gt;
</code></pre>

<p><strong>Data:</strong></p>

<pre><code>keyword &lt;- c(""apple"",""peach"",""grape"",""berry"",""kiwi fruit"")
keyword &lt;- data.frame(keyword, stringsAsFactors = FALSE)
sentence &lt;- c(""I like apple"",""I hate apple"",""grape is good"")
url &lt;- c(""url1"",""url2"",""url3"")
sentence &lt;- data.frame(sentence,url, stringsAsFactors = FALSE)
</code></pre>
",2,1,200,2017-10-10 14:57:50,https://stackoverflow.com/questions/46669905/tag-text-using-grep-and-paste-in-r
Remove ngrams with leading and trailing stopwords,"<p>I want to identify major n-grams in a bunch of academic papers, including n-grams with nested stopwords, but not n-grams with leading or trailing stopwords.</p>

<p>I have about 100 pdf files. I converted them to plain-text files through an Adobe batch command and collected them within a single directory. From there I use R. (It's a patchwork of code because I'm just getting started with text mining.)</p>

<p>My code:</p>

<pre><code>library(tm)
# Make path for sub-dir which contains corpus files 
path &lt;- file.path(getwd(), ""txt"")
# Load corpus files
docs &lt;- Corpus(DirSource(path), readerControl=list(reader=readPlain, language=""en""))

#Cleaning
docs &lt;- tm_map(docs, tolower)
docs &lt;- tm_map(docs, stripWhitespace)
docs &lt;- tm_map(docs, removeNumbers)
docs &lt;- tm_map(docs, removePunctuation)

# Merge corpus (Corpus class to character vector)
txt &lt;- c(docs, recursive=T)

# Find trigrams (but I might look for other ngrams as well)
library(quanteda)
myDfm &lt;- dfm(txt, ngrams = 3)
# Remove sparse features
myDfm &lt;- dfm_trim(myDfm, min_count = 5)
# Display top features
topfeatures(myDfm)
#                  as_well_as             of_the_ecosystem                  in_order_to         a_business_ecosystem       the_business_ecosystem strategic_management_journal 
#603                          543                          458                          431                          431                          359 
#in_the_ecosystem        academy_of_management                  the_role_of                the_number_of 
#336                          311                          289                          276
</code></pre>

<p>For example, in the top ngrams sample provided here, I'd want to keep ""academy of management"", but not ""as well as"", nor ""the_role_of"". I'd like the code to work for any n-gram (preferably including less than 3-grams, although I understand it's simpler in this case to just remove stopwords first).</p>
","r, text-mining, tm, quanteda","<p>Here's how in <strong>quanteda</strong>: use <code>dfm_remove()</code>, where the pattern you want to remove is the stopword list followed by the concatenator character, for the beginning and end of the expression.  (Note here that for reproducibility, I have used a built-in text object.)</p>

<pre><code>library(""quanteda"")

# remove for your own txt
txt &lt;- data_char_ukimmig2010

(myDfm &lt;- dfm(txt, remove_numbers = TRUE, remove_punct = TRUE, ngrams = 3))
## Document-feature matrix of: 9 documents, 5,518 features (88.5% sparse).

(myDfm2 &lt;- dfm_remove(myDfm, 
                     pattern = c(paste0(""^"", stopwords(""english""), ""_""), 
                                 paste0(""_"", stopwords(""english""), ""$"")), 
                     valuetype = ""regex""))
## Document-feature matrix of: 9 documents, 1,763 features (88.6% sparse).
head(featnames(myDfm2))
## [1] ""immigration_an_unparalleled"" ""bnp_can_solve""               ""solve_at_current""           
## [4] ""immigration_and_birth""       ""birth_rates_indigenous""      ""rates_indigenous_british"" 
</code></pre>

<h3>Bonus answer:</h3>

<p>You can read your pdfs using the <strong>readtext</strong> package, which also works just fine with <strong>quanteda</strong> using the above code.</p>

<pre><code>library(""readtext"")
txt &lt;- readtext(""yourpdfolder/*.pdf"") %&gt;% corpus()
</code></pre>
",1,3,1833,2017-10-11 10:10:25,https://stackoverflow.com/questions/46685498/remove-ngrams-with-leading-and-trailing-stopwords
matching highest ranking word with text in dataframe column R,"<p>I have two data frames, 
df1:</p>

<pre><code>df1 &lt;- c(""A large bunch of purple grapes"", ""large green potato sack"", ""small red tomatoes"", ""yellow and black bananas"")
df1 &lt;- data.frame(df1)
</code></pre>

<p>df2:</p>

<pre><code>Word &lt;- c(""green"", ""purple"", ""grapes"", ""small"", ""sack"", ""yellow"", ""bananas"", ""large)

Rank &lt;- c(20,18,22,16,15,17,6,12)

df2 &lt;- data.frame(Word,Rank)
</code></pre>

<p>df1:</p>

<pre><code>ID      Sentence  
 1      A large bunch of purple grapes  
 2      large green potato sack 
 3      small red tomatoes  
 4      yellow and black bananas   
</code></pre>

<p>df2:</p>

<pre><code>ID      Word      Rank
 1      green      20
 2      purple     18
 3      grapes     22
 4      small      16
 5      Sack       15
 6      yellow     17
 7      bananas    6
 8      large      12
</code></pre>

<p>What I want to do is; match the words in df2 to the words contained in the ""Sentence"" column and insert a new column in df1 containing the highest ranking matched word from df2. So something like this: </p>

<p>df1:</p>

<pre><code>ID     Sentence                         Word
 1     A large bunch of purple grapes   grapes
 2     large green potato sack          green
 3     small red tomatoes               small
 4     yellow and black bananas         yellow
</code></pre>

<p>I initially used to following code to match words, but of course this creates a column containing all of the words matched: </p>

<pre><code>x &lt;- sapply(df2$Word, function(x) grepl(tolower(x), tolower(df1$Sentence)))

df1$top_match &lt;- apply(x, 1, function(i) paste0(names(i)[i], collapse = "" ""))
</code></pre>
","r, dataframe, text-mining","<p>Here's a <code>tidyverse</code> + <code>stringr</code> solution:</p>

<pre><code>library(tidyverse)
library(stringr)

df1$Sentence %&gt;%
  str_split_fixed("" "", Inf) %&gt;%
  as.data.frame(stringsAsFactors = FALSE) %&gt;%
  cbind(ID = rownames(df1), .) %&gt;%
  gather(word_count, Word, -ID) %&gt;%
  inner_join(df2, by = ""Word"") %&gt;%
  group_by(ID) %&gt;%
  filter(Rank == max(Rank)) %&gt;%
  select(ID, Word) %&gt;%
  right_join(rownames_to_column(df1, ""ID""), by = ""ID"") %&gt;%
  select(ID, Sentence, Word)
</code></pre>

<p><strong>Result:</strong></p>

<pre><code># A tibble: 4 x 3
# Groups:   ID [4]
     ID                       Sentence   Word
  &lt;chr&gt;                          &lt;chr&gt;  &lt;chr&gt;
1     1 A large bunch of purple grapes grapes
2     2        large green potato sack  green
3     3             small red tomatoes  small
4     4       yellow and black bananas yellow
</code></pre>

<p><strong>Note:</strong></p>

<p>You can ignore the warning that says coercing ID from factor into character. I also modified your datasets to include a proper column name for <code>df1</code> and to suppress automatically coercing characters to factors.</p>

<p><strong>Data:</strong></p>

<pre><code>df1 &lt;- c(""A large bunch of purple grapes"", ""large green potato sack"", ""small red tomatoes"", ""yellow and black bananas"")
df1 &lt;- data.frame(Sentence = df1, stringsAsFactors = FALSE)

Word &lt;- c(""green"", ""purple"", ""grapes"", ""small"", ""sack"", ""yellow"", ""bananas"", ""large"")
Rank &lt;- c(20,18,22,16,15,17,6,12)
df2 &lt;- data.frame(Word,Rank, stringsAsFactors = FALSE)
</code></pre>
",0,2,105,2017-10-11 13:46:54,https://stackoverflow.com/questions/46689898/matching-highest-ranking-word-with-text-in-dataframe-column-r
Add training data to existing model (bin file),"<p>I'm trying to add extra training data to my <code>nl-personTest.bin</code> file with <code>OpenNLP</code>.
Now is my problem that when I run my code to add the extra trainingsdata it removes the already existing data and only add my new data.</p>

<p>How can I just add extra trainingsdata instead of replacing it?</p>

<p>I did use the following code, (got it from <a href=""https://stackoverflow.com/questions/44043876/open-nlp-ner-is-not-properly-trained"">Open NLP NER is not properly trained</a>)</p>

<pre><code>public class TrainNames
    {
    public static void main(String[] args) 
    {
        train(""nl"", ""person"", ""namen.txt"", ""nl-ner-personTest.bin"");
    }

    public static String train(String lang, String entity,InputStreamFactory inputStream, FileOutputStream modelStream) {

        Charset charset = Charset.forName(""UTF-8"");
        TokenNameFinderModel model = null;
        ObjectStream&lt;NameSample&gt; sampleStream = null;
        try {
            ObjectStream&lt;String&gt; lineStream = new PlainTextByLineStream(inputStream, charset);
            sampleStream = new NameSampleDataStream(lineStream);
            TokenNameFinderFactory nameFinderFactory = new TokenNameFinderFactory();
            model = NameFinderME.train(""nl"", ""person"", sampleStream, TrainingParameters.defaultParams(),
                nameFinderFactory);
        } catch (FileNotFoundException fio) {

        } catch (IOException io) {

        } finally {
            try {
                sampleStream.close();
            } catch (IOException io) {

            }
        }
        BufferedOutputStream modelOut = null;
        try {
            modelOut = new BufferedOutputStream(modelStream);
            model.serialize(modelOut);
        } catch (IOException io) {

        } finally {
            if (modelOut != null) {
                try {
                    modelOut.close();
                } catch (IOException io) {

                }
            }
        }
        return ""Something goes wrong with training module."";
    }

    public static String train(String lang, String entity, String taggedCoprusFile,
                               String modelFile) {
        try {
            InputStreamFactory inputStream = new InputStreamFactory() {
                FileInputStream fileInputStream = new FileInputStream(""namen.txt"");

                public InputStream createInputStream() throws IOException {
                    return fileInputStream;
                }
            };

            return train(lang, entity, inputStream,
                new FileOutputStream(modelFile));
        } catch (Exception e) {
            e.printStackTrace();
        }
        return ""Something goes wrong with training module."";
    } }
</code></pre>

<p>Anyone any ideas to solve this problem?</p>

<p>Because If I want to have an accurate trainingset I need to have at least 15K
sentences says the documation.</p>
","java, text-mining, training-data, opennlp","<p>I think that OpenNLP does not support to expand existing binary NLP models. </p>

<p>If you have all training data available, collect them all and then train them at once. You can use <a href=""http://docs.oracle.com/javase/7/docs/api/java/io/SequenceInputStream.html"" rel=""nofollow noreferrer""><code>SequenceInputStream</code></a>. I modified your example to use another <code>InputStreamFactory</code></p>

<pre><code>public String train(String lang, String entity, InputStreamFactory inputStream, FileOutputStream modelStream) {

    // ....
    try {
        ObjectStream&lt;String&gt; lineStream = new PlainTextByLineStream(trainingDataInputStreamFactory(Arrays.asList(
                new File(""trainingdata1.txt""),
                new File(""trainingdata2.txt""),
                new File(""trainingdata3.txt"")
        )), charset);

        // ...
    } 

    // ...
}

private InputStreamFactory trainingDataInputStreamFactory(List&lt;File&gt; trainingFiles) {
    return new InputStreamFactory() {
        @Override
        public InputStream createInputStream() throws IOException {
            List&lt;InputStream&gt; inputStreams = trainingFiles.stream()
                    .map(f -&gt; {
                        try {
                            return new FileInputStream(f);
                        } catch (FileNotFoundException e) {
                            e.printStackTrace();
                            return null;
                        }
                    })
                    .filter(Objects::nonNull)
                    .collect(Collectors.toList());

            return new SequenceInputStream(new Vector&lt;&gt;(inputStreams).elements());
        }
    };
}
</code></pre>
",0,1,770,2017-10-12 11:06:04,https://stackoverflow.com/questions/46708048/add-training-data-to-existing-model-bin-file
OpenNLP find() method,"<p>At the moment im trying to find names inside a document. Im using the following method to find the names:</p>

<p><code>find(String[] tokens)</code> </p>

<p>I also found this method below:</p>

<pre><code>find(String[] tokens,String[][] additionalContext)
</code></pre>

<p>what can I do with this method and how do I use it ?</p>
","java, text, text-mining, opennlp","<p>According to <a href=""https://opennlp.apache.org/docs/1.8.3/apidocs/opennlp-tools/opennlp/tools/namefind/NameFinderME.html#find-java.lang.String:A-java.lang.String:A:A-"" rel=""nofollow noreferrer"">opennlp.tools.namefind.NameFinderME apidocs</a>:</p>

<blockquote>
<pre><code>public Span[] find(String[] tokens, String[][] additionalContext)
</code></pre>
  
  <p>Generates name tags for the given sequence, typically a sentence,
  returning token spans for any identified names.</p>
  
  <p>Parameters:</p>
  
  <ul>
  <li><code>tokens</code> - an array of the tokens or words of the sequence, typically    a sentence.</li>
  <li><strong><code>additionalContext</code> - features which are based on context outside of    the sentence but which should also be used.</strong></li>
  </ul>
  
  <p>Returns:
      an array of spans for each of the names identified.</p>
</blockquote>

<p>That being said, consider your tokens are:</p>

<pre><code>String[] tokens = { ""lorem"", ""ipsum"", ""dolor"", ""sit"", ""amet"", ""adipiscing"", ""elit"" };
</code></pre>

<p>But you also want to take into account the following features, ""<em>which are based on context outside of the sentence but which should also be used</em>"":</p>

<pre><code>String[][] additionalContext = { 
    { ""nullam"", ""fermentum"", ""justo"", ""non"", ""leo"", ""rhoncus"", ""blandit"" },
    { ""phasellus"", ""at"", ""diam"", ""mattis"", ""arcu"", ""congue"", ""consequat"" },
    { ""integer"", ""at"", ""tincidunt"", ""turpis"", ""eget"", ""pulvinar"", ""nisl"" } };
</code></pre>

<p>This way you can call <code>find(tokens, additionalContext)</code>.</p>

<p>Note that, according to the <a href=""https://github.com/apache/opennlp/blob/master/opennlp-tools/src/main/java/opennlp/tools/namefind/NameFinderME.java#L110"" rel=""nofollow noreferrer"">code on GitHub</a>, <code>find(String[] tokens)</code> is actually <code>find(tokens, EMPTY)</code> (and <code>String[][] EMPTY = new String[0][0]</code>).</p>
",2,1,99,2017-10-13 08:00:06,https://stackoverflow.com/questions/46725220/opennlp-find-method
data mining with unstructured data how to implement?,"<p>I have unstructured data (screenshot of app) and semi-structured data(screen dumping file), i chose store it in hbase. my goal is find defect or issue on app (meaningfull data). Now, I'd like to apply data mining on these, so that is kind of text mining ? and how can i apply some data mining technical on this data ? </p>
","bigdata, data-mining, text-mining","<ul>
<li>To begin with, you can use rule based approach where you define set of rules which detects the defect scenario.</li>
<li>Then you can prepare training data set which has many instances of defect, non-defect scenarios. In this step, for each screenshot or screen dump file you collect; you would manually tag it as defect or non-defect.</li>
<li>Then you can train classifier using this training data. Classifier would try to generalize training samples to predict the output label for the samples not seen in the past. </li>
<li>Since, your input is non-standard you might need some preprocessing to convert your input to standard form. For example, to process screenshots you might need some image processing, OCR, computer vision libraries.</li>
</ul>
",0,-3,70,2017-10-18 04:25:28,https://stackoverflow.com/questions/46802799/data-mining-with-unstructured-data-how-to-implement
OpenNLP categorizer Version 1.8,"<p>Im trying to build a categorizer in version 1.8 of openNLP but with the code below I keep getting a <code>NullPointerException</code>. What am I doing wrong?</p>

<pre><code>public class test 
{

        public static void main(String[] args) throws IOException 
        {
            InputStream is = new FileInputStream(""D:/training.txt"");
            DoccatModel m = new DoccatModel(is);
            Tokenizer tokenizer = WhitespaceTokenizer.INSTANCE;
            String tweet = ""testing sentence"";
            String[] tokens = tokenizer.tokenize(tweet);
            DocumentCategorizerME myCategorizer = new DocumentCategorizerME(m);
            double[] outcomes = myCategorizer.categorize(tokens);
            String category = myCategorizer.getBestCategory(outcomes);

        }
}
</code></pre>
","java, text-mining, opennlp, categorization","<p>You should have a look at following tutorial. They are useing <code>OpenNLP</code> version 1.7.2. This may be a more recent example to work with.</p>

<p><a href=""https://www.tutorialkart.com/opennlp/training-of-document-categorizer-using-naive-bayes-algorithm-in-opennlp/"" rel=""nofollow noreferrer"">https://www.tutorialkart.com/opennlp/training-of-document-categorizer-using-naive-bayes-algorithm-in-opennlp/</a></p>

<p>Hope it helps.</p>
",1,1,262,2017-10-18 06:57:24,https://stackoverflow.com/questions/46804382/opennlp-categorizer-version-1-8
Is it possible to use orange software as web-service,"<p>Is it possible to use orange software as web-service then use it with .Net framework? i want to use it on my website with database it's possible ?</p>
",".net, web-services, text-mining, orange","<p>No, Orange is an interactive desktop application and was not intended to be used as a web service where all the interactivity and visualizations would be useless.</p>
",1,0,119,2017-10-18 15:10:08,https://stackoverflow.com/questions/46813514/is-it-possible-to-use-orange-software-as-web-service
Use gsub to replace curly apostrophe with straight apostrophe in R list of character vectors,"<p>Looking for some guidance on how to replace a curly apostrophe with a straight apostrophe in an R list of character vectors.  </p>

<p>The reason I'm replacing the curly apostrophes - later in the script, I check each list item, to see if it's found in a dictionary (using qdapDictionary) to ensure it's a real word and not garbage.  The dictionary uses straight apostrophes, so words with the curly apostrophes are being ""rejected."" </p>

<p>A sample of the code I have currently follows.  In my test list, item #6 contains a curly apostrophe, and item #2 has a straight apostrophe.</p>

<p>Example:</p>

<pre><code>list_TestWords &lt;- as.list(c(""this"", ""isn't"", ""ideal"", ""but"", ""we"", ""can’t"", ""fix"", ""it""))

func_ReplaceTypographicApostrophes &lt;- function(x) {
   gsub(""’"", ""'"", x, ignore.case = TRUE)
 }

list_TestWords_Fixed &lt;- lapply(list_TestWords, func_ReplaceTypographicApostrophes)
</code></pre>

<p>The result:  No change.  Item 6 still using curly apostrophe.  See output below.</p>

<pre><code>list_TestWords_Fixed
[[1]]
[1] ""this""

[[2]]
[1] ""isn't""

[[3]]
[1] ""ideal""

[[4]]
[1] ""but""

[[5]]
[1] ""we""

[[6]]
[1] ""can’t""

[[7]]
[1] ""fix""

[[8]]
[1] ""it""
</code></pre>

<p>Any help you can offer will be most appreciated!</p>
","r, special-characters, text-mining, gsub","<p>You might be running up against a <a href=""https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17329"" rel=""nofollow noreferrer"">bug in R</a> on Windows. Try using <code>utf8::as_utf8</code> on your input. Alternatively, this also works:</p>

<pre><code>library(utf8)
list_TestWords &lt;- as.list(c(""this"", ""isn't"", ""ideal"", ""but"", ""we"", ""can’t"", ""fix"", ""it""))
lapply(list_TestWords, utf8_normalize, map_quote = TRUE)
</code></pre>

<p>This will replace the following characters with ASCII apostrophe:</p>

<pre><code>U+055A ARMENIAN APOSTROPHE
U+2018 LEFT SINGLE QUOTATION MARK
U+2019 RIGHT SINGLE QUOTATION MARK
U+201B SINGLE HIGH-REVERSED-9 QUOTATION MARK
U+FF07 FULLWIDTH APOSTROPHE
</code></pre>

<p>It will also convert your text to composed normal form (NFC).</p>
",2,5,3924,2017-10-18 16:23:03,https://stackoverflow.com/questions/46814856/use-gsub-to-replace-curly-apostrophe-with-straight-apostrophe-in-r-list-of-chara
"Write text and tables in to word, with whitespaces/enters","<p>I'm writing <code>text</code> and <code>text</code> from <code>tables</code> into a word document.</p>

<p>With the following code the <code>tables</code> are placed under the right <code>paragraphs</code>.</p>

<pre><code> Iterator&lt;IBodyElement&gt; iter = xdoc.getBodyElementsIterator();
               while (iter.hasNext())
               {
                  IBodyElement elem = iter.next();
                  if (elem instanceof XWPFParagraph)
                  { 
                      relevantText.setText(((XWPFParagraph) elem).getText());

                  } else if (elem instanceof XWPFTable)
                  {     
                      tabellen.setText(((XWPFTable) elem).getText());

                  }
               }
</code></pre>

<p>Now when I try to make a <code>whitespace/enter</code> with <code>addBreak()</code> or <code>addCarriageReturn()</code> the order of my document is wrong. The <code>table text</code> is placed after all the <code>text</code>.</p>

<p>Has anyone a solution for this?</p>
","java, ms-word, apache-poi, text-mining, opennlp","<p>I had the same problem a couple of days ago. did you create 2 diffrent runs for the paragraphs and the tables?</p>

<p>Because I did, and when I changed it to 1 run it did work for me. </p>

<p>Like this:</p>

<pre><code>XWPFRun text = paragraph.createRun();
</code></pre>
",2,1,62,2017-10-19 08:55:19,https://stackoverflow.com/questions/46826157/write-text-and-tables-in-to-word-with-whitespaces-enters
Find a keyword in a text file and catch the n words after this word,"<p>I'm doing a basic text-mining application and I'd need to find a definite word (keyword) and capture just the n words after this word. For example, in this text I'd want to catch the 3 words after the keyword POPULATION:</p>

<p><em>The Supplemental Tables consist of 59 detailed tables tabulated on the 2016 1-year microdata for geographies with populations</em> <strong>of 20,000 people</strong> <em>or more. These Supplemental Estimates are available through American FactFinder and the Census Bureau’s application programming interface at the same geographic summary levels as those in the American Community Survey.</em></p>

<p>Next step will be to split the string and find the number, but this is the point I've solved. I've tried with different methods (regex, etc.) with no success. How can I do it?</p>
","python, text-mining","<p>Split the text into words, find the index of the keyword, grab the words at the next indices:</p>

<pre><code>text = 'The Supplemental Tables consist of 59 detailed tables tabulated on the 2016 1-year microdata for geographies with populations of 20,000 people or more. These Supplemental Estimates are available through American FactFinder and the Census Bureau’s application programming interface at the same geographic summary levels as those in the American Community Survey.'
keyword = 'populations'
words = text.split()
index = words.index(keyword)
wanted_words = words[index + 1:index + 4]
</code></pre>

<p>If you wish to make the list of three words <code>wanted_words</code> back into a string, use</p>

<pre><code>wanted_text = ' '.join(wanted_words)
</code></pre>
",2,-1,103,2017-10-23 15:35:35,https://stackoverflow.com/questions/46893216/find-a-keyword-in-a-text-file-and-catch-the-n-words-after-this-word
Text Mining of Non-Isolated words,"<p>I have a dataset (one very long column from a PostgresDB) where the data is saved in the following way:</p>

<pre><code>**Variable_1** 
honey-pot 
treesarecool 
this-is-my-honeypot 
thisismyhonepot 
honey-is-sweet 
treesfurniture 
honeybees-production 
themoonisgone 
tableproduction
</code></pre>

<p>Sometimes words stand isolated, like <em>""honey""</em> for example, sometimes they are part of a longer word like <em>""honeypot""</em> or <em>""honeybees""</em>. I ultimately want a frequency table with the most frequent words like this.:</p>

<pre><code>Frequency Table: 
Honey     4 
trees     2 
Table     1 
pot       1 
namek     1
gone      1 
furniture 1 
his       n 
are       n 
pro       n 
duc       n 
tio       n 
...     ...
</code></pre>

<p>I DO NOT have a list of specific words to look for (I would use grep() in this case). I have no experience in text mining, but after some research I figured out that most text mining tools like (tm) need words to be isolated <em>(""The honey is sweet"")</em> in order to be able to aggregate and analyze them. So I figured that what Im looking for is a tool which uses brute force to compare strings. Like, find all similar strings longer than 3 characters (characterized in the frequency table by ""n"". </p>

<p>Is my assumption correct? Is there a tool in R that can accomplish this? Any other ideas, suggestions?</p>

<p>Appreciations!</p>

<p><strong>Update 1</strong></p>

<p>Having tried out the solution by Adam for a week now, I can make the following recomendations for analyzing strings like the ones mentioned above:</p>

<ol>
<li><p>Removing all ""-"", ""_"" and ""."" greatly helps reducing the number of irrelevant/uninteresting ngrams. Im interested in finding the frequency of a given word in my database and these signs don't contribute anything to such an analysis and instead just inflate the resulting dataset.</p></li>
<li><p>Removing Numbers [0-9] also hels reducing the amount of ngrams. Unless you also want the frequency of specific numbers (like 0041 predial codes or such ...). I would remove them and make a separate number analysis later (by removing all characters and signs and just leaving numbers). But it highly depends what your goal is!</p></li>
<li><p>Clean your Data!!! This is my first analysis of strings and my most important takeaway is that clean data, as always, goes a long way in reaching your goal!</p></li>
</ol>
","r, text, text-mining","<p>You could use the <code>quanteda</code> package to <code>tokenize()</code> each word into a set of character ngrams &amp; then tabulate the results.  </p>

<p>The below code iterates through the words and splits them into ngrams of lengths in the range: <code>[3, nchar(word)]</code>.</p>

<p>The size of the <code>char_ngrams</code> object will grow pretty quickly as the number of input words increases.  So not sure how well this will scale.</p>

<pre><code>library(quanteda)

#create exapmle data
words = c(""honey-pot"",
          ""treesarecool"",
          ""this-is-my-honeypot"",
          ""thisismyhonepot"",
          ""honey-is-sweet"",
          ""treesfurniture"",
          ""honeybees-production"",
          ""themoonisgone"",
          ""tableproduction"")

#perform char ngram tokenization
char_ngrams = unlist(
  lapply(words, function(w) {
    unlist(
      tokenize(w,
               ""character"",
               ngrams=3L:nchar(w),
               conc="""")
      )
    })
)

#show most popular character ngrams
head(sort(table(char_ngrams), decreasing = TRUE))
#char_ngrams
# one   hon  hone honey   ney  oney 
# 6     5     5     4     4     4 
</code></pre>
",2,2,129,2017-10-24 14:53:30,https://stackoverflow.com/questions/46913739/text-mining-of-non-isolated-words
Error in creating TermDocumentMatrix using tm package in R,"<p>I am unable to create a term document matrix using tm package in R which throws the following error as I try to create one out of a preprocessed corpus. </p>

<pre><code>Error in UseMethod(""TermDocumentMatrix"", x) : 
  no applicable method for 'TermDocumentMatrix' applied to an object of class 
""character""
</code></pre>

<p>Below is my script that I am using. I am using R v3.4.1 with tm package v0.7-1. </p>

<pre><code>data &lt;- readLines(""Data/en_US/en_US_sample.txt"", n = 100)
data &lt;- Corpus(VectorSource(data))
data &lt;- tm_map(data, removePunctuation)
data &lt;- tm_map(data, removeNumbers)
data &lt;- tm_map(data, content_transformer(tolower))
data &lt;- tm_map(data, removeWords, stopwords(""en""))
data &lt;- tm_map(data, stripWhitespace)
words &lt;- TermDocumentMatrix(""data"")
</code></pre>

<p>I believe TermDocumentMatrix requires the corpus to be in some specified text document format so I tried coercing my corpus to PlainTextDocument using tm_map but it doesn't solve the problem. When I am loading the my text data using Corpus on VectorSource, object created shows the class as <em>SimpleCorpus</em> which might be the problem but I am not totally sure. </p>

<p>Any help would be much appreciated. Thanks!</p>
","r, text-mining, tm","<p>You did everything right, just in your last line you accidentally passed a character <code>""data""</code> (note the quotation marks) to the function <code>TermDocumentMatrix()</code> instead of the object <code>data</code>.</p>
",0,0,559,2017-10-26 07:31:26,https://stackoverflow.com/questions/46948275/error-in-creating-termdocumentmatrix-using-tm-package-in-r
NLTK title classifier,"<p>Apologies in advance if this has already been questioned/answered, but I couldn't find any answer close to my problem. I am also somewhat noob as to dealing with Python, so sorry too for the long post.</p>
<p>I am trying to build a Python script that, based on a user-given Pubmed query (i.e., &quot;cancer&quot;), retrieves a file with N article titles, and evaluates their relevance to the subject in question.</p>
<p>I have successfully built the &quot;pubmed search and save&quot; part, having it return a .txt file containing titles of articles (each line corresponds to a different article title), for instance:</p>
<blockquote>
<p>Feasibility of an ovarian cancer quality-of-life psychoeducational intervention.</p>
<p>A randomized trial to increase physical activity in breast cancer survivors.</p>
</blockquote>
<p>Having this file, the idea is to use it into a classifier and get it to answer if the titles in the .txt file are relevant to a subject, for which I have a &quot;gold standard&quot; of titles that I know are relevant (i.e., I want to know the precision and recall of the queried set of titles against my gold standard). For example: Title 1 has the word &quot;neoplasm&quot; X times and &quot;study&quot; N times, therefore it is considered as relevant to &quot;cancer&quot; (Y/N).</p>
<p>For this, I have been using NLTK to (try to) classify my text. I have pursued 2 different approaches, both unsuccessfully:</p>
<p><strong>Approach 1</strong></p>
<p>Loading the .txt file, preprocessing it (tokenization, lower-casing, removing stopwords), converting the text to NLTK text format, finding the N most-common words. All this runs without problems.</p>
<pre><code>f = open('SR_titles.txt')
raw = f.read() 
tokens = word_tokenize(raw)
words = [w.lower() for w in tokens]
words = [w for w in words if not w in stopwords.words(&quot;english&quot;)]
text = nltk.Text(words)
fdist = FreqDist(text)
&gt;&gt;&gt;&lt;FreqDist with 116 samples and 304 outcomes&gt;
</code></pre>
<p>I am also able to find colocations/bigrams in the text, which is something that might be important afterward.</p>
<pre><code>text.collocations()
&gt;&gt;&gt;randomized controlled; breast cancer; controlled trial; physical
&gt;&gt;&gt;activity; metastatic breast; prostate cancer; randomised study; early
&gt;&gt;&gt;breast; cancer patients; feasibility study; psychosocial support;
&gt;&gt;&gt;group psychosocial; group intervention; randomized trial
</code></pre>
<p>Following <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow noreferrer"">NLTKs tutorial</a>, I built a feature extractor, so the classifier will know which aspects of the data it should pay attention to.</p>
<pre><code>def document_features(document):
  document_words = set(document)
  features = {}
  for word in word_features:
      features['contains({})'.format(word)] = (word in document_words)
  return features
</code></pre>
<p>This would, for instance, return something like this:</p>
<pre><code>{'contains(series)': False, 'contains(disorders)': False,
'contains(group)': True, 'contains(neurodegeneration)': False,
'contains(human)': False, 'contains(breast)': True}
</code></pre>
<p>The next thing would be to use the feature extractor to train a classifier to label new article titles, and following NLTKs example, I tried this:</p>
<pre><code>featuresets = [(document_features(d), c) for (d,c) in text]
</code></pre>
<p>Which gives me the error:</p>
<pre><code>ValueError: too many values to unpack
</code></pre>
<p>Quickly googled this and found that this has something to do with tuples, but did not get how can I solve it (like I said, I'm somewhat noob in this), unless by creating a categorized corpus (I would still like to understand how can I solve this tuple problem).</p>
<p>Therefore, I tried <strong>approach 2</strong>, following Jacob Perkings Text Processing with NLTK Cookbook:</p>
<p>Started by creating a corpus and attributing categories. This time I had 2 different .txt files, one for each subject of title articles.</p>
<pre><code>reader = CategorizedPlaintextCorpusReader('.', r'.*\,
    cat_map={'hd_titles.txt': ['HD'], 'SR_titles.txt': ['Cancer']})
</code></pre>
<p>With &quot;reader.raw()&quot; I get something like this:</p>
<blockquote>
<p>u&quot;A pilot investigation of a multidisciplinary quality of life intervention for men with biochemical recurrence of prostate cancer.\nA randomized controlled pilot feasibility study of the physical and psychological effects of an integrated support programme in breast cancer.\n&quot;</p>
</blockquote>
<p>The categories for the corpus seem to be right:</p>
<pre><code>reader.categories()
&gt;&gt;&gt;['Cancer', 'HD']
</code></pre>
<p>Then, I try to construct a list of documents, labeled with the appropriate categories:</p>
<pre><code>documents = [(list(reader.words(fileid)), category)
          for category in reader.categories()
          for fileid in reader.fileids(category)]
</code></pre>
<p>Which returns me something like this:</p>
<pre><code>[([u'A', u'pilot', u'investigation', u'of', u'a', u'multidisciplinary',
u'quality', u'of', u'life', u'intervention', u'for', u'men', u'with', 
u'biochemical', u'recurrence', u'of', u'prostate', u'cancer', u'.'], 
'Cancer'), 
 ([u'Trends', u'in', u'the', u'incidence', u'of', u'dementia', u':', 
u'design', u'and', u'methods', u'in', u'the', u'Alzheimer', u'Cohorts', 
u'Consortium', u'.'], 'HD')]
</code></pre>
<p>Next step would be creating a list of labeled feature sets, for which I used the next function, that takes a corpus and a feature_detector function  (that would be document_features referred above). It then constructs and returns a mapping of the form {label: [featureset]}.</p>
<pre><code>def label_feats_from_corpus(corp, feature_detector=document_features):
    label_feats = collections.defaultdict(list)
    for label in corp.categories():
        for fileid in corp.fileids(categories=[label]):
            feats = feature_detector(corp.words(fileids=[fileid]))
            label_feats[label].append(feats)
    return label_feats 

lfeats = label_feats_from_corpus(reader)
&gt;&gt;&gt;defaultdict(&lt;type 'list'&gt;, {'HD': [{'contains(series)': True, 
'contains(disorders)': True, 'contains(neurodegeneration)': True, 
'contains(anilinoquinazoline)': True}], 'Cancer': [{'contains(cancer)': 
True, 'contains(of)': True, 'contains(group)': True, 'contains(After)': 
True, 'contains(breast)': True}]})
</code></pre>
<p>(the list is a lot bigger and everything is set as True).</p>
<p>Then I want to construct a list of labeled training instances and testing instances.</p>
<p>The split_label_feats() function takes a mapping returned from
label_feats_from_corpus() and splits each list of feature sets
into labeled training and testing instances.</p>
<pre><code>def split_label_feats(lfeats, split=0.75):
    train_feats = []
    test_feats = []
    for label, feats in lfeats.items():
        cutoff = int(len(feats) * split)
        train_feats.extend([(feat, label) for feat in feats[:cutoff]])
        test_feats.extend([(feat, label) for feat in feats[cutoff:]])
    return train_feats, test_feats

train_feats, test_feats = split_label_feats(lfeats, split=0.75)
len(train_feats)
&gt;&gt;&gt;0
len(test_feats)
&gt;&gt;&gt;2
print(test_feats)
&gt;&gt;&gt;[({'contains(series)': True, 'contains(China)': True, 
'contains(disorders)': True, 'contains(neurodegeneration)': True}, 
'HD'), ({'contains(cancer)': True, 'contains(of)': True, 
'contains(group)': True, 'contains(After)': True, 'contains(breast)': 
True}, 'Cancer')]
</code></pre>
<p>I should've ended up with a lot more labeled training instances and labeled testing instances, I guess.</p>
<p><strong>This brings me to where I am now</strong>. I searched stackoverflow, biostars, etc and could not find how to deal with both problems, so any help would be deeply appreciated.</p>
<p><strong>TL;DR</strong>: Can't label a single .txt file to classify text, and can't get a corpus correctly labeled (again, to classify text).</p>
<p>If you've read this far, thank you as well.</p>
","python, nlp, nltk, text-mining, text-classification","<p>You're getting an error on the following line:</p>

<pre><code>featuresets = [(document_features(d), c) for (d,c) in text]
</code></pre>

<p>Here, you are supposed to convent each document (i.e. each title) to a dictionary of features. But to train with the results, the <code>train()</code> method needs both the feature dictionaries and the correct answer (""label""). So the normal workflow is to have a list of <code>(document, label)</code> pairs, which you transform to <code>(features, label)</code> pairs. It looks like your variable <code>documents</code> has the right structure, so if you just use it instead of <code>text</code>, this should work correctly:</p>

<pre><code>featuresets = [(document_features(d), c) for (d,c) in documents]
</code></pre>

<p>As you go forward, get in the habit of inspecting your data carefully and figuring out what will (and should) happen to them. If <code>text</code> is a list of titles, it makes no sense to unpack each title to a pair <code>(d, c)</code>. That should have pointed you in the right direction. </p>
",0,2,1847,2017-10-27 18:47:04,https://stackoverflow.com/questions/46981605/nltk-title-classifier
"For text mining in R, how do I combine DocumentTermMatrix with original Data Frame?","<p>What I am looking to do is create code that will allow me to classify tweets. So in the example below I would want to take tweets talking about a credit card and determine if they are related to the issue of travel.</p>

<p>Here is the initial dataset: </p>

<pre><code>id&lt;- c(123,124,125,126,127) 
text&lt;- c(""Since I love to travel, this is what I rely on every time."", 
        ""I got this card for the no international transaction fee"", 
        ""I got this card mainly for the flight perks"",
        ""Very good card, easy application process"",
        ""The customer service is outstanding!"") 
travel_cat&lt;- c(1,0,1,0,0) 
df_all&lt;- data.frame(id,text,travel) 
</code></pre>

<p>Output 1:</p>

<pre><code>id  text                                                        travel_cat
123 Since I love to travel, this is what I rely on every time.  1
124 I got this card for the no international transaction fee    0
125 I got this card mainly for the flight perks                 1
126 Very good card, easy application process                    0
127 The customer service is outstanding!                        0
</code></pre>

<p>I am then creating a data frame with only the text field and then doing the text analytics:</p>

<pre><code>myvars&lt;- c(""text"")
df&lt;- df_all[myvars]

library(tm)
corpus&lt;- Corpus(DataframeSource(df))
corpus&lt;- tm_map(corpus, content_transformer(tolower))
corpus&lt;- tm_map(corpus, removePunctuation)
corpus&lt;- tm_map(corpus, removeWords, stopwords(""english""))
corpus&lt;- tm_map(corpus, stripWhitespace)
dtm&lt;- as.matrix(DocumentTermMatrix(corpus))
</code></pre>

<p>Output 2 (dtm):</p>

<pre><code>Docs    application card    customer    easy    every ... etc.
1       0           0       0           1       0
2       0           1       0           0       1
3       0           1       0           0       0
4       1           1       0           0       0
5       0           0       1           0       0
</code></pre>

<p>How do I then tie this back to the original data so that it contains the fields from the original dataset and the matrix (Output 1 + Output 2):
    id,text,travel_cat + application,card,customer,easy,every...</p>
","r, nlp, text-mining","<p>Just try a <code>cbind()</code></p>

<pre><code>allcombined &lt;- cbind(dtm,df_all)
</code></pre>

<p>Is this what you are looking for?</p>
",2,2,543,2017-10-28 02:52:31,https://stackoverflow.com/questions/46985765/for-text-mining-in-r-how-do-i-combine-documenttermmatrix-with-original-data-fra
Count number of verbs for each speech in data frame R,"<p>I have a data frame as the following:</p>

<pre><code>str(data)
'data.frame':   255 obs. of  3 variables:
$ Group      : Factor w/ 255 levels ""AlzGroup1"",""AlzGroup10"",..: 1 112 179 190 201 212 223 234 245 2 ...
$ Gender     : int  1 1 0 0 0 0 0 1 0 0 ...
$ Description: Factor w/ 255 levels ""A boy's on the uh falling off the stool picking up cookies . The girl's reaching up for it . The girl the lady ""| __truncated__,..: 63 69 38 134 111 242 196 85 84 233 ...
</code></pre>

<p>in the Description column I have 255 speeches and I want to add a column to my data frame containing number of verbs in each speech, I know how to get number of verbs but the following code gives me total number of verbs in Description column:</p>

<pre><code>&gt; library(NLP);
&gt; library(tm);
&gt; library(openNLP);
NumOfVerbs=sapply(strsplit(as.character(tagPOS(data$Description)),""[[:punct:]]*/VB.?""),function(x) {res = sub(""(^.*\\s)(\\w+$)"", ""\\2"", x); res[!grepl(""\\s"",res)]} )
</code></pre>

<p>Does anyone know how can I get number of verbs in each speech?</p>

<p>Thanks for any help!</p>

<p>Elahe</p>
","r, nlp, text-mining, tm, opennlp","<p>Assuming you are using function similar to this one (found here: <a href=""https://stackoverflow.com/questions/28764056/could-not-find-function-tagpos"">could not find function tagPOS</a>):</p>

<pre><code>tagPOS &lt;-  function(x, ...) {
  s &lt;- as.String(x)
  word_token_annotator &lt;- Maxent_Word_Token_Annotator()
  a2 &lt;- Annotation(1L, ""sentence"", 1L, nchar(s))
  a2 &lt;- annotate(s, word_token_annotator, a2)
  a3 &lt;- annotate(s, Maxent_POS_Tag_Annotator(), a2)
  a3w &lt;- a3[a3$type == ""word""]
  POStags &lt;- unlist(lapply(a3w$features, `[[`, ""POS""))
  POStagged &lt;- paste(sprintf(""%s/%s"", s[a3w], POStags), collapse = "" "")
  list(POStagged = POStagged, POStags = POStags)
}
</code></pre>

<p>Create a function that counts the number of POS tags that contain the letters <code>'VB'</code></p>

<pre><code>count_verbs &lt;-function(x) {
  pos_tags &lt;- tagPOS(x)$POStags
  sum(grepl(""VB"", pos_tags))
  }
</code></pre>

<p>And use <code>dplyr</code> to group by <code>Group</code> and summarise using <code>count_verbs()</code>:</p>

<pre><code>library(dplyr)
data %&gt;% 
  group_by(Group) %&gt;%
  summarise(num_verbs = count_verbs(Description))
</code></pre>
",0,0,609,2017-10-29 18:53:36,https://stackoverflow.com/questions/47003997/count-number-of-verbs-for-each-speech-in-data-frame-r
Removing Custom Words From Text Variables in R,"<p>I have Data set which looks like following:</p>

<pre><code>dat &lt;- data.frame(ID=c(1,2,3,4,5),ADDRESS=c(""EAST SS BLVD"",""SOUTH AA STREET"",""XX EAST ST"",""ZZ NORTH ROAD"",""WEST TR TRAIL""))

&gt; dat
  ID         ADDRESS
1  1    EAST SS BLVD
2  2 SOUTH AA STREET
3  3      XX EAST ST
4  4   ZZ NORTH ROAD
5  5   WEST TR TRAIL
</code></pre>

<p>I want to remove all details in address not in list of words I want. I am using following code which is not proper and is not working.</p>

<pre><code> dat$FEATURE &lt;- gsub(""^[(BLVD)|(BOULEVARD)|(DRIVE)|(DR)|(ROAD)|(RD)|(PL)|(PLACE)
                |(SL)|(CIRCLE)|(CT)|(COURT)|(WY)|(WAY)|(ST)|(STREET)|(AVE)
                |(AVENUE)|(PKWY)|(WAY)|(PARKWAY)|(LN)|(LANE)|(HWY)|(HIGHWAY)
                |(TRAIL$)|(CIR$)]"","""",dat$ADDRESS)

&gt; dat
  ID         ADDRESS        FEATURE
1  1    EAST SS BLVD    AST SS BLVD
2  2 SOUTH AA STREET OUTH AA STREET
3  3      XX EAST ST     XX EAST ST
4  4   ZZ NORTH ROAD  ZZ NORTH ROAD
5  5   WEST TR TRAIL   EST TR TRAIL
</code></pre>

<p>Output that I want is :</p>

<pre><code>&gt; dat1
  ID         ADDRESS FEATURE
1  1    EAST SS BLVD    BLVD
2  2 SOUTH AA STREET  STREET
3  3      XX EAST ST      ST
4  4   ZZ NORTH ROAD    ROAD
5  5   WEST TR TRAIL   TRAIL
</code></pre>

<p>I am not great regex any help is appreciated and any references for regex in R will be helpful.</p>
","r, regex, text-mining, data-science, feature-extraction","<p>You may use</p>

<pre><code>(?xs).*\b        # any 0+ chars, as many as possible, then word boundary
 (               # Group 1 start:
   BLVD|BOULEVARD|DR(?:IVE)?|R(?:OA)?D|PL(?:ACE)?      # Various words
   |SL|CIRCLE|CT|COURT|WA?Y|ST(?:REET)?|AVE(?:NUE)?    # you need to keep
   |PKWY|(PARK)?:WAY|LN|LANE|HWY|HIGHWAY               # here
   |TRAIL$|CIR$                                        # and here
 )               # Group 1 end
 \b              # Word boundary
 .*              # Rest of the string.
</code></pre>

<p>See the <a href=""https://regex101.com/r/K0OzBe/1"" rel=""nofollow noreferrer"">regex demo</a></p>

<p>Here, <code>(?x)</code> is a free spacing/comment/verbose modifier enabling formatting whitespace inside the pattern and comments inside. <code>(?s)</code> is a DOTALL modifier allowing <code>.</code> match any char including a newline (it is necessary as it is a PCRE pattern, pay attention to <code>perl=TRUE</code>).</p>

<p>The <code>""\\1""</code> replacement inserts the value in Group 1 back into the replaced string.</p>

<p>See the <a href=""http://rextester.com/QPEJO16593"" rel=""nofollow noreferrer"">R demo</a>:</p>

<pre><code>dat &lt;- data.frame(ID=c(1,2,3,4,5),ADDRESS=c(""EAST SS BLVD"",""SOUTH AA STREET"",""XX EAST ST"",""ZZ NORTH ROAD"",""WEST TR TRAIL""))
dat$FEATURE &lt;- gsub(""(?xs).*\\b(BLVD|BOULEVARD|DR(?:IVE)?|R(?:OA)?D|PL(?:ACE)?
                |SL|CIRCLE|CT|COURT|WA?Y|ST(?:REET)?|AVE(?:NUE)?
                |PKWY|(PARK)?:WAY|LN|LANE|HWY|HIGHWAY
                |TRAIL$|CIR$)\\b.*"",""\\1"",dat$ADDRESS, perl=TRUE)
dat
</code></pre>

<p>Output:</p>

<pre><code>  ID         ADDRESS FEATURE
1  1    EAST SS BLVD    BLVD
2  2 SOUTH AA STREET  STREET
3  3      XX EAST ST      ST
4  4   ZZ NORTH ROAD    ROAD
5  5   WEST TR TRAIL   TRAIL
</code></pre>
",2,0,89,2017-11-03 18:09:37,https://stackoverflow.com/questions/47102073/removing-custom-words-from-text-variables-in-r
keyword matching gives repeated words in pandas column?,"<p>I have a pandas dataframe consists two columns as:-</p>

<pre><code>ID           text_data                               

1         companies are mainly working on two 
          technologies that is ai and health care.
          Company need to improve on health care.

2         Current trend are mainly depends on block chain
          and IOT where IOT is
          highly used.

3         ............
.         ...........
.         ...........
.         so on.
</code></pre>

<p>Now I have a another list as <code>Techlist=[""block chain"",""health care"",""ai"",""IOT""]</code></p>

<p>I need to match list <code>Techlist</code> with <code>text_data</code> column of pandas dataframe so I've use this code:-</p>

<pre><code>df['tech_match']=df['text_data'].apply(lambda x: [reduce(op.add, re.findall(act,x)) for act in Techlist if re.findall(act,x) &lt;&gt; []] )
</code></pre>

<p>so what I've got is something different as:-</p>

<pre><code>ID         text_data                                           tech_match
1     companies are mainly working on two          [ai,healthcarehealthcare]             
      technologies that is ai and health care.
      Company need to improve on health care.

2     current trend are mainly                     [block chain,IOTIOT]
      depends on block chain and 
      IOT where IOT is highly used.

3    .................
.    ................             
.    ...............
.    so on.
</code></pre>

<p>List and text data got matched correctly but the matched list words are repeating in <code>tech_match</code> column.</p>

<p>What I need is:-</p>

<pre><code>ID            text_data                             tech_match
1     companies are mainly working on two           [heatlh care,ai]
      technologies that is ai and health care.
      Company need to improve on health care.

2     Current trend are mainly depends on          [block chain,IOT]
      blockchain and IOT where IOT is
      highly used. 

3     ..................
.     ..................
.     .................
.     son on.
</code></pre>

<p>how can I delete these repeating words in <code>tech_match</code> column?</p>
","python, pandas, text-mining","<p>Use <code>str.split</code> and then call <code>set.intersection</code>:</p>

<pre><code>s = set([""blockchain"", ""healthcare"", ""ai"", ""IOT""])

df['matches'] = df.text_data.str.split(r'[^\w]+')\
                   .apply(lambda x: list(s.intersection(x)))
df

                                           text_data            matches
0  companies are mainly working on two technologi...   [healthcare, ai]
1  Current trend are mainly depends on blockchain...  [IOT, blockchain]
</code></pre>

<p>Thanks to <a href=""https://stackoverflow.com/a/47120018/4909087"">Bharath</a> for the setup data.</p>
",3,2,1039,2017-11-05 09:17:07,https://stackoverflow.com/questions/47119965/keyword-matching-gives-repeated-words-in-pandas-column
Cluster word2vec vectors using Affinity Propagation in python (sklearn),"<p>I want to cluster my word2vec clusters using Affinity Propagation and get the cluster center words.</p>

<p>My current code is as follows.</p>

<pre><code>model = word2vec.Word2Vec.load(""word2vec"")
word_vectors = model.wv.syn0
affprop = AffinityPropagation(affinity=""precomputed"", damping=0.5)
af= affprop.fit(word_vectors)
</code></pre>

<p>However, this raise the following error:
<code>ValueError: S must be a square array (shape=(77, 300))</code></p>

<p>As I understand 300 means word2vec hidden layer dimension and 77 is my vocabulary size.</p>

<p>I am just wondering how to use Affinity Propagation for word2vec vectors that are not square arrays.</p>

<p>Please help me!</p>
","python, scikit-learn, cluster-analysis, text-mining, word2vec","<p>It's because you stated that the affinity between samples is already computed, which is always a square matrix. So you can either use Euclidean distance which is implemented, or if you want to use a different metric you have to precompute it, see the example code below:</p>

<pre><code># some dummy data
word_vectors = np.random.random((77, 300))

# using eucliden distance
affprop = AffinityPropagation(affinity='euclidean', damping=0.5)
af = affprop.fit(word_vectors)

# using cosine
from sklearn.metrics.pairwise import cosine_distances
word_cosine = cosine_distances(word_vectors)
affprop = AffinityPropagation(affinity='precomputed', damping=0.5)
af = affprop.fit(word_cosine)
</code></pre>
",5,4,2256,2017-11-07 08:31:19,https://stackoverflow.com/questions/47153203/cluster-word2vec-vectors-using-affinity-propagation-in-python-sklearn
Text Mining R Package &amp; Regex to handle Replace Smart Curly Quotes,"<p>I've got a bunch of texts like this below with different smart quotes - for single and double quotes. All I could end up with the packages I'm aware of is to remove those characters but I want them to replaced with the normal quotes. </p>

<pre><code>textclean::replace_non_ascii(""You don‘t get “your” money’s worth"")
</code></pre>

<p>Received Output: <code>""You dont get your moneys worth""</code></p>

<p>Expected Output: <code>""You don't get ""your"" money's worth""</code></p>

<p>Also would appreciate if someone's got the regex to replace every such quotes in one shot. </p>

<p>Thanks!</p>
","r, regex, text-mining","<p>Use two <code>gsub</code> operations: 1) to replace double curly quotes, 2) to replace single quotes:</p>

<pre><code>&gt; gsub(""[“”]"", ""\"""", gsub(""[‘’]"", ""'"", text))
[1] ""You don't get \""your\"" money's worth""
</code></pre>

<p>See the <a href=""https://ideone.com/VPDT3Q"" rel=""nofollow noreferrer"">online R demo</a>. Tested in both Linux and Windows, and works the same.</p>

<p>The <code>[“”]</code> construct is a positive <a href=""https://www.regular-expressions.info/charclass.html"" rel=""nofollow noreferrer"">character class</a> that matches any single char defined in the class.</p>

<p>To normalize all chars similar to double quotes, you might want to use </p>

<pre><code>&gt; sngl_quot_rx = ""[ʻʼʽ٬‘’‚‛՚︐]""
&gt; dbl_quot_rx = ""[«»““”„‟≪≫《》〝〞〟\＂″‶]""
&gt; res = gsub(dbl_quot_rx, ""\"""", gsub(sngl_quot_rx, ""'"", `Encoding&lt;-`(text, ""UTF8""))) 
&gt; cat(res, sep=""\n"")
You don't get ""your"" money's worth
</code></pre>

<p>Here, <code>[«»““”„‟≪≫《》〝〞〟＂″‶]</code> matches</p>

<pre><code>«   00AB  LEFT-POINTING DOUBLE ANGLE QUOTATION MARK
»   00BB  RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK
“   05F4  HEBREW PUNCTUATION GERSHAYIM
“   201C  LEFT DOUBLE QUOTATION MARK
”   201D  RIGHT DOUBLE QUOTATION MARK
„   201E  DOUBLE LOW-9 QUOTATION MARK
‟   201F  DOUBLE HIGH-REVERSED-9 QUOTATION MARK
≪  226A  MUCH LESS-THAN
≫  226B  MUCH GREATER-THAN
《  300A  LEFT DOUBLE ANGLE BRACKET
》  300B  RIGHT DOUBLE ANGLE BRACKET
〝  301D  REVERSED DOUBLE PRIME QUOTATION MARK
〞  301E  DOUBLE PRIME QUOTATION MARK
〟  301F  LOW DOUBLE PRIME QUOTATION MARK
＂  FF02  FULLWIDTH QUOTATION MARK
″   2033  DOUBLE PRIME
‶   2036  REVERSED DOUBLE PRIME
</code></pre>

<p>The <code>[ʻʼʽ٬‘’‚‛՚︐]</code> is used to normalize some chars similar to single quotes:</p>

<pre><code>ʻ  02BB  MODIFIER LETTER TURNED COMMA
ʼ  02BC  MODIFIER LETTER APOSTROPHE
ʽ  02BD  MODIFIER LETTER REVERSED COMMA
٬  066C  ARABIC THOUSANDS SEPARATOR
‘  2018  LEFT SINGLE QUOTATION MARK
’  2019  RIGHT SINGLE QUOTATION MARK
‚  201A  SINGLE LOW-9 QUOTATION MARK
‛  201B  SINGLE HIGH-REVERSED-9 QUOTATION MARK
՚   055A  ARMENIAN APOSTROPHE
︐  FE10  PRESENTATION FORM FOR VERTICAL COMMA
</code></pre>
",5,5,2639,2017-11-08 07:08:25,https://stackoverflow.com/questions/47173557/text-mining-r-package-regex-to-handle-replace-smart-curly-quotes
Web scraping using Python BeautifulSoup,"<pre><code>from urllib.request import urlopen as uReq 
from bs4 import BeautifulSoup as soup
my_url=""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061""

uClient=uReq(my_url)

page_html=uClient.read()

page_soup=soup(page_html,""html.parser"")

containers=page_soup.findAll(""div"",{""class"":""row review-article""})
print(len(containers))
print(containers[0].a)
</code></pre>

<p>I want to get the link of profile (Chitanverma in the given picture) as my output but I get the link of Reliance Jio services as my output.</p>

<p><img src=""https://i.sstatic.net/MsDpO.png"" alt=""picture[1]""></p>

<p>I would be grateful if someone help me correct my code to get the expected output and also explain why I am getting the link of Reliance Jio services as my output.</p>

<p>My intention is to scrape the names of the profile from the webpage <a href=""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061"" rel=""nofollow noreferrer"">http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061</a>.</p>
","python, web-scraping, beautifulsoup, containers, text-mining","<p>In this case you have to rely on any browser simulator to grab the dynamically generated content. Selenium can be an option to go with. Try the below example if you have installed selenium in your machine already.</p>

<pre><code>from bs4 import BeautifulSoup
from selenium  import webdriver

driver = webdriver.Chrome()
driver.get('http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061')
soup = BeautifulSoup(driver.page_source,""lxml"")
for link in soup.select("".profile""):
    try:
        profile = link.select(""p:nth-of-type(1) a"")[0]
    except:pass      
    print(profile.text, profile['href'])

driver.quit()
</code></pre>

<p>Partial output:</p>

<pre><code>chintanverma http://www.mouthshut.com/chintanverma
ganeshgauttam http://www.mouthshut.com/ganeshgauttam
viratvenkat1 http://www.mouthshut.com/viratvenkat1
ms37872 http://www.mouthshut.com/ms37872
bibekdas http://www.mouthshut.com/bibekdas
</code></pre>
",3,0,278,2017-11-11 13:15:31,https://stackoverflow.com/questions/47238292/web-scraping-using-python-beautifulsoup
Fast lexicon lookup with phrases and stemming in python,"<p>I'm building a text classifier in python and I have a list of key phrases for each class. For example the classes can be ""travel"" and ""science"" and lists could contain: </p>

<ul>
<li>travel: ""New York"", ""South Korea"", ""Seoul"", etc.</li>
<li>science: ""scientist"", ""chemical"", etc.</li>
</ul>

<p>I'm looking for the best way to match phrases from such lists in python. </p>

<p>For instance, the result for the document:</p>

<blockquote>
  <p>A famous scientist traveled from New York to Seoul, South Korea</p>
</blockquote>

<p>should be:
""science"": 1
""travel"": 3</p>

<p>Even if the ""in"" operator for strings is well optimized, there are a few situations which should be handled as well:</p>

<ul>
<li>word boundaries: at some point I can have ""to"" in the dictionary, and wouldn't want to match ""to"" in ""tomorrow"". Tokenization would work in this case, but some custom logic would be required for phrases, maybe sub-list lookup in the list of tokens.</li>
<li>stemming: ""scientists discovered"" should also be matched when there is ""scientist discover"" in the list</li>
</ul>

<p>Is there a python library which could handle this efficiently? If I need to implement it from scratch, what would be the best way to handle the above issues in terms of performance?</p>
","python, nlp, text-mining","<p>What you try to achieve is phrasal search on stems. It's text mining
task I think and is implemented in search engines.</p>

<p>First you need <code>tokenize</code> and <code>stemmer</code> functions. Tokenize can be
as simple as:</p>

<pre><code>def tokenize(string):
    return fiter(lambda x: len(x) &lt; 1, remove_punctuation(string).split())
</code></pre>

<p>There is various stemmers on pypi.</p>

<p>You will end up with a function like the following:</p>

<pre><code>def preprocess(string):
    return [stemmer(word) for word in tokenize(string)]
</code></pre>

<p>Then the function you are looking for looks like the following:</p>

<pre><code>from collections import Counter


def count(dictionary, phrase):
    counter = Count()
    phrase = preprocess(phrase)
    for topic, string in dictionary.items():
        stems = preprocess(string)
        indices = find(phrase, stem[0])
        for index in indices:
            found = True
            for stem in stems[1:]:
                if phrase[index + 1] == stem:
                   continue
                else:
                   found = False
                   break
            if found:
               counter[topic] +=1
    return counter
</code></pre>

<p>The <code>dictionary</code> variable  contains the following information:</p>

<ul>
<li>travel: ""New York"", ""South Korea"", ""Seoul"", etc.</li>
<li>science: ""scientist"", ""chemical"", etc.</li>
</ul>
",1,1,793,2017-11-15 12:44:07,https://stackoverflow.com/questions/47307765/fast-lexicon-lookup-with-phrases-and-stemming-in-python
Extract top features by frequency per document from a dtm in R,"<p>I have a dtm and want to extract the top 5 terms by frequency for each document from the document term matrix.</p>

<p>I have a <strong>dtm</strong> built using the tm package </p>

<pre><code>  Terms                     
Docs aaaa aac abrt abused accept accepted
1 0 0 0 0 0 0 
2 0 0 0 0 0 0
3 0 0 0 0 0 0
4 0 0 0 0 0 0
5 0 0 0 0 0 0
6 0 0 0 0 0 0
</code></pre>

<p><strong>required output should be of the form:</strong></p>

<pre><code>Id   
1   Term1 Term2 Term3 Term4 Term5
2   Term1 Term2 Term3 Term4 Term5
and so on for all the documents.
</code></pre>

<p>I have tried all the solutions available from stackoverflow ans other sources
like <a href=""https://stackoverflow.com/questions/15506118/make-dataframe-of-top-n-frequent-terms-for-multiple-corpora-using-tm-package-in"">Make dataframe of top N frequent terms for multiple corpora using tm package in R</a> (converted to tdm and tried to bring to the output form but did not work)and others but noting seem to work.</p>
","r, text-mining, tm, term-document-matrix","<p>Using Quanteda:</p>

<pre><code>library(quanteda)
txt &lt;- c(""hello world world fizz"", ""foo bar bar buzz"")
dfm &lt;- dfm(txt)
topfeatures(dfm, n = 2, groups = seq_len(ndoc(dfm)))
# $`1`
# world hello 
# 2     1 
# 
# $`2`
# bar foo 
# 2   1 
</code></pre>

<p>You can also convert between <code>DocumentTermMatrix</code> and <code>dfm</code>. </p>

<p>Or using the classical <code>tm</code>: </p>

<pre><code>library(tm)
packageVersion(""tm"")
# [1] ‘0.7.1’
txt &lt;- c(doc1=""hello world world"", doc2=""foo bar bar fizz buzz"")
dtm &lt;- DocumentTermMatrix(Corpus(VectorSource(txt)))
n &lt;- 5
(top &lt;- findMostFreqTerms(dtm, n = n))
# $doc1
# world hello 
# 2     1 
# 
# $doc2
# bar buzz fizz  foo 
# 2    1    1    1 
do.call(rbind, lapply(top, function(x) { x &lt;- names(x);length(x)&lt;-n;x }))
# [,1]    [,2]    [,3]   [,4]  [,5]
# doc1 ""world"" ""hello"" NA     NA    NA  
# doc2 ""bar""   ""buzz""  ""fizz"" ""foo"" NA 
</code></pre>

<p><code>findMostFreqTerms</code> is available since <a href=""https://cran.r-project.org/web/packages/tm/news.html"" rel=""nofollow noreferrer"">tm version 0.7-1</a>.</p>
",1,0,890,2017-11-16 11:42:58,https://stackoverflow.com/questions/47328799/extract-top-features-by-frequency-per-document-from-a-dtm-in-r
Convert Large Document Term Document Matrix into Matrix,"<p>I've got a large Term Document Matrix. (6 elements, 44.3 Mb)</p>

<p>I need to covert it into a matrix but when trying to do it I get the magical error message: ""cannot allocate 100 GBs"".</p>

<p>Is there any package/library that allows to do this transformation in chunks?</p>

<p>I've tried ff and bigmemory but they do not seem to allow conversions from DTMs to Matrix. </p>
","r, out-of-memory, bigdata, text-mining, data-conversion","<p>Before converting to matrix, remove sparse terms from Term Document Matrix. This will reduce your matrix size significantly. To remove sparse terms, you can do as below:</p>

<pre><code> library(tm)
 ## tdm - Term Document Matrix
 tdm2 &lt;- removeSparseTerms(tdm, sparse = 0.2)
 tdm_Matrix &lt;- as.matrix(tdm2)
</code></pre>

<p>Note: I put 0.2 for sparse just for an example. You should decide that value based on your tdm.</p>

<p>Here are some link that would shed light on <code>removeSparseTerms</code> function and <code>sparse</code> value:</p>

<p><a href=""https://stackoverflow.com/questions/28763389/how-does-the-removesparseterms-in-r-work"">How does the removeSparseTerms in R work?</a></p>

<p><a href=""https://www.rdocumentation.org/packages/tm/versions/0.7-1/topics/removeSparseTerms"" rel=""nofollow noreferrer"">https://www.rdocumentation.org/packages/tm/versions/0.7-1/topics/removeSparseTerms</a></p>
",4,0,2096,2017-11-21 00:59:53,https://stackoverflow.com/questions/47403591/convert-large-document-term-document-matrix-into-matrix
Text Mining in R | memory management,"<p>I am using a text file of 160 MB and doing data mining, but seems once I convert it to matrix to know the word frequency then its demanding too much memory, can someone one please help me in this </p>

<pre><code>&gt; dtm &lt;- DocumentTermMatrix(clean)
&gt; dtm
&lt;&lt;DocumentTermMatrix (documents: 472029, terms: 171548)&gt;&gt;
Non-/sparse entries: 3346670/80972284222
Sparsity           : 100%
Maximal term length: 126
Weighting          : term frequency (tf)
&gt; as.matrix(dtm)
</code></pre>

<blockquote>
  <p>Error: cannot allocate vector of size 603.3 Gb</p>
</blockquote>
","r, text-mining","<p>@Vineet here is the math that shows why R tried to allocate 603Gb to convert the document term matrix to a non-sparse matrix. Each number cell in a matrix in R consumes 8 bytes. Based on the size of the document term matrix in the question, the math looks like:</p>

<pre><code>&gt; # 
&gt; # calculate memory consumed by matrix
&gt; #
&gt; 
&gt; rows &lt;- 472029 # 
&gt; cols &lt;- 171548
&gt; # memory in gigabytes
&gt; rows * cols * 8 / (1024 * 1024 * 1024)
[1] 603.3155
</code></pre>

<p>If you want to calculate the word frequencies, you're better off generating 1-grams and then summarizing them into a frequency distribution. </p>

<p>With the <code>quanteda</code> package the code would look like this.</p>

<pre><code>words &lt;- tokenize(...) 
ngram1 &lt;- unlist(tokens_ngrams(words,n=1))
ngram1freq &lt;- data.frame(table(ngram1))
</code></pre>

<p>regards,</p>

<p>Len</p>

<p><strong>2017-11-24 UPDATE:</strong> Here is a complete example from the quanteda package that generates the frequency distribution from a document feature matrix using the <code>textstat_frequency()</code> function, as well as a <code>barplot()</code> for the top 20 features. </p>

<p>This approach does not require the generation &amp; aggregation of n-grams into a frequency distribution. </p>

<pre><code>library(quanteda)
myCorpus &lt;- corpus(data_char_ukimmig2010)
system.time(theDFM &lt;- dfm(myCorpus,tolower=TRUE,
                      remove=c(stopwords(),"","",""."",""-"",""\"""",""'"",""("","")"","";"","":"")))
system.time(textFreq &lt;- textstat_frequency(theDFM))

hist(textFreq$frequency,
     main=""Frequency Distribution of Words: UK 2010 Election Manifestos"")

top20 &lt;- textFreq[1:20,]
barplot(height=top20$frequency,
        names.arg=top20$feature,
        horiz=FALSE,
        las=2,
        main=""Top 20 Words: UK 2010 Election Manifestos"")
</code></pre>

<p>...and the resulting barplot: </p>

<p><a href=""https://i.sstatic.net/tTIRJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tTIRJ.png"" alt=""enter image description here""></a></p>
",3,1,937,2017-11-21 13:33:19,https://stackoverflow.com/questions/47414401/text-mining-in-r-memory-management
Count Number of Pages per AGENDA- Text Mining in r,"<p>I have to count the number of pages per AGENDA ITEM. I have extracted text from pdf document into a data frame, essentially one row of this data frame contains one page of text. This is how my data looks like: </p>

<pre><code>mydf &lt;- data.frame(text = c(""AGENDA ITEM 1
        4"", ""This particular row contains a lot of text, really its all text present in one page"", 
        ""So ineffect, one page of text per row"", ""This is another page of text in this row"", 
        ""lets include another page for agenda 1"", ""AGENDA ITEM 2
        9"",
        ""now all the text in agenda 2 is included here"",""the 2nd page text of agenda 2"", 
        ""AGENDA ITEM 3
        12"", ""Now lets just add one row for this agenda, meaning it only has one page inside it""))
</code></pre>

<p>Under the AGENDA TEXT (Same row), the number is the page number and it's in the same row. To count the number of Pages per Agenda, I just need to count the Number of rows until the next AGENDA ITEM appears. Considering the above example the answer should be </p>

<pre><code>AGENDA ITEM 1 = 4 Pages, AGENDA ITEM 2 = 2 Pages and AGENDA ITEM 3 = 1 Page.
</code></pre>

<p>How will I do this? 
I am fairly new to analysing text. Thanks</p>
","r, nlp, text-mining","<p>In case the pattern ""AGENDA ITEM ##"" does not appear within your normal text you may use the following approach using <code>grep()</code>. I hope this works for you.</p>

<pre><code>#get all rownumbers of rows starting with the pattern
start_rows &lt;- grep(""AGENDA ITEM \\d+"", mydf$text)

#get the end of each ""AGENDA ITEM chapter""
#a chapter ends one line before the next chapter starts, hence, 
#-1 and offset -1 from startrows
#and the final chapter ends with the last line
end_rows &lt;- c(start_rows[-1]-1
              ,length(mydf$text))

end_rows-start_rows
#[1] 4 2 1
</code></pre>
",1,0,52,2017-11-27 10:31:28,https://stackoverflow.com/questions/47508792/count-number-of-pages-per-agenda-text-mining-in-r
"Removing all &quot;H&quot; within the strings, EXCEPT the ones including &quot;CH&quot;","<p>I am trying to remove all ""H"" within the strings, EXCEPT the ones including ""CH"" in the following example:</p>

<pre><code>strings &lt;- c(""Cash"",""Wishes"",""Chain"",""Chip"",""Check"")
</code></pre>

<p>I found that the code below remove only  ""H""</p>

<pre><code>data&lt;- gsub(""H"", """", strings)
</code></pre>
","r, regex, text-mining, data-cleaning","<p>You can do this with a negative look-behind. </p>

<pre><code>gsub(""(?&lt;!c)h"", """", strings, perl=TRUE, ignore.case = TRUE)
</code></pre>
",4,4,63,2017-11-28 18:47:27,https://stackoverflow.com/questions/47538826/removing-all-h-within-the-strings-except-the-ones-including-ch
A regex to remove all words which contains number in R,"<p>I want to write a regex in R to remove all words of a string containing numbers.</p>

<p>For example:</p>

<pre><code>first_text = ""a2c if3 clean 001mn10 string asw21""
second_text = ""clean string
</code></pre>
","r, regex, text-mining","<p>Try with <code>gsub</code></p>

<pre><code>trimws(gsub(""\\w*[0-9]+\\w*\\s*"", """", first_text))
#[1] ""clean string""
</code></pre>
",10,3,3697,2017-12-03 13:40:13,https://stackoverflow.com/questions/47618951/a-regex-to-remove-all-words-which-contains-number-in-r
Sentiment analysis for tidytext in R,"<p>I am trying to perform sentiment analysis in R.
I want to use either afinn or bing lexicon, but the problem is i cant tokenize the words.</p>

<p>Here are the words for which i need the sentiments for : </p>

<p><a href=""https://i.sstatic.net/hnSdD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hnSdD.png"" alt=""sentiment_words""></a></p>

<p>So there are 6 words for whom i want sentiments for : 
Pass
Fail
Not Ready
Out of Business
Pass w/conditions
No entry</p>

<p>How do i use any of the lexicons to assign sentiments to these words</p>

<p>Here is my code : </p>

<pre><code>d&lt;- as.data.frame(data$Results)
d&lt;- as.data.frame(d[1:2000,])

colnames(d) &lt;- ""text""



#Making preprocessed file for raw data
preprocess&lt;-data.frame(text=sapply(tweet_corpus_clean, identity), 
                       stringsAsFactors=F)

# tokenize
tokens &lt;- data_frame(text = preprocess$text) %&gt;% unnest_tokens(word, text)
</code></pre>

<p>When run this i get : </p>

<p><a href=""https://i.sstatic.net/2w2DH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2w2DH.png"" alt=""senti_new""></a></p>

<p>Because for lexicons to assign sentiments it has to be one token per row</p>

<p>So i had to merge those words together.
Now when i use afinn its not able to understand what outofbusiness is obvioulsy</p>

<pre><code>tokens &lt;- data_frame(text = preprocess$text) %&gt;% unnest_tokens(word, text)


contributions = tokens %&gt;%ungroup()%&gt;%
  inner_join(get_sentiments(""afinn""), by = ""word"") %&gt;%
  group_by(word) %&gt;%
  summarize(score = as.numeric(sum(score * n) / sum(n))) %&gt;%
  arrange(desc(sentiment))
</code></pre>

<p>how do i do sentiment analysis for those 6 tpes of words?</p>
","r, text-mining, sentiment-analysis, tidyverse, tidytext","<p>Hmmmm, this doesn't sounds like a sentiment analysis problem to me. You have six words/phrases that you know about exactly, and you know what they mean in your context. This sounds like you just want to assign these words/phrases scores, or even just levels of a factor.</p>

<p>You could do something like what I show here, where <em>you</em> as the analyst decide what score each of your phrases should have. Here, <code>scores</code> is the dataframe that you as the analyst construct with sensibly chosen scores for each text options, and <code>df</code> is the data you are analyzing.

<br/></p>

<pre class=""lang-r prettyprint-override""><code>library(dplyr)

scores &lt;- data_frame(text = c(""pass"",
                              ""fail"",
                              ""not ready"",
                              ""out of business"",
                              ""pass w/conditions"",
                              ""no entry""),
                     score = c(3, -1, 0, 0, 2, 1))

scores
#&gt; # A tibble: 6 x 2
#&gt;   text              score
#&gt;   &lt;chr&gt;             &lt;dbl&gt;
#&gt; 1 pass               3.00
#&gt; 2 fail              -1.00
#&gt; 3 not ready          0   
#&gt; 4 out of business    0   
#&gt; 5 pass w/conditions  2.00
#&gt; 6 no entry           1.00

df &lt;- data_frame(text = c(""pass"",
                          ""pass"",
                          ""fail"",
                          ""not ready"",
                          ""out of business"",
                          ""no entry"",
                          ""fail"",
                          ""pass w/conditions"",
                          ""fail"",
                          ""no entry"",
                          ""pass w/conditions""))

df %&gt;%
  left_join(scores)
#&gt; Joining, by = ""text""
#&gt; # A tibble: 11 x 2
#&gt;    text              score
#&gt;    &lt;chr&gt;             &lt;dbl&gt;
#&gt;  1 pass               3.00
#&gt;  2 pass               3.00
#&gt;  3 fail              -1.00
#&gt;  4 not ready          0   
#&gt;  5 out of business    0   
#&gt;  6 no entry           1.00
#&gt;  7 fail              -1.00
#&gt;  8 pass w/conditions  2.00
#&gt;  9 fail              -1.00
#&gt; 10 no entry           1.00
#&gt; 11 pass w/conditions  2.00
</code></pre>

<p>Sentiment analysis is most appropriate where you have large amounts of unstructured text that you need to extract insight from. Here you have only six text elements, and you can use what you know about your domain and context to assign scores.</p>
",1,0,627,2017-12-03 22:01:15,https://stackoverflow.com/questions/47623809/sentiment-analysis-for-tidytext-in-r
How can I make my code more efficient in R - It&#39;s to repetitive,"<p>I have a question regarding the efficiency of my code. I have 9 data frames in my environment and for each of them I need to perform the same steps. The steps and the code is (only shown for two of the data frames):</p>

<pre><code>CDL &lt;- aggregate(A$Frequency, by=list(Category=A$Words), FUN=sum)
wordcloud(words = CDL$Category, freq = CDL$x, min.freq = 2,
      max.words=250, random.order=FALSE, rot.per=0.35, 
      colors=brewer.pal(6, ""Dark2""))

Ltd &lt;- aggregate(B$Frequency, by=list(Category=B$Words), FUN=sum)
wordcloud(words = Ltd$Category, freq = Ltd$x, min.freq = 2,
      max.words=250, random.order=FALSE, rot.per=0.35, 
      colors=brewer.pal(6, ""Dark2""))
</code></pre>

<p>I first aggregate all the same words, sum their frequencies and then create a world cloud based on the aggregated results. </p>

<p>The object names in the environment start from 'A' and go all the way to 'I'. The variable 'Frequency' is just a number, the variable 'Words' contains a list of the words. </p>

<p>For the wordcloud: 
The variable 'Category' contains the unique words taken from the 'Words' variable and 'x' is the aggregated sum of Frequencies taken from 'Frequency'</p>

<p>Is there any way I could perform the same but without repeating my code? Thanks</p>
","r, nlp, text-mining","<p>If I am not mistaken, defining a function and loop it should work! </p>

<pre><code>word_cloud &lt;- function(df) {
  temp &lt;- aggregate(df[,""Frequency""], by=list(Category=df[,""Words""]), FUN=sum)
  result &lt;- wordcloud(words = temp[,""Category""], 
    freq = temp[,""x""],
    min.freq = 2,
    max.words=250,
    random.order=FALSE,
    rot.per=0.35,
    colors=brewer.pal(6, ""Dark2""))
  return(result)
}

input_list &lt;- list(A,B,C,D,E,F,G,H,I)

for (df_inp in input_list) {
  word_cloud(df_inp)
}
</code></pre>

<p>I do not know the output mode of wordcloud(), so you may want to save the results to a list or plot it! I hope that helps!</p>
",2,1,73,2017-12-04 11:07:19,https://stackoverflow.com/questions/47631945/how-can-i-make-my-code-more-efficient-in-r-its-to-repetitive
Match and replace misspelled words in a string in R,"<p>I have a list of phrases, in which I want to replace certain words with a similar word, in case it is misspelled.</p>

<pre><code>library(stringr)
a4 &lt;- ""I would like a cheseburger and friees please""
badwords.corpus &lt;- c(""cheseburger"", ""friees"")
goodwords.corpus &lt;- c(""cheeseburger"", ""fries"")

vect.corpus &lt;- goodwords.corpus
names(vect.corpus) &lt;- badwords.corpus

str_replace_all(a4, vect.corpus)
# [1] ""I would like a cheeseburger and fries please""
</code></pre>

<p>everything works perfectly, until it finds a similar string, and replaces it with another word</p>

<p>if I have a pattern like the following:</p>

<p><code>""plea""</code>, the correct one is <code>""please""</code>, but when I execute it removes it and replaces it with <code>""pleased""</code>.</p>

<p>What I am looking for is that if a string is already correct, it is no longer modified, in case it finds a similar pattern.</p>
","r, regex, string, text-mining, text-processing","<p><code>str_replace_all</code> takes regex as the pattern, so you can <code>paste0</code> word boundaries <code>\\b</code> around each <code>badwords</code> so that a replacement will only be made if the whole word is matched:</p>

<pre><code>library(stringr)
string &lt;- c(""tre"", ""tree"", ""teeasing"", ""tesing"") 
goodwords &lt;- c(""tree"", ""three"", ""teasing"", ""testing"") 
badwords &lt;- c(""tre"", ""thre"", ""teeasing"", ""tesing"") 

# Paste word boundaries around badwords
badwords &lt;- paste0(""\\b"", badwords, ""\\b"")

vect.corpus &lt;- goodwords 
names(vect.corpus) &lt;- badwords 

str_replace_all(string, vect.corpus) 
[1] ""tree""    ""tree""    ""teasing"" ""testing""
</code></pre>

<p>The advantage of this is that you don't have to keep track of which strings are the longer strings. </p>

<p>This is what <code>badwords</code> looks like after pasting:</p>

<pre><code>&gt; badwords
[1] ""\\btre\\b""      ""\\bthre\\b""     ""\\bteeasing\\b"" ""\\btesing\\b""
</code></pre>
",0,1,1528,2017-12-08 20:24:52,https://stackoverflow.com/questions/47721652/match-and-replace-misspelled-words-in-a-string-in-r
Get the word from stem (stemming),"<p>I am using porter stemmer as follows to get the stem of my words.</p>

<pre><code>from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()
def stem_tokens(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed
</code></pre>

<p>Now, I want to know the possibility of some word from the stem to make it readable. For example <code>environ</code> to <code>environment</code> or <code>educ</code> to <code>education</code> etc. Is it possible to do?</p>
","nlp, nltk, text-mining, stemming","<p>What, so you want to take a stem and map it to a list of possible words in a dictionary that stem back to it?</p>

<p>This is difficult because the stemming process is lossy and because it's not a 1:1 transformation.</p>

<p>That said, in some cases like <code>environ -&gt; {environment, environments, environmental}</code> and <code>educ -&gt; {educate, educational, education, educated, educating}</code> you can get by with a trie structure where you do a prefix lookup. Things get more interesting for stems like <code>happi</code> which has to map back to <code>happy</code></p>

<p>In the general case, you would have to start with a dictionary and then produce an inverted index by stemming each word and mapping the stem back to the source word in the index. Using the inverted index you can then look up matches given a stem.</p>

<p>Hope this helps..</p>
",3,2,1378,2017-12-09 08:39:08,https://stackoverflow.com/questions/47726833/get-the-word-from-stem-stemming
R - create wordcloud from most used categories,"<p>I am trying to create a word-cloud from the most used categories tags in some videos.</p>

<p>Everything runs OK, BUT when the document matrix is created some of the categories split into individual words. these affected categories use the ""&amp;"" symbol between words. </p>

<p>(examples: River &amp; Lake, Sea &amp; Islands, Beach &amp; Cliffs,...)</p>

<p>How to keep those words together and create the word-cloud correctly?</p>

<pre><code>library(""tm"")
library(""SnowballC"")
library(""wordcloud"")
library(""RColorBrewer"")

#load the text data into docs variable
docs &lt;- Corpus(VectorSource(textos))
toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, "" "", x))

#Text Mining. 
docs &lt;- tm_map(docs, toSpace, ""/"")
docs &lt;- tm_map(docs, toSpace, ""@"")
docs &lt;- tm_map(docs, toSpace, ""\\|"")
docs &lt;- tm_map(docs, stripWhitespace)
</code></pre>

<p><a href=""https://i.sstatic.net/oyzpR.png"" rel=""nofollow noreferrer"">screenshot of function inspect(docs) showing the words</a></p>

<pre><code>#Document matrix is a table containing the frequency of the words. 
#Column names are words and row names are documents. 
#The function TermDocumentMatrix() from text mining package can be used as follow

dtm &lt;- TermDocumentMatrix(docs)
m &lt;- as.matrix(dtm)
v &lt;- sort(rowSums(m),decreasing=TRUE)
d &lt;- data.frame(word = names(v),freq=v)
head(d, 10)
</code></pre>

<p><a href=""https://i.sstatic.net/5TMEp.png"" rel=""nofollow noreferrer"">after applying TermDocumentMatrix. the categories with ""&amp; symbol are separated in individual words</a></p>

<pre><code>#plot the wordcloud

wordcloud(words = d$word, freq = d$freq, scale = c(3,.4), min.freq = 1,
          max.words=Inf, random.order=FALSE, rot.per=0.15, 
          colors=brewer.pal(6, ""Dark2""))
</code></pre>

<p><a href=""https://i.sstatic.net/OqdbS.png"" rel=""nofollow noreferrer"">result of wordcloud showing the most used categories</a></p>
","r, text-mining, word-cloud","<p>Your first screenshot shows that you can create a vector of words like this:</p>

<pre><code>docs = c(""A &amp; B"", ""A &amp; B"", ""C"", ""C"", ""C"", NA, ""A &amp; B"", ""A &amp; B"", ""A &amp; B"", NA)
</code></pre>

<p>Where your words still include <code>&amp;</code>.</p>

<p>Then you can just skip the process that splits on <code>&amp;</code> and run this instead:</p>

<pre><code>library(dplyr)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)

df_docs_counts = data.frame(docs, stringsAsFactors = F) %&gt;%  # create a dataframe of words
      na.omit() %&gt;%                                          # exclude NAs
      count(docs, sort=T)                                    # count number for each word

wordcloud(df_docs_counts$docs, df_docs_counts$n)
</code></pre>
",0,0,471,2017-12-11 11:55:11,https://stackoverflow.com/questions/47752408/r-create-wordcloud-from-most-used-categories
Text mining with tm.plugin.webmining package using GoogleFinanceSource function,"<p>I am studying text mining on the online book <a href=""http://tidytextmining.com/"" rel=""nofollow noreferrer"">http://tidytextmining.com/</a>.
In the fifth chapter:
<a href=""http://tidytextmining.com/dtm.html#financial"" rel=""nofollow noreferrer"">http://tidytextmining.com/dtm.html#financial</a></p>

<p>the following code:</p>

<pre><code>library(tm.plugin.webmining)
library(purrr)

company &lt;- c(""Microsoft"", ""Apple"", ""Google"", ""Amazon"", ""Facebook"",
             ""Twitter"", ""IBM"", ""Yahoo"", ""Netflix"")
symbol &lt;- c(""MSFT"", ""AAPL"", ""GOOG"", ""AMZN"", ""FB"", ""TWTR"", ""IBM"", ""YHOO"", ""NFLX"")

download_articles &lt;- function(symbol) {
    WebCorpus(GoogleFinanceSource(paste0(""NASDAQ:"", symbol)))
}
stock_articles &lt;- data_frame(company = company,
                             symbol = symbol) %&gt;%
    mutate(corpus = map(symbol, download_articles))
</code></pre>

<p>gives me the error:</p>

<pre><code>StartTag: invalid element name
Extra content at the end of the document
Error: 1: StartTag: invalid element name
2: Extra content at the end of the document
</code></pre>

<p>Any hints? 
Someone suggested to remove company and symbol related to ""Twitter"", but it still doesn't work and returns the same error.
Many thanks in advance</p>
","r, text-mining, tm","<p>The problem is the package <code>tm.plugin.webmining</code> is out of date. </p>

<p>Only the <code>YahooFinanceSource</code> and <code>YahooNewsSource</code> are alive at the time of this reply. </p>

<hr>

<p>Here is a quick reference and test.</p>

<p>From the <a href=""https://cran.r-project.org/web/packages/tm.plugin.webmining/vignettes/ShortIntro.pdf"" rel=""nofollow noreferrer"">Vignette page</a> written by the author, there should be 8 possible source sites:</p>

<ol>
<li>GoogleBlogSearchSource</li>
<li>GoogleFinaceSource</li>
<li>GoogleNewsSource</li>
<li>NYTimesSource</li>
<li>ReutersNewsSource</li>
<li>YahooFinanceSource</li>
<li>YahooInplaySource</li>
<li>YahooNewsSource</li>
</ol>

<p>But according to the <a href=""https://github.com/mannau/tm.plugin.webmining"" rel=""nofollow noreferrer"">Github page</a>, the first one ""GoogleBlogSearchSource"" has already been proven to be discontinued. For the 7 sources remained, I did a simple test to see if they work:</p>

<pre><code>library(tm)
library(tm.plugin.webmining)

googlefinance &lt;- WebCorpus(GoogleFinanceSource(""A""))
googlenews &lt;- WebCorpus(GoogleNewsSource(""A""))
nytimes &lt;- WebCorpus(NYTimesSource(""A"", appid = nytimes_appid))
reutersnews &lt;- WebCorpus(ReutersNewsSource(""A""))
yahoofinance &lt;- WebCorpus(YahooFinanceSource(""A""))
yahooinplay &lt;- WebCorpus(YahooInplaySource())
yahoonews &lt;- WebCorpus(YahooNewsSource(""M""))
</code></pre>

<p>The result shows that all the yahoo's sourses are technically still running, but the <code>YahooInplaySource</code> returns 0 documents no matter what parameter I chose. </p>

<pre><code>&gt; googlefinance &lt;- WebCorpus(GoogleFinanceSource(""NASDAQ:MSFT""))
StartTag: invalid element name
Extra content at the end of the document
Error in inherits(x, ""WebSource"") : 1: StartTag: invalid element name
2: Extra content at the end of the document
&gt; googlefinance &lt;- WebCorpus(GoogleFinanceSource(""A""))
StartTag: invalid element name
Extra content at the end of the document
Error in inherits(x, ""WebSource"") : 1: StartTag: invalid element name
2: Extra content at the end of the document
&gt; googlenews &lt;- WebCorpus(GoogleNewsSource(""A""))
Unknown IO errorfailed to load external entity ""http://news.google.com/news?hl=en&amp;q=A&amp;ie=utf-8&amp;num=100&amp;output=rss""
Error in inherits(x, ""WebSource"") : 
  1: Unknown IO error2: failed to load external entity ""http://news.google.com/news?hl=en&amp;q=A&amp;ie=utf-8&amp;num=100&amp;output=rss""
&gt; nytimes &lt;- WebCorpus(NYTimesSource(""A"", appid = nytimes_appid))
Error in inherits(x, ""WebSource"") : object 'nytimes_appid' not found
&gt; reutersnews &lt;- WebCorpus(ReutersNewsSource(""A""))
Entity 'ldquo' not defined
Entity 'rdquo' not defined
Opening and ending tag mismatch: div line 60 and body
Opening and ending tag mismatch: body line 59 and html
Premature end of data in tag html line 1
Error in inherits(x, ""WebSource"") : 1: Entity 'ldquo' not defined
2: Entity 'rdquo' not defined
3: Opening and ending tag mismatch: div line 60 and body
4: Opening and ending tag mismatch: body line 59 and html
5: Premature end of data in tag html line 1
&gt; yahoofinance &lt;- WebCorpus(YahooFinanceSource(""A""))
&gt; yahoofinance
&lt;&lt;WebCorpus&gt;&gt;
Metadata:  corpus specific: 3, document level (indexed): 0
Content:  documents: 16
&gt; yahooinplay &lt;- WebCorpus(YahooInplaySource())
&gt; yahooinplay
&lt;&lt;WebCorpus&gt;&gt;
Metadata:  corpus specific: 3, document level (indexed): 0
Content:  documents: 0
&gt; yahoonews &lt;- WebCorpus(YahooNewsSource(""A""))
&gt; yahoonews
&lt;&lt;WebCorpus&gt;&gt;
Metadata:  corpus specific: 3, document level (indexed): 0
Content:  documents: 0
&gt; yahoonews &lt;- WebCorpus(YahooNewsSource(""M""))
&gt; yahoonews
&lt;&lt;WebCorpus&gt;&gt;
Metadata:  corpus specific: 3, document level (indexed): 0
Content:  documents: 10
</code></pre>

<p>Also it worth to be mentioned that even though <code>YahooFinanceSourse</code> is working, it won't return the similar content as <code>GoogleFinanceSource</code> was supposed to do. If you want to play with the examples in <em></em>, I think you may use <code>YahooNewsSource</code> with a customized list of queries. </p>
",1,2,2113,2017-12-13 09:59:05,https://stackoverflow.com/questions/47790148/text-mining-with-tm-plugin-webmining-package-using-googlefinancesource-function
Sort a Cell Alphabetically in R,"<p>Here is my sample data:</p>

<pre><code>id                           text
1  1 ['a','good', 'fresh', 'apple']
2  2     ['fresh', 'apple', 'good']
3  3               ['bad', 'apple']

id &lt;- c(1,2,3)
text &lt;- c(""['a','good', 'fresh', 'apple']"",""['fresh', 'apple', 'good']"",""
['bad', 'apple']"")
data.frame(id,text)
</code></pre>

<p>The text column needs to be sorted alphabetically like bellow:</p>

<pre><code>['a','apple', 'fresh', 'good']
['apple', 'fresh', 'good']
['apple', 'bad']
</code></pre>

<p>I've tried this code, but did not work.</p>

<pre><code>data[lapply(strsplit(as.character(data$Lem), ','), sort)),]
</code></pre>
","r, dataframe, text, text-mining","<p>Most of the work is getting it in and out of the brackets and quotes. This will not work if the substrings contain commas. I would recommend running it one line at a time to understand what happens in each step.</p>

<pre><code>library(stringr)
library(magrittr)
df$sorted_text = str_replace_all(text, ""\\[|\\]|'"", """") %&gt;%
    str_split("","") %&gt;%
    lapply(str_trim) %&gt;%
    lapply(sort) %&gt;%
    lapply(function(x) paste(""'"", x, ""'"", sep = """", collapse = "", "")) %&gt;%
    unlist %&gt;%
    paste0(""["", ., ""]"")
df
#   id                           text                     sorted_text
# 1  1 ['a','good', 'fresh', 'apple'] ['a', 'apple', 'fresh', 'good']
# 2  2     ['fresh', 'apple', 'good']      ['apple', 'fresh', 'good']
# 3  3               ['bad', 'apple']                ['apple', 'bad']
</code></pre>
",2,0,596,2017-12-15 20:47:47,https://stackoverflow.com/questions/47839654/sort-a-cell-alphabetically-in-r
Chronological Sentiment Analysis -- Cannot group by lines,"<p>My data <em>text</em> is a novel in plain text. I used packages tm and tidytext. Data processing went well and I created my DocumentTermMatrix without trouble.  </p>

<pre><code>text &lt;- read_lines(""GoneWithTheWind2.txt"")
set.seed(314) 
text &lt;- iconv(text,'UTF-8',sub="""")
myCorpus &lt;- tm_map(myCorpus, removeWords, c(stopwords(""english""), 
stopwords(""SMART""), mystopwords, Top200Words))  
myDtm &lt;- TermDocumentMatrix(myCorpus, control=list(minWordLength= 1))`
</code></pre>

<p>However, I could not run the coding using <em>inner_join</em> between bing lexicon and the DocumentTermMatrix to do chronological sentiment analysis of this novel over time. I wrote the function below based on an online example but did not know what to group by in count(sentiment) (I place ???? in hold), because the plain text and the DocumentTermMatrix has no ""lines"" columns.  </p>

<pre><code>bing &lt;- get_sentiments(""bing"")  
m &lt;- as.matrix(myDtm)
v &lt;- sort(rowSums(m),decreasing=TRUE)
myNames &lt;- names(v)
d &lt;- data.frame(term=myNames, freq = v)
wind_polarity &lt;- d %&gt;%
# Inner join to the lexicon
inner_join(bing, by=c(""term""=""word"")) %&gt;%
# Count by sentiment, **????**
count(sentiment, **????**) %&gt;%
# Spread sentiments
spread(sentiment, n, fill=0) %&gt;%
mutate(
# Add polarity field
polarity = positive - negative,
# Add line number field
line_number = row_number())
Then plot by ggplot.
</code></pre>

<p>I tried adding a column ""Index"" indicating the line number for each document (line) in <em>text</em> but this column disappears somewhere in the process. Any suggestions would be highly appreciated.</p>
","r, text-mining, sentiment-analysis","<p>Below an approach that calculates the polarity per line (based on a minimum example of three lines). You might join your dtm with the lexicon directly to maintain information on the counts. Then turn polarity information into numeric representation and do your calculations per line. You might certainly rewrite the code and make it more elegant (I am not very familiar with dplyr vocabulary, sorry). I hope that helps anyway.</p>

<pre><code>library(tm)
library(tidytext)

text &lt;- c(""I like coffe.""
          ,""I rather like tea.""
          ,""I hate coffee and tea, but I love orange juice."")

myDtm &lt;- TermDocumentMatrix(VCorpus(VectorSource(text)),
                          control = list(removePunctuation = TRUE,
                                         stopwords = TRUE))

bing &lt;- tidytext::get_sentiments(""bing"")  

wind_polarity &lt;- as.matrix(myDtm) %&gt;%
  data.frame(terms = rownames(myDtm), ., stringsAsFactors = FALSE) %&gt;% 
  inner_join(bing, by= c(""terms""=""word"")) %&gt;%
  mutate(terms = NULL,
         polarity = ifelse( (.[,""sentiment""] == ""positive""), 1,-1),
         sentiment = NULL) %&gt;%
  { . * .$polarity } %&gt;% 
  mutate(polarity = NULL) %&gt;% 
  colSums

#the polarity per line which you may plot, e.g., with base or ggplot
# X1 X2 X3 
# 1  1  0 
</code></pre>
",0,1,133,2017-12-17 04:32:49,https://stackoverflow.com/questions/47852211/chronological-sentiment-analysis-cannot-group-by-lines
Add a condition to one of the scores output in Rmarkdown,"<p>I am outputting some readability scores as a table using Rmarkdown and was wondering if there any way I could add a condition to one of the rows saying ""if the score is greater or equal to 14 then output the colour as red otherwise green""
The following is the code to generate the sample data:</p>

<pre><code>FGL &lt;- 16
    Readability_score &lt;- data.frame(Type = c(""SMOG"",""Flesch Reading Ease"",""Flesch-Kincaid Grade Level"", 
                                         ""Gunning Fog Score"", ""Automated Readability Index""), 
                                Score = c(17,23,FGL,22, 19))
</code></pre>

<p>This is the code to output the table in html using Rmarkdown:</p>

<pre><code>    kable(Readability_score, ""html"") %&gt;%
  kable_styling(bootstrap_options = ""striped"", full_width = F, position = ""left"") %&gt;%
  row_spec(3, bold = T, color = ""white"", background = ""grey"")
</code></pre>

<p>I would like the 3rd element of the data frame to be highlighted in red (Flesch-Kincaid Grade level, 16) if the score is greater than 14. 
So far I tried using <code>cell_spec()</code> with the below code but could not get it to work</p>

<pre><code>FGL &lt;- 16

Readability_score &lt;- data.frame(Type = c(""SMOG"",""Flesch Reading Ease"",""Flesch-Kincaid Grade Level"", 
                                         ""Gunning Fog Score"", ""Automated Readability Index""), 
                                Score = c(17,23,cell_spec(FGL, ""html"", color = ifelse(FGL &gt;= 14, ""red"", ""green"")),22, 19))
</code></pre>

<p>Maybe using <code>cell_spec()</code> here is not a good idea. Any other suggestions regarding ways to achieve what I am trying to achieve here or if anyone could point out any mistake in my code - would be really helpful, Thanks.</p>

<p>PS. To run the above code: the following packages would be required</p>

<pre><code>library(""dplyr"")
library(""knitr"")
library(""kableExtra"")
</code></pre>
","r, r-markdown, text-mining","<p>You just need to set <code>escape=F</code> argument in the kable function</p>

<pre><code>library(""dplyr"")
library(""knitr"")
library(""kableExtra"")

FGL &lt;- 16

Readability_score &lt;- data.frame(Type = c(""SMOG"",
                                     ""Flesch Reading Ease"",
                                     ""Flesch-Kincaid Grade Level"", 
                                     ""Gunning Fog Score"", 
                                     ""Automated Readability Index""), 
                            Score = c(17,23,
                                      cell_spec(FGL, ""html"", 
                                                      color = ifelse(FGL &gt;= 14, ""red"", ""green"")),22, 19))
</code></pre>

<p>In the below line <code>escape=F</code> is added to the code you already wrote</p>

<pre><code>kable(Readability_score, ""html"", escape = F) %&gt;%
  kable_styling(bootstrap_options = ""striped"", full_width = F, position = ""left"") 
</code></pre>
",3,1,215,2017-12-19 11:38:55,https://stackoverflow.com/questions/47886180/add-a-condition-to-one-of-the-scores-output-in-rmarkdown
How to build a Corpus of hashtags (Text Mining),"<p>I am trying to analyze a twitter data by mining all the hashtags. I want to put all hashtags in a corpus and mapping this corpus to a list of words. Do you have any idea how I can to manage this problem ? 
Here is a snap of my data</p>

<p><img src=""https://i.sstatic.net/PWYNz.png"" alt=""""></p>

<p>Here is the code that I used but I have a problem in my DTM with a 100% of sparsity</p>

<pre><code>step1 &lt;- strsplit(newFile$Hashtag, ""#"")
step2 &lt;- lapply(step1, tail, -1)
result &lt;- lapply(step2, function(x){
sapply(strsplit(x, "" ""), head, 1)
})
result2&lt;-do.call(c, unlist(result, recursive=FALSE))
myCorpus &lt;- tm::Corpus(VectorSource(result2)) # create a corpus
</code></pre>

<p>Here is information about my Corpus</p>

<pre><code>myCorpus
  &lt;&lt;SimpleCorpus&gt;&gt;
 Metadata:  corpus specific: 1, document level (indexed): 0
 Content:  documents: 12635
</code></pre>

<p>And my DTM</p>

<pre><code>&lt;&lt;DocumentTermMatrix (documents: 12635, terms: 6280)&gt;&gt;
Non-/sparse entries: 12285/79335515
Sparsity           : 100%
Maximal term length: 36
Weighting          : term frequency (tf)
</code></pre>
","r, text-mining, corpus, topic-modeling","<p>Your problem is you are using <code>str_split</code>. You should try: </p>

<p><code>str_extract_all(""This all are hashtag #hello #I #am #a #buch #of #hashtags"", ""#\\S+"")</code></p>

<pre><code>As results this list:
[[1]]
[1] ""#hello""    ""#I""        ""#am""       ""#a""        ""#buch""     ""#of""      
[7] ""#hashtags""
</code></pre>

<p>If your desired result is a data frame use <code>simplify = T</code>:</p>

<pre><code>str_extract_all(""This all are hashtag #hello #I #am #a #buch #of #hashtags"", ""#\\S+"", simplify = T)
</code></pre>

<p>As result:</p>

<pre><code>     [,1]     [,2] [,3]  [,4] [,5]    [,6]  [,7]       
[1,] ""#hello"" ""#I"" ""#am"" ""#a"" ""#buch"" ""#of"" ""#hashtags""
</code></pre>
",0,0,423,2017-12-20 10:58:52,https://stackoverflow.com/questions/47904469/how-to-build-a-corpus-of-hashtags-text-mining
How to extract a comment with beautifulsoup?,"<p>I am new to python and data mining at all, so I have a question about extracting a part from an output. I am using Python in 3.6 and have updated all stuff today in the morning. I have anonymized the ouput and removed all lines containing passwords, tokens and so on.</p>

<pre><code>from bs4 import BeautifulSoup

soup = BeautifulSoup(open(""facebookoutput.html""), ""html.parser"")

comments = soup.findAll('div', class_=""_2b06"")

print(comments[0]) # show print of first entry:

&lt;div class=""_2b06""&gt;&lt;div class=""_2b05""&gt;&lt;a href=""/stuartd?fref=nf&amp;amp;rc=p&amp;    amp;__tn__=R-R""&gt;some Name &lt;/a&gt;&lt;/div&gt;&lt;div data-commentid=""100000000000000000222222000000000000000"" data-sigil=""comment-body""&gt;There is nice comment. I like stackoverflow. &lt;/div&gt;&lt;/div&gt;
</code></pre>

<p>I am stucking to get `There is nice comment. I like stackoverflow.´ out of it.</p>

<p>Thanks in advance.</p>
","python, beautifulsoup, text-mining","<p>Try this:</p>

<pre><code>from bs4 import BeautifulSoup

content=""""""
&lt;div class=""_2b06""&gt;&lt;div class=""_2b05""&gt;&lt;a href=""/stuartd?fref=nf&amp;amp;rc=p&amp;    amp;__tn__=R-R""&gt;some Name &lt;/a&gt;&lt;/div&gt;&lt;div data-commentid=""100000000000000000222222000000000000000"" data-sigil=""comment-body""&gt;There is nice comment. I like stackoverflow. &lt;/div&gt;&lt;/div&gt;
""""""

soup = BeautifulSoup(content, ""html.parser"")
comments = ' '.join([item.text for item in soup.select(""[data-sigil='comment-body']"")])
print(comments)
</code></pre>

<p>Output:</p>

<pre><code>There is nice comment. I like stackoverflow.
</code></pre>
",1,0,665,2017-12-20 15:07:13,https://stackoverflow.com/questions/47909011/how-to-extract-a-comment-with-beautifulsoup
Regular expression to match all punctuation except that inside of a URL,"<p>I'm looking for a regular expression to select all punctuation except for that which is inside of a URL.</p>
<p>If I have the string:</p>
<pre><code>This is a URL: https://test.com/ThisIsAURL !
</code></pre>
<p>And remove all matches it should become:</p>
<pre><code>This is a URL https://test.com/ThisIsAURL
</code></pre>
<p><code>gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, x)</code> removes all punctuation including from URLs. I've tried using negative look behinds to select punctuation used after https but this was unsuccessful.</p>
<p>In the situation I need it for, all URLs are Twitter link-style URLs <code>https://t.co/</code>. They do not end in <code>.com</code>. Nor do they have more than one backslashed slug (<code>/ThisIsAURL</code>). However, IDEALLY, I'd like the regex to be as versatile as possible, able to perform this operation successfully on any URL.</p>
","r, regex, text-mining","<p>You may match and capture into Group 1 a URL-like pattern like <code>https?://\S*</code> and then match any punctuation and replace with a backreference to Group 1 to restore the URL in the resulting string:</p>

<pre><code>x &lt;- ""This is a URL: https://test.com/ThisIsAURL !""
trimws(gsub(""(https?://\\S*)|[[:punct:]]+"", ""\\1"", x, ignore.case=TRUE))
## =&gt; [1] ""This is a URL https://test.com/ThisIsAURL""
</code></pre>

<p>See the <a href=""https://ideone.com/LMiHtf"" rel=""noreferrer"">R demo online</a>.</p>

<p>The regex is</p>

<pre><code>(https?://\S*)|[[:punct:]]+
</code></pre>

<p>See the <a href=""https://regex101.com/r/7lCnKJ/2"" rel=""noreferrer"">regex demo</a>.</p>

<p><strong>Details</strong></p>

<ul>
<li><code>(https?://\S*)</code> - Group 1 (referenced to with <code>\1</code> from the replacement pattern):

<ul>
<li><code>https?://</code> - <code>https://</code> or <code>http://</code></li>
<li><code>\S*</code> - 0+ non-whitespace chars</li>
</ul></li>
<li><code>|</code> - or</li>
<li><code>[[:punct:]]+</code> - 1+ punctuation (proper punctuation, symbols and <code>_</code>)</li>
</ul>
",9,4,5317,2017-12-21 22:11:35,https://stackoverflow.com/questions/47933405/regular-expression-to-match-all-punctuation-except-that-inside-of-a-url
Dynamic topic models/topic over time in R,"<p>I have a database of newspaper articles about the water policy from 1998 to 2008. I would like to see how the newspaper release changes during this period. My question is, should I use Dynamic Topic Modeling or Topic Over Time model to handle this task? Would they be significantly better than the traditional LDA model (in which I fit the topic model base on the entire set of text corpus, and plot the trend of topic based on how each of the document is tagged)? If yes, is there a package I could use for the DTA/ToT model in R? </p>
","r, text-mining, topic-modeling","<p>So it depends on what your research question is.</p>

<p>A dynamic topic model allows the words that are most strongly associated with a given topic to vary over time. The paper that introduces the model gives a great example of this using journal entries [1]. If you are interested in whether the characteristics of individual topics vary over time, then this is the correct approach.</p>

<p>I have not dealt with the ToT model before, but it appears similar to a structural topic model whose time covariates are continuous. This means that topics are fixed, but their relative prevalence and correlations can vary. If you group your articles into say - months - then a structural or ToT model can show you whether certain topics become more or less prevalent over time.</p>

<p>So in sum, do you want the variation to be within topics or between topics? Do you want to study how the articles vary in the topics they speak on, or do you want to study how these articles construct certain topics?</p>

<p>In terms of R, you'll run into some problems. The <code>stm</code> package can deal with a STM with discrete time periods, but there is no pre-packaged implementation of a ToT model that I am aware of. For a DTM, I know there is a C++ implementation that was released with the introductory paper, and I have a python version which I can find for you.</p>

<p>Note: I would never recommend someone to use a simple LDA for text documents. I would always take a correlated topic model as a base, and build from there.</p>

<p><strong>Edit: to explain more on <code>stm</code> package.</strong></p>

<p>This package is an implementation of the structural topic model [2]. The STM is an extension to the correlated topic model [3] but permits the inclusion of covariates at the document level. You can then explore the relationship between topic prevalence and these covariates. If you include a covariate for date, then you can explore how individual topics become more or less important over time, relative to others. The package itself is excellent, fast and intuitive, and includes functions to choose the most appropriate number of topics etc.</p>

<p><em>[1] Blei, David M., and John D. Lafferty. ""Dynamic topic models."" Proceedings of the 23rd international conference on Machine learning. ACM, 2006.</em></p>

<p><em>[2] Roberts, Margaret E., et al. ""Structural Topic Models for Open‐Ended Survey Responses."" American Journal of Political Science 58.4 (2014): 1064-1082.</em></p>

<p><em>[3] Lafferty, John D., and David M. Blei. ""Correlated topic models."" Advances in neural information processing systems. 2006.</em></p>
",7,4,4469,2017-12-23 13:32:31,https://stackoverflow.com/questions/47952931/dynamic-topic-models-topic-over-time-in-r
stemCompletion is not working properly,"<p>I am trying to use stemCompletion to convert the stemmed words into complete words. </p>

<p>Following is the code I am using </p>

<pre><code>txt &lt;- c(""Once we have a corpus we typically want to modify the documents in it"",
     ""e.g., stemming, stopword removal, et cetera."",
     ""In tm, all this functionality is subsumed into the concept of a transformation."")

myCorpus &lt;- Corpus(VectorSource(txt))
myCorpus &lt;- tm_map(myCorpus, content_transformer(tolower))
myCorpus &lt;- tm_map(myCorpus, removePunctuation)
myCorpusCopy &lt;- myCorpus

# *Removing common word endings* (e.g., ""ing"", ""es"") 
myCorpus.stemmed &lt;- tm_map(myCorpus, stemDocument, language = ""english"")
myCorpus.unstemmed &lt;- tm_map(myCorpus.stemmed, stemCompletion, dictionary=myCorpusCopy)
</code></pre>

<p>if I check the first element for stemmed corpus, it shows me the element correctly</p>

<pre><code>myCorpus.stemmed[[1]][1]
$content
[1] ""onc we have a corpus we typic want to modifi the document in it""
</code></pre>

<p>But if I check the first element of unstemmed corpus, it throws out junk</p>

<pre><code>myCorpus.unstemmed[[1]][1]
$content
[1] NA
</code></pre>

<p>Why is the unstemmed corpus not showing the right content? </p>
","r, text-mining, tm, stemming","<blockquote>
  <p>Why is the unstemmed corpus not showing the right content?</p>
</blockquote>

<p>Since you got a simple corpus object, you are effectively calling </p>

<pre><code>stemCompletion(
  x = c(""once we have a corpus we typically want to modify the documents in it"", 
        ""eg stemming stopword removal et cetera"", 
        ""in tm all this functionality is subsumed into the concept of a transformation""),
  dictionary=myCorpusCopy
)
</code></pre>

<p>which yields </p>

<pre><code># once we have a corpus we typically want to modify the documents in it 
# NA 
# eg stemming stopword removal et cetera 
# NA 
# in tm all this functionality is subsumed into the concept of a transformation 
# NA 
</code></pre>

<p>due to <code>stemCompletion</code> awaiting a character vector of stems as a first argument (<code>c(""once"", ""we"", ""have"")</code>), not a character vector of stemmed texts (<code>c(""once we have"")</code>).</p>

<p>If you want to complete the stems in your corpus, whatever this is supposed to be good for, you have to pass a character vector of single stems to <code>stemCompletion</code> (i.e. tokenize each text document, stem-complete the stems, then paste them together again). </p>
",1,0,691,2017-12-29 11:42:09,https://stackoverflow.com/questions/48022087/stemcompletion-is-not-working-properly
MiniBatchSparsePCA on Text Data,"<p><strong>Goal</strong></p>

<p>I'm trying to replicate an application described in this <a href=""https://people.eecs.berkeley.edu/~elghaoui/Pubs/SPCAhandbookSV.pdf"" rel=""nofollow noreferrer"">paper</a> (section 4.1), where Sparse Principal Component Analysis is applied to a text corpus with the output being K principal components, each displaying a 'structure that is otherwise hidden'. In other words, the principal components should each contain a list of words, all of which share a common theme.</p>

<p>I have used sklearn's MiniBatchSparsePCA package to try to replicate the application, though my output is a matrix of zeros.</p>

<p><strong>Data</strong><br>
My data comes from a survey which was cleaned in Stata. It is a vector of 386 answers; which are sentences.</p>

<p><strong>My Attempt</strong></p>

<pre><code># IMPORT LIBRARIES #
####################################
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from sklearn import decomposition
####################################

# USE SKLEARN TO IMPORT STATA DATA. #
# Data comes from a survey, which was cleaned using Stata.

####################################
data_source = ""/Users/****/q19_free_text.dta""
raw_data = pd.read_stata(data_source) #Reading in the data from a Stata file.  
text_data = raw_data.iloc[:,1] #Cleaning out Observation ID number.
text_data.shape     # Out[268]: (368, ) - There are 368 text (sentence) answers.
####################################

# Term Frequency – Inverse Document- Word Frequency
####################################
    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,stop_words='english')
X_train = vectorizer.fit_transform(text_data)

spca = decomposition.MiniBatchSparsePCA(n_components=2, alpha=0.5)
spca.fit(X_train) 
#TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.

X_train2 = X_train.toarray() #Trying with a dense array...
spca.fit(X_train2)

components = spca.components_


print(components)  #Out: [[ 0.  0.  0. ...,  0.  0.  0.]
                   #     [ 0.  0.  0. ...,  0.  0.  0.]]

components.shape   #Out: (2, 916)

# Empty output!
</code></pre>

<p><strong>Other Notes</strong></p>

<p>I used these sources to write the above code:   </p>

<p><a href=""http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"" rel=""nofollow noreferrer"">Official Example</a>    </p>

<p><a href=""http://scikit-learn.org/stable/modules/feature_extraction.html"" rel=""nofollow noreferrer"">Vectorising Text data</a></p>

<p><a href=""https://stackoverflow.com/questions/47906412/sparse-principal-component-analysis-using-sklearn"">Previous question on the same problem</a></p>
","machine-learning, scikit-learn, text-mining, pca, sklearn-pandas","<blockquote>
  <p>(...) to do something similar to that which is done in section 4.1 in the paper linked. There they 'summarize' a text corpus by using SPCA and the output is K components, where each component is a list of words (or, features).</p>
</blockquote>

<p>If I understand you correctly, you ask how to retrieve words for the components.</p>

<p>You can do this by retrieving indices of nonzero entries in components (use appropriate <code>numpy</code> code on <code>components</code>). Then using <code>vectorizer.vocabulary_</code> you can find out which indices (words/tokens) are found in your components.</p>

<p>See <a href=""https://github.com/lambdaofgod/stackexchange/blob/master/stackoverflow/SPCA%20Word%20Clusters.ipynb"" rel=""nofollow noreferrer"">this notebook</a> for an example implementation (I used 20 newsgroups dataset).</p>
",0,1,306,2017-12-30 14:13:16,https://stackoverflow.com/questions/48034724/minibatchsparsepca-on-text-data
Capturing multiple groups in a regex does not return any result,"<p>I have a python function </p>

<pre><code>def regex(series, regex):
    series = series.str.extract(regex)
    series1 = series.dropna()
    return (series1)
</code></pre>

<p>Aim to match the regex with the pattern as below:</p>

<ul>
<li><p>anything with 'no' followed by (group of words) or a 'not' should not be matched. Below is the regex used in a python function:</p>

<p><code>result = regex(df['col'],r'(^(?!.*\bno\b.*\b(text|sample text )\b)(?!.*\b(text|sample text)\b .*not).+$)')</code></p></li>
</ul>

<p>I do not get any results (just an empty data frame)when applying the regex in a function, <a href=""https://i.sstatic.net/ENMhn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ENMhn.png"" alt=""enter image description here""></a></p>

<p>but testing the regex in this link works well <a href=""https://regex101.com/r/Epq0Ns/21"" rel=""nofollow noreferrer"">https://regex101.com/r/Epq0Ns/21</a></p>
","python, regex, pandas, text-mining","<h2>Code</h2>

<p>For simplicity sake, you can actually just use lists and list comprehension to build simple regular expression patterns.</p>

<h3>Usage</h3>

<p><a href=""https://ideone.com/uvmoaU"" rel=""nofollow noreferrer"">See code in use here</a></p>

<pre><code>import re

negations = [""no"", ""not""]
words = [""text"", ""sample text"", ""text book"", ""notebook""]
sentences = [
    ""first sentence with no and sample text"",
    ""second with a text but also a not"",
    ""third has a no, a text and a not"",
    ""fourth alone is what is neeeded with just text"",
    ""keep putting line here no""
] 

for sentence in sentences:
    negationsRegex = re.compile(r""\b(?:"" + ""|"".join([re.escape(n) for n in negations]) + r"")\b"")
    wordsRegex = re.compile(r""\b(?:"" + ""|"".join([re.escape(w) for w in words]) + r"")\b"")
    if not (re.search(negationsRegex, sentence) and re.search(wordsRegex, sentence)):
        print sentence
</code></pre>

<p><strong>Above code outputs</strong>:</p>

<pre><code>fourth alone is what is neeeded with just text
keep putting line here no
</code></pre>

<hr>

<h2>Explanation</h2>

<p>The code compiles a joined list of regex-escaped words ensuring word boundaries are set. The resulting regular expressions generated (given the lists <code>negations</code> and `words) will be as follows:</p>

<pre><code>\b(?:no|not)\b
\b(?:text|sample text|text book|notebook)\b
</code></pre>

<p>The <code>if</code> statement then checks to see if both generated patterns (the negation regex and word regex) match the sentence. If both expressions don't match (one or both don't match), then the string is returned.</p>
",1,0,153,2018-01-05 15:22:55,https://stackoverflow.com/questions/48116509/capturing-multiple-groups-in-a-regex-does-not-return-any-result
Access document-term matrix without calling .fit_transform() each time,"<p>If I've already called <code>vectorizer.fit_transform(corpus)</code>, is the only way to later print the document-term matrix to call <code>vectorizer.fit_transform(corpus)</code> again?</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
corpus = ['the', 'quick','brown','fox']
vectorizer = CountVectorizer(stop_words='english')
vectorizer.fit_transform(corpus) # Returns the document-term matrix
</code></pre>

<p>My understanding is by doing above, I've now saved terms into the <code>vectorizer</code> object. I assume this because I can now call <code>vectorizer.vocabulary_</code> without passing in <code>corpus</code> again.</p>

<p>So I wondered why there is not a method like <code>.document_term_matrix</code>?</p>

<p>Its seems weird that I have to pass in the <code>corpus</code> again if the data is now already stored in <code>vectorizer</code> object. But per the docs, only <code>.fit</code>, <code>.transform</code>, and <code>.fit_transform</code>return the mattrix.</p>

<p><strong>Docs:</strong> <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit</a></p>

<p><strong>Other Info:</strong></p>

<p>I'm using Anaconda and Jupyter Notebook.</p>
","python-3.x, scikit-learn, nlp, text-mining, countvectorizer","<p>You can simply assign the fit to a variable <code>dtm</code>, and, since it is a <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"" rel=""nofollow noreferrer"">Scipy sparse matrix</a>, use the <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.toarray.html#scipy.sparse.csr_matrix.toarray"" rel=""nofollow noreferrer""><code>toarray</code></a> method to print it:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
corpus = ['the', 'quick','brown','fox']
vectorizer = CountVectorizer(stop_words='english')
dtm = vectorizer.fit_transform(corpus)

# vectorizer object is still fit:
vectorizer.vocabulary_
# {'brown': 0, 'fox': 1, 'quick': 2}

dtm.toarray()
# array([[0, 0, 0],
#        [0, 0, 1],
#        [1, 0, 0],
#        [0, 1, 0]], dtype=int64)
</code></pre>

<p>although I guess for any realistic document-term matrix this will be <em>really</em> impractical... You could use the <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.nonzero.html#scipy.sparse.csr_matrix.nonzero"" rel=""nofollow noreferrer""><code>nonzero</code></a> method instead:</p>

<pre><code>dtm.nonzero()
# (array([1, 2, 3], dtype=int32), array([2, 0, 1], dtype=int32))
</code></pre>
",2,0,598,2018-01-11 17:22:34,https://stackoverflow.com/questions/48212697/access-document-term-matrix-without-calling-fit-transform-each-time
Combine corpora in tm 0.7.3,"<p>Using the text mining package <code>tm</code> for R, the following works in version 0.6.2, R version 3.4.3:</p>

<pre><code>library(tm)
a = ""This is the first document.""
b = ""This is the second document.""
c = ""This is the third document.""
d = ""This is the fourth document.""
docs1 = VectorSource(c(a,b))
docs2 = VectorSource(c(c,d))
corpus1 = Corpus(docs1)
corpus2 = Corpus(docs2)
corpus3 = c(corpus1,corpus2)
inspect(corpus3)
&lt;&lt;VCorpus&gt;&gt;
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 4
</code></pre>

<p>However, the same code in <code>tm</code> version 0.7.3  (R version 3.4.2) gives an error:</p>

<pre><code>Error in UseMethod(""inspect"", x) :
  no applicable method for 'inspect' applied to an object of class ""list""
</code></pre>

<p>According to <code>vignette(""tm"",package=""tm"")</code>, the <code>c()</code> function is overloaded:</p>

<blockquote>
  <p>Many standard operators and functions (<code>[, [&lt;-, [[, [[&lt;-, c(), lapply()</code>) are available for corpora with semantics similar to standard
  R routines. E.g., <code>c()</code> concatenates two (or more) corpora. Applied to
  several text documents it returns a corpus. The metadata is
  automatically updated, if corpora are concatenated (i.e., merged).</p>
</blockquote>

<p>However, for the new version this is apparently no longer the case. How can two corpora be combined in <code>tm</code> 0.7.3? An obvious solution is to combine the documents first and create the corpus afterwards, but I'm looking for a solution to combine two already existing corpora.</p>
","r, version, text-mining, backwards-compatibility","<p>I do not have much experience with the <code>tm</code> package so my answer may lack some nuance in understanding of <code>SimpleCorpus</code> vs <code>VCorpus</code> vs other <code>tm</code> object classes.</p>

<p>The inputs to your call to <code>c</code> are the class <code>SimpleCorpus</code>; it doesn't look like <code>tm</code> comes with a <code>c</code> method specifically for this class.  So method dispatch isn't calling the right <code>c</code> to combine the Corpora in the way you'd want.  However, there is a <code>c</code> method for the <code>VCorpus</code> class (<code>tm:::c.VCorpus</code>).</p>

<p>There are 2 different ways to get past the issue of coercing <code>corpus3</code> to a <code>list</code>, but they seem to result in different structures.  I present both below and leave it up to you if they are accomplishing your end goal.</p>

<h3>1) You can call <code>tm:::c.VCorpus</code> directly when defining <code>corpus3</code>:</h3>

<pre><code>&gt; library(tm)
&gt; 
&gt; a = ""This is the first document.""
&gt; b = ""This is the second document.""
&gt; c = ""This is the third document.""
&gt; d = ""This is the fourth document.""
&gt; docs1 = VectorSource(c(a,b))
&gt; docs2 = VectorSource(c(c,d))
&gt; corpus1 = Corpus(docs1)
&gt; corpus2 = Corpus(docs2)
&gt; 
&gt; corpus3 = tm:::c.VCorpus(corpus1,corpus2)
&gt; 
&gt; inspect(corpus3)
&lt;&lt;VCorpus&gt;&gt;
Metadata:  corpus specific: 2, document level (indexed): 0
Content:  documents: 4

[1] This is the first document.  This is the second document. This is the third document. 
[4] This is the fourth document.
</code></pre>

<h3>2) You can use <code>VCorpus</code> when defining <code>corpus1</code> &amp; <code>corpus2</code>:</h3>

<pre><code>&gt; library(tm)
&gt; 
&gt; a = ""This is the first document.""
&gt; b = ""This is the second document.""
&gt; c = ""This is the third document.""
&gt; d = ""This is the fourth document.""
&gt; docs1 = VectorSource(c(a,b))
&gt; docs2 = VectorSource(c(c,d))
&gt; corpus1 = VCorpus(docs1)
&gt; corpus2 = VCorpus(docs2)
&gt; 
&gt; corpus3 = c(corpus1,corpus2)
&gt; 
&gt; inspect(corpus3)
&lt;&lt;VCorpus&gt;&gt;
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 4

[[1]]
&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  7
Content:  chars: 27

[[2]]
&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  7
Content:  chars: 28

[[3]]
&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  7
Content:  chars: 27

[[4]]
&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  7
Content:  chars: 28
</code></pre>
",2,0,2185,2018-01-12 10:22:33,https://stackoverflow.com/questions/48224166/combine-corpora-in-tm-0-7-3
R - finding max value of Corpus vector,"<p>I'm new to R coding. 
I've been trying to use the TM library to get the percentage of sentiment in each element.</p>

<p>I started by using the:</p>

<pre><code>   sc &lt;- Corpus(VectorSource(email))
</code></pre>

<p>After that is tried to minimize the unnecessary words by using the:</p>

<pre><code>sclean&lt;- tm_map(sc, removePunctuation)
sclean &lt;- tm_map(sclean, content_transformer(tolower))
sclean &lt;- tm_map(sclean, removeWords, stopwords(kind=""en""))
sclean &lt;- tm_map(sclean, removeNumbers)
sclean &lt;- tm_map(sclean, stripWhitespace)
sclean &lt;- tm_map(sclean, removeWords, commonwords)
sent&lt;-sent_Analysed&lt;-get_nrc_sentiment(unlist(as.list(sclean)))
</code></pre>

<p>I'm getting an answer that looks like (each row is a ""sent"" ): </p>

<p><a href=""https://i.sstatic.net/fKsCF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fKsCF.png"" alt=""feelings""></a></p>

<p>From this i want to find the maximum value and calculate the percentage of it (excluding the negative and positive).
For example for line 2:</p>

<p>The max value will be: trust (40) 
And the percentage will be: 32.5 ( max / sum (= 123) * 100)</p>

<p>I'm struggling with finding the max value, and the sum (each line is printed by a for loop) of all numbers without the last 2 columns </p>
","r, nlp, text-mining, corpus","<p>Using a smaller example than yours...</p>

<pre><code>sent &lt;- data.frame(a1=c(1,2),a2=c(2,3),a3=c(4,1))
sent
  a1 a2 a3
1  1  2  4
2  2  3  1
</code></pre>

<p>You can do this in base R using <code>apply</code> as follows...</p>

<pre><code>sentsum &lt;- data.frame(best=names(sent)[apply(sent,1,which.max)], #name of highest column
                      score=apply(sent,1,max), #value of highest column
                      stringsAsFactors = FALSE)
sentsum$percent &lt;- 100*sentsum$score/rowSums(sent) #percent of row sum

sentsum
  best score  percent
1   a3     4 57.14286
2   a2     3 50.00000
</code></pre>
",1,0,71,2018-01-12 17:10:08,https://stackoverflow.com/questions/48230961/r-finding-max-value-of-corpus-vector
Classifying new text using LDA in R,"<p>I am trying out topic modeling using R for the first time. So, this might be a very dumb question but I am stuck and googling has not given a definitive answer. </p>

<p>Given a corpus of documents, I used the LDA function to identify the different topics in the corpus. Once, the model has been fitted, how can I apply the model on a new batch of documents to classify them among the topics discovered so far?</p>

<p>example code:</p>

<pre><code>data(""AssociatedPress"", package = ""topicmodels"")

n &lt;- nrow(AssociatedPress)
train_data &lt;- sample(1:n,0.75*n,replace = FALSE)
AssociatedPress_train &lt;- AssociatedPress[(train_data),]
AssociatedPress_test &lt;- AssociatedPress[!(train_data),]

ap_lda &lt;- LDA(AssociatedPress_train, k = 5, control = list(seed = 1234))
</code></pre>

<p>Now, can I classify the documents in AssociatedPress_test using the fitted model ap_lda? If yes, how? If not, what would be the best way to create a model for such future classification? </p>
","r, text-mining, lda, topic-modeling","<p>You can use the <a href=""https://www.rdocumentation.org/packages/topicmodels/versions/0.2-6/topics/posterior-methods"" rel=""noreferrer""><code>topicmodels::posterior()</code></a> function as means of finding the ""top topic"" per new document in your <code>AssociatedPress_test</code> object.  Below is a snippet showing how to accomplish this.</p>

<pre><code># code provided in quesiton------------------------------------------
library(tm)
data(""AssociatedPress"", package = ""topicmodels"")

n &lt;- nrow(AssociatedPress)
train_data &lt;- sample(1:n, 0.75*n, replace = FALSE)
AssociatedPress_train &lt;- AssociatedPress[ train_data, ]
AssociatedPress_test  &lt;- AssociatedPress[-train_data, ]

ap_lda &lt;- topicmodels::LDA(AssociatedPress_train, k = 5, 
                           control = list(seed = 1234))
#--------------------------------------------------------------------

#posterior probabilities of topics for each document &amp; terms
post_probs &lt;- topicmodels::posterior(ap_lda, AssociatedPress_test)

#classify documents by finding topic with max prob per doc
top_topic_per_doc &lt;- apply(post$topics, 1, which.max)

head(top_topic_per_doc)

#OUTPUT
# [1] 4 2 4 2 2 2
</code></pre>
",5,5,1376,2018-01-15 18:14:15,https://stackoverflow.com/questions/48268570/classifying-new-text-using-lda-in-r
auto correct misspelled words in a list of lists - python,"<p>I have a list of lists containing words/tokens.</p>

<pre><code>Eg:- [[u'note', u'subject', u'mussage', u'aar', u'install'],[ u'accomplishment', u'survice', u'hte', u'skinn', u'damaged', u'location', u'shown'] 
</code></pre>

<p>Need a python script which auto corrects the misspelled words and provides the results as.  </p>

<pre><code>Eg:- [[u'note', u'subject', u'message', u'air', u'install'],[ u'accomplishment', u'service', u'the', u'skin', u'damaged', u'location', u'shown']
</code></pre>

<p>I have around 2 million lists each having more than 5000 words/tokens. How to do a script which completes the job in very short time</p>
","python, multithreading, nlp, nltk, text-mining","<p>You can use the <strong><a href=""https://pypi.python.org/pypi/autocorrect"" rel=""nofollow noreferrer"">autocorrect</a></strong> lib to accomplish your task.</p>

<pre><code>from autocorrect import spell

k = [[u'note', u'subject', u'mussage', u'aar', u'install'],[ u'accomplishment', u'survice', u'hte', u'skinn', u'damaged', u'location', u'shown']]

res = [map(spell, l) for l in k]
print res
</code></pre>

<p>Result:</p>

<pre><code>[[u'note', u'subject', u'message', u'Aar', u'install'], [u'accomplishment', u'service', u'the', u'skin', u'damaged', u'location', u'shown']]
</code></pre>
",1,-3,614,2018-01-16 08:08:40,https://stackoverflow.com/questions/48276616/auto-correct-misspelled-words-in-a-list-of-lists-python
Is there any way to extract header and footer and title page of a PDF document?,"<p>I want to know if there is any package to detect and extrac the header and footer or title page from PDF document ? I am new in text mining using python and I want to know for example pdfminer.layout could help to find any text block in pdfs? </p>
","python, pdf, text-mining","<p>Apache Tika also does metadata extraction. You can also extract names, title/multiple-titles, date, number of pages, modified dates, and many more. </p>

<pre><code>import tika
from tika import parser

filename = ""your file name here""
parsedPDF = parser.from_file(file_name)
print(parsedPDF['content'])
print(parsedPDF['metadata']) # its in a dictionary format. 
</code></pre>
",-2,2,13629,2018-01-17 16:57:59,https://stackoverflow.com/questions/48306295/is-there-any-way-to-extract-header-and-footer-and-title-page-of-a-pdf-document
R remove s at the end of each word,"<p>I've got the following R code:</p>

<pre><code>temp &lt;- strsplit(unlist(test_data$`Product Description`), split="" "")
temp &lt;- lapply(temp, function(x) gsub(""s$"", '', x))
</code></pre>

<p>What I'm trying to do is to strip out the s at the end of every word in the 'Product Description' Column.</p>

<p>The first step of the code works perfectly and it splits the data as it should by creating a list of words per description.</p>

<p>However, the second step does not work. It does not strip out the 's'</p>
","r, lapply, text-mining, gsub","<p>Use <code>sub</code> with the pattern <code>(.*)s$</code>, and then replace with the first capture group.</p>

<pre><code>temp &lt;- lapply(temp, function(x) sub(""(.*)s$"", '\\1', x))
</code></pre>

<p>The idea here is that if the pattern does match, we will replace with the final <code>s</code> stripped off.  If the pattern does not match, then <code>sub</code> would just return the entire untouched string.</p>

<p><a href=""http://rextester.com/EKC24837"" rel=""nofollow noreferrer""><h2>Demo</h2></a></p>
",2,0,1232,2018-01-18 03:46:45,https://stackoverflow.com/questions/48313528/r-remove-s-at-the-end-of-each-word
tweepy Streaming API : full text,"<p>I am using <code>tweepy</code> streaming API to get the tweets containing a particular hashtag . The problem that I am facing is that I am unable to extract full text of the tweet from the Streaming API . Only 140 characters are available and after that it gets truncated.</p>
<p>Here is the code:</p>
<pre class=""lang-py prettyprint-override""><code>auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)
auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
api = tweepy.API(auth)


def analyze_status(text):

    if 'RT' in text[0:3]:
        return True
    else:
        return False

    class MyStreamListener(tweepy.StreamListener):

    def on_status(self, status):

    if not analyze_status(status.text):

        with open('fetched_tweets.txt', 'a') as tf:
            tf.write(status.text.encode('utf-8') + '\n\n')

        print(status.text)

    def on_error(self, status):
    print(&quot;Error Code : &quot; + status)

    def test_rate_limit(api, wait=True, buffer=.1):
        &quot;&quot;&quot;
        Tests whether the rate limit of the last request has been reached.
        :param api: The `tweepy` api instance.
        :param wait: A flag indicating whether to wait for the rate limit reset
                 if the rate limit has been reached.
        :param buffer: A buffer time in seconds that is added on to the waiting
                   time as an extra safety margin.
        :return: True if it is ok to proceed with the next request. False otherwise.
        &quot;&quot;&quot;
        # Get the number of remaining requests
        remaining = int(api.last_response.getheader('x-rate-limit-remaining'))
        # Check if we have reached the limit
        if remaining == 0:
        limit = int(api.last_response.getheader('x-rate-limit-limit'))
        reset = int(api.last_response.getheader('x-rate-limit-reset'))
        # Parse the UTC time
        reset = datetime.fromtimestamp(reset)
        # Let the user know we have reached the rate limit
        print &quot;0 of {} requests remaining until {}.&quot;.format(limit, reset)

        if wait:
            # Determine the delay and sleep
            delay = (reset - datetime.now()).total_seconds() + buffer
            print &quot;Sleeping for {}s...&quot;.format(delay)
            sleep(delay)
            # We have waited for the rate limit reset. OK to proceed.
            return True
        else:
            # We have reached the rate limit. The user needs to handle the rate limit manually.
            return False

        # We have not reached the rate limit
        return True

    myStreamListener = MyStreamListener()
    myStream = tweepy.Stream(auth=api.auth, listener=myStreamListener,
                             tweet_mode='extended')

    myStream.filter(track=['#bitcoin'], async=True)

</code></pre>
<p>Does any one have a solution ?</p>
","twitter, text-mining, tweepy","<p><code>tweet_mode=extended</code> will have no effect in this code, since the Streaming API does not support that parameter. If a Tweet contains longer text, it will contain an additional object in the JSON response called <code>extended_tweet</code>, which will in turn contain a field called <code>full_text</code>. </p>

<p>In that case, you'll want something like <code>print(status.extended_tweet.full_text)</code> to extract the longer text.</p>
",8,6,11088,2018-01-18 10:37:05,https://stackoverflow.com/questions/48319243/tweepy-streaming-api-full-text
Python - match and parse strings containing numeric/currency amounts,"<p>Say I have the following strings (inputs) in python:</p>

<p>1) <code>""$ 1,350,000""</code> 
2) <code>""1.35 MM $""</code> 
3) <code>""$ 1.35 M""</code>
4) <code>1350000</code> (now it is a numeric value)</p>

<p>Obviously the number is the same although the string representation is different. How can I achieve a string matching or in other words classify them as equal strings?</p>

<p>One way would be to model -using regular expressions- the possible patterns. However there might be a case that I haven't thought of.</p>

<p>Does someone see a NLP solution to this problem?</p>

<p>Thanks</p>
","python, regex, parsing, currency, text-mining","<p>This is not an NLP problem, just a job for regexes, plus some code to ignore order, and lookup a dictionary of known abbreviations(/ontology) like ""MM"".</p>

<ul>
<li>First, you can completely disregard the '$' character here (unless you need to disambiguate against other currencies or symbols).</li>
<li>So all this boils down to is parsing number formats, and mapping 'M'/'MM'/'million' -> a 1e6 multiplier. And doing that parsing in an order-independent way (e.g. the multiplier, currency symbol and amount can appear in any relative order, or not at all)</li>
</ul>

<p>Here's some working code:</p>

<pre><code>def parse_numeric_string(s):

    if isinstance(s, int): s = str(s)

    amount = None
    currency = ''
    multiplier = 1.0

    for token in s.split(' '):

        token = token.lower()

        if token in ['$','€','£','¥']:
            currency = token

        # Extract multipliers from their string names/abbrevs
        if token in ['million','m','mm']:
            multiplier = 1e6
        # ... or you could use a dict:
        # multiplier = {'million': 1e6, 'm': 1e6...}.get(token, 1.0)

        # Assume anything else is some string format of number/int/float/scientific
        try:
            token = token.replace(',', '')
            amount = float(token)
        except:
            pass # Process your parse failures...

    # Return a tuple, or whatever you prefer
    return (currency, amount * multiplier)

parse_numeric_string(""$ 1,350,000"")
parse_numeric_string(""1.35 MM $"")
parse_numeric_string(""$ 1.35 M"")
parse_numeric_string(1350000)
</code></pre>

<ul>
<li>For internationalization, you may want to beware that <code>,</code> and <code>.</code> as thousands separator and decimal point can be switched, or <code>'</code> as (Arabic) thousands separator. There's also a third-party Python package 'parse', e.g. <code>parse.parse('{fn}', '1,350,000')</code> (it's the reverse of <code>format()</code>) </li>
<li>Using an ontology or general NLP library would probably be way more trouble than it's worth. For example, you'd need to disambiguate between 'mm' as in ""accounting abbreviation for millions"" vs ""millimeters"" vs 'Mm' as in 'Megameters, 10^6 meters' which is an almost-never-used but valid metric unit for distance. So, less generality probably better for this task.</li>
<li>and you could also use a dict-based approach to map other currency signifiers e.g. 'dollars','US','USD','US$', 'EU'...</li>
<li>here I tokenized on whitespace, but you might want to tokenize on any word/numeric/whitespace/punctuation boundaries so you can parse e.g. <code>USD1.3m</code></li>
</ul>
",5,4,2309,2018-01-20 17:47:17,https://stackoverflow.com/questions/48359255/python-match-and-parse-strings-containing-numeric-currency-amounts
Text Mining - Splitting Texts Into Individual Observations,"<p>I have a dataset of a unique ID and a sentence for each ID. I would like to break up the sentence by words and remove the stopwords to clean the data for further analysis. </p>

<pre><code>Example of dataset: 
ID  Sentence
1  The quick brown fox 
2  Feel free to be

Breaking up sentence: 
ID  Word 
1  The 
1  quick 
1  brown 
1  fox 
2  Feel 
2  free 
2  to 
2  be 

Removing the stopwords: 
ID  Word
1  quick 
1  brown 
1  fox 
2  Feel 
2  free
</code></pre>

<p>I already have the IDs and sentences in a dataframe. What would be a suitable function to break up the texts including removing of punctuations after each word if any and then removing the rows with stopwords. </p>
","r, text-mining, data-cleaning","<p>Using the <code>tidytext</code> package, you can do the following. The package has stopwords. You need to call the data. Then, you apply <code>unnest_tokens()</code> to the text column. You need to specify two names. One for the target column, and the other for a new column in the output. Once you tease apart the sentences, you subset data. Here I used <code>filter()</code> in the <code>dplyr</code> package.</p>

<pre><code>library(dplyr)
library(tidytext)

foo &lt;- data.frame(ID = c(1, 2),
                  Sentence = c(""The quick brown fox"", ""Feel free to be""),
                  stringsAsFactors = FALSE)

data(stop_words)

unnest_tokens(foo, input = Sentence, output = word) %&gt;%
filter(!word %in% stop_words$word)

  ID  word
1  1 quick
2  1 brown
3  1   fox
4  2  feel
5  2  free
</code></pre>
",2,0,537,2018-01-25 02:54:48,https://stackoverflow.com/questions/48434902/text-mining-splitting-texts-into-individual-observations
Text mining - word frequency from a single column containing list,"<p>Here is my dataset:</p>

<p><a href=""https://app.box.com/s/yotsy58ud2k9yk7vs7sj8ksc0favhevv"" rel=""nofollow noreferrer"">https://app.box.com/s/yotsy58ud2k9yk7vs7sj8ksc0favhevv</a></p>

<p>I'm trying to create a frequency table of the tags from a single column with following structure:</p>

<p><a href=""https://i.sstatic.net/dLG7e.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dLG7e.png"" alt=""enter image description here""></a></p>

<p>I tried using <code>qdap</code> for simplicity, but the result is not correct</p>

<pre><code>library(qdap)
tags_df &lt;- read.csv(file.choose())
freq_terms(tags_df$tags)
</code></pre>

<p><strong>Solution</strong></p>

<p>Just improving (creating a data frame and sorting) the solution given by Rui:</p>

<pre><code>sp &lt;- unlist(strsplit(as.character(unlist(tags_df$tags)),'^c\\(|,|""|\\)'))

inx &lt;- sapply(sp, function(y) nchar(trimws(y)) &gt; 0 &amp; !is.na(y))

data &lt;- as_data_frame(table(tolower(sp[inx])))

data &lt;- data[with(data,order(-n)),]

data &lt;- data[1:10,]
</code></pre>
","r, text-mining, tm, word-frequency","<p>If all you want or need is a frequency count, you can do without external packages, base R has a function <code>table</code>.</p>

<pre><code>sp &lt;- unlist(strsplit(as.character(unlist(tags_df$tags)), '^c\\(|,|""|\\)'))
inx &lt;- sapply(sp, function(y) nchar(trimws(y)) &gt; 0 &amp; !is.na(y))
table(sp[inx])
#    Android        CSS3      Design      Hiring  JavaScript      NextJS 
#          1           1           1           1           4           1 
#     NodeJS programming Programming     ReactJS     Testing          UI 
#          1           1           3           3           1           1 
#         UX   WebDesign      webdev      WebDev 
#          1           2           1           4
</code></pre>

<p><strong>EDIT.</strong>  </p>

<p>I have just realized that you have <code>""programming""</code> and <code>""Programming""</code>, <code>""webdev""</code> and <code>""WebDev""</code> as tags, maybe you want to do a case-insensitive count. If this is the case, try instead</p>

<pre><code>table(tolower(sp[inx]))
</code></pre>
",1,0,709,2018-01-25 09:23:45,https://stackoverflow.com/questions/48439429/text-mining-word-frequency-from-a-single-column-containing-list
Save and load scikit-learn machine learning model and function,"<p>I trained Naive Bayes model with scikit-learn to classify articles in my web application.To avoid learning the model repeatedly, I want to save the model and deploy it to the application later. When i search for this problem, many people recommend the <code>pickle</code> library.</p>

<p>I have this model :</p>

<pre><code>import pickle
import os
def custom_tokenizer (doc) :
    tokens = vect_tokenizer(doc)
    return [lemmatizer.lemmatize(token) for token in tokens]

tfidf = TfidfVectorizer(tokenizer = custom_tokenizer,stop_words = ""english"")
clf = MultinomialNB()
</code></pre>

<p>I have already executed <code>tfidf.fit_transform()</code> and trained <code>clf</code>. Finally, i got a model and saved <code>clf</code> classifier using this code :</p>

<pre><code>dest = os.path.join('classifier','pkl_object')
f = open(os.path.join(dest,'classifier.pkl'),'wb')
pickle.dump(best_classifier,f,protocol = 4)
f.close()
</code></pre>

<p>I also tried to save my Vectorizer as a file this way.</p>

<pre><code>f =  open(os.path.join(dest,'vect.pkl'),'wb')
pickle.dump(custom_tokenizer,f,protocol = 4)
pickle.dump(best_vector,f,protocol = 4)
f.close()
</code></pre>

<p>There was no error. but when i tried to load the file, this error message popped up.</p>

<pre><code>import pickle
import os

with open(os.path.join('pkl_object','classifier.pkl'),'rb') as file :
    clf = pickle.load(file)

with open(os.path.join('pkl_vect','vect.pkl'),'rb') as file:
    vect = pickle.load(file)
</code></pre>

<p>error message : </p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-55-d4b562870a02&gt; in &lt;module&gt;()
     11 
     12 with open(os.path.join('pkl_vect','vect.pkl'),'rb') as file:
---&gt; 13     vect = pickle.load(file)
     14 
     15 '''

AttributeError: Can't get attribute 'custom_tokenizer' on &lt;module '__main__'&gt;
</code></pre>

<p>I think the <code>pickle</code> library does not have the ability to store function properly. How can i serialize my custom <code>TfidfVectorizer</code> as a file.</p>
","python, serialization, scikit-learn, pickle, text-mining","<p>In the second program also include:</p>

<pre><code>def custom_tokenizer (doc) :
    tokens = vect_tokenizer(doc)
    return [lemmatizer.lemmatize(token) for token in tokens]
</code></pre>

<p>becuase pickle doesn't actually store information about how a class/object is constructed, as this line in your error log says <code>AttributeError: Can't get attribute 'custom_tokenizer' on &lt;module '__main__'&gt;</code> it has no idea what is <code>custom_tokenizer</code>.Refer <a href=""https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multipile-modules"">this</a> for better understanding.</p>
",2,2,2402,2018-01-28 14:18:49,https://stackoverflow.com/questions/48487323/save-and-load-scikit-learn-machine-learning-model-and-function
Text Mining using Jaro-Winkler fuzzy matching in R,"<p>Im attempting to do some distance matching in R and am struggling to achieve a usable output. </p>

<p>I have a dataframe <code>terms</code> that contains 5 strings of text, along with a category for each string. I have a second dataframe <code>notes</code> that contains 10 poorly spelt words, along with a NoteID. </p>

<p>I want to be able to compare each of my 5 <code>terms</code> against each of my 10 <code>notes</code> using a distance algorithm to try to grab simple spelling errors. I have tried: </p>

<p><code>near_match&lt;- subset(notes, jarowinkler(notes$word, terms$word) &gt;0.9)</code> </p>

<pre><code>   NoteID    Note
5      e5 thought
10     e5   tough
</code></pre>

<p>and</p>

<p><code>jarowinkler(notes$word, terms$word)</code></p>

<p><code>[1] 0.8000000 0.7777778 0.8266667 0.8833333 0.9714286 0.8000000 0.8000000 0.8266667 0.8833333 0.9500000</code></p>

<p>The first instance is almost what I need, it just lacks the word from <code>terms</code> that has caused the match. The second returns 10 scores but I'm not sure if the algorithm checked each of the 5 <code>terms</code> against each of the 10 <code>notes</code> in turn and just returned the closest match (highest score) or not.</p>

<p>How can I alter the above to achieve my desired output if what I want is achievable using <code>jarowinkler()</code> or is there a better option? </p>

<p>I'm relatively new to R so appreciate any help in furthering my understanding how the algorithm generates the scores and what the approach to achieve my desired output would be.</p>

<p>example dataframes below</p>

<p>Thanks</p>

<pre><code>&gt; notes
   NoteID    word
1      a1     hit
2      b2     hot
3      c3   shirt
4      d4    than
5      e5 thought
6      a1     hat
7      b2     get
8      c3   shirt
9      d4    than
10     e5   tough

&gt; terms
  Category   word
1        a    hot
2        b    got
3        a   shot
4        d   that
5        c though
</code></pre>
","r, text-mining, levenshtein-distance, jaro-winkler","<p>Your data.frames:</p>

<pre><code>notes&lt;-data.frame(NoteID=c(""a1"",""b2"",""c3"",""d4"",""e5"",""a1"",""b2"",""c3"",""d4"",""e5""),
                  word=c(""hit"",""hot"",""shirt"",""than"",""thought"",""hat"",""get"",""shirt"",""that"",""tough""))
terms&lt;-data.frame(Category=c(""a"",""b"",""c"",""d"",""e""),
                  word=c(""hot"",""got"",""shot"",""that"",""though""))
</code></pre>

<p>Use <code>stringdistmatrix</code> (package <code>stringdist</code>) with method ""jw"" (jarowinkler)</p>

<pre><code>library(stringdist)
dist&lt;-stringdistmatrix(notes$word,terms$word,method = ""jw"")
row.names(dist)&lt;-as.character(notes$word)
colnames(dist)&lt;-as.character(terms$word)
</code></pre>

<p>Now you have all distances:</p>

<pre><code>dist
              hot       got       shot       that     though
hit     0.2222222 0.4444444 0.27777778 0.27777778 0.50000000
hot     0.0000000 0.2222222 0.08333333 0.27777778 0.33333333
shirt   0.4888889 1.0000000 0.21666667 0.36666667 0.54444444
than    0.4722222 1.0000000 0.50000000 0.16666667 0.38888889
thought 0.3571429 0.5158730 0.40476190 0.40476190 0.04761905
hat     0.2222222 0.4444444 0.27777778 0.08333333 0.50000000
get     0.4444444 0.2222222 0.47222222 0.47222222 0.50000000
shirt   0.4888889 1.0000000 0.21666667 0.36666667 0.54444444
that    0.2777778 0.4722222 0.33333333 0.00000000 0.38888889
tough   0.4888889 0.4888889 0.51666667 0.51666667 0.05555556
</code></pre>

<p>Find the word more close to notes</p>

<pre><code>output&lt;-cbind(notes,word_close=terms[as.numeric(apply(dist, 1, which.min)),""word""],dist_min=apply(dist, 1, min))
output
       NoteID    word word_close   dist_min
    1      a1     hit        hot 0.22222222
    2      b2     hot        hot 0.00000000
    3      c3   shirt       shot 0.21666667
    4      d4    than       that 0.16666667
    5      e5 thought     though 0.04761905
    6      a1     hat       that 0.08333333
    7      b2     get        got 0.22222222
    8      c3   shirt       shot 0.21666667
    9      d4    that       that 0.00000000
    10     e5   tough     though 0.05555556
</code></pre>

<p>If you want have just in word_close the words under a certain distance threshold (in this case 0.1), you can do this:</p>

<pre><code>output[output$dist_min&gt;=0.1,c(""word_close"",""dist_min"")]&lt;-NA
output
   NoteID    word word_close   dist_min
1      a1     hit       &lt;NA&gt;         NA
2      b2     hot        hot 0.00000000
3      c3   shirt       &lt;NA&gt;         NA
4      d4    than       &lt;NA&gt;         NA
5      e5 thought     though 0.04761905
6      a1     hat       that 0.08333333
7      b2     get       &lt;NA&gt;         NA
8      c3   shirt       &lt;NA&gt;         NA
9      d4    that       that 0.00000000
10     e5   tough     though 0.05555556
</code></pre>
",1,1,2176,2018-01-31 15:22:23,https://stackoverflow.com/questions/48545611/text-mining-using-jaro-winkler-fuzzy-matching-in-r
Extract n Words Around Defined Term (Multicase),"<p>I have a <strong>vector of text string</strong>s, such as:</p>

<pre><code>Sentences &lt;- c(""I would have gotten the promotion, but TEST my attendance wasn’t good enough.Let me help you with your baggage."",
               ""Everyone was busy, so I went to the movie alone. Two seats were vacant."",
               ""TEST Rock music approaches at high velocity."",
               ""I am happy to take your TEST donation; any amount will be greatly TEST appreciated."",
               ""A purple pig and a green donkey TEST flew a TEST kite in the middle of the night and ended up sunburnt."",
               ""Rock music approaches at high velocity TEST."")
</code></pre>

<p>I would like to <strong>extract n</strong> (for example: three) <strong>words</strong> (a word is characterized by a whitespace before and after character(s)) <strong>AROUND (i.e., before and after) a particular term</strong> (e.g., 'TEST').
Improtant: <strong>Several</strong> <strong>matches</strong> should be <strong>allowed</strong> (i.e., if a particular term occurs more than one times, the intended solution should capture those cases).</p>

<p>The result might look like this (the format can be improved):</p>

<pre><code>S1  &lt;- c(before = ""the promotion, but"", after = ""my attendance wasn’t"")
S2  &lt;- c(before = """",                   after = """")
S3  &lt;- c(before = """",                   after = ""Rock music approaches"")
S4a &lt;- c(before = ""to take your"",       after = ""donation; any amount"")
S4b &lt;- c(before = ""will be greatly"",    after = ""appreciated."")
S5a &lt;- c(before = ""a green donkey"",     after = ""flew a TEST"")
S5b &lt;- c(before = ""TEST flew"",          after = ""kite in the"")
S6  &lt;- c(before = ""at high velocit"",    after = """")  
</code></pre>

<p>How can I do this? I already figured out other psots, which are either o<a href=""https://stackoverflow.com/questions/34403346/extract-a-sample-of-words-around-a-particular-word-using-stringr-in-r"">nly for one-case-matches</a> or relate to <a href=""https://stackoverflow.com/questions/33856148/regex-in-r-extracting-words-from-a-string"">fixed sentence structures</a>.</p>
","r, text, text-mining, tm","<p>The <strong>quanteda</strong> package has a great function for this: <code>kwic()</code> (keywords in context).  </p>

<p>Out of the box, this works pretty well on your example:</p>

<pre><code>library(""quanteda"")
names(Sentences) &lt;- paste0(""S"", seq_along(Sentences))
(kw &lt;- kwic(Sentences, ""TEST"", window = 3))
# 
# [S1, 9]   promotion, but | TEST | my attendance wasn't 
# [S3, 1]                  | TEST | Rock music approaches
# [S4, 7]     to take your | TEST | donation; any        
# [S4, 15] will be greatly | TEST | appreciated.         
# [S5, 8]   a green donkey | TEST | flew a TEST          
# [S5, 11]     TEST flew a | TEST | kite in the          
# [S6, 7] at high velocity | TEST | .               

(kw2 &lt;- as.data.frame(kw)[, c(""docname"", ""pre"", ""post"")])
#   docname              pre                  post
# 1      S1  promotion , but  my attendance wasn't
# 2      S3                  Rock music approaches
# 3      S4     to take your        donation ; any
# 4      S4  will be greatly         appreciated .
# 5      S5   a green donkey           flew a TEST
# 6      S5      TEST flew a           kite in the
# 7      S6 at high velocity                     .
</code></pre>

<p>That's probably a better format than the separate objects you ask for you in the question.  But to get as close as possible to your target, you can further transform it as follows.</p>

<pre><code># this picks up the empty matching sentence S2
(kw3 &lt;- merge(kw2, 
              data.frame(docname = names(Sentences), stringsAsFactors = FALSE), 
              all.y = TRUE))
# replaces the NA with the empty string
kw4 &lt;- as.data.frame(lapply(kw3, function(x) { x[is.na(x)] &lt;- """"; x} ), 
                     stringsAsFactors = FALSE)
# renames pre/post to before/after
names(kw4)[2:3] &lt;- c(""before"", ""after"")
# makes the docname unique
kw4$docname &lt;- make.unique(kw4$docname)

kw4
#   docname           before                 after
# 1      S1  promotion , but  my attendance wasn't
# 2      S2                                       
# 3      S3                  Rock music approaches
# 4      S4     to take your        donation ; any
# 5    S4.1  will be greatly         appreciated .
# 6      S5   a green donkey           flew a TEST
# 7    S5.1      TEST flew a           kite in the
# 8      S6 at high velocity                     .
</code></pre>
",0,1,118,2018-02-11 02:03:17,https://stackoverflow.com/questions/48727546/extract-n-words-around-defined-term-multicase
"tidytext, quanteda, and tm returning different tf-idf scores","<p>I am trying to work on tf-idf weighted corpus (where I expect tf to be a proportion by document rather than simple count). I would expect the same values to be returned by all the classic text mining libraries, but I am getting different values. Is there an error in my code (e.g. do I need to transpose an object?) or do the default parameters of tf-idf counts differ accross the packages?</p>

<pre><code>library(tm)
library(tidyverse) 
library(quanteda)
df &lt;- as.data.frame(cbind(doc = c(""doc1"", ""doc2""), text = c(""the quick brown fox jumps over the lazy dog"", ""The quick brown foxy ox jumps over the lazy god"")), stringsAsFactors = F)

df.count1 &lt;- df %&gt;% unnest_tokens(word, text) %&gt;% 
  count(doc, word) %&gt;% 
  bind_tf_idf(word, doc, n) %&gt;% 
  select(doc, word, tf_idf) %&gt;% 
  spread(word, tf_idf, fill = 0) 

df.count2 &lt;- df %&gt;% unnest_tokens(word, text) %&gt;% 
  count(doc, word) %&gt;% 
  cast_dtm(document = doc,term = word, value = n, weighting = weightTfIdf) %&gt;% 
  as.matrix() %&gt;% as.data.frame()

df.count3 &lt;- df %&gt;% unnest_tokens(word, text) %&gt;% 
  count(doc, word) %&gt;% 
  cast_dfm(document = doc,term = word, value = n) %&gt;% 
  dfm_tfidf() %&gt;% as.data.frame()

   &gt; df.count1
# A tibble: 2 x 12
  doc   brown    dog    fox   foxy    god jumps  lazy  over     ox quick   the
  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1 doc1      0 0.0770 0.0770 0      0          0     0     0 0          0     0
2 doc2      0 0      0      0.0693 0.0693     0     0     0 0.0693     0     0

&gt; df.count2
     brown       dog       fox jumps lazy over quick the foxy god  ox
doc1     0 0.1111111 0.1111111     0    0    0     0   0  0.0 0.0 0.0
doc2     0 0.0000000 0.0000000     0    0    0     0   0  0.1 0.1 0.1

&gt; df.count3
     brown     dog     fox jumps lazy over quick the    foxy     god      ox
doc1     0 0.30103 0.30103     0    0    0     0   0 0.00000 0.00000 0.00000
doc2     0 0.00000 0.00000     0    0    0     0   0 0.30103 0.30103 0.30103
</code></pre>
","r, text-mining, tm, quanteda, tidytext","<p>You stumbled upon the differences in calculating the term frequencies. </p>

<p>Standard definitions:</p>

<blockquote>
  <p>TF: Term Frequency: TF(t) = (Number of times term t appears in a
  document) / (Total number of terms in the document).</p>
  
  <p>IDF: Inverse Document Frequency: IDF(t) = log(Total number of
  documents / Number of documents with term t in it)</p>
  
  <p>Tf-idf weight is the product of these quantities TF * IDF</p>
</blockquote>

<p>Looks simple, but it isn't. Let's calculate the tf_idf for the word dog in doc1.</p>

<p>First TF for dog: That is 1 term / 9 terms in doc = 0.11111</p>

<pre><code>1/9 = 0.1111111
</code></pre>

<p>Now IDF for dog: the log of (2 documents / 1 term). Now there are multiple possibilities, namely: log (or natural log), log2 or log10! </p>

<pre><code>log(2) = 0.6931472
log2(2) = 1
log10(2) = 0.30103

#tf_idf on log:
1/9 * log(2) = 0.07701635

#tf_idf on log2:
1/9 * log2(2)  = 0.11111

#tf_idf on log10:
1/9 * log10(2) = 0.03344778
</code></pre>

<p>Now it gets interesting. <code>Tidytext</code> gives you a correct weighting based on log. <code>tm</code> returns the tf_idf based on log2. I expected the value 0.03344778 from quanteda because their base is log10. </p>

<p>But looking into quanteda, it returns the result correctly, but uses a count as default instead of a proportional count. To get everything as it should be, try the code as follows:</p>

<pre><code>df.count3 &lt;- df %&gt;% unnest_tokens(word, text) %&gt;% 
  count(doc, word) %&gt;% 
  cast_dfm(document = doc,term = word, value = n)


dfm_tfidf(df.count3, scheme_tf = ""prop"", scheme_df = ""inverse"")
Document-feature matrix of: 2 documents, 11 features (22.7% sparse).
2 x 11 sparse Matrix of class ""dfm""
      features
docs   brown        fox        god jumps lazy over quick the      dog     foxy       ox
  doc1     0 0.03344778 0.03344778     0    0    0     0   0 0        0        0       
  doc2     0 0          0              0    0    0     0   0 0.030103 0.030103 0.030103
</code></pre>

<p>That looks better and this is based on log10. </p>

<p>If you use <code>quanteda</code> with adjustments to the parameters, you can get the <code>tidytext</code> or <code>tm</code> outcome by changing the <code>base</code> parameter.</p>

<pre><code># same as tidytext the natural log
dfm_tfidf(df.count3, scheme_tf = ""prop"", scheme_df = ""inverse"", base = exp(1))

# same as tm
dfm_tfidf(df.count3, scheme_tf = ""prop"", scheme_df = ""inverse"", base = 2)
</code></pre>
",17,6,2524,2018-02-15 11:56:11,https://stackoverflow.com/questions/48806699/tidytext-quanteda-and-tm-returning-different-tf-idf-scores
Create Frequency table using R and Term document Matrix,"<p>I have created the following dataframe consisting of a few e-mail subject lines. </p>

<pre><code> df &lt;- data.frame(subject=c('Free ! Free! Free ! Clear Cover with New Phone',
                            'Offer ! Buy New phone and get earphone at 1000. Limited Offer!'))
</code></pre>

<p>I have created a list of frequent words derived from the above dataframe. I have added these keywords to the dataframe and dummy coded them as 0 </p>

<pre><code> most_freq_words &lt;- c('Free', 'New', 'Limited', 'Offer')



Subject                                               Free New Limited Offer                                                    

 'Free Free Free! Clear Cover with New Phone',          0   0     0      0
 'Offer ! Buy New phone and get earphone at             0   0     0      0
 1000. Limited Offer!'
</code></pre>

<p>I want to obtain a frequency count of the words in the e mail subject. The output should as follows</p>

<pre><code>  Subject                                             Free New Limited Offer                                                    

 'Free Free Free!  Clear Cover with New Phone',         3   1     0      0
 'Offer ! Buy New phone and get earphone at             0   1     1      2
 1000. Limited Offer!'
</code></pre>

<p>I have tried the following code</p>

<pre><code>for (i in 1:length(most_freq_words)){
df[[most_freq_words[i]]] &lt;- as.numeric(grepl(tolower(most_freq_words[i]), 
tolower(df$subject)))}
</code></pre>

<p>This however tells if the word is present or not in the sentence. I need the output given above. I request someone to help me</p>
","r, frequency, text-mining, grepl, term-document-matrix","<p>Here is another option with <code>tidyverse</code>.  We use <code>map</code> to loop over the 'most_freq_words', get its count from 'subject' column of 'df' with <code>str_count</code>, convert to <code>tibble</code>, set the names of the column from the 'most_freq_words' and bind the columns with the original dataset 'df'</p>

<pre><code>library(tidyverse)
most_freq_words %&gt;% 
      map(~ str_count(df$subject, .x) %&gt;%
                    as_tibble %&gt;% 
                    set_names(.x)) %&gt;% 
      bind_cols(df, .)
#                                                         subject Free New Limited Offer
#1                 Free ! Free! Free ! Clear Cover with New Phone    3   1       0     0
#2 Offer ! Buy New phone and get earphone at 1000. Limited Offer!    0   1       1     2
</code></pre>
",3,2,1038,2018-02-16 06:42:52,https://stackoverflow.com/questions/48821375/create-frequency-table-using-r-and-term-document-matrix
Matching documens with text2vec -- scaling problems,"<p>I am having a few issues with scaling a text matching program.  I am using text2vec which provides very good and fast results.</p>

<p>The main problem I am having is manipulating a large matrix which is returned by the text2vec::sim2() function.</p>

<p>First, some details of my hardware / OS setup:  Windows 7 with 12 cores about 3.5 GHz and 128 Gb of memory.  Its a pretty good machine.</p>

<p>Second, some basic details of what my R program is trying to achieve.  </p>

<p>We have a database of 10 million unique canonical addresses for every house / business in address. These reference addresses also have latitude and longitude information for each entry.</p>

<p>I am trying to match these reference addresses to customer addresses in our database. We have about 600,000 customer addresses. The quality of these customer addresses is not good.  Not good at all! They are stored as a single string field with absolutely zero checks on input.</p>

<p>The techical strategy to match these addresses is quite simple.  Create two document term matrices (DTM) of the customer addresses and reference addresses and use cosine similarity to find the reference address which is the most similar to a specific customer address.  Some customer addresses are so poor that will result in a very low cosine similarity -- so, for these addresses a ""no match"" would be assigned.</p>

<p>Despite being a pretty simple solution, the results obtained are very encouraging.</p>

<p>But, I am having problems scaling things....? And I am wondering if anyone has any suggestions.</p>

<p>There is a copy of my code below. Its pretty simple. Obviously, I cannot include real data but it should provide readers a clear idea of what I am trying to do.</p>

<p>SECTION A - Works very well even on the full 600,000 * 10 million input data set. </p>

<p>SECTION B - the text2vec::sim2() function causes R studio to shut down when the vocabulary exceeds about 140,000 tokens (i.e columns).  To avoid this, I process the customer addresses in chunks of about 200.</p>

<p>SECTION C - This is the most expensive section. When processing addresses in chunks of 200, SECTION A and SECTION B take about 2 minutes.  But SECTION C, using (what I would have thought to be super quick functions) take about 5 minutes to process to process a 10 million row * 200 column matrix.</p>

<p>Combined, SECIONS A:C take about 7 minutes to process 200 addresses.  As there are 600,000 addresses to process, this will take about 14 days to process.</p>

<p>Are they are ideas to make this code run faster...?</p>

<pre><code>rm(list = ls())
library(text2vec)
library(dplyr)


# Create some test data

# example is 10 entries.  
# but in reality we have 10 million addresses
vct_ref_address &lt;- c(""15 smith street beaconsfield 2506 NSW"", 
""107 orange grove linfield 2659 NSW"",
""88 melon drive calton 3922 VIC"", 
""949 eyre street sunnybank 4053 QLD"",
""12 black avenue kingston 2605 ACT"", 
""5 sweet lane 2004 wynyard NSW"",
""32 mugga way 2688 manuka ACT"",
""4 black swan avenue freemantle 5943 WA"",
""832 big street narrabeet 2543 NSW"", 
""5 dust road 5040 NT"")


# example is 4 entries
# but in reality, we have 1.5 million addresses
vct_test_address &lt;- c(""949 eyre street sunnybank 4053 QLD"",  
""1113 completely invalid suburb with no post code QLD"", 
""12 black road kingston 2605 ACT"",  
""949 eyre roaod sunnybank 4053 QLD"" )

# ==========================
# SECTION A ===== prepare data
# A.1 create vocabulary 
t2v_token &lt;- text2vec::itoken(c(vct_test_address, vct_ref_address),  progressbar = FALSE)
t2v_vocab &lt;- text2vec::create_vocabulary(t2v_token)
t2v_vectorizer &lt;- text2vec::vocab_vectorizer(t2v_vocab)
# A.2 create document term matrices dtm
t2v_dtm_test &lt;- text2vec::create_dtm(itoken(vct_test_address, progressbar = FALSE), t2v_vectorizer)
t2v_dtm_reference &lt;- text2vec::create_dtm(itoken(vct_ref_address, progressbar = FALSE), t2v_vectorizer)

# ===========================
# SECTION B ===== similarity matrix
mat_sim &lt;- text2vec::sim2(t2v_dtm_reference, t2v_dtm_test,  method = 'cosine', norm = 'l2')

# ===========================
# SECTION C ===== process matrix
vct_which_reference &lt;- apply(mat_sim, 2, which.max)
vct_sim_score &lt;- apply(mat_sim, 2, max)

# ============================
# SECTION D ===== apply results
# D.1 assemble results
df_results &lt;- data.frame(
test_addr = vct_test_address,
matched_addr = vct_ref_address[vct_which_reference],
similarity =  vct_sim_score )

# D.2 print results
df_results %&gt;% arrange(desc(similarity))
</code></pre>
","r, text-mining, text2vec","<p>The issue in step <strong>C</strong> is that <code>mat_sim</code> is sparse and all the <code>apply</code> calls make column/row subsetting which are super slow (and convert sparse vectors to dense).</p>

<p>There could be several solutions:</p>

<ol>
<li>if <code>mat_sim</code> is not very huge convert to the dense with <code>as.matrix</code> and then use <code>apply</code></li>
<li><p>Better you can convert <code>mat_sim</code> to sparse matrix in a triplet format with <code>as(mat_sim, ""TsparseMatrix"")</code> and then use <code>data.table</code> to get indices of the max elements. Here is an example:</p>

<pre><code>library(text2vec)
library(Matrix)
data(""movie_review"")
it = itoken(movie_review$review, tolower, word_tokenizer)
dtm = create_dtm(it, hash_vectorizer(2**14))


mat_sim = sim2(dtm[1:100, ], dtm[101:5000, ])
mat_sim = as(mat_sim, ""TsparseMatrix"")

library(data.table)

# we add 1 because indices in sparse matrices in Matrix package start from 1
mat_sim_dt = data.table(row_index = mat_sim@i + 1L, col_index = mat_sim@j + 1L, value = mat_sim@x)

res = mat_sim_dt[, 
        { k = which.max(value); list(max_sim = value[[k]], row_index = row_index[[k]]) }, 
        keyby = col_index]  
res
</code></pre></li>
</ol>

<p>Also as a side suggestion - I recommend to try <code>char_tokenizer()</code> with ngrams (for example of the size <code>c(3, 3)</code>) to ""fuzzy"" match different spelling and abbreviations of addresses.</p>
",1,0,707,2018-02-16 07:21:19,https://stackoverflow.com/questions/48821866/matching-documens-with-text2vec-scaling-problems
Relative Position of Words in String in R,"<p>I have the following term documnet matrix and dataframe.</p>

<pre><code>tdm &lt;- c('Free', 'New', 'Limited', 'Offer')



Subject                                               Free New Limited Offer                                                    

'Free Free Free! Clear Cover with New Phone',          0   0     0      0
'Offer ! Buy New phone and get earphone at             0   0     0      0
1000. Limited Offer!'
</code></pre>

<p>I want to derive the following dataframe as the output</p>

<pre><code>Subject                                              Free  New Limited Offer    
'Free Free Free! Clear Cover with New Phone',        1,2,3  8   NA     NA
Offer ! Buy New phone and get earphone at  1000.      NA    3   12      1,13
Limited Offer!'                                                                         
</code></pre>

<p>I tried the following code and got a result but this only gives me the position of the word along a string. I need the position of the words as in Hell0 - 1 new- 2.</p>

<pre><code>for(i in 1:length(tdm))
{    word.locations &lt;- 
gsub("")"","""",gsub(""c("","""",unlist(paste(gregexpr(pattern 
= tdm[i], DF$Subject))), fixed = TRUE), fixed = TRUE)
  df &lt;- cbind(DF,word.locations)
  }
  colnames(DF) &lt;- c(""text"", word)
</code></pre>

<p>I request someone to help. </p>
","r, regex, position, text-mining","<p>Given the inputs:</p>

<pre><code>tdm &lt;- c('Free', 'New', 'Limited', 'Offer')
subject &lt;- c(""Free Free Free! Clear Cover with New Phone"",
             ""Offer ! Buy New phone and get earphone at 1000. Limited Offer!"")
</code></pre>

<p>I'd do something like:</p>

<pre><code>sapply(tolower(tdm), function(x) {
    lapply(strsplit(tolower(subject), ""(\\s+)|(?!')(?=[[:punct:]])"", perl = TRUE), 
      function(y) {
        y &lt;- y[nzchar(y)]
        toString(grep(x, y))
      })
})
##      free      new limited offer  
## [1,] ""1, 2, 3"" ""8"" """"      """"     
## [2,] """"        ""4"" ""12""    ""1, 13""
</code></pre>

<p>What's going on:</p>

<ul>
<li>Use <code>tolower</code> on both the string to match against and the terms being matched.</li>
<li>Use <code>strsplit</code> to split words and punctuation into separate items in a <code>list</code> element.</li>
<li>Remove any empty vectors with <code>nzchar()</code>.</li>
<li>Use <code>grep()</code> to find the location of the matches.</li>
<li>Use <code>toString()</code> to paste the locations together as a comma-separated string.</li>
</ul>
",1,1,593,2018-02-20 06:19:36,https://stackoverflow.com/questions/48879037/relative-position-of-words-in-string-in-r
How do I include stopwords(terms) in text2vec,"<p>In <code>text2vec</code> package, I am using create_vocabulary function. For eg: 
My text is ""This book is very good"" and suppose I am not using stopwords and an ngram of 1L to 3L. so the vocab terms will be</p>

<p>This, book, is, very, good, This book,..... book is very, very good. I just want to remove the term ""book is very"" (and host of other terms using a vector). Since I just want to remove a phrase I cant use stopwords. I have coded the below code:</p>

<pre><code>vocab&lt;-create_vocabulary(it,ngram=c(1L,3L))
vocab_mod&lt;- subset(vocab,!(term %in% stp) # where stp is stop phrases.

x&lt;- read.csv(Filename') #these are all stop phrases
stp&lt;-as.vector(x$term)
</code></pre>

<p>When I do the above step, the metainformation in attributes get lost in vocab_mod and so can't be used in <code>create_dtm</code>.</p>
","r, text-mining, text2vec","<p>@Dmitriy even this lets to drop the attributes... So the way out that I found was just adding the attributes manually for now using attr function</p>

<p>attr(vocab_mod,""ngram"")&lt;-c(ngram_min = 1L,ngram_max=3L) and son one for other attributes as well. We can get attribute details from vocab.</p>
",0,1,424,2018-02-22 09:39:40,https://stackoverflow.com/questions/48924017/how-do-i-include-stopwordsterms-in-text2vec
Add new document to R corpus to find unique words,"<p>I have a corpus of speeches and I would like to identify the unique words within one kind of speeches.</p>

<p>This is what I did, I extracted two corpora from the larger one. In the script EUP_control_corpus and IMF_control_corpus. I made IMF_control_corpus into one text file which I want to combine with EUP_control_corpus, then by using tf.idf I want to find out which terms are unique for the IMF speeches in relation to EUP speeches. </p>

<p>However, I'm stuck at the part of adding to (combining with) a corpus. To me it seems like this should be very simple so I don't understand why I can't find anything on it. Is it so simple that no-one has asked this question?</p>

<p>I tried making both into a dfm and then joining them, or turning the text file back into a corpus to join them, but in both instances, the single text file turned out to have, once more, a great number of documents.</p>

<pre><code>  #Create date format
base_corpus$documents$int_date &lt;- 
  as.Date( base_corpus$documents$date,  format = ""%d-%m-%Y"")
head(as.Date( base_corpus$documents$date,  format = ""%d-%m-%Y""))


#Select pre-crisis EUP speeches for control group
EUP_control_corpus&lt;- 
  corpus_subset(base_corpus, country == ""European Parliament"" &amp; int_date &lt; as.Date( '31-12-2012', format = ""%d-%m-%Y""))
head(docnames(EUP_control_corpus), 50)
ndoc(EUP_control_corpus)


#Create dfm out of EUP corpus
EUP_control_dfm &lt;- 
  dfm(EUP_control_corpus, tolower = TRUE, stem = FALSE)
ndoc(EUP_control_dfm)


#Select pre-crisis IMF speeches for control group
IMF_control_corpus&lt;- 
  corpus_subset(base_corpus, country == ""International Monetary Fund"" &amp; int_date &lt; as.Date( '31-12-2012', format = ""%d-%m-%Y""))
head(docnames(IMF_control_corpus), 50)
ndoc(IMF_control_corpus)


#Combine IMF_control_corpus into one text
IMF_control_text&lt;-
  texts(corpus(texts(IMF_control_corpus, groups = ""texts"")))
IMF_control_dfm&lt;-
  dfm(IMF_control_text)
ndoc(IMF_control_dfm)


#Add IMF_control_text to EUP_control_dfm
plus_dfm&lt;-
  dfm(rbind(EUP_control_dfm, IMF_control_dfm))
ndoc((plus_dfm))


#Add IMF_control_text to EUP_control_corpus/ doesn't work, make text into single text corpus and then add?
total_control_corpus&lt;-
  corpus(EUP_control_corpus, IMF_control_text)
ndoc(total_control_corpus)
</code></pre>

<p>I have the idea that the group function in quanteda could be useful to do this in another way, but I decided to post the question first as have been on the search for a couple of days already. </p>

<p>Thank you for reading this question. </p>
","r, text-mining, quanteda","<p>This is not a question with a reproducible example, so it is hard to provide a correct answer.  Here are some suggestions:</p>

<ol>
<li><p>Create a new document variable called <code>control</code> that takes on one of two values, <code>IMF</code> or <code>EU</code>.  Use this using the conditionals that you were previously using with the <code>corpus_subset()</code> command.  From that, you can easily create a dfm that will continue to include this docvar, or you can use the <code>groups = ""control""</code> argument to <code>dfm()</code> to collapse the counts by the values of this variable.</p></li>
<li><p>Use <code>docvars(thecorpus, ""thevariable"") &lt;- newvalue</code> instead of addressing the inner contents of the corpus object.  That method is not stable since we may change the internal contents of the corpus at any time.</p></li>
</ol>
",0,1,533,2018-02-22 10:07:29,https://stackoverflow.com/questions/48924625/add-new-document-to-r-corpus-to-find-unique-words
Finding unusual phrases using a &quot;bag of usual phrases&quot;,"<p>My goal is to input an array of phrases as in</p>

<pre><code>array = [""Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua."",""At vero eos et accusam et justo duo dolores et ea rebum."",""Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.""]
</code></pre>

<p>and to present a new phrase to it, like </p>

<pre><code>""Felix qui potuit rerum cognoscere causas""
</code></pre>

<p>and I want it to tell me whether this is likely part of the group in the aforementioned <code>array</code> or not.</p>

<p>I found how to detect frequencies of words, but how do I find <code>unsimilarity</code>? After all, my goal is to find unusual phrases, not the frequency of certain words.</p>
","python, python-3.x, pandas, scikit-learn, text-mining","<p>You can build a simple ""language model"" for this purpose. It will estimate probability of a phrase, and mark phrases with low average per-word probability as unusual.</p>

<p>For word probability estimation, it can use a smoothed word count.</p>

<p>This is how the model could look like:</p>

<pre><code>import re
import numpy as np
from collections import Counter

class LanguageModel:
    """""" A simple model to measure 'unusualness' of sentences. 
    delta is a smoothing parameter. 
    The larger delta is, the higher is the penalty for unseen words.
    """"""
    def __init__(self, delta=0.01):
        self.delta = delta
    def preprocess(self, sentence):
        words = sentence.lower().split()
        return [re.sub(r""[^A-Za-z]+"", '', word) for word in words]
    def fit(self, corpus):
        """""" Estimate counts from an array of texts """"""
        self.counter_ = Counter(word 
                                 for sentence in corpus 
                                 for word in self.preprocess(sentence))
        self.total_count_ = sum(self.counter_.values())
        self.vocabulary_size_ = len(self.counter_.values())
    def perplexity(self, sentence):
        """""" Calculate negative mean log probability of a word in a sentence 
        The higher this number, the more unusual the sentence is.
        """"""
        words = self.preprocess(sentence)
        mean_log_proba = 0.0
        for word in words:
            # use a smoothed version of ""probability"" to work with unseen words
            word_count = self.counter_.get(word, 0) + self.delta
            total_count = self.total_count_ + self.vocabulary_size_ * self.delta
            word_probability = word_count / total_count
            mean_log_proba += np.log(word_probability) / len(words)
        return -mean_log_proba

    def relative_perplexity(self, sentence):
        """""" Perplexity, normalized between 0 (the most usual sentence) and 1 (the most unusual)""""""
        return (self.perplexity(sentence) - self.min_perplexity) / (self.max_perplexity - self.min_perplexity)

    @property
    def max_perplexity(self):
        """""" Perplexity of an unseen word """"""
        return -np.log(self.delta / (self.total_count_ + self.vocabulary_size_ * self.delta))

    @property
    def min_perplexity(self):
        """""" Perplexity of the most likely word """"""
        return self.perplexity(self.counter_.most_common(1)[0][0])
</code></pre>

<p>You can train this model and apply it to different sentences. </p>

<pre><code>train = [""Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua."",
                 ""At vero eos et accusam et justo duo dolores et ea rebum."",
                 ""Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.""]
test = [""Felix qui potuit rerum cognoscere causas"", # an ""unlikely"" phrase
        'sed diam nonumy eirmod sanctus sit amet', # a ""likely"" phrase
       ]

lm = LanguageModel()
lm.fit(train)

for sent in test:
    print(lm.perplexity(sent).round(3), sent)
</code></pre>

<p>which prints to you</p>

<pre><code>8.525 Felix qui potuit rerum cognoscere causas
3.517 sed diam nonumy eirmod sanctus sit amet
</code></pre>

<p>You can see that ""unusualness"" is higher for the first phrase than for the second, because the second one is made from the training words.</p>

<p>If your corpus of ""usual"" phrases is large enough, you can switch from 1-gram models I use to N-grams (for English, sensible N is 2 or 3). Alternatively, you can use recurrent neural nets to predict probability of each word conditional on all the previous words. But this requires a really huge training corpus.</p>

<p>If you work with a highly flective language, like Turkish, you can use character-level N-grams instead of a word-level model, or just preprocess your texts using a lemmatization algorithm from NLTK.  </p>
",3,1,676,2018-02-22 10:23:33,https://stackoverflow.com/questions/48924970/finding-unusual-phrases-using-a-bag-of-usual-phrases
Word matching with dictionaries using R. Analyzing Survey Comments with existing dictionaries,"<p>I'm trying to analyze a list of survey comments with existing word dictionaries in R. Survey comments range from one word to multiple sentences. I have multiple word dictionaries like the ones sampled below. Ideally I would like Column 1 to be survey comments, followed by a column for each dictionary. </p>

<p>The dictionary columns would return a ""True"" or ""False"" if a word within the dictionary appears in the survey comment. Some survey comments should have multiple tallies, indicating multiple categories are in the comment.</p>

<p>Survey Comments</p>

<pre><code>Survey&lt;- c(""Benefits are great"", ""I like the flexible hours"", ""my manager is bad"", ""the manager is great"", ""my manager gives me flexible hours to work"")
</code></pre>

<p>Dictionaries</p>

<pre><code>CompDictionary&lt;- c(""benefit"", ""benefits"", ""pay"")
FlexDictionary&lt;- c(""flexible"", ""flex day"", ""flex time"")
LeadDictionary&lt;- c(""manager"", ""boss"", ""director"")
</code></pre>

<p>Any help with this would be much appreciated. Let me know if there's anything else I can provide that would help.</p>
","r, string, text-mining","<p>Given your problem as stated, here's a solution using base R.</p>

<pre><code>Survey&lt;- c(""Benefits are great"", ""I like the flexible hours"", ""my manager is bad"", ""the manager is great"", ""my manager gives me flexible hours to work"")

CompDictionary&lt;- c(""benefit"", ""benefits"", ""pay"")
FlexDictionary&lt;- c(""flexible"", ""flex day"", ""flex time"")
LeadDictionary&lt;- c(""manager"", ""boss"", ""director"")

f = function(dict,Survey){
  apply(do.call(rbind,lapply(dict,grepl,Survey,ignore.case=T)),2,any)
}

res = lapply(list(""Comp""=CompDictionary,""Flex""=FlexDictionary,""Lead""=LeadDictionary),f,Survey)

df = as.data.frame(res)
df$Survey = Survey
</code></pre>

<p>Since you have a list of target words and your survey responses are unprocessed, you would need to use <code>grepl</code> which searches for a word and returns a logical TRUE/FALSE if it is present.  However, <code>grepl</code> only accepts one input, so we need to <code>apply</code> over the list of dictionary terms.  Then we need to check if <code>any</code> of them are TRUE.  I wrapped all this up in a function to make calling it later easy.</p>

<p>Now, we need to run that function for each Dictionary. I created a named list (to make the data.frame step easier) and used <code>lapply</code> to pass each dictionary into the function I made.  The resulting named list is turned into a data.frame and I append the survey comments.</p>

<p><strong>Potential Point of Failure</strong></p>

<p><code>grepl</code> uses regular expressions which is a form of pattern matching. I already set the <code>ignore.case</code> flag to TRUE so that 'benefits' will match 'Benefits'.  Still, for more complex matches (e.g. 'flex day') you will only get TRUE on an <em>exact match</em> so 'flexible hours' isn't matched by any of your dictionaries right now. So while you have 'benefit' and 'benefits' in your dictionaries (unnecessary in this case) you would have to have 'flexible hour' and 'flex hour' to catch people using the term 'flexible' or 'flex'.</p>

<p><strong>Things to Consider</strong></p>

<p>Tokenizing your text and then applying a lemmatizer (or even stemming) could help to reduce variation in terms (making 'flex' and 'flexible' the same word).  This would require that you then learn what the new word is and insert it into your dictionaries.</p>
",1,0,347,2018-02-26 16:23:45,https://stackoverflow.com/questions/48992792/word-matching-with-dictionaries-using-r-analyzing-survey-comments-with-existing
Find common words within two list,"<pre><code>x1 &lt;- c(""I like apple"", ""she enjoys reading"")
x2 &lt;- c(""he likes apple"", ""Mike wants to read this book"")  
w1 &lt;- strsplit(x1, "" "")
w2 &lt;- strsplit(x2, "" "")  
</code></pre>

<p>I get two lists:  </p>

<pre><code>w1  
[[1]]  
[1] ""I""      ""like""   ""apple""  

[[2]]  
[1] ""she""      ""enjoys"" ""reading""

w2  
[[1]]  
[1] ""he""    ""likes""  ""apple""

[[2]]  
[1] ""Mike""  ""wants"" ""to""    ""read""  ""this""  ""book""  
</code></pre>

<p>I want to get </p>

<pre><code>intersect(w1[[1]], w2[[1]])
intersect(w1[[2]], w2[[2]])
</code></pre>

<p>Suppose the lengths of <code>w1</code> and <code>w2</code> are very large, thus using for loop is not an efficient way. is there any more convenient way to get the corresponding intersect?</p>
","r, list, text-mining, intersect","<p>We can use <code>Map</code> for appylying functions on corresponding elements</p>

<pre><code>Map(intersect, w1, w2)
#[[1]]
#[1] ""apple""

#[[2]]
# character(0)
</code></pre>

<hr>

<p>Or using <code>pmap/map2</code> from <code>purrr</code></p>

<pre><code>library(purrr)
pmap(list(w1, w2), intersect)
</code></pre>
",3,2,881,2018-02-28 01:08:40,https://stackoverflow.com/questions/49020454/find-common-words-within-two-list
What is the conceptual difference between topic extraction and text categorization?,"<p>I'm confused that very similar services for text mining have different names, like topic extraction and text categorization/classification. What is the conceptual difference between them?</p>

<p>Topic extraction example:
<a href=""https://www.uclassify.com/browse/uclassify/topics?input=Text"" rel=""nofollow noreferrer"">https://www.uclassify.com/browse/uclassify/topics?input=Text</a></p>

<p>Categorization example:
<a href=""https://dandelion.eu/semantic-text/text-classification-demo/"" rel=""nofollow noreferrer"">https://dandelion.eu/semantic-text/text-classification-demo/</a></p>
","data-mining, text-mining, categorical-data, topic-modeling","<p>Topic Model approaches (Topic Extraction) are unsupervised approaches. So, you don't need to know that each document belongs to what categories  (classes) [<a href=""https://en.wikipedia.org/wiki/Topic_model]"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Topic_model]</a>. 
Latent Dirichlet allocation (LDA) is a method for Topic Modeling. LDA divides the documents into topics and assigns a name to the topics. [<a href=""https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation]"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation]</a>
Topic Model needs the number of output clusters as the same as clustering methods. But they assign a topic name to each output cluster.
In contrast to Topic Model approaches, Document Classification approaches (Categorization) are supervised. So, they need the class labels. [<a href=""https://en.wikipedia.org/wiki/Document_classification]"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Document_classification]</a></p>
",2,1,831,2018-02-28 11:29:56,https://stackoverflow.com/questions/49028540/what-is-the-conceptual-difference-between-topic-extraction-and-text-categorizati
apply function to textreuse corpus,"<p>I have a data frame as follows:</p>

<pre><code>df&lt;-data.frame(revtext=c('the dog that chased the cat', 'the dog which chased the cat', 'World Cup Hair 2014 very funny.i can change', 'BowBow', 'this is'), rid=c('r01','r02','r03','r04','r05'), stringsAsFactors = FALSE)

                             revtext        rid
             the dog that chased the cat    r01
             the dog which chased the cat   r02
World Cup Hair 2014 very funny.i can change r03
             Bow Bow                        r04
             this is                        r05
</code></pre>

<p>I'm using the package <code>textreuse</code> to convert <code>df</code> to a <code>corpus</code> doing:</p>

<pre><code>#install.packages(textreuse)
library(textreuse)
d&lt;-df$revtext
names(d)&lt;-df$rid
corpus &lt;- TextReuseCorpus(text = d,
                      tokenizer = tokenize_character, k=3,
                      progress = FALSE,
                      keep_tokens = TRUE)
</code></pre>

<p>where <code>tokenize_character</code> is a function I programmed as:</p>

<pre><code> tokenize_character &lt;- function(document, k) {
                       shingles&lt;-c()
                 for( i in 1:( nchar(document) - k + 1 ) ) {
                         shingles[i] &lt;- substr(document,start=i,stop= (i+k-1))
                     }
return( unique(shingles) )  
}   
</code></pre>

<p>However, I'm prompted with some warnings: <code>Skipping document with ID 'r04' because it has too few words to create at least two n-grams with n = 3.</code>. But note that my tokenizer works on a character level. The text of <code>r04</code> is long enough. In fact, if we run <code>tokenize_character('BowBow',3)</code> we get: <code>""Bow"" ""owB"" ""wBo""</code> as desired. </p>

<p>Note also that for <code>r01</code>, <code>TextReuseCorpus</code> is working as it is supposed, returning: <code>tokens(corpus)$`r01= ""the"" ""he "" ""e d"" "" do"" ""dog"" ""og "" ""g t"" "" th"" ""tha"" ""hat"" ""at "" ""t c"" "" ch"" ""cha"" ""has"" ""ase"" ""sed"" ""ed "" ""d t"" ""e c"" "" ca"" ""cat""</code></p>

<p>Any suggestions? I don't know what I'm missing here. </p>
","r, nlp, text-mining, corpus","<p>From the details section of <code>textreuse::TextReuseCorpus</code> <a href=""https://cran.r-project.org/web/packages/textreuse/index.html"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>If skip_short = TRUE, this function will skip very short or empty
  documents. A very short document is one where there are two few words
  to create at least two n-grams. For example, if five-grams are
  desired, then a document must be at least six words long.  If no value of n is 
  provided, then the function assumes a value of n = 3.</p>
</blockquote>

<p>From this, we know documents having &lt; 4 words will be skipped as short documents (as n=3 in your example), which is what we see for <code>r04</code> &amp; <code>r05</code> that have 1 &amp; 2 words, respectively. 
To not skip these documents, you can use <code>skip_short = F</code> which will return the output as intended:</p>

<pre><code>corpus &lt;- TextReuseCorpus(text = d, tokenizer = tokenize_character, k=3,
                      skip_short = F, progress = FALSE, keep_tokens = TRUE)
tokens(corpus)$r04
[1] ""Bow"" ""owB"" ""wBo""
</code></pre>
",2,3,490,2018-03-01 02:12:54,https://stackoverflow.com/questions/49041597/apply-function-to-textreuse-corpus
subscript out of bounds in R,"<p>I want to remove certain element from <code>names</code>.</p>

<p><code>names</code> is a list of characters. After I run the following loop:</p>

<pre><code>for (i in 1:length(names)){
  if((str_detect(names[[i]], ""  Organisation Name"")) || 
  (str_detect(names[[i]], ""^ $"")) || (str_detect(names[[i]], ""^0$"")) || 
  (str_detect(names[[i]], ""^$""))  ){
   names[[i]] &lt;- NULL
 }
}
</code></pre>

<p>I get an error. The error is: </p>

<blockquote>
  <p>Error in names[[i]] : subscript out of bounds</p>
</blockquote>
","r, regex, text-mining, stringr","<p>Here's some code illustrating what I think is going on based on my comment.</p>

<pre><code>names &lt;- lapply(1:5, list)
for (i in 1:length(names)) {
  names[[i]] &lt;- NULL
  print(sprintf('Length is now %d, i is now %i', length(names), i))
  print(names[[i]])
}
</code></pre>

<p>This outputs</p>

<pre><code>[1] ""Length is now 4, i is now 1""
[[1]]
[1] 2

[1] ""Length is now 3, i is now 2""
[[1]]
[1] 4

[1] ""Length is now 2, i is now 3""
Error in names[[i]] : subscript out of bounds
</code></pre>

<p>If you iterate backwards, as with <code>for (i in length(names):1)</code> that might work</p>
",2,0,14016,2018-03-02 21:27:07,https://stackoverflow.com/questions/49077678/subscript-out-of-bounds-in-r
Text extraction from PDF returns strange results in R,"<p>I am trying to mine text from a bunch of PDFs, but when I read them into R using <code>pdf_text</code> from the <code>pdftools</code> package, the text it produces is just strange and nothing like what is actually on the PDF file. Onedrive link: <a href=""https://1drv.ms/b/s!AlTtlgN0WIa3s2qeq4yrv9fUu-Z6"" rel=""nofollow noreferrer"">https://1drv.ms/b/s!AlTtlgN0WIa3s2qeq4yrv9fUu-Z6</a> .
Here's the sample code I use:</p>

<pre><code>library(pdftools)
pdf1 &lt;- pdf_text(""https://dl.dropboxusercontent.com/s/308gpdijvnw18mf/2018REQ118030709.pdf?dl=0"")
pdf1   

     ## c(""(’-*)&amp;&amp;$(&amp;’-’’’’)*,&amp;’$)’&amp;/.\r\n     itiCHMON&amp;\\     4Q\\a WN BQKPUWVL
     ##FQZOQVQI                                          )’(/ 7QZ[\\ 9ITN BMIT
     ##6[\\I\\M DI‘ 3QTT\r\n                    5Q^Q[QWV WN 4WTTMK\\QWV[\r\n                   
     ##FE 8_h -10+0\r\n                    HYSX]_^T’ L7 -.-1,(10+0                                                 
     ##3QTT &gt;]UJMZ (/’*’.’0\r\n   IBKHHO F7L;HI ?D9                                                        
     ##@TMI[M ZMKWZL 3QTT &gt;]UJMZ QV UMUW [MK\\QWV WN KPMKS\r\n   ,0+, L7BB;O H:\r\n  
     ##H?9&gt;CED: L7 -.---(0/+1                                                         
     ##IVL QVKT]LM QV ITT WVTQVM JIVSQVO \\ZIV[IK\\QWV[\r\n                                
     ##@ZWXMZ\\a :VNWZUI\\QWV                                                          
     ##DI‘ :VNWZUI\\QWV\r\n     JQh OUQb5                                                          
     ##-+,3 J_dQ\\ 7TZecdUT 7^^eQ\\ 9XQbWUc5                                     
     ##!,+’/+/)++\r\n     3QTT &gt;]UJMZ1                                .
     ##.. &lt;truncated&gt;
</code></pre>

<p>I am pretty new to R, any idea what I may be doing wrong?
Please, any help with this would be appreciated.</p>

<p>Edit: I have replaced the url with a working url and I have  also included the results that I am getting.</p>
","r, pdf, text-mining","<p>You pdf is a pdf image. It looks like a scan. <code>pdftools</code> cannot convert this directly into text. You can use the package <code>tesseract</code> to get the data and <code>pdftools</code> to convert it into an png.</p>

<p>Code below will transform the first page into text. I will let you do the rest of the pages. Rembember that OCR to text isn't perfect. You need to check the outcome.</p>

<pre><code>library(pdftools)
library(tesseract)
pdf_convert(""https://dl.dropboxusercontent.com/s/308gpdijvnw18mf/2018REQ118030709.pdf?dl=0"", 
                       pages = 1, 
                       dpi = 600, 
                       filenames = ""page1.png"")
text &lt;- ocr(""page1.png"")
cat(text)
</code></pre>

<p>More information is available in the tesseract vignette.</p>

<p>You also might want to remove access to this pdf. I'm not sure it this data should be publicly available</p>
",5,3,2092,2018-03-03 08:37:41,https://stackoverflow.com/questions/49082299/text-extraction-from-pdf-returns-strange-results-in-r
Replace ending of words with a new ending in python dataframe,"<p>I have a Dataframe full of french words, endings and new endings. I want to create a 4th column with the alternative to the word as such:</p>

<pre><code>word   |ending|new ending|what i want|
--------------------------------------
placer |cer   |ceras     |placeras   |
placer |cer   |cerait    |placerait  |
placer |cer   |ceront    |placeront  |
finir  |ir    |iras      |finiras    |
</code></pre>

<p>So it's basically to replace, in column 1, what's equivalent in column 2, by what I have in column 3.</p>

<p>Any ideas ?</p>
","python, string, pandas, dataframe, text-mining","<p>Using <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>apply()</code></a>:</p>

<pre><code>df['new_word'] = df.apply(
    lambda row: row['word'].replace(row['ending'], row['new ending']),
    axis=1
)
#     word ending new ending   new_word
#0  placer    cer      ceras   placeras
#1  placer    cer     cerait  placerait
#2  placer    cer     ceront  placeront
#3   finir     ir       iras    finiras
</code></pre>

<p>As @jpp pointed out, a caveat to this approach is that it won't work correctly if the ending is present in the middle of the string. </p>

<p>In that case, refer to <a href=""https://stackoverflow.com/questions/3675318/how-to-replace-the-some-characters-from-the-end-of-a-string"">this post</a> on how to replace at the end of the string.</p>
",2,3,491,2018-03-08 19:58:44,https://stackoverflow.com/questions/49181480/replace-ending-of-words-with-a-new-ending-in-python-dataframe
Python nltk cannot tokenize Arabic text,"<p>When using nltk package to tokenize Arabic text, the results appear as numbers! There is no problem when tokenizing English text.</p>

<blockquote>
  <p>UnicodeDecodeError: 'ascii' codec can't decode byte 0xd8 in position 0: ordinal not in range(128)</p>
</blockquote>

<pre><code># -*- coding: utf-8 -*-
import nltk
from nltk.tokenize import word_tokenize
import stop_words
from stop_words import get_stop_words
doc_a = ""ذهب محمد الى المدرسه على دراجته. هذا اول يوم له في المدرسة""
sw = get_stop_words('ar')
tokens = nltk.word_tokenize(doc_a)
stopped_tokens = [i for i in tokens if not i in sw]
print(stopped_tokens)
</code></pre>

<p>When set <code>tokens = nltk.word_tokenize(unicode(doc_a,""utf-8""))</code></p>

<p>Result:</p>

<blockquote>
  <p>[u'\u0630\u0647\u0628', u'\u0645\u062d\u0645\u062f', u'\u0627\u0644\u0645\u062f\u0631\u0633\u0647', u'\u062f\u0631\u0627\u062c\u062a\u0647', u'.', u'\u0627\u0644\u0645\u062f\u0631\u0633\u0629']</p>
</blockquote>
","python, anaconda, nltk, text-mining","<p>For me the following code was working for me under python 3.X:</p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

doc_a = ""ذهب محمد الى المدرسه على دراجته. هذا اول يوم له في المدرسة""
doc_a = doc_a.decode('utf-8')
sw = stopwords.words('arabic')
tokens = nltk.word_tokenize(doc_a)
stopped_tokens = [i for i in tokens if not i in sw]
for item in stopped_tokens:
    print(item)      
</code></pre>

<p>This line gets you the right stopwords: <code>sw = stopwords.words('arabic')</code>.</p>
",0,1,3096,2018-03-09 12:36:01,https://stackoverflow.com/questions/49193966/python-nltk-cannot-tokenize-arabic-text
Add date and time to wordcloud plot,"<p>I am running a loop all day and during its execution, it saves different wordcloud graphs. I need to include or add the time in the graph bottom, footnote or even subtitle.</p>

<p>Here is a basic example of how my code is structured:</p>

<pre><code>jpeg(paste(""C:/Users/,
i,format(Sys.time(), ""_%d-%m-%y %H-%M""), "".jpg"", sep=""""),
width=5, height=5, units=""in"",  res=1300)

w=wordcloud(dm$word, dm$freq, min.freq=25, random.order = F, 
colors = sample(colors()[3:79]))
print(w)
dev.off()
</code></pre>

<p>I did try with </p>

<pre><code>title(paste(Sys.time()))
</code></pre>

<p>but didn't work</p>
","r, text, text-mining, word-cloud","<p>Try to adapt this code:</p>

<pre><code>library(""tm"")
library(""wordcloud"")
x&lt;-""Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura, ché la diritta via era smarrita.""

x&lt;-unlist( strsplit(gsub(""[[:punct:]]"", '', x), ""\ "")) 

layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, Sys.time())
wordcloud(x, main=""Title"")
</code></pre>

<p><a href=""https://i.sstatic.net/IPY5s.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IPY5s.jpg"" alt=""enter image description here""></a>
Example adapted from this previous <a href=""https://stackoverflow.com/questions/15224913/r-add-title-to-wordcloud-graphics-png"">post</a></p>
",0,0,1497,2018-03-09 15:45:27,https://stackoverflow.com/questions/49197347/add-date-and-time-to-wordcloud-plot
Replace words from list with a dataframe,"<p>I have a list of words. Say:</p>

<pre><code>list = ['this', 'that', 'and', 'more']
</code></pre>

<p>I want to replace words in such fashion:</p>

<pre><code>x    |y
-----------
this |that
plus |more
</code></pre>

<p>Every time a word from the list is in column <code>y</code>, I want to replace it with what's found in column <code>x</code> on the same row. If the word isn't in <code>y</code>, it should remain as is. How can this be done?</p>
","python, python-3.x, pandas, replace, text-mining","<p>You can convert this translation table, call it <code>df</code>, to a <code>dict</code>, then the following will act as the desired replacement function.</p>

<pre><code>d = dict(df['y', 'x'].iterrows())

new_list = [d.get(word, word) for word in list]

# new_list: ['this', 'this', 'and', 'plus']
</code></pre>
",2,0,426,2018-03-12 14:59:09,https://stackoverflow.com/questions/49238308/replace-words-from-list-with-a-dataframe
Text Processing Tools for German and Spanish Languages,"<p>I'm trying to process text in German and Spanish languages. Working on English text is straight forward because of myriad NLP packages on this language. But it's not easy for other languages. I Found some packages for German text but I don't know which one is more accurate. Also, It's more difficult to find NLP package for Spanish text considering that there are some special characters in this language. Some steps that I need to do on the text are: Sentence Splitting, Tokenizing, Pos tagging and Stemming. In other words, I am looking for something that works on one or both of these two languages in Java.</p>

<p>Any information on this topic is appreciated.. </p>
","java, nlp, stanford-nlp, text-mining, linguistics","<p>I can recommend you <strong>Freeling</strong>, check its <a href=""http://nlp.lsi.upc.edu/freeling/demo/demo.php"" rel=""nofollow noreferrer"">Freeling_online_demo</a>, it includes Sentence Splitting, Tokenizing, Pos tagging and other functionalities for several language. I dont know how good it's for german but for analyze spanish is the best tool I know. I've just used Freeling via python+command line, but there are interfaces for java too, for example <a href=""https://github.com/TALP-UPC/FreeLing/tree/master/APIs/java"" rel=""nofollow noreferrer"">Freeling_jaVa_API</a>.</p>

<p>Good luck!</p>
",1,0,589,2018-03-13 08:22:28,https://stackoverflow.com/questions/49251361/text-processing-tools-for-german-and-spanish-languages
Keyword/keyphrase extraction from text,"<p>I am working on a project where I need to extract ""technology related keywords/keyphrases"" from text. For example, my text is: </p>

<p><strong>""ABC Inc has been working on a project related to machine learning which makes use of the existing libraries for finding information from big data.""</strong> </p>

<p>The extracted keywords/keyphrase should be: {machine learning, big data}.</p>

<p>My text documents are stored as BSON documents in MongoDb.</p>

<p>What are the best nlp libraries(with sufficient documentation and examples) out there to perform this task and how? </p>

<p>Thanks!</p>
","machine-learning, nlp, text-mining, jnlp, text-extraction","<p>It looks you need to narrow down more than just keywords/key phrases and find the subject and object per sentence.
For subject/object recognition, I recommend the Stanford Parser or the Google Language API, where you send a string and get a dependency tree response.</p>

<p>You can test the Google API first to see if it works well with your corpus: <a href=""https://cloud.google.com/natural-language/"" rel=""noreferrer"">https://cloud.google.com/natural-language/</a></p>

<p>The outcome here is a subject predicate object (SPO) triplet, where your predicate describes the relationship. You'll need to traverse the dependency graph and write a script to parse out the triplet.</p>

<p>Other Packages:
I use NLTK, Spacy, and Textblob frequently. If the corpus is simple, generic, and straightforward, Spacy and Textblob work well OOTB. If the corpus is highly customized, domain-specific, messy (incorrect spelling or grammar), etc. I'll use NLTK and spend more time customizing my NLP text processing pipeline with scrubbing, lemmatizing, etc.  You may want to add your own custom dictionary of technology related keywords and keyphrases so that your parser can catch these if you decide to go with one of these packages.</p>

<p>NLTK Tutorial: <a href=""http://www.nltk.org/book/"" rel=""noreferrer"">http://www.nltk.org/book/</a></p>

<p>Spacy Quickstart: <a href=""https://spacy.io/usage/"" rel=""noreferrer"">https://spacy.io/usage/</a></p>

<p>Textblob Quickstart: <a href=""http://textblob.readthedocs.io/en/dev/quickstart.html"" rel=""noreferrer"">http://textblob.readthedocs.io/en/dev/quickstart.html</a></p>
",6,6,8364,2018-03-13 18:28:38,https://stackoverflow.com/questions/49263374/keyword-keyphrase-extraction-from-text
Extracting character font size from PDF files with R,"<p>I've been trying to reproduce a similar dataset (not exactly the same, I stress) explained in this <a href=""https://pdfs.semanticscholar.org/7955/9bc8cff7f81699a63e8c43548753041cd920.pdf"" rel=""nofollow noreferrer"">paper</a> for a similar purpose. But I'm having trouble in coming up with an idea for getting font size while coding in R. Other solutions seem to be available in other coding languages.  </p>

<p>For instance, one could very easily extract information regarding number of characters in a page or transforming each page in a image and obtaining data regarding number of pixels and such - which will be part of my metadata anyway. Such as in the example below:</p>

<pre><code>library(pdftools)
library(png)

download.file(""http://arxiv.org/pdf/1403.2805.pdf"", ""1403.2805.pdf"", mode = ""wb"")

txt &lt;- pdf_text(""1403.2805.pdf"")

num_char_page = unlist(lapply(txt,nchar))

height = 1:length(txt)
width =1:length(txt)

for (i in 1:length(txt)) {

  bitmap &lt;- pdf_render_page(""1403.2805.pdf"", page = i)

  png::writePNG(bitmap, paste0(""page"",i,"".png""))

  photo=readPNG(paste0(""page"",i,"".png""))

  height[i]  = dim(photo)[1]

  width[i] = dim(photo)[2]

}

layout_df = data.frame(page=1:length(txt), num_char_page=num_char_page, height=height, width=width)
</code></pre>

<p>So this is fairly straightforward, although the code could be made faster with some lapply version of it in the loop part (maybe). But I have no idea on how to obtain font size. How would I do it? Specially if we assume a scanned version of the documents, such as in the aforementioned paper.</p>

<hr>

<p><strong>Observation</strong>: I will ask it in a separate question probably, but I would cherish if someone could pinpoint to some ideas regarding margin sizes and spacing between lines in the comments.</p>

<p><strong>Second Observation</strong>: I think (in this particular case) the PDF that I've used as an example could have meta-data which could enable font-size extraction. But I am trying to obtain font size from scanned (and maybe OCR'd) PDFs. One could transform the pages of the PDF (in the example) into images and then transform them again into non-OCR'd PDFs, which might be somewhat similar to the scanned PDF situation.   </p>
","r, pdf, text-mining","<p>It's probably not possible to determine the actual font size, at least not without knowing the exact font and its specifications. <a href=""https://graphicdesign.stackexchange.com/questions/4035/what-does-the-size-of-the-font-translate-to-exactly"">see here for an explanation of why</a></p>

<p>If you just want to compare font size between documents, it might be sufficient to use average line height as the comparison, which might be easier to do. If you don't care about the actual values and only need to know relative size between documents, the following might work. You would have to consider, or avoid, the potential effect of different document sizes and/or DPI.</p>

<pre><code>library(tesseract)
library(dplyr)
library(tidyr)

df &lt;- ocr_data(""http://arxiv.org/pdf/1403.2805.pdf"")

df %&gt;% 
  separate(bbox, c('x1', 'y1', 'x2', 'y2'), convert = T) %&gt;% 
  mutate(line_height = y2 - y1) %&gt;% 
  summarise(avg_line_height = mean(line_height))

# # A tibble: 1 x 1
#   avg_line_height
#             &lt;dbl&gt;
# 1            58.7
</code></pre>

<p>example for average letter height and width....</p>

<pre><code>df %&gt;%
  separate(bbox, c('x1', 'y1', 'x2', 'y2'), convert = T) %&gt;%
  mutate(word_height = y2 - y1) %&gt;%
  mutate(word_width = x2 - x1) %&gt;%
  mutate(num_letters = nchar(word)) %&gt;%
  mutate(avg_letter_width = word_width / num_letters) %&gt;%
  summarise(avg_letter_height = mean(word_height),
            avg_letter_width = mean(avg_letter_width))

# # A tibble: 1 x 2
#   avg_letter_height avg_letter_width
#               &lt;dbl&gt;            &lt;dbl&gt;
# 1              58.7             37.3
</code></pre>

<p>and if you want to do it per page, you can use <code>pdftools</code> to render each page of a multi-page PDF individually and the run <code>ocr_data</code> on each one, then combine...</p>

<pre><code>library(pdftools)
library(tesseract)
library(dplyr)
library(tidyr)

download.file(url = ""http://arxiv.org/pdf/1403.2805.pdf"",
              destfile = pdf_path &lt;- tempfile(fileext = "".pdf""))

page_pngs &lt;-
  lapply(seq_len(pdf_info(pdf_path)$pages), function(page_num) {
    pdf_convert(pdf_path, pages = page_num, dpi = 300)
  })

df &lt;-
  bind_rows(
    lapply(seq_len(length(page_pngs)), function(page_num) {
      ocr_data(page_pngs[[page_num]]) %&gt;%
        separate(bbox, c('x1', 'y1', 'x2', 'y2'), convert = T) %&gt;%
        mutate(word_height = y2 - y1) %&gt;%
        mutate(word_width = x2 - x1) %&gt;%
        mutate(num_letters = nchar(word)) %&gt;%
        mutate(avg_letter_width = word_width / num_letters) %&gt;%
        mutate(page = page_num) %&gt;%
        select(page, letter_height = word_height, letter_width = avg_letter_width)
    })
  )

df %&gt;%
  group_by(page) %&gt;%
  summarise(avg_letter_height = mean(letter_height),
            avg_letter_width = mean(letter_width)) %&gt;%
  mutate(avg_letter_area = avg_letter_height * avg_letter_width)

# # A tibble: 29 x 4
#     page avg_letter_height avg_letter_width avg_letter_area
#    &lt;int&gt;             &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;
#  1     1              29.4             17.9            525.
#  2     2              29.3             18.9            554.
#  3     3              30.0             19.1            574.
#  4     4              30.2             18.7            565.
#  5     5              29.8             19.0            566.
#  6     6              28.2             17.7            498.
#  7     7              28.9             18.3            529.
#  8     8              29.8             18.6            554.
#  9     9              29.1             18.6            541.
# 10    10              28.3             18.3            519.
# # ... with 19 more rows
</code></pre>
",3,2,1469,2018-03-16 12:52:03,https://stackoverflow.com/questions/49321238/extracting-character-font-size-from-pdf-files-with-r
Trim each column values at pandas,"<p>I am working on .xls files after import data to a data frame with pandas, need to trim them. I have a lot of columns. Each data starting xxx: or yyy: and in a column
for example:</p>

<ol>
<li>xxx:abc yyy:def \n</li>
<li>xxx:def yyy:ghi \n</li>
<li>xxx:ghi yyy:jkl \n</li>
<li>...</li>
</ol>

<p>I need to trim that xxx: and yyy: for each column. Researched and tried some issue solves but they doesn't worked. How can I trim that, I need an effective code. Already thanks.</p>

<p>(Unnecessary chars don't have static length I just know what are them look like stop words. For example: </p>

<ol>
<li>['Comp:Apple', 'Product:iPhone', 'Year:2018', '128GB', ...]</li>
<li>['Comp:Samsung', 'Product:Note', 'Year:2017', '64GB', ...]</li>
</ol>

<p>i want to new dataset look like:</p>

<ol>
<li>['Apple', 'iPhone', '2018', '128GB', ...]</li>
<li>['Samsung', 'Note', '2017', '64GB', ...]</li>
</ol>

<p>So I want to trim ('Comp:', 'Product:', 'Year:', ...) stop words for each column.</p>
","python, pandas, text-mining, strip","<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html"" rel=""nofollow noreferrer""><code>pd.Series.str.split</code></a> for this:</p>

<pre><code>import pandas as pd

df = pd.DataFrame([['Comp:Apple', 'Product:iPhone', 'Year:2018', '128GB'],
                   ['Comp:Samsung', 'Product:Note', 'Year:2017', '64GB']],
                  columns=['Comp', 'Product', 'Year', 'Memory'])

for col in ['Comp', 'Product', 'Year']:
    df[col] = df[col].str.split(':').str.get(1)

#       Comp Product  Year Memory
# 0    Apple  iPhone  2018  128GB
# 1  Samsung    Note  2017   64GB
</code></pre>
",1,1,1664,2018-03-16 23:03:43,https://stackoverflow.com/questions/49330627/trim-each-column-values-at-pandas
Creating a list from list of a list in python,"<p>I am creating a tree-like structure where every leaf node has 5 documents to it. To get the document of parent node all the documents of the child will be assigned to it. </p>

<p>For e.g. A is the parent node and B, C are child nodes both of whom have 5 documents each. So, the documents for A will be 5+5=10. Similarly, the parent of A will get 10 documents of A + no of a document of the sibling of A. We will repeat this until we reach the root node.</p>

<p>I want to store documents of A as a list of size 10 and similarly parent of A as total no of documents of their child. But it is storing it as a list of size 2 and under each list, there are 5 documents each. And the parent of A is also storing documents of A as a list of 3 not <code>3*5=15</code> what I want. </p>

<p>How can I store the document at each node as total no of documents and not a list of lists?
Below is the code which I am using.</p>

<pre><code>from anytree import Node, RenderTree
import pandas as pd
import numpy as np

class Node(Node):
    Node.documents = None
    Node.vector = None

### Creating tree by giving documnets to leaf ###
### Tree Creation ###    
# L1    
Finance = Node(""Finance"")
# L2
Credit_and_Lending = Node(""Credit and Lending"", parent=Finance)
# L3
Credit_Cards = Node(""Credit Cards"", parent=Credit_and_Lending)

Loans = Node(""Loans"", parent=Credit_and_Lending)

# L4
Low_Interest_and_No_Interest_Credit_Cards = Node(""Low Interest &amp; No Interest Credit Cards"", parent=Credit_Cards, documents=[(fvc.loc[(fvc['keyword']=='low interest &amp; no interest credit cards') &amp; (fvc['organic_rank']==1)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='low interest &amp; no interest credit cards') &amp; (fvc['organic_rank']==2)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='low interest &amp; no interest credit cards') &amp; (fvc['organic_rank']==3)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='low interest &amp; no interest credit cards') &amp; (fvc['organic_rank']==4)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='low interest &amp; no interest credit cards') &amp; (fvc['organic_rank']==5)])['vocab'].tolist()[0]])

Rewards_Cards = Node(""Rewards Cards"", parent=Credit_Cards, documents=[(fvc.loc[(fvc['keyword']=='rewards cards') &amp; (fvc['organic_rank']==1)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='rewards cards') &amp; (fvc['organic_rank']==2)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='rewards cards') &amp; (fvc['organic_rank']==3)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='rewards cards') &amp; (fvc['organic_rank']==4)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='rewards cards') &amp; (fvc['organic_rank']==5)])['vocab'].tolist()[0]])

Student_Credit_Cards = Node(""Student Credit Cards"", parent=Credit_Cards, documents=[(fvc.loc[(fvc['keyword']=='student credit cards') &amp; (fvc['organic_rank']==1)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='student credit cards') &amp; (fvc['organic_rank']==2)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='student credit cards') &amp; (fvc['organic_rank']==3)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='student credit cards') &amp; (fvc['organic_rank']==4)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='student credit cards') &amp; (fvc['organic_rank']==5)])['vocab'].tolist()[0]])

Auto_Financing = Node(""Auto Financing"", parent=Loans, documents=[(fvc.loc[(fvc['keyword']=='auto financing') &amp; (fvc['organic_rank']==1)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='auto financing') &amp; (fvc['organic_rank']==2)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='auto financing') &amp; (fvc['organic_rank']==3)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='auto financing') &amp; (fvc['organic_rank']==4)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='auto financing') &amp; (fvc['organic_rank']==5)])['vocab'].tolist()[0]])
Commercial_Lending = Node(""Commercial Lending"", parent=Loans, documents=[(fvc.loc[(fvc['keyword']=='commercial lending') &amp; (fvc['organic_rank']==1)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='commercial lending') &amp; (fvc['organic_rank']==2)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='commercial lending') &amp; (fvc['organic_rank']==3)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='commercial lending') &amp; (fvc['organic_rank']==4)])['vocab'].tolist()[0]
                            , (fvc.loc[(fvc['keyword']=='commercial lending') &amp; (fvc['organic_rank']==5)])['vocab'].tolist()[0]])

##### Visualizing the created tree #####
for pre, fill, node in RenderTree(Finance):
    print(""%s%s"" % (pre, node.name))

##### Getting documents for parent nodes #####
def get_documents(node):    
    if node.documents is not None:
        return node.documents
    else:
        child_nodes = node.children
        lis = []
        for child in child_nodes:
            child_docs = get_documents(child)
            lis.append(child_docs)
        node.documents = lis
        return lis


get_documents(Finance)
</code></pre>
","python, nlp, text-mining","<p>You can use this syntax:</p>

<pre><code>lis = lis + child_docs
</code></pre>

<p>Instead of </p>

<pre><code> lis.append(child_docs)
</code></pre>
",2,2,72,2018-03-20 06:54:32,https://stackoverflow.com/questions/49377971/creating-a-list-from-list-of-a-list-in-python
Remove html tags from a corpus in R,"<p>I am trying to remove the html tag from a corpus (docs) in R:</p>

<pre><code>tags : &lt;/P&gt;&lt;/TEXT&gt;  &lt;/BODY&gt; &lt;TRAILER&gt; NYT-06-22-98 1759EDT &amp;QL; &lt;/TRAILER&gt; &lt;/DOC&gt; 
</code></pre>

<p>The code I am using:</p>

<pre><code>tun&lt;-function(x) gsub(""&lt;TRAILER&gt;,&lt;HTML&gt;,&lt;BODY&gt;,&lt;P&gt;,&lt;TEXT&gt;,&lt;/P&gt;,&lt;/TEXT&gt;,
&lt;/BODY&gt;,&lt;/HTML&gt;"", """", x)
docs &lt;- tm_map(docs, tun)
</code></pre>

<p>But its not able to remove the tags from the corpus , why is that?</p>
","r, text-mining, gsub, information-retrieval","<p>If you want to remove all opening and closing HTML tags, then you may try finding the pattern <code>&lt;/?[^&gt;]+&gt;</code> and replacing with empty string:</p>

<pre><code>x &lt;- ""tags : &lt;/P&gt;&lt;/TEXT&gt;  &lt;/BODY&gt; &lt;TRAILER&gt; NYT-06-22-98 1759EDT &amp;QL; &lt;/TRAILER&gt; &lt;/DOC&gt;""
gsub(""&lt;/?[^&gt;]+&gt;"", """", x)


[1] ""tags :     NYT-06-22-98 1759EDT &amp;QL;  ""
</code></pre>

<p><a href=""http://rextester.com/BCQZ49944"" rel=""nofollow noreferrer""><h2>Demo</h2></a></p>

<p>As a major disclaimer, in general you should <em>not</em> use regex to parse HTML/XML content.  In this particular case, if you just want to strip off all tags, <code>gsub</code> may be a viable option.</p>
",2,1,2555,2018-03-21 09:26:18,https://stackoverflow.com/questions/49402450/remove-html-tags-from-a-corpus-in-r
text mining with R: how to see positive-negative sentiments in my document?,"<p>I am new to R. I have found the number of positive-negative words (953 negative, 458 positive) in my document, but I want to see these words. How can I do it? </p>

<pre><code>library(readr)
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
library(dplyr)
davos &lt;- read_file(""davos.txt"")
fileText &lt;- glue(read_file(davos))
fileText &lt;- gsub(""\\$"", """", fileText)
tokens &lt;- data_frame(text = fileText) %&gt;% unnest_tokens(word, text)
tokens %&gt;% inner_join(get_sentiments(""bing"")) %&gt;% count(sentiment) %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment = positive - negative)

# Joining, by = ""word""
#
# # A tibble: 1 x 3
#     negative positive sentiment
#        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
#   1     953.     458.     -495.
</code></pre>
","r, text-mining, sentiment-analysis","<p>Something like this ?</p>

<pre><code>library(tidy_text)
fileText &lt;- ""Sometimes I'm happy, sometimes I'm sad""
tokens &lt;- data_frame(text = fileText) %&gt;% unnest_tokens(word, text)
tokens %&gt;%
  inner_join(get_sentiments(""bing""))

# # A tibble: 2 x 2
#    word sentiment
#   &lt;chr&gt;     &lt;chr&gt;
# 1 happy  positive
# 2   sad  negative
</code></pre>
",3,0,849,2018-03-24 12:58:28,https://stackoverflow.com/questions/49464958/text-mining-with-r-how-to-see-positive-negative-sentiments-in-my-document
NLTK Python word_tokenize,"<p>I have loaded a txt file that contains 6000 lines of sentences. I have tried to <code>split(""/n"")</code> and <code>word_tokenize</code> the sentences, but I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""final.py"", line 52, in &lt;module&gt;
    short_pos_words = word_tokenize(short_pos)
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/__init__.py"", line 128, in word_tokenize
    sentences = [text] if preserve_line else sent_tokenize(text, language)
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/__init__.py"", line 95, in sent_tokenize
    return tokenizer.tokenize(text)
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1237, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1285, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1276, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1316, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 313, in _pair_iter
    for el in it:
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1291, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1337, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 1472, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 312, in _pair_iter
    prev = next(it)
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 581, in _annotate_first_pass
    for aug_tok in tokens:
  File ""/home/tuanct1997/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.py"", line 546, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6: ordinal not in range(128)
</code></pre>
","python, nltk, text-mining","<p>The issue is related to the encoding of file's content. Assuming that you want to decode <code>str</code> to UTF-8 <code>unicode</code>:</p>

<p><strong>Option 1 (Deprecated in Python 3):</strong>  </p>

<pre><code>import sys
reload(sys)
sys.setdefaultencoding('utf8')
</code></pre>

<p><strong>Option 2:</strong><br>
Pass the <code>encode</code> parameter to the <code>open</code> function when trying to open your text file:</p>

<pre><code>f = open('/path/to/txt/file', 'r+', encoding=""utf-8"")
</code></pre>
",0,0,409,2018-03-25 12:29:17,https://stackoverflow.com/questions/49475847/nltk-python-word-tokenize
Text Categorization Test NLTK python,"<p>I have using nltk packages and train a model using Naive Bayes. I have save the model to a file using pickle package. Now i wonder how can i use this model to test like a random text not in the dataset and the model will tell if the sentence belong to which categorize?</p>

<p>Like my idea is i have a sentence : "" Ronaldo have scored 2 goals against Egypt"" And pass it to the model file and return categorize ""sport"".</p>
","python, nltk, text-mining, naivebayes","<p>Just saving the model will not help. You should also save your VectorModel (like tfidfvectorizer or countvectorizer what ever you have used for fitting the train data). You can save those the same way using pickle. Also save all those models you used for pre-processing the train data like normalization/scaling models, etc. For the test data repeat the same steps by loading the pickle models that you saved and transform the test data in train data format that you used for model building and then you will be able to classify.</p>
",0,0,160,2018-03-26 05:57:47,https://stackoverflow.com/questions/49484820/text-categorization-test-nltk-python
Science paper information extraction with Python?,"<p>I am just new to Python and it just happens to me that I need to extraction some information from a few science papers.</p>

<p>If given something plain text like:</p>

<ol>
<li><strong>Introduction</strong><br>
some long writings</li>
<li><strong>Methodology</strong><br>
some long writings</li>
<li><strong>Results</strong><br>
some long writings</li>
</ol>

<p>How could I put one paper into a dictionary like the follow?</p>

<pre><code>paper_1 = {
           'Introduction': some long writings,
           'Methodology': some long writings,
           'Results': some long writings
          }
</code></pre>

<p>So many thanks :-)</p>

<hr>

<p>After trying, I got some code running but it does not work perfectly:</p>

<pre><code>text = 'introduction This is the FIRST part.' \
       'Methodologies This is the SECOND part.' \
       'results This is the THIRD part.'

import re
from re import finditer

d={}
first =[]
second =[]
title_list=[]
all =[]

for match in finditer(""Methodology|results|methodologies|introduction|"", text, re.IGNORECASE):
    if match.group() is not '':
        title = match.group()
        location = match.span()
        first.append(location[0])
        second.append(location[1])
        title_list.append(title)

all.append(first)
all.append(second)

a=[]
for i in range(2):
    j = i+1
    section = text[all[1][i]:all[0][j]]
    a.append(section)

for i in zip(title_list, a):
    d[i[0]] = i[1]
print (d)
</code></pre>

<p>This would give the following results:</p>

<pre><code>{
'introduction': ' This is the FIRST part.', 
'Methodologies': ' This is the SECOND part.'
}
</code></pre>

<p>However, </p>

<p>i) it is not able to extract the last bit, which is the RESULTS part.</p>

<p>ii). In the loop, I gave the range() function an input of 2 because I know there are only 3 sections(introduction, methodology and results) but in some papers, people would add more sections, how could I automatically assign the correct value to range()? For example some papers may have the following sections:</p>

<ol>
<li><strong>Introduction</strong><br>
some long writings</li>
<li><strong>General background about something</strong><br>
some long writings</li>
<li><strong>some sort of section title</strong><br>
some long writings</li>
<li><strong>Methodology</strong><br>
some long writings</li>
<li><strong>Results</strong><br>
some long writings</li>
</ol>

<p>iii). Is there a more efficient way that I could build the dictionary in each loop? So I do not need to use the second loop.</p>

<hr>

<p>30-03-2018 update:</p>

<p>Code updated as follow:</p>

<pre><code>def section_detection(text):
    title_list=[]
    all =[[],[]]
    dic={}
    count = 0
    pattern = '\d\. [A-Z][a-z]*'

    for match in finditer(pattern, text, re.IGNORECASE):
        if match.group() is not '':
            all[0].append(match.span()[0])
            all[1].append(match.span()[1])
            title_list.append(match.group())
            count += 1

    for i in range(count):
        j = i+1
        try:
            dic[title_list[i]]=text[all[1][i]:all[0][j]]
        except IndexError:
            dic[title_list[i]]=text[all[1][i]:]

    return dic
</code></pre>

<p>if executed as follow:</p>

<pre><code>import re
from re import finditer
text = '1. introduction This is the FIRST part.' \
       '2. Methodologies This is the SECOND part.' \
       '3. results This is the THIRD part.'\
       '4. somesection This SOME section'

dic = section_detection(text)
print(dic)
</code></pre>

<p>Gives:</p>

<pre><code>{'1. introduction': ' This is the FIRST part.', '2. Methodologies': ' This is the SECOND part.', '3. results': ' This is the THIRD part.', '4. somesection': ' This SOME section'}
</code></pre>

<p>Many thanks for all of you! :-)</p>
","python, dictionary, text, web-scraping, text-mining","<p>Really liked the regex written by @Franz Forstmayr. Just want to point out some way to break it though.</p>

<pre><code>text = '''
introduction This is the FIRST part.
introductionMethodologies This is the SECOND part.
results This is the THIRD part.
'''

import re
#### Regex based on https://stackoverflow.com/a/49546458/8083313
kw = ['methodology', 'results', 'methodologies', 'introduction']
pat = re.compile(r'(%s)' % '|'.join(kw), re.IGNORECASE)

sp = [x for x  in re.split(pat, text) if x]
print sp
dic = {k:v for k,v in zip(sp[0::2],sp[1::2])}

print(dic)


# {'\n': 'introduction',
#  'Methodologies': ' This is the SECOND part.\n',
#  ' This is the FIRST part.\n': 'introduction', 
#  'results': ' This is the THIRD part.\n'}
</code></pre>

<p>You can see that the list is shifted due to the \n character and the dictionary is broken. Hence I would suggest to place a hard slicing</p>

<pre><code>out = re.split(pat, text)
lead = out[0:1]; ### Keep the lead available in case needed
sp = out[1:]

print sp
dic = {k:v for k,v in zip(sp[0::2],sp[1::2])}

print(dic)

# {'introduction': '',
#  'Methodologies': ' This is the SECOND part.\n',
#  'results': ' This is the THIRD part.\n'}
</code></pre>
",1,2,1169,2018-03-28 19:47:58,https://stackoverflow.com/questions/49542962/science-paper-information-extraction-with-python
LDA Returning numbers instead of words from Term Document Matrix,"<p>I am trying to use the LDA function to evaluate a corpus of text in R. However, when I do so, it seems to use the row names of the observations rather than the actual words in the corpus. I can't find anything else about this online so I imagine I must be doing something very basic incorrectly.</p>

<pre><code>library(tm)
library(SnowballC)
library(tidytext)
library(stringr)
library(tidyr)
library(topicmodels)
library(dplyr)

#read in data
data &lt;- read.csv('CSV_format_data.csv',sep=',')
#Create corpus/DTM
interviews &lt;- as.matrix(data[,2])
ints.corpus &lt;- Corpus(VectorSource(interviews))
ints.dtm &lt;- TermDocumentMatrix(ints.corpus)

chapters_lda &lt;- LDA(ints.dtm, k = 4, control = list(seed = 5421685))
chapters_lda_td &lt;- tidy(chapters_lda,matrix=""beta"")
chapters_lda_td

head(ints.dtm$dimnames$Terms)
</code></pre>

<p>The 'chapters_lda_td' command outputs</p>

<pre><code># A tibble: 4,084 x 3
   topic term        beta
   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;
 1     1 1     0.000555  
 2     2 1     0.00399   
 3     3 1     0.000614  
 4     4 1     0.000699  
 5     1 2     0.0000195 
 6     2 2     0.000708  
 7     3 2     0.000731  
 8     4 2     0.00000155
 9     1 3     0.000974  
10     2 3     0.0000363 
# ... with 4,074 more rows
</code></pre>

<p>Note that there are numbers instead of words as there should be in the ""term"" column. The number of rows matches the number of documents times the number of topics, rather than the number of terms times the number of topics, as it should be. The 'head(ints.dtm$dimnames$Terms)' is to check that there are actually words in the DTM, which there are. The result is:</p>

<pre><code>[1] ""aaye""      ""able""      ""adjust""    ""admission"" ""after""     ""age"" 
</code></pre>

<p>The data file itself is a pretty standard two-column CSV file with an ID and a block of text, and hasn't given me any problem while doing other text-mining stuff with it and the tm package. Any help would be appreciated, thank you!</p>
","r, text-mining, lda","<p>I figured it out! It is because I am using the command</p>

<pre><code>ints.dtm &lt;- TermDocumentMatrix(ints.corpus)
</code></pre>

<p>rather than</p>

<pre><code>ints.dtm &lt;- DocumentTermMatrix(ints.corpus)
</code></pre>

<p>I guess the ordering of Term and Document switches their dimnames order around, so LDA grabs the wrong one.</p>
",3,1,833,2018-03-28 22:20:11,https://stackoverflow.com/questions/49545100/lda-returning-numbers-instead-of-words-from-term-document-matrix
how to get most common phrases or words in python or R,"<p>Given some text, how can i get the most common n-gram across n=1 to 6?
I've seen methods to get it for 3-gram, or 2-gram, one n at a time, but is there any way to extract the max-length phrase that makes the most sense, and all the rest too?</p>

<p>for example, in this text for demo-purpose only: 
<code>fri evening commute can be long. some people avoid fri evening commute by choosing off-peak hours. there are much less traffic during off-peak.</code></p>

<p>The ideal outcome of n-gram and their counter would be:</p>

<pre><code>fri evening commute: 3,
off-peak: 2,
rest of the words: 1
</code></pre>

<p>any advice appreciated. Thanks.</p>
","python, r, nlp, text-mining","<p>I would advise this if you plan to use R: <a href=""https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-postagging-lemmatisation.html"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-postagging-lemmatisation.html</a></p>
",1,0,3726,2018-03-31 16:27:52,https://stackoverflow.com/questions/49589974/how-to-get-most-common-phrases-or-words-in-python-or-r
Sentence Similarity Estimate with word2vec,"<p>If I have a list of sentences, how can I get a score for the similarity for each pair of sentences using word2vec? Besides, for sentence similarity, is using word2vec better than other methods, such as cosine similarity?</p>
","machine-learning, text-mining","<p>If I remember correctly, word2vec produces vector mappings from words to vectors, but doesn’t propose a vector representation for a larger structure such as a sentence. In order to find the similarity between two sentences you need to go from the vector representations of the words in each sentence to a vector representation of each sentence itself. From there you can use the usual vector distance metric (cosine similarity, computer as the dot product of the normalised vectors) to compare them.</p>

<p>It’s up to you how to combine word vectors into sentence vectors, but a simple way would be to add the word vectors all together.</p>
",1,-1,278,2018-04-01 18:46:14,https://stackoverflow.com/questions/49601472/sentence-similarity-estimate-with-word2vec
AttributeError: module &#39;urllib3&#39; has no attribute &#39;urlretrieve&#39;,"<p>I'm trying the code in <a href=""https://github.com/adventuresinML/adventures-in-ml-code/blob/master/keras_word2vec.py"" rel=""nofollow noreferrer"">this link</a> for doing word2vec by keras.</p>

<p>I receive error on this line:</p>

<pre><code>filename, _ = urllib.urlretrieve(url + filename, filename)
</code></pre>

<p>the error is:</p>

<blockquote>
  <p>AttributeError: module 'urllib' has no attribute 'urlretrieve'</p>
</blockquote>

<p>for solving it I installed and imported urllib3 and change that line to:</p>

<pre><code>filename, _ = urllib3.urlretrieve(url + filename, filename)
</code></pre>

<p>but I receive again with that error:</p>

<blockquote>
  <p>AttributeError: module 'urllib3' has no attribute 'urlretrieve'</p>
</blockquote>

<p>How can I fix it?</p>
","python, urllib, text-mining","<p>Extending from comments section:</p>

<p>As stated by documentation, you can access urlretrieve like this</p>

<pre><code>urllib.request.urlretrieve
</code></pre>

<p><a href=""https://docs.python.org/3.4/library/urllib.request.html#urllib.request.urlretrieve"" rel=""noreferrer"">https://docs.python.org/3.4/library/urllib.request.html#urllib.request.urlretrieve</a></p>
",7,1,12414,2018-04-03 10:50:04,https://stackoverflow.com/questions/49628211/attributeerror-module-urllib3-has-no-attribute-urlretrieve
Find words in a corpus based on lemma,"<p>I am doing text mining with R and I get an ""issue"" I would like to solve... 
In order to find the reports in corpus that contain the most a given word or expression, I use <code>kwic</code>function from <code>quanteda</code>package like this :</p>

<pre><code>result &lt;- kwic (corp2,c(phrase(""trous oblongs"")))
</code></pre>

<p>where <code>corp2</code>is a corpus. <code>trous oblongs</code>is in french and it is a plural. When I do this however, I will only get the reports containing the expression at the plural. I would also like to take into account the occurences of the singular form <code>trou oblong</code>(and vice versa if I initially put in the code <code>trou oblong</code>, get the plural also).</p>

<p>I know that <code>udpipe</code>package, thanks to its <code>udpipe_annotate</code> function :<a href=""https://www.rdocumentation.org/packages/udpipe/versions/0.3/topics/udpipe_annotate"" rel=""nofollow noreferrer"">https://www.rdocumentation.org/packages/udpipe/versions/0.3/topics/udpipe_annotate</a>, is able to extract the lemma of the words in the text.</p>

<p>So I would like to know if <code>udpipe</code> has a function that could manage to find all the occurences of the words having the same lemma in a corpus, or if it possible to do that with <code>kwic</code>.</p>

<p>Thanks in advance</p>
","r, text-mining, quanteda, udpipe","<p>Quanteda has <code>tokens_wordstem()</code> which uses SnoballC's stemmer:</p>

<pre><code>toks &lt;- tokens(corp2)
toks_stem &lt;- tokens_wordstem(toks, ""french"")
kwic(toks_stem, phrase(""trous oblong""))
</code></pre>

<p>Alternatively, you can also use * wildcard to search for stems:</p>

<pre><code>toks &lt;- tokens(corp2)
kwic(toks, phrase(""trou* oblong*""))
</code></pre>
",2,1,766,2018-04-07 12:26:56,https://stackoverflow.com/questions/49707381/find-words-in-a-corpus-based-on-lemma
Combining strings in vector,"<p>first time posting. I hope you forgive any mistake I might make.</p>

<p>I have to get data from USA Today.
After getting all the paragraphs, I'm kind of stuck because I need to put the 17 paragraphs into 1 piece of text so I can export to a .txt.
Help would be much appreciated</p>

<pre><code>USATodayURL &lt;- 'https://www.usatoday.com/story/news/2018/04/05/youtube-shooter-legally-bought-handgun-reloaded-least-once-during-attack-police-say/490969002/'

text_USAToday &lt;- 
  USATodayURL %&gt;% 
  read_html() %&gt;%
  html_nodes('.p-text') %&gt;% 
  html_text()
text_USAToday
</code></pre>
","r, text, text-mining, rvest","<p>Just use <code>paste</code>:</p>

<pre><code>write(paste(text_USAToday, collapse=""\n""), file = ""usa.txt"")
</code></pre>

<p>The argument <code>collapse=""\n""</code> tells <code>paste</code> to insert a line break between the elements. </p>
",1,0,39,2018-04-08 22:39:19,https://stackoverflow.com/questions/49723449/combining-strings-in-vector
How to matching employee&#39;s title in databes to simplify names,"<p>I'm preparing a database analysis from the website:</p>

<pre> https://www.kaggle.com/c/predicting-loan-default/data </pre>

<p>My variable emp_length takes about 3000 different values. Some values are the same or have the same keyword (for example account, accountant, accounting, account specialist, acct.). Some words contain errors or are shortcuts. I want to decrease the values to simplify the names and encode as numeric values. I tried to find keywords with text mining in R, but I'm not convinced that this is the right way. Does anyone have any idea for this? </p>
","python, r, machine-learning, text-mining, data-science","<p>Try to adapt this ""data science"" approach:</p>

<p>Example input data:</p>

<pre><code>emp_length&lt;-c(""account"",""accountant"",""accounting"",""account specialist"",""Data Scientist"",""Data Science Expert"")
</code></pre>

<p>String distance + clustering</p>

<pre><code>cluster&lt;-kmeans(stringdistmatrix(emp_length,emp_length,method=""jw""),centers=2)
cluster_n&lt;-cluster$cluster
</code></pre>

<p>A possible grouping of the labels</p>

<pre><code>cbind(emp_length,cluster_n)
     emp_length            cluster_n
[1,] ""account""             ""2""      
[2,] ""accountant""          ""2""      
[3,] ""accounting""          ""2""      
[4,] ""account specialist""  ""2""      
[5,] ""Data Scientist""      ""1""      
[6,] ""Data Science Expert"" ""1"" 
</code></pre>

<p>This could help in the detection of the label to group and convert in numeric format.</p>
",0,-4,62,2018-04-11 10:32:36,https://stackoverflow.com/questions/49772772/how-to-matching-employees-title-in-databes-to-simplify-names
How to do an approx match and replace with correct word in R?,"<pre><code>list1 &lt;- c(""prmum"",""prum"",""primium"",""prm"",""prim"",""primum"",""prem"",""premum"",
           ""wrng"",""wng"",
           ""hug"",""hung"",
           ""amut"",
           ""chq"",""chquked"",""cheuq"",""chek"",""cheq"",
           ""cus"",""cust"",
           ""cbk"",""cb"",
           ""ringirng"",""rining"",""rigirigi"")


list2 &lt;- c(""premium"",""wrong"",""hang"",""amount"",""cheque"",""customer"",""callback"",""ringing"")
dat &lt;- as.data.frame(list1)
for(i in length(list1)){
t &lt;- agrep(list1[i],list2,value=FALSE)
 dat[t] &lt;- list2[i]
}
</code></pre>

<p>I have two lists one having wrong_words and other correct_words.
I am trying to do the following:</p>

<p>1)Take first word.<br/>
2)Do approx match from list of correct_words and get the index location.<br/>
3)Replace the wrong word with the correct word at that particular location 
in the dataframe or a list.</p>
","r, text-mining","<p>You can do it using <code>stringdistmatrix</code> from package <code>stringdist</code>. It uses Levenshtein distance just like agrep. You find which word has the closest match, and replace it in your original list.</p>

<pre><code> library(stringdist)
 dist_mat &lt;- stringdistmatrix(list1, list2)
 clean_list1 &lt;- list2[apply(dist_mat, 1, which.min)]
 clean_list1
</code></pre>

<p>Now this solution may be inappropriate if you have very long lists (assume they are of length l1 and l2, you will get a matrix of size l1*l2). You may need to go through a loop to reduce memory consumption.</p>

<pre><code>clean_list1 &lt;- list1
for (i in length(list1)){
     dist_vect &lt;- stringdistmatrix(list1[i],list2)
     clean_list1 &lt;- list2[which.min(dist_vect)]
}
</code></pre>
",0,0,59,2018-04-11 12:07:52,https://stackoverflow.com/questions/49774738/how-to-do-an-approx-match-and-replace-with-correct-word-in-r
Text mining and extracting words,"<p>I am trying to extract table names from a SQL statement in R. For example, I would import SQL queries into R, and one row would contain:</p>

<pre><code>SELECT A , B
FROM Table.1 p
JOIN Table.2 pv
ON p.ProdID.1 = ProdID.1
JOIN Table.3 v
ON pv.BusID.1 = v.BusID
WHERE SubID = 15
ORDER BY v.Name;
</code></pre>

<p>In R, I have been trying to use strsplit to the SQL statement that splits every word into a column, creating a data-frame and then finding the match to the word ""from"" and extracting the next word which would be Table.1.</p>

<p>I am having trouble on how to extract the other tables from multiple Joins or if there is a more efficient way or a package I haven't come across during my research. Any help would be much appreciated !</p>
","r, text-mining, text-extraction","<p>Here's one way using regular expressions:</p>

<pre><code>lines &lt;- strsplit(""SELECT A, B
                   FROM Table.1 p
                   JOIN Table.2 pv
                   ON p.ProdID.1 = ProdID.1
                   JOIN Table.3 v
                   ON pv.BusID.1 = v.BusID
                   WHERE SubID = 15
                   ORDER BY v.Name;"", split = ""\\n"")[[1]]

sub("".*(FROM|JOIN) ([^ ]+).*"", ""\\2"", lines[grep(""(FROM|JOIN)"", lines)]) # ""Table.1"" ""Table.2"" ""Table.3""
</code></pre>

<p>Broken down:</p>

<pre><code># Use grep to find the indeces of any line containing 'FROM' or 'JOIN'
keywords_regex &lt;- ""(FROM|JOIN)""
line_indeces &lt;- grep(keywords_regex, lines) # gives: 2 3 5
table_lines &lt;- lines[line_indeces] # get just the lines that have table names

# Build regular expression to capture the next word after either keyword
table_name_regex &lt;- paste0("".*"", keywords_regex, "" ([^ ]+).*"")

# The ""\\2"" means to replace each match with the contents of the second capture 
# group, where a capture group is defined by parentheses in the regex
sub(table_name_regex, ""\\2"", table_lines)
</code></pre>
",0,0,153,2018-04-11 18:57:12,https://stackoverflow.com/questions/49782589/text-mining-and-extracting-words
Imposing a cap on word count in scikit learn,"<p>I'm analyzing song lyrics where repetition doesn't necessarily mean higher importance, so I'd like to cap the word count <em>per document</em>. For example, if a word appears <code>n</code> times in a song, where <code>n &gt; threshold</code>, then I would replace <code>n</code>with <code>threshold</code>.</p>

<p>I've checked the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorizer docs</a>, and there's an option for a <code>min_df</code> and <code>max_df</code>, but these can only disregard words that appear in some m <em>documents</em>, not words that appear n <em>times</em> in a <em>single</em> document.</p>

<p>I was thinking of changing the elements of the sparse matrix (say, find all elements > threshold, then replace), but I couldn't find a way to that either. Thanks in advance!</p>
","python, scikit-learn, nlp, sparse-matrix, text-mining","<p>I don't know of any prebuilt feature in scikit learn for this, but you could definitely edit your doc-term matrix directly, with <a href=""https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.where.html"" rel=""nofollow noreferrer"">numpy.where</a> for example :</p>

<pre><code>x = numpy.where(x &lt; threshold, x, threshold)
</code></pre>

<p>where <code>x</code> is your doc-term matrix and <code>threshold</code> is, well, your threshold.</p>

<p>EDIT :
I hadn't realized numpy.where didn't work on scipy sparse matrices. You can use the <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.find.html"" rel=""nofollow noreferrer""><code>find</code> function</a> from <code>scipy.sparse</code> that will return all non-0 indices in a sparse matrix in order to access and modify those values directly:</p>

<pre><code>from scipy.sparse import find

results = find(x &gt; threshold)
for i in range(len(results[0])):
    x[results[0][i], results[1][i]] = threshold
</code></pre>

<p>It's significantly less elegant but it works.</p>
",3,2,95,2018-04-13 07:27:43,https://stackoverflow.com/questions/49811556/imposing-a-cap-on-word-count-in-scikit-learn
Text mining help in R,"<p>I'm pretty new here, I am looking for help with R as.matrix() in the process of text mining the code will be below.</p>

<p>I am working with the 378661 obs. of 1 variable. </p>

<pre><code>title_txt &lt;- ""H:/Desktop/IST719/titles.txt""
titlefile &lt;- scan(title_txt, character(0), sep = ""\n"")
head(titlefile)

title.vec &lt;- VectorSource(titlefile)
title.corpus &lt;- Corpus(title.vec)
title.corpus

title.corpus &lt;- tm_map(title.corpus, content_transformer(tolower))
title.corpus &lt;- tm_map(title.corpus, removePunctuation)
title.corpus &lt;- tm_map(title.corpus, removeNumbers)
title.corpus &lt;- tm_map(title.corpus, removeWords, stopwords(""english""))

tdm &lt;- TermDocumentMatrix(title.corpus)
tdm

m &lt;- as.matrix(tdm)
</code></pre>

<p>On the console it is coming with the following error: </p>

<p><strong><em>Error: cannot allocate vector of size 466.8 Gb</em></strong></p>

<p>When I am using the as.matrix(). I quite lost and I would greatly appreciate help on this issue. Thank you.</p>

<p><strong>EDIT: macOS High Sierra version 10.13.4 &amp; Memory 16GB</strong> </p>
","r, text-mining","<p>As I note in my answer to <a href=""https://stackoverflow.com/questions/47414401/text-mining-in-r-memory-management/47466115#47466115"">Text Mining in R | memory management</a>, each cell in a matrix in R consumes 8 bytes. Therefore, the size of the matrix (in bytes) is number of documents * number of terms * 8. When one converts a sparse document term matrix to a full matrix, the empty cells in the sparse DTM consume lots of RAM.</p>

<p>Based on the data you've provided in the question, there are approximately 165,459 terms in the DTM you're trying to convert to a matrix.</p>

<pre><code>&gt; sizeInGb &lt;- 466.8
&gt; docs &lt;- 378661
&gt; # calculate number of terms in DTM 
&gt; sizeInGb * (1024 * 1024 * 1024) / (docs * 8)
[1] 165458.9
</code></pre>

<p>Depending on the type of analysis you're trying to do, you'll need to either use tools from the text mining package that work with document term matrices to analyze the data, or aggregate to an object that is smaller than the amount of RAM you have on your machine (minus the RAM consumed by the object(s) used to create it). </p>
",0,0,98,2018-04-14 20:03:30,https://stackoverflow.com/questions/49835677/text-mining-help-in-r
Use R to search for a specific text pattern and return entire sentence(s) where pattern appears,"<p>So I scanned in a physical document, changed it to a tiff image and used the package Tesseract to import it into R. However, I need R to look for specific keywords, find it in the text file and return the entire line that the keyword is in. </p>

<p>For example, if I had the text file:</p>

<blockquote>
  <p>This is also straightforward. Look at the years of experience required and see if that matches the years of experience that the candidate has. It is important to note that if the candidate matches or exceeds the years of experience required, you would rate both of those scenarios a “5”.  </p>
</blockquote>

<p>And I tell R to search for the keyword ""straightforward"", how do I get it to return ""This is also straightforward...see if that matches the""?</p>
","r, text-mining","<p>Here is a solution using the <code>quanteda</code> package that breaks the text into sentences, and then uses <code>grep()</code> to return the sentence containing the word ""straightforward"". </p>

<pre><code>aText &lt;- ""This is also straightforward. Look at the years of experience required and see if that matches the years of experience that the candidate has. It is important to note that if the candidate matches or exceeds the years of experience required, you would rate both of those scenarios a “5”.""
library(quanteda)
aCorpus &lt;- corpus(aText)
theSentences &lt;- tokens(aCorpus,what=""sentence"")
grep(""straightforward"",theSentences,value=TRUE)
</code></pre>

<p>and the output:</p>

<pre><code>&gt; grep(""straightforward"",theSentences,value=TRUE)
                          text1 
""This is also straightforward."" 
</code></pre>

<p>To search for multiple keywords, add them in the <code>grep()</code> function via the or operator | . </p>

<pre><code>grep(""straightforward|exceeds"",theSentences,value=TRUE)
</code></pre>

<p>...and the output:</p>

<pre><code>&gt; grep(""straightforward|exceeds"",theSentences,value=TRUE)

text1 

""This is also straightforward."" 

&lt;NA&gt; 
""It is important to note that if the candidate matches or exceeds the years of experience required, you would rate both of those scenarios a \""5\""."" 
</code></pre>

<blockquote>
  <p></p>
</blockquote>
",0,0,2948,2018-04-17 01:24:27,https://stackoverflow.com/questions/49868453/use-r-to-search-for-a-specific-text-pattern-and-return-entire-sentences-where
R tolower only within function,"<p>I would like to remove words from a character vector. This is how I do:</p>

<pre><code>library(tm)
words = c(""the"", ""The"", ""Intelligent"", ""this"", ""This"")
words_to_remove = c(""the"", ""This"")
removeWords(tolower(words), tolower(words_to_remove))
</code></pre>

<p>This is really nice, but I would like the word ""Intelligent"" to be returned as it was, meaning ""Intelligent"" instead of ""intelligent.
Is there a possibility to use the function <code>tolower</code> only within the function <code>removeWords</code>?</p>
","r, text, text-mining, tolower","<p>You could just use a base R approach with <code>grepl</code> here:</p>

<pre><code>words_to_remove = c(""the"", ""This"")
pattern &lt;- paste0(""\\b"", words_to_remove, ""\\b"", collapse=""|"")
words = c(""the"", ""The"", ""Intelligent"", ""this"", ""This"")

res &lt;- grepl(pattern, words, ignore.case=TRUE)
words[!res]

[1] ""Intelligent""
</code></pre>

<p><a href=""http://rextester.com/DZUV90458"" rel=""nofollow noreferrer""><h2>Demo</h2></a></p>

<p>The trick I use here is in my call to <code>paste</code> to generate the following pattern:</p>

<pre><code>\bthe\b|\bThis\b
</code></pre>

<p>This pattern can, in a single regex evaluation, determine if any string in <code>words</code> is a match to be removed.</p>
",3,1,160,2018-04-17 11:08:40,https://stackoverflow.com/questions/49876503/r-tolower-only-within-function
Remove words from stopword list,"<p>I previously asked a question how to remove words from a stop list in a character vector by keeping the original format. The task was to remove words of ""words_to_remove"" in the vector ""words"".
I accepted this solution:</p>

<pre><code>words_to_remove = c(""the"", ""This"")
pattern &lt;- paste0(""\\b"", words_to_remove, ""\\b"", collapse=""|"")
words = c(""the"", ""The"", ""Intelligent"", ""this"", ""This"")

res &lt;- grepl(pattern, words, ignore.case=TRUE)
words[!res]
</code></pre>

<p>Now I have the problem that I have multiple words in an entry of ""words"". Then the whole entry is deleted if it contains a stop word.</p>

<pre><code>words = c(""the"", ""The Book"", ""Intelligent"", ""this"", ""This"")
</code></pre>

<p>I receive the output</p>

<pre><code>[1] ""Intelligent""
</code></pre>

<p>but I want it to be</p>

<pre><code>[1] ""Book""   ""Intelligent""
</code></pre>

<p>Is this possible?</p>
","r, text, text-mining, stop-words","<p>You can try using <code>gsub</code>, i.e.</p>

<pre><code>v1 &lt;- gsub(paste(words_to_remove, collapse = '|'), '', words, ignore.case = TRUE)

#Tidy up your output

trimws(v1)[v1 != '']
#[1] ""Book""        ""Intelligent""
</code></pre>
",1,1,685,2018-04-18 12:33:57,https://stackoverflow.com/questions/49899838/remove-words-from-stopword-list
Using R to text mine and extract words,"<p>I asked a similar questions before but i still need some help/be pointed into the right direction. </p>

<p>I am trying to locate certain words within a column that consists of a SQL statement on all the rows and extract the next word in R studio. </p>

<p>Example: lets call this dataframe ""SQL</p>

<pre><code>      |    **UserID**    |      **SQL Statement** 

 1   |    N781          |   ""SELECT A, B FROM Table.1 p JOIN Table.2 pv ON 
                            p.ProdID.1ProdID.1 JOIN Table.3 v ON pv.BusID.1 = 
                            v.BusID WHERE SubID = 1 ORDER BY v.Name;""

2      |  N283          |   ""SELECT D, E FROM Table.11 p JOIN Table.2 pv ON 
                           p.ProdID.1ProdID.1 JOIN Table.3 v ON pv.BusID.1 = 
                           v.BusID WHERE SubID = 1 ORDER BY v.Name;""
</code></pre>

<p>So I am trying to pull out the table name. So I am trying to find the words ""From"" and ""Join"" and pulling the next table names. </p>

<p>I have been using some code with help from earlier: </p>

<p>I make the column ""SQL Statement"" in a list of 2 name ""b""</p>

<p>I use the code:</p>

<pre><code>z &lt;- mapply(grepl,""(FROM|JOIN)"",b)
</code></pre>

<p>which gives me a True and fasle for each word in each list.</p>

<pre><code>z &lt;- mapply(grep,""(FROM|JOIN)"",b)
</code></pre>

<p>The above is close. It give me a position of every match in each of the lists.</p>

<p>But I am just trying to find the word Join or From and take the text word out. I was trying to get an output something  like </p>

<pre><code>      |    **UserID**    |      **SQL Statement**                                | Tables 

 1   |    N781          |   ""SELECT A, B FROM Table.1 p JOIN Table.2 pv ON       | Table.1, Table.2          
                            p.ProdID.1ProdID.1 JOIN Table.3 v ON pv.BusID.1 =       
                            v.BusID WHERE SubID = 1 ORDER BY v.Name;""

2      |  N283          |   ""SELECT D, E FROM Table.11 p JOIN Table.2 pv ON 
                           p.ProdID.1ProdID.1 JOIN Table.3 v ON pv.BusID.1 =    | Table.11, Table.31 
                           v.BusID WHERE SubID = 1 ORDER BY v.Name;""
</code></pre>
","r, regex, substring, text-mining","<p>Here is a working script which uses base R options.  The inspiration here is to leverage <code>strsplit</code> to split the query string on the keywords <code>FROM</code> or <code>JOIN</code>.  Then, the first separate word of each resulting term (except for the first term) should be a table name.</p>

<pre><code>sql &lt;- ""SELECT A, B FROM Table.1 p JOIN Table.2 pv ON 
        p.ProdID.1ProdID.1 JOIN Table.3 v ON pv.BusID.1 = 
        v.BusID WHERE SubID = 1 ORDER BY v.Name;""

terms &lt;- strsplit(sql, ""(FROM|JOIN)\\s+"")

out &lt;- unlist(lapply(terms, function(x) gsub(""^([^[:space:]]+).*"", ""\\1"", x)))

out &lt;- out[2:length(out)]
out

[1] ""Table.1"" ""Table.2"" ""Table.3""
</code></pre>

<p><a href=""http://rextester.com/TDRLLG2801"" rel=""nofollow noreferrer""><h2>Demo</h2></a></p>

<p>To understand better what I did, follow the demo and have a look at the <code>terms</code> list which resulted from splitting.</p>

<p><strong>Edit:</strong></p>

<p>Here is a link to another demo which shows how you might use the above logic on a vector of query strings, to generate a list of vector of tables, for each query</p>

<p><a href=""http://rextester.com/CGWY82845"" rel=""nofollow noreferrer""><h2>Demo</h2></a></p>
",1,1,610,2018-04-18 13:29:24,https://stackoverflow.com/questions/49901022/using-r-to-text-mine-and-extract-words
Text mining in R: How to deal with incomplete and non-sense words in a pdf file when creating a frequency table for most frequent terms?,"<p>I'm having some troubles making the table for most frequent words in a pdf file because some words appear as incomplete or look ""strange"". To explain myself better, first the file (in spanish), that can be downloaded from: </p>

<p><a href=""https://drive.google.com/file/d/178s_tfbqbXmnxsknxF8DP154_N1DYjgf/view"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/178s_tfbqbXmnxsknxF8DP154_N1DYjgf/view</a></p>

<p>Second, the code: (Just include your own path and run the code)</p>

<pre><code>library(rJava)
library(tm)
library(qdap)
library(tidyverse)
library(pdftools)   
library(stringr)
library(tidytext)
library(stringi)
library(wordcloud)

stop_es &lt;- c(stopwords(""es""))   #This is the vector I'll be feeding with additional stopwords

cce &lt;- pdf_text(""path/file.pdf"")     #Reading the file
corpus &lt;- Corpus(VectorSource(cce))  #Create corpus

#Cleaning and pre-processing
CCE  &lt;- tm_map(corpus, tolower) %&gt;% 
tm_map(stripWhitespace) %&gt;%
tm_map(removePunctuation) %&gt;% 
tm_map (removeWords, stop_es) %&gt;%
stri_trans_general(""Latin-ASCII"")   #Remove accents for words in spanish

##Create corpus again. (stri_trans_general has a strange behavior that forces me to make again a corpus) 
CCEGTO &lt;- Corpus(VectorSource(CCE))
</code></pre>

<p>After previous steps, I explore for most frequent terms with:</p>

<pre><code>ft &lt;- freq_terms(CCEGTO, 50, stopwords=stop_es) ##Create the table for most frequent terms
ft
</code></pre>

<p>That give us the following output (I remove some words to focus my attention on incomplete or ""strange"" ones)</p>

<pre><code>   WORD        FREQ
2  ca           105  ## No idea about this one
3  guanajuato    94
5  vo            86
6  ufb           75   ##¿¿??
9  va            69
10 propuestas    68
11 nivel         64
12 par           58    #For example this one could stand for ""parte"" or ""participacion""
27 ins           42    #This one could stand for ""instituto"", ""institucion"" or some else related
28 n             42    #No idea why this simple term appears as a frequent term
30 vos           41
33 numero        40
34 vas           40
35 l             39
38 d             37
39 s             37
42 poli          35  #This one could stand for ""policia"", ""politica"", ""politicas""
43 vidad         35  #This one could be a bad output for ""vida"" or maybe for ""actividad""
44 cas           34
45 r             34   #Single character...
46 cipacion      33   #This one could be the complement for ""parti"" in order to form ""participacion""
47 i             33
</code></pre>

<p>Am I missing something on cleaning and pre-processing or maybe is the pdf structure itself that doesn't allow to do a proper text mining job?</p>

<p>Any advice and help will be much appreciated. </p>
","r, pdf, text, text-mining","<p>If I run a bit of adjusted code with less packages loaded I can create a frequency table which looks normal. Checking the outcome of some functions before going to the next is also useful. See <code>writeLines</code> statement to see if everything transforms correctly from the pdf extraction. You might want to use <code>stri_trans_general</code> before creating a corpus instead of in the pipeline of the corpus. But then you need to do this to the stopword list as well. </p>

<p>Depending on what you exactly want to do with Spanish text you might want to look into <code>udpipe</code>. But try to contain your work with as few packages as possible. So most of the work with <code>tm</code> or with any of the other text mining packages like <code>qdap</code>, <code>quanteda</code>, <code>tidytext</code> or <code>udpipe</code>. </p>

<pre><code>library(tm)
library(dplyr)
library(pdftools)   

cce &lt;- pdf_text(""PROPUESTAS GTO 2018 FINAL.pdf"")     #Reading the file

# have a look at page 4 output not printed in answer!
writeLines(stringi::stri_trans_general(cce[4], ""latin-ascii""))

stop_es &lt;- stopwords(""spanish"")
corpus &lt;- Corpus(VectorSource(cce))  #Create corpus

CCE  &lt;- tm_map(corpus, tolower) %&gt;% 
  tm_map(stripWhitespace) %&gt;%
  tm_map(removePunctuation) %&gt;% 
  tm_map (removeWords, stop_es) %&gt;%
  stringi::stri_trans_general(""Latin-ASCII"")


CCEGTO &lt;- Corpus(VectorSource(CCE))

# create frequency table
dtm &lt;- DocumentTermMatrix(CCEGTO)
m &lt;- as.matrix(dtm)
df &lt;- data.frame(words = names(colSums(m)), freq = colSums(m))

# filter frequencies 
df %&gt;% 
  filter(freq &gt; 50) %&gt;% 
  arrange(desc(freq))

        words freq
1        fb01  236
2  desarrollo  107
3  guanajuato   94
4    nacional   90
5    problema   73
6      social   69
7  propuestas   68
8       nivel   64
9         par   58
10 ciudadanos   55
11       pais   55
12       leon   53
13        asi   52
14   gobierno   52
</code></pre>
",2,1,344,2018-04-19 03:54:28,https://stackoverflow.com/questions/49912453/text-mining-in-r-how-to-deal-with-incomplete-and-non-sense-words-in-a-pdf-file
Counting Number of Rows in R data.frame and Storing as Additional Variable,"<p>I have a data frame that returns two column variables - word1 and word2 like this:</p>

<pre><code>head(bigrams_filtered2, 20)
# A tibble: 20 x 2
   word1       word2      
   &lt;chr&gt;       &lt;chr&gt;      
 1 practice    risk       
 2 risk        management 
 3 management  rational   
 4 rational    meansend   
 5 meansend    based      
 6 based       process    
 7 process     risks      
 8 risks       identified 
 9 identified  analysed   
10 analysed    solved     
11 solved      mitigated  
12 objective   involves   
13 involves    human      
14 human       perceptions
15 perceptions biases     
16 opportunity jack       
17 differences stakeholder
18 stakeholder perceptions
19 perceptions broader    
20 broader     risk  
</code></pre>

<p>I am trying to add two additional column variables to this data.frame so that my output looks like this:</p>

<pre><code>##     word1     word2    n totalbigrams           tf
## 1     st     louis 1930      3426965 0.0005631805
## 2  happy  birthday 1802      3426965 0.0005258297
## 3      1         2 1701      3426965 0.0004963576
## 4    los   angeles 1385      3426965 0.0004041477
## 5 social     media 1256      3426965 0.0003665051
## 6    san francisco 1245      3426965 0.0003632952
</code></pre>

<p>I'm following an example from here <a href=""http://www.rpubs.com/pnice421/347328"" rel=""nofollow noreferrer"">http://www.rpubs.com/pnice421/347328</a></p>

<p>Under the heading ""Generating Bigrams"" they provide the following code as a way of achieving this, but I am returning an error:</p>

<pre><code>totalbigrams &lt;- bigrams_filtered2 %&gt;%
    summarize(total=sum(n))

Error in summarise_impl(.data, dots) : 
Evaluation error: invalid 'type' (closure) of argument.
</code></pre>

<p>If anyone has any advice on where I might be going wrong it would be greatly appreciated! Thank you.</p>
","r, dplyr, text-mining, tidytext","<p>First, let's make an example data set that has the same structure as what you are dealing with.</p>



<pre class=""lang-r prettyprint-override""><code>library(tidyverse)
library(tidytext)
library(janeaustenr)


bigram_df &lt;- data_frame(txt = prideprejudice) %&gt;%
    unnest_tokens(bigram, txt, token = ""ngrams"", n = 2) %&gt;%
    separate(bigram, c(""word1"", ""word2""), sep = "" "")

bigram_df

#&gt; # A tibble: 122,203 x 2
#&gt;    word1     word2    
#&gt;    &lt;chr&gt;     &lt;chr&gt;    
#&gt;  1 pride     and      
#&gt;  2 and       prejudice
#&gt;  3 prejudice by       
#&gt;  4 by        jane     
#&gt;  5 jane      austen   
#&gt;  6 austen    chapter  
#&gt;  7 chapter   1        
#&gt;  8 1         it       
#&gt;  9 it        is       
#&gt; 10 is        a        
#&gt; # ... with 122,193 more rows
</code></pre>

<p>Now we can find the number of times each bigram is used using dplyr's <code>count()</code>, the total number of bigrams altogether, and term frequency <code>tf</code>. The key here is to use tidyr's <code>unite()</code> and <code>separate()</code> to stick the columns with the two words together and then break them apart again.</p>



<pre class=""lang-r prettyprint-override""><code>bigram_df %&gt;%
    unite(bigram, word1, word2, sep = "" "") %&gt;%
    count(bigram, sort = TRUE) %&gt;%
    separate(bigram, c(""word1"", ""word2""), sep = "" "") %&gt;% 
    mutate(totalbigrams = sum(n),
           tf = n / totalbigrams)

#&gt; # A tibble: 54,998 x 5
#&gt;    word1 word2     n totalbigrams      tf
#&gt;    &lt;chr&gt; &lt;chr&gt; &lt;int&gt;        &lt;int&gt;   &lt;dbl&gt;
#&gt;  1 of    the     464       122203 0.00380
#&gt;  2 to    be      443       122203 0.00363
#&gt;  3 in    the     382       122203 0.00313
#&gt;  4 i     am      302       122203 0.00247
#&gt;  5 of    her     260       122203 0.00213
#&gt;  6 to    the     252       122203 0.00206
#&gt;  7 it    was     251       122203 0.00205
#&gt;  8 mr    darcy   243       122203 0.00199
#&gt;  9 of    his     234       122203 0.00191
#&gt; 10 she   was     209       122203 0.00171
#&gt; # ... with 54,988 more rows
</code></pre>

<p>Created on 2018-04-22 by the <a href=""http://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v0.2.0).</p>

<p>It sounds like you have done some filtering. You certainly can do that with dplyr's <code>filter()</code> whenever the words are separated out into two columns.</p>
",1,0,1902,2018-04-20 02:22:30,https://stackoverflow.com/questions/49932941/counting-number-of-rows-in-r-data-frame-and-storing-as-additional-variable
Plotting Bigrams in Bar Chart with ggplot2,"<p>My data looks like this:</p>

<pre><code>&gt; str(bigrams_joined)
Classes ‘tbl_df’, ‘tbl’ and 'data.frame':   71319 obs. of  2 variables:
 $ line   : int  1 1 1 1 1 1 1 1 1 1 ...
 $ bigrams: chr  ""in practice"" ""practice risk"" ""risk management"" ""management is""
</code></pre>

<p>I would like to plot the top 10 or 15 most frequently occurring bigrams in my dataset to a bar chart in ggplot2 and have the bars running horizontally with the labels on the y-axis.</p>

<p>Any help with this is greatly appreciated!</p>

<p>Thank you</p>
","r, ggplot2, text-mining, tidytext","<p>Looks like you need to <code>count()</code> your bigrams (from dplyr), and then you need to order them in your plot. For that these days, I prefer to use something like <code>fct_reorder()</code> from forcats.</p>



<pre class=""lang-r prettyprint-override""><code>library(janeaustenr)
library(tidyverse)
library(tidytext)

data_frame(txt = prideprejudice) %&gt;%
    unnest_tokens(bigram, txt, token = ""ngrams"", n = 2) %&gt;%
    count(bigram, sort = TRUE) %&gt;%
    top_n(15) %&gt;%
    ggplot(aes(fct_reorder(bigram, n), n)) +
    geom_col() +
    coord_flip() +
    labs(x = NULL)
#&gt; Selecting by n
</code></pre>

<p><img src=""https://i.sstatic.net/KaY5B.png"" alt=""""></p>

<p>Created on 2018-04-22 by the <a href=""http://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v0.2.0).</p>
",1,1,1398,2018-04-20 11:04:08,https://stackoverflow.com/questions/49940216/plotting-bigrams-in-bar-chart-with-ggplot2
Counting the number of stop words in a text,"<p>I was wondering if anyone could help me with the following problem:
I am trying to determine the number (count) of stop words in customer review texts. I am using the ""quanteda"" package stop words list in R.
I have tokenised the text and filtered out the stop words by using the following code:</p>

<pre><code>stop.words &lt;- tokens_select(corpus2.tokens, stopwords())
</code></pre>

<p>However, I am now having trouble saving these results in such a way that I can count the actual number of stopwords included in each review.</p>

<p>Any tipps would be greatly appreciated. Thanks in advance!</p>
","r, text-mining, stop-words","<p>You can use <code>str_detect</code> from <code>stringr</code> (or <code>stri_detect</code> from <code>stringi</code>) to count the number of stopwords. str_detect will return <code>TRUE</code> or <code>FALSE</code> and these you can just count. Depending on which stopword list you have you can get different results. <code>stopwords(""en"")</code> from <code>stopwords</code> package will return 28. If you use <code>stopwords(source = ""smart"")</code> you will get a count of 61.</p>

<pre><code>text &lt;- ""I've never had a better pulled pork pizza! The amount of toppings that they layered on it was astounding...bacon, corn, more pulled pork, and the sauce was delicious. I shared my pizza with 2 other people. I can't wait to go back.""
stopwords &lt;- stopwords::stopwords(""en"")

sum(stringr::str_detect(tolower(text), stopwords))
28
</code></pre>
",0,1,1827,2018-04-21 11:18:45,https://stackoverflow.com/questions/49955206/counting-the-number-of-stop-words-in-a-text
Text mining error in R : non-numeric argument to binary operator,"<p>I've done google searches, viewed the current Errata for the book, and searched the error in stack overflow and haven't found an answer. I'm following along in the book on pages 4-10.</p>

<p>This part runs fine:</p>

<pre><code> original_books &lt;- austen_books() %&gt;%
 group_by(book) %&gt;%
 mutate(linenumber = row_number(),
     chapter = cumsum(str_detect(text, regex(""^chapter [\\divxlc]"",
                                             ignore_case = TRUE)))) %&gt;%
ungroup()
original_books

tidy_books &lt;- original_books %&gt;%
unnest_tokens(word, text)
tidy_books

data(stop_words)

tidy_books&lt;- tidy_books %&gt;%
  anti_join(stop_words)

tidy_books %&gt;%
  count(word, sort = TRUE)

tidy_books %&gt;%
  count(word, sort= TRUE) %&gt;%
filter(n&gt;600) %&gt;%
mutate(word = reorder(word, n)) %&gt;%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()

hgwells &lt;- gutenberg_download(c(35, 36, 5230, 159))

tidy_hgwells &lt;- hgwells %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words)

tidy_hgwells %&gt;%
count(word, sort=TRUE) 

bronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 767))  

tidy_bronte &lt;- bronte %&gt;%
unnest_tokens(word, text) %&gt;%
anti_join(stop_words)

tidy_bronte %&gt;%
count(word, sort=TRUE)

frequency &lt;- bind_rows(mutate(tidy_bronte, author=""Bronte Sisters""),
                   mutate(tidy_hgwells, author = ""H.G. Wells""),
                   mutate(tidy_books, author = ""Jane Austen"")) %&gt;%
        mutate(word = str_extract(word, ""[a-z']+"")) %&gt;%
        count(author, word) %&gt;%
        group_by(author) %&gt;%
                 mutate(proportion = n / sum(n)) %&gt;%
                 select(-n) %&gt;%
                 spread(author, proportion) %&gt;%
                 gather(author, proportion, 'Bronte Sisters':'H.G. Wells')
   frequency
</code></pre>

<p>But when I run this code:</p>

<pre><code>ggplot(frequency, aes(x=proportion, y='Jane Austen', 
                  color=abs('Jane Austen' - proportion))) +
geom_abline(color=""gray40"", lty=2) +
geom_jitter(alpha=0.1, size=2.5, width=0.3, height=0.3) +
geom_text(aes(label= word), check_overlap=TRUE, vjust=1.5) +
scale_x_log10(labels= percent_format()) +
scale_y_log10(labels= percent_format()) +
scale_color_gradient(limits= c(0, 0.001), 
                   low= ""darkslategray4"", high = ""gray75"") +
facet_wrap(~author, ncol=2) +
theme(legend.position=""none"") +
labs(y=""Jane Austen"", x=NULL) 
</code></pre>

<p>I get this error:  Error in ""Jane Austen"" - proportion : 
  non-numeric argument to binary operator</p>

<p>This is the structure of frequency:</p>

<pre><code>&gt; str(frequency)
Classes ‘tbl_df’, ‘tbl’ and 'data.frame':   57818 obs. of  4 variables:
 $ word       : chr  ""a"" ""a'most"" ""a'n't"" ""aback"" ...
 $ Jane Austen: num  9.19e-06 NA 4.60e-06 NA NA ...
 $ author     : chr  ""Bronte Sisters"" ""Bronte Sisters"" ""Bronte Sisters"" 
               ""Bronte Sisters"" ...
 $ proportion : num  3.19e-05 1.59e-05 NA 3.98e-06 3.98e-06 ...
</code></pre>

<p>Proportion and Jane Austin have numeric values but there are also NAs.  I tried to remove them but it didn't help, plus I figured the book would have brought this up as a potential issue.</p>

<p>These are the libraries I'm using. When I run them I don't see any conflicts that might mask a function:</p>

<pre><code>library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(tidyr)
library(ggplot2)
library(gutenbergr)
library(scales)
</code></pre>

<p>I'm using RStudio Version 1.1.442 on Windows 10. I'm using R 3.4.4</p>

<p>Any ideas on what could be wrong?</p>
","r, ggplot2, dplyr, text-mining, tidyr","<p>Your problem is easily overlooked. You need backticks `` around Jane Austen not quotes. Jane Austen is not a name in this case, but a column name in <code>frequency</code>. And column names with a space need backticks. </p>

<p>It should be:</p>

<pre><code>ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
.....
</code></pre>

<p>not</p>

<pre><code>ggplot(frequency, aes(x=proportion, y='Jane Austen', color = abs('Jane Austen' - proportion))) +
.....
</code></pre>
",3,0,646,2018-04-24 16:06:32,https://stackoverflow.com/questions/50006289/text-mining-error-in-r-non-numeric-argument-to-binary-operator
Parse a text to a data model,"<p>I'm trying to parse a text into a data model i created, specifically monsters from and RPG book. I extract the text from pdf, it looks like this:</p>

<pre><code>Dire Rat
From age to age, dire rats vary in size. In this age, they're only half
as big as they sometimes get, but they're also twice as vicious.
1
st
 level mook [beast ]
Initiative: +2
Infected bite +5 vs. AC-4 ongoing damage
AC 15
PD 15  HP 6 (mook)
MD 10
Mook: Kill one dire rat mook for every 6 damage you deal to the mob.
</code></pre>

<p>And here's my desired data model, happened to write it in C#:</p>

<pre><code>public interface IMonsterData
    {
        string Name { get; }
        int Level { get; set; }
        string Role { get; set; }
        string Type { get; set; }
        int Initiative { get; set; }
        int ArmorClass { get; set; }
        int PhysicalDefense { get; set; }
        int MentalDefense { get; set; }
        int HealthPoints { get; set; }
    }
</code></pre>

<p>The desired result would look like this:</p>

<pre><code>Name = Dire Rat
Level = 1
Role = mook
Type = beast
Initiative = 2
ArmourClass = 15
PhysicalDefense = 15
MentalDefense = 10
HealthPoints = 6
</code></pre>

<p>The source text has these monsters spread out in a section of a book, so the majority of pages have no monsters, then comes a section whith monsters. Some fit on a page, some are split, but i don't think that matters since i've parsed out all text out of the book as a string using a tool i found.</p>

<p>I've approached this problem with manual parsing using C# and .Net, byt finding rows that only have ""st, nd, rd, th"" and finding the AC, PD in between such rows, but i have a feeling that this is a good case to solve with Text Mining or some other form of Machine Learning.</p>

<p>Problem is i don't know anything about machine learning or the correct algorithm from text mining to use on this, and i can only get so far googling around without knowing any keywords. </p>

<p>What sort of library, algorithm or similar should i use to achieve my goal?</p>
","algorithm, parsing, machine-learning, text-mining","<p>This looks like a case to be solved by good-old regex.
I'd approach this as follows:</p>

<ul>
<li>Filter out the useless records(the ones that dont contain the relevant fields) from whatever you've been able to scrape so far.</li>
<li>flatten each of those relevant records into a line so that it is easy
to parse with a regex query. </li>
<li>Try a regex query builder such as
<a href=""https://regex101.com/"" rel=""nofollow noreferrer"">this</a> to extract the fields and place them into your model.</li>
</ul>
",2,1,271,2018-05-02 20:02:41,https://stackoverflow.com/questions/50142664/parse-a-text-to-a-data-model
How to find matching words in a DF from list of words and returning the matched words in new column,"<p>I have a DF with 2 columns and I have a list of words. </p>

<pre><code>list_of_words &lt;- c(""tiger"",""elephant"",""rabbit"", ""hen"", ""dog"", ""Lion"", ""camel"", ""horse"")

df &lt;- tibble::tibble(page=c(12,6,9,18,2,15,81,65),
               text=c(""I have two pets: a dog and a hen"",
                      ""lion and Tiger are dangerous animals"",
                      ""I have tried to ride a horse"",
                      ""Why elephants are so big in size"",
                      ""dogs are very loyal pets"",
                      ""I saw a tiger in the zoo"",
                      ""the lion was eating a buffalo"",
                      ""parrot and crow are very clever birds""))

animals &lt;- c(""dog,hen"", ""lion,tiger"", ""horse"", FALSE, ""dog"", ""tiger"", ""lion"", FALSE)

cbind(df, animals)
#&gt;   page                                  text    animals
#&gt; 1   12      I have two pets: a dog and a hen    dog,hen
#&gt; 2    6  lion and Tiger are dangerous animals lion,tiger
#&gt; 3    9          I have tried to ride a horse      horse
#&gt; 4   18      Why elephants are so big in size      FALSE
#&gt; 5    2              dogs are very loyal pets        dog
#&gt; 6   15              I saw a tiger in the zoo      tiger
#&gt; 7   81         the lion was eating a buffalo       lion
#&gt; 8   65 parrot and crow are very clever birds      FALSE
</code></pre>

<p>I need to find out if any of the words from list are present in one of the column of the DF or not. If yes, then return the word/words to a new column in the DF. This is the list of words ->(tiger,elephant,rabbit, hen, dog, Lion, camel, horse).
<a href=""https://i.sstatic.net/CIFWd.jpg"" rel=""nofollow noreferrer"">This is how my DF Looks like</a>
<a href=""https://i.sstatic.net/nUp9S.jpg"" rel=""nofollow noreferrer"">I want something like this</a></p>
","r, text-mining","<pre class=""lang-r prettyprint-override""><code>library(dplyr)

df %&gt;% 
  rowwise() %&gt;%
  mutate(animals = paste(list_of_words[unlist(
    lapply(list_of_words, function(x) grepl(x, text, ignore.case = T)))], collapse=&quot;,&quot;)) %&gt;%
  data.frame()
</code></pre>
<p><strong>Output is:</strong></p>
<pre><code>  page                                  text    animals
1   12                       pets: dog &amp; hen    hen,dog
2    6 Lions and tigers are dangerous animal tiger,Lion
3    9          I have tried to ride a horse      horse
4   65   parrot &amp; crow are very clever birds           
</code></pre>
<p><strong>Sample data:</strong></p>
<pre><code>df &lt;- structure(list(page = c(12, 6, 9, 65), text = structure(c(4L, 
2L, 1L, 3L), .Label = c(&quot;I have tried to ride a horse&quot;, &quot;Lions and tigers are dangerous animal&quot;, 
&quot;parrot &amp; crow are very clever birds&quot;, &quot;pets: dog &amp; hen&quot;), class = &quot;factor&quot;)), .Names = c(&quot;page&quot;, 
&quot;text&quot;), row.names = c(NA, -4L), class = &quot;data.frame&quot;)

list_of_words &lt;- c(&quot;tiger&quot;, &quot;elephant&quot;, &quot;rabbit&quot;, &quot;hen&quot;, &quot;dog&quot;, &quot;Lion&quot;, &quot;camel&quot;, &quot;horse&quot;)
</code></pre>
<br>
**Another approach:**
<pre><code>library(data.table)
setDT(df)[, animals := paste(list_of_words[unlist(lapply(list_of_words, function(x) grepl(x, text, ignore.case = T)))], collapse = &quot;,&quot;), by = 1:nrow(df)]

#&gt; df
#   page                                  text    animals
#1:   12                       pets: dog &amp; hen    hen,dog
#2:    6 Lions and tigers are dangerous animal tiger,Lion
#3:    9          I have tried to ride a horse      horse
#4:   65   parrot &amp; crow are very clever birds           
</code></pre>
",0,-1,1264,2018-05-03 08:52:04,https://stackoverflow.com/questions/50150811/how-to-find-matching-words-in-a-df-from-list-of-words-and-returning-the-matched
Pyspark CountVectorizer and Word Frequency in a corpus,"<p>I'm currently studying a text corpus. <br/>Let's say that I cleaned my verbatims and that I have the following pyspark DataFrame :</p>



<pre><code>df = spark.createDataFrame([(0, [""a"", ""b"", ""c""]),
                            (1, [""a"", ""b"", ""b"", ""c"", ""a""])],
                            [""label"", ""raw""])
df.show()

+-----+---------------+
|label|            raw|
+-----+---------------+
|    0|      [a, b, c]|
|    1|[a, b, b, c, a]|
+-----+---------------+
</code></pre>

<p>I now want to implement a CountVectorizer. So, I used <code>pyspark.ml.feature.CountVectorizer</code> as follows :</p>



<pre><code>cv = CountVectorizer(inputCol=""raw"", outputCol=""vectors"")
model = cv.fit(df)
model.transform(df).show(truncate=False)

+-----+---------------+-------------------------+
|label|raw            |vectors                  |
+-----+---------------+-------------------------+
|0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|
|1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|
+-----+---------------+-------------------------+
</code></pre>

<p>Now, I would also like to get the vocabulary that the CountVectorizer selected, <strong>as well as</strong> the corresponding word frequencies in the corpus. <br/> Using <code>cvmodel.vocabulary</code> only provides the vocabulary list :</p>



<pre><code>voc = cvmodel.vocabulary
voc
[u'b', u'a', u'c']
</code></pre>

<p>I would like to get something like that :</p>



<pre><code>voc = {u'a':3,u'b':3,u'c':2}
</code></pre>

<p>Would you have any idea to do such a things?</p>

<p><strong>Edit</strong>:
<br/> I am using Spark 2.1</p>
","python, pyspark, text-mining","

<p>Calling <code>cv.fit()</code> returns a <code>CountVectorizerModel</code>, which (AFAIK) stores the vocabulary but it does not store the counts. The vocabulary is property of the model (it needs to know what words to count), but the counts are a property of the DataFrame (not the model). You can apply the transform function of the fitted model to get the counts for any DataFrame.</p>

<p>That being said, here are two ways to get the output you desire. </p>

<p><strong>1. Using Existing Count Vectorizer Model</strong></p>

<p>You can use <a href=""https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.explode"" rel=""noreferrer""><code>pyspark.sql.functions.explode()</code></a> and <a href=""https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.collect_list"" rel=""noreferrer""><code>pyspark.sql.functions.collect_list()</code></a> to gather the entire corpus into a single row. For illustrative purposes, let's consider a new DataFrame <code>df2</code> which contains some words unseen by the fitted <code>CountVectorizer</code>:</p>

<pre class=""lang-python prettyprint-override""><code>import pyspark.sql.functions as f

df2 = sqlCtx.createDataFrame([(0, [""a"", ""b"", ""c"", ""x"", ""y""]),
                            (1, [""a"", ""b"", ""b"", ""c"", ""a""])],
                            [""label"", ""raw""])

combined_df = (
    df2.select(f.explode('raw').alias('col'))
      .select(f.collect_list('col').alias('raw'))
)
combined_df.show(truncate=False)
#+------------------------------+
#|raw                           |
#+------------------------------+
#|[a, b, c, x, y, a, b, b, c, a]|
#+------------------------------+
</code></pre>

<p>Then use the fitted model to transform this into counts and collect the results:</p>

<pre class=""lang-python prettyprint-override""><code>counts = model.transform(combined_df).select('vectors').collect()
print(counts)
#[Row(vectors=SparseVector(3, {0: 3.0, 1: 3.0, 2: 2.0}))]
</code></pre>

<p>Next <code>zip</code> the counts and the vocabulary together and use the <code>dict</code> constructor to get the desired output:</p>

<pre class=""lang-python prettyprint-override""><code>print(dict(zip(model.vocabulary, counts[0]['vectors'].values)))
#{u'a': 3.0, u'b': 3.0, u'c': 2.0}
</code></pre>

<p>As you correctly pointed out <a href=""https://stackoverflow.com/questions/50255356/pyspark-countvectorizer-and-word-frequency-in-a-corpus/50259673#comment87598686_50259673"">in the comments</a>, this will only consider the words that are part of the <code>CountVectorizerModel</code>'s vocabulary. Any other words will be ignored. Hence we don't see any entries for <code>""x""</code> or <code>""y""</code>.</p>

<p><strong>2. Use DataFrame aggregate functions</strong></p>

<p>Or you can skip the <code>CountVectorizer</code> and get your output using a <code>groupBy()</code>. This is a more generic solution in that it will give the counts for <em>all</em> words in the DataFrame, not just those in the vocabulary:</p>

<pre class=""lang-python prettyprint-override""><code>counts = df2.select(f.explode('raw').alias('col')).groupBy('col').count().collect()
print(counts)
#[Row(col=u'x', count=1), Row(col=u'y', count=1), Row(col=u'c', count=2), 
# Row(col=u'b', count=3), Row(col=u'a', count=3)]
</code></pre>

<p>Now simply use a <code>dict</code> comprehension:</p>

<pre class=""lang-python prettyprint-override""><code>print({row['col']: row['count'] for row in counts})
#{u'a': 3, u'b': 3, u'c': 2, u'x': 1, u'y': 1}
</code></pre>

<p>Here we have the counts for <code>""x""</code> and <code>""y""</code> as well.</p>
",7,7,12039,2018-05-09 14:01:39,https://stackoverflow.com/questions/50255356/pyspark-countvectorizer-and-word-frequency-in-a-corpus
Using grep function for text mining,"<p>I have problem while scoring my data. Below is the data set. <em>text</em> are the tweets from where I want to do text mining and sentiment analysis</p>

<pre><code>**text**                                         **call    bills    location**
-the bill was not generated                           0        bill       0
-tried to raise the complaint                         0         0         0 
-the location update failed                           0         0       location
-the call drop has increased in my location         call        0       location
-nobody in the location received bill,so call ASAP  call      bill      location
</code></pre>

<p>THIS IS THE DUMMY DATA, where Text is the column from where I am trying to do text mining, I have used grep function in R to create columns(e.g. bills, calls, location) and if bills is there in any row, under the column name write bill and likewise for all the other categories.</p>

<pre><code>vdftweet$app = ifelse(grepl('app',tolower(vdftweet$text)),'app',0)
table(vdftweet$app)
</code></pre>

<p>Now, the problem which I am not able to understand is</p>

<p>I want to create a new column ""category_name"", under which each row should give the name of the category they fall into. if there are more than 3 category for each tweet mark it as 'other'. Else give the names of category.</p>
","r, twitter, text-mining","<p>There are a couple of ways you could do this using the <code>tidyverse</code> package. In the first method, <code>mutate</code> is used to add the category names as columns to the text data.frame similar to what you have. <code>gather</code> is then used to transform that to key-value format in which the categories are values in the <code>category_name</code> column.  </p>

<p>The Alternative approach is to go directly to the key-value format in which categories are values in the <code>category_name</code> column. Rows are repeated if they fall into multiple categories.  If you don't need the first form with the categories as column names, the Alternative approach is more flexible for adding new categories and requires less processing.  </p>

<p>In both methods, <code>str_match</code> contains the regular expression matching the category to the text. The pattern here is trivial but a more complex pattern could be used if needed. </p>

<p>The code follows:</p>

<pre><code>library(tidyverse)
#
# read dummy data into data frame
#
   dummy_dat &lt;- read.table(header = TRUE,stringsAsFactors = FALSE, 
                      strip.white=TRUE, sep=""\n"",
          text= ""text
            -the bill was not generated
          -tried to raise the complaint
          -the location update failed
          -the call drop has increased in my location
          -nobody in the location received bill,so call ASAP"")
#
#  form data frame with categories as columns
#
   dummy_cats &lt;-  dummy_dat %&gt;% mutate(text = tolower(text),
                               bill = str_match(.$text, pattern=""bill""), 
                               call = str_match(.$text,  pattern=""call""), 
                               location = str_match(.$text, pattern=""location""),
                               other = ifelse(is.na(bill) &amp; is.na(call) &amp;
                                              is.na(location), ""other"",NA))
#
#  convert categories as columns to key-value format
#  withcategories as values in category_name column
#

   dummy_cat_name &lt;- dummy_cats %&gt;% 
               gather(key = type, value=category_name, -text,na.rm = TRUE) %&gt;%
               select(-type) 

#
#---------------------------------------------------------------------------
#
#  ALTERNATIVE:  go directly from text data to key-value format with categories
#  as values under category_name
#  Rows are repeated if they fall into multiple categories
#  Rows with no categories are put in category other
#
   dummy_dat &lt;- dummy_dat %&gt;% mutate(text=tolower(text))
   dummy_cat_name1 &lt;- data.frame(text = NULL, category_name =NULL)
   for( cat in c(""bill"", ""call"", ""location"")) {
      temp &lt;-  dummy_dat %&gt;% mutate(category_name = str_match(.$text, pattern=cat)) %&gt;% na.omit() 
      dummy_cat_name1 &lt;- dummy_cat_name1 %&gt;% bind_rows(temp) 
    }
    dummy_cat_name1 &lt;- left_join(dummy_dat, dummy_cat_name1, by = ""text"") %&gt;%
               mutate(category_name = ifelse(is.na(category_name), ""other"", category_name))
</code></pre>

<p>The result is </p>

<pre><code> dummy_cat_name1
                                            text      category_name
                            -the bill was not generated          bill
                          -tried to raise the complaint         other
                            -the location update failed      location
            -the call drop has increased in my location          call
            -the call drop has increased in my location      location
     -nobody in the location received bill,so call asap          bill
     -nobody in the location received bill,so call asap          call
     -nobody in the location received bill,so call asap      location
</code></pre>
",1,3,329,2018-05-10 05:37:06,https://stackoverflow.com/questions/50266156/using-grep-function-for-text-mining
Regular expression not working in R but works on website. Text mining,"<p>I have a regex which works on the regular expression website but doesn't work when I copy it in R. Below is the code to recreate my data frame:</p>

<pre><code>text &lt;- data.frame(page = c(1,1,2,3), sen = c(1,2,1,1),
                   text = c(""Dear Mr case 1"",
                            ""the value of my property is £500,000.00 and it was built in 1980"", 
                            ""The protected percentage is 0% for 2 years"",
                            ""The interest rate is fixed for 2 years at 4.8%""))
</code></pre>

<p>regex working on website: <a href=""https://regex101.com/r/OcVN5r/2"" rel=""nofollow noreferrer"">https://regex101.com/r/OcVN5r/2</a> </p>

<p>Below is the R codes I have tried so far and neither works. </p>

<pre><code>library(stringr)
patt = ""dear\\s+(mr|mrs|miss|ms)\\b[^£]+(£[\\d,.]+)(?:\\D|\\d(?![\\d.]*%))+([\\d.]+%)(?:\\D|\\d(?![\\d.]*%))+([\\d.]+%)""
str_extract(text, patt)
grepl(pattern = patt, x = text)
</code></pre>

<p>I'm getting an error saying the regex is wrong but it works on the website. Not sure how to get it to work in r. 
Basically I am trying to extract pieces of information from the text. Below are the details:
From the above dataframe, I need to extract the following:</p>

<p>1: Gender of the person. In this case it would be Male (looking at <code>Mr</code>)</p>

<p>2: The number that represents the property value. in this case would be <code>£500,000.00</code>.</p>

<p>3: The protected percentage value, which in our case would be <code>0%</code>.</p>

<p>4: The interest rate value and in our case it is <code>4.8%</code>.</p>
","r, regex, text-mining","<p>I think the issue is your regex isn't giving alternate or ""OR"" matches. See below based on your bullet list</p>

<pre><code>library(stringi)
rgx &lt;- ""(?&lt;=dear\\s?)(m(r(s)?|s|iss))|\\p{S}([0-9]\\S+)|([0-9]+)((\\.[0-9]{1,})?)\\%""
stri_extract_all_regex(
   text$text, rgx, opts_regex = stri_opts_regex(case_insensitive = T)
) %&gt;% unlist()
</code></pre>

<p>Which gives   </p>

<pre><code>[1] ""Mr""          ""£500,000.00""      ""0%""          ""4.8%"" 
</code></pre>

<p>The pattern says:  </p>

<ul>
<li><code>""(?&lt;=dear\\s?)(m(r(s)?|s|iss))""</code> = <strong><em>find a match where the word dear appears before a mr, ms, mrs or miss... but don't capture the dear or the leading space</em></strong></li>
<li><code>|</code> = <strong><em>OR</em></strong></li>
<li><code>""\\p{S}([0-9]\\S+)""</code> = <strong><em>find a match where a sequence of numbers occurs, after a symbol (see ?stringi-search-charclass), until there is a white space. But It must have a symbol at the beginning</em></strong></li>
<li><code>|</code> = <strong><em>OR</em></strong></li>
<li><code>""([0-9]+)((\\.[0-9]{1,})?)\\%""</code> = <strong><em>find a match where a number occurs one or more times, that may have a decimal with numbers after it, but will end in a percent sign</em></strong></li>
</ul>
",1,0,616,2018-05-10 14:51:56,https://stackoverflow.com/questions/50275752/regular-expression-not-working-in-r-but-works-on-website-text-mining
Extracting the POS tags in R using,"<p>In my dataset I am trying to create variables containing the number of nouns, verbs and adjectives, respectively for each observation. Using the openNLP package I have managed to get this far:</p>

<pre><code>s &lt;- paste(c(""Pierre Vinken, 61 years old, will join the board as a "",
             ""nonexecutive director Nov. 29.\n"",
             ""Mr. Vinken is chairman of Elsevier N.V., "",
             ""the Dutch publishing group.""),
           collapse = """")
s &lt;- as.String(s)
s

sent_token_annotator &lt;- Maxent_Sent_Token_Annotator()
word_token_annotator &lt;- Maxent_Word_Token_Annotator()
a2 &lt;- annotate(s, list(sent_token_annotator, word_token_annotator))
pos_tag_annotator &lt;- Maxent_POS_Tag_Annotator()
pos_tag_annotator
a3 &lt;- annotate(s, pos_tag_annotator, a2)
a3
a3w &lt;- subset(a3, type == ""word"")
a3w
</code></pre>

<p>This gives me the output:</p>

<pre><code>id type     start end features
1 sentence     1  84 constituents=&lt;&lt;integer,18&gt;&gt;
2 sentence    86 153 constituents=&lt;&lt;integer,13&gt;&gt;
3 word         1   6 POS=NNP
4 word         8  13 POS=NNP
5 word        14  14 POS=,
</code></pre>

<p>And so on.</p>

<p>My question is, how do I extract for example the number of nouns per observation so I can use this for further analysis.</p>

<p>Thanks!</p>
","r, text-mining, pos-tagger","<p>I don't use <code>openNLP</code>, but use different packages for POS tagging. If someone has an answer for <code>openNLP</code> that can help you that would be great.</p>

<p>But I will give you a solution using <code>udpipe</code>. You might find it useful.</p>

<pre><code>s &lt;- paste(c(""Pierre Vinken, 61 years old, will join the board as a "",
             ""nonexecutive director Nov. 29.\n"",
             ""Mr. Vinken is chairman of Elsevier N.V., "",
             ""the Dutch publishing group.""),
           collapse = """")

library(udpipe)

if (file.exists(""english-ud-2.0-170801.udpipe"")) 
  ud_model &lt;- udpipe_load_model(file = ""english-ud-2.0-170801.udpipe"") else {
    ud_model &lt;- udpipe_download_model(language = ""english"")
    ud_model &lt;- udpipe_load_model(ud_model$file_model)
}

x &lt;- udpipe_annotate(ud_model, s)
x &lt;- as.data.frame(x)
table(x$upos)

  ADJ   ADP   AUX   DET  NOUN   NUM PROPN PUNCT  VERB 
    2     2     2     3     6     2     8     5     1 
</code></pre>

<p>edit: counts per sentence:</p>

<pre><code>table(x$sentence_id, x$upos)
    ADJ ADP AUX DET NOUN NUM PROPN PUNCT VERB
  1   2   1   1   2    3   2     3     3    1
  2   0   1   1   1    3   0     5     2    0
</code></pre>

<p>When you create a data.frame from x after the annotations, you have access to doc_id, paragraph_id, sentence_id, etc etc. You can create a whole range of statistics per document / sentence etc. The vignettes give a good overview of what is possible.</p>
",0,0,1061,2018-05-14 08:04:03,https://stackoverflow.com/questions/50325625/extracting-the-pos-tags-in-r-using
How to generate Word2vec Vectors in Python?,"<p>I am trying to generate Word2vec vectors.</p>

<p>I have pandas data frame.</p>

<p>I transformed it into tokens.</p>

<p><code>df[""token""]</code></p>

<p>Used Word2vec from gensim.models</p>

<pre><code>model = w2v.Word2Vec(
sentences=df[""token""],
seed=seed,
workers=num_workers,
size=num_features,
min_count=min_word_count,
window=context_size,
sample=downsampling
)
</code></pre>

<p>How do I transform my dataframe df now?</p>

<p>That is what is the equivalent of doing </p>

<pre><code>model.transform(df)
</code></pre>
","python, neural-network, nlp, text-mining, word2vec","<p>If your dataframe is composed only of words, you could just make</p>

<pre><code>df['new_column'] = model[df['words']]
</code></pre>

<p><code>model['word']</code> or <code>model[list()]</code> both give you the vector representation of your <code>word</code> or of your <code>list</code></p>
",1,2,3467,2018-05-16 14:11:43,https://stackoverflow.com/questions/50373248/how-to-generate-word2vec-vectors-in-python
Creating a data frame in R with the content of multiple text files,"<p>I'm new to R programming and wondering how I can take the contents of 1,172 text files and create a data frame with the contents of each text file in individual rows in the data frame.</p>

<p>So I want to go from having 1,172 text documents to having a data frame with 1,172 rows and 1 column, with each row having the contents of each individual text file. So the fifth row of the data frame would include the text from the fifth text document in the list I feed into R.</p>

<p>Thanks,</p>

<p>Tyler</p>
","r, text-mining","<pre><code># get all  files with extension ""txt"" in the current directory
file.list &lt;- list.files(path = ""."", pattern=""*.txt"", full.names=TRUE)

# this creates a vector where each element contains one file
all.files &lt;- sapply(file.list, FUN = function(x)readChar(x, file.info(x)$size))

# create a dataframe
df &lt;- data.frame( files= all.files, stringsAsFactors=FALSE)
</code></pre>

<p>The last 2 steps could be united into one to avoid creating an extra vector:</p>

<pre><code>df &lt;- data.frame( files= sapply(file.list, 
                                FUN = function(x)readChar(x, file.info(x)$size)),
                  stringsAsFactors=FALSE)
</code></pre>
",3,0,3452,2018-05-18 19:54:58,https://stackoverflow.com/questions/50418420/creating-a-data-frame-in-r-with-the-content-of-multiple-text-files
How to print JSON objects in AWK,"<p>I was looking for some built-in functions inside awk to easily generate JSON objects. I came across several answers and decided to create my own.</p>

<p>I'd like to generate JSON from multidimensional arrays, where I store table style data, and to use separate and dynamic definition of JSON schema to be generated from that data.</p>

<p>Desired output:</p>

<pre><code>{
""Name"": JanA
""Surname"": NowakA
""ID"": 1234A
""Role"": PrezesA
}
{
""Name"": JanD
""Surname"": NowakD
""ID"": 12341D
""Role"": PrezesD
}
{
""Name"": JanC
""Surname"": NowakC
""ID"": 12342C
""Role"": PrezesC
}
</code></pre>

<p>Input file:</p>

<pre><code>pierwsza linia
druga linia
trzecia linia

dane wspólników
imie JanA
nazwisko NowakA
pesel 11111111111A
funkcja PrezesA

imie Ja""nD
nazwisko NowakD
pesel 11111111111
funkcja PrezesD

imie JanC
nazwisko NowakC
pesel 12342C
funkcja PrezesC

czwarta linia

reprezentanci

imie Tomek
</code></pre>

<p>Based on input file i created a multidimensional array:</p>

<pre><code>JanA  NowaA 1234A PrezesA
JanD  NowakD 12341D PrezesD
JanC  NowakC 12342C PrezesC
</code></pre>
","json, awk, text-mining","<p>My updated awk implementation of simple array printer with regex based validation for each column(running using gawk):</p>

<pre><code>function ltrim(s) { sub(/^[ \t]+/, """", s); return s }
function rtrim(s) { sub(/[ \t]+$/, """", s); return s }
function sTrim(s){
    return rtrim(ltrim(s));
}

function jsonEscape(jsValue) {
    gsub(/\\/, ""\\\\"", jsValue)
    gsub(/""/,  ""\\\"""", jsValue)
    gsub(/\b/, ""\\b"",  jsValue)
    gsub(/\f/, ""\\f"",  jsValue)
    gsub(/\n/, ""\\n"",  jsValue)
    gsub(/\r/, ""\\r"",  jsValue)
    gsub(/\t/, ""\\t"",  jsValue)

    return jsValue
}

function jsonStringEscapeAndWrap(jsValue) {
    return ""\42"" jsonEscape(jsValue) ""\42""
}

function jsonPrint(contentArray, contentRowsCount, schemaArray){    
    result = """"
    schemaLength = length(schemaArray)
    for (x = 1; x &lt;= contentRowsCount; x++) {
        result = result ""{""
        for(y = 1; y &lt;= schemaLength; y++){

            result = result ""\42"" sTrim(schemaArray[y]) ""\42:"" sTrim(contentArray[x, y])

             if(y &lt; schemaLength){
                result = result "",""
            }

        }        
        result = result ""}""
        if(x &lt; contentRowsCount){
            result = result "",\n""
        }
    }
    return result
}

function jsonValidateAndPrint(contentArray, contentRowsCount, schemaArray, schemaColumnsCount, errorArray){  
    result = """"
    errorsCount = 1
    for (x = 1; x &lt;= contentRowsCount; x++) {
        jsonRow = ""{""
        for(y = 1; y &lt;= schemaColumnsCount; y++){

            regexValue = schemaArray[y, 2]
            jsonValue = sTrim(contentArray[x, y])
            isValid = jsonValue ~ regexValue            

            if(isValid == 0){
                errorArray[errorsCount, 1] = ""\42"" sTrim(schemaArray[y, 1]) ""\42""
                errorArray[errorsCount, 2] = ""\42Value "" jsonValue "" not match format: "" regexValue "" \42""
                errorArray[errorsCount, 3] = x
                errorsCount++
                jsonValue = ""null""
            }            

            jsonRow = jsonRow ""\42"" sTrim(schemaArray[y, 1]) ""\42:"" jsonValue

            if(y &lt; schemaColumnsCount){
                jsonRow =  jsonRow "",""
            }
        }        
        jsonRow = jsonRow ""}""
        result = result jsonRow

        if(x &lt; contentRowsCount){
            result = result "",\n""
        }
    }

    return result
}

BEGIN{  
    rowsCount =1
    matchCount = 0
    errorsCount = 0
    shareholdersJsonSchema[1, 1] = ""Imie""
    shareholdersJsonSchema[2, 1] = ""Nazwisko""
    shareholdersJsonSchema[3, 1] = ""PESEL""
    shareholdersJsonSchema[4, 1] = ""Funkcja""

    shareholdersJsonSchema[1, 2] = ""\\.*""
    shareholdersJsonSchema[2, 2] = ""\\.*""
    shareholdersJsonSchema[3, 2] = ""^[0-9]{11}$""
    shareholdersJsonSchema[4, 2] = ""\\.*""


    errorsSchema[1] = ""PropertyName""    
    errorsSchema[2] = ""Message""
    errorsSchema[3] = ""PositionIndex""

    resultSchema[1]= ""ShareHolders""
    resultSchema[2]= ""Errors""
}

/dane wspólników/,/czwarta linia/{      

    if(/imie/ || /nazwisko/ || /pesel/ || /funkcja/){

        if(/imie/){
            shareholdersArray[rowsCount, 1] = jsonStringEscapeAndWrap($2)
            matchCount++ 
        }
        if(/nazwisko/){ 
            shareholdersArray[rowsCount, 2] = jsonStringEscapeAndWrap($2)
            matchCount ++ 
        }
        if(/pesel/){             
            shareholdersArray[rowsCount, 3] = $2
            matchCount ++ 
        }
        if(/funkcja/){ 
            shareholdersArray[rowsCount, 4] = jsonStringEscapeAndWrap($2)
            matchCount ++
        }

        if(matchCount==4){
            rowsCount++
            matchCount = 0;
        }         
    } 
}

END{
    shareHolders = jsonValidateAndPrint(shareholdersArray, rowsCount - 1, shareholdersJsonSchema, 4, errorArray)

    shareHoldersErrors = jsonPrint(errorArray, length(errorArray) / length(errorsSchema), errorsSchema)

    resultArray[1,1] = ""\n[\n"" shareHolders ""\n]\n""
    resultArray[1,2] = ""\n[\n"" shareHoldersErrors ""\n]\n""

    resultJson = jsonPrint(resultArray, 1, resultSchema)
    print resultJson    
}
</code></pre>

<p>Produces output:</p>

<pre><code>{""ShareHolders"":
[
{""Imie"":""JanA"",""Nazwisko"":""NowakA"",""PESEL"":null,""Funkcja"":""PrezesA""},
{""Imie"":""Ja\""nD"",""Nazwisko"":""NowakD"",""PESEL"":11111111111,""Funkcja"":""PrezesD""},
{""Imie"":""JanC"",""Nazwisko"":""NowakC"",""PESEL"":null,""Funkcja"":""PrezesC""}
]
,""Errors"":
[
{""PropertyName"":""PESEL"",""Message"":""Value 11111111111A not match format: ^[0-9]{11}$ "",""PositionIndex"":1},
{""PropertyName"":""PESEL"",""Message"":""Value 12342C not match format: ^[0-9]{11}$ "",""PositionIndex"":3}
]
}
</code></pre>
",1,2,3133,2018-05-22 23:39:35,https://stackoverflow.com/questions/50477885/how-to-print-json-objects-in-awk
Gensim doc2vec most_similar equivalent to get full documents,"<p>In Gensim's doc2vec implementation, <code>gensim.models.keyedvectors.Doc2VecKeyedVectors.most_similar</code> returns the tags and cosine similarity of the documents most similar to the query document. What if I want the actual documents themselves and not the tags? Is there a way to do that directly without searching for the document associated with the tag returned by <code>most_similar</code>?</p>

<p>Also, is there documentation on this? I can't seem to find the documentation for half of Gensim's classes.</p>
","python-3.x, nlp, text-mining, gensim, doc2vec","<p>The <code>Doc2Vec</code> class doesn't serve as a full document database that stores the original documents in their original formats. That would require a lot of extra complexity and state. </p>

<p>Instead, you just present the docs, with their particular tags, in the tokenized format it needs for training, and the model only learns and retains their vector representations. </p>

<p>If you need to then look-up the original documents, you must maintain your own (tags -> documents) lookup – which many projects will already have as the original source of the docs. </p>

<p>The <code>Doc2Vec</code> class docs are at <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a> but it may also be helpful to look at the example Jupyter notebooks included in the <code>gensim</code> <code>docs/notebooks</code> directory but also viewable online at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks</a></p>

<p>The three notebooks related to <code>Doc2Vec</code> have filenames beginning <code>doc2vec-</code>. </p>
",2,2,1156,2018-05-25 13:50:44,https://stackoverflow.com/questions/50530747/gensim-doc2vec-most-similar-equivalent-to-get-full-documents
"SQL query to Python , Using Pandas","<p>I have a SQL table TweetData, it has a Column Tweets, so to mine few keywords from Tweets I'm currently using SQL query as</p>

<pre><code>SELECT Tweets, 'Camera Quality'  as CameraQuality
FROM TweetData
WHERE Tweets like '%camera%'
AND Tweets like '%amazing%'
</code></pre>

<p>my question is, is it possible to do the same thing in python using pandas?</p>
","python, sql, sql-server, pandas, text-mining","<p>you need first to load your data into a collection holder object like dataframe. Then you are able to use code like this:</p>

<pre><code>df[['Tweets', 'Camera Quality']][(df['Tweets'].str.contains(""camera"")) &amp; (df['Tweets'].str.contains(""amazing""))]
</code></pre>

<p>this returns what you need.</p>
",0,-1,63,2018-05-26 06:25:30,https://stackoverflow.com/questions/50539970/sql-query-to-python-using-pandas
R: is it possible to calculate word burstiness with quanteda or any other text mining R package?,"<p>We are using burstiness for terminology/lexicon induction from text corpora.</p>

<p>We have currently implemented a R script based on one of the formulas of Burstiness Similarity described in Section 2.6 of the following article:
Ann Irvine and Chris Callison-Burch (2017). A Comprehensive Analysis of Bilingual Lexicon Induction. Computational Linguistics
Volume 43 | Issue 2 | June 2017 p.273-310.
<a href=""https://www.mitpressjournals.org/doi/full/10.1162/COLI_a_00284"" rel=""nofollow noreferrer"">https://www.mitpressjournals.org/doi/full/10.1162/COLI_a_00284</a></p>

<p>As far as I know, Katz was one of the first scientists who used the concept of burstiness for language modelling (see Justeson, J. S. and Katz, S. M. (1995). Technical terminology: some linguistic properties and
an algorithm for identification in text. Natural Language Engineering, 1:9–27; Katz, S. (1996). Distribution of content words and phrases in text and language modelling.Natural Language Engineering, 2(1):15–60.)</p>

<p>We would like to use off-the-shelf burstiness implementations for comparison and for the evaluation of our script. </p>

<p>I would like to know whether there exist R packages or R functions that identify bursty words in text corpora. I would be particularly interested in any solutions based or leveraging on Quanteda, since Quanteda an extremely versatile package for text statistics. </p>

<p>The only R package that I found so far is Package ‘bursts’ (February 19, 2015), which implements Kleinberg's burstiness. Kleinberg’s burst detection algorithm ""identifies time periods in which a target event is uncharacteristically frequent, or “bursty.” This is not what I need since this approach is based on time series. </p>

<p>Help, suggestions, references are appreciated.</p>

<p>Cheers, 
Marina</p>
","r, text-mining, quanteda","<p>I haven't found many public references about burstiness related to text analyses. I did come across <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2670513/"" rel=""nofollow noreferrer"">Modeling Statistical Properties of Written Text</a>. </p>

<p>If I'm reading the formula correctly in section 2.6 from the article you supplied, then it is the relative proportion of the words divided by the percentage of the documents in which the words appear.</p>

<p>I had hoped that using the <code>dfm_tfidf</code> function would get me there. But the <code>scheme_df</code> part of the function does not have a proportional document frequency option.</p>

<p>But we can use parts of quanteda's existing functions to put everything together.</p>

<p>Lets assume we having a document-feature matrix (<code>dfm</code>) called ""docfm"". Then the steps are like this</p>

<ol>
<li><p>the relative proportion of the terms can be calculated by <code>dfm_weight(docfm, scheme = ""prop"")</code></p></li>
<li><p>Getting the proportional document frequency is <code>docfreq(docfm) / ndocs(docfm)</code>.</p></li>
</ol>

<p>Now some matrix division calculations. Either <code>apply</code> or <code>sweep</code> will work. <code>apply</code> will return a matrix and needs to be transposed, sweep will return a dgeMatrix. In both cases you can turn them back into a <code>dmf</code> with <code>as.dfm</code>. Unfortunately both are dense matrices, so you might need to take this into account. Putting it all together:</p>

<p>Using <code>apply</code>:</p>

<pre><code>t(apply(X = dfm_weight(docfm, scheme = ""prop""), 1, ""/"",  (docfreq(docfm) / ndoc(docfm))))
</code></pre>

<p>Using <code>sweep</code>:</p>

<pre><code>sweep(dfm_weight(docfm, scheme = ""prop""), MARGIN = 2, STATS = docfreq(docfm) / ndoc(docfm), FUN = ""/"")
</code></pre>
",1,1,321,2018-06-03 13:20:22,https://stackoverflow.com/questions/50666860/r-is-it-possible-to-calculate-word-burstiness-with-quanteda-or-any-other-text-m
Get term frequencies within categories in R dictionary,"<p>I have a dictionary with multiple subcategories and I would like to find the most frequent words and bigrams within each subcategory using R. </p>

<p>I am using a large dataset but here's an example of what I have looks like:</p>

<pre><code>s &lt;-  ""Day after day, day after day,
We stuck, nor breath nor motion;""

library(stringi)
x &lt;- stri_replace_all(s, """", regex=""&lt;.*?&gt;"") 
x &lt;- stri_trim(s)
x &lt;- stri_trans_tolower(s) 

library(quanteda)
toks &lt;- tokens(x) 
toks &lt;- tokens_wordstem(toks) 

dtm &lt;- dfm(toks, 
       tolower=TRUE, stem=TRUE,
       remove=stopwords(""english""))

dict1 &lt;- dictionary(list(a=c(""day*"", ""week*"", ""month*""),
                    b=c(""breath*"",""motion*"")))

dict_dtm2 &lt;- dfm_lookup(dtm, dict1, nomatch=""_unmatched"")                                 
tail(dict_dtm2)    
</code></pre>

<p>This gives me the total frequencies per subcategory but not the frequency of each individual word within these subcategories. 
The results I am looking for would look something like this:</p>

<pre><code>words(a)   freq
day         4
week        0
month       0

words(b)   freq
breath     1
motion     1 
</code></pre>

<p>I would appreciate any help with that!</p>
","r, text-mining, text-analysis, quanteda","<p>As far as I understand your question, I believe you are in the look for the <code>table()</code> command. You need to work a little bit of regular expressions to treat the first sentence, but I believe you can do it. An idea can be as following:</p>

<pre><code>s &lt;-  ""day after day day after day We stuck nor breath nor motion""
s &lt;- strsplit(s, ""\\s+"")

dict &lt;- list(a&lt;- c(""day"", ""week"", ""month""),
                        b&lt;-c(""breath"",""motion""))
lapply(dict, function(x){
                Wordsinvect&lt;-intersect(unlist(x),unlist(s))
                return(table(s)[Wordsinvect])}
)


# [[1]]
# day 
# 4 
# 
# [[2]]
# s
# breath motion 
# 1      1 
</code></pre>

<p>I hope it helps. Cheers !</p>
",1,1,240,2018-06-04 23:03:00,https://stackoverflow.com/questions/50689908/get-term-frequencies-within-categories-in-r-dictionary
Keep only sentences in corpus that contain specific key words (in R),"<p>I have a corpus with <code>.txt</code> documents. From these <code>.txt</code> documents, I do not need all sentences, but I only want to keep certain sentences that contain specific key words. From there on, I will perform similarity measures etc.</p>
<p>So, here is an example.
From the data_corpus_inaugural data set of the quanteda package, I only want to keep the sentences in my corpus that contain the words &quot;future&quot; and/or &quot;children&quot;.</p>
<p>I load my packages and create the corpus:</p>
<pre><code>library(quanteda)
library(stringr)


## corpus with data_corpus_inaugural of the quanteda package
corpus &lt;- corpus(data_corpus_inaugural)
summary(corpus)
</code></pre>
<p>Then I want to keep only those sentences that contain my key words</p>
<pre><code>## keep only those sentences of a document that contain words future or/and 
children
</code></pre>
<p>First, let's see which documents contain these key words</p>
<pre><code>## extract all matches of future or children
str_extract_all(corpus, pattern = &quot;future|children&quot;)
</code></pre>
<p>So far, I only found out how to exclude the sentences that contain my key words, which is the opposite of what I want to do.</p>
<pre><code>## excluded sentences that contains future or children or both (?)
corpustrim &lt;- corpus_trimsentences(corpus, exclude_pattern = 
&quot;future|children&quot;)
summary(corpustrim)
</code></pre>
<p>The above command excludes sentences containing my key words.
My idea here with the corpus_trimsentences function is to exclude all sentences BUT those containing &quot;future&quot; and/or &quot;children&quot;.</p>
<p>I tried with regular expression. However, I did not manage to do it. It does not return what I want.</p>
<p>I looked into the <code>corpus_reshape</code> and <code>corpus_subset</code> functions of the quanteda package but I can't figure out how to use them for my purpose.</p>
","r, nlp, text-mining, corpus, quanteda","<p>You are correct that it's <code>corpus_reshape()</code> and <code>corpus_subset()</code> that you want here.  Here's how to use them.</p>

<p>First, reshape the corpus to sentences.</p>

<pre><code>library(""quanteda"")

data_corpus_inauguralsents &lt;- 
  corpus_reshape(data_corpus_inaugural, to = ""sentences"")
data_corpus_inauguralsents
</code></pre>

<p>The use <strong>stringr</strong> to create a logical (Boolean) that indicates the presence or absence of the pattern, equal in length to the new sentence corpus.</p>

<pre><code>containstarget &lt;- 
  stringr::str_detect(texts(data_corpus_inauguralsents), ""future|children"")
summary(containstarget)
##    Mode   FALSE    TRUE 
## logical    4879     137 
</code></pre>

<p>Then use <code>corpus_subset()</code> to keep only those with the pattern:</p>

<pre><code>data_corpus_inauguralsentssub &lt;- 
  corpus_subset(data_corpus_inauguralsents, containstarget)
tail(texts(data_corpus_inauguralsentssub), 2)
## 2017-Trump.30 
## ""But for too many of our citizens, a different reality exists: mothers and children trapped in poverty in our inner cities; rusted-out factories scattered like tombstones across the landscape of our nation; an education system, flush with cash, but which leaves our young and beautiful students deprived of all knowledge; and the crime and the gangs and the drugs that have stolen too many lives and robbed our country of so much unrealized potential."" 
## 2017-Trump.41 
## ""And now we are looking only to the future."" 
</code></pre>

<p>Finally, if you want to put these selected sentences back into their original document containers, but without the sentences that did not contain the target words, then reshape again:</p>

<pre><code># reshape back to documents that contain only sentences with the target terms
corpus_reshape(data_corpus_inauguralsentssub, to = ""documents"")
## Corpus consisting of 49 documents and 3 docvars.
</code></pre>
",3,2,2051,2018-06-13 15:55:56,https://stackoverflow.com/questions/50841518/keep-only-sentences-in-corpus-that-contain-specific-key-words-in-r
How to read csv file for text mining,"<p>I will be using <code>tm</code> for text mining purpose.However, my file CSV file is weired .Below is the <code>dput</code>,after I used <code>read.table</code>  function in r. There are three column lie, sentiment and review. However the fourth coulmn contain review with no column name.I am New to R and Text mining. If I use <code>read.csv</code>it is getting me an error. Please suggest better approach for reading csv file.</p>

<p><strong>Update:</strong></p>

<pre><code>  &gt; dput(head(df))

structure(list(V1 = c(""lie,sentiment,review"", ""f,n,'Mike\\'s"", 
""f,n,'i"", ""f,n,'After"", ""f,n,'Olive"", ""f,n,'I""), V2 = c("""", ""Pizza"", 
""really"", ""I"", ""Oil"", ""went""), V3 = c("""", ""High"", ""like"", ""went"", 
""Garden"", ""to""), V4 = c("""", ""Point,"", ""this"", ""shopping"", ""was"", 
""the""), V5 = c("""", ""NY"", ""buffet"", ""with"", ""very"", ""Chilis""), 
    V6 = c("""", ""Service"", ""restaurant"", ""some"", ""disappointing."", 
    ""on""), V7 = c("""", ""was"", ""in"", ""of"", ""I"", ""Erie""), V8 = c("""", 
    ""very"", ""Marshall"", ""my"", ""expect"", ""Blvd""), V9 = c("""", ""slow"", 
    ""street."", ""friend,"", ""good"", ""and""), V10 = c("""", ""and"", 
    ""they"", ""we"", ""food"", ""had""), V11 = c("""", ""the"", ""have"", 
    ""went"", ""and"", ""the""), V12 = c("""", ""quality"", ""a"", ""to"", 
    ""good"", ""worst""), V13 = c("""", ""was"", ""lot"", ""DODO"", ""service"", 
    ""meal""), V14 = c("""", ""low."", ""of"", ""restaurant"", ""(at"", ""of""
    ), V15 = c("""", ""You"", ""selection"", ""for"", ""least!!)"", ""my""
    ), V16 = c("""", ""would"", ""of"", ""dinner."", ""when"", ""life.""), 
    V17 = c("""", ""think"", ""american,"", ""I"", ""I"", ""We""), V18 = c("""", 
    ""they"", ""japanese,"", ""found"", ""go"", ""arrived""), V19 = c("""", 
    ""would"", ""and"", ""worm"", ""out"", ""and""), V20 = c("""", ""know"", 
    ""chinese"", ""in"", ""to"", ""waited""), V21 = c("""", ""at"", ""dishes."", 
    ""one"", ""eat."", ""5""), V22 = c("""", ""least"", ""we"", ""of"", ""The"", 
    ""minutes""), V23 = c("""", ""how"", ""also"", ""the"", ""meal"", ""for""
    ), V24 = c("""", ""to"", ""got"", ""dishes"", ""was"", ""a""), V25 = c("""", 
    ""make"", ""a"", "".'"", ""cold"", ""hostess,""), V26 = c("""", ""good"", 
    ""free"", """", ""when"", ""and""), V27 = c("""", ""pizza,"", ""drink"", 
    """", ""we"", ""then""), V28 = c("""", ""not."", ""and"", """", ""got"", 
    ""were""), V29 = c("""", ""Stick"", ""free"", """", ""it,"", ""seated""
    ), V30 = c("""", ""to"", ""refill."", """", ""and"", ""by""), V31 = c("""", 
    ""pre-made"", ""there"", """", ""the"", ""a""), V32 = c("""", ""dishes"", 
    ""are"", """", ""waitor"", ""waiter""), V33 = c("""", ""like"", ""also"", 
    """", ""had"", ""who""), V34 = c("""", ""stuffed"", ""different"", """", 
    ""no"", ""was""), V35 = c("""", ""pasta"", ""kinds"", """", ""manners"", 
    ""obviously""), V36 = c("""", ""or"", ""of"", """", ""whatsoever."", 
    ""in""), V37 = c("""", ""a"", ""dessert."", """", ""Don\\'t"", ""a""), 
    V38 = c("""", ""salad."", ""the"", """", ""go"", ""terrible""), V39 = c("""", 
    ""You"", ""staff"", """", ""to"", ""mood.""), V40 = c("""", ""should"", 
    ""is"", """", ""the"", ""We""), V41 = c("""", ""consider"", ""very"", """", 
    ""Olive"", ""order""), V42 = c("""", ""dining"", ""friendly."", """", 
    ""Oil"", ""drinks""), V43 = c("""", ""else"", ""it"", """", ""Garden."", 
    ""and""), V44 = c("""", ""where.'"", ""is"", """", ""\nf,n,"", ""it""), 
    V45 = c("""", """", ""also"", """", ""The"", ""took""), V46 = c("""", """", 
    ""quite"", """", ""Seven"", ""them""), V47 = c("""", """", ""cheap"", """", 
    ""Heaven"", ""15""), V48 = c("""", """", ""compared"", """", ""restaurant"", 
    ""minutes""), V49 = c("""", """", ""with"", """", ""was"", ""to""), V50 = c("""", 
    """", ""the"", """", ""never"", ""bring""), V51 = c("""", """", ""other"", 
    """", ""known"", ""us""), V52 = c("""", """", ""restaurant"", """", ""for"", 
    ""both""), V53 = c("""", """", ""in"", """", ""a"", ""the""), V54 = c("""", 
    """", ""syracuse"", """", ""superior"", ""wrong""), V55 = c("""", """", 
    ""area."", """", ""service"", ""beers""), V56 = c("""", """", ""i"", """", 
    ""but"", ""which""), V57 = c("""", """", ""will"", """", ""what"", ""were""
    ), V58 = c("""", """", ""definitely"", """", ""we"", ""barely""), V59 = c("""", 
    """", ""coming"", """", ""experienced"", ""cold.""), V60 = c("""", """", 
    ""back"", """", ""last"", ""Then""), V61 = c("""", """", ""here.'"", """", 
    ""week"", ""we""), V62 = c("""", """", """", """", ""was"", ""order""), V63 = c("""", 
    """", """", """", ""a"", ""an""), V64 = c("""", """", """", """", ""disaster."", 
    ""appetizer""), V65 = c("""", """", """", """", ""The"", ""and""), V66 = c("""", 
    """", """", """", ""waiter"", ""wait""), V67 = c("""", """", """", """", ""would"", 
    ""25""), V68 = c("""", """", """", """", ""not"", ""minutes""), V69 = c("""", 
    """", """", """", ""notice"", ""for""), V70 = c("""", """", """", """", ""us"", 
    ""cold""), V71 = c("""", """", """", """", ""until"", ""southwest""), V72 = c("""", 
    """", """", """", ""we"", ""egg""), V73 = c("""", """", """", """", ""asked"", 
    ""rolls,""), V74 = c("""", """", """", """", ""him"", ""at""), V75 = c("""", 
    """", """", """", ""4"", ""which""), V76 = c("""", """", """", """", ""times"", 
    ""point""), V77 = c("""", """", """", """", ""to"", ""we""), V78 = c("""", 
    """", """", """", ""bring"", ""just""), V79 = c("""", """", """", """", ""us"", 
    ""paid""), V80 = c("""", """", """", """", ""the"", ""and""), V81 = c("""", 
    """", """", """", ""menu."", ""left.""), V82 = c("""", """", """", """", ""The"", 
    ""Don\\'t""), V83 = c("""", """", """", """", ""food"", ""go.'""), V84 = c("""", 
    """", """", """", ""was"", """"), V85 = c("""", """", """", """", ""not"", """"
    ), V86 = c("""", """", """", """", ""exceptional"", """"), V87 = c("""", 
    """", """", """", ""either."", """"), V88 = c("""", """", """", """", ""It"", 
    """"), V89 = c("""", """", """", """", ""took"", """"), V90 = c("""", """", 
    """", """", ""them"", """"), V91 = c("""", """", """", """", ""though"", """"
    ), V92 = c("""", """", """", """", ""2"", """"), V93 = c("""", """", """", 
    """", ""minutes"", """"), V94 = c("""", """", """", """", ""to"", """"), V95 = c("""", 
    """", """", """", ""bring"", """"), V96 = c("""", """", """", """", ""us"", """"
    ), V97 = c("""", """", """", """", ""a"", """"), V98 = c("""", """", """", 
    """", ""check"", """"), V99 = c("""", """", """", """", ""after"", """"), V100 = c("""", 
    """", """", """", ""they"", """"), V101 = c("""", """", """", """", ""spotted"", 
    """"), V102 = c("""", """", """", """", ""we"", """"), V103 = c("""", """", 
    """", """", ""finished"", """"), V104 = c("""", """", """", """", ""eating"", 
    """"), V105 = c("""", """", """", """", ""and"", """"), V106 = c("""", """", 
    """", """", ""are"", """"), V107 = c("""", """", """", """", ""not"", """"), 
    V108 = c("""", """", """", """", ""ordering"", """"), V109 = c("""", """", 
    """", """", ""more."", """"), V110 = c("""", """", """", """", ""Well,"", """"
    ), V111 = c("""", """", """", """", ""never"", """"), V112 = c("""", """", 
    """", """", ""more."", """"), V113 = c("""", """", """", """", ""\nf,n,"", 
    """"), V114 = c("""", """", """", """", ""I"", """"), V115 = c("""", """", 
    """", """", ""went"", """"), V116 = c("""", """", """", """", ""to"", """"), 
    V117 = c("""", """", """", """", ""XYZ"", """"), V118 = c("""", """", """", 
    """", ""restaurant"", """"), V119 = c("""", """", """", """", ""and"", """"
    ), V120 = c("""", """", """", """", ""had"", """"), V121 = c("""", """", 
    """", """", ""a"", """"), V122 = c("""", """", """", """", ""terrible"", """"
    ), V123 = c("""", """", """", """", ""experience."", """"), V124 = c("""", 
    """", """", """", ""I"", """"), V125 = c("""", """", """", """", ""had"", """"), 
    V126 = c("""", """", """", """", ""a"", """"), V127 = c("""", """", """", """", 
    ""YELP"", """"), V128 = c("""", """", """", """", ""Free"", """"), V129 = c("""", 
    """", """", """", ""Appetizer"", """"), V130 = c("""", """", """", """", ""coupon"", 
    """"), V131 = c("""", """", """", """", ""which"", """"), V132 = c("""", 
    """", """", """", ""could"", """"), V133 = c("""", """", """", """", ""be"", 
    """"), V134 = c("""", """", """", """", ""applied"", """"), V135 = c("""", 
    """", """", """", ""upon"", """"), V136 = c("""", """", """", """", ""checking"", 
    """"), V137 = c("""", """", """", """", ""in"", """"), V138 = c("""", """", 
    """", """", ""to"", """"), V139 = c("""", """", """", """", ""the"", """"), V140 = c("""", 
    """", """", """", ""restaurant."", """"), V141 = c("""", """", """", """", 
    ""The"", """"), V142 = c("""", """", """", """", ""person"", """"), V143 = c("""", 
    """", """", """", ""serving"", """"), V144 = c("""", """", """", """", ""us"", 
    """"), V145 = c("""", """", """", """", ""was"", """"), V146 = c("""", """", 
    """", """", ""very"", """"), V147 = c("""", """", """", """", ""rude"", """"), 
    V148 = c("""", """", """", """", ""and"", """"), V149 = c("""", """", """", 
    """", ""didn\\'t"", """"), V150 = c("""", """", """", """", ""acknowledge"", 
    """"), V151 = c("""", """", """", """", ""the"", """"), V152 = c("""", """", 
    """", """", ""coupon."", """"), V153 = c("""", """", """", """", ""When"", 
    """"), V154 = c("""", """", """", """", ""I"", """"), V155 = c("""", """", 
    """", """", ""asked"", """"), V156 = c("""", """", """", """", ""her"", """"), 
    V157 = c("""", """", """", """", ""about"", """"), V158 = c("""", """", """", 
    """", ""it,"", """"), V159 = c("""", """", """", """", ""she"", """"), V160 = c("""", 
    """", """", """", ""rudely"", """"), V161 = c("""", """", """", """", ""replied"", 
    """"), V162 = c("""", """", """", """", ""back"", """"), V163 = c("""", """", 
    """", """", ""saying"", """"), V164 = c("""", """", """", """", ""she"", """"
    ), V165 = c("""", """", """", """", ""had"", """"), V166 = c("""", """", 
    """", """", ""already"", """"), V167 = c("""", """", """", """", ""applied"", 
    """"), V168 = c("""", """", """", """", ""it."", """"), V169 = c("""", """", 
    """", """", ""Then"", """"), V170 = c("""", """", """", """", ""I"", """"), V171 = c("""", 
    """", """", """", ""inquired"", """"), V172 = c("""", """", """", """", ""about"", 
    """"), V173 = c("""", """", """", """", ""the"", """"), V174 = c("""", """", 
    """", """", ""free"", """"), V175 = c("""", """", """", """", ""salad"", """"
    ), V176 = c("""", """", """", """", ""that"", """"), V177 = c("""", """", 
    """", """", ""they"", """"), V178 = c("""", """", """", """", ""serve."", """"
    ), V179 = c("""", """", """", """", ""She"", """"), V180 = c("""", """", 
    """", """", ""rudely"", """"), V181 = c("""", """", """", """", ""said"", """"
    ), V182 = c("""", """", """", """", ""that"", """"), V183 = c("""", """", 
    """", """", ""you"", """"), V184 = c("""", """", """", """", ""have"", """"), 
    V185 = c("""", """", """", """", ""to"", """"), V186 = c("""", """", """", 
    """", ""order"", """"), V187 = c("""", """", """", """", ""the"", """"), V188 = c("""", 
    """", """", """", ""main"", """"), V189 = c("""", """", """", """", ""course"", 
    """"), V190 = c("""", """", """", """", ""to"", """"), V191 = c("""", """", 
    """", """", ""get"", """"), V192 = c("""", """", """", """", ""that."", """"), 
    V193 = c("""", """", """", """", ""Overall,"", """"), V194 = c("""", """", 
    """", """", ""I"", """"), V195 = c("""", """", """", """", ""had"", """"), V196 = c("""", 
    """", """", """", ""a"", """"), V197 = c("""", """", """", """", ""bad"", """"), 
    V198 = c("""", """", """", """", ""experience"", """"), V199 = c("""", 
    """", """", """", ""as"", """"), V200 = c("""", """", """", """", ""I"", """"), 
    V201 = c("""", """", """", """", ""had"", """"), V202 = c("""", """", """", 
    """", ""taken"", """"), V203 = c("""", """", """", """", ""my"", """"), V204 = c("""", 
    """", """", """", ""family"", """"), V205 = c("""", """", """", """", ""to"", 
    """"), V206 = c("""", """", """", """", ""that"", """"), V207 = c("""", """", 
    """", """", ""restaurant"", """"), V208 = c("""", """", """", """", ""for"", 
    """"), V209 = c("""", """", """", """", ""the"", """"), V210 = c("""", """", 
    """", """", ""first"", """"), V211 = c("""", """", """", """", ""time"", """"
    ), V212 = c("""", """", """", """", ""and"", """"), V213 = c("""", """", 
    """", """", ""I"", """"), V214 = c("""", """", """", """", ""had"", """"), V215 = c("""", 
    """", """", """", ""high"", """"), V216 = c("""", """", """", """", ""hopes"", 
    """"), V217 = c("""", """", """", """", ""from"", """"), V218 = c("""", """", 
    """", """", ""the"", """"), V219 = c("""", """", """", """", ""restaurant"", 
    """"), V220 = c("""", """", """", """", ""which"", """"), V221 = c("""", 
    """", """", """", ""is,"", """"), V222 = c("""", """", """", """", ""otherwise,"", 
    """"), V223 = c("""", """", """", """", ""my"", """"), V224 = c("""", """", 
    """", """", ""favorite"", """"), V225 = c("""", """", """", """", ""place"", 
    """"), V226 = c("""", """", """", """", ""to"", """"), V227 = c("""", """", 
    """", """", ""dine."", """"), V228 = c("""", """", """", """", ""\nf,n,"", 
    """"), V229 = c("""", """", """", """", ""I"", """"), V230 = c("""", """", 
    """", """", ""went"", """"), V231 = c("""", """", """", """", ""to"", """"), 
    V232 = c("""", """", """", """", ""ABC"", """"), V233 = c("""", """", """", 
    """", ""restaurant"", """"), V234 = c("""", """", """", """", ""two"", """"
    ), V235 = c("""", """", """", """", ""days"", """"), V236 = c("""", """", 
    """", """", ""ago"", """"), V237 = c("""", """", """", """", ""and"", """"), 
    V238 = c("""", """", """", """", ""I"", """"), V239 = c("""", """", """", """", 
    ""hated"", """"), V240 = c("""", """", """", """", ""the"", """"), V241 = c("""", 
    """", """", """", ""food"", """"), V242 = c("""", """", """", """", ""and"", 
    """"), V243 = c("""", """", """", """", ""the"", """"), V244 = c("""", """", 
    """", """", ""service."", """"), V245 = c("""", """", """", """", ""We"", """"
    ), V246 = c("""", """", """", """", ""were"", """"), V247 = c("""", """", 
    """", """", ""kept"", """"), V248 = c("""", """", """", """", ""waiting"", 
    """"), V249 = c("""", """", """", """", ""for"", """"), V250 = c("""", """", 
    """", """", ""over"", """"), V251 = c("""", """", """", """", ""an"", """"), 
    V252 = c("""", """", """", """", ""hour"", """"), V253 = c("""", """", """", 
    """", ""just"", """"), V254 = c("""", """", """", """", ""to"", """"), V255 = c("""", 
    """", """", """", ""get"", """"), V256 = c("""", """", """", """", ""seated"", 
    """"), V257 = c("""", """", """", """", ""and"", """"), V258 = c("""", """", 
    """", """", ""once"", """"), V259 = c("""", """", """", """", ""we"", """"), 
    V260 = c("""", """", """", """", ""ordered,"", """"), V261 = c("""", """", 
    """", """", ""our"", """"), V262 = c("""", """", """", """", ""food"", """"), 
    V263 = c("""", """", """", """", ""came"", """"), V264 = c("""", """", """", 
    """", ""out"", """"), V265 = c("""", """", """", """", ""cold."", """"), V266 = c("""", 
    """", """", """", ""I"", """"), V267 = c("""", """", """", """", ""ordered"", 
    """"), V268 = c("""", """", """", """", ""the"", """"), V269 = c("""", """", 
    """", """", ""pasta"", """"), V270 = c("""", """", """", """", ""and"", """"), 
    V271 = c("""", """", """", """", ""it"", """"), V272 = c("""", """", """", 
    """", ""was"", """"), V273 = c("""", """", """", """", ""terrible"", """"), 
    V274 = c("""", """", """", """", ""-"", """"), V275 = c("""", """", """", """", 
    ""completely"", """"), V276 = c("""", """", """", """", ""bland"", """"), 
    V277 = c("""", """", """", """", ""and"", """"), V278 = c("""", """", """", 
    """", ""very"", """"), V279 = c("""", """", """", """", ""unappatizing."", 
    """"), V280 = c("""", """", """", """", ""I"", """"), V281 = c("""", """", 
    """", """", ""definitely"", """"), V282 = c("""", """", """", """", ""would"", 
    """"), V283 = c("""", """", """", """", ""not"", """"), V284 = c("""", """", 
    """", """", ""recommend"", """"), V285 = c("""", """", """", """", ""going"", 
    """"), V286 = c("""", """", """", """", ""there,"", """"), V287 = c("""", 
    """", """", """", ""especially"", """"), V288 = c("""", """", """", """", ""if"", 
    """"), V289 = c("""", """", """", """", ""you\\'re"", """"), V290 = c("""", 
    """", """", """", ""in"", """"), V291 = c("""", """", """", """", ""a"", """"), 
    V292 = c("""", """", """", """", ""hurry!'"", """")), .Names = c(""V1"", 
""V2"", ""V3"", ""V4"", ""V5"", ""V6"", ""V7"", ""V8"", ""V9"", ""V10"", ""V11"", 
""V12"", ""V13"", ""V14"", ""V15"", ""V16"", ""V17"", ""V18"", ""V19"", ""V20"", 
""V21"", ""V22"", ""V23"", ""V24"", ""V25"", ""V26"", ""V27"", ""V28"", ""V29"", 
""V30"", ""V31"", ""V32"", ""V33"", ""V34"", ""V35"", ""V36"", ""V37"", ""V38"", 
""V39"", ""V40"", ""V41"", ""V42"", ""V43"", ""V44"", ""V45"", ""V46"", ""V47"", 
""V48"", ""V49"", ""V50"", ""V51"", ""V52"", ""V53"", ""V54"", ""V55"", ""V56"", 
""V57"", ""V58"", ""V59"", ""V60"", ""V61"", ""V62"", ""V63"", ""V64"", ""V65"", 
""V66"", ""V67"", ""V68"", ""V69"", ""V70"", ""V71"", ""V72"", ""V73"", ""V74"", 
""V75"", ""V76"", ""V77"", ""V78"", ""V79"", ""V80"", ""V81"", ""V82"", ""V83"", 
""V84"", ""V85"", ""V86"", ""V87"", ""V88"", ""V89"", ""V90"", ""V91"", ""V92"", 
""V93"", ""V94"", ""V95"", ""V96"", ""V97"", ""V98"", ""V99"", ""V100"", ""V101"", 
""V102"", ""V103"", ""V104"", ""V105"", ""V106"", ""V107"", ""V108"", ""V109"", 
""V110"", ""V111"", ""V112"", ""V113"", ""V114"", ""V115"", ""V116"", ""V117"", 
""V118"", ""V119"", ""V120"", ""V121"", ""V122"", ""V123"", ""V124"", ""V125"", 
""V126"", ""V127"", ""V128"", ""V129"", ""V130"", ""V131"", ""V132"", ""V133"", 
""V134"", ""V135"", ""V136"", ""V137"", ""V138"", ""V139"", ""V140"", ""V141"", 
""V142"", ""V143"", ""V144"", ""V145"", ""V146"", ""V147"", ""V148"", ""V149"", 
""V150"", ""V151"", ""V152"", ""V153"", ""V154"", ""V155"", ""V156"", ""V157"", 
""V158"", ""V159"", ""V160"", ""V161"", ""V162"", ""V163"", ""V164"", ""V165"", 
""V166"", ""V167"", ""V168"", ""V169"", ""V170"", ""V171"", ""V172"", ""V173"", 
""V174"", ""V175"", ""V176"", ""V177"", ""V178"", ""V179"", ""V180"", ""V181"", 
""V182"", ""V183"", ""V184"", ""V185"", ""V186"", ""V187"", ""V188"", ""V189"", 
""V190"", ""V191"", ""V192"", ""V193"", ""V194"", ""V195"", ""V196"", ""V197"", 
""V198"", ""V199"", ""V200"", ""V201"", ""V202"", ""V203"", ""V204"", ""V205"", 
""V206"", ""V207"", ""V208"", ""V209"", ""V210"", ""V211"", ""V212"", ""V213"", 
""V214"", ""V215"", ""V216"", ""V217"", ""V218"", ""V219"", ""V220"", ""V221"", 
""V222"", ""V223"", ""V224"", ""V225"", ""V226"", ""V227"", ""V228"", ""V229"", 
""V230"", ""V231"", ""V232"", ""V233"", ""V234"", ""V235"", ""V236"", ""V237"", 
""V238"", ""V239"", ""V240"", ""V241"", ""V242"", ""V243"", ""V244"", ""V245"", 
""V246"", ""V247"", ""V248"", ""V249"", ""V250"", ""V251"", ""V252"", ""V253"", 
""V254"", ""V255"", ""V256"", ""V257"", ""V258"", ""V259"", ""V260"", ""V261"", 
""V262"", ""V263"", ""V264"", ""V265"", ""V266"", ""V267"", ""V268"", ""V269"", 
""V270"", ""V271"", ""V272"", ""V273"", ""V274"", ""V275"", ""V276"", ""V277"", 
""V278"", ""V279"", ""V280"", ""V281"", ""V282"", ""V283"", ""V284"", ""V285"", 
""V286"", ""V287"", ""V288"", ""V289"", ""V290"", ""V291"", ""V292""), row.names = c(NA, 
6L), class = ""data.frame"")
</code></pre>

<p><strong>Dataset</strong>:</p>

<pre><code>lie sentiment   review                                                                                  
f   n   'Mike\'s Pizza High Point    NY Service was very slow and the quality was low. You would think they would know at least how to make good pizza   not. Stick to pre-made dishes like stuffed pasta or a salad. You should consider dining else where.'                                                                           
f   n   'i really like this buffet restaurant in Marshall street. they have a lot of selection of american   japanese    and chinese dishes. we also got a free drink and free refill. there are also different kinds of dessert. the staff is very friendly. it is also quite cheap compared with the other restaurant in syracuse area. i will definitely coming back here.'                                                                          
f   n   'After I went shopping with some of my friend    we went to DODO restaurant for dinner. I found worm in one of the dishes .'                                                                                
f   n   'Olive Oil Garden was very disappointing. I expect good food and good service (at least!!) when I go out to eat. The meal was cold when we got it    and the waitor had no manners whatsoever. Don\'t go to the Olive Oil Garden. '                                                                             
f   n   'The Seven Heaven restaurant was never known for a superior service but what we experienced last week was a disaster. The waiter would not notice us until we asked him 4 times to bring us the menu. The food was not exceptional either. It took them though 2 minutes to bring us a check after they spotted we finished eating and are not ordering more. Well   never more. '                                                                              
f   n   'I went to XYZ restaurant and had a terrible experience. I had a YELP Free Appetizer coupon which could be applied upon checking in to the restaurant. The person serving us was very rude and didn\'t acknowledge the coupon. When I asked her about it     she rudely replied back saying she had already applied it. Then I inquired about the free salad that they serve. She rudely said that you have to order the main course to get that. Overall    I had a bad experience as I had taken my family to that restaurant for the first time and I had high hopes from the restaurant which is     otherwise   my favorite place to dine. '                                                                   
f   n   'I went to ABC restaurant two days ago and I hated the food and the service. We were kept waiting for over an hour just to get seated and once we ordered    our food came out cold. I ordered the pasta and it was terrible - completely bland and very unappatizing. I definitely would not recommend going there  especially if you\'re in a hurry!'                                                                         
f   n   'I went to the Chilis on Erie Blvd and had the worst meal of my life. We arrived and waited 5 minutes for a hostess  and then were seated by a waiter who was obviously in a terrible mood. We order drinks and it took them 15 minutes to bring us both the wrong beers which were barely cold. Then we order an appetizer and wait 25 minutes for cold southwest egg rolls     at which point we just paid and left. Don\'t go.'                                                                          
f   n   'OMG. This restaurant is horrible. The receptionist did not greet us     we just stood there and waited for five minutes. The food came late and served not warm. Me and my pet ordered a bowl of salad and a cheese pizza. The salad was not fresh  the crust of a pizza was so hard like plastics. My dog didn\'t even eat that pizza. I hate this place!!!!!!!!!!'                                       
</code></pre>

<p>Thanks in advance, </p>
","r, csv, text-mining","<p>I don't know why you removed the file from the original post, @Yes Boss but this answer is based on this file, rather than your <code>dput</code> output. The file basically had two problems why you couldn't read it in. 1. Your quote character was <code>'</code> instead of the more common <code>""</code>; 2. <code>'</code> is also used in the column <code>review</code> which is a bit too much for base (it tries to split into new columns in these instances). Luckily, the package data.table is a bit smarter and can take care of problem #2:</p>

<pre><code>library(data.table)

df &lt;- fread(file = ""deception.csv"", quote=""\'"")
</code></pre>

<p>The resulting object will be a data.table instead of a data.frame:</p>

<pre><code>&gt; str(df)
Classes ‘data.table’ and 'data.frame':  92 obs. of  3 variables:
 $ lie      : chr  ""f"" ""f"" ""f"" ""f"" ...
 $ sentiment: chr  ""n"" ""n"" ""n"" ""n"" ...
 $ review   : chr  ""Mike\\'s Pizza High Point, NY Service was very slow and the quality was low. You would think they would know at""| __truncated__ ""i really like this buffet restaurant in Marshall street. they have a lot of selection of american, japanese, an""| __truncated__ ""After I went shopping with some of my friend, we went to DODO restaurant for dinner. I found worm in one of the dishes ."" ""Olive Oil Garden was very disappointing. I expect good food and good service (at least!!) when I go out to eat.""| __truncated__ ...
 - attr(*, "".internal.selfref"")=&lt;externalptr&gt; 
</code></pre>

<p>You can turn this behaviour off by setting <code>data.table = FALSE</code> in <code>fread()</code> (if you want to, I recommend learning how to work with data.table).</p>

<p>A personal opinionated note: If you want to get into text mining, look into the quanteda package instead of tm. It is a lot faster and has a more modern approach to many tasks.</p>
",2,0,818,2018-06-16 18:43:58,https://stackoverflow.com/questions/50890737/how-to-read-csv-file-for-text-mining
Naive Bayes model NOT predicting anything on applying model- Predict function returning with 0 factor level,"<p>My dataset looks like the following, and I followed <a href=""https://rstudio-pubs-static.s3.amazonaws.com/194717_4639802819a342eaa274067c9dbb657e.html"" rel=""nofollow noreferrer"">Classification using Naive Bayes tutorial</a> to develop my <code>Naive bayes</code> model  for textmining However, I cannot predict the result of my <code>naive bayes</code>, even though model is built. The <code>predict</code>  function is returning with 0 factor level. Below is my dataset and code so far.</p>

<pre><code>**Dataset:**
lie sentiment   review                                                                                  
f   n   'Mike\'s Pizza High Point    NY Service was very slow and the quality was low. You would think they would know at least how to make good pizza   not. Stick to pre-made dishes like stuffed pasta or a salad. You should consider dining else where.'                                                                           
f   n   'i really like this buffet restaurant in Marshall street. they have a lot of selection of american   japanese    and chinese dishes. we also got a free drink and free refill. there are also different kinds of dessert. the staff is very friendly. it is also quite cheap compared with the other restaurant in syracuse area. i will definitely coming back here.'                                                                          
f   n   'After I went shopping with some of my friend    we went to DODO restaurant for dinner. I found worm in one of the dishes .'                                                                                
f   n   'Olive Oil Garden was very disappointing. I expect good food and good service (at least!!) when I go out to eat. The meal was cold when we got it    and the waitor had no manners whatsoever. Don\'t go to the Olive Oil Garden. '                                                                             
f   n   'The Seven Heaven restaurant was never known for a superior service but what we experienced last week was a disaster. The waiter would not notice us until we asked him 4 times to bring us the menu. The food was not exceptional either. It took them though 2 minutes to bring us a check after they spotted we finished eating and are not ordering more. Well   never more. '                                                                              
f   n   'I went to XYZ restaurant and had a terrible experience. I had a YELP Free Appetizer coupon which could be applied upon checking in to the restaurant. The person serving us was very rude and didn\'t acknowledge the coupon. When I asked her about it     she rudely replied back saying she had already applied it. Then I inquired about the free salad that they serve. She rudely said that you have to order the main course to get that. Overall    I had a bad experience as I had taken my family to that restaurant for the first time and I had high hopes from the restaurant which is     otherwise   my favorite place to dine. '                                                                   
f   n   'I went to ABC restaurant two days ago and I hated the food and the service. We were kept waiting for over an hour just to get seated and once we ordered    our food came out cold. I ordered the pasta and it was terrible - completely bland and very unappatizing. I definitely would not recommend going there  especially if you\'re in a hurry!'                                                                         
f   n   'I went to the Chilis on Erie Blvd and had the worst meal of my life. We arrived and waited 5 minutes for a hostess  and then were seated by a waiter who was obviously in a terrible mood. We order drinks and it took them 15 minutes to bring us both the wrong beers which were barely cold. Then we order an appetizer and wait 25 minutes for cold southwest egg rolls     at which point we just paid and left. Don\'t go.'                                                                          
f   n   'OMG. This restaurant is horrible. The receptionist did not greet us     we just stood there and waited for five minutes. The food came late and served not warm. Me and my pet ordered a bowl of salad and a cheese pizza. The salad was not fresh  the crust of a pizza was so hard like plastics. My dog didn\'t even eat that pizza. I hate this place!!!!!!!!!!'   
</code></pre>

<p><strong>dput(df)</strong></p>

<pre><code>&gt; dput(head(lie))
structure(list(lie = c(""f"", ""f"", ""f"", ""f"", ""f"", ""f""), sentiment = c(""n"", 
""n"", ""n"", ""n"", ""n"", ""n""), review = c(""Mike\\'s Pizza High Point, NY Service was very slow and the quality was low. You would think they would know at least how to make good pizza, not. Stick to pre-made dishes like stuffed pasta or a salad. You should consider dining else where."", 
""i really like this buffet restaurant in Marshall street. they have a lot of selection of american, japanese, and chinese dishes. we also got a free drink and free refill. there are also different kinds of dessert. the staff is very friendly. it is also quite cheap compared with the other restaurant in syracuse area. i will definitely coming back here."", 
""After I went shopping with some of my friend, we went to DODO restaurant for dinner. I found worm in one of the dishes ."", 
""Olive Oil Garden was very disappointing. I expect good food and good service (at least!!) when I go out to eat. The meal was cold when we got it, and the waitor had no manners whatsoever. Don\\'t go to the Olive Oil Garden. "", 
""The Seven Heaven restaurant was never known for a superior service but what we experienced last week was a disaster. The waiter would not notice us until we asked him 4 times to bring us the menu. The food was not exceptional either. It took them though 2 minutes to bring us a check after they spotted we finished eating and are not ordering more. Well, never more. "", 
""I went to XYZ restaurant and had a terrible experience. I had a YELP Free Appetizer coupon which could be applied upon checking in to the restaurant. The person serving us was very rude and didn\\'t acknowledge the coupon. When I asked her about it, she rudely replied back saying she had already applied it. Then I inquired about the free salad that they serve. She rudely said that you have to order the main course to get that. Overall, I had a bad experience as I had taken my family to that restaurant for the first time and I had high hopes from the restaurant which is, otherwise, my favorite place to dine. ""
)), .Names = c(""lie"", ""sentiment"", ""review""), class = c(""data.table"", 
""data.frame""), row.names = c(NA, -6L), .internal.selfref = &lt;pointer: 0x0000000000180788&gt;)
</code></pre>

<p><strong>R code:</strong></p>

<pre><code>library(gmodels)

lie&lt;- fread('deception.csv',header = T,fill = T,quote = ""\'"")
str(lie)
lie
#Corpus Building
words.vec&lt;- VectorSource(lie$review)
words.corpus&lt;- Corpus(words.vec)
words.corpus&lt;-tm_map(words.corpus,content_transformer(tolower)) #lower case
words.corpus&lt;-tm_map(words.corpus,removePunctuation) # remove punctuation
words.corpus&lt;-tm_map(words.corpus,removeNumbers) # remove numbers
words.corpus&lt;-tm_map(words.corpus,removeWords,stopwords('english')) # remove stopwords
words.corpus&lt;-tm_map(words.corpus,stripWhitespace) # remove unnecessary whitespace

#==========================================================================
#Document term Matrix
dtm&lt;-DocumentTermMatrix(words.corpus)
dtm
class(dtm)

#dtm_df&lt;-as.data.frame(as.matrix(dtm))
#class(dtm_df)

freq &lt;- colSums(as.matrix(dtm))
length(freq)
ord &lt;- order(freq,decreasing=TRUE)
freq[head(ord)]
freq[tail(ord)]

#===========================================================================
#Data frame partition
#Splitting DTM

dtm_train &lt;- dtm[1:61, ]
dtm_test &lt;- dtm[62:92, ]

train_labels &lt;- lie[1:61, ]$lie
test_labels &lt;-lie[62:92, ]$lie

str(train_labels)
str(test_labels)

prop.table(table(train_labels))
prop.table(table(test_labels))


freq_words &lt;- findFreqTerms(dtm_train, 10)
freq_words
dtm_freq_train&lt;- dtm_train[ , freq_words]
dtm_freq_test &lt;- dtm_test[ , freq_words]
dtm_freq_test


convert_counts &lt;- function(x) {
  x &lt;- ifelse(x &gt; 0, 'yes','No')
}

train &lt;- apply(dtm_freq_train, MARGIN = 2, convert_counts)
test &lt;- apply(dtm_freq_test, MARGIN = 2, convert_counts)
str(test)


nb_classifier&lt;-naiveBayes(train,train_labels)
nb_classifier

test_pred&lt;-predict(nb_classifier,test)
</code></pre>

<p>Thanks in advance for help,</p>
","r, text-mining, naivebayes","<p><em>Naive Bayes</em> requires the <em>response</em> variable as a <em>categorical</em> class variable:
Convert <code>lie</code> column of your <code>lie</code> data-frame to <code>factor</code>and re run analysis:</p>

<pre><code>lie$lie &lt;- as.factor(lie$lie)
</code></pre>
",2,1,296,2018-06-16 22:57:02,https://stackoverflow.com/questions/50892402/naive-bayes-model-not-predicting-anything-on-applying-model-predict-function-re
Extract variations of a string from R column,"<p>I have list of keywords</p>

<pre><code>keywords=c(""Minister"", ""President"",""Secretary"")
</code></pre>

<p>I have a column which has different text in different rows</p>

<pre><code>column=c(""he is general Secretary of Ozon group"", ""He is vice president of 
our college"", ""He is health minister"", ""He is education minister"")
</code></pre>

<p>is there any way to extract variations present in the column based on keywords?</p>

<p>the output I am looking is</p>

<pre><code>output=c(""general Secretary"",""vice president"", ""education minister"", ""health minister"")
</code></pre>
","r, nlp, text-mining","<p>If you're trying to extract the keyword + any preceeding word, you could do it like this:</p>

<pre><code>pat &lt;- paste0(""\\w+\\s("", paste(keywords, collapse = ""|""), "")"")
regmatches(column, gregexpr(pat, column, ignore.case = TRUE))
#[[1]]
#[1] ""general Secretary""
#
#[[2]]
#[1] ""vice president""
#
#[[3]]
#[1] ""health minister""
#
#[[4]]
#[1] ""education minister""
</code></pre>

<p>Or using stringr</p>

<pre><code>library(stringr)
pat &lt;- paste0(""\\w+\\s("", paste(tolower(keywords), collapse = ""|""), "")"")
str_extract_all(tolower(column), pat)
</code></pre>
",0,0,71,2018-06-18 11:00:32,https://stackoverflow.com/questions/50907974/extract-variations-of-a-string-from-r-column
Split mixed string into columns in R,"<p>A newbie to textmining analysis and R coding. </p>

<p>I have 200 genes with mixed string. I want to split them and paste strings (eg, cadherins, orphan receptors) in one column and numbers (eg, 2/3), number+string (eg, 7D, 7TM) in another column. 
I used strssplit to split the words. Please any suggestion on how to parse them would be helpful.  </p>

<pre><code>example:
 &gt; Genes &lt;- c(""7D cadherins"", ""7TM orphan receptors"", ""7TM orphan receptors RNA18S"", ""28S ribosomal RNAs  RNA28S"", ""45S pre-ribosomal RNAs  RNA45S"", ""5.8S ribosomal RNAs"", ""Actin related protein 2/3 complex”)

Expected result(2nd and 3rd column):

7D cadherins        cadherins       7D 
7TM orphan receptors        orphan receptors        7TM   
18S ribosomal RNAs  RNA18S  ribosomal RNAs  RNA18S  18S RNA18S
28S ribosomal RNAs  RNA28S  ribosomal RNAs  RNA28S  28S  RNA28S
45S pre-ribosomal RNAs  RNA45S  pre-ribosomal RNAs      45S  RNA45S
5.8S ribosomal RNAs ribosomal RNAs  5.8S
Actin related protein 2/3 complex   Actin related protein complex    2/3 
</code></pre>
","r, string, split, text-mining","<p>Using <code>strsplit</code> to split the names, <code>grep</code> to detect words with or without numbers and <code>paste</code> to collapse the words. Put everithing in a function to avoid repetition:</p>

<pre><code>wordS &lt;- function(x, invert = TRUE) {
  clean &lt;- gsub( '[[:space:]]+', ' ', x )  # to remove extra spaces
  split &lt;- strsplit( clean, ' ' )
  detec &lt;- lapply( split, function(y) grep('[0-9]', y, invert = invert, value = TRUE) )
  words &lt;- sapply( detec, paste, collapse = ' ' )
  return( words )
}

data.frame(
  Gene = Genes,
  column2 = wordS(Genes),
  column3 = wordS(Genes, invert = FALSE)
)

                               Gene                       column2    column3
1                      7D cadherins                     cadherins         7D
2              7TM orphan receptors              orphan receptors        7TM
3       7TM orphan receptors RNA18S              orphan receptors 7TM RNA18S
4         28S ribosomal RNAs RNA28S                ribosomal RNAs 28S RNA28S
5     45S pre-ribosomal RNAs RNA45S            pre-ribosomal RNAs 45S RNA45S
6               5.8S ribosomal RNAs                ribosomal RNAs       5.8S
7 Actin related protein 2/3 complex Actin related protein complex        2/3
</code></pre>
",1,1,127,2018-06-18 12:37:15,https://stackoverflow.com/questions/50909699/split-mixed-string-into-columns-in-r
Grouping word frequency,"<p>I'm trying to text mine social policy cases.  Each case is in a row and I want to know how many of my cases refer to say Universal Credit or some new unknown issue.  I'm starting with word frequencies.</p>

<p>I've got as far as getting my data into this format. Basically ID takes value 1,2 or 3 as there are three case studies.  Word takes value of dog or cat.</p>

<pre><code>dd &lt;- read.table(text=""ID       Word
1   dog
1   cat
2   cat
2   cat
3   cat"", header=TRUE)
</code></pre>

<p>I want a count of unique ID for each Word i.e there are three case studies that mention cats</p>

<pre><code>Word Count
cat      3
dog      1
</code></pre>

<p>I'm not even sure if this is now a text mining question or whether it's some basic group or count question.</p>
","r, text-mining","<p>I think you can do this with a simple dplyr call. For example</p>

<pre><code>library(dplyr)
dd %&gt;% group_by(Word) %&gt;% summarize(Count=n_distinct(ID))
#   Word  Count
#    &lt;fct&gt; &lt;int&gt;
# 1 cat       3
# 2 dog       1
</code></pre>
",0,0,376,2018-06-20 19:12:14,https://stackoverflow.com/questions/50955457/grouping-word-frequency
How to extract all words after the nth word from string in R?,"<p>The first column in my data.frame consists of strings, and the second column are unique keys. </p>

<p>I want to extract all words after the nth word from each string, and if the string has &lt;= n words, extract the entire string.</p>

<p>I have over 10k rows in my data.frame and was wondering if there is a quick way of doing this other than using for loops?</p>

<p>Thanks.</p>
","r, text-mining, data-cleaning","<p>How about the following:</p>

<pre><code># Generate some sample data
library(tidyverse)
df &lt;- data.frame(
    one = c(""Entries from row one"", ""Entries from row two"", ""Entries from row three""),
    two = runif(3))


# Define function to extract all words after the n=1 word 
# (or return the full string if n &gt; # of words in string)
crop_string &lt;- function(ss, n) {
    lapply(strsplit(as.character(ss), ""\\s""), function(v)
        if (length(v) &gt; n) paste(v[(n + 1):length(v)], collapse = "" "")
        else paste(v, collapse = "" ""))
}

# Let's crop strings from column one by removing the first 3 words (n = 3)
n &lt;- 3;
df %&gt;%
    mutate(words_after_n = crop_string(one, n))
#                     one       two words_after_n
#1   Entries from row one 0.5120053           one
#2   Entries from row two 0.1873522           two
#3 Entries from row three 0.0725107         three


# If n &gt; # of words, return the full string
n &lt;- 10;
df %&gt;%
    mutate(words_after_n = crop_string(one, n))
#                     one       two          words_after_n
#1   Entries from row one 0.9363278   Entries from row one
#2   Entries from row two 0.3024628   Entries from row two
#3 Entries from row three 0.6666226 Entries from row three
</code></pre>
",2,0,869,2018-06-20 21:01:42,https://stackoverflow.com/questions/50956959/how-to-extract-all-words-after-the-nth-word-from-string-in-r
Extract text from epub in Python,"<p>I have written following code to extract the words of a ebook and add them to a corpus for text-mining purposes.   </p>

<pre><code># loading the german corpus
from ebooklib import epub
import ebooklib
import os
import nltk
input_path = r""C:\Users\jzeh\Desktop\Directory""
german_corpus = []
book = epub.read_epub(os.path.join(input_path,'grimms-maerchen.epub'))
for doc in book.get_items():
    german_corpus += str(doc.content)
    german_corpus = [w.lower() for w in nltk.word_tokenize(german_corpus)]
</code></pre>

<p>Unfortunately running the code gives me the error: </p>

<pre><code>TypeError  ---&gt; 12     german_corpus = [w.lower() for w in nltk.word_tokenize(german_corpus)]
TypeError: expected string or bytes-like object
</code></pre>

<p>Could anyone tell me, what I am missing?</p>
","python, nltk, text-mining, epub","<p><code>nltk.word_tokenize</code> takes a string as an input, you have passed it a list. If I understand correctly, I think you want this:</p>

<pre><code>...

for doc in book.get_items():
    doc_content = str(doc.content)
    for w in nltk.word_tokenize(doc_content):
        german_corpus.append(w.lower())
</code></pre>
",2,3,6968,2018-06-21 09:58:14,https://stackoverflow.com/questions/50965671/extract-text-from-epub-in-python
Cosine Similarity with two Term Frequency vectors in R,"<p>I made using<code>tm</code> in <code>R</code> a DocumentTermMatrix (dtm). if I understand correctly, this matrix displays for each document how often each possible term occurs. Now I can inspect this matrix and I get</p>

<pre><code>    Terms
Docs     can design door easy finish include light provide use water
  176004   1      2   11    8      0       3     3       4   4     4
  181288   1      2   11    8      0       2     3       4   4     4
  182465   4      4    0    2      0       0    42      13   6     0
etc.
</code></pre>

<p>How can I now retrieve the vector of (for example) document 181288? So I will get something like</p>

<pre><code>1      2   11    8      0       2     3       4   4     4 ………
</code></pre>

<p>Also, it says my dtm's sparsity is 100%, is it (by approximation) 100% empty?</p>
","r, text-mining, data-analysis, tm, cosine-similarity","<p>To retrieve your vector you can do it in multiple ways.</p>

<p>simple, but not recommended unless for quick test:</p>

<pre><code>my_doc &lt;- inspect(dtm[dtm$dimnames$Docs == ""181288"",])
</code></pre>

<p>Doing it like this limits you to what <code>inspect</code> does and this only shows a maximum of 10 documents. </p>

<p>Better way, create a selection list if you want to and filter the dtm. This keeps the sparse matrix format, then transform what you need into a data.frame for further manipulation if needed.</p>

<pre><code>my_selection &lt;- c(""181288"", ""182465"")

# selection in case of dtm
my_dtm_selection &lt;- dtm[dtm$dimnames$Docs %in% my_selection, ]

# selection in case of tdm
my_tdm_selection &lt;- tdm[, tdm$dimnames$Docs %in% my_selection]

# create data.frame with document names as first column, followed by the terms
my_df_selection &lt;- data.frame(docs = Docs(my_dtm_selection), as.matrix(my_dtm_selection))
</code></pre>

<p>The answer to your second question: yes, almost empty. Or better framed, a lot of empty cells. But you might have more data than you think if you have a lot of documents and terms.  </p>
",1,0,385,2018-06-21 10:00:04,https://stackoverflow.com/questions/50965708/cosine-similarity-with-two-term-frequency-vectors-in-r
How to do bi-grams topic modeling using tidy text in r?,"<p>So I tried using the tidytext package to do bigrams topic modeling, by following the steps on the tidytext website: <a href=""https://www.tidytextmining.com/ngrams.html"" rel=""nofollow noreferrer"">https://www.tidytextmining.com/ngrams.html</a>.</p>

<p>I was able to get to the ""word_counts"" part, where R calculates each bi-gram's frequency. </p>

<p>""word_counts"" returned a the following:</p>

<pre><code>   customer_id       word          n
   &lt;chr&gt;            &lt;chr&gt;        &lt;int&gt;
 1 00000001234  sample text        45
 2 00000002345  good morning       30
 3 00000003456  happy friday       24
</code></pre>

<p>The next step was to put information from above into a dtm format</p>

<p>My code is below:</p>

<pre><code>lda_dtm &lt;- word_counts %&gt;%
  cast_dtm(customer_id, word, n)
</code></pre>

<p>A warning message was raised:</p>

<pre><code>Warning message:
Trying to compute distinct() for variables not found in the data:
- `row_col`, `column_col`
This is an error, but only a warning is raised for compatibility reasons.
The operation will return the input unchanged. 
</code></pre>

<p>But the ""lda_dtm"" looks like its in the right format.</p>

<pre><code>lda_dtm
&lt;&lt;DocumentTermMatrix (documents: 9517, terms: 341545)&gt;&gt;
Non-/sparse entries: 773250/3249710515
Sparsity           : 100%
Maximal term length: NA
Weighting          : term frequency (tf)
</code></pre>

<p>However when I tried to run lda, it did not work.</p>

<pre><code>burnin &lt;- 4000
iter &lt;- 300
thin &lt;- 500
seed &lt;-list(2003,5,63,100001,765)
nstart &lt;- 5
best &lt;- TRUE
k &lt;- 6

out_LDA &lt;- LDA(lda_dtm, 
                            k = k, 
                            method=""Gibbs"", 
                            control = list(nstart=nstart, 
                                           seed = seed, 
                                           best=best, 
                                           burnin = burnin, 
                                           iter = iter, 
                                           thin = thin))
</code></pre>

<p>The following warning was raised:</p>

<pre><code>Error in seq.default(CONTROL_i@iter, control@burnin + control@iter, by = control@thin) : 
  wrong sign in 'by' argument
</code></pre>

<p>I don't see a topic modeling tutorial on the tidy text website for <strong>bi-grams</strong>, the tutorial was specifically for <strong>unigrams</strong>. How should I adjust the format for it to work with <strong>bi-grams</strong>?</p>
","r, text-mining, n-gram, topic-modeling, tidytext","<p>1: The message you get from cast_dtm actually comes from cast_sparse. There are two issues, #120 and #121, on github that deal with this. At the moment this is fixed in the package on github but this is not releases yet to cran.</p>

<p>If you want to, you can install it from github with <code>devtools::install_github(""juliasilge/tidytext"")</code>.</p>

<p>2: The error you get from LDA has nothing to do with 1. If you just run <code>out_LDA &lt;- LDA(lda_dtm, k = k)</code> LDA will run just fine. The problem lies in your control option <code>thin</code>. This should be less than or equal to the iter paramater. In your case this is set as 500, while iter is at 300. Hence the error. You can see the error appearing when thin is 1 higher than iter.</p>
",5,2,1484,2018-06-29 17:22:22,https://stackoverflow.com/questions/51106727/how-to-do-bi-grams-topic-modeling-using-tidy-text-in-r
R: compare words histogram by document,"<p>I am looking for a method to compare word histograms by documents belong to a folder corpus with several documnets. I did try to made:</p>

<pre><code>freq &lt;- sort(colSums(as.matrix(dtm), group=Docs), decreasing=TRUE) 
</code></pre>

<p>Also and did try in ggplot option:</p>

<pre><code>p &lt;- p + geom_bar(stat=""identity"") +   facet_wrap(~ Docs)   
</code></pre>

<p>but lamentably I got error.</p>

<p>An modified example of my code is below, but lamentably my 3 documents are plot like one and nor segmented by Docs:</p>

<pre><code>c= c(""hola como  hola como  hola como"", ""hola me fui hola me fui hola me fui hola me fui"", ""hola como estas hola como estas hola como estas"" )
corpus= VCorpus(VectorSource(c))

dtm &lt;- DocumentTermMatrix(corpus)

m &lt;- as.matrix(dtm)   
m 
freq &lt;- sort(colSums(as.matrix(dtm)), decreasing=TRUE)  
wf &lt;- data.frame(word=names(freq), freq=freq)   

p &lt;- ggplot(subset(wf, freq&gt;1), aes(word, freq))    
p &lt;- p + geom_bar(stat=""identity"") 
p &lt;- p + theme(axis.text.x=element_text(angle=45, hjust=1)) 
p   
</code></pre>
","r, text, text-mining, corpus","<p>The way you create wf means you are losing your docs and they are not available for ggplot2. I create wf as a combination of the document names and the dtm. (Be careful here if you have a big corpus.) Then I turn wf in a long format so ggplot is very nice format for ggplot2. Then just docs in whatever way you need when you create your plots. In the example below I split the graphs between the docs.</p>

<pre><code>library(tm)

c= c(""hola como  hola como  hola como"", ""hola me fui hola me fui hola me fui hola me fui"", ""hola como estas hola como estas hola como estas"" )
corpus= VCorpus(VectorSource(c))

dtm &lt;- DocumentTermMatrix(corpus)

wf &lt;- data.frame(docs=Docs(dtm), as.matrix(dtm)) 

library(tidyr)
wf &lt;- wf %&gt;% gather(key = ""terms"", value = ""freq"", -docs)

library(ggplot2)
ggplot(wf, aes(terms, freq)) + 
  geom_bar(stat=""identity"") +
  facet_wrap(~ docs) + 
  theme(axis.text.x=element_text(angle=45, hjust=1)) 
</code></pre>

<p><a href=""https://i.sstatic.net/EnUNB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EnUNB.png"" alt=""enter image description here""></a></p>
",0,0,255,2018-07-01 07:47:04,https://stackoverflow.com/questions/51121368/r-compare-words-histogram-by-document
Is it possible to convert words stored in a matrix of N*1 into single sentence in R?,"<p>I have a matrix of 1196*1 with each row containing one word. I need to write these words into a single sentence.   </p>

<pre><code>I  
Have  
A  
Matrix 
</code></pre>

<p>Aim: <code>I Have A Matrix.</code>  </p>
","r, text-mining","<p>Here is your matrix as a reproducible example:</p>

<pre><code>my_matrix = matrix(c(""I"", ""Have"", ""A"", ""Matrix""), ncol = 1)
</code></pre>

<p>Here is a method to make it a sentence:</p>

<pre><code>&gt; paste(my_matrix, collapse = "" "")
[1] ""I Have A Matrix""
</code></pre>
",2,1,48,2018-07-03 12:32:02,https://stackoverflow.com/questions/51154745/is-it-possible-to-convert-words-stored-in-a-matrix-of-n1-into-single-sentence-i
Trying to get random forest for text classification running,"<p>I'm trying to get a randomForest running for a school project. I am trying to build a test classifier, that predicts a category (column label) based on some text.</p>

<p>Curretly I am stuck as there seems to be a problem with my document term matrix.
This is the error:</p>

<pre><code>&gt; rfmodel &lt;- randomForest(df$label, data = events_dtm)
Error in if (n == 0) stop(""data (x) has 0 rows"") : 
  argument is of length zero
</code></pre>

<p>This is what the code currently looks like. The data is representative.</p>

<pre><code>library(tidyverse)
library(tidytext)
library(stringr)
library(caret)
library(tm)
library(dplyr)
library(randomForest)

text = c(""this is a random text"",
         ""another rnd text"",
         ""hi there"",
         ""not so rnd"",
         ""what's that?"",
         ""kinda boring"",
         ""this is a random text"",
         ""another rnd text"",
         ""hi there"",
         ""not so rnd"",
         ""what's that?"",
         ""kinda boring"")

label = c(1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2)

df &lt;- data.frame(text= text, label=label)
df$label &lt;- as.factor(df$label)
df$text &lt;- as.character(df$text)

df$ID &lt;- seq.int(nrow(df))

df &lt;- df[1:5,]

as_tibble(df) %&gt;%
  mutate(text = as.character(text)) -&gt; type

data(""stop_words"")
type %&gt;%
  unnest_tokens(output = word, input = text) %&gt;%
  anti_join(stop_words) %&gt;%
  mutate(word = SnowballC::wordStem(word)) -&gt; type_tokens


type_tokens %&gt;%
  count(ID, word) %&gt;%
  cast_dtm(document = ID, term = word, value = n,
           weighting = weightTfIdf) -&gt; type_dtm


print(type_dtm)

rfmodel &lt;- randomForest(df$label, data = type_dtm)

print(rfmodel)

dfT &lt;- data.frame(text= text)
dfT$ID &lt;- seq.int(nrow(dfT))

as_tibble(dfT) %&gt;%
  mutate(text = as.character(text)) -&gt; typeT

typeT %&gt;%
  unnest_tokens(output = word, input = text) -&gt; typeT

typeT %&gt;%
  count(ID, word) %&gt;%
  cast_dtm(document = ID, term = word, value = n,
           weighting = weightTfIdf) -&gt; typeT

pred_test &lt;- predict(rfmodel, newdata = dfT, type = ""class"")

print(pred_test)
</code></pre>

<p>Since I am rather new to both random forest and R there probably is a conceptual mistake.
Any idea how to solve the problem?</p>
","r, text-mining, random-forest","<p>There are several issues with your code:</p>

<p>First your randomForest call:
<code>rfmodel &lt;- randomForest(df$label, data = type_dtm)</code> </p>

<p>You cannot call df$label and specify data type_dtm where the label is not present.
Secondly sparsematrices are not accepted by randomForest. You need to do something with that. You can solve that by merging the label info with the type_dtm. Search SO on how to do that. Thirdly, you tell randomForest that y = label, but either you need to give a formula interface like label ~ . and specify data = .... or you need to specify y and x as y = label and x = ... see <code>?randomForest</code> for more info.</p>

<p>All of these issues combined result in this error you receive. Start solving them one by one and when you get stuck again, post a question. Your code is a good start of creating a reproducible example so +1 for that effort. </p>
",0,1,475,2018-07-03 17:34:14,https://stackoverflow.com/questions/51160176/trying-to-get-random-forest-for-text-classification-running
R: How get file name with Quanteda: char_segment,"<p>I am using  char_segment from Quanteda library to separate multiple documents from one file separatted by a pattern, this command works great and easily! (I did try with str_match and strsplit but without success).</p>

<p>Lamentably I am unable to get the filename as a Variable, this is key to next analysis.<a href=""https://i.sstatic.net/dl8jA.png"" rel=""nofollow noreferrer"">example</a></p>

<p>Example of my commands:</p>

<pre><code>Library(quanteda)
doc &lt;- readtext(paste0(""PATH/*.docx""))
View(doc)

docc=char_segment(doc$text,  pattern = "","", remove_pattern = TRUE)
</code></pre>

<p>Please any suggestion or other options to split documents are welcome.</p>
","r, split, text-mining, quanteda","<p>Simply get the list of your docx files first, it will yield the name of the files. Then run the char_segment function on them them by a lapply, loop, or purrr::map()</p>

<p>The following code assumes that your target documents are stored in a directory called ""docx"" within your working directory.</p>

<pre><code>library(quanteda)
library(readtext)  ## Remember to include in your posts the libraries required to replicate the code.


list_of_docx &lt;- list.files(path = ""./docx"", ## Looks inside the ./docx directory
                       full.names = TRUE,   ## retrieves the full path to the documents
                       pattern = ""[.]docx$"", ## retrieves al documents whose names ends in "".docx""
                       ignore.case = TRUE)  ## ignores the letter case of the document's names
</code></pre>

<h2>Preparing the for loop</h2>

<pre><code>df_docx &lt;- data.frame() ## Create an empty dataframe to store your data

for (d in seq_along(list_of_docx)) {  ## Tell R to run the loop/iterate along the number of elements within thte list of doccument paths
    temp_object &lt;-readtext(list_of_docx[d])
    temp_segmented_object &lt;- char_segment(temp_object$text, pattern = "","", remove_pattern = TRUE)
    temp_df &lt;- as.data.frame(temp_segmented_object)
    colnames(temp_df) &lt;- ""segments""
    temp_df$title &lt;- as.character(list_of_docx[d])  ## Create a variable with the title of the source document
    temp_df &lt;- temp_df[, c(""title"", ""segments"")]
    df_docx &lt;- rbind(df_docx, temp_df) ## Append each dataframe to the previously created empty dataframe
    rm(temp_df, temp_object, d)
    df_docx
 }


head(df_docx)
</code></pre>
",0,0,167,2018-07-05 01:10:06,https://stackoverflow.com/questions/51182222/r-how-get-file-name-with-quanteda-char-segment
R - NLP - text cleaning,"<p>I am new to text mining and, currently, I stuck with this kind of pattern </p>

<pre><code>pattern = c(
    ""&lt;f0&gt;&lt;U+009F&gt;&lt;U+0098&gt;&lt;U+00AD&gt;"", 
    ""&lt;f0&gt;&lt;U+009F&gt;&lt;U+0099&gt;&lt;U+008F&gt;"",
    ""&lt;f0&gt;&lt;U+009F&gt;&lt;U+008F&gt;&lt;U+00BF&gt; "",
    ""&lt;f0&gt;&lt;U+009F&gt;&lt;U+0098&gt;&lt;U+0082&gt;"", 
    "" &lt;f0&gt;&lt;U+009F&gt;&lt;U+00A4&gt;&lt;U+00B7&gt;"",
    ""  &lt;f0&gt;&lt;U+009F&gt;&lt;U+008F&gt;&lt;U+00BD&gt;&lt;U+200D&gt;&lt;U+2640&gt;&lt;U+FE0F&gt;\r\nBody"",
    "" &lt;f0&gt;&lt;U+009F&gt;&lt;U+00A4&gt;&lt;U+00A3&gt;"", 
    "" &lt;f0&gt;&lt;U+009F&gt;&lt;U+0099&gt;&lt;U+0084&gt; "", 
    ""  &lt;f0&gt;&lt;U+009F&gt;&lt;U+0099&gt;&lt;U+0084&gt;"",
    ""  &lt;f0&gt;&lt;U+009F&gt;&lt;U+0099&gt;&lt;U+0083&gt;"",
      ""&lt;f0&gt;&lt;U+009F&gt;&lt;U+0098&gt;&lt;U+00B4&gt;"",
     ""Hello"")
</code></pre>

<p>I would like to receive only pattern = ""Hello"" and exclude all the other text.</p>

<p>I tried the following but I failed immediately: </p>

<pre><code>gsub(c, ""&lt;f0&gt;&lt;U+00F&gt;&lt;U+[0-9]&gt;&lt;U+[a-zA-Z0-9]&gt;*, replacement = """")
</code></pre>

<p>So, I tried to break it down:</p>

<pre><code>a = gsub(c, pattern = ""&lt;f0&gt;"", replacement = """")
</code></pre>

<p>->result <code>&lt;fo&gt;</code> drops, so it is a good sign but when I do the next step</p>

<pre><code>gsub(a, pattern = ""&lt;U+009F&gt;"", replacement = """")
</code></pre>

<p>->result: <code>&lt;U+009F&gt;</code> remains.
Do you have some ideas?
I appreciate any kind of suggestions!
Thanks in advance!</p>
","r, nlp, text-mining","<p>Two ways to clean your text. There were no criteria given to allow removal of ""Body"".</p>

<pre><code>x &lt;- pattern # to avoid ambiguity in function parameters

# by finding words longer than two letters (so not 'a' or 'I' either)
words &lt;- unlist(regmatches(x, gregexpr(""\\b[[:alpha:]]{2,}\\b"", x, perl=TRUE)))
words

#[1] ""Body""  ""Hello""

# by removing unwanted characters and character sequences
cleaned &lt;- gsub(""(&lt;[^&gt;]{0,}&gt;|\\r|\\n)"", """", x, perl=TRUE)
# and removing leading and trailing spaces
cleaned &lt;- gsub(""^ {1,}| {1,}$"", """", cleaned, perl=TRUE)
cleaned[cleaned != """"]

#[1] ""Body""  ""Hello""
</code></pre>
",1,1,392,2018-07-08 14:52:31,https://stackoverflow.com/questions/51233157/r-nlp-text-cleaning
R - Finding top words in each NRC sentiment and emotion using syuzhet package,"<p>Snapshot of the dataset:</p>

<p><a href=""https://i.sstatic.net/yVmeA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yVmeA.png"" alt=""enter image description here""></a></p>

<p>I'm getting following chart:</p>

<p><a href=""https://i.sstatic.net/S7fql.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S7fql.png"" alt=""enter image description here""></a></p>

<p>Here is the code:</p>

<pre><code>library(tidytext)
library(syuzhet)

lyrics$lyric &lt;- as.character(lyrics$lyric)

tidy_lyrics &lt;- lyrics %&gt;% 
  unnest_tokens(word,lyric)

song_wrd_count &lt;- tidy_lyrics %&gt;% count(track_title)

lyric_counts &lt;- tidy_lyrics %&gt;%
  left_join(song_wrd_count, by = ""track_title"") %&gt;% 
  rename(total_words=n)

lyric_sentiment &lt;- tidy_lyrics %&gt;% 
  inner_join(get_sentiments(""nrc""),by=""word"")

lyric_sentiment %&gt;% 
count(word,sentiment,sort=TRUE) %&gt;%
group_by(sentiment)%&gt;%top_n(n=10) %&gt;% 
ungroup() %&gt;%
  ggplot(aes(x=reorder(word,n),y=n,fill=sentiment)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~sentiment,scales=""free"") + 
  coord_flip()
</code></pre>

<p>The issue is that I'm not sure if the result I'm getting is correct or not. For instance, you can see 'bad' is part of multiple emotions. Also, if we inspect <code>lyric_sentiment</code>, we'd see that word 'shame' is present four times for 'Tim McGraw'. In reality it appears only twice in this song.</p>

<p><strong>What's the right approach?</strong></p>
","r, text-mining, sentiment-analysis, tidytext","<p>You are doing it correct. nrc sentiments can place words in multiple sentiment sections. You can see this in the following example. You can also look up values on the <a href=""http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm"" rel=""nofollow noreferrer"">nrc homepage</a></p>

<pre><code>library(dplyr)
library(tidytext)

nrc &lt;- get_sentiments(""nrc"")
nrc %&gt;% filter(word %in% c(""bad"", ""shame""))
# A tibble: 9 x 2
  word  sentiment
  &lt;chr&gt; &lt;chr&gt;    
1 bad   anger    
2 bad   disgust  
3 bad   fear     
4 bad   negative 
5 bad   sadness  
6 shame disgust  
7 shame fear     
8 shame negative 
9 shame sadness  
</code></pre>
",2,1,1947,2018-07-11 12:51:27,https://stackoverflow.com/questions/51285942/r-finding-top-words-in-each-nrc-sentiment-and-emotion-using-syuzhet-package
Remove all sentences where number to character ratio is greater than the average in the text,"<p>Is it possible to find and delete all sentences containing a higher number to character ratio?
I created the following function to calculate the ratio in a given string:</p>

<pre><code>a &lt;- ""1aaaaaa2bbbbbbb3""

Num_Char_Ration &lt;- function(string){
length(unlist(regmatches(string,gregexpr(""[[:digit:]]"",string))))/nchar(as.character(string))
}
Num_Char_Ration(a)
#0.1875
</code></pre>

<p>The task is now to find a method to calculate the ratio for a sentence(so for a character sequence between ending with a ""."") and then to delete sentences with a higher ratio from the text. For example:</p>

<pre><code>input:
a &lt;- "" aa111111. bbbbbb22. cccccc3."" 
output:
#""bbbbbb22. cccccc3.""
</code></pre>
","r, regex, text, substring, text-mining","<p>I would use <code>stringr</code> package to count digits and characters:</p>

<pre><code># Original data
input &lt;- "" aa111111. bbbbbb22. cccccc3."" 
# Split by . 
inputSplit &lt;- strsplit(input, ""\\."")[[1]]

# Count digits and all alnum in splitted string
counts &lt;- sapply(inputSplit, stringr::str_count, c(""[[:digit:]]"", ""[[:alnum:]]""))

# Get ratios and collapse text back
paste(inputSplit[counts[1, ] / counts[2, ] &lt; 0.5], collapse = ""."")
# [1] "" bbbbbb22. cccccc3""
</code></pre>

<p><code>counts</code> looks like this: </p>

<pre><code># To get ratio between digits and string
# Divide first row by second row
      aa111111  bbbbbb22  cccccc3
[1,]         6         2        1
[2,]         8         8        7
</code></pre>
",4,3,111,2018-07-12 15:34:25,https://stackoverflow.com/questions/51309601/remove-all-sentences-where-number-to-character-ratio-is-greater-than-the-average
finding row-wise important words in text dataframe,"<p>I have a dataframe which looks like this:</p>

<pre><code>sentences &lt;- data.frame(sentences = 
                          c('You can apply for or renew your Medical Assistance benefits online by using COMPASS.',
                            'COMPASS is the name of the website where you can apply for Medical Assistance and many other services that can help you make ends meet.',
                          'Medical tourism refers to people traveling to a country other than their own to obtain medical treatment. In the past this usually referred to those who traveled from less-developed countries to major medical centers in highly developed countries for treatment unavailable at home.',
                          'Health tourism is a wider term for travel that focus on medical treatments and the use of healthcare services. It covers a wide field of health-oriented, tourism ranging from preventive and health-conductive treatment to rehabilitational and curative forms of travel.',
                          'Medical tourism carries some risks that locally provided medical care either does not carry or carries to a much lesser degree.',
                          'Receiving medical care abroad may subject medical tourists to unfamiliar legal issues. The limited nature of litigation in various countries is a reason for accessbility of care overseas.', 
                          'While some countries currently presenting themselves as attractive medical tourism destinations provide some form of legal remedies for medical malpractice, these legal avenues may be unappealing to the medical tourist.'))
</code></pre>

<p>All I want to do is to find important words in each row and create a new column that should look like this:</p>

<pre><code>sentences$ImpWords &lt;- c(""apply, renew, Medical, Assistance, benefits, online, COMPASS"",
                    ""COMPASS, name, website, apply, Medical, Assistance, services, help, meet"") 

and so forth
</code></pre>

<p>I am not sure how this can be done?</p>

<p>I was trying bag of words, cleaning and preprocessing etc. using various packages such as <code>tm</code>, tidytext etc. But unable to get the desired result.</p>

<p>Is there any alternative possible?</p>
","r, dplyr, text-mining, tidytext","<p>This will achieve what you're after. If you want to remove more words, simply find a bigger/different list (many are available through different packages). Here I've used tm's English stopwords.</p>

<pre><code>library(tm)
stopwords &lt;- stopwords('en')

sentences &lt;- data.frame(sentences = 
                          c('You can apply for or renew your Medical Assistance benefits online by using COMPASS.',
                            'COMPASS is the name of the website where you can apply for Medical Assistance and many other services that can help you make ends meet.',
                            'Medical tourism refers to people traveling to a country other than their own to obtain medical treatment. In the past this usually referred to those who traveled from less-developed countries to major medical centers in highly developed countries for treatment unavailable at home.',
                            'Health tourism is a wider term for travel that focus on medical treatments and the use of healthcare services. It covers a wide field of health-oriented, tourism ranging from preventive and health-conductive treatment to rehabilitational and curative forms of travel.',
                            'Medical tourism carries some risks that locally provided medical care either does not carry or carries to a much lesser degree.',
                            'Receiving medical care abroad may subject medical tourists to unfamiliar legal issues. The limited nature of litigation in various countries is a reason for accessbility of care overseas.', 
                            'While some countries currently presenting themselves as attractive medical tourism destinations provide some form of legal remedies for medical malpractice, these legal avenues may be unappealing to the medical tourist.'))


sentences[,""sentences""] &lt;- sentences[,""sentences""] %&gt;% as.character()


ImpWords &lt;- c()
for (i in 1:nrow(sentences)) {

  originalWords &lt;- gsub('[[:punct:] ]+',' ',sentences[i, ""sentences""]) %&gt;% trimws(.) %&gt;% strsplit(., "" "") 
  lowerCaseWords &lt;- gsub('[[:punct:] ]+',' ',tolower(sentences[i, ""sentences""])) %&gt;% trimws(.) %&gt;% strsplit(., "" "")
  wordsNotInStopWords &lt;- originalWords[[1]][which(!lowerCaseWords[[1]] %in% stopwords)]
  wordsNotInStopWordsGreaterThanThreeChar &lt;- wordsNotInStopWords[which(nchar(wordsNotInStopWords) &gt; 3)]
  ImpWords[i] &lt;- paste(wordsNotInStopWordsGreaterThanThreeChar, collapse = "", "")

}

sentences$ImpWords &lt;- ImpWords
sentences$ImpWords
</code></pre>
",1,0,189,2018-07-21 13:13:25,https://stackoverflow.com/questions/51456436/finding-row-wise-important-words-in-text-dataframe
To replace internet acronyms in a dataframe using dictionary,"<p>I'm working on a text mining project where I'm trying to replace abbreviations, slang words and internet acronyms present in text (In a dataframe column) using a manually prepared dictionary. </p>

<p>The problem I'm facing is the code stops with the first word of the text in the dataframe column and does not replace it with lookup words from dict </p>

<p>Here is the sample dictionary and code I use:</p>

<pre><code>abbr_dict = {""abt"":""about"", ""b/c"":""because""}

def _lookup_words(input_text):
    words = input_text.split()
    new_words = [] 
    for word in words:
        if word.lower() in abbr_dict:
            word = abbr_dict[word.lower()]
        new_words.append(word)
        new_text = "" "".join(new_words) 
        return new_text
df['new_text'] = df['text'].apply(_lookup_words)
</code></pre>

<p>Example Input:</p>

<pre><code>df['text'] =
However, industry experts are divided ab whether a Bitcoin ETF is necessary or not.
</code></pre>

<p>Desired Output:</p>

<pre><code>df['New_text'] =
However, industry experts are divided about whether a Bitcoin ETF is necessary or not.
</code></pre>

<p>Current Output:</p>

<pre><code>df['New_text'] =
However
</code></pre>
","python, pandas, text-mining","<p>You can try as following with using <code>lambda</code> and <code>join</code> along with <code>split</code>:</p>

<pre><code>import pandas as pd

abbr_dict = {""abt"":""about"", ""b/c"":""because""}

df = pd.DataFrame({'text': ['However, industry experts are divided abt whether a Bitcoin ETF is necessary or not.']})

df['new_text'] = df['text'].apply(lambda row: "" "".join(abbr_dict[w] 
                             if w.lower() in abbr_dict else w for w in row.split()))
</code></pre>

<hr>

<p>Or to fix the code above, I think you need to move the <code>join</code> for <code>new_text</code> and <code>return</code> statement outside of the <code>for</code> loop:</p>

<pre><code>def _lookup_words(input_text):
    words = input_text.split()
    new_words = [] 
    for word in words:
        if word.lower() in abbr_dict:
            word = abbr_dict[word.lower()]
        new_words.append(word)
    new_text = "" "".join(new_words) # ..... change here
    return new_text # ..... change here also
df['new_text'] = df['text'].apply(_lookup_words)
</code></pre>
",3,4,1036,2018-07-31 04:07:19,https://stackoverflow.com/questions/51605300/to-replace-internet-acronyms-in-a-dataframe-using-dictionary
Create Co-occurrence matrix with bigrams,"<p>I am looking to create a co-occurrence matrix with bigrams in stead of unigrams from a single string. I am referring the following links</p>

<p><a href=""http://text2vec.org/glove.html"" rel=""nofollow noreferrer"">http://text2vec.org/glove.html</a></p>

<p><a href=""https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html#3_statistical_significance"" rel=""nofollow noreferrer"">https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html#3_statistical_significance</a></p>

<p>I want to create  the matrix and traverse it to create dataset as follows</p>

<pre><code>Trem1     Term2     Score
</code></pre>

<p>The biggest catch being traversing the sentence with bigrams. Any help on this would be great</p>
","r, nlp, sparse-matrix, text-mining, text2vec","<p>Just specify your bigrams and create the co-occurence matrices. Below are some (really) simple examples. Choose 1 package and do everything with that one. Both quanteda and text2vec can use multiple cores / threads. Traversing over the resulting co-occurence matrices can be done with reshape2::melt, like this <code>reshape2::melt(as.matrix(my_cooccurence_matrix))</code>.</p>

<pre><code>txt &lt;- c(""The quick brown fox jumped over the lazy dog."",
         ""The dog jumped and ate the fox."")
</code></pre>

<p>using quanteda to create a feature co-occurrence matrix:</p>

<pre><code>library(quanteda)
toks &lt;- tokens(char_tolower(txt), remove_punct = TRUE, ngrams = 2)
f &lt;- fcm(toks, context = ""document"")

Feature co-occurrence matrix of: 14 by 14 features.
14 x 14 sparse Matrix of class ""fcm""
             features
features      the_quick quick_brown brown_fox fox_jumped jumped_over over_the the_lazy lazy_dog the_dog dog_jumped jumped_and and_ate
  the_quick           0           1         1          1           1        1        1        1       0          0          0       0
  quick_brown         0           0         1          1           1        1        1        1       0          0          0       0
  brown_fox           0           0         0          1           1        1        1        1       0          0          0       0
  fox_jumped          0           0         0          0           1        1        1        1       0          0          0       0
  jumped_over         0           0         0          0           0        1        1        1       0          0          0       0
  over_the            0           0         0          0           0        0        1        1       0          0          0       0
  the_lazy            0           0         0          0           0        0        0        1       0          0          0       0
  lazy_dog            0           0         0          0           0        0        0        0       0          0          0       0
  the_dog             0           0         0          0           0        0        0        0       0          1          1       1
  dog_jumped          0           0         0          0           0        0        0        0       0          0          1       1
  jumped_and          0           0         0          0           0        0        0        0       0          0          0       1
  and_ate             0           0         0          0           0        0        0        0       0          0          0       0
  ate_the             0           0         0          0           0        0        0        0       0          0          0       0
  the_fox             0           0         0          0           0        0        0        0       0          0          0       0
             features
features      ate_the the_fox
  the_quick         0       0
  quick_brown       0       0
  brown_fox         0       0
  fox_jumped        0       0
  jumped_over       0       0
  over_the          0       0
  the_lazy          0       0
  lazy_dog          0       0
  the_dog           1       1
  dog_jumped        1       1
  jumped_and        1       1
  and_ate           1       1
  ate_the           0       1
  the_fox           0       0
</code></pre>

<p>using text2vec to create a feature co-occurrence matrix:</p>

<pre><code>library(text2vec)
i &lt;- itoken(txt)
v &lt;- create_vocabulary(i, ngram = c(2L, 2L))
vectorizer &lt;- vocab_vectorizer(v) 
f2 &lt;- create_tcm(i, vectorizer)

14 sparse Matrix of class ""dgTMatrix""
   [[ suppressing 14 column names ‘the_lazy’, ‘and_ate’, ‘The_quick’ ... ]]

the_lazy    . . . 0.25 1.0 . 0.2 0.3333333 .         .   1.0000000 .         0.5000000 .        
and_ate     . . . .    .   1 .   .         0.5000000 1.0 .         0.3333333 .         0.5000000
The_quick   . . . 0.50 .   . 1.0 0.3333333 .         .   0.2000000 .         0.2500000 .        
brown_fox   . . . .    0.2 . 1.0 1.0000000 .         .   0.3333333 .         0.5000000 .        
lazy_dog.   . . . .    .   . .   0.2500000 .         .   0.5000000 .         0.3333333 .        
jumped_and  . . . .    .   . .   .         0.3333333 0.5 .         0.5000000 .         1.0000000
quick_brown . . . .    .   . .   0.5000000 .         .   0.2500000 .         0.3333333 .        
fox_jumped  . . . .    .   . .   .         .         .   0.5000000 .         1.0000000 .        
the_fox.    . . . .    .   . .   .         .         1.0 .         0.2000000 .         0.2500000
ate_the     . . . .    .   . .   .         .         .   .         0.2500000 .         0.3333333
over_the    . . . .    .   . .   .         .         .   .         .         1.0000000 .        
The_dog     . . . .    .   . .   .         .         .   .         .         .         1.0000000
jumped_over . . . .    .   . .   .         .         .   .         .         .         .        
dog_jumped  . . . .    .   . .   .         .         .   .         .         .         .        
</code></pre>
",1,0,1013,2018-08-01 06:41:14,https://stackoverflow.com/questions/51626867/create-co-occurrence-matrix-with-bigrams
"Python parsing file that has single and double quotes, as well as contractions","<p>I am trying to parse a file where some of the lines may contain a combination of single quotes, double quotes, and contractions.  Each observation includes a string as shown above.  When trying to parse the data, I am running into problems when trying to parse the reviews.  For example:</p>

<pre><code>\'text\' : \'This is the first time I've tried really ""fancy food"" at a...\' 
</code></pre>

<p>or</p>

<pre><code>\'text\' : \'I' be happy to go back ""next hollidy""\' 
</code></pre>
","python, json, parsing, text-mining","<p>Preprocess your string with a simple double replace - first escape all the quotation marks, then replace all the escaped apostrophes with a quotation mark - which will simply invert the escapes, e.g.:</p>

<pre><code># we'll define it as an object to keep the validity
src = ""{\\'text\\' : \\'This is the first time I've tried really \""fancy food\"" at a...\\'}""
# The double escapes are just so we can type it properly in Python.
# It's still the same underneath:
# {\'text\' : \'This is the first time I've tried really ""fancy food"" at a...\'}

preprocessed = src.replace(""\"""", ""\\\"""").replace(""\\'"", ""\"""")
# Now it looks like:
# {""text"" : ""This is the first time I've tried really \""fancy food\"" at a...""}
</code></pre>

<p>Which is now a valid JSON (and a Python dictionary, incidentally) so you can go ahead and parse it:</p>

<pre><code>import json

parsed = json.loads(preprocessed)
# {'text': 'This is the first time I\'ve tried really ""fancy food"" at a...'}
</code></pre>

<p>Or:</p>

<pre><code>import ast

parsed = ast.literal_eval(preprocessed)
# {'text': 'This is the first time I\'ve tried really ""fancy food"" at a...'}
</code></pre>

<p><strong>UPDATE</strong>:</p>

<p>Based on the posted line, you actually have a (valid) representation of a 7-element tuple containing a string representation of a dictionary as its third element, you don't need to preprocess the string at all. What you need is to first evaluate the tuple, then post-process the inner <code>dict</code> with another level of evaluation, i.e.:</p>

<pre><code>import ast

# lets first read the data from a 'input.txt' file so we don't have to manually escape it
with open(""input.txt"", ""r"") as f:
    data = f.read()

data = ast.literal_eval(data)  # first evaluate the main structure
data = data[:2] + (ast.literal_eval(data[2]), ) + data[3:]  # .. and then the inner dict

# this gives you `data` containing your 'serialized' tuple, i.e.:
print(data[4])  # 31.328237,-85.811893
# and you can access the children of the inner dict as well, i.e.:
print(data[2][""types""])  # ['restaurant', 'food', 'point_of_interest', 'establishment']
print(data[2][""opening_hours""][""weekday_text""][3])  # Thursday: 7:00 AM – 9:00 PM
# etc.
</code></pre>

<p>That being said, I'd suggest tracking down whoever is generating data like this and convince them to use some proper form of serialization, even the most basic JSON would be better than this.</p>
",0,-1,1790,2018-08-01 13:56:53,https://stackoverflow.com/questions/51635184/python-parsing-file-that-has-single-and-double-quotes-as-well-as-contractions
How word association mining is generalization of n-gram language model,"<p>I am working on text mining (reading book...) author said <strong>word association mining is actually the generalization of n-gram language model</strong> Can you please tell how word association mining is generalization of n-gram language model.
For Me <strong>word association mining</strong> is finding symptomatic relation (finding co-occurring) words and <strong>n-gram language model</strong> is compare all n-words in query to suggest or return relevant documents. </p>
","data-mining, text-mining, language-model","<p>Association rule mining will try to cover <em>frequent</em> concurrences of arbitrary length.</p>

<p>If you apply this (not just two term correlations) to text, you would indeed find n-grams without a fixed n.</p>
",1,0,146,2018-08-01 17:14:51,https://stackoverflow.com/questions/51638889/how-word-association-mining-is-generalization-of-n-gram-language-model
Why can&#39;t R read the text file,"<p>Try to get R read my text file and do a text mining, but following the steps it's not working, don't know what's wrong. Someone plz help me</p>

<pre><code>library(tm)
setwd(""E://"")
path=""E:/KEYWORDS""
text&lt;-readLines(""KEYWORDS.txt"")
corpus&lt;- Corpus(VectorSource(text))
corpus&lt;- tm_map(corpus,tolower)
corpus&lt;- tm_map(corpus,removePunctuation)
corpus&lt;-tm_map(corpus,stripWhitespace)
corpus&lt;-Corpus(VectorSource(corpus))
tdm =TermDocumentMatrix(corpus,PlainTextDocument)
findFreTerms(tdm,lowfreq=2)
</code></pre>

<p>And it shows:</p>

<pre><code>Warning message:
In tm_map.SimpleCorpus(corpus, removePunctuation) :
transformation drops documents
tdm =TermDocumentMatrix(corpus,PlainTextDocument)
Error: is.list(control) is not TRUE
</code></pre>

<p>And if you do this</p>

<pre><code>str(readLines(""KEYWORDS.txt""))
paste(str(readLines(""KEYWORDS.txt"")),collapse="" "")
text&lt;-paste(str(readLines(""KEYWORDS.txt"")),collapse="" "")
gsub(pattern=""//W"", replace=""  "", text)
text&lt;-gsub(pattern=""//W"",replace="" "",text)
gsub(pattern=""//d"", replace="" "", text)
text&lt;-gsub(pattern=""//d"", replace="" "", text1)
tolower(text)
text&lt;-tolower(text)
text
</code></pre>

<p>It shows the text is null or contains 0 characters
why?</p>
","r, text-mining","<pre><code>tdm =TermDocumentMatrix(corpus,PlainTextDocument)
Error: is.list(control) is not TRUE
</code></pre>

<p>that's because you've given the second parameter to <code>TermDocumentMatrix</code> as <code>PlainTextDocument</code> rather than a list of control arguments. Read the documentation for TermDocumentMatrix to see what is a valid set of control arguments.</p>

<p>You say you are doing this by ""following the steps"" but you should understand the steps first.</p>
",3,-1,136,2018-08-10 11:32:05,https://stackoverflow.com/questions/51785587/why-cant-r-read-the-text-file
Extracting specific segments from PDF document,"<p>I have a few research papers in pdf format and I want to extract just the introduction/background etc from the paper. also, I can only use python. can someone please help?</p>
","python-3.x, text-mining, pdf-extraction","<p>I got help, right here, with something similar a couple weeks back.  It can be easy, or VERY HARD, to work with PDF files, and there are all different kinds of PDF files.  Having said that, you should consider converting all PDF files to text files.  Try the code sample below.</p>

<p>First, convert PDFs to text.</p>

<pre><code>from io import StringIO
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage
import os
import sys, getopt

#converts pdf, returns its text content as a string
def convert(fname, pages=None):
    if not pages:
        pagenums = set()
    else:
        pagenums = set(pages)

    output = io.StringIO()
    manager = PDFResourceManager()
    converter = TextConverter(manager, output, laparams=LAParams())
    interpreter = PDFPageInterpreter(manager, converter)

    infile = open(fname, 'rb')
    for page in PDFPage.get_pages(infile, pagenums):
        interpreter.process_page(page)
    infile.close()
    converter.close()
    text = output.getvalue()
    output.close
    return text 

#converts all pdfs in directory pdfDir, saves all resulting txt files to txtdir
def convertMultiple(pdfDir, txtDir):
    if pdfDir == """": pdfDir = os.getcwd() + ""\\"" #if no pdfDir passed in 
    for pdf in os.listdir(pdfDir): #iterate through pdfs in pdf directory
        fileExtension = pdf.split(""."")[-1]
        if fileExtension == ""pdf"":
            pdfFilename = pdfDir + pdf 
            text = convert(pdfFilename) #get string of text content of pdf
            textFilename = txtDir + pdf + "".txt""
            textFile = open(textFilename, ""w"") #make text file
            textFile.write(text) #write text to text file

# set paths accordingly:
pdfDir = ""C:/your_path_here/PDF_in/""
txtDir = ""C:/your_path_here/TEXT_out/""
convertMultiple(pdfDir, txtDir)
</code></pre>

<p>Second, look for all text between a beginning tag (""New York State Real Property Law"") and an ending tag (""common elements of the property."").</p>

<pre><code># Loop through all TEXT files in a folder
# Pull out all text between two anchors: ""New York State Real Property Law"" &amp; ""common elements of the property.""
import re
import os
myRegex=re.compile(""New York State Real Property Law.*?common elements of the property\."",re.DOTALL)
for foldername,subfolders,files in os.walk(r""C:/your_path_here/text_files/""):
    for file in files:
        print(file)
        object=open(os.path.join(foldername,file))
        Text=object.read()
        for subText in myRegex.findall(Text):
            print(subText)

object.close()
</code></pre>

<p>Perhaps you can do all the work without converting PDFs to text files, but I haven't found any way to do it.  </p>
",0,1,2139,2018-08-12 09:49:01,https://stackoverflow.com/questions/51807658/extracting-specific-segments-from-pdf-document
From pdf text to tidy dataframe with file names in document column,"<p>I want to analyse text from almost 300 pdf documents. Now I used the <code>pdftools</code> and <code>tm</code>, <code>tidytext</code> packages to read the text, coverted it to a corpus, then to a document-term-matrix and I finally want to structure it in a tidy dataframe.</p>

<p>I've got a couple questions:</p>

<ul>
<li>How do I get rid of page data (at the top and/or bottom of every pdf page)</li>
<li>I would rather want the filenames as the values in the <code>document</code> column instead of indexed numbers.</li>
<li>The following code contents only 2 pdf files for reproducibility. When I run all my files I get 294 documents in my <code>corpus</code> object, but when I tidy it I seem to loose some files because <code>converted %&gt;% distinct(document)</code> gives 275 back. I wonder why that is.</li>
</ul>

<p>I've got the The following reproducible script: </p>

<pre><code>library(tidyverse)
library(tidytext)
library(pdftools)
library(tm)
library(broom)

# Create a temporary empty directory 
# (don't worry at the end of this script I'll remove this directory and its files)

dir.create(""~/Desktop/sample-pdfs"")

# Fill directory with 2 pdf files from my github repo

download.file(""https://github.com/thomasdebeus/colourful-facts/raw/master/projects/sample-data/'s-Gravenhage_coalitieakkoord.pdf"", destfile = ""~/Desktop/sample-pdfs/'s-Gravenhage_coalitieakkoord.pdf"")
download.file(""https://github.com/thomasdebeus/colourful-facts/raw/master/projects/sample-data/Aa%20en%20Hunze_coalitieakkoord.pdf"", destfile = ""~/Desktop/sample-pdfs/Aa en Hunze_coalitieakkoord.pdf"")

# Create vector of file paths

dir &lt;- ""~/Desktop/sample-pdfs""
pdfs &lt;- paste(dir, ""/"", list.files(dir, pattern = ""*.pdf""), sep = """")

# Read the text from pdf's with pdftools package

pdfs_text &lt;- map(pdfs, pdf_text)

# Convert to document-term-matrix

converted &lt;- Corpus(VectorSource(pdfs_text)) %&gt;%
          DocumentTermMatrix()

# Now I want to convert this to a tidy format

converted %&gt;%
          tidy() %&gt;%
          filter(!grepl(""[0-9]+"", term))
</code></pre>

<p>With the following output:</p>

<pre><code># A tibble: 5,305 x 3
   document term           count
   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
 1 1        aan              158
 2 1        aanbesteding       2
 3 1        aanbestedingen     1
 4 1        aanbevelingen      1
 5 1        aanbieden          3
 6 1        aanbieders         1
 7 1        aanbod             8
 8 1        aandacht          16
 9 1        aandachtspunt      3
10 1        aandeel            1
# ... with 5,295 more rows
</code></pre>

<p>This seems to work out nicely but I would rather want the filenames (<code>""'s-Gravenhage""</code> and <code>""Aa en Hunze""</code>) as the values in the document column instead of indexed numbers. How do I do this?</p>

<p>Desired output:</p>

<pre><code># A tibble: 5,305 x 3
   document      term           count
   &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt;
 1 's-Gravenhage aan              158
 2 's-Gravenhage aanbesteding       2
 3 's-Gravenhage aanbestedingen     1
 4 's-Gravenhage aanbevelingen      1
 5 's-Gravenhage aanbieden          3
 6 's-Gravenhage aanbieders         1
 7 's-Gravenhage aanbod             8
 8 's-Gravenhage aandacht          16
 9 's-Gravenhage aandachtspunt      3
10 's-Gravenhage aandeel            1
# ... with 5,295 more rows
</code></pre>

<p>Delete downloaded files and its directory from desktop running the following line:</p>

<pre><code>unlink(""~/Desktop/sample-pdfs"", recursive = TRUE)
</code></pre>

<p>All help is much appreciated! 💐</p>
","r, pdf, text-mining, corpus, tidytext","<p>You can read the documents straight into a corpus with tm. the reader readPDF uses pdftools as an engine. No need to first create a set of text, put it through a corpus to get your output. I created 2 examples. The first one in line with what you were doing, but first going through a corpus. The second purely based on tidyverse + tidytext. No need for switching between tm, tidytext etc.</p>

<p>The differences in number of tokens between the examples is due to automatic cleaning in tidytext / tokenizer. </p>

<p>If you have a lot of documents to do, you might want to use <code>quanteda</code> to be your workhorse as that one can work on multiple cores out of the box and might speed up the tokenizer part. Don't forget to use the <code>stopwords</code> package for getting a good list of dutch stopwords. If you need POS tagging for Dutch words, you check the <code>updipe</code> package.</p>

<pre><code>library(tidyverse)
library(tidytext)
library(tm)

directory &lt;- ""D:/sample-pdfs""

# create corpus from pdfs
converted &lt;- VCorpus(DirSource(directory), readerControl = list(reader = readPDF)) %&gt;% 
  DocumentTermMatrix()


converted %&gt;%
  tidy() %&gt;%
  filter(!grepl(""[0-9]+"", term))

# A tibble: 5,707 x 3
   document                          term           count
   &lt;chr&gt;                             &lt;chr&gt;          &lt;dbl&gt;
 1 's-Gravenhage_coalitieakkoord.pdf ""\ade""             4
 2 's-Gravenhage_coalitieakkoord.pdf ""\adeze""           1
 3 's-Gravenhage_coalitieakkoord.pdf ""\aeen""            2
 4 's-Gravenhage_coalitieakkoord.pdf ""\aer""             2
 5 's-Gravenhage_coalitieakkoord.pdf ""\aextra""          2
 6 's-Gravenhage_coalitieakkoord.pdf ""\agroei""          1
 7 's-Gravenhage_coalitieakkoord.pdf ""\ahet""            1
 8 's-Gravenhage_coalitieakkoord.pdf ""\amet""            1
 9 's-Gravenhage_coalitieakkoord.pdf ""\aonderwijs,""     1
10 's-Gravenhage_coalitieakkoord.pdf ""\aop""            11
# ... with 5,697 more rows
</code></pre>

<p>Just using tidytext and not tm</p>

<pre><code>directory &lt;- ""D:/sample-pdfs""

pdfs &lt;- paste(directory, ""/"", list.files(directory, pattern = ""*.pdf""), sep = """")
pdf_names &lt;- list.files(directory, pattern = ""*.pdf"")
pdfs_text &lt;- map(pdfs, pdftools::pdf_text)


my_data &lt;- data_frame(document = pdf_names, text = pdfs_text)

my_data %&gt;% 
  unnest %&gt;% # pdfs_text is a list
  unnest_tokens(word, text, strip_numeric = TRUE) %&gt;%  # removing all numbers
  group_by(document, word) %&gt;% 
  summarise(count = n())
# A tibble: 4,646 x 3
# Groups:   document [?]
   document                          word                    count
   &lt;chr&gt;                             &lt;chr&gt;                   &lt;int&gt;
 1 's-Gravenhage_coalitieakkoord.pdf 1e                          2
 2 's-Gravenhage_coalitieakkoord.pdf 2e                          2
 3 's-Gravenhage_coalitieakkoord.pdf 3e                          1
 4 's-Gravenhage_coalitieakkoord.pdf 4e                          1
 5 's-Gravenhage_coalitieakkoord.pdf aan                       164
 6 's-Gravenhage_coalitieakkoord.pdf aanbesteding                2
 7 's-Gravenhage_coalitieakkoord.pdf aanbestedingen              1
 8 's-Gravenhage_coalitieakkoord.pdf aanbestedingsprocedures     1
 9 's-Gravenhage_coalitieakkoord.pdf aanbevelingen               1
10 's-Gravenhage_coalitieakkoord.pdf aanbieden                   4
# ... with 4,636 more rows
</code></pre>
",3,0,3046,2018-08-16 13:57:26,https://stackoverflow.com/questions/51878854/from-pdf-text-to-tidy-dataframe-with-file-names-in-document-column
Applying &quot;String matching to estimate similarity&quot; to data frame,"<p><a href=""https://stackoverflow.com/questions/22936951/string-matching-to-estimate-similarity"">String matching to estimate similarity</a></p>

<p>The above code is exactly what I am looking for, except I cannot seem to figure out how to compare the strings between columns (the ""correct"" answer and ""given"" answer) in a data frame and then storing the output from sim.per as a new column (""similarity"") in that same data frame. I have tried .e.g, </p>

<pre><code>df$similarity &lt;- sim.per(df$answer, df$given) 

df$similarity &lt;- mapply(sim.per, df$answer, df$given)
</code></pre>

<p>The latter also results in an error when the row is empty, which is acceptable in my dataset and should be calculated as 0 instead. </p>

<pre><code>Error in str2[[1]] : subscript out of bounds
</code></pre>

<p>Expected output should be:</p>

<pre><code>    answer                   given                              similarity
1   Best way to waste money  Instrument to waste money and time 0.6
2   Roy travels to Africa    He is in Africa                    0.25
3   I go to work                                                0
</code></pre>

<p>Any help would be appreciated! Thanks!</p>

<p>Subset of the data:</p>

<pre><code>df &lt;- structure(list(trial = 1:10, answer = structure(c(9L, 2L, 4L, 7L, 1L, 5L, 3L, 6L, 8L, 10L), .Label = c(""Best way to waste money"", ""He ran out of money, so he had to stop playing poker"", ""I go to work"", ""Lets all be unique together until we realise we are all the same"", ""Roy travels to Africa"", ""She borrowed the book from him many years ago and did not returned it yet"", ""She did her best to help him"", ""Students did not cheat on the test, for it was not the right thing to do"", ""The stranger officiates the meal"", ""We have a lot of rain in June""), class = ""factor""), given = structure(c(10L, 3L, 6L, 8L, 4L, 2L, 1L, 7L, 9L, 5L), .Label = c("""", ""He is in Africa Roy"", ""He lost money because he had played poker"", ""Instrument to waste money and time"", ""It was raining in June"", ""People are unique until they try to fit in"", ""She borrowed the book from the library and forgot to return it"", ""She did her very best to help him out"", ""Students know not to cheat"", ""The guests ate the meal""), class = ""factor"")), class = ""data.frame"", row.names = c(NA, -10L))
</code></pre>
","r, string, text-mining, text-analysis","<p>Here's an example using <code>tidyverse</code> syntax to avoid manual loops and make things a bit more concise and probably faster. In particular, the format step is vectorised so only the score calculation requires iteration.</p>



<pre class=""lang-r prettyprint-override""><code>library(tidyverse)

df &lt;- structure(list(trial = 1:10, answer = structure(c(9L, 2L, 4L, 7L, 1L, 5L, 3L, 6L, 8L, 10L), .Label = c(""Best way to waste money"", ""He ran out of money, so he had to stop playing poker"", ""I go to work"", ""Lets all be unique together until we realise we are all the same"", ""Roy travels to Africa"", ""She borrowed the book from him many years ago and did not returned it yet"", ""She did her best to help him"", ""Students did not cheat on the test, for it was not the right thing to do"", ""The stranger officiates the meal"", ""We have a lot of rain in June""), class = ""factor""), given = structure(c(10L, 3L, 6L, 8L, 4L, 2L, 1L, 7L, 9L, 5L), .Label = c("""", ""He is in Africa Roy"", ""He lost money because he had played poker"", ""Instrument to waste money and time"", ""It was raining in June"", ""People are unique until they try to fit in"", ""She borrowed the book from the library and forgot to return it"", ""She did her very best to help him out"", ""Students know not to cheat"", ""The guests ate the meal""), class = ""factor"")), class = ""data.frame"", row.names = c(NA, -10L))

format_str &lt;- function(string) {
  string %&gt;%
    str_to_lower %&gt;%
    str_remove_all(""[:punct:]"") %&gt;%
    str_squish %&gt;%
    str_split("" "")
}

df %&gt;%
  mutate(
    similarity = map2_dbl(
      .x = format_str(answer),
      .y = format_str(given),
      .f = ~ length(intersect(.x, .y)) / length(.x)
    )
  ) %&gt;%
  as_tibble
#&gt; # A tibble: 10 x 4
#&gt;    trial answer                        given                    similarity
#&gt;    &lt;int&gt; &lt;fct&gt;                         &lt;fct&gt;                         &lt;dbl&gt;
#&gt;  1     1 The stranger officiates the ~ The guests ate the meal       0.4  
#&gt;  2     2 He ran out of money, so he h~ He lost money because h~      0.333
#&gt;  3     3 Lets all be unique together ~ People are unique until~      0.231
#&gt;  4     4 She did her best to help him  She did her very best t~      1    
#&gt;  5     5 Best way to waste money       Instrument to waste mon~      0.6  
#&gt;  6     6 Roy travels to Africa         He is in Africa Roy           0.5  
#&gt;  7     7 I go to work                  """"                            0    
#&gt;  8     8 She borrowed the book from h~ She borrowed the book f~      0.467
#&gt;  9     9 Students did not cheat on th~ Students know not to ch~      0.25 
#&gt; 10    10 We have a lot of rain in June It was raining in June        0.25
</code></pre>

<p>Created on 2018-08-17 by the <a href=""http://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v0.2.0).</p>
",0,0,86,2018-08-17 16:49:22,https://stackoverflow.com/questions/51899830/applying-string-matching-to-estimate-similarity-to-data-frame
Add a new stemmer to nltk,"<p>I have this python function that works as expected. Is it possible to save the logic as NLP stemmer?
If yes, what changes needs to be done?</p>

<pre><code>import itertools, re
def dropdup(mytuple):
    newtup=list()
    for i in mytuple:
        i = i[:-3] if i.endswith('bai') else i
        for r in ((""tha"", ""ta""), (""i"", ""e"")):
            i = i.replace(*r)
            i = re.sub(r'(\w)\1+',r'\1', i)
        newtup.append(''.join(i for i, _ in itertools.groupby(i)))
    return tuple(newtup)

dropdup(('savithabai', 'samiiir', 'aaaabaa'))
('saveta', 'samer', 'aba')
</code></pre>

<p>I will like the users to import something like this...</p>

<pre><code>from nltk.stemmer import indianNameStemmer
</code></pre>

<p>There are a few more rules to be added to the logic. I just want to know if this is a valid (pythonic) idea.</p>
","python, nlp, nltk, text-mining, stemming","<p>First see <a href=""https://www.python-course.eu/python3_inheritance.php"" rel=""nofollow noreferrer"">https://www.python-course.eu/python3_inheritance.php</a></p>

<p>Create a file <code>mytools.py</code></p>

<pre><code>import itertools, re
from nltk.stem import StemmerI

class MyStemmer(StemmerI):
    def stem(self, token):
        itoken = token[:-3] if token.endswith('bai') else token
        for r in ((""tha"", ""ta""), (""i"", ""e"")):
            token = token.replace(*r)
            token = re.sub(r'(\w)\1+',r'\1', token)
        return ''.join(i for i, _ in itertools.groupby(token))
</code></pre>

<p>Usage:</p>

<pre><code>&gt;&gt;&gt; from mystemmer import MyStemmer
&gt;&gt;&gt; s = MyStemmer()
&gt;&gt;&gt; s.stem('savithabai')
'savetabae'
</code></pre>
",1,0,194,2018-08-18 06:05:25,https://stackoverflow.com/questions/51905788/add-a-new-stemmer-to-nltk
"R: Separate Text String by Space and Remove Tabs, Line Breaks, Etc","<p>After reading an HTML table, my <code>name</code> column appears with records as follows:</p>

<pre><code>\n\t\t\t\t\t\t\t\t\t\t\t\t\tMike Moon\n\t\t\t\t\t\t\t\t
</code></pre>

<p>The following code fails to generate the correct values in the First and Last name columns</p>

<pre><code>separate(data=nametable, col = Name, into = c(""First"",""Last""), sep= "" "")
</code></pre>

<p>Curiously, the <code>First</code> column is blank, while the <code>Last</code> column contains only the person's first name.</p>

<p>How could I correctly turn this column into the <code>First</code> and <code>Last</code> column desired (i.e...</p>

<pre><code>First     Last
Mike      Moon
</code></pre>

<p>Data example per recommendation of @r2evans and as appearing in correct answer code below:</p>

<pre><code>nametable &lt;- data.frame(Name=""\n\t\t\t\t\t\t\t\t\t\t\t\t\tMike Moon\n\t\t\t\t\t\t\t\t"", stringsAsFactors=FALSE)
</code></pre>
","r, text-mining, stringr","<p>It might help to trim whitespace from the field before moving on. <code>trimws</code> removes <em>""leading and/or trailing whitespace from character strings""</em> (from <a href=""https://stat.ethz.ch/R-manual/R-patched/library/base/html/trimws.html"" rel=""nofollow noreferrer""><code>?trimws</code></a>).</p>

<p>Data:</p>

<pre><code>nametable &lt;- data.frame(Name=""\n\t\t\t\t\t\t\t\t\t\t\t\t\tMike Moon\n\t\t\t\t\t\t\t\t"", stringsAsFactors=FALSE)

library(dplyr)
nametable %&gt;% mutate(Name = trimws(Name))
#        Name
# 1 Mike Moon
</code></pre>

<p>I infer that you are using <code>dplyr</code> as well as <code>tidyr</code>, so I'm using it here. It is also really straight-forward to do <code>nametable$Name &lt;- trimws(nametable$Name)</code> without the <code>dplyr</code> usage.
From here, it's as you initially coded:</p>

<pre><code>nametable %&gt;%
  mutate(Name = trimws(Name)) %&gt;%
  tidyr::separate(col=Name, into=c(""First"", ""Last""))
#   First Last
# 1  Mike Moon
</code></pre>
",1,0,979,2018-08-21 14:45:06,https://stackoverflow.com/questions/51951273/r-separate-text-string-by-space-and-remove-tabs-line-breaks-etc
Replacing emojis in a text,"<p>I try to replace emojis with their meaning. </p>

<pre><code>Tweets$text[19]
""I ❤️ flying  . ☺️\U0001f44d""
</code></pre>

<p>For this task, I use the <code>textclean</code> package. The lexicon does not only include the emoji description but also the byte code representation (x: column):</p>

<pre><code>hash_emojis[1:3]
              x                        y
1: &lt;e2&gt;&lt;86&gt;&lt;95&gt;            up-down arrow
2: &lt;e2&gt;&lt;86&gt;&lt;99&gt;          down-left arrow
3: &lt;e2&gt;&lt;86&gt;&lt;a9&gt; right arrow curving left
</code></pre>

<p>So the result looks like this:</p>

<pre><code>Tweets$text[19] = replace_emoji(Tweets$text[19], emoji_dt = lexicon::hash_emojis)

Tweets$text[19]

 ""I red heart &lt;ef&gt;&lt;b8&gt;&lt;8f&gt; flying . smiling face &lt;ef&gt;&lt;b8&gt;&lt;8f&gt; thumbs up ""
</code></pre>

<p>I only want to get the description without the byte code representation because I have to clean it again. How can I apply only the ""y column"" to the text? Is their maybe a better way to deal with emojis in R?</p>
","r, text-mining","<p>After using <code>replace_emoji</code>, you can use <code>replace_non_ascii</code> to get rid of the ascii codes</p>

<pre><code>text &lt;- ""I ❤️ flying  . ☺️\U0001f44d""
t &lt;- replace_emoji(text)
replace_non_ascii(t)
""I red heart flying . smiling face thumbs up""
</code></pre>
",2,2,1122,2018-08-28 14:18:59,https://stackoverflow.com/questions/52060107/replacing-emojis-in-a-text
Regex to extract a number and its unit of measure that are separated by a string from a word of interest,"<p>I'm learning R and I'm trying to use regex to extract specific text. I would like to capture a number and the unit of measure from a recipe for a specific ingredient.</p>

<p>For example for the following text:</p>

<pre><code>text &lt;-  c(""0.5 Tb of butter"",""3 grams (0.75 sticks) of chilled butter"",""2 tbs softened butter"", ""0.3 Tb of milk"")
</code></pre>

<p>I would like to extract the numbers and units relating only to butter, i.e:</p>

<pre><code>0.5 Tb
3 grams
2 tbs
</code></pre>

<p>I think this would be best done using regex, but I'm quite new to this so I'm struggling somewhat.</p>

<p>Using str_match I can get the number in front of specific unit like this:</p>

<pre><code>str_match(text, ""\\s*(\\d+)\\s*Tb"")
     [,1]   [,2]
[1,] ""5 Tb"" ""5"" 
[2,] NA     NA  
[3,] NA     NA  
[4,] ""3 Tb"" ""3"" 
</code></pre>

<p>But how could I get only the values that relate to butter and for a range of units. Is it possible to make a list of possible units (i.e. grams, tbs, Tb etc.) and ask to match any of them (so that in this example grams would match but not sticks)? </p>

<p>Or perhaps this would be done better with some loop? I could put each sentence into a dataframe, loop through each row asking if there is 'butter' in the row search for a number in it and extract the the number and the word that follows, which should be the unit of measure.</p>

<p>Thanks for the help. </p>
","r, regex, string, text-mining","<p>A base R solution would be to <code>grep</code> out the butter lines and then use <code>read.table</code> to parse them given that the matched items are always the first two fields.  No packages are used and the only regular expression used is the simple expression <code>butter</code>.</p>

<pre><code>butter &lt;- grep(""butter"", text, value = TRUE)
read.table(text = butter, fill = TRUE, as.is = TRUE)[1:2]
</code></pre>

<p>giving:</p>

<pre><code>   V1    V2
1 0.5    Tb
2 3.0 grams
3 2.0   tbs
</code></pre>
",1,-1,2312,2018-09-05 15:37:09,https://stackoverflow.com/questions/52188924/regex-to-extract-a-number-and-its-unit-of-measure-that-are-separated-by-a-string
Problem with Analysing Turkish Text while using stopwords &quot;tr&quot; with R,"<p>I am analysing Turkish text in R. But there is a problem when using stopwords""tr"" 
Although, in indicated link, Turkish language is represented with ""tr"" But it still does not recognize it.</p>

<p>here is the error: </p>

<p>Error: Language ""tr"" not available in source ""snowball"". See <code>stopwords_getlanguages</code> for more information on supported languages.</p>

<p>Any help would be appreciated. </p>
","r, text-mining","<p>You are almost there. You just need to change the <code>source</code> of where the <code>stopwords::stopwords</code> get the language from.</p>

<h3>tldr:</h3>

<p>For running your code you need:</p>

<pre><code>stopwords::stopwords(""tr"", source = ""stopwords-iso"")
[1] ""acaba""      ""acep""       ""adamakıllı"" ""adeta""      ""ait""        ""altmýþ""  ... 
</code></pre>

<h3>Explanation:</h3>

<p>These are the languages available in the default source = ""snowball""</p>

<pre><code>stopwords::stopwords_getlanguages(source = ""snowball"")
[1] ""da"" ""de"" ""en"" ""es"" ""fi"" ""fr"" ""hu"" ""ir"" ""it"" ""nl"" ""no"" ""pt"" ""ro"" ""ru"" ""sv""
</code></pre>

<p>To get Turkish you just need to change the source to <code>source = ""stopwords-iso""</code>. Below you can see all the stopwords available in this source.</p>

<pre><code>stopwords::stopwords_getlanguages(source = ""stopwords-iso"")
 [1] ""af"" ""ar"" ""hy"" ""eu"" ""bn"" ""br"" ""bg"" ""ca"" ""zh"" ""hr"" ""cs"" ""da"" ""nl"" ""en"" ""eo"" ""et"" ""fi"" ""fr"" ""gl"" ""de"" ""el"" ""ha"" ""he"" ""hi"" ""hu"" ""id"" ""ga""
[28] ""it"" ""ja"" ""ko"" ""ku"" ""la"" ""lt"" ""lv"" ""ms"" ""mr"" ""no"" ""fa"" ""pl"" ""pt"" ""ro"" ""ru"" ""sk"" ""sl"" ""so"" ""st"" ""es"" ""sw"" ""sv"" ""th"" ""tl"" ""tr"" ""uk"" ""ur""
[55] ""vi"" ""yo"" ""zu""
</code></pre>

<p>Which means that for running your code you need:</p>

<pre><code>stopwords::stopwords(""tr"", source = ""stopwords-iso"")
[1] ""acaba""      ""acep""       ""adamakıllı"" ""adeta""      ""ait""        ""altmýþ""  ... 
</code></pre>
",0,0,1023,2018-09-06 08:07:30,https://stackoverflow.com/questions/52199374/problem-with-analysing-turkish-text-while-using-stopwords-tr-with-r
Summarizing R corpus with doc ID,"<p>I've created a DocumentTermMatrix similar to the one in this post:</p>

<p><a href=""https://stackoverflow.com/questions/24501514/keep-document-id-with-r-corpus"">Keep document ID with R corpus</a></p>

<p>Where I've maintained the doc_id so I can join the data back to a larger data set.</p>

<p>My issue is that I can't figure out how to summarize the words and word count and keep the doc_id. I'd like to be able to join this data to an existing data set using only 3 columns (doc_id, word, freq).</p>

<p>Without needing the doc_id, this is straight forward and I use this code to get my end result.</p>

<pre><code>df_source=DataframeSource(df)
df_corpus=VCorpus(df_source)
tdm=TermDocumentMatrix(df_corpus) 
tdm_m=as.matrix(tdm)

word_freqs=sort(rowSums(tdm_m), decreasing = TRUE)
tdm_sorted=data.frame(word = names(word_freqs), freq = word_freqs)
</code></pre>

<p>I've tried several different approaches to this and just cannot get it to work. This is where I am now (<a href=""https://i.sstatic.net/TPxYp.png"" rel=""nofollow noreferrer"">image</a>). I've used this code:</p>

<pre><code>tdm_m=cbind(""doc.id"" =rownames(tdm_m),tdm_m)
</code></pre>

<p>to move the doc_id into a column in the matrix, but cannot get the numeric columns to sum and keep the doc_id associated.</p>

<p>Any help, greatly appreciated, thanks!</p>

<p>Expected result:</p>

<p>doc.id    |    word      |   frequency<br>
1         |     Apple    |        2<br>
2         |     Apple    |        1<br>
3         |     Banana   |        4<br>
3         |     Orange   |        1<br>
4         |     Pear     |       3</p>
","r, text-mining, tm, corpus","<p>If I look at your expected output, you don't need to use this line of code <code>word_freqs=sort(rowSums(tdm_m), decreasing = TRUE)</code>. Because this creates a total sum of the word, like Apple = 3  instead of 2 and 1 over multiple documents. </p>

<p>To get to the output you want, instead of using <code>TermDocumentMatrix</code>, using <code>DocumentTermMatrix</code> is slightly easier. No need in switching columns around. I'm showing you two examples on how to get the result. One with <code>melt</code> from the reshape2 package and one with the <code>tidy</code> function from the tidytext package.</p>

<pre><code># example 1
dtm &lt;- DocumentTermMatrix(df_corpus)
dtm_df &lt;- reshape2::melt(as.matrix(dtm))
# remove 0 values and order the data.frame
dtm_df &lt;- dtm_df[dtm_df$value &gt; 0, ]
dtm_df &lt;- dtm_df[order(dtm_df$value, decreasing = TRUE), ]
</code></pre>

<p>or using <code>tidytext::tidy</code> to get the data into a tidy format. No need to remove the 0 values as tidytext doesn't transform it into a matrix before casting it into a data.frame</p>

<pre><code># example 2
dtm_tidy &lt;- tidytext::tidy(dtm)
# order the data.frame or start using dplyr syntax if needed
dtm_tidy &lt;- dtm_tidy[order(dtm_tidy$count, decreasing = TRUE), ] 
</code></pre>

<p>In my tests tidytext is a lot faster and uses less memory as there is no need to first create a dense matrix.</p>
",0,0,611,2018-09-07 15:45:49,https://stackoverflow.com/questions/52225861/summarizing-r-corpus-with-doc-id
R - Text Analysis - Misleading results,"<p>I am doing some text analysis of comments from bank customers related to mortgages and I find a couple of things I do understand.</p>

<p>1) After cleaning data without applying Stemming Words and checking the dimension of the TDM the number of terms (2173) is smaller than the number of documents (2373)(This is before remove stop words and being the TDM a 1-gram).</p>

<p>2) Also, I wanted to check the 2-words frequency (rowSums(Matrix)) of the bi-gram tokenizing the TDM. The issue is that for example I have gotten as the most repeated result the 2-words ""Proble miss"". Since this grouping was already strange, I have gone to the dataset, ""Control +F"", to try to find and i could not. Questions: it seems that the code some how has stemmed these words, how is it possible? (From the top 25 bi-words, this one is the only one that seems to be stemmed). Is this not supposed to ONLY create bi-grams that are always together?</p>

<pre><code>{file_cleaning &lt;-  replace_number(files$VERBATIM)
file_cleaning &lt;-  replace_abbreviation(file_cleaning)
file_cleaning &lt;-  replace_contraction(file_cleaning)
file_cleaning &lt;- tolower(file_cleaning)
file_cleaning &lt;- removePunctuation(file_cleaning)
file_cleaning[467]
file_cleaned &lt;- stripWhitespace(file_cleaning)

custom_stops &lt;- c(""Bank"")
file_cleaning_stops &lt;- c(custom_stops, stopwords(""en""))
file_cleaned_stopped&lt;- removeWords(file_cleaning,file_cleaning_stops)

file_cleaned_corups&lt;- VCorpus(VectorSource(file_cleaned))
file_cleaned_tdm &lt;-TermDocumentMatrix(file_cleaned_corups)
dim(file_cleaned_tdm) # Number of terms &lt;number of documents
file_cleaned_mx &lt;- as.matrix(file_cleaned_tdm)

file_cleaned_corups&lt;- VCorpus(VectorSource(file_cleaned_stopped))
file_cleaned_tdm &lt;-TermDocumentMatrix(file_cleaned_corups)
file_cleaned_mx &lt;- as.matrix(file_cleaned_tdm)

dim(file_cleaned_mx)
file_cleaned_mx[220:225, 475:478]

coffee_m &lt;- as.matrix(coffee_tdm)

term_frequency &lt;- rowSums(file_cleaned_mx)
term_frequency &lt;- sort(term_frequency, decreasing = TRUE)
term_frequency[1:10]


BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigram_dtm &lt;- TermDocumentMatrix(file_cleaned_corups, control = list(tokenize = BigramTokenizer))
dim(bigram_dtm)

bigram_bi_mx &lt;- as.matrix(bigram_dtm)
term_frequency &lt;- rowSums(bigram_bi_mx)
term_frequency &lt;- sort(term_frequency, decreasing = TRUE)
term_frequency[1:15]

freq_bigrams &lt;- findFreqTerms(bigram_dtm, 25)
freq_bigrams}
</code></pre>

<p>SAMPLE of DATASET:</p>

<pre><code>&gt; dput(droplevels(head(files,4)))

structure(list(Score = c(10L, 10L, 10L, 7L), Comments = structure(c(4L,

3L, 1L, 2L), .Label = c(""They are nice an quick. 3 years with them, and no issue."",

""Staff not very friendly."",

""I have to called them 3 times. They are very slow."",

""Quick and easy. High value.""

), class = ""factor"")), row.names = c(NA, 4L), class = ""data.frame"")
</code></pre>
","r, text-mining, tm, text-analysis, qdap","<p>Q1: There are situations where you can end up with less terms than documents. </p>

<p>First you are using vectorsource; the number of documents are the number of vectors you have in your txt. This is not really representative of the number of documents. A vector with a space in it would count as a document. Secondly you are removing stopwords. If there are many of these in your text, a lot of words will disappear. Finally <code>TermDocumentMatrix</code> by default removes all words smaller than 3. So if there are any small words left after removing stopwords, these will be removed as well. You can adjust this by adjusting the option <code>wordLengths</code> when creating a <code>TermDocumentMatrix</code> / <code>DocumentTermMatrix</code>.</p>

<pre><code># wordlengths starting at length 1, default is 3
TermDocumentMatrix(corpus, control=list(wordLengths=c(1, Inf)))
</code></pre>

<p>Q2: without a sample document this is a bit of a guess.</p>

<p>Likely a combination of the functions <code>replace_number</code>, <code>replace_contraction</code>, <code>replace_abbreviation</code>, <code>removePunctuation</code> and <code>stripWhitespace</code>. This might result in a word that you can't find very fast. Best bet is to look for each word starting with prob. ""proble"" is as far as I can see, not a correct stem. Also qdap and tm don't do any stemming without you specifying it. </p>

<p>You also have a mistake in your custom_stops. All stopwords are in lowercase and you specified that your text should be in lowercase. So your custom_stops should also be in lowercase. ""bank"" instead of ""Bank"".</p>
",1,0,234,2018-09-09 23:20:04,https://stackoverflow.com/questions/52249382/r-text-analysis-misleading-results
Sequential string chunking,"<p>I have a list of strings that I am looking to chunk into sub-lists comprising three  elements, the element in the list (i), the preceding element (i-1), and the next element (i+1). I would also be looking to iterate over <em>every other</em> element in the list rather than every element. More specifically, how can I go from this...</p>

<pre><code>mylist = ['red','green','blue','yellow','orange','purple','black']
</code></pre>

<p>To this... (starting from index position 1 to enable a preceding element);</p>

<pre><code>mysublists = [['green','red','blue'],['yellow','blue','orange'],['purple','orange','black']]
</code></pre>

<p>So each item in the sub-list contains every second element as its' first item, followed by the item that <em>preceded</em> it in the original list, followed by the item that <em>succeeded</em> it in the original list.</p>
","python, list, list-comprehension, text-mining, nested-lists","<p>You can use a list comprehension with <code>zip</code> and list slicing:</p>

<pre><code>res = [[j, i, k] for i, j, k in zip(mylist[::2], mylist[1::2], mylist[2::2])]

[['green', 'red', 'blue'],
 ['yellow', 'blue', 'orange'],
 ['purple', 'orange', 'black']]
</code></pre>

<p>Or use <code>list</code> with <code>zip</code> directly for a list of tuples:</p>

<pre><code>res = list(zip(mylist[1::2], mylist[::2], mylist[2::2]))
</code></pre>

<p>If you want a list of lists with similar syntax, you can use <code>map</code>:</p>

<pre><code>res = list(map(list, zip(mylist[1::2], mylist[::2], mylist[2::2])))
</code></pre>
",2,0,41,2018-09-11 15:31:33,https://stackoverflow.com/questions/52279479/sequential-string-chunking
Report the mean number of characters in Corpus document,"<p>So I have a corpus setup reading bunch of text file with paragraphs in them. </p>

<pre><code>    library('tm')
my.text.location &lt;- ""C:/Users//.../*/""
apapers &lt;- VCorpus(DirSource(my.text.location))
</code></pre>

<p>Now I need to find the mean of the characters in each text. Running a
 <code>mean(nchar(apapers), na.rm =T)</code> results in a very weird output, more than the number of characters. 
Any other way to get the mean?</p>
","r, rstudio, mean, text-mining","<p>You didn't supply a reproducible example, but <code>rowMeans(sapply(apapers, nchar))</code> will return the mean number of characters over all documents. ""Content"" is the column you need. </p>

<p>A longer version is running a sapply over the corpus counting the number of per document. Transpose this data and turn it into a data.frame. The data.frame will contain two columns, content and meta. Content is the one you need. Taking the mean of the content column will give you the average number of characters in a document. The advantage of this is that you have the table in case you need to report the numbers. </p>

<pre><code># your code
my_count &lt;- data.frame(t(sapply(apapers, nchar)))
mean(my_count$content)
</code></pre>

<p>Reproducible example using the crude dataset:</p>

<pre><code>library(tm)
data(""crude"")
crude &lt;- as.VCorpus(crude)

# in one statement
rowMeans(sapply(crude, nchar))
    content    meta 
    1220.30  453.15

# longer version keeping intermediate results.
my_count &lt;- data.frame(t(sapply(crude, nchar)))
mean(my_count$content)
[1] 1220.3

my_count
    content meta
127     527  440
144    2634  458
191     330  444
194     394  441
211     552  441
236    2774  455
237    2747  477
242     930  453
246    2115  440
248    2066  466
273    2241  458
349     593  492
352     621  468
353     591  445
368     629  440
489     876  445
502    1166  446
543     463  447
704    1797  456
708     360  451
</code></pre>
",0,0,578,2018-09-14 16:43:15,https://stackoverflow.com/questions/52336135/report-the-mean-number-of-characters-in-corpus-document
Python regex or other solution to extract text items from a string?,"<p>I have a string that lookls like this:</p>

<pre><code>\nInhaltse / techn. Angaben*\n\nAQUA • COCO-GLUCOSIDE • COCOSULFATE • SODIUM\n\n\
</code></pre>

<p>And I need to get a list of the items between dots, as follows:</p>

<pre><code>AQUA COCO-GLUCOSIDE COCOSULFATE  SODIUM
</code></pre>

<p>I have tried with regex and other tools but I cant find the right, flexible* answer. </p>

<p>*flexible = the list might have something between 1 and N elements</p>
","python, text, beautifulsoup, text-mining","<p>You should define a little bit better what are the possibilities, and which rules you want to apply.<br>
I think that a rule like <em>'any word with only at least 2 uppercase characters or dash preceded and followed by a space or \n'</em> may work for you. If thats the case, here's your RegEx:</p>

<pre><code>import re

my_string = ""\nInhaltse / techn. Angaben*\n\nAQUA • COCO-GLUCOSIDE • COCOSULFATE • SODIUM\n\n""

print(re.findall(r""(?&lt;=\n|\s)[A-Z-]{2,}(?=\n|\s)"", my_string))
</code></pre>

<p>Output:</p>

<blockquote>
  <p>['AQUA', 'COCO-GLUCOSIDE', 'COCOSULFATE', 'SODIUM']</p>
</blockquote>

<p>and here's how you read the RegEx:</p>

<p><code>(?&lt;=\n|\s)</code> means <em>preceded by (</em><code>?&lt;=</code><em>) a new line(</em><code>\n</code><em>) or (</em><code>|</code><em>) a space (</em><code>\s</code><em>)</em><br>
<code>[A-Z-\s]{2,}</code> means <em>at least two (</em><code>{2,}</code><em>) uppercase letters, dash and spaces (</em><code>[A-Z-\s]</code><em>)</em><br>
<code>(?=\n|\s)</code> means <em>followed by (</em><code>?=</code><em>) a new line(</em><code>\n</code><em>) or (</em><code>|</code><em>) a space (</em><code>\s</code><em>)</em>  </p>

<p>or for fitting better your request:</p>

<blockquote>
  <p>get a list of the items between dots</p>
</blockquote>

<p>you may use:  </p>

<pre><code>r""(?&lt;=\n\n|\•\s)[A-Z-\s]{2,}(?=\n\n|\s\•)""
</code></pre>

<p>which means:</p>

<blockquote>
  <p>at least 2 uppercase letters, dash or spaces, preceded by two new line or a dot and a space and followed by two new lines or a space and a dot</p>
</blockquote>
",0,0,34,2018-09-20 14:23:11,https://stackoverflow.com/questions/52427376/python-regex-or-other-solution-to-extract-text-items-from-a-string
Wrting each item in a list into a separate txt file with auto-assigned filename (python=3.6),"<p>I'm using textract to get plain text from PDF files. For the plain text of each PDF file in the directory, I append it to the list <code>filetext_list</code>. I want to write each item of the list to a separate txt file with an auto-assigned filename like ""article_1"". Here is what I did so far:</p>

<pre><code>import textract
import os
filetext_list = []
directory=os.getcwd()
for file in os.listdir(directory):
    txt = textract.process(file, method = 'pdfminer')
    filetext_list.append(txt)
    for i in range(2):
        for filetext in filetext_list:    
            with open('artile_{0}.txt'.format(i),'w') as f:
                f.write(str(filetext))
        f.close()
</code></pre>

<p>The output files are ""article_0"" and ""article_1"", which are named properly. However, both files contain the text of the same item in the list. I intended them to each contain the text of a separate item in the list. Any idea why the code failed? Also, I would like to eliminate the ""\n"" elements in the text by doing something like <code>.replace('\n', ' ')</code>, but I don't know where this would fit in the code. Thank you!</p>
","python-3.x, text-mining, text-processing, text-extraction, pdfminer","<p>What is it you're trying to do exactly?  I think I may be misunderstanding you.</p>

<p>The end result is basically that you want to convert each pdf to a txt file right?</p>

<pre><code>import textract
import os
filetext_list = []
directory=os.getcwd()

for file in os.listdir(directory):
     txt = textract.process(file, method = 'pdfminer')
     filetext_list.append(txt)

for index, text in enumerate(filetext_list) :
     with open('article_{0}.txt'.format(index),'w') as f:
          f.write(str(text).replace(""\n"",""""))
</code></pre>

<p>thus if your directory contents were [thing1.pdf, thing2.pdf, thing3.pdf] then the new files you generated would be article_0 (with contents of thing1.pdf), article_1 (with thing2.pdf's contents), article_2 (with contents of thing3.pdf)</p>

<p>I'm not sure how to give you pointers on where you went wrong with your code asynchronously.  But you would do better to restart your thinking on this than to try to continue using the logic that you currently have. </p>

<p>Every new ""open"" coupled with a write is overwriting the original file.</p>

<p>So what you're getting is article_0 and article_1 with the contents of the last pdf in the list of the files in the directory.</p>

<p>Your loop with ""range(2)"" is also being called once for each file in your directory, so the contents of article_0 looks like ""contents of the first file, then the contents of the first file again, then the contents of the second file, then the contents of the first file then the contents of the second file, then th e third, then the 1st -> 2nd -> 3rd -> 4th. and so on.</p>

<p>Additionally if you do ""with open"" you don't need to close the file.  It close automatically afaik.</p>
",1,0,62,2018-09-22 21:32:52,https://stackoverflow.com/questions/52461070/wrting-each-item-in-a-list-into-a-separate-txt-file-with-auto-assigned-filename
Extract emotions calculation for every row of a dataframe,"<p>I have a dataframe with rows of text. I would like to extract for each row of text a vector of specific emotion which will be a binary 0 is not exist this emotion or 1 is exist.<br> Totally they are 5 emotions but I would like to have the 1 only for the emotion which seem to be the most.</p>

<p>Example of what I have tried:</p>

<pre><code>library(tidytext)
text = data.frame(id = c(11,12,13), text=c(""bad movie"",""good movie"",""I think it would benefit religious people to see things like this, not just to learn about our home, the Universe, in a fun and easy way, but also to understand that non- religious explanations don't leave people hopeless and"",))
nrc_lexicon &lt;- get_sentiments(""nrc"")
</code></pre>

<p>Example of expected output:</p>

<pre><code>    id text sadness anger joy love neutral
11 ""bad movie"" 1 0 0 0 0
12 ""good movie"" 0 0 1 0 0 
</code></pre>

<p>Any hints will be helpful for me.</p>

<p>Example to make it for every row what is the next step?<br> How can I call every line with the nrc lexicon analysis?</p>

<pre><code>for (i in 1:nrow(text)) {
(text$text[i], nrc_lexicon)
}
</code></pre>
","r, text-mining, tidyr, sentiment-analysis","<p>What about this:</p>

<pre><code>library(tidytext)   # library for text
library(dplyr)

# your data
text &lt;- data.frame(id = c(11,12,13),
 text=c(""bad movie"",""good movie"",""I think it would benefit religious
 people to see things like this, not just to learn about our home, 
the Universe, in a fun and easy way, but also to understand that non- religious
 explanations don't leave people hopeless and""), stringsAsFactors = FALSE)  # here put this option, stringAsFactors = FALSE!

# the lexicon
nrc_lexicon &lt;- get_sentiments(""nrc"")

# now the job
unnested &lt;- text %&gt;%
             unnest_tokens(word, text) %&gt;%  # unnest the words
             left_join(nrc_lexicon) %&gt;%     # join with the lexicon to have sentiments
             left_join(text)                # join with your data to have titles
</code></pre>

<p>Here the output with the <code>id</code>, you can have it also with the titles, but I did not put it due the long third title, you can easily put it as <code>unnested$text</code> in place of <code>unnested$id</code>:</p>

<pre><code>table_sentiment &lt;- table(unnested$id, unnested$sentiment)
table_sentiment
     anger anticipation disgust fear joy negative positive sadness surprise trust
  11     1            0       1    1   0        1        0       1        0     0
  12     0            1       0    0   1        0        1       0        1     1
  13     0            1       0    1   1        2        3       2        1     0
</code></pre>

<p>And if you want it as <code>data.frame</code>:</p>

<pre><code> df_sentiment &lt;- as.data.frame.matrix(table_sentiment)
</code></pre>

<p>Now you can do everything you want, for example, if I remember well, you want a binary output if exist or not a sentiment:</p>

<pre><code>df_sentiment[df_sentiment&gt;1]&lt;-1
df_sentiment
   anger anticipation disgust fear joy negative positive sadness surprise trust
11     1            0       1    1   0        1        0       1        0     0
12     0            1       0    0   1        0        1       0        1     1
13     0            1       0    1   1        1        1       1        1     0
</code></pre>
",1,0,609,2018-09-30 08:41:45,https://stackoverflow.com/questions/52576238/extract-emotions-calculation-for-every-row-of-a-dataframe
Creating a DTM on Alteryx Designer,"<p>I am new to Alteryx and am trying to use it for analysing unstructured data. I have a column of description in text form and I intend to use the K-Means Clustering tool for topic modelling. For K-means to work on text, I will need to convert my text into a Document Term Matrix (DTM) so that they appear as continuous variables to the clustering tool. However, I am struggling to find a way I can convert my text to a DTM.</p>

<p>Does anyone know a way to do so? I am currently looking at the R tool but am not exactly sure how to start too. Hoping that all of you experts here can help me out!</p>

<p>I have looked through posts on text analysis and realized that most fell back on the Microsoft Azure ML Text Analysis Macro. However, I would like to avoid using the macro (to not be restricted to limited runs every month for scalability) and instead use tools that are available in Alteryx.</p>

<p>Thanks to everyone in advance!</p>
","azure, cluster-analysis, text-mining, alteryx","<p>with Alteryx being more of a pictoral drag-and-drop workflow, it's not trivial to explain here, however I've created the following workflow and included the actual workflow itself on the Alteryx forum <a href=""https://community.alteryx.com/t5/Alteryx-Designer-Discussions/Document-Term-Matrix-in-Alteryx/td-p/307268"" rel=""nofollow noreferrer"">here</a>.  The workflow utilizes term frequencies from Inauguration speeches but should apply to any collection of documents. It just splits the words based on various non-numeric characters and does a summary. This is what the workflow looks like:</p>

<p><a href=""https://i.sstatic.net/tBjk2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tBjk2.png"" alt=""enter image description here""></a></p>
",1,0,191,2018-10-01 09:05:27,https://stackoverflow.com/questions/52587708/creating-a-dtm-on-alteryx-designer
R Term frequency from large document set,"<p>I have a data frame like this</p>

<pre><code>ID       content
 1       hello you how are you
 1       you are ok
 2       test
</code></pre>

<p>I need to get the frequency by id for each word in the content which is space separated. Which is basically finding the unique terms in the column and finding the frequency and display grouped by Id</p>

<pre><code>ID      hello    you   how   are  ok    test
 1        1       3     1    2     1     0
 2        0       0     0    0     0     1    
</code></pre>

<p>I tried </p>

<pre><code>test&lt;- unique(unlist(strsplit(temp$val, split="" "")))

df&lt;- cbind(temp, sapply(test, function(y) apply(temp, 1, function(x) as.integer(y %in% unlist(strsplit(x, split="" ""))))))
</code></pre>

<p>This gives the ungrouped solution which I'm trying to group now, but I have over 20000 unique values in content, is there an efficient way to do this?</p>
","r, sorting, text-mining","<p>What about a package made for text-mining?</p>

<pre><code># your data
text &lt;- read.table(text = ""
ID      content
1       'hello you how are you'
1       'you are ok'
2       'test'"", header = T,  stringsAsFactors = FALSE) # remember the stringAsFactors life saver!
</code></pre>

<hr>

<pre><code>library(dplyr)
library(tidytext)
# here we put in column all the words
unnested &lt;- text %&gt;%
            unnest_tokens(word, content)

# a classic data.frame from a table of frequencies
as.data.frame.matrix(table(unnested$ID, unnested$word))
  are hello how ok test you
1   2     1   1  1    0   3
2   0     0   0  0    1   0
</code></pre>
",1,0,32,2018-10-01 13:21:52,https://stackoverflow.com/questions/52592139/r-term-frequency-from-large-document-set
&#39;Word2Vec&#39; object has no attribute &#39;index2word&#39;,"<p>I'm getting this error ""AttributeError: 'Word2Vec' object has no attribute 'index2word'"" in following code in python. Anyone knows how can I solve it?
Acctually ""tfidf_weighted_averaged_word_vectorizer"" throws the error. ""obli.csv"" contains line of sentences. 
Thank you.</p>

<pre><code>from feature_extractors import tfidf_weighted_averaged_word_vectorizer

    dataset = get_data2()
    corpus, labels = dataset.data, dataset.target
    corpus, labels = remove_empty_docs(corpus, labels)
    # print('Actual class label:', dataset.target_names[labels[10]])

    train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,
                                                                            labels,
                                                                            test_data_proportion=0.3)
    tfidf_vectorizer, tfidf_train_features = tfidf_extractor(train_corpus)


    vocab = tfidf_vectorizer.vocabulary_
        tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train,
                                                                          tfidf_vectors=tfidf_train_features,
                                                                          tfidf_vocabulary=vocab,
                                                                          model=model,
                                                                          num_features=100)



    def get_data2():

        obli = pd.read_csv('db/obli.csv').values.ravel().tolist()
        cl0 = [0 for x in range(len(obli))]

        nonObli = pd.read_csv('db/nonObli.csv').values.ravel().tolist()
        cl1 = [1 for x in range(len(nonObli))]

        all = obli + nonObli


        db =  Db(all,cl0 + cl1)
        db.data = all
        db.target = cl0 + cl1

        return db
</code></pre>
","python-3.x, text-mining, word2vec","<p>This is code from chapter 4 of Text Analytics for Python by Dipanjan Sarkar.</p>

<p>index2word in gensim has been moved since that text was published.</p>

<p>Instead of <code>model.index2word</code> you should use <code>model.wv.index2word</code>.</p>
",9,2,6147,2018-10-03 14:14:25,https://stackoverflow.com/questions/52629055/word2vec-object-has-no-attribute-index2word
What R package can I use for counting the occurrence of unique strings in an array,"<p>I need to count the occurrence of unique words in a series of words and assign a value of 1 when a new word appears in the series. I wonder what R package or function could do that.</p>

<pre><code>    test &lt;- c(cat, dog, table, cat, chair, car, bus, 
    chair, school, car, chair, table)

    #expected result (1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0)
</code></pre>

<p>Thanks,
Chamil</p>
","r, text-mining, tm, word-count","<p>We can get a logical vector with <code>duplicated</code> and coerce it to binary with <code>as.integer</code>.  The <code>duplicated</code> returns TRUE for every duplicate elements, by negating (<code>!</code>) the TRUE returns FALSE and FALSE as TRUE which is converted to 1 (TRUE)  or 0 (FALSE) with <code>as.integer</code></p>

<pre><code>as.integer(!duplicated(test))
#[1] 1 1 1 0 1 1 1 0 1 0 0 0
</code></pre>

<h3>data</h3>

<pre><code>test &lt;- c(""cat"", ""dog"", ""table"", ""cat"", ""chair"", ""car"", ""bus"",
          ""chair"", ""school"", ""car"", ""chair"", ""table"")
</code></pre>
",2,0,37,2018-10-03 15:47:10,https://stackoverflow.com/questions/52631008/what-r-package-can-i-use-for-counting-the-occurrence-of-unique-strings-in-an-arr
substituting several ngrams in quanteda,"<p>In my text of news articles I would like to convert several different ngrams that refer to the same political party to an acronym.  I would like to do this because I would like to avoid any sentiment dictionaries confusing the words in the party's name (Liberal Party) with the same word in different contexts (liberal helping).</p>

<p>I can do this below with <code>str_replace_all</code> and I know about the <code>token_compound()</code> function in quanteda, but it doesn't seem to do exactly what I need.</p>

<pre><code>library(stringr)
text&lt;-c('a text about some political parties called the new democratic party the new democrats and the liberal party and the liberals')
text1&lt;-str_replace_all(text, '(liberal party)|liberals', 'olp')
text2&lt;-str_replace_all(text1, '(new democrats)|new democratic party', 'ndp')
</code></pre>

<p>Should I somehow just preprocess the text before turning it into a corpus? Or is there a way to do this after turning it into a corpus in <code>quanteda</code>. </p>

<p>Here is some expanded sample code that specifies the problem a little better:</p>

<pre><code>`text&lt;-c('a text about some political parties called the new democratic party 
the new democrats and the liberal party and the liberals. I would like the 
word democratic to be counted in the dfm but not the words new democratic. 
The same goes for liberal helpings but not liberal party')
partydict &lt;- dictionary(list(
olp = c(""liberal party"", ""liberals""),
ndp = c(""new democrats"", ""new democratic party""),
sentiment=c('liberal', 'democratic')
))

dfm(text, dictionary=partydict)`
</code></pre>

<p>This example counts <code>democratic</code> in both the <code>new democratic</code> and the <code>democratic</code> sense, but I would those counted separately. </p>
","r, text-mining, quanteda","<p>You want the function <code>tokens_lookup()</code>, after having defined a dictionary that defines the canonical party labels as keys, and lists all the ngram variations of the party names as values.  By setting <code>exclusive = FALSE</code> it will keep the tokens that are not matched, in effect acting as a substitution of all variations with the canonical party names. </p>

<p>In the example below, I've modified your input text a bit to illustrate the ways that the party names will be combined to be different from the phrases using ""liberal"" but not ""liberal party"".  </p>

<pre><code>library(""quanteda"")

text&lt;-c('a text about some political parties called the new democratic party 
         which is conservative the new democrats and the liberal party and the 
         liberals which are liberal helping poor people')
toks &lt;- tokens(text)

partydict &lt;- dictionary(list(
    olp = c(""liberal party"", ""the liberals""),
    ndp = c(""new democrats"", ""new democratic party"")
))

(toks2 &lt;- tokens_lookup(toks, partydict, exclusive = FALSE))
## tokens from 1 document.
## text1 :
##  [1] ""a""            ""text""         ""about""        ""some""         ""political""    ""parties""     
##  [7] ""called""       ""the""          ""NDP""          ""which""        ""is""           ""conservative""
## [13] ""the""          ""NDP""          ""and""          ""the""          ""OLP""          ""and""         
## [19] ""OLP""          ""which""        ""are""          ""liberal""      ""helping""      ""poor""        
## [25] ""people""   
</code></pre>

<p>So that has replaced the party name variances with the party keys. 
 Constructing a dfm from this new tokens now occurs on these new tokens, preserving the uses of (e.g.) ""liberal"" that might be linked to sentiment, but having already combined the ""liberal party"" and replaced it with ""OLP"".  Applying a dictionary to the dfm will now work for your example of ""liberal"" in ""liberal helping"" without having confused it with the use of ""liberal"" in the party name.</p>

<pre><code>sentdict &lt;- dictionary(list(
    left = c(""liberal"", ""left""),
    right = c(""conservative"", """")
))

dfm(toks2) %&gt;%
    dfm_lookup(dictionary = sentdict, exclusive = FALSE)
## Document-feature matrix of: 1 document, 19 features (0% sparse).
## 1 x 19 sparse Matrix of class ""dfm""
##        features
## docs    olp ndp a text about some political parties called the which is RIGHT and LEFT are helping
##  text1   2   2 1    1     1    1         1       1      1   3     2  1     1   2    1   1       1
##        features
## docs    poor people
##  text1    1      1
</code></pre>

<p>Two additional notes:</p>

<ol>
<li><p>If you do not want the keys uppercased in the replacement tokens, set <code>capkeys = FALSE</code>.</p></li>
<li><p>You can set different matching types using the <code>valuetype</code> argument, including <code>valuetype = regex</code>.  (And note that your regular expression in the example is probably not correctly formed, since the scope of your <code>|</code> operator in the ndp example will get ""new democrats"" OR ""new"" and then "" democratic party"".  But with <code>tokens_lookup()</code> you won't need to worry about that!)</p></li>
</ol>
",1,2,137,2018-10-05 14:12:38,https://stackoverflow.com/questions/52667562/substituting-several-ngrams-in-quanteda
Remove any text inside square brackets in r,"<p>I would like to remove all the words inside square brackets as well as the brackets themselves. For example,</p>

<pre><code>text = c('[Verse 1]', '[Verse 1: Dua Lipa]', '[Corus]', '[Corus: Ann Marie &amp; Ed Sheeran]')
</code></pre>

<p>Like above, the length of words inside the bracket are not constant. So I need a function that can identify the position of <code>[</code> and <code>]</code> in order to erase all the words, numbers and symbols in between. Is there any function able to do that?  </p>
","r, regex, text-mining, square-bracket","<p>You may remove all substrings within square brackets using</p>

<pre><code>gsub(""\\[[^][]*]"", """", text)
</code></pre>

<p>The pattern matches an open square bracket, then any zero or more chars other than square brackets, and then a close square bracket.</p>
",5,1,1706,2018-10-06 08:41:04,https://stackoverflow.com/questions/52677243/remove-any-text-inside-square-brackets-in-r
Cosine similarity of Documents,"<p>Data format CSV</p>

<p>Total number of documents 500. number of fields 10.</p>

<p>view of data <a href=""https://i.sstatic.net/z5uoV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/z5uoV.png"" alt=""enter image description here""></a> </p>

<p>i want to calculate parallel cosine similarity  of Each ""Docs"" with all 500 documents, </p>

<p>expected out put </p>

<p><a href=""https://i.sstatic.net/3DjUM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3DjUM.png"" alt=""enter image description here""></a></p>
","r, text-mining, cosine-similarity","<p>Does this do what you want? To compute the similarity of all (500*499)/2 combinations, you can do something like this:</p>

<pre><code># Create some mock data
df &lt;-replicate(10, rnorm(500))
rownames(df) &lt;- paste0(""doc"", seq_len(nrow(df)))
colnames(df) &lt;- paste0(""field"", seq_len(ncol(df)))


# Vector lengths
vl &lt;- sqrt(rowSums(df*df))

# Matrix of all combinations
comb &lt;- t(combn(1:nrow(df), 2))

# Compute cosine similarity for all combinations
csim &lt;- apply(comb, 1, FUN = function(i) sum(apply(df[i, ], 2, prod))/prod(vl[i]))

# Create a data.frame of the results
res &lt;- data.frame(docA = rownames(df)[comb[,1]],
                  docB = rownames(df)[comb[,2]],
                  csim = csim)
head(res)
#  docA docB       csim
#1 doc1 doc2 -0.6431972
#2 doc1 doc3 -0.2560444
#3 doc1 doc4 -0.4911942
#4 doc1 doc5 -0.2207487
#5 doc1 doc6  0.4764924
#6 doc1 doc7  0.5867607

tail(res)
#         docA   docB      csim
#124745 doc497 doc498 1.0714338
#124746 doc497 doc499 0.8439304
#124747 doc497 doc500 1.1806366
#124748 doc498 doc499 0.9326781
#124749 doc498 doc500 1.4783254
#124750 doc499 doc500 1.3626494
</code></pre>

<p>Note, it does not really make sense to have the original vector values of the fields in this output table. Each number is a comparison and coputation of two rows in your data.</p>

<p><strong>Edit</strong>:</p>

<p>If you want it no matrix form, you can compute it directly by:</p>

<pre><code>res_mat &lt;- tcrossprod(df)/tcrossprod(vl)
print(res_mat[1:5, 1:5])
#           doc1       doc2       doc3       doc4       doc5
#doc1  1.0000000 -0.6431972 -0.2560444 -0.4911942 -0.2207487
#doc2 -0.6431972  1.0000000  0.3996618  0.3365490 -0.1434239
#doc3 -0.2560444  0.3996618  1.0000000  0.2856842  0.2781019
#doc4 -0.4911942  0.3365490  0.2856842  1.0000000  0.2287057
#doc5 -0.2207487 -0.1434239  0.2781019  0.2287057  1.0000000
</code></pre>
",2,0,1055,2018-10-09 11:38:48,https://stackoverflow.com/questions/52720178/cosine-similarity-of-documents
Subset/select from a DFM using a dictionary in quanteda,"<p>I have a corpus of texts from various countries. I am trying to see how often a specific term appears in the texts for each country. To do so, I am following the example here: <a href=""https://quanteda.io/articles/pkgdown/examples/plotting.html#frequency-plots"" rel=""nofollow noreferrer"">https://quanteda.io/articles/pkgdown/examples/plotting.html#frequency-plots</a></p>

<pre><code>freq_grouped &lt;- textstat_frequency(dfm(full_corpus), 
                                   groups = ""Country"")

freq_const &lt;- subset(freq_grouped, freq_grouped$feature %in% ""constitution"")
</code></pre>

<p>This works fine, except that this only captures the exact term (""constitution""). I'd like to be able to capture variations of the term (e.g. ""charter of rights and freedoms"") use globs (e.g. ""<code>*constitution*</code>""), and count the results under the same category. I tried using a dictionary for this, but I get zero results.</p>

<pre><code>dict &lt;- dictionary(list(constitution = c('*constitution*', 'charter of rights and freedoms', 
                                         'canadian charter', 'constituição*', '*constitucion*')))

freq_const &lt;- subset(freq_grouped, freq_grouped$feature %in% dict)

freq_const
    [1] feature   frequency rank      docfreq   group    
    &lt;0 rows&gt; (or 0-length row.names)
</code></pre>

<p>How can I go about achieving this?</p>
","r, text-mining, quanteda","<p>The basic answer is that you cannot subset a dfm using a dictionary or any other sort of pattern match, because <code>dfm_subset()</code> requires a logical value for its subset match that matches 1:1 with <em>documents</em>.  A dictionary would match features, not documents.</p>

<p>If you wanted to match <em>features</em> while not selecting documents, however -- which I think is what you intended -- then you can use <code>dfm_select()</code>, and a <strong>quanteda</strong> dictionary is a valid input for the <code>pattern</code> argument of that command.  With the <code>valuetype = ""glob""</code> argument, furthermore, you can specify that your pattern match is a glob rather than a regex.</p>

<pre><code>library(""quanteda"")

subdfm &lt;- dfm(data_corpus_inaugural) %&gt;%
    dfm_select(pattern = dict, valuetype = ""glob"")

head(subdfm)
## Document-feature matrix of: 6 documents, 5 features (66.7% sparse).
## 6 x 5 sparse Matrix of class ""dfm""
##                  features
## docs              constitutional constitution constitutions constitutionally unconstitutional
##   1789-Washington              1            1             0                0                0
##   1793-Washington              1            1             0                0                0
##   1797-Adams                   0            8             1                0                0
##   1801-Jefferson               1            2             0                0                0
##   1805-Jefferson               0            6             0                0                0
##   1809-Madison                 0            1             0                0                0

textstat_frequency(subdfm)
##            feature frequency rank docfreq group
## 1     constitution       206    1      37   all
## 2   constitutional        53    2      24   all
## 3    constitutions         4    3       3   all
## 4 constitutionally         4    4       3   all
## 5 unconstitutional         3    5       3   all
</code></pre>

<p>If you have docvars for the corpus from which you create the dfm, you can also feed these to the <code>textstat_frequency()</code> call - they will be attached to dfm.</p>
",2,1,1180,2018-10-16 00:20:32,https://stackoverflow.com/questions/52826341/subset-select-from-a-dfm-using-a-dictionary-in-quanteda
R - How to simplify this text clean-up of special characters?,"<p>I suspect there is a way to simplify this text pre-preprocessing. However, I could not find a solution how to merge all these character replacements into a single row. Hence, to avoid all the repetition in my current solution (see below):</p>

<pre><code>Encoding(posts2$caption_clean) &lt;- ""UTF-8""
posts2$caption_clean &lt;- iconv(posts2$caption_clean, ""latin1"", ""UTF-8"")
posts2$caption_clean &lt;- gsub(""Ã\\S*"","""",posts2$caption_clean) 
posts2$caption_clean &lt;- gsub(""â\\S*"","""",posts2$caption_clean)
posts2$caption_clean &lt;- gsub(""ð\\S*"","""",posts2$caption_clean)
posts2$caption_clean &lt;- gsub(""Â\\S*"","""",posts2$caption_clean) 
posts2$caption_clean &lt;- gsub(""å\\S*"","""",posts2$caption_clean)
posts2$caption_clean &lt;- gsub(""Ð\\S*"","""",posts2$caption_clean)
posts2$caption_clean &lt;- gsub(""Ñ\\S*"","""",posts2$caption_clean)
posts2$caption_clean &lt;- gsub(""Ù\\S*"","""",posts2$caption_clean)
posts2$caption_clean &lt;- gsub(""Ø\\S*"","""",posts2$caption_clean) 
posts2$caption_clean &lt;- gsub(""Ú\\S*"","""",posts2$caption_clean) 
posts2$caption_clean &lt;- gsub(""ì\\S*"","""",posts2$caption_clean) 
posts2$caption_clean &lt;- gsub(""Õ\\S*"","""",posts2$caption_clean) 
posts2$caption_clean &lt;- gsub(""ã\\S*"","""",posts2$caption_clean) 
posts2$caption_clean &lt;- gsub(""Û\\S*"","""",posts2$caption_clean) 
posts2$caption_clean &lt;- gsub(""ë\\S*"","""",posts2$caption_clean)
posts2$caption_clean &lt;- gsub(""ê\\S*"","""",posts2$caption_clean)
posts2$caption_clean &lt;- gsub(""è¿½\\S*"","""",posts2$caption_clean)
</code></pre>

<p>Does anyone know how I can simplify this?</p>

<p>Thanks!</p>
","r, text, replace, text-mining, tm","<pre><code># construct regex where each target pattern is a group ()
# enclose groups in [] to target any of those groups

regex &lt;- ""[(Ã\\S*)(â\\S*)(ð\\S*)]"" 
string &lt;- ""Ã  x â x ð y ""
gsub(regex, """", string)
</code></pre>

<p>result:</p>

<pre><code>[1] ""  x  x  y ""
</code></pre>
",1,0,354,2018-10-28 13:54:58,https://stackoverflow.com/questions/53032256/r-how-to-simplify-this-text-clean-up-of-special-characters
How to remove urls without http in a text document using r,"<p>I am trying to remove urls that may or may not start with http/https from a large text file, which I saved in urldoc in R. The url may start like tinyurl.com/ydyzzlkk or aclj.us/2y6dQKw or pic.twitter.com/ZH08wej40K. Basically I want to remove data before a '/' after finding the space and after a ""/"" until I find a space. I tried with many patterns and searched many places. Couldn't complete the task. I would help me a lot if you could give some input. </p>

<p>This is the last statement I tried and got stuck for the above problem. 
urldoc = gsub(""?[a-z]+\..<em>\/.</em>[\s]$"","""", urldoc)</p>

<p>Input would be: A disgrace to his profession. pic.twitter.com/ZH08wej40K In a major victory for religious liberty, the Admin. has eviscerated institution continuing this path. goo.gl/YmNELW nothing like the admin. proposal: tinyurl.com/ydyzzlkk</p>

<p>Output I am expecting is: A disgrace to his profession.  In a major victory for religious liberty, the Admin. has eviscerated institution continuing this path.  nothing like the admin. proposal: </p>

<p>Thanks.</p>
","r, regex, text-mining","<p>According to your specs, you may use the following regex:</p>

<pre><code>\s*[^ /]+/[^ /]+
</code></pre>

<p>See the <a href=""https://regex101.com/r/4pqRly/1"" rel=""nofollow noreferrer"">regex demo</a>.</p>

<p><strong>Details</strong></p>

<ul>
<li><code>\s*</code> - 0 or more whitespace chars</li>
<li><code>[^ /]+</code> (or <code>[^[:space:]/]</code>)  - any 1 or more chars other than space (or whitespace) and <code>/</code></li>
<li><code>/</code> - a slash</li>
<li><code>[^ /]+</code> (or <code>[^[:space:]/]</code>)  - any 1 or more chars other than space (or whitespace) and <code>/</code>.</li>
</ul>

<p><a href=""https://ideone.com/MSqU8m"" rel=""nofollow noreferrer"">R demo</a>:</p>

<pre><code>urldoc = gsub(""\\s*[^ /]+/[^ /]+"","""", urldoc)
</code></pre>

<p>If you want to account for any whitespace, replace the literal space with <code>[:space:]</code>,</p>

<pre><code>urldoc = gsub(""\\s*[^[:space:]/]+/[^[:space:]/]+"","""", urldoc)
</code></pre>
",2,3,897,2018-10-30 19:05:10,https://stackoverflow.com/questions/53071255/how-to-remove-urls-without-http-in-a-text-document-using-r
Fuzzy Matching/Join Two Data Frames of University Names,"<p>I have a list of university names input with spelling errors and inconsistencies. I need to match them against an official list of university names to link my data together. </p>

<p>I know fuzzy matching/join is my way to go, but I'm a bit lost on the correct method. Any help would be greatly appreciated. </p>

<pre><code>d&lt;-data.frame(name=c(""University of New Yorkk"", ""The University of South
 Carolina"", ""Syracuuse University"", ""University of South Texas"", 
""The University of No Carolina""), score = c(1,3,6,10,4))

y&lt;-data.frame(name=c(""University of South Texas"",  ""The University of North
 Carolina"", ""University of South Carolina"", ""Syracuse
 University"",""University of New York""), distance = c(100, 400, 200, 20, 70))
</code></pre>

<p>And I desire an output that has them merged together as closely as possible</p>

<pre><code>matched&lt;-data.frame(name=c(""University of New Yorkk"", ""The University of South Carolina"", 
""Syracuuse University"",""University of South Texas"",""The University of No Carolina""), 
correctmatch = c(""University of New York"", ""University of South Carolina"", 
""Syracuse University"",""University of South Texas"", ""The University of North Carolina""))
</code></pre>
","r, merge, text-mining, fuzzy, fuzzyjoin","<p>I use <code>adist()</code> for things like this and have little wrapper function called <code>closest_match()</code> to help compare a value against a set of ""good/permitted"" values.</p>

<pre><code>library(magrittr) # for the %&gt;%

closest_match &lt;- function(bad_value, good_values) {
  distances &lt;- adist(bad_value, good_values, ignore.case = TRUE) %&gt;%
    as.numeric() %&gt;%
    setNames(good_values)

  distances[distances == min(distances)] %&gt;%
    names()
}

sapply(d$name, function(x) closest_match(x, y$name)) %&gt;%
  setNames(d$name)

University of New Yorkk The University of South\n Carolina               Syracuuse University 
""University of New York""     ""University of South Carolina""           ""University of New York"" 
University of South Texas      The University of No Carolina 
""University of South Texas""     ""University of South Carolina"" 
</code></pre>

<p><code>adist()</code> utilizes <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein distance</a> to compare similarity between two strings.</p>
",1,0,639,2018-10-30 19:53:47,https://stackoverflow.com/questions/53071949/fuzzy-matching-join-two-data-frames-of-university-names
How to get information with python when data is heavily nested,"<p>I have a text file which contains some data to be mined. </p>

<p>The structure is shown below</p>

<pre><code>name (personA {
    field1 : data1
    field2 : data2
    fieldN : dataN
    subfield() {
        fieldx1 : datax1
        fieldxN : dataxN
        }
    }
   name (personB {
    field1 : data11
    field2 : data12
    fieldN : data1N
    }
</code></pre>

<p>In some person's record the subfield is absent and output should specify subfield to be unknown in that case. Now below is the code I use to extract the data</p>

<pre><code>import re
data = dict()
with open('data.txt', 'r') as fin:
    FLAG, FLAGP, FLAGS = False, False, False
    for line in fin:
        if FLAG:
            if re.search('field1', line):
                d1 = line.split()[2]
                data['field1'] = d1
            if re.search('fieldN', line):
                dN = line.split()[2]
                data['fieldN'] = dN
                data['fieldxn'] = 'unknown'
                FLAGP = True
        if FLAGS:
            if re.search('fieldxN', line):
                dsN = line.split()[2]
                data['fieldxn'] = dsN

        if re.search('name\ \(', line):
            pn = line.split()[1]
            FLAG = True
            data['name'] = pn
        if re.search('subfield', line):
            FLAGS = True
        if len(data) == 4:
           if FLAGP:
               print data
               FLAGP = False
               FLAG = False
               FLAGS = False
</code></pre>

<p>The output is shown below</p>

<pre><code>{'field1': 'data1', 'fieldN': 'dataN', 'name': '(personA', 'fieldxn': 'unknown'}
{'field1': 'data11', 'fieldN': 'data1N', 'name': '(personB', 'fieldxn': 'unknown'}
</code></pre>

<p>The problem has been that I don't know where to print data so current I am using below statment to print data which is wrong       </p>

<pre><code>if len(data) == 4:
    if FLAGP:
       print data
       FLAGP = False
       FLAG = False
       FLAGS = False
</code></pre>

<p>I would appreciate if someone could give any suggestion to retrieve the data correctly</p>
","python, parsing, text, text-mining","<p>I would take a different approach to parsing, storing the subfields (and other fields) in a dictionary.</p>

<pre><code>data = open('data.txt', 'rt').read()

### Given a string containing lines of ""fieldX : valueY"" 
### return a dictionary of values
def getFields(field_data):
    fields = {}
    if (field_data != None):
        field_lines = field_data.strip().split(""\n"")
        for pair in field_lines:
            name, value = pair.split("":"")
            fields[name.strip()] = value.strip()
    return fields


### Split the data by name
people_data = data.strip().split(""name ("")[1:]

### Loop though every person record
for person_data in people_data:
    name, person_data = person_data.split("" {"", 1)   # split the name and the fields
    # Split out the subfield data, if any
    subfield_data = None
    if (person_data.find(""subfield()"") &gt; -1):
        field_data, subfield_data = person_data.split(""subfield() {"", 1)
        subfield_data = subfield_data.split(""}"")[0]
    # Separate the fields into single lines of pairs
    fields = getFields(field_data)
    # and any subfields
    subfields = getFields(subfield_data)

    print(""Person: ""+str(name))
    print(""Fields: ""+str(fields))
    print(""Sub_Fields:""+str(subfields))
</code></pre>

<p>Which gives me:</p>

<pre><code>Person: personA
Fields: {'field1': 'data1', 'field2': 'data2', 'fieldN': 'dataN'}
Sub_Fields:{'fieldx1': 'datax1', 'fieldxN': 'dataxN'}
Person: personB
Fields: {'field1': 'data1', 'field2': 'data2', 'fieldN': 'dataN'}
Sub_Fields:{}
</code></pre>

<p>So you could just adjust your output based on whether subfields was <code>None</code>, or otherwise.  The idea is to get your data input into more flexible structures, rather than ""brute-force"" parsing like you have done.  In the above I use <code>split()</code> a lot to give a more flexible way through, rather than relying on finding exact names.  Obviously it depends on your design requirements too.</p>
",1,0,768,2018-11-01 05:48:37,https://stackoverflow.com/questions/53095800/how-to-get-information-with-python-when-data-is-heavily-nested
Comparison Fuzzy R,"<p>I have two datasets, the dataset df1 has a column with the names of companies registered in our CRM and another column with the name of the sales manager. Dataset df2 has a column with the names of companies that have visited an IT event.</p>

<p>The dataset df2, because it was manually entered by the participants, was written with spelling mistakes, abbreviations, etc. That is, similar names for the names of companies registered in CRM.</p>

<p>So the goal is to compare the names of the companies that visited the event in dataset df2, with the names of the companies registered in the dataset df1 and assign these comparisons to the sales manager. Of course, names that are not found or that have a very distant comparison should have the NA value for the salesperson.</p>

<p>I'm new to R and I'm trying various things with little success.</p>

<p>Could you help me build this script?</p>

<p>Below is the example:</p>

<pre><code>                 df1                                 df2  
    |----------------|----------------|       |----------------|
    |    Company     |  Sales Manager |       | Company Event  |
    |----------------|----------------|       |----------------|
    |Customer 1 SA   |Erik            |       |Customer 1      |
    |Customer 2 S\A  |Selma           |       |Customer 1 SA.  |
    |Customer 3 Ltda.|Juca            |       |Customer2       |
    |Customer 4      |Batista         |       |cUSTOIMER 3     |
    |----------------|----------------|       |Customer 10     |
                                              |----------------|
</code></pre>

<p>The final result expected is to have another df with crossed data.</p>

<pre><code>                             matched df  
        |----------------|----------------|----------------|
        | Company Event  |    Company     | Sales Manager  |
        |----------------|----------------|----------------|
        |Customer 1      |Customer 1 SA   |Erik            |
        |Customer 1 SA.  |Customer 1 SA   |Erik            |
        |Customer2       |Customer 2 S\A  |Selma           |
        |cUSTOIMER 3     |Customer 3 Ltda.|Juca            |
        |Customer 10     |NA              |NA              |
        |----------------|----------------|----------------|
</code></pre>
","r, dataframe, data-mining, data-science, text-mining","<p>The following should work. It involves cleaning names, getting the minimum distances, and then getting the sales manager information. </p>

<pre><code>library(stringdist)
# declare data ------------------------------------------------------------
Company &lt;- c(""Customer 1 SA"" ,""Customer 2 S/A"", ""Customer 3 Ltda."", ""Customer 4"")   
SalesManager &lt;- c(""Erik"", ""Selma"", ""Juca"", ""Batista"")
CompanyEvent &lt;- c(""Customer 1"", ""Customer 1 SA."", ""Customer2"" , ""cUSTOIMER 3"", ""Customer 10"")
df1 &lt;- data.frame(Company, SalesManager, stringsAsFactors = F)
df2 &lt;- data.frame(CompanyEvent, stringsAsFactors = F)

# clean 'dirty' names -----------------------------------------------------
df1$cleannames &lt;- gsub(""S/A"", """", df1$Company)
df1$cleannames &lt;- gsub(""SA"", """", df1$cleannames)
df1$cleannames &lt;- gsub(""Ltda."", """", df1$cleannames)
df1$cleannames &lt;- gsub("" "", """", df1$cleannames)
df1$cleannames &lt;-tolower(df1$cleannames)

df2$cleannames  &lt;- gsub(""S/A"", """", df2$CompanyEvent)
df2$cleannames &lt;- gsub(""SA"", """", df2$cleannames)
df2$cleannames &lt;- gsub(""Ltda."", """", df2$cleannames)
df2$cleannames &lt;- gsub("" "", """", df2$cleannames)
df2$cleannames &lt;-tolower(df2$cleannames)

# Get the closest matches and distances -----------------------------------
df2$closestentry &lt;- apply(df2,1, function(x) df1$cleannames[which.min(stringdist(x[""cleannames""], df1$cleannames ))] )
df2$levdistance &lt;- apply(df2,1, function(x) min(stringdist(x[""cleannames""], df1$cleannames )))

#Get sales mgr data using closest matches
df2$salesmgr &lt;- df1$SalesManager[match(df2$closestentry,df1$cleannames )]
df2
&gt; df2
    CompanyEvent cleannames closestentry levdistance salesmgr
1     Customer 1  customer1    customer1           0     Erik
2 Customer 1 SA. customer1.    customer1           1     Erik
3      Customer2  customer2    customer2           0    Selma
4    cUSTOIMER 3 custoimer3    customer3           1     Juca
5    Customer 10 customer10    customer1           1     Erik
</code></pre>

<p>Fuzzy string matching is .. well, fuzzy, so you might have cases that are not what you expect, however you should be fine after a bit of tweaking (here that would be adding <code>customer10</code> into <code>df1</code> for example)</p>

<p>What I mean here by distance is the string distance, see <code>?stringdist</code></p>
",0,0,143,2018-11-01 12:49:37,https://stackoverflow.com/questions/53101625/comparison-fuzzy-r
Extract text between any combination of expressions (list),"<p>I need to extract text between two expressions (beginning &amp; end) from a textfile (the beginning and the end of a letter, which is embedded in a larger file). The problem that I face is that there are multiple potential expressions for both, the beginning and the end of the letter.</p>

<p>I have a list of expressions, which potentially qualify as beginning / end expressions. I need to extract all text between any combination of those expressions from a larger text (including beginning and end expression) and write it to a new file. </p>

<pre><code>sample_text = """"""Some random text 
asdasd
asdasd
asdasd
**Dear my friend,
this is the text I want to extract.
Sincerly,
David**
some other random text
adasdsasd""""""
</code></pre>

<p>My code so far:</p>

<pre><code>letter_begin = [""dear"", ""to our"", ""estimated"", ...]
letter_end = [""sincerly"", ""yours"", ""best regards"", ...]

with open('path/to/input') as infile, open('path/to/output', 'w') as outfile:
    copy = False
    for line in infile:
        if line.strip() == ""dear"": #shortcomming: only 1 Expression possible here
            copy = True
        elif line.strip() == ""sincerly"": #shortcomming: only 1 Expression possible here
            copy = False
        elif copy:
            outfile.write(line)
</code></pre>

<p>The above example includes ""Dear"" as letter_begin expression and ""Sincerly"" as letter_end expression. I need to have a flexible code, which is able to catch any beginning and ending letter expression from the above lists (any potential combination of the expressions; e.g. ""Dear [...] rest regards"" or ""Estimated [...] Sincerly"")</p>
","python, regex, text-mining, text-extraction","<p>We can try using <code>re.findall</code> in dot all and multiline mode, with the following pattern:</p>

<pre><code>Dear\s+.*?Sincerely,\n\S+
</code></pre>

<p>This would capture, and include, everything from the word <code>Dear</code>, up and including <code>Sincerely</code>, followed by everything which follows the next line after <code>Sincerely</code>.  Here is a code sample:</p>

<pre><code>output = re.findall(r""Dear\s+.*?Sincerely,\n\S+"", sample_text, re.MULTILINE|re.DOTALL)
print(output)
</code></pre>

<p><strong>Edit:</strong></p>

<p>If you want to match <em>multiple</em> possible greetings and closings, then we can use an alternation:</p>

<pre><code>letter_begin = [""dear"", ""to our"", ""estimated""]
openings = '|'.join(letter_begin)
print(openings)
letter_end = [""sincerely"", ""yours"", ""best regards""]
closings = '|'.join(letter_end)
regex = r""(?:"" + openings + r"")\s+.*?"" + r""(?:"" + closings + r""),\n\S+""
output = re.findall(regex, sample_text, re.MULTILINE|re.DOTALL|re.IGNORECASE)
print(output)

['Dear my friend,\nthis is the text I want to extract.\nSincerely,\nDavid**']
</code></pre>
",1,2,101,2018-11-05 15:05:23,https://stackoverflow.com/questions/53156977/extract-text-between-any-combination-of-expressions-list
"r dplyr text mining Error in eval(rhs, env, env) : object &#39;score&#39; not found","<p>I'm currently working on an R project and I was defining a function that would perform text mining on a specific dataset. </p>

<p>The general idea is to have a function that counts the number of the text mined and multiply that number with each text's score. </p>

<p>So far I've defined the function with : </p>

<pre><code>function_a &lt;- function(data, dict) {
  data %&gt;% inner_join(dict) %&gt;% count(word) %&gt;% n*score
}
</code></pre>

<p>I am trying to calculate the score by multiplying the number of word appearances with its weightage but I got an error: </p>

<pre><code>Error in eval(rhs, env, env) : object 'score' not found
</code></pre>

<p>Does this mean I have to define score as a function variable? Because it is a column within the dictionary. </p>

<p>Would greatly appreciate any help and insight into this issue. Thank you!</p>

<hr>
","r, function, dplyr, text-mining","<p>You can only reference column variables like <code>score</code> in <code>dplyr</code> functions like <code>select</code>, <code>inner_join</code>, and so on. You tried to reference to <code>score</code> outside of a <code>dplyr</code> function, so R is looking for a variable called <code>score</code> and can not find it. The solution is to use <code>score</code> inside a dplyr function.</p>

<p>Here is an alternative way that should achieve your result by grouping by <code>word</code> and using <code>summarise</code> to get the word count:</p>

<pre><code> function_a &lt;- function(data, dict) {
   data %&gt;% inner_join(dict) %&gt;% group_by(word) %&gt;% summarise(WeightedCount = n()*score[1])
 }
</code></pre>
",1,0,834,2018-11-06 05:03:30,https://stackoverflow.com/questions/53166000/r-dplyr-text-mining-error-in-evalrhs-env-env-object-score-not-found
Python Regex - Extract text between (multiple) expressions in a textfile,"<p>I am a Python beginner and would be very thankful if you could help me with my text extraction problem. </p>

<p>I want to extract all text, which lies between two expressions in a textfile (the beginning and end of a letter). For both, the beginning and the end of the letter there are multiple possible expressions (defined in the lists ""letter_begin"" and ""letter_end"", e.g. ""Dear"", ""to our"", etc.). I want to analyze this for a bunch of files, find below an example of how such a textfile looks like -> I want to extract all text starting from ""Dear"" till ""Douglas"". In cases where the ""letter_end"" has no match, i.e. no letter_end expression is found, the output should start from the letter_beginning and end at the very end of the text file to be analyzed.</p>

<p>Edit: the end of ""the recorded text"" has to be after the match of ""letter_end"" and before the first line with 20 characters or more (as is the case for ""Random text here as well"" -> len=24.</p>

<pre><code>""""""Some random text here
 
Dear Shareholders We
are pleased to provide you with this semiannual report for Fund for the six-month period ended April 30, 2018. For additional information about the Fund, please visit our website a, where you can access quarterly commentaries. We value the trust that you place in us and look forward to serving your investment needs in the years to come.
Best regards 
Douglas

Random text here as well""""""
</code></pre>

<p>This is my code so far - but it is not able to flexible catch the text between the expressions (there can be anything (lines, text, numbers, signs, etc.) before the ""letter_begin"" and after the ""letter_end"")</p>

<pre><code>import re

letter_begin = [""dear"", ""to our"", ""estimated""] # All expressions for ""beginning"" of letter 
openings = ""|"".join(letter_begin)
letter_end = [""sincerely"", ""yours"", ""best regards""] # All expressions for ""ending"" of Letter 
closings = ""|"".join(letter_end)
regex = r""(?:"" + openings + r"")\s+.*?"" + r""(?:"" + closings + r""),\n\S+""


with open(filename, 'r', encoding=""utf-8"") as infile:
         text = infile.read()
         text = str(text)
         output = re.findall(regex, text, re.MULTILINE|re.DOTALL|re.IGNORECASE) # record all text between Regex (Beginning and End Expressions)
         print (output)
</code></pre>

<p>I am very thankful for every help!</p>
","python, regex, text-mining, text-extraction","<p>You may use</p>

<pre><code>regex = r""(?:{})[\s\S]*?(?:{}).*(?:\n.*){{0,2}}"".format(openings, closings)
</code></pre>

<p>This pattern will result in a regex like</p>

<pre><code>(?:dear|to our|estimated)[\s\S]*?(?:sincerely|yours|best regards).*(?:\n.*){0,2}
</code></pre>

<p>See the <a href=""https://regex101.com/r/PmU3Ti/2"" rel=""nofollow noreferrer"">regex demo</a>. Note you should not use <code>re.DOTALL</code> with this pattern, and the <code>re.MULTILINE</code> option is also redundant.</p>

<p><strong>Details</strong></p>

<ul>
<li><code>(?:dear|to our|estimated)</code> - any of the three values</li>
<li><code>[\s\S]*?</code> - any 0+ chars, as few as possible</li>
<li><code>(?:sincerely|yours|best regards)</code> - any of the three values</li>
<li><code>.*</code> - any 0+ chars other than newline</li>
<li><code>(?:\n.*){0,2}</code> - zero, one or two repetitions of a newline followed with any 0+ chars other than newline.</li>
</ul>

<p><a href=""https://ideone.com/i6Be5n"" rel=""nofollow noreferrer"">Python demo code</a>:</p>

<pre><code>import re
text=""""""Some random text here

Dear Shareholders We
are pleased to provide you with this semiannual report for Fund for the six-month period ended April 30, 2018. For additional information about the Fund, please visit our website a, where you can access quarterly commentaries. We value the trust that you place in us and look forward to serving your investment needs in the years to come.
Best regards 
Douglas

Random text here as well""""""
letter_begin = [""dear"", ""to our"", ""estimated""] # All expressions for ""beginning"" of letter 
openings = ""|"".join(letter_begin)
letter_end = [""sincerely"", ""yours"", ""best regards""] # All expressions for ""ending"" of Letter 
closings = ""|"".join(letter_end)
regex = r""(?:{})[\s\S]*?(?:{}).*(?:\n.*){{0,2}}"".format(openings, closings)
print(regex)
print(re.findall(regex, text, re.IGNORECASE))
</code></pre>

<p>Output:</p>

<pre><code>['Dear Shareholders We\nare pleased to provide you with this semiannual report for Fund for the six-month period ended April 30, 2018. For additional information about the Fund, please visit our website a, where you can access quarterly commentaries. We value the trust that you place in us and look forward to serving your investment needs in the years to come.\nBest regards \nDouglas\n']
</code></pre>
",1,1,1717,2018-11-06 09:55:14,https://stackoverflow.com/questions/53169493/python-regex-extract-text-between-multiple-expressions-in-a-textfile
Removing string pattern from dataframe (Twitter data in RStudio),"<p>I have a large dataframe (~500,000 observations) consisting of Twitter data (i.e. username, rewtweet counts, text) in RStudio. I want to run a text analysis on the tweets, but I first need to remove retweet tags so they don't affect my keyword searches. </p>

<p>For example, in tweets that are retweets, the text looks like this: <code>RT @BobsAccount Great article! Can't wait to learn more.</code> I want to remove the string attached to <code>RT @....</code>.   </p>

<p>I have used <code>lapply</code> and <code>gsub</code> to remove specific characters. For example, this successfully removed ""@"" : <code>data &lt;- data.frame(lapply(data, function(x) {gsub(""@"","""", x)}))</code></p>

<p>But I can't figure out how to remove a ""string pattern"" (i.e. any text attached to ""RT @""). Any help would be greatly appreciated!</p>
","r, string, pattern-matching, lapply, text-mining","<p>You may use</p>

<pre><code>data &lt;- data.frame(lapply(data, function(x) {gsub(""\\bRT\\s+@\\S*\\s*"","""", x)}))
</code></pre>

<p>The <code>\bRT\s+@\S*\s*</code> pattern matches</p>

<ul>
<li><code>\bRT</code> - a whole word <code>RT</code></li>
<li><code>\s+</code> - 1+ whitespaces</li>
<li><code>@</code>  - a <code>@</code> char</li>
<li><code>\S*</code> - 0+ non-whitespace chars</li>
<li><code>\s*</code> - 0+ whitespace chars</li>
</ul>

<p>See the <a href=""https://regex101.com/r/EbejUw/2"" rel=""nofollow noreferrer"">regex demo</a>.</p>

<p>R code sample:</p>

<pre><code>text &lt;- c(""RT @BobsAccount Great article! Can't wait to learn more."")
data &lt;- data.frame(text)
data &lt;- data.frame(lapply(data, function(x) {gsub(""\\bRT\\s+@\\S*\\s*"","""", x)}))
data
## =&gt;                                       text
##     1 Great article! Can't wait to learn more.
</code></pre>
",0,1,142,2018-11-07 08:32:38,https://stackoverflow.com/questions/53185829/removing-string-pattern-from-dataframe-twitter-data-in-rstudio
Extract rows from dataframe that have keywords in them (Twitter data in RStudio),"<p>I have a large dataframe (~500,000 observations) consisting of structured Twitter data (i.e. username, rewtweet counts, text) in RStudio. I want to run a text analysis on the tweets so I can extract observations that have one or more keywords in the tweet text.</p>

<p>I have uploaded my keywords as <code>keywords_C &lt;- c(""climate change"",""climate"",""climatechange"",""global warming"",""globalwarming"")</code> . Tweet text is stored in my dataframe in a column labelled <code>text</code>. </p>

<p>How do I make a new dataframe containing only observations where one or more of the keywords are present in the <code>text</code> column? Alternatively, can I delete observations where the keywords are not present? </p>

<hr>

<p>My dataframe is called <code>NewCData</code></p>

<p><code>dput(droplevels(head(NewCData, 10)))</code></p>

<pre><code>  structure(list(timestamp = structure(c(1L, 3L, 2L, 6L, 4L, 4L, 
    5L, 8L, 7L, 9L), .Label = c(""2015-10-30 21:37:58"", ""2015-10-30 21:38:02"", 
    ""2015-10-30 21:38:03"", ""2015-10-30 21:38:06"", ""2015-10-30 21:38:07"", 
    ""2015-10-30 21:38:10"", ""2015-10-30 21:38:14"", ""2015-10-30 21:38:32"", 
    ""2015-10-30 21:39:04""), class = ""factor""), id_str = structure(c(1L, 
    3L, 2L, 7L, 4L, 5L, 6L, 9L, 8L, 10L), .Label = c(""660209050429186048"", 
    ""660209067584016384"", ""660209072768212992"", ""660209083505504256"", 
    ""660209086143688704"", ""660209087628578816"", ""660209102790914048"", 
    ""660209119152893952"", ""660209195162206208"", ""660209325986549760""
    ), class = ""factor""), user.id_str = structure(c(1L, 3L, 8L, 5L, 
    5L, 2L, 4L, 6L, 9L, 7L), .Label = c(""277335277"", ""32380087"", 
    ""325105950"", ""33398863"", ""68956490"", ""808114195"", ""87712431"", 
    ""90280824"", ""949996219""), class = ""factor""), user.followers_count = structure(c(7L, 
    2L, 8L, 4L, 4L, 3L, 6L, 9L, 5L, 1L), .Label = c(""10212"", ""1062"", 
    ""1389"", ""15227"", ""2214"", ""2851"", ""38"", ""4137"", ""55""), class = ""factor""), 
        ideology = structure(c(2L, 4L, 3L, 9L, 9L, 5L, 8L, 6L, 1L, 
        7L), .Label = c(""-0.309303177803536"", ""-0.393703659798908"", 
        ""-0.795976086971656"", ""-0.811321629152632"", ""-0.946143178314071"", 
        ""-1.16317298915931"", ""0.353843466445817"", ""1.09919837237897"", 
        ""2.29286233202781""), class = ""factor""), text = structure(c(2L, 
        9L, 4L, 1L, 3L, 10L, 5L, 7L, 6L, 8L), .Label = c(""Better Dead than Red! Bill Gates says that only socialism can save us "", 
        ""Expert briefing on  #disarmament #SDGs @NMUN "", 
        ""I see red people Bill Gates says that only socialism can save us from climate change "", 
        ""RT: Oddly enough, some Republicans think climate change is real: Oddly enough,…  #UniteBlue "", 
        ""Ted Cruz: ‘Climate change is not science, it’s religion’  via @glennbeck"", 
        ""This is an amusing headline: \""Bill Gates says that only socialism can save us from climate change\"""", 
        ""Unusual Weather Kills Gulf of Maine Cod : Discovery News #globalwarming  "", 
        ""What do the remaining Republican candidates have to say about climate change? #FixGov"", 
        ""Who Uses #NASA Earth Science Data? He looks at impact of #aerosols on #climate #weather!"", 
        ""Why go for ecosystem basses conservation! #ClimateChange #Raajje #Maldives""
        ), class = ""factor"")), .Names = c(""timestamp"", ""id_str"", 
    ""user.id_str"", ""user.followers_count"", ""ideology"", ""text""), row.names = c(NA, 
    10L), class = ""data.frame"")
</code></pre>
","twitter, rstudio, extract, keyword, text-mining","<p>You may use</p>

<pre><code>new_df &lt;- NewCData[with(NewCData, grepl(paste0(""\\b(?:"",paste(keywords_C, collapse=""|""),"")\\b""), text)),]
</code></pre>

<p>See the <a href=""https://ideone.com/dMkwTA"" rel=""nofollow noreferrer"">R demo online</a></p>

<p>The point here is to combine the keywords into a pattern like</p>

<pre><code>\b(?:climate change|climate|climatechange|global warming|globalwarming)\b
</code></pre>

<p>It will match the words as whole words and if there is a match in the <code>text</code> column, the row will be returned, else, the row will get discarded.</p>
",1,1,239,2018-11-07 11:27:38,https://stackoverflow.com/questions/53188579/extract-rows-from-dataframe-that-have-keywords-in-them-twitter-data-in-rstudio
How to quickly apply over Document Term Matrix in R,"<p>I am working on a project that requires me to iterate over a Document Term Matrix, converting all non-zero values to 1 and keeping zero values at zero. The function I'm using now takes forever to run, and I would like help optimizing the code.</p>

<p>My code as it is right now is</p>

<pre><code>convert_counts &lt;- function(x) {
                    x &lt;- ifelse(x &gt; 0, 1, 0)
                    x &lt;- factor(x, levels = c(0, 1), 
                    labels = c(""No"", ""Yes""))}

data_exp &lt;- apply(data_dtm, 2, convert_counts)
</code></pre>

<p>Where <code>data_dtm</code> is a large Document Term Matrix.</p>
","r, text-mining","<p>The function you have transforms a sparse matrix to a full character matrix. If you have a large document term matrix this will result in long running times and a good chance of getting a memory error. Replacing values in a sparse matrix can be done quickly if you make use of how the matrix is built. A sparse matrix values are stored in the <code>v</code> (values) part of the matrix. See <code>?slam::simple_triplet_matrix</code>. </p>

<p>Using any of the apply family on a sparse matrix, without using functions that are designed to work with a sparse matrix will turn it into a normal (dense) matrix. With accordingly long run times and memory issues.  </p>

<p>To change all values different from 0 in your case, just use the following:</p>

<p><code>data_dtm$v[data_dtm$v &gt; 0] &lt;- 1</code>
<code>inspect(data_dtm) # show first 10 columns and rows</code></p>

<p>This replaces all the values to 1 and keeps the data as a document term matrix (aka nice and sparse). </p>

<p>Depending on your follow up data analysis you really should make use of sparse matrix functions. If you want to transform a large document term matrix into a data.frame or data.table you have a good chance of running out of memory. </p>

<p><em>For any follow up questions, please include a reproducible example and an expected output.</em></p>
",0,0,298,2018-11-14 15:53:16,https://stackoverflow.com/questions/53304091/how-to-quickly-apply-over-document-term-matrix-in-r
Extracting speaker interventions from a text using R? Or something else?,"<p>We're working on a text mining project for school on the proportion of environment-oriented speech in Quebec's National Assembly. We wanna extract a list of every speaker's interventions throughout the years.</p>

<p>Our documents are all formatted this way:</p>

<pre><code>Mr. Smith : Blablabla

Mrs. Jones : Blablabla
</code></pre>

<p>What I would like to do is write the simplest thing possible that would allow me to extract these interventions. I'm thinking something along the lines of: </p>

<p><em>""Every time you see [Mr. **** : ] OR [Mrs. **** : ], extract ALL the text until you see another occurrence of [Mr. **** : ] OR [Mrs. **** : ]. And, ideally, extract all the Mr. Smiths and the Mrs. Joneses and the Mr. Williams in separate files while keeping track of which file the interventions came from.</em></p>

<p>I started writing a very basic <code>gsub</code> line which allowed me to replace the occurrences I wanted to replace with an @, only to realize I don't want to replace them completely but rather maybe just add an <code>@</code> in front which would probably make it easier to write something that would just separate the @s in distinct files.</p>

<pre><code>gsub(""(Mr.|Mrs.)\\s\\w*\\s:\\s"", ""@"", test)
</code></pre>

<p>I've just started teaching myself R for this project and I need some insight on how I should proceed next. Or should I use something else instead?</p>
","r, text-mining","<p>If you don't want to replace the speaker names, you can use what is called 'positive look ahead', like this:</p>

<pre><code># some example data:
bla &lt;- c(""Mr. X : blablabla bla bla bla. Mrs. Y : bla bla blablablab Mr. XY : bla bla balblabla blabl abl"" )

# replace with look ahead:
gsub(""(?=(Mr.|Mrs.))"", ""@ "", bla, perl = T)
""@ Mr. X : blablabla bla bla bla. @ Mrs. Y : bla bla blablablab @ Mr. XY : bla bla balblabla blabl abl""
</code></pre>

<p>The @ is a good starting point for extracting the individual interventions. This can be done thus:</p>

<pre><code>pattern &lt;- ""@.[^@]*"" 
matches &lt;- gregexpr(pattern, bla)
interventions &lt;- regmatches(bla, matches)
interventions &lt;- unlist(interventions)
interventions
[1] ""@ Mr. X : blablabla bla bla bla. ""      ""@ Mrs. Y : bla bla blablablab ""         ""@ Mr. XY : bla bla balblabla blabl abl""
</code></pre>
",0,0,178,2018-11-19 04:12:04,https://stackoverflow.com/questions/53368202/extracting-speaker-interventions-from-a-text-using-r-or-something-else
Find nonsense words in a text,"<p>I have a dataset with answers of user if they know a brand or not. Some of the users just answered nonsense, as you can see in my example. </p>

<pre><code>meinstring &lt;- c(""----asdada"", ""no idea"", ""C&amp;A"", ""aaaaaaaaaa"", ""---"", ""adaosdjasodajsdoad"")


spamidenfifier &lt;- function(x) {
  verhaeltnis &lt;- str_count(tolower(x), ""[aeoiu]"") / str_count(x)
  sequenz &lt;- sum(sequence(rle(as.character(data.frame(strsplit(as.character(x), """"))[,1]))$lengths) &gt;= 3, na.rm = TRUE)
  if(str_count(x) &gt; 4) { weight &lt;- 0.9 }  else {  weight &lt;- 1  } ## Gewicht, weil unwahrscheinlicher bei längerem String
  variation_buchstaben &lt;- (length(unique(data.frame(strsplit(as.character(x), """"))[,1])) / str_count(x) * weight)
  if(verhaeltnis &lt; 0.2 | verhaeltnis &gt; 0.8 | sequenz &gt; 0 | variation_buchstaben &lt; 0.5) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}


sapply(meinstring, spamidenfifier)
</code></pre>

<p>Output:</p>

<pre><code>----asdada            no idea                C&amp;A         aaaaaaaaaa                --- adaosdjasodajsdoad 
      TRUE              FALSE              FALSE               TRUE               TRUE              FALSE 
</code></pre>

<p>My function does not work too bad, however there might be better solutions. Is there a package or better method to identify if a word was just misspelled or a person answered nonsense. 
If not, suggestions to improve that function are highly appreciated! </p>

<p>edit: Updated some improvements :-)</p>
","r, text-mining, stringr, stringi","<p>Just my spontaneous idea:</p>

<pre><code>meinstring &lt;- c(""----asdada"", ""no idea"", ""C&amp;A"", ""aaaaaaaaaa"", ""---"", ""adaosdjasodajsdoad"", ""+-*-"", ""*-+-"", ""adfpdflrraaeea"")

grepl('^\\W+$|(?:[-!@#$%^&amp;*\\[\\]()"";:_&lt;&gt;.,=+/ ]){2,}|[-!@#$%^&amp;*\\[\\]()"";:_&lt;&gt;.,=+/ ]{3,}|[aeoiu]{3,}',
meinstring , perl = T) &amp; !grepl(""iou|zweieiig"", meinstring) # add the exceptions in the second grepl.

[1]  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
</code></pre>

<hr>

<p>There is no neat perfect solution.</p>
",0,3,440,2018-11-19 15:54:21,https://stackoverflow.com/questions/53378325/find-nonsense-words-in-a-text
Remove all punctuation from text including apostrophes for tm package,"<p>I have a of vector consisting of Tweets (just the message text) that I am cleaning for text mining purposes. I have used <code>removePunctuation</code> from the <code>tm</code> package like so: </p>

<pre><code>clean_tweet_text = removePunctuation(tweet_text)
</code></pre>

<p>This have resulted in a vector with all punctuation removed from the text <em>except</em> apostrophes, which ruins my keyword searches because words touching apostrophes are not registered. For example, one of my keywords is <code>climate</code> but if a tweet has <code>'climate</code> it won't be counted.</p>

<p>How can I removes all the apostrophes/single quotes from my vector?</p>

<p>Here is the header from <code>dput</code> for a reproducible example:</p>

<pre><code>c(""expert briefing on climatechange disarmament sdgs nmun httpstco5gqkngpkap"", 
""who uses nasa earth science data he looks at impact of aerosols on climateamp weather httpstcof4azsiqkw1 https…"", 
""rt oddly enough some republicans think climate change is real oddly enough… httpstcomtlfx1mnuf uniteblue https…"", 
""better dead than red bill gates says that only socialism can save us from climate change httpstcopypqmd1fok"", 
""i see red people bill gates says that only socialism can save us from climate change httpstcopypqmd1fok"", 
""why go for ecosystem basses conservation climatechange raajje maldives ecocaremv httpstcorauhjbasyl"", 
""ted cruz ‘climate change is not science it’s religion’ httpstco0qqtbofe0h via glennbeck"", 
""unusual warming kills gulf of maine cod  discovery news globalwarming  httpstco39uvock3xe"", 
""this is an amusing headline bill gates says that only socialism can save us from climate change httpstcobfs5zbcijc"", 
""what do the remaining republican candidates have to say about climate change fixgov httpstcoxpszwbrcnh httpstcodgqyidkw6o""
)
</code></pre>
","r, text-mining, tm","<p>To remove all punctuation (including apostrophes and single quotes), you can just use <code>gsub()</code>:</p>



<pre class=""lang-r prettyprint-override""><code>x &lt;- c(""expert briefing on climatechange disarmament sdgs nmun httpstco5gqkngpkap"",
       ""who uses nasa earth science data he looks at impact of aerosols on climateamp weather httpstcof4azsiqkw1 https…"",
       ""rt oddly enough some republicans think climate change is real oddly enough… httpstcomtlfx1mnuf uniteblue https…"",
       ""better dead than red bill gates says that only socialism can save us from climate change httpstcopypqmd1fok"",
       ""i see red people bill gates says that only socialism can save us from climate change httpstcopypqmd1fok"",
       ""why go for ecosystem basses conservation climatechange raajje maldives ecocaremv httpstcorauhjbasyl"",
       ""ted cruz ‘climate change is not science it’s religion’ httpstco0qqtbofe0h via glennbeck"",
       ""unusual warming kills gulf of maine cod discovery news globalwarming httpstco39uvock3xe"",
       ""this is an amusing headline bill gates says that only socialism can save us from climate change httpstcobfs5zbcijc"",
       ""what do the remaining republican candidates have to say about climate change fixgov httpstcoxpszwbrcnh httpstcodgqyidkw6o"")

gsub(""[[:punct:]]"", """", x)
#&gt;  [1] ""expert briefing on climatechange disarmament sdgs nmun httpstco5gqkngpkap""                                                
#&gt;  [2] ""who uses nasa earth science data he looks at impact of aerosols on climateamp weather httpstcof4azsiqkw1 https""           
#&gt;  [3] ""rt oddly enough some republicans think climate change is real oddly enough httpstcomtlfx1mnuf uniteblue https""            
#&gt;  [4] ""better dead than red bill gates says that only socialism can save us from climate change httpstcopypqmd1fok""              
#&gt;  [5] ""i see red people bill gates says that only socialism can save us from climate change httpstcopypqmd1fok""                  
#&gt;  [6] ""why go for ecosystem basses conservation climatechange raajje maldives ecocaremv httpstcorauhjbasyl""                      
#&gt;  [7] ""ted cruz climate change is not science its religion httpstco0qqtbofe0h via glennbeck""                                     
#&gt;  [8] ""unusual warming kills gulf of maine cod discovery news globalwarming httpstco39uvock3xe""                                  
#&gt;  [9] ""this is an amusing headline bill gates says that only socialism can save us from climate change httpstcobfs5zbcijc""       
#&gt; [10] ""what do the remaining republican candidates have to say about climate change fixgov httpstcoxpszwbrcnh httpstcodgqyidkw6o""
</code></pre>

<p><code>gsub()</code> replaces all occurrences of its first argument in its third argument with its second argument (see <code>help(""gsub"")</code>). Here, that means it replaces all occurrences in our vector <code>x</code> of any of the characters in the set <code>[[:punct:]]</code> with <code>""""</code> (remove them).</p>

<p>What characters does that remove? From <code>help(""regex"")</code>:</p>

<blockquote>
  <p>[:punct:]  </p>
  
  <p>&nbsp;&nbsp;&nbsp;&nbsp;Punctuation characters:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;! "" # $ % &amp; ' ( ) * + , - . / : ; &lt; = > ? @ [ \ ] ^ _ ` { | } ~.</p>
</blockquote>

<h3>Update</h3>

<p>It appears this occurs because your apostrophes are like <code>‘</code> instead of like <code>'</code>. So, if you want to stick with <code>tm::removePunctuation()</code>, you can also use</p>

<pre class=""lang-r prettyprint-override""><code>tm::removePunctuation(x, ucp = TRUE)
#&gt;  [1] ""expert briefing on climatechange disarmament sdgs nmun httpstco5gqkngpkap""                                                
#&gt;  [2] ""who uses nasa earth science data he looks at impact of aerosols on climateamp weather httpstcof4azsiqkw1 https""           
#&gt;  [3] ""rt oddly enough some republicans think climate change is real oddly enough httpstcomtlfx1mnuf uniteblue https""            
#&gt;  [4] ""better dead than red bill gates says that only socialism can save us from climate change httpstcopypqmd1fok""              
#&gt;  [5] ""i see red people bill gates says that only socialism can save us from climate change httpstcopypqmd1fok""                  
#&gt;  [6] ""why go for ecosystem basses conservation climatechange raajje maldives ecocaremv httpstcorauhjbasyl""                      
#&gt;  [7] ""ted cruz climate change is not science its religion httpstco0qqtbofe0h via glennbeck""                                     
#&gt;  [8] ""unusual warming kills gulf of maine cod discovery news globalwarming httpstco39uvock3xe""                                  
#&gt;  [9] ""this is an amusing headline bill gates says that only socialism can save us from climate change httpstcobfs5zbcijc""       
#&gt; [10] ""what do the remaining republican candidates have to say about climate change fixgov httpstcoxpszwbrcnh httpstcodgqyidkw6o""
</code></pre>
",7,2,1443,2018-11-20 12:14:22,https://stackoverflow.com/questions/53392785/remove-all-punctuation-from-text-including-apostrophes-for-tm-package
Splitting and grouping plain text (grouping text by chapter in dataframe)?,"<p>I have a data frame/tibble where I've imported a file of plain text (txt). The text very consistent and is grouped by chapter. Sometimes the chapter text is only one row, sometimes it's multiple row. Data is in one column like this:</p>

<pre><code># A tibble: 10,708 x 1
   x                                                                     
   &lt;chr&gt;                                                                                                                                   
 1 ""Chapter 1 ""                                                          
 2 ""Chapter text. ""     
 3 ""Chapter 2 ""                                                          
 4 ""Chapter text. ""    
 5 ""Chapter 3 ""
 6 ""Chapter text. ""
 7 ""Chapter text. ""
 8 ""Chapter 4 ""   
</code></pre>

<p>I'm trying to clean the data to have a new column for Chapter and the text from each chapter in another column, like this:</p>

<pre><code># A tibble: 10,548 x 2
   x                                Chapter   
   &lt;chr&gt;                             &lt;chr&gt;
 1 ""Chapter text. ""               ""Chapter 1 ""
 2 ""Chapter text. ""               ""Chapter 2 ""
 3 ""Chapter text. ""               ""Chapter 3 "" 
 4 ""Chapter text. ""               ""Chapter 4 "" 
</code></pre>

<p>I've been trying to use regex to split the and group the data at each occurance of the word 'Chapter #' (chapter followed by a number, but cannot get the result I want. Any advice is much appreciated.</p>
","r, nlp, text-mining, tidytext","<p>Based on <em>""Sometimes the chapter text is only one row, sometimes it's multiple row""</em> I am assuming text in rows 6 and 7 belong to chapter 3 and there is no text for chapter 4 in your test data (Your desired output is probably a bit wrong).</p>

<p>Here's a way using <code>dplyr</code> and <code>tidyr</code>. Just run it piece-by-piece and you'll see how the data gets transformed.</p>

<pre><code>df %&gt;% 
  mutate(
    id = cumsum(grepl(""[0-9].$"", x)),
    x = ifelse(grepl(""[0-9].$"", x), paste0(x, "":""), x)
  ) %&gt;% 
  group_by(id) %&gt;% 
  summarize(
    chapter = paste0(x, collapse = """")
  ) %&gt;% 
  separate(chapter, into = c(""chapter"", ""text""), sep = "":"", extra = ""merge"")

# A tibble: 4 x 3
     id chapter      text                          
  &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;                         
1     1 ""Chapter 1 "" ""Chapter text. ""              
2     2 ""Chapter 2 "" ""Chapter text. ""              
3     3 ""Chapter 3 "" ""Chapter text. Chapter text. ""
4     4 ""Chapter 4 "" """"     
</code></pre>

<p>Data -</p>

<pre><code>df &lt;- structure(list(x = c(""Chapter 1 "", ""Chapter text. "", ""Chapter 2 "", 
""Chapter text. "", ""Chapter 3 "", ""Chapter text. "", ""Chapter text. "", 
""Chapter 4 "")), .Names = ""x"", class = ""data.frame"", row.names = c(NA, 
-8L))
</code></pre>
",1,1,637,2018-11-20 23:44:31,https://stackoverflow.com/questions/53403287/splitting-and-grouping-plain-text-grouping-text-by-chapter-in-dataframe
TypeError: test() missing 1 required positional argument,"<p>I want to predict a score for each sentence in a text. I have written this test method:</p>

<pre><code>def test(sent):
    # Predict for a given sentence
    if sent != """":
        input, seq_lengths, target = make_variables([sent], [])
        output = classifier(input, seq_lengths)
        pred = output.data.max(1, keepdim=True)[1]
        score = pred.cpu().numpy()[0][0]
        print(""The sentence is:"",sent, ""The score is:"", score)
        return


    print(""evaluating trained model ..."")
    total_mse=0

    for sents, scores in test_loader:
        input, seq_lengths, target = make_variables(sents, scores)
        output = classifier(input, seq_lengths)
        pred = output.data.max(1, keepdim=True)[1]
        error=mean_squared_error(pred,target.data.view_as(pred.float()))
        total_mse +=error
    print("" **********  Total MSE is   **********"",total_mse)
    return
</code></pre>

<p>In a part of main method, I have:</p>

<pre><code># Testing
test("""")
# Testing for a given sample _a sentence_
test(""For instance, wolves prey on moose, which are too big for coyotes."")
</code></pre>

<p>But I received this error:</p>

<blockquote>
  <p>Error Traceback (most recent call last):   File
  ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/unittest/case.py"",
  line 59, in testPartExecutor
      yield   File ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/unittest/case.py"",
  line 601, in run
      testMethod()   File ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/site-packages/nose/case.py"",
  line 198, in runTest
      self.test(*self.arg) Exception: test() missing 1 required positional argument: 'sent'
  -------------------- >> begin captured logging &lt;&lt; -------------------- gensim.models.doc2vec: DEBUG: Fast version of gensim.models.doc2vec is
  being used summa.preprocessing.cleaner: INFO: 'pattern' package not
  found; tag filters are not available for English gensim.utils: INFO:
  loading KeyedVectors object from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  gensim.utils: INFO: loading syn0 from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved.syn0.npy
  with mmap=None gensim.utils: INFO: setting ignored attribute syn0norm
  to None gensim.utils: INFO: loaded
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  --------------------- >> end captured logging &lt;&lt; ---------------------</p>
  
  <p>E
  ====================================================================== ERROR: mahsa_rnn_sent_classification.test
  ---------------------------------------------------------------------- Traceback (most recent call last):   File
  ""/home/mahsa/anaconda3/envs/pytorch_env/lib/python3.5/site-packages/nose/case.py"",
  line 198, in runTest
      self.test(*self.arg) TypeError: test() missing 1 required positional argument: 'sent'
  -------------------- >> begin captured logging &lt;&lt; -------------------- gensim.models.doc2vec: DEBUG: Fast version of gensim.models.doc2vec is
  being used summa.preprocessing.cleaner: INFO: 'pattern' package not
  found; tag filters are not available for English gensim.utils: INFO:
  loading KeyedVectors object from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  gensim.utils: INFO: loading syn0 from
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved.syn0.npy
  with mmap=None gensim.utils: INFO: setting ignored attribute syn0norm
  to None gensim.utils: INFO: loaded
  /home/mahsa/PycharmProjects/PyTorch_env_project/Thesis/proj2/glove_saved
  --------------------- >> end captured logging &lt;&lt; ---------------------</p>
  
  <p>---------------------------------------------------------------------- Ran 1 test in 0.004s</p>
  
  <p>FAILED (errors=1)</p>
</blockquote>

<p>I think that test() has its argument 'sent'. How can I correct this error?</p>
","python, pycharm, pytest, text-mining, nose","<p>Running tests with <code>nose</code> or <a href=""https://docs.python.org/3.8/library/unittest.html"" rel=""nofollow noreferrer""><code>unittest</code></a> is not trivial, if you're new at this, I think you would have an easier time getting started with something like <a href=""https://docs.pytest.org/en/latest/"" rel=""nofollow noreferrer""><code>pytest</code></a></p>
",-1,0,2657,2018-11-24 06:43:55,https://stackoverflow.com/questions/53455834/typeerror-test-missing-1-required-positional-argument
R Creating co-occurrence matrix,"<p>My question is about text mining, and text processing.
I would like to build a co-occurrence matrix from my data.
My data is:</p>

<pre><code>dat &lt;- read.table(text=""id_reférence id_paper
        621107   621100
        621100   621101
        621107   621102
        621109   621103
        621105   621104
        621103   621105
        621109   621106
        621106   621107
        621107   621108
        621106   621109"", header=T)

expected &lt;- matrix(0,10,10)
### Article 1 has been cited by article 2
expected[2, 1] &lt;- 1
</code></pre>

<p>Thanks in advance :)</p>
","r, matrix, text, text-mining, adjacency-matrix","<p>Here another approach using <code>data.table</code>. A bottleneck might be that below approach does not end up in a <code>sparseMatrix</code>. Depending on the size of your data set it might be worth checking an approach aiming at a sparse data object. </p>

<pre><code>library(data.table)
setDT(dat)
# split id_reférence column into multiple rows by comma
# code for this step taken from: #https://stackoverflow.com/questions/13773770/split-comma-separated-strings-in-a-column-into-separate-rows
dat = dat[, strsplit(as.character(id_reférence), "","", fixed=TRUE),
   by = .(id_paper, id_reférence)][, id_reférence := NULL][
    , setnames(.SD, ""V1"", ""id_reférence"")]
# add value column for casting
dat[, cite:= 1]
# cast you data into long format
dat = dcast(dat, id_paper ~ id_reférence, fill = 0)[, id_paper:= NULL]
</code></pre>
",0,0,1049,2018-11-24 19:48:24,https://stackoverflow.com/questions/53461799/r-creating-co-occurrence-matrix
Splitting a string using a pattern in R,"<p>This question is building on my previous question regarding <a href=""https://stackoverflow.com/questions/53403287/splitting-and-grouping-plain-text-grouping-text-by-chapter-in-dataframe"">Splitting and grouping plain text (grouping text by chapter in dataframe)?</a></p>

<p>With Shree's help I've been able to get most of my document cleaned up! Have been able to create two column from a list - the first column is chapter number and the second column is the text that belongs to that chapter, but I ran into some messier text.</p>

<p>This is a worst case scenario example of my data:</p>

<pre><code>                                               x
1                                     Chapter 1.
2                              Chapter one text.
3 Chapter one text. Chapter 2. Chapter two text.
4                              Chapter two text.
5                                     Chapter 3.
6                            Chapter three text.
7                            Chapter three text.
8                   Chapter 4. Chapter four text
9                             Chapter four text.

df &lt;- structure(list(x = c(""Chapter 1. "", ""Chapter one text. "", ""Chapter one text. Chapter 2. Chapter two text. "", 
                           ""Chapter two text. "", ""Chapter 3. "", ""Chapter three text. "", ""Chapter three text. "", 
                           ""Chapter 4. Chapter four text "",""Chapter four text. "")), 
                .Names = ""x"", class = ""data.frame"", row.names = c(NA, -9L))
</code></pre>

<p>I need to get it structured like this (Chapter number and then chapter text for that chapter in ID order), so that I can apply the function from my previous post and split it cleanly:</p>

<pre><code>                       x
1           Chapter 1. 
2    Chapter one text. 
3     Chapter one text.
4            Chapter 2.
5    Chapter two text. 
6    Chapter two text. 
7           Chapter 3. 
8  Chapter three text. 
9  Chapter three text. 
10           Chapter 4.
11   Chapter four text 
12  Chapter four text. 
</code></pre>

<p>This seems like a straightforward problem where I could split the string using regex looking for Chapter # (""Chapter [0-9]"") and then split it again with similar logic to get the chapter and the text into separate rows. However, I'm stuck here after trying many attempts with <code>str_split</code>, <code>gsub</code>, <code>separate_rows</code> functions.</p>

<p>Any help is appreciated.</p>
","r, regex, text-mining, strsplit","<p>We could use <code>separate_rows</code> by splitting at the space after the <code>.</code> (Here, we used a regex lookaround to match the space (<code>\\s</code>) after a dot.</p>

<pre><code>library(tidyverse)
df %&gt;% 
   separate_rows(x, sep=""(?&lt;=[.])\\s"") %&gt;% 
   filter(x!='')
#                  x
#1           Chapter 1.
#2    Chapter one text.
#3    Chapter one text.
#4           Chapter 2.
#5    Chapter two text.
#6    Chapter two text.
#7           Chapter 3.
#8  Chapter three text.
#9  Chapter three text.
#10          Chapter 4.
#11  Chapter four text 
#12  Chapter four text.
</code></pre>
",1,1,113,2018-11-26 19:43:05,https://stackoverflow.com/questions/53487947/splitting-a-string-using-a-pattern-in-r
R feature extraction for text,"<p>My question is about text mining, and text processing.</p>

<p>I would like to build a dataframe from my text.</p>

<p>My data is:</p>

<pre><code>text &lt;- c(""#*TeX: The Program,
#@Donald E. Knuth,
#t1986,
#c,
#index68,
""""
#*Foundations of Databases.,
#@Serge Abiteboul,Richard Hull,Victor Vianu,
#t1995,
#c,
#index69,
#%1118192,
#%189,
#%1088975,
#%971271,
#%832272,
#!From the Book: This book will teach you how to write specifications of computer systems, using the language TLA+."")
</code></pre>

<p>My expected output is : </p>

<pre><code>expected &lt;- data.frame(title=c(""#*TeX: The Program"", ""#*Foundations of Databases.""), authors=c(""#@Donald E. Knuth"", ""#@Serge Abiteboul,Richard Hull,Victor Vianu""), year=c(""#t1986"", ""#t1995""), revue=c(""#c"", ""#c""), id_paper=c(""#index68"", ""#index69""),
                       id_ref=c(NA,""#%1118192, #%189, #%1088975, #%971271, #%832272""), abstract=c(NA, ""#!From the Book: This book will teach you how to write specifications of computer systems, using the language TLA+.""))
</code></pre>

<p>My code is:</p>

<pre><code>coln &lt;- c(""title"", ""authors"", ""year"", ""revue"",""id_paper"", ""id_ref"", ""abstract"")
      title_index &lt;- grep(""^#[*]"", text)
      authors_index &lt;- grep(""#@"", text)
      year_index &lt;- grep(""#t"", text)
      revue_index &lt;- grep(""#c"", text)
      id_paper_index &lt;- grep(""#index"", text)
      id_refindex &lt;- grep(""#%"", text)
      abstract_index &lt;- grep(""#!"", text)
      df &lt;- matrix(NA, nrow=length(title_index), ncol=length(coln))
      colnames(df) &lt;- coln
      stoc_index &lt;- grep(""#cSTOC"", text)
      sigir_index &lt;- grep(""#cSIGIR"", text)}


  ########## titre
  {der_pos &lt;- length(title_index)
    tit_position  &lt;- c(title_index , der_pos)
    for(i in 1:length(title_position)){
      if(i != length(title_position)){
        df[i, ""title""] &lt;- text[title_position[i]]
      }
    }
  }

  ########## author 
{der_pos &lt;- length(authors_index)
    authors_position  &lt;- c(authors_index )
    for(i in 1:length(auteur_position)){
      if(i != length(auteur_position)){
        df[i, ""auteur""] &lt;- text[auteur_position[i]]
      }
    }
  }

  ########## year
{der_pos &lt;- length(year_index)
    year_position  &lt;- c(year_index , der_pos)
    for(i in 1:length(year_position)){
      if(i != length(year_position)){
        df[i, ""année""] &lt;- text[year_position[i]]
      }
    }
  }

  ##########??? revue
  {der_pos &lt;- length(revue_index)
    revue_position  &lt;- c(revue_index )
    for(i in 1:length(revue_position)){
      if(i != length(revue_position)){
        df[i, ""revue""] &lt;- text[revue_position[i]]
      }
    }
  }

  ########## id_paper
  {der_pos &lt;- length(id_paper_index)
    id_paper_position  &lt;- c(id_paper_index , dern_pos)
    for(i in 1:length(id_paper_position)){
      if(i != length(id_paper_position)){
        df[i, ""id_paper""] &lt;- text[id_paper_position[i]]
      }
    }
  }

  ########## id_ref
  {der_pos &lt;- length(id_ref_index)
    id_ref_position  &lt;- c(id_ref_index , der_pos)
    for(i in 1:length(id_ref_position)){
      if(i != length(id_ref_position)){
        df[i, ""id_ref""] &lt;- text[id_ref_position[i]]
      }
    }
  }
  ########## abstract
  {der_pos &lt;- length(abstract_index)
    abstract_position  &lt;- c(abstract_index , der_pos)
    for(i in 1:length(abstract_position)){
      if(i != length(abstract_position)){
        df[i, ""abstract""] &lt;- text[abstract_position[i]]
      }
    }
  }
</code></pre>

<p>So I would like to extract the reference in a single line</p>

<p>Thank you in advance if you have solution for concatenate many citation in one column separated by coma for one article.</p>

<p>Thank you :)</p>
","r, text, nlp, text-mining, feature-extraction","<p>New and improved</p>

<pre><code>text.n &lt;- strsplit(text, ""\n(?=#\\*)"", perl=TRUE)[[1]]; text.n

text.s &lt;- lapply(text.n, function(x) strsplit(x, ""\n"")[[1]])

patterns &lt;- list(title=""^#\\*"", 
                autors=""^#@"",
                  year=""^#t"",
                 revue=""^#c"",
              id_paper=""^#index"",
                id_ref=""^#%"",
              abstract=""^#!"")

tex.l &lt;- lapply(text.s, function(x)
  lapply(patterns, function(y)
    paste(sub(y, """", grep(y, x, value=TRUE)), collapse="","")
  )
) 

tex.m &lt;- matrix(unlist(tex.l), ncol=length(tex.l[[1]]), byrow=TRUE)
tex.df &lt;- as.data.frame(tex.m, stringsAsFactors=FALSE)
colnames(tex.df) &lt;- names(patterns)

str(tex.df)

# 'data.frame': 2 obs. of  7 variables:
# $ title   : chr ""TeX: The Program"" ""Foundations of Databases.""
# $ autors  : chr ""Donald E. Knuth"" ""Serge Abiteboul,Richard Hull,Victor Vianu""
# $ year    : chr ""1986"" ""1995""
# $ revue   : chr """" """"
# $ id_paper: chr ""68"" ""69""
# $ id_ref  : chr """" ""1118192,189,1088975,971271,832272""
# $ abstract: chr """" ""From the Book: This book will teach you how to write 
#                     specifications of computer systems, using the language TLA+.""
</code></pre>
",1,1,323,2018-11-26 20:07:31,https://stackoverflow.com/questions/53488266/r-feature-extraction-for-text
Python and Regex to convert wrtitten numbers to numeric,"<p>I am trying to convert written numbers to numeric values. </p>

<p>For example, to extract millions from this string:</p>

<pre><code>text = 'I need $ 150000000, or 150 million,1 millions, 15 Million, 15million, 15Million, 15 m, 15 M, 15m, 15M, 15 MM, 15MM, 5 thousand'
</code></pre>

<p>To:</p>

<pre><code>'I need $ 150000000, or 150000000,1000000, 15000000, 15000000, 15000000, 15000000, 15000000, 15000000, 15000000, 15000000, 15000000, 5 thousand'
</code></pre>

<p>I use this function to remove any separators in the numbers first:</p>

<pre><code>def foldNumbers(text):
    """""" to remove "","" or ""."" from numbers """"""""
    text = re.sub('(?&lt;=[0-9])\,(?=[0-9])', """", text) # remove commas
    text = re.sub('(?&lt;=[0-9])\.(?=[0-9])', """", text) # remove points
return text
</code></pre>

<p>And I have written this regex to findall of the possible patterns for common Million notations. This 1) finds digits and does a look ahead for 2) common notation for millions, 3) The ""[a-z]?"" part is to handle optional ""s"" on million or millions where I have already removed ""'"".</p>

<pre><code>re.findall(r'(?:[\d\.]+)(?= million[a-z]?|million[a-z]?| Million[a-z]?|Million[a-z]?|m| m|M| M|MM| MM)',text)
</code></pre>

<p>which correctly matches Million numbers and returns:</p>

<pre><code>['150', '1', '15', '15', '15', '15', '15', '15', '15', '15', '15']
</code></pre>

<p>What I need to do now is to write a replacement pattern to insert ""000000"" after the digits, or to iterate through and multiply the digits by 100000. I have tried this so far:</p>

<pre><code>re.sub(r'(?:[\d\.]+)(?= million[a-z]?|million[a-z]?| Million[a-z]?|Million[a-z]?|m| m|M| M|MM| MM)', ""000000 "", text)
</code></pre>

<p>which returns:</p>

<pre><code>'I need $ 150,000,000, or 000000  million,000000  millions, 000000  Million, 000000 million, 000000 Million, 000000  m, 000000  M, 000000 m, 000000 M, 000000  MM, 000000 MM, 5 thousand'
</code></pre>

<p>I think I need to do a <a href=""https://www.regular-expressions.info/lookaround.html"" rel=""nofollow noreferrer"">look behind</a> (?&lt;=), however I haven't worked with this before and after several attempts I cant seem to work it through. </p>

<p>FYI: My plan is to tackle ""Millions"" first and then to replicate the solution for Thousands (K), Billions (B), Trillions (T) and possibly for other units such as distances, currencies etc. I have searched SO and google for any solutions in NLP, text cleaning and mining articles but did not find anything. </p>
","python, regex, replace, nlp, text-mining","<p>You can accomplish this with a relatively simple <code>re.sub</code>: match</p>

<pre><code>(?i)\b(\d+) ?m(?:m|illions?)?\b
</code></pre>

<p><em>capturing</em> the initial digits in a group, and replace with that group concatenated with 6 zeros:</p>

<pre><code>r'\g&lt;1&gt;000000'
</code></pre>

<p><a href=""https://regex101.com/r/IedRP4/1"" rel=""nofollow noreferrer"">https://regex101.com/r/IedRP4/1</a></p>

<p>Code:</p>

<pre><code>text = 'I need $ 150000000, or 150 million,1 millions, 15 Million, 15million, 15Million, 15 m, 15 M, 15m, 15M, 15 MM, 15MM, 5 thousand'
output = re.sub(r'(?i)\b(\d+) ?m(?:m|illions?)?\b', r'\g&lt;1&gt;000000', text)
</code></pre>

<p>(because the group in the replacement is followed by digits, <a href=""https://stackoverflow.com/questions/5984633/python-re-sub-group-number-after-number"">make sure</a> to use <code>\g&lt;#&gt;</code> syntax rather than <code>\#</code> syntax)</p>
",1,1,857,2018-12-08 09:36:04,https://stackoverflow.com/questions/53681245/python-and-regex-to-convert-wrtitten-numbers-to-numeric
Remove non-English observations from dataframe,"<p>I have a dataframe with Twitter data. I have cleaned the Tweet text and added it as a vector, <code>clean_text</code>, but there are numerous observations in a non-English language that affect my text analysis. How do I remove all observations in the dataframe that are not written in English?</p>

<hr>

<p>Here is a reproducible sample of my dataframe, <code>BrexitTweets</code>.</p>

<pre><code>structure(list(`Tweet ID` = c(746280472381107968, 746280472355929984, 
746280472154603008, 746280472129342976, 746280472083332992, 746280472037170944, 
746280471831645952, 746280471814888960, 746280471777185024, 746280471756180992, 
746280471743565056, 746280471705844992, 746280471680658944, 746280471676488960, 
746280471676455936, 746280471617757056, 746280471613570944, 746280471600992000, 
746280471525469952, 746280471403847040), Time = c(""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04""), `Tweet Type` = c(""Tweet"", ""Retweet"", 
""Retweet"", ""Retweet"", ""Retweet"", ""Retweet"", ""Tweet"", ""Retweet"", 
""Tweet"", ""Retweet"", ""Tweet"", ""Tweet"", ""Retweet"", ""Tweet"", ""Retweet"", 
""Retweet"", ""Retweet"", ""Tweet"", ""Retweet"", ""Retweet""), `Retweeted By` = c(NA, 
""misyed_"", ""Skuys"", ""priyadarshibbc"", ""Amaranta_2012"", ""ECCA_Nordic"", 
NA, ""Dat_Sync"", NA, ""SirDeGuz"", NA, NA, ""RoGreca_"", NA, ""30SecondsToMoon"", 
""StuartGray"", ""DataDebate"", NA, ""alek_dev"", ""addi_GrBj""), `Number of Retweets` = c(0, 
251, 4, 14, 2, 39, 0, 6462, 0, 1391, 0, 0, 31595, 0, 27, 15, 
35, 0, 6462, 20521), `Number of Followers` = c(6079, 434717, 
16036, 345319, 4566, 3223810, 109145, 560, 78, 1957, 766, 1299, 
2155087, 235, 1925, 735, 8045, 159, 560, 128027), `Number Following` = c(2314, 
1994, 12403, 344855, 1012, 765, 333, 236, 132, 1407, 294, 1381, 
1, 338, 725, 1601, 831, 969, 236, 1606), clean_text = c(""mayagoodfellow as always making sense of it all for us ive never felt less welcome in this country brexit  httpstcoiai5xa9ywv"", 
""never underestimate power of stupid people in a democracy brexit"", 
""gana el brexit reino unido decide abandonar la unión europea httpstco66cwudtsxu vía elmundoes"", 
""uk prime minister set to resign brexit httpstco0bxbdmiswm"", 
""oye junckereu que dice la ciudadanía de uk que tus tratados se los pasan por sus urnas brexit httpstcoedqfkl"", 
""a quick guide to brexit and beyond after britain votes to quit eu httpstcos1xkzrumvg httpstcocniutojkt0"", 
""this selfinflicted wound will be his legacy cameron falls on sword after brexit euref httpstcoegph3qonbj httpstcohbyhxodeda"", 
""so the uk is out cameron resigned scotland wants to leave great britain sinn fein plans to unify ireland and its o"", 
""this is a very good summary no biasspinagenda of the legal ramifications of the leave result brexit httpstcolobtyo48ng"", 
""you cant make this up cornwall votes out immediately pleads to keep eu cash this was never a rehearsal httpstco"", 
""brexit httpstconwutx2owcs"", ""brexit primer anàlisi de les conseqüencies en món de lesport httpstcon3bdrqz5cf via iusport unioesports"", 
""no matter the outcome brexit polls demonstrate how quickly half of any population can be convinced to vote against itself q"", 
""es ist nicht immer klug das volk entscheiden zu lassen brexit"", 
""gli studenti europei verranno considerati extraeuropei e rimarranno senza assistenza sanitaria assurdo brexit"", 
""i wouldnt mind so much but the result is based on a pack of lies and unaccountable promises democracy didnt win brexit pro"", 
""brexit einfach erklärt httpstcou7jhlhrpim"", ""brexit httpstcoiive3hsj26"", 
""so the uk is out cameron resigned scotland wants to leave great britain sinn fein plans to unify ireland and its o"", 
""absolutely brilliant poll on brexit by yougov httpstcoepevg1moaw""
)), .Names = c(""Tweet ID"", ""Time"", ""Tweet Type"", ""Retweeted By"", 
""Number of Retweets"", ""Number of Followers"", ""Number Following"", 
""clean_text""), row.names = c(NA, 20L), class = c(""tbl_df"", ""tbl"", 
""data.frame""))
</code></pre>
","r, text, twitter, text-mining","<p>Check out the text cat package </p>

<pre><code># install.packages(""textcat"") - install this package 
require(textcat)
require(dplyr)
data$Languages &lt;- textcat(data$clean_text)
data &lt;- data %&gt;% filter(Languages == ""english"")
</code></pre>
",3,2,1560,2018-12-10 08:59:26,https://stackoverflow.com/questions/53702293/remove-non-english-observations-from-dataframe
R speed up sapply,"<p>I've got the following script in a loop:</p>

<pre><code>number_of_rows_similar_addresses &lt;- as.data.table(cbind(
    distinct_similar_addresses,
    sapply(distinct_similar_addresses, function(x) {
        length(similar_addresses[Original_Address == x]$people_names) / length(unique(similar_addresses[Original_Address == x]$people_names))
    })
))
</code></pre>

<p>The problem is that it slows down considerably the loop.</p>

<p>The data looks like this:</p>

<p>distinct_similar_addresses:</p>

<pre><code>""U 2 5 TIMPERLEY ST NICHOLLS VIC""       
""U 1 3 TIMPERLEY ST NICHOLLS VIC""                            
""U 1 11 TIMPERLEY ST NICHOLLS VIC""                            
""U 1 33 TIMPERLEY ST NICHOLLS VIC""                           
""U 1 2 TIMPERLEY ST NICHOLLS VIC""                            
""U 1 3 TIMPERLEY ST NICHOLLS VIC""                            
""U 1 5 TIMPERLEY ST NICHOLLS VIC"" 
</code></pre>

<p>similar_addresses:</p>

<pre><code>    people_names,Original_Address,Numbers,street_Name,street_type,post_code,suburb,PO,UID
Giuseppe Conte,U 1 3 TIMPERLEY ST NICHOLLS VIC,1,TIMPERLEY,ST,5469,NICHOLLS,,
Giuseppe Conte,U 1 3 TIMPERLEY ST NICHOLLS VIC,TIMPERLEY,ST,5469,NICHOLLS,,
Mario Pertini,U 2 5 TIMPERLEY ST NICHOLLS VIC,TIMPERLEY,ST,5469,NICHOLLS,,
Mario Pertini,U 2 5 TIMPERLEY ST NICHOLLS VIC,5,TIMPERLEY,ST,5469,NICHOLLS,,
</code></pre>

<p>The script is assessing if the address is referring to a unit or a single house.
Is there any way to perform this task faster?</p>

<p>I'm adding a result set and an explaination so that what it does become more understandable.</p>

<p>Result set:</p>

<pre><code>   distinct_similar_addresses      V2
""U 2 5 TIMPERLEY ST NICHOLLS VIC""   2
""U 1 3 TIMPERLEY ST NICHOLLS VIC""   2
</code></pre>

<p>The code is just counting the number of names associated to a single row of address.
Indeed if the address is repeated it means that it's referring to a unit otherwise it's a single house.</p>
","r, performance, apply, text-mining","<p>Thanks Gregor,
This is probably better:   </p>

<pre><code> x &lt;- similar_addresses[, .N, by = Original_Address]$N
 y &lt;- similar_addresses[, length(unique(people_names)) , by = Original_Address]$V1
 number_of_rows_similar_addresses &lt;- cbind(unique(similar_addresses$Original_Address), x/y)
</code></pre>
",0,-1,302,2018-12-11 04:17:57,https://stackoverflow.com/questions/53717312/r-speed-up-sapply
How to do K-NN on Bag of words,"<p>I have a training and test set (equal in size). I have done the bag of words model and I am trying to do K-nearest neighbor on it and I'm unsure how to do the fit. </p>

<p>Bag of words model: </p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer = CountVectorizer(max_features=100, stop_words='english')

bow = bow_vectorizer.fit(TrainData)
print(bow_vectorizer.vocabulary_)
bowTrain = bow_vectorizer.fit_transform(TrainData)
bowTest = bow_vectorizer.fit_transform(TestData)
</code></pre>

<p>Trying to do KNN on the Bag of Words model and I'm unsure what i'm supposed to put in the ""knn.fit"" portion</p>

<pre><code>from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(bowTrain, ???? )
predict = knn.predict(bowTest[0:5000])
</code></pre>
","python, text-mining","<pre><code>from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer = CountVectorizer(max_features=100, stop_words='english')

X_train = TrainData
#y_train = your array of labels goes here
bowVect = bow_vectorizer.fit(X_train)
</code></pre>

<p>You should probably use the same vectorizer as there is a chance that the vocabluary may change.</p>

<pre><code>bowTrain = bowVect.transform(X)
bowTest = bowVect.transform(TestData)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(bowTrain, y_train )
predict = knn.predict(bowTest[0:5000])
</code></pre>
",3,1,4074,2018-12-15 18:12:18,https://stackoverflow.com/questions/53795927/how-to-do-k-nn-on-bag-of-words
python group of words search implemented recursively. How to proceed?,"<p>I have to look for concepts within texts. The concepts are expressed in the following way:</p>

<p><code>""blue 5 house""</code> >>> would mean that I have to find the hits where the words <code>blue</code> and <code>house</code> appear within a <code>distance of 5 or less words</code>.
<code>""little 3 cat""</code> would then mean finding the hits where the words <code>little</code> and <code>cat</code> appear within a <code>distance of max 3 words</code>. (i.e. ""little cat"", ""little annoying cat"" but not ""the cat of my grandmother is little"")</p>

<p>I guess you get it.</p>

<p>I have so far a (not very sophisticated) code as follows. I just implemented two nested loops that go over all the words of the text and when there is a hit of the first one start to look for the other one in the words around and adds the result to a list:</p>

<pre><code>with open('applicationtext.txt', 'r') as f:
content=f.read()
# content = ' Lorem ipsum dolor sit amet, consectetur (23) adipiscing elit, sed do ( 23 , 45 ) eiusmod ( 23, 45 ) tempor incididunt ut  '
# Note: the text contains several times: ""sit amet eros vestibulum""

elasticTerm1=""sit""
elasticTerm2=""vestibulum""
distance=5

content=content.strip()
# replace all the line breaks and two spaces.
content = content.replace('\n', ' ').replace('\r', '').replace('  ',' ')

listofHits=[]
content_tokenized = content.split("" "")

for i,word in enumerate(content_tokenized):
    if word==elasticTerm1:
        for j in range(distance):
            if content_tokenized[i+j]==elasticTerm2:
                # I got a hit
                position1=i
                myhitTupple=(i,elasticTerm1)
                listofHits.append(myhitTupple)

for i,tupple in enumerate(listofHits):
    print(tupple)
</code></pre>

<p>So far it works ok.</p>

<p>Imagine I am thinking about how to build on that in order to construct code something recursively that would give me the hits of:</p>

<p><code>(little 3 cat) 4 third_word</code> or even
<code>concept1 5 concept2</code>; where <code>concept1=(""blue 3 cat"")</code> and <code>concept2=(""little 4 dollar"")</code>???</p>

<p>what should I think about? a class? is that already somehow contained in scikit-learn? More than a code (which I guess would be complicated) I am asking you for orientation. How to think about a problem solved with code recursively.</p>

<p>Thanks</p>

<p>NOTE 1: Please forget about the order ""little cat"" vs ""cat little"" thats another issue.</p>

<p>NOTE 2: (after first answer) Be aware that this is a very simplified case, in reality I am looking at cases like this: <code>((concept1 n1 concept2) n2 concept 3)) n3 (concept1 n4 concept 5)</code></p>
","python, recursion, scikit-learn, text-mining","<p>Key observations underlying the solution:</p>

<ul>
<li>when we take the leap from tokens to ""concepts"",we need range instead of index.</li>
<li>We need to define a function to find ""distance"" between two concepts i.e their corresponding range. (<code>dist</code> below)</li>
<li>another function to combine concepts i.e their ranges. (<code>comb</code> below)</li>
</ul>

<p>Now in our main recursive function, we first find out all occurrences of both concepts. Then we can simply find the pairs which have distance lower than specified. In this implementation, our main <code>hits()</code> takes a ""concept"":which is either simply a word in base case, or a 3-element tuple that have two concepts and an <code>int</code> specifying max possible distance between them. Output of this function is an array of ranges, where each of these ranges contain both concepts withing max distance. This array can be thought of as all occurrences of the input concept.</p>

<p>Here is the full code.</p>

<pre><code>#Find distance between two concept's ranges
#ex1: dist([2,9],[11,13]) = 2
#ex2: dist([2,9],[4,99]) = 0
def dist(r1,r2):
    #check for overlap
    if r2[0]&lt;=r1[0]&lt;=r2[1] or r1[0]&lt;=r2[0]&lt;=r1[1]:
        return 0

    return max(r1[0],r2[0]) - min(r1[1],r2[1])

#Combine two concept's ranges
#ex1: comb([1,3],[6,9]) = [1,9]
#ex2: comb([4,11],[1,7]) = [1,11]
def comb(r1,r2): 
    return [min(r1[0],r2[0]),max(r1[1],r2[1])]

def hits(concept):
    if type(concept)==str:
        return [(i,i) for i,w in enumerate(tokens) if w==concept]

    c1,c2,R = concept
    ans = []
    for r1 in hits(c1):
        for r2 in hits(c2):
            if dist(r1,r2)&lt;=R:
                ans.append(comb(r1,r2))
    return ans
</code></pre>

<p>To test this, case 1: (this outputs [[0-9]])</p>

<pre><code>tokens = ""python group of words search implemented recursively How to proceed"".split()
c1 = (""python"",""words"",3)
c2 = (""recursively"",""proceed"",4)
print(hits((c1,c2,3))) 
</code></pre>

<p>case 2: (this outputs [[0-8]])</p>

<pre><code>c1 = (""python"",""of"",3)
c2 = (""search"",""recursively"",4)
print(hits(((c1,c2,3),""to"",3)))
</code></pre>

<p>Case 3: (This outputs [[0, 3], [6, 8]])</p>

<pre><code>tokens = ""A B B X C C X Q A W"".split()
c1 = (""A"",""X"",4)
print(hits(c1))
</code></pre>

<p>For performance, preprocess the base case of recursion.</p>
",1,1,132,2018-12-19 15:12:20,https://stackoverflow.com/questions/53854082/python-group-of-words-search-implemented-recursively-how-to-proceed
Complex regular expression getting less than expected,"<p>I am trying to fiddle with a regex in <em>Python 2.7</em> in order to catch numbered footnotes in a text. My text as converted from PDF looks like:</p>

<pre><code>test_str = u""""""
7. On 6 March 2013, the Appeals Chamber filed the Decision on Victim 
Participation, in which it decided that the victims “may, through their legal 

1
 The full citation, including the ICC registration reference of all designations and abbreviations used in 
this judgment are included in Annex 1. 
2
 A more detailed procedural history is set out in Annex 2 of this judgment. 
ICC-01/04-02/12-271-Corr  07-04-2015  7/117  EK  A

 8/117 
representatives, participate in the present appeal proceedings for the purpose of 
presenting their views and concerns in respect of their personal interests in the issues 
on appeal”.3

8. On 19 March 2013, the Prosecutor filed, confidentially, ex parte, available to the 
Prosecutor and Mr Ngudjolo only, the Document in Support of the Appeal. The 
Prosecutor filed a confidential redacted version of the Document in Support of the 
Appeal on 22 March 2013, and a public redacted version of the Document in Support 
of the Appeal on 3 April 2013. In the redacted version of the Document in Support of 
the Appeal, the Prosecutor’s entire third ground of appeal was redacted. 

""""""
</code></pre>

<p>Please note that <em>numbered paragraphs</em> that are the regular content of my text, are <strong>prefixed with a number and dot (like '5.')</strong>.
 Ideally, I 'd like to get something like:</p>

<pre><code>[(1,""The full citation, including the ICC registration reference of all designations and abbreviations used in 
this judgment are included in Annex 1. ""), (2, ""A more detailed procedural history is set out in Annex 2 of this judgment."" 
</code></pre>

<p>My Python code for getting the footnotes is: </p>

<pre><code>regex = ur""""""
(\r?\n)(?P&lt;num&gt;\d+)(?!\.) #first line
(?P&lt;text&gt;(?:\s(.|\r?\n)+?\s?(?:\n\n|\Z))) #following lines
""""""
result = re.findall(regex, test_str, re.U|re.VERBOSE | re.X |re.MULTILINE)
</code></pre>

<p>which gives me:</p>

<pre><code>[(u'\n', u'1', u'\n The full citation, including the ICC registration reference of all designations and abbreviations used in \nthis judgment are included in Annex 1. \n\n', u'.')]
</code></pre>

<p>i.e. only the <em>first footnote</em>, while I need both off course</p>

<p>Any ideas are welcome!</p>
","python, regex, text-mining","<p>I believe this regex: <code>(^\d+(?!\.).*?)(?=^\s*\d)</code> works as you describe.</p>

<p><a href=""https://regex101.com/r/n59wZV/3/"" rel=""nofollow noreferrer"">Demo</a></p>

<p>Python demo:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; print ''.join(re.findall(r'(^\d+(?!\.).*?)(?=^\s*\d)', test_str, flags=re.M|re.S))
1
 The full citation, including the ICC registration reference of all designations and abbreviations used in 
this judgment are included in Annex 1. 
2
 A more detailed procedural history is set out in Annex 2 of this judgment. 
ICC-01/04-02/12-271-Corr  07-04-2015  7/117  EK  A
</code></pre>

<p>If you want to capture the footnote number separate from the text:</p>

<pre><code>&gt;&gt;&gt; re.findall(r'^(\d+)((?!\.).*?)(?=\s*^\d)', test_str, flags=re.M|re.S)
[(u'1', u'\n The full citation, including the ICC registration reference of all designations and abbreviations used in \nthis judgment are included in Annex 1. \n'), (u'2', u'\n A more detailed procedural history is set out in Annex 2 of this judgment. \nICC-01/04-02/12-271-Corr  07-04-2015  7/117  EK  A\n')]
</code></pre>
",1,0,83,2018-12-22 20:34:48,https://stackoverflow.com/questions/53899120/complex-regular-expression-getting-less-than-expected
save a text of each row without merge,"<p>I wrote a code in R 
To do some change in a text after count the number of character 
And if the number of character grater than 5 do the change 
And it is working here is my code </p>

<pre><code>dataset&lt;-  c (""there is a rain "" , ""I am student"" )
dataset &lt;-data.frame(x= dataset)
dataset $x&lt;-as.character(dataset $x)
words &lt;- unlist(strsplit(dataset $x, "" ""))
nchar(words)
K &lt;- character(length(words))
K[nchar(words) &lt; 6] &lt;- words[nchar(words) &lt; 6]
K[nchar(words) &gt; 5] &lt;- gsub('e', 'X', 
                            words[nchar(words) &gt; 5], perl = TRUE)
</code></pre>

<p>the result </p>

<pre><code>[1] ""there""   ""is""      ""a""       ""rain""    ""I""       ""am""      ""studXnt""
</code></pre>

<p>As you can see it do the change but my problem is that It merge between the texts 
So If I have 50 rows then I do not know the text belong of which row 
Because at the end I need to save the change on original text </p>

<p>The result I expect </p>

<pre><code>[1] There is a rain 
[2] I am studXnt
</code></pre>

<p>Thank you</p>
","r, dataframe, text-mining","<p>Here is the correct syntax to do it,</p>

<pre><code>sapply(strsplit(df$x, ' '), function(i){i[nchar(i) &gt; 5] &lt;- gsub('e', 'X', i[nchar(i) &gt; 5]);
                                        paste(i, collapse = ' ')})

#[1] ""there is a rain"" ""I am studXnt""
</code></pre>
",1,-2,42,2018-12-27 08:29:47,https://stackoverflow.com/questions/53941964/save-a-text-of-each-row-without-merge
Extract found word and 20 Words before and after it,"<p>I am using stringr to scan through a very long text. If the word is found. I want to extract not only the word, but some context, lets say twenty words before and after the word has been detected.</p>

<p>So If I have ""Hello there, how are you?"" and I look for ""there"", I want to extract there +-1 Word:
""Hello there, how""</p>

<p>However, I am having problems in combining str_locate and str_word since one expresses location as the character-number and the other works with the word-number</p>

<p>How do I go about this? I know how to locate a word and I know how to extract words. But How do I extract words around a specific word?</p>

<pre><code>library(tidyverse)
library(stringr)

text &lt;- ""Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.""

# Lets say I am looking for ""sit""

str_locate_all(text,""sit"") # I get the positions for ""sit"" ... but expressed in the number of letters


# assuming sit-position is expressed as  word-number and not character number
sit_position &lt;- c(4,20,30,40)  # not the real positions of ""sit"" just to simulate


#the word plus minus two 
sit_position_d &lt;- sit_position-2
sit_position_u &lt;- sit_position+2

wordcontext &lt;- rep(NA,NROW(sit_position))

for (i in c(1:NROW(sit_position))) {

  wordcontext[i] &lt;- word(text, sit_position_d[i],sit_position_u[i])


}
</code></pre>

<p>How do I change this code to tell word() that it needs to start from a specific word?</p>
","r, text-mining","<p>To do this with <code>stringr</code> as you started, you can use an expression like the one below.  To make it easier to check, I just took up to 6 word on either side,  but it should be easy to change this to 20 words. </p>

<pre><code>str_extract(text, ""(\\w+\\W+){0,6}sit(\\W+\\w+){0,6}"")
[1] ""Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam""
</code></pre>

<p><strong>Details</strong></p>

<p>The expression is centered on the word that you want to find, in this case ""sit"".<br>
<code>(\\w+\\W+){0,6}</code> matches up to 6 words before ""sit"".<br>
<code>(\\W+\\w+){0,6}</code> matches up to 6 words after ""sit"". </p>
",0,1,363,2018-12-29 19:44:20,https://stackoverflow.com/questions/53974148/extract-found-word-and-20-words-before-and-after-it
How to find and replace certain keywords in a specific column of a data frame in R?,"<p>I want to find some specific keywords in a specific column of a data frame and replace them with other keywords that already exists in that column. For example technology (freq=2) with technologies (freq=3).
I need to do this without changing the rest of the columns in the data frame and save it in the same column in the same data  frame. In this way I can have 5 keywords of ""technologies"".
However, I have  no clue how to start doing this in rstudio specially because I have to keep the output as a data frame. Can you please guide me where to begin with?</p>
","r, dataframe, text-mining","<p>Say this is your data:</p>

<pre><code>dat &lt;- data.frame(C1=c(""Hi"", ""My"", ""Example"", ""Hi""), 
                  C2=c(""This"", ""Is"", ""An"", ""Example""), 
                  stringsAsFactors = F)
</code></pre>

<p>You can use <code>gsub</code> to replace all occurrences of a value in one columns like this:</p>

<pre><code>dat$C1 &lt;- gsub(pattern=""Example"", replacement=""NEW"", dat$C1)
</code></pre>

<p>You can go through all columns like this:</p>

<pre><code>lapply(a, gsub, pattern=""Hi"", replacement=""NEW"") 
</code></pre>

<p>Does that do what you are after? </p>
",1,-1,414,2019-01-02 03:17:40,https://stackoverflow.com/questions/54000845/how-to-find-and-replace-certain-keywords-in-a-specific-column-of-a-data-frame-in
removing prepositions from a text file in linux,"<p>What I want to do is that i want to remove all prepositions in a text file in CentOS. Things like 'on of to the in at ....'. Here is my script:</p>

<pre><code>!/bin/bash
list='i me my myself we our ours ourselves you your yours yourself ..... '
cat Hamlet.txt | for item in $list
do
sed 's/$item//g' 
done &gt; newHam.txt
</code></pre>

<p>but at the end when i open newHam.txt nothing changes! It's the same as Ham.txt. I don't know whether this is a good approach or not. Any suggestion? Any approach??   </p>
","linux, shell, sed, text-mining","<p>Assuming your <code>sed</code> understands <code>\&lt;</code> and <code>\&gt;</code> for word boundaries,</p>

<pre><code>sed 's/\&lt;\(i\|me\|my\|myself|\we|\our|\ours|\ourselves|\you|\your|\yours|\yourself\)\&gt; \?//g' Hamlet.txt &gt;newHam.txt
</code></pre>

<p>You want to make sure you include word boundaries; your original attempt would replace e.g. <code>i</code> everywhere n the nput.</p>

<p>If you already have the words in a string, you can interpolate it in Bash with</p>

<pre><code>sed ""s/\\&lt;\\(${list// /\\|}\\)\\&gt; \\?//g"" Hamlet.txt &gt;newHam.txt
</code></pre>

<p>but the <code>${variable//pattern/substitution}</code> parameter expansion is not portable to e.g. <code>/bin/sh</code>. Notice also how double quotes instead of single are necessary for the shell to be allowed to perform variable substitutions within the script, and how all literal backslashes need to be escaped with another backslash within double quotes.</p>

<p>Unfortunately, many details of <code>sed</code> are poorly standardized. Ironically, switching to a tool which isn't standard at all might be the most portable solution.</p>

<pre><code>perl -pe 'BEGIN {
    @list = qw(i me my myself we our ours ourselves you your yours yourself .....);
    $re = join(""|"", @list); }
    s/\b($re)\b ?//go' Hamlet.txt &gt;newHam.txt
</code></pre>

<p>If you want this as a standalone script,</p>

<pre><code>#!/usr/bin/perl

BEGIN {
    @list = qw(i me my myself we our ours ourselves you your yours yourself .....);
    $re = join(""|"", @list);
}
while (&lt;&gt;) {
    s/\b($re)\b ?//go;
    print
}
</code></pre>

<p>These words are pronouns, not prepositions.</p>

<p>Finally, take care to fix the shebang of your script; the first line of the script needs to start with exactly the two characters <code>#!</code> because that's what makes it a shebang. You'll also want to avoid the <a href=""/questions/11710552/useless-use-of-cat"">useless <code>cat</code></a> in the future.</p>
",1,-1,140,2019-01-06 06:04:34,https://stackoverflow.com/questions/54059077/removing-prepositions-from-a-text-file-in-linux
Counting words and word stems in a large dataframe (RStudio),"<p>I have a large dataframe consisting of tweets, and a keyword dictionary loaded as a list that has words and word stems associated with emotion (<code>kw_Emo</code>). <strong>I need to find a way to count how many times any given word/word stem from <code>kw_Emo</code> is present each tweet.</strong> In <code>kw_Emo</code>, word stems are marked with an asterisk ( * ). For example, one word stem is <code>ador*</code>, meaning that I need to account for the presence of <code>adorable</code>, <code>adore</code>, <code>adoring</code>, or any pattern of letters that <em>starts</em> with <code>ador…</code>. </p>

<hr>

<p>From a previous Stack Overflow discussion (see previous question on my profile), I was greatly helped with the following solution, but it only counts exact character matches (Ex. only <code>ador</code>, not <code>adorable</code>):</p>

<ol>
<li><p>Load relevant package.</p>

<p><code>library(stringr)</code> </p></li>
<li><p>Identify and remove the <code>*</code> from word stems in <code>kw_Emo</code>.</p>

<p><code>for (x in 1:length(kw_Emo)) {
  if (grepl(""[*]"", kw_Emo[x]) == TRUE) {
    kw_Emo[x] &lt;- substr(kw_Emo[x],1,nchar(kw_Emo[x])-1)
      }</code>
    }</p></li>
<li><p>Create new columns, one for each word/word stem from <code>kw_Emo</code>, with default value 0.</p>

<p><code>for (x in 1:length(keywords)) {
  dataframe[, keywords[x]] &lt;- 0}</code></p></li>
<li><p>Split each Tweet to a vector of words, see if the keyword is equal to any, add +1 to the appropriate word/word stems' column.</p>

<p><code>for (x in 1:nrow(dataframe)) {
  partials &lt;- data.frame(str_split(dataframe[x,2], "" ""), stringsAsFactors=FALSE)
  partials &lt;- partials[partials[] != """"]
  for(y in 1:length(partials)) {
    for (z in 1:length(keywords)) {
      if (keywords[z] == partials[y]) {
        dataframe[x, keywords[z]] &lt;- dataframe[x, keywords[z]] + 1
      }
    }
  }
}</code></p></li>
</ol>

<p>Is there a way to alter this solution to account for word stems? I'm wondering if it's possible to first use a stringr pattern to replace occurrences of a word stem with the exact characters, and then use this exact match solution. For instance, something like <code>stringr::str_replace_all(x, ""ador[a-z]+"", ""ador"")</code>. But I'm unsure how to do this with my large dictionary and numerous word stems. Maybe the loop removing <code>[*]</code>, which essentially identifies all word stems, can be adapted somehow? </p>

<hr>

<p>Here is a reproducible sample of my dataframe, called <code>TestTweets</code> with the text to be analysed in a column called <code>clean_text</code>:</p>

<p><code>dput(droplevels(head(TestTweets, 20)))</code></p>

<pre><code>structure(list(Time = c(""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", ""24/06/2016 10:55:04"", 
""24/06/2016 10:55:04"", ""24/06/2016 10:55:03"", ""24/06/2016 10:55:03""
), clean_text = c(""mayagoodfellow as always making sense of it all for us ive never felt less welcome in this country brexit  httpstcoiai5xa9ywv"", 
""never underestimate power of stupid people in a democracy brexit"", 
""a quick guide to brexit and beyond after britain votes to quit eu httpstcos1xkzrumvg httpstcocniutojkt0"", 
""this selfinflicted wound will be his legacy cameron falls on sword after brexit euref httpstcoegph3qonbj httpstcohbyhxodeda"", 
""so the uk is out cameron resigned scotland wants to leave great britain sinn fein plans to unify ireland and its o"", 
""this is a very good summary no biasspinagenda of the legal ramifications of the leave result brexit httpstcolobtyo48ng"", 
""you cant make this up cornwall votes out immediately pleads to keep eu cash this was never a rehearsal httpstco"", 
""no matter the outcome brexit polls demonstrate how quickly half of any population can be convinced to vote against itself q"", 
""i wouldnt mind so much but the result is based on a pack of lies and unaccountable promises democracy didnt win brexit pro"", 
""so the uk is out cameron resigned scotland wants to leave great britain sinn fein plans to unify ireland and its o"", 
""absolutely brilliant poll on brexit by yougov httpstcoepevg1moaw"", 
""retweeted mikhail golub golub\r\n\r\nbrexit to be followed by grexit departugal italeave fruckoff czechout httpstcoavkpfesddz"", 
""think the brexit campaign relies on the same sort of logic that drpepper does whats the worst that can happen thingsthatarewellbrexit"", 
""am baffled by nigel farages claim that brexit is a victory for real people as if the 47 voting remain are fucking smu"", 
""not one of the uks problems has been solved by brexit vote migration inequality the uks centurylong decline as"", 
""scotland should never leave eu  calls for new independence vote grow httpstcorudiyvthia brexit"", 
""the most articulate take on brexit is actually this ft reader comment today httpstco98b4dwsrtv"", 
""65 million refugees half of them are children  maybe instead of fighting each other we should be working hand in hand "", 
""im laughing at people who voted for brexit but are complaining about the exchange rate affecting their holiday\r\nremain"", 
""life is too short to wear boring shoes  brexit"")), .Names = c(""Time"", 
""clean_text""), row.names = c(NA, 20L), class = c(""tbl_df"", ""tbl"", 
""data.frame""))
</code></pre>

<p>Here is <code>kw_Emo</code>:</p>

<p><code>kw_Emo &lt;- c(""abusi*"", ""accept"", ""accepta*"", ""accepted"", 
        ""accepting"", ""accepts"", ""ache*"", ""aching"", ""active*"", ""admir*"", 
        ""ador*"", ""advantag*"", ""adventur*"", ""advers*"", ""affection*"", ""afraid"", 
        ""aggravat*"", ""aggress*"", ""agoniz*"", ""agony"", ""agree"", ""agreeab*"", 
        ""agreed"", ""agreeing"", ""agreement*"", ""agrees"", ""alarm*"", ""alone"", 
        ""alright*"", ""amaz*"", ""amor*"", ""amus*"", ""anger*"", ""angr*"", ""anguish*"", 
        ""annoy*"", ""antagoni*"", ""anxi*"", ""aok"", ""apath*"", ""appall*"", ""appreciat*"", 
        ""apprehens*"", ""argh*"", ""argu*"", ""arrogan*"", ""asham*"", ""assault*"", 
        ""asshole*"", ""assur*"", ""attachment*"", ""attract*"", ""aversi*"", ""avoid*"", 
        ""award*"", ""awesome"", ""awful"", ""awkward*"", ""bashful*"", ""bastard*"", 
        ""battl*"", ""beaten"", ""beaut*"", ""beloved"", ""benefic*"", ""benevolen*"", 
        ""benign*"", ""best"", ""better"", ""bitch*"", ""bitter*"", ""blam*"", ""bless*"", 
        ""bold*"", ""bonus*"", ""bore*"", ""boring"", ""bother*"", ""brave*"", ""bright*"", 
        ""brillian*"", ""broke"", ""burden*"", ""calm*"", ""cared"", ""carefree"", 
        ""careful*"", ""careless*"", ""cares"", ""casual"", ""casually"", ""certain*"", 
        ""challeng*"", ""champ*"", ""charit*"", ""charm*"", ""cheer*"", ""cherish*"", 
        ""chuckl*"", ""clever*"", ""comed*"", ""comfort*"", ""commitment*"", ""complain*"", 
        ""compliment*"", ""concerned"", ""confidence"", ""confident"", ""confidently"", 
        ""confront*"", ""confus*"", ""considerate"", ""contempt*"", ""contented*"", 
        ""contentment"", ""contradic*"", ""convinc*"", ""cool"", ""courag*"", ""crap"", 
        ""crappy"", ""craz*"", ""create*"", ""creati*"", ""credit*"", ""cried"", 
        ""cries"", ""critical"", ""critici*"", ""crude*"", ""cry"", ""crying"", ""cunt*"", 
        ""cut"", ""cute*"", ""cutie*"", ""cynic"", ""danger*"", ""daring"", ""darlin*"", 
        ""daze*"", ""dear*"", ""decay*"", ""defeat*"", ""defect*"", ""definite"", 
        ""definitely"", ""degrad*"", ""delectabl*"", ""delicate*"", ""delicious*"", 
        ""deligh*"", ""depress*"", ""depriv*"", ""despair*"", ""desperat*"", ""despis*"", 
        ""destruct*"", ""determina*"", ""determined"", ""devastat*"", ""difficult*"", 
        ""digni*"", ""disadvantage*"", ""disagree*"", ""disappoint*"", ""disaster*"", 
        ""discomfort*"", ""discourag*"", ""dishearten*"", ""disillusion*"", ""dislike"", 
        ""disliked"", ""dislikes"", ""disliking"", ""dismay*"", ""dissatisf*"", 
        ""distract*"", ""distraught"", ""distress*"", ""distrust*"", ""disturb*"", 
        ""divin*"", ""domina*"", ""doom*"", ""dork*"", ""doubt*"", ""dread*"", ""dull*"", 
        ""dumb*"", ""dump*"", ""dwell*"", ""dynam*"", ""eager*"", ""ease*"", ""easie*"", 
        ""easily"", ""easiness"", ""easing"", ""easy*"", ""ecsta*"", ""efficien*"", 
        ""egotis*"", ""elegan*"", ""embarrass*"", ""emotion"", ""emotional"", ""empt*"", 
        ""encourag*"", ""energ*"", ""engag*"", ""enjoy*"", ""enrag*"", ""entertain*"", 
        ""enthus*"", ""envie*"", ""envious"", ""excel*"", ""excit*"", ""excruciat*"", 
        ""exhaust*"", ""fab"", ""fabulous*"", ""fail*"", ""fake"", ""fantastic*"", 
        ""fatal*"", ""fatigu*"", ""favor*"", ""favour*"", ""fear"", ""feared"", ""fearful*"", 
        ""fearing"", ""fearless*"", ""fears"", ""feroc*"", ""festiv*"", ""feud*"", 
        ""fiery"", ""fiesta*"", ""fine"", ""fired"", ""flatter*"", ""flawless*"", 
        ""flexib*"", ""flirt*"", ""flunk*"", ""foe*"", ""fond"", ""fondly"", ""fondness"", 
        ""fool*"", ""forgave"", ""forgiv*"", ""fought"", ""frantic*"", ""freak*"", 
        ""free"", ""freeb*"", ""freed*"", ""freeing"", ""freely"", ""freeness"", 
        ""freer"", ""frees*"", ""friend*"", ""fright*"", ""frustrat*"", ""fuck"", 
        ""fucked*"", ""fucker*"", ""fuckin*"", ""fucks"", ""fume*"", ""fuming"", 
        ""fun"", ""funn*"", ""furious*"", ""fury"", ""geek*"", ""genero*"", ""gentle"", 
        ""gentler"", ""gentlest"", ""gently"", ""giggl*"", ""giver*"", ""giving"", 
        ""glad"", ""gladly"", ""glamor*"", ""glamour*"", ""gloom*"", ""glori*"", 
        ""glory"", ""goddam*"", ""gorgeous*"", ""gossip*"", ""grace"", ""graced"", 
        ""graceful*"", ""graces"", ""graci*"", ""grand"", ""grande*"", ""gratef*"", 
        ""grati*"", ""grave*"", ""great"", ""grief"", ""griev*"", ""grim*"", ""grin"", 
        ""grinn*"", ""grins"", ""grouch*"", ""grr*"", ""guilt*"", ""ha"", ""haha*"", 
        ""handsom*"", ""happi*"", ""happy"", ""harass*"", ""hated"", ""hateful*"", 
        ""hater*"", ""hates"", ""hating"", ""hatred"", ""hazy"", ""heartbreak*"", 
        ""heartbroke*"", ""heartfelt"", ""heartless*"", ""heartwarm*"", ""heh*"", 
        ""hellish"", ""helper*"", ""helpful*"", ""helping"", ""helpless*"", ""helps"", 
        ""hesita*"", ""hilarious"", ""hoho*"", ""homesick*"", ""honour*"", ""hope"", 
        ""hoped"", ""hopeful"", ""hopefully"", ""hopefulness"", ""hopeless*"", 
        ""hopes"", ""hoping"", ""horr*"", ""hostil*"", ""hug"", ""hugg*"", ""hugs"", 
        ""humiliat*"", ""humor*"", ""humour*"", ""hurra*"", ""idiot"", ""ignor*"", 
        ""impatien*"", ""impersonal"", ""impolite*"", ""importan*"", ""impress*"", 
        ""improve*"", ""improving"", ""inadequa*"", ""incentive*"", ""indecis*"", 
        ""ineffect*"", ""inferior*"", ""inhib*"", ""innocen*"", ""insecur*"", ""insincer*"", 
        ""inspir*"", ""insult*"", ""intell*"", ""interest*"", ""interrup*"", ""intimidat*"", 
        ""invigor*"", ""irrational*"", ""irrita*"", ""isolat*"", ""jaded"", ""jealous*"", 
        ""jerk"", ""jerked"", ""jerks"", ""joke*"", ""joking"", ""joll*"", ""joy*"", 
        ""keen*"", ""kidding"", ""kind"", ""kindly"", ""kindn*"", ""kiss*"", ""laidback"", 
        ""lame*"", ""laugh*"", ""lazie*"", ""lazy"", ""liabilit*"", ""libert*"", 
        ""lied"", ""lies"", ""like"", ""likeab*"", ""liked"", ""likes"", ""liking"", 
        ""livel*"", ""LMAO"", ""LOL"", ""lone*"", ""longing*"", ""lose"", ""loser*"", 
        ""loses"", ""losing"", ""loss*"", ""lost"", ""lous*"", ""love"", ""loved"", 
        ""lovely"", ""lover*"", ""loves"", ""loving*"", ""low*"", ""luck"", ""lucked"", 
        ""lucki*"", ""luckless*"", ""lucks"", ""lucky"", ""ludicrous*"", ""lying"", 
        ""mad"", ""maddening"", ""madder"", ""maddest"", ""madly"", ""magnific*"", 
        ""maniac*"", ""masochis*"", ""melanchol*"", ""merit*"", ""merr*"", ""mess"", 
        ""messy"", ""miser*"", ""miss"", ""missed"", ""misses"", ""missing"", ""mistak*"", 
        ""mock"", ""mocked"", ""mocker*"", ""mocking"", ""mocks"", ""molest*"", ""mooch*"", 
        ""mood"", ""moodi*"", ""moods"", ""moody"", ""moron*"", ""mourn*"", ""nag*"", 
        ""nast*"", ""neat*"", ""needy"", ""neglect*"", ""nerd*"", ""nervous*"", ""neurotic*"", 
        ""nice*"", ""numb*"", ""nurtur*"", ""obnoxious*"", ""obsess*"", ""offence*"", 
        ""offens*"", ""ok"", ""okay"", ""okays"", ""oks"", ""openminded*"", ""openness"", 
        ""opportun*"", ""optimal*"", ""optimi*"", ""original"", ""outgoing"", ""outrag*"", 
        ""overwhelm*"", ""pained"", ""painf*"", ""paining"", ""painl*"", ""pains"", 
        ""palatabl*"", ""panic*"", ""paradise"", ""paranoi*"", ""partie*"", ""party*"", 
        ""passion*"", ""pathetic*"", ""peculiar*"", ""perfect*"", ""personal"", 
        ""perver*"", ""pessimis*"", ""petrif*"", ""pettie*"", ""petty*"", ""phobi*"", 
        ""piss*"", ""piti*"", ""pity*"", ""play"", ""played"", ""playful*"", ""playing"", 
        ""plays"", ""pleasant*"", ""please*"", ""pleasing"", ""pleasur*"", ""poison*"", 
        ""popular*"", ""positiv*"", ""prais*"", ""precious*"", ""pressur*"", ""prettie*"", 
        ""pretty"", ""prick*"", ""pride"", ""privileg*"", ""prize*"", ""problem*"", 
        ""profit*"", ""promis*"", ""protested"", ""protesting"", ""proud*"", ""puk*"", 
        ""radian*"", ""rage*"", ""raging"", ""rancid*"", ""rape*"", ""raping"", ""rapist*"", 
        ""readiness"", ""ready"", ""reassur*"", ""reek*"", ""regret*"", ""reject*"", 
        ""relax*"", ""relief"", ""reliev*"", ""reluctan*"", ""remorse*"", ""repress*"", 
        ""resent*"", ""resign*"", ""resolv*"", ""restless*"", ""revigor*"", ""reward*"", 
        ""rich*"", ""ridicul*"", ""rigid*"", ""risk*"", ""ROFL"", ""romanc*"", ""romantic*"", 
        ""rotten"", ""rude*"", ""sad"", ""sadde*"", ""sadly"", ""sadness"", ""sarcas*"", 
        ""satisf*"", ""savage*"", ""scare*"", ""scaring"", ""scary"", ""sceptic*"", 
        ""scream*"", ""screw*"", ""selfish*"", ""sentimental*"", ""serious"", ""seriously"", 
        ""seriousness"", ""severe*"", ""shake*"", ""shaki*"", ""shaky"", ""share"", 
        ""shared"", ""shares"", ""sharing"", ""shit*"", ""shock*"", ""shook"", ""shy*"", 
        ""sigh"", ""sighed"", ""sighing"", ""sighs"", ""silli*"", ""silly"", ""sincer*"", 
        ""skeptic*"", ""smart*"", ""smil*"", ""smother*"", ""smug*"", ""snob*"", 
        ""sob"", ""sobbed"", ""sobbing"", ""sobs"", ""sociab*"", ""solemn*"", ""sorrow*"", 
        ""sorry"", ""soulmate*"", ""special"", ""splend*"", ""stammer*"", ""stank"", 
        ""startl*"", ""stink*"", ""strain*"", ""strange"", ""strength*"", ""stress*"", 
        ""strong*"", ""struggl*"", ""stubborn*"", ""stunk"", ""stunned"", ""stuns"", 
        ""stupid*"", ""stutter*"", ""succeed*"", ""success*"", ""suck"", ""sucked"", 
        ""sucker*"", ""sucks"", ""sucky"", ""sunnier"", ""sunniest"", ""sunny"", 
        ""sunshin*"", ""super"", ""superior*"", ""support"", ""supported"", ""supporter*"", 
        ""supporting"", ""supportive*"", ""supports"", ""suprem*"", ""sure*"", 
        ""surpris*"", ""suspicio*"", ""sweet"", ""sweetheart*"", ""sweetie*"", 
        ""sweetly"", ""sweetness*"", ""sweets"", ""talent*"", ""tantrum*"", ""tears"", 
        ""teas*"", ""tehe"", ""temper"", ""tempers"", ""tender*"", ""tense*"", ""tensing"", 
        ""tension*"", ""terribl*"", ""terrific*"", ""terrified"", ""terrifies"", 
        ""terrify"", ""terrifying"", ""terror*"", ""thank"", ""thanked"", ""thankf*"", 
        ""thanks"", ""thief"", ""thieve*"", ""thoughtful*"", ""threat*"", ""thrill*"", 
        ""ticked"", ""timid*"", ""toleran*"", ""tortur*"", ""tough*"", ""traged*"", 
        ""tragic*"", ""tranquil*"", ""trauma*"", ""treasur*"", ""treat"", ""trembl*"", 
        ""trick*"", ""trite"", ""triumph*"", ""trivi*"", ""troubl*"", ""TRUE"", ""trueness"", 
        ""truer"", ""truest"", ""truly"", ""trust*"", ""truth*"", ""turmoil"", ""ugh"", 
        ""ugl*"", ""unattractive"", ""uncertain*"", ""uncomfortabl*"", ""uncontrol*"", 
        ""uneas*"", ""unfortunate*"", ""unfriendly"", ""ungrateful*"", ""unhapp*"", 
        ""unimportant"", ""unimpress*"", ""unkind"", ""unlov*"", ""unpleasant"", 
        ""unprotected"", ""unsavo*"", ""unsuccessful*"", ""unsure*"", ""unwelcom*"", 
        ""upset*"", ""uptight*"", ""useful*"", ""useless*"", ""vain"", ""valuabl*"", 
        ""valuing"", ""vanity"", ""vicious*"", ""vigor*"", ""vigour*"", ""villain*"", 
        ""violat*"", ""virtuo*"", ""vital*"", ""vulnerab*"", ""vulture*"", ""warfare*"", 
        ""warm*"", ""warred"", ""weak*"", ""wealth*"", ""weapon*"", ""weep*"", ""weird*"", 
        ""welcom*"", ""well*"", ""wept"", ""whine*"", ""whining"", ""willing"", ""wimp*"", 
        ""win"", ""winn*"", ""wins"", ""wisdom"", ""wise*"", ""witch"", ""woe*"", ""won"", 
        ""wonderf*"", ""worr*"", ""worse*"", ""worship*"", ""worst"", ""wow*"", ""yay"", 
        ""yays"",""yearn*"",""stench*"")</code></p>

<hr>

<p>Code used from MRau's answer that hasn't worked for me:</p>

<pre><code>ind_stem &lt;- grep(""[*]"", kw_Emo)
kw_stem  &lt;- gsub(""[*]"", """", kw_Emo[ind_stem])
kw_word  &lt;- kw_Emo[-ind_stem]
tweets &lt;- strsplit(TestTweets[, ""clean_text""], ""\\s+"")

for (kws in kw_stem) {
  count_i &lt;- unlist(lapply(tweets, function(x) length(grep(kws, x))))
  TestTweets &lt;- cbind(TestTweets, count_i)
  colnames(TestTweets)[ncol(TestTweets)] &lt;- paste0(kws, ""*"")
}
for (kww in kw_word) {
  count_i &lt;- unlist(lapply(tweets, function(x) length(grep(paste0(""^"", kww, ""$""), x))))
  TestTweets &lt;- cbind(TestTweets, count_i)
  colnames(TestTweets)[ncol(TestTweets)] &lt;- kww
}
</code></pre>
","r, dictionary, text, text-mining, stringr","<p>So first of all I would get rid of some of the <code>for</code> loops:</p>

<pre><code>ind_stem &lt;- grep(""[*]"", kw_Emo)
kw_stem  &lt;- gsub(""[*]"", """", kw_Emo[ind_stem])
kw_word  &lt;- kw_Emo[-ind_stem]
tweets &lt;- strsplit(TestTweets[, ""clean_text""], ""\\s+"")
</code></pre>

<p>I generated a different vector for words and stems. <code>tweets</code> is a list of vector of words - <code>strsplit</code> splits the strings using blank space (<code>\\s+</code>) as separator.</p>

<p>When it comes to the matching of words/stems, you can use <code>grep</code> for both. By default, it will find all the words containing the given pattern:</p>

<pre><code>&gt; grep(""Abc"", c(""Abc"", ""Abcdef""))
[1] 1 2
</code></pre>

<p>But you can get the ""exact"" match if you use <code>^</code> and <code>$</code>:</p>

<pre><code>&gt; grep(""^Abc$"", c(""Abc"", ""Abcdef""))
[1] 1
</code></pre>

<p>In you code, you want to look at the length of the <code>grep</code> output and e.g. append it to your <code>data.frame</code>:</p>

<pre><code>for (kws in kw_stem) {
    count_i &lt;- unlist(lapply(tweets, function(x) length(grep(kws, x))))
    TestTweets &lt;- cbind(TestTweets, count_i)
    colnames(TestTweets)[ncol(TestTweets)] &lt;- paste0(kws, ""*"")
}
for (kww in kw_word) {
    count_i &lt;- unlist(lapply(tweets, function(x) length(grep(paste0(""^"", kww, ""$""), x))))
    TestTweets &lt;- cbind(TestTweets, count_i)
    colnames(TestTweets)[ncol(TestTweets)] &lt;- kww
}
</code></pre>

<p>Fragment of output:</p>

<pre><code>&gt; TestTweets[19:20, c(""clean_text"", ""boring"")]
                                                                                                                    clean_text boring
19 im laughing at people who voted for brexit but are complaining about the exchange rate affecting their holiday\r\nremain      0
20                                                                           life is too short to wear boring shoes  brexit      1
</code></pre>

<p>Of course, you can further optimize this code or decide whether or not use <code>grep(paste0(""^"", kws), x)</code> instead of <code>grep(kws, x)</code> in the first loop depending on your problem etc.</p>
",1,0,390,2019-01-08 10:35:01,https://stackoverflow.com/questions/54089957/counting-words-and-word-stems-in-a-large-dataframe-rstudio
R: save vector of text values into .txt files (each element of vector to seperate .txt file),"<p>I have a vector, where each values is a text value. To make it simple let's assume its:  <code>c('a','b','c','d')</code>
I managed only to save it as one big file:  </p>

<pre><code>write.table(myVector,""textFile.txt"",sep=""\t"",row.names=FALSE, col.names=FALSE, quote = FALSE)
</code></pre>

<p>How could I save each element to a separate file for example 1.txt, 2.txt, 3.txt etc?</p>
","r, text-mining","<p>This will work.</p>

<pre><code>myVector &lt;- c('a','b','c','d')

lapply(myVector, function(x) write.table(x, paste0(match(x, myVector), "".txt""), sep=""\t"",row.names=FALSE, col.names=FALSE, quote = FALSE))
</code></pre>
",1,0,708,2019-01-14 22:04:57,https://stackoverflow.com/questions/54189878/r-save-vector-of-text-values-into-txt-files-each-element-of-vector-to-seperat
How is the correct use of stemDocument?,"<p>I have already read <a href=""https://stackoverflow.com/questions/24311561/how-to-use-stemdocument-in-r"">this</a> and <a href=""https://stackoverflow.com/questions/31438688/why-isnt-stemdocument-stemming"">this</a> questions, but I still didn't understand the use of <code>stemDocument</code> in <code>tm_map</code>. Let's follow this example:</p>

<pre><code>q17 &lt;- VCorpus(VectorSource(x = c(""poder"", ""pode"")),
               readerControl = list(language = ""pt"",
                                    load = TRUE))
lapply(q17, content)
$`character(0)`
[1] ""poder""

$`character(0)`
[1] ""pode""
</code></pre>

<p>If I use:</p>

<pre><code>&gt; stemDocument(""poder"", language = ""portuguese"")
[1] ""pod""
&gt; stemDocument(""pode"", language = ""portuguese"")
[1] ""pod""
</code></pre>

<p>it does work! But if I use:</p>

<pre><code>&gt; q17 &lt;- tm_map(q17, FUN = stemDocument, language = ""portuguese"")
&gt; lapply(q17, content)
$`character(0)`
[1] ""poder""

$`character(0)`
[1] ""pode""
</code></pre>

<p>it doesn't work. Why so?</p>
","r, text-mining, tm, stemming, snowball","<p>Unfortunately you stumbled on a bug. <code>stemDocument</code> works if you pass on the language when you do: </p>

<pre><code>stemDocument(x = c(""poder"", ""pode""), language = ""pt"")
[1] ""pod"" ""pod""
</code></pre>

<p>But when using this in <code>tm_map</code>, the function starts of with <code>stemDocument.PlainTextDocument</code>. In this function the language of the corpus is checked against the language you supply in the function. This works correctly. But at the end of this function everything is passed on to the function <code>stemDocument.character</code>, but <strong><em>without the language component</em></strong>. In <code>stemDocument.character</code> the default language is specified as English. So within the <code>tm_map</code> call (or the <code>DocumentTermMatrix</code>) the language you supply with it will revert back to English and the stemming doesn't work correctly. </p>

<p>A workaround could be using the package quanteda:</p>

<pre><code>library(quanteda)
my_dfm &lt;- dfm(x = c(""poder"", ""pode""))
my_dfm &lt;- dfm_wordstem(my_dfm, language = ""pt"")

my_dfm

Document-feature matrix of: 2 documents, 1 feature (0.0% sparse).
2 x 1 sparse Matrix of class ""dfm""
       features
docs    pod
  text1   1
  text2   1
</code></pre>

<p>Since you are working with Portuguese, I suggest using the packages quanteda, udpipe, or both. Both packages handle non-English languages a lot better than tm. </p>
",2,3,935,2019-01-15 11:06:06,https://stackoverflow.com/questions/54197636/how-is-the-correct-use-of-stemdocument
How to import a lexicon in XML-LMF format for sentiment analysis in R,"<p>I'm trying to import the following lexicon in R, to be used with text mining packages such as <code>quanteda</code>, or to export it as a list or data frame:</p>

<p><a href=""https://github.com/opener-project/VU-sentiment-lexicon/tree/master/VUSentimentLexicon/IT-lexicon"" rel=""nofollow noreferrer"">https://github.com/opener-project/VU-sentiment-lexicon/tree/master/VUSentimentLexicon/IT-lexicon</a></p>

<p>The format is XML-LMF. I could not find any way to parse such a format with R. </p>

<p>(see <a href=""https://en.wikipedia.org/wiki/Lexical_Markup_Framework"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Lexical_Markup_Framework</a>)</p>

<p>As a workaround I tried to use the <code>XML</code> package, but the structure is a bit different from usual XML, and I did not manage to parse all the nodes. </p>
","r, xml-parsing, text-mining, sentiment-analysis, quanteda","<p>I managed to make it work using the <code>xml2</code> package. Here's my code:</p>

<pre><code>library(xml2)
library(quanteda)

# Read file and find the nodes
opeNER_xml &lt;- read_xml(""it-sentiment_lexicon.lmf.xml"")
entries &lt;- xml_find_all(opeNER_xml, "".//LexicalEntry"")
lemmas &lt;- xml_find_all(opeNER_xml, "".//Lemma"")
confidence &lt;- xml_find_all(opeNER_xml, "".//Confidence"")
sentiment &lt;- xml_find_all(opeNER_xml, "".//Sentiment"")

# Parse and put in a data frame
opeNER_df &lt;- data.frame(
  id = xml_attr(entries, ""id""),
  lemma = xml_attr(lemmas, ""writtenForm""),
  partOfSpeech = xml_attr(entries, ""partOfSpeech""),
  confidenceScore = as.numeric(xml_attr(confidence, ""score"")),
  method = xml_attr(confidence, ""method""),
  polarity = as.character(xml_attr(sentiment, ""polarity"")),
  stringsAsFactors = F
)
# Fix a mistake
opeNER_df$polarity &lt;- ifelse(opeNER_df$polarity == ""nneutral"", 
                             ""neutral"", opeNER_df$polarity)

# Make quanteda dictionary
opeNER_dict &lt;- quanteda::dictionary(with(opeNER_df, split(lemma, polarity)))
</code></pre>
",0,1,258,2019-01-22 10:45:07,https://stackoverflow.com/questions/54306513/how-to-import-a-lexicon-in-xml-lmf-format-for-sentiment-analysis-in-r
How do I extract the first word from a list of words?,"<p>I'm having trouble extracting the first word from a list of words. I've tried substring, gsub, and str_extract but still haven't figured it out. Please advise. Thank you. Here is what I'm trying to do:</p>

<pre><code>Word
""c(""print"", ""printing"", ""prints"")""
""c(""take"", ""takes"", ""taking"")""
""c(""score"", ""scoring"", ""scored"")""
</code></pre>

<p>I'm trying to extract the first word from the list that looks like this:</p>

<pre><code>Extracted
print
take
score
</code></pre>
","r, text-mining","<p>Using base R alone</p>

<pre><code>##Just to recreate the data
df &lt;- tibble(
  Word= list(c(""print"", ""printing"", ""prints""),c(""take"", ""takes"", ""taking""),c(""score"", ""scoring"", ""scored"")))

###
df$Extracted &lt;- sapply(1:length(df$Word), function(i)df$Word[[i]][1])
</code></pre>
",0,0,98,2019-01-24 17:05:33,https://stackoverflow.com/questions/54351949/how-do-i-extract-the-first-word-from-a-list-of-words
How to filter a dfm by documents with at least n terms in quanteda?,"<p>I am analyzing text data from a round table, and I would like to know if it is possible to filter only those documents which have more than ""n"" terms?</p>

<p>My corpus has documents which contains only 1 word, such as: ""Thanks"", ""Sometimes"", ""Really"", ""go"". I would like to remove then in order to decrease sparsity.</p>

<p>I tried <code>dfm_trim</code> from <code>quanteda</code> but I couldn't handle it:</p>

<pre><code>corpus_post80inaug &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1980)
dfm &lt;- dfm(corpus_post80inaug)
ntoken(dfm)
1981-Reagan  1985-Reagan    1989-Bush 1993-Clinton 1997-Clinton 
       2790         2921         2681         1833         2449 
  2001-Bush    2005-Bush   2009-Obama   2013-Obama   2017-Trump 
       1808         2319         2711         2317         1660 
dfm &lt;- dfm_trim(dfm, min_docfreq = 2000)
ntoken(dfm)
1981-Reagan  1985-Reagan    1989-Bush 1993-Clinton 1997-Clinton 
          0            0            0            0            0 
  2001-Bush    2005-Bush   2009-Obama   2013-Obama   2017-Trump 
          0            0            0            0            0 
</code></pre>

<p>I would expect that only 1993-Clinton, 2001-Bush and 2017-Trump would have 0, or get rid off <code>dfm</code>.
Obs.: This example is only for illustration purpose, it is not the text data I am analyzing.</p>
","r, text-mining, quanteda","<p>You should use <code>dfm_subset</code>, not <code>dfm_trim</code>. <code>dfm_trim</code> calculates frequencies across all documents, not per document. Though you can specify that the minimum (or maximum) documents that the term should appear in. For removing documents, we use <code>dfm_subset</code>.</p>

<pre><code>corpus_post80inaug &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1980)
dfm &lt;- dfm(corpus_post80inaug)

# remove documents with less than 2000 tokens. 
my_dfm &lt;- dfm_subset(dfm, ntoken(dfm) &gt;= 2000)

ntoken(my_dfm)
 1981-Reagan  1985-Reagan    1989-Bush 1997-Clinton    2005-Bush   2009-Obama   2013-Obama 
        2790         2921         2681         2449         2319         2711         2317 
</code></pre>
",2,1,972,2019-01-26 17:29:22,https://stackoverflow.com/questions/54380857/how-to-filter-a-dfm-by-documents-with-at-least-n-terms-in-quanteda
Adding text between 2 html tags,"<p>I am a 2 years student and I am working on text mining.</p>

<p>For general let me tell you about the code it first accept pdf type text and convert that in to <b>doc.txt</b> file, then I process that data for couple of hundred lines then after i store all sentences in that text to the list called <b>all_text</b> (for th future  use) and also I select some texts and store them in to a list called <b>summary</b>.</p>

<p>Finally the problem is on this part:</p>

<p>Summary list look like this</p>

<pre><code>summary=['Artificial Intelligence (AI) is a science and a set of computational technologies that are inspired by—but typically operate quite differently from—the ways people use their nervous systems and bodies to sense, learn, reason, and take action.','In reality, AI is already changing our daily lives, almost entirely in ways that improve human health, safety,and productivity.','AI is also changing how people interact with technology.']
</code></pre>

<p>What I want is read from doc.txt sentence by sentence and if that sentence is in the summary list modify that sentence by put it in to BOLD tag ""<b> the sentence</b>"" for all in the summary list here is small code i tried for that specific part it not help full but here it is </p>

<pre><code>while i &lt; len(lis):
    if lis[i] in txt:
        txt = txt.replace(lis[i], ""&lt;b&gt;"" + lis[i] + ""&lt;/b&gt;"")

        print(lis[i])

   i += 1
</code></pre>

<p>This code did not work as I expected, I mean it works for some short sentences, but it doesn't work for the sentences like those I don't have any idea why it's not working help me please?</p>
","python, html, text-mining","<p>For that purpose you might use list comprehension, example:</p>

<pre><code>summary = ['sentenceE','sentenceA']
text = ['sentenceA','sentenceB','sentenceC','sentenceD','sentenceE']
output = ['&lt;b&gt;'+i+'&lt;/b&gt;' if (i in summary) else i for i in text]
print(output) #prints ['&lt;b&gt;sentenceA&lt;/b&gt;', 'sentenceB', 'sentenceC', 'sentenceD', '&lt;b&gt;sentenceE&lt;/b&gt;']
</code></pre>

<p>Note that <code>summary</code> and <code>text</code> should be <code>list</code>s of <code>str</code>s.</p>
",0,1,87,2019-01-27 14:24:47,https://stackoverflow.com/questions/54389157/adding-text-between-2-html-tags
How to add list items to a new dataframe column inside another list in r?,"<p>I'm trying to extract co-author's names and affiliations for all publications on pubmed. I was able to get the list of author's name in a dataframe, but I now need to add the affiliation with the name. I've been trying to do this, but I'm not sure how. </p>

<p>I need to combine two lists: authors and affiliations for each authors into one.</p>

<pre><code>query = ""an author's name""

res &lt;- EUtilsSummary(query, db=""pubmed"", mindate=2015, maxdate=2019)
QueryCount(res)

auths &lt;- Author(EUtilsGet(res))
affs &lt;- Affiliation(EUtilsGet(res))

Last&lt;-sapply(auths, function(x)paste(x$LastName, x$ForeName, sep = "", ""))
auths2&lt;-as.data.frame(sort(table(unlist(Last)), dec=TRUE))
names(auths2)&lt;-c(""name"")
auths2
</code></pre>

<p>I'm using RISmed to extract the data. I want the data in the following format: </p>

<p>Lastname, Firstname  Affiliation</p>

<p>I don't care about the count. </p>

<p>I suppose the other way to look at this is the following: </p>

<p>Combine two lists together. </p>

<p>list A is a list of dataframe: There are multiple items in this list where each item has the following</p>

<pre><code>LastName   ForeName   Initials
A          B          AB
C          D          CD
</code></pre>

<p>list B is a list of lists: </p>

<pre><code>Affiliations:
""X university""
""Y University""
</code></pre>

<p>What I want to do is to combine these two lists together such that the affiliations show up for each authors as a column on the dataframe. The final result would be the following:</p>

<pre><code>LastName   ForeName   Initials   Affiliations
A          B          AB         ""X University""
C          D          CD         ""Y University""
</code></pre>
","r, dplyr, text-mining, pubmed","<p>Since some of the queries can return <code>NA</code> values for authors and zero length vectors for affiliations I made a small function that only <code>cbind()</code> the values if both lists entries are correct:</p>

<pre><code>special_cbind = function(authors,affiliations){
  if(length(affiliations) == 0 | all(is.na(authors)) ){
    authors
  }
  else if(nrow(authors) == length(affiliations)){
    cbind(authors,affiliations)
  }
  else{
    affiliations = rep(affiliations,nrow(authors))
    cbind(authors,affiliations)
  }

}
</code></pre>

<p>Then apply it to every list entry with Map.</p>

<pre><code>Map(special_cbind,auths,affs)
</code></pre>

<p>Does this work for your data?</p>
",2,1,113,2019-01-28 15:45:45,https://stackoverflow.com/questions/54405561/how-to-add-list-items-to-a-new-dataframe-column-inside-another-list-in-r
Pandas dataframe Merge text rows group by ID,"<p>I have a dataframe as follows:                                           </p>

<pre><code>ID    Date          Text  
1     01/01/2019    abcd
1     01/01/2019    pqrs
2     01/02/2019    abcd
2     01/02/2019    xyze
</code></pre>

<p>I want to merge <code>Text</code> by <code>ID</code> in Python using group by clause. </p>

<p>I want to merge '<code>Text</code>' columns by grouping <code>ID</code>.</p>

<pre><code>ID    Date        Text
1     01/01/2019  abcdpqrs
2     01/02/2019  abcdxyze
</code></pre>

<p>I want to do this in Python. </p>

<p>I have attempted following code chunks but it didn't work:</p>

<ol>
<li><p><code>groups = groupby(dataset_new, key=ID(1))</code></p></li>
<li><p><code>dataset_new.group_by{row['Reference']}.values.each do |group| 
puts [group.first['Reference'], group.map{|r| r['Text']} * ' '] * ' | ' 
end</code></p></li>
</ol>

<p>I also attempted to merge text in excel using formulas but it is also not giving required results.</p>
","python-3.x, merge, excel-formula, pandas-groupby, text-mining","<p>Try <code>groupby</code> and <code>sum</code>. Judging from your error message and the output of <code>df.info()</code> it seems there are mixed dtypes and <code>NaN</code> in column <code>Text</code>. I suggest converting <code>NaN</code> to empty string using <code>fillna('')</code>, then convert all elements in the column to string using <code>astype(str)</code>.  </p>

<pre><code>df = pd.DataFrame({'ID': [1,1,2,2], 
                   'Date': ['01/01/2019', '01/01/2019', '01/02/2019', '01/02/2019'],
                   'Text': ['abcd', 'pqrs', 'abcd', 'xyze']})

df['Text'] = df['Text'].fillna('').astype(str)
df_grouped = df.groupby(['ID', 'Date'])['Text'].sum()
print(df_grouped)
</code></pre>

<p>This should return</p>

<pre><code>ID  Date      
1   01/01/2019    abcdpqrs
2   01/02/2019    abcdxyze
</code></pre>
",3,5,3528,2019-01-30 08:27:24,https://stackoverflow.com/questions/54436161/pandas-dataframe-merge-text-rows-group-by-id
How to replace a list of misspelled words with a list of correct words?,"<p>I'm trying to figure out how to replace a long list of misspelled words from a list of correct words but not sure how to do it. Please advise if possible. Thank you.</p>

<p>I tried str_replace and gsub but it seems like because I want to implement the changes from a dataframe so it doesn't really work that way.</p>

<pre><code>df = tibble(Movie_Name = list(""Black Panthet"", ""Irom Man"", ""Captain Anerica"", ""Black Panthers"", ""Iron Men"", ""Captain America"", ""Avangers""))

correct = tibble(correct_movie_name = list(""Black Panther"", ""Iron Man"", ""Captain American"", ""Avengers""))
</code></pre>

<p>I expect the output to be like this:</p>

<pre><code>df = tibble(Movie_Name = list(""Black Panther"", ""Iron Man"", ""Captain America"", ""Black Panther"", ""Iron Man"", ""Captain America"", ""Avengers""))
</code></pre>
","r, text-mining","<p>Here is a solution based on answers by @G5W and avid_useR</p>

<pre><code>library(tidyverse)
library(stringdist)

Movie_Name = list(""Black Panthet"", ""Irom Man"", ""Captain Anerica"", ""Black Panthers"", ""Iron Men"", ""Captain America"", ""Avangers"")

correct_movie_name = list(""Black Panther"", ""Iron Man"", ""Captain America"", ""Avengers"")

New_Movie_name &lt;- lapply(Movie_Name, function(x) {
  lapply(correct_movie_name, function(y) {
    stringdist(x,y)
  }) %&gt;% unlist() %&gt;% which.min() %&gt;% correct_movie_name[[.]]
})

# New_Movie_name is a list of the same length as Movie_Name but with correct movie names based on elements in list correct_movie_name
</code></pre>
",0,3,481,2019-01-30 21:04:17,https://stackoverflow.com/questions/54449506/how-to-replace-a-list-of-misspelled-words-with-a-list-of-correct-words
Cannot seem to extract more than 88 tweets despite mining for trending keywords,"<p>I'm trying to search for around 20,000 tweets using keywords that are currently trending on my timeline.</p>

<p>However, I am only getting about 88 tweets. These are trending keywords in the entire country and it is highly unlikely that there are only 88 tweets available.</p>

<p>Here is my code</p>

<pre><code>library(rtweet)
sona_tweets &lt;- search_tweets(
    q = ""SONA19 OR SONA2019 OR SONA"", 
    n = 25000, 
    type = ""popular"",
    include_rts = FALSE,
    retryonratelimit = TRUE
)
</code></pre>
","r, twitter, text-mining","<p>When using <code>rtweet::search_tweets()</code>, you should take note of a few limitations and <code>type</code> argument. </p>

<p>First, <code>search_tweets()</code> only returns data from the past 6-9 days. In addition, to return more than 18,000 statuses in a single call, you must set <code>retryonlimit = TRUE</code>. </p>

<p>From the documentation, the <code>type</code> argument is defined as: </p>

<blockquote>
  <p>Character string specifying which type of search results to return
  from Twitter's REST API. The current default is type = ""recent"", other
  valid types include type = ""mixed"" and type = ""popular"".</p>
</blockquote>

<p>Therefore to get ""everything"" from the last 6-9 days, you'll want to use <code>type = ""mixed""</code>. This means you should change your code to this:</p>

<pre><code>library(rtweet)
sona_tweets &lt;- search_tweets(
    q = ""SONA19 OR SONA2019 OR SONA"", 
    n = 25000, 
    type = ""mixed"",
    include_rts = FALSE,
    retryonratelimit = TRUE
)
</code></pre>

<p>and you should return your expected results.</p>
",1,0,61,2019-02-08 14:25:44,https://stackoverflow.com/questions/54594478/cannot-seem-to-extract-more-than-88-tweets-despite-mining-for-trending-keywords
I need help dropping empty rows and rows with empty spaces from dataframe,"<p>I am trying to remove all empty rows from my dataframe. Problem is the rows aren't entirely empty, some have one space, other multiple spaces and new lines.</p>

<p>Here are examples:</p>

<pre><code>new_tweetsdf[[35]]
[1] "" \n                  \n \n""
#second example
new_tweetsdf[[102]]
[1] "" \n""
#third example
new_tweetsdf[[188]]
[1] "" ""
#fourth example
new_tweetsdf[[4671]]
[1] ""\n\n""
#fourth example
new_tweetsdf[[11326]]
[1] ""\n\n\n\n""
#fifth example
new_tweetsdf[[27137]]
[1] ""\n\n\n\n \n""
</code></pre>

<p>I have tried a few solutions. Firstly I tried removing all rows that are empty and contain no space</p>

<pre><code>new_tweetsdf &lt;- new_tweetsdf[rowSums(new_tweetsdf=="" "") | rowSums(new_tweetsdf=="""") !=ncol(new_tweetsdf), ] 
</code></pre>

<p>But i'm left with empty rows that contain \n or multiple lines. This also makes it difficult to remove rows based on number of characters</p>

<p>I also thought about removing all rows that do not start with a letter</p>

<pre><code>new&lt;- new_tweetsdf[grep('^[a-z]',new_tweetsdf)]
</code></pre>

<p>However this removes about a 5th of my rows, from observation it doesn't seem likely that there are that many empty spaces in my dataframe. This probably also removes rows that begin with a space but actually have letters</p>

<p>Here is a link to the data I am using:
<a href=""https://drive.google.com/open?id=1kF93MKgsvp1CoFdSGV-ZkUbaTTvanI4F"" rel=""nofollow noreferrer"">Data</a></p>
","r, text-mining","<p>You can remove all the indices with a simple regex.</p>

<pre><code>pattern = ""^[[:space:]]*$""
</code></pre>

<p>This pattern will match all rows which only contain ""Space, tab, vertical tab, newline, form feed and carriage return"". Therefore we need to invert our result, since we want the opposite.</p>

<pre><code>new &lt;- new_tweetsdf[grep(pattern, new_tweetsdf, invert = TRUE)]
</code></pre>
",1,0,69,2019-02-09 12:01:37,https://stackoverflow.com/questions/54606004/i-need-help-dropping-empty-rows-and-rows-with-empty-spaces-from-dataframe
Parse Parts of Speech Tagged Tree Corpus with Python without NLTK,"<p>I have tree corpus as below</p>

<pre><code>(TOP END_OF_TEXT_UNIT)

(TOP (S (NP (DT The)
            (NNP Fulton)
            (NNP County)
            (NNP Grand)
            (NNP Jury))
        (VP (VBD said)
            (NP (NNP Friday))
            (SBAR (-NONE- 0)
                  (S (NP (DT an)
                         (NN investigation)
                         (PP (IN of)
                             (NP (NP (NNP Atlanta))
                                 (POS 's)
                                 (JJ recent)
                                 (JJ primary)
                                 (NN election))))
                     (VP (VBD produced)
                         (NP (`` ``)
                             (DT no)
                             (NN evidence)
                             ('' '')
                             (SBAR (IN that)
                                   (S (NP (DT any)
                                          (NNS irregularities))
                                      (VP (VBD took)
                                          (NP (NN place)))))))))))
     (. .))
</code></pre>

<p>I need to parse this tree and convert into a sentence form as below </p>

<pre><code>DT The NNP Fulton NNP County NNP Grand NNP Jury VBD said NNP Friday DT
an NN investigation ...
</code></pre>

<p>Is there any algorithm to parse the above content or we need to use regular expressions to do this and I do not want to use NLTK packages to do this. </p>
","python, regex, parsing, nlp, text-mining","<p>Pyparsing makes quick work of nested expression parsing.</p>

<pre><code>import pyparsing as pp

LPAR, RPAR = map(pp.Suppress, ""()"")
expr = pp.Forward()
label = pp.Word(pp.alphas.upper()+'-') | ""''"" | ""``"" | "".""
word = pp.Literal(""."") | ""''"" | ""``"" | pp.Word(pp.printables, excludeChars=""()"")

expr &lt;&lt;= LPAR + label + (word | pp.OneOrMore(expr)) + RPAR

sample = """"""
(TOP (S (NP (DT The)
            (NNP Fulton)
            (NNP County)
            (NNP Grand)
            (NNP Jury))
        (VP (VBD said)
            (NP (NNP Friday))
            (SBAR (-NONE- 0)
                  (S (NP (DT an)
                         (NN investigation)
                         (PP (IN of)
                             (NP (NP (NNP Atlanta))
                                 (POS 's)
                                 (JJ recent)
                                 (JJ primary)
                                 (NN election))))
                     (VP (VBD produced)
                         (NP (`` ``)
                             (DT no)
                             (NN evidence)
                             ('' '')
                             (SBAR (IN that)
                                   (S (NP (DT any)
                                          (NNS irregularities))
                                      (VP (VBD took)
                                          (NP (NN place)))))))))))
     (. .))
""""""

result = pp.OneOrMore(expr).parseString(sample)
print(' '.join(result))
</code></pre>

<p>Prints:</p>

<pre><code>TOP S NP DT The NNP Fulton NNP County NNP Grand NNP Jury VP VBD said NP NNP Friday SBAR -NONE- 0 S NP DT an NN investigation PP IN of NP NP NNP Atlanta POS 's JJ recent JJ primary NN election VP VBD produced NP `` `` DT no NN evidence '' '' SBAR IN that S NP DT any NNS irregularities VP VBD took NP NN place . .
</code></pre>

<p>Normally, parsers like this will use <code>pp.Group(expr)</code> to retain the grouping of the nested elements. But in your case, since you eventually want a flat list anyway, we just leave that out - pyparsing's default behavior is to just return a flat list of the matched strings.</p>
",1,0,229,2019-02-10 03:30:30,https://stackoverflow.com/questions/54613100/parse-parts-of-speech-tagged-tree-corpus-with-python-without-nltk
Structuring a large text file into a dataframe using R,"<p>I have a text file of around 20 pages with around 200 paragraphs. Each paragraph contains three lines describing information about a person like so:</p>

<pre><code>Name: John
Age: 26
Phone number: 123421

Name: Mary
Age: 80
Phone number: NA

...
</code></pre>

<p>Now I wish to convert this large file into a dataframe where the columns represent the three variables Name, Age and Phone number and where the rows correspond to the persons.</p>

<pre><code>Name      Age      Phone number
John      26       123421
Mary      80       NA
...       ...      ...
</code></pre>

<p>How can I convert the large text file into such a dataframe?</p>
","r, text, text-mining","<p>Not pretty but here is a regex option that may work depending on how the data is read in,</p>

<pre><code>test&lt;-
""Name: John
Age: 26
Phone number: 123421

Name: Mary
Age: 80
Phone number: NA
""
</code></pre>

<p>This is read in as:</p>

<pre><code>[1] ""Name: John\nAge: 26\nPhone number: 123421\n\nName: Mary\nAge: 80\nPhone number: NA\n""
</code></pre>

<p>Now using regex to get all matches, always catching NA's to ensure same number of rows:</p>

<pre><code>Names&lt;-regmatches(test, gregexpr(""(?&lt;=Name: )[a-zA-Z]+"", test, perl=TRUE))

Numbers&lt;-regmatches(test, gregexpr(""(?&lt;=Phone number: )[a-zA-Z0-9]+"", test, perl=TRUE))

Age&lt;-regmatches(test, gregexpr(""(?&lt;=Age: )[a-zA-Z0-9]+"", test, perl=TRUE))

df&lt;-data.frame(Names,Numbers,Age)
names(df)&lt;-c(""Name"",""Number"",""Age"")

&gt; df
  Name Number Age
1 John 123421  26
2 Mary     NA  80
</code></pre>

<p>Here is how to format the data for this approach if it is read in using <code>read.csv</code></p>

<pre><code>test&lt;-read.csv(text=test, header=F, stringsAsFactors=FALSE)
test&lt;-list(test$V1)
test&lt;-paste(unlist(test), collapse ="" "")
&gt;test
[1] ""Name: John Age: 26 Phone number: 123421 Name: Mary Age: 80 Phone number: NA""
</code></pre>

<p>If you have last names our regex for the <code>Names</code> argument will need to be changed too:</p>

<p><code>(?&lt;=Name: ).+?(?=Age)</code></p>
",3,2,254,2019-02-15 15:18:53,https://stackoverflow.com/questions/54712257/structuring-a-large-text-file-into-a-dataframe-using-r
Extracting full name and country code of geolocation with Twitter,"<p>I am trying to extract full name and country code of a ""place"" from a json file that has thousands of tweets mined from Twitter API. I have tried to write the code several different times, but the code below makes the most sense to me. I was able to extract other data (hashtags, dates, followers with similar coding).</p>

<pre><code>for i in range(len(data)):    

    if data[i][""place""] != ""null"":
        get_place_info  = data[i][""place""].get(""full_name"")
        print(get_place_info)

    else:
        print(""No place defined"")
</code></pre>

<p>This is the error I get:</p>

<hr>

<p>AttributeError                            Traceback (most recent call last)
 in 
      3 
      4     if data[i][""place""] != ""null"":
----> 5         get_place_info  = data[i][""place""].get(""full_name"")
      6         print(get_place_info)
      7 </p>

<p>AttributeError: 'NoneType' object has no attribute 'get'</p>
","python-3.x, twitter, text-mining","<p><em>adding an answer from my previous comment...</em></p>

<p>have you tried changing your if statement to </p>

<pre><code>if data[i]['place'] is not None:
</code></pre>
",1,1,145,2019-02-16 21:40:17,https://stackoverflow.com/questions/54727925/extracting-full-name-and-country-code-of-geolocation-with-twitter
determine the temporality of a sentence with POS tagging,"<p>I want to find out whether an action has been carried out if will be carried out from a series of sentences.
For example: 
<code>""I will prescribe this medication""</code> versus <code>""I prescribed this medication""</code> or <code>""He had already taken the stuff""</code> versus <code>""he may take the stuff later""</code></p>

<p>I was trying a <code>tidytext</code> approach and decided to simply look for past participle versus future participle verbs. However when I POS tag using the only types of verbs I get are <code>""Verb intransitive""</code>, <code>""Verb (usu participle)""</code> and <code>""Verb (transitive)""</code>. How can I get an idea of past or future verbs or is there another POS tagger I can use?</p>

<p>I am keen to use <code>tidytext</code> because I cannot install <code>rjava</code> which some of the other text mining packages use.</p>
","r, text-mining, tidytext","<p>Look at the <strong>morphological features</strong> from the <code>udpipe</code> annotation. These are put in the <strong>feats</strong> column of the annotation. And you can put these as extra columns in the dataset by using <code>cbind_morphological</code>.
All the features are defined at <a href=""https://universaldependencies.org/u/feat/index.html"" rel=""nofollow noreferrer"">https://universaldependencies.org/u/feat/index.html</a>
You'll see below that prescribed from the sentence 'I prescribed this medication' is <strong>past tense</strong> as well as the word taken and had from 'he had already taken'.</p>

<pre><code>library(udpipe)
x &lt;- data.frame(doc_id = 1:4, 
                text = c(""I will prescribe this medication"", 
                         ""I prescribed this medication"", 
                         ""He had already taken the stuff"", 
                         ""he may take the stuff later""), 
                stringsAsFactors = FALSE)
anno &lt;- udpipe(x, ""english"")
anno &lt;- cbind_morphological(anno)

anno[, c(""doc_id"", ""token"", ""lemma"", ""feats"", ""morph_verbform"", ""morph_tense"")]

 doc_id      token      lemma                                                  feats morph_verbform morph_tense
      1          I          I             Case=Nom|Number=Sing|Person=1|PronType=Prs           &lt;NA&gt;        &lt;NA&gt;
      1       will       will                                           VerbForm=Fin            Fin        &lt;NA&gt;
      1  prescribe  prescribe                                           VerbForm=Inf            Inf        &lt;NA&gt;
      1       this       this                               Number=Sing|PronType=Dem           &lt;NA&gt;        &lt;NA&gt;
      1 medication medication                                            Number=Sing           &lt;NA&gt;        &lt;NA&gt;
      2          I          I             Case=Nom|Number=Sing|Person=1|PronType=Prs           &lt;NA&gt;        &lt;NA&gt;
      2 prescribed  prescribe                       Mood=Ind|Tense=Past|VerbForm=Fin            Fin        Past
      2       this       this                               Number=Sing|PronType=Dem           &lt;NA&gt;        &lt;NA&gt;
      2 medication medication                                            Number=Sing           &lt;NA&gt;        &lt;NA&gt;
      3         He         he Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs           &lt;NA&gt;        &lt;NA&gt;
      3        had       have                       Mood=Ind|Tense=Past|VerbForm=Fin            Fin        Past
      3    already    already                                                   &lt;NA&gt;           &lt;NA&gt;        &lt;NA&gt;
      3      taken       take                               Tense=Past|VerbForm=Part           Part        Past
      3        the        the                              Definite=Def|PronType=Art           &lt;NA&gt;        &lt;NA&gt;
      3      stuff      stuff                                            Number=Sing           &lt;NA&gt;        &lt;NA&gt;
      4         he         he Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs           &lt;NA&gt;        &lt;NA&gt;
      4        may        may                                           VerbForm=Fin            Fin        &lt;NA&gt;
      4       take       take                                           VerbForm=Inf            Inf        &lt;NA&gt;
      4        the        the                              Definite=Def|PronType=Art           &lt;NA&gt;        &lt;NA&gt;
      4      stuff      stuff                                            Number=Sing           &lt;NA&gt;        &lt;NA&gt;
      4      later      later                                                   &lt;NA&gt;           &lt;NA&gt;        &lt;NA&gt;
</code></pre>
",1,0,307,2019-02-18 13:22:14,https://stackoverflow.com/questions/54748306/determine-the-temporality-of-a-sentence-with-pos-tagging
Python: Check if the sentence contains any word from List (with fuzzy match),"<p>I would like to extract keywords from a sentence given a list_of_keywords.</p>

<p>I managed to extract the exact words</p>

<pre><code>[word for word in Sentence if word in set(list_of_keywords)]
</code></pre>

<p>Is it possible to extract words that have good similarity with the given list_of_keywords, i.e cosine similarity between two words is > 0.8</p>

<p>For example, the keyword in the given list is 'allergy' and now the sentence is written as</p>

<p>'a severe allergic reaction to nuts in the meal she had consumed.'</p>

<p>the cosine distance between 'allergy' and 'allergic' can be calculated as below</p>

<pre><code>cosdis(word2vec('allergy'), word2vec('allergic'))
Out[861]: 0.8432740427115677
</code></pre>

<p>How to extract 'allergic' from the sentence as well based on the cosine similarity? </p>
","python, text-mining, fuzzy-search","<pre><code>def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the ""length"" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]


list_of_keywords = ['allergy', 'something']
Sentence = 'a severe allergic reaction to nuts in the meal she had consumed.'

threshold = 0.80
for key in list_of_keywords:
    for word in Sentence.split():
        try:
            # print(key)
            # print(word)
            res = cosdis(word2vec(word), word2vec(key))
            # print(res)
            if res &gt; threshold:
                print(""Found a word with cosine distance &gt; 80 : {} with original word: {}"".format(word, key))
        except IndexError:
            pass
</code></pre>

<p><strong>OUTPUT</strong>:</p>

<pre><code>Found a word with cosine distance &gt; 80 : allergic with original word: allergy
</code></pre>

<p><strong>EDIT</strong>:</p>

<p>one-liner killer:</p>

<pre><code>print([x for x in Sentence.split() for y in list_of_keywords if cosdis(word2vec(x), word2vec(y)) &gt; 0.8])
</code></pre>

<p><strong>OUTPUT</strong>:</p>

<pre><code>['allergic']
</code></pre>
",4,2,1250,2019-02-21 13:01:35,https://stackoverflow.com/questions/54807745/python-check-if-the-sentence-contains-any-word-from-list-with-fuzzy-match
matching keywords to a series of text comments,"<p>I have two sets of information:</p>

<ol>
<li><p>A csv file where every row has a comment, e.g:</p>

<p>a. I love football
  b. Rugby is a tough game
  c. Hello World</p></li>
<li><p>Another csv file that list words related to sports, e.g:</p>

<p>a. tennis
  b. football
  c. rugby</p></li>
</ol>

<p>What I want to do is: 
  a. find whether any of the words in the second file appears at least once in every individual rows of the first file.
  b. If it appears at least once, it should be categorize as sports against every comment, else others.</p>

<p>The output file should look like:</p>

<pre><code>Comments                          category
  a. I love football               sports
  b. Rugby is a tough game         sports
  c. Hello World                   others
</code></pre>

<p>I want to do this exercise in R. I explored str_detect &amp; grepl function in R but not achieving the desire output.</p>

<p>Your help is appreciated.</p>

<p>Thanks</p>
","r, text, text-mining","<p>Here is one approach that iterates over the keywords and matches the sentences using <code>grepl</code>.  Depending on how clean the sentence data is you may want to consider <code>agrepl</code> which allows for fuzzy matching (but may also result in false positives).</p>

<pre><code>df &lt;- data.frame(sentences=c(""I love football"", ""Rugby is a tough game"", ""Hello World""))
keywords &lt;- c(""tennis"", ""football"", ""rugby"")

cbind(df, sapply(keywords, function(x) grepl(x, df$sentences, ignore.case = TRUE)))

               sentences tennis football rugby
 1       I love football  FALSE     TRUE FALSE
 2 Rugby is a tough game  FALSE    FALSE  TRUE
 3           Hello World  FALSE    FALSE FALSE
</code></pre>

<p>Re-reading your post, if you just want any rather than individual sports to be flagged you can do:</p>

<pre><code>cbind(df, sports = rowSums(sapply(keywords, function(x) grepl(x, df$sentences, ignore.case = TRUE))) &gt; 0)
              sentences sports
1       I love football   TRUE
2 Rugby is a tough game   TRUE
3           Hello World  FALSE
</code></pre>
",0,0,70,2019-02-22 04:27:50,https://stackoverflow.com/questions/54820070/matching-keywords-to-a-series-of-text-comments
Lemmatizing whole sentence in python does not work,"<p>I am using WordNetLemmatizer() function in NLTK package in python to lemmatize the entire sentence of movie review dataset.</p>

<p>Here is my code:</p>

<pre><code>from nltk.stem import LancasterStemmer, WordNetLemmatizer
lemmer = WordNetLemmatizer()

def preprocess(x):

    #Lemmatization
    x = ' '.join([lemmer.lemmatize(w) for w in x.rstrip().split()])

    # Lower case
    x = x.lower()

    # Remove punctuation
    x = re.sub(r'[^\w\s]', '', x)

    # Remove stop words
    x = ' '.join([w for w in x.split() if w not in stop_words])    
    ## EDIT CODE HERE ## 

    return x

df['review_clean'] = df['review'].apply(preprocess)
</code></pre>

<p>review in df is the column of text reviews that I wanted to process</p>

<p>After using the preprocess function on df, the new column review_clean contains cleaned text data but it still does not have lemmatized text. eg. I can see a lot words ends with -ed, -ing. </p>

<p>Thanks in advance.</p>
","python, pandas, scikit-learn, nltk, text-mining","<p>You have to pass 'v' (verb) to lemmatize:</p>

<pre><code>x = ' '.join([lemmer.lemmatize(w, 'w') for w in x.rstrip().split()])
</code></pre>

<hr>

<p>Example:</p>

<pre><code>In [11]: words = [""answered"", ""answering""]

In [12]: [lemmer.lemmatize(w) for w in words]
Out[12]: ['answered', 'answering']

In [13]: [lemmer.lemmatize(w, 'v') for w in words]
Out[13]: ['answer', 'answer']
</code></pre>
",1,1,1398,2019-02-23 20:19:16,https://stackoverflow.com/questions/54845821/lemmatizing-whole-sentence-in-python-does-not-work
Find similar/synonyms/context words Python,"<p>Hello i'm looking to find a solution of my issue :
I Want to find a list of similar words with french and english
For example :
name could be : first name, last name, nom, prénom, username....
Postal address could be : city, country, street, ville, pays, code postale ....</p>
","python, text-mining, wordnet","<p>The other answer, and comments, describe how to get synonyms, but I think you want more than that?</p>

<p>I can suggest two broad approaches: WordNet and word embeddings.</p>

<p>Using nltk and wordnet, you want to explore the adjacent graph nodes. See <a href=""http://www.nltk.org/howto/wordnet.html"" rel=""noreferrer"">http://www.nltk.org/howto/wordnet.html</a> for an overview of the functions available. I'd suggest that once you've found your start word in Wordnet, follow all its relations, but also go up to the hypernym, and do the same there.</p>

<p>Finding the start word is not always easy:
<a href=""http://wordnetweb.princeton.edu/perl/webwn?s=Postal+address&amp;sub=Search+WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h="" rel=""noreferrer"">http://wordnetweb.princeton.edu/perl/webwn?s=Postal+address&amp;sub=Search+WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h=</a></p>

<p>Instead it seems I have to use ""address"": <a href=""http://wordnetweb.princeton.edu/perl/webwn?s=address&amp;sub=Search+WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h="" rel=""noreferrer"">http://wordnetweb.princeton.edu/perl/webwn?s=address&amp;sub=Search+WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h=</a>
and then decide which of those is the correct sense here. Then try clicking the hypernym, hyponym, sister term, etc.
To be honest, none of those feels quite right.</p>

<p>Open Multilingual WordNet tries to link different languages. <a href=""http://compling.hss.ntu.edu.sg/omw/"" rel=""noreferrer"">http://compling.hss.ntu.edu.sg/omw/</a>  So you could take your English WordNet code, and move to the French WordNet with it, or vice versa.</p>

<p>The other approach is to use word embeddings. You find the, say, 300 dimensional, vector of your source word, and then hunt for the nearest words in that vector space. This will be returning words that are used in similar contexts, so they could be similar meaning, or similar syntactically.</p>

<p>Spacy has a good implementation, see <a href=""https://spacy.io/usage/spacy-101#vectors-similarity"" rel=""noreferrer"">https://spacy.io/usage/spacy-101#vectors-similarity</a> and <a href=""https://spacy.io/usage/vectors-similarity"" rel=""noreferrer"">https://spacy.io/usage/vectors-similarity</a></p>

<p>Regarding English and French, normally you would work in the two languages independently. But if you search for ""multilingual word embeddings"" you will find some papers and projects where the vector stays the same for the same concept in different languages.</p>

<p>Note: the API is geared towards telling you how two words are similar, not finding similar words. To find similar words you need to take your vector and compare with every other word vector, which is O(N) in the size of the vocabulary. So you might want to do this offline, and build your own ""synonyms-and-similar"" dictionary for each word of interest.</p>
",8,3,13259,2019-02-26 10:00:28,https://stackoverflow.com/questions/54882858/find-similar-synonyms-context-words-python
"Error in aggregate.data.frame(as.data.frame(x), ...) : arguments must have same length","<p>Hi I'm working with the last example in this tutorial: Topics proportions over time.
<a href=""https://tm4ss.github.io/docs/Tutorial_6_Topic_Models.html"" rel=""nofollow noreferrer"">https://tm4ss.github.io/docs/Tutorial_6_Topic_Models.html</a></p>

<p>I run it for my data with this code</p>

<pre><code>library(readxl)
library(tm)
# Import text data

tweets &lt;- read_xlsx(""C:/R/data.xlsx"")

textdata &lt;- tweets$text

#Load in the library 'stringr' so we can use the str_replace_all function. 
library('stringr')

#Remove URL's 
textdata &lt;- str_replace_all(textdata, ""https://t.co/[a-z,A-Z,0-9]*"","""")


textdata &lt;- gsub(""@\\w+"", "" "", textdata)  # Remove user names (all proper names if you're wise!)

textdata &lt;- iconv(textdata, to = ""ASCII"", sub = "" "")  # Convert to basic ASCII text to avoid silly characters
textdata &lt;- gsub(""#\\w+"", "" "", textdata)

textdata &lt;- gsub(""http.+ |http.+$"", "" "", textdata)  # Remove links

textdata &lt;- gsub(""[[:punct:]]"", "" "", textdata)  # Remove punctuation


#Change all the text to lower case
textdata &lt;- tolower(textdata)



#Remove Stopwords. ""SMART"" is in reference to english stopwords from the SMART information retrieval system and stopwords from other European Languages.
textdata &lt;- tm::removeWords(x = textdata, c(stopwords(kind = ""SMART"")))


textdata &lt;- gsub("" +"", "" "", textdata) # General spaces (should just do all whitespaces no?)

# Convert to tm corpus and use its API for some additional fun
corpus &lt;- Corpus(VectorSource(textdata))  # Create corpus object


#Make a Document Term Matrix
dtm &lt;- DocumentTermMatrix(corpus)

ui = unique(dtm$i)
dtm.new = dtm[ui,]

#Fixes this error: ""Each row of the input matrix needs to contain at least one non-zero entry"" See: https://stackoverflow.com/questions/13944252/remove-empty-documents-from-documenttermmatrix-in-r-topicmodels
#rowTotals &lt;- apply(datatm , 1, sum) #Find the sum of words in each Document
#dtm.new   &lt;- datatm[rowTotals&gt; 0, ]

library(""ldatuning"")
library(""topicmodels"")

k &lt;- 7

ldaTopics &lt;- LDA(dtm.new, method = ""Gibbs"", control=list(alpha = 0.1, seed = 77), k = k)


#####################################################
#topics by year

tmResult &lt;- posterior(ldaTopics)
tmResult
theta &lt;- tmResult$topics
dim(theta)
library(ggplot2)
terms(ldaTopics, 7)

tweets$decade &lt;- paste0(substr(tweets$date2, 0, 3), ""0"")

topic_proportion_per_decade &lt;- aggregate(theta, by = list(decade = tweets$decade), mean)


top5termsPerTopic &lt;- terms(topicModel, 7)
topicNames &lt;- apply(top5termsPerTopic, 2, paste, collapse="" "")

# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] &lt;- topicNames


# reshape data frame
vizDataFrame &lt;- melt(topic_proportion_per_decade, id.vars = ""decade"")

# plot topic proportions per deacde as bar plot
require(pals)
ggplot(vizDataFrame, aes(x=decade, y=value, fill=variable)) + 
  geom_bar(stat = ""identity"") + ylab(""proportion"") + 
  scale_fill_manual(values = paste0(alphabet(20), ""FF""), name = ""decade"") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
</code></pre>

<p>Here is the excel file to the input data
<a href=""https://www.mediafire.com/file/4w2hkgzzzaaax88/data.xlsx/file"" rel=""nofollow noreferrer"">https://www.mediafire.com/file/4w2hkgzzzaaax88/data.xlsx/file</a></p>

<p>I got the error when I run the line with the aggregate function, I can't find out what is going on with the aggregate, I created the ""decade"" variable the same as in the tutoria, I show it and looks ok, the theta variable is also ok.. I changed several times the aggregate function according for example to this post
 <a href=""https://stackoverflow.com/questions/28325922/error-in-aggregate-data-frame-arguments-must-have-same-length"">Error in aggregate.data.frame : arguments must have same length</a></p>

<p>But still have the same error.. please help</p>
","r, text-mining, topic-modeling","<p>I am not sure what you want to achieve with the command</p>

<pre><code>topic_proportion_per_decade &lt;- aggregate(theta, by = list(decade = tweets$decade), mean)
</code></pre>

<p>As far as I see you produce only one decade with</p>

<pre><code>tweets$decade &lt;- paste0(substr(tweets$date2, 0, 3), ""0"")
table(tweets$decade)

2010 
3481 
</code></pre>

<p>With all the preprocessing from <code>tweets</code> to <code>textdata</code> you're producing a few empty lines. This is where your problem starts. 
Textdata with its new empty lines is the basis of your <code>corpus</code> and your <code>dtm</code>. You get rid of them with the lines:</p>

<pre><code>ui = unique(dtm$i)
dtm.new = dtm[ui,]
</code></pre>

<p>At the same time you're basically deleting the empty columns in the dtm, thereby changing the length of your object. This new dtm without the empty cells is
then your new basis for the topic model. This is coming back to haunt you, when you try to use <code>aggregate()</code> with two objects of different lengths: <code>tweets$decade</code>, which is still the old length of 3418 with <code>theta</code>, that is produced by the topic model, which in turn is based on dtm.new -- remember, the one with fewer rows.</p>

<p>What I would suggest is to, first, get an ID-column in <code>tweets</code>. Later on you can use the IDs to find out what texts later on get deleted by your preprocessing and match the length of <code>tweet$decade</code> and <code>theta</code>.</p>

<p>I rewrote your code -- try this out: </p>

<pre><code>library(readxl)
library(tm)
# Import text data

tweets &lt;- read_xlsx(""data.xlsx"")

## Include ID for later
tweets$ID &lt;- 1:nrow(tweets)

textdata &lt;- tweets$text

#Load in the library 'stringr' so we can use the str_replace_all function. 
library('stringr')

#Remove URL's 
textdata &lt;- str_replace_all(textdata, ""https://t.co/[a-z,A-Z,0-9]*"","""")


textdata &lt;- gsub(""@\\w+"", "" "", textdata)  # Remove user names (all proper names if you're wise!)

textdata &lt;- iconv(textdata, to = ""ASCII"", sub = "" "")  # Convert to basic ASCII text to avoid silly characters
textdata &lt;- gsub(""#\\w+"", "" "", textdata)

textdata &lt;- gsub(""http.+ |http.+$"", "" "", textdata)  # Remove links

textdata &lt;- gsub(""[[:punct:]]"", "" "", textdata)  # Remove punctuation

#Change all the text to lower case
textdata &lt;- tolower(textdata)

#Remove Stopwords. ""SMART"" is in reference to english stopwords from the SMART information retrieval system and stopwords from other European Languages.
textdata &lt;- tm::removeWords(x = textdata, c(stopwords(kind = ""SMART"")))

textdata &lt;- gsub("" +"", "" "", textdata) # General spaces (should just do all whitespaces no?)

# Convert to tm corpus and use its API for some additional fun
corpus &lt;- Corpus(VectorSource(textdata))  # Create corpus object

#Make a Document Term Matrix
dtm &lt;- DocumentTermMatrix(corpus)
ui = unique(dtm$i)
dtm.new = dtm[ui,]

#Fixes this error: ""Each row of the input matrix needs to contain at least one non-zero entry"" See: https://stackoverflow.com/questions/13944252/remove-empty-documents-from-documenttermmatrix-in-r-topicmodels
#rowTotals &lt;- apply(datatm , 1, sum) #Find the sum of words in each Document
#dtm.new   &lt;- datatm[rowTotals&gt; 0, ]

library(""ldatuning"")
library(""topicmodels"")

k &lt;- 7

ldaTopics &lt;- LDA(dtm.new, method = ""Gibbs"", control=list(alpha = 0.1, seed = 77), k = k)

#####################################################
#topics by year

tmResult &lt;- posterior(ldaTopics)
tmResult
theta &lt;- tmResult$topics
dim(theta)
library(ggplot2)
terms(ldaTopics, 7)

id &lt;- data.frame(ID = dtm.new$dimnames$Docs)
colnames(id) &lt;- ""ID""
tweets$decade &lt;- paste0(substr(tweets$date2, 0, 3), ""0"")

tweets_new &lt;- merge(id, tweets, by.x=""ID"", by.y = ""ID"", all.x = T)

topic_proportion_per_decade &lt;- aggregate(theta, by = list(decade = tweets_new$decade), mean)
</code></pre>
",3,0,3743,2019-02-27 17:12:22,https://stackoverflow.com/questions/54910980/error-in-aggregate-data-frameas-data-framex-arguments-must-have-same
How do I split text with multiple sentences in a column into multiple rows in Python pandas?,"<p>I am trying to split Comments column into multiple rows containing each sentence. I used the following StackOverflow thread for my reference as it tends to give similar result.
<strong>Reference Link:</strong> <a href=""https://stackoverflow.com/questions/17116814/pandas-how-do-i-split-text-in-a-column-into-multiple-rows"">pandas: How do I split text in a column into multiple rows?</a>
Sample data of dataframe is as below.</p>

<p>Id  Team    Food_Text
1   X   Food is good. It is very well cooked. Delicious!
2   X   I hate the squid. Food is not cooked well. At all indeed. 
3   X   Please do not good anytime over here
4   Y   I love the fish. Awesome delicacy.
5   Y   Good for desserts. Meat tastes bad</p>

<p>Each record for 'Food_Text' can be of multiple sentences delimited by full-stop or period. I have used the following code</p>

<pre><code>import numpy as np
import pandas as pd

survey_data = pd.read_csv(""Food_Dummy.csv"")
survey_text = survey_data[['Id','Team','Food_Text']]

# Getting s as pandas series which has split on full stop and new sentence a new line         
s = survey_text[""Food_Text""].str.split('.').apply(pd.Series,1).stack()
s.index = s.index.droplevel(-1) # to line up with df's index
s.name = 'Food_Text' # needs a name to join

# There are blank or emplty cell values after above process. Removing them
s.replace('', np.nan, inplace=True)
s.dropna(inplace=True)
x=s.to_frame(name='Food_Text1')
x.head(10)

# Joining should ideally get me proper output. But I am getting original dataframe instead of split one.
survey_text.join(x)
survey_text.head(10)
</code></pre>

<p>I am not sure why the join is not giving me a proper dataframe with more number of rows. Repetition of other columns based on index of split. So Id=1 has 3 sentences so we should have 3 records with all other data same and Food_Text column with a new sentence from a comment by ID=1. Similarly for other records. </p>

<p>Thank You in advance for your help!
Regards,
Sohil Shah</p>
","python, pandas, text-mining, sentence-synthesis","<p>In the example that you put in your code, The result of the <code>join</code> was printed, so if you want to change the value of your survey_text, the code should be:</p>

<p><code>survey_text = survey_text.join(x)</code></p>

<p>or if you wanted to simplify your code, this code below is just fine:</p>

<pre><code>import numpy as np
import pandas as pd

survey_data = pd.read_csv(""Food_Dummy.csv"")
survey_text = survey_data[['Id','Team','Food_Text']]

# Getting s as pandas series which has split on full stop and new sentence a new line
s = survey_text[""Food_Text""].str.split('.').apply(pd.Series,1).stack()
s.index = s.index.droplevel(-1) # to line up with df's index
s.name = 'Food_Text' # needs a name to join

# There are blank or emplty cell values after above process. Removing them
s.replace('', np.nan, inplace=True)
s.dropna(inplace=True)

# Joining should ideally get me proper output. But I am getting original dataframe instead of split one.
del survey_text['Food_Text']
survey_text = survey_text.join(s)
survey_text.head(10)
</code></pre>

<p>This way you will not have multiple ""Food_Text"" columns in yout DataFrame.</p>
",4,1,3574,2019-02-27 22:33:50,https://stackoverflow.com/questions/54915584/how-do-i-split-text-with-multiple-sentences-in-a-column-into-multiple-rows-in-py
Remove username from a list of lists,"<p>I have a list of lists regarding tweets and I need to remove the username.</p>

<pre><code>[['@Hegelbon','That','heart','sliding','into','the','waste','basket','.',':('],['“','@ketchBurning',':','I','hate','Japanese','call','him','""','bani','""',
':(',':(','”','Me','too'], ... ]
</code></pre>

<p>The main problem is that I don't know how to work with a list of lists. I tried the following code among other things but did not work:</p>

<pre><code>import re

    for element in tweets:
        for word in element:
            re.sub('@[^\s]+','', tweets)
</code></pre>

<p>Please help.</p>
","python, list, text-mining","<p>You can use a nested list comprehension to filter strings that starts with <code>@</code> (assuming your list of lists is stored as variable <code>l</code>):</p>

<pre><code>[[i for i in s if not i.startswith('@')] for s in l]
</code></pre>

<p>This returns:</p>

<pre><code>[['That', 'heart', 'sliding', 'into', 'the', 'waste', 'basket', '.', ':('], ['“', ':', 'I', 'hate', 'Japanese', 'call', 'him', '""', 'bani', '""', ':(', ':(', '”', 'Me', 'too']]
</code></pre>
",2,1,177,2019-03-05 23:29:20,https://stackoverflow.com/questions/55013239/remove-username-from-a-list-of-lists
Replace values in a list of lists,"<p>I have a list of lists and I need to replace some values. I need to replace smiles with their meaning, remove # (leaving just the word after) and remove links</p>

<pre><code> [['Dang', 'starting', 'next', 'week', 'I', 'work', ':('],
        ['oh', 'god', ',', 'babies', 'faces', ':(','https: //any/website'],
        ['make', 'smile', ':('],['Athabasca','glacier','#1948',':-(','#athabasca',
    '#glacier','#jasper','#jaspernationalpark','https: //any/website2'], ...]
</code></pre>

<p>I tried to do something like:</p>

<pre><code>[[re.sub('#','',mylist) for word in sublist] for sublist in mylist]
</code></pre>

<p>or </p>

<pre><code>[[re.sub('[:;=][\(]','sad',mylist) for word in sublist] for sublist in mylist]
</code></pre>

<p>but it doesn't work. I get an error saying ""expected string or bytes-like object"". 
Please help me!</p>
","python, list, replace, text-mining","<p>Use the following code as your syntax is incorrect.</p>

<pre><code>[re.sub('#','',word) for mylist in sublist for word in mylist]
</code></pre>
",2,1,124,2019-03-06 11:12:24,https://stackoverflow.com/questions/55021711/replace-values-in-a-list-of-lists
Why should I use ggraph() with set.seed() in R?,"<p>I've recently been learning text mining with <code>tidytext</code>.
Today, I encountered the following:</p>

<pre><code>set.seed(2017)
ggraph(bigram_graph, layout=""fr"") + geom_edge_link() + geom_node_point() + 
geom_node_text(aes(label=name), vjust=1, hjust=1)
</code></pre>

<p>I've used the <code>set.seed()</code> function with other functions like <code>sample()</code>. But here, I don't understand why <code>ggraph</code> should be used with <code>set.seed()</code>.
Can anyone help me?</p>
","r, text-mining, ggraph","<p>We use a set.seed function because the results vary when performing a random performance. Figuratively speaking, imagine planting seeds. You can sow seeds anywhere in the land. The first seed planted and the second seed planted produce clearly different results. This is because the shape of the stem and the leaf are different. If you use the set.seed function, you will get the same results.</p>
",1,2,787,2019-03-06 12:39:26,https://stackoverflow.com/questions/55023338/why-should-i-use-ggraph-with-set-seed-in-r
Normalizing text using regex,"<p>I am working with tweets and I would like to have all the variations of aa aaaa aaah ahhh replaced by a single expression 'ah'. However, using my code I also replace the single 'a' and the 'and' which I don't want to change. </p>

<pre><code>a = 'trying a aa aaaaaa aaaah and aaaahhh aaaaaaaahhh '
re.sub('a+h*','ah',a)
</code></pre>

<p>This way i get:</p>

<pre><code>Current output: 'trying ah ah ah ah ahnd ah ah '
</code></pre>

<p>But what I want is:</p>

<pre><code>Desired output: 'trying a ah ah ah and ah ah '
</code></pre>
","python, regex, replace, text-mining, tweets","<p>In your current expression <code>a+</code> matches one <code>a</code> or more. You want the match to start with at least <em>two</em> a's.</p>

<pre><code>s = 'a ah aah aa
re.sub('aa+h*','ah',s) # 'a ah ah ah'
</code></pre>

<p>This can be generalized with the quantifier <code>{x,[y]}</code> which matches <code>x</code> occurrences or more, optionally up to <code>y</code>.</p>

<pre><code>re.sub('a{2,}h*','ah',s)
</code></pre>
",2,0,364,2019-03-06 12:45:07,https://stackoverflow.com/questions/55023456/normalizing-text-using-regex
Text Mining: Query search,"<p>I have a dictionary:</p>

<pre><code>{'Farage': [0, 5, 9, 192,233,341],
 'EU': [0, 1, 5, 6, 9, 23]}

Query1: “Farage” and “EU”
Query2: “Farage” or “EU”
</code></pre>

<p>I need to return the documents that contain these queries. For query1, for example, the answer should be [0,5,9].
I believe the answer should be something like that but in python:</p>

<pre><code>final_list = []
while x≠Null and y≠Null
    do if docID(x)=docID(y)
       then ADD(final_list, docID(x))
          x← next(x)
          y ←next(y)
        else if docID(x) &lt; docID(y)
          then x← next(x)
          else y ←next(y)
return final_list
</code></pre>

<p>Please help. </p>
","python, text-mining","<p>You can create a <code>dict</code> of operators and throw <code>set</code> operations to get the final results. It assumes that queries follow strict rule of <code>key1 operator key2 operator key3</code></p>

<p>For arbitrary number of arguments</p>

<pre><code>import operator
d1={'Farage': [0, 5, 9, 192,233,341],
    'EU': [0, 1, 5, 6, 9, 23],
    'hopeless': [0, 341, 19999]}

d={'and':operator.and_,
  'or':operator.or_}

Queries= ['Farage and EU','Farage and EU or hopeless','Farage or EU']

for query in Queries:
    res=set()
    temp_arr = query.split()
    k1 = temp_arr[0]

    for value in range(1,len(temp_arr),2):
        op = temp_arr[value]
        k2 = temp_arr[value+1]
        if res:
            res = d[op](res, set(d1.get(k2, [])))
        else:
            res = d[op](set(d1.get(k1, [])), set(d1.get(k2, [])))
    print(res)
</code></pre>

<p>Output</p>

<pre><code>set([0, 9, 5])
set([0, 192, 5, 233, 9, 19999, 341])
set([0, 192, 5, 6, 1, 233, 23, 341, 9])
</code></pre>
",1,1,86,2019-03-06 20:20:59,https://stackoverflow.com/questions/55031560/text-mining-query-search
Calculate similarity between list of words,"<p>I want to calculate the similarity between two list of words, for example :</p>

<p><code>['email','user','this','email','address','customer']</code></p>

<p>is similar to this list:</p>

<p><code>['email','mail','address','netmail']</code></p>

<p>I want to have a higher percentage of similarity than another list, for example:
<code>['address','ip','network']</code> even if <strong><code>address</code></strong> exists in the list.</p>
","python, data-mining, text-mining, similarity","<p>Since you haven't really been able to demonstrate a crystal output, here is my best shot:</p>

<pre><code>list_A = ['email','user','this','email','address','customer']
list_B = ['email','mail','address','netmail']
</code></pre>

<p>In the above two list, we will find the cosine similarity between each element of the list with the rest. i.e. <code>email</code> from <code>list_B</code> with every element in <code>list_A</code>:</p>

<pre><code>def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the ""length"" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]


list_A = ['email','user','this','email','address','customer']
list_B = ['email','mail','address','netmail']

threshold = 0.80     # if needed
for key in list_A:
    for word in list_B:
        try:
            # print(key)
            # print(word)
            res = cosdis(word2vec(word), word2vec(key))
            # print(res)
            print(""The cosine similarity between : {} and : {} is: {}"".format(word, key, res*100))
            # if res &gt; threshold:
            #     print(""Found a word with cosine distance &gt; 80 : {} with original word: {}"".format(word, key))
        except IndexError:
            pass
</code></pre>

<p><strong>OUTPUT</strong>:</p>

<pre><code>The cosine similarity between : email and : email is: 100.0
The cosine similarity between : mail and : email is: 89.44271909999159
The cosine similarity between : address and : email is: 26.967994498529684
The cosine similarity between : netmail and : email is: 84.51542547285166
The cosine similarity between : email and : user is: 22.360679774997898
The cosine similarity between : mail and : user is: 0.0
The cosine similarity between : address and : user is: 60.30226891555272
The cosine similarity between : netmail and : user is: 18.89822365046136
The cosine similarity between : email and : this is: 22.360679774997898
The cosine similarity between : mail and : this is: 25.0
The cosine similarity between : address and : this is: 30.15113445777636
The cosine similarity between : netmail and : this is: 37.79644730092272
The cosine similarity between : email and : email is: 100.0
The cosine similarity between : mail and : email is: 89.44271909999159
The cosine similarity between : address and : email is: 26.967994498529684
The cosine similarity between : netmail and : email is: 84.51542547285166
The cosine similarity between : email and : address is: 26.967994498529684
The cosine similarity between : mail and : address is: 15.07556722888818
The cosine similarity between : address and : address is: 100.0
The cosine similarity between : netmail and : address is: 22.79211529192759
The cosine similarity between : email and : customer is: 31.62277660168379
The cosine similarity between : mail and : customer is: 17.677669529663685
The cosine similarity between : address and : customer is: 42.640143271122085
The cosine similarity between : netmail and : customer is: 40.08918628686365
</code></pre>

<blockquote>
  <p>Note: I have also commented the <code>threshold</code> part in the code, in case
  you only want the words if their similarity exceeds a certain
  threshold i.e. 80%</p>
</blockquote>

<p><strong>EDIT</strong>:</p>

<p><strong>OP</strong>:  <em>but what i want exactly to do in not the comparaison word by word but, list by list</em></p>

<p>Using <code>Counter</code> and <code>math</code>:</p>

<pre><code>from collections import Counter
import math

counterA = Counter(list_A)
counterB = Counter(list_B)


def counter_cosine_similarity(c1, c2):
    terms = set(c1).union(c2)
    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)
    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))
    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))
    return dotprod / (magA * magB)

print(counter_cosine_similarity(counterA, counterB) * 100)
</code></pre>

<p><strong>OUTPUT</strong>:</p>

<pre><code>53.03300858899106
</code></pre>
",13,5,15190,2019-03-14 12:34:01,https://stackoverflow.com/questions/55162668/calculate-similarity-between-list-of-words
Create custom dictionary from character vector,"<p>I am trying to look for specific words in corpus using dfm_lookup().</p>

<p>I am really struggling with the dictionaries needed for the dfm_loopup().</p>

<p>I created a character vector named ""words"" which contains all the words that should go into the dictionary.</p>

<p>dictionary needs a list, so I am creating a list from the character vector before I am using dictionary().</p>

<pre><code>dict &lt;- dictionary(list(words))
</code></pre>

<p>But then I get</p>

<pre><code>Error in validate_dictionary(object) : 


 Dictionary elements must be named: digital digital-tv digitalis ...
</code></pre>

<p>What do I have to change in the list command to get the proper output for dictionary()?</p>

<p>Is there a simplier version to look for specific words in a dfm? Because it was really easy with the tm() package.</p>
","r, text-mining, quanteda","<p>I believe you need to name the items in a list in order to use dictionary with <code>quanteda</code>. Here is an example:</p>

<pre><code>library(quanteda)

words = c(""cat"",""dog"",""bird"")

word.list = as.list(words)
names(word.list) = words

dictionary(word.list)
Dictionary object with 3 key entries.
- [cat]:
  - cat
- [dog]:
  - dog
- [bird]:
  - bird
</code></pre>
",2,2,1393,2019-03-18 09:24:25,https://stackoverflow.com/questions/55218093/create-custom-dictionary-from-character-vector
Remove specific words with specific punctuation in R,"<p>I'm working on a corpus in R that contains interrogatories in Russian.</p>

<p>In the beginning of each question there is the names of the person speaking written.</p>

<p>By example:</p>

<blockquote>
  <p>President. - Are you Nikolaj Khvostov?</p>
</blockquote>

<p>For an analysis of these interrogatories I would like to remove these names when they come at the beginning of the line (ie, when they are used to identify the speaker), but not when they come in the middle of the text (ie, when the speaker actually says that name). So that there is a specific punctuation which follows.</p>

<p>I tried the following code:</p>

<pre><code>corpus3 &lt;- tm_map(corpus2, removeWords, c(""Председатель. —"", ""Хвостов. -""))
</code></pre>

<p>or </p>

<pre><code>corpus3 &lt;- tm_map(corpus2, removeWords, c(""President. —"", ""Khvostov. -""))
</code></pre>

<p>But nothing is removed and if I try:</p>

<pre><code>corpus3 &lt;- tm_map(corpus2, removeWords, c(""President"", ""Khvostov""))
</code></pre>

<p>All the terms President and Khvostov are removed. But I need to use these names when they're used by during the intervention.</p>

<p>What I mean is that by example:</p>

<p><strong>Input strings:</strong></p>

<blockquote>
  <p>President. - Are you Nikolaj Khvostov?</p>
  
  <p>Khvostov. - Yes Mr. President.</p>
</blockquote>

<p><strong>Desired output:</strong></p>

<blockquote>
  <p>Are You Nikolaj Khvostov?</p>
  
  <p>Yes Mr. President.</p>
</blockquote>

<p><strong>Undesired output generated by the third code block:</strong></p>

<blockquote>
  <p>Are you Nikolaj?</p>
  
  <p>Yes Mr..</p>
</blockquote>

<p>There an example of my data:</p>

<p>It would be easy to do it in word, but I actually have more than 5 000 pages of interrogatory I would like to study. Word is not able to open the document without crashing my computer. That's why I hoped a code would match to help me.</p>

<p>My data is added in R as a Large VCorpus</p>

<blockquote>
  <p>Председатель. — Алексей Николаевич, вы уже были допрошены один раз 15
  марта — не правда ли?</p>
  
  <p>Хвостов. — Да.</p>
  
  <p>Председатель. — Вам известно, что вы в заседании Чрезвычайной
  Следственной Комиссии?</p>
  
  <p>Хвостов. — Да.</p>
  
  <p>Председатель. — Я просил бы вас, не стесняясь рамками допроса, который
  с вас снят, рассказать нам все, что вам известно и как члену
  Государственной Думы и по должности министра внутренних дел, — о
  действиях бывших министров и прочих высших должностных лиц,
  расследованием действий которых мы заняты. Нас интересует, конечно, и
  та тема, которую вы задели при показании, данном вами г. Коровиченко,
  т.-е. тема о тех кружках, которые стояли рядом с правительством или,
  быть может, позади его и оказывали известное влияние. Эта тема
  подлежит углублению… В частности, мы просим вас остановиться на
  следующем: в своей деятельности министра внутренних дел испытывали ли
  вы, и в какой мере, и при каких обстоятельствах, — давление этих
  кружков, о которых вы уже давали показание? Вот канва… Благоволите
  начать.</p>
  
  <p>Хвостов. — Я должен доложить, что вполне подтверждаю все, уже данное
  мною в показании. В сущности, это показание я считаю своей
  обязанностью дать, при чем оно не касается дел должностных и вверенных
  мне по должности министра внутренних дел, а лишь того, что случалось
  узнавать по этому поводу и что не входило непосредственно в задачу
  министра внутренних дел…</p>
  
  <p>Председатель. — Я не понял вашу мысль… У нас не может быть здесь
  никаких тайн: вы не только в праве, но вы обязаны показать нам
  абсолютно все то, что вы знаете.</p>
  
  <p>Хвостов. — Кроме тех обязанностей, которые на меня возлагались по
  должности, я руководствовался долгом русского человека, потому что
  вопрос касался больного для меня места — я разумею вопрос о шпионаже…</p>
  
  <p>Председатель. — Не расскажете ли вы в такой последовательности: при
  каких обстоятельствах вы были назначены министром внутренних дел? Это
  — сперва… [3]</p>
  
  <p>Хвостов. — Может быть, вы мне разрешите взять более глубоко? Вы
  изволили сказать, что я должен показать то, что мне известно было
  раньше в качестве члена Государственной Думы… Уже издавна, в качестве
  члена Государственной Думы, я обратил внимание на немецкое влияние,
  которое, мне казалось, имелось в правительстве. Я занялся…</p>
  
  <p>Председатель. — Вы когда обратили на это внимание?</p>
  
  <p>Хвостов. — Еще членом Государственной Думы, почти в самом начале
  прибытия моего в Петроград — в 1912 году… До этого для меня, как
  служившего в провинции, те или другие влияния на петроградские сферы
  должны были оставаться в стороне… Единственный раз, когда мне пришлось
  встретиться с Распутиным, — это было в Нижнем Новгороде, когда я был
  губернатором. Ко мне приехал Распутин, мне в то время мало известный,
  о котором я слышал в виде толков, доходивших до провинции. Он
  предложил мне место министра внутренних дел. Было это, насколько я
  помню, — за неделю или дней за десять до убийства Столыпина. Я был
  удивлен его появлением, не придавал ему такого значения, какое
  впоследствии обнаружилось… Я крайне удивился возможности ухода
  Столыпина, так как в провинции нам казалось, что Столыпин — сила
  непререкаемая, нам казалось невозможным, чтобы он колебался, шатался
  или уходил… Распутин объявил мне, что он должен поговорить со мной,
  так как он послан, как он сказал, — «посмотреть мою душу»…</p>
  
  <p>Председатель. — Кем послан?</p>
  
  <p>Хвостов. — Неопределенно: из Царского послан — посмотреть мою душу…
  Это казалось мне, в то время непосвященному, несколько смешным, и я с
  ним поговорил шутовским образом, а потом, через несколько времени, я
  послал полицмейстера свезти его на вокзал. Распутин уехал…</p>
  
  <p>Председатель. — Вы говорите, вам показалось странным, что это
  происходило еще при жизни министра внутренних дел, — вы выразили
  сомнение по этому поводу?</p>
  
  <p>Хвостов. — Я выразил сомнение. Он сказал, что Столыпин должен уйти.
  Но, по правде сказать, я с ним серьезно не говорил… Я считал, что он
  одно из духовных развлечений в Царском Селе, но не считал серьезным,
  чтобы он мог иметь значение при назначении министров… Я знал, что в
  это время ко мне относился в высшей степени благосклонно бывший
  император Николай II… Его хорошие отношения ко мне завязались впервые,
  когда я был губернатором в Вологде и докладывал о возможности
  соединения вологодских рек с Сибирью чрез Урал. На это обстоятельство
  я, главным образом, напираю. Это его очень интересовало, я делал ему
  часто доклады, которые были более радужны, в смысле экономических
  перспектив, чем все остальное, что делалось в России в это время. Вот
  этим я обратил на себя его внимание… [4]В предпоследний раз, перед
  указанным приездом Распутина, я был принят государем сидя, что
  считалось высшим знаком благоволения. Разговор велся об общих
  предметах.</p>
  
  <p>Председатель. — А когда в последний раз перед этим визитом Распутина
  вы были в Царском?</p>
  
  <p>Хвостов. — Я с точностью не помню, но так месяца за полтора… Мне было
  известно от близких лиц, от иностранных посольств, что обо мне
  постоянно ведется разговор на охоте… Но этот самый приезд Распутина в
  высшей степени поразил меня, и я к нему отнесся не серьезно. После
  этого мне пришлось быть уже в Царском…</p>
  
  <p>Председатель. — Алексей Николаевич, вы говорите, что ваше отношение к
  этому приезду было отрицательным; но это не избавляет нас от
  необходимости несколько подробнее остановиться на нем: только ли для
  этого приезжал Распутин?</p>
  
  <p>Хвостов. — Исключительно для этого.</p>
  
  <p>Председатель. — Какой еще разговор был между вами?</p>
  
  <p>Хвостов. — Разговор исключительно этот. Он сказал: «Приехал посмотреть
  твою душу»…</p>
  
  <p>Председатель. — Т.-е. только эти несколько слов?</p>
  
  <p>Хвостов. — Он изъявил еще желание посмотреть мою семью… Моей семьи еще
  не было… Я считал излишним вводить его в мою семью. Я не серьезно к
  этому отнесся… Тут была ярмарка: она кончалась. Мне было не до того,
  чтобы беседовать с ним. Я отнесся к этому в высшей степени легко…
  Когда, через месяц после этого, я приехал сюда, то я увидел ко мне
  совершенно обратное отношение. Я был принят в высшей степени
  неприязненно, в высшей степени сухо, что, после предшествующих
  приемов, мне показалось неособенно приятным. Это послужило основанием
  тому, что тогда казалось странным: я ушел из губернии, пошел в
  Государственную Думу… Уйти я сразу не мог, прошло порядочное время,
  около года: но во всяком случае, в этот промежуток времени, я делал
  все возможное, чтобы попасть в Государственную Думу по Орловской
  губернии, и уже все мои мысли были за то, чтобы уйти, — видя, что
  здесь мне отрезаны все дальнейшие пути…</p>
  
  <p>Председатель. — Что же, уезжая, Распутин какие-нибудь угрозы делал по
  вашему адресу?</p>
  
  <p>Хвостов. — Болтал по обыкновению: говорил, что он уже обо мне послал
  телеграмму.</p>
  
  <p>Председатель. — Какого содержания?</p>
  
  <p>Хвостов. — Содержания совершенно не помню в подробностях… Мне потом
  достали текст (всегда на почте есть свои люди, которые сообщают потом
  подробное содержание). Но я текста не помню.</p>
  
  <p>Председатель. — Приблизительно, какого содержания? [5]</p>
  
  <p>Хвостов. — Отрицательное ко мне отношение… Что-то такое: «Хотя бог на
  нем почиет, но чего-то не достает»…</p>
  
  <p>Председатель. — Скажите еще: он вам говорил, от чьего имени из
  Царского Села он являлся — от отрекшегося государя или от государыни?</p>
</blockquote>

<p>There the begining of my code:</p>

<pre><code>## Package ##
library(tm)
library(NLP)
library(slam)
library(FactoMineR)
library(explor)
library(R.temis)
library(zoo)
library(lattice)
library(RcmdrPlugin.temis)
library(tidyverse)

## Importation du corpus

corpus &lt;- import_corpus(""./data/CorpusPadenie"", format = ""txt"", language = ""ru"")

## Importation metadonnees

don &lt;- read.csv2(""./data/2019_PDI_M1_MEM_DATA_DOP.csv"")

## Association des metadonnees et du corpus

corpus &lt;- set_corpus_variables(corpus, don)
corpus

## Creation d'une copie de secours

corpus2 &lt;- corpus

### Suppression des mots inutiles ###

## Suppressions des noms des intervenants - pour pouvoir identifier les noms utiliser dans les interogatoires - suprressions des seuls intervenants possible du fait de la mise en page.


corpus3 &lt;- tm_map(corpus2, removeWords, c(""Председатель. —"", ""Хвостов""))
removeWords(corpus2[[1]], character(c(""Председатель. —"", ""Хвостов. —"")))
corpus2[[1]]
removeWords(corpus2[[1]], ""Председатель"")


writeCorpus(corpus2, path = ""./data/Test"", filenames = NULL)
</code></pre>

<p>Last part of the result for dput(head(corpus2))</p>

<pre><code> ""1917_08_21_TCHESSKA_7_79_POK_F_A_GOLOVIN.txt"", ""1917_08_25_TCHESSKA_7_80_DOP_A_A_POLIVANOV.txt"", 
""1917_08_25_TCHESSKA_7_81_DOP_V_N_KOKOVTSOV.txt"", ""1917_09_04_TCHESSKA_7_83_DOP_M_V_RODZIANKO.txt"", 
""1917_09_20_TCHESSKA_7_84_DOP_A_A_POLAVINOV.txt"", ""1917_09_27_TCHESSKA_7_85_DOP_N_B_CHTCHERBATOV.txt"", 
""1917_09_27_TCHESSKA_7_86_POK_A_P_LEDNITSKIJ.txt"", ""1917_10_11_TCHESSKA_7_87_DOP_A_B_LIADOV.txt"", 
""1917_10_11_TCHESSKA_7_88_DOP_D_S_CHOUVAEV.txt""), class = ""factor""), 
    int_id = structure(1:6, .Label = c(""1"", ""2"", ""3"", ""4"", 
    ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", 
    ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", 
    ""24"", ""25"", ""26"", ""27"", ""28"", ""29"", ""30"", ""31"", ""32"", 
    ""33"", ""34"", ""35"", ""36"", ""37"", ""38"", ""39"", ""40"", ""41"", 
    ""42"", ""43"", ""44"", ""45"", ""46"", ""47"", ""48"", ""49"", ""50"", 
    ""51"", ""52"", ""53"", ""54"", ""55"", ""56"", ""57"", ""58"", ""59"", 
    ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", ""68"", 
    ""69"", ""70"", ""71"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", 
    ""78"", ""79"", ""80"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86""), class = ""factor""), 
    ind_id = structure(1:6, .Label = c(""1"", ""2"", ""3"", ""4"", 
    ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", 
    ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", 
    ""24"", ""25"", ""26"", ""27"", ""28"", ""29"", ""30"", ""31"", ""32"", 
    ""33"", ""34"", ""35"", ""36"", ""37"", ""38"", ""39"", ""40"", ""41"", 
    ""42"", ""43"", ""44"", ""45"", ""46"", ""47"", ""48"", ""49"", ""50"", 
    ""51"", ""52"", ""53"", ""54"", ""55"", ""56"", ""57"", ""58"", ""59"", 
    ""60""), class = ""factor""), ind_nom = structure(c(23L, 
    24L, 43L, 22L, 52L, 9L), .Label = c(""Andronikov"", ""Beletskij"", 
    ""Belyaev"", ""Bourtsev"", ""Chingerev"", ""Chouvaev"", ""Chtcheglovitov"", 
    ""Chtcherbatov"", ""Chtiourmer"", ""Djounkovskij"", ""Dobrovilskij"", 
    ""Doubenskij"", ""Frederiks"", ""Golitsyn"", ""Golovin"", ""Goremykin"", 
    ""Goutchkov"", ""Guerassimov"", ""Ignatev"", ""Ivanov"", ""Kafafov"", 
    ""Khabalov"", ""Khvostov"", ""Klimovitch"", ""Kokovtsov"", ""Komissarov"", 
    ""Kourlov"", ""Kryjanovskij"", ""Lednitskij"", ""Liadov"", ""Lodyjenskij"", 
    ""Lokhtina"", ""Makarov"", ""Maklakov"", ""Manassievitch-Manoujlov"", 
    ""Markov"", ""Milioukov"", ""Naoumov"", ""Neratov"", ""Pleve"", 
    ""Pokrovskij"", ""Polivanov"", ""Protopopov"", ""Rejn"", ""Rejnbot"", 
    ""Rodzianko"", ""Spiridovitch"", ""Tchaplin"", ""Tchelnokov"", 
    ""Tchkhejdze"", ""Trousevitch"", ""Vassiliev"", ""Velepolskij"", 
    ""Verevkin"", ""Vissarionov"", ""Voejkov"", ""Volkonskij"", ""Vyroubova"", 
    ""Zolotarev""), class = ""factor""), ind_initiales = structure(c(5L, 
    12L, 3L, 39L, 7L, 9L), .Label = c(""AA"", ""AB"", ""AD"", ""AI"", 
    ""AN"", ""AP"", ""AT"", ""AV"", ""BV"", ""DN"", ""DS"", ""EK"", ""FA"", 
    ""GE"", ""IF"", ""IG"", ""IL"", ""IM"", ""IN"", ""KD"", ""MA"", ""MI"", 
    ""MM"", ""MS"", ""MV"", ""NB"", ""ND"", ""NE"", ""NI"", ""NN"", ""NS"", 
    ""NV"", ""OA"", ""PG"", ""PN"", ""SE"", ""SI"", ""SP"", ""SS"", ""VB"", 
    ""VF"", ""VL"", ""VM"", ""VN""), class = ""factor""), int_mois = structure(c(1L, 
    1L, 1L, 1L, 1L, 1L), .Label = c(""3"", ""4"", ""5"", ""6"", ""7"", 
    ""8"", ""9"", ""10""), class = ""factor""), int_jour = structure(c(15L, 
    16L, 18L, 19L, 19L, 19L), .Label = c(""1"", ""2"", ""4"", ""6"", 
    ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"", ""15"", ""17"", 
    ""18"", ""19"", ""20"", ""21"", ""22"", ""24"", ""25"", ""26"", ""27"", 
    ""28"", ""29"", ""30"", ""31""), class = ""factor""), int_tome = structure(c(1L, 
    1L, 1L, 1L, 1L, 1L), .Label = c(""1"", ""2"", ""3"", ""5"", ""6"", 
    ""7""), class = ""factor""), int_num = structure(1:6, .Label = c(""1"", 
    ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", 
    ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", 
    ""22"", ""23"", ""24"", ""25"", ""26"", ""27"", ""28"", ""29"", ""30"", 
    ""31"", ""32"", ""33"", ""34"", ""35"", ""36"", ""37"", ""38"", ""39"", 
    ""40"", ""41"", ""42"", ""44"", ""45"", ""46"", ""47"", ""48"", ""49"", 
    ""50"", ""51"", ""52"", ""53"", ""54"", ""55"", ""56"", ""57"", ""58"", 
    ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", 
    ""68"", ""69"", ""70"", ""71"", ""72"", ""73"", ""74"", ""75"", ""76"", 
    ""77"", ""78"", ""79"", ""80"", ""81"", ""83"", ""84"", ""85"", ""86"", 
    ""87"", ""88""), class = ""factor""), dop_pok = structure(c(1L, 
    1L, 1L, 1L, 1L, 1L), .Label = c(""DOP"", ""POK""), class = ""factor"")), row.names = c(NA, 
6L), class = ""data.frame"")), class = c(""VCorpus"", ""Corpus""
</code></pre>

<p>))</p>
","r, string, text-mining, tm","<p>There are several ways to do this. Using <code>sub</code> will only replace the first match with whatever you specify. <code>gsub</code> will replace every instance. You could also check out the package <code>stringi</code> which has several functions for this kind of task. The below example matches everything before the first dash (with a space after it) and replaces it with """". If you wanted to be more conservative you could remove the space after and use <code>trimws</code> if there is an inconsistent spacing pattern after the first dash. </p>

<pre><code>sub(("".* \\- ""), """", c(""President. - Are you Nikolaj Khvostov?"", ""Khvostov. - Yes Mr. President.""))
[1] ""Are you Nikolaj Khvostov?"" ""Yes Mr. President.""   
</code></pre>

<p>EDIT:</p>

<p>Without understanding your data better we are just guessing, which is why having an example of the data <em>in</em> R is very helpful. However, in theory, these solutions may work depending on your data structure. </p>

<p>If it is a list this should work:</p>

<pre><code>text_as_list &lt;- list(""President. — Are you Nikolaj Khvostov?"", ""Khvostov. — Yes Mr. President."")

lapply(text_as_list, sub, pattern = ""^.* \\— "", replacement = """")
[[1]]
[1] ""Are you Nikolaj Khvostov?""

[[2]]
[1] ""Yes Mr. President.""
</code></pre>

<p>And if it is one long vector, this should work (this uses <code>gsub</code> now):</p>

<pre><code>long_vector &lt;- c(""President. — Are you Nikolaj Khvostov?\nKhvostov. — Yes Mr. President."")

cat(long_vector)
President. — Are you Nikolaj Khvostov?
Khvostov. — Yes Mr. President.

long_vector_fixed &lt;- gsub((""Khvostov. \\— ""), """", gsub((""President. \\— ""), """", long_vector))

cat(long_vector_fixed)

Are you Nikolaj Khvostov?
Yes Mr. President.
</code></pre>

<p>Possible solution using the <code>tm_map</code> function you were using:</p>

<pre><code>tm_map(corpus2, sub, pattern = ""^.* \\— "", replacement = """")
</code></pre>
",1,2,269,2019-03-19 13:06:14,https://stackoverflow.com/questions/55241806/remove-specific-words-with-specific-punctuation-in-r
Term frequencies from VCorpus and DTM do not match,"<p>I calculated term frequency of test documents both from Corpus and DTM as below. But they didn't match with each other.
Can anyone tell me where the mismatch came from? Is it because I used wrong methods to extract term frequency?</p>

<pre><code>library(""tm"")
library(""stringr"")
library(""dplyr"")
test1 &lt;- VCorpus(DirSource(""test_papers""))
mytable1 &lt;- lapply(test1, function(x){str_extract_all(x, boundary(""word""))}) %&gt;% unlist() %&gt;% table() %&gt;% sort(decreasing=T)
test2 &lt;- DocumentTermMatrix(test1)
mytable2 &lt;- apply(test2, 2, sum) %&gt;% sort(decreasing=T)
head(mytable1)
.
and  of the  to  in  on 
148 116 111  69  61  54 
head(mytable2)
      and       the      this      that       are political 
      145       120        35        34        33        33 
</code></pre>
","r, text-mining, tm, corpus","<p>Difference in methods used.</p>

<p><code>str_extract_all</code> with <code>boundary(""word"")</code> removes the punctuations in the sentences. Turning the text into a document term matrix doesn't. To get the same numbers you need to use <code>DocumentTermMatrix(test1, control = list(removePunctuation = TRUE))</code>.</p>

<p><strong>Detailed explanation:</strong></p>

<p>In the first case: ""this is a text."" would return the four words without the period. In the second case you would get text with a period (""text."") in the document term matrix. Now if text appears like this: ""text and text."" the first case would count ""text"" = 2, and the document term matrix would count it as ""text"" = 1 and ""text."" = 1.</p>

<p>Using removePunction will remove the period and the counts will be equal. </p>

<p>You might also want to remove numbers first as well, because removePunctuation removes points and comma's from the numbers. </p>
",0,0,85,2019-03-20 11:16:41,https://stackoverflow.com/questions/55259497/term-frequencies-from-vcorpus-and-dtm-do-not-match
(R) About stopwords in DocumentTermMatrix,"<p>I have some questions about <code>DocumentTermMatrix()</code> and about its stopwords.
I typed as below, but couldn't get the results that I wanted.</p>

<pre><code>text &lt;- ""text is my text but also his text.""
mycorpus &lt;- VCorpus(VectorSource(text))
mydtm &lt;- DocumentTermMatrix(mycorpus, control=list(stopwords=F))
lapply(mycorpus, function(x){str_extract_all(x, boundary(""word""))}) %&gt;% unlist() %&gt;% table()
.
also  but  his   is   my text 
   1    1    1    1    1    3 
apply(mydtm, 2, sum)
 also   but   his  text text. 
    1     1     1     2     1 
</code></pre>

<p>First is that even though I used <code>stopwords=F</code>, the dtm still removed some stopwords such as ""is."" However, it didn't remove ""his"" although it is listed in both <code>stopwords(""en"")</code> and <code>stopwords(""SMART"")</code>.
So I really don't understand what stopwords that DTM uses and why <code>stopwords=F</code> doesn't work. and What should I do to make it work?</p>
","text-mining, tm, stop-words","<p>You could try an alternative package: <strong>quanteda</strong>.  It allows you to remove stopwords after tokenizing, or after creating the document-feature matrix.  Below, I used <code>pad = TRUE</code> simply to show the slots where the tokens matching stopwords have been removed.</p>



<pre class=""lang-r prettyprint-override""><code>library(""quanteda"")
## Package version: 1.4.1
## Parallel computing: 2 of 12 threads used.
## See https://quanteda.io for tutorials and examples.
## 
## Attaching package: 'quanteda'
## The following object is masked from 'package:utils':
## 
##     View

text &lt;- ""text is my text but also his text.""

tokens(text) %&gt;%
  tokens_remove(stopwords(""en""), pad = TRUE)
## tokens from 1 document.
## text1 :
## [1] ""text"" """"     """"     ""text"" """"     ""also"" """"     ""text"" "".""
</code></pre>

<p>Alternatively:</p>

<pre class=""lang-r prettyprint-override""><code>dfm(text)
## Document-feature matrix of: 1 document, 7 features (0.0% sparse).
## 1 x 7 sparse Matrix of class ""dfm""
##        features
## docs    text is my but also his .
##   text1    3  1  1   1    1   1 1

dfm(text, remove_punct = TRUE) %&gt;%
  dfm_remove(stopwords(""en""))
## Document-feature matrix of: 1 document, 2 features (0.0% sparse).
## 1 x 2 sparse Matrix of class ""dfm""
##        features
## docs    text also
##   text1    3    1
</code></pre>

<p>The list of English stopwords is just a character vector returned by the <code>stopwords()</code> function (which actually comes from the <strong>stopwords</strong> package).  The default English list is the same as <code>tm::stopwords(""en"")</code> except the <strong>tm</strong> package includes ""will"".  (If you want the SMART list, it's <code>stopwords(""en"", source = ""smart"")</code>.)</p>

<pre class=""lang-r prettyprint-override""><code>stopwords(""en"")
##   [1] ""i""          ""me""         ""my""         ""myself""     ""we""        
##   [6] ""our""        ""ours""       ""ourselves""  ""you""        ""your""      
##  [11] ""yours""      ""yourself""   ""yourselves"" ""he""         ""him""       
##  [16] ""his""        ""himself""    ""she""        ""her""        ""hers""      
##  [21] ""herself""    ""it""         ""its""        ""itself""     ""they""      
##  [26] ""them""       ""their""      ""theirs""     ""themselves"" ""what""      
##  [31] ""which""      ""who""        ""whom""       ""this""       ""that""      
##  [36] ""these""      ""those""      ""am""         ""is""         ""are""       
##  [41] ""was""        ""were""       ""be""         ""been""       ""being""     
##  [46] ""have""       ""has""        ""had""        ""having""     ""do""        
##  [51] ""does""       ""did""        ""doing""      ""would""      ""should""    
##  [56] ""could""      ""ought""      ""i'm""        ""you're""     ""he's""      
##  [61] ""she's""      ""it's""       ""we're""      ""they're""    ""i've""      
##  [66] ""you've""     ""we've""      ""they've""    ""i'd""        ""you'd""     
##  [71] ""he'd""       ""she'd""      ""we'd""       ""they'd""     ""i'll""      
##  [76] ""you'll""     ""he'll""      ""she'll""     ""we'll""      ""they'll""   
##  [81] ""isn't""      ""aren't""     ""wasn't""     ""weren't""    ""hasn't""    
##  [86] ""haven't""    ""hadn't""     ""doesn't""    ""don't""      ""didn't""    
##  [91] ""won't""      ""wouldn't""   ""shan't""     ""shouldn't""  ""can't""     
##  [96] ""cannot""     ""couldn't""   ""mustn't""    ""let's""      ""that's""    
## [101] ""who's""      ""what's""     ""here's""     ""there's""    ""when's""    
## [106] ""where's""    ""why's""      ""how's""      ""a""          ""an""        
## [111] ""the""        ""and""        ""but""        ""if""         ""or""        
## [116] ""because""    ""as""         ""until""      ""while""      ""of""        
## [121] ""at""         ""by""         ""for""        ""with""       ""about""     
## [126] ""against""    ""between""    ""into""       ""through""    ""during""    
## [131] ""before""     ""after""      ""above""      ""below""      ""to""        
## [136] ""from""       ""up""         ""down""       ""in""         ""out""       
## [141] ""on""         ""off""        ""over""       ""under""      ""again""     
## [146] ""further""    ""then""       ""once""       ""here""       ""there""     
## [151] ""when""       ""where""      ""why""        ""how""        ""all""       
## [156] ""any""        ""both""       ""each""       ""few""        ""more""      
## [161] ""most""       ""other""      ""some""       ""such""       ""no""        
## [166] ""nor""        ""not""        ""only""       ""own""        ""same""      
## [171] ""so""         ""than""       ""too""        ""very""       ""will""
</code></pre>
",0,0,1156,2019-03-21 08:44:49,https://stackoverflow.com/questions/55276570/r-about-stopwords-in-documenttermmatrix
Moving words in a cell to individual columns,"<p>I have a csv file that has a column with multiple words in each cell. I wonder if there's any R function to move words in each cell to individual cells. 
The following are data in two cells in the dataset:</p>

<p>arecapalm,betelnut,konkan,nature,traveldiaries,mirrorlessframes
passangerstories,chakarmanee,atranginikhil,maharashtra,india</p>

<p>Thanks. Any help appreciated.</p>

<p>Chamil</p>
","r, text, text-mining","<p>Let's assume this data.frame:</p>

<pre><code>require(dplyr)
require(tidyr)
df&lt;-data.frame(id=1:2, words=c(""arecapalm,betelnut,konkan,nature,traveldiaries,mirrorlessframes"",""passangerstories,chakarmanee,atranginikhil,maharashtra,india""))
df

#  id                                                           words
#1  1 arecapalm,betelnut,konkan,nature,traveldiaries,mirrorlessframes
#2  2    passangerstories,chakarmanee,atranginikhil,maharashtra,india
</code></pre>

<p>Then we can run this using <code>dplyr</code> and <code>tidyr</code> to break down the words cells into multiple columns:</p>

<pre><code>df %&gt;% separate_rows(words) %&gt;% 
   group_by(id) %&gt;% 
   mutate(wordid=row_number()) %&gt;% 
   spread(wordid,words,sep=""."")

# A tibble: 2 x 7
# Groups:   id [2]
     id wordid.1         wordid.2    wordid.3      wordid.4    wordid.5      wordid.6        
  &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;           
1     1 arecapalm        betelnut    konkan        nature      traveldiaries mirrorlessframes
2     2 passangerstories chakarmanee atranginikhil maharashtra india         NA              
</code></pre>
",0,0,42,2019-03-22 14:43:42,https://stackoverflow.com/questions/55302128/moving-words-in-a-cell-to-individual-columns
Word search from string in python and providing output into CSV column,"<p><strong>A program that row-wise checks strings if it contains from the list of words and writes 1/0 in the next column named ""Result""</strong></p>

<p>I am looking for filtering text messages containing words like 'PNR' and Airport code (like 'LHR','JFK' etc.)</p>

<pre><code>word=['JFK','LHR','DXB']

String=['London airport is LGW','Dubai airport is DXB','My flight is from JFK']

if set(word).intersection(string.split()):
    print(""Found One"")
</code></pre>

<p>Check this link for summary <a href=""https://i.sstatic.net/aGid6.jpg"" rel=""nofollow noreferrer"">https://i.sstatic.net/aGid6.jpg</a>
I have 1 million row in a CSV file having text messages. </p>

<p>How can I produce a simple boolean output in the next column with 0 or 1 depending upon message contains any of the words from Set of words.</p>

<p>I am not an advanced programmer, I am working with python and has basic knowledge of programming.
I have done simple extraction from strings. </p>
","python-3.x, text-mining","<p>There are several ways to solve your problem. Here's a solution with regular expressions. All airport codes from <code>word</code> are joined into a pattern that matches any of the codes as words:</p>

<pre><code>import re
pattern = '\\b(' + '|'.join(word) + ')\\b'
#'\\b(JFK|LHR|DXB)\\b'
matches = [1 if re.search(pattern, s) else 0 for s in String]
#[0, 1, 1]
</code></pre>

<p>You can make your original approach with splitting work, too, but if a code is immediately followed by a period then you will get a false negative. </p>

<pre><code>matches = [1 if word_set &amp; set(s.split()) else 0 for s in String]
#[0, 1, 1]
</code></pre>

<p>That can be fixed by switching from <code>split()</code> to <code>nltk.word_tokenize()</code>:</p>

<pre><code>from nltk import word_tokenize as tokens
word_set = set(word)
matches = [1 if word_set &amp; set(tokens(s)) else 0 for s in String]
#[0, 1, 1]
</code></pre>

<p>The NLTK-based solution is <em>much</em> (x50) slower than the re-based solution. The split-based solution is the fastest, but the least accurate.</p>
",0,0,84,2019-03-23 19:45:26,https://stackoverflow.com/questions/55317688/word-search-from-string-in-python-and-providing-output-into-csv-column
How to output in R all possible deviations of a word for a fixed distance value?,"<p>I have a word and want to output in R all possible deviatons (replacement, substitution, insertion) for a fixed distance value into a vector.</p>

<p>For instance, the word ""Cat"" and a fixed distance value of 1 results in a vector with the elements ""cot"", ""at"", ...</p>
","r, text-mining, tidyverse, stringr, quanteda","<p>I'm going to assume that you want all actual words, not just permutations of the characters with an edit distance of 1 that would include non-words such as ""zat"".</p>

<p>We can do this using <code>adist()</code> to compute the edit distance between your target word and all eligible English words, taken from some word list.  Here, I used the English syllable dictionary from the <strong>quanteda</strong> package (you did tag this question as <code>quanteda</code> after all) but this could have been any vector of English dictionary words from any other source as well.</p>

<p>To narrow things down, we first exclude all words that are different in length from the target word by your distance value.</p>



<pre class=""lang-r prettyprint-override""><code>distfn &lt;- function(word, distance = 1) {
  # select eligible words for efficiency
  eligible_y_words &lt;- names(quanteda::data_int_syllables)
  wordlengths &lt;- nchar(eligible_y_words)
  eligible_y_words &lt;- eligible_y_words[wordlengths &gt;= (nchar(word) - distance) &amp;
    wordlengths &lt;= (nchar(word) + distance)]
  # compute Levenshtein distance
  distances &lt;- utils::adist(word, eligible_y_words)[1, ]
  # return only those for the requested distance value
  eligible_y_words[distances == distance]
}

distfn(""cat"", 1)
##  [1] ""at""   ""bat""  ""ca""   ""cab""  ""cac""  ""cad""  ""cai""  ""cal""  ""cam""  ""can"" 
## [11] ""cant"" ""cao""  ""cap""  ""caq""  ""car""  ""cart"" ""cas""  ""cast"" ""cate"" ""cato""
## [21] ""cats"" ""catt"" ""cau""  ""caw""  ""cay""  ""chat"" ""coat"" ""cot""  ""ct""   ""cut"" 
## [31] ""dat""  ""eat""  ""fat""  ""gat""  ""hat""  ""kat""  ""lat""  ""mat""  ""nat""  ""oat"" 
## [41] ""pat""  ""rat""  ""sat""  ""scat"" ""tat""  ""vat""  ""wat""
</code></pre>

<p>To demonstrate how this works on longer words, with alternative distance values.</p>

<pre class=""lang-r prettyprint-override""><code>distfn(""coffee"", 1)
## [1] ""caffee""  ""coffeen"" ""coffees"" ""coffel""  ""coffer""  ""coffey""  ""cuffee"" 
## [8] ""toffee""

distfn(""coffee"", 2)
##  [1] ""caffey""   ""calfee""   ""chafee""   ""chaffee""  ""cofer""    ""coffee's""
##  [7] ""coffelt""  ""coffers""  ""coffin""   ""cofide""   ""cohee""    ""coiffe""  
## [13] ""coiffed""  ""colee""    ""colfer""   ""combee""   ""comfed""   ""confer""  
## [19] ""conlee""   ""coppee""   ""cottee""   ""coulee""   ""coutee""   ""cuffe""   
## [25] ""cuffed""   ""diffee""   ""duffee""   ""hoffer""   ""jaffee""   ""joffe""   
## [31] ""mcaffee""  ""moffet""   ""noffke""   ""offen""    ""offer""    ""roffe""   
## [37] ""scoffed""  ""soffel""   ""soffer""   ""yoffie""
</code></pre>

<p>(Yes, according to the CMU pronunciation dictionary, those are all actual words...)</p>

<p><strong>EDIT: Make for all permutations of letters, not just actual words</strong></p>

<p>This involves permutations from the alphabet that have the fixed edit distances from the input word.  Here I've done it not particular efficiently by forming all permutations of letters within the eligible ranges, and then computing their edit distance from the target word, and then selecting them.  So it's a variation of above, except instead of a dictionary, it uses permuted words.</p>

<pre class=""lang-r prettyprint-override""><code>distfn2 &lt;- function(word, distance = 1) {
  result &lt;- character()

  # start with deletions
  for (i in max((nchar(word) - distance), 0):(nchar(word) - 1)) {
    result &lt;- c(
      result,
      combn(unlist(strsplit(word, """", fixed = TRUE)), i,
        paste,
        collapse = """", simplify = TRUE
      )
    )
  }

  # now for changes and insertions
  for (i in (nchar(word)):(nchar(word) + distance)) {
    # all possible edits
    edits &lt;- apply(expand.grid(rep(list(letters), i)),
      1, paste0,
      collapse = """"
    )
    # remove original word
    edits &lt;- edits[edits != word]
    # get all distances, add to result
    distances &lt;- utils::adist(word, edits)[1, ]
    result &lt;- c(result, edits[distances == distance])
  }

  result
}
</code></pre>

<p>For the OP example:</p>

<pre class=""lang-r prettyprint-override""><code>distfn2(""cat"", 1)
##   [1] ""ca""   ""ct""   ""at""   ""caa""  ""cab""  ""cac""  ""cad""  ""cae""  ""caf""  ""cag"" 
##  [11] ""cah""  ""cai""  ""caj""  ""cak""  ""cal""  ""cam""  ""can""  ""cao""  ""cap""  ""caq"" 
##  [21] ""car""  ""cas""  ""aat""  ""bat""  ""dat""  ""eat""  ""fat""  ""gat""  ""hat""  ""iat"" 
##  [31] ""jat""  ""kat""  ""lat""  ""mat""  ""nat""  ""oat""  ""pat""  ""qat""  ""rat""  ""sat"" 
##  [41] ""tat""  ""uat""  ""vat""  ""wat""  ""xat""  ""yat""  ""zat""  ""cbt""  ""cct""  ""cdt"" 
##  [51] ""cet""  ""cft""  ""cgt""  ""cht""  ""cit""  ""cjt""  ""ckt""  ""clt""  ""cmt""  ""cnt"" 
##  [61] ""cot""  ""cpt""  ""cqt""  ""crt""  ""cst""  ""ctt""  ""cut""  ""cvt""  ""cwt""  ""cxt"" 
##  [71] ""cyt""  ""czt""  ""cau""  ""cav""  ""caw""  ""cax""  ""cay""  ""caz""  ""cata"" ""catb""
##  [81] ""catc"" ""catd"" ""cate"" ""catf"" ""catg"" ""cath"" ""cati"" ""catj"" ""catk"" ""catl""
##  [91] ""catm"" ""catn"" ""cato"" ""catp"" ""catq"" ""catr"" ""cats"" ""caat"" ""cbat"" ""acat""
## [101] ""bcat"" ""ccat"" ""dcat"" ""ecat"" ""fcat"" ""gcat"" ""hcat"" ""icat"" ""jcat"" ""kcat""
## [111] ""lcat"" ""mcat"" ""ncat"" ""ocat"" ""pcat"" ""qcat"" ""rcat"" ""scat"" ""tcat"" ""ucat""
## [121] ""vcat"" ""wcat"" ""xcat"" ""ycat"" ""zcat"" ""cdat"" ""ceat"" ""cfat"" ""cgat"" ""chat""
## [131] ""ciat"" ""cjat"" ""ckat"" ""clat"" ""cmat"" ""cnat"" ""coat"" ""cpat"" ""cqat"" ""crat""
## [141] ""csat"" ""ctat"" ""cuat"" ""cvat"" ""cwat"" ""cxat"" ""cyat"" ""czat"" ""cabt"" ""cact""
## [151] ""cadt"" ""caet"" ""caft"" ""cagt"" ""caht"" ""cait"" ""cajt"" ""cakt"" ""calt"" ""camt""
## [161] ""cant"" ""caot"" ""capt"" ""caqt"" ""cart"" ""cast"" ""catt"" ""caut"" ""cavt"" ""cawt""
## [171] ""caxt"" ""cayt"" ""cazt"" ""catu"" ""catv"" ""catw"" ""catx"" ""caty"" ""catz""
</code></pre>

<p>Also works with other edit distances, although it becomes very slow for longer words.</p>

<pre class=""lang-r prettyprint-override""><code>d2 &lt;- distfn2(""cat"", 2)
set.seed(100)
c(head(d2, 50), sample(d2, 50), tail(d2, 50))
##   [1] ""c""     ""a""     ""t""     ""ca""    ""ct""    ""at""    ""aaa""   ""baa""  
##   [9] ""daa""   ""eaa""   ""faa""   ""gaa""   ""haa""   ""iaa""   ""jaa""   ""kaa""  
##  [17] ""laa""   ""maa""   ""naa""   ""oaa""   ""paa""   ""qaa""   ""raa""   ""saa""  
##  [25] ""taa""   ""uaa""   ""vaa""   ""waa""   ""xaa""   ""yaa""   ""zaa""   ""cba""  
##  [33] ""aca""   ""bca""   ""cca""   ""dca""   ""eca""   ""fca""   ""gca""   ""hca""  
##  [41] ""ica""   ""jca""   ""kca""   ""lca""   ""mca""   ""nca""   ""oca""   ""pca""  
##  [49] ""qca""   ""rca""   ""cnts""  ""cian""  ""pcatb"" ""cqo""   ""uawt""  ""hazt"" 
##  [57] ""cpxat"" ""aaet""  ""ckata"" ""caod""  ""ncatl"" ""qcamt"" ""cdtp""  ""qajt"" 
##  [65] ""bckat"" ""qcatr"" ""cqah""  ""rcbt""  ""cvbt""  ""bbcat"" ""vcaz""  ""ylcat""
##  [73] ""cahz""  ""jcgat"" ""mant""  ""jatd""  ""czlat"" ""cbamt"" ""cajta"" ""cafp"" 
##  [81] ""cizt""  ""cmaut"" ""qwat""  ""jcazt"" ""hdcat"" ""ucant"" ""hate""  ""cajtl""
##  [89] ""caaty"" ""cix""   ""nmat""  ""cajit"" ""cmnat"" ""caobt"" ""catoi"" ""ncau"" 
##  [97] ""ucoat"" ""ncamt"" ""jath""  ""oats""  ""chatz"" ""ciatz"" ""cjatz"" ""ckatz""
## [105] ""clatz"" ""cmatz"" ""cnatz"" ""coatz"" ""cpatz"" ""cqatz"" ""cratz"" ""csatz""
## [113] ""ctatz"" ""cuatz"" ""cvatz"" ""cwatz"" ""cxatz"" ""cyatz"" ""czatz"" ""cabtz""
## [121] ""cactz"" ""cadtz"" ""caetz"" ""caftz"" ""cagtz"" ""cahtz"" ""caitz"" ""cajtz""
## [129] ""caktz"" ""caltz"" ""camtz"" ""cantz"" ""caotz"" ""captz"" ""caqtz"" ""cartz""
## [137] ""castz"" ""cattz"" ""cautz"" ""cavtz"" ""cawtz"" ""caxtz"" ""caytz"" ""caztz""
## [145] ""catuz"" ""catvz"" ""catwz"" ""catxz"" ""catyz"" ""catzz""
</code></pre>

<p>This could be speeded up by less brute force formation of all permutations and then applying <code>adist()</code> to them - it could consist of changes or insertions of known edit distances generated algorithmically from <code>letters</code>.</p>
",1,1,151,2019-03-26 08:12:58,https://stackoverflow.com/questions/55352498/how-to-output-in-r-all-possible-deviations-of-a-word-for-a-fixed-distance-value
Efficiently break up a string based on the nth occurrence of a substring using R,"<p><strong>Introduction</strong></p>

<p>Given a string in R, is it possible to get a vectorized solution (i.e. no loops) where we can break the string into blocks where each block is determined by the nth occurrence of a substring in the string.</p>

<p><strong>Work done with Reproducible Example</strong></p>

<p>Suppose we have several paragraphs of the famous Lorem Ipsum text.</p>

<pre><code>library(strex)
# devtools::install_github(""aakosm/lipsum"")
library(lipsum)

my.string = capture.output(lipsum(5))
my.string = paste(my.string, collapse = "" "")

&gt; my.string # (partial output)
# [1] ""Lorem ipsum dolor ... id est laborum. ""
</code></pre>

<p>We would like to break this text into segments at every <strong><em>3rd</em></strong> occurrence of the the word <strong><em>"" in""</em></strong> (a space is included in order to distinguish from words which contain ""in"" as part of them, such as ""min"").</p>

<p>I have the following solution with a while loop:</p>

<pre><code># We wish to break up the string at every 
# 3rd occurence of the worn ""in""

break.character = "" in""
break.occurrence = 3
string.list = list()
i = 1

# initialize string to send into the loop
current.string = my.string

while(length(current.string) &gt; 0){

  # Enter segment into the list which occurs BEFORE nth occurence character of interest
  string.list[[i]] = str_before_nth(current.string, break.character, break.occurrence)

  # Update next string to exmine.
  # Next string to examine is current string AFTER nth occurence of character of interest
  current.string = str_after_nth(current.string, break.character, break.occurrence)

  i = i + 1
}
</code></pre>

<p>We are able to get the desired output in a list with a warning (warning not shown)</p>

<pre><code>&gt; string.list (#partial output shown)
[[1]]
[1] ""Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit""

[[2]]
[1] "" voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.  Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor""
...

[[6]]
[1] "" voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.  Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor""
</code></pre>

<p><strong>Goal</strong></p>

<p>Is it possible to improve this solution by vectorizing (i.e. using <code>apply()</code>, <code>lapply()</code>, <code>mapply()</code> etc.). Also, my current solution cut's off the last occurrence of the substring in a block.</p>

<p>The current solution may not work well on extremely long strings (such as DNA sequences where we are looking for blocks with the nth occurrence of a substring of nucleotides).</p>
","r, string, text, text-mining","<p>Try with this: </p>

<pre><code>text_split=strsplit(text,"" in "")[[1]]

l=length(text_split)
n = floor(l/3)
Seq = seq(1,by=2,length.out = n)

L= list()
L=sapply(Seq, function(x){
  paste0(paste(text_split[x:(x+2)],collapse="" in ""),"" in "")
})
if (l&gt;(n*3)){
L = c(L,paste(text_split[(n*3+1):l],collapse="" in ""))
}
</code></pre>

<p>Last conditional is in case number of <code>in</code> is not divisible by 3. Also, the last <code>in</code> pasted in the <code>sapply()</code> is there because I don't know what you want to do with the one <code>in</code> that separates your blocks.</p>
",1,1,206,2019-04-04 16:01:05,https://stackoverflow.com/questions/55520430/efficiently-break-up-a-string-based-on-the-nth-occurrence-of-a-substring-using-r
Remove characters which repeat more than twice in a string,"<p>I have this text:</p>

<pre><code>F &lt;- ""hhhappy birthhhhhhdayyy""
</code></pre>

<p>and I want to remove the repeat characters, I tried this code </p>

<p><a href=""https://stackoverflow.com/a/11165145/10718214"">https://stackoverflow.com/a/11165145/10718214</a></p>

<p>and it works, but I need to remove repeat characters if it repeats more than 2, and if it repeated 2 times keep it.</p>

<p>so the output that I expect is</p>

<pre><code>""happy birthday""
</code></pre>

<p>any help?</p>
","r, regex, text-mining","<p>Try using <code>sub</code>, with the pattern <code>(.)\\1{2,}</code>:</p>

<pre><code>F &lt;- (""hhhappy birthhhhhhdayyy"")
gsub(""(.)\\1{2,}"", ""\\1"", F)

[1] ""happy birthday""
</code></pre>

<p>Explanation of regex:</p>

<pre><code>(.)          match and capture any single character
\\1{2,}      then match the same character two or more times
</code></pre>

<p>We replace with just the single matching character.  The quantity <code>\\1</code> represents the first capture group in <code>sub</code>.</p>
",5,5,3074,2019-04-10 07:39:21,https://stackoverflow.com/questions/55607198/remove-characters-which-repeat-more-than-twice-in-a-string
How to perfom stemming and drop columns in pandas dataframe in python?,"<p>Below is the subset of my dataset. I am trying to clean my dataset using <code>Porter stemmer</code> that is available in <code>nltk</code> package. I would like to drop columns that are similar in their stems for example ""abandon','abondoned','abondening' should be just abondoned in my dataset. Below is the code I am trying, where I can see words/columns being stemmed. But I am not sure about how to drop those columns? I have already tokeninze and removed punctuation from the corpus.</p>

<p><em>Note:</em> I am new to <code>Python</code> and <code>Textmining</code>.</p>

<p><strong>Dataset Subset</strong></p>

<pre><code>{
   'aaaahhhs':{
      0:0,
      1:0,
      2:0,
      3:0,
      4:0,
      5:0
   },
   'aahs':{
      0:0,
      1:0,
      2:0,
      3:0,
      4:0,
      5:0
   },
   'aamir':{
      0:0,
      1:0,
      2:0,
      3:0,
      4:0,
      5:0
   },
   'aardman':{
      0:0,
      1:0,
      2:0,
      3:0,
      4:0,
      5:0
   },
   'aaron':{
      0:0,
      1:0,
      2:0,
      3:0,
      4:0,
      5:0
   },
   'abandon':{
      0:0,
      1:0,
      2:0,
      3:0,
      4:0,
      5:0
   },
   'abandoned':{
      0:0,
      1:0,
      2:0,
      3:0,
      4:0,
      5:0
   },
   'abandoning':{
      0:0,
      1:0,
      2:0,
      3:0,
      4:0,
      5:0
   },
   'abandonment':{
      0:0,
      1:0,
      2:0,
      3:0,
      4:0,
      5:0
   },
   'abandons':{
      0:0,
      1:0,
      2:0,
      3:0,
      4:0,
      5:0
   }
}
</code></pre>

<p><strong>code so far..</strong></p>

<pre><code>from nltk.stem import PorterStemmer 
from nltk.tokenize import word_tokenize   
ps = PorterStemmer() 
for w in clean_df.columns:
    print(ps.stem(w))
</code></pre>
","python, pandas, text-mining, stemming, porter-stemmer","<p>I think something like this does what you want:</p>

<pre class=""lang-py prettyprint-override""><code>import collections

# Here the assotiations between stems and column names are built:
stems = collections.defaultdict(list)
for column_name in clean_df.columns:
    stems[ps.stem(column_name)].append(column_name)

# Here for each stem the first (in lexicographical order) is gotten:
new_columns = [sorted(columns)[0] for _, columns in stems.items()]

# Here the new `DataFrame` is created which contains selected columns:
new_df = clean_df[new_columns]
</code></pre>
",1,0,235,2019-04-13 14:39:49,https://stackoverflow.com/questions/55666673/how-to-perfom-stemming-and-drop-columns-in-pandas-dataframe-in-python
How to return all possible categories separated by | under one column,"<p>I have a dataset ""movie"" that has a column named ""genre"", its values are like ""Action"", ""Action|Animation"", ""Animation|Fantasy"". A movie can have more than one genre. I would like to output a list of all possible single categories (such as Adventure, Fantasy) and their frequencies. In other words, I want to know how many movies have genre ""action"", how many have ""fantasy"". I don't care about the combinations. Are there any advice on this?</p>
","r, text-mining","<p>Here is one simple way to do in base <code>R</code> using <code>sapply</code></p>

<pre><code># sample data frame
df &lt;- data.frame(genre=c(""Action"", ""Action|Animation"", ""Animation|Fantasy""), stringsAsFactors = F)

# get uniq genre
uniq.genre &lt;- unique(unlist(strsplit(df$genre, split = '\\|')))

# get frequency
sapply(uniq.genre, function(genre) {
  sum(grepl(genre, df$genre))
})
#&gt;    Action Animation   Fantasy 
#&gt;         2         2         1
</code></pre>
",0,1,67,2019-04-14 19:05:26,https://stackoverflow.com/questions/55678993/how-to-return-all-possible-categories-separated-by-under-one-column
The words that come always together in R,"<p>I am using R  and there is a text column in my data set, and I need to know if there is any way to know what is the words are always come together.
like most two words come together or three words ...etc</p>

<p>For example:</p>

<pre><code>Happy birthday to you 
Happy weekend 
Have a nice day
Be close 
Be smart 
Happy birthday 
It was a nice day
Happy birthday mama
</code></pre>

<p>So the results should be something like this </p>

<pre><code>Happy birthday  - freq 3 
Nice day - freq 2
</code></pre>
","r, text-mining, word-frequency","<p>It seems that what you need is to create bi-grams and count the features. Here is a way to do with <code>quanteda</code>.</p>

<pre class=""lang-r prettyprint-override""><code>
library(quanteda) 
text &lt;- c(""Happy birthday to you "", ""Happy weekend "", ""Have a nice day"", 
          ""Be close "", ""Be smart "", ""Happy birthday "", ""It was a nice day"", 
          ""Happy birthday mama"")
text %&gt;% tokens() %&gt;% 
  tokens_ngrams(n = 2, concatenator = "" "") %&gt;% dfm() %&gt;% topfeatures()

## happy birthday         a nice       nice day    birthday to         to you       be smart 
##              3              2              2              1              1              1 
##  happy weekend         it was          was a         have a 
##              1              1              1              1 

</code></pre>

<p>What it does is:</p>

<ol>
<li>Tokenize</li>
<li>Create bigrams (concatenated with a single white space)</li>
<li>Create a document future matrix (as <code>topfeatures</code> requires it)</li>
<li>Count the most frequent features</li>
</ol>
",3,0,94,2019-04-16 15:48:57,https://stackoverflow.com/questions/55712105/the-words-that-come-always-together-in-r
How to find most frequnet words in a corpus in Pandas dataframe (Python),"<p>I have Pandas dataframe that looks like following.I have tokenized my text files and used <code>NLTK</code> <code>Countvectorizer</code> to convert into <code>pandas</code> dataframe. In addition, I have already removed stopwords and punctuation from my coupus. I am trying to find most frequent words in my corpus in <code>pandas</code> dataframe. In below dataframe,words such as ""aaron"" and ""abandon"" aprreared >10 times, thus those words should be in new dataframe.</p>

<p><em>Note: I am new to python, and I am  not sure how to implement this. Provide explanation with code.</em></p>

<p><strong>Subset of the dataframe</strong></p>

<p>I already already clean my corpus and my <code>dataframe</code> looks like following</p>

<pre><code>{'aaaahhhs': {990: 0, 991: 0, 992: 0, 993: 0, 994: 0, 995: 0, 996: 0, 997: 0, 998: 0, 999: 0, 1000: 1}, 'aahs': {990: 0, 991: 0, 992: 0, 993: 0, 994: 0, 995: 0, 996: 0, 997: 0, 998: 0, 999: 0, 1000: 1}, 'aamir': {990: 0, 991: 0, 992: 0, 993: 0, 994: 0, 995: 0, 996: 0, 997: 0, 998: 0, 999: 0, 1000: 1}, 'aardman': {990: 0, 991: 0, 992: 0, 993: 0, 994: 0, 995: 0, 996: 0, 997: 0, 998: 0, 999: 0, 1000: 2}, 'aaron': {990: 0, 991: 0, 992: 0, 993: 0, 994: 0, 995: 0, 996: 4, 997: 0, 998: 0, 999: 0, 1000: 14}, 'abandon': {990: 0, 991: 0, 992: 0, 993: 0, 994: 0, 995: 0, 996: 0, 997: 0, 998: 0, 999: 0, 1000: 16}}
</code></pre>

<p><a href=""https://i.sstatic.net/1W7lX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1W7lX.png"" alt=""enter image description here""></a></p>
","python-3.x, pandas, nltk, text-mining, countvectorizer","<p>If need top N words:</p>

<pre><code>N = 2 
print (df.sum().nlargest(N).index)
Index(['aaron', 'abandon'], dtype='object')
</code></pre>

<p>Another solution:</p>

<pre><code>print (df.sum().sort_values(ascending=False).index[:N])
Index(['aaron', 'abandon'], dtype='object')
</code></pre>

<p>If need also counts in one column <code>DataFrame</code> or <code>Series</code> (remove <code>to_frame</code>):</p>

<pre><code>N = 2
print (df.sum().nlargest(N).to_frame('count'))
         count
aaron       18
abandon     16
print (df.sum().sort_values(ascending=False).iloc[:N].to_frame('count'))
         count
aaron       18
abandon     16
</code></pre>

<p>If need 2 column <code>DataFrame</code>:</p>

<pre><code>print (df.sum().nlargest(N).rename_axis('word').reset_index(name='count'))
      word  count
0    aaron     18
1  abandon     16

print (df.sum()
         .sort_values(ascending=False).iloc[:N]
         .rename_axis('word')
         .reset_index(name='count'))
      word  count
0    aaron     18
1  abandon     16
</code></pre>
",1,0,326,2019-04-20 14:04:18,https://stackoverflow.com/questions/55774268/how-to-find-most-frequnet-words-in-a-corpus-in-pandas-dataframe-python
"Is there an R technique to group_by, search, and match a long data structure?","<p>This is a problem of finding which <code>id</code>s have matching <code>word</code>s, from a list of 5 words for each <code>id</code>.</p>

<p>We have a long data structure from a text mining project with an <code>id</code> and the <code>word</code>. Each group_id has 5 words. We would like to measure which <code>word</code>s from one id are in another <code>id</code>. i.e. which id are similar based on there words. </p>

<p>We have tried to use a for loop on the [row, column] but it seems there is a better way.</p>

<pre><code>library(tidyverse)

data &lt;- tibble(id = factor(c(1234, 1234, 1234, 1234, 1234, 
                             4523, 4523, 4523, 4523, 4523, 
                             0984, 0984, 0984, 0984, 0984)),
       word = c(""hello"", ""today"", ""the"", ""monkey"", ""boy"",
                ""go"", ""me"", ""key"", ""wind"", ""hello"",
                ""monkey"", ""yes"", ""no"", ""wild"", ""quit""))


output &lt;- matrix(1, length(data$id), length(data$id))

for (j in 1 : length(data$id)) {
  for (i in 1 : length(data$id)) {
    output[i,j] &lt;-  data[i,2] == data[j,2]

  }
}

output

## from the output we see that 4 and 11 match.

data[4,]
data[11,]

</code></pre>

<p>My end goal is to have a matrix with <code>id</code> by <code>id</code> and the intersections are the number of matching words (0-5).</p>

<p>This is the desired output:</p>

<pre><code>#      1234 4523 0984 
# 1234    5    1    1  
# 4523    1    5    0  
# 0984    1    0    5 
</code></pre>

<p>Any suggestion on completely reorganizing the data structure or solutions with this structure are welcome. Thanks!</p>
","r, boolean, match, tidyverse, text-mining","<p>We can <code>split</code> <code>word</code> by <code>id</code> and then use <code>outer</code> with a custom function to calculate number of times a word occur between different <code>id</code>s.</p>

<pre><code>count_value &lt;- function(x, y) {
    colSums(mapply(`%in%`, x, y))
}

outer(split(data$word, data$id),split(data$word, data$id), count_value)

#     984 1234 4523
#984    5    1    0
#1234   1    5    1
#4523   0    1    5
</code></pre>
",1,0,44,2019-04-24 23:30:42,https://stackoverflow.com/questions/55839875/is-there-an-r-technique-to-group-by-search-and-match-a-long-data-structure
How to remove white spaces within a word using python?,"<p>This is the input given <code>John plays chess and l u d o.</code>  I want the output to be in this format (given below)</p>

<p><code>John plays chess and ludo.</code></p>

<p>I have tried Regular expression for removing spaces
but doesn't work for me.</p>

<pre><code>import re
sentence='John plays chess and l u d o'
sentence = re.sub(r""\s+"", """", sentence, flags=re.UNICODE)

print(sentence)
</code></pre>

<p>I expected the output <code>John plays chess and ludo.</code> .<br>
But the output I got is <code>Johnplayschessandludo</code></p>
","python, python-3.x, text-mining, spacy, removing-whitespace","<p>This should work! In essence, the solution extracts the single characters out of the sentence, makes it a word and joins it back to the remaining sentence.</p>

<pre><code>s = 'John plays chess and l u d o'

chars = []
idx = 0

#Get the word which is divided into single characters
while idx &lt; len(s)-1:

    #This will get the single characters around single spaces
    if s[idx-1] == ' ' and s[idx].isalpha() and s[idx+1] == ' ':
        chars.append(s[idx])

    idx+=1

#This is get the single character if it is present as the last item
if s[len(s)-2] == ' ' and s[len(s)-1].isalpha():
    chars.append(s[len(s)-1])

#Create the word out of single character
join_word = ''.join(chars)

#Get the other words
old_words = [item for item in s.split() if len(item) &gt; 1]

#Form the final string
res = ' '.join(old_words + [join_word])

print(res)
</code></pre>

<p>The output will then look like</p>

<pre><code>John plays chess and ludo
</code></pre>
",4,2,228,2019-04-26 09:04:23,https://stackoverflow.com/questions/55864233/how-to-remove-white-spaces-within-a-word-using-python
How can I generate a word cloud from tokenized words in Python?,"<p>I have a code to import a txt file and get tokenized words using NLTK library (just like it is done in <a href=""https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk"" rel=""nofollow noreferrer"">https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk</a>). I did almost everything I needed easily, however I'm struggling to build a word cloud with the words I have now and I don't have any clue even after hours of search on the web.</p>

<p>This is my code so far:</p>

<pre><code># Carrega bibliotecas
!pip install nltk
import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')
from nltk.tokenize import word_tokenize

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

# Import file
f = open('PNAD2002.txt','r')
pnad2002 = """"
while 1:
    line = f.readline()
    if not line:break
    pnad2002 += line

f.close()

tokenized_word=word_tokenize(pnad2002)

tokenized_word_2 = [w.lower() for w in tokenized_word]
</code></pre>

<p>I wanted to use the following code (from <a href=""https://github.com/amueller/word_cloud/blob/master/examples/simple.py"" rel=""nofollow noreferrer"">https://github.com/amueller/word_cloud/blob/master/examples/simple.py</a>):</p>

<pre><code># Read the whole text.
text = open(path.join(d, 'constitution.txt')).read()

# Generate a word cloud image
wordcloud = WordCloud().generate(text)

# Display the generated image:
# the matplotlib way:
import matplotlib.pyplot as plt
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis(""off"")

# lower max_font_size
wordcloud = WordCloud(max_font_size=40).generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation=""bilinear"")
plt.axis(""off"")
plt.show()
</code></pre>

<p>But I don't know how to use my tokenized words with this.</p>
","python, text-mining, word-cloud","<p>You need to instanciate a <code>WordCloud</code> object then call <code>generate_from_text</code>:</p>

<pre class=""lang-py prettyprint-override""><code>wc = WordCloud()
img = wc.generate_from_text(' '.join(tokenized_word_2))
img.to_file('worcloud.jpeg') # example of something you can do with the img
</code></pre>

<p>There's a bunch of customization you can pass to <code>WordCloud</code>, you can find examples online such as this: <a href=""https://www.datacamp.com/community/tutorials/wordcloud-python"" rel=""nofollow noreferrer"">https://www.datacamp.com/community/tutorials/wordcloud-python</a></p>
",4,1,5887,2019-05-03 18:50:04,https://stackoverflow.com/questions/55975609/how-can-i-generate-a-word-cloud-from-tokenized-words-in-python
Extracting university names from affiliation in Pubmed data with R,"<p>I've been using the extremely useful rentrez package in R to get information about author, article ID and author affiliation from the Pubmed database. This works fine but now I would like to extract information from the affiliation field. Unfortunately the affiliation field is widely unstructured, not standardized string with various types of information such as the name of university, name of department, address and more delimited by commas. Therefore text mining approach is necessary to get any useful information from this field. </p>

<p>I tried the package easyPubmed in combination with rentrez, and even though easyPubmed package can extract some information from the affiliation field (e.g. email address, which is very useful), to my knowledge it cannot extract university name. I also tried the package pubmed.mineR, but unfortunately this also does not provide university name extraction. I startet to experiment with grep and regex functions but as I am no R expert I could not make this work.</p>

<p>I was able to find very similar threads solving the issue with python:</p>

<p><a href=""https://stackoverflow.com/questions/5939127/regex-for-extracting-names-of-colleges-universities-and-institutes"">Regex for extracting names of colleges, universities, and institutes?</a></p>

<p><a href=""https://stackoverflow.com/questions/53645622/how-to-extract-university-school-college-name-from-string-in-python-using-regula"">How to extract university/school/college name from string in python using regular expression?</a></p>

<p>But unfortunately I do not know how to convert the python regex function to an R regex function as I am not familiar with python.</p>

<p>Here is some example data:</p>

<pre><code>PMID = c(121,122,123,124,125)
author=c(""author1"",""author2"",""author3"",""author4"",""author5"")
Affiliation = c(""blabla,University Ghent,blablabla"", ""University Washington, blabla, blablabla, blablabalbalba"",""blabla,University of Florence,blabla"", ""University Chicago, Harvard University"", ""Oxford University"")
df = as.data.frame(cbind(PMID,author,Affiliation))

df
PMID  author                                              Affiliation
1  121 author1                        blabla,University Ghent,blablabla
2  122 author2 University Washington, blabla, blablabla, blablabalbalba
3  123 author3                        blabla,University of Florence,blabla
4  124 author4                        University Chicago, Harvard University
5  125 author5                        Oxford University
</code></pre>

<p>What I would like to get:</p>

<pre><code>PMID  author    Affiliation                        University
1  121 author1  blabla,University Ghent,blablabla  University Ghent
2  122 author2  University Washington,ba, bla, bla University Washington
3  123 author3  blabla,University Florence,blabla  University of Florence
4  124 author4  University Chicago, Harvard Univ   University Chicago, Harvard University
5  125 author5  Oxford University                  Oxford University
</code></pre>

<p>Please sorry if there is already a solution online, but I honestly googled a lot and did not find any clear solution for R. I would be very thankful for any hints and solutions to this task.</p>
","r, regex, text, text-mining, pubmed","<p>In general, regex expressions can be ported to R with some changes. For example, using the php link you included, you can create a new variable with extracted text using that regex expression, and only changing the escape character (""\\"" instead ""\""). So, using <code>dplyr</code> and <code>stringr</code> packages:</p>

<pre><code>library(dplyr)
library(stringr)
df &lt;- df %&gt;% 
  mutate(Organization=str_extract(Affiliation,
      ""([A-Z][^\\s,.]+[.]?\\s[(]?)*(College|University|Institute|Law School|School of|Academy)[^,\\d]*(?=,|\\d)""))
</code></pre>
",2,3,1068,2019-05-07 09:46:46,https://stackoverflow.com/questions/56019837/extracting-university-names-from-affiliation-in-pubmed-data-with-r
Read multiple pdf files at once and extract sentences that contain a keyword using R,"<p>Let's assume that I have few pdf files stored in a directory and I want to read all those pdf files at one and extract all the sentences that contain a specific keyword (in this case 'provisions') instead of manually opening each file and looking for that keyword.</p>

<p>I have tried reading the files but how can I make R go through each pdf file to search for that keyword and output those sentences?
Here's a small piece that I have written:</p>

<pre><code>library(pdftools)
files &lt;- list.files(""filepath"",pattern = ""pdf$"", full.names = TRUE)
comb &lt;- lapply(files, pdf_text)
</code></pre>

<p>For file reference purpose, the links for pdf files are:</p>

<pre><code>&lt;https://www.supremecourt.gov/opinions/14pdf/13-1314_3ea4.pdf&gt; 
&lt;https://www.supremecourt.gov/opinions/14pdf/14-7955_aplc.pdf&gt;
&lt;https://www.supremecourt.gov/opinions/14pdf/14-46_bqmc.pdf&gt;
</code></pre>

<p>I have created a directory and saved the pdf files in it. </p>
","r, pdf, text-mining","<p>An update to the question:
I have found the solution which can be achieved with the below code:</p>

<pre><code>install.packages(""textreadr"")
install.packages(""tidyverse"")
install.packages(""pdfsearch"")
library(textreadr)
library(tidyverse)
library(pdfsearch)

dirct &lt;- directory_path
result &lt;- keyword_directory(dirct, 
                            keyword = 'input_the_keyword_you_want_to_extract',
                            surround_lines = 0, full_names = TRUE)
head(result$line_text, n = 20)
</code></pre>
",4,0,1650,2019-05-08 19:45:49,https://stackoverflow.com/questions/56048016/read-multiple-pdf-files-at-once-and-extract-sentences-that-contain-a-keyword-usi
Text mining a large list of Notes for Vehicle Identification Number (VIN#) with Python,"<p>I have a large data set of Insurance Claims data with 2 columns. One column is a claim identifier. The other is a large string of notes that go with the claim.</p>

<p>My goal is to text mine the Claims Notes for a specific VIN number. Typically a VIN# is in a 17 digit format. See Below: <a href=""https://www.autocheck.com/vehiclehistory/autocheck/en/vinbasics"" rel=""nofollow noreferrer"">https://www.autocheck.com/vehiclehistory/autocheck/en/vinbasics</a></p>

<p>However, with my data, some issues arise. Sometimes only the last 6 digits were input for a VIN#. I basically need a way to process my data and grab anything that looks like a 17 digit VIN Number and return it to that row of data. I am using Python 3 and am a rookie text miner but have some basic experience using regular expressions.</p>

<p>I am trying to create a function in python in which I can lambda apply it to the column of notes.</p>

<p>Attempt so far:</p>

<pre><code>C_Notes['VIN#s'] = C_Notes['ClaimsNotes'].str.findall(r'[0-9]{1}[0-9a-zA-Z]{16}') 
</code></pre>

<p>I am trying to mimic the format of the VIN in the link I provided.</p>

<p>So something that looks for a string with following qualities:</p>

<p>EDIT: Changed code snippet. This code example works if I make some toy examples of VINs with made up text but I am not having any success iterating through my pandas column. Each row entry has a large paragraph of text I want the function to go through each row at a time.</p>

<p>Thank you.</p>
","python, python-3.x, pandas, text-mining, vin","<p>But with which VIN system <em>exactly</em> you are dealing?</p>

<p>Wikipedia has <a href=""https://en.wikipedia.org/wiki/Vehicle_identification_number#Components"" rel=""nofollow noreferrer"">article</a> describing 17-digit VIN number, describing three different systems: ISO 3779, European and North American.</p>

<p>Apparently there is not general formal rule dictating what (onlyletter/onlydigit/letterordigit) occupies which position.</p>

<p>First 3 characters depend on manufacturer country/region and first of them is digit for Oceania and both Americas but letter for everyone else.  </p>

<p>For North American 9th, 13th, 14th, 15th, 16th and 17th positions are always digits and letters <code>I,O,Q</code> are never used.</p>

<p>Taking in account above considerations following pattern could be used:</p>

<pre><code>[0-9][0-9A-Za-z^IiOoQq]{7}[0-9][0-9A-Za-z^IiOoQq]{3}[0-9]{5}
</code></pre>

<p>Which consist of digits (<code>[0-9]</code>) and letterordigits but which are not prohibited characters. <code>^</code> inside <code>[]</code> mean that blacklist of characters will follow.</p>

<p>As more general note I advise against guessing regular expression based on limited subset of legal strings.</p>
",1,0,1343,2019-05-15 17:35:36,https://stackoverflow.com/questions/56154851/text-mining-a-large-list-of-notes-for-vehicle-identification-number-vin-with
String Concatenation in Ruta,"<p>does somebody know what is wrong with my String Concatenation in Ruta?</p>

<pre>

    FOREACH (d) IngredientConcept{} {
    d{->CREATE(Entity, ""label""=""Drug"", ""value""= d.conceptID + ""_"" + d.dictCanon)};
    }

</pre>

<p>Caused by: org.apache.uima.ruta.extensions.RutaParseRuntimeException: Error in Anonymous, line 28, ""+"": expected RPAREN, but found PLUS</p>

<p>Thanks for your help.
Philipp</p>
","nlp, text-mining, uima, ruta","<p>Have you tried this:
    ""value""= """" + d.conceptID + ""_"" + d.dictCanon</p>
",4,3,132,2019-05-16 06:15:12,https://stackoverflow.com/questions/56161904/string-concatenation-in-ruta
Link 2 annotations together in a window of up to 10 words using Ruta,"<p>is there a way to link 2 annotations that are within a windows of 10 words, together in a new one?</p>

<p>The following doesn't work:</p>

<p><code>Entity W{1,10} Entity{-&gt;CREATE(Entity)};</code></p>

<p>Thanks and all the best<br>
Philipp</p>
","nlp, text-mining, uima, ruta","<p>You can specify the offsets of a newly created annotation by putting the action at a composed rule element:</p>

<pre><code>(Entity W[1,10]{-PARTOF(Entity)} Entity){-&gt; Entity};
</code></pre>

<p>Please mind the condition at the rule element with the quantifier. The quantifier is greedy and will consume the ANYs in the Entity, whereby the rule won't match as expected or not at all.</p>

<p><em>DISCLAIMER: I am a developer of UIMA Ruta</em></p>
",1,0,29,2019-05-16 14:28:32,https://stackoverflow.com/questions/56170914/link-2-annotations-together-in-a-window-of-up-to-10-words-using-ruta
how to remove empty value after we do preprocessing text in python,"<p>e.g
I have a tweet ""@cintya @groot @smanela <a href=""https://blog"" rel=""nofollow noreferrer"">https://blog</a>...""
and I do a preprocessing process that link and mention has been deleted, and I think it should be lost. 
But in CSV, they return an empty value. How can I fix them?
Here is my code</p>

<pre><code>def replaceMultiple(mainString, toBeReplaces, newString):
    for elem in toBeReplaces :
        if elem in mainString :
            mainString = mainString.replace(elem, newString)
    return  mainString

with open('datalatihNegatif.csv', encoding='utf-8') as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',')
    for row in readCSV:
        _word = []
        username = row[0]
        date = row[1]
        text = row[2].lower()
        text = re.sub(r'@[A-Za-z0-9_]+','',text)
        text = re.sub(r'http\S+', '',text)

        text = replaceMultiple(text, [""!"",""@"",""#"",""$"",""%"",""^"",""&amp;"",""*"",""("",
                                      "")"",""_"",""-"",""+"",""="",""{"",""}"",""["",""]"",
                                      ""\\"",""/"","","",""."",""?"",""&lt;"",""&gt;"","":"","";"",
                                      ""'"",'""',""~"",""0"",""1"",""2"",""3"",""4"",""5"",""6"",""7"",""8"",""9""], '')
        text = text.strip()
        nltk_tokens = nltk.word_tokenize(text)
        stop_words = set(stopwords.words(""indonesian""))
        stop_words_new = ['i','liked','video','an','at','ba','da','do','ka','ma','ta','uh','yg','al','eh','ha','ah','ng']
        new_stopwords_list = stop_words.union(stop_words_new)

        print(username)
        print(date)

        for word in nltk_tokens:
            if word not in new_stopwords_list:
                if stemmer.stem(word) != """":
                    _word.append(stemmer.stem(word))
        print(_word)
        csvFile = open('preprocessingDLNegatif.csv', 'a', newline='')
        csvWriter = csv.writer(csvFile)
        csvWriter.writerow(_word)
        csvFile.close()
</code></pre>

<p>I expect the result in CSV is deleted, but the actual output is empty value 1 row in CSV
<a href=""https://i.sstatic.net/fgTOF.png"" rel=""nofollow noreferrer"">481 is empty value, how can i remove it?</a></p>
","python, text-mining, preprocessor","<p>How about checking for anything in <code>_word</code> before you write it out:</p>

<pre><code>if len(_word) != 0:
    csvFile = open('preprocessingDLNegatif.csv', 'a', newline='')
    csvWriter = csv.writer(csvFile)
    csvWriter.writerow(_word)
    csvFile.close()
</code></pre>

<p>I also wouldn't open an close the output file for every record you write. Open it once before looping and close it when done. Doing so would make my answer look like:</p>

<pre><code>if len(_word) != 0:
    csvWriter.writerow(_word)
</code></pre>
",0,0,229,2019-05-18 13:52:25,https://stackoverflow.com/questions/56199594/how-to-remove-empty-value-after-we-do-preprocessing-text-in-python
Subsetting a character vector column into multiple columns,"<p>I have the following tibble:</p>

<pre><code>colours = tribble(
  ~all,
  c('blue','green', 'red', 'pink', 'yellow', 'gold', 'orange', 'ivory', 'brown', 'beige'),
  c('green', 'red', 'pink', 'orange', 'ivory', 'beige')
)
</code></pre>

<p>I would like to split the colours into multiple columns according to their colour family: <code>Cool</code>, <code>Warm</code>, <code>Neutral</code>, with one column for each family.</p>

<p>I can do this using <code>mutate</code> with <code>map</code> and <code>str_subset</code>:</p>

<pre><code>colours %&gt;%
  mutate(
    'Cool' = map(all, ~str_subset(., '^(blue|green)$')), 
    'Warm' = map(all, ~str_subset(., '^(red|pink|yellow|gold|orange)$')),
    'Neutral' = map(all, ~str_subset(., '^(ivory|brown|beige)$'))
  )

# A tibble: 2 x 4
  all        Cool      Warm      Neutral  
  &lt;list&gt;     &lt;list&gt;    &lt;list&gt;    &lt;list&gt;   
1 &lt;chr [10]&gt; &lt;chr [2]&gt; &lt;chr [5]&gt; &lt;chr [3]&gt;
2 &lt;chr [6]&gt;  &lt;chr [1]&gt; &lt;chr [3]&gt; &lt;chr [2]&gt;
</code></pre>

<p><strong>But I was wondering if there was a more succinct way of achieving the same result?</strong> I've tried <code>tidyr::extract()</code> but can't seem to get the regex right:</p>

<pre><code>colours %&gt;% 
  mutate(all = map(all, ~paste(., collapse = ' '))) %&gt;% 
  extract(all, into = c('Cool', 'Warm', 'Neutral'), 
          regex = '(blue|green)|(red|pink|yellow|gold|orange)|(ivory|brown|beige)')
</code></pre>

<p>I'm guessing it's incorrect because the OR statement matches the individual words in each group rather than breaking the string up into three substrings that contain all matched words for each group? <a href=""https://regex101.com/r/0AxrMl/1"" rel=""nofollow noreferrer"">Here is the demo</a>. </p>
","r, dplyr, text-mining, tidyr, data-manipulation","<p>I was pretty convinced that <code>extract</code> wouldn't work, but it does with the right regex. It's really not that much more ""succinct"" than your first solution, but I think it is probably about as succinct as it can get. (If you want to shorten things think about collapsing your colors into a two element character vector, rather than a dataframe with a list column.)</p>

<p>The issue with your regex pattern is your use of <code>|</code>. You want to target collections of words, and not ""x OR y OR z"", which is what your pattern does, and is why you only get one match per row. To create a collection of possible matches use <code>[]</code>. Include <code>*</code> for ""zero or more"" matches. Using your example data above:</p>

<pre class=""lang-r prettyprint-override""><code>library(tidyverse)

colours %&gt;% 
    mutate(all = map(all, str_c, collapse = "" "")) %&gt;% 
    extract(all, c(""cool"", ""warm"", ""neutral""),
            ""([blue green]*) ([red pink yellow gold orange]*) ([ivory brown beige]*)"",
            remove = F # Include the `all` column.
    )

#### OUTPUT ####

# A tibble: 2 x 4
  all       cool       warm                        neutral          
  &lt;list&gt;    &lt;chr&gt;      &lt;chr&gt;                       &lt;chr&gt;            
1 &lt;chr [1]&gt; blue green red pink yellow gold orange ivory brown beige
2 &lt;chr [1]&gt; green      red pink orange             ivory beige      
</code></pre>

<p>The main caveat is that the <strong>color categories</strong> need to be in the right order, i.e. the string has to contain groups of color words in the order <code>cool</code> → <code>warm</code> → <code>neutral</code>. If they're random it won't work. In fact, I don't think <code>extract</code> would work anymore if the color words were random because there's no way to extract individual words and then concatenate them. You also lose your list columns – if that's important to you.</p>

<p>If the order isn't guaranteed, or if there is a possibility that some category words are missing, then you could do something like the following. Using a random sample of category words (note that I drop the list columns so you can see what's happening):</p>

<pre class=""lang-r prettyprint-override""><code>col_rand &lt;- tribble(
    ~all,
    sample(c('blue','green', 'red', 'pink', 'yellow', 'gold', 'orange', 'ivory', 'brown', 'beige'), 5),
    sample(c('green', 'red', 'pink', 'orange', 'ivory', 'beige'), 4)
) %&gt;% 
    mutate(all = map(all, str_c, collapse = "" "") %&gt;% unlist())

#### OUTPUT ####

# A tibble: 2 x 1
  all                       
  &lt;chr&gt;                     
1 blue yellow red beige pink
2 ivory pink beige orange   
</code></pre>

<p>And with the following patterns:</p>

<pre class=""lang-r prettyprint-override""><code>patts &lt;- c(cool = ""blue|green"",
           warm = ""red|pink|yellow|gold|orange"",
           neutral = ""ivory|brown|beige""
           )
</code></pre>

<p>You could do something like the following, which extracts matches and concatenates them, or returns <code>NA</code> if there are no matches:</p>

<pre class=""lang-r prettyprint-override""><code>library(magrittr)

unlist(col_rand$all) %&gt;% 
    map_dfr(function(x) {str_extract_all(x, patts) %&gt;%
            map(function(x) ifelse(length(x) == 0,
                                   NA,
                                   str_c(x, collapse = "" "")
                                   )
                ) %&gt;% 
            bind_cols()}) %&gt;% 
    set_colnames(names(patts)) %&gt;% bind_cols(col_rand, .)

#### OUTPUT ####

# A tibble: 2 x 4
  all                        cool  warm            neutral    
  &lt;chr&gt;                      &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;      
1 blue yellow red beige pink blue  yellow red pink beige      
2 ivory pink beige orange    NA    pink orange     ivory beige
</code></pre>

<blockquote>
  <p>Note that the <code>magrittr</code> library is loaded for the <code>set_colnames</code>. If you load <code>magrittr</code> after <code>tidyverse</code>/<code>tidyr</code> you'll need to use <code>tidyr::extract()</code> above because both libraries have an <code>extract</code> function.</p>
</blockquote>
",1,1,109,2019-05-19 11:06:11,https://stackoverflow.com/questions/56207168/subsetting-a-character-vector-column-into-multiple-columns
Topic label of each document in LDA model using textmineR,"<p>I'm using textmineR to fit a LDA model to documents similar to <a href=""https://cran.r-project.org/web/packages/textmineR/vignettes/c_topic_modeling.html"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/textmineR/vignettes/c_topic_modeling.html</a>. Is it possible to get the topic label for each document in the data set?</p>

<pre><code>&gt;library(textmineR)
&gt;data(nih_sample)
&gt; # create a document term matrix 
&gt; dtm &lt;- CreateDtm(doc_vec = nih_sample$ABSTRACT_TEXT,doc_names = 
 nih_sample$APPLICATION_ID, ngram_window = c(1, 2), stopword_vec = 
 c(stopwords::stopwords(""en""), stopwords::stopwords(source = ""smart"")),lower 
 = TRUE, remove_punctuation = TRUE,remove_numbers = TRUE, verbose = FALSE, 
 cpus = 2) 
 &gt;dtm &lt;- dtm[,colSums(dtm) &gt; 2]
 &gt;set.seed(123)
 &gt; model &lt;- FitLdaModel(dtm = dtm, k = 20,iterations = 200,burnin = 
 180,alpha = 0.1, beta = 0.05, optimize_alpha = TRUE, calc_likelihood = 
 TRUE,calc_coherence = TRUE,calc_r2 = TRUE,cpus = 2)
</code></pre>

<p>then adding the labels to the model: </p>

<pre><code> &gt; model$labels &lt;- LabelTopics(assignments = model$theta &gt; 0.05, dtm = dtm, 
   M = 1)
</code></pre>

<p>now I want the topic labels for each of 100 document in <code>nih_sample$ABSTRACT_TEXT</code> </p>
","r, nlp, text-mining, lda, topic-modeling","<p>Are you looking to label each document by the label of its most prevalent topic? IF so, this is how you could do it:</p>

<pre><code># convert labels to a data frame so we can merge 
label_df &lt;- data.frame(topic = rownames(model$labels), label = model$labels, stringsAsFactors = FALSE)

# get the top topic for each document
top_topics &lt;- apply(model$theta, 1, function(x) names(x)[which.max(x)][1])

# convert the top topics for each document so we can merge
top_topics &lt;- data.frame(document = names(top_topics), top_topic = top_topics, stringsAsFactors = FALSE)

# merge together. Now each document has a label from its top topic
top_topics &lt;- merge(top_topics, label_df, by.x = ""top_topic"", by.y = ""topic"", all.x = TRUE)

</code></pre>

<p>This kind of throws away some information that you'd get from LDA though. One advantage of LDA is that each document can have more than one topic. Another is that we can see how much of each topic is in that document. You can do that here by</p>

<pre><code># set the plot margins to see the labels on the bottom
par(mar = c(8.1,4.1,4.1,2.1))

# barplot the first document's topic distribution with labels
barplot(model$theta[1,], names.arg = model$labels, las = 2)

</code></pre>
",1,0,592,2019-05-19 11:34:35,https://stackoverflow.com/questions/56207379/topic-label-of-each-document-in-lda-model-using-textminer
Passing multiple arguments as a list in R,"<p>I wish to pass a list of arguments as a vector to another command in R. I do not want to repeat the same set of arguments every time.</p>

<p>This is the code that I have to run 6 times for each <code>$full_text</code> column of data frames ranging <code>t1 to t6</code>.</p>

<pre><code>    library(quanteda)

t1t &lt;- tokens(t1$full_text, what = 'word', remove_numbers = TRUE,
                 remove_punct = TRUE,
                 remove_symbols = TRUE,
                 remove_separators = TRUE,
                 remove_twitter = TRUE,
                 remove_hyphens = TRUE,
                 remove_url = TRUE)
t1t &lt;- tokens_tolower(t1t)
t1t &lt;- tokens_select(t1t, stopwords(), selection = ""remove"")
t1t &lt;- unlist(t1t)
t1t &lt;- unique(t1t)
t1t &lt;- as.data.frame(t1t)
t1t &lt;- as.data.frame.matrix(t1t)
</code></pre>

<p>Is there a way to pass a one-time argument.</p>
","r, text-mining, quanteda","<p>As mentioned in the error message <code>tokens</code> expect character vector, corpus or tokens as input. You are passing a dataframe to it. Pass the respective column of text to it instead. </p>

<p>Also <code>tokens</code> can process vectors so you can pass multiple columns together as one vector.</p>

<pre><code>library(quanteda)

tokens(c(t1$colname, t2$colname, t3$colname), what = ""word"", remove_numbers = TRUE, 
  remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, 
  remove_twitter = TRUE, remove_hyphens  =TRUE, remove_url = TRUE)
</code></pre>

<p>Based on the update and taking an example from the help page of <code>?tokens</code> </p>

<pre><code>t1 &lt;- data.frame(full_text = ""#textanalysis is MY &lt;3 4U @myhandle gr8 #stuff :-)"", 
              stringsAsFactors = FALSE)
t2 &lt;- data.frame(full_text = c(""This is $10 in 999 different ways,\n up and down; 
    left and right!"", ""@kenbenoit working: on #quanteda 2day\t4ever, 
    http://textasdata.com?page=123.""), stringsAsFactors = FALSE)
</code></pre>

<p>We can create a function to apply it to all dataframes</p>

<pre><code> complete_function &lt;- function(x) {
   t1t &lt;- tokens(x, what = 'word', remove_numbers = TRUE,
                  remove_punct = TRUE,
                  remove_symbols = TRUE,
                  remove_separators = TRUE,
                  remove_twitter = TRUE,
                  remove_hyphens = TRUE,
                  remove_url = TRUE)
   t1t &lt;- tokens_tolower(t1t)
   t1t &lt;- tokens_select(t1t, stopwords(), selection = ""remove"")
   t1t &lt;- unlist(t1t)
   t1t &lt;- unique(t1t)
   t1t &lt;- as.data.frame(t1t)
   t1t &lt;- as.data.frame.matrix(t1t)
}
</code></pre>

<p>Then use <code>mget</code> to get dataframes <code>t1</code>, <code>t2</code>, <code>t3</code> etc and apply the function to <code>""full_text""</code> column of each dataframe.</p>

<pre><code>lapply(mget(ls(pattern = ""^t\\d+"")), function(x) complete_function(x$full_text))

#$t1
#           t1t
#1 textanalysis
#2           4u
#3     myhandle
#4          gr8
#5        stuff

#$t2
#        t1t
#1 different
#2      ways
#3      left
#4     right
#5 kenbenoit
#6   working
#7  quanteda
#8      2day
#9     4ever
</code></pre>
",1,0,502,2019-06-02 09:47:10,https://stackoverflow.com/questions/56413916/passing-multiple-arguments-as-a-list-in-r
Sentiment Analysis in R using TDM/DTM,"<p>I am trying to apply a sentiment analysis in R with the help of my DTM (document term matrix) or TDM (term document matrix). I could not find any similar topic in the forum and on google. Thus, I created a corpus and from that corpus I generated a dtm/tdm in R. My next step would be to apply the sentiment analysis which I need later for stock prediction via SVM. My give code is that:</p>

<pre><code>    dtm &lt;- DocumentTermMatrix(docs)
    dtm &lt;- removeSparseTerms(dtm, 0.99)
    dtm &lt;- as.data.frame(as.matrix(dtm))

    tdm &lt;- TermDocumentMatrix(docs)
    tdm &lt;- removeSparseTerms(tdm, 0.99)
    tdm &lt;- as.data.frame(as.matrix(tdm))
</code></pre>

<p>I read that it is possible through the tidytext package with the help of the get_sentiments() function. But it was not possible to apply that with a DTM/TDM. How can I run a sentiment analysis for my cleaned filter words which are already stemmed, tokenized etc.? I saw that a lot of people did the sentiment analysis for a hole sentence, but I would like to apply it for my single words in order to see if they are positive, negative, score etc. Many thanks in advance!</p>
","r, text-mining, data-analysis, sentiment-analysis, sentimentr","<p><code>SentimentAnalysis</code> has good integration with <code>tm</code>.</p>

<pre><code>library(tm)
library(SentimentAnalysis)

documents &lt;- c(""Wow, I really like the new light sabers!"",
               ""That book was excellent."",
               ""R is a fantastic language."",
               ""The service in this restaurant was miserable."",
               ""This is neither positive or negative."",
               ""The waiter forget about my dessert -- what poor service!"")

vc &lt;- VCorpus(VectorSource(documents))
dtm &lt;- DocumentTermMatrix(vc)

analyzeSentiment(dtm, 
  rules=list(
    ""SentimentLM""=list(
      ruleSentiment, loadDictionaryLM()
    ),
    ""SentimentQDAP""=list(
      ruleSentiment, loadDictionaryQDAP()
    )
  )
)
#   SentimentLM SentimentQDAP
# 1       0.000     0.1428571
# 2       0.000     0.0000000
# 3       0.000     0.0000000
# 4       0.000     0.0000000
# 5       0.000     0.0000000
# 6      -0.125    -0.2500000
</code></pre>
",1,0,1659,2019-06-09 16:22:02,https://stackoverflow.com/questions/56516317/sentiment-analysis-in-r-using-tdm-dtm
Count the occurrences of words in a string row wise based on existing words in other columns,"<p>I have a data frame that has rows of strings. I want to count the occurrence of words in the rows based on what words appear in the column. How can I achieve this with the code below? <strong><em>Can the below code be modified somehow to achieve this or can anyone suggest another piece of code that doesn't require loops</em></strong>? Thanks so much in advance!</p>

<pre><code>df &lt;- data.frame(
  words = c(""I want want to compare each "",
            ""column to the values in"",
            ""If any word from the list any"",
            ""replace the word in the respective the word want""),
  want= c(""want"", ""want"", ""want"", ""want""),
  word= c(""word"", ""word"", ""word"", ""word""),
  any= c(""any"", ""any"", ""any"", ""any""))

#add 1 for match and 0 for no match
for (i in 2:ncol(df))
{
  for (j in 1:nrow(df))
  {                 
    df[j,i] &lt;- ifelse (grepl (df[j,i] , df$words[j]) %in% ""TRUE"", 1, 0)
  }
  print(i)
}

*'data.frame':  4 obs. of  4 variables:
 $ words: chr  ""I want want to compare each "" ""column to the values in "" ""If any word from the words any"" ""replace the word in the respective the word""
 $ want : chr  ""want"" ""want"" ""want"" ""want""
 $ word : chr  ""word"" ""word"" ""word"" ""word""
 $ any  : chr  ""any"" ""any"" ""any"" ""any""*
</code></pre>

<p><strong>The output should look like below:</strong></p>

<pre><code>    words                                                 want word any
1   I want want to compare each                            2    0   0
2   column to the values in                                0    0   0
3   If any word from the list any                          0    1   2
4   replace the word in the respective the word want       1    2   0
</code></pre>

<p><strong>Current output with existing code looks like this:</strong> </p>

<pre><code>    words                                                 want word any
1   I want want to compare each                            1    0   0
2   column to the values in                                0    0   0
3   If any word from the list any                          0    1   1
4   replace the word in the respective the word want       1    1   0
</code></pre>
","r, nlp, text-mining","<p>With <code>tidyverse</code>(slight violation of syntax by using <code>$</code>):</p>

<pre><code>library(tidyverse)

df %&gt;% 
     mutate_at(vars(-words),function(x) str_count(df$words,x))
                                             words want word any
1                     I want want to compare each     2    0   0
2                          column to the values in    0    0   0
3                    If any word from the list any    0    1   2
4 replace the word in the respective the word want    1    2   0
</code></pre>

<p>Or using <code>modify_at</code> and as suggested by @Sotos we can use <code>.</code> to maintain <code>tidyverse</code> syntax.</p>

<pre><code>df %&gt;% 
      modify_at(2:ncol(.),function(x) str_count(.$words,x))
                                             words want word any
1                     I want want to compare each     2    0   0
2                          column to the values in    0    0   0
3                    If any word from the list any    0    1   2
4 replace the word in the respective the word want    1    2   0
</code></pre>
",3,4,437,2019-06-11 12:34:33,https://stackoverflow.com/questions/56543866/count-the-occurrences-of-words-in-a-string-row-wise-based-on-existing-words-in-o
Stopword not removing one word,"<p>i want to remove 'dan' in filtering process, but didnt work.
here is my code</p>

<pre><code>for row in readCSV:
        _word = []
        username = row[0]
        date = row[1]
        text = row[2].lower()
        text = re.sub(r'@[A-Za-z0-9_]+','',text)
        text = re.sub(r'http\S+', '',text)

        text = replaceMultiple(text, [""!"",""@"",""#"",""$"",""%"",""^"",""&amp;"",""*"",""("",
                                      "")"",""_"",""-"",""+"",""="",""{"",""}"",""["",""]"",
                                      ""\\"",""/"","","",""."",""?"",""&lt;"",""&gt;"","":"","";"",
                                      ""'"",'""',""~"",""0"",""1"",""2"",""3"",""4"",""5"",""6"",""7"",""8"",""9""], '')
        text = text.strip()
        nltk_tokens = nltk.word_tokenize(text)
        stop_words = set(stopwords.words(""indonesian""))
        stop_words_new = ['aku','dan','duh','hhhmmm','thn','nih','tgl',
                          'hai','jazz','bro','broo','msh','']
        new_stopwords_list = stop_words.union(stop_words_new)   
</code></pre>

<p>words in stop_words_new is removed except 'dan'.
why?     </p>
","python, filtering, text-mining","<p>The code should not be working because you are joining a set with a list. Try making the stop_words_new a set instead of a list</p>
",0,1,51,2019-06-21 14:10:13,https://stackoverflow.com/questions/56705074/stopword-not-removing-one-word
Problem with adist function in text comparison,"<p>I have a problem with adist function. Basically I am using the example of the RDocumentation. </p>

<pre><code>attr(adist(c(""kitten"", ""sitting""), counts = TRUE), ""trafos"") here
</code></pre>

<p>However, when I am trying to run added one more word </p>

<pre><code>attr(adist(c(""kitten"", ""sitting"", ""hi""), counts = TRUE), ""trafos"") 
</code></pre>

<p>I am taking these results:</p>

<pre><code>     [,1]      [,2]      [,3]     
[1,] ""MMMMMM""  ""SMMMSMI"" ""SMDDDDI""

[2,] ""SMMMSMD"" ""MMMMMMM"" ""SDDDMDD""

[3,] ""SMIIIID"" ""SIIIMII"" ""MMI"" 
</code></pre>

<p>In the third column, third row, I am taking MMI, but I can not understand why as it is the same word ""hi"". So it has to be MM. (match, match and no insertion)</p>

<p>Reference: <a href=""https://www.rdocumentation.org/packages/utils/versions/3.6.0/topics/adist"" rel=""nofollow noreferrer"">https://www.rdocumentation.org/packages/utils/versions/3.6.0/topics/adist</a></p>

<p>I am using another example: </p>

<pre><code>test &lt;- c('x','hi', 'y','x')

attr(adist(test, y=NULL , counts = TRUE), ""trafos"")
</code></pre>

<p>I am taking these results. But at least the diagonal needs to be M as is the same word. </p>

<pre><code>     [,1] [,2] [,3] [,4]
[1,] ""M""  ""SI"" ""SI"" ""MI""

[2,] ""SD"" ""MM"" ""SD"" ""SD""

[3,] ""SD"" ""SI"" ""MI"" ""SI""

[4,] ""MI"" ""SI"" ""SI"" ""MI""
</code></pre>

<p>I can not understand what it is going wrong.</p>
","r, text, comparison, text-mining, levenshtein-distance","<p>As others have already pointed out it looks like a bug. Using the source from <a href=""https://cran.r-project.org/src/base/R-3/R-3.5.3.tar.gz"" rel=""noreferrer"">https://cran.r-project.org/src/base/R-3/R-3.5.3.tar.gz</a> and looking at lines <strong>429-432</strong> in file <em>src/main/agrep.c</em>, there is code that is reversing a buffer:</p>

<pre><code>/* Now reverse the transcript. */
for(k = 0, l = --m; l &gt;= nz; k++, l--)
  buf[k] = buf[l];
buf[++k] = '\0';
</code></pre>

<p>Walking through what happens in gdb:</p>

<pre><code>$ R -d gdb
GNU gdb (Debian 7.12-6) 7.12.0.20161007-git
...
(gdb) b agrep.c:430
Breakpoint 1 at 0x7222e: file agrep.c, line 430.
(gdb) r
Starting program: /usr/local/lib64/R/bin/exec/R
...
R version 3.5.3 (2019-03-11) -- ""Great Truth""
...
</code></pre>

<p>Then executing the following R code:</p>

<pre><code>&gt; attr(adist(c(""kitten"", ""sitting"", ""hi""), counts = TRUE), ""trafos"")

Breakpoint 1, adist_full (x=0x555557995a48, y=0x555557995a48, costs=0x5555561567a8, opt_counts=TRUE) at agrep.c:430
430                                 for(k = 0, l = --m; l &gt;= nz; k++, l--)
</code></pre>

<p>Continue 8 at the break to reach the last diagonal entry:</p>

<pre><code>(gdb) c 8
Will ignore next 7 crossings of breakpoint 1.  Continuing.

Breakpoint 1, adist_full (x=0x555557995a48, y=0x555557995a48, costs=0x5555561567a8, opt_counts=TRUE) at agrep.c:430
430                                 for(k = 0, l = --m; l &gt;= nz; k++, l--)
</code></pre>

<p>Examine the buffer before reversal:</p>

<pre><code>(gdb) x/6c buf
0x5555566a8da0: 83 'S'  73 'I'  73 'I'  73 'I'  77 'M'  77 'M'
</code></pre>

<p>Stepping through the code shows that <code>buf[0]</code> and <code>buf[1]</code> are copied from the end of the buffer:</p>

<pre><code>(gdb) n
431                                     buf[k] = buf[l];
(gdb) n
430                                 for(k = 0, l = --m; l &gt;= nz; k++, l--)
(gdb) p k
$1 = 0
(gdb) n
431                                     buf[k] = buf[l];
(gdb) n
430                                 for(k = 0, l = --m; l &gt;= nz; k++, l--)
(gdb) p k
$2 = 1
</code></pre>

<p>Exiting the loop k=2:</p>

<pre><code>(gdb) n
432                                 buf[++k] = '\0';
(gdb) p k
$3 = 2
</code></pre>

<p>And ++k is 3:</p>

<pre><code>(gdb) n
433                                 COUNTS(i, j, 0) = nins;
(gdb) p k
$4 = 3
</code></pre>

<p>Examining the reversed buffer shows that <code>buf[2]</code> was not set to NUL:</p>

<pre><code>(gdb) x/6c buf
0x5555566a8da0: 77 'M'  77 'M'  73 'I'  0 '\000'        77 'M'  77 'M'
</code></pre>

<p>This results in:</p>

<pre><code>     [,1]      [,2]      [,3]
[1,] ""MMMMMM""  ""SMMMSMI"" ""SMDDDDI""
[2,] ""SMMMSMD"" ""MMMMMMM"" ""SDDDMDD""
[3,] ""SMIIIID"" ""SIIIMII"" ""MMI""
</code></pre>

<p>Replacing <code>buf[++k] = '\0'</code> with <code>buf[k] = '\0'</code> appears to put the NUL in the correct location:</p>

<pre><code>&gt; attr(adist(c(""kitten"", ""sitting"", ""hi""), counts = TRUE), ""trafos"")

Breakpoint 1, adist_full (x=0x555557995cb8, y=0x555557995cb8, costs=0x5555561567a8, opt_counts=TRUE) at agrep.c:430
430                                 for(k = 0, l = --m; l &gt;= nz; k++, l--)
(gdb) c 8
Will ignore next 7 crossings of breakpoint 1.  Continuing.

Breakpoint 1, adist_full (x=0x555557995cb8, y=0x555557995cb8, costs=0x5555561567a8, opt_counts=TRUE) at agrep.c:430
430                                 for(k = 0, l = --m; l &gt;= nz; k++, l--)
(gdb) x/6c buf
0x5555566a8da0: 83 'S'  73 'I'  73 'I'  73 'I'  77 'M'  77 'M'
(gdb) n
431                                     buf[k] = buf[l];
(gdb) n
430                                 for(k = 0, l = --m; l &gt;= nz; k++, l--)
(gdb) p k
$1 = 0
(gdb) n
431                                     buf[k] = buf[l];
(gdb) n
430                                 for(k = 0, l = --m; l &gt;= nz; k++, l--)
(gdb) p k
$2 = 1
(gdb) n
432                                 buf[k] = '\0';
(gdb) p k
$3 = 2
(gdb) n
433                                 COUNTS(i, j, 0) = nins;
(gdb) p k
$4 = 2
(gdb) x/6c buf
0x5555566a8da0: 77 'M'  77 'M'  0 '\000'        73 'I'  77 'M'  77 'M'
</code></pre>

<p>Resulting in the expected output:</p>

<pre><code>     [,1]      [,2]      [,3]     
[1,] ""MMMMMM""  ""SMMMSMI"" ""SMDDDD"" 
[2,] ""SMMMSMD"" ""MMMMMMM"" ""SDDDMDD""
[3,] ""SMIIII""  ""SIIIMII"" ""MM"" 
</code></pre>

<p>After the fix your second example results in:</p>

<pre><code>&gt; test &lt;- c('x','hi', 'y','x')
&gt; attr(adist(test, y=NULL , counts = TRUE), ""trafos"")
     [,1] [,2] [,3] [,4]
[1,] ""M""  ""SI"" ""S""  ""M""
[2,] ""SD"" ""MM"" ""SD"" ""SD""
[3,] ""S""  ""SI"" ""M""  ""S""
[4,] ""M""  ""SI"" ""S""  ""M""
</code></pre>

<p>The result appears to be consistent with the other attributes for ins, sub and del.</p>

<pre><code>&gt; adist(c('x', 'hi', 'y', 'x'), counts=TRUE)
     [,1] [,2] [,3] [,4]
[1,]    0    2    1    0
[2,]    2    0    2    2
[3,]    1    2    0    1
[4,]    0    2    1    0
attr(,""counts"")
, , ins

     [,1] [,2] [,3] [,4]
[1,]    0    1    0    0
[2,]    0    0    0    0
[3,]    0    1    0    0
[4,]    0    1    0    0

, , del

     [,1] [,2] [,3] [,4]
[1,]    0    0    0    0
[2,]    1    0    1    1
[3,]    0    0    0    0
[4,]    0    0    0    0

, , sub

     [,1] [,2] [,3] [,4]
[1,]    0    1    1    0
[2,]    1    0    1    1
[3,]    1    1    0    1
[4,]    0    1    1    0

attr(,""trafos"")
     [,1] [,2] [,3] [,4]
[1,] ""M""  ""SI"" ""S""  ""M""
[2,] ""SD"" ""MM"" ""SD"" ""SD""
[3,] ""S""  ""SI"" ""M""  ""S""
[4,] ""M""  ""SI"" ""S""  ""M""
</code></pre>
",6,7,325,2019-07-02 14:20:27,https://stackoverflow.com/questions/56854594/problem-with-adist-function-in-text-comparison
Compare the words from a data frame and calculate a matrix with the length of the biggest word for each pair,"<p>I have a data frame with a number of unique words. I want to create code in R, where each word will be compared with all the words and  creates a matrix with the length of the biggest word from each pair. </p>

<p>To be more comprehensive lets consider the follow example. </p>

<pre><code>test &lt;- c(""hello"", ""hi"", ""play"", ""kid"") 
</code></pre>

<p>I want to create a matrix that compares each word in the test and gives me the length of the biggest word. </p>

<p>For the previous example I want to take the below matrix:</p>

<pre><code>       hello  hi play kid
 hello  5     5   5    5

  hi    5     2   4    3

 play   5     4   4    4

  kid   5     3   4    3
</code></pre>

<p>How Can I do it in R?</p>
","r, dataframe, matrix, distance, text-mining","<p>You can do this:</p>

<pre><code>outer(test, test, function(x,y) pmax(nchar(x), nchar(y)))

     [,1] [,2] [,3] [,4]
[1,]    5    5    5    5
[2,]    5    2    4    3
[3,]    5    4    4    4
[4,]    5    3    4    3

</code></pre>

<p>Or even shorter, as suggested by @Ronak Shah</p>

<pre><code>outer(nchar(test), nchar(test), pmax)
</code></pre>
",6,5,76,2019-07-03 11:28:46,https://stackoverflow.com/questions/56869118/compare-the-words-from-a-data-frame-and-calculate-a-matrix-with-the-length-of-th
Gensim Word2Vec Vocabulary: Unclear output,"<p>I'm starting to get familiar with Word2Vec, but I'm struggeling with a problem and coudln't find something similar...
I want to use gensims Word2Vec on an imported PDF document (a book). To import I used PyPDF2 and stored the whole book into a list. Furthermore, I used gensims simple_preprocess in order to preprocess the data. This worked so far, I got the following output:</p>

<pre class=""lang-py prettyprint-override""><code>text=['schottky','diode','semiconductors',...]
</code></pre>

<p>So then I tried to use the Word2Vec:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec
model=Word2Vec(text, size=100, window=5, min_count=5, workers=4)
words=list(model.wv.vocab)

</code></pre>

<p>but the output was like this:</p>

<pre class=""lang-py prettyprint-override""><code>print(words)
['c','h','t','k','d',...]
</code></pre>

<p>I expected also the same words as in the text list and not just some characters. When I tried to find relations between words (e.g. 'schottky' and 'diode') I got the error-message that none of these words is included in the vocabulary.</p>

<p>My first thought was that the import is wrong, but I got the same result with textract instead of PyPDF2.</p>

<p>Does someone know what's the problem? Thanks for your help!</p>

<p>Appendix:</p>

<p>Importing the book</p>

<p>content_text=[]
number_of_inputs=len(os.listdir(path))</p>

<pre><code>    file_to_open=path
open_file=open(file_to_open,'rb')
read_pdf=PyPDF2.PdfFileReader(open_file)
number_of_pages=read_pdf.getNumPages()
page_content=""""
for page_number in range(number_of_pages):
    page = read_pdf.getPage(page_number)
    page_content += page.extractText()
content_text.append(page_content)
</code></pre>
","python, python-3.x, text-mining, gensim, word2vec","<p><code>Word2Vec</code> requires as its <code>sentences</code> parameter a training corpus that is:</p>

<ul>
<li>an iterable sequence (such as a list)</li>
<li>where each item is itself a list of string-tokens</li>
</ul>

<p>If you supply just a list-of-strings, each string is seen as a list-of-one-character-strings, resulting in all the one-letter words you're seeing. </p>

<p>So, use a list-of-lists-of-words, more like:</p>

<pre><code>[
 ['schottky','diode','semiconductors'],
]
</code></pre>

<p>(Note also that you generally won't get interesting <code>Word2Vec</code> results on tiny toy-sized data sets of just a few texts and just dozens to hundreds of words. You need many thousands of unique words, across many dozens of contrasting examples of each word, to induce the useful word-vector arrangements that <code>Word2Vec</code> is known for.)</p>
",1,0,967,2019-07-04 13:49:34,https://stackoverflow.com/questions/56889408/gensim-word2vec-vocabulary-unclear-output
Package ‘Rstem’ is not available (for R version 3.5.1),"<p>I am trying to install the <code>Rstem package</code>, but I am getting the message that there is no version available for the version of <code>R 3.5.1</code>. I'm using the <code>macOs El Captain</code>.</p>

<p>The error is:</p>

<pre><code>&gt; install.packages('Rstem', repos = 'https://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz')
Installing package into ‘/Users/ls_rafael/Library/R/3.5/library’
(as ‘lib’ is unspecified)
Warning in install.packages :
  unable to access index for repository https://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz/src/contrib:
  cannot open URL 'https://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz/src/contrib/PACKAGES'
Warning in install.packages :
  package ‘Rstem’ is not available (for R version 3.5.1)
Warning in install.packages :
  unable to access index for repository https://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz/bin/macosx/el-capitan/contrib/3.5:
  cannot open URL 'https://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz/bin/macosx/el-capitan/contrib/3.5/PACKAGES'
</code></pre>

<p>I already tried the suggested options in this link <a href=""https://stackoverflow.com/questions/9945775/issues-in-installing-rstem-package"">issues in installing Rstem package</a> and also downloading the package locally from the official website <a href=""http://www.omegahat.net/Rstem/"" rel=""nofollow noreferrer"">Rstem Package</a>, but the result is also unsatisfactory.</p>

<p>I'm studying how to do an <code>sentiment analysis</code> with <code>Twitter</code> data. I would like to know if there is any alternative to this package or if there is any <code>trick</code> to install it.</p>
","r, rstudio, data-science, text-mining, sentiment-analysis","<p>RStem package has been removed from the CRAN repository. You can download using the following command:- </p>

<pre><code>install.packages('Rstem', repos = ""http://www.omegahat.net/R"")
</code></pre>

<p>Make sure you have RTools installed on your machine. You can download it from this link - </p>

<p><a href=""https://cran.rstudio.com/bin/windows/Rtools/"" rel=""nofollow noreferrer"">Building R for Windows</a></p>
",2,1,2197,2019-07-08 20:51:40,https://stackoverflow.com/questions/56942406/package-rstem-is-not-available-for-r-version-3-5-1
Is it bad to not remove stopwords when I&#39;ve already set a ceiling on document frequency?,"<p>I'm using <code>sklearn.feature_extraction.text.TfidfVectorizer</code>. I'm processing text. It seems standard to remove stop words. However, it seems to me that if I already have a ceiling on document frequency, meaning I will not include tokens that are in a large percent of the document (eg <code>max_df=0.8</code>), dropping stop words doesn't seem necessary. Theoretically, stop words are words that appear often and should be excluded. This way, we don't have to debate on what to include in our list of stop words, right? It's my understanding that there is disagreement over what words are used often enough that they should be considered stop words, right? For example, scikit-learn includes &quot;whereby&quot; in its built-in list of English stop words.</p>
","python, scikit-learn, nlp, text-mining, text-processing","<p>You are right. It could be the definition of stop words. However, do not forget that one reason to remove the stop words in the first phase, is to prevent counting them and reduce the computation time. </p>

<p>Notice that your intuition behind stop words is correct.  </p>
",1,1,148,2019-07-10 21:59:13,https://stackoverflow.com/questions/56979185/is-it-bad-to-not-remove-stopwords-when-ive-already-set-a-ceiling-on-document-fr
Manually inserting topic-specific stopwords,"<p>I'm using <code>tidytext</code>'s built-in <code>anti_join(get_stopwords())</code> command to clean documents from a data of customer review of tech products, but I found out the output corpus consists primarily of tech specification (e.g., Windows 10, 720p Camera, 380.6 x 258.2 x 22.45 (inches), IntelCore, etc.) and comes with little adjectives and nouns indicative a customer's satisfaction of a product).</p>

<p>Is there any handy ways to compile a list of tech terms to remove (such as those listed earlier) and manually insert it into <code>get_stopwords()</code> or equivalent functions to better identify those non-tech adjectives and nouns in customer reviews?</p>
","dplyr, text-mining, stop-words, tidytext","<p>You can create a data frame of your own stop words. This example uses a novel by HG Wells and two user-specified stop words (thanks to <a href=""https://www.tidytextmining.com/tidytext.html"" rel=""nofollow noreferrer"">https://www.tidytextmining.com/tidytext.html</a>). I don't know if there is a reputable corpus out there of tech-related stop words.</p>

<pre class=""lang-r prettyprint-override""><code>hgwells &lt;- gutenberg_download(35)
my_stop_words &lt;- data.frame(word=c('time','machine')) # list of your stop words
hgwells %&gt;% unnest_tokens(word,text) %&gt;% 
  anti_join(my_stop_words) # removes words 'time' and 'machine'
</code></pre>
",2,0,222,2019-07-16 00:40:55,https://stackoverflow.com/questions/57048701/manually-inserting-topic-specific-stopwords
Removing words containing a certain substring,"<p>So I'm making a function to receive in a word corpus, and then spit out a cleaned product:</p>

<pre><code>corpus_creater &lt;- function(corpus){
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, content_transformer(tolower))
corpus &lt;- tm_map(corpus, removeWords, stopwords(""english""))
corpus &lt;- tm_map(corpus, stripWhitespace)
corpus &lt;- tm_map(corpus, stemDocument)
}
</code></pre>

<p>This works great for the most part, however when I look at the resulting word cloud I generated I notice one thing that stands out:
the word cloud includes random words that have the term ""html"" in them.</p>

<p>I figure I can fix this by simply adding a line in the function that removes any word that contains the substring ""http"", but I can't for the life of get around to doing that, and all the existing answers I've found seem to have to do with replacing a substring, or removing only that substring.</p>

<p>What I want to do is:
if a substring is a part of the word, then remove that entire word.</p>

<p>Word Cloud code I use to generate the word cloud from the corpus:</p>

<pre><code>color_scheme &lt;- brewer.pal(9,""YlGnBu"")
color_scheme &lt;- color_scheme[-(1:4)]
set.seed(103)
wordcloud(words = manu_corpus_final, max.words=200, random.order=FALSE, 
          rot.per=0.35, use.r.layout=FALSE, colors=color_scheme)
</code></pre>
","r, text-mining, tm, corpus","<p>If you are directly getting corpus as input, you could extract the <code>content</code> of corpus using <code>sapply</code> and then remove the document from the corpus which has the required string. </p>

<p>You could integrate it into your function in the following way : </p>

<pre><code>corpus_creater &lt;- function(corpus){
   corpus &lt;- tm_map(corpus, removePunctuation)
   corpus &lt;- tm_map(corpus, content_transformer(tolower))
   corpus &lt;- tm_map(corpus, removeWords, stopwords(""english""))
   corpus &lt;- tm_map(corpus, stripWhitespace)
   corpus &lt;- tm_map(corpus, stemDocument)
   #Added the below line
   corpus &lt;- corpus[-grep(""http"", sapply(corpus, `[`, 1))]
}
</code></pre>
",0,-1,364,2019-07-20 04:17:23,https://stackoverflow.com/questions/57121768/removing-words-containing-a-certain-substring
Extracting links from pdfs in R with a regex,"<p>I am trying to clean a list of pdfs of links. I want to include this in my cleaning function and therefore use regexes. And yes, I spend more time than I like to admit googling and browsing though questions here.
My pdfs are split into lines, so it is not one consecutive string.
I have a piece of code that gives me only one link as result (even though there should be many). 
All other options I tried included a lot of text I want to keep in my dataset. </p>

<p>I have tried multiple options outside my function but they will not run on texts, only on examples. </p>

<p>I want to catch everything from the www to the first white space after all the things that come after the .org or .html or whatever (e.g. /questions/ask/somethingelse</p>

<p>I tried simulating some things</p>

<pre><code>w &lt;- ""www.smthing.org/knowledge/school/principal.\r""
z &lt;- ""www.oecd.de\r""
x &lt;- ""www.bla.pdfwerr\r .irgendwas"" # should not catch that, too many characters after the . 
m &lt;-  ""           www.cognitioninstitute.org/index.php/Publications/ 
 bla test smth 
  .gtw, www.stmthing-else.html.\r""
n &lt;- ""decoy""


l &lt;- list(w,z,x,m,n)

regmatches(l, regexpr(""w{3}\\.[a-z]*\\.[a-z]{2,4}.*?[[:space:]]"", l))
</code></pre>

<p>My current working state also only catches the first occurence in that particular line, instead stopping at the space (line m in my example) and then including the next link as well.</p>
","r, regex, text-mining","<p>You may use</p>

<pre><code>regmatches(l, gregexpr(""w{3}\\.\\S*\\b"", l))
</code></pre>

<p>The <code>gregexpr</code> function will let you extract all occurrences of the pattern.</p>

<p>Note that most users prefer spelling out <code>www</code> instead of using <code>w{3}</code>.</p>

<p><strong>Pattern details</strong></p>

<ul>
<li><code>w{3}</code> - three <code>w</code> chars</li>
<li><code>\\.</code> - a dot</li>
<li><code>\\S*</code> - zero or more non-whitespace chars</li>
<li><code>\\b</code> - word boundary.</li>
</ul>
",2,1,72,2019-07-22 12:15:10,https://stackoverflow.com/questions/57145858/extracting-links-from-pdfs-in-r-with-a-regex
Extracting all different options of references from pdf document in R with regex (multiple options/capture groups?),"<p>I am trying to clean some pdf documents for text analysis. I am trying to grab all the references on the text and remove them. My problem is, that there are so many options to cite...
My documents are split up into single lines. 
I have a working regex, that only captures the standard format </p>

<p>a) Author (year), something .
       ""Author, firstname, someone, else (1996), something: Analysis, Paris.\r""</p>

<p>I want option a, </p>

<p>b) Author (year(character)), something .</p>

<pre><code>  ""Author, firstname, someone, else (1996a), something: Analysis, Paris.\r""
</code></pre>

<p>c) Author (forthcoming), something . </p>

<pre><code>  ""Author, firstname, someone, else (forthcoming), something: Analysis, Paris.\r""
</code></pre>

<p>d) Author/s (eds.) (year), ....</p>

<pre><code>  ""Author, firstname, someone, else (eds.) (1996), something: Analysis, Paris.\r""
</code></pre>

<p>e) Author (n.d.), ....</p>

<pre><code>  ""Author, firstname, someone, else (n.d.), something: Analysis, Paris.\r""
</code></pre>

<p>I have found all of those in my documents... There might be options I have not found yet, so if you have examples or something that grabs that as well, I'm grateful for every it of help.</p>

<p>The working code is the following: </p>

<pre><code>   [ ]*[A-Z].*\([0-9]{4}\),[[:space:]][“A-Z]
</code></pre>

<p>My latest try is this: </p>

<pre><code>   [ ]*[A-Z].*(\([a-z]{3,4}\.?\))?(\([0-9]{4}[a-z]?\))?(\(forthcoming\))?,[[:space:]][“A-Z]
</code></pre>

<p>I tried to make as many pieces optional as I could, but now it grabs too much. </p>

<p>I expect a list of all the References the regex finds, if possible with all the options. At the moment it grabs not enough (first case) or too much (second case).</p>
","r, regex, text-mining","<blockquote>
  <p>My latest try is this:</p>

<pre><code>   [ ]*[A-Z].*(\([a-z]{3,4}\.?\))?(\([0-9]{4}[a-z]?\))?(\(forthcoming\))?,[[:space:]][“A-Z]
</code></pre>
  
  <p>I tried to make as many pieces optional as I could, but now it grabs too much.</p>
</blockquote>

<p>You almost perfectly made up the three option <em>pieces</em>, but since you made them all <em>optional</em>, the expression matches even if none of them is present. Better use the <em>alternation</em> operator <code>|</code>, which requires one subexpression piece to match, i. e. instead of <code>X?Y?Z?</code> write <code>(X|Y|Z)</code>; this makes:</p>

<pre><code>  [ ]*[A-Z].*(\([.a-z]{3,4}\.?\)|\([0-9]{4}[a-z]?\)|\(forthcoming\)),[[:space:]][“A-Z]
</code></pre>

<p>(Note that I changed the first <code>[a-z]</code> to <code>[.a-z]</code> in order to also cover the <code>(n.d.)</code> case.)</p>
",1,1,474,2019-07-22 15:00:54,https://stackoverflow.com/questions/57148763/extracting-all-different-options-of-references-from-pdf-document-in-r-with-regex
How to Implement a (statistical) Thematic Comparison of Texts via Text-Mining?,"<p>I try to compare texts in form of 'text-files' concerning their content.<br>
<strong>e.g.</strong>: I got 100 texts about animals and I want to analyze each text about what animals it discusses.<br>
I am looking for an analysis output like: <code>doc1: 60% cats, 10% rabbits, 10% dogs, 0% elephants, 20% else"", ""doc2: 0% cats, 10% rabbits, 40% dogs, ...</code></p>

<p>I have read a lot about Latent Dirichlet Allocation (and the word-probabilities for each topic) for Text Classification but a completely unsupervised approach seemed not to fit my set of documents.</p>

<p>Trying to implement the LDA-Stuff in Python I understood to prepare the data (tokenizing, lemmatizing/stemming) but I don't get the next steps. Do I have to generate training data for each topic (animal) and how could I implement this? </p>

<p>Also I've seen a tutorial manipulating the topics via the <code>eta-value</code> in <code>gensim</code> but I don't know how I could use this in my favor. </p>

<p>I am grateful for any advice that can lead me to the right direction. Thanks!</p>
","python, text-mining, gensim, text-classification, lda","<p>If you want to use LDA you're done with proprocessing (tokenizing, stemming/lemmatizing), the next step would be to create <code>gensim</code> dictionary and corpus. Assuming your set of documents is a list of lists like <code>[['my', 'first', 'doc'], ['the', 'second', 'doc']]</code> you could proceed like this: </p>

<pre><code>dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
</code></pre>

<p>Then create your model with appropriate number of topics (=animals):</p>

<pre><code>model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_animals)
</code></pre>

<p>You don't need to generate training data yourself at all. After a number of iterations, the LDA algorithm itself performs a quality check on a set of randomly chosen held out test documents which were not used for training. The corresponding measure is often called ""perplexity"" or ""log likelihood"" and will usually be displayed during iteration.</p>

<p>When your model is finally created you can have a look at the words in your topics:</p>

<pre><code>model.print_topics()
</code></pre>

<p>In many cases, you have a collection of documents and a rough idea of the number of topics contained. So the most relevant parameter to play around with is the topic number. 
Since you already know your topic number, you are left to tinker with other parameters. I could imagine that it's difficult to get topics that can be easily attributed to precisely one animal. Keep in mind though that every word appears in every topic, so even ""elephant"" is going to show up in the ""cat"" topic somewhere.</p>

<p>Things to try:</p>

<ul>
<li>Be more rigorous with your stemming/lemmatization to merge more tokens with the same meaning</li>
<li>Check out <code>filter_extremes</code> function of your dictionary to filter for very common or very rare tokens</li>
<li>Apply or expand your stopword filter to get rid of irrelevant terms</li>
<li>Play around with alpha (prevalence of topics per document) and eta (prevalence of tokens per topic) values</li>
</ul>
",0,0,277,2019-07-24 18:26:00,https://stackoverflow.com/questions/57189149/how-to-implement-a-statistical-thematic-comparison-of-texts-via-text-mining
separating an Arabic sentence into words results in a different number of words with different functions,"<p>I am trying to separate one Arabic sentence, Verse 38:1 of Quran, with the <code>tm</code> and <code>tokenizers</code> packages but they split the sentence differently into 3 and 4 words, respectively. Can someone explain (1) why this is and (2) what is the meaning of this difference from NLP  and Arabic-language points of view? Also, is one of them wrong? I am by no means expert in NLP nor Arabic but trying to run the codes.</p>

<p>Here are the codes I tried:</p>

<pre class=""lang-r prettyprint-override""><code>library(tm)
library(tokenizers)
# Verse 38:1
verse&lt;- ""ص والقرآن ذي الذكر""

# This separates into to 3 words by tm library 
a &lt;- colnames(DocumentTermMatrix(Corpus(VectorSource(verse) )))
a
# ""الذكر""   ""ذي""      ""والقرآن""

# This separates into 4 words by 
b &lt;- tokenizers::tokenize_words(verse)
b
# ""ص""       ""والقرآن"" ""ذي""      ""الذكر""  
</code></pre>

<p>I would expect them to be equal but they are different. Can someone explain what is going on here?</p>
","r, nlp, text-mining, arabic, arabic-support","<p>It doesn't have anything to do with NLP or the Arabic language, there are simply some defaults you have to watch out for. <code>DocumentTermMatrix</code> has a number of default parameters that can be changed via <code>control</code>. Run <code>?termFreq</code> to see them all.</p>

<p>One of those defaults is <code>wordLengths</code>:</p>

<blockquote>
  <p>An integer vector of length 2. Words shorter than the minimum word length wordLengths[1] or longer than the maximum word length wordLengths[2] are discarded. Defaults to c(3, Inf), i.e., a minimum word length of 3 characters.</p>
</blockquote>

<p>So, if we run the following we get 3 words because the dropped word has fewer than 3 characters:</p>

<pre class=""lang-r prettyprint-override""><code>dtm &lt;- DocumentTermMatrix(Corpus(VectorSource(verse)))
inspect(dtm)

#### OUTPUT ####

&lt;&lt;DocumentTermMatrix (documents: 1, terms: 3)&gt;&gt;
Non-/sparse entries: 3/0
Sparsity           : 0%
Maximal term length: 7
Weighting          : term frequency (tf)
Sample             :
    Terms
Docs الذكر ذي والقرآن
   1     1  1       1
</code></pre>

<p>To return all words, regardless of length, we need to change <code>c(3, Inf)</code> to <code>c(1, Inf)</code>:</p>

<pre class=""lang-r prettyprint-override""><code>dtm &lt;- DocumentTermMatrix(Corpus(VectorSource(verse)),
                          control = list(wordLengths = c(1, Inf))
                          )
inspect(dtm)

#### OUTPUT ####

&lt;&lt;DocumentTermMatrix (documents: 1, terms: 4)&gt;&gt;
Non-/sparse entries: 4/0
Sparsity           : 0%
Maximal term length: 7
Weighting          : term frequency (tf)
Sample             :
    Terms
Docs الذكر ذي ص والقرآن
   1     1  1 1       1
</code></pre>

<p>The default makes sense because the default language is English, where words with less than three characters are articles, prepositions, etc., but it might make less sense with other languages. Definitely take the time to play around with the other parameters related to different tokenizers, language settings, etc. The current results look pretty good, but you might have to tweak some settings as your text becomes more complicated.</p>
",1,2,610,2019-07-29 00:53:16,https://stackoverflow.com/questions/57246078/separating-an-arabic-sentence-into-words-results-in-a-different-number-of-words
Remove rows with character(0) from a data.frame before proceeding to dtm,"<p>I'm analyzing a data frame of product reviews that contain some empty entries or text written in foreign language. The data also contain some customer attributes which can be used as ""features"" in later analysis.</p>

<p>To begin with, I will first convert the <code>reviews</code> column into <code>DocumentTermMatrix</code> and then convert it to <code>lda</code> format, I then plan to throw in the <code>documents</code> and <code>vocab</code> objects generated from the lda process along with selected columns from the original data frame into <code>stm</code>'s <a href=""https://www.rdocumentation.org/packages/stm/versions/1.3.3/topics/prepDocuments"" rel=""nofollow noreferrer""><code>prepDocuments()</code></a> function such that I can leverage the more versatile estimation functions from that package, using customer attributes as features to predict topic salience.</p>

<p>However, because some of the empty cells, punctuation, and foreign characters might be removed during the pre-processing and thereby creating some <code>character(0)</code> rows in the lda's <code>documents</code> object, making those reviews unable to match their corresponding rows in the original data frame. Eventually, this will prevent me from generating the desired <code>stm</code> object from <code>prepDocuments()</code>.</p>

<p>Methods to remove empty documents certainly exist (such as the methods recommended in this previous <a href=""https://stackoverflow.com/questions/13944252/remove-empty-documents-from-documenttermmatrix-in-r-topicmodels"">thread</a>), but I am wondering if there're ways to also remove the rows correspond to the empty documents from the original data frame such that the number of lda <code>documents</code> and the row dimension of the data frame that will be used as <code>meta</code> in the <code>stm</code> functions are aligned? Will indexing help?</p>

<p>Part of my data is listed at below.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-css lang-css prettyprint-override""><code>df = data.frame(reviews = c(""buenisimoooooo"", ""excelente"", ""excelent"", 
""awesome phone awesome price almost month issue highly use blu manufacturer high speed processor blu iphone"", 
""phone multiple failure poorly touch screen 2 slot sim card work responsible disappoint brand good team shop store wine money unfortunately precaution purchase"", 
""//:"", ""//:"", ""phone work card non sim card description"", ""perfect reliable kinda fast even simple mobile sim digicel never problem far strongly anyone need nice expensive dual sim phone perfect gift love friend"", ""1111111"", ""great bang buck"", ""actually happy little sister really first good great picture late"", 
""good phone good reception home fringe area screen lovely just right size good buy"", ""@#haha"", ""phone verizon contract phone buyer beware"", ""这东西太棒了"", 
""excellent product total satisfaction"", ""dreadful phone home button never screen unresponsive answer call easily month phone test automatically emergency police round supplier network nothing never electricals amazon good buy locally refund"", 
""good phone price fine"", ""phone star battery little soon yes""), 
rating = c(4, 4, 4, 4, 4, 3, 2, 4, 1, 4, 3, 1, 4, 3, 1, 2, 4, 4, 1, 1), 
source = c(""amazon"", ""bestbuy"", ""amazon"", ""newegg"", ""amazon"", 
           ""amazon"", ""zappos"", ""newegg"", ""amazon"", ""amazon"", 
           ""amazon"", ""amazon"", ""amazon"", ""zappos"", ""amazon"", 
           ""amazon"", ""newegg"", ""amazon"", ""amazon"", ""amazon""))</code></pre>
</div>
</div>
</p>
","text-mining, lda, topic-modeling, tidytext","<p>This is a situation where embracing tidy data principles can really offer a nice solution. To start with, ""annotate"" the dataframe you presented with a new column that keeps track of <code>doc_id</code>, which document each word belongs to, and then use <code>unnest_tokens()</code> to transform this to a tidy data structure.</p>



<pre class=""lang-r prettyprint-override""><code>library(tidyverse)
library(tidytext)
library(stm)

df &lt;- tibble(reviews = c(""buenisimoooooo"", ""excelente"", ""excelent"", 
                         ""awesome phone awesome price almost month issue highly use blu manufacturer high speed processor blu iphone"", 
                         ""phone multiple failure poorly touch screen 2 slot sim card work responsible disappoint brand good team shop store wine money unfortunately precaution purchase"", 
                         ""//:"", ""//:"", ""phone work card non sim card description"", ""perfect reliable kinda fast even simple mobile sim digicel never problem far strongly anyone need nice expensive dual sim phone perfect gift love friend"", ""1111111"", ""great bang buck"", ""actually happy little sister really first good great picture late"", 
                         ""good phone good reception home fringe area screen lovely just right size good buy"", ""@#haha"", ""phone verizon contract phone buyer beware"", ""这东西太棒了"", 
                         ""excellent product total satisfaction"", ""dreadful phone home button never screen unresponsive answer call easily month phone test automatically emergency police round supplier network nothing never electricals amazon good buy locally refund"", 
                         ""good phone price fine"", ""phone star battery little soon yes""), 
             rating = c(4, 4, 4, 4, 4, 3, 2, 4, 1, 4, 3, 1, 4, 3, 1, 2, 4, 4, 1, 1), 
             source = c(""amazon"", ""bestbuy"", ""amazon"", ""newegg"", ""amazon"", 
                        ""amazon"", ""zappos"", ""newegg"", ""amazon"", ""amazon"", 
                        ""amazon"", ""amazon"", ""amazon"", ""zappos"", ""amazon"", 
                        ""amazon"", ""newegg"", ""amazon"", ""amazon"", ""amazon""))


tidy_df &lt;- df %&gt;%
  mutate(doc_id = row_number()) %&gt;%
  unnest_tokens(word, reviews)

tidy_df
#&gt; # A tibble: 154 x 4
#&gt;    rating source  doc_id word          
#&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;         
#&gt;  1      4 amazon       1 buenisimoooooo
#&gt;  2      4 bestbuy      2 excelente     
#&gt;  3      4 amazon       3 excelent      
#&gt;  4      4 newegg       4 awesome       
#&gt;  5      4 newegg       4 phone         
#&gt;  6      4 newegg       4 awesome       
#&gt;  7      4 newegg       4 price         
#&gt;  8      4 newegg       4 almost        
#&gt;  9      4 newegg       4 month         
#&gt; 10      4 newegg       4 issue         
#&gt; # … with 144 more rows
</code></pre>

<p>Notice that you still have all the information you had before; all the information is still there, but it is arranged in a different structure. You can fine-tune the tokenization process to fit your particular analysis needs, perhaps dealing with non-English however you need, or keeping/not keeping punctuation, etc. This is where empty documents get thrown out, if appropriate for you.</p>

<p>Next, transform this tidy data structure into a sparse matrix, for use in topic modeling. The columns correspond to the words and the rows correspond to the documents.</p>



<pre class=""lang-r prettyprint-override""><code>sparse_reviews &lt;- tidy_df %&gt;%
  count(doc_id, word) %&gt;%
  cast_sparse(doc_id, word, n)

colnames(sparse_reviews) %&gt;% head()
#&gt; [1] ""buenisimoooooo"" ""excelente""      ""excelent""       ""almost""        
#&gt; [5] ""awesome""        ""blu""
rownames(sparse_reviews) %&gt;% head()
#&gt; [1] ""1"" ""2"" ""3"" ""4"" ""5"" ""8""
</code></pre>

<p>Next, make a dataframe of covariate (i.e. meta) information to use in topic modeling <strong>from the tidy dataset you already have</strong>.</p>



<pre class=""lang-r prettyprint-override""><code>covariates &lt;- tidy_df %&gt;%
  distinct(doc_id, rating, source)

covariates
#&gt; # A tibble: 18 x 3
#&gt;    doc_id rating source 
#&gt;     &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;  
#&gt;  1      1      4 amazon 
#&gt;  2      2      4 bestbuy
#&gt;  3      3      4 amazon 
#&gt;  4      4      4 newegg 
#&gt;  5      5      4 amazon 
#&gt;  6      8      4 newegg 
#&gt;  7      9      1 amazon 
#&gt;  8     10      4 amazon 
#&gt;  9     11      3 amazon 
#&gt; 10     12      1 amazon 
#&gt; 11     13      4 amazon 
#&gt; 12     14      3 zappos 
#&gt; 13     15      1 amazon 
#&gt; 14     16      2 amazon 
#&gt; 15     17      4 newegg 
#&gt; 16     18      4 amazon 
#&gt; 17     19      1 amazon 
#&gt; 18     20      1 amazon
</code></pre>

<p>Now you can put this together into <code>stm()</code>. For example, if you want to train a topic model with the document-level covariates looking at whether topics change a) with source and b) smoothly with rating, you would do something like this:</p>



<pre class=""lang-r prettyprint-override""><code>topic_model &lt;- stm(sparse_reviews, K = 0, init.type = ""Spectral"",
                   prevalence = ~source + s(rating),
                   data = covariates,
                   verbose = FALSE)
</code></pre>

<p><sup>Created on 2019-08-03 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v0.3.0)</sup></p>
",2,0,230,2019-07-30 19:30:16,https://stackoverflow.com/questions/57278841/remove-rows-with-character0-from-a-data-frame-before-proceeding-to-dtm
How to locate specific sequences of words in a sentence efficiently,"<p>The problem is to find a time efficient function that receives as inputs a sentence of words and a list of sequences of varying amounts of words (also known as ngrams) and returns for every sequence a list of indexes indicating where they occur in the sentence, and do it as efficiently as possible for large amounts of sequences.</p>

<p>What I ultimately want is to replace the occurrences of ngrams in the sentence for a concatenation of the words in the sequence by ""_"".</p>

<p>For example if my sequences are [""hello"", ""world""] and [""my"", ""problem""], and the sentence is ""hello world this is my problem can you solve it please?"" the function should return ""hello_world this is my_problem can you solve it please?""</p>

<p>What I did is group the sequences by the amount of words each have and save that in a dictionary where the key is the amount and the value is a list of the sequences of that length.</p>

<p>The variable ngrams is this dictionary:</p>

<pre class=""lang-py prettyprint-override""><code>def replaceNgrams(line, ngrams):
    words = line.split()
    #Iterates backwards in the length of the sequences
    for n in list(ngrams.keys())[::-1]: #O(L*T)
        newWords = []
        if len(words) &gt;= n:
            terms = ngrams[n]
            i = 0
            while i &lt; len(words)+1-n: #O(L*Tn)
                #Gets a sequences of words from the sentences of the same length of the ngrams currently checking
                nwords = words[i:i+n].copy()
                #Checks if that sequence is in my list of sequences
                if nwords in terms: #O(Tn)
                    newWords.append(""_"".join(nwords))
                    i+=n
                else:
                    newWords.append(words[i])
                    i+=1
            newWords += words[i:].copy()
            words = newWords.copy()
    return "" "".join(words)
</code></pre>

<p>This works as desired but I have too many sequences and too many lines to apply this too and this is way too slow for me (it would take a month to finish).</p>
","python, python-3.x, nlp, text-mining","<p>I think this can be achieved by basic string operations. I'll first join all the <code>sequences</code> into single strings and then look for them in the <code>full_text</code>. 
If found, I'll keep track of them in the <code>output_dict</code> with their start and end index. You can use these indices as you require.</p>

<pre class=""lang-py prettyprint-override""><code>
full_text = ""hello world this is my problem can you solve it please?""

sequences = [[""hello"", ""world""], [""my"", ""problem""]]

joined_sequences = ["" "".join(sequence) for sequence in sequences]

def find_location(message, seq):
    if seq in message:
        return message.find(seq)
    else:
        return None

output_dict = {}

for sequence in joined_sequences:
    start_index = find_location(full_text, sequence)
    if start_index &gt; -1:
        output_dict[sequence] = [start_index, start_index+len(sequence)]

print(output_dict)

</code></pre>

<p>This will output:</p>

<pre><code>{'hello world': [0, 11], 'my problem': [20, 30]}
</code></pre>

<p>Then you can do whatever you want with the start and end indices.</p>

<p>If you only need to replace the values with underscores in the middle, you might not even need the indices.</p>

<pre class=""lang-py prettyprint-override""><code>for sequence in joined_sequences:
    if sequence in full_text:
        full_text = full_text.replace(sequence, ""_"".join(sequence.split()))

print(full_text)
</code></pre>

<p>This should give you:</p>

<pre><code>hello_world this is my_problem can you solve it please?
</code></pre>
",2,3,676,2019-07-31 02:31:31,https://stackoverflow.com/questions/57282383/how-to-locate-specific-sequences-of-words-in-a-sentence-efficiently
how can i classify the chapters of a pdf file and analyze the content per chapter?,"<p>I want to classify and analyze chapters and subchapters from a book in PDF format. So count the number of words and examine which word occurs how often and in which chapter. </p>

<p><code>pip install PyPDF2</code></p>

<pre><code>import PyPDF2
from PyPDF2 import PdfFileReader

# Creating a pdf file object
pdf = open('C:/Users/Dominik/Desktop/bsc/pdf1.pdf',""rb"")
# creating pdf reader object
pdf_reader = PyPDF2.PdfFileReader(pdf)
# checking number of pages in a pdf file
print(pdf_reader.numPages)
print(pdf_reader.getDocumentInfo())
# creating a page object
page = pdf_reader.getPage(0)
# finally extracting text from the page
print(page.extractText())
# Extracting entire PDF
for i in range(pdf_reader.getNumPages()):
   page = pdf_reader.getPage(i)
   a = str(1+pdf_reader.getPageNumber(page))
   print (a)
   page_content = page.extractText()
   print (page_content)
# closing the pdf file
pdf.close()
</code></pre>

<p>this code already works. now I want to do more analysis like </p>

<ol>
<li>store each chapter in its own variable and count the number of words. 
In the end, everything should be stored in an excel file.</li>
</ol>
","python, python-3.x, pdf, text-mining, event-log","<p>I tried something similar like this with CVs in PDF format. But all I came to know is the following:</p>
<p>PDF is an unstructured format. It is not possible to extract information from all the PDFs in a structured way. But if you know the structure of the books in PDF format, you can divide the Title of the chapters by using their unique identity like if they are written on BOLD or Italic format. <a href=""https://stackoverflow.com/a/52267853/8748098"">This link</a> can help you extract those information.
You can then traverse through the chapter till it hits the next chapter title.</p>
",3,2,5176,2019-08-10 14:31:58,https://stackoverflow.com/questions/57443019/how-can-i-classify-the-chapters-of-a-pdf-file-and-analyze-the-content-per-chapte
R: Multiple matches when one includes another,"<p>I'm trying to extract ""Maya is ,. nice"" from the string written below ("""" are not part of the string):</p>

<p>""something ransom Maya wants to go for dinner with Shawn Maya is ,. nice""</p>

<p>However, I keep getting ""Maya wants to go for dinner with Shawn Maya is ,. nice"" which is not what I was looking for.</p>

<p>Any insights? I'm using stringr in R</p>
","r, string, pattern-matching, text-mining","<p>An option in <code>base R</code> where we match the word 'Maya' followed by 'is' and other characters (.<code>*) till the end (</code>$<code>) of the string, capture as a group (</code>(...)<code>) and replace with the backreference (</code>\1`) of the captured group</p>
<pre><code>sub(&quot;.*\\b(Maya is .*$)&quot;, &quot;\\1&quot;, str1)
#1] &quot;Maya is ,. nice&quot;
</code></pre>
<hr />
<p>Or with <code>regexpr/regmatches</code></p>
<pre><code>regmatches(str1, regexpr(&quot;Maya is .*$&quot;, str1))
#[1] &quot;Maya is ,. nice&quot;
</code></pre>
<hr />
<p>Or with <code>stringr</code></p>
<pre><code>library(stringr)
str_extract(str1, &quot;Maya is .*$&quot;)
</code></pre>
<p>NOTE: The Op's expected output is already showed in the post</p>
<h3>data</h3>
<pre><code>str1 &lt;- &quot;something ransom Maya wants to go for dinner with Shawn Maya is ,. nice&quot;
</code></pre>
",1,-1,39,2019-08-24 17:17:39,https://stackoverflow.com/questions/57640064/r-multiple-matches-when-one-includes-another
Printing text between a specific word and a closing paranthesis,"<p>I have a text document out of which I want to extract specific Names based on the context. For example, a part of a sentence in the document goes like- ""...TO INTERVIEW VICTIM #1 (!ARIEL B. JOHNSON) ..."". I want to print just the name between the parenthesis but also want it to be searched using ""VICTIM #1"" for context.</p>

<p>I have tried the following code. doc['sentence'] is the dataframe column where all the sentences of the document are stored as rows.</p>

<pre class=""lang-py prettyprint-override""><code>SearchStr = 'VICTIM \#1 (.*?\))'

victim = re.search(SearchStr, str(doc['sentence']))

if victim:
    print(victim.groups())
print(victim)
</code></pre>

<p>It should be printing 'ARIEL B. JOHNSON'</p>
","python, regex, python-3.x, text-mining","<pre><code>match = re.search(r""VICTIM #[0-9]+ \(.*?\)"",STRING)
</code></pre>

<p>returns</p>

<p>'VICTIM #1 (!ARIEL B. JOHNSON)'</p>

<p>which you could split into a dictionary of victim #s and names if you wanted? if that helps?</p>

<pre><code>name = re.sub(r'VICTIM #[0-9]+\s+|!*\(*\)*','',match.group())
num = re.search('[0-9]+',match.group()).group()

v = {num,name}
</code></pre>

<p>this can be adapted to any victim # or name</p>

<p>to apply this over every row in the df use it as a function then apply it over the column:</p>

<pre><code>def victim(STRING):
    match = re.search(r""VICTIM #[0-9]+ \(.*?\)"",STRING)
    name = re.sub(r'VICTIM #[0-9]+\s+|!*\(*\)*','',match.group())
    num = re.search('[0-9]+',match.group()).group()
    v.update({num:name})

doc['sentence'].apply(victim)
</code></pre>

<p>this gives you a dict with all the victim #s/names</p>
",0,0,64,2019-08-26 01:25:06,https://stackoverflow.com/questions/57650901/printing-text-between-a-specific-word-and-a-closing-paranthesis
how to extract title from a pdf documment with R,"<p>I need help to extract information from a pdf file in r
(for example <a href=""https://arxiv.org/pdf/1701.07008.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1701.07008.pdf</a>) </p>

<p>I'm using <code>pdftools</code>, but sometimes <code>pdf_info()</code> doesn't work and in that case I can't manage to do it automatically with <code>pdf_text()</code></p>

<p>NB notice that tabulizer didn't work on my PC.<br>
Here is the treatment I'm doing (Sorry you need to save the pdf and do it with your own path):</p>

<pre><code> info &lt;- pdf_info(paste0(path_folder,""/"",pdf_path))
 title &lt;- c(title,info$keys$Title)
 key &lt;- c(key,info$keys$Keywords)
 auth &lt;- c(auth,info$keys$Author)
 dom &lt;- c(dom,info$keys$Subject)
 metadata &lt;- c(metadata,info$metadata)
</code></pre>

<p>I would like to get title and abstract most of the time.</p>
","r, pdf, text-mining, tabulizer","<p>We will need to make some assumptions about the structure of the pdf we wish to scrape. The code below makes the following assumptions:</p>

<ol>
<li>Title and abstract are on page 1 (fair assumption?)</li>
<li>Title is of height 15</li>
<li>The abstract is between the first occurrence of the word ""Abstract"" and first occurrence of the word ""Introduction""</li>
</ol>

<pre><code>library(tidyverse)
library(pdftools)

data = pdf_data(""~/Desktop/scrape.pdf"")

#Get First page
page_1 = data[[1]]

# Get Title, here we assume its of size 15
title = page_1%&gt;%
  filter(height == 15)%&gt;%
  .$text%&gt;%
  paste0(collapse = "" "")


#Get Abstract
abstract_start = which(page_1$text == ""Abstract."")[1]
introduction_start = which(page_1$text == ""Introduction"")[1]

abstract = page_1$text[abstract_start:(introduction_start-2)]%&gt;%
  paste0(collapse = "" "")

</code></pre>

<p>You can, of course, work off of this and impose stricter constraints for your scraper.</p>
",2,2,1487,2019-09-03 12:06:37,https://stackoverflow.com/questions/57771444/how-to-extract-title-from-a-pdf-documment-with-r
Scikit Learn K-means Clustering &amp; TfidfVectorizer: How to pass top n terms with highest tf-idf score to k-means,"<p>I am clustering the text data based on TFIDF vectorizer. The code works fine. It takes entire TFIDF vectorizer output as input to the K-Means clustering and generate a scatter plots. Instead I would like to send only top n-terms based on TF-IDF scores as input to the k-means clustering. Is there a way to achieve that ? </p>

<pre><code>vect = TfidfVectorizer(ngram_range=(1,3),stop_words='english')

tfidf_matrix = vect.fit_transform(df_doc_wholetext['csv_text'])


'''create k-means model with custom config '''
clustering_model = KMeans(
    n_clusters=num_clusters,
    max_iter=max_iterations,
    precompute_distances=""auto"",
    n_jobs=-1
)

labels = clustering_model.fit_predict(tfidf_matrix)

x = tfidf_matrix.todense()

reduced_data = PCA(n_components=pca_num_components).fit_transform(x)


fig, ax = plt.subplots()
for index, instance in enumerate(reduced_data):        
    pca_comp_1, pca_comp_2 = reduced_data[index]
    color = labels_color_map[labels[index]]
    ax.scatter(pca_comp_1,pca_comp_2, c = color)
plt.show()
</code></pre>
","python, scikit-learn, k-means, text-mining, tfidfvectorizer","<p>use max_features in TfidfVectorizer to consider the top n features </p>

<pre><code>vect = TfidfVectorizer(ngram_range=(1,3),stop_words='english', max_features=n)
</code></pre>

<p>According to scikit-learn's documentation, max_features takes values of int or None (default=None). If not None, TfidfVectorizer builds a vocabulary that only consider the top max_features ordered by term frequency across the corpus.</p>

<p>Here is the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">link</a></p>
",3,2,1600,2019-09-09 14:31:35,https://stackoverflow.com/questions/57856087/scikit-learn-k-means-clustering-tfidfvectorizer-how-to-pass-top-n-terms-with
Corpus object missing text,"<p>Working with 'tm' library in R.</p>

<p>When aplying this code:</p>

<pre><code>abstract &lt;- VectorSource(data$Abstract)
</code></pre>

<p>It works and gives this outcome:</p>

<pre><code>[1] Accurate text...
[2] Accurate text...
[3] Accurate text...
</code></pre>

<p>Then I turn it into a Corpus object so I can work on it for applying some cluster analysis further on.</p>

<pre><code>abstract &lt;- tm::Corpus(tm::VectorSource(data$Abstract)) 
</code></pre>

<p>While checking the raw data, I found out that it saves the lines as NULL when turning it into a data frame with this:</p>

<pre><code>dataframe &lt;- data.frame(text=unlist(sapply(abstract, `[`, ""content"")), 
                        stringsAsFactors=F)
text
1   NA
2   NA
3   NA
4   NA
5   NA
6   NA
7   NA
8   NA
Showing 1 to 8 of 23,600 entries, 1 total columns
</code></pre>

<p>So I don't get how to particularly turn the text into Corpus.</p>
","r, text-mining, tm, corpus","<p>I'll be answearing my own question with this:</p>

<pre><code>writeLines(as.character(abstract[[1]]))
content(abstract[[1]])
</code></pre>

<p>But still don't know how to get the full column as an outcome.</p>
",0,0,80,2019-09-09 21:58:37,https://stackoverflow.com/questions/57861680/corpus-object-missing-text
How to get offset of a matched an n-gram in text,"<p>I would like to match a string ( n-gram) in a text, with a way to get offsets with it :</p>

<p><code>string_to_match = ""many workers are very underpaid"" 
 text = ""The new york times claimed in a report that many workers are very underpaid in some africans countries.""</code></p>

<p>so as result I want to get a tuple like this <code>(""matched"", 44, 75)</code> where 44 is the start and 75 is the end occurrence.</p>

<p>here is the code I have build, but it works only for unigram.</p>

<pre><code>def extract_offsets(line, _len=len):
    words = line.split()
    index = line.index
    offsets = []
    append = offsets.append
    running_offset = 0
    for word in words:
        word_offset = index(word, running_offset)
        word_len = _len(word)
        running_offset = word_offset + word_len
        append((""matched"", word_offset, running_offset - 1))
    return offsets

def get_entities(offsets):
    entities = []
    for elm in offsets:
        if elm[0] == ""string_to_match"": # here string_to_match is only one word
            entities.append(elm)
    return entities

offsets = extract_offsets(text)
entities = get_entities(offsets) # [(""matched"", start, end)]
</code></pre>

<p>any tips to make that work for sequence of strings or n-grams!!</p>
","python, text-mining, string-matching, n-gram","<p>You can <code>re.finditer()</code> and call <code>span()</code> method on the matched object to get the beginning and the ending indices of the matched substring-</p>

<pre><code>def m():
    string_to_match = ""many workers are very underpaid""
    text = ""The new york times claimed in a report that many workers are very underpaid in some africans countries.""
    m = re.finditer(r'%s'%(string_to_match),text)
    for x in m:
        print x.group(0), x.span()     # x.span() will return the beginning and the ending indices of the matched substring as a tuple
</code></pre>
",1,0,208,2019-09-10 10:00:29,https://stackoverflow.com/questions/57868383/how-to-get-offset-of-a-matched-an-n-gram-in-text
The &#39;dictionary&#39; parameter of TermDocumentMatrix does not work in R,"<p>Even though I added the keyword to 'dictionary' as below code, it doesn't extract from the sentence.</p>

<h3>Sample code</h3>

<pre><code>library(tm)

data = c('a', 'a b', 'c')
keyword = c('a', 'b')

data = VectorSource(data)
corpus = VCorpus(data)
tdm = TermDocumentMatrix(corpus, control = list(dictionary = keyword))
</code></pre>

<h3>Result of my code above</h3>

<pre><code>inspect(tdm)

&lt;&lt;TermDocumentMatrix (terms: 2, documents: 3)&gt;&gt;
Non-/sparse entries: 0/6
Sparsity           : 100%
Maximal term length: 1
Weighting          : term frequency (tf)
Sample             :
Docs
Terms 1 2 3
    a 0 0 0
    b 0 0 0
</code></pre>

<h3>Normal result should be as follows:</h3>

<pre><code>Terms 1 2 3
    a 1 1 0
    b 0 1 0
</code></pre>
","r, text-mining, term-document-matrix","<p>You have to pass the minimum word length to <code>termFreq</code> <code>control</code>. </p>

<pre><code>tdm = TermDocumentMatrix(corpus, control = list(dictionary = keyword, wordLengths = c(1, Inf)))
as.matrix(tdm)

     Docs
Terms 1 2 3
    a 1 1 0
    b 0 1 0
</code></pre>
",0,0,143,2019-09-11 07:39:49,https://stackoverflow.com/questions/57884333/the-dictionary-parameter-of-termdocumentmatrix-does-not-work-in-r
R Randomforest undefined column issue,"<p>I am working on a text mining process and using Random forest to classify text to categories.
I am using caret package after processing my text.
I split the data to train and test,
Below is the R code after the same:</p>

<pre><code>traindata &lt;- tdm_df[s,] # training set

testdata &lt;- tdm_df[-s,] # testing set

rf.tfidf &lt;- train(traindata[,c(1:69)], train[,70],
                  method = ""rf"", trControl = ctrl) # train random forest
rf.tfidf
</code></pre>

<p>When I run the last line, I get the below error:</p>

<pre><code>Error in `[.data.frame`(train, , c(1:56)) : undefined columns selected
</code></pre>

<p>Edit 1: next error after correction:
 <code>Error in train[1:5, ] : object of type 'closure' is not subsettable</code></p>

<p>I see the term_sparse is giving me an issue and may be the text mining part, how can i improve my outcome?</p>

<p>Not sure what the issue is.
Please help out!</p>
","r, random-forest, text-mining","<p>Replace <code>train[,70]</code> with <code>traindata[,70]</code>:</p>

<pre><code>rf.tfidf &lt;- train(traindata[,c(1:69)], traindata[,70],
              method = ""rf"", trControl = ctrl)
</code></pre>
",1,0,135,2019-09-13 10:12:30,https://stackoverflow.com/questions/57921647/r-randomforest-undefined-column-issue
Regular expressions extracting multiple product attributes from product description,"<p>I have a set of product descriptions from which i want to extract product attributes through regular expressions.</p>
<p><a href=""https://regex101.com/r/HTTfNR/1"" rel=""nofollow noreferrer"">https://regex101.com/r/HTTfNR/1</a></p>
<h2>Product Description</h2>
<pre><code>BL460c G6 X5550 6G 1P Svr  
BL460c G6 E5540 6G 1P Svr  
BL460c G6 E5540 6G 1P Svr  
BL460c G6 E5530 6G 1P Svr  
BL460c G6 L5520 6G 1P Svr  
BL460c G6 E5520 6G 1P Svr  
BL460c G6 E5506 6G 1P Svr  
BL460c G6 E5502 6G 1P Svr  
BL280c G6 L5520 2G LP 1P Svr  
BL280c G6 E5520 2G 1P Svr  
BL280c G6 E5540 2G 1P Svr  
BL280c G6 E5502 2G 1P Svr  
S-Buy BL460c G6 E5540 8G 2P Svr  
S-Buy BL460c G6 E5530 4G 1P Svr  
S-Buy BL460c G6 E5530 4G 1P Svr  
BL2x220c G6 E5540 24G 2P 250GB Svr  
BL2x220c G6 E5530 24G 2P 250GB Svr  
BL2x220c G6 L5530 24G 2P 250GB Svr  
BL2x220c G6 L5520 24G 2P  
BL2x220c G6 E5640 2x2P 24G Svr  
BL2x220c G6 E5630 2x2P 24G Svr  
BL2x220c G6 L5640 2x2P 24G Svr  
BL2x220c G6 Mod0 Svr  
BL280c G6 X5650 6G 1P Svr  
BL280c G6 E5630 4G 1P Svr  
BL280c G6 L5640 4G 1P Svr  
BL280c G6 E5506 2G 1P Svr  
BL620c G7 E7-2860 32G Svr  
BL620c G7 E7-2850 32G Svr  
BL620c G7 E7-2830 32G Svr  
BL680c G7 E7-4860 64G Svr  
BL680c G7 E7-4860 64G Svr  
BL680c G7 E7-4850 64G Svr  
BL680c G7 E7-4830 64G Svr
BL680c G7 E7 4830 64G Svr   
</code></pre>
<p>I want to solve this using regular expressions.</p>
<p>I have tried this but i am unable to get this working for all use cases of my 1step.</p>
<pre><code>\b(?!\d)([ELX0-9-])\w{1,}
</code></pre>
<p>I want to Extract <code>x5550</code>/<code>E5540</code>/<code>E7-2860</code>/<code>E7-2860</code>/<code>E7 4830</code> as my 1st step. Can someone help me with a code to extract this text from above text?</p>
","regex, text-mining","<p>If the match should start with either <code>E</code> <code>X</code> or <code>L</code> you can omit the negative lookahead <code>(?!\d)</code> and only use those in the character class without the hyphen and the digits. </p>

<p>Then match an optional digit followed by either a space or hyphen.</p>

<pre><code>\b[EXL](?:\d[ -])?\d+(?!\S)
</code></pre>

<p>In parts</p>

<ul>
<li><code>\b[EXL]</code> Word boundary, then match either <code>E</code> <code>X</code> or <code>L</code></li>
<li><code>(?:\d[ -])?</code> Optionally match a digit followed by a space or hyphen</li>
<li><code>\d+</code> Match 1+ digits</li>
<li><code>(?!\S)</code> Negative lookahead, assert what is directly on the right is not a non whitespace character</li>
</ul>

<p><a href=""https://regex101.com/r/DSlJAh/1"" rel=""nofollow noreferrer"">Regex demo</a></p>
",2,0,74,2019-09-16 07:45:34,https://stackoverflow.com/questions/57952408/regular-expressions-extracting-multiple-product-attributes-from-product-descript
Add metadata to VectorSource corpus using &#39;tm&#39; library in R,"<p>I have a csv file and I'm trying to convert it into Corpus to use the tm_map later and the apply some clustering. </p>

<p>I read the file</p>

<pre><code>data &lt;- read.csv(""data.csv"", header = TRUE, sep = "","",stringsAsFactors = FALSE)
</code></pre>

<p>Turn what I need into corpus</p>

<pre><code>corp &lt;- Corpus(VectorSource(data$text)) 
</code></pre>

<p>This is the outcome for the metadata</p>

<pre><code>&gt; meta(corp[[1]])
  author       : character(0)
  datetimestamp: 2019-09-20 20:48:45
  description  : character(0)
  heading      : character(0)
  id           : 1
  language     : en
  origin       : character(0)
</code></pre>

<p>Then I try to add the author info, so I can add the date and title afterwards, like this</p>

<pre><code>&gt; for(i in 1:length(corp)) {
+ corp[[i]]$meta$author == data$author[i]
+ }
</code></pre>

<p>but I keep on getting this</p>

<pre><code>&gt; abstract[[1]]$meta$author
character(0)
&gt; meta(abstract[[1]], tag = 'author')
character(0)
</code></pre>

<p>when</p>

<pre><code>&gt; data$author[1]
[1] ""Juan Vásquez Córdoba""
</code></pre>

<p>How can I add the right metadata info to my Corpus?</p>
","nlp, text-mining, tm, corpus","<p>I found the answear, object corpus must be this way:</p>

<pre><code>corp &lt;- VCorpus(VectorSource(data$text)) 
</code></pre>

<p>With the V everything works out</p>
",0,0,354,2019-09-20 20:57:04,https://stackoverflow.com/questions/58035095/add-metadata-to-vectorsource-corpus-using-tm-library-in-r
Transforming my Counter tuple results (from counter) into string,"<p>I'm trying to store my Counter results in a different way than default (preferably with spacy library) . For this case i need the results to be Word,value + the next separated by a new line. Right now i have the results as tuples ('word' : value). </p>

<p>I've tried storing the results from the counter into string by using str() and by using the join function. However, i end up with all word characters on a newline. </p>

<pre><code>freqtab = Counter(nout)

freqtab2 = freqtab.most_common()
</code></pre>

<p>Where nout is the list with words. So here i got the right results stored from most common to least common (which is what I want but not in the right format)</p>

<p>So instead of getting <code>('Elephant', 12), ('Orca', 9), ('Seal', 2)</code>, I want:</p>

<pre><code>Elephant,12
Orca,9
Seal,2
</code></pre>
","string, counter, text-mining, spacy","<p>To get all the data from the list of tuples joined in a single multiline string you may use</p>

<pre><code>freqtab2 = [('Elephant', 12), ('Orca', 9), ('Seal', 2)]
result = ""\n"".join([""{},{}"".format(wrd, freq) for wrd,freq in freqtab2])
print(result)
</code></pre>

<p>See the <a href=""https://ideone.com/NBoYr8"" rel=""nofollow noreferrer"">Python demo</a>.</p>

<p>The list comprehension <code>[""{},{}"".format(wrd, freq) for wrd,freq in freqtab2]</code> re-formats the <code>freqtab2</code> list of tuples into a list of strings, and <code>""\n"".join(...)</code> makes a single multiline string out of it.</p>
",1,1,52,2019-09-23 11:34:50,https://stackoverflow.com/questions/58061584/transforming-my-counter-tuple-results-from-counter-into-string
How to replace an internal capital letter in a string,"<p>I have a range of strings as follows:</p>

<pre><code>vec&lt;-c(""Peronospora boniNhenrici"",""Cystoseira abiesNmarina"",""Niplommatina rubra"",
 ""Padina sanctaeNcrucis"",""Nachygrapsus NaurusNliguricus"",""Melphidippa borealis"")
</code></pre>

<p>I would like to replace the internal capital ""N"" in the second word for each element with ""-"", so that it would like:</p>

<pre><code>(""Peronospora boni-henrici"",""Cystoseira abies-marina"",""Niplommatina rubra"",
 ""Padina sanctae-crucis,""Nachygrapsus Naurus-liguricus"",""Melphidippa borealis"")
</code></pre>

<p>Any suggestions? I've got the locations using the following:</p>

<pre><code>stri_locate_all(vec,regex = ""[N]"")
</code></pre>

<p>but I'm not sure how to replace the ""N"" if it's internal. When I try to replace the capital letter ""N"" using gsub, it replaces all occurrences of N, rather than only the internal ""N"".</p>
","r, regex, text-mining","<p>We can look for any N's surrounded by <code>\w</code>, which in regex matches any alphanumeric characters or underscores. If that's too broad you could replace <code>\w</code> with <code>[a-zA-Z]</code> to only match letters:</p>

<pre><code>stringr::str_replace_all(vec, ""(\\w)N(\\w)"", ""\\1-\\2"")
</code></pre>
",2,4,134,2019-09-26 00:21:18,https://stackoverflow.com/questions/58108025/how-to-replace-an-internal-capital-letter-in-a-string
Using tm() to mine PDFs for two and three word phrases,"<p>I'm trying to mine a set of PDFs for <em>specific</em> two and three word phrases.
I know this question has been asked under various circumstances and   </p>

<p>This <a href=""https://stackoverflow.com/questions/51597297/how-to-search-for-specific-terms-in-a-dtm"">solution</a> partly works. However, the list does not return strings containing more than one word.  </p>

<p>I've tried the solutions offered in these threads <a href=""https://stackoverflow.com/questions/50222657/find-frequency-of-a-custom-word-in-r-termdocumentmatrix-using-tm-package"">here</a>, <a href=""https://stackoverflow.com/questions/31426501/finding-key-phrases-using-tm-package-in-r"">here</a>, for example (as well as many others). Unfortunately nothing works.</p>

<p>Also, the qdap library won't load and I wasted an hour trying to solve that problem, so <a href=""https://stackoverflow.com/questions/29397235/matching-a-list-of-phrases-to-a-corpus-of-documents-and-returning-phrase-frequen"">this solution</a> won't work either, even though it seems reasonably easy. </p>

<pre><code>library(tm)

data(""crude"")
crude &lt;- as.VCorpus(crude)
crude &lt;- tm_map(crude, content_transformer(tolower))

my_words &lt;- c(""contract"", ""prices"", ""contract prices"", ""diamond"", ""shamrock"", ""diamond shamrock"")

dtm &lt;- DocumentTermMatrix(crude, control=list(dictionary = my_words))

# create data.frame from documenttermmatrix
df1 &lt;- data.frame(docs = dtm$dimnames$Docs, as.matrix(dtm), row.names = NULL)
head(df1)
</code></pre>

<p>As you can see, the output returns ""contract.prices"" instead of ""contract prices"" so I'm looking for a simple solution to this. File 127 includes the phrase 'contract prices' so the table should record at least one instance of this.</p>

<p>I'm also happy to share my actual data, but I'm not sure how to save a small portion of it (it's gigantic). 
So for now I'm using a substitute with the 'crude' data.</p>
","r, pdf, text, text-mining, tm","<p>Here is a way to get what you want using the tm package together with RWeka. You need to create a separate tokenizer function that you plug into the <code>DocumentTermMatrix</code> function. RWeka plays very nicely with <code>tm</code> for this.</p>
<p>If you don't want to install RWeka due to java dependencies, you can use any other package like tidytext or quanteda. If you have need of speed because of the size of your data, I advice using the quanteda package (example below the tm code). Quanteda runs in parallel and with <code>quanteda_options</code> you can specify how many cores you want to use (2 cores are the default).</p>
<h3>note:</h3>
<p>Note that the unigrams and bigrams in your dictionary overlap . In the example used you will see that in text 127 &quot;prices&quot; (3) and &quot;contract prices&quot; (1) will double count the prices.</p>
<pre><code>library(tm)
library(RWeka)

data(&quot;crude&quot;)
crude &lt;- as.VCorpus(crude)
crude &lt;- tm_map(crude, content_transformer(tolower))

my_words &lt;- c(&quot;contract&quot;, &quot;prices&quot;, &quot;contract prices&quot;, &quot;diamond&quot;, &quot;shamrock&quot;, &quot;diamond shamrock&quot;)


# adjust to min = 2 and max = 3 for 2 and 3 word ngrams
RWeka_tokenizer &lt;- function(x) {
  NGramTokenizer(x, Weka_control(min = 1, max = 2)) 
}

dtm &lt;- DocumentTermMatrix(crude, control=list(tokenize = RWeka_tokenizer,
                                              dictionary = my_words))

# create data.frame from documenttermmatrix
df1 &lt;- data.frame(docs = dtm$dimnames$Docs, as.matrix(dtm), row.names = NULL, check.names = FALSE)
</code></pre>
<p>For speed if you have a big corpus quanteda might be better:</p>
<pre><code>library(quanteda)

corp_crude &lt;- corpus(crude)
# adjust ngrams to 2:3 for 2 and 3 word ngrams
toks_crude &lt;- tokens(corp_crude, ngrams = 1:2, concatenator = &quot; &quot;)
toks_crude &lt;- tokens_keep(toks_crude, pattern = dictionary(list(words = my_words)), valuetype = &quot;fixed&quot;)
dfm_crude &lt;- dfm(toks_crude)
df1 &lt;- convert(dfm_crude, to = &quot;data.frame&quot;)
</code></pre>
",2,2,195,2019-09-28 02:51:27,https://stackoverflow.com/questions/58142992/using-tm-to-mine-pdfs-for-two-and-three-word-phrases
Returning Specific String found in text,"<p>I have the following column in a df</p>

<pre><code>c(""I love bananas and apples."",
  ""I hate apples and pears."",
  ""I love to eat food."",
  ""I hate lettuce and bananas"")
</code></pre>

<p>and I have a vector of fruit</p>

<pre><code>fruit &lt;- c(""apples"", ""bananas"", ""pears"")
</code></pre>

<p>I know using str_detect  can return <code>TRUE</code> or <code>FALSE</code> per observation using</p>

<pre><code>str_detect(df$text, paste(fruit, collapse='|'))
</code></pre>

<p>but what I would like is a column that has the variables that matched up, like the following</p>

<pre><code>""I love bananas and apples.""       ""bananas"",""apples""
""I hate apples and pears.""         ""apples"",""pears""
""I love to eat food.""              
""I hate lettuce and bananas.""      ""bananas""
</code></pre>

<p>is there a way to accomplish this? Is this outside the str_detect domain?</p>
","r, regex, string, text-mining, data-manipulation","<pre><code>sapply(v, function(s){
    toString(unlist(lapply(fruit, function(f){
        if(grepl(f, s)) f
    })))
},
USE.NAMES = FALSE)
#[1] ""apples, bananas"" ""apples, pears""   """"                ""bananas""        
</code></pre>
",2,2,431,2019-10-01 17:38:33,https://stackoverflow.com/questions/58189806/returning-specific-string-found-in-text
How to extract string between numbers? (And keep first number in the string?),"<p>I am trying to extract data from a change log using RegEx. Here is an example how the change log is structured:</p>

<pre><code>96545
this is some changes in the ticket
some new version: x.x.22
another change
new version: x.y.2.2
120091
this is some changes in the ticket
some new version: z.z.22
another change
another change
another change
new version: z.y.2.2
120092
...
...
...
</code></pre>

<ul>
<li>Each data point starts with an ID which has a range of 5 to 6 digits. </li>
<li>Moreover there is a variable amount of changes (lines) in the log per ID.</li>
<li>Each data point ends with <code>new version: ***</code>. <code>***</code> is string which is variable for every ID.</li>
</ul>

<p>I was using the <a href=""http://regexstorm.net/tester"" rel=""nofollow noreferrer"">RegExStrom Tester</a> to test my RegEx.</p>

<p>So far I have: <code>^\w{5,6}(.|\n)*?\d{5,6}</code> however the result includes the ID from the next ticket, which I need to avoid.</p>

<p>Result:</p>

<pre><code>96545
this is some changes in the ticket
some new version: x.x.22
another change
new version: x.y.2.2
120091 
</code></pre>

<p>Expected Result:</p>

<pre><code>96545
this is some changes in the ticket
some new version: x.x.22
another change
new version: x.y.2.2
</code></pre>
","python, regex, text, text-mining, changelog","<p>If the problem was that you capture the ID of the next Ticket just use positive look ahead to mach it but not capture it, or consume it:</p>

<pre class=""lang-py prettyprint-override""><code># end of tickets is the end of line that the line after it contains the Id of the next ticket
pattern = r""\d{5,6}[\s\S]*?(?=\n\d{5,6})""

# to extract first ticket info just use search
print(re.search(pattern, text).group(0))

# to extract all tickets info in a list use findall
print(re.findall(pattern, text))

# if the file is to big and you want to extract tickets in lazy mode
for ticket in re.finditer(pattern,text):
    print(ticket.group(0))
</code></pre>
",1,2,1160,2019-10-04 17:36:53,https://stackoverflow.com/questions/58240959/how-to-extract-string-between-numbers-and-keep-first-number-in-the-string
How to obtain TF using only TfidfVectorizer?,"<p>I have a code like this one:</p>

<pre><code> corpus = [
        'This is the first document.',
        'This document is the second document.',
        'And this is the third one.',
        'This document is the fourth document.',
        'And this is the fifth one.',
        'This document is the sixth.',
        'And this is the seventh one document.',
        'This document is the eighth.',
        'And this is the nineth one document.',
        'This document is the second.',
        'And this is the tenth one document.',
    ]

    vectorizer = skln.TfidfVectorizer() 
    X = vectorizer.fit_transform(corpus)
    tfidf_matrix = X.toarray()
    accumulated = [0] * len(vectorizer.get_feature_names())

    for i in range(tfidf_matrix.shape[0]):
        for j in range(len(vectorizer.get_feature_names())):
            accumulated[j] += tfidf_matrix[i][j]

    accumulated = sorted(accumulated)[-CENTRAL_TERMS:]
    print(accumulated)
</code></pre>

<p>where I print the <code>CENTRAL_TERMS</code> words which get the highest tf-idf scores over all the documents of the corpus. </p>

<p>However, I also want to get the <code>MOST_REPEATED_TERMS</code> words over all the documents of the corpus. These are the words which have the highest tf scores. I know I can obtain by simply using <code>CountVectorizer</code>, but I want to use only <code>TfidfVectorizer</code> (in order to not performing first the <code>vectorizer.fit_transform(corpus)</code> for the <code>TfidfVectorizer</code> and then the <code>vectorizer.fit_transform(corpus)</code> for the <code>CountVectorizer</code>. I also know that I could use first <code>CountVectorizer</code> (to obtain tf scores) followed by <code>TfidfTransformer</code> (to obtain tf-idf scores). However, I think that there must be a way to this only using <code>TfidfVectorizer</code>.</p>

<p>Let me know if there is a way to do this (any information is welcome).</p>
","python, scikit-learn, text-mining","<p>By default, <code>TfidfVectorizer</code> does the <code>l2</code> normalization after multiplying the <code>tf</code> and <code>idf</code>. Hence we cannot get the term frequency, when you have the <code>norm='l2'</code>. Refer <a href=""https://github.com/scikit-learn/scikit-learn/blob/16f4ac90f/sklearn/feature_extraction/text.py#L1430-L1435"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/scikit-learn/scikit-learn/blob/16f4ac90f/sklearn/feature_extraction/text.py#L1810-L1812"" rel=""nofollow noreferrer"">here</a></p>

<p>If you can work without norm, then there is a solution. </p>

<pre><code>import scipy.sparse as sp
import pandas as pd 

vectorizer = TfidfVectorizer(norm=None) 
X = vectorizer.fit_transform(corpus)
features = vectorizer.get_feature_names()
n = len(features)
inverse_idf = sp.diags(1/vectorizer.idf_,
                       offsets=0,
                       shape=(n, n),
                       format='csr',
                       dtype=np.float64).toarray()

pd.DataFrame(X*inverse_idf, 
            columns=features)
</code></pre>

<p><a href=""https://i.sstatic.net/QhN5u.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QhN5u.png"" alt=""enter image description here""></a></p>
",2,1,1550,2019-10-05 13:20:40,https://stackoverflow.com/questions/58248692/how-to-obtain-tf-using-only-tfidfvectorizer
How to remove lines with a pattern if the next line matches the same pattern?,"<p>I have a dataframe with a column containing logs for a ticket per row. Here is an example of the log:</p>

<pre><code>99645,
\Submitted',
 '\Modifications made 2015/01/01',
 'x_change0:   --&gt;  info0',
 'y_status1:   --&gt;  info1',
 'z_change2:   --&gt;  info2',
 'y_change3:   --&gt;  info3',
 '\Modifications made 2015/01/03',
 '\Modifications made 2015/01/05',
 '\Modifications made 2015/01/07',
 'w_change0:   --&gt;  info0',
 'a_status1:   --&gt;  info1',
 '\Modifications made 2015/01/07',
.
.
.
</code></pre>

<p>I want to delete all lines, which are not followed by changes. The following regex matches the content I am looking for <a href=""https://regex101.com/r/i53Qzw/2"" rel=""nofollow noreferrer"">RegEx101</a>:</p>

<pre><code>pattern = '(?sm)Modifications\s*((?!Modifications\s*).)*'
re.findall(pattern, dataframe['log'])
</code></pre>

<p>Expected result per cell in dataframe['log']:</p>

<pre><code>Modifications made 2015/01/01',
'change0:   --&gt;  info0',
'change1:   --&gt;  info1',
'change2:   --&gt;  info2',
'change3:   --&gt;  info3',
'Modifications made 2015/01/07',
'change0:   --&gt;  info0',
'change1:   --&gt;  info1',
'
</code></pre>

<p>How do I delete the unwanted lines in the cells? Or how can I replace the string inside the cells with the filtered string?</p>
","python, regex, pandas, text, text-mining","<p>Solved with @Code Maniac's RegEx solution: 
<code>(?sm)Modifications[^,]+,(?:(?!^\s*'\\Modifications).)*\b</code>.</p>

<p>Replace cell string with the following loop:</p>

<pre><code>pattern = r""(?sm)Modifications[^,]+,(?:(?!^\s*'\\Modifications).)*\b""
pattern = re.compile(pattern=pattern)
df['tickethist'] = """"

for i in range(len(df['log'])):
    search = []
    log = df.at[i, 'log']
    for match in pattern.findall(str(log)):
        search.append(match)
    df.at[i, 'tickethist'] = search
</code></pre>
",1,0,90,2019-10-06 14:18:31,https://stackoverflow.com/questions/58258139/how-to-remove-lines-with-a-pattern-if-the-next-line-matches-the-same-pattern
Preserve Hyphenated words in ngrams analysis with tidytext,"<p>I am doing text analysis of biograms. I want to preserve ""complex"" words made of many ""simple"" words linked by hyphens.</p>

<p>for example, if I have the following vector:</p>

<pre><code>Example&lt;- c(""bovine retention-of-placenta sulpha-trimethoprim mineral-vitamin-liquid-mixture)
</code></pre>

<p>***I edited this section to make my the output I need clearer****</p>

<p>I want my biograms in a data.frame of dimensions 3x1 (which is the format you obtain when using <code>unnest_tokens</code> from <code>tidytext</code> :</p>

<pre><code>
1 bovine                   retention-of-placenta
2 retention-of-placenta    sulpha-trimethoprim
3 sulpha-trimethoprim      mineral-vitamin-liquid-mixture
</code></pre>

<p>**** end of the edition****</p>

<p>My problem is that with tidytext, the option token gets used with either ""ngrams"" (which is the sort of analysis I am performing) or with ""regex"" (which is the command I may use to condition on these hyphens)</p>

<p>This is the code I am using at the moment:</p>

<pre><code>spdiag_bigrams&lt;-diagnostics%&gt;%unnest_tokens(bigram, text, token = ""ngrams"", n = 2)

</code></pre>

<p>How can I do both things at the same time?</p>

<p>thank you</p>
","r, regex, text-mining, tidytext","<p><code>unnest_tokens</code> cleans the punctuations. This removes the hyphens between the complex words. </p>

<p>You can either use quanteda or tm for this as these packages do not by default remove the punctuation. The below examples are assuming that you have a data.frame and are working with a corpus. But quanteda's <code>tokens</code> function can work directly on text columns. </p>

<pre><code>example &lt;- c(""bovine retention-of-placenta sulpha-trimethoprim mineral-vitamin-liquid-mixture"")
diagnostics &lt;- data.frame(text = example, stringsAsFactors = FALSE)
</code></pre>

<p>with quanteda:</p>

<pre><code>library(quanteda)

qcorp &lt;- corpus(diagnostics)

bigrams &lt;- tokens_ngrams(tokens(qcorp), n = 2, concatenator = "" "")
qdfm &lt;- dfm(bigrams)
convert(qdfm, ""data.frame"")

  document bovine retention-of-placenta retention-of-placenta sulpha-trimethoprim sulpha-trimethoprim mineral-vitamin-liquid-mixture
1    text1                            1                                         1                                                  1
</code></pre>

<p>just quanteda's <code>tokens_ngrams</code> using the example vector:</p>

<pre><code>tokens_ngrams(tokens(example), n = 2, concatenator = "" "")
tokens from 1 document.
text1 :
[1] ""bovine retention-of-placenta""                       ""retention-of-placenta sulpha-trimethoprim""         
[3] ""sulpha-trimethoprim mineral-vitamin-liquid-mixture""
</code></pre>

<p><strong>Edit:</strong></p>

<p>To get a vector of your terms, you could use one of the other convert options and use the $vocab to get the terms. </p>

<pre><code>convert(qdfm, ""lda"")$vocab
[1] ""bovine retention-of-placenta""                       ""retention-of-placenta sulpha-trimethoprim""         
[3] ""sulpha-trimethoprim mineral-vitamin-liquid-mixture""
</code></pre>

<p><strong>Tidy data.frame:</strong></p>

<p>tidytext has a <code>tidy</code> function to transform data from diverse packages into a tidy form. Both quanteda and tm are included. So after getting the data into a dfm, you can use tidy to get the data into a tibble. After that remove all columns you are not interested in with the usual dplyr syntax. </p>

<pre><code>tidy(qdfm)

# A tibble: 3 x 3
  document term                                               count
  &lt;chr&gt;    &lt;chr&gt;                                              &lt;dbl&gt;
1 text1    bovine retention-of-placenta                           1
2 text1    retention-of-placenta sulpha-trimethoprim              1
3 text1    sulpha-trimethoprim mineral-vitamin-liquid-mixture     1
</code></pre>

<p><strong>end edit:</strong></p>

<p>with tm:</p>

<pre><code>library(tm)
NLPBigramTokenizer &lt;- function(x) {
  unlist(lapply(ngrams(words(x), 2), paste, collapse = "" ""), use.names = FALSE)
}

corp &lt;- VCorpus(VectorSource(example))

dtm &lt;- DocumentTermMatrix(corp, control=list(tokenize = NLPBigramTokenizer))
inspect(dtm)

&lt;&lt;DocumentTermMatrix (documents: 1, terms: 3)&gt;&gt;
Non-/sparse entries: 3/0
Sparsity           : 0%
Maximal term length: 50
Weighting          : term frequency (tf)
Sample             :
    Terms
Docs bovine retention-of-placenta retention-of-placenta sulpha-trimethoprim sulpha-trimethoprim mineral-vitamin-liquid-mixture
   1                            1                                         1                                                  1
</code></pre>
",1,2,970,2019-10-08 06:23:55,https://stackoverflow.com/questions/58281091/preserve-hyphenated-words-in-ngrams-analysis-with-tidytext
How to find phrases that are the same between strings in R,"<p>Let's say i have the following character string</p>

<pre><code>c(""&gt;Date of Procedure 01/09/2018&lt;"", ""&gt;Date of Procedure 01/10/2018&lt;"", 
""&gt;Date of Procedure 03/09/2018&lt;"", ""&gt;Date of Procedure 04/09/2018&lt;"", 
""Patient name Bilbo baggins"", ""Patient name: Jonny Begood"", 
""Patient name Elma Fudd"", ""Patient name Miss Puddleduck"", ""Patient name: Itsy Bitsy"", 
""Patient name: Lala"", ""Type of procedure: OGD"", ""Type of procedure: OGD"", 
""Type of procedure: Colonoscopy"", ""Type of procedure Colonoscopy"", 
""Type of procedure: Colonoscopy"", ""Label 35252"", ""Label 543 "", 
""Label 5254 "", ""Label 23"", ""Label 555555 "", ""Label 54354"")
</code></pre>

<p>I want to extract only those words or phrases that are shared between strings so that the result should be: <code>""Date of Procedure""</code>,<code>""Patient name""</code>,<code>""Type of procedure""</code>,<code>""Label""</code>. I tried using <code>tidytext</code> but it forces me to say the n-gram size I want whereas there may be one, two or three word phrases that are shared.</p>
","r, text-mining","<p>When using <code>unnest_tokens</code> from tidytext with ngrams, you can't specify to remove numbers or other not wanted characters. Switching to the quanteda package will help you in this case. Comments in code for explanations.</p>

<pre><code>library(quanteda)
text &lt;- c(""&gt;Date of Procedure 01/09/2018&lt;"", ""&gt;Date of Procedure 01/10/2018&lt;"", 
          ""&gt;Date of Procedure 03/09/2018&lt;"", ""&gt;Date of Procedure 04/09/2018&lt;"", 
          ""Patient name Bilbo baggins"", ""Patient name: Jonny Begood"", 
          ""Patient name Elma Fudd"", ""Patient name Miss Puddleduck"", ""Patient name: Itsy Bitsy"", 
          ""Patient name: Lala"", ""Type of procedure: OGD"", ""Type of procedure: OGD"", 
          ""Type of procedure: Colonoscopy"", ""Type of procedure Colonoscopy"", 
          ""Type of procedure: Colonoscopy"", ""Label 35252"", ""Label 543 "", 
          ""Label 5254 "", ""Label 23"", ""Label 555555 "", ""Label 54354"")

# tokenize text and remove punctuation and numbers 
toks &lt;- tokens(text, remove_numbers = TRUE, remove_punct = TRUE)

# create 1, 2 and 3 ngrams.
toks_grams &lt;- tokens_ngrams(toks, n = 1:3)

# transform into a document feature matrix (step can be included in next one)    
my_dfm &lt;- dfm(toks_grams)

# turn the terms into a frequency table and filter out the ones that have a count of 1
# depending on needs you can filter out words ngrams or choose a higher occuring frequency to filter on.
freqs &lt;- textstat_frequency(my_dfm)
freqs[freqs$frequency &gt; 1, ]


                    feature frequency rank docfreq group
1                        of         9    1       9   all
2                 procedure         9    1       9   all
3              of_procedure         9    1       9   all
4                   patient         6    4       6   all
5                      name         6    4       6   all
6              patient_name         6    4       6   all
7                     label         6    4       6   all
8                      type         5    8       5   all
9                   type_of         5    8       5   all
10        type_of_procedure         5    8       5   all
11                     date         4   11       4   all
12                  date_of         4   11       4   all
13        date_of_procedure         4   11       4   all
14              colonoscopy         3   14       3   all
15    procedure_colonoscopy         3   14       3   all
16 of_procedure_colonoscopy         3   14       3   all
17                      ogd         2   17       2   all
18            procedure_ogd         2   17       2   all
19         of_procedure_ogd         2   17       2   all
</code></pre>
",1,2,67,2019-10-15 17:52:17,https://stackoverflow.com/questions/58400164/how-to-find-phrases-that-are-the-same-between-strings-in-r
How to do tokenizing by n-gram for pdf file in R,"<p>I want to tokenize a pdf document by ngrams in R.
I tried to follow the instructions here 
 at <a href=""https://www.tidytextmining.com/ngrams.html"" rel=""nofollow noreferrer"">https://www.tidytextmining.com/ngrams.html</a>,
but get stuck with the <code>unnest_tokens()</code> function.</p>

<pre><code>library(tm)
library(dplyr)
library(tidytext)
library(tidyverse)


filedoc &lt;- ""Document2019.pdf""
cname &lt;- file.path(filedoc)
docs &lt;- Corpus(URISource(cname), readerControl=list(reader=readPDF, language = ""en"")) 

docs_bigrams &lt;- docs %&gt;%
  unnest_tokens(bigram, text, token = ""ngrams"", n = 2)

</code></pre>

<p>I keep getting this error message:
<code>Error in UseMethod(""unnest_tokens_"") : no applicable method for 'unnest_tokens_' applied to an object of class ""c('VCorpus', 'Corpus')""</code></p>

<p>Is there anything I need to do before running the unnest_tokens function?
Thank you.</p>
","r, tokenize, text-mining, tidytext","<p>I go with @phiver's suggestion, using tidy function, and repost the answer here so that this thread can be closed/answered.</p>

<blockquote>
  <p>""use the tidy function before unnest_tokens. Tidytext uses the tidy function to transform from tm objects to tibbles.""</p>
</blockquote>

<p>Thanks!</p>
",0,1,459,2019-10-18 21:17:13,https://stackoverflow.com/questions/58458313/how-to-do-tokenizing-by-n-gram-for-pdf-file-in-r
Perplexity issues using text2vec,"<p>I am ussing text2vec on 230k docs, as I always mention. I am trying to find the best topic number for my document term matrix by using perplexity. When I use it one by one it works perfectly fine, but when I try to use a loop to get it for a range from 2 to 25 it doesn't work and I can't tell why, could  someone please tell me what is wrong?</p>

<pre><code>##Using perplexity for hold out set
t1 &lt;- Sys.time()
perplex &lt;- c()
for (i in 2:25){

  set.seed(17)

    lda_model &lt;- LDA$new(n_topics = i)
    doc_topic_distr &lt;- lda_model$fit_transform(x = dtm,  progressbar = F)

    perplex[i]  &lt;- text2vec::perplexity(sample.dtm, topic_word_distribution = 
    lda_model$topic_word_distribution, doc_topic_distribution = new_doc_topic_distr) 
}
print(difftime(Sys.time(), t1, units = 'sec'))

</code></pre>

<pre><code>INFO [2019-10-23 13:01:43] early stopping at 80 iteration
INFO [2019-10-23 13:01:45] early stopping at 20 iteration
INFO [2019-10-23 13:01:53] early stopping at 70 iteration
INFO [2019-10-23 13:01:55] early stopping at 20 iteration
Error in text2vec::perplexity(sample.dtm, topic_word_distribution = lda_model$topic_word_distribution,  : 
  nrow(topic_word_distribution) == ncol(doc_topic_distribution) is not TRUE
</code></pre>
","for-loop, text-mining, text2vec","<p>This is because you need to re-calculate <code>new_doc_topic_distr</code> inside the loop</p>
",0,0,138,2019-10-23 18:12:24,https://stackoverflow.com/questions/58528772/perplexity-issues-using-text2vec
Error while finding topics quantity on Latent Dirichlet Allocation model using ldatuning library,"<p>This is the outcome error and I can tell this is because there is at least one document without some term, but I don't get why and how I can solve it.</p>

<pre><code>prep_fun = function(x) {
  x %&gt;% 
    str_to_lower                         %&gt;%   #make text lower case
    str_replace_all(""[^[:alpha:]]"", "" "") %&gt;%   #remove non-alpha symbols - chao punctuation y #
    str_replace_all(""\\s+"", "" "")         %&gt;%   #collapse multiple spaces 
    str_replace_all(""\\W*\\b\\w\\b\\W*"", "" "")  #Remuevo letras individuales
}
tok_fun &lt;- function(x) {
  tokens &lt;- word_tokenizer(x)
  textstem::lemmatize_words(tokens)
}
it_patentes &lt;- itoken(data$Abstract, 
                      preprocessor = prep_fun, 
                      tokenizer = tok_fun, 
                      ids = data$id,
                      progressbar = F)
vocab &lt;- create_vocabulary(it_patentes, ngram = c(ngram_min = 1L, ngram_max = 3L), 
                           stopwords = tm::stopwords(""english""))
pruned_vocab &lt;- prune_vocabulary(vocab, term_count_min =  max(vocab$term_count)*.01, 
                                 doc_proportion_min = 0.001)   
vectorizer &lt;- vocab_vectorizer(pruned_vocab) 
dtm &lt;- create_dtm(it_patentes, vectorizer,type = ""dgTMatrix"", progressbar = FALSE)   

&gt; #Plot the metrics to get number of topics 
&gt; t1 &lt;- Sys.time()
&gt; tunes &lt;- FindTopicsNumber(
+   dtm = dtm,
+   topics = c(2:25),
+   metrics = c(""Griffiths2004"", ""CaoJuan2009"", ""Arun2010""),
+   method = ""Gibbs"",
+   control = list(seed = 17),
+   mc.cores = 4L,
+   verbose = TRUE
+ )
fit models...Error in checkForRemoteErrors(val) : 
  4 nodes produced errors; first error: Each row of the input matrix needs to contain at least one non-zero entry
&gt; print(difftime(Sys.time(), t1, units = 'sec'))
Time difference of 9.155343 secs
&gt; FindTopicsNumber_plot(tunes)
Error in base::subset(values, select = 2:ncol(values)) : 
  object 'tunes' not found
</code></pre>

<p>Even though I know ldatuning is made for topicmodels, I don't think there might be a huge difference to get a number to start testing, is there?</p>
","text-mining, lda, text2vec","<p><code>ldatuning</code> expects input <code>dtm</code> matrix in a different format (format from <code>topicmodels</code> package). You need to convert <code>dtm</code> (sparse matrix from Matrix package) to a format which <code>ldatuning</code> can understand</p>
",0,0,255,2019-10-24 16:58:38,https://stackoverflow.com/questions/58545968/error-while-finding-topics-quantity-on-latent-dirichlet-allocation-model-using-l
I can&#39;t create tf-idf matrix for my test data using text2vec,"<p>I'm following <a href=""http://text2vec.org/vectorization.html#normalization"" rel=""nofollow noreferrer"">this tutorial</a> and doing it as I did the training set, but it keeps saying the same thing. Someone know what's wrong with this?</p>

<pre><code>&gt; #Construct sample document-term matrix con el vectorizer inicial
&gt; sample.it &lt;- itoken(rawsample$Abstract, 
+                     preprocessor = prep_fun, 
+                     tokenizer = tok_fun, 
+                     ids = rawsample$id,
+                     progressbar = F) 
&gt; sample.dtm &lt;- create_dtm (sample.it, vectorizer, vtype = ""dgTMatrix"", progressbar = FALSE)
&gt; sample.tfidf &lt;- TfIdf$new() #define tfidf model
&gt; sample.tfidf &lt;- fit_transform(sample.dtm, tfidf)
Error in fit_transform.Matrix(sample.dtm, tfidf) : 
  inherits(model, ""mlapiTransformation"") is not TRUE
&gt; sample.tfidf  = create_dtm(sample.it, vectorizer, vtype = ""dgTMatrix"", progressbar = FALSE) %&gt;% 
+   transform(tfidf)
Error in transform.Matrix(., tfidf) : 
  inherits(model, ""mlapiTransformation"") is not TRUE

</code></pre>
","text, text-mining, tf-idf, text2vec","<pre><code>sample.tfidf &lt;- TfIdf$new() #define tfidf model
sample.tfidf &lt;- fit_transform(sample.dtm, tfidf)
</code></pre>

<p>Where do you define <code>tfidf</code> ? May be you need something like:</p>

<pre><code>model =  TfIdf$new() #define tfidf model
sample.tfidf = fit_transform(sample.dtm, model)

</code></pre>
",1,0,136,2019-10-25 17:18:35,https://stackoverflow.com/questions/58563022/i-cant-create-tf-idf-matrix-for-my-test-data-using-text2vec
Detect part of a string in R (not exact match),"<p>Consider the following dataset :</p>

<pre><code>a &lt;- c(""my house"", ""green"", ""the cat is"", ""a girl"")
b &lt;- c(""my beautiful house is cool"", ""the apple is green"", ""I m looking at the cat that is sleeping"", ""a boy"")
c &lt;- c(""T"", ""T"", ""T"", ""F"")
df &lt;- data.frame(string1=a, string2=b, returns=c)
</code></pre>

<p>I m trying to detect string1 in string2 BUT my goal is to not only detect exact matching. I m looking for a way to detect the presence of string1 words in string2, whatever the order words appear. As an example, the string ""my beautiful house is cool"" should return TRUE when searching for ""my house"".</p>

<p>I have tried to illustrate the expected behaviour of the script in the ""return"" column of above the example dataset.</p>

<p>I have tried grepl() and str_detect() functions but it only works with exact match. Can you please help ? Thanks in advance</p>
","r, string, text-mining, stringr, grepl","<p>The trick here is to not use str_detect as is but to first split the <code>search_words</code> into individual words. This is done in <code>strsplit()</code> below. We then pass this into <code>str_detect</code> to check if <strong>all</strong> words are matched.</p>

<pre><code>library(stringr)
search_words &lt;- c(""my house"", ""green"", ""the cat is"", ""a girl"")
words &lt;- c(""my beautiful house is cool"", ""the apple is green"", ""I m looking at the cat that is sleeping"", ""a boy"")

patterns &lt;- strsplit(search_words,"" "")

mapply(function(word,string) all(str_detect(word,string)),words,patterns)

</code></pre>
",2,2,1954,2019-10-27 15:50:13,https://stackoverflow.com/questions/58580948/detect-part-of-a-string-in-r-not-exact-match
Extracting N number of matches from a text string in R?,"<p>I am using stringr in R, and I have a string of text that lists titles of news articles. I want to extract these titles, but only the first N-number of titles that appear. In my example string of text, I have three article titles, but I only want to extract the first two. </p>

<p>How can I tell str_extract to only collect the first 2 titles? Thank you.</p>

<p>Here is my current code with the example texts.</p>

<pre><code>library(stringr)
</code></pre>

<p>Here is the example text. </p>

<pre><code>texting &lt;- (""Time: Friday, September 14, 2018 4:34:00 PM EDT\r\nJob Number: 73591483\r\nDocuments (100)\r\n 1. U.S. Stocks Rebound Slightly After Tech-Driven Slump\r\n   Client/Matter: -None-\r\n   Search Terms: trade war or US-China trade or china tariff and not dealbook\r\n   Search Type: Terms and Connectors\r\n   Narrowed by:\r\n             Content Type                         Narrowed by\r\n             News                                 Sources: The New York Times; Content Type: News;\r\n                                                  Timeline: Jan 01, 2018 to Dec 31, 2018\r\n 2. Shifting Strategy on Tariffs\r\n   Client/Matter: -None-\r\n   Search Terms: trade war or US-China trade or china tariff and not dealbook\r\n 100. Example"")
</code></pre>

<pre><code>titles.1 &lt;- str_extract_all(texting, ""\\d+\\.\\s.+"")
titles.1
</code></pre>

<p>The current code brings back all three matches in the string: </p>

<pre><code>[[1]]

[1] ""1. U.S. Stocks Rebound Slightly After Tech-Driven Slump""

[2] ""2. Shifting Strategy on Tariffs""                        

[3] ""100. Example""
</code></pre>

<p>I only want it to collect the first two matches.</p>
","r, text, text-mining, stringr","<p>You can use the option <code>simplify = TRUE</code> to get a vector as result, rather than a list. Then, just pick the first N elements from the vector</p>

<pre><code>titles.1 &lt;- str_extract_all(texting, ""\\d+\\.\\s.+"", simplify = TRUE)[1:2]
</code></pre>
",3,-1,51,2019-11-05 14:23:22,https://stackoverflow.com/questions/58713385/extracting-n-number-of-matches-from-a-text-string-in-r
How to extract a subset from a text file and store it in a separate file?,"<p>I am currently trying to extract information from a text file using Python. I want to extract a subset from the file and store it in a separate file from everywhere it occurs in the text file. To give you an idea of what my file looks like, here is a sample:</p>

<pre class=""lang-none prettyprint-override""><code>C"",""datatype"":""double"",""value"":25.71,""measurement"":""Temperature"",""timestamp"":1573039331258250},
{""unit"":""%RH"",""datatype"":""double"",""value"":66.09,""measurement"":""Humidity"",""timestamp"":1573039331258250}]
</code></pre>

<p>Here, I want to extract <code>""value""</code> and the corresponding number beside it. I have tried various techniques but have been unsuccessful. I tried to iterate through the file and stop at where I have <code>""value""</code> but that did not work.</p>

<p>Here is a sample of the code:</p>

<pre><code>with open(""DOTemp.txt"") as openfile:
    for line in openfile:
        for part in line.split():
            if ""value"" in part:
                print(part)
</code></pre>
","python, text-mining","<p>A simple solution to return the value marked by the ""value"" key:</p>

<pre><code>with open(""DOTemp.txt"") as openfile:
    for line in openfile:
        line = line.replace('""', '')
        for part in line.split(','):
            if ""value"" in part:
                print(part.split(':')[1])
</code></pre>

<p>Note that by default <code>str.split()</code> splits on whitespace. In the last line, if we printed element zero of the list it would just be ""value"". If you wish to use this as an int or float, simply cast it as such and return it.</p>
",1,0,1280,2019-11-08 05:21:47,https://stackoverflow.com/questions/58760827/how-to-extract-a-subset-from-a-text-file-and-store-it-in-a-separate-file
text mining python keys,"<p>I have a multiline file, tab separated, which might include (or not) some keywords int the second column,</p>
<blockquote>
<p>Place1______________fish</p>
<p>Place2______________fishing someting</p>
<p>Placexx_____________something missing</p>
<p>Place_somwhere______something else missing</p>
<p>EHDN_______________fishing something</p>
<p>HDGFE______________looking for something</p>
</blockquote>
<p>(the lines are uggly but i couldn't manage to make the data look like a table)</p>
<p>I would need to, each time that the line contains 'something missing', to add an annotation at the end of the line, like &quot;ACTION NEEDED THERE&quot;;</p>
<p>I've tried someting like:</p>
<pre><code>pattern=&quot;something missing&quot;
OUT=open('/Users/user/output.tab','w')

for line in file:
  field=line.split('\t')
  if pattern in field[1]:
    action = ';'.join(&quot;ACTION NEEDED&quot;)
    OUT.write(action.strip().replace('&quot;',' '))
</code></pre>
<p>or findall re function without success...</p>
<p>Can you help me please ? Should re.findall work here ?
I've tried pattern=re.findall(&quot;something missing&quot;, line) but it's not working....
Sorry for asking that but i did not manage to find the right answer in the previous posts.....
Many Thanks in advance !</p>
","python, regex, text-mining","<p>Change this,</p>

<pre><code>if pattern in field[1]:
</code></pre>

<p>to</p>

<pre><code>if any([True for word in pattern.split() if word in line]):
</code></pre>

<p>You can add the annotation by,</p>

<pre><code>line+"" ""+your_annotation
</code></pre>
",1,0,61,2019-11-15 10:27:34,https://stackoverflow.com/questions/58875047/text-mining-python-keys
list of vectors in R - extract an element of the vectors,"<p>I have a list which contains some texts. So each element of the list is a text. And a text is a vector of words. So I have a list of vectors.
I am doing some text-mining on that.
Now, I'm trying to extract the words that are after the word ""no"". I transformed my vectors, so now they are vectors of two words. Such as :
<code>list(c(""want friend"", ""friend funny"", ""funny nice"", ""nice glad"", ""glad become"", ""become no"", ""no more"", ""more guys""), c(""no comfort"", ""comfort written"", ""written conduct"",""conduct prevent"", ""prevent manners"", ""matters no"", ""no one"", ""one want"", ""want be"", ""be fired""))</code></p>

<p>My aim is to have a list of vectors which will be like :
<code>list(c(""more""), c(""comfort"", ""one""))</code>
So I would be able to see for a text i the vectoe of results by liste[i].</p>

<p>So I have a formula to extract the word after ""no"" (in the first vector it will be ""more"").
But when I have several ""no"" in my text it doesn't work.</p>

<p>Here is my code :</p>

<pre><code>liste_negation &lt;- vector(length = length(data))
for (i in 1:length(data)){
  for (j in 1:length(data[[i]])){
    if (startsWith((data[[i]])[[j]], 'no') == TRUE){
      liste_neg[i] &lt;- c(liste_neg[i], tail(strsplit((data[[i]])[[j]],split="" "")[[1]],1))
    } else{
      liste_neg[i] &lt;- c(liste_neg[i])
    }
    liste_negation[[i]] &lt;- c(liste_neg[[i]])
  }
}
</code></pre>

<p>That one works for a vector when there is only one ""no"" :</p>

<pre><code>data &lt;- list(c(""want friend"", ""friend funny"", ""funny nice"", ""nice glad"", ""glad become"", ""become no"", ""no more"", ""more guys""), c(""no comfort"", ""comfort written"", ""written conduct"",""conduct prevent"", ""prevent manners"", ""matters no"", ""no one"", ""one want"", ""want be"", ""be fired""))
data

liste_neg &lt;- c()
liste_negation &lt;- vector(length = length(data))
if (startsWith((data[[1]])[[9]], 'no') == TRUE){
  liste_neg[1] &lt;- c(liste_neg[1], tail(strsplit((data[[1]])[[9]],split="" "")[[1]],1))
}

liste_negation[[1]] &lt;- c(liste_neg[[1]])
</code></pre>

<p>But if I try to adapt it with a loop to see each element of the vector, and there are more than one ""no"" in the text, it doesn't work.</p>

<p>Code :</p>

<pre><code>liste_neg &lt;- c()
liste_negation &lt;- vector(length = length(data))
for (j in 1:length(data[[2]])){
  if (startsWith((data[[2]])[[j]], 'no') == TRUE){
    liste_neg[2] &lt;- append(liste_neg[2], tail(strsplit((data[[2]])[[j]],split="" "")[[1]],1))
  }
}
liste_neg
liste_negation[[2]] &lt;- c(liste_neg[[2]])
liste_negation
</code></pre>

<p>Warning message :</p>

<pre><code>Warning message:
In liste_neg[2] &lt;- append(liste_neg[2], tail(strsplit((data[[2]])[[j]],  :
  number of items to replace is not a multiple of replacement length
&gt; liste_neg
[1] NA        ""comfort""
&gt; liste_negation[[2]] &lt;- c(liste_neg[[2]])
&gt; liste_negation
[1] ""FALSE""   ""comfort""
</code></pre>

<p>As you can see I have only the second word which is there.</p>

<p>I tried many things and I tried to split the code and run it and work on it piece by piece, but after spending all the morning on it I haven't found a solution..</p>

<p>Did someone have an idea top help me ?</p>

<p>Thank you in advance (and sorry for my english, I'm french ^^')</p>
","r, list, vector, text-mining","<pre><code>lapply(data, function(x) substr(x[startsWith(x, ""no"")], 4, 1000))


[[1]]
[1] ""more""

[[2]]
[1] ""comfort"" ""one""    
</code></pre>
",3,3,713,2019-11-22 11:12:25,https://stackoverflow.com/questions/58993053/list-of-vectors-in-r-extract-an-element-of-the-vectors
Extract some part of text and format it desirably in Python?,"<p>I want to extract some specific portion of information (text) from a bigger text, and export it based on my desired format. Below is an example</p>

<pre><code>#Input
text={ Line 1: sergefdsgwerh Date is 10,29,2017
       Line 2aergsdfgsdfgasfdhgfasd
       Line 3: company Microsoft}

# Output
exported_text={ D 10 29 2017 C Microsoft}

x = []
with open(""myfile.txt"") as file:
    for l in file:
        x.append(l.strip())

X=[gives my lines as list item]




</code></pre>
","python-3.x, text, text-mining","<p>I would think of three different ways to achieve what you want to do depending on what your input looks like and if you want to do it on multiple pieces of text or not.</p>

<p>For the following examples, let's say that your input is as such:</p>

<pre><code>text = ""Line 1: sergefdsgwerh Date is 10,29,2017\nLine 2aergsdfgsdfgasfdhgfasd\nLine 3: company Microsoft""
</code></pre>

<hr>

<p>First, if you have only one piece of text or if your input is <strong>always formatted in the exact same way</strong>, you can retrieve the portion of information that you need quite easily using basic string manipulation in Python.</p>

<p>To write a working example, you can get what you want using:</p>

<pre><code>date = text.replace(""\n"", "" "").split("" "")[5].replace("","", "" "")
company = text.replace(""\n"", "" "").split("" "")[-1]
output_text = ""D ""+date+"" C ""+company

print(output_text)

# &gt; D 10 29 2017 C Microsoft
</code></pre>

<p>Notice that we're using a bunch of functions here (there are many resources only to get to know these functions better and I suggest that you look into them if you're not familiar with these string manipulations):</p>

<ul>
<li><code>text.replace(""\n"", "" "")</code> replaces the skip line strings:<code>""\n""</code> strings with a space:<code>"" ""</code> in the entire string</li>
<li><code>text.split("" "")</code> splits the string into a list, with each split being done when a space:<code>"" ""</code> is encountered</li>
<li>etc...</li>
</ul>

<hr>

<p>Second, <strong>if your input is formatted in ways that vary by only a little</strong>  between every input (for instance, the name of the company is always written just after the word ""company""), one possible thing would be to use the <code>regex</code> library. Here's an instance if you want to isolate the company name. There are many ways to do what you want here using <code>regex</code> so I won't go into more details than this:</p>

<pre><code>company = re.search(r'company (\S+)', text)

print(company.group(1))

# &gt; Microsoft
</code></pre>

<hr>

<p>Finally, <strong>if your input is constantly changing</strong>, what you want to do is much more complex and much more difficult to explain if you are new to Python. However, it can be worth noting that it can still be achieved using Named Entity Recognition which, for instance, can be done using the library SpaCy (<a href=""https://spacy.io"" rel=""nofollow noreferrer"">here</a>).</p>

<p>That said, given what your input seems to look like, using the standard models from this kind of such library wouldn't help you much (as they are trained on real instances of meaningful text), and you would need to train a model that would fit your specific use...</p>
",0,1,433,2019-11-22 15:12:27,https://stackoverflow.com/questions/58997029/extract-some-part-of-text-and-format-it-desirably-in-python
Function to find word in list and then print following 50 lines,"<p>I have a gigantic txt file that I read in and cleaned into a list.<br>
I'm looking for certain words, so I wrote a quick function</p>

<pre><code>def find_words(lines):

    for line in lines:
        if ""my words"" in line:
            print(line)
</code></pre>

<p>which works fine, but how would I write the function so that it prints the word, plus the following next 50 lines or so? 
Summarizing, I want to find the text that comes after that word.  </p>

<p>From then, I would want to create an empty df, and have the function fill in the df with a new row with the word + next 50 rows, every time it found that word.  </p>
","python, list, text, text-mining","<p>Quick &amp; dirty solution:</p>
<pre class=""lang-py prettyprint-override""><code>for i, line in enumerate(lines):
    if &quot;my words&quot; in line:
        print(*lines[i:i+50], sep=&quot;\n&quot;)
</code></pre>
<ul>
<li>enumerate will set <code>i</code> to the index of the current iterated <code>line</code> on the <code>lines</code> array</li>
<li>when your desired line is found, you print out a slice of the <code>lines</code> array from the current index, until 50 forward positions.</li>
<li>print each line separated by a <code>\n</code> (line break)</li>
</ul>
<p>If your document has a huge number of lines, you might want to avoid loading all the lines at once in memory (check <a href=""https://stackoverflow.com/a/48124263/11245195"">https://stackoverflow.com/a/48124263/11245195</a> - but the workaround for your problem might be different).</p>
",1,0,593,2019-11-26 18:18:33,https://stackoverflow.com/questions/59057161/function-to-find-word-in-list-and-then-print-following-50-lines
Extracting text between two delimiters when the delimiters are in different formats using Python,"<p>I'm a new Python programmer (more experience in R) using Pycharm community edition v2019 2.4, using a laptop running Windows 10.  I'm attempting to extract a block of text between two delimiters which is usually in the following format. (text is between the delimiters but on separate lines)</p>

<pre><code>Item 7.
text, text, text, text
text, text, text, text
Item 7A.
</code></pre>

<p>The problem I'm experiencing is that <code>Item 7</code> and <code>Item 7A</code> can come in many different formats due to the initial pre-processing of the text files, for example.</p>

<pre><code>Item 7.  
text 
Item 7A.
</code></pre>

<p>or</p>

<pre><code>ITEM 7  
text
ITEM 7A.
</code></pre>

<p>or</p>

<pre><code>ITEM 7 
text  
ITEM 7A:
</code></pre>

<p>or</p>

<pre><code>Item 
7
text
Item 
7A.
</code></pre>

<p><code>Item 7</code> and <code>Item 7A</code> can, also appear in larger blocks of text.  This is an issue  beyond my control.</p>

<p>I've examined 100 text files so far and have written the following code.</p>

<pre><code>import glob
import os
from os.path import isfile

path = filepath` 
for filename in glob.glob(os.path.join(path, '*.txt')):
     with open(filename) as f:
     data = f.read()

     x = re.findall(r'Item 7(.*?)Item 7A',data, re.DOTALL)
     """".join(x).replace('\n',' ')
     print(x)

     file = open('C:/R_Practice/dale1.txt', 'w')
     file.write(str(x))

     file.close()  
</code></pre>

<p>This deals with some, but not all of the cases, and even then it's not detecting everything. It won't be possible to analyse the full set of text files as there will be close to 250,000 for the full study.  My questions are as follows.</p>

<ol>
<li>Is there a ""catch all"" code which will search for all occurences of the delimiters, even if parts of the string are on separate lines?</li>
<li>Can each individual text block be written to a separate text file on hard drive, once identified?</li>
<li>Can a logfile be written showing which text files were not processed because the algorithm ""missed"" the delimiters owing to formatting issues?</li>
</ol>

<p>Any help would be appreciated.</p>
","python, regex, text, text-mining","<p>Instead of static space, use <code>\s</code> (that means any kind of spaces, including linebreak) between <code>item</code> &amp; <code>7</code></p>

<pre><code>import glob
import os
from os.path import isfile

path = filepath
for filename in glob.glob(os.path.join(path, '*.txt')):
   with open(filename) as f:
     data = f.read()

     x = re.findall(r'Item\s+7(.*?)Item\s+7A',data, re.DOTALL | re.IGNORECASE)
     #            here ___^^^   and ___^^^
     """".join(x).replace('\n',' ')
     print(x)

     file = open('C:/R_Practice/dale1.txt', 'w')
     file.write(str(x))

     file.close()  
</code></pre>
",0,0,80,2019-11-29 10:35:06,https://stackoverflow.com/questions/59103156/extracting-text-between-two-delimiters-when-the-delimiters-are-in-different-form
Fastest way to filter non frequent words inside lists of words,"<p>I've a dataset containing lists of tokens in csv format like this:</p>

<pre><code>song, tokens
aaa,""['everyon', 'pict', 'becom', 'somebody', 'know']""
bbb,""['tak', 'money', 'tak', 'prid', 'tak', 'littl']""
</code></pre>

<p>First i want to find all the words that appears in text at least a certain amount of time, let's say 5, and this is easily done:</p>

<pre><code># converters simply reconstruct the string of tokens in a list of tokens
tokens = pd.read_csv('dataset.csv',
                      converters={'tokens': lambda x: x.strip(""[]"").replace(""'"", """").split("", "")})

# List of all words
allwords = [word for tokens in darklyrics['tokens'] for word in tokens]
allwords = pd.DataFrame(allwords, columns=['word'])

more5 = allwords[allwords.groupby(""word"")[""word""].transform('size') &gt;= 5]
more5 = set(more5['word'])
frequentwords = [token.strip() for token in more5]
frequentwords.sort()
</code></pre>

<p>Now i want to remove for each list of tokens those who appear inside frequentwords, to do so i'm using this code:</p>

<pre><code>def remove_non_frequent(x):
    global frequentwords
    output = []

    for token in x:
        if token in frequentwords:
            output.append(token)

    return output

def remove_on_chunk(df):
    df['tokens'] = df.apply(lambda x: remove_non_frequent(x['tokens']), axis=1)

    return df


def parallelize_dataframe(df, func, n_split=10, n_cores=4):
    df_split = np.array_split(df, n_split)
    pool = Pool(n_cores)
    df = pd.concat(pool.map(func, df_split))
    pool.close()
    pool.join()
    return df

lyrics_reconstructed = parallelize_dataframe(lyrics, remove_on_chunk)
</code></pre>

<p>The non multiprocess version take around 2.30-3 hours to compute, while this versione takes 1 hour.</p>

<p>Surely it's a slow process because i've to perform the search of circa 130 milions tokens in a list of 30k elements, but i'm quite sure my code is not particularly good.</p>

<p>Is there a faster and surely better way to achieve something like this?</p>
","python-3.x, pandas, multiprocessing, apply, text-mining","<p>It's been a while but i'll post the correct solution to the problem, thantk sto Marek because it's just a slightly modification of his code.
He uses sets which can't handle duplicates, so the obvious idea is to reuse the same code but with multisets.
I've worked with this implementation <a href=""https://pypi.org/project/multiset/"" rel=""nofollow noreferrer"">https://pypi.org/project/multiset/</a></p>

<pre><code>from collections import Counter
import re
from multiset import Multiset

rgx = re.compile(r""[\[\]\""' \n]"")  # data cleanup

# load and pre-process the data
counter = Counter()
data = []
with open('tt1', 'r') as o:
    o.readline()
    for line in o:
        parts = line.split(',')

        clean_parts = [re.sub(rgx, """", i) for i in parts[1:]]
        counter.update(clean_parts)

        ms = Multiset()
        for word in clean_parts:
            ms.add(word)

        data.append([parts[0], ms])

n = 2  # &lt;- here set threshold for number of occurences

common_words = Multiset()

# I'm using intersection with the most common words since
# common_words is way smaller than uncommon_words

# Intersection returns the lowest value count between two multisets
# E.g ('sky', 10) and ('sky', 1) will produce ('sky', 1)
# I want the number of repeated words in my document so i set the
# common words counter to be very high
for item in counter.items():
    if item[1] &gt;= n:
        common_words.add(item[0], 100)

# process the data
clean_data = []
for s, r in data:
    clean_data.append((s, r.intersection(common_words)))

output_data = []
for s, ms in clean_data:
    tokens = []
    for item in ms.items():
        for i in range(0, item[1]):
            tokens.append(item[0])

    output_data.append([s] + [tokens])
</code></pre>

<p>This code extracts the most frequent words and filters each document according to this list, on a 110 MB dataset performs the job in less than 2 minutes.</p>
",0,0,218,2019-11-29 15:03:50,https://stackoverflow.com/questions/59107153/fastest-way-to-filter-non-frequent-words-inside-lists-of-words
Extract a string or value based on specific word before and a % sign after in R,"<p>I have a Text column with thousands of rows of paragraphs, and I want to extract the values of ""<code>Capacity &gt; x%</code>"". The operation sign can be <code>&gt;,&lt;,=, ~...</code> I basically need the operation sign and integer value (e.g. &lt;40%) and place it in a column next to the it, same row. I have tried, removing before/after text, <code>gsub, grep</code>, <code>grepl, string_extract</code>, etc. None with good results.  I am not sure if the percentage sign is throwing it or I am just not getting the code structure. Appreciate your assistance please.
Here are some codes I have tried (aa is the df, TEXT is col name):</p>

<pre><code>str_extract(string =aa$TEXT, pattern = perl(""(?&lt;=LVEF).*(?=%)""))

gsub("".*[Capacity]([^.]+)[%].*"", ""\\1"", aa$TEXT)

genXtract(aa$TEXT, ""Capacity"", ""%"")

gsub(""%.*$"", ""%"", aa$TEXT)

grep(""^Capacity.*%$"",aa$TEXT)
</code></pre>
","r, string, filtering, text-mining, text-extraction","<p><strong>gsub solution</strong></p>

<p>I think your gsub solution was pretty close, but didn't bring along the percentage sign as it's outside the brackets. So something like this should work (the result is assigned to the <code>capacity</code> column):</p>

<p><code>aa$capacity &lt;- gsub("".*[Capacity]([^.]+%).*"", ""\\1"", aa$TEXT)</code></p>

<p><strong>Alternative method</strong></p>

<p>The gsub approach will match the whole string when there is no operator match. To avoid this, we can use the stringr package with a more specific regular expression:</p>

<pre><code>library(magrittr)
library(dplyr)
library(stringr)

aa %&gt;% 
  mutate(capacity = str_extract(TEXT, ""(?&lt;=Capacity\\s)\\W\\s?\\d+\\s?%"")) %&gt;%
  mutate(Capacity = str_squish(Capacity)) # Remove excess white space
</code></pre>

<p>This code will give <code>NA</code> when there is no match, which I believe is your desired behaviour.</p>
",1,1,4952,2019-11-30 05:31:23,https://stackoverflow.com/questions/59113479/extract-a-string-or-value-based-on-specific-word-before-and-a-sign-after-in-r
R text mining with TM: Does a document contain words that are rare,"<p>Using TM package in R, how can I score a document in term of its uniqueness? I want to somehow separate documents with very unique words from documents that contain often used words.</p>

<p>I know how to find the frequently used words and least used words with e.g. findFreqTerms, but how do I score a document with regards to it's uniqueness?</p>

<p>I am struggling to come up with a good solution.</p>
","r, text-mining, tm","<p>A good starting point for assessing which words are used only in some documents is the so-called <em>tf-idf</em> weighting (<a href=""https://cran.r-project.org/web/packages/tidytext/vignettes/tf_idf.html"" rel=""nofollow noreferrer"">tidytext package vignette</a>). This assigns a score to each (word, document) combination, so once you have that calculated you can try summarizing along the 'document' margin, maybe literally just <code>colMeans</code>, to get a sense of how many relatively unique terms it uses.</p>

<p>To separate documents, a weighting scheme like tf-idf may be better than just finding the rarest overall tokens: a rare word used once in most documents is treated quite differently than a word used several times in just a few documents.</p>

<p>R packages TM, tidytext, and quanteda all have functions to calculate this.</p>
",1,2,150,2019-11-30 19:35:01,https://stackoverflow.com/questions/59119559/r-text-mining-with-tm-does-a-document-contain-words-that-are-rare
How can I use the PCA for a term-document matrix in Python?,"<p>I have a list of stemmed words:</p>

<pre><code>text = ['uplink platz windows zukunft spiel effizient virtuell zukunft thema spiel zukunft lang serv spiel gemeinsam episod nachhor herunterlad kiosk brows app ios android fruh episod podcast',
'monat linux zufall eingebaut start einsatz jeweil sogenannt jeweil linux hardwar blick schlecht intel cor intel c’t schlussel linux zufall security arm teil amds million national syst spat chips entwicklung intel system googl tatsach einsatz welt angab memory linux chips', 
'redakteur video test gerat test apps redakteur video ausfuhr test', 
'windows richtig test hannov programmi stark netzwerk haufig bewerb kolleg entwickl les haufig frag lieb stell eigent kolleg link schlecht vergang ergebnis woch mail',
'webseit video frag hilft bewerb frag video', 'vergang onlin vergang onlin moglich datei moglich onlin gesetz angab vorlieg zugriff moglich lieb moglich lieb moglich entwickelt tim zustand tag antwort', 
'c’t kaspersky person erschein verbind kaspersky weis kaspersky nutz inhalt verbind haufig serv brows ungefahr brows modern kaspersky zugriff jeweil sit kaspersky schutz probl microsoft websit cod websit webseit vergang kaspersky mitt august herstell offenbar patch idee nutz googl rahm weis verschied verbind ergebnis prozent prozent herstell alternativ kaspersky kaspersky offent tag herstell probl moglich probl kaspersky person datenleck ausgab zufall id websit nutz probl id websit id websit einzeln besuch brows kaspersky notig websit moglich probl kaspersky verbesser kaspersky kaspersky juni juni automat lang id nutz rei', 
'ifa stand ding helf ifa besuch entwicklung erhalt vergang mensch markt weltweit grosst weiss fest ifa person besitz herstell hom gerat ifa gerat bess system system verfug hoh ifa notig quell probl entsprech oled million samsung divers vorlieg verfugbar preis herstell ifa modell kunftig samsung ifa herstell ifa smartphon eingebaut samsung besuch zukunft ifa ifa zukunft euro euro euro euro euro euro tag euro euro euro euro euro', 
'les les geschicht inhalt weis podcast ausgab redaktion story folg technisch zukunft geschicht geschicht ulrich hilgefort stori rss-feed podcast onlin buch usa deutschland hannov hannov lokal kurz onlin onlin zustand verschied projekt sprech geschicht hannov regelmass',
'bess august vergang erschein million serv spiel stark updat passend kart spiel spiel spiel bess speziell august besitz welt klassisch stand patch spat entwickl verbessert erreich inhalt august stund vollig offenbar arm team zockt serv vergang updat angab notig spiel notig']
</code></pre>

<p>I am using this function to convert the list into a term-document matrix:</p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
def tdm(data):
    vec = CountVectorizer()
    X = vec.fit_transform(data)
    df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
    return df
</code></pre>

<p>How can I use the PCA to reduce the dimensions of the generated matrix?</p>
","python, text-mining, pca","<p>The following codes apply <code>PCA</code> to your problem:</p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import PCA

def tdm(data):
    vec = CountVectorizer()
    X = vec.fit_transform(data)
    df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
    return df

def find_principal_components(n, data):
    pca = PCA(n_components = n)
    principalComponents = pca.fit_transform(data)
    return pd.DataFrame(pca.components_, columns=data.columns)

text = ['uplink platz windows zukunft spiel effizient virtuell zukunft thema spiel zukunft lang serv spiel gemeinsam episod nachhor herunterlad kiosk brows app ios android fruh episod podcast',
'monat linux zufall eingebaut start einsatz jeweil sogenannt jeweil linux hardwar blick schlecht intel cor intel c’t schlussel linux zufall security arm teil amds million national syst spat chips entwicklung intel system googl tatsach einsatz welt angab memory linux chips', 
'redakteur video test gerat test apps redakteur video ausfuhr test', 
'windows richtig test hannov programmi stark netzwerk haufig bewerb kolleg entwickl les haufig frag lieb stell eigent kolleg link schlecht vergang ergebnis woch mail',
'webseit video frag hilft bewerb frag video', 'vergang onlin vergang onlin moglich datei moglich onlin gesetz angab vorlieg zugriff moglich lieb moglich lieb moglich entwickelt tim zustand tag antwort', 
'c’t kaspersky person erschein verbind kaspersky weis kaspersky nutz inhalt verbind haufig serv brows ungefahr brows modern kaspersky zugriff jeweil sit kaspersky schutz probl microsoft websit cod websit webseit vergang kaspersky mitt august herstell offenbar patch idee nutz googl rahm weis verschied verbind ergebnis prozent prozent herstell alternativ kaspersky kaspersky offent tag herstell probl moglich probl kaspersky person datenleck ausgab zufall id websit nutz probl id websit id websit einzeln besuch brows kaspersky notig websit moglich probl kaspersky verbesser kaspersky kaspersky juni juni automat lang id nutz rei', 
'ifa stand ding helf ifa besuch entwicklung erhalt vergang mensch markt weltweit grosst weiss fest ifa person besitz herstell hom gerat ifa gerat bess system system verfug hoh ifa notig quell probl entsprech oled million samsung divers vorlieg verfugbar preis herstell ifa modell kunftig samsung ifa herstell ifa smartphon eingebaut samsung besuch zukunft ifa ifa zukunft euro euro euro euro euro euro tag euro euro euro euro euro', 
'les les geschicht inhalt weis podcast ausgab redaktion story folg technisch zukunft geschicht geschicht ulrich hilgefort stori rss-feed podcast onlin buch usa deutschland hannov hannov lokal kurz onlin onlin zustand verschied projekt sprech geschicht hannov regelmass']

df = tdm(text)

print(df) # 9 rows x 170 columns

principalDF = find_principal_components(2, df)

print(principalDF) # 9 rows x 2 columns
</code></pre>

<p>It should be noted this isn't doing much -- knowing <em>what</em> <code>PCA</code> is, if its being effective, etc is a bigger question than <a href=""https://meta.stackoverflow.com/questions/254770/what-is-stack-overflow-s-goal"">the scope of Stack Overflow</a>.</p>

<p>I suggest checking out the following:</p>

<ul>
<li><a href=""https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"" rel=""nofollow noreferrer"">https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60</a></li>
<li><a href=""https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-8-dimensionality-reduction-chi2-pca-c6d06fb3fcf3"" rel=""nofollow noreferrer"">https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-8-dimensionality-reduction-chi2-pca-c6d06fb3fcf3</a></li>
<li><a href=""https://github.com/tthustla/twitter_sentiment_analysis_part8/blob/master/Capstone_part4-Copy6.ipynb"" rel=""nofollow noreferrer"">https://github.com/tthustla/twitter_sentiment_analysis_part8/blob/master/Capstone_part4-Copy6.ipynb</a></li>
<li><a href=""https://stackoverflow.com/questions/22984335/recovering-features-names-of-explained-variance-ratio-in-pca-with-sklearn"">Recovering features names of explained_variance_ratio_ in PCA with sklearn</a></li>
</ul>
",1,1,729,2019-12-03 17:09:23,https://stackoverflow.com/questions/59162130/how-can-i-use-the-pca-for-a-term-document-matrix-in-python
How to remove just the set of numbers with / in between among other strings?,"<p>I need to extract the blood pressure values from a text note that is typically reported as one larger number, ""/"" over a smaller number, with the units mm HG (it's not a fraction, and only written as such).  In the 4 examples below, I want to extract 114/46, 135/67, 109/50 and 188/98 only, without space before or after and place the top number in column called SBP, and the bottom number into a column called DBP. 
Thank you in advance for your assistance.  </p>

<pre><code>bb &lt;- c(""PATIENT/TEST INFORMATION (m2): 1.61 m2\n BP (mm Hg): 114/46 HR 60 (bpm)"", ""PATIENT/TEST INFORMATION:\ 63\n Weight (lb): 100\nBSA (m2): 1.44 m2\nBP (mm Hg): 135/67 HR 75 (bpm)"", ""PATIENT/TEST INFORMATION:\nIndication: Coronary artery disease. Hypertension. Myocardial infarction.\nWeight (lb): 146\nBP (mm Hg): 109/50 HR (bpm)"", ""PATIENT/TEST INFORMATION:\nIndication: Aortic stenosis. Congestive heart failure. Shortness of breath.\nHeight: (in) 64\nWeight (lb): 165\nBSA (m2): 1.80 m2\nBP (mm Hg): 188/98 HR 140 (bpm) "")   

BP &lt;- head(bb,4)
dput(bb)
</code></pre>
","r, regex, string, text-mining","<p>We can use <code>regmatches/regexpr</code> from <code>base R</code> to extract the required values, and then with <code>read.table</code>, create a two column data.frame</p>

<pre><code>read.table(text = regmatches(bb, regexpr('\\d+/\\d+', bb)), 
      sep=""/"", header =  FALSE, stringsAsFactors = FALSE)
#   V1 V2
#1 114 46
#2 135 67
#3 109 50
#4 188 98
</code></pre>

<hr>

<p>Or using <code>strcapture</code> from <code>base R</code></p>

<pre><code>strcapture( ""(\\d+)\\/(\\d+)"", bb, data.frame(X1 = integer(), X2 = integer()))
#   X1 X2
#1 114 46
#2 135 67
#3 109 50
#4 188 98
</code></pre>

<p>To create this as new columnss in the original data.frame, use either <code>cbind</code> to bind the output with the original dataset</p>

<pre><code>cbind(data, read.table(text = ...))
</code></pre>

<p>Or</p>

<pre><code>data[c(""V1"", ""V2"")] &lt;- read.table(text = ...)
</code></pre>

<hr>

<p>Or using <code>extract</code> from <code>tidyr</code></p>

<pre><code>library(dplyr)
library(tidyr)
tibble(bb) %&gt;%
      extract(bb, into = c(""X1"", ""X2""), "".*\\b(\\d+)/(\\d+).*"", convert = TRUE)
# A tibble: 4 x 2
#     X1    X2
#  &lt;int&gt; &lt;int&gt;
#1   114    46
#2   135    67
#3   109    50
#4   188    98
</code></pre>

<p>If we don't want to remove the original column, use <code>remove = FALSE</code> in <code>extract</code></p>
",1,1,139,2019-12-07 03:53:52,https://stackoverflow.com/questions/59222838/how-to-remove-just-the-set-of-numbers-with-in-between-among-other-strings
Multiprocessing with text scraping,"<p>I want to scrape <code>&lt;p&gt;</code> from pages and since there will be a couple thousands of them I want to use multiprocessing. However, it doesn't work when I try to append the result to some variable</p>

<p>I want to append the result of scraping to the <code>data = []</code></p>

<p>I made a <code>url_common</code> for a base website since some pages don't start with HTTP etc.</p>

<pre><code>from tqdm import tqdm

import faster_than_requests as requests   #20% faster on average in my case than urllib.request
import bs4 as bs

def scrape(link, data):
    for i in tqdm(link):
        if i[:3] !='htt':
            url_common = 'https://www.common_url.com/'
        else:
             url_common = ''
        try: 
             ht = requests.get2str(url_common + str(i))
        except:
            pass
        parsed = bs.BeautifulSoup(ht,'lxml')
        paragraphs = parsed.find_all('p')
        for p in paragraphs:
            data.append(p.text)
</code></pre>

<p>Above doesn't work, since <code>map()</code> doesn't accept function like above</p>

<p>I tried to use it another way:</p>

<pre class=""lang-py prettyprint-override""><code>def scrape(link):
    for i in tqdm(link):
        if i[:3] !='htt':
            url_common = 'https://www.common_url.com/'
        else:
             url_common = ''
        try: 
             ht = requests.get2str(url_common + str(i))
        except:
            pass
        parsed = bs.BeautifulSoup(ht,'lxml')
        paragraphs = parsed.find_all('p')
        for p in paragraphs:
            print(p.text)

from multiprocessing import Pool
p = Pool(10)

links = ['link', 'other_link', 'another_link']
data = p.map(scrape, links) 
</code></pre>

<p>I get this error while using  above function:</p>

<pre><code>  Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\multiprocessing\process.py"", line 297, in _bootstrap
    self.run()
  File ""C:\ProgramData\Anaconda3\lib\multiprocessing\process.py"", line 99, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\ProgramData\Anaconda3\lib\multiprocessing\pool.py"", line 110, in worker
    task = get()
  File ""C:\ProgramData\Anaconda3\lib\multiprocessing\queues.py"", line 354, in get
    return _ForkingPickler.loads(res)
AttributeError: Can't get attribute 'scrape' on &lt;module '__main__' (built-in)&gt;
</code></pre>

<p>I have not figured a way to do it so that it uses <code>Pool</code> and at the same time appending the result of scraping to the given variable </p>

<p><strong>EDIT</strong></p>

<p>I change a little bit to see where it stops:</p>

<pre><code>def scrape(link):
    for i in tqdm(link):
        if i[:3] !='htt':
            url_common = 'https://www.investing.com/'
        else:
            url_common = ''
        try: #tries are always halpful with url as you never know
            ht = requests.get2str(url_common + str(i))
        except:
            pass
        print('works1')
        parsed = bs.BeautifulSoup(ht,'lxml')
        paragraphs = parsed.find_all('p')
        print('works2')
        for p in paragraphs:
            print(p.text)

links = ['link', 'other_link', 'another_link']
scrape(links) 
#WORKS PROPERLY AND PRINTS EVERYTHING 

if __name__ == '__main__':
    p = Pool(5)
    print(p.map(scrape, links))
#DOESN'T WORK, NOTHING PRINTS. Error like above
</code></pre>
","python, multiprocessing, text-mining","<p>You are using the <code>map</code> function incorrectly.</p>

<p>It iterates over each element of the iterable and calls the function on each element.</p>

<p>You can see the map function as doing something like the following:</p>

<pre class=""lang-py prettyprint-override""><code>to_be_mapped = [1, 2, 3]
mapped = []

def mapping(x): # &lt;-- note that the mapping accepts a single value
    return x**2

for item in to_be_mapped:
    res = mapping(item)
    mapped.append(res)

</code></pre>

<p>So to solve your problem remove the outermost for-loop as iterating is handled by the map function</p>

<pre><code>def scrape(link):
  if link[:3] !='htt':
      url_common = 'https://www.common_url.com/'
  else:
        url_common = ''
  try: 
        ht = requests.get2str(url_common + str(link))
  except:
      pass
  parsed = bs.BeautifulSoup(ht,'lxml')
  paragraphs = parsed.find_all('p')
  for p in paragraphs:
      print(p.text)
</code></pre>
",0,0,155,2019-12-17 10:19:37,https://stackoverflow.com/questions/59372062/multiprocessing-with-text-scraping
Combine two words in a corpus with R,"<p>So here is my code </p>

<pre><code>ny &lt;- read.csv2(""nyt.csv"", sep = ""\t"", header = T)
ny_texte &lt;- as.vector(ny)

iterator &lt;- itoken(ny_texte,
                   preprocessor=tolower, 
                   tokenizer=word_tokenizer, 
                   progressbar=FALSE)

vocabulary &lt;- create_vocabulary(iterator)
</code></pre>

<p>My .csv is articles from the new york times. 
I would like to combine words like ""new york"", ""south africa"", ""ellis island"" in vocabulary and not just have token like this :  ""new"" , ""york"", etc </p>

<p>How can I do this ? </p>

<p>Thank You </p>

<p>for more precision: I m using these libraries</p>

<pre><code>library(text2vec)
library(stopwords)
library(tm)
library(dplyr)
library(readr)
</code></pre>

<ul>
<li>and for example about my results </li>
</ul>

<pre><code>ny[1]
</code></pre>

<p><a href=""https://i.sstatic.net/NVB68.png"" rel=""nofollow noreferrer"">1</a> "" LEAD Governor Cuomo with possible Presidential campaign waiting the wings took the oath office New Year Eve for second term New York chief executive LEAD Governor Cuomo with possible Presidential campaign waiting the wings ...</p>

<ul>
<li><code>vocabulary</code>
<a href=""https://i.sstatic.net/NVB68.png"" rel=""nofollow noreferrer"">enter image description here</a></li>
</ul>
","r, text-mining, corpus, text2vec","<p>It's still a little hard to answer your question: we can't run your code because we don't have ""nyt.csv."" But it seems that <code>gsub()</code> will do what you want:</p>

<pre><code>ny &lt;- read.csv2(""nyt.csv"", sep = ""\t"", header = TRUE)
ny &lt;– gsub(""new york"", ""newyork"", ny, ignore.case = TRUE)
ny &lt;– gsub(""south africa"", ""southafrica"", ny, ignore.case = TRUE)
ny_texte &lt;- as.vector(ny)
</code></pre>

<p>(And then run the <code>itoken()</code> and <code>create_vocabulary()</code> commands from your example.)</p>
",0,-1,535,2019-12-23 23:52:21,https://stackoverflow.com/questions/59462415/combine-two-words-in-a-corpus-with-r
How do I extract noun/ verbal phrases for portuguese?,"<p>I've found various tools to extract verbal and noun phrases in English, including in some questions here in stackoverflow. Yet, the techniques I've found only seem to work for English texts. I've tried spacy and textblob but they won't return anything for Portuguese texts (works perfectly in English).</p>

<p>Here is what I've tried for Portuguese:
<a href=""https://stackoverflow.com/questions/44661200/spacy-to-extract-specific-noun-phrase"">Spacy to extract specific noun phrase</a>
The chunk in doc.noun_chunks works perfectly for English, but does anyone knows an already existent technique for Portuguese? I'm searching everywhere I know.</p>
","python, nlp, text-mining, spacy, textblob","<p><code>noun_chunks</code> is implemented for each language individually because the base noun phrases will look different: what order do determiners and adjectives appear in, what are the relevant dependency relations and part-of-speech tags, etc.</p>

<p>Some of the minor details may be different, but I would guess that Portuguese noun chunks are fairly similar to Spanish noun chunks, so you could use the <a href=""https://github.com/explosion/spaCy/blob/db9257559c0642262a46d7acb7855e1e23b50e56/spacy/lang/es/syntax_iterators.py"" rel=""nofollow noreferrer"">Spanish noun chunks iterator</a> as a starting point. Both Spanish and Portuguese use dependency relations and simple POS tags from Universal Dependencies so I hope it would be easy to adapt.</p>

<p>Spacy doesn't have any built-in verb phrase extractors, but the basic idea would be similar to noun chunks: define patterns based on POS tags and dependency trees to identify the phrases you want to extract.</p>
",1,2,745,2019-12-30 15:33:46,https://stackoverflow.com/questions/59533218/how-do-i-extract-noun-verbal-phrases-for-portuguese
"How do check if a text column in my dataframe, contains a list of possible patterns, allowing mistyping?","<p>I have a column called 'text' in my dataframe, where there is a lot of things written. I am trying to verify if in this column there is any of the strings from a list of patterns (e.g pattern1, pattern2, pattern3). I hope to create another boolean column stating if any of those patterns were found or not. </p>

<p>But, an important thing is to match the pattern when there are little mistyping issues. For example, if in my list of patterns I have 'mickey' and 'mouse', I want it to match with 'm0use' and 'muckey' too, not only the full correct pattern string.</p>

<p>I tried this, using regex lib:</p>

<pre><code>import regex
list_of_patterns = ['pattern1','pattern2','pattern3','pattern4']
df['contains_any_pattern'] = df['text'].apply(lambda x: regex.search(pattern=('^(' + '|'.join(list_of_patterns) + ').${e&lt;=2:[a-zA-Z]}'),string=x,flags=re.IGNORECASE))
</code></pre>

<p>I checked the text afterwards and could se that this is not working. Does anyone have a better idea to solve this problem?</p>

<p>Here is a short example:</p>

<pre><code>df = pd.DataFrame({'id':[1,2,3,4,5],
                      'text':['my name is mickey mouse',
                              'my name is donkey kong',
                              'my name is mockey',
                              'my surname is m0use',
                              'hey, its me, mario!'
                             ]})

list_of_patterns = ['mickey','mouse']    
df['contains_pattern'] = df['text'].apply(lambda x: regex.search(pattern=r'(?i)^('+ '|'.join(list_of_patterns) +'){s&lt;=2:[a-zA-Z]}',string=x))
</code></pre>

<p>And here is the resulting df:</p>

<pre><code>id                       text      contains_pattern
1     my name is mickey mouse                  None
2      my name is donkey kong                  None
3           my name is mockey                  None
4         my surname is m0use                  None
5           hey,its me, mario                  None
</code></pre>
","python, regex, python-3.x, dataframe, text-mining","<p>You can fix the code by using something like</p>

<pre><code>df['contains_any_pattern'] = df['text'].apply(lambda x: regex.search(r'(?i)\b(?:' + '|'.join(list_of_patterns) + r'){e&lt;=2}\b', x))
</code></pre>

<p>Or, if the search words may contain special chars use</p>

<pre><code>pat = r'(?i)(?&lt;!\w)(?:' + '|'.join([re.escape(p) for p in list_of_patterns]) + r'){e&lt;=2}(?!\w)'
df['contains_any_pattern'] = df['text'].apply(lambda x: regex.search(pat, x))
</code></pre>

<p>The pattern will look like <code>(?i)\b(?:mouse|mickey){e&lt;=2}\b</code> now. Adjust as you see fit, but <strong>make sure that the quantifier is right after the group</strong>.</p>

<p>The <code>re.IGNORECASE</code> is from the <code>re</code> package, you may simply use the inline modifier, <code>(?i)</code>, to enable case insensitive matching with the current <code>regex</code> library.</p>

<p>If you need to handle hundreds or thousands of search terms, you may leverage the approach described in <a href=""https://stackoverflow.com/a/42789508/3832970"">Speed up millions of regex replacements in Python 3</a>.</p>
",3,3,1547,2020-01-02 22:40:09,https://stackoverflow.com/questions/59570950/how-do-check-if-a-text-column-in-my-dataframe-contains-a-list-of-possible-patte
Which clustering method is the standard way to go for text analytics?,"<p>Assume you have lot of text sentences which may have (or not) similarities. Now you want to cluster similar sentences for finding centroids of each cluster. Which method is the prefered way for doing this kind of clustering? K-means with TF-IDF sounds promising. Nevertheless, are there more sophisticated algorithms or better ones? Data structure is tokenized and in a one-hot encoded format.</p>
","python, cluster-analysis, text-mining","<p>Basically you can cluster texts using different techniques. As you pointed out, K-means with TF-IDF is one of the ways to do this. Unfortunately, only using tf-idf won't be able to ""detect"" semantics and to project smantically similar texts near one another in the space. However, instead of using tf-idf, you can use word embeddings, such as word2vec or glove - there is a lot of information on the net about them, just google it. Have you ever heard of topic models? Latent Dirichlet allocation (LDA) is a topic model and it observes each document as a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics (see the wikipedia <a href=""https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"" rel=""nofollow noreferrer"">link</a>). So, basically, using a topic model you can also do some kind of grouping and assign similar texts (with a similar topic) to groups. I recommend you to read about topic models, since they are more common for such problems connected with text clustering.
I hope my answer was helpful.</p>
",1,0,149,2020-01-02 22:49:19,https://stackoverflow.com/questions/59571031/which-clustering-method-is-the-standard-way-to-go-for-text-analytics
Text Mining in R with Persian,"<p>I'm looking to do some v. simple data mining (frequency, bigrams, trigrams) on some facebook posts in Persian that I've collected and archived in a csv. Below is the script I would use with english language csv of facebook comments to unnest all individual words into their own column. </p>

<pre><code>stp_tidy &lt;- stc2 %&gt;%
  filter(!str_detect(Message, ""^RT"")) %&gt;%
  mutate(text = str_replace_all(Message, ""https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;amp;|&amp;lt;|&amp;gt;|RT"","""")) %&gt;%
  unnest_tokens(word, text, token = ""regex"", pattern = reg_words) %&gt;%
  filter(!word %in% stop_words$word,
         str_detect(word, ""[a-z]""))
</code></pre>

<p>Does anyone know of any method for applying unnest_tokens in Persian (or Dari to be specific) script? </p>
","r, data-mining, text-mining","<p>2 options. First example is using quanteda, second example is using udpipe.</p>

<p>Note that the printing of the tibbles with farsi is weird, a.k.a features and values tend to be printed in the wrong columns, but the data is correctly stored inside the objects for further processing. There is a slight difference in output between the 2 options. But these tend to be negligable. Note that for reading in the data I used the readtext package. This tends to play nice with quanteda. </p>

<p>1 quanteda</p>

<pre><code>library(quanteda)
library(readtext)
# library(stopwords)

stp_test &lt;- readtext(""stp_test.csv"", encoding = ""UTF-8"")

stp_test$Message[stp_test$Message != """"]
stp_test$text[stp_test$text != """"]

# remove records with empty messages

stp_test &lt;- stp_test[stp_test$Message != """", ]

stp_corp &lt;- corpus(stp_test, 
                   docid_field = ""doc_id"",
                   text_field = ""Message"")


stp_toks &lt;- tokens(stp_corp, remove_punct = TRUE)
stp_toks &lt;- tokens_remove(stp_toks, stopwords::stopwords(language = ""fa"", source = ""stopwords-iso""))


# step for creating ngrams 1-3 can be done here, after removing stopwords. 
# stp_ngrams &lt;- tokens_ngrams(stp_toks, n = 1L:3L, concatenator = ""_"")

stp_dfm &lt;- dfm(stp_toks)
textstat_frequency(stp_dfm)

# transform into tidy data.frame
library(dplyr)
library(tidyr)
quanteda_tidy_out &lt;- convert(stp_dfm, to = ""data.frame"") %&gt;% 
  pivot_longer(-document, names_to = ""features"")
</code></pre>

<p>2 udpipe</p>

<pre><code>library(udpipe)
model &lt;- udpipe_download_model(language = ""persian-seraji"")
ud_farsi &lt;- udpipe_load_model(model$file_model)

# use stp_test from quanteda example.
x &lt;- udpipe_annotate(ud_farsi, doc_id = stp_test$doc_id, stp_test$Message)
stp_df &lt;- as.data.frame(x)


# selecting only nouns and verbs and removing stopwords 
ud_tidy_out &lt;- stp_df %&gt;% 
  filter(upos %in% c(""NOUN"", ""VERB""),
         !token %in% stopwords::stopwords(language = ""fa"", source = ""stopwords-iso"")) 
</code></pre>

<p>Both packages have a good vignettes and support pages. </p>
",0,1,453,2020-01-05 18:16:40,https://stackoverflow.com/questions/59602847/text-mining-in-r-with-persian
"how to remove everything but letters, numbers and ! ? . ; , @ &#39; using regex in python pandas df?","<p>I am trying to remove everythin but letters, numbers and ! ? . ; , @ ' from my python pandas column text.
I have already read some other questions on the topic, but still can not make mine work. </p>

<p>Here is an example of what I am doing:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'id':[1,2,3,4],
                  'text':['hey+ guys! wuzup',
                              'hello p3ople!What\'s up?',
                              'hey, how-  thing == do##n',
                              'my name is bond, james b0nd']}
                )
</code></pre>

<p>Then we have the following table:</p>

<pre><code>id                         text
1              hey+ guys! wuzup
2      hello p3ople!What\'s up?
3     hey, how-  thing == do##n
4   my name is bond, james b0nd
</code></pre>

<p>Now, tryng to remove everything but letters, numbers and ! ? . ; , @ '</p>

<p>First try:</p>

<pre><code>df.loc[:,'text'] = df['text'].str.replace(r""^(?!(([a-zA-z]|[\!\?\.\;\,\@\'\""]|\d))+)$"",' ',regex=True)
</code></pre>

<p>output</p>

<pre><code>id                         text
1              hey+ guys! wuzup
2       hello p3ople!What's up?
3      hey, how- thing == do##n
4   my name is bond, james b0nd
</code></pre>

<p>Second try</p>

<pre><code>df.loc[:,'text'] = df['text'].str.replace(r""(?i)\b(?:(([a-zA-Z\!\?\.\;\,\@\'\""\:\d])))"",' ',regex=True)
</code></pre>

<p>output</p>

<pre><code>id                         text
1                  ey+ uys uzup
2              ello 3ople hat p
3            ey ow- hing == o##
4          y ame s ond ames 0nd
</code></pre>

<p>Third try</p>

<pre><code>df.loc[:,'text'] = df['text'].str.replace(r'(?i)(?&lt;!\w)(?:[a-zA-Z\!\?\.\;\,\@\'\""\:\d])',' ',regex=True)
</code></pre>

<p>output</p>

<pre><code>id                         text
1                 ey+ uys! uzup
2           ello 3ople! hat' p?
3           ey, ow- hing == o##
4         y ame s ond, ames 0nd
</code></pre>

<p>Afterwars, I also tried using re.sub() function using the same regex patterns, but still did not manage to have the expected the result. Being this expected result as follows:</p>

<pre><code>id                         text
1               hey guys! wuzup
2       hello p3ople!What's up?
3          hey, how-  thing don
4   my name is bond, james b0nd
</code></pre>

<p>Can anyone help me with that?</p>

<p>Links that I have seen over the topic:</p>

<p><a href=""https://stackoverflow.com/questions/31364852/is-there-a-way-to-remove-everything-except-characters-numbers-and-from-a-st"">Is there a way to remove everything except characters, numbers and &#39;-&#39; from a string</a></p>

<p><a href=""https://stackoverflow.com/questions/59570950/how-do-check-if-a-text-column-in-my-dataframe-contains-a-list-of-possible-patte"">How do check if a text column in my dataframe, contains a list of possible patterns, allowing mistyping?</a></p>

<p><a href=""https://stackoverflow.com/questions/44227748/removing-newlines-from-messy-strings-in-pandas-dataframe-cells"">removing newlines from messy strings in pandas dataframe cells?</a></p>

<p><a href=""https://stackabuse.com/using-regex-for-text-manipulation-in-python/"" rel=""nofollow noreferrer"">https://stackabuse.com/using-regex-for-text-manipulation-in-python/</a></p>
","regex, python-3.x, string, pandas, text-mining","<p>Is this what you are looking for?</p>

<pre><code>df.text.str.replace(""(?i)[^0-9a-z!?.;,@' -]"",'')
Out: 
0                hey guys! wuzup
1        hello p3ople!What's up?
2          hey, how-  thing  don
3    my name is bond, james b0nd
Name: text, dtype: object
</code></pre>
",2,0,1635,2020-01-07 22:46:15,https://stackoverflow.com/questions/59637153/how-to-remove-everything-but-letters-numbers-and-using-regex-in-p
How to get the close words in WordNet in python,"<p>I am using WordNet as follows to get the synsets using python.</p>

<pre><code>import nltk 
from nltk.corpus import wordnet
synonyms = []
for syn in wordnet.synsets(""alzheimer""): 
    for l in syn.lemmas(): 
        synonyms.append(l.name())
print(set(synonyms))
</code></pre>

<p>However, the word <code>alzheimer</code> does not seem to be in WordNet as I get an empty synsets list. Then, I tried different other variants such as <code>alzheimer disease</code>, <code>alzheimer's disease</code>, <code>alzheimers</code>, <code>alzheimer's</code>, <code>alzhemimers disease</code>.</p>

<p>My question is; is it possible to get the word close to the word <code>alzheimer</code> in WordNet, so that I do not need to manually verify what is the term in WordNet to get the synsets.</p>

<p>I am happy to provide more details if needed.</p>
","python, nlp, nltk, text-mining, wordnet","<p>you can find similar word from a given word in wordnet vocabulary. </p>

<pre><code>from nltk.corpus import wordnet as wn
wordnet_vocab = list(wn.all_lemma_names())

similar_string = 'alzheimer'
[word for word in wordnet_vocab if similar_string in word]
#op if exact word is not present,  you can get similar word which are present in wordnet vocab
[""alzheimer's"", ""alzheimer's_disease"", 'alzheimers']
</code></pre>
",1,0,507,2020-01-13 04:00:00,https://stackoverflow.com/questions/59710485/how-to-get-the-close-words-in-wordnet-in-python
How to do fuzzy pattern matching with quanteda and kwic?,"<p>I have texts written by doctors and I want to be able to highlight specific words in their context (5 words before and 5 words after the word I search for in their text). Say I want to search for the word 'suicidal'. I would then use the kwic function in the quanteda package:</p>

<p>kwic(dataset, pattern = “suicidal”, window = 5)</p>

<p>So far, so good, but say I want to allow for the possibility of typos. In this case I want to allow for three deviating characters, with no restriction on where in the word these are made.</p>

<p>Is it possible to do this using quanteda's kwic-function?</p>

<p>Example:</p>

<pre><code>dataset &lt;- data.frame(""patient"" = 1:9, ""text"" = c(""On his first appointment, the patient was suicidal when he showed up in my office"", 
                                  ""On his first appointment, the patient was suicidaa when he showed up in my office"",
                                  ""On his first appointment, the patient was suiciaaa when he showed up in my office"",
                                  ""On his first appointment, the patient was suicaaal when he showed up in my office"",
                                  ""On his first appointment, the patient was suiaaaal when he showed up in my office"",
                                  ""On his first appointment, the patient was saacidal when he showed up in my office"",
                                  ""On his first appointment, the patient was suaaadal when he showed up in my office"",
                                  ""On his first appointment, the patient was icidal when he showed up in my office"",
                                  ""On his first appointment, the patient was uicida when he showed up in my office""))

dataset$text &lt;- as.character(dataset$text)
kwic(dataset$text, pattern = ""suicidal"", window = 5)
</code></pre>

<p>would only give me the first, correctly spelled, sentence. </p>
","r, text-mining, quanteda","<p>Great question.  We don't have approximate matching as a ""valuetype"" but that's an interesting idea for future development.  In the meantime, I'd suggest generating a list of fixed fuzzy matches using <code>base::agrep()</code> and then matching on those.  So this would look like:</p>



<pre class=""lang-r prettyprint-override""><code>library(""quanteda"")
## Package version: 1.5.2

dataset &lt;- data.frame(
  ""patient"" = 1:9, ""text"" = c(
    ""On his first appointment, the patient was suicidal when he showed up in my office"",
    ""On his first appointment, the patient was suicidaa when he showed up in my office"",
    ""On his first appointment, the patient was suiciaaa when he showed up in my office"",
    ""On his first appointment, the patient was suicaaal when he showed up in my office"",
    ""On his first appointment, the patient was suiaaaal when he showed up in my office"",
    ""On his first appointment, the patient was saacidal when he showed up in my office"",
    ""On his first appointment, the patient was suaaadal when he showed up in my office"",
    ""On his first appointment, the patient was icidal when he showed up in my office"",
    ""On his first appointment, the patient was uicida when he showed up in my office""
  ),
  stringsAsFactors = FALSE
)
corp &lt;- corpus(dataset)

# get unique words
vocab &lt;- tokens(corp, remove_numbers = TRUE, remove_punct = TRUE) %&gt;%
  types()
</code></pre>

<p>The use <code>agrep()</code> to generate closest fuzzy matches - and here I ran tihs a few times, increasing <code>max.distance</code> each time slightly from the default of 0.1.</p>

<pre class=""lang-r prettyprint-override""><code># get closest matches to ""suicidal""
near_matches &lt;- agrep(""suicidal"", vocab,
  max.distance = 0.3,
  ignore.case = TRUE, fixed = TRUE, value = TRUE
)
near_matches
## [1] ""suicidal"" ""suicidaa"" ""suiciaaa"" ""suicaaal"" ""suiaaaal"" ""saacidal"" ""suaaadal""
## [8] ""icidal""   ""uicida""
</code></pre>

<p>Then, use this as the <code>pattern</code> argument to <code>kwic()</code>:</p>

<pre class=""lang-r prettyprint-override""><code># use these for fuzzy matching
kwic(corp, near_matches, window = 3)
##                                                        
##  [text1, 9] the patient was | suicidal | when he showed
##  [text2, 9] the patient was | suicidaa | when he showed
##  [text3, 9] the patient was | suiciaaa | when he showed
##  [text4, 9] the patient was | suicaaal | when he showed
##  [text5, 9] the patient was | suiaaaal | when he showed
##  [text6, 9] the patient was | saacidal | when he showed
##  [text7, 9] the patient was | suaaadal | when he showed
##  [text8, 9] the patient was |  icidal  | when he showed
##  [text9, 9] the patient was |  uicida  | when he showed
</code></pre>

<p>There are other possibilities based on similar solutions, for instance the <strong>fuzzyjoin</strong> or <strong>stringdist</strong> packages, but this is a simple solution from the <strong>base</strong> package that should work pretty well.</p>
",4,4,304,2020-01-13 19:19:49,https://stackoverflow.com/questions/59722865/how-to-do-fuzzy-pattern-matching-with-quanteda-and-kwic
"Extract substrings in a text, on columns using Pandas","<p>I'm new in python, so.... I have a dataframe like this:</p>

<pre class=""lang-py prettyprint-override""><code>    id   city      name     text
    1    Boston    Rosie    I have some text here, as you can see.
    2    New York  Liza     I love my cat

</code></pre>

<p>So I'l like to search inside each row the text and have some result like:</p>

<p>I rearch in the text ""love"" or ""love"" &amp;&amp; ""cat"" and I want return the city or the name.</p>

<p>I tried the follow code:</p>

<pre class=""lang-py prettyprint-override""><code>   if df[df['text'].str.contains(""love"") | df['text'].str.contains(""cat"")]:
    print(df['name'])
</code></pre>

<p>It's throwing an error of the form ""The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().""</p>
","python, string, pandas, text-mining","<p>Use boolean index with <code>pandas.Series.str.contains</code>:</p>

<pre><code>df['name'][df['text'].str.contains(""cat|love"")]
</code></pre>

<p>Output:</p>

<pre><code>1    Liza
Name: name, dtype: object
</code></pre>
",2,0,101,2020-01-22 23:35:39,https://stackoverflow.com/questions/59869712/extract-substrings-in-a-text-on-columns-using-pandas
How to classify derived words that share meaning as the same tokens?,"<p>I would like to count unrelated words in an article but I have troubles with grouping words of the same meaning derived from one another.</p>

<p>For instance, I would like <code>gasoline</code> and <code>gas</code> to be treated as the same token in sentences like <code>The price of gasoline has risen.</code> and <code>""Gas"" is a colloquial form of the word gasoline in North American English. Conversely, in BE the term would be ""petrol"".</code> Therefore, if these two sentences comprised the entire article, the count for <code>gas</code> (or <code>gasoline</code>) would be 3 (<code>petrol</code> would not be counted).</p>

<p>I have tried using NLTK's stemmers and lemmatizers but to no avail. Most seem to reproduce <code>gas</code> as <code>gas</code> and <code>gasoline</code> as <code>gasolin</code> which is not helpful for my purposes at all. I understand that this is the usual behaviour. I have checked out a <a href=""https://stackoverflow.com/questions/42698919/classify-words-with-the-same-meaning"">thread</a> that seems to be a little bit similar, however the answers there are not completely applicable to my case as I require the words to be derived from one another. </p>

<p>How to treat derived words of the same meaning as same tokens in order to count them together?</p>
","python, nlp, nltk, text-mining","<p>I propose a two steps approach:</p>

<p>First, find synonyms by comparing word embeddings (only non-stopwords). This should remove similar written words, which mean something else, such as <code>gasoline</code>and <code>gaseous</code>.</p>

<p>Then, check if synonyms share some of their stem. Essentially <code>if ""gas"" is in ""gasolin""</code> and the other way around. This shall suffice because you only compare your synonyms.</p>

<pre><code>import spacy
import itertools
from nltk.stem.porter import *
threshold = 0.6

#compare the stems of the synonyms
stemmer = PorterStemmer()
def compare_stems(a, b):
  if stemmer.stem(a) in stemmer.stem(b):
    return True
  if stemmer.stem(b) in stemmer.stem(a):
    return True
  return False

candidate_synonyms = {}
#add a candidate to the candidate dictionary of sets
def add_to_synonym_dict(a,b):
  if a not in candidate_synonyms:
    if b not in candidate_synonyms:
      candidate_synonyms[a] = {a, b}
      return
    a, b = b,a
  candidate_synonyms[a].add(b)

nlp = spacy.load('en_core_web_lg') 

text = u'The price of gasoline has risen. ""Gas"" is a colloquial form of the word gasoline in North American English. Conversely in BE the term would be petrol. A gaseous state has nothing to do with oil.'

words = nlp(text)

#compare every word with every other word, if they are similar
for a, b in itertools.combinations(words, 2):
  #check if one of the word pairs are stopwords or punctuation
  if a.is_stop or b.is_stop or a.is_punct or b.is_punct:
    continue
  if a.similarity(b) &gt; threshold:
    if compare_stems(a.text.lower(), b.text.lower()):
      add_to_synonym_dict(a.text.lower(), b.text.lower())



print(candidate_synonyms)
#output: {'gasoline': {'gas', 'gasoline'}}
</code></pre>

<p>Then you can count your synonym candidates based on their appearances in the text.</p>

<p><strong>Note:</strong> I chose the threshold for synonyms with 0.6 by chance. You would probably test which threshold suits your task. Also my code is just a quick and dirty example, this could be done a lot cleaner.
`</p>
",2,1,154,2020-01-30 17:34:50,https://stackoverflow.com/questions/59991499/how-to-classify-derived-words-that-share-meaning-as-the-same-tokens
find which string in a list is closest to a character,"<p>I have a pdf document that I have parsed into a list, say:</p>

<pre><code>listTxt = ['met een motor, losse delen van caravans, losse delen van ',
           'aanhangwagens die in uw woonhuis, schuur of garage op ',
           'hetzelfde adres staan tot maximaal € 1.250,-.',
           ' ',
           ' horen deze losse delen bij een bedrijf? Of zijn ze bedoeld ',
           'aanhangwagens die niet kapot zijn verzekerd',  '• Schade door grondwater dat onverwacht het woonhuis ',
           'binnenstroomt door afvoerleidingen en apparaten die daarop ',
           'zijn aangesloten.',
           '• Schade door water dat uit een aquarium stroomt als het ',
           'aquarium onverwacht kapot is gegaan. We betalen ook voor de ',
           'inhoud van het aquarium tot maximaal € 1.250,-.',
           '• Schade door water dat uit een waterbed stroomt. Maar alleen als ',
           'het waterbed onverwacht kapot is gegaan.']
</code></pre>

<p>Now I want to return the string that is closest (in distance) to the euro symbol (€). I have looked at various algos like levenshtein distance etc., but my task is actually quite simple and this distance can be merely number of characters.</p>

<p>Looping with a condition kind of works:</p>

<pre><code>for t in list:
    if 'aanhangwagens' and '€' in t:
        print(t)
</code></pre>

<p>Result:</p>

<pre><code>hetzelfde adres staan tot maximaal € 1.250,-.
inhoud van het aquarium tot maximaal € 1.250,-.
</code></pre>

<p>But I want that <code>'aanhangwagens'</code> that is in <code>listTxt [1]</code> is really close to the next text <code>listTxt [2]</code> (with the €), so the desired output is:</p>

<pre><code>'aanhangwagens die in uw woonhuis, schuur of garage op ', 'hetzelfde adres staan tot maximaal € 1.250,-.'
</code></pre>

<p>for the phrase aquarium, it works fine because aquarium and € are in the same string i.e. <code>listTxt[11]</code></p>

<p><code>'hetzelfde adres staan tot maximaal € 1.250,-.'</code></p>
","python, text-mining","<p>You could try to generate a score to each sentence and then find groups of scores that correspond to groups of useful sentences. Then you'd end up with a total score for each 'match'. I made a crude implementation below.</p>

<pre><code>import numpy as np


listTxt = ['met een motor, losse delen van caravans, losse delen van ',
           'aanhangwagens die in uw woonhuis, schuur of garage op ',
           'hetzelfde adres staan tot maximaal € 1.250,-.',
           ' ',
           ' horen deze losse delen bij een bedrijf? Of zijn ze bedoeld ',
           'aanhangwagens die niet kapot zijn verzekerd',  '• Schade door grondwater dat onverwacht het woonhuis ',
           'binnenstroomt door afvoerleidingen en apparaten die daarop ',
           'zijn aangesloten.',
           '• Schade door water dat uit een aquarium stroomt als het ',
           'aquarium onverwacht kapot is gegaan. We betalen ook voor de ',
           'inhoud van het aquarium tot maximaal € 1.250,-.',
           '• Schade door water dat uit een waterbed stroomt. Maar alleen als ',
           'het waterbed onverwacht kapot is gegaan.']

euro = np.array([string.count('€') for string in listTxt])
ahw = np.array([string.count('aanhangwagen') for string in listTxt])

all_values = np.add(euro,ahw)


score = []
matches = []
for i, value in enumerate(all_values):
    if value &gt; 0:
        score.append(value)
        matches.append(listTxt[i])
    elif score:
        print(sum(score), matches)
        score = []
        matches = []
</code></pre>

<p>It counts the amount of times either '€' or 'aanhangwagen' is found in each sentence, then summates the result. Then make a small loop that finds the groups of 'close' values in between the zeroes.</p>

<p>That way you get a ranking of different (groups of) sentences and a score next to them on how many times your search words were in theses sentences.</p>

<p>In this case, the result is:</p>

<pre><code>2 ['aanhangwagens die in uw woonhuis, schuur of garage op ', 'hetzelfde adres staan tot maximaal € 1.250,-.']
1 ['aanhangwagens die niet kapot zijn verzekerd']
1 ['inhoud van het aquarium tot maximaal € 1.250,-.']
</code></pre>

<p>Which is what you wanted!</p>
",1,0,60,2020-01-31 11:58:38,https://stackoverflow.com/questions/60003478/find-which-string-in-a-list-is-closest-to-a-character
How to do Keyword matching across different dataframes in Pandas?,"<p>I have 2 dataframes across which I need to map the keywords. 
The input data(df1) looks like this:</p>

<pre><code>    keyword            subtopic     
    post office        Brand        
    uspshelp uspshelp  Help         
    package delivery   Shipping     
    fed ex             Brand        
    ups fedex          Brand        
    delivery done      Shipping     
    united states      location     
    rt ups             retweet      
</code></pre>

<p>This is the other dataframe (df2) which is to be used for keyword matching:</p>

<pre><code>Key     Media_type  cleaned_text
910040  facebook    will take post office
409535  twitter     need help with upshelp upshelp
218658  facebook    there no section post office alabama ups fedex
218658  facebook    there no section post office alabama ups fedex
518903  twitter     cant wait see exactly ups fedex truck package
2423281 twitter     fed ex messed seedless
763587  twitter     crazy package delivery rammed car
827572  twitter     formatting idead delivery done
2404106 facebook    supoused mexico united states america
1077739 twitter     rt ups
</code></pre>

<p>I want to map the 'keyword' column in df1 to the 'cleaned_text' column in df2 based on few conditions: </p>

<ol>
<li>One row in 'keyword' can be mapped to more than one row in 'cleaned_text' (One to many relationship)</li>
<li>It should select the whole keyword together and not just individual words. </li>
<li>If a 'keyword' matches to more than one row in 'cleaned_Text' it should create new records in the output dataframe(df3)</li>
</ol>

<p>This is how the output dataframe(df3) should look like:</p>

<pre><code>Key     Media_type  cleaned_text                                    keyword               subtopic  
910040  facebook    will take post office                           post office           Brand 
409535  twitter     need help with upshelp upshelp                  uspshelp uspshelp     Help  
218658  facebook    there no section post office alabama ups fedex  post office           Brand 
218658  facebook    there no section post office alabama ups fedex  ups fedex             Brand 
518903  twitter     cant wait see exactly ups fedex truck package   ups fedex             Brand 
2423281 twitter     fed ex messed seedless                          fed ex messed         Brand 
763587  twitter     crazy package delivery rammed car               package delivery      Shipping  
827572  twitter     formatting idead delivery done                  delivery done         Shipping  
2404106 facebook    supoused mexico united states america           united states america location  
1077739 twitter     rt ups                                          rt ups                retweet               
</code></pre>
","python, pandas, dataframe, text-mining, keyword","<p>How about converting your df1 into a dictionary? And then loop through your df2 and search for matches. It is maybe not the most efficient way, but it is very readable </p>

<pre><code>keyword_dict = {row.keyword: row.subtopic for row in df1.itertuples()}
df3_data = []
for row in df2.itertuples():
    text = row.cleaned_text
    for keyword in keyword_dict:
        if keyword in text:
            df3_row = [row.Key, row.Media_type, row.cleaned_text, keyword, keyword_dict[keyword]]
            df3_data.append(df3_row)

df3_columns = list(df2.columns) + list(df1.columns)
df3 = pd.DataFrame(df3_data, columns=df3_columns)
</code></pre>
",1,0,219,2020-02-11 20:53:47,https://stackoverflow.com/questions/60177208/how-to-do-keyword-matching-across-different-dataframes-in-pandas
Filename too long when using keyword_search to detect pdf?,"<p>I am trying to do some text mining of a pdf by searching for certain keywords.</p>

<p>This is my code:</p>

<pre><code>library(pdftools)
library(tidyverse)
library(pdfsearch)

UC_text &lt;- pdf_text(""https://wilmar-iframe.todayir.com/attachment/20190411162436345449392_en.pdf"") 

result &lt;- keyword_search(UC_text, 
                         keyword = c('SUBSTANTIAL SHAREHOLDERS'),
                         path = TRUE, surround_lines = 1)
</code></pre>

<p>However, I got the error message of a filename too long. How can I get over this issue?</p>
","r, text-mining, pdftools","<p>Given the explanation in the cran manual of pdfsearch, you can directly pass the PDF link to the <code>keyword_search()</code>. In this way, I do not see the error message you provided. I rather got the following result. </p>

<pre><code>result &lt;- keyword_search(""https://wilmar-iframe.todayir.com/attachment/20190411162436345449392_en.pdf"", 
                         keyword = c('SUBSTANTIAL SHAREHOLDERS'),
                         path = TRUE, surround_lines = 1)

  keyword                  page_num line_num line_text token_text
  &lt;chr&gt;                       &lt;int&gt;    &lt;int&gt; &lt;list&gt;    &lt;list&gt;    
1 SUBSTANTIAL SHAREHOLDERS       49     2010 &lt;chr [3]&gt; &lt;list [3]&gt;
</code></pre>
",1,1,149,2020-02-15 01:32:04,https://stackoverflow.com/questions/60235311/filename-too-long-when-using-keyword-search-to-detect-pdf
PDF: how to convert one column lists to multiple column data frame? - lists of people in subgroups inside groups to multiple columns,"<p>I have near 15 PDFs containing <strong>lists of people</strong>. This PDFs are only one column width, so it is a pure list. But in some way these lists are nested (subgroups inside subgroups inside groups...). There is no numerical data apart from the first number of each person in the list (which is very important for my analysis), and similar order info.</p>

<p>I need to pull out from the PDF this lists and convert them into a conventional data frame.</p>

<p>Here is an example of the structure of one PDF:</p>

<pre><code>TERRITORY ONE
1. GROUP ONE
1. Name Surname
2. Name Surname
3. Name Surname
4. Name Surname
2. GROUP TWO
1. Name Surname
2. Name Surname
3. Name Surname
4. Name Surname
TERRITORY TWO
(...)
</code></pre>

<p>This is the first PDF: <a href=""http://bocyl.jcyl.es/boletines/1983/04/02/pdf/BOCYL-D-02041983-1.pdf"" rel=""nofollow noreferrer"">http://bocyl.jcyl.es/boletines/1983/04/02/pdf/BOCYL-D-02041983-1.pdf</a></p>

<hr>

<p><strong>!!!</strong> I found these documents also stored in the webpage, so in HTML format: <a href=""http://bocyl.jcyl.es/html/1983/04/02/html/BOCYL-D-02041983-1.do"" rel=""nofollow noreferrer"">http://bocyl.jcyl.es/html/1983/04/02/html/BOCYL-D-02041983-1.do</a>
Maybe it is easier to take the content from them instead from the PDF?</p>

<hr>

<p>This follows as you can imagine (territory two, three, four..., with subsequent subgroups one, two, three, four,... etc.). This goes up to near 600 lines per PDF and more in the latest PDF.</p>

<p>I need to create a data frame that follows this example structure:</p>

<pre><code>   PERSON    |    TERRITORY  |  GROUP  | POSITION IN LIST
Name Surname | TERRITORY ONE | GROUP 1 |         1
(...)
Name Surname | TERRITORY ONE | GROUP 2 |         4
(...)
Name Surname | TERRITORY TWO | GROUP 1 |         3
</code></pre>

<p><strong>One row should be one person</strong></p>

<p><code>POSITION IN LIST</code> should refer to the order in which person <code>Name Surname</code> appeared in a given year (each PDF is for a year), in his <code>TERRITORY</code>, in his <code>GROUP</code>.</p>

<p>Consider it to be something like a ranking, in which is important the order of the person.
Very few of the people of the PDF1 (year 1) will appear again in PDF2 (year 2), and then in PDF3 (year 3), etc. So, one objective behind all of this is to know how many and who does repeat year after year in this list.</p>

<p>And also, it is important for the analysis to know the position of that person who does repeat in every year, to draw the evolution of this person, or to know if this person disappears after year X, etc.</p>

<p>PS: pardon my English, is not my first language :(</p>
","r, list, pdf, text-mining","<p>Here's a fuller answer, based on scraping the web page rather than the PDF, and still only using one source. So, not yet tested with more than one web page to scrape. If you have additional web pages of source data then add them to the vector at the top of the code below.</p>

<p>I may leave that as an exercise for you @pbstckvrflw!</p>

<p>This was a lot of work but luckily I enjoyed doing it and learning stuff as I went.</p>

<p>However <strong>please note</strong> that tasks of this scale are not usually appropriate for SO questions, and it is better to try hard to make your own attempt at a solution and then ask very specific questions about problems you discover.</p>

<p>I hope you can read carefully through the code I have written and try to understand what is happening at each step. The main thing you may need to learn is about <code>map</code> and how it can apply a function to every item in a list. I have used <code>map</code> extensively here because we're working with nested lists. There are also some good regexps.</p>

<p>It's far from perfect code and there may be errors or inefficiencies. It would be better if some of it were decomposed into repeatable functions. And it generates several intermediate objects, which is a bit messy, but that's just the way it is. Partly the code is in blocks for clarity, partly because I don't have any more time to integrate the blocks into a more fluid workflow without excessive risk of things breaking.</p>

<pre><code>library(rvest)
library(stringr)
library(purrr)
library(dplyr)
library(tibble)
library(conflicted)
conflict_prefer(""pluck"", ""purrr"")

# you should add any further URLs to this vector
urls &lt;- c(""http://bocyl.jcyl.es/html/1983/04/02/html/BOCYL-D-02041983-1.do"")

# scrape text from the relevant part of the webpage
# (assume that any additional URLs have the same structure)
text &lt;- urls %&gt;%
  map(., ~ {xml2::read_html(.) %&gt;%
      rvest::html_nodes(""#presentDocumentos p:not([class])"") %&gt;% 
      html_text})

# extract a manageable name from the URL and use it to name each text
names(text) &lt;- urls %&gt;% 
  str_extract_all(., pattern = ""(?&lt;=/)BOCYL.*(?=\\.do$)"")

# do any manual fixes for errors in source data
text1 &lt;- text %&gt;% 
  map(., ~
  str_replace_all(., ""PARTIDO COMUNISTA DE ESPAÑA PARTIDO COMUNISTA DE CASTILLA- LEON"", ""2. PARTIDO COMUNISTA DE ESPAÑA PARTIDO COMUNISTA DE CASTILLA- LEON""))

text2 &lt;- text1 %&gt;%
  map(., ~ 
        str_replace_all(., ""(\\.)*(\\s)*$"", """") %&gt;% 
        str_replace_all(., ""(\\s)+"", "" "") %&gt;% 
        str_replace_all(., ""^Suplente.*"", """") %&gt;% 
        str_c(., collapse = "";"") %&gt;% 
        str_split(., pattern = ""JUNTA ELECTORAL DE "") %&gt;% 
        map(., ~ tail(., -1) %&gt;% 
              str_split(.,
                        pattern = "";(?=\\d{1,2}\\.\\s([:upper:]|\\s){2,})"") %&gt;% 
              set_names(str_to_title(map(., 1))) %&gt;% 
              map(., ~ tail(., -1))
        )
  )


text3 &lt;- text2 %&gt;% 
  map(., ~
        map(., ~ 
              map(., ~ str_split(., pattern = "";(?=\\d{1,2}\\.\\s)"") %&gt;% 
                    set_names(map(., 1) %&gt;% 
                                str_extract(., pattern = ""(?&lt;=\\d{1,2}\\.\\s)[:upper:].*"")) %&gt;% 
                    map(., ~ tail(., -1) %&gt;% 
                          enframe(., name = ""list_position"", value = ""person_name"") %&gt;% 
                          mutate_at(
                            vars(""person_name""),
                            ~ str_extract_all(.,
                                              pattern = ""(?&lt;=\\d{1,2}\\.\\s)[:alpha:]+.*""))))))

text4 &lt;- text3 %&gt;% 
  map(., ~
        map(., ~ 
              map(., ~
                    map_df(., c, .id = ""political_group""))))

text5 &lt;- text4 %&gt;% 
  map(., ~
        map(., ~ 
              map_df(., c, .id = ""territory"")))

# EXAMPLE
# To look at just the data frame produced from the first web page supplied
# (with columns rearranged as desired):
data_frame1 &lt;- text5 %&gt;% 
  pluck(1, 1) %&gt;% 
  select(person_name, everything())
data_frame1
</code></pre>

<p>I have pushed the latest code to <a href=""https://github.com/francisbarton/tidy_spanish_elections"" rel=""nofollow noreferrer"">the GitHub repo I made</a>.
If you are happy with this answer to your question, please tick this as the accepted answer.</p>
",1,1,441,2020-02-21 13:27:13,https://stackoverflow.com/questions/60339617/pdf-how-to-convert-one-column-lists-to-multiple-column-data-frame-lists-of-p
Pulling specific words from PDF using regex in R,"<p><a href=""https://i.sstatic.net/Mktt8.png"" rel=""nofollow noreferrer"">Click here to see the PDF file</a></p>

<p>I am trying to pull <code>Start</code>,<code>Finish</code>,<code>Job Number</code> <code>Description</code> and <code>Tutor</code> from the PDF file above.</p>

<p>The following code is giving me <code>Start</code>,<code>Finish</code> and <code>Job Number</code>, but I also need to extract the rest of the texts for respective <code>Job numbers</code>.</p>

<pre><code>a&lt;-str_match_all(extracted_pdf, ""(\\d{1,2}/\\d{1,2}/\\d{4})\\W+(\\d{1,2}/\\d{1,2}/\\d{4})\\W+(\\d+)"") 
</code></pre>

<p>Can you please help me on how to pull those texts and turn it into a table? </p>
","r, regex, text-mining","<p>Here is my attempt for you. I provided explanation in my code. Dealing with PDF data is not easy; it requires some coding. But once you can find patterns, you can clean up data.</p>

<pre><code>library(tidyverse)
library(pdftools)
library(splitstackshape)

# Import the PDF, create a data frame, remove unnecessary columns and rows.
# I removed rows that were not included in the table.

foo &lt;- pdf_data(pdf = ""Client.pdf"", opw = """", upw = """") %&gt;% 
       as.data.frame %&gt;% 
       select(-c(width, height, space)) %&gt;% 
       slice(-(1:(grep(x = text, pattern = ""^Start"") - 1))) # Remove irrelevant rows.

# Get indices for dates. First, I identified which rows contain /, since / indicates dates.
# Then, I further subset the indices so that I can identify where each row of the
# table begins.

ind &lt;- grep(x = foo$text, pattern = ""/"")
dummy &lt;- c(diff(ind), NA)
ind &lt;- ind[dummy == 1 &amp; complete.cases(dummy)]

# Create a group variable with findInterval(). Concatenate texts for each group.
# Handle some string manipulation. Split the string and create columns.

group_by(foo, id = findInterval(x = 1:n(), vec = ind)) %&gt;% 
summarize(text = paste0(text, collapse = ""_"")) %&gt;% 
mutate(text = sub(x = text, pattern = ""(Job|Tutor)_(Number|Name)"", replacement = ""\\1 \\2""),
       text = gsub(x = text, pattern = ""(?&lt;=,|Project)_"", replacement = "" "", perl = TRUE)) %&gt;% 
cSplit(splitCols = ""text"", sep = ""_"", direction = ""wide"", type.convert = FALSE) -&gt; res

# Set up column names
names(res) &lt;- setNames(nm = c(""id"", unlist(res[1, ])[-1]))

# Remove the first row since it contains column names.
slice(res, -1) -&gt; res

  id        Start     Finish Job Number                                  Description      Tutor
1  1   29/11/2019 29/11/2019    2288288 Project Name, Project Location, Project Type Tutor Name
2  2    2/12/2019  2/12/2019    8282888 Project Name, Project Location, Project type Tutor Name
3  3    9/12/2019  9/12/2019    2828288 Project Name, Project Location, Project Type Tutor Name
4  4 18/12//2019. 20/12/2019    2828289 Project Name, Project Location, Project Type Tutor Name
</code></pre>
",4,0,165,2020-02-22 07:35:04,https://stackoverflow.com/questions/60350043/pulling-specific-words-from-pdf-using-regex-in-r
How to retrieve messages from non-owned discord server,"<p>I'm writing a code for text analysis and Matlab and want to get messages from a discord server that I don't own according to a search query. First question? is that possible?</p>

<p>Second: are there any good tutorials on how to do that in Python? (discord.py client.run() gives me a asyncio error even after installing nest_async)</p>

<p>Thanks a lot,
Omar</p>
","python, api, discord, text-mining","<p>The best way to do it would be via Discord Web and Selenium</p>

<pre><code>pip install selenium
</code></pre>

<p>Set up selenium to go to Discord Web, Log in and type the query in the search field. Use BeautifulSoup to parse it.</p>
",1,0,267,2020-02-23 12:50:44,https://stackoverflow.com/questions/60362355/how-to-retrieve-messages-from-non-owned-discord-server
how to assign the topics retried via LDA in R using &quot;textmineR&quot; package to the specific documents,"<p>I have got 787 documents (speech - text file). Using ""textmineR"" package i got the topics for the same. I have got 3 topics as below:</p>

<pre><code> topic label      coherence   prevalence    top_terms
 t_1   policy     0.092       37.374        policy, inflation, monetary, rate, federal, economic
 t_2   financial  0.030       37.677        financial, banks, risk, capital, market, not
 t_3   community  0.004       24.949        community, federal, reserve, more, return, mortgage 
</code></pre>

<p>Can someone please suggest how do i assign each topic to the relevant document? and create a datable for the same:</p>

<pre><code>Document Number          Topic
1                           t_1
</code></pre>

<p>and so on.</p>
","r, text-mining, lda","<p>found it, one can use the theta matrix generated as a result of fitLDAmodel. that is the significance of each topic in each speech(document).</p>
",0,0,359,2020-02-25 09:06:24,https://stackoverflow.com/questions/60390942/how-to-assign-the-topics-retried-via-lda-in-r-using-textminer-package-to-the-s
Regex to match sentences with adjacent and non-adjacent word repetition in R,"<p>I have a dataframe with sentences; in some sentences, words get used more than once:</p>

<pre><code>df &lt;- data.frame(Turn = c(""well this is what the grumble about do n't they ?"",
                          ""it 's like being in a play-group , in n it ?"",
                          ""oh is that that steak i got the other night ?"",
                          ""well where have the middle sized soda stream bottle gone ?"",
                          ""this is a half day , right ? needs a full day"",
                          ""yourself , everybody 'd be changing your hair in n it ?"",
                          ""cos he finishes at four o'clock on that day anyway ."",
                          ""no no no i 'm dave and you 're alan ."",
                          ""yeah , i mean the the film was quite long though"",
                          ""it had steve martin in it , it 's a comedy"",
                          ""oh it is a dreary old day in n it ?"",
                          ""no it 's not mother theresa , it 's saint theresa ."",
                          ""oh have you seen that face lift job he wants ?"",
                          ""yeah bolshoi 's right so which one is it then ?""))
</code></pre>

<p>I want to match those sentences in which a word, any word, gets repeated once or more times. </p>

<p><strong>EDIT 1</strong>: </p>

<p>The repeated words **can* be adjacent but they need not be. That's the reason why <a href=""https://stackoverflow.com/questions/2823016/regular-expression-for-consecutive-duplicate-words"">Regular Expression For Consecutive Duplicate Words</a> does not provide an answer to my question. </p>

<p>I've been modestly successful with this code:</p>

<pre><code>df[grepl(""(\\w+\\b\\s)\\1{1,}"", df$Turn),]
[1] well this is what the grumble about do n't they ?      
[2] it 's like being in a play-group , in n it ?           
[3] oh is that that steak i got the other night ?          
[4] this is a half day , right ? needs a full day          
[5] yourself , everybody 'd be changing your hair in n it ?
[6] no no no i 'm dave and you 're alan .                  
[7] yeah , i mean the the film was quite long though       
[8] it had steve martin in it , it 's a comedy             
[9] oh it is a dreary old day in n it ?
</code></pre>

<p>The success is just modest because some sentences are matched that <em>should not</em> be matched, e.g., <code>yourself , everybody 'd be changing your hair in n it ?</code>, while others are not matched that <em>should be</em>, e.g., <code>no it 's not mother theresa , it 's saint theresa .</code>. How can the code be improved to produce exact matches?</p>

<p><strong>Expected result</strong>:</p>

<pre><code>df
                                                         Turn
2                it 's like being in a play-group , in n it ?
3               oh is that that steak i got the other night ?
5               this is a half day , right ? needs a full day
8                       no no no i 'm dave and you 're alan .
9            yeah , i mean the the film was quite long though
10                 it had steve martin in it , it 's a comedy
11                        oh it is a dreary old day in n it ?
12        no it 's not mother theresa , it 's saint theresa .
</code></pre>

<p><strong>EDIT 2</strong>:</p>

<p>Another question would be how to define the <em>exact amount</em> of repeated words. The above, imperfect, regex matches words that are repeated at least once. If I change the quantifier to <code>{2}</code>, thus looking for a triple occurrence of a word, I'd get this code and this result:</p>

<pre><code>df[grepl(""(\\w+\\b\\s)\\1{2}"", df$Turn),]
[1] no no no i 'm dave and you 're alan .         # ""no"" occurs 3 times
</code></pre>

<p>But again the match is imperfect as the <strong>expected</strong> result would be:</p>

<pre><code>[1] no no no i 'm dave and you 're alan .          # ""no"" occurs 3 times
[2] it had steve martin in it , it 's a comedy     # ""it"" occurs 3 times
</code></pre>

<p>Any help is much appreciated!</p>
","r, regex, text, text-mining","<p>An option for defining the exact amount of repeated words.</p>

<p><strong>extract sentences in which the same words occur 3 times</strong></p>

<ol>
<li><p>change regex.</p>

<p><strong>(\s?\b\w+\b\s)(.*\1){2}</strong></p>

<blockquote>
  <p>(\s?\b\w+\b\s) captured by Group 1</p>
</blockquote>

<ul>
<li>\s? : blank space occurs zero or once. </li>
<li>\b\w+\b : the exact word character. </li>
<li><p>\s : blank space occurs once.</p>

<blockquote>
  <p>(.*\1) captured by Group 2<br> </p>
</blockquote>

<ul>
<li><p>(.*\1) : <strong>any characters that occur zero or more times before Group 1 matches again.</strong></p></li>
<li><p>(.*\1){2} :   Group 2 matches twice. </p></li>
</ul></li>
</ul></li>
</ol>

<p><strong>Code</strong>  </p>

<pre><code>df$Turn[grepl(""(\\s?\\b\\w+\\b\\s)(.*\\1){2}"", df$Turn, perl = T)]
# [1] ""no no no i 'm dave and you 're alan .""     
# [2] ""it had steve martin in it , it 's a comedy""
</code></pre>

<ol start=""2"">
<li>Use <code>strsplit(split=""\\s"")</code> split sentences into words.

<ul>
<li>use <code>sapply</code> and <code>table</code> to count the number of occurrence of words in each list element, and then select  sentences that satisfy the requirement.</li>
</ul></li>
</ol>

<p><strong>Code</strong>  </p>

<pre><code>library(magrittr)
df$Turn %&lt;&gt;% as.character()
s&lt;-strsplit(df$Turn,""\\s"") %&gt;% sapply(.,function(i)table(i) %&gt;% .[.==3])
df$Turn[which(s!=0)]
# [1] ""no no no i 'm dave and you 're alan .""     
# [2] ""it had steve martin in it , it 's a comedy""
</code></pre>

<p>Hope this may help you :)</p>
",1,1,282,2020-02-28 09:48:26,https://stackoverflow.com/questions/60449112/regex-to-match-sentences-with-adjacent-and-non-adjacent-word-repetition-in-r
"word_tokenize with same code and same dataset, but different result, why?","<p>Last month, I tried to tokenize text and create a of words to see which word shows up frequently. Today, I want do it again in the same dataset with the same code. It still works but the result is different and obviously today's outcome is wrong because the frequency of appearing words decrease significantly. </p>

<p>Here is my code:</p>

<pre><code>from nltk.tokenize import sent_tokenize, word_tokenize
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from nltk.stem import WordNetLemmatizer
import nltk
from collections import Counter

sent = nltk.word_tokenize(str(df.description))
lower_token = [t.lower() for t in sent]
alpha = [t for t in lower_token if t.isalpha()]
stop_word =  [t for t in alpha if t not in ENGLISH_STOP_WORDS]
k = WordNetLemmatizer()
lemma = [k.lemmatize(t) for t in stop_word]
bow = Counter(lemma)
print(bow.most_common(20))
</code></pre>

<p><a href=""https://i.sstatic.net/08fkb.png"" rel=""nofollow noreferrer"">Here is a sample of my dataset</a></p>

<p>This dataset is from Kaggle and the name of it is ""Wine Reviews"".</p>
","python, nltk, tokenize, text-mining","<p>Welcome to StackOverflow.</p>

<p>There could be two causes for your problem. </p>

<p>1) It could be that you modified the dataset. For this, I would check the dataset and see if you made any changes to the data itself. Because your code works on other examples and will not change from day to day because there are no random elements to it. </p>

<p>2) The second issue could be your use of <code>df.description</code> when you call a dataframe column in this line:</p>

<pre><code>sent = nltk.word_tokenize(str(df.description))
</code></pre>

<p>you get a truncated output. Look at the type of <code>df.description</code> and it is a <code>Series</code> object.</p>

<p>I created another example and it is as follows:</p>

<pre><code>from nltk.tokenize import word_tokenize
import pandas as pd

df = pd.DataFrame({'description' : ['The OP is asking a question and I referred him to the Minimum Verifible Example page which states: When asking a question, people will be better able to provide help if you provide code that they can easily understand and use to reproduce the problem. This is referred to by community members as creating a minimal, reproducible example (reprex), a minimal, complete and verifiable example (mcve), or a minimal, workable example (mwe). Regardless of how it\'s communicated to you, it boils down to ensuring your code that reproduces the problem follows the following guidelines:']})


print(df.description)

0    The OP is asking a question and I referred him...
Name: description, dtype: object
</code></pre>

<p>As you see above, it is truncated and it is not the full text in the <code>description</code> column.</p>

<p>My recommendation to your code is to look into this line of code and find a different way of doing it:</p>

<pre><code>sent = nltk.word_tokenize(str(df.description))
</code></pre>

<p>Note that the method that you used in your code will include the index number (which I understand you filtered by <code>isalpha</code>) and also this <code>Name: description, dtype: object</code> in the data that you are processing.</p>

<p>One way would be to use <code>map</code> to process your data. An example is:</p>

<pre><code>pd.set_option('display.max_colwidth', -1)
df['tokenized'] = df['description'].map(str).map(nltk.word_tokenize)
</code></pre>

<p>Proceed to do this for other operations as well. An easy way to do it would be to build a preprocessing function that applies all the pre-processing operations (that you want to use) on your dataframe.</p>

<p>I hope this helps.</p>
",0,0,102,2020-03-01 20:13:21,https://stackoverflow.com/questions/60479331/word-tokenize-with-same-code-and-same-dataset-but-different-result-why
How to view tokens in quanteda after applying a dictionary,"<p>This is my first time asking a question here, so pleace excuse, if I am not handling it appropriately.
I used the R package quanteda to analyse text documents. </p>

<p>My problem is now that I would like to see the text after I applied the dictionary that I developed. In order to apply the dictionary I tokenized the corpus, but then I couldn't find a function or method which allows me to see the tokenized text. I looked at the quanteda website and the cheat sheet but couldn't find any solution. 
This is basically the important part of my code: </p>

<pre><code>tokens_text_dict &lt;- tokens_text %&gt;% 
  tokens_lookup(dict_Info_priv, exclusive = FALSE)
</code></pre>

<p>EDIT: Moved code from comment to question:</p>

<pre><code>text1 &lt;- ""a b c""
corpus1 &lt;- corpus(text1)
tokens &lt;- tokens(corpus1)
dict1 &lt;- dictionary(list(A = ""a"")
tokens1_dict &lt;- tokens_text %&gt;% 
   tokens_lookup(dict1, exclusive = FALSE)
</code></pre>

<p>I am looking for the command which would give the ""A b c"", which should now be in tokens1_dict.</p>

<p>I would appreciate some help a lot!</p>

<p>Best wishes</p>

<p>Yannick</p>
","r, nlp, access-token, text-mining, quanteda","<p>There are two easy ways to view your tokens.  In quanteda v2, there are options for printing the tokens object to the console.  (See <code>?`print-quanteda`</code>)</p>

<pre><code>&gt; print(tokens1_dict, max_ndoc = -1, max_ntok = -1)
Tokens consisting of 1 document.
text1 :
[1] ""A"" ""b"" ""c""
</code></pre>

<p>Or, can use the <code>View()</code> function, which calls the display method for inspecting a list (of which a tokens object is a special type).  This action is also triggered in RStudio by clicking on the object's name from the Environment pane.</p>

<pre><code>&gt; View(tokens1_dict)
</code></pre>

<p><a href=""https://i.sstatic.net/KrKyQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KrKyQ.png"" alt=""enter image description here""></a></p>
",3,1,552,2020-03-06 11:50:49,https://stackoverflow.com/questions/60563521/how-to-view-tokens-in-quanteda-after-applying-a-dictionary
Accessing words in a string,"<p>I just want to know how to get two different words present in a string.</p>

<p>For Example: I have string <code>A</code> defined</p>

<pre class=""lang-py prettyprint-override""><code>A=""aaed gyh thn ujn""
</code></pre>

<p>Q: I want to know how to get the words <code>""aaed""</code> and <code>""thn""</code> only from the string <code>A</code>. I don't need other words</p>

<p>Note this is a part of text mining project which I am doing currently</p>
","python, text-mining","<p>If you want search it by re.findall you can use the concept below!</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; a=""aaed gyh thn ujn""
&gt;&gt;&gt; re.findall(r""aaed\Wgyh"", a)
['aaed gyh']
</code></pre>

<p>hope it helps!!</p>
",0,-2,555,2020-03-17 08:30:49,https://stackoverflow.com/questions/60718740/accessing-words-in-a-string
Python glove missing module &#39;glove&#39; &#39;Glove&#39;,"<p>Here is what I performed:</p>

<p>Installed pip3 install glove_py ok.
In Jupyter Python, import glove works ok.</p>

<pre><code>from glove import *
</code></pre>

<p>Problem:</p>

<p>When I try a basic setup code to ensure all the modules are loaded and working. I have this code, which errors on message: ""NameError: name 'glove' is not defined"". Now since module glove import works ok, I have tried function 'glove' and 'Glove', both with NameError not defined. </p>

<p>I did find libraries like 'git clone <a href=""http://github.com/stanfordnlp/glove"" rel=""nofollow noreferrer"">http://github.com/stanfordnlp/glove</a>' and did download and build the code with make. This code ran ok in the console for a sample. </p>

<pre><code>pip3 install glove_py
</code></pre>

<p>Pip install for glove_py installed ok. </p>

<pre><code>pip3 install glove_python
</code></pre>

<p>But pip install for glove_python failed to install with ""Error Command errored out with exit status 1:"". </p>

<pre><code>glove &amp;&amp; make
mkdir -p build
</code></pre>

<p>glove 'git clone <a href=""http://github.com/stanfordnlp/glove"" rel=""nofollow noreferrer"">http://github.com/stanfordnlp/glove</a>' download ok and build with make ok. But even with this make'd version, I was not able to get the Python import glove to find this c code make realized inside the Jupyter Python environment.</p>

<p>I suspect that I am missing something simple, I would appreciate any insight.</p>

<p>Python code, test run. Here is my Python code test run which failed on module not found.</p>

<pre><code>model = glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)
model.train(df)
model.to_txt()
words = model.most_similary(""one"", 10)
</code></pre>

<pre><code>NameError                                 Traceback (most recent call last)
&lt;ipython-input-11-517b339bba36&gt; in &lt;module&gt;
----&gt; 1 model = glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)
      2 model.train(df)
      3 model.to_txt()
      4 words = model.most_similary(""one"", 10)
      5 print(words)

NameError: name 'glove' is not defined
</code></pre>

<p>Directory function to see the functions inside 'gl' module, imported from glove package, no module function names showed. So this clearly shows that the import of glove as gl, had some issues.</p>

<pre><code>dir(gl)
</code></pre>

<pre><code>['__doc__',
 '__file__',
 '__loader__',
 '__name__',
 '__package__',
 '__path__',
 '__spec__']
</code></pre>
","python-3.x, nlp, text-mining, glove","<p>What you want is the <code>Glove</code> class inside the module; note the capital letter.</p>

<p>I think this line</p>

<p><code>glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)</code></p>

<p>should be</p>

<p><code>Glove(df, vocab_size=3, d=50, alpha=0.75, x_max=100.0)</code></p>
",0,0,2944,2020-03-21 22:51:53,https://stackoverflow.com/questions/60794145/python-glove-missing-module-glove-glove
r: unnest_tokens() not working with particular file,"<p>i am trying to run <code>unnest_tokens()</code> on the <code>essay4</code> column of this dataset:</p>

<p><a href=""https://github.com/rudeboybert/JSE_OkCupid/blob/master/profiles.csv.zip"" rel=""nofollow noreferrer"">https://github.com/rudeboybert/JSE_OkCupid/blob/master/profiles.csv.zip</a></p>

<p>i have tried both <code>unnest_tokens()</code> and <code>unnest_tokens_()</code>, as well as running <code>dput(as_tibble())</code> on profiles.csv to try to get the program working because of an answer i saw to a similar question that worked for somebody else, but i always get one of two errors.</p>

<p>when i run this:</p>

<pre><code>tidy_essays &lt;- dput_tbl_profiles %&gt;%
   unnest_tokens(word, dput_tbl_profiles$essay4)
</code></pre>

<p>i get this error:</p>

<pre><code>Error in check_input(x) : 
  Input must be a character vector of any length or a list of character
  vectors, each of which has a length of 1.
</code></pre>

<p>when i run this:</p>

<pre><code>tidy_essays &lt;- dput_tbl_profiles %&gt;%
   unnest_tokens_(word, dput_tbl_profiles$essay4)
</code></pre>

<p>i get this error:</p>

<pre><code>Error: Can't convert a closure to a quosure
</code></pre>

<p>i have also tried running the same operations on a version of profiles.csv which hasn't had <code>dput(as_tibble())</code> run on it.</p>

<p>i can't figure out what to do here. it seems that other people have had trouble with this function because they aren't passing character vectors to it (like sending a list instead), or they forget to set <code>stringsAsFactors = FALSE</code> when reading in the data, which i've made sure to do.</p>

<p>any advice for how to proceed? i wish i could link the data directly instead of linking a zip file, but the file is 1/3 of the size when it's zipped. oh, and it's not my github account, so i don't get to decide how the data is stored.</p>

<p>anyway, thank you in advance for any insight.</p>
","r, nlp, text-mining, tidytext","<p>We need to only specify the unquoted column name</p>

<pre><code>library(dplyr)
library(tidytext)
df1 &lt;- read.csv(""profiles.csv"", stringsAsFactors = FALSE)
df1 %&gt;%
     unnest_tokens(word, essay4)
# age      body_type              diet     drinks     drugs                         education
#1       22 a little extra strictly anything   socially     never     working on college/university
#1.1     22 a little extra strictly anything   socially     never     working on college/university
#1.2     22 a little extra strictly anything   socially     never     working on college/university
#1.3     22 a little extra strictly anything   socially     never     working on college/university
#1.4     22 a little extra strictly anything   socially     never     working on college/university
# ...
</code></pre>
",2,1,146,2020-03-27 21:48:08,https://stackoverflow.com/questions/60894547/r-unnest-tokens-not-working-with-particular-file
How do I find most frequent words by each observation in R?,"<p><em>I am very new to NLP. Please, don't judge me strictly.</em></p>

<p>I have got a very big data-frame on customers' feedback, my goal is to analyze feedbacks. I tokenized words in feedbacks, deleted stop-words (SMART). Now, I need to receive a table of most and less frequent used words.</p>

<p>The code looks like this:</p>

<pre><code>library(tokenizers)
library(stopwords)
words_as_tokens &lt;- 
     tokenize_words(dat$description, 
                    stopwords = stopwords(language = ""en"", source = ""smart""))
</code></pre>

<p>The dataframe looks like this: there are lots of feedbacks (variable ""description"") and customers by whom the feedbacks were given (each customer is not unique, they can be repeated). I want to receive a table with 3 columns: a) customer name b) word c) its frequency. This ""ranking"" should be in a decreasing order.</p>
","r, nlp, text-mining","<p>Try this</p>

<pre><code>library(tokenizers)
library(stopwords)
library(tidyverse)

# count freq of words
words_as_tokens &lt;- setNames(lapply(sapply(dat$description, 
                                 tokenize_words, 
                                 stopwords = stopwords(language = ""en"", source = ""smart"")), 
                          function(x) as.data.frame(sort(table(x), TRUE), stringsAsFactors = F)), dat$name)

# tidyverse's job
df &lt;- words_as_tokens %&gt;%
  bind_rows(, .id = ""name"") %&gt;%
  rename(word = x)

# output
df

#    name          word Freq
# 1  John    experience    2
# 2  John          word    2
# 3  John    absolutely    1
# 4  John        action    1
# 5  John        amazon    1
# 6  John     amazon.ae    1
# 7  John     answering    1
# ....
# 42 Alex         break    2
# 43 Alex          nice    2
# 44 Alex         times    2
# 45 Alex             8    1
# 46 Alex        accent    1
# 47 Alex        africa    1
# 48 Alex        agents    1
# ....
</code></pre>

<p><strong>Data</strong></p>

<pre><code>dat &lt;- data.frame(name = c(""John"", ""Alex""),
                  description = c(""Unprecedented. The perfect word to describe Amazon. In every positive sense of that word! All because of one man - Jeff Bezos. What an entrepreneur! What a vision! This is from personal experience. Let me explain. I had given up all hope, after a horrible experience with Amazon.ae (formerly Souq.com) - due to a Herculean effort to get an order cancelled and the subsequent refund issued. I have never faced such a feedback-resistant team in my life! They were robotically answering my calls and sending me monotonous, unhelpful emails, followed by absolutely zero action!"",
                                 ""Not only does Amazon have great products but their Customer Service for the most part is wonderful. Although most times you are outsourced to a different country, I personally have found that when I call it's either South Africa or Philippines and they speak so well, understand me and my NY accent and are quite nice. Let’s face it. Most times you are calling CS with a problem or issue. These agents have to listen to 8 hours of complaints so they themselves need a break. No matter how annoyed I am I try to be on my best behavior and as nice as can be because they too need a break with how nasty we as a society can be.""), stringsAsFactors = F)

</code></pre>
",1,0,3236,2020-03-28 18:18:00,https://stackoverflow.com/questions/60905016/how-do-i-find-most-frequent-words-by-each-observation-in-r
Enumerate two lists at the same time,"<p>I have two lists:
1.List of IPA symbols - M
2.List of single words - N</p>

<p>Now I need to create a third list X = [N,M] where for each IPA symbol found in a single word I have to assign 1 to the new list and 0. For example if M = ['ɓ', 'u', 'l', 'i', 'r', 't', 'ə', 'w', 'a', 'b'] and for simplicity N has only two words = ['ɓuli', 'rutə'], then the output should look like 
X = [[1,1,1,1,0,0,0,0,0,0],
     [0,1,0,0,1,1,1,0,0,0]]</p>

<p>So it's kind of co-occurence matrix but simpler - because I do not need to hold count of how many times the symbol occur in the word. I just need to assign 1 to X when a symbol occur in a word in a proper position. Maybe I am overthinking this but I can't seem to find a way to hold index of both lists.
Here is my code snippet:</p>

<pre><code>M = ['ɓ', 'u', 'l', 'i', 'r', 't', 'ə', 'w', 'a', 'b'] 
N = ['ɓuli', 'rutə']
X = np.zeros((len(N), len(M)))

for n_idx in range(len(N)):
    print('Current word index', n_idx)
    for symbol in N[n_idx]:
        if symbol in M:
            print(symbol, 'found, at word index', n_idx, ', and symbol index')
            # if found then ad to X at proper position

#Expected result
X = [[1,1,1,1,0,0,0,0,0,0], 
     [0,1,0,0,1,1,1,0,0,0]]
</code></pre>
","python, string, list, nlp, text-mining","<p>You can build such an index with this line :</p>

<pre><code>X = [[1 if e in s else 0 for e in M] for s in N]
</code></pre>

<p>which is a double comprehension list looping on letters and words. However you should use libraries such as sklearn to perform such operations more efficiently (e.g. <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html</a>)</p>
",1,1,505,2020-03-30 07:18:46,https://stackoverflow.com/questions/60925728/enumerate-two-lists-at-the-same-time
"How do I find differing words in two strings, sentence-wise?","<p>I am comparing two similar texts. <code>x1</code> is the model text and <code>x2</code> is the text with mistakes (e.g spelling, new characters etc.). I am trying to remove words found in both texts. Since my actual text is not in English I cannot use the dictionary. </p>

<p>What I have tried is to step through each character of <code>x1</code> and if it is same character in <code>x2</code> then delete from <code>x2</code> and move to next character of <code>x1</code>. </p>

<p>Code I've been working on:</p>

<pre><code>x1 &lt;- ""This is a test. Weather is fine. What do I do? I am clueless this coding. Let’s do this as soon as possible."" 
x2 &lt;- ""This text is a test. This weather is fine. What id I do? I am clueless thius coding. Ley’s do ythis as soon as possiblke.""

library(tidyverse)
x1 &lt;- str_split(x1, ""(?&lt;=\\.)\\s"")
x1 &lt;- lapply(x1,tolower)
x2 &lt;- str_split(x2, ""(?&lt;=\\.)\\s"")
x2 &lt;- lapply(x2,tolower)

delete_a_from_b &lt;- function(a,b) {

  a_as_list &lt;- str_remove_all(a,""word"") %&gt;% 
    str_split(boundary(""character"")) %&gt;% unlist

  b_n &lt;- nchar(b)

  b_as_list &lt;- str_remove_all(b,""word"") %&gt;% 
    str_split(boundary(""character"")) %&gt;% unlist

  previous_j &lt;-1

  for (i in 1:length(a_as_list)) {
    if(previous_j &gt; length(b_as_list)) 
      break
    for (j in previous_j:length(b_as_list)){
      if(a_as_list[[i]]==b_as_list[[j]]){
        b_as_list[[j]] &lt;- """"
        previous_j &lt;- j+1
        break
      }
    }
  }

  print(paste0(b_as_list,collapse = """"))
  paste0(b_as_list,collapse = """")
}

x3 &lt;- delete_a_from_b(x1,x2)
x3 &lt;- strsplit(x3,""\\s"")
</code></pre>

<p>Output:  </p>

<pre><code>x3
[[1]]
 [1] ""text""       ""this""       ""i""          ""i""          ""d?am""       ""clueless""   ""thius""      ""coing.\"","" 
 [9] ""\""ley’s""    ""dythsssoon"" ""as""         ""possibk""   
</code></pre>

<p>What I want as result is:  <code>'text' 'this' 'id'  'thius' 'ley’s' 'ythis' 'possiblke'</code> </p>
","r, text-mining, strsplit","<p>I take it you want to compare the two strings <code>x1</code> and <code>x2</code> <em>by sentence</em> - not really clear in the question. The previous solutions do not take this into account. 
Try this:</p>

<p>First split, both strings into sentences:</p>

<pre><code>x1_sentences &lt;- unlist(strsplit(tolower(x1), split = ""[.?!] ""))
x2_sentences &lt;- unlist(strsplit(tolower(x2), split = ""[.?!] ""))
length(x1_sentences)  == length(x2_sentences) # Make sure same number of resulting sentences
</code></pre>

<p>Then, for each sentence, split the two vectors again and show difference in words:</p>

<pre><code>for (i in 1:length(x1_sentences)) {
  x1_vector &lt;- unlist(strsplit(x1_sentences[i], split = ""[ ]""))
  x2_vector &lt;- unlist(strsplit(x2_sentences[i], split = ""[ ]""))
  print(setdiff(x2_vector, x1_vector)) # The order here is important!
}
</code></pre>

<p>Gives (which you can easily turn into a new vector):</p>

<pre><code>[1] ""text""
[1] ""this""
[1] ""id""
[1] ""thius""
[1] ""ley’s""      ""ythis""      ""possiblke.""
</code></pre>
",1,1,171,2020-04-01 07:57:22,https://stackoverflow.com/questions/60966056/how-do-i-find-differing-words-in-two-strings-sentence-wise
"Counting name occurence in textfile, being sensitive to duplicates","<p>I have a list of names and want to count the occurrences throughout a corpus of text files.</p>

<p>I am using a simple regex search with a dictionary to do this:</p>

<pre><code>    for k,v in eng_names_dict.items():
        for i in v:
            pattern = re.compile(str(i).strip(' '))
            matches = re.search(pattern, text)
            if matches:
                namesDict[k] += 1
                break
    return
</code></pre>

<p><strong>The catch:</strong></p>

<p>I have a mix of titles and names (with different name formats as seen in the example below), with some duplicates between them.</p>

<p>For example: 
My list includes two different people - ""Dr. Mark"" (title + surname) and ""Mark Smith"" (first name + surname).</p>

<p>If a text file includes the string ""Dr. Mark Smith said that..."" my function marks a count for both people (instead of only for ""Mark Smith"").</p>

<p>Is there any way to ensure only one count per substring? </p>
","python, regex, text, nlp, text-mining","<p>Ah thanks for including the data structure. I think what you need is ""or"" functionality in regex. Consider this example</p>

<pre><code>regex = r'Mr\. John Smith|John Smith'
re.findall(regex, ""I hate Mr. John Smith)

# -&gt; ['Mr. John Smith'] 
</code></pre>

<p>So to explain, the pipe in the regex acts as an ""or"", i.e. match either this or that but not both, and regex being greedy will match the longest of the patterns if there is nesting between them.</p>

<p>In the example I gave, both ""Mr. John Smith"" and ""John Smith"" were a match, but regex chose to match to the longer one. Note as well that findall() returns a list of all matches. So, applying this to your case:</p>

<pre><code>for k,v in eng_names_dict.items():

    # Convert list of matches into one regex string
    regex = r'|'.join(v)
    matches = re.findall(regex, text)
    namesDict[k] += len(matches)

</code></pre>

<h1>EDIT<hr></h1>

<p>Okay so from your comment it seems there could be ambiguity across the values of different keys of eng_names_dict, whereas so far my answer only deals with ambiguity between values within one key.</p>

<p>Here are two ways to deal with the situation, and the limitations of each. With regex, sometimes there's ambiguity that has to be settled with hard coded rules.</p>

<h2>Scenario 1: a small number of such ambiguous cases.</h2>

<p>If the amount of overlap between values is small and manageable, you could order your regex statements according to preference and remove the matching phrase in the text bit by bit.</p>

<p>So for example if we have:</p>

<pre><code>{'Mark Smith': ['Dr. Mark Smith', 'Mark Smith'],
 'Andrew Mark': ['Dr. Mark', 'Andrew Mark']
</code></pre>

<p><em>Note I am assuming that Mark Smith has a value ""Dr. Mark Smith"" somewhere, even though you don't say this is necessarily the case. Because if this is not true, then the problem is something totally different (in that case it would be how to match 'Mark Smith' and NOT match 'Dr. Mark Smith'.</em></p>

<p>We can clearly see that one of Andrew's values is nested in one of Mark's. So we can choose to do Mark first (according to some rule) and then remove the phrase from the text.</p>

<pre><code>from collections import OrderedDict 

od = OrderedDict()
od['Mark Smith'] = eng_names_dict['Mark_Smith']
od['Andrew Mark'] = eng_names_dict['Andrew Mark']

for k,v in eng_names_dict.items():

    # Convert list of matches into one regex string
    regex = r'|'.join(v)
    matches = re.findall(regex, text)
    for match in set(matches):
        text=re.sub(r'{}'.format(match, '', text)
    namesDict[k] += len(matches)
</code></pre>

<p>The disadvantage here is the manual requirement to determine the order of operations for consuming the eng_name_dicts entries.</p>

<h2>Scenario 2: the amount of cases is just too large</h2>

<hr>

<p>In this case we can just continue to use the natural behaviour of regex to choose the longest string in matches with ""or"". Rework the original solution a little bit. Instead of creating a small regex for each eng_names_dict key, let's create one really big one for every possible value. Regex will decide for us what the proper order is.</p>

<pre><code># Create one list containing all values from dict
match_vals = []
for dict_val in list(eng_names_dict.values()):
    for match_val in dict_val:
        match_vals.extend(match_val)

# Do a match on this full regex
regex = r'|'.join(match_vals)
matches = re.findall(regex, text)

# Loop through every match, and count it if it's in the vals of an entry's key
for match in matches:
    for k, v in eng_names_dict.items():
        # Nested loops will be slow; open to suggestions to improve
        if match in v:
            namesDict[k] + 1
            # Any match is unique to one person; break loop after match found
            break
</code></pre>

<p>The advantage is that regex will naturally determine the most accurate order, so you don't need to figure that out manually. The disadvantage here is that it is heavy handed, hard to debug and might impact relationships between name values that you aren't aware of yourself.</p>
",1,0,105,2020-04-01 14:34:11,https://stackoverflow.com/questions/60973374/counting-name-occurence-in-textfile-being-sensitive-to-duplicates
"How to do Text Mining from a HTML document, and convert it into a CSV file?","<p>So I'm trying to do a bit of text mining from this website ""<a href=""https://www.bmkg.go.id/gempabumi/gempabumi-terkini.bmkg"" rel=""nofollow noreferrer"">https://www.bmkg.go.id/gempabumi/gempabumi-terkini.bmkg</a>"" - particularly from lines 452 until 1050 through the Developer's Sources. I haven't been able to do that successfully; and my goal is, after I succeed in doing so, I'll have to convert it into a dataframe with custom labels, then save it as a CSV file into my local drive. </p>

<p>Is my logic on achieving this goal correct, or am I getting it wrong to even begin with?</p>

<p>Here's what I have so far:</p>

<pre><code>    library(httr)
    library(dplyr)

    bmkg_current &lt;- GET(""https://www.bmkg.go.id/gempabumi/gempabumi-terkini.bmkg"")

    stringi::stri_enc_detect(content(bmkg_current, ""raw""))      //just to check encoding type
    bmkg_text &lt;- content(bmkg_current, type =""text"", encoding = ""ISO-8859-1"")
    bmkg_df &lt;- tibble(line = 452:1050, text = bmkg_text)
    bmkg_df          //tried to output, but not want I wanted
</code></pre>

<p>Output:</p>

<pre><code> # A tibble: 599 x 2
   line text                                                      
   &lt;int&gt; &lt;chr&gt;                                                     
   1   452 ""&lt;!DOCTYPE html&gt;\r\n&lt;!--[if IE 8]&gt; &lt;html lang=\""en\"" clas~
   2   453 ""&lt;!DOCTYPE html&gt;\r\n&lt;!--[if IE 8]&gt; &lt;html lang=\""en\"" clas~
   3   454 ""&lt;!DOCTYPE html&gt;\r\n&lt;!--[if IE 8]&gt; &lt;html lang=\""en\"" clas~
   4   455 ""&lt;!DOCTYPE html&gt;\r\n&lt;!--[if IE 8]&gt; &lt;html lang=\""en\"" clas~
   5   456 ""&lt;!DOCTYPE html&gt;\r\n&lt;!--[if IE 8]&gt; &lt;html lang=\""en\"" clas~
</code></pre>

<p>These are what lines 452 - 1050 look like in the HTML, from Developer Source:</p>

<pre><code>                            &lt;tr&gt;
                                &lt;td&gt;2&lt;/td&gt;
                                &lt;td&gt;29-Mar-20 &lt;br&gt;06:10:35 WIB&lt;/td&gt;
                                &lt;td&gt;-7.39&lt;/td&gt;
                                &lt;td&gt;124.19&lt;/td&gt;
                                &lt;td&gt;5.2&lt;/td&gt;
                                &lt;td&gt;631 Km&lt;/td&gt;
                                &lt;td&gt;108 km BaratLaut ALOR-NTT&lt;/td&gt;
                            &lt;/tr&gt;
</code></pre>

<p>Any help on this would be much appreciated! Thank you :)</p>
","r, csv, dataframe, text-mining, cran","<p>If you need the information from the table on the website using <code>rvest</code> you can do : </p>

<pre><code>library(rvest)
url &lt;- 'https://www.bmkg.go.id/gempabumi/gempabumi-terkini.bmkg'
out_df &lt;- url %&gt;% read_html() %&gt;% html_table() %&gt;% .[[1]]

head(out_df)
#  #            Waktu Gempa Lintang  Bujur Magnitudo Kedalaman                                   Wilayah
#1 1 02-Apr-20 09:13:13 WIB   -7.93 125.62       5.5     10 Km                 125 km TimurLaut ALOR-NTT
#2 2 29-Mar-20 06:10:35 WIB   -7.39 124.19       5.2    631 Km                 108 km BaratLaut ALOR-NTT
#3 3 28-Mar-20 22:43:17 WIB   -1.72 120.14       5.8     10 Km               46 km Tenggara SIGI-SULTENG
#4 4 27-Mar-20 21:32:48 WIB    0.28 133.53       5.5     10 Km       139 km BaratLaut MANOKWARI-PAPUABRT
#5 5 27-Mar-20 04:36:40 WIB   -2.72 139.26       5.9     11 Km        72 km BaratLaut KAB-JAYAPURA-PAPUA
#6 6 26-Mar-20 22:38:03 WIB    5.58 125.16       6.3     10 Km 221 km BaratLaut TAHUNA-KEP.SANGIHE-SULUT
</code></pre>

<p>You could use <code>write.csv</code> to write this data into csv</p>

<pre><code>write.csv(out_df, 'earthquake_data.csc', row.names = FALSE)
</code></pre>
",2,0,375,2020-04-03 03:19:14,https://stackoverflow.com/questions/61004602/how-to-do-text-mining-from-a-html-document-and-convert-it-into-a-csv-file
String replacements in Pandas,"<p>I need to replace words starting with <code>www.</code> with the rest of the link. For example: </p>

<pre><code>www.stackoverflow.com 
</code></pre>

<p>with</p>

<pre><code>stackoverflow.com
</code></pre>

<p>I am using pandas. The column that contains the links is called COL1. I have 1000 rows. 
I have tried with  </p>

<pre><code>df.loc[df['COL1'].str.startswith('www.', na=False), 'NEW_COL'] 
</code></pre>

<p>but I do not know what I should replace with in order to take the rest of the link. </p>

<p>Could you please give me advice on it?</p>
","python, pandas, text-mining","<p>You can use <code>str.replace</code> with regex pattern:</p>

<pre><code>df['COL1'] = df['COL1'].str.replace('^(www\.)', '')
</code></pre>
",1,0,54,2020-04-04 23:45:41,https://stackoverflow.com/questions/61036252/string-replacements-in-pandas
String manipulation for classification,"<p>I have a list of links such as: </p>

<pre><code>Website
www.uk_nation.co.uk
www.nation_ny.com
www.unitednation.com
www.nation.of.freedom.es
www.freedom.org
</code></pre>

<p>and so on. </p>

<p>The above is how the column of my datadrame looks like.
As you can see, they have in common the word '<code>nation</code>'.
I would like to label/group them and add one column in my dataframe to respond with a boolean (True/false; e.g. columns: <code>Nation?</code> option: <code>True/False</code>). </p>

<pre><code>Website                       Nation?
www.uk_nation.co.uk           True
www.nation_ny.com             True
www.unitednation.com          True
www.nation.of.freedom.es      True
www.freedom.org               False
</code></pre>

<p>I would need to do this in order to classify websites in a easier (and possible quicker) way. 
Do you have any suggestions on how to do it?</p>

<p>Any help will be welcome. </p>
","python, pandas, classification, text-mining","<p>this is my suggestion: </p>

<pre><code>import pandas as pd

df = pd.DataFrame({'Website': ['www.uk_nation.co.uk', 
                                'www.nation_ny.com', 
                                'www.unitednation.com', 
                                'www.nation.of.freedom.es', 
                                'www.freedom.org']})

df['Nation?'] = df['Website'].str.contains(""nation"")
print(df)
</code></pre>

<p>output: </p>

<pre><code>                    Website  Nation?
0       www.uk_nation.co.uk     True
1         www.nation_ny.com     True
2      www.unitednation.com     True
3  www.nation.of.freedom.es     True
4           www.freedom.org    False
</code></pre>
",0,1,69,2020-04-07 18:24:57,https://stackoverflow.com/questions/61086842/string-manipulation-for-classification
How do i get the full strings with a RegEx search in python that only captures part of the word?,"<p>My assignment is to search through a document, and grab the words that contain ch, cH,Ch, CH, sh, sH, Sh, and SH. What is the most effective way to grab the whole word? right now using re.findall() i get the correct count of words and location, but am only able to print the ch or sh, not the whole word that contained the letters.
 Here is my code!</p>

<pre><code>import re

#f = open(""dreamMLK.txt"",'r')

with open(""dreamMLK.txt"",'r') as fp:
    line = fp.readline()
    count = 1
    while line:
        x = re.findall(""ch|sh"",line)
        if(len(x) &gt; 0):
            print(x)
            print(str(count) +"": ""+line)
        line = fp.readline()
        count += 1
</code></pre>

<p>and here is the output:</p>

<pre><code>['sh']
3: Five score years ago, a great American, in whose symbolic shadow we stand today, signed the Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of Negro slaves [Audience:] (Yeah) who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the long night of their captivity. (Hmm)

['ch', 'sh', 'sh']
5: But one hundred years later (All right), the Negro still is not free. (My Lord, Yeah) One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. (Hmm) One hundred years later (All right), the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. One hundred years later (My Lord) [applause], the Negro is still languished in the corners of American society and finds himself in exile in his own land. (Yes, yes) And so we’ve come here today to dramatize a shameful condition.
</code></pre>

<p>I want line 3 to print value Shadow, not 'sh'. And line 5 to print Chains, Languished, and Shameful. Here is the assignment verbatim, if interested:</p>

<p><strong>Open a file and using a while loop to read in each line, use regular expressions (re.search()) to find those lines that contain any lower/upper case version of the strings ""ch"" or sh"", i.e.  {ch Ch cH CH sh sH Sh SH}.  NOTE - do not enumerate all 8 possibilities in the regular expression, rather, your regular expression should be 7 characters long including the [ ] charachters.  For each sentenct that contains a ""ch"" or ""sh"" (or Ch or CH or cH etc) print out: a) the line number and the sentance; and b) the list of the words in that sentence contaiing some version of ""sh"" or ""ch"".</strong></p>
","python, regex, string, text, text-mining","<p>Try using the following regex pattern, in case insensitive mode:</p>

<pre><code>\b\S*[cs]h\S*\b
</code></pre>

<p>This will match all words containing <code>ch</code> or <code>sh</code>.  Here is a sample script:</p>

<pre><code># -*- coding: utf-8 -*-
inp = """"""3: Five score years ago, a great American, in whose symbolic shadow we stand today, signed the Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of Negro slaves [Audience:] (Yeah) who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the long night of their captivity.
5: But one hundred years later (All right), the Negro still is not free. (My Lord, Yeah) One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. (Hmm) One hundred years later (All right), the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. One hundred years later (My Lord) [applause], the Negro is still languished in the corners of American society and finds himself in exile in his own land. (Yes, yes) And so we’ve come here today to dramatize a shameful condition.""""""

matches = re.findall(r'\b\S*[cs]h\S*\b', inp, flags=re.IGNORECASE)
print(matches)
</code></pre>

<p>This prints:</p>

<pre><code>['shadow', 'chains', 'languished', 'shameful']
</code></pre>
",0,1,51,2020-04-09 02:42:01,https://stackoverflow.com/questions/61113124/how-do-i-get-the-full-strings-with-a-regex-search-in-python-that-only-captures-p
Stop words list for r,"<p><code>stopwords</code> (of package <code>tm</code>) returns various kinds of stopwords with support for different languages. E.g.</p>

<pre><code>stopwords()
</code></pre>

<p>returns 175 english stop words. I would like to know if there are some tools that provide more stop words.</p>
","r, text-mining, stop-words","<p>If you use the package <code>stopwords</code>, You can specify the source with a longer list.</p>

<pre><code>&gt; install.packages(""stopwords"")
&gt; library(""stopwords"")
&gt; SW = stopwords(""en"", source = ""stopwords-iso"")

&gt; length(SW)
1298
</code></pre>
",1,1,1708,2020-04-10 12:14:45,https://stackoverflow.com/questions/61140034/stop-words-list-for-r
R Tidytext unnest_tokens error when using a txt file as source,"<p>Very new to this topic. I am having trouble with the unnest_tokens function in the tidytext package. I have some texts stored in .txt format that I want to analyze. </p>

<p>An example would be putting the following sentences in a txt file then read it into R:</p>

<pre><code>Emily Dickinson wrote some lovely text in her time.

text &lt;- c(""Because I could not stop for Death -"",
          ""He kindly stopped for me -"",
          ""The Carriage held but just Ourselves -"",
          ""and Immortality"")
</code></pre>

<p>Below is my code:</p>

<pre><code>library(dplyr)
library(tidytext)
library(readtext)
my_data &lt;- read_file(""exp.txt"")
my_data_tibble &lt;- tibble(text = my_data)
my_data_tibble %&gt;% 
  unnest_tokens(word, my_data)
</code></pre>

<p>Then I would get the error message below:</p>

<pre><code>Error in check_input(x) : 
  Input must be a character vector of any length or a list of character
  vectors, each of which has a length of 1.
</code></pre>

<p>Does anyone have a solution to my problem? Thank you in advance!</p>
","r, text-mining, tidytext","<p>First input is the column name of output column that you want and second one is that of input. </p>

<pre><code>library(tidytext)

my_data_tibble %&gt;% unnest_tokens(word, text)

# A tibble: 20 x 1
#   word       
#   &lt;chr&gt;      
# 1 because    
# 2 i          
# 3 could      
# 4 not        
# 5 stop       
# 6 for        
# 7 death      
# 8 he         
#...
#....
</code></pre>
",1,0,156,2020-04-14 06:30:07,https://stackoverflow.com/questions/61201861/r-tidytext-unnest-tokens-error-when-using-a-txt-file-as-source
How do I get feature_importances_ from GridsearchCV,"<p>I am fairly new to programming and this problem might be pretty easy to fix, but I have been stuck on it for a while now and I think my approach is just plainly wrong.
As the title indicates, I have been trying to implement a gridsearch on my RandomForest prediction to find the best possible parameters for my model and then see the most important features of the model with the best parameters.
The packages I've used:</p>

<pre><code>import nltk
from nltk.corpus import stopwords
import pandas as pd
import string
import re
import pickle
import os
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
</code></pre>

<p>After some datacleaning and preprocessing, I made a gridsearch like this, where x_features is the DataFrame with the tfidfvectorized features of my data:</p>

<pre><code>param = {'n_estimators':[10, 50, 150], 'max_depth':[10, 30, 50, None], 'min_impurity_decrease':[0, 0.01, 0.05, 0.1], 'class_weight':[""balanced"", None]}
gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)
gs_fit = gs.fit(x_features, mydata['label'])
optimal_param = pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[0:5]
optimal_param1 = gs_fit.best_params_
</code></pre>

<p>My idea here was, that maybe I could make it easy for myself and copy in the optimal_param1 into my RandomForestClassifier(), and fit it on my training data more or less like this:</p>

<pre><code>rf = RandomForestClassifier(optimal_param2)
rf_model= rf.fit(x_train, y_train)
</code></pre>

<p>but optimal_param2 is a dict. Therefore I thought transforming it to a string and getting rid of the signs that are too much ( sub : for =, delete {, delete } ) would make it work. That obviously failed as the numbers for n_estimators, max_depth etc. were still strings and it expected integers. What i wanted to achieve in the end was to get an output of the most important features more or less like this:</p>

<pre><code>top25_features = sorted(zip(rf_model.feature_importances_, x_train.columns),reverse=True)[0:25]
</code></pre>

<p>I realize that gs is already a complete RF model, but it does not have the attribute feature_importances_ which i was looking for.
I would be very thankful for any ideas on how to make it work.</p>
","python, scikit-learn, random-forest, text-mining, gridsearchcv","<p>Once you have run <code>gs_fit=gs.fit(X,y)</code>, you have everything you need and you don't need to do any retraining.</p>

<p>First, you can access what was the best model by doing:</p>

<pre><code>best_estimator = gs_fit.best_estimator_
</code></pre>

<p>This is returning the Random Forest that yielded the best results. Then you can access this model's feature importances by doing</p>

<pre><code>best_features = best_estimator.feature_importances_
</code></pre>

<p>Obviously, you can chain these and directly do:</p>

<pre><code>best_features = gs_fit.best_estimator_.feature_importances_
</code></pre>
",1,2,1617,2020-04-15 19:58:36,https://stackoverflow.com/questions/61237556/how-do-i-get-feature-importances-from-gridsearchcv
Text correlation with R,"<p>I'm working with a DF that contains several rows with Text ID, Text Corpus and count of words in said corpus. It looks something like this:</p>

<pre><code>    ID                        Text     W_Count
Text_1         I love green apples           4
Text_2    I love yellow submarines           4
Text_3 Remember to buy some apples           5
Text_4               No match here           3
</code></pre>

<p>With that DF I want to calculate the number of words that all rows have in common with one another. For example <code>Text_1</code> and <code>Text_2</code> have two words in common while <code>Text_1</code> and <code>Text_3</code> have just one.</p>

<p>Once I have that, I need to display the data in a matrix similar to this one:</p>

<pre><code>      ID Text_1 Text_2 Text_3 Text_4
Text_1      4      2      1      0
Text_2      2      4      0      0
Text_3      1      0      5      0
Text_4      0      0      0      3
</code></pre>

<p>I managed to do this with only two rows, for example <code>Text_1</code> and <code>Text_2</code>:</p>

<pre><code>Text_1 = df[1, 2]
Text_2 = df[2, 2]
Text_1_split &lt;- unlist(strsplit(Text_1, split ="" ""))
Text_2_split &lt;- unlist(strsplit(Text_2, split ="" ""))
count = length(intersect(Text_1_split, Text_2_split))
count
[1] 2
</code></pre>

<p>But I don't know how to apply this sistematically for all rows and then display the matrix I need.</p>

<p>Any help would be very much appreciated.</p>
","r, text-mining","<p>You're probably looking for the <code>vapply</code> function. Consider the following:</p>

<pre><code>vapply(df$ID, 
           function(x){
                sapply(df$ID, 
                       function(y){
                          x_split &lt;- unlist(strsplit(df$Text[df$ID == x], split = "" ""))
                          y_split &lt;- unlist(strsplit(df$Text[df$ID == y], split = "" ""))

                          return(length(intersect(x_split, y_split)))
                       })
            }, 
           integer(nrow(df)))
</code></pre>

<p>The <code>vapply</code> function (""vector-apply"") applies a function across a series of inputs and returns a vector in the form of its third argument (in this case, an integer of length equal to the length of your data input.</p>
",3,3,204,2020-04-23 21:40:25,https://stackoverflow.com/questions/61397473/text-correlation-with-r
How to combine multiple text entries for a variable once dplyr has grouped by another variable,"<p>For hundreds of matters, my data frame has daily text entries by dozens of timekeepers.  Not every timekeeper enters time each day for each matter.  Text entries can be any length.  Each entry for a matter is for work done on a different day (but for my purposes, figuring out readability measures for the text, dates don't matter).  What I would like to do is to combine for each matter all of its text entries.</p>

<p>Here is a toy data set and what it looks like:</p>

<pre><code>&gt; dput(df)
structure(list(Matter = structure(c(1L, 1L, 1L, 1L, 2L, 2L, 2L, 
3L, 4L, 4L), .Label = c(""MatterA"", ""MatterB"", ""MatterC"", ""MatterD""
), class = ""factor""), Timekeeper = structure(c(1L, 2L, 3L, 4L, 
2L, 3L, 1L, 1L, 3L, 4L), .Label = c(""Alpha"", ""Baker"", ""Charlie"", 
""Delta""), class = ""factor""), Text = structure(c(5L, 8L, 1L, 3L, 
7L, 6L, 9L, 2L, 10L, 4L), .Label = c(""all"", ""all we have"", ""good men to come to"", 
""in these times that try men's souls"", ""Now is"", ""of"", ""the aid"", 
""the time for"", ""their country since"", ""to fear is fear itself""
), class = ""factor"")), class = ""data.frame"", row.names = c(NA, 
-10L))
</code></pre>

<p><code>Dplyr</code> groups the time records by matter, but I am stumped as to how to combine the text entries for each matter so that the result is along these lines -- all text gathered for a matter:</p>

<pre><code>1   MatterA Now is the time for all good men to come to
5   MatterB the aid of their country since
8   MatterC all we have
9   MatterD to fear is fear itself in these times that try men's souls
</code></pre>

<p><code>dplyr::mutate()</code> does not work with various concatenation functions:</p>

<pre><code>textCombined &lt;- df %&gt;% group_by(Matter) %&gt;% mutate(ComboText = str_c(Text))
textCombined2 &lt;- df %&gt;% group_by(Matter) %&gt;% mutate(ComboText = paste(Text))
textCombined3 &lt;- df %&gt;% group_by(Matter) %&gt;% mutate(ComboText = c(Text)) # creates numbers
</code></pre>

<p>Maybe a loop will do the job, as in ""while the matter stays the same, combine the text"" but I don't know how to write that.  Or maybe <code>dplyr</code> has a conditional mutate, as in ""mutate(while the matter stays the same, combine the text).""</p>

<p>Thank you for your help.</p>
","r, dplyr, text-mining","<p>Hi you can use group by and summarise with paste,</p>

<pre><code>&gt; df %&gt;% group_by(Matter) %&gt;% summarise(line= paste(Text, collapse = "" ""))


# A tibble: 4 x 2
#  Matter  line                                                      
#  &lt;fct&gt;   &lt;chr&gt;                                                     
#1 MatterA Now is the time for all good men to come to               
#2 MatterB the aid of their country since                            
#3 MatterC all we have                                               
#4 MatterD to fear is fear itself in these times that try men's souls



</code></pre>
",1,0,36,2020-04-28 11:37:29,https://stackoverflow.com/questions/61479299/how-to-combine-multiple-text-entries-for-a-variable-once-dplyr-has-grouped-by-an
Text-mining/word correlation in R,"<p>I'm trying to make text mining or rather word correlation work in R.</p>

<p>The bigger picture of what I'm trying to do is, I query the entire exported OpenStreetMap database for all features that are within a specific distance to various longitude-latitude locations. So far, this is working like a charm and I have gotten to the point where I have a data frame column of type <code>character</code> that contains all features in that specific distance where one row represents one longitude-latitude location. The data frame column can be found in <a href=""https://pastebin.com/XrDmz822"" rel=""nofollow noreferrer"">this csv</a> and a catalogue of all possible features can be found in <a href=""https://pastebin.com/8KwAy9F0"" rel=""nofollow noreferrer"">this csv</a>.</p>

<p>My next step would now be to categorise the locations depending on their surrounding features. To do this, I would like to use a text mining/word correlation algorithm that is able to create categories based on features that are often present at the same locations.</p>

<p><strong>So in short:</strong> I have a column of type <code>character</code> (words separated by commas) where one row contains all features that are within a certain vicinity to a longitude-latitude location. Based on those surrounding features I would like to categorise my locations relying on correlating features.</p>

<p>I have tried <a href=""https://www.rdocumentation.org/packages/tm/versions/0.7-7/topics/findAssocs"" rel=""nofollow noreferrer"">findAssocs</a> from the tm package, which unfortunately doesn't work for neither type <code>list</code>, <code>data.frame</code> nor <code>character</code>.
I have also found <a href=""https://rstudio-pubs-static.s3.amazonaws.com/265713_cbef910aee7642dc8b62996e38d2825d.html"" rel=""nofollow noreferrer"">this wonderful documentation</a> that guides through basic text mining in R. The problem here is that it seems like I would have to convert each row of my data frame column into a document to prepare a corpus for further processing. While this might be feasible for my test case of 61 locations, it won't be so much for my final analysis of several tens-of-thousands of locations.</p>

<p>Can anyone prod me in the right direction here? Preferably, without relying on 3rd party software like 'rapidminer'. Having everything in one R script would be a lot better for my use case.</p>

<p>Thank you in advance. If you require any additional information, please let me know.</p>
","r, correlation, text-mining","<p>I have found a step-by-step guide to convert data from my format to one that can be used for text mining. The guide can be found <a href=""https://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html"" rel=""nofollow noreferrer"">here</a>. This does answer my problem for now.
I do apologise for the post.</p>
",0,0,190,2020-04-29 09:29:14,https://stackoverflow.com/questions/61498540/text-mining-word-correlation-in-r
How to remove words that start with digits from tokens?,"<p>How to remove words that start with digits from tokens in quanteda? Sample words: 21st, 80s, 8th, 5k, but they can be completely different and I don't know them in advance.</p>

<p>I have a data frame with english sentences. I transformed it to corpus by using quanteda. Next I transformed corpus to tokens and I did some cleaning like <code>remove_punct</code>, <code>remove_symbols</code>, <code>remove_numbers</code>, etc. However, the <code>remove_numbers</code> function does not delete words that start with digits. I would like to delete such words, but I don't know their exact form - it can be e.g. 21st, 22nd, etc.</p>

<pre class=""lang-r prettyprint-override""><code>library(""quanteda"")

data = data.frame(
  text = c(""R is free software and 2k comes with ABSOLUTELY NO WARRANTY."",
           ""You are welcome to redistribute it under 80s certain conditions."",
           ""Type 'license()' or 21st 'licence()' for distribution details."",
           ""R is a collaborative 6th project with many contributors."",
           ""Type 'contributors()' for more information and"",
           ""'citation()' on how to cite R or R packages in publications.""),
  stringsAsFactors = FALSE
)

corp = corpus(data, text_field = ""text"")
toks = tokens(corp, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
              remove_separators = TRUE, split_hyphens = TRUE)
dfmat = dfm(toks, tolower = TRUE, stem = TRUE, remove = stopwords(""english""))
</code></pre>
","r, text-mining, quanteda","<p>You just need to delete them explicitly since they are not managed by <code>remove_numbers = TRUE</code>. Just use a simple regular expression which looks for some digits before a character. In the example below, I look for a sequence of digits between 1 and 5 (e.g. <code>(?&lt;=\\d{1,5}</code>). You can adjust the two numbers to fine tune your regular expression. </p>

<p>Here is the example which only uses <strong>quanteda</strong> but adds <code>tokens_remove()</code> explicitly.</p>



<pre class=""lang-r prettyprint-override""><code>library(""quanteda"")
#&gt; Package version: 2.0.0
#&gt; Parallel computing: 2 of 8 threads used.
#&gt; See https://quanteda.io for tutorials and examples.
#&gt; 
#&gt; Attaching package: 'quanteda'
#&gt; The following object is masked from 'package:utils':
#&gt; 
#&gt;     View

data = data.frame(
  text = c(""R is free software and 2k comes with ABSOLUTELY NO WARRANTY."",
           ""You are welcome to redistribute it under 80s certain conditions."",
           ""Type 'license()' or 21st 'licence()' for distribution details."",
           ""R is a collaborative 6th project with many contributors."",
           ""Type 'contributors()' for more information and"",
           ""'citation()' on how to cite R or R packages in publications.""),
  stringsAsFactors = FALSE
)

corp = corpus(data, text_field = ""text"")
toks = tokens(corp, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE,
              remove_separators = TRUE, split_hyphens = TRUE)
toks = tokens_remove(toks, pattern = ""(?&lt;=\\d{1,5})\\w+"", valuetype = ""regex"" )
dfmat = dfm(toks, tolower = TRUE, stem = TRUE, remove = stopwords(""english""))
</code></pre>

<p><sup>Created on 2020-05-03 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v0.3.0)</sup></p>
",2,1,1134,2020-05-03 18:31:46,https://stackoverflow.com/questions/61579657/how-to-remove-words-that-start-with-digits-from-tokens
How to find the co-occurences of a specific term with udpipe in R?,"<p>I am new to the udpipe package, and I think it has great potential for the social sciences.</p>

<p>A current project of mine to study how news articles write about networks and networking (i.e. the people kind, not computer networks). For this, I webscraped 500 articles with the search string ""network"" from a Dutch site for news about the flexible economy (this is the major source of news and discussion about e.g. self-employment). The data is in Dutch, but that should not matter for my question.</p>

<p>What I like to use udpipe for, is to find out in what context the noun ""netwerk"" or verb ""netwerken"" is used. I tried <code>kwic</code> to get this (from <code>quanteda</code>), but that gives me just the ""window in which it occurs.</p>

<p>I would like to use the lemma (netwerk/netwerken) with the co-occurences operator, but without specifying a second term, and only limited to that specific lemma, rather than calculating all co-occurences.</p>

<p>Is this possible, and how?
A normal language example:
In my network, I contact a lot of people through Facebook -> I would like to get co-occurrence of network and contact (a verb)
I found most of my clients through my network -> here I would like ""my network"" + ""found my clients"".</p>

<p>Any help is mightily appreciated!</p>
","text-mining, quanteda, udpipe","<p>It looks like that udpipe makes more sense about ""context"" than kwic. If sentence level, lemma and limiting word types suffices it should be rather straight forward. Udpipe had dutch model also available prebuilt.</p>

<pre class=""lang-r prettyprint-override""><code>#install.packages(""udpipe"")
library(udpipe)
#dl &lt;- udpipe_download_model(language = ""english"")
# Check the name on download result
udmodel_en &lt;- udpipe_load_model(file = ""english-ud-2.0-170801.udpipe"")

# Single and multisentence samples
txt &lt;- c(""Is this possible, and how? A normal language example: In
my network, I contact a lot of people through Facebook -&gt; I would like to get co-occurrence of
network and contact (a verb) I found most of my clients through my network"")
txtb &lt;- c(""I found most of my clients through my network"")
x &lt;- udpipe_annotate(udmodel_en, x = txt)
x &lt;- as.data.frame(x)
xb &lt;- udpipe_annotate(udmodel_en, x = txtb)
xb &lt;- as.data.frame(xb)

# Raw preview
table(x$sentence[x$lemma == 'network'])

# Use x or xb here 
xn &lt;- udpipe_annotate(udmodel_en, x = x$sentence[x$lemma == 'network'])
xdf &lt;- as.data.frame(xn)

# Reduce noise and group by sentence ~ doc_id to table
df_view = subset(xdf, xdf$upos %in% c('PRON','NOUN','VERB','PROPN'))
library(tidyverse)
df_view %&gt;% group_by(doc_id) %&gt;% 
summarize(lemma = paste(sort(unique(lemma)),collapse="", ""))
</code></pre>

<p>On quick test, the prebuilt model defines network and networking as independent root lemmas so some rough stemmer might work better. I did however ensure that including networks in sentences created new match.</p>

<pre><code>                    I found most of my clients through my network 
                                                                1 
I would like to get co-occurrence of network and contact (a verb) 
                                                                1 
     In my network, I contact a lot of people through Facebook -&gt; 
                                                                1 
A tibble: 3 × 2
doc_id  lemma
&lt;chr&gt;   &lt;chr&gt;
doc1    contact, Facebook, I, lot, my, network, people
doc2    co-occurrence, contact, get, I, like, network, verb
doc3    client, find, I, my, network

</code></pre>

<p>It is totally possible to also find previous and following words as context by stepping up and down from matching lemma indexes but that felt closer to what kwic was allready doing. I did not include dynamic co-occurring tabulation and ordering but I would imagine it should be rather trivial part now when contextul words are extracted. I think it might need some stop words etc but those should become more apparent with bigger data.</p>
",0,0,283,2020-05-04 10:04:45,https://stackoverflow.com/questions/61589671/how-to-find-the-co-occurences-of-a-specific-term-with-udpipe-in-r
How can I analyse a text from a pandas column?,"<p>I'm used to make some analysis from text files in Python. I usually do something like:</p>

<pre><code>f = open('filename.txt','r')
text = """"
while 1:
    line = f.readline()
    if not line:break
    text += line

f.close()

# tokenize
tokenized_word=word_tokenize(text)
.
.
.
</code></pre>

<p>However, now I'm not working with a text file, but with a Pandas dataframe. How can I get the 'text' object from a Pandas column?</p>

<p>I tried taking a look at the post
<a href=""https://stackoverflow.com/questions/53489987/text-mining-with-python-and-pandas"">Text mining with Python and pandas</a>, but it's not exactly what I'm looking for.</p>
","python, pandas, text, text-mining","<p>Let's imagine this is your datafame:</p>

<pre><code>import pandas as pd 
df = pd.DataFrame({ ""Text"": ['bla bla bla', 'Hello', 'Other sentence', 'Lets see']})
</code></pre>

<p>You can get the synonym to your code by using the <code>agg</code> function:</p>

<pre><code>text = df['Text'].agg(lambda x: ' '.join(x.dropna())) 
text
</code></pre>

<p>Result:</p>

<pre><code>'bla bla bla Hello Other sentence Lets see'
</code></pre>

<p>Then you can tokenize:</p>

<pre><code>tokenized_word=word_tokenize(text)
</code></pre>
",1,0,430,2020-05-05 19:43:05,https://stackoverflow.com/questions/61621686/how-can-i-analyse-a-text-from-a-pandas-column
Beautiful Soup: How to extract text from this structure:,"<p>I would like to access the timestamp text inside <code>title = """"</code></p>

<p>And get this string ""<code>23.12.2019 13:05:24</code>""</p>

<pre><code>[&lt;div class=""pull_right date details"" title=""23.12.2019 13:05:24""&gt;
 13:05
        &lt;/div&gt;]
</code></pre>

<p>I already know to access the proper text inside this div. But it happens it's just the hour. The full timestamp is what I need.</p>

<p>I'm using this structure currently:</p>

<pre><code>ltimestamp = []
for tag in divTag:
    tdTags = tag.find_all(""div"", {""class"": ""pull_right date details""})    
for tag in tdTags:
    ltimestamp.append(tag.text)
</code></pre>
","python, web-scraping, beautifulsoup, text-mining","<p>when you have this element 
<code>&lt;div class=""pull_right date details"" title=""23.12.2019 13:05:24""&gt;13:05&lt;/div&gt;</code></p>

<p>if you want to get the '13:05' => value inside  tag, you do this
<code>print(tdTags.text)</code></p>

<p>to get the value ('23.12.2019 13:05:24') of an attribute ('title'), do this
<code>print(tdTags['title'])</code></p>

<p>I'll come back with beautifulsoup link to this explanation. read it somewhere before</p>

<p>documentation url:
<code>https://www.crummy.com/software/BeautifulSoup/bs4/doc/#attributes</code></p>
",1,0,125,2020-05-15 01:37:12,https://stackoverflow.com/questions/61810271/beautiful-soup-how-to-extract-text-from-this-structure
removing gibberish from sentences,"<p>During text cleaning, is it possible to detect and remove junk like this from sentences:</p>

<pre><code>x &lt;- c(""Thisisaverylongexample and I was to removeitnow"", ""thisisjustjunk but I do I remove it"")
</code></pre>

<p>currently I'm doing something like this:</p>

<pre><code>str_detect(x, pattern = 'Thisisaverylongexample'))
</code></pre>

<p>but the more I review my dataframe, I found more sentences with this type of junk. How do I use something like regex to detect and remove rows with something junk like this? </p>
","r, text-mining","<p>If 'junk' is detectable via its unusual length, you can define a rule accordingly. For example, if you want to get rid of words of 10 or more characters, this would extract them:</p>

<pre><code>library(stringr)
str_extract_all(x, ""\\b\\w{10,}\\b"")
[[1]]
[1] ""Thisisaverylongexample"" ""removeitnow""           

[[2]]
[1] ""thisisjustjunk""
</code></pre>

<p>and this would get rid of them:</p>

<pre><code>trimws(gsub(""\\b\\w{10,}\\b"", """", x))
[1] ""and I was to""         ""but I do I remove it""
</code></pre>

<p>Data:</p>

<pre><code>x &lt;- c(""Thisisaverylongexample and I was to removeitnow"", ""thisisjustjunk but I do I remove it"")
</code></pre>
",2,0,388,2020-05-19 08:55:45,https://stackoverflow.com/questions/61887090/removing-gibberish-from-sentences
Find a Pattern across Multiple lines with R,"<p>I am trying to identify a pattern across multiple lines, to be exact 2 lines. Since the pattern in either individual line is not unique I am using this approach.</p>

<p>So far I have tried to go with the function ""grep"" but I think I am missing the correct regular expression here.</p>

<pre class=""lang-r prettyprint-override""><code>grep(""^Item\\s{0,}2[^A]"", f.text, ignore.case = TRUE)
</code></pre>

<p>This part is a modified version of the edgar package function ""getfillings"" and tries to extract only the Management's Comment/Item 2 for quarterly results. If possible I would include something after ... <code>2[^A]</code> in the function that reacts to the new line and then the string ""Management...""</p>

<p>The pattern in the plain txts which I have, looks like this:</p>

<p><em>Item 2.<br>
Management   Discussion and Analysis of Financial Condition and Results of Operations</em></p>

<p>I would appreciate any comment on how to capture this best in a regular expression with R.</p>

<p><strong>Example Input looks like this:</strong></p>

<p><em>21 
Item 2.<br>
Management   Discussion and Analysis of Financial Condition and Results of Operations 
This section and other parts of this Quarterly Report on Form 10
Item 3.<br>
Quantitative and Qualitative Disclosures About Market Risk 
There have been no material changes to the Company   market risk</em></p>

<p><strong>and the desired output would be</strong></p>

<p><em>Management   Discussion and Analysis of Financial Condition and Results of Operations 
This section and other parts of this Quarterly Report on Form 10</em></p>

<p>I need to match ""Item 2. ... Management  Discussion"" since Item 2 is not unique. How could I formulate a regular expression across two lines?</p>
","r, regex, text-mining, sec","<p>Not very sophisticated since I'm no expert in string manipulation: Using package <code>tidyverse</code> provides some powerful tools to get your desired output.</p>

<pre class=""lang-r prettyprint-override""><code>text &lt;- ""21 Item 2.
Management Discussion and Analysis of Financial Condition and Results of Operations This section and other parts of this Quarterly Report on Form 10 Item 3.
Quantitative and Qualitative Disclosures About Market Risk There have been no material changes to the Company market risk Item 4.
Fluffy Text example Item 5.
Lorem ipsum dolor sit amet, consectetur adipisici elit""
</code></pre>

<p>Now</p>

<pre class=""lang-r prettyprint-override""><code>text %&gt;%
  str_extract_all(""(?&lt;=Item\\s\\d[[:punct:]]\\n).*"", simplify = TRUE) %&gt;%
  str_remove(""\\s+Item\\s\\d[[:punct:]]"")
</code></pre>

<p>gives you</p>

<pre class=""lang-r prettyprint-override""><code>[1] ""Management Discussion and Analysis of Financial Condition and Results of Operations This section and other parts of this Quarterly Report on Form 10""
[2] ""Quantitative and Qualitative Disclosures About Market Risk There have been no material changes to the Company market risk""                           
[3] ""Fluffy Text example""                                                                                                                                 
[4] ""Lorem ipsum dolor sit amet, consectetur adipisici elit"" 
</code></pre>

<p>If you just want to extract <em>Item 2</em>, replace the <code>\\d</code> inside <code>str_extract_all</code> with 2.</p>
",0,0,718,2020-05-24 18:58:10,https://stackoverflow.com/questions/61990933/find-a-pattern-across-multiple-lines-with-r
How to mine multiwords from a given text in R?,"<pre><code>library(tm)
library(stringr)
txt &lt;- ""Netherland Belgium UK Sweden France Russia Government and People""
words &lt;- c(""land"", ""Sweden"", ""Government and People"", ""Government"", ""People"")
pattern &lt;- str_c(words,collapse =""|"")
cntry &lt;- str_extract_all(txt, pattern)
</code></pre>

<p>Although <code>land</code> is not found as a separate word in my text, the code takes from the last portion of <code>Netherland</code>. How I can enforce the code to strictly look for words included in <code>words</code> only? 
Output for the variable <code>cntry</code>:</p>

<pre><code> ""land""  ""Sweden""  ""Government and People""
</code></pre>

<p>An output that I need for <code>cntry</code>:</p>

<pre><code> ""Sweden""  ""Government and People""
</code></pre>
","r, string, text-mining","<p>Here's a not-so-elegant workaround, too long for comments, so posting as an answer. It appears that <code>land</code> is the problem string while others can be extracted using <code>str_extract_all</code> as posted in the comments. </p>

<p>In this answer I'm focusing on extracting <code>Netherland</code> given the pattern <code>land</code>. Another similar example would be extracting <code>Sweden</code> based on the pattern <code>den</code>. </p>

<p>Here's a function to achieve this using <code>regmatches</code> and <code>regexec</code>: </p>

<p><strong>FUNCTION</strong></p>

<pre><code>return_partials &lt;- function(txt, problem_patterns){
  ret_vec &lt;- sapply(problem_patterns, function(z){
    list_output &lt;- regmatches(x = txt, 
                    m = regexec(pattern = paste('[[:space:]]{0,1}?(.*', z, ')', sep = ''), 
                                text = txt))
    return(list_output[[1]][2])
  })
  return(unname(ret_vec))
}
</code></pre>

<p><strong>OUTPUT</strong></p>

<pre><code>&gt; return_partials(txt = txt, problem_patterns = c('land', 'den'))
[1] ""Netherland"" ""Sweden""
</code></pre>

<p>You can combine it with the answer by Chris Ruehlemann: </p>

<pre><code>words &lt;- c(""\\bland"", ""Sweden"", ""Government and People"", ""Government"", ""People"")
pattern &lt;- str_c(words,collapse = ""|"")
sol1 &lt;- unlist(str_extract_all(txt, pattern))

sol2 &lt;- return_partials(txt = txt, problem_patterns = c('land', 'den'))

&gt; unique(c(sol1, sol2))
[1] ""Sweden""                ""Government and People"" ""Netherland"" 
</code></pre>
",0,0,41,2020-05-25 14:12:13,https://stackoverflow.com/questions/62004190/how-to-mine-multiwords-from-a-given-text-in-r
How to mine multiwords by taking into account their position in the text?,"<p>I want to extract certain words positioned between years and the following comma in a given text. Although the term <code>Mining</code> appears before &amp; after <code>2020</code> in <code>text</code>, I need the later one which is found between <code>(2020)</code> and <code>,</code>. The same concept apply for the term <code>Computer Science</code> in the following <code>text</code>.</p>

<pre><code>library(stringr)
text &lt;- ""This is text Mining exercise (2020) Mining, p. 628508; Computer Science text analysis (1998) Computer Science, p.345-355; Introduction to data mining (2015) J. Data Science, pp. 31-33""
comp &lt;- c(""Mining"", ""Computer Science"", ""J. Data Science"")
pattern &lt;- str_c(comp,collapse =""|"")
data &lt;- str_extract_all(text, pattern)
</code></pre>

<p>The last line of the above code gives an output of:</p>

<pre><code>[1] ""Mining"" ""Mining"" ""Computer Science"" ""Computer Science"" ""J. Data Science"" 
</code></pre>

<p>The output that I'm looking for is:</p>

<pre><code>[1] ""Mining"" ""Computer Science"" ""J. Data Science"" 
</code></pre>

<p><strong>Note</strong>: The position of those words matter. Any help is highly appreciated!</p>
","r, string, text-mining","<p>If we need to extract between the <code>)</code> after the digit and the <code>,</code>, create a regex lookaround</p>

<pre><code>library(stringr)
str_extract_all(text, str_c(""(?&lt;=\\(\\d{4}\\)\\s)("", pattern, "")(?=,)""))[[1]]
#[1] ""Mining""           ""Computer Science"" ""J. Data Science"" 
</code></pre>
",2,2,71,2020-05-25 20:07:46,https://stackoverflow.com/questions/62009962/how-to-mine-multiwords-by-taking-into-account-their-position-in-the-text
Is there any function to extract the text which has a specific heading from pdf,"<p>I have multiple paragraphs in my pdf document. Each paragraph has a unique Heading to it. How can I extract the text from the pdf under a specific heading that I am looking for</p>
","python, pdf, text-mining","<p>you can use PyPDF2 python library for that, sample snippets : </p>

<pre><code># importing required modules
import PyPDF2

# creating a pdf file object
pdfFileObj = open('example.pdf', 'rb')

# creating a pdf reader object
pdfReader = PyPDF2.PdfFileReader(pdfFileObj)

# printing number of pages in pdf file
print(pdfReader.numPages)

# creating a page object
pageObj = pdfReader.getPage(0)

# extracting text from page
print(pageObj.extractText())

# closing the pdf file object
pdfFileObj.close()
</code></pre>
",0,-1,133,2020-05-26 07:27:21,https://stackoverflow.com/questions/62016738/is-there-any-function-to-extract-the-text-which-has-a-specific-heading-from-pdf
Extract from string information on date/time,"<p>I have some texts that generally starts with:</p>

<pre><code>“12 minutes ago - There was a meeting...”
“2 hours ago - Apologies for being...”
“1 day ago - It is a sunny day in London...”
</code></pre>

<p>and so on.
Basically I have information on:</p>

<pre><code>Minutes 
Hours
Day (starting from today)
</code></pre>

<p>I would like to transform this kind of information into valuable time serie information, in order to extract this part and create a new column from that (Datetime).
In my dataset, I have one column (Date) where I have already the date of when the research was performed (for example, today), in this format: 26/05/2020 and when the search was submitted (e.g. 8:41am). 
So if the text starts with “12 minutes ago”, I should have:</p>

<pre><code>26/05/2020 - 8:29 (datetime format in Python)
</code></pre>

<p>And for others:</p>

<pre><code>26/05/2020 - 6:41
25/05/2020 - 8:41
</code></pre>

<p>The important thing is to have something (string, numeric, date format) that I can plot as time series (I would like to see how many texts where posted in terms of time interval).
Any idea on how I could do this?</p>
","python, pandas, string, text-mining","<p>If the format stays simple : <code>&lt;digits&gt; &lt;unit&gt; ago ...</code> it's pretty to parse with <code>""^(\d+) (\w+) ago""</code>.</p>

<p>Then, once you have <code>('minutes', '12')</code> you'll pass these to <code>timedelta</code> which accepts every unit as a keyword argument <code>timedelta(minutes=12)</code>, you'll do that by passing a mapping <code>**{unit:value}</code></p>

<pre><code>def parse(content):
    timeparts = re.search(r""^(\d+) (\w+) ago"", content)
    if not timeparts:
        return None, content
    unit = timeparts.group(2).rstrip('s') + 's' # ensure ends with 's'
    #return datetime.now()-timedelta(**{unit:int(timeparts.group(1))})           # Now date
    return datetime(2020,5,26,8,0,0)-timedelta(**{unit:int(timeparts.group(1))}) # Fixed date
</code></pre>

<p>Demo</p>

<pre><code>values = [""12 minutes ago - There was a meeting..."",""2 hours ago - Apologies for being..."",""1 day ago - It is a sunny day in London...""]

for value in values:
  res = parse(value)
  print(res)


2020-05-26 07:48:00
2020-05-26 06:00:00
2020-05-25 08:00:00
</code></pre>
",2,0,407,2020-05-26 07:49:10,https://stackoverflow.com/questions/62017077/extract-from-string-information-on-date-time
R: Possible to extract groups of words from each sentence(rows)? and create data frame(or matrix)?,"<p>I created lists for each word to extract words from sentences, for example like this</p>

<pre><code>hello&lt;- NULL
for (i in 1:length(text)){
hello[i]&lt;-as.character(regmatches(text[i], gregexpr(""[H|h]ello?"", text[i])))
}
</code></pre>

<p>But I have more than 25 words list to extract, that's very long coding. 
<strong>Is it possible to extract a group of characters(words) from text data?</strong></p>

<p>Below is just a pseudo set.</p>

<pre><code>words&lt;-c(""[H|h]ello"",""you"",""so"",""tea"",""egg"")

text=c(""Hello! How's you and how did saturday go?"",  
       ""hello, I was just texting to see if you'd decided to do anything later"",
       ""U dun say so early."",
       ""WINNER!! As a valued network customer you have been selected"" ,
       ""Lol you're always so convincing."",
       ""Did you catch the bus ? Are you frying an egg ? "",
       ""Did you make a tea and egg?""
)

subsets&lt;-NULL
for ( i in 1:length(text)){
.....???
   }
</code></pre>

<p>Expected output as below</p>

<pre><code>[1] Hello you
[2] hello you
[3] you
[4] you so
[5] you you egg
[6] you tea egg
</code></pre>
","r, extract, text-mining","<p>You say that you have a long list of word-sets.  Here's a way to turn each wordset into a regex, apply it to a corpus (a list of sentences) and pull out the hits as character-vectors.  It's case-insensitive, and it checks for word boundaries, so you don't pull <em>age</em> out of <em>agent</em> or <em>rage</em>.</p>

<pre><code>wordsets &lt;- c(
  ""oak dogs cheese age"",
  ""fire open jail"",
  ""act speed three product""
)

library(tidyverse)
harvSent &lt;- read_table(""SENTENCE
    Oak is strong and also gives shade.
    Cats and dogs each hate the other.
    The pipe began to rust while new.
    Open the crate but don't break the glass.
    Add the sum to the product of these three.
    Thieves who rob friends deserve jail.
    The ripe taste of cheese improves with age.
    Act on these orders with great speed.
    The hog crawled under the high fence.
    Move the vat over the hot fire."") %&gt;% 
  pull(SENTENCE)
</code></pre>

<p><code>aWset</code> builds the regexs from the wordsets, and applies them to the sentences    </p>

<pre><code>aWset &lt;- function(harvSent, wordsets){
  # Turn out a vector of regex like ""(?ix) \\b (oak|dogs|cheese) \\b""
  regexS &lt;- paste0(""(?ix) \\b ("",
              str_replace_all(wordsets, "" "", ""|"" ),
               "") \\b"")
  # Apply each regex to the sentences
  map(regexS,
      ~  str_extract_all(harvSent, .x, simplify = TRUE) %&gt;% 
         # str_extract_all return a character matrix of hits.  Paste it together by row.
        apply( MARGIN = 1, 
               FUN = function(x){
                    str_trim(paste(x, collapse = "" ""))}))
}
</code></pre>

<p>Giving us</p>

<pre><code>aWset(harvSent , wordsets)
[[1]]
 [1] ""Oak""        ""dogs""       """"           """"           """"           """"           ""cheese age"" """"          
 [9] """"           """"          

[[2]]
 [1] """"     """"     """"     ""Open"" """"     ""jail"" """"     """"     """"     ""fire""

[[3]]
 [1] """"              """"              """"              """"              ""product three"" """"              """"             
</code></pre>
",2,1,318,2020-05-26 23:23:07,https://stackoverflow.com/questions/62033013/r-possible-to-extract-groups-of-words-from-each-sentencerows-and-create-data
Strings analysis: splitting strings into n parts by percentage of words,"<p>I'd need to calculate the length of each string included in the list: </p>

<pre><code>list_strings=[""I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best"",""So many books, so little time."",""In three words I can sum up everything I've learned about life: it goes on."",""if you tell the truth, you don't have to remember anything."",""Always forgive your enemies; nothing annoys them so much.""]
</code></pre>

<p>to split each of them into three parts: </p>

<ul>
<li>30 % (first part)</li>
<li>30 % (second part)</li>
<li>40 % (third part)</li>
</ul>

<p>I'd be able to calculate the length of each string into the list, but I do not know how to split each string into three parts and saved them. E.g.: 
the first sentence <code>""I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best""</code> has length 201 (tokenisation) so I'd need to take </p>

<ul>
<li>30% of 201 and save these words into an array (first 60 words approximately); </li>
<li>30% of the remaining (i.e. next 60 words); </li>
<li>finally 40%, i.e. the last 80 words.</li>
</ul>

<p>I read about the use of chunk but I've no idea on how I could apply. Also, I'd need a condition that can ensure me that I am taking integer (elements such words cannot be consider 1/2) words and I am not going beyond the length. </p>
","python, string, text-mining","<p>Splitting text according to percents on punctuation marks</p>

<pre><code>def split_text(s):
  """""" Partitions text into three parts
      in proportion 30%, 40%, 30%""""""

  i1 = int(0.3*len(s))  # first part from 0 to i1
  i2 = int(0.7*len(s))  # 2nd for i1 to i2, 3rd i2 onward

  # Use isalpha() to check when we are at a punctuation
  # i.e. . or ; or , or ? "" or ' etc.
  # Find nearest alphanumeric boundary
  # backup as long as we are in an alphanumeric
  while s[i1].isalpha() and i1 &gt; 0:
    i1 -= 1

  # Find nearest alphanumeric boundary (for 2nd part)
  while s[i2].isalpha() and i2 &gt; i1:
    i2 -= 1

  # Returns the three parts
  return s[:i1], s[i1:i2], s[i2:]


for s in list_strings:
  # Loop over list reporting lengths of parts
  # Three parts are a, b, c
  a, b, c = split_text(s)
  print(f'{s}\nLengths: {len(a)}, {len(b)}, {len(c)}')
  print()
</code></pre>

<p><strong>Output</strong></p>

<pre><code>I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best
Lengths: 52, 86, 63

So many books, so little time.
Lengths: 7, 10, 13

In three words I can sum up everything I've learned about life: it goes on.
Lengths: 20, 31, 24

if you tell the truth, you don't have to remember anything.
Lengths: 15, 25, 19

Always forgive your enemies; nothing annoys them so much.
Lengths: 14, 22, 21
</code></pre>

<p><strong>Output of split_text</strong></p>

<p><em>Code</em></p>

<pre><code>for s in list_strings:
    a, b, c = split_text(s)
    print(a)
    print(b)
    print(c)
    print()
</code></pre>

<p><em>Result</em></p>

<pre><code>I'm selfish, impatient and a little insecure. I make
 mistakes, I am out of control and at times hard to handle. But if you can't handle me
 at my worst, then you sure as hell don't deserve me at my best

So many
 books, so
 little time.

In three words I can
 sum up everything I've learned
 about life: it goes on.

if you tell the
 truth, you don't have to
 remember anything.

Always forgive
 your enemies; nothing
 annoys them so much.
</code></pre>

<p><strong>To Capture the Partitions</strong></p>

<pre><code>result_a, result_b, result_c = [], [], []
for s in list_strings:
      # Loop over list reporting lengths of parts
      # Three parts are a, b, c
      a, b, c = split_text(s)
      result_a.append(a)
      result_b.append(b)
      result_c.append(c)
</code></pre>
",1,1,259,2020-05-26 23:36:51,https://stackoverflow.com/questions/62033133/strings-analysis-splitting-strings-into-n-parts-by-percentage-of-words
Character-Matrix - how to operate by row?,"<p>I have a matrix of character data</p>

<pre><code>charMatrix &lt;- structure(c(""Bolt"", ""Nut Plate"", ""Magnet"", """", ""Clevis"", ""welded"", 
     """", ""Receptacle""), .Dim = c(4L, 2L))

[,1]            [,2]        
[1,] ""Bolt""      ""Clevis""    
[2,] ""Nut Plate"" ""welded""    
[3,] ""Magnet""    """"          
[4,] """"          ""Receptacle""
</code></pre>

<p>I want to <code>paste</code> the rows together and <code>trim</code> them to get the vector.</p>

<pre><code>[1] ""Bolt Clevis"" ""Nut Plate welded"" ""Magnet"" ""Receptacle""
</code></pre>

<p>I solved it this way, but I think there must be something much simpler, either in Base R or Tidyverse.</p>

<pre><code>vec &lt;- charMatrix %&gt;% t() %&gt;% 
  as_tibble(.name_repair = ""universal"") %&gt;% 
  summarise_all(~ str_trim(paste(., collapse = "" ""))) %&gt;% 
  unlist() %&gt;% 
  as.character()

vec
[1] ""Bolt Clevis""      ""Nut Plate welded"" ""Magnet""           ""Receptacle""     
</code></pre>

<p>Can you show me a more direct way to get this answer?</p>
","r, matrix, text-mining","<p>Since you have a matrix you can use rowwise <code>apply</code> and <code>trimws</code> to remove leading/trailing whitespaces </p>

<pre><code>trimws(apply(charMatrix, 1, paste, collapse = ' '))
#[1] ""BoltClevis""      ""Nut Platewelded"" ""Magnet""          ""Receptacle""     
</code></pre>

<p>Or remove empty values and paste. </p>

<pre><code>apply(charMatrix, 1, function(x) paste(x[x!=''], collapse = ' '))
</code></pre>
",1,2,56,2020-05-27 23:45:57,https://stackoverflow.com/questions/62054680/character-matrix-how-to-operate-by-row
Text Mining in R: Creating a Corpus creates unusual text,"<p>I'm reading in a single text file and my code below. It reads in fine but places a \t in random places throughout the corpus.</p>

<p>Examples:
<strong>Original in text file</strong>
5.  If you are responding as an individual,.....
<strong>In Corpus</strong>
""5.\tIf you are responding as an individual,...</p>

<p>or
Q1. What lessons can we learn from elsewhere....
""Q1.\tWhat lessons can we learn from elsewhere.....</p>

<p>It seems like a tab is being translated into a \t in the corpus</p>

<p>Any ideas how to fix this?</p>

<p>Thanks</p>

<pre><code># set pathway to text files
folder&lt;-""C:\\xxxxxx\\Text files""
folder
# lists all files in pathway 
list.files(path=folder)
# filters text files only
list.files(path=folder, pattern=""*.txt"")

# set vector
filelist&lt;-list.files(path=folder, pattern=""*.txt"")

# assign pathways to files
paste(folder, ""\\"", filelist)
# removes separations in pathways by setting as empty
filelist&lt;-paste(folder, ""\\"", filelist, sep="""")
filelist

# apply a function to read in multiple txt files - warnings are OK
a&lt;-lapply(filelist, FUN=readLines)
# apply a function to collaspe into a single element
corpus&lt;-lapply(a, FUN=paste, collaspe="" "")
</code></pre>
","r, text-mining","<p><code>gsub()</code> is a great function that will substitute all instances of a pattern with a different string. For your case, this should help:</p>

<pre><code># apply a function to read in multiple txt files - warnings are OK
a&lt;-lapply(filelist, FUN=readLines)
# apply a function to collaspe into a single element
corpus&lt;-lapply(a, FUN=paste, collaspe="" "")
# replace all '\t' with '   '
corpus&lt;-gsub(pattern = '\\\t', replacement = '', corpus)
</code></pre>
",2,1,73,2020-05-28 01:27:06,https://stackoverflow.com/questions/62055451/text-mining-in-r-creating-a-corpus-creates-unusual-text
"&quot;None of [Float64Index([nan, nan], dtype=&#39;float64&#39;)] are in the [index]&quot; setting col A value if col B contains string","<p>I have a dataframe (called <code>corpus</code>) with one column (<code>tweet</code>) and 2 rows:</p>

<pre><code>['check, tihs, out, this, bear, love, jumping, on, this, plant']
['i, can, t, bear, the, noise, from, that, power, plant, it, make, me, jump']
</code></pre>

<p>I have a list (called <code>vocab</code>) of unique words in the column:</p>

<pre><code>['check',
 'tihs',
 'out',
 'this',
 'bear',
 'love',
 'jumping',
 'on',
 'plant',
 'i',
 'can',
 't',
 'the',
 'noise',
 'from',
 'that',
 'power',
 'it',
 'make',
 'me',
 'jump']
</code></pre>

<p>I want to add a new column for each word in vocab. I want all values for the new columns to be zero, except for when the <code>tweet</code> contains the word, in which case I want the value of the word column to be 1.</p>

<p>So I tried running the code below:</p>

<pre><code>for word in vocab:
    corpus[word] = 0
    corpus.loc[corpus[""tweet""].str.contains(word), word] = 1
</code></pre>

<p>...and the following error was displayed:</p>

<pre><code>""None of [Float64Index([nan, nan], dtype='float64')] are in the [index]""
</code></pre>

<p>How can I check to see if the tweet contains the word, and then if so, set the value of the new column for the word to 1?</p>
","python, pandas, nlp, text-mining","<p>Your <code>corpus['tweet']</code> is list type, each is a skeleton. So <code>.str.contains</code> would returns <code>NaN</code>. You may want to do:</p>

<pre><code># turn tweets into strings
corpus[""tweet""] = [x[0] for x in corpus['tweet']]

# one-hot-encode
for word in vocab:
    corpus[word] = 0
    corpus.loc[corpus[""tweet""].str.contains(word), word] = 1
</code></pre>

<p>But then this may not be what you want, because <code>contains</code> will search for all substrings, e.g. <code>this girl goes to school</code> will returns <code>1</code> in both columns <code>is</code> and <code>this</code>.</p>

<p>Based on your data, you can do:</p>

<pre><code>corpus[""tweet""] = [x[0] for x in corpus['tweet']]

corpus = corpus.join(corpus['tweet'].str.get_dummies(', ')
                         .reindex(vocab, axis=1, fill_value=0)
                    )
</code></pre>
",1,1,7082,2020-05-28 15:45:48,https://stackoverflow.com/questions/62068860/none-of-float64indexnan-nan-dtype-float64-are-in-the-index-setting
Extract words from dataframe formatted text column,"<p>I need to create a new column from another one. 
The dataset is created by this code (I extracted only a few rows): </p>

<pre><code>import pandas as pd

new_dataframe = pd.DataFrame({
    ""Name"": ['John', 'Lukas', 'Bridget', 'Carol','Madison'],
    ""Notes"": [""__ years old. NA"", ""__ years old. NA"", 
        ""__ years old. NA"", ""__ years old. Old account."", 
        ""__ years old. New VIP account.""], 
    ""Status"": [True, False, True, True, True]})
</code></pre>

<p>which generates the following </p>

<pre><code>Name        Notes                           Status
John     23 years old. NA                    True
Lukas    52 years old. NA                    False
Bridget  64 years old. NA                    True
Carol    31 years old. Old account           True
Madison  54 years old. New VIP account.      True
</code></pre>

<p>I need to create two new columns that contain age information in the format: </p>

<ol>
<li>__ years old (three words): e.g. 23 years old;</li>
<li>__ (only numbers): e.g. 23</li>
</ol>

<p>At the end I should have</p>

<pre><code>Name        Notes                           Status          L_Age           S_Age
    John     23 years old. NA                    True      23 years old       23
    Lukas    52 years old. NA                    False     52 years old       52
    Bridget  64 years old. NA                    True      64 years old       64
    Carol    31 years old. Old account           True      31 years old       31
    Madison  54 years old. New VIP account.      True      54 years old       54
</code></pre>

<p>I do not know how to extract the first three words, then only the first, to create new columns. I have tried with</p>

<pre><code>new_dataframe.loc[new_dataframe.Notes == '', 'L_Age'] = new_dataframe.Notes.str.split()[:3]
new_dataframe.loc[new_dataframe.Notes == '', 'S_Age'] = new_dataframe.Notes.str.split()[0]
</code></pre>

<p>but it is wrong (<code>ValueError: Must have equal len keys and value when setting with an iterable</code>). </p>

<p>Help will be appreciated.</p>
","python, pandas, text-mining","<p>You can use this pattern to extract the information and join:</p>

<pre><code>pattern = '^(?P&lt;L_Age&gt;(?P&lt;S_Age&gt;\d+) years? old)'

new_dataframe = new_dataframe.join(new_dataframe.Notes.str.extract(pattern))
</code></pre>

<p>Output:</p>

<pre><code>      Name                           Notes  Status         L_Age S_Age
0     John                23 years old. NA    True  23 years old    23
1    Lukas                52 years old. NA   False  52 years old    52
2  Bridget                64 years old. NA    True  64 years old    64
3    Carol       31 years old. Old account    True  31 years old    31
4  Madison  54 years old. New VIP account.    True  54 years old    54
</code></pre>
",2,4,446,2020-05-28 18:40:51,https://stackoverflow.com/questions/62072170/extract-words-from-dataframe-formatted-text-column
Combining two columns to create a datetime object,"<p>I would need to create a datetime by combining two columns in a dataframe. 
My original dataset has these columns: </p>

<pre><code>Date                Time
05/29/2020         00:12
05/29/2020         00:32
05/28/2020         00.59
</code></pre>

<p>I would like to create a new column which combines <code>Date + Time</code>, i.e. which creates a datetime object:</p>

<pre><code>Date_time
05/22/2020 00:12
05/22/2020 00:32
05/28/2020 00:59
</code></pre>

<p>I used </p>

<pre><code>df['Time'] = df['Time'].astype('datetime64[ns]') 
</code></pre>

<p>but the output gave me today's date plus values from column <code>Time 2020-05-29 00:22:00</code>.
How could I get the right date by using the columns Date and Time above?</p>
","python, pandas, string, text-mining","<p>You can try </p>

<pre><code>df['Date_time']=pd.to_datetime(df.Date+ ' '+df.Time, format='%m/%d/%Y %H:%M:%S')
</code></pre>
",1,0,748,2020-05-28 23:56:08,https://stackoverflow.com/questions/62076437/combining-two-columns-to-create-a-datetime-object
Store multiple corpus via for loop by different names,"<p>I have multiple text documents per ticker which I want to store as an individual corpus. 
I've read about creating ''lists in lists'', but this doesn't work for me. For example, ''text mining and termdocumentmatrix'' give the following error: <em>no applicable method for 'TermDocumentMatrix' applied to an object of class ""list</em>.</p>

<p>I could possibly put everything within the for loop, but that's not what I want since I want some flexibility to play with the corpus.</p>

<p>Could someone help me out how I can effectively work around this problem? My code is below. Thank you in advance!</p>

<pre><code>Stocks &lt;- list(""AAPL"", ""AMZN"", ""BIG"", ""BYD"", ""CTWS"", ""EAT"", ""FB"", ""GOOG"", ""GRMC"", ""HRL"", ""MGM"", ""MSFT"",
               ""NEM"", ""PKS"", ""RGLD"", ""SCCO"", ""SLP"", ""TCO"", ""USGL"", ""WDFC""
)

BigList &lt;- list()
for (stock in Stocks) {
  filepath &lt;- file.path(""C:/Users/......./Stocks10K"", stock)
  a &lt;- Corpus(DirSource(filepath))
  a &lt;- tm_map(a, removePunctuation)
  a &lt;- tm_map(a, removeNumbers)
  a &lt;- tm_map(a, tolower)
  a &lt;- tm_map(a, removeWords, stopwords(""en""))
  a &lt;- tm_map(a, stripWhitespace)
  name &lt;- paste('Data:', stock, sep='')
  tmp &lt;- list(Text = a)
  BigList[name] &lt;- tmp
  rm(tmp, stock, name, filepath, a)
}

#Create Term Document Matrix and create Matrix
tdm &lt;- TermDocumentMatrix(BigList['Data:AAPL'])
m &lt;- as.matrix(tdm)
</code></pre>
","r, for-loop, text-mining, tm, corpus","<p>It looks like you've done everything right, except getting your entry out of <code>BigList</code>. <code>[</code> will return a list (containing one element in your case) - you need <code>[[</code> instead.  Try:</p>

<pre><code>tdm &lt;- TermDocumentMatrix(BigList[['Data:AAPL']])
</code></pre>

<p>instead.</p>

<p><a href=""https://cran.r-project.org/doc/manuals/R-lang.html#Indexing"" rel=""nofollow noreferrer"">https://cran.r-project.org/doc/manuals/R-lang.html#Indexing</a> has more info, including this note (in case what I said above isn't clear):</p>

<blockquote>
  <p>For lists, one generally uses [[ to select any single element, whereas
  [ returns a list of the selected elements.</p>
</blockquote>
",0,1,76,2020-05-29 11:51:51,https://stackoverflow.com/questions/62085340/store-multiple-corpus-via-for-loop-by-different-names
Text comparison based on numbers/digits,"<p>I would need to compare texts by extracting only numbers from the following two texts:  </p>

<pre><code>text_1=""source=""The previous low was 27,523, recorded in May 1900. The 1.35 trillion ($22.5 million ) program could start in October. The number of people who left the country plunged 99.8 percent from a year earlier to 2,750, according to the data from the agency.""

text_2=""The subsidies, totalling 1.35tn, are expected to form part of a second budget. New plans to allocate $22.5 billion to a new reimbursement programme.""
</code></pre>

<p>However, it seems also to be relevant the next words (for example trillion /tn, billion). 
Do you know how I could get this information?</p>

<p>I have tried with</p>

<pre><code>t_1=[int(s) for s in text_1.split() if s.isdigit()]
t_2=[int(s) for s in text_2.split() if s.isdigit()]
</code></pre>

<p>then to compare them, but it gives me not all numbers in texts. </p>

<p>Expected output: </p>

<pre><code>differences

text_1: {27,523, 1900, 99.8, 2,750}

text_2: {}

common
    {1.35,22.5}
</code></pre>
","python, text-mining","<p>It is not impossible to it do the way you propose, but that is best achieved with regular expressions:</p>

<pre><code>import re

text_1=""The previous low was 27,523, recorded in May 1900. The 1.35 trillion ($22.5 million ) program could start in October. The number of people who left the country plunged 99.8 percent from a year earlier to 2,750, according to the data from the agency.""

print(re.findall(""\d+[,.\d]\d+"", text_1))
</code></pre>

<p>In case you are not familiar with it, check <a href=""https://www.rexegg.com/regex-quickstart.html"" rel=""nofollow noreferrer"">cheatsheet</a> and try it with <a href=""https://regex101.com/"" rel=""nofollow noreferrer"">tester</a>. Once you got that, it is straight forward to get your expected output:</p>

<pre><code>nums_1 = re.findall(""\d+[,.\d]\d+"", text_1)
nums_2 = re.findall(""\d+[,.\d]\d+"", text_2)

common_nums = []
for num in nums_1:
  if num in nums_2: common_nums.append(num)

print(common_nums)
</code></pre>
",0,-1,39,2020-06-01 00:59:56,https://stackoverflow.com/questions/62124265/text-comparison-based-on-numbers-digits
Remove empty rows within a dataframe and check similarity,"<p>I am having some difficulties to select not empty fields using regex (findall) within my dataframe, looking for words contained into a text source:</p>

<pre><code>text = ""Be careful otherwise police will capture you quickly.""
</code></pre>

<p>I will need to look for words that ends with <code>ful</code> in my text string, then looking for words that ends with full in my dataset. </p>

<pre><code>Author      DF_Text

31       Better the devil you know than the one you don't      
53       Beware the door with too many keys.      
563      Be careful what you tolerate. You are teaching people how to treat you. 
41       Fear the Greeks bearing gifts.      
539      NaN
51       The honey is sweet but the bee has a sting.      
21       Be careful what you ask for; you may get it.
</code></pre>

<p>(from csv/txt file).
I need to extract words ending with <code>ful</code> in <code>text</code>, then look at both DF_Text (thus Author) which contains words ending with <code>ful</code> and appending results in a list.    </p>

<pre><code>n=0
for i in df['DF_Text']:
        print(re.findall(r""\w+ful"", i))
        n=n+1
        print(n)
</code></pre>

<p>My question is: how can I remove empty rows(<code>[]</code>) from the analysis (<code>NaN</code>) and report the author names (e.g. <code>563</code>, <code>21</code>) related to?
I will be happy to provide further information, in case it would be not clear. </p>
","python, regex, pandas, text-mining","<p>Use <code>str.findall</code> instead of looping with <code>re.findall</code>:</p>

<pre><code>df[""found""] = df[""DF_Text""].str.findall(r""(\w+ful)"")

df.loc[df[""found""].str.len().eq(0),""found""] = df[""Author""]

print (df)

   Author                                            DF_Text      found
0      31   Better the devil you know than the one you don't         31
1      53                Beware the door with too many keys.         53
2     563  Be careful what you tolerate. You are teaching...  [careful]
3      41                     Fear the Greeks bearing gifts.         41
4     539                                                NaN        NaN
5      51        The honey is sweet but the bee has a sting.         51
6      21       Be careful what you ask for; you may get it.  [careful]
</code></pre>
",1,1,249,2020-06-01 02:59:02,https://stackoverflow.com/questions/62125050/remove-empty-rows-within-a-dataframe-and-check-similarity
How to turn txt file to nice dataframe,"<p>I have a txt file containing Track ID, Song ID, Artist Name and Song name. I'd like to convert it into a dataframe in R to do some analysis. What would be a good function to use to separate the data? Below is the top row of the dataset. Thanks!</p>

<pre><code>TRMMMKD128F425225D&lt;SEP&gt;SOVFVAK12A8C1350D9&lt;SEP&gt;Karkkiautomaatti&lt;SEP&gt;Tanssi vaan
</code></pre>
","r, text-mining","<p>We can use <code>read.table</code> to directly read the file as a dataframe but separator (<code>sep</code>) between columns can be of only one character. </p>

<p>So we can first use <code>readLines</code> to read the text file, replace <code>'&lt;SEP&gt;'</code> using <code>gsub</code> with a single character (<code>'\t'</code>) and then use <code>read.table</code> specifying column names. </p>

<pre><code>data &lt;- read.table(text = gsub('&lt;SEP&gt;', '\t', 
         readLines('filename.txt'), fixed = TRUE), 
         col.names = c('TrackID', 'SongID', 'ArtistName', 'SongName'),sep = ""\t"")
data

#             TrackID             SongID       ArtistName    SongName
#1 TRMMMKD128F425225D SOVFVAK12A8C1350D9 Karkkiautomaatti Tanssi vaan
</code></pre>
",2,0,168,2020-06-02 21:24:42,https://stackoverflow.com/questions/62161650/how-to-turn-txt-file-to-nice-dataframe
Merge several txt. files with multiple lines to one csv file (1 line = 1 document) for Topic Modeling,"<p>I have 30 text files so far which all have <strong>multiple</strong> lines. I want to apply a LDA Model based on <a href=""https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"" rel=""nofollow noreferrer"">this tutorial</a> .
So, for me it should look this:</p>

<pre><code>text of document1
text of document2
text of document3 
.....
text of document30
</code></pre>

<p>But the whole text of a specific document has to be on <strong>one</strong> line.</p>

<p>I tried <a href=""https://stackoverflow.com/questions/47073072/converting-1000-text-files-into-a-single-csv-file"">this post</a> and for some reason it keeps saying: <code>csv_output.writerow(row[1] for row in csv_text) IndexError: list index out of range</code> . Any thoughts? I named the documents in a same way and edited the range, of course.</p>

<p>Basically, I don't care if we can solve this problem with python or not. I'm just done with my nerves so I really appreciate every help</p>
","python, export-to-csv, text-mining, lda, topic-modeling","<p>I'm not exactly sure what you are trying to accomplish, but to remove the newlines for textfiles and make one big text file with the results, something among the following should work:</p>

<pre><code>for i in *.txt; do NEW=` cat $i | tr '\n' ' '` ; echo $NEW  &gt;&gt; output.txt; done
</code></pre>
",0,0,153,2020-06-03 14:56:11,https://stackoverflow.com/questions/62175969/merge-several-txt-files-with-multiple-lines-to-one-csv-file-1-line-1-documen
How does gensim word2vec word embedding extract training word pair for 1 word sentence?,"<p>Refer to below image (the process of how word2vec skipgram extract training datasets-the word pair from the input sentences). </p>

<p>E.G. ""I love you."" ==> [(I,love), (I, you)]</p>

<p>May I ask what is the word pair when the sentence contains only one word? </p>

<p>Is it  ""Happy!"" ==> [(happy,happy)] ?</p>

<p>I tested the word2vec algorithm in genism, when there is just one word in the training set sentences, (and this word is not included in other sentences), the word2vec algorithm can still construct an embedding vector for this specific word. I am not sure how the algorithm is able to do so.</p>

<p><a href=""https://i.sstatic.net/zQPX6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zQPX6.png"" alt=""enter image description here""></a></p>

<p>===============UPDATE===============================</p>

<p>As the answer posted below, I think the word embedding vector created for the word in the 1-word-sentence is just the random initialization of neural network weights.</p>
","nlp, text-mining, gensim, word2vec, word-embedding","<p>No word2vec training is possible from a 1-word sentence, because there's no neighbor words to use as input to predict a center/target word. Essentially, that sentence is skipped.</p>

<p>If that was the only appearance of the word in the corpus, and you're seeing a vector for that word, it's just the starting random-initialization of the word, with no further training. (And, you should probably use a higher <code>min_count</code>, as keeping such rare words is usually a mistake in word2vec: they won't get good vectors, and other nearby words' vectors will improve if the 'noise' from all such insufficiently model-able rare words is removed.)</p>

<p>If that 1-word sentence actually appeared next-to other real sentences in your corpus, it could make sense to combine it with surrounding texts. There's nothing magic about actual sentences for this kind word-from-surroundings modeling - the algorithm is just working on 'neighbors', and it's common to use multi-sentence chunks as the texts for training, and sometimes even punctuation (like sentence-ending periods) is also retained as 'words'. Then words from an actually-separate sentence – but still related by having appeared in the same document – will appear in each other's contexts.</p>
",1,0,728,2020-06-05 08:42:07,https://stackoverflow.com/questions/62211396/how-does-gensim-word2vec-word-embedding-extract-training-word-pair-for-1-word-se
Text Mining in R: Counting 2-3 word phrases,"<p>I found a very useful piece of code within Stackoverflow - <a href=""https://stackoverflow.com/questions/8898521/finding-2-3-word-phrases-using-r-tm-package"">Finding 2 &amp; 3 word Phrases Using R TM Package</a>
(credit @patrick perry) to show the frequency of 2 and 3 word phrases within a corpus:</p>

<pre><code>library(corpus)
corpus &lt;- gutenberg_corpus(55) # Project Gutenberg #55, _The Wizard of Oz_
text_filter(corpus)$drop_punct &lt;- TRUE # ignore punctuation
term_stats(corpus, ngrams = 2:3)
##    term             count support
## 1  of the             336       1
## 2  the scarecrow      208       1
## 3  to the             185       1
## 4  and the            166       1
## 5  said the           152       1
## 6  in the             147       1
## 7  the lion           141       1
## 8  the tin            123       1
## 9  the tin woodman    114       1
## 10 tin woodman        114       1
## 11 i am                84       1
## 12 it was              69       1
## 13 in a                64       1
## 14 the great           63       1
## 15 the wicked          61       1
## 16 wicked witch        60       1
## 17 at the              59       1
## 18 the little          59       1
## 19 the wicked witch    58       1
## 20 back to             57       1
## ⋮  (52511 rows total)
</code></pre>

<p>How do you ensure that frequency counts of phrases like ""the tin"" are not also included in the frequency count of ""the tin woodman"" or the ""tin woodman""?</p>

<p>Thanks</p>
","r, text-mining","<p>Removing stopwords can remove noise from the data, causing issues such as those you are having a above: </p>

<pre><code>library(tm)
library(corpus)
library(dplyr)
corpus &lt;- Corpus(VectorSource(gutenberg_corpus(55)))
corpus &lt;- tm_map(corpus, content_transformer(tolower))
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, removeWords, stopwords(""english""))
term_stats(corpus, ngrams = 2:3) %&gt;% 
  arrange(desc(count)) %&gt;%
  group_by(grp = str_extract(as.character(term), ""\\w+\\s+\\w+"")) %&gt;% 
  mutate(count_unique = ifelse(length(unique(count)) &gt; 1, max(count) - min(count), count)) %&gt;% 
  ungroup() %&gt;% 
  select(-grp)
</code></pre>
",1,0,437,2020-06-06 09:32:25,https://stackoverflow.com/questions/62229830/text-mining-in-r-counting-2-3-word-phrases
"How to clean abbreviations containing a &quot;period-punctuation&quot; (&quot;e.g.&quot;, &quot;st.&quot;, &quot;rd.&quot;) but leave the &quot;.&quot; at the end of a sentence?","<p>I am working on a sentence-level LDA in R and am currently trying to split my text data into individual sentences with the <code>sent_detect()</code> function from the <code>openNLP</code> package. </p>

<p>However, my text data contains a lot of abbreviations that have a ""period symbol"" but do <strong>not</strong> mark the end of a sentence. Here are some examples: ""<strong>st.</strong> patricks day"", ""oxford <strong>st.</strong>"", ""blue <strong>rd.</strong>"", ""<strong>e.g.</strong>""</p>

<p>Is there a way to create a <code>gsub()</code> function to account for such 2-character abbreviations and remove their "".""-symbol so that it is not wrongly detected by the <code>sent_detect()</code> function? Unfortunately, these abbreviations are not always in between two words but sometimes they could indeed also mark the end of a sentence:</p>

<p>Example:</p>

<blockquote>
  <p><em>""I really liked Oxford st.""</em> - the ""st."" marks the end of a sentence and the ""."" should remain.</p>
</blockquote>

<p>vs</p>

<blockquote>
  <p><em>""Oxford st. was very busy.""</em> - the ""st."" does not stand at the end of a sentence, thus, the "".""-symbol should be replaced. </p>
</blockquote>

<p>I am not sure whether there is a solution for this, but maybe someone else who is more familiar with sentence-level analysis knows a way of how to deal with such issues.
Thank you!</p>
","r, regex, text-mining, topic-modeling","<p>Looking at your previously asked questions, I would suggest looking into the textclean package. A lot of what you want has been included in that package. Any missing functions can be appropriated or reused or expanded upon. </p>

<p>Just replacing ""st."" with something is going to lead to problems as it could mean street or saint, but ""st. patricks day"" is easy to find. The problem what you will have is to make a list of possible occurences and find alternatives for them. The easiest to use are translation tables. Below I create a table for a few abbreviations and their expected long names. Now it is up to you (or your client) to specify what you want as an end result. The best way is to create a table in excel or database and load this into a data.frame (and store somewhere for easy access). Depending on your text this might be a lot of work, but it will improve the quality of your outcome.</p>

<p>Example:</p>

<pre><code>library(textclean)

text &lt;- c(""I really liked Oxford st."", ""Oxford st. was very busy."",
          ""e.g. st. Patricks day was on oxford st. and blue rd."")


# Create abbreviations table, making sure that we are looking for rd. and not just rd. Also should it be road or could it mean something else?

abbreviations &lt;- data.frame(abbreviation = c(""st. patricks day"", ""oxford st."", ""rd\\."", ""e.g.""),
                            replacement = c(""saint patricks day"",""oxford street"",""road"", ""eg""))


# I use the replace_contraction function since you can replace the default contraction table with your own table.

text &lt;- replace_contraction(text, abbreviations)

text
[1] ""I really liked oxford street""                             ""oxford street was very busy.""                            
[3] ""eg saint patricks day was on oxford street and blue road""

# as the result from above show missing end marks we use the following function to add them again.

text &lt;- add_missing_endmark(text, ""."")

text
[1] ""I really liked oxford street.""                             ""oxford street was very busy.""                             
[3] ""eg saint patricks day was on oxford street and blue road.""
</code></pre>

<p>textclean has a range of replace_zzz functions, most are based on the <code>mgsub</code> function that is in the package. Check the documentation with all the functions to get an idea of what they do. </p>
",1,0,324,2020-06-06 16:20:50,https://stackoverflow.com/questions/62234590/how-to-clean-abbreviations-containing-a-period-punctuation-e-g-st-rd
Extract values and attributes from a list and convert them into a dataframe in R,"<p>I got the following list for my model:
</p>

<pre><code>List of 9
 $ phi           : num [1:5, 1:1500] 1.8e-04 1.8e-04 1.8e-04 1.8e-04 1.8e-04 ...
  ..- attr(*, ""dimnames"")=List of 2
  .. ..$ : chr [1:5] ""t_1"" ""t_2"" ""t_3"" ""t_4"" ...
  .. ..$ : chr [1:1500] ""word1"" ""word2"" ""word3"" ""word4"" ...
 $ theta         : num [1:500, 1:5] 0.1234 0.4567 0.01234 0.04567 0.02345 ...
  ..- attr(*, ""dimnames"")=List of 2
  .. ..$ : chr [1:500] ""1"" ""2"" ""3"" ""4"" ...
  .. ..$ : chr [1:5] ""t_1"" ""t_2"" ""t_3"" ""t_4"" ...
 $ gamma         : num [1:5, 1:1500] 0.20 0.70 0.10 0.1 0.11 ...
  ..- attr(*, ""dimnames"")=List of 2
  .. ..$ : chr [1:5] ""t_1"" ""t_2"" ""t_3"" ""t_4"" ...
  .. ..$ : chr [1:1500] ""word1"" ""word2"" ""word3"" ""word4"" ...
 $ data          :Formal class 'dgCMatrix' [package ""Matrix""] with 6 slots
  .. ..@ i       : int [1:10000] 1234 6789 2233 1367 1123 1123 145 145 156 1325 ...
  .. ..@ p       : int [1:1500] 0 1 2 3 4 5 6 7 8 9 ...
  .. ..@ Dim     : int [1:2] 1234 1500
  .. ..@ Dimnames:List of 2
  .. .. ..$ : chr [1:500] ""1"" ""2"" ""3"" ""4"" ...
  .. .. ..$ : chr [1:1500] ""word1"" ""word2"" ""word3"" ""word4"" ...
  .. ..@ x       : num [1:100000] 1 1 1 1 1 1 1 1 1 1 ...
  .. ..@ factors : list()
 $ alpha         : Named num [1:5] 0.1 0.1 0.1 0.1  ...
  ..- attr(*, ""names"")= chr [1:5] ""t_1"" ""t_2"" ""t_3"" ""t_4"" ...
 $ beta          : Named num [1:1500] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 ...
  ..- attr(*, ""names"")= chr [1:1500] ""word1"" ""word2"" ""word3"" ""word4""
</code></pre>

<p>Is there a way of how to select $theta and all its attributes and save them as a data frame? In other words, I want to extract this part from the list: </p>

<pre><code>$ theta         : num [1:500, 1:5] 0.1234 0.4567 0.01234 0.04567 0.02345 ...
  ..- attr(*, ""dimnames"")=List of 2
  .. ..$ : chr [1:500] ""1"" ""2"" ""3"" ""4"" ...
  .. ..$ : chr [1:5] ""t_1"" ""t_2"" ""t_3"" ""t_4"" ...
</code></pre>

<p>and have a dataframe that looks like this (the column order does not matter): </p>

<pre><code>Theta  | var1 | var2 |
0.1234 | 1    | t_1  |
0.4567 | 2    | t_2  |
0.01234| 3    | t_3  |
</code></pre>

<p>I have tried <code>lapply</code> and many other suggestions that I found in terms of list extraction  but failed to extract the part shown above.</p>

<p>Thanks a lot!</p>
","r, list, dataframe, lapply, text-mining","<p>As already metioned in comments, you can easily access $theta with list subsetting either <code>model$theta</code> or <code>model[['theta']]</code>.</p>

<p>$theta is a numeric matrix 500 x 5. To convert it into desirable format just melt it:</p>

<pre><code>theta_matrix = model$theta
theta_df = reshape2::melt(theta_matrix, value.name = ""Theta"")
</code></pre>
",4,2,2784,2020-06-11 20:46:52,https://stackoverflow.com/questions/62333298/extract-values-and-attributes-from-a-list-and-convert-them-into-a-dataframe-in-r
R: How to delete words other than specific words in a corpus,"<p>In the corpus ""tkn_pb"" , I would like to delete all words except for some keywords I chose (ex. ""attack"" and ""gunman""). Is it possicle to do this?</p>

<p><a href=""https://i.sstatic.net/mURs4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mURs4.png"" alt=""enter image description here""></a></p>
","r, text-mining, corpus","<p>You can use <code>which</code>and <code>grepl</code> to subset your corpus:</p>

<p>Data:</p>

<pre><code>sample_tokens &lt;- c(""word"", ""another"",""a"", ""new"", ""word token"", ""one"", ""more"", ""and"", ""another one"")
</code></pre>

<p>Remove all words except ""a"" and ""and"":</p>

<pre><code>sample_tokens[which(grepl(""\\b(a|and)\\b"", sample_tokens))]
[1] ""a""   ""and""
</code></pre>

<p><strong>EDIT</strong>:</p>

<p>If the corpus is a list, then this solution suggested by @John would work:</p>

<p>Data:</p>

<pre><code>sample_tokens &lt;- list(c(""word"", ""another"",""a"", ""new"", ""word token"", ""one"", ""more"", ""and"", ""another one""),
               c(""yet"", ""a"", ""few"", ""more"", ""words""),
               c(""and"", ""so on""))

lapply(sample_tokens, function(x) x[which(grepl(""\\b(a|and)\\b"", x))])
[[1]]
[1] ""a""   ""and""

[[2]]
[1] ""a""

[[3]]
[1] ""and""
</code></pre>
",2,0,394,2020-06-12 07:59:49,https://stackoverflow.com/questions/62339990/r-how-to-delete-words-other-than-specific-words-in-a-corpus
Merge two dataframes in R column-wise and sort columns by one value,"<p>I have two dataframes in R that look like the following examples:</p>

<pre><code>Dataframe 1 
|word  |a1 |a2 |a3 |...|
|apple |0.5|0.3|0.2|...|
|pear  |0.2|0.2|0.6|...|
|banana|0.6|0.1|0.3|...|
|cherry|0.4|0.5|0.1|...|

    Dataframe 2 
|a1     |a2     | a3    |...|
|banana |cherry |pear   |...|
|apple  |apple  |banana |...|
|cherry |pear   |apple  |...|
|pear   |banana |cherry |...|
</code></pre>

<p>The names in Dataframe 2 are sorted by their value in Dataframe 1 - these are the top terms I got from the <code>textmineR</code> package with the <code>GetTopTerms</code> function from my model. However, I do not know how I can combine the phi values I have with each word that the value belongs to. In other words, what I want as an output is a combination of the two dataframes above - where the phi value is listed from highest to lowest in each single column as seen below: </p>

<pre><code>|a1_term |a1_phi | a2_term |a2_phi | a3_term  |a3_phi |...|
|banana  |0.6    |cherry   |0.5    |pear      |0.6    |...|
|apple   |0.5    |apple    |0.3    |banana    |0.3    |...|  
|cherry  |0.4    |pear     |0.2    |apple     |0.2    |...|
|pear    |0.2    |banana   |0.1    |cherry    |0.1    |...|
</code></pre>

<p>Is there an easy function to merge these two tables as seen above as well as to sort each phi-value from lowest to highest while merging. Thank you!</p>
","r, dataframe, merge, text-mining","<p>Here's a solution using <code>dplyr</code> and <code>reshape2</code>. If you are sorting by phi, you don't need the second data frame. Here, <code>df</code> is the first data frame.</p>

<pre><code>library(dplyr)
library(reshape2)
library(tidyselect)

do.call(""cbind"", melt(df) %&gt;%
                 split(.$variable) %&gt;% 
                 lapply(function(x) x %&gt;% arrange(-value))) %&gt;% 
select(!ends_with(""variable""))

#&gt;   a1.word a1.value a2.word a2.value a3.word a3.value
#&gt; 1  banana      0.6  cherry      0.5    pear      0.6
#&gt; 2   apple      0.5   apple      0.3  banana      0.3
#&gt; 3  cherry      0.4    pear      0.2   apple      0.2
#&gt; 4    pear      0.2  banana      0.1  cherry      0.1

</code></pre>

<hr>

<p><strong>Data</strong></p>

<pre><code>df &lt;- structure(list(word = c(""apple"", ""pear"", ""banana"", ""cherry""), 
    a1 = c(0.5, 0.2, 0.6, 0.4), a2 = c(0.3, 0.2, 0.1, 0.5), a3 = c(0.2, 
    0.6, 0.3, 0.1)), class = ""data.frame"", row.names = c(NA, -4L))

df
#&gt;     word  a1  a2  a3
#&gt; 1  apple 0.5 0.3 0.2
#&gt; 2   pear 0.2 0.2 0.6
#&gt; 3 banana 0.6 0.1 0.3
#&gt; 4 cherry 0.4 0.5 0.1

</code></pre>
",4,0,156,2020-06-13 10:37:07,https://stackoverflow.com/questions/62358565/merge-two-dataframes-in-r-column-wise-and-sort-columns-by-one-value
R- Word co-occurrence frequency within paragraph,"<p>The dataset contains text data of 26 news articles.
I would like to count word co-occurrence frequency within each paragraph, but it seems that my codes below are doing within a document (a whole article). 
Can you designate the level (sentence, paragraph...) for calculating co-occurrence frequency with fcm()? 
Or is there any other package to do so?</p>

<pre><code>library(quanteda)
library(readtext)
library(tm)

##corpus
tf_pb &lt;- readtext(""PB_articles.csv"",text_field = ""text"")
tf2_pb  &lt;- gsub(pattern = ""\\b(rifle|rifles|weapon|weapons)\\b"", replace = ""gun"", x = tf_pb)
corpus_pb &lt;- corpus(tf2_pb)

summary(corpus_pb)

##Tokenization&amp;Cleaning
tkn_pb &lt;- tokens(corpus_pb,
                 remove_url = TRUE,
                 remove_numbers = TRUE,
                 remove_punct = TRUE,
                 remove_symbols = TRUE,
                 remove_separators = TRUE)

##removeing stopwords &amp; stemming
stm_pb &lt;- tokens_wordstem(tkn_pb)
stw_pb &lt;- tokens_remove(stm_pb, pattern = stopwords('en'))


##multi-word expression
multiword &lt;- c(""social media"", ""house of worship"")
comp_toks &lt;- tokens_compound(stw_pb, pattern = phrase(multiword))
comp_toks

##keyword_list
kw_pb &lt;- lapply(comp_toks, function(x){ x[which(grepl(""\\b(synagogu|jewish|rabbi|jew|anti-semit|pittsburgh|congre|communiti|member|hous|worship|weapon|rifle|gun|shooting|assault|attack|hate|hatr|twitter|social_media|morn|gab|white|post|gun|funer|victim|prayer|rabinowitz|suspect|religi|account|nation|door|friend|charge|shiva|wax|speech|home|leader)\\b"", x))]})
head(kw_pb)

##tokenにする
tkn2_pb &lt;- as.tokens(kw_pb)

##remove words
tkn3_pb &lt;- tokens_select(tkn2_pb,c(""next-door"",""consumer-friend"",""ar-15-gun-mass-shootings.html"",
                                 ""hate-fuel"",""hate-fil"",""prayer.ImageOutsid"",""door.Th"",
                                 ""we-need-to-stop-the-hate-rabbi-tells-elected-leaders.html"",""speech.CreditCredit"",
                                 ""hate-rel"",""shooting.Credit"",""shooting.Polic"",""pittsburgh-shooting-synagogue.html"",
                                 ""media.Ar"",""shooting.Speedi"",""speech.Gab"",""shooting.Thi"",""shooting.On"",""gun-control"",
                                 ""gun.ImageAR-15"",""shooting.In"",""gun-safeti"",""pic.twitter.com"",""post-World"",""home.But"",""worship.Th""),
                         selection = ""remove"", padding = FALSE)

##co-occurrence frequency
fcm_pb &lt;- fcm(tkn3_pb,
             count = ""frequency"")

</code></pre>
","r, text-mining, quanteda","<p>The answer is first to reshape the corpus into paragraphs, so that the new ""documents"" are then paragraphs from the original documents, and then compute the fcm with a ""document"" co-occurrence context.  </p>

<p>Here's an example you can adapt, using the first three documents from the built-in inaugural address corpus.</p>



<pre class=""lang-r prettyprint-override""><code>library(""quanteda"")
## Package version: 2.0.1

data_corpus_inauguralpara &lt;-
  corpus_reshape(data_corpus_inaugural[1:3], to = ""paragraphs"")
summary(data_corpus_inauguralpara)
## Corpus consisting of 23 documents, showing 23 documents:
## 
##               Text Types Tokens Sentences Year  President FirstName      Party
##  1789-Washington.1     8     11         1 1789 Washington    George       none
##  1789-Washington.2   184    341         5 1789 Washington    George       none
##  1789-Washington.3   192    328         6 1789 Washington    George       none
##  1789-Washington.4   214    391         5 1789 Washington    George       none
##  1789-Washington.5   120    182         2 1789 Washington    George       none
##  1789-Washington.6   102    164         4 1789 Washington    George       none
##  1789-Washington.7    88    120         1 1789 Washington    George       none
##  1793-Washington.1    47     64         2 1793 Washington    George       none
##  1793-Washington.2    61     83         2 1793 Washington    George       none
##       1797-Adams.1   114    180         2 1797      Adams      John Federalist
##       1797-Adams.2    88    137         3 1797      Adams      John Federalist
##       1797-Adams.3    63    101         1 1797      Adams      John Federalist
##       1797-Adams.4    60     82         3 1797      Adams      John Federalist
##       1797-Adams.5   145    277         6 1797      Adams      John Federalist
##       1797-Adams.6    62    108         2 1797      Adams      John Federalist
##       1797-Adams.7    16     17         1 1797      Adams      John Federalist
##       1797-Adams.8   158    303         8 1797      Adams      John Federalist
##       1797-Adams.9    97    184         4 1797      Adams      John Federalist
##      1797-Adams.10    80    128         1 1797      Adams      John Federalist
##      1797-Adams.11    74    119         3 1797      Adams      John Federalist
##      1797-Adams.12   329    808         1 1797      Adams      John Federalist
##      1797-Adams.13    51     75         1 1797      Adams      John Federalist
##      1797-Adams.14    41     58         1 1797      Adams      John Federalist
</code></pre>

<p>You can see here how the documents are now paragraphs.  Now, tokenize it and add your own manipulations to the tokens (you had several in your question), and then compute the fcm.</p>

<pre class=""lang-r prettyprint-override""><code># add your own additional manipulation of tokens here: compounding, etc
toks &lt;- data_corpus_inauguralpara %&gt;%
  tokens(remove_punct = TRUE) %&gt;%
  tokens_remove(stopwords(""en""))

# this creates the fcm within paragraph
fcmat &lt;- fcm(toks, context = ""document"")
fcmat
## Feature co-occurrence matrix of: 1,093 by 1,093 features.
##                  features
## features          Fellow-Citizens Senate House Representatives Among
##   Fellow-Citizens               0      1     1               1     0
##   Senate                        0      0     1               1     0
##   House                         0      0     0               2     0
##   Representatives               0      0     0               0     0
##   Among                         0      0     0               0     0
##   vicissitudes                  0      0     0               0     0
##   incident                      0      0     0               0     0
##   life                          0      0     0               0     0
##   event                         0      0     0               0     0
##   filled                        0      0     0               0     0
##                  features
## features          vicissitudes incident life event filled
##   Fellow-Citizens            0        0    0     0      0
##   Senate                     0        0    0     0      0
##   House                      0        0    0     0      0
##   Representatives            0        0    0     0      0
##   Among                      1        1    1     1      1
##   vicissitudes               0        1    1     1      1
##   incident                   0        0    1     1      1
##   life                       0        0    1     1      1
##   event                      0        0    0     0      1
##   filled                     0        0    0     0      0
## [ reached max_feat ... 1,083 more features, reached max_nfeat ... 1,083 more features ]
</code></pre>
",0,1,566,2020-06-16 11:02:39,https://stackoverflow.com/questions/62406952/r-word-co-occurrence-frequency-within-paragraph
PANDAS find exact given string/word from a column,"<p>So, I have a pandas column name <strong>Notes</strong> which contains a sentence or explanation of some event. I am trying find some given words from that column and when I find that word I am adding that to the next column as <strong>Type</strong></p>

<p>The problem is for some specific word for example <strong>Liar</strong>, <strong>Lies</strong> its picking up word like <strong>familiar</strong> and <strong>families</strong> because they both have liar and lies in them.</p>

<pre><code>Notes                                  Type
2 families are living in the address   Lies
He is a liar                           Liar
We are not familiar with this          Liar
</code></pre>

<p>As you can see from above only the second sentence is correct. How do I only pick up separate word like liar, lies and not families or familiar.</p>

<p>This was my approach,</p>

<pre><code>word= [""Lies""]

for i in range(0, len(df)):
    for f in word:
        if f in df[""Notes""][i]:
            df[""Type""][i] = ""Lies""
</code></pre>

<p>Appreciate any help. Thanks </p>
","python, pandas, text-mining","<p>Use <code>\b</code> for word boundary in  <code>regex</code>, and <code>.str.extract</code> to find pattern:</p>

<pre><code> df.Notes.str.extract(r'\b(lies|liar)\b')
</code></pre>

<p>To label those rows containing that word, do:</p>

<pre><code>df['Type'] = np.where(df.Notes.str.contains(r'\b(lies|liar)\b'), 'Lies', 'Not Lies')
</code></pre>
",1,0,1593,2020-06-17 15:00:09,https://stackoverflow.com/questions/62432059/pandas-find-exact-given-string-word-from-a-column
Mapping the topic of the review in R,"<p>I have two data sets, <strong>Review Data</strong> &amp; <strong>Topic Data</strong></p>
<p>Dput code of my <strong>Review Data</strong></p>
<pre><code>structure(list(Review = structure(2:1, .Label = c(&quot;Canteen Food could be improved&quot;, 
&quot;Sports and physical exercise need to be given importance&quot;), class = &quot;factor&quot;)), class = &quot;data.frame&quot;, row.names = c(NA, 
-2L))
</code></pre>
<p>Dput code of my <strong>Topic Data</strong></p>
<pre><code>structure(list(word = structure(2:1, .Label = c(&quot;canteen food&quot;, 
&quot;sports and physical&quot;), class = &quot;factor&quot;), Topic = structure(2:1, .Label = c(&quot;Canteen&quot;, 
&quot;Sports &quot;), class = &quot;factor&quot;)), class = &quot;data.frame&quot;, row.names = c(NA, 
-2L))
</code></pre>
<blockquote>
<p>Dput of my <strong>Desired Output</strong>, I want to look up the words which are appearing in <strong>Topic Data</strong> and map the same to the <strong>Review Data</strong></p>
</blockquote>
<pre><code>structure(list(Review = structure(2:1, .Label = c(&quot;Canteen Food could be improved&quot;, 
&quot;Sports and physical exercise need to be given importance&quot;), class = &quot;factor&quot;), 
    Topic = structure(2:1, .Label = c(&quot;Canteen&quot;, &quot;Sports &quot;), class = &quot;factor&quot;)), class = &quot;data.frame&quot;, row.names = c(NA, 
-2L))
</code></pre>
","r, dplyr, text-mining, tm, tidytext","<p>What you want is something like a fuzzy join. Here's a brute-force looking for strict substring (but case-insensitive):</p>
<pre class=""lang-r prettyprint-override""><code>library(dplyr)
review %&gt;%
  full_join(topic, by = character()) %&gt;% # full cartesian expansion
  group_by(word) %&gt;%
  mutate(matched = grepl(word[1], Review, ignore.case = TRUE)) %&gt;%
  ungroup() %&gt;%
  filter(matched) %&gt;%
  select(-word, -matched)
# # A tibble: 2 x 2
#   Review                                                   Topic    
#   &lt;fct&gt;                                                    &lt;fct&gt;    
# 1 Sports and physical exercise need to be given importance &quot;Sports &quot;
# 2 Canteen Food could be improved                           &quot;Canteen&quot;
</code></pre>
<p>It's a little brute-force in that it does a cartesian join of the frames before testing with <code>grepl</code>, but ... you can't really avoid some parts of that.</p>
<p>You can also use the <code>fuzzyjoin</code> package, which is meant for <strong>join</strong>s on <strong>fuzzy</strong> things (appropriately named).</p>
<pre class=""lang-r prettyprint-override""><code>fuzzyjoin::regex_left_join(review, topic, by = c(Review = &quot;word&quot;), ignore_case = TRUE)
# Warning: Coercing `pattern` to a plain character vector.
#                                                     Review                word   Topic
# 1 Sports and physical exercise need to be given importance sports and physical Sports 
# 2                           Canteen Food could be improved        canteen food Canteen
</code></pre>
<p>The warning is because your columns are <code>factor</code>s, not <code>character</code>, it should be harmless. If you want to hide the warning, you can use <code>suppressWarnings</code> (a little strong); if you want to prevent the warning, convert all applicable columns from <code>factor</code> to <code>character</code> (e.g., <code>topic[] &lt;- lapply(topic, as.character)</code>, same for <code>review$Review</code>, though modify it if you have numeric columns).</p>
",2,1,115,2020-06-22 14:50:10,https://stackoverflow.com/questions/62517201/mapping-the-topic-of-the-review-in-r
Removing Stop words from a list of strings in R,"<p><strong>Sample data</strong></p>
<p>Dput code of my data</p>
<pre><code>  x &lt;-  structure(list(Comments = structure(2:1, .Label = c(&quot;I have a lot of home-work to be completed..&quot;, 
    &quot;I want to vist my teacher today only!!&quot;), class = &quot;factor&quot;), 
        Comment_ID = c(704, 802)), class = &quot;data.frame&quot;, row.names = c(NA, 
    -2L))
</code></pre>
<p>I want to remove the stop words from the above data set using <code>tidytext::stop_words$word</code> and also retain the same columns in the output. Along with this how can I remove punctuation in <code>tidytext</code> package?</p>
<p><em>Note: I don't want to change my dataset into corpus</em></p>
","r, dplyr, text-mining, tidytext","<p>You can collapse all the words in <code>tidytext::stop_words$word</code> into one regex adding word boundaries. However, <code>tidytext::stop_words$word</code> is of length 1149 and this might be too big for regex to handle so you can remove few words which are not needed and apply this.</p>
<p>For example taking only first 10 words from <code>tidytext::stop_words$word</code>, you can do :</p>
<pre><code>gsub(paste0(paste0('\\b', tidytext::stop_words$word[1:10], '\\b', 
     collapse = &quot;|&quot;), '|[[:punct:]]+'), '', x$Comments)


#[1] &quot;I want to vist my teacher today only&quot;    
#    &quot;I have  lot of homework to be completed&quot;
</code></pre>
",2,0,1287,2020-06-24 13:01:59,https://stackoverflow.com/questions/62555690/removing-stop-words-from-a-list-of-strings-in-r
How to proceed after annotating text data for ML?,"<p>I am currently working on a project where I want to classify some text. For that, I first had to annotate text data. I did it using a web tool and have now the corresponding json file (containing the annotations) and the plain txt files (containing the raw text).
I now want to use different classifiers to train the data and eventually predict the desired outcome.</p>
<p>However, I am struggling with where to start. I haven't really found what I've been looking for in the internet so that's why I try it here.</p>
<p>How would I proceed with the json and txt. files? As far as I understood I'd have to somehow convert these info to a .csv where I have information about the labels, the text but also &quot;none&quot; for thext that has not been annotated. So I guess that's why I use the .txt files to somehow merge them with the annotations files and being able to detect if a text sentence (or word) has a label or not. And then I could use the .csv data to load it into the model.</p>
<p>Could someone give me a hint on where to start or how I should proceed now?
Everything I've found so far is covering the case that data is already converted and ready to preprocess but I am struggling with what to do with the results from the annotation process.</p>
<p>My JSON looks something like that:</p>
<pre><code>{&quot;annotatable&quot;:{&quot;parts&quot;:[&quot;s1p1&quot;]},
 &quot;anncomplete&quot;:true,
 &quot;sources&quot;:[],
 &quot;metas&quot;:{},
 &quot;entities&quot;:[{&quot;classId&quot;:&quot;e_1&quot;,&quot;part&quot;:&quot;s1p1&quot;,&quot;offsets&quot;: 
 [{&quot;start&quot;:11,&quot;text&quot;:&quot;This is the text&quot;}],&quot;coordinates&quot;:[],&quot;confidence&quot;: 
 {&quot;state&quot;:&quot;pre-added&quot;,&quot;who&quot;:[&quot;user:1&quot;],&quot;prob&quot;:1},&quot;fields&quot;:{&quot;f_4&quot;: 
 {&quot;value&quot;:&quot;3&quot;,&quot;confidence&quot;:{&quot;state&quot;:&quot;pre-added&quot;,&quot;who&quot;: 
 [&quot;user:1&quot;],&quot;prob&quot;:1}}},&quot;normalizations&quot;:{}},&quot;normalizations&quot;:{}}],
 &quot;relations&quot;:[]}
</code></pre>
<p>Each text is given a <code>classId</code> (<code>e_1</code> in this case) and a <code>field_value</code> (<code>f_4</code> given the value <code>3</code> in this case). I'd need to extract it step by step. First extracting the entity with the corresponding text (and adding &quot;none&quot; to where no annotation has been annotated) and in a second step retrieving the field information with the corresponding text.
The corresponding .txt file is just simply like that:
This is the text</p>
<p>I have all .json files in one folder and all .txt in another.</p>
","python, machine-learning, nlp, text-mining, data-handling","<p>So, let's assume you have a <code>JSON</code> file where the labels are indexed by the corresponding line in your raw <code>txt</code> file:</p>
<pre><code>{
  0: &quot;politics&quot;
  1: &quot;sports&quot;,
  2: &quot;weather&quot;,
}
</code></pre>
<p>And a <code>txt</code> file with the correspondingly indexed raw text:</p>
<pre><code>0 The American government has launched ... today.
1 FC Barcelona has won ... the country.
2 The forecast looks ... okay.
</code></pre>
<p>Then first, you would need to indeed connect the examples with their labels, before you go on featurizing the text and build a machine learning model. If your examples are, such as in my example, are aligned by index or an ID or any other identifying information, you could do:</p>
<pre><code>import json

with open('labels.json') as json_file:
    labels = json.load(json_file)
    # This results in a Python dictionary where you can look-up a label given an index.

with open(raw.txt) as txt_file:
    raw_texts = txt_file.readlines()
    # This results in a list where you can retrieve the raw text by index like this: raw_texts[index].
</code></pre>
<p>Now that you can match your raw text to your labels, you may want to put them in one single dataframe for ease of use (assuming they are ordered the same way for now):</p>
<pre><code>import pandas as pd

data = pd.DataFrame(
    {'label': labels.values(),
     'text': raw_texts
    })

#    label      text
# 0  politics   Sentence_1
# 1  sports     Sentence_2
# 2  weather    Sentence_3
</code></pre>
<p>Now, you can use different machine learning libraries, but the one I would recommend for starters is definitely <a href=""https://scikit-learn.org/stable/index.html"" rel=""nofollow noreferrer""><code>scikit-learn</code></a>. It provides a good explanation on how to convert your raw text strings into machine learning usable features:</p>
<p><a href=""https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#extracting-features-from-text-files"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#extracting-features-from-text-files</a></p>
<p>And afterwards, how to train a classifier using these features:</p>
<p><a href=""https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#training-a-classifier"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#training-a-classifier</a></p>
<p>The provided <code>DataFrame</code> I showed should be just right to start testing out these <code>scikit-learn</code> techniques.</p>
",1,0,775,2020-07-07 10:57:47,https://stackoverflow.com/questions/62773717/how-to-proceed-after-annotating-text-data-for-ml
How to tokenize my dataset in R using the tidytext library?,"<p>I have been trying to follow Text Mining with R by Julia Silge, however, I cannot tokenize my dataset with the unnest_tokens function.</p>
<p>Here are the packages I have loaded:</p>
<pre><code># Load
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(corpus)
library(corpustools)
library(dplyr)
library(tidyverse)
library(tidytext)
library(tokenizers)
library(stringr)
</code></pre>
<p>Here is the dataset I tried to use which is online, so the results should be reproducible:</p>
<pre><code>bible &lt;- readLines('http://bereanbible.com/bsb.txt')
</code></pre>
<p>And here is where everything falls apart.</p>
<p>Input:</p>
<pre><code> bible &lt;- bible %&gt;%
      unnest_tokens(word, text)
</code></pre>
<p>Output:</p>
<p><code>Error in tbl[[input]] : subscript out of bounds</code></p>
<p>From what I have read about this error, in Rstudio, the issue is that the dataset needs to be a matrix, so I tried transforming the dataset into a matrix table and I received the same error message.</p>
<p>Input:</p>
<pre><code> bible &lt;- readLines('http://bereanbible.com/bsb.txt')


bible &lt;- as.matrix(bible, nrow = 31105, ncol = 2 )
      
bible &lt;- bible %&gt;%
  unnest_tokens(word, text)
</code></pre>
<p>Output:</p>
<pre><code>Error in tbl[[input]] : subscript out of bounds
</code></pre>
<p>Any recommendations for what next steps I could take or maybe some good Text mining sources I could use as I continue to dive into this would be very much appreciated.</p>
","r, text, text-mining, tidytext","<p>The problem is that <code>readLines()</code>creates a vector, not a dataframe, as expected by <code>unnest_tokens()</code>, so you need to convert it. It is also helpful to separate the verse to it's own column:</p>
<pre class=""lang-r prettyprint-override""><code>library(tidytext)
library(tidyverse)

bible_orig &lt;- readLines('http://bereanbible.com/bsb.txt')

# Get rid of the copyright etc.
bible_orig &lt;- bible_orig[4:length(bible_orig)]

# Convert to df
bible &lt;- enframe(bible_orig)

# Separate verse from text
bible &lt;- bible %&gt;% 
 separate(value, into = c(&quot;verse&quot;, &quot;text&quot;), sep = &quot;\t&quot;)

tidy_bible &lt;- bible %&gt;% 
  unnest_tokens(word, text)

tidy_bible
#&gt; # A tibble: 730,130 x 3
#&gt;     name verse       word     
#&gt;    &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;    
#&gt;  1     1 Genesis 1:1 in       
#&gt;  2     1 Genesis 1:1 the      
#&gt;  3     1 Genesis 1:1 beginning
#&gt;  4     1 Genesis 1:1 god      
#&gt;  5     1 Genesis 1:1 created  
#&gt;  6     1 Genesis 1:1 the      
#&gt;  7     1 Genesis 1:1 heavens  
#&gt;  8     1 Genesis 1:1 and      
#&gt;  9     1 Genesis 1:1 the      
#&gt; 10     1 Genesis 1:1 earth    
#&gt; # … with 730,120 more rows
</code></pre>
<p><sup>Created on 2020-07-14 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v0.3.0)</sup></p>
",2,0,551,2020-07-14 11:34:57,https://stackoverflow.com/questions/62894249/how-to-tokenize-my-dataset-in-r-using-the-tidytext-library
Extract text from cells marked by regions,"<p>I don't know how else to describe this problem. I apologize for the most vague title ever.</p>
<p>this is what the data looks like</p>
<blockquote>
<p>[us]Deftek<br />
[jp]&lt;U+306F&gt;&lt;U+3061&gt;&lt;U+307F&gt;&lt;U+3064&gt; (Honey)<br />
Hampern<br />
[jp]&lt;U+3067&gt;&lt;U+3055&gt;&lt;U+3093&gt;&lt;U+3068&gt; (Descente)<br />
[jp]&lt;U+5E73&gt;&lt;U+30DC&gt;&lt;U+30E0&gt; (Hirabomb)<br />
[jp]&lt;U+30A2&gt;&lt;U+30AD&gt;&lt;U+30E9&gt; (Akira)<br />
Balls Out<br />
[jp]Teguru<br />
[jp]Melty</p>
</blockquote>
<p>So the names Hampern and Balls Out extract just fine, but the others I cannot extract anything from.</p>
<pre><code>library(httr)
library(tidyverse)
library(jsonlite)

fromJSON(rawToChar(GET(&quot;https://www.speedrun.com/api/v1/runs?game=o1y9wo6q&amp;category=wkpoo02r&amp;max=200&quot;)$content))$data %&gt;% 
  select(players) %&gt;% 
  unnest(players) %&gt;% 
  select(name) %&gt;% 
  mutate(name_extract = str_extract(name, &quot;[A-Za-z]*&quot;)) %&gt;% 
  na.omit()
</code></pre>
","r, text, unicode, text-mining, stringr","<p>You can remove the <code>[us][jp]</code> part from the <code>name</code>.</p>
<pre><code>library(httr)
library(dplyr)
library(jsonlite)

fromJSON(rawToChar(GET(&quot;https://www.speedrun.com/api/v1/runs?game=o1y9wo6q&amp;category=wkpoo02r&amp;max=200&quot;)$content))$data %&gt;% 
   select(players) %&gt;% 
   unnest(players) %&gt;%
   select(name) %&gt;% 
   mutate(name_extract = sub('\\[.*\\]', '', name)) %&gt;%
   na.omit

#   name                    name_extract       
#   &lt;chr&gt;                   &lt;chr&gt;              
# 1 [us]Deftek              Deftek             
# 2 [jp]はちみつ (Honey)    はちみつ (Honey)   
# 3 Hampern                 Hampern            
# 4 [jp]でさんと (Descente) でさんと (Descente)
# 5 [jp]平ボム (Hirabomb)   平ボム (Hirabomb)  
# 6 [jp]アキラ (Akira)      アキラ (Akira)     
# 7 Balls Out               Balls Out          
# 8 [jp]Teguru              Teguru             
# 9 [jp]えるも (Erumo)      えるも (Erumo)     
#10 [jp]Melty               Melty              
# … with 88 more rows
</code></pre>
",0,2,42,2020-07-15 06:57:03,https://stackoverflow.com/questions/62909180/extract-text-from-cells-marked-by-regions
Mapping of two text documents with python,"<p>I have annotated some textual data and now I am trying to map it with the original text file to get more information out.
I have all information of the annotations in a JSON file, from which I successfully parsed all the relevant information. I stored the information as seen below.</p>
<ol>
<li>Column = entity class</li>
<li>Column = starting point of the text</li>
<li>Column = length of the text (in char)</li>
<li>Column = value of entity label</li>
<li>Column = actual text that was annotated</li>
</ol>
<p>My goal now is to include non-annotated text, as well. Not every single sentence or character of a text document has been annotated, but I want to include them to feed all the information into a DL-Algorithm. So every sentence that has not been annotated should be included and showing &quot;None&quot; as of entity class and entity label.</p>
<p>Appreciate any hint or help on that!</p>
<p>Thanks!</p>
","python, dataframe, deep-learning, nlp, text-mining","<p>The information in your annotation file is not quite accurate. Since you stripped out white spaces, the length of the text should be adjusted properly.</p>
<pre><code>def map_with_text(data_file, ann_file, out_file):

    annots = []
    # Read annotation information
    with open(ann_file, 'r') as file_in:
        for line in file_in:
            components = line.split('t')
            components = line.split(&quot;\t&quot;)
            label = components[0]
            begin = int(components[1])
            length = int(components[2])
            f_4 = int(components[3])
            f_5 = int(components[4])
            text = components[5].strip()
            annots.append((label, begin, length, f_4, f_5, text))

    annots = sorted(annots, key=lambda c: c[1])

    # Read text data
    with open(data_file, 'r') as original:
        original_text = original.read()

    length_original = len(original_text)

    # Get positions of text already annotated. Since it was 
    # stripped, we cannot use the length. You can modify it if
    # you think your information is accurate.
    # pos_tup = [(begin, begin+length)
    #           for _, begin, length, _, _, text in annots]

    pos_tup = [(begin, begin+len(text))
               for _, begin, length, _, _, text in annots]

    # Get position marker
    pos_marker = [0] + [e for l in pos_tup for e in l] + [length_original]
    
    # Ranges of positions of text which have not been annotated
    not_ann_pos = [(x, y)
                   for x, y in zip(pos_marker[::2], pos_marker[1::2])]

    # Texts which have not been annotated
    not_ann_txt = [original_text[start:stop]
                   for start, stop in not_ann_pos]

    # Include it in the list
    all_components = [(None, start, len(txt.strip()), None, None, txt.strip())
                      for start, txt in zip(pos_marker[::2], not_ann_txt) if len(txt.strip()) != 0]

    # Add annotated information
    all_components += annots

    # Sort by the start index
    all_components = sorted(all_components, key=lambda c: c[1])

    # Write ot the output file
    with open(out_file, 'w') as f:
        for a in all_components:
            f.write(str(a[0]) + &quot;\t&quot; + str(a[1]) + &quot;\t&quot; + str(a[2]) +
                    &quot;\t&quot; + str(a[3]) + &quot;\t&quot; + str(a[4]) + &quot;\t&quot; + str(a[5]) + &quot;\n&quot;)


map_with_text('0.txt', '0.ann', 'out0.tsv')

# You can loop calling the function
#
#
</code></pre>
",1,0,846,2020-07-17 05:42:17,https://stackoverflow.com/questions/62947799/mapping-of-two-text-documents-with-python
R: Select option by introducing a number,"<p>I would like to build a very simple, sketchy, function where you can introduce a word (in Spanish) and get the most feasible next word (kind of a text predictor). So when the user introduces <em>me</em>, suggestions are <em>ha</em>, <em>he</em>, <em>lo</em>, <em>da</em> and <em>gusta</em>. Now, I would like to enable the user to select one of those five suggestions by typing 1, 2, 3, 4, or 5. Is there any way to do this in R?</p>
<pre><code>predictor&lt;-function(){
  word&lt;-readLines(stdin(),n=1)
  words&lt;-sort(table[first==word],decreasing=TRUE)[2:6]
  suggestions&lt;-gsub(&quot;.* &quot;,&quot;&quot;,names(words))
  return(suggestions)
}

#table = table with unigram and bigram frequencies of a corpus
#first = vector with the first word in each bigram

&gt; predictor()
me
[1] &quot;ha&quot;    &quot;he&quot;    &quot;lo&quot;    &quot;da&quot;    &quot;gusta&quot;
</code></pre>
","r, nlp, user-input, stdin, text-mining","<p>You already prompt for user input once. You can do it again to get the number of the item you want to get. But you'll have to print the suggestions in a way that allows the user to choose.</p>
<pre><code>predictor = function(){
  ### Since I have neither the table nor the list, I just made a makeshift words vector
  words = c(&quot;ha&quot;,&quot;he&quot;,&quot;lo&quot;,&quot;da&quot;,&quot;gusta&quot;)
  names(words)=words
  
  suggestions&lt;-paste(1:length(words),names(words),sep=&quot;: &quot;)
  print(paste(&quot;Possible Words:&quot;,paste(suggestions,collapse=&quot;; &quot;)))
  print(&quot;Please enter a number to select:&quot;)
  #input = scan(&quot;&quot;,what=&quot;character&quot;,n=1)
  input&lt;-readLines(stdin(),n=1)
  print(paste(&quot;You selected Item #&quot;,input,&quot;: &quot;,names(words)[as.integer(input)],sep=&quot;&quot;))
}

</code></pre>
<p>This will give you the output:</p>
<pre><code>[1] &quot;Possible Words: 1: ha; 2: he; 3: lo; 4: da; 5: gusta&quot;
[1] &quot;Please enter a number to select:&quot;
4
[1] &quot;You selected Item #4: da&quot;
</code></pre>
",1,0,216,2020-07-22 08:38:51,https://stackoverflow.com/questions/63030159/r-select-option-by-introducing-a-number
How to optimize string detection for speed?,"<p>When I do text analysis, I frequently want to figure out whether a large number of documents contains any element of a list of strings. If I have millions of documents (e.g. tweets) and a long list of patterns, this can take a long time.</p>
<p>I usually use the following packages to optimize for speed:
<code>data.table</code>
<code>dtplyr</code>
<code>stringr</code></p>
<p>What are some best practices to optimize string detection and analysis thereof? Are there packages that would allow me to optimize code like this:</p>
<pre><code>library(data.table)
library(dtplyr)
library(stringr)

my_dt &lt;- data.table(text = c(&quot;this is some text&quot;, &quot;this is some more text&quot;)) #imagine many more strings
my_string &lt;- paste(words, collapse = &quot;|&quot;)

lazy_dt(my_dt, immutable = F) %&gt;%
filter(filtered_text = str_detect(text, my_string)) %&gt;%
as.data.table()
</code></pre>
<p>I would assume that using data.table directly instead of the dtplyr implementation would increase speed. Are there any other ways to improve performance for this kind of application?</p>
<hr />
<p>I looked at <a href=""https://stackoverflow.com/questions/45101045/why-use-purrrmap-instead-of-lapply/"">this</a> question and was hoping I could get some similar guidance. Hopefully, the question is specific enough as it is now.</p>
","r, string, performance, nlp, text-mining","<p>As I mentioned in the comments <code>str_detect(text, my_string)</code> is the bottleneck in your code. Also note that is does not exactly do what you are expecting. It does a regex search, so all the words that have an &quot;a&quot; in the text would be counted as well. See examples below.</p>
<pre><code>library(data.table)
library(dtplyr)
library(stringr)
library(dplyr)


my_dt &lt;- data.table(id = 1:300000,
                    text = rep(c(&quot;this is some text&quot;, &quot;this is some more text&quot;, 
                             &quot;text palabras&quot;), 100000)) #imagine many more strings
my_string &lt;- paste(stringr::words, collapse = &quot;|&quot;)

# start counting time (note System.time() is slightly faster but doesn't print the results)
timing &lt;- Sys.time()

run code
lazy_dt(my_dt, immutable = F) %&gt;%
  filter(filtered_text = str_detect(text, my_string)) %&gt;%
  as.data.table()

            id                   text
     1:      1      this is some text
     2:      2 this is some more text
     3:      3          text palabras
     4:      4      this is some text
     5:      5 this is some more text
    ---                              
299996: 299996 this is some more text
299997: 299997          text palabras
299998: 299998      this is some text
299999: 299999 this is some more text
300000: 300000          text palabras

Sys.time() - timing
Time difference of 6.708245 secs
</code></pre>
<p>Note: the equivalent data.table code of your code above is the following:</p>
<pre><code>my_dt[str_detect(text, my_string), ]
</code></pre>
<p>Timing this is about 6.52 seconds, so not much of an improvement.</p>
<p>As you can see from the result above, this selection returns all the sentences because there is an &quot;a&quot; in palabras. This shouldn't be here. Now data.table has a function called <code>%chin%</code> which is like <code>%in%</code> but for character vectors and a lot faster. To get the match on words we just need to tokenize the lot, which can be done with <code>unnest_tokens</code> from tidytext. This function respects the data.table format. Afterwards I filter the data on the matching words, drop the word column and take distinct (unique) the data.table. The reason is that the result can have duplicate lines as multiple words can be a match. Even though there are more function calls this is about 3 times as fast.</p>
<pre><code>library(tidytext)

timing &lt;- Sys.time()
my_dt &lt;- unnest_tokens(my_dt, word, text, drop = F)
my_dt &lt;- unique(my_dt[word %chin% words, ], by = c(&quot;id&quot;, &quot;text&quot;))[, c(&quot;id&quot;, &quot;text&quot;)]


           id                   text
     1:     1      this is some text
     2:     2 this is some more text
     3:     4      this is some text
     4:     5 this is some more text
     5:     7      this is some text
    ---                             
199996: 299993 this is some more text
199997: 299995      this is some text
199998: 299996 this is some more text
199999: 299998      this is some text
200000: 299999 this is some more text

Sys.time() - timing
Time difference of 2.380911 secs
</code></pre>
<p>Now to speed things up a bit more, you can set the threads data.table uses. By default (on my system) this is set to 2. You can check this with <code>getDTthreads()</code>. When I add 1 thread with <code>setDTthreads(3)</code> the new code returns in about 1.6 secs.</p>
<p>Now maybe someone can speed this up a bit more, by doing this in the .SD part of data.table.</p>
",2,3,1057,2020-07-27 17:32:26,https://stackoverflow.com/questions/63120555/how-to-optimize-string-detection-for-speed
Searching for a word group with TFidfvectorizer,"<p>I'm using sklearn to receive the TF-IDF for a given keyword list. It works fine but the only thing not working is that it doesn't count word groups such as &quot;car manufacturers&quot;. How could I fix this? Should I use a different module ?</p>
<p>Pfa, the first lines of code so you see which modules I used. Thanks in advance !</p>
<pre><code>import numpy as np
import os
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from pathlib import Path


# root dir
root = '/Users/Tom/PycharmProjects/TextMining/'
#
words_to_find = ['vehicle', 'automotive', 'car manufacturers']
# tf_idf file writing
wrote_tf_idf_header = False
tf_idf_file_idx = 0
#
vectorizer_tf_idf = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, use_idf=True, norm=None, vocabulary=words_to_find)
vectorizer_cnt = CountVectorizer(stop_words=None, vocabulary=words_to_find)
</code></pre>
","scikit-learn, text-mining, tf-idf, tfidfvectorizer, countvectorizer","<p>You need to pass the <code>ngram_range</code> parameter in the CountVectorizer to get the result you are expecting. You can read the documentation with an example here.</p>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html</a></p>
<p>You can fix this like this.</p>
<pre><code>import numpy as np
import os
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from pathlib import Path


# root dir
# root = '/Users/Tom/PycharmProjects/TextMining/'
root = ['car manufacturers vehicle vehicales vehicle automotive car house manufacturers']
#
words_to_find = ['vehicle', 'automotive', 'car manufacturers']
# tf_idf file writing
wrote_tf_idf_header = False
tf_idf_file_idx = 0
#
vectorizer_tf_idf = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, use_idf=True, norm=None, vocabulary=words_to_find)
vectorizer_cnt = CountVectorizer(stop_words=None, vocabulary=words_to_find, ngram_range=(1,2))
x = vectorizer_cnt.fit_transform(root)
print(vectorizer_cnt.get_feature_names())
print(x.toarray())
</code></pre>
<p>Output:</p>
<pre><code>['vehicle', 'automotive', 'car manufacturers']
[[2 1 1]]
</code></pre>
",2,1,244,2020-07-28 08:56:49,https://stackoverflow.com/questions/63130475/searching-for-a-word-group-with-tfidfvectorizer
Keyword in context (kwic) for skipgrams?,"<p>I do keyword in context analysis with quanteda for ngrams and tokens and it works well.
I now want to do it for skipgrams, capture the context of &quot;barriers to entry&quot; but also &quot;barriers to [...] [and] entry.</p>
<p>The following code a kwic object which is empty but I don't know what I did wrong.  dcc.corpus refers to the text document. I also used the tokenized version but nothing changes.</p>
<p>The result is:</p>
<p>&quot;kwic object with 0 rows&quot;</p>
<pre><code>x &lt;- tokens(&quot;barriers entry&quot;)
ntoken_test &lt;- tokens_ngrams(x, n = 2, skip = 0:4, concatenator = &quot; &quot;)
twic_skipgram &lt;-  kwic(doc.corpus, pattern = list(ntoken_test), window=20, valuetype= &quot;glob&quot;)

twic_skipgram

</code></pre>
","r, nlp, text-mining, n-gram, quanteda","<p>Probably the easiest way is wildcarding to represent the &quot;skip&quot;.</p>

<pre class=""lang-r prettyprint-override""><code>library(&quot;quanteda&quot;)
## Package version: 2.1.1

txt &lt;- c(
  &quot;There are barriers to entry.&quot;,
  &quot;Also barriers against entry.&quot;,
  &quot;Just barriers entry.&quot;
)

# for skip of 1
kwic(txt, phrase(&quot;barriers * entry&quot;))
##                                                     
##  [text1, 3:5] There are |   barriers to entry    | .
##  [text2, 2:4]      Also | barriers against entry | .

# for skip of 0 and 1
kwic(txt, phrase(c(&quot;barriers * entry&quot;, &quot;barriers entry&quot;)))
##                                                     
##  [text1, 3:5] There are |   barriers to entry    | .
##  [text2, 2:4]      Also | barriers against entry | .
##  [text3, 2:3]      Just |     barriers entry     | .
</code></pre>
",2,2,160,2020-07-29 09:50:51,https://stackoverflow.com/questions/63150922/keyword-in-context-kwic-for-skipgrams
searching for words in text paragraph and then flagging them in R,"<p>I have a text data set, and want to search for various words in it, then flag those when when I find them. Here is sample data:</p>
<pre><code>df &lt;- data.table(&quot;id&quot; = c(1:3), &quot;report&quot; = c(&quot;Travel opens our eyes to art, history, and culture – but it also introduces us to culinary adventures we may have never imagined otherwise.&quot;
                                             , &quot;We quickly observed that no one in Sicily cooks with recipes (just with the heart), so we now do the same.&quot;
                                             , &quot;We quickly observed that no one in Sicily cooks with recipes so we now do the same.&quot;), &quot;summary&quot; = c(&quot;On our first trip to Sicily to discover our family roots,&quot;
                                                                      , &quot;If you’re not a gardener, an Internet search for where to find zucchini flowers results.&quot;
                                                                      , &quot;add some fresh cream to make the mixture a bit more liquid,&quot;))
</code></pre>
<p>So far I have been using SQL to process through this, but it gets challenging when you have a lot list of words to look for.</p>
<pre><code>dfOne &lt;- sqldf(&quot;select id
              , case when lower(report) like '%opens%' then 1 else 0 end as opens
, case when lower(report) like '%cooks%' then 1 else 0 end as cooks
, case when lower(report) like '%internet%' then 1 else 0 end as internet
, case when lower(report) like '%zucchini%' then 1 else 0 end as zucchini
, case when lower(report) like '%fresh%' then 1 else 0 end as fresh
      from df
      &quot;)
</code></pre>
<p>I'm looking for ideas to do this in a more efficient way. Imagine if you have a long list of target terms, this code can get unnecessarily too long.</p>
<p>Thanks,</p>
<p>SM.</p>
","r, text-mining","<p><strong>1) sqldf</strong></p>
<p>Define the vector of words and then convert it to SQL.  Note that <code>case when</code> is not needed since <code>like</code> already produces a 0/1 result.  Prefacing <code>sqldf</code> with <code>fn$</code> enables <code>$like</code> to substitute the R <code>like</code> character string into the SQL statement.  Use the <code>verbose=TRUE</code> argument to <code>sqldf</code> to view the SQL statement generated.  This is only two lines of code no matter how long <code>words</code> is.</p>
<pre><code>words &lt;- c(&quot;opens&quot;, &quot;cooks&quot;, &quot;internet&quot;, &quot;zucchini&quot;, &quot;fresh&quot;, &quot;test me&quot;)

like &lt;- toString(sprintf(&quot;\nlower(report) like '%%%s%%' as '%s'&quot;, words, words))
fn$sqldf(&quot;select id, $like from df&quot;, verbose = TRUE)
</code></pre>
<p>giving:</p>
<pre><code>  id opens cooks internet zucchini fresh test me
1  1     1     0        0        0     0       0
2  2     0     1        0        0     0       0
3  3     0     1        0        0     0       0
</code></pre>
<p><strong>2) outer</strong></p>
<p>Using <code>words</code> from above we can use <code>outer</code> as follows.  Note that the function (third argument) in outer must be vectorized and we can make <code>grepl</code> vectorized as shown.  Omit <code>check.names = FALSE</code> if you don't mind the column names associated with words having spaces or puncutation munged into syntactic R variable names.  This produces the same output as (1).</p>
<pre><code>with(df, data.frame(
    id, 
    +t(outer(setNames(words, words), report, Vectorize(grepl))), 
    check.names = FALSE
))
</code></pre>
<p><strong>3) sapply</strong></p>
<p>Using <code>sapply</code> we can get a slightly shorter solution along the same lines as (2).  The output is the same as in (1) and (2).</p>
<pre><code>with(df, data.frame(id, +sapply(words, grepl, report), check.names = FALSE))
</code></pre>
",2,1,828,2020-08-18 09:26:34,https://stackoverflow.com/questions/63465690/searching-for-words-in-text-paragraph-and-then-flagging-them-in-r
Use Spark for text mining with nltk,"<p>I have a problem with spark and text mining. Help me, please. I attached all of the errors for better identifying. I can't find anything for debugging this error. I don't know why Python doesn't answer when I type words.collect().
I downloaded the spark for Apache Hadoop 2.6 and unpacked. I'm trying to turn this code but throws me an error.</p>
<p><strong>This is my code:</strong></p>
<pre><code>from pyspark import SparkConf
from pyspark import SparkContext

conf = SparkConf()
conf.setAppName('spark-NLTK')
sc = SparkContext.getOrCreate();

import nltk

data = sc.textFile('c:/Users/Ramin/Desktop/Nixon.txt')

#word tokenization
def word_tokenize(x):
    lowerW = x.lower()
    return nltk.word_tokenize(x)

words = data.flatMap(word_tokenize)
words.collect()
</code></pre>
<p><strong>I get this error:</strong></p>
<pre><code>Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File &quot;C:\Bigdata\SPARK\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\worker.py&quot;, line 364, in main
  File &quot;C:\Bigdata\SPARK\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\worker.py&quot;, line 69, in read_command
  File &quot;C:\Bigdata\SPARK\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\serializers.py&quot;, line 173, in _read_with_length
    return self.loads(obj)
  File &quot;C:\Bigdata\SPARK\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\serializers.py&quot;, line 587, in loads
    return pickle.loads(obj, encoding=encoding)
  File &quot;C:\Bigdata\SPARK\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\cloudpickle.py&quot;, line 875, in subimport
    __import__(name)
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\nltk\__init__.py&quot;, line 143, in &lt;module&gt;
    from nltk.chunk import *
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\nltk\chunk\__init__.py&quot;, line 157, in &lt;module&gt;
    from nltk.chunk.api import ChunkParserI
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\nltk\chunk\api.py&quot;, line 13, in &lt;module&gt;
    from nltk.parse import ParserI
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\nltk\parse\__init__.py&quot;, line 100, in &lt;module&gt;
    from nltk.parse.transitionparser import TransitionParser
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\nltk\parse\transitionparser.py&quot;, line 22, in &lt;module&gt;
    from sklearn.datasets import load_svmlight_file
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\datasets\__init__.py&quot;, line 22, in &lt;module&gt;
    from .twenty_newsgroups import fetch_20newsgroups
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\datasets\twenty_newsgroups.py&quot;, line 44, in &lt;module&gt;
    from ..feature_extraction.text import CountVectorizer
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\feature_extraction\__init__.py&quot;, line 10, in &lt;module&gt;
    from . import text
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py&quot;, line 28, in &lt;module&gt;
    from ..preprocessing import normalize
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\preprocessing\__init__.py&quot;, line 6, in &lt;module&gt;
    from ._function_transformer import FunctionTransformer
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\preprocessing\_function_transformer.py&quot;, line 5, in &lt;module&gt;
    from ..utils.testing import assert_allclose_dense_sparse
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\utils\testing.py&quot;, line 718, in &lt;module&gt;
    import pytest
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\pytest.py&quot;, line 6, in &lt;module&gt;
    from _pytest.assertion import register_assert_rewrite
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\_pytest\assertion\__init__.py&quot;, line 7, in &lt;module&gt;
    from _pytest.assertion import rewrite
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\_pytest\assertion\rewrite.py&quot;, line 26, in &lt;module&gt;
    from _pytest.assertion import util
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\_pytest\assertion\util.py&quot;, line 8, in &lt;module&gt;
    import _pytest._code
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\_pytest\_code\__init__.py&quot;, line 2, in &lt;module&gt;
    from .code import Code  # noqa
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\_pytest\_code\code.py&quot;, line 23, in &lt;module&gt;
    import pluggy
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\pluggy\__init__.py&quot;, line 16, in &lt;module&gt;
    from .manager import PluginManager, PluginValidationError
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\pluggy\manager.py&quot;, line 11, in &lt;module&gt;
    import importlib_metadata
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 547, in &lt;module&gt;
    __version__ = version(__name__)
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 509, in version
    return distribution(distribution_name).version
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 482, in distribution
    return Distribution.from_name(distribution_name)
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 183, in from_name
    dist = next(dists, None)
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 425, in &lt;genexpr&gt;
    for path in map(cls._switch_path, paths)
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 449, in _search_path
    if not root.is_dir():
  File &quot;C:\Users\Ramin\Anaconda3\lib\pathlib.py&quot;, line 1358, in is_dir
    return S_ISDIR(self.stat().st_mode)
  File &quot;C:\Users\Ramin\Anaconda3\lib\pathlib.py&quot;, line 1168, in stat
    return self._accessor.stat(self)
OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'C:\\C:\\Bigdata\\SPARK\\spark-2.4.6-bin-hadoop2.7\\jars\\spark-core_2.11-2.4.6.jar'

    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)
    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)
    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)
    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)
    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
    at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
    at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
    at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
    at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
    at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
    at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
    at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
    at org.apache.spark.rdd.RDD.collect(RDD.scala:989)
    at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)
    at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Unknown Source)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File &quot;C:\Bigdata\SPARK\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\worker.py&quot;, line 364, in main
  File &quot;C:\Bigdata\SPARK\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\worker.py&quot;, line 69, in read_command
  File &quot;C:\Bigdata\SPARK\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\serializers.py&quot;, line 173, in _read_with_length
    return self.loads(obj)
  File &quot;C:\Bigdata\SPARK\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\serializers.py&quot;, line 587, in loads
    return pickle.loads(obj, encoding=encoding)
  File &quot;C:\Bigdata\SPARK\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\cloudpickle.py&quot;, line 875, in subimport
    __import__(name)
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\nltk\__init__.py&quot;, line 143, in &lt;module&gt;
    from nltk.chunk import *
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\nltk\chunk\__init__.py&quot;, line 157, in &lt;module&gt;
    from nltk.chunk.api import ChunkParserI
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\nltk\chunk\api.py&quot;, line 13, in &lt;module&gt;
    from nltk.parse import ParserI
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\nltk\parse\__init__.py&quot;, line 100, in &lt;module&gt;
    from nltk.parse.transitionparser import TransitionParser
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\nltk\parse\transitionparser.py&quot;, line 22, in &lt;module&gt;
    from sklearn.datasets import load_svmlight_file
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\datasets\__init__.py&quot;, line 22, in &lt;module&gt;
    from .twenty_newsgroups import fetch_20newsgroups
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\datasets\twenty_newsgroups.py&quot;, line 44, in &lt;module&gt;
    from ..feature_extraction.text import CountVectorizer
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\feature_extraction\__init__.py&quot;, line 10, in &lt;module&gt;
    from . import text
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py&quot;, line 28, in &lt;module&gt;
    from ..preprocessing import normalize
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\preprocessing\__init__.py&quot;, line 6, in &lt;module&gt;
    from ._function_transformer import FunctionTransformer
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\preprocessing\_function_transformer.py&quot;, line 5, in &lt;module&gt;
    from ..utils.testing import assert_allclose_dense_sparse
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\sklearn\utils\testing.py&quot;, line 718, in &lt;module&gt;
    import pytest
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\pytest.py&quot;, line 6, in &lt;module&gt;
    from _pytest.assertion import register_assert_rewrite
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\_pytest\assertion\__init__.py&quot;, line 7, in &lt;module&gt;
    from _pytest.assertion import rewrite
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\_pytest\assertion\rewrite.py&quot;, line 26, in &lt;module&gt;
    from _pytest.assertion import util
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\_pytest\assertion\util.py&quot;, line 8, in &lt;module&gt;
    import _pytest._code
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\_pytest\_code\__init__.py&quot;, line 2, in &lt;module&gt;
    from .code import Code  # noqa
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\_pytest\_code\code.py&quot;, line 23, in &lt;module&gt;
    import pluggy
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\pluggy\__init__.py&quot;, line 16, in &lt;module&gt;
    from .manager import PluginManager, PluginValidationError
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\pluggy\manager.py&quot;, line 11, in &lt;module&gt;
    import importlib_metadata
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 547, in &lt;module&gt;
    __version__ = version(__name__)
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 509, in version
    return distribution(distribution_name).version
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 482, in distribution
    return Distribution.from_name(distribution_name)
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 183, in from_name
    dist = next(dists, None)
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 425, in &lt;genexpr&gt;
    for path in map(cls._switch_path, paths)
  File &quot;C:\Users\Ramin\Anaconda3\lib\site-packages\importlib_metadata\__init__.py&quot;, line 449, in _search_path
    if not root.is_dir():
  File &quot;C:\Users\Ramin\Anaconda3\lib\pathlib.py&quot;, line 1358, in is_dir
    return S_ISDIR(self.stat().st_mode)
  File &quot;C:\Users\Ramin\Anaconda3\lib\pathlib.py&quot;, line 1168, in stat
    return self._accessor.stat(self)
OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'C:\\C:\\Bigdata\\SPARK\\spark-2.4.6-bin-hadoop2.7\\jars\\spark-core_2.11-2.4.6.jar'

    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)
    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)
    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)
    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)
    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
    at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
    at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
    at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
    at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
    at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
    at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
    at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    ... 1 more
</code></pre>
","apache-spark, pyspark, nlp, nltk, text-mining","<p>Some part of your Spark configuration is incorrect, as it thinks it needs to prepend the C drive twice to the path of your Spark libraries</p>
<pre><code>C:\\C:\\Bigdata\\SPARK
</code></pre>
<p>So, I would attempt at running included Spark sample code like <code>SparkPi</code> before jumping in with code elsewhere</p>
",0,0,501,2020-08-24 18:28:43,https://stackoverflow.com/questions/63566723/use-spark-for-text-mining-with-nltk
Construct word2vec (CBOW) training data from beginning of sentence,"<p>When constructing training data for CBOW, <a href=""https://arxiv.org/abs/1301.3781"" rel=""nofollow noreferrer"">Mikolov et al.</a> suggest using the word from the center of a context window. What is the &quot;best&quot; approach to capturing words at the beginning/end of a sentence (I put best in quotes because I'm sure this depends on the task). Implementations I see online do something like the this:</p>
<pre><code>for i in range(2, len(raw_text) - 2):
    context = [raw_text[i - 2], raw_text[i - 1],
               raw_text[i + 1], raw_text[i + 2]]
</code></pre>
<p>I see two issues arising from this approach.</p>
<ul>
<li><strong>Issue 1:</strong> The approach gives imbalanced focus to the middle of the sentence. For example, the first word of the sentence can only appear in 1 context window and will never appear as the target word. Compare this to the 4th word in the sentence which will appear in 4 context windows and will also be a target word. This will be an issue as some words appear frequently at the beginning of sentences (i.e. however, thus, etc.). Wouldn't this approach minimize their use?</li>
<li><strong>Issue 2:</strong> Sentences with 4 or fewer words are completely ignored, and the importance of short sentences is minimized. For example, a sentence with 5 words can only contribute one training sample while a sentence of length 8 will contribute 4 training samples.</li>
</ul>
<p>Can anyone offer insight as to how much these issues affect the results or any alternative approaches for constructing the training data? (I considered letting the first word be the target word and using the next N words as the context, but this creates issues of it's own).</p>
<p>Related question on Stack Exchange:
<a href=""https://datascience.stackexchange.com/questions/81249/construct-word2vec-cbow-training-data-from-beginning-of-sentence"">Construct word2vec (CBOW) training data from beginning of sentence</a></p>
","neural-network, nlp, text-mining, word2vec, word-embedding","<p>All actual implementations I've seen, going back to the original <code>word2vec.c</code> by Mikolov, tend to let every word take turns being the 'center target word', but truncate the context-window to whatever is available.</p>
<p>So for example, with a <code>window=5</code> (on both sides), and the 'center word' as the 1st word of a text, only the 5 following words are used. If the center word is the 2nd word, 1 word preceding, and 5 words following, will be used.</p>
<p>This is easy to implement and works fine in practice.</p>
<p>In CBOW mode, every center word is still part of the same same number of neural-network forward-propagations (roughly, prediction attempts), though words 'near the ends' participate as inputs slightly less often. But even then, they're subject to an incrementally larger update - such as when they're 1 of just 5 words, instead of 1 of just 10.</p>
<p>(In SG mode, words near-the-ends will both inputs and target-words slightly less often.)</p>
<p>Your example code – showing words without full context windows never being the center target – is not something I've seen, and I'd only expect that choice in a buggy/unsophisticated implementation.</p>
<p>So neither of your issues arise in common implementations, where texts are longer than 1 word long. (In even a text of 2 words, the 1st word will be predicted using a window of just the 2nd, and the 2nd will be predicted with a window of just the 1st.)</p>
<p>While the actual word-sampling does result in slightly-different treatment of words at either end, it's hard for me to imagine these slight differences in word-treatment making any difference in results, in appropriate training corpuses for word2vec – large &amp; varied with plentiful contrasting examples for all relevant words.</p>
<p>(Maybe it'd be an issue in some small or synthetic corpus, where some rare-but-important tokens only appear in leading- or ending-positions. But that's far from the usual use of word2vec.)</p>
<p>Note also that while some descriptions &amp; APIs describe the units of word2vec training as 'sentences', the algorithm really just works on 'lists of tokens'. Often each list-of-tokens will span paragraphs or documents. Sometimes they retain things like punctuation, including sentence-ending periods, as pseudo-words. Bleeding the windows across sentence-boundaries rarely hurts, and often help, as the cooccurrences of words leading out of one sentence and into the next may be just as instructive as the cooccurrences of words inside one sentence. So in common practice of many-sentence training text, even fewer 'near-the-ends' words have even a slightly-different sampling treatment that you may have thought.</p>
",2,0,583,2020-09-04 21:02:41,https://stackoverflow.com/questions/63747999/construct-word2vec-cbow-training-data-from-beginning-of-sentence
find if any word in a dataset appears in sentences in other dataset with R,"<p>I have a dataset that contains a variable called &quot;sentence&quot;, which contains sentences.
Here is a reproducible small version of it</p>
<pre><code>
structure(list(section = c(&quot;id111&quot;, &quot;id111&quot;, &quot;id111&quot;, &quot;id111&quot;, 
&quot;id111&quot;, &quot;id111&quot;, &quot;id111&quot;, &quot;id111&quot;, &quot;id111&quot;, &quot;id111&quot;, &quot;id111&quot;, 
&quot;id111&quot;, &quot;id111&quot;, &quot;id111&quot;, &quot;id111&quot;), sentence = c(&quot;Es wird Abschied genommen, aber auf Wiedersehen  Die Großmama hatte einen Tag vor ihrer Ankunft noch einen Brief nach der Alp hinauf geschrieben, damit sie oben bestimmt wüßten, daß sie komme.&quot;, 
&quot;Diesen Brief brachte am andern Tage der Peter in der Frühe mit sich, als er auf die Weide zog.&quot;, 
&quot;Schon war der Großvater mit den Kindern aus der Hütte getreten, und auch Schwänli und Bärli standen beide draußen und schüttelten lustig ihre Köpfe in der frischen Morgenluft, während die Kinder sie streichelten und ihnen glückliche Reise wünschten zu ihrer Bergfahrt.&quot;, 
&quot;Behaglich stand der Öhi dabei und schaute bald auf die frischen Gesichter der Kinder, bald auf seine sauber glänzenden Geißen nieder.&quot;, 
&quot;Beides mußte ihm gefallen, denn er lächelte vergnüglich.&quot;, 
&quot;Jetzt kam der Peter heran.&quot;, &quot;Als er die Gruppe gewahr wurde, näherte er sich langsam, streckte den Brief dem Öhi entgegen, und sobald dieser ihn erfaßt hatte, sprang er scheu zurück, so als ob ihn etwas erschreckt habe, und dann guckte er schnell hinter sich, gerade als ob von hinten ihn auch noch etwas hätte erschrecken wollen; dann machte er einen Sprung und lief davon, den Berg hinauf.&quot;, 
&quot;»Großvater«, sagte das Heidi, das dem Vorgang verwundert zugeschaut hatte, »warum tut der Peter jetzt immer wie der große Türk, wenn der eine Rute hinter sich merkt; dann scheut er mit dem Kopf und schüttelt ihn nach allen Seiten und macht auf einmal Sprünge in die Luft hinauf.«&quot;, 
&quot;»Vielleicht merkt der Peter auch eine Rute hinter sich, die er verdient«, antwortete der Großvater.&quot;, 
&quot;Nur die erste Halde hinauf lief der Peter so in einem Zuge davon; sobald man ihn von unten nicht mehr sehen konnte, kam es anders.&quot;, 
&quot;Da stand er still und drehte scheu den Kopf nach allen Seiten.&quot;, 
&quot;Plötzlich tat er einen Sprung und schaute hinter sich, so erschreckt, als habe ihn eben einer im Genick gepackt.&quot;, 
&quot;Hinter jedem Busch hervor, aus jeder Hecke heraus meinte jetzt der Peter den Polizeidiener aus Frankfurt auf sich losstürzen zu sehen.&quot;, 
&quot;Je länger aber diese gespannte Erwartung dauerte, je schreckhafter wurde es dem Peter zumute, er hatte keinen ruhigen Augenblick mehr.&quot;, 
&quot;Nun mußte das Heidi seine Hütte aufräumen, denn die Großmama sollte doch alles in guter Ordnung finden, wenn sie kam.&quot;
), type = c(&quot;chapter&quot;, &quot;chapter&quot;, &quot;chapter&quot;, &quot;chapter&quot;, &quot;chapter&quot;, 
&quot;chapter&quot;, &quot;chapter&quot;, &quot;chapter&quot;, &quot;chapter&quot;, &quot;chapter&quot;, &quot;chapter&quot;, 
&quot;chapter&quot;, &quot;chapter&quot;, &quot;chapter&quot;, &quot;chapter&quot;), ch_n = c(1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), book = structure(c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c(&quot;book1&quot;, 
&quot;book2&quot;), class = &quot;factor&quot;), sentence_id = 1:15, freq = c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L)), row.names = c(NA, 
-15L), groups = structure(list(sentence = c(&quot;»Großvater«, sagte das Heidi, das dem Vorgang verwundert zugeschaut hatte, »warum tut der Peter jetzt immer wie der große Türk, wenn der eine Rute hinter sich merkt; dann scheut er mit dem Kopf und schüttelt ihn nach allen Seiten und macht auf einmal Sprünge in die Luft hinauf.«&quot;, 
&quot;»Vielleicht merkt der Peter auch eine Rute hinter sich, die er verdient«, antwortete der Großvater.&quot;, 
&quot;Als er die Gruppe gewahr wurde, näherte er sich langsam, streckte den Brief dem Öhi entgegen, und sobald dieser ihn erfaßt hatte, sprang er scheu zurück, so als ob ihn etwas erschreckt habe, und dann guckte er schnell hinter sich, gerade als ob von hinten ihn auch noch etwas hätte erschrecken wollen; dann machte er einen Sprung und lief davon, den Berg hinauf.&quot;, 
&quot;Behaglich stand der Öhi dabei und schaute bald auf die frischen Gesichter der Kinder, bald auf seine sauber glänzenden Geißen nieder.&quot;, 
&quot;Beides mußte ihm gefallen, denn er lächelte vergnüglich.&quot;, 
&quot;Da stand er still und drehte scheu den Kopf nach allen Seiten.&quot;, 
&quot;Diesen Brief brachte am andern Tage der Peter in der Frühe mit sich, als er auf die Weide zog.&quot;, 
&quot;Es wird Abschied genommen, aber auf Wiedersehen  Die Großmama hatte einen Tag vor ihrer Ankunft noch einen Brief nach der Alp hinauf geschrieben, damit sie oben bestimmt wüßten, daß sie komme.&quot;, 
&quot;Hinter jedem Busch hervor, aus jeder Hecke heraus meinte jetzt der Peter den Polizeidiener aus Frankfurt auf sich losstürzen zu sehen.&quot;, 
&quot;Je länger aber diese gespannte Erwartung dauerte, je schreckhafter wurde es dem Peter zumute, er hatte keinen ruhigen Augenblick mehr.&quot;, 
&quot;Jetzt kam der Peter heran.&quot;, &quot;Nun mußte das Heidi seine Hütte aufräumen, denn die Großmama sollte doch alles in guter Ordnung finden, wenn sie kam.&quot;, 
&quot;Nur die erste Halde hinauf lief der Peter so in einem Zuge davon; sobald man ihn von unten nicht mehr sehen konnte, kam es anders.&quot;, 
&quot;Plötzlich tat er einen Sprung und schaute hinter sich, so erschreckt, als habe ihn eben einer im Genick gepackt.&quot;, 
&quot;Schon war der Großvater mit den Kindern aus der Hütte getreten, und auch Schwänli und Bärli standen beide draußen und schüttelten lustig ihre Köpfe in der frischen Morgenluft, während die Kinder sie streichelten und ihnen glückliche Reise wünschten zu ihrer Bergfahrt.&quot;
), .rows = structure(list(8L, 9L, 7L, 4L, 5L, 11L, 2L, 1L, 13L, 
    14L, 6L, 15L, 10L, 12L, 3L), ptype = integer(0), class = c(&quot;vctrs_list_of&quot;, 
&quot;vctrs_vctr&quot;, &quot;list&quot;))), row.names = c(NA, 15L), class = c(&quot;tbl_df&quot;, 
&quot;tbl&quot;, &quot;data.frame&quot;), .drop = TRUE), class = c(&quot;grouped_df&quot;, 
&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))

</code></pre>
<p>I then have a list of words, below a reduced version:</p>
<pre><code>structure(list(word = c(&quot;Ära&quot;, &quot;Arsenfahlerz&quot;, &quot;Arsenikbleispath&quot;, 
&quot;Arsensaures&quot;, &quot;Blei&quot;, &quot;Aufschiebung&quot;, &quot;Ausliegerberg&quot;, &quot;Baltischer&quot;, 
&quot;Schild&quot;, &quot;Bänderton&quot;, &quot;Bändertondatierung&quot;, &quot;Bändertonkalender&quot;, 
&quot;Beben&quot;, &quot;Bebenherd&quot;, &quot;Belgit&quot;, &quot;Hüttenwesen&quot;, &quot;Bergbau&quot;, &quot;Bergkamm&quot;, 
&quot;Bergmassiv&quot;, &quot;Bergmehl&quot;, &quot;Bergsporn&quot;, &quot;Bergstock&quot;, &quot;Binnit&quot;, 
&quot;Biostratigrafie&quot;, &quot;Biostratigraphie&quot;, &quot;Blattsilikat&quot;, &quot;Blattverschiebung&quot;, 
&quot;Bleiapatit&quot;, &quot;Block&quot;, &quot;Blutjaspis&quot;, &quot;Bodenerhebung&quot;, &quot;Bodenkunde&quot;, 
&quot;Bodenwissenschaft&quot;, &quot;Böhmische&quot;, &quot;Masse&quot;, &quot;Böhmisches&quot;, &quot;Hochland&quot;, 
&quot;Massiv&quot;, &quot;Bolus&quot;, &quot;Alba&quot;, &quot;Bongo&quot;, &quot;Berge&quot;, &quot;Braunauer&quot;, &quot;Wände&quot;, 
&quot;Braunbleierz&quot;, &quot;Brauner&quot;, &quot;Glaskopf&quot;, &quot;Jura&quot;, &quot;Braunjura&quot;, &quot;Bruch&quot;, 
&quot;Bruchflügel&quot;, &quot;Bruchlinie&quot;, &quot;Bruchschollentektonik&quot;, &quot;Bruchtektonik&quot;, 
&quot;Buckel&quot;, &quot;Budweis&quot;, &quot;Wittingauer&quot;, &quot;Tiefplatte&quot;, &quot;Bühel&quot;, &quot;Bühl&quot;, 
&quot;Buntbleierz&quot;, &quot;Calciovolborthit&quot;, &quot;Calciumcarbonat&quot;, &quot;Calciumkarbonat&quot;, 
&quot;Calciumsulfat&quot;, &quot;Cassiterit&quot;, &quot;Celit&quot;, &quot;Cevennen&quot;, &quot;Chalcedon&quot;, 
&quot;Chalkosin&quot;, &quot;Chalzedon&quot;, &quot;Chilenische&quot;, &quot;Schweiz&quot;, &quot;Chronostratigrafie&quot;, 
&quot;Chronostratigraphie&quot;, &quot;Coltan&quot;, &quot;Columbeisen&quot;, &quot;Columbit&quot;, &quot;Conichalcit&quot;, 
&quot;Danakil&quot;, &quot;Deflation&quot;, &quot;Depression&quot;, &quot;Desertation&quot;, &quot;Desertifikation&quot;, 
&quot;Diatomeenerde&quot;, &quot;Diatomeenpelit&quot;, &quot;Diatomit&quot;, &quot;Diatrema&quot;, &quot;Dislokation&quot;, 
&quot;Dolomit&quot;, &quot;Dolomitstein&quot;, &quot;Donau&quot;, &quot;Iller&quot;, &quot;Lech&quot;, &quot;dunkles&quot;, 
&quot;Fahlerz&quot;, &quot;Durchschlagsröhre&quot;, &quot;Edaphologie&quot;, &quot;Eem&quot;, &quot;Interglazial&quot;, 
&quot;Warmzeit&quot;, &quot;Effusivgestein&quot;, &quot;einfache&quot;, &quot;Scherung&quot;, &quot;Einzugsgebiet&quot;, 
&quot;Eisenerz&quot;, &quot;Eisenkies&quot;, &quot;Eisensulfid&quot;, &quot;Eisvulkan&quot;, &quot;Eiszeit&quot;, 
&quot;Eiszeitalter&quot;, &quot;Endmoräne&quot;, &quot;endogene&quot;, &quot;Dynamik&quot;, &quot;Prozesse&quot;, 
&quot;Entwässerungsgebiet&quot;, &quot;Epirogenese&quot;, &quot;Epizentrum&quot;, &quot;Erdbeben&quot;, 
&quot;Erdbebenforscher&quot;, &quot;Erdbebenschwarm&quot;, &quot;Erdbebenwarte&quot;, &quot;Erdbebenwelle&quot;, 
&quot;Erdbebenzentrum&quot;, &quot;Erdkruste&quot;, &quot;Erdrinde&quot;, &quot;Erdstoß&quot;, &quot;Erdzeitalter&quot;, 
&quot;Ergussgestein&quot;, &quot;Erhebung&quot;, &quot;Erhöhung&quot;, &quot;Erosion&quot;, &quot;Erratiker&quot;, 
&quot;erratischer&quot;, &quot;Eruptionsschlot&quot;, &quot;Eruptivgestein&quot;, &quot;Erz&quot;, &quot;Expansionshypothese&quot;, 
&quot;Extrusivgestein&quot;, &quot;Falkengebirge&quot;, &quot;Faulschlamm&quot;, &quot;Fels&quot;, &quot;Felsblock&quot;, 
&quot;Felsburg&quot;, &quot;Felsen&quot;, &quot;Felsenkunde&quot;, &quot;Felsmechanik&quot;, &quot;Felsnadel&quot;, 
&quot;Felsnase&quot;, &quot;Felssporn&quot;, &quot;Felsturm&quot;)), row.names = c(NA, -151L
), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
<p>How do I add a column to my first dataset, such as that it will be called &quot;geo&quot; and it will say &quot;yes&quot; to each sentence, if ANY of the words in the second dataset appears in the sentence, and &quot;no&quot; if not?</p>
","r, text-mining, dplyr, grepl","<p>If you're still looking for a solution, you might try this which is based on the tidyverse version of R's string capabilities.</p>
<p>For each sentence in the original data frame you want to check every word in the word list to see if they appear in the sentence. Note that I changed the name of your <code>df</code> to <code>word_df</code></p>
<pre><code>   library(stringr)

   for (i in 1:nrow(word_df)) {
   word_check &lt;- function (x) {
   str_detect(word_df$sentence[i],x)
   }
   word_df$geo[i]&lt;- any(sapply(word_list, word_check))
   }
   word_df
</code></pre>
<p>I created the &quot;word_check&quot; function which uses the <code>str_detect</code> function in <code>stringr</code> to check if a word from the word list appears in the sentence. I then used <code>sapply</code> to check every word in the word list against the sentence. I then used <code>any</code> to assign a value of <code>TRUE</code> for the sentence if any of the words from the word list appear in it. The <code>for</code> loop then repeats this process for each sentence (i.e. changing the index of the original df against which the word list is checked).</p>
<p>I believe your original data didn't have any matches, which makes it harder to check if the solution is correct, but I changed some words around so that words from the list appear in the sentences and it appears to be working.</p>
<p>You may also considering nesting <code>sapply</code> statements if you feel the <code>for</code> loop runs too slowly.</p>
",1,0,84,2020-09-15 13:18:10,https://stackoverflow.com/questions/63902715/find-if-any-word-in-a-dataset-appears-in-sentences-in-other-dataset-with-r
Extracting unique URLs in Python,"<p>I would like to extract entire unique url items in my list in order to move on a web scraping project. Although I have huge list of URLs on my side, I would like to generate here minimalist scenario to explain main issue on my side. Assume that my list is like that:</p>
<pre><code>url_list = [&quot;https://www.ox.ac.uk/&quot;,
            &quot;http://www.ox.ac.uk/&quot;,
            &quot;https://www.ox.ac.uk&quot;,
            &quot;http://www.ox.ac.uk&quot;,
            &quot;https://www.ox.ac.uk/index.php&quot;,
            &quot;https://www.ox.ac.uk/index.html&quot;,
            &quot;http://www.ox.ac.uk/index.php&quot;,
            &quot;http://www.ox.ac.uk/index.html&quot;,
            &quot;www.ox.ac.uk/&quot;,
            &quot;ox.ac.uk&quot;,
            &quot;https://www.ox.ac.uk/research&quot;        
            ]
def ExtractUniqueUrls(urls):
    pass

ExtractUniqueUrls(url_list)
</code></pre>
<p>For the minimalist scenario, I am expecting there are only two unique urls which are &quot;https://www.ox.ac.uk&quot; and &quot;https://www.ox.ac.uk/research&quot;. Although each url element have some differences such as &quot;<strong>http</strong>&quot;, &quot;<strong>https</strong>&quot;, with ending &quot;<strong>/</strong>&quot;, without ending &quot;<strong>/</strong>&quot;, <strong>index.php</strong>, <strong>index.html</strong>; they are all pointing exactly the same web page. There might be some other possibilities which I already missed them (Please remember them if you catch any). Anyway, what is the proper and efficient way to handle this issue using Python 3?</p>
<p>I am not looking for a hard-coded solution like focusing on each case individually. For instance, I do not want to manually check whether the url has &quot;/&quot; at the end or not. Possibly there is a much better solution with other packages such as urllib? For that reason, I looked the method of urllib.parse, but I could not come up a proper solution so far.
Thanks</p>
<p><strong>Edit</strong>: I added one more example into my list at the end in order to explain in a better way. Otherwise, you might assume that I am looking for the root url, but this not the case at all.</p>
","python, python-3.x, url, web-scraping, text-mining","<p>By only following all cases you've reveiled:</p>
<pre><code>url_list = [&quot;https://www.ox.ac.uk/&quot;,
            &quot;http://www.ox.ac.uk/&quot;,
            &quot;https://www.ox.ac.uk&quot;,
            &quot;http://www.ox.ac.uk&quot;,
            &quot;https://www.ox.ac.uk/index.php&quot;,
            &quot;https://www.ox.ac.uk/index.html&quot;,
            &quot;http://www.ox.ac.uk/index.php&quot;,
            &quot;http://www.ox.ac.uk/index.html&quot;,
            &quot;www.ox.ac.uk/&quot;,
            &quot;ox.ac.uk&quot;,
            &quot;ox.ac.uk/research&quot;,
            &quot;ox.ac.uk/index.php?12&quot;]

def url_strip_gen(source: list):
    replace_dict = {&quot;.php&quot;: &quot;&quot;, &quot;.html&quot;: &quot;&quot;, &quot;http://&quot;: &quot;&quot;, &quot;https://&quot;: &quot;&quot;}

    for url in source:
        for key, val in replace_dict.items():
            url = url.replace(key, val, 1)
        url = url.rstrip('/')

        yield url[4:] if url.startswith(&quot;www.&quot;) else url


print(set(url_strip_gen(url_list)))
</code></pre>
<pre><code>{'ox.ac.uk/index?12', 'ox.ac.uk/index', 'ox.ac.uk/research', 'ox.ac.uk'}
</code></pre>
<p>This won't cover case if url contains <code>.html</code> like <code>www.htmlsomething</code>, in that case it can be compensated with <code>urlparse</code> as it stores path and url separately like below:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import pprint
&gt;&gt;&gt; from urllib.parse import urlparse
&gt;&gt;&gt; a = urlparse(&quot;http://ox.ac.uk/index.php?12&quot;)
&gt;&gt;&gt; pprint.pprint(a)
ParseResult(scheme='http', netloc='ox.ac.uk', path='/index.php', params='', query='12', fragment='')
</code></pre>
<p>However, if without scheme:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; a = urlparse(&quot;ox.ac.uk/index.php?12&quot;)
&gt;&gt;&gt; pprint.pprint(a)
ParseResult(scheme='', netloc='', path='ox.ac.uk/index.php', params='', query='12', fragment='')
</code></pre>
<p>All host goes to <code>path</code> attribute.</p>
<p>To compensate this we either need to remove scheme and add one for all or check if url starts with scheme else add one. Prior is easier to implement.</p>
<pre><code>replace_dict = {&quot;http://&quot;: &quot;&quot;, &quot;https://&quot;: &quot;&quot;}

    for url in source:
        # Unify scheme to HTTP
        for key, val in replace_dict.items():
            url = url.replace(key, val, 1)

        url = &quot;http://&quot; + (url[4:] if url.startswith(&quot;www.&quot;) else url)
        parsed = urlparse(url)
</code></pre>
<p>With this you are guaranteed to get separate control of each sections for your url via <code>urlparse</code>. However as you do not specified which parameter should be considered for url to be <em>unique</em> enough, I'll leave that task to you.</p>
",1,-4,759,2020-09-22 13:57:42,https://stackoverflow.com/questions/64011345/extracting-unique-urls-in-python
"R inspect() function, from tm package, only returns 10 outputs when using dictionary terms","<p>I have 70 PDFs of scientific papers that I'm trying to narrow down by looking for specific terms within them, using the dictionary function of <code>inspect()</code>, which is part of the tm package. My PDFs are stored in a VCorpus object. Here's an example of what my code looks like using the crude dataset and common terms that would show up in (probably) every example paper in crude:</p>
<pre><code>library(tm)
output.matrix &lt;- inspect(DocumentTermMatrix(crude,
                                      list(dictionary = c(&quot;i&quot;,&quot;and&quot;,
                                                          &quot;all&quot;,&quot;of&quot;,
                                                          &quot;the&quot;,&quot;if&quot;,
                                                          &quot;i'm&quot;,&quot;looking&quot;,
                                                          &quot;for&quot;,&quot;but&quot;,&quot;because&quot;,&quot;has&quot;,
                                                          &quot;it&quot;,&quot;was&quot;))))
output &lt;- data.frame(output.matrix)
</code></pre>
<p>This search only ever returns 10 papers into output.matrix. The outcome given is:</p>
<pre><code>Docs  all and because but for has i i'm the was
  144   0   9       0   5   5   2 0   0  17   1
  236   0   7       4   2   4   5 0   0  15   7
  237   1  11       1   3   3   2 0   0  30   2
  246   0   9       0   0   6   1 0   0  18   2
  248   1   6       1   1   2   0 0   0  27   4
  273   0   5       2   2   4   1 0   0  21   1
  368   0   1       0   1   0   0 0   0  11   2
  489   0   5       0   0   4   0 0   0   8   0
  502   0   6       0   1   5   0 0   0  13   0
  704   0   5       1   0   3   2 0   0  21   0
</code></pre>
<p>For my actual dataset of 70 papers, I know there should be greater than 10 because as I add more PDFs to my VCorpus, which I know contain at least one of my search terms, I still only get 10 in the output. I want to adjust the outcome to be a list, like the one shown, that gives <em>every</em> paper from the VCorpus that contains a term, not just what I assume is the first 10.</p>
<p>Using R version 4.0.2, macOS High Sierra 10.13.6</p>
","r, pdf, text-mining","<p>You are misinterpreting what <code>inspect</code> does. For a document term matrix it show the first 10 rows and columns. <code>inspect</code> should only be used to check your corpus or document term matrix if it looks as you expect. Never for transforming data to a data.frame. If you want the data of the document term matrix in a data.frame, the following piece of code does this, using your example code and removing all the rows and columns that don't have a value for any of the documents or terms.</p>
<pre><code># do not use inspect as this will give a wrong result!
output.matrix &lt;- DocumentTermMatrix(crude,
                                    list(dictionary = c(&quot;i&quot;,&quot;and&quot;,
                                                        &quot;all&quot;,&quot;of&quot;,
                                                        &quot;the&quot;,&quot;if&quot;,
                                                        &quot;i'm&quot;,&quot;looking&quot;,
                                                        &quot;for&quot;,&quot;but&quot;,&quot;because&quot;,&quot;has&quot;,
                                                        &quot;it&quot;,&quot;was&quot;)))


# remove rows and columns that are 0 staying inside a sparse matrix for speed
out &lt;- output.matrix[slam::row_sums(output.matrix) &gt; 0,
                     slam::col_sums(output.matrix) &gt; 0]


# transform to data.frame
out_df &lt;- data.frame(docs = row.names(out), as.matrix(out), row.names = NULL)

out_df
   docs all and because but for. has the was
1   127   0   1       0   0    2   0   5   1
2   144   0   9       0   5    5   2  17   1
3   191   0   0       0   0    2   0   4   0
4   194   1   1       0   0    2   0   4   1
5   211   0   2       0   0    2   0   8   0
6   236   0   7       4   2    4   5  15   7
7   237   1  11       1   3    3   2  30   2
8   242   0   3       0   1    1   1   6   1
9   246   0   9       0   0    6   1  18   2
10  248   1   6       1   1    2   0  27   4
11  273   0   5       2   2    4   1  21   1
12  349   0   2       0   0    0   0   5   0
13  352   0   3       0   0    0   0   7   1
14  353   0   1       0   0    2   1   4   3
15  368   0   1       0   1    0   0  11   2
16  489   0   5       0   0    4   0   8   0
17  502   0   6       0   1    5   0  13   0
18  543   0   0       0   0    3   0   5   1
19  704   0   5       1   0    3   2  21   0
20  708   0   0       0   0    0   0   0   1
</code></pre>
",3,2,3209,2020-09-22 21:12:10,https://stackoverflow.com/questions/64017754/r-inspect-function-from-tm-package-only-returns-10-outputs-when-using-dictio
Using pdftools in R to extract specific table after a string,"<p>I have couple of pdfs and I wish to extract the shareholders table. How can I specify such that only table appearing after the string 'TWENTY LARGEST SHAREHOLDERS' is extracted?</p>
<p>I tried but was not quite sure of the function part.</p>
<pre><code>library(&quot;pdftools&quot;)
library(&quot;tidyverse&quot;)

url &lt;- c(&quot;https://www.computershare.com/News/Annual%20Report%202019.pdf?2&quot;)

raw_text &lt;- map(url, pdf_text)


clean_table &lt;- function(table){
  table &lt;- str_split(table, &quot;\n&quot;, simplify = TRUE)
  table_start &lt;- stringr::str_which(table, &quot;TWENTY LARGEST SHAREHOLDERS&quot;)
  table &lt;- table[1, (table_start +1 ):(table_end - 1)]
  table &lt;- str_replace_all(table, &quot;\\s{2,}&quot;, &quot;|&quot;)
  text_con &lt;- textConnection(table)
  data_table &lt;- read.csv(text_con, sep = &quot;|&quot;)
  colnames(data_table) &lt;- c(&quot;Name&quot;, &quot;Number of Shares&quot;, &quot;Percentage&quot;)
}

shares &lt;- map_df(raw_text, clean_table) 

</code></pre>
","r, text-mining, data-extraction, pdftools","<p>Try this. Besides some minor issues the main change is that I first get the page which contains the desired table. BTW: You have to search for &quot;Twenty Largest Shareholders&quot; and not &quot;TWENTY LARGEST SHAREHOLDERS&quot;.</p>

<pre class=""lang-r prettyprint-override""><code>library(pdftools)
library(tidyverse)

# download pdf
url &lt;- c(&quot;https://www.computershare.com/News/Annual%20Report%202019.pdf?2&quot;)

raw_text &lt;- map(url, pdf_text)

clean_table1 &lt;- function(raw) {
  
  # Split the single pages
  raw &lt;- map(raw, ~ str_split(.x, &quot;\\n&quot;) %&gt;% unlist())
  # Concatenate the splitted pages
  raw &lt;- reduce(raw, c)
  
  table_start &lt;- stringr::str_which(tolower(raw), &quot;twenty largest shareholders&quot;)
  table_end &lt;- stringr::str_which(tolower(raw), &quot;total&quot;)
  table_end &lt;- table_end[min(which(table_end &gt; table_start))]
  
  table &lt;- raw[(table_start + 3 ):(table_end - 1)]
  table &lt;- str_replace_all(table, &quot;\\s{2,}&quot;, &quot;|&quot;)
  text_con &lt;- textConnection(table)
  data_table &lt;- read.csv(text_con, sep = &quot;|&quot;)
  colnames(data_table) &lt;- c(&quot;Name&quot;, &quot;Number of Shares&quot;, &quot;Percentage&quot;)
  data_table
}

shares &lt;- map_df(raw_text, clean_table1) 
head(shares)
#&gt;                                                    Name Number of Shares
#&gt; 1             J P Morgan Nominees Australia Pty Limited      109,500,852
#&gt; 2                         Citicorp Nominees Pty Limited       57,714,777
#&gt; 3                                       Mr Chris Morris       32,231,000
#&gt; 4                             National Nominees Limited       19,355,892
#&gt; 5                                         Welas Pty Ltd       18,950,000
#&gt; 6 BNP Paribas Nominees Pty Ltd &lt;Agency Lending DRP A/C&gt;       11,520,882
#&gt;   Percentage
#&gt; 1      20.17
#&gt; 2      10.63
#&gt; 3       5.94
#&gt; 4       3.56
#&gt; 5       3.49
#&gt; 6       2.12
</code></pre>
",2,0,3882,2020-10-10 05:23:39,https://stackoverflow.com/questions/64290260/using-pdftools-in-r-to-extract-specific-table-after-a-string
"How to get the string, and the string count whole word in the data frame in R (TextMining)","<p>Want to get the string with their counting in r in data frame</p>
<p>The data set is like:</p>
<p><code>No Str</code></p>
<p><code>1 &quot;I like travelling in Australia.&quot;</code></p>
<p><code>2 &quot;I like travelling is America.&quot; </code></p>
<p>The result should like:</p>
<p><code>No Str count</code></p>
<p><code>1 I 1</code></p>
<p><code>1 like 1</code></p>
<p><code>1 to 1</code></p>
<p><code>1 travelling 1</code></p>
<p><code>1 in 1</code></p>
<p><code>1 Australia 1</code></p>
<p><code>2 I 1</code></p>
<p><code>2 like 1</code></p>
<p><code>2 to 1</code></p>
<p><code>2 travelling 1</code></p>
<p><code>2 in 1</code></p>
<p><code>2 America 1</code></p>
<p>I have tried to use split the first row and it works but it cannot count the whole word</p>
<p><code>strsplit(data[1,2], &quot; &quot;))</code></p>
<p>Can anyone help me how to do that result?</p>
","r, dataframe, text-mining","<p>You can use <code>separate_rows</code> to get each word in different row and use <code>count</code> to count the word frequency in each <code>No</code>.</p>
<pre><code>library(dplyr)
library(tidyr)

result &lt;- data %&gt;% separate_rows(Str, sep = '\\s') %&gt;% count(No, Str)
</code></pre>
",0,0,41,2020-10-11 10:06:13,https://stackoverflow.com/questions/64302802/how-to-get-the-string-and-the-string-count-whole-word-in-the-data-frame-in-r-t
pandas: Split and convert series of alphanumeric texts to columns and rows,"<p><strong>Current data frame:</strong> I have a pandas data frame where each employee has a text code(all codes start with T) and an associated frequency right next to the code. All text codes have 8 characters.</p>
<pre><code>+----------+-------------------------------------------------------------+
|  emp_id  |   text                                                      |
+----------+-------------------------------------------------------------+
|   E0001  | [T0431516,-8,T0401531,-12,T0517519,12]                      |
|   E0002  | [T0701540,-1,T0431516,-2]                                   |
|   E0003  | [T0517519,-1,T0421531,-7,T0516319,9,T0500371,-6,T0309711,-3]|
|   E0004  | [T0516319,-3]                                               |
|   E0005  | [T0431516,2]                                                |
+----------+-------------------------------------------------------------+
</code></pre>
<p><strong>Expected data frame:</strong> I am trying to make the text codes present in the data frame as individual columns and if an employee has a frequency for that code then populate frequency else 0.</p>
<pre><code>+----------+----------------------------------------------------------------------------------------+
|  emp_id  | T0431516 | T0401531 | T0517519 | T0701540 | T0421531 |  T0516319 | T0500371 | T0309711 |                                      
+----------+----------------------------------------------------------------------------------------+
|   E0001  | -8       | -12      | 12       | 0        | 0        | 0         | 0        | 0        |
|   E0002  | -2       | 0        | 0        | -1       | 0        | 0         | 0        | 0        |
|   E0003  | 0        | 0        | -1       | 0        | -7       | 9         | -6       | -3       |
|   E0004  | 0        | 0        | 0        | 0        | 0        | -3        | 0        | 0        |
|   E0005  | 2        | 0        | 0        | 0        | 0        | 0         | 0        | 0        |
+----------+----------------------------------------------------------------------------------------+
</code></pre>
<p><strong>Sample data</strong>:</p>
<pre><code>pd.DataFrame({'emp_id' : {0: 'E0001', 1: 'E0002', 2: 'E0003', 3: 'E0004', 4: 'E0005'},
                'text' :  {0: '[T0431516,-8,T0401531,-12,T0517519,12]', 1: '[T0701540,-1,T0431516,-2]', 2: '[T0517519,-1,T0421531,-7,T0516319,9,T0500371,-6,T0309711,-3]', 3: '[T0516319,-3]', 4: '[T0431516,2]'}
                })
</code></pre>
<p>So, far my attempts were unsuccessful. Any pointers/help is much appreciated!</p>
","python, pandas, parsing, nlp, text-mining","<p>You can <code>explode</code> the dataframe and then create a <code>pivot_table</code>:</p>
<pre><code>df = pd.DataFrame({'emp_id' : ['E0001', 'E0002', 'E0003', 'E0004', 'E0005'],
                  'text' : [['T0431516',-8,'T0401531',-12,'T0517519',12],
                 ['T0701540',-1,'T0431516',-2],['T0517519',-1,'T0421531',-7,'T0516319',9,'T0500371',-6,'T0309711',-3],
                 ['T0516319',-3], ['T0431516',2]]})
df = df.explode('text')
df['freq'] = df['text'].shift(-1)
df = df[df['text'].str[0] == 'T']
df['freq'] = df['freq'].astype(int)
df = pd.pivot_table(df, index='emp_id', columns='text', values='freq',aggfunc = 'sum').fillna(0).astype(int)
df
Out[1]: 
text    T0309711  T0401531  T0421531  T0431516  T0500371  T0516319  T0517519  \
emp_id                                                                         
E0001          0       -12         0        -8         0         0        12   
E0002          0         0         0        -2         0         0         0   
E0003         -3         0        -7         0        -6         9        -1   
E0004          0         0         0         0         0        -3         0   
E0005          0         0         0         2         0         0         0   

text    T0701540  
emp_id            
E0001          0  
E0002         -1  
E0003          0  
E0004          0  
E0005          0  
</code></pre>
",1,0,154,2020-10-19 21:06:39,https://stackoverflow.com/questions/64435099/pandas-split-and-convert-series-of-alphanumeric-texts-to-columns-and-rows
pandas - join words after specific word,"<p>I have a pandas dataframe with a column of comments in the form</p>
<pre><code>import pandas as pd
df = pd.DataFrame({'comment':['aaa bbb ccc not verb ddd']}) 
df.loc[0,'comment']

'aaa bbb ccc not verb ddd'
</code></pre>
<p>I want to join together the <code>not</code> with the word after it, in the example <code>verb</code> as <code>not_verb</code>, and return the rest of the row as is:</p>
<pre><code>'aaa bbb ccc not_verb ddd'
</code></pre>
<p>Any help is appreciated.</p>
<p><strong>EDIT:</strong></p>
<p>Basically I'd like to join from &quot; not &quot; to the end of the word following it.</p>
","python, pandas, text-mining","<p>Use <code>str.replace</code>:</p>
<pre><code>df.comment.str.replace(r'\b(not\s)', 'not_')
</code></pre>
<p>Output:</p>
<pre><code>0    aaa bbb ccc not_verb ddd
Name: comment, dtype: object
</code></pre>
",2,-1,107,2020-10-22 14:50:54,https://stackoverflow.com/questions/64484854/pandas-join-words-after-specific-word
How replace character next to numeric but not next to alphanumeric in R,"<p>I have this string</p>
<pre><code>char &lt;- &quot;866224; Genoma viral SARS-CoV-2: Detectable; 1096628; Genoma viral SARS-CoV-2: No detectable&quot;
</code></pre>
<p>and I need replace ; next to numbers with | but keep ; next to alphanumeric like this:</p>
<pre><code>&quot;866224| Genoma viral SARS-CoV-2: Detectable; 1096628| Genoma viral SARS-CoV-2: No detectable&quot;
</code></pre>
<p>I was trying with str_replace_all</p>
<pre><code>str_replace_all(char, &quot;[0-9];&quot;, &quot;|&quot;)
</code></pre>
<p>but remove the last number.</p>
<pre><code>&quot;86622| Genoma viral SARS-CoV-2: Detectable; 109662| Genoma viral SARS-CoV-2: No detectable&quot;
</code></pre>
<p>Thanks in advance.</p>
","r, text-mining, stringr","<p>the <code>{stringr}</code> package allows for lookaheads and lookbehinds you could use them instead of actually capturing the last number then pasting it:</p>
<pre class=""lang-r prettyprint-override""><code>char &lt;- &quot;866224; Genoma viral SARS-CoV-2: Detectable; 1096628; Genoma viral SARS-CoV-2: No detectable&quot;
str_replace_all(char, &quot;(?&lt;=[0-9]);&quot;, &quot;|&quot;)
#&gt; [1] &quot;866224| Genoma viral SARS-CoV-2: Detectable; 1096628| Genoma viral SARS-CoV-2: No detectable&quot;
</code></pre>
<p>the lookbehind <code>(?&lt;=...)</code> basically checks if the expression that follows is preceded by <code>...</code>.
if you want to use this in <code>baseR</code> then:</p>
<pre><code>gsub(&quot;(?&lt;=[0-9]);&quot;, &quot;|&quot;, char, perl=TRUE)
#&gt; [1] &quot;866224| Genoma viral SARS-CoV-2: Detectable; 1096628| Genoma viral SARS-CoV-2: No detectable&quot;
</code></pre>
",1,0,54,2020-10-23 00:06:21,https://stackoverflow.com/questions/64492166/how-replace-character-next-to-numeric-but-not-next-to-alphanumeric-in-r
Extracting a List from Text using Regular Expression in Python,"<p>I am looking to extract a list of tuples from the following string:</p>
<pre><code>text='''Consumer Price Index:
        +0.2% in Sep 2020

        Unemployment Rate:
        +7.9% in Sep 2020

        Producer Price Index:
        +0.4% in Sep 2020

        Employment Cost Index:
        +0.5% in 2nd Qtr of 2020

        Productivity:
        +10.1% in 2nd Qtr of 2020

        Import Price Index:
        +0.3% in Sep 2020

        Export Price Index:
        +0.6% in Sep 2020'''
</code></pre>
<p>I am using 'import re' for the process.</p>
<p>The output should be something like: [('Consumer Price Index', '+0.2%', 'Sep 2020'), ...]</p>
<p>I want to use a re.findall function that produces the above output, so far I have this:</p>
<pre><code>re.findall(r&quot;(:\Z)\s+(%\Z+)(\Ain )&quot;, text)
</code></pre>
<p>Where I am identifying the characters prior to ':', then the characters prior to '%' and then the characters after 'in'.</p>
<p>I'm really just clueless on how to continue. Any help would be appreciated. Thanks!</p>
","python, regex, text-mining","<p>You can use</p>
<pre class=""lang-py prettyprint-override""><code>re.findall(r'(\S.*):\n\s*(\+?\d[\d.]*%)\s+in\s+(.*)', text)
# =&gt; [('Consumer Price Index', '+0.2%', 'Sep 2020'), ('Unemployment Rate', '+7.9%', 'Sep 2020'), ('Producer Price Index', '+0.4%', 'Sep 2020'), ('Employment Cost Index', '+0.5%', '2nd Qtr of 2020'), ('Productivity', '+10.1%', '2nd Qtr of 2020'), ('Import Price Index', '+0.3%', 'Sep 2020'), ('Export Price Index', '+0.6%', 'Sep 2020')]
</code></pre>
<p>See the <a href=""https://regex101.com/r/H8WQSY/2"" rel=""noreferrer"">regex demo</a> and the <a href=""https://ideone.com/D9EviJ"" rel=""noreferrer"">Python demo</a>.</p>
<p><strong>Details</strong></p>
<ul>
<li><code>(\S.*)</code> - Group 1: a non-whitespace char followed with any zero or more chars other than line break chars as many as possible</li>
<li><code>:</code> - a colon</li>
<li><code>\n</code> - a newline</li>
<li><code>\s*</code> - 0 or more whitespaces</li>
<li><code>(\+?\d[\d.]*%)</code> - Group 2: optional <code>+</code>, a digit, zero or more digits/dots, and a <code>%</code></li>
<li><code>\s+in\s+</code> - <code>in</code> enclosed with 1+ whitespaces</li>
<li><code>(.*)</code> - Group 3: any zero or more chars other than line break chars as many as possible</li>
</ul>
",5,4,1140,2020-11-01 13:52:08,https://stackoverflow.com/questions/64632626/extracting-a-list-from-text-using-regular-expression-in-python
Using spacy to get rid of stopwords in pandas series,"<p>I had been trying to get rid of stopwords using spacy library.</p>
<p><strong>Code</strong></p>
<pre><code>import spacy
import pandas as pd
import numpy as np

nlp= spacy.load('en_core_web_sm')
</code></pre>
<p><strong>my_series:</strong></p>
<pre><code>my_series

0        this laptop sits at just over 4 stars while so...
1        i ordered this monitor because i wanted to mak...
2        this monitor is a great deal for the price and...
3        bought this for the height adjustment. the swi...
4        worked for a month and then it died. after 5 c...
                               ...                        
30618                                           great deal
30619                                      pour le travail
30620                                         business use
30621                                            good size
30622    pour mon ordinateur.plus grande image.vraiment...
Name: text_body, Length: 30623, dtype: object
</code></pre>
<p><strong>Tokenize</strong></p>
<pre><code>s_tokenized=my_series.apply(lambda x: nlp(x))
</code></pre>
<p><strong>Remove stopwords</strong></p>
<pre><code>all_stopwords = nlp.Defaults.stop_words
filtered_text=s_tokenized.apply(lambda x: [w for w in x if not w in all_stopwords])
filtered_text

0        [this, laptop, sits, at, just, over, 4, stars,...
1        [i, ordered, this, monitor, because, i, wanted...
2        [this, monitor, is, a, great, deal, for, the, ...
3        [bought, this, for, the, height, adjustment, ....
4        [worked, for, a, month, and, then, it, died, ....
                               ...                        
30618                                        [great, deal]
30619                                  [pour, le, travail]
30620                                      [business, use]
30621                                         [good, size]
30622    [pour, mon, ordinateur.plus, grande, image.vra...
Name: text_body, Length: 30623, dtype: object
</code></pre>
<p>tokenize seems to be working fine but removing stopwords does not seems to remove any word at all nor raising any errors. Is there something I miss or did wrong?</p>
","python, nlp, data-science, text-mining, spacy","<p>You have a problem with this line:</p>
<pre><code>filtered_text=s_tokenized.apply(lambda x: [w for w in x if not w in all_stopwords])
</code></pre>
<p>correct it to:</p>
<pre><code>filtered_text=s_tokenized.apply(lambda x: [w for w in x if not w.text in all_stopwords])
</code></pre>
<p>and you're fine to go:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
nlp=spacy.load(&quot;en_core_web_sm&quot;)
s_tokenized = my_series.apply(nlp)
all_stopwords = nlp.Defaults.stop_words
filtered_text=s_tokenized.apply(lambda x: [w for w in x if not w.text in all_stopwords])
filtered_text
0      [laptop, sits, 4, stars]
1    [ordered, monitor, wanted]
dtype: object
</code></pre>
<p>Note, you do not need pandas Series to hold your data. Just string or list of strings is enough. The Spacy way of doing the same, that will scale for even out of memory data is:</p>
<pre><code>import spacy
nlp=spacy.load(&quot;en_core_web_sm&quot;)
texts = [&quot;this laptop sits at just over 4 stars while&quot;, &quot;i ordered this monitor because i wanted&quot;]
docs = nlp.pipe(texts)
filtered_text= []
for doc in docs:
#     yield [tok for tok in doc if not tok.is_stop]
    filtered_text.append([tok for tok in doc if not tok.is_stop])
print(filtered_text)

[[laptop, sits, 4, stars], [ordered, monitor, wanted]]
</code></pre>
",1,1,345,2020-11-03 12:42:05,https://stackoverflow.com/questions/64663068/using-spacy-to-get-rid-of-stopwords-in-pandas-series
"Split a string at uppercase letters, but only if a lowercase letter follows in Python","<p>I am using pdfminer.six in Python to extract long text data. Unfortunately, the Miner does not always work very well, especially with paragraphs and text wrapping. For example I got the following output:</p>
<pre><code>&quot;2018Annual ReportInvesting for Growth and Market LeadershipOur CEO will provide you with all further details below.&quot;

--&gt; &quot;2018 Annual Report Investing for Growth and Market Leadership Our CEO will provide you with all further details below.&quot;
</code></pre>
<p>Now I would like to insert a space whenever a lowercase letter is followed by a capital letter and then a smaller letter (and for numbers). So that in the end <code>&quot;2018Annual&quot;</code> becomes <code>&quot;2018 Annual&quot;</code> and <code>&quot;ReportInvesting&quot;</code> becomes <code>&quot;Report Investing&quot;</code>, but <code>&quot;...CEO...&quot;</code> remains <code>&quot;...CEO...&quot;</code>.</p>
<p>I only found solutions to <a href=""https://stackoverflow.com/questions/2277352/split-a-string-at-uppercase-letters"">Split a string at uppercase letters</a> and <a href=""https://stackoverflow.com/a/3216204/14635557"">https://stackoverflow.com/a/3216204/14635557</a> but could not rewrite it. Unfortunately I am totally new in the field of Python.</p>
","python, split, text-mining, uppercase","<p>We can try using <code>re.sub</code> here for a regex approach:</p>
<pre class=""lang-py prettyprint-override""><code>inp = &quot;2018Annual ReportInvesting for Growth and Market LeadershipOur CEO will provide you with all further details below.&quot;
inp = re.sub(r'(?&lt;![A-Z\W])(?=[A-Z])', ' ', inp)
print(inp)
</code></pre>
<p>This prints:</p>
<pre><code>2018 Annual Report Investing for Growth and Market Leadership Our CEO will provide you with all further details below.
</code></pre>
<p>The regex used here says to insert a space at any point for which:</p>
<pre><code>(?&lt;![A-Z\W])  what precedes is a word character EXCEPT
              for capital letters
(?=[A-Z])     and what follows is a capital letter
</code></pre>
",7,-3,4377,2020-11-14 14:03:39,https://stackoverflow.com/questions/64834708/split-a-string-at-uppercase-letters-but-only-if-a-lowercase-letter-follows-in-p
SQL/BigQuery text classification,"<p>I need to implement a simple text classification using regex, and for this I thought to apply a simple CASE WHEN statement, but rather than in case one condition is met, I want to iterate over all the CASEs.</p>
<p>For example,</p>
<pre class=""lang-sql prettyprint-override""><code>with `table` as(
SELECT 'It is undeniable that AI will change the landscape of the future. There is a frequent increase in the demand for AI-related jobs, especially in data science and machine learning positions. It is believed that artificial intelligence will change the world, just like how electricity changed the world about 100 years ago. As Professor Andrew NG has famously stated multiple times “Artificial Intelligence is the new electricity.” We have advanced immensely in the field of artificial intelligence. With the increase in the processing and computational power, thanks to graphical processing units (GPUs), and also due to the abundance of data, we have reached a position of supremacy in Deep Learning and modern algorithms.' as text
)
SELECT
  CASE
    WHEN REGEXP_CONTAINS(text, r'(?i)ai') THEN 'AI'
    WHEN REGEXP_CONTAINS(text, r'(?i)computational power') THEN 'Engineering'
    WHEN REGEXP_CONTAINS(text, r'(?i)deep learning') THEN 'Deep Learning'
  END as topic,
  text
FROM `table`
</code></pre>
<p>With this query, the text is classified as AI, because it is the first condition that is met, but it should be classified as AI, Engineering and <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""nofollow noreferrer"">deep learning</a> in an array or in three different rows, because all three conditions are met.</p>
<p>How can I classify the text applying all the regex/conditions?</p>
","sql, text, google-bigquery, text-mining, mining","<p>I feel the below is the most generic and reusable solution (BigQuery Standard SQL):</p>
<pre><code>#standardSQL
with `table` as(
select 'It is undeniable that AI will change the landscape of the future. There is a frequent increase in the demand for AI-related jobs, especially in data science and machine learning positions. It is believed that artificial intelligence will change the world, just like how electricity changed the world about 100 years ago. As Professor Andrew NG has famously stated multiple times “Artificial Intelligence is the new electricity.” We have advanced immensely in the field of artificial intelligence. With the increase in the processing and computational power, thanks to graphical processing units (GPUs), and also due to the abundance of data, we have reached a position of supremacy in Deep Learning and modern algorithms.' as text
), classification as (
  select 'ai' term, 'AI' topic union all
  select 'computational power', 'Engineering' union all
  select 'deep learning', 'Deep Learning'
), pattern as (
  select r'(?i)' || string_agg(term, '|') as regexp_pattern
  from classification
)
select
   array_to_string(array(
    select distinct topic
    from unnest(regexp_extract_all(lower(text), regexp_pattern)) term
    join classification using(term)
   ), ', ') topics,
  text
from `table`, pattern
</code></pre>
<p>With output:</p>
<p><a href=""https://i.sstatic.net/C7Quy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/C7Quy.png"" alt=""Enter image description here"" /></a></p>
",1,0,537,2020-11-17 18:35:04,https://stackoverflow.com/questions/64881264/sql-bigquery-text-classification
Convert the locations to pages and then compute the average sentiment in each page. Plot that average score by page,"<p>Need help with 9. But I've included the relevant code.</p>
5. We will later make a plot of sentiment versus location in the book. For this, it will be useful to add a column with the word number to the table.
<pre><code>words &lt;- words %&gt;% 
  mutate(word_num = 1:length(word))
head(words)
</code></pre>
6. Remove the stop words and numbers from the words object. Hint: use the anti_join.
<pre><code>data(&quot;stop_words&quot;)
words_clean &lt;- words %&gt;% 
  anti_join(stop_words) %&gt;% 
  filter(!str_detect(word,&quot;^\\d+$&quot;))
head(words_clean)
</code></pre>
7. Now use the AFINN lexicon to assign a sentiment value to each word.
<pre><code>library(textdata)
afinn &lt;- get_sentiments(&quot;afinn&quot;)
words_sentiments &lt;- words_clean %&gt;% 
  inner_join(afinn, by = &quot;word&quot;)  
head(words_sentiments)
</code></pre>
8. Make a plot of sentiment score versus location in the book and add a smoother.
<pre><code>library(ggplot2)
words_sentiments %&gt;% ggplot(aes(x= word_num, y=value)) +
  geom_point()+
  geom_smooth()
  
  
  
</code></pre>
9. Assume there are 300 words per page. Convert the locations to pages and then compute the average sentiment in each page. Plot that average score by page. Add a smoother that appears to go through data.
<pre><code># word_sentiments2 &lt;- words_sentiments %&gt;% 
  cut(words_sentiments$word_num,seq(1,max(words_sentiments$word_num), by=300))
word
</code></pre>
","r, text-mining","<p>To get the <code>page_number</code> we use ceiling of the quotient, then we <code>group_by</code> the <code>page_number</code> and summarise the values</p>
<pre class=""lang-r prettyprint-override""><code>words_sentiments %&gt;%
  mutate(page_number = ceiling(word_num / 300)) %&gt;%
   group_by(page_number) %&gt;%
    summarise(average=mean(value)) -&gt; summed
summed %&gt;%  ggplot(aes(x=page_number, y=average)) + geom_point() + geom_smooth()
</code></pre>
",1,0,176,2020-11-18 21:29:41,https://stackoverflow.com/questions/64901688/convert-the-locations-to-pages-and-then-compute-the-average-sentiment-in-each-pa
How to use quanteda to find instances of appearance of certain words before certain others in a sentence,"<p>As an R newbie, by using quanteda I am trying to find instances when a certain word sequentially appears somewhere before another certain word in a sentence. To be more specific, I am looking for instances when the word &quot;investors&quot; is located somewhere before the word &quot;shall&quot; in a sentence in the corpus consisted of an international treaty concluded between Morocco and Nigeria (the text can be found here: <a href=""https://edit.wti.org/app.php/document/show/bde2bcf4-e20b-4d05-a3f1-5b9eb86d3b3b"" rel=""nofollow noreferrer"">https://edit.wti.org/app.php/document/show/bde2bcf4-e20b-4d05-a3f1-5b9eb86d3b3b</a>).</p>
<p>The problem is that sometimes there are multiple words between these two words. For instance,  sometimes it is written as &quot;investors and investments shall&quot;. I tried to apply similar solutions offered on this website. When I tried the solution on (<a href=""https://stackoverflow.com/questions/63150922/keyword-in-context-kwic-for-skipgrams"">Keyword in context (kwic) for skipgrams?</a>) and ran the following code:</p>
<pre><code> kwic(corpus_mar_nga, phrase(&quot;investors * shall&quot;))
</code></pre>
<p>I get 0 observations since this counts only instances when there is only one word between &quot;investors&quot; and &quot;shall&quot;.</p>
<p>And when I follow another solution offered on (<a href=""https://stackoverflow.com/questions/49907577/is-it-possible-to-use-kwic-function-to-find-words-near-to-each-other"">Is it possible to use `kwic` function to find words near to each other?</a>) and ran the following code:</p>
<pre><code>toks &lt;- tokens(corpus_mar_nga)
toks_investors &lt;- tokens_select(toks, &quot;investors&quot;, window = 10)
kwic(toks_investors, &quot;shall&quot;)
</code></pre>
<p>I get instances when &quot;investor&quot; appear also after &quot;shall&quot; and this changes the context fundamentally since in that case, the subject of the sentence is something different.</p>
<p>At the end, in addition to instances of &quot;investors shall&quot;, I should also be getting, for example the instances when it reads as &quot;Investors, their investment and host state authorities shall&quot;, but I can't do it with the above codes.</p>
<p>Could anyone offer me a solution on this issue?</p>
<p>Huge thanks in advance!</p>
","r, nlp, text-mining, quanteda","<p>Good question.  Here are two methods, one relying on regular expressions on the corpus text, and the second using (as @Kohei_Watanabe suggests in the comment) using window for <code>tokens_select()</code>.</p>
<p>First, create some sample text.</p>

<pre class=""lang-r prettyprint-override""><code>library(&quot;quanteda&quot;)
## Package version: 2.1.2

# sample text
txt &lt;- c(&quot;The investors and their supporters shall do something.
          Shall we tell the investors?  Investors shall invest.
          Shall someone else do something?&quot;)
</code></pre>
<p>Now reshape this into sentences, since your search occurs within sentence.</p>
<pre class=""lang-r prettyprint-override""><code># reshape to sentences
corp &lt;- txt %&gt;%
  corpus() %&gt;%
  corpus_reshape(to = &quot;sentences&quot;)
</code></pre>
<p>Method 1 uses regular expressions.  We add a boundary (<code>\\b</code>) before &quot;investors&quot;, and the <code>.+</code> says one or more of any character in between &quot;investors&quot; and &quot;shall&quot;.  (This would not catch newlines, but <code>corpus_reshape(x, to = &quot;sentences&quot;)</code> will remove them.)</p>
<pre class=""lang-r prettyprint-override""><code># method 1: regular expressions
corp$flag &lt;- stringi::stri_detect_regex(corp, &quot;\\binvestors.+shall&quot;,
  case_insensitive = TRUE
)
print(corpus_subset(corp, flag == TRUE), -1, -1)
## Corpus consisting of 2 documents and 1 docvar.
## text1.1 :
## &quot;The investors and their supporters shall do something.&quot;
## 
## text1.2 :
## &quot;Investors shall invest.&quot;
</code></pre>
<p>A second method applies <code>tokens_select()</code> with an asymmetric window, with <code>kwic()</code>.  First we select all documents (which are sentences) containing &quot;investors&quot;, but discarding tokens before and keeping all tokens after.  1000 tokens after should be enough.  Then, apply the <code>kwic()</code> where we keep all context words but focus on the word after, which by definition must be after, since the first word was &quot;investors&quot;.</p>
<pre class=""lang-r prettyprint-override""><code># method 2: tokens_select()
toks &lt;- tokens(corp)
tokens_select(toks, &quot;investors&quot;, window = c(0, 1000)) %&gt;%
  kwic(&quot;shall&quot;, window = 1000)
##                                                                     
##  [text1.1, 5] investors and their supporters | shall | do something.
##  [text1.3, 2]                      Investors | shall | invest.
</code></pre>
<p>The choice depends on what suits your needs best.</p>
",2,1,742,2020-11-20 14:16:02,https://stackoverflow.com/questions/64931046/how-to-use-quanteda-to-find-instances-of-appearance-of-certain-words-before-cert
Extract specific numbers from txt files and insert them into a data frame,"<p>Good evening,
I would need some help extracting two numbers from a text file and inserting them into a dataframe.</p>
<p>This is my txt file:</p>
<pre><code>PING 10.0.12.100 (10.0.12.100) 56(84) bytes of data.
64 bytes from 10.0.12.100: icmp_seq=1 ttl=59 time=0.094 ms
64 bytes from 10.0.12.100: icmp_seq=2 ttl=59 time=0.070 ms
64 bytes from 10.0.12.100: icmp_seq=3 ttl=59 time=0.076 ms
64 bytes from 10.0.12.100: icmp_seq=4 ttl=59 time=0.075 ms
64 bytes from 10.0.12.100: icmp_seq=5 ttl=59 time=0.070 ms
64 bytes from 10.0.12.100: icmp_seq=6 ttl=59 time=0.060 ms
64 bytes from 10.0.12.100: icmp_seq=7 ttl=59 time=0.093 ms
64 bytes from 10.0.12.100: icmp_seq=8 ttl=59 time=0.080 ms
64 bytes from 10.0.12.100: icmp_seq=9 ttl=59 time=0.077 ms
64 bytes from 10.0.12.100: icmp_seq=10 ttl=59 time=0.082 ms
64 bytes from 10.0.12.100: icmp_seq=11 ttl=59 time=0.070 ms
64 bytes from 10.0.12.100: icmp_seq=12 ttl=59 time=0.075 ms
64 bytes from 10.0.12.100: icmp_seq=13 ttl=59 time=0.087 ms
64 bytes from 10.0.12.100: icmp_seq=14 ttl=59 time=0.069 ms
64 bytes from 10.0.12.100: icmp_seq=15 ttl=59 time=0.072 ms
64 bytes from 10.0.12.100: icmp_seq=16 ttl=59 time=0.079 ms
64 bytes from 10.0.12.100: icmp_seq=17 ttl=59 time=0.096 ms
64 bytes from 10.0.12.100: icmp_seq=18 ttl=59 time=0.071 ms

--- 10.0.12.100 ping statistics ---
18 packets transmitted, 18 received, 0% packet loss, time 17429ms
rtt min/avg/max/mdev = 0.060/0.077/0.096/0.013 ms
</code></pre>
<p>I would like to have a dataframe like this:</p>
<p><a href=""https://i.sstatic.net/Dsn67.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Dsn67.jpg"" alt=""enter image description here"" /></a></p>
<p>This is my code:</p>
<pre><code>import pandas as pd

df = pd.DataFrame(columns=[&quot;ICMP_SEQ&quot;, &quot;TIME&quot;])

with open(&quot;/content/H11H22_ping.txt&quot;, &quot;r&quot;) as f:
  txt = f.read() 
  print(txt)

  // code

  df = df.append({&quot;ICMP_SEQ&quot;: icmp_seq, &quot;TIME&quot;: time})
</code></pre>
<p>Thanks</p>
","python, dataframe, text-mining","<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html"" rel=""nofollow noreferrer"">str.extract</a>:</p>
<pre><code>df = pd.read_csv('/content/H11H22_ping.txt', skiprows=1, header=None, names=['logs'])
res = df['logs'].str.extract(r'icmp_seq=(?P&lt;icmp_seq&gt;\d+)\b.+\btime=(?P&lt;time&gt;\d+\.\d+)', expand=True)
print(res)
</code></pre>
<p><strong>Output</strong> <em>(partial)</em></p>
<pre><code>   icmp_seq   time
0         1  0.094
1         2  0.070
2         3  0.076
3         4  0.075
4         5  0.070
5         6  0.060
6         7  0.093
7         8  0.080
8         9  0.077
9        10  0.082
10       11  0.070
11       12  0.075
12       13  0.087
13       14  0.069
14       15  0.072
15       16  0.079
16       17  0.096
17       18  0.071
...
</code></pre>
",1,2,100,2020-12-01 17:04:45,https://stackoverflow.com/questions/65095083/extract-specific-numbers-from-txt-files-and-insert-them-into-a-data-frame
Removing stopwords from R data frame column,"<p>Here's the situation, one whose solution seemed to be simple at first, but that has turned out to be more complicated than I expected.</p>
<p>I have an R data frame with three columns: an ID, a column with texts (reviews), and one with numeric values which I want to predict based on the text.</p>
<p>I have already done some preprocessing on the text column, so it is free of punctuation, in lower case, and ready to be tokenized and turned into a matrix so I can train a model on it. The problem is I can't figure out how to remove the stop words from that text.</p>
<p>Here's what I am trying to do with the text2vec package. I was planning on doing the stop-word removal before this chunk at first. But anywhere will do.</p>
<pre><code>library(text2vec)

test_data &lt;- data.frame(review_id=c(1,2,3),
                        review=c('is a masterpiece a work of art',
                        'sporting some of the best writing and voice work',
                        'better in every possible way when compared'),
                         score=c(90, 100, 100))

tokens &lt;- word_tokenizer(test_data$review)
document_term_matrix &lt;- create_dtm(itoken(tokens), hash_vectorizer())
model_tfidf &lt;- TfIdf$new()
document_term_matrix &lt;- model_tfidf$fit_transform(document_term_matrix)

document_term_matrix &lt;- as.matrix(document_term_matrix)
</code></pre>
<p>I am hoping to get the review column to be something like:</p>
<pre><code>review=c('masterpiec work art',
         'sporting best writing voice work',
         'better possible way compared')
</code></pre>
","r, text-mining","<p>You can use <code>tidytext</code> package for this :</p>
<pre><code>library(tidytext)
library(dplyr)

test_data %&gt;%
  unnest_tokens(review, review) %&gt;%
  anti_join(stop_words, by= c(&quot;review&quot; = &quot;word&quot;))

#    review_id      review score
#1.2         1 masterpiece    90
#1.6         1         art    90
#2           2    sporting   100
#2.5         2     writing   100
#2.7         2       voice   100
#3.6         3    compared   100
</code></pre>
<p>To get the words back in one row you could do :</p>
<pre><code>test_data %&gt;%
  unnest_tokens(review, review) %&gt;%
  anti_join(stop_words, by= c(&quot;review&quot; = &quot;word&quot;)) %&gt;%
  group_by(review_id, score) %&gt;%
  summarise(review = paste0(review, collapse = ' '))

#  review_id score review                
#      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                 
#1         1    90 masterpiece art       
#2         2   100 sporting writing voice
#3         3   100 compared              
</code></pre>
",3,0,1587,2020-12-21 23:55:55,https://stackoverflow.com/questions/65401533/removing-stopwords-from-r-data-frame-column
How to receive text from pdf in R properly?,"<p>I want to make my own word embedding in R. I tried to open and receive text from pdf but it gives me this error: Error in normalizePath(path.expand(path), winslash, mustWork) :
path[1]=&quot;goethe_faust.pdf&quot;: No file found</p>
<p>Weird is that this file exists and I can open it with any pdf reader. It's not password locked or something like that.
My code:</p>
<pre><code>library(pdftools)
file_vector &lt;- list.files(path = &quot;pdf_collections&quot;)
pdf_text &lt;- pdf_text(file_vector[1]) 
</code></pre>
","r, text, text-mining","<p>By default <code>list.files</code> just includes the file names. To open these files,  you will need to include your path (pdf_collections).  You can fix this by specifying that you want the full path to the files.</p>
<pre><code>file_vector &lt;- list.files(path = &quot;pdf_collections&quot;, full.names=TRUE)
</code></pre>
",1,0,87,2020-12-29 15:14:08,https://stackoverflow.com/questions/65494346/how-to-receive-text-from-pdf-in-r-properly
How to convert dataframe from list with different number of rows in R?,"<p>I'm reading text data from many pdf files. I have object that is a list</p>
<p>list description</p>
<p><a href=""https://i.sstatic.net/YvGyO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YvGyO.png"" alt=""enter image description here"" /></a>
As you can see, the elements differ in number of rows. I need to convert that into dataframe for text mining, but when I use function as.data.frame() it gives me an error:</p>
<pre><code>as.data.frame(text_from_all_pdf)
</code></pre>
<blockquote>
<p>Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
arguments imply differing number of rows: 353, 457, 101, 517, 74, 102, 57, 5, 93</p>
</blockquote>
","r, text-mining","<p>Try unlisting <code>text_from_all_pdf</code> and create a dataframe with one column.</p>
<pre><code>data &lt;- data.frame(text = unlist(text_from_all_pdf))
</code></pre>
",0,-1,465,2020-12-30 09:22:08,https://stackoverflow.com/questions/65504612/how-to-convert-dataframe-from-list-with-different-number-of-rows-in-r
How to keep specific group of words or phrases of a text column in R?,"<p>I have a dataframe with a text column and I would like to create another column only with specific words or phrases matching the text column.
Let's say I have these 4 rows in the dataframe:</p>
<pre><code>   TEXT_COLUMN
1 &quot;discovering the hidden themes in the collection.&quot;
2 &quot;classifying the documents into the discovered themes.&quot;
3 &quot;using the classification to organize/summarize/search the documents.&quot;
4 &quot;alternatively, we can set a threshold on the score&quot;
</code></pre>
<p>And, on the other hand, I have a list of words and phrases I want to keep. For example:</p>
<pre><code>x &lt;- c(&quot;hidden themes&quot;, &quot;the documents&quot;, &quot;discovered themes&quot;, &quot;classification to organize&quot;, &quot;search&quot;)
</code></pre>
<p>So, I would like to create a new column &quot;KEYWORDS&quot; with the words in &quot;x&quot; which match the text column separated by a comma:</p>
<pre><code>   TEXT_COLUMN                                                             |  KEYWORDS
1 &quot;discovering the hidden themes in the collection.&quot;                       |  &quot;hidden themes&quot;
2 &quot;classifying the documents into the discovered themes.&quot;                  |  &quot;the documents&quot;, &quot;discovered themes&quot;
3 &quot;using the classification to organize/summarize/search the documents.&quot;   |  &quot;classification to organize&quot;, &quot;search&quot;
4 &quot;alternatively, we can set a threshold on the score&quot;                     |  NA
</code></pre>
<p>Do you know any way to do this?</p>
<p>Thank you very much in advance.</p>
","r, text-mining","<p>An option is to create a pattern from 'x' by joining with <code>str_c</code></p>
<pre><code>library(stringr)
library(dplyr)
pat &lt;- str_c(&quot;\\b(&quot;, str_c(x, collapse=&quot;|&quot;), &quot;)\\b&quot;)
</code></pre>
<p>Then, using this pattern, extract the substring from the 'TEXT_COLUMN' into a <code>list</code> column of <code>vector</code>s</p>
<pre><code>df1 &lt;- df1 %&gt;% 
      mutate(KEYWORDS = str_extract_all(TEXT_COLUMN, pat))
</code></pre>
<p>-output</p>
<pre><code>df1
#TEXT_COLUMN                                          KEYWORDS
#1                     discovering the hidden themes in the collection.                                     hidden themes
#2                classifying the documents into the discovered themes.                  the documents, discovered themes
#3 using the classification to organize/summarize/search the documents. classification to organize, search, the documents
#4                   alternatively, we can set a threshold on the score                                                  
</code></pre>
<h3>data</h3>
<pre><code>df1 &lt;- structure(list(TEXT_COLUMN = c(&quot;discovering the hidden themes in the collection.&quot;, 
&quot;classifying the documents into the discovered themes.&quot;, &quot;using the classification to organize/summarize/search the documents.&quot;, 
&quot;alternatively, we can set a threshold on the score&quot;)), 
class = &quot;data.frame&quot;, row.names = c(&quot;1&quot;, 
&quot;2&quot;, &quot;3&quot;, &quot;4&quot;))
</code></pre>
",0,1,465,2020-12-30 18:38:03,https://stackoverflow.com/questions/65511947/how-to-keep-specific-group-of-words-or-phrases-of-a-text-column-in-r
How to assign an item in a pandas dataframe after checking for conditions?,"<p>I am iterating through a pandas dataframe (originally a csv file) and checking for specific keywords in each row of a certain column. If it appears at least once, I add 1 to a score. There are like 7 keywords, and if the score is &gt;=6, I would like to assign an item of another column (but in this row) with a string (here it is &quot;Software and application developer&quot;) and safe the score. Unfortunately, the score is everywhere the same what I find hard to believe. This is my code so far:</p>
<pre><code>for row in data.iterrows():
devScore=0
if row[1].str.contains(&quot;developer&quot;).any() | row[1].str.contains(&quot;developpeur&quot;).any():
    devScore=devScore+1
if row[1].str.contains(&quot;symfony&quot;).any():
    devScore=devScore+1
if row[1].str.contains(&quot;javascript&quot;).any():
    devScore=devScore+1
if row[1].str.contains(&quot;java&quot;).any() | row[1].str.contains(&quot;jee&quot;).any():
    devScore=devScore+1
if row[1].str.contains(&quot;php&quot;).any():
    devScore=devScore+1
if row[1].str.contains(&quot;html&quot;).any() | row[1].str.contains(&quot;html5&quot;).any():
    devScore=devScore+1
if row[1].str.contains(&quot;application&quot;).any() | row[1].str.contains(&quot;applications&quot;).any():
    devScore=devScore+1
if devScore&gt;=6:
    data[&quot;occupation&quot;]=&quot;Software and application developer&quot;
    data[&quot;score&quot;]=devScore
</code></pre>
","python, pandas, csv, text-mining","<p>You assign a constant onto the whole column here:</p>
<pre class=""lang-py prettyprint-override""><code>data[&quot;occupation&quot;]=&quot;Software and application developer&quot;
data[&quot;score&quot;]=devScore
</code></pre>
<p>They are supposed to be:</p>
<pre class=""lang-py prettyprint-override""><code>for idx, row in data.iterrows():
    # blah blah
    #
    .
    .
    data.loc[idx, &quot;occupation&quot;]=&quot;Software and application developer&quot;
    data.loc[idx, &quot;score&quot;]=devScore
</code></pre>
",1,0,43,2021-01-10 20:25:22,https://stackoverflow.com/questions/65658131/how-to-assign-an-item-in-a-pandas-dataframe-after-checking-for-conditions
lemmatization of german words (Capital letters and lower case letters),"<p>I would like to lemmatize a list of German words, including nouns and verbs. The struggle here is that this implies words beginning with capital letters and others with lower case letters. Until now I worked with a lookup list. Here, the sample</p>
<pre><code>lookup_list &lt;- 
  data.frame(
  cbind(
  c(&quot;mache&quot;,&quot;tust&quot;,&quot;Tuns&quot;,&quot;Reisen&quot;,&quot;genaue&quot;,&quot;genauer&quot;,&quot;pflanze&quot;,&quot;Pflanzen&quot;,&quot;reise&quot;),
                     c(&quot;machen&quot;,&quot;tuen&quot;,&quot;Tun&quot;,&quot;Reise&quot;,&quot;genau&quot;,&quot;genau&quot;,&quot;pflanzen&quot;,&quot;Pflanze&quot;,&quot;reisen&quot;)
)
)
names(lookup_list) &lt;- c(&quot;word&quot;,&quot;lemma&quot;)

Text2Lemmatize &lt;- &quot;mache tust Tuns Reisen genaue genauer pflanze Pflanzen reise&quot;
</code></pre>
<p>The problem is that '''lemmatize()''' ignores the word in the list that begin with capital letters.</p>
<pre><code>lemmatize_strings(Text2Lemmatize, lookup_list)

&gt; lemmatize_strings(Text2Lemmatize, lookup_list)
[1] &quot;machen tuen Tuns Reisen genau genau pflanzen Pflanzen reisen&quot;
</code></pre>
<p>Can anybody help me out with this little problem?</p>
<p>Thanks in advance!</p>
","r, text-mining, capitalization, lemmatization, tolower","<p>If you want lemmatization of German words or text I advice using udpipe.</p>
<pre><code>library(udpipe)

# download german ud model
ud_model &lt;- udpipe_download_model(&quot;german&quot;)
ud_model &lt;- udpipe_load_model(ud_model)

Text2Lemmatize &lt;- &quot;mache tust Tuns Reisen genaue genauer pflanze Pflanzen reise&quot;

x &lt;- udpipe_annotate(ud_model, Text2Lemmatize)
x &lt;- as.data.frame(x)

x[, c(&quot;token&quot;, &quot;lemma&quot;, &quot;upos&quot;)]

     token            lemma upos
1    mache             mach PRON
2     tust            tusen VERB
3     Tuns             Twir PRON
4   Reisen     Reise|Reisen NOUN
5   genaue            genau VERB
6  genauer            genau  ADJ
7  pflanze           pflanz  ADJ
8 Pflanzen Pflanze|Pflanzen NOUN
9    reise           reisen VERB
</code></pre>
<p>Works better when it is actual text, taken from wikipedia:</p>
<blockquote>
<p>&quot;Das Matterhorn ist einer der höchsten Berge der Alpen. Wegen seiner
markanten Gestalt und seiner Besteigungsgeschichte ist das Matterhorn
einer der bekanntesten Berge der Welt. Für die Schweiz ist es ein
Wahrzeichen und eine der meistfotografierten Touristenattraktionen.&quot;</p>
</blockquote>
<pre><code>german_text

x &lt;- udpipe_annotate(ud_model, german_text)
x &lt;- as.data.frame(x)

# show first 10 results
head(x[, c(&quot;token&quot;, &quot;lemma&quot;, &quot;upos&quot;)], 10)
                   token                 lemma  upos
1                    Das                   der   DET
2             Matterhorn            Matterhorn PROPN
3                    ist                  sein   AUX
4                  einer                   ein  PRON
5                    der                   der   DET
6               höchsten                  hoch   ADJ
7                  Berge                  Berg  NOUN
8                    der                   der   DET
9                  Alpen                   Alp  NOUN
10                     .                     . PUNCT
</code></pre>
<p>If it is wordstemming you need, then use quanteda. It works a lot better with non-English languages.</p>
<pre><code>library(quanteda)

my_toks &lt;- tokens(Text2Lemmatize)
my_toks_stemmed &lt;- tokens_wordstem(my_toks, language = &quot;de&quot;)
my_toks_stemmed
Tokens consisting of 1 document.
text1 :
[1] &quot;mach&quot;   &quot;tust&quot;   &quot;Tun&quot;    &quot;Reis&quot;   &quot;genau&quot;  &quot;genau&quot;  &quot;pflanz&quot; &quot;Pflanz&quot; &quot;reis&quot;  
</code></pre>
",2,3,1672,2021-01-11 09:20:35,https://stackoverflow.com/questions/65664123/lemmatization-of-german-words-capital-letters-and-lower-case-letters
GSDMM topic by each row of text,"<p>have been trying to figure out how to get the topic assigned to each row of text in GSDMM.</p>
<p>Attempted to follow (<a href=""https://stackoverflow.com/questions/62108771/a-practical-example-of-gsdmm-in-python"">A practical example of GSDMM in python?</a>) example by Pie-ton but get an error</p>
<p>AttributeError: 'MovieGroupProcess' object has no attribute 'fit'.</p>
<p>Is there a different package to install other than <a href=""https://pypi.org/project/GPyM-TM/"" rel=""nofollow noreferrer"">https://pypi.org/project/GPyM-TM/</a> to have the 'fit' option work?</p>
<p>Have also tried <a href=""https://github.com/rwalk/gsdmm"" rel=""nofollow noreferrer"">https://github.com/rwalk/gsdmm</a></p>
<p>but after running 'from gsdmm import MovieGroupProcess'
get an error of 'No module named 'gsdmm'</p>
","python, text-mining","<p>You have to first checkout the code from the github repo.
Steps are given below:</p>
<pre><code>% mkdir gsdmm
% cd gsdmm 
% git clone https://github.com/rwalk/gsdmm .
% python
&gt;&gt;&gt; from gsdmm import MovieGroupProcess
</code></pre>
",0,1,394,2021-01-11 16:09:29,https://stackoverflow.com/questions/65670457/gsdmm-topic-by-each-row-of-text
Table of n-grams and identifying the row in which the text appeared,"<p>I want to construct a table in which n-grams appear as a column and the row numbers of the dataframe from which they were constructed.</p>
<p>For example, the below code was used to construct n-grams (quadgram in this case):</p>
<pre><code># Libraries
library(quanteda)
library(data.table)
library(tidyverse)
library(stringr)

# Dataframe
Data &lt;- data.frame(Column1 = c(1.222, 3.445, 5.621, 8.501, 9.302), 
                  Column2 = c(654231, 12347, -2365, 90000, 12897), 
                  Column3 = c('A1', 'B2', 'E3', 'C1', 'F5'), 
                  Column4 = c('I bought it', 'The flower has a beautiful fragrance', 'It was bought by me', 'I have bought it', 'The flower smells good'), 
                  Column5 = c('Good', 'Bad', 'Ok', 'Moderate', 'Perfect'))

# Text column of interest
TextColumn &lt;- Data$Column4

# Corpus
Content &lt;-  corpus(TextColumn)

# Tokenization
Tokens &lt;- tokens(Content, what = &quot;word&quot;,
                               remove_punct = TRUE,
                               remove_symbols = TRUE,
                               remove_numbers = FALSE,
                               remove_url = TRUE,
                               remove_separators = TRUE,
                               split_hyphens = FALSE,
                               include_docvars = TRUE,
                               padding = FALSE)

Tokens &lt;- tokens_tolower(Tokens)

# n-grams

quadgrams &lt;- dfm(tokens_ngrams(Tokens, n = 4))
quadgrams_freq &lt;- textstat_frequency(quadgrams)                  # quadgram frequency
quadgrs &lt;- subset(quadgrams_freq,select=c(feature,frequency))
names(quadgrs) &lt;- c(&quot;ngram&quot;,&quot;freq&quot;)
quadgrs &lt;- as.data.table(quadgrs)
</code></pre>
<p>The result is</p>
<p><a href=""https://i.sstatic.net/3NEMw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3NEMw.png"" alt=""enter image description here"" /></a></p>
<p>Is there a way to extract the row number too from which the words were considered from Column4. For example, a column containing 2 (row number) must be there in the above table corresponding to &quot;the_flower_has_a&quot; and again 2 (row number) as an entry for &quot;flower_has_a_beautiful&quot; etc.</p>
","r, nlp, text-mining, n-gram, quanteda","<p>You can specify a group in <code>textstat_frequency()</code> that corresponds to the group name, and this will provide a reference to your original &quot;row number&quot;.</p>

<pre class=""lang-r prettyprint-override""><code>library(&quot;quanteda&quot;)
## Package version: 2.1.2

library(&quot;data.table&quot;)

# Dataframe
Data &lt;- data.frame(
  Column1 = c(1.222, 3.445, 5.621, 8.501, 9.302),
  Column2 = c(654231, 12347, -2365, 90000, 12897),
  Column3 = c(&quot;A1&quot;, &quot;B2&quot;, &quot;E3&quot;, &quot;C1&quot;, &quot;F5&quot;),
  Column4 = c(&quot;I bought it&quot;, &quot;The flower has a beautiful fragrance&quot;, &quot;It was bought by me&quot;, &quot;I have bought it&quot;, &quot;The flower smells good&quot;),
  Column5 = c(&quot;Good&quot;, &quot;Bad&quot;, &quot;Ok&quot;, &quot;Moderate&quot;, &quot;Perfect&quot;)
)

# Corpus
Content &lt;- corpus(Data, text_field = &quot;Column4&quot;)
docnames(Content) &lt;- seq_len(nrow(Data))

# Tokenization and ngrams
Tokens &lt;- tokens(Content,
  what = &quot;word&quot;,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE
) %&gt;%
  tokens_tolower() %&gt;%
  tokens_ngrams(n = 4)
</code></pre>
<p>Now comes the groups part:</p>
<pre class=""lang-r prettyprint-override""><code># form the result
quadgrs &lt;- textstat_frequency(dfm(Tokens), groups = docnames(Tokens)) %&gt;%
  as.data.table()
setnames(quadgrs, &quot;group&quot;, &quot;rownumber&quot;)

quadgrs[, c(&quot;feature&quot;, &quot;frequency&quot;, &quot;rownumber&quot;)]
##                      feature frequency rownumber
## 1:          the_flower_has_a         1         2
## 2:    flower_has_a_beautiful         1         2
## 3: has_a_beautiful_fragrance         1         2
## 4:          it_was_bought_by         1         3
## 5:          was_bought_by_me         1         3
## 6:          i_have_bought_it         1         4
## 7:    the_flower_smells_good         1         5
</code></pre>
<p>Note that:</p>
<ol>
<li>I simplified your code a bit, since some of it was unnecessary or could be streamlined.</li>
<li>The frequency counts are now within row (document), so if you have the same ngram in multiple rows, it will occur more than once in the output table, with the frequency within the row.  If you want to repeat the overall frequency for an ngram that occurs in multiple rows, then this code could be easily modified to reflect that.  (Let me know if you wanted that.)</li>
</ol>
",1,1,242,2021-01-13 10:00:45,https://stackoverflow.com/questions/65699579/table-of-n-grams-and-identifying-the-row-in-which-the-text-appeared
Frequency of strings and their IDs in a dataframe using R,"<p>The goal is to generate the frequency of a text variable and associate the corresponding IDs with it.</p>
<p>Suppose Sample is a dataframe as shown below:</p>
<pre><code>Sample &lt;- data.frame(ID = c('1', '2', '3', '4', '5', '6'), 
                        Var = c('How are you', 
                                 'Do not go', 
                                 'How are you', 
                                 'Please go',  
                                 'How are you',
                                 'Do not go'))
</code></pre>
<p>The following command generates the frequency of the strings in the column Var as follows:</p>
<pre><code>as.data.frame(table(unlist(strsplit(tolower(Sample$Var), ', '))))
</code></pre>
<p><a href=""https://i.sstatic.net/22DIW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/22DIW.png"" alt=""enter image description here"" /></a></p>
<p>Is there a way to generate the associated IDs together in the table, say as?:</p>
<p><a href=""https://i.sstatic.net/1N77h.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1N77h.png"" alt=""enter image description here"" /></a></p>
","r, dataframe, aggregate, frequency, text-mining","<p>Base R solution:</p>
<pre><code>data.frame(do.call(rbind, lapply(with(Sample, split(Sample, Var)), function(x){
      with(x, data.frame(Var = unique(Var), Freq = nrow(x), ID = toString(ID)))
   }
  )
), row.names = NULL, stringsAsFactors = FALSE)
</code></pre>
",1,2,271,2021-01-13 13:50:15,https://stackoverflow.com/questions/65703315/frequency-of-strings-and-their-ids-in-a-dataframe-using-r
Word and Char ngram with different ngram range on TFIDFVectorizer Pipeline,"<p>I'm trying to run a Pipeline with 3 to 5 char n-grams and 1 to 2 word n-grams, with Pipeline and GridSearch, but im getting some errors. Here the full code:</p>
<pre><code>def prediction(dataFrame):
     allText = []
     for index, row in dataFrame.iterrows():
        allText.append(res)

    pipeline = Pipeline([
          ('vect', TfidfVectorizer(min_df=2,analyzer=&quot;char&quot;,sublinear_tf=True,max_df=0.01,ngram_range=(3,5))),
          ('vec', TfidfVectorizer(min_df=2,sublinear_tf=True,analyzer=&quot;word&quot;,max_df=0.01,ngram_range=(1,2))),
          ('clf', LinearSVC()),
     ])
     parameters = [{
          'clf__C': [0.1, 0.5, 1, 1.5, 5]
     }]
     grid_search = GridSearchCV(pipeline, parameters,scoring=&quot;accuracy&quot;,cv=5)
     grid_search.fit(allText,dataFrame.gender)
     print(&quot;Best parameter (CV score=%0.3f):&quot; % grid_search.best_score_)
</code></pre>
<p>I'm getting some erors like &quot;AttributeError: lower not found&quot; on FeatureExtraction, besides all text seems correct.</p>
<p>What is wrong in this case?</p>
","python, scikit-learn, text-mining, tf-idf, grid-search","<p>Using <code>Pipeline</code>, you chain two <code>TfidfVectorizer</code> vectorizers so after the first vectorizer you get <em>numerical</em> features, which are then passed into the second one. But your goal is to <em>concatenate</em> two different <code>TfidfVectorizer</code> feature matrices. Pipelines apply transformers (and a final estimator if given) sequentially, while <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html"" rel=""nofollow noreferrer""><code>FeatureUnion</code></a> runs all the transformers separately and concatenates the results into a single feature space.</p>
<p>Solution:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.svm import LinearSVC

# Replace your pipeline with this:
char_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5))
word_tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2))
tfidf = FeatureUnion([('char', char_tfidf), ('word', word_tfidf)])
pipeline = Pipeline([('tfidf', tfidf), ('clf', LinearSVC())])
</code></pre>
",2,3,956,2021-01-17 21:07:38,https://stackoverflow.com/questions/65765954/word-and-char-ngram-with-different-ngram-range-on-tfidfvectorizer-pipeline
Is there a way to divide a dataset having the same proportion of a categorical value in each sample?,"<p>I'm quite new to R but I have a question. I have a dataset(with a length of 1593 obs) that includes a character type variable that has several strings inside and a factor variable with 2 levels -0 and 1- corresponding to each string. In order to create a classification, I want to divide 75% of this dataset as test and 25% as training samples but I also want to have the same proportion of 0s in both of the test and training samples. Are there any ways to do this?</p>
<p>here is the structure of my dataset</p>
<pre><code>data.frame':    1593 obs. of  6 variables:
 $ match_id: int  0 0 0 0 0 0 0 0 0 0 ...
 $ Binary  : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ...
 $ key     : chr  &quot;force it&quot; &quot;space created&quot; &quot;hah&quot; &quot;ez 500&quot; ...
</code></pre>
<p>Note: I am actually following the codes from the book &quot;Machine Learning with R&quot; by Brett Lantz and applying them to my dataset. The part that I want to achieve in my dataset is this part from the book :</p>
<pre><code>To confirm that the subsets are representative of the complete set of SMS data, let's
compare the proportion of spam in the training and test data frames:
&gt; prop.table(table(sms_raw_train$type))
ham        spam
0.8647158 0.1352842
&gt; prop.table(table(sms_raw_test$type))
ham         spam
0.8683453 0.1316547
Both the training data and test data contain about 13 percent spam. This suggests
that the spam messages were divided evenly between the two datasets.
</code></pre>
<p>thanks for any help</p>
","r, machine-learning, text-mining, sampling","<p>The <code>createDataPartition()</code> function from the <a href=""https://topepo.github.io/caret/"" rel=""nofollow noreferrer"">caret</a> package is typically used for this purpose, e.g.</p>
<pre><code>library(caret)
set.seed(300)
trainIndex &lt;- createDataPartition(iris$Species, p = .75, 
                                  list = FALSE, 
                                  times = 1)
irisTrain &lt;- iris[ trainIndex,]
irisTest  &lt;- iris[-trainIndex,]

str(irisTrain)
&gt;'data.frame':  114 obs. of  5 variables:
&gt; $ Sepal.Length: num  5.1 4.9 4.7 5 5.4 4.6 5 4.4 5.4 4.8 ...
&gt; $ Sepal.Width : num  3.5 3 3.2 3.6 3.9 3.4 3.4 2.9 3.7 3.4 ...
&gt; $ Petal.Length: num  1.4 1.4 1.3 1.4 1.7 1.4 1.5 1.4 1.5 1.6 ...
&gt; $ Petal.Width : num  0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.2 0.2 ...
&gt; $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 &gt;...

str(irisTest)
&gt;'data.frame':  36 obs. of  5 variables:
&gt; $ Sepal.Length: num  4.6 4.9 5.1 5.1 4.6 4.8 5.2 5.5 5.5 5.1 ...
&gt; $ Sepal.Width : num  3.1 3.1 3.5 3.8 3.6 3.1 4.1 4.2 3.5 3.8 ...
&gt; $ Petal.Length: num  1.5 1.5 1.4 1.5 1 1.6 1.5 1.4 1.3 1.9 ...
&gt; $ Petal.Width : num  0.2 0.1 0.3 0.3 0.2 0.2 0.1 0.2 0.2 0.4 ...
&gt; $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 &gt;...

prop.table(table(irisTrain$Species))
&gt;    setosa versicolor  virginica 
&gt; 0.3333333  0.3333333  0.3333333 

prop.table(table(irisTest$Species))
&gt;   setosa versicolor  virginica 
&gt; 0.3333333  0.3333333  0.3333333 
</code></pre>
<p>This provides a pseudorandom ~stratified sampling into train and test cohorts and it's what I use in my own work.</p>
",1,2,1021,2021-01-18 23:49:18,https://stackoverflow.com/questions/65783655/is-there-a-way-to-divide-a-dataset-having-the-same-proportion-of-a-categorical-v
How to split a text / string with pattern of upper case letters?,"<p>I’m looking to Split a text according to each interlocutor.</p>
<p>The original text has this form:</p>
<blockquote>
<p>this is a speech text. FIRST PERSON: hi all, thank you for coming.
SECOND PERSON: thank you for inviting us. TERCER PERSONA QUE SE LLAMA
PEDRO: soy de acuerdo. CUARTA PERSONA (JOHN): Hi. How are you</p>
</blockquote>
<p>I’m searching for a final result like this:</p>
<p>first column: FIRST PERSON  |SECOND PERSON | TERCER PERSONA QUE SE LLAMA PEDRO | CUARTA PERSONA (JOHN)</p>
<p>second column: hi all, thank you for coming | thank you for inviting us
|   soy de acuerdo |    Hi. How are you</p>
<p>The final result can also be in other format or reshaped.</p>
<p>The Pattern to split is one or more Upper Word and a &quot;:&quot;, but one difficulty is that the pattern in capital letters can have optional characters like: ():,;</p>
<p>In fact the original text that I am searching to split is this one:
<a href=""https://lopezobrador.org.mx/2021/01/14/version-estenografica-de-la-conferencia-de-prensa-matutina-del-presidente-andres-manuel-lopez-obrador-458/"" rel=""nofollow noreferrer"">https://lopezobrador.org.mx/2021/01/14/version-estenografica-de-la-conferencia-de-prensa-matutina-del-presidente-andres-manuel-lopez-obrador-458/</a></p>
<p>I have tried different things using stringr rebus and qdap.
First trying this pattern:</p>
<pre><code>pattern_mayusc &lt;- UPPER %R% one_or_more(UPPER) %R% optional(&quot;) &quot;) %R% &quot;:&quot;
</code></pre>
<p>Following I tried to extract a vector with the name of each interlocutor to use them as pattern next:</p>
<pre><code>mayuscula&lt;-sapply(str_extract_all(text, &quot;.([A-Z]+:)&quot;), paste, collapse= ' ')
</code></pre>
<p>I am close to obtain what I desire but cannot achieve it. Anyone to help me?
Thanks a lot in advance</p>
","r, regex, string, split, text-mining","<p>You may use <code>strsplit</code> on a pattern that matches either <code>:</code> preceded by a sequence of words with any upper case letters <code>\p{Lu}</code>, spaces (<code>\s</code>) and parentheses (and more if you need), or (<code>|</code>) the space, followed by the same sequence. We want the first <code>el</code>ement from the resulting list and cleaned with <code>trimws</code>. The result is an alternating pattern of speaker and text, which we can easily convert into a two-column <code>matrix</code> by row.</p>
<pre><code>pat &lt;- r&quot;{(?&gt;\p{Lu}+?\s?)+\(?\p{Lu}+\)?\K(:)|(?&lt;!\w)(\s)(?=\p{Lu}{2,})}&quot;
# pat &lt;- &quot;(?&gt;\\p{Lu}+?\\s?)+\\(?\\p{Lu}+\\)?\\K(:)|(?&lt;!\\w)(\\s)(?=\\p{Lu}{2,})&quot;  ## for R &lt; 4.0.0

tmp &lt;- trimws(el(strsplit(x, pat, perl=TRUE)))[-1]
res &lt;- matrix(tmp, ncol=2, byrow=TRUE)
res
#      [,1]                                 [,2]                           
# [1,] &quot;FIRST PERSON&quot;                       &quot;hi all, thank you for coming.&quot;
# [2,] &quot;SECOND PERSON&quot;                      &quot;thank you for inviting us.&quot;   
# [3,] &quot;TERCER PERSONA QUE SE LLAMA ANDRÉS&quot; &quot;soy de acuerdo.&quot;              
# [4,] &quot;CUARTA PERSONA (JOHN)&quot;              &quot;Hi. How are you?&quot;             
# [5,] &quot;ANDRÉS&quot;                             &quot;Hola buenos días!&quot;   
</code></pre>
<p>See the <a href=""https://regex101.com/r/5O8gh3/3"" rel=""nofollow noreferrer"">regex demo</a></p>
<hr />
<p><em>Data:</em></p>
<pre><code>x &lt;- &quot;this is a speech text. FIRST PERSON: hi all, thank you for coming. SECOND PERSON: thank you for inviting us. TERCER PERSONA QUE SE LLAMA ANDRÉS: soy de: acuerdo. CUARTA PERSONA (JOHN): Hi. How are you? ANDRÉS: Hola buenos días!&quot;
</code></pre>
",0,1,153,2021-01-20 02:26:11,https://stackoverflow.com/questions/65802238/how-to-split-a-text-string-with-pattern-of-upper-case-letters
Is there an R function to clean via a custom dictionary,"<p>I would like to use a custom dictionary (upwards of 400,000 words) when cleaning my data in R. I already have the dictionary loaded as a large character list and I am trying to have it so that the content within my data (VCorpus) compromises of only the words in my dictionary.<br />
For example:</p>
<pre><code>#[1] &quot;never give up uouo cbbuk jeez&quot;  
</code></pre>
<p>would become</p>
<pre><code>#[1*] &quot;never give up&quot;  
</code></pre>
<p>as the words &quot;never&quot;,&quot;give&quot;,and &quot;up&quot; are all in the custom dictionary.
I have previously tried the following:</p>
<pre><code>#Reading the custom dictionary as a function
    english.words  &lt;- function(x) x %in% custom.dictionary
#Filtering based on words in the dictionary
    DF2 &lt;- DF1[(english.words(DF1$Text)),]
</code></pre>
<p>but my result is a character list with one word. Any advice?</p>
","r, text-mining, data-cleaning, sentiment-analysis","<p>You can split the sentences into words, keep only words that are part of your dictionary and paste them in one sentence again.</p>
<pre><code>DF1$Text1 &lt;- sapply(strsplit(DF1$Text, '\\s+'), function(x) 
                    paste0(Filter(english.words, x), collapse = ' '))
</code></pre>
<p>Here I have created a new column called <code>Text1</code> with only english words, if you want to replace the original column you can save the output in  <code>DF1$Text</code>.</p>
",2,0,258,2021-01-25 07:40:24,https://stackoverflow.com/questions/65880680/is-there-an-r-function-to-clean-via-a-custom-dictionary
Regex - Match certain patterns while excluding others?,"<p>I have text data that I want to clean (i.e. keep only alphanumeric characters) with Python. However, most of the text data I encounter contain emoji(s). I want to clean the text from non-alphanumerics, but <strong>still keep the emoji</strong>.</p>
<p>First, I used the <code>emoji</code> library in Python to convert each emoji in a text to a certain string pattern to make it distinguishable. An example of an emoji that has been &quot;demojized&quot; (a literal function in the library) is shown below:</p>
<pre><code>':smiley_face:' # a &quot;demojized&quot; emoji.
</code></pre>
<p>After scrolling through the data, I find that these emojis (once &quot;demojized&quot;) exhibit the same pattern, which in regex terms seems to be</p>
<pre><code>':[a-z_]+:' # regex for matching emojis.
</code></pre>
<p>Ok, so I know the pattern for emojis and I can extract every emoji from the text data I have. The problem is, I want to clean the text data from non-alphanumerics <strong>without altering the emoji pattern</strong> simultaneously. My initial attempt to clean the data:</p>
<pre><code>&gt;&gt;&gt; text = 'Wow.. :smiley_face: this is delicious!' # A string containing emoji
&gt;&gt;&gt; cleaned_text = re.sub('[^a-zA-Z0-9]+',' ',text) # regex to keep only alphanumerics
&gt;&gt;&gt; print(cleaned_text)
Wow smiley face this is delicious
</code></pre>
<p>Clearly this isn't my desired output. I want to keep the emoji text intact, as shown below:</p>
<pre><code>'Wow :smiley_face: this is delicious' # Desired output
</code></pre>
<p>So far I have looked into things like lookahead assertion, but to no avail. Is it possible with regex to remove non-alphanumerics whilst excluding the <code>':[a-z_]+:'</code> pattern from the match? Apologies if question is unclear.</p>
","python, regex, text-mining","<p>If you just want to remove all special chars except the colons and underscores inside  <code>colon-word(s)-colon</code> contexts, you can use</p>
<pre class=""lang-py prettyprint-override""><code>re.sub(r'(:[a-z_]+:)|[^\w\s]|_', r'\1', text)
</code></pre>
<p>See the <a href=""https://regex101.com/r/Ew8Izj/1"" rel=""nofollow noreferrer"">regex demo</a>. <em>Details</em>:</p>
<ul>
<li><code>(:[a-z_]+:)</code>  - Capturing group 1 (<code>\1</code>): <code>:</code>, one or more lowercase ASCII letters or <code>_</code>, and a <code>:</code></li>
<li><code>|</code> - or</li>
<li><code>[^\w\s]|_</code> - any char other than a word and whitespace char or a <code>_</code> (it is a word char, hence it needs to be added as an alternative).</li>
</ul>
<p>See <a href=""https://ideone.com/vQqzKX"" rel=""nofollow noreferrer"">the Python demo</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import re
text = 'Wow.. :smiley_face: this is delicious!' # A string containing emoji
print( re.sub(r'(:[a-z_]+:)|[^\w\s]|_', r'\1', text) )
# =&gt; Wow :smiley_face: this is delicious
</code></pre>
",4,1,132,2021-01-26 18:15:52,https://stackoverflow.com/questions/65906945/regex-match-certain-patterns-while-excluding-others
Quanteda merging unigrams and bigrams,"<p>I want to experiment if having both unigrams and bigrams in one DFM improves my document classification. I would like to create both unigrams and bigrams in one DFM. From there, I can then get my TF-IDF weighted DFM considering both unigrams and bigrams. Possibly, I can possibly create unigram and bigram dfms separately and then I can merge them. But, I would like to know if quanteda has a more efficient way of doing this. I appreciate your responses.</p>
","text-mining, quanteda","<p>Got it from the quanteda page. It works with something like this.</p>
<pre><code>toks_skip &lt;- tokens_ngrams(toks, n = 1:2)
</code></pre>
",0,0,295,2021-02-04 11:14:48,https://stackoverflow.com/questions/66044497/quanteda-merging-unigrams-and-bigrams
LookupError: Resource stopwords not found. Please use the NLTK Downloader to obtain the resource,"<p>I wanted to use nltk library in python.
but when I run the code I have this error:</p>
<pre><code>LookupError: 
**********************************************************************
  Resource stopwords not found.
  Please use the NLTK Downloader to obtain the resource:

  &gt;&gt;&gt; import nltk
  &gt;&gt;&gt; nltk.download('stopwords')
  
  For more information see: https://www.nltk.org/data.html

  Attempted to load corpora/stopwords

  Searched in:
    - 'C:\\Users\\Hossein M/nltk_data'
    - 'C:\\Users\\Hossein M\\AppData\\Local\\Programs\\Python\\Python39\\nltk_data'
    - 'C:\\Users\\Hossein M\\AppData\\Local\\Programs\\Python\\Python39\\share\\nltk_data'
    - 'C:\\Users\\Hossein M\\AppData\\Local\\Programs\\Python\\Python39\\lib\\nltk_data'
    - 'C:\\Users\\Hossein M\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************
</code></pre>
<p>But I did it before by <code>nltk.download()</code> command:</p>
<pre><code>import nltk
nltk.download()
nltk.download('stopwords')

from nltk.corpus import stopwords

set(stopwords.words(&quot;english&quot;))
</code></pre>
","python, text, nlp, nltk, text-mining","<p>I solved this problem by download the corresponding zip file on <a href=""http://nltk.org"" rel=""nofollow noreferrer"">nltk.org</a> then manually setup the <code>C:\nltk_data\corpora</code> dir.</p>
",0,1,6509,2021-02-27 12:36:26,https://stackoverflow.com/questions/66398873/lookuperror-resource-stopwords-not-found-please-use-the-nltk-downloader-to-obt
Finding the dominant topic in each sentence in topic modeling,"<p>One question that I can't find the answer for in R is that how I can find the dominant topic in NLP model for each sentence?
Imagine I have data frame like this:</p>
<pre><code>comment &lt;- c(&quot;outstanding renovation all improvements are topoftheline and done with energy efficiency in mind low monthly utilities even the interior&quot;,
             &quot;solidly constructed lovingly maintained sf crest built&quot;,
             &quot;one year since built new this well designed storey home&quot;,
             &quot;beautiful street large bdm in the heart of lynn valley over sqft bathrooms&quot;,
             &quot;rare to find legal beautiful upgr in port moody centre with a mountain view all bedroom units were nicely renovated&quot;,
             &quot;fantastic opportunity to get value for the money excellent family home in desirable blueridge with legal selfcontained bachelor suite on the main floor great location close to swimming ice skating community&quot;,
             &quot;original owner tired but rock solid perfect location half a block to norquay elementary school and short quiet blocks to slocan park and sky train station&quot;)

id &lt;- c(1,2,3,4,5,6,7)

data &lt;- data.frame(id, comment)
</code></pre>
<p>I do preprocess as shown below:</p>
<pre><code>text_cleaning_tokens &lt;- data %&gt;% 
  tidytext::unnest_tokens(word, comment)
text_cleaning_tokens$word &lt;- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens$word &lt;- gsub('[[:punct:]]+', '', text_cleaning_tokens$word)


text_cleaning_tokens &lt;- text_cleaning_tokens %&gt;% filter(!(nchar(word) == 1))%&gt;% 
  anti_join(stop_words)

stemmed_token &lt;- text_cleaning_tokens %&gt;% mutate(word=wordStem(word))


tokens &lt;- stemmed_token %&gt;% filter(!(word==&quot;&quot;))
tokens &lt;- tokens %&gt;% mutate(ind = row_number())
tokens &lt;- tokens %&gt;% group_by(id) %&gt;% mutate(ind = row_number()) %&gt;%
  tidyr::spread(key = ind, value = word)
tokens [is.na(tokens)] &lt;- &quot;&quot;
tokens &lt;- tidyr::unite(tokens, clean_remark,-id,sep =&quot; &quot; )
tokens$clean_remark &lt;- trimws(tokens$clean_remark)
</code></pre>
<p>The I ran <code>FitLdaModel</code> function on this data and finally, found the best topics based on 2 groups:</p>
<pre><code>             t_1            t_2
1         beauti          built
2          block           home
3          renov          legal
4       bathroom          locat
5            bdm       bachelor
6      bdm_heart  bachelor_suit
7  beauti_street  block_norquai
8    beauti_upgr       blueridg
9        bedroom blueridg_legal
10  bedroom_unit   built_design
</code></pre>
<p>now based on the result I have, I want to find the most dominant topic in each sentence in topic modelling. For example, I want to know that for comment 1 (&quot;outstanding renovation all improvements are topoftheline and done with energy efficiency in mind low monthly utilities even the interior&quot;), which topic (topic 1 or topic 2) is the most dominant?</p>
<p>Can anyone help me with this question? do we have any package that can do this?</p>
","r, nlp, text-mining, topic-modeling","<p>It is pretty easy to work with <strong>quanteda</strong> and <strong>topicmodels</strong>. The former is for data management and quantitative analysis of textual data, the latter is for topic modeling inference.</p>
<p>Here I take your <code>comment</code> object and transform it to a <code>corpus</code> and then to a <code>dfm</code>. I then convert it to be understandable by <strong>topicmodels</strong>.</p>
<p>The function <code>LDA()</code> gives you all you need to easily extract information. In particular, with <code>get_topics()</code> you get the most probable topic for each document. If you instead want to see the document-topic-weights you can do so with <code>ldamodel@gamma</code>. You will see that <code>get_topics()</code> does exactly what you asked.</p>
<p>Please, see if this works for you.</p>
<pre class=""lang-r prettyprint-override""><code>library(quanteda)
#&gt; Package version: 2.1.2
#&gt; Parallel computing: 2 of 16 threads used.
#&gt; See https://quanteda.io for tutorials and examples.
#&gt; 
#&gt; Attaching package: 'quanteda'
#&gt; The following object is masked from 'package:utils':
#&gt; 
#&gt;     View
library(topicmodels)


comment &lt;- c(&quot;outstanding renovation all improvements are topoftheline and done with energy efficiency in mind low monthly utilities even the interior&quot;,
             &quot;solidly constructed lovingly maintained sf crest built&quot;,
             &quot;one year since built new this well designed storey home&quot;,
             &quot;beautiful street large bdm in the heart of lynn valley over sqft bathrooms&quot;,
             &quot;rare to find legal beautiful upgr in port moody centre with a mountain view all bedroom units were nicely renovated&quot;,
             &quot;fantastic opportunity to get value for the money excellent family home in desirable blueridge with legal selfcontained bachelor suite on the main floor great location close to swimming ice skating community&quot;,
             &quot;original owner tired but rock solid perfect location half a block to norquay elementary school and short quiet blocks to slocan park and sky train station&quot;)

mycorp &lt;- corpus(comment)
docvars(mycorp, &quot;id&quot;) &lt;- 1L:7L

mydfm &lt;- dfm(mycorp)

# convert the DFM to a Document Matrix for topicmodels
forTM &lt;- convert(mydfm, to = &quot;topicmodels&quot;)

myLDA &lt;- LDA(forTM, k = 2)

dominant_topics &lt;- get_topics(myLDA)
dominant_topics
#&gt; text1 text2 text3 text4 text5 text6 text7 
#&gt;     2     2     2     2     1     1     1

dtw &lt;- myLDA@gamma
dtw
#&gt;           [,1]      [,2]
#&gt; [1,] 0.4870600 0.5129400
#&gt; [2,] 0.4994974 0.5005026
#&gt; [3,] 0.4980144 0.5019856
#&gt; [4,] 0.4938985 0.5061015
#&gt; [5,] 0.5037667 0.4962333
#&gt; [6,] 0.5000727 0.4999273
#&gt; [7,] 0.5176960 0.4823040
</code></pre>
<p><sup>Created on 2021-03-18 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v1.0.0)</sup></p>
",4,0,1037,2021-03-18 06:22:26,https://stackoverflow.com/questions/66685754/finding-the-dominant-topic-in-each-sentence-in-topic-modeling
Using variable input for str_extract_all in R,"<p>I am pretty green when it comes to R and coding in general. I've been working on a CS project recently for a linguistics course through which I'm finding the words that surround various natural landscape words in <em>The Lord of the Rings</em>. For instance, I'm interested in finding the descriptive words used around words like &quot;stream&quot;, &quot;mountain&quot;, etc.</p>
<p>Anyhow, to extract all of these words from the text, I've been working off of <a href=""https://stackoverflow.com/questions/34403346/extract-a-sample-of-words-around-a-particular-word-using-stringr-in-r"">this</a> post. When running this command by itself, it works:</p>
<p><code>stringr::str_extract_all(text, &quot;([^\\s]+\\s){4}stream(\\s[^\\s]+){6}&quot;)</code></p>
<p>where &quot;stream&quot; is the specific word I'm going after. The numbers before and after specify how many words before and after I want to extract along with it.</p>
<p>However, I'm interested in combining this (and some other things) into a single function, where all you need to plug in the text you want to search, and the word you want to get context for. However, as far as I've tinkered, I can't get anything other than a specific word to work in the above code. Would there be a way to, in the context of writing a function in R, include the above code, but with a variable input, for instance</p>
<p><code>stringr::str_extract_all(text, &quot;([^\\s]+\\s){4}WORD(\\s[^\\s]+){6}&quot;)</code></p>
<p>where WORD is whatever you specify in the overall function:</p>
<p><code>function(text,WORD)</code><br />
I apologize for the generally apparent newb-ness of this post. I am very new to all of this but would greatly appreciate any help you could offer.</p>
","r, text-mining, stringr","<p>This is what you are looking for, if I understood you correctly,</p>
<pre><code>my_fun &lt;- function(input_text, word) {
    
    
    stringr::str_extract_all(
        string = input_text,
        pattern = paste(&quot;([^\\s]+\\s){4}&quot;, word,  &quot;(\\s[^\\s]+){6}&quot;, sep = &quot;&quot;)
    )
    
    
    
}
</code></pre>
<p>May the light of Eärendil ever shine upon you!</p>
",0,0,128,2021-03-21 21:27:16,https://stackoverflow.com/questions/66737734/using-variable-input-for-str-extract-all-in-r
Text Mining Scraped Data (R),"<p>I wrote the code below to look for the word &quot;nationality&quot; in a job postings dataset, where I am essentially trying to see how many employers specify that a given candidate must of a particular visa type or nationality.</p>
<p>I know that in the raw data itself (in excel), there are several cases where the job description where the word &quot;nationality&quot; is mentioned.</p>
<pre><code>nationality_finder = function(string){
 
  nationality = c(&quot; &quot;)
  split_string = strsplit(string, split = NULL)
  split_string = split_string[[1]]
  flag = 0
 
    for(letter in split_string){
      if(flag &gt; 0){nationality = append(nationality, letter)}
      if(letter == &quot;nationality &quot;){flag = 1}
      if(letter == &quot; &quot;){flag = flag-0.5}
    }
  nationality = paste(nationality, collapse = '')
  return(nationality)
}


for(n in 1:length(df2$description)){
  df2$nationality[n] &lt;- nationality_finder(df2$description[n])
}

df2%&gt;%
  view()
</code></pre>
<p>Furthermore, the code is working w/out errors, but it is not producing what I am looking for. I am essentially looking to create another variable where 1 indicates that the word &quot;nationality&quot; is mention, and 0 otherwise. Specifically, I am looking for words such as &quot;citizen&quot; and &quot;nationality&quot; under the job description variable. And the text under each job description is extremely long but here, I just gave a summarized version for brevity.</p>
<p>Text example for a job description in the dataset</p>
<pre><code>Title: Event Planner

Nationality: Saudi National

Location: Riyadh, Saudi Arabia

Salary: Open

Salary depends on the candidates skills, experience, and other attributes.
</code></pre>
<p>Another job description:</p>
<pre><code>- Have recently graduated or looking for a career change and be looking for
an entry level role (we will offer full training)  

- Priority will be taken for applications by U.S. nationality holders 
</code></pre>
","r, web-scraping, text-mining","<p>You can try something like this. I'm assuming you've a <code>data.frame</code> as data, and you want to add a new column.</p>
<pre><code>dats$check &lt;- as.numeric(grepl(&quot;nationality&quot;,dats$description,ignore.case=TRUE))
dats$check
[1] 1 1 0 1
</code></pre>
<p><code>grepl()</code> is going to detect in the column <code>dats$description</code> the string nationality, ignoring case (<code>ignore.case = TRUE</code>) and <code>as.numeric()</code> is going to convert <code>TRUE</code> <code>FALSE</code> into <code>1</code> <code>0</code>.</p>
<p>With fake data:</p>
<pre><code>dats &lt;- structure(list(description = c(&quot;Title: Event Planner\n \n Nationality: Saudi National\n \n Location: Riyadh, Saudi Arabia\n \n Salary: Open\n \n Salary depends on the candidates skills, experience, and other attributes.&quot;, 
&quot;- Have recently graduated or looking for a career change and be looking for\n an entry level role (we will offer full training)  \n \n - Priority will be taken for applications by U.S. nationality holders &quot;, 
&quot;do not have that word here&quot;, &quot;aaaaNationalitybb&quot;), check = c(1, 
1, 0, 1)), row.names = c(NA, -4L), class = &quot;data.frame&quot;)
</code></pre>
",1,0,68,2021-03-22 18:07:14,https://stackoverflow.com/questions/66751581/text-mining-scraped-data-r
how to create a matrix from sub-elements of a list?( in R),"<p>to put it simply, I have a list of DFMs created by quanteda package(LD1). each DFM has different texts of different lengths.</p>
<p>now, I want to calculate and compare lexical diversity for each text within DFMs and among DFMs.</p>
<pre><code>lex.div &lt;-lapply(LD1, function(x) {textstat_lexdiv(x,measure = &quot;all&quot;)})
</code></pre>
<p>this leaves me with a list of S3 type data, and within each of which, there are different attributes that are lexical diversity measures.</p>
<pre><code>lex.div[[1]]$TTR
[1] 0.2940000 0.2285000 0.2110000 0.1912500 0.1802000 0.1671667 0.1531429 0.1483750 0.1392222
[10] 0.1269000

lex.div[[2]]$TTR
[1] 0.3840000 0.2895000 0.2273333 0.2047500 0.1922000 0.1808333 0.1677143 0.1616250 0.1530000
[10] 0.1439000 0.1352727 0.1279167 0.1197692 0.1125000 0.1069333
</code></pre>
<p>here comes the problem. I need all the TTR values in one matrix. i want <code>lex.div[[1]]$TTR</code> to be the first row of the matrix, <code>lex.div[[2]]$TTR</code> to be the second, and so on. note that the length of <code>lex.div[[1]]$TTR</code> ≠ <code>lex.div[[2]]$TTR</code>.</p>
<p>here is what I've done so far:</p>
<pre><code>m1 &lt;-matrix(lex.div[[1]]$TTR, nrow = 1, ncol = length(lex.div[[1]]$TTR))
m.sup &lt;- if(ncol(m1) &lt; 30) {mat.to.add = matrix(NA, nrow = nrow(m1), ncol = 30 - ncol(m1))}
m1 &lt;-cbind(m1, m.sup)

m2 &lt;-matrix(lex.div[[2]]$TTR, nrow = 1, ncol = length(lex.div[[2]]$TTR))
m.sup &lt;- if(ncol(m2) &lt; 30) {mat.to.add = matrix(NA, nrow = nrow(m2), ncol = 30 - ncol(m2))}
m2 &lt;-cbind(m2, m.sup)

m3 &lt;-matrix(lex.div[[3]]$TTR, nrow = 1, ncol = length(lex.div[[3]]$TTR))
m.sup &lt;- if(ncol(m3) &lt; 30) {mat.to.add = matrix(NA, nrow = nrow(m3), ncol = 30 - ncol(m3))}
m3 &lt;-cbind(m3, m.sup)
...

m.total &lt;-rbind (m1,m2,m3...)
</code></pre>
<p>but I cannot do it this way. can you help me write a for loop or sth to get it done easier and  quicker?</p>
","r, list, matrix, text-mining, quanteda","<p>You can try the code below</p>
<pre><code>TTRs &lt;- lapply(lex.div, `[[`, &quot;TTR&quot;)
m &lt;- t(sapply(TTRs, `length&lt;-`, max(lengths(TTRs))))
</code></pre>
",1,0,41,2021-03-23 15:10:41,https://stackoverflow.com/questions/66765941/how-to-create-a-matrix-from-sub-elements-of-a-list-in-r
Generating a dummy variable using grepl(),"<p>I wrote the following and it works w/out errors.</p>
<pre><code>df2$qualifications &lt;- as.numeric(grepl(&quot;high school|Bachelor|master|phd&quot;,df2$description,ignore.case=TRUE))
df2$qualifications
</code></pre>
<p>This is the output, which shows 1 if any of the words above is mentioned and 0 otherwise.</p>
<pre><code>[1] 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0
 [51] 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1
[101] 0 1 0 0
</code></pre>
<p>This is a dataset with job postings along with the education qualifications they are searching for and I am interested in assigning a dummy variable for each educational level mentioned in a job's description.</p>
<p>Specifically, I am looking for something that looks like below, where
0 is where no qualifications is mentioned
1 High school
2 Bachelor
3 masters
4 phd</p>
<pre><code>1] 0 2 4 1 3 1 0 1 0 1 1 1 2 1 0 1 
</code></pre>
","r, tidyverse, text-mining","<p>Using for-loops:</p>
<pre><code>df2 = data.frame(description = sample(educ, 100, TRUE))
df2$qualifications = NA #creating empty column

#placing the possible levels into a vector
educ = c(&quot;high school&quot;, &quot;Bachelor&quot;, &quot;master&quot;, &quot;phd&quot;)

#for each value in educ, if description has that value assign the new column one of the 4 numbers
for(i in educ){
  value = grepl(i, df2$description, ignore.case=TRUE)
  df2$qualifications[which(value)] = (1:4)[educ==i]}
</code></pre>
<p>As you're already creating a categorical variable, i'd recommend using the</p>
",2,0,266,2021-03-23 18:18:25,https://stackoverflow.com/questions/66769001/generating-a-dummy-variable-using-grepl
how to calculate h-point (in R),"<p>I am trying to write a function to calculate h-point. the function is defined over a rank frequency data frame. consider the following <code>data.frame</code> :</p>
<pre><code>DATA &lt;-data.frame(frequency=c(64,58,54,32,29,29,25,17,17,15,12,12,10), rank=c(seq(1, 13)))
</code></pre>
<p>and the formula for h-point is :</p>
<p>if {there is an r = f(r), h-point = r } else { h-point = f(i)j-f(j)i / j-i+f(i)-f(j) } where f(i) and f(j) are corresponding frequencies for ith and jth ranks and i and j are adjacent ranks that i&lt;f(i) and j&gt;f(j).</p>
<p>this is what I`ve done so far:</p>
<pre><code>h_point &lt;- function(data){
  x &lt;- seq(nrow(data))
  f_x &lt;- data[[&quot;frequency&quot;]][x]
  h &lt;- which(x == f_x)
  if(length(h)&gt;1) h
  else{
    i &lt;- which(x &lt; f_x)
    j &lt;- which(x &gt; f_x)
    s &lt;- which(outer(i,j,&quot;-&quot;) == -1, TRUE)
    i &lt;- i[s[,1]]
    j &lt;- j[s[,2]]
    cat(&quot;i: &quot;,i, &quot;j: &quot;, j,&quot;\n&quot;)
    f_x[i]*j - f_x[j]*i / (i-j + f_x[i]-f_x[j])
  }
}
</code></pre>
<p>in <code>DATA</code> , the h-point is <code>12</code>—because x = f_x. HOWEVER,</p>
<pre><code>h_point(DATA)
i:   j:   
numeric(0)
</code></pre>
<p>what am I doing wrong here?</p>
","r, function, if-statement, text-mining","<p>I had a look at your previous post <a href=""https://stackoverflow.com/questions/63460984/how-to-calculate-h-point"">how to calculate h-point</a> but must say that I don't quite follow your method for calculating the h-point.</p>
<p>Based on the definition of the h-point I found</p>
<p><a href=""https://i.sstatic.net/0HizM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0HizM.png"" alt=""enter image description here"" /></a></p>
<p><sub>Reference: <a href=""https://www.researchgate.net/figure/The-definition-of-the-h-point-cf-Popescu-Altmann-2006-25_fig1_281010850"" rel=""nofollow noreferrer"">https://www.researchgate.net/figure/The-definition-of-the-h-point-cf-Popescu-Altmann-2006-25_fig1_281010850</a></sub></p>
<p>I think a simpler approach would be to use <code>approxfun</code> to create a function frequency(rank), and then use <code>uniroot</code> to find the h-point:</p>
<pre class=""lang-r prettyprint-override""><code>get_h_point &lt;- function(DATA) {
    fn_interp &lt;- approxfun(DATA$rank, DATA$frequency)
    fn_root &lt;- function(x) fn_interp(x) - x
    uniroot(fn_root, range(DATA$rank))$root
}

get_h_point(DATA)
#[1] 12
</code></pre>
",1,0,132,2021-03-25 23:01:51,https://stackoverflow.com/questions/66808766/how-to-calculate-h-point-in-r
TextBlob error: too many values to unpack,"<p>I am trying to run the following code, but I have gotten an error that are too many values to unpack</p>
<p>The code is:</p>
<pre><code>import csv
import json
import pandas as pd

df = pd.read_csv(&quot;job/my_data_frame_test.csv&quot;, encoding=&quot;utf-8&quot;)

df.info()
print(df)
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>TEXT</th>
<th>text recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td>ABC</td>
<td>yes</td>
</tr>
<tr>
<td>DEF</td>
<td>no</td>
</tr>
</tbody>
</table>
</div>
<pre><code>from textblob import TextBlob
    
from textblob.classifiers import NaiveBayesClassifier
    
cl = NaiveBayesClassifier(df)
</code></pre>
<p>After running this code, I have the following error (in full)</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-7-3d683b8c482a&gt; in &lt;module&gt;
----&gt; 1 cl = NaiveBayesClassifier(df)

/usr/local/lib/python3.8/dist-packages/textblob/classifiers.py in __init__(self, train_set, feature_extractor, format, **kwargs)
    203     def __init__(self, train_set,
    204                  feature_extractor=basic_extractor, format=None, **kwargs):
--&gt; 205         super(NLTKClassifier, self).__init__(train_set, feature_extractor, format, **kwargs)
    206         self.train_features = [(self.extract_features(d), c) for d, c in self.train_set]
    207 

/usr/local/lib/python3.8/dist-packages/textblob/classifiers.py in __init__(self, train_set, feature_extractor, format, **kwargs)
    137         else:  # train_set is a list of tuples
    138             self.train_set = train_set
--&gt; 139         self._word_set = _get_words_from_dataset(self.train_set)  # Keep a hidden set of unique words.
    140         self.train_features = None
    141 

/usr/local/lib/python3.8/dist-packages/textblob/classifiers.py in _get_words_from_dataset(dataset)
     61             return words
     62     all_words = chain.from_iterable(tokenize(words) for words, _ in dataset)
---&gt; 63     return set(all_words)
     64 
     65 def _get_document_tokens(document):

/usr/local/lib/python3.8/dist-packages/textblob/classifiers.py in &lt;genexpr&gt;(.0)
     60         else:
     61             return words
---&gt; 62     all_words = chain.from_iterable(tokenize(words) for words, _ in dataset)
     63     return set(all_words)
     64 

ValueError: too many values to unpack (expected 2)
</code></pre>
","python, pandas, dataframe, text-mining, textblob","<p><code>NaiveBayesClassifier()</code> <a href=""https://textblob.readthedocs.io/en/dev/classifiers.html"" rel=""nofollow noreferrer"">expects a list of tuples</a> of the form  <code>(text, label)</code>:</p>
<pre class=""lang-py prettyprint-override""><code>train = list(zip(df['TEXT'], df['text recommended']))
# [('ABC', 'yes'), ('DEF', 'no')]
</code></pre>
<pre class=""lang-py prettyprint-override""><code>cl = NaiveBayesClassifier(train)
# &lt;NaiveBayesClassifier trained on 2 instances&gt;
</code></pre>
",1,1,211,2021-03-28 21:05:44,https://stackoverflow.com/questions/66846209/textblob-error-too-many-values-to-unpack
Scale cosine distance to 0-1 using Gensim,"<p>I've built a Doc2Vec model with around 3M documents, now I want to compare it to another model I've previously built. The second model has been scaled to 0-1 so I now also want to scale the gensim model to the same range so that they are comparable.
This is my first time using gensim so I'm not sure how this is done. It's nothing fancy but this is the code I have so far (model generation code ommited). I thought about scaling (minmax scaling with max/min in the union of  vectors) the inferred vectors (v1 and v2) but I don't think this would be correct approach.
The idea here is to compare two documents (with tokens likely to be in the corpus) and output a similarity score between them. I've seen a few Gensim's tutorials and they often compare a single string to the corpus' documents, which is not really the idea here.</p>
<blockquote>
<pre><code> def get_similarity_score(self,string_1, string_2):
    split_tokens1 = string_1.split()
    split_tokens2 = string_2.split()
    v1 = self.model.infer_vector(split_tokens1)
    v2 = self.model.infer_vector(split_tokens2)
    text_score = nltk.cluster.util.cosine_distance(v1, v2)
    return text_score
</code></pre>
</blockquote>
<p>Any recommendations?</p>
","python, math, nlp, text-mining, gensim","<p>Note that 'cosine similarity' &amp; 'cosine distance' are different things.</p>
<p>A cosine-similarity can range from <code>-1.0</code> to <code>1.0</code> – but in some models, such as those based only on positive word counts, you might only practically see values from <code>0.0</code> to <code>1.0</code>. But in both cases, items with similarities close to <code>1.0</code> are <em>most-similar</em>.</p>
<p>On the other hand, a cosine-distance can range from <code>0.0</code> to <code>2.0</code>, and items with a distance of <code>0.0</code> are <em>least-distant</em> (or <em>nearest</em>). A cosine-distance can be larger than <code>1.0</code> - but you might only see such distances in models which use the dense coordinate space (like <code>Doc2Vec</code>), not in word-count models which leave half the coordinate space empty (all negative coordinates).</p>
<p>So: you shouldn't really be calling your function <code>similarity</code> if it's returning a distance, and if it's now returning surprise numbers over <code>1.0</code>, there's nothing wrong: that's possible in some models, but not others.</p>
<p>You could naively rescale the <code>0.0</code> to <code>2.0</code> distances that your calculation will get with <code>Doc2Vec</code> vectors, with some crude hammer like:</p>
<pre><code>new_distance = old_distance / 2
</code></pre>
<p>However, note that in general, the <em>absolute</em> similarities from different models are still not necessarily meaningfully comparable. This is even true between two diferent <code>Doc2Vec</code> models. Their magnitudes are highly influenced by things like the model-metaparameters.</p>
<p>For example, if you used the exact same sufficiently-large set of texts to train a 100-dimensional <code>Doc2Vec</code> model, and a 300-dimensional <code>Doc2Vec</code> model, both models might wind up very similarly-useful. And for a doc A, its nearest-neighbor might consistently be doc B. Indeed, its top-10 neighbors might be very similar or identical.</p>
<p>But the cosine-similarities may have far different maxes/ranges, like the same neighbor B having the similarity <code>0.9</code> in one but <code>0.6</code> in another. Thy're the same docs, and correctly identified as 'most-similar', and neither <code>0.9</code> or <code>0.6</code> is truly a worse number to report, because in both cases the proper most-similar doc is at the top of the rankings. The models have just wound up using the available coordinate space differently. So, you shouldn't compare that <code>0.6</code> or <code>0.9</code> similarity (or in your case other distance numbers) against some other model – <em>especially</em> if the models use different algorithms, as seems to be the case for you. (If looks like you may be comparing absolute cosine-distances from a word-counting model against a dense learned <code>Doc2Vec</code> model.)</p>
<p>It <em>may</em> make more sense to compare result-<em>rankings</em> between the models. That is, ignore the raw similarity numbers, but care whether desirable documents appear in the top-N for other documents.  Alternatively, it might be possible to learn some scaling-rule for making the distances more comparable, but it's hard to make a more specific recommendation, or even know if that's a good step to take, without knowing your ultimate goal in comparing the models.</p>
",2,1,1892,2021-03-30 08:33:10,https://stackoverflow.com/questions/66867375/scale-cosine-distance-to-0-1-using-gensim
Text data clustering with python,"<p>I am currently trying to cluster a list of sequences based on their similarity using python.</p>
<p>ex:</p>
<blockquote>
<p>DFKLKSLFD</p>
<p>DLFKFKDLD</p>
<p>LDPELDKSL<br />
...</p>
</blockquote>
<p>The way I pre process my data is by computing the pairwise distances using for example the <a href=""https://pypi.org/project/textdistance/"" rel=""nofollow noreferrer"">Levenshtein distance</a>. After calculating all the pairwise distances and creating the distance matrix, I want to use it as input for the clustering algorithm.</p>
<p>I have already tried using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html"" rel=""nofollow noreferrer"">Affinity Propagation</a>, but convergence is a bit unpredictable and I would like to go around this problem.</p>
<p>Does anyone have any suggestions regarding other suitable clustering algorithms for this case?</p>
<p>Thank you!!</p>
","python, scikit-learn, cluster-analysis, text-mining","<p><a href=""https://scikit-learn.org/stable/faq.html#how-do-i-deal-with-string-data-or-trees-graphs"" rel=""nofollow noreferrer"">sklearn</a> actually does show this example using DBSCAN, just like <a href=""https://stackoverflow.com/questions/38720283/python-string-clustering-with-scikit-learns-dbscan-using-levenshtein-distance"">Luke</a> once answered here.</p>
<p>This is based on that example, using <code>!pip install python-Levenshtein</code>.
But if you have pre-calculated all distances, you could change the custom metric, as shown below.</p>
<pre><code>from Levenshtein import distance

import numpy as np
from sklearn.cluster import dbscan

data = [&quot;DFKLKSLFD&quot;, &quot;DLFKFKDLD&quot;, &quot;LDPELDKSL&quot;]

def z:
    i, j = int(x[0]), int(y[0])     # extract indices
    return distance(data[i], data[j])

X = np.arange(len(data)).reshape(-1, 1)

dbscan(X, metric=lev_metric, eps=5, min_samples=2)
</code></pre>
<p>And if you pre-calculated you could define <code>pre_lev_metric(x, y)</code> along the lines of</p>
<pre><code>def pre_lev_metric(x, y):
    i, j = int(x[0]), int(y[0])     # extract indices
    return DISTANCES[i,j]
</code></pre>
<hr />
<p>Alternative answer based on <a href=""https://en.wikipedia.org/wiki/K-medoids"" rel=""nofollow noreferrer"">K-Medoids</a> using <a href=""https://scikit-learn-extra.readthedocs.io/en/latest/generated/sklearn_extra.cluster.KMedoids.html"" rel=""nofollow noreferrer"">sklearn_extra.cluster.KMedoids</a>. K-Medoids is not yet that well known, but only needs distance as well.</p>
<p>I had to install like this</p>
<pre><code>!pip uninstall -y enum34
!pip install scikit-learn-extra
</code></pre>
<p>Than I was able to create clusters with;</p>
<pre><code>from sklearn_extra.cluster import KMedoids
import numpy as np
</code></pre>
<pre><code>from Levenshtein import distance

data = [&quot;DFKLKSLFD&quot;, &quot;DLFKFKDLD&quot;, &quot;LDPELDKSL&quot;]

def lev_metric(x, y):
    i, j = int(x[0]), int(y[0])     # extract indices
    return distance(data[i], data[j])

X = np.arange(len(data)).reshape(-1, 1)
</code></pre>
<pre><code>kmedoids = KMedoids(n_clusters=2, random_state=0, metric=lev_metric).fit(X)
</code></pre>
<p>The labels/centers are in</p>
<pre><code>kmedoids.labels_
kmedoids.cluster_centers_
</code></pre>
",1,1,2239,2021-03-31 08:34:35,https://stackoverflow.com/questions/66884270/text-data-clustering-with-python
Python extract paragraph in text file using regex,"<p>I am using Python 3.7 and I am trying to extract some paragraph from some text files using regex.</p>
<p>Here is a sample of the txt file content.</p>
<pre><code>AREA: OMBEYI MARKET, ST. RITA RAMULA

DATE: Thursday 25.03.2021, TIME: 9.00 A.M. = 5.00 P.M.

Ombeyi Mk, Kiliti Mkt, Masogo Mkt, Miwani, Kasongo, Onyango Midika, St. Rita Ramula, Onyalo
Biro, Yawo Pri, Obino, Rutek, Keyo Pri &amp; adjacent customers.

AREA: NYAMACHE FACTORY

DATE: Thursday 25.03.2021, TIME: 830 A.M. - 3.00 P.M.

Nyamache Fact, Suguta, Gionseri, Igare, Kionduso, Nyationgongo, Enchoro, Kebuko, Emenwa, Maji
Mazuri, Borangi &amp; adjacent customers.

AREA: SUNEKA MARKET, RIANA MARKET

DATE: Thursday 25.03.2021, TIME: 8.00 A.M. - 3.00 P.M.

Suneka Mk, Riana Mk, Kiabusura, Gesonso, Chisaro, Sugunana, Nyamira Ndogo &amp; adjacent
customers.

AREA: ITIATI, GITUNDUTI

DATE: Thursday 25.03.2021, TIME: 9.00 A.M. = 2.00 P.M.

General China, Gachuiro, Gathuini Pri, Itiati Campus, Kianjugum, Gikore, Kihuri TBC, Gitunduti &amp;
adjacent customers.

</code></pre>
<p>Currently I am able to extract the Area, Date and Time using regex:</p>
<pre><code>area_pattern = re.compile(&quot;^AREA:((.*))&quot;)
date_pattern = re.compile(&quot;^DATE:(.*),&quot;)
time_pattern = re.compile(&quot;TIME:(.*).&quot;)
</code></pre>
<p>I would like to be able to extract the paragraph after <strong>DATE/TIME</strong> and before <strong>AREA</strong> containing locations separated by commas. So I will be able to match the following:</p>
<pre><code>1.
Ombeyi Mk, Kiliti Mkt, Masogo Mkt, Miwani, Kasongo, Onyango Midika, St. Rita Ramula, Onyalo
Biro, Yawo Pri, Obino, Rutek, Keyo Pri &amp; adjacent customers.

2.
Nyamache Fact, Suguta, Gionseri, Igare, Kionduso, Nyationgongo, Enchoro, Kebuko, Emenwa, Maji
Mazuri, Borangi &amp; adjacent customers.

3.
Suneka Mk, Riana Mk, Kiabusura, Gesonso, Chisaro, Sugunana, Nyamira Ndogo &amp; adjacent
customers.

4.
General China, Gachuiro, Gathuini Pri, Itiati Campus, Kianjugum, Gikore, Kihuri TBC, Gitunduti &amp;
adjacent customers.

</code></pre>
<p>If anyone could help with suggesting a regex that would help with this use case, as well as improvements to my current regex, I would really appreciate it. Thanks</p>
","python, regex, text-mining, text-extraction, string-parsing","<p>You may use this regex with a capture group to be used in <code>re.findall</code>:</p>
<pre class=""lang-sh prettyprint-override""><code>\nDATE:.*\n*((?:\n.*)+?)(?=\nAREA:|\Z)
</code></pre>
<p><a href=""https://regex101.com/r/bjkUMd/1"" rel=""nofollow noreferrer"">RegEx Demo</a></p>
<p><strong>RegEx Details:</strong></p>
<ul>
<li><code>\nDATE:</code>: Match text <code>DATE:</code> after matching a line break</li>
<li><code>.*\n*</code>: Match rest of the line followed by 0 or more line breaks</li>
<li><code>((?:\n.*)+?)</code>: Capture group 1 to capture our text which 1 or lines of everything until next condition is satisfied</li>
<li><code>(?=\nAREA:|\Z)</code>: Assert that we have a line break followed by <code>AREA:</code> or end of input right ahead of the current position</li>
</ul>
",3,1,772,2021-03-31 13:32:18,https://stackoverflow.com/questions/66888902/python-extract-paragraph-in-text-file-using-regex
Difficulty converting a list: &#39;str&#39; object has no attribute &#39;items&#39;,"<p>I am trying to create a classifier using NLTK, however, I believe that I have a problem in the format of my data that I cannot get over.</p>
<p>My data looks like this:</p>
<pre><code>data = [(&quot;TEXT 1&quot;, 'no'), (&quot;TEXT 2&quot;, 'yes'), (&quot;TEXT 3&quot;, 'no'), (&quot;TEXT 4&quot;, 'no'), (&quot;TEXT 5&quot;, 'yes')]
</code></pre>
<p>Then, I run the following code:</p>
<pre><code> import nltk
    from nltk.classify import maxent
    
    classifier = maxent.MaxentClassifier.train(data, bernoulli=False, max_iter=10)
</code></pre>
<p>But, unfortunately I have the following error. What does this error consist of and how do I overcome it?</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-93-e1b29adbeebb&gt; in &lt;module&gt;
----&gt; 1 classifier = maxent.MaxentClassifier.train(data, bernoulli=False, max_iter=10)

/usr/local/lib/python3.8/dist-packages/nltk/classify/maxent.py in train(cls, train_toks, algorithm, trace, encoding, labels, gaussian_prior_sigma, **cutoffs)
    324         algorithm = algorithm.lower()
    325         if algorithm == &quot;iis&quot;:
--&gt; 326             return train_maxent_classifier_with_iis(
    327                 train_toks, trace, encoding, labels, **cutoffs
    328             )

/usr/local/lib/python3.8/dist-packages/nltk/classify/maxent.py in train_maxent_classifier_with_iis(train_toks, trace, encoding, labels, **cutoffs)
   1175     # Construct an encoding from the training data.
   1176     if encoding is None:
-&gt; 1177         encoding = BinaryMaxentFeatureEncoding.train(train_toks, labels=labels)
   1178 
   1179     # Count how many times each feature occurs in the training data.

/usr/local/lib/python3.8/dist-packages/nltk/classify/maxent.py in train(cls, train_toks, count_cutoff, labels, **options)
    665 
    666             # Record each of the features.
--&gt; 667             for (fname, fval) in tok.items():
    668 
    669                 # If a count cutoff is given, then only add a joint

AttributeError: 'str' object has no attribute 'items'
</code></pre>
","python, nlp, nltk, text-mining","<p>From the documentation:</p>
<blockquote>
<p>train(train_toks, algorithm=None, trace=3, encoding=None, labels=None, gaussian_prior_sigma=0, **cutoffs)</p>
</blockquote>
<p><a href=""https://www.nltk.org/api/nltk.classify.html#nltk.classify.maxent.MaxentClassifier.train"" rel=""nofollow noreferrer"">Train Docs</a></p>
<blockquote>
<p>Parameters train_toks (list) – Training data, represented as a list of pairs, the first member of which is a <a href=""http://www.nltk.org/api/nltk.classify.html#featuresets"" rel=""nofollow noreferrer"">featureset</a>, and the second of which is a classification label.</p>
</blockquote>
<p>Your tuples need to have the first element be a <code>dict</code> that &quot;map[s] strings to either numbers, booleans or strings&quot; then you need to have your second element be the classification label.</p>
<pre class=""lang-py prettyprint-override""><code>from nltk.classify import maxent

data = [({&quot;TEXT 1&quot;: 'no'}, &quot;Label&quot;)]
classifier = maxent.MaxentClassifier.train(data, bernoulli=False, max_iter=10)
</code></pre>
",1,2,515,2021-04-07 22:09:54,https://stackoverflow.com/questions/66994785/difficulty-converting-a-list-str-object-has-no-attribute-items
R: Converting Tibbles to a Term Document Matrix,"<p>I am using the R programming language. I learned how to take pdf files from the internet and load them into R. For example, below I load 3 different books by Shakespeare into R:</p>
<pre><code>library(pdftools)
library(tidytext)
library(textrank)
library(tm)

#1st document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/hamlet_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_1 &lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)

#2nd document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/macbeth_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_2&lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)


#3rd document
url &lt;- &quot;https://shakespeare.folger.edu/downloads/pdf/othello_PDF_FolgerShakespeare.pdf&quot;

article &lt;- pdf_text(url)
article_sentences &lt;- tibble(text = article) %&gt;%
  unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;%
  mutate(sentence_id = row_number()) %&gt;%
  select(sentence_id, sentence)


article_words &lt;- article_sentences %&gt;%
  unnest_tokens(word, sentence)


article_words_3 &lt;- article_words %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)
</code></pre>
<p>Each one of these files (e.g. article_words_1) is now a &quot;tibble&quot; file. From here, I want to convert these into a &quot;document term matrix&quot; so that I can perform text mining and NLP on these :</p>
<pre><code>#convert to document term matrix
myCorpus &lt;- Corpus(VectorSource(article_words_1, article_words_2, article_words_3))
tdm &lt;- TermDocumentMatrix(myCorpus)
inspect(tdm)
</code></pre>
<p>But this seems to result in an error:</p>
<pre><code>Error in VectorSource(article_words_1, article_words_2, article_words_3) : 
  unused arguments (article_words_2, article_words_3)
</code></pre>
<p>Can someone please show me what I am doing wrong?</p>
<p>Thanks</p>
","r, text, nlp, text-mining, term-document-matrix","<p>As the error message suggests, <code>VectorSource</code> only takes 1 argument. You can <code>rbind</code> the datasets together and pass it to <code>VectorSource</code> function.</p>
<pre><code>library(tm)

tdm &lt;- TermDocumentMatrix(Corpus(VectorSource(rbind(article_words_1, article_words_2, article_words_3))))
inspect(tdm)

#&lt;&lt;TermDocumentMatrix (terms: 14952, documents: 2)&gt;&gt;
#Non-/sparse entries: 14952/14952
#Sparsity           : 50%
#Maximal term length: 25
#Weighting          : term frequency (tf)
#Sample             :
#            Docs
#Terms        1     2
#  &quot;act&quot;,     0   397
#  &quot;cassio&quot;,  0   258
#  &quot;ftln&quot;,    0 10303
#  &quot;hamlet&quot;,  0   617
#  &quot;iago&quot;,    0   371
#  &quot;lord&quot;,    0   355
#  &quot;macbeth&quot;, 0   386
#  &quot;othello&quot;, 0   462
#  &quot;sc&quot;,      0   337
#  &quot;thou&quot;,    0   346
</code></pre>
",2,0,262,2021-04-09 06:21:53,https://stackoverflow.com/questions/67016046/r-converting-tibbles-to-a-term-document-matrix
undo the tokenization in python,"<p>I would like to reverse the tokenization that I have applied to my data.</p>
<pre><code>data = [['this', 'is', 'a', 'sentence'], ['this', 'is', 'a', 'sentence', '2']]
</code></pre>
<p>Expected output:</p>
<pre><code>['this is a sentence', 'this is a sentence 2']
</code></pre>
<p>I tried to do this with the following code block:</p>
<pre><code>from nltk.tokenize.treebank import TreebankWordDetokenizer
data_untoken= []
for i, text in enumerate(data):
    data_untoken.append(text)
    data_untoken = TreebankWordDetokenizer().detokenize(text)
</code></pre>
<p>But I have the following error</p>
<pre><code>'str' object has no attribute 'append'
</code></pre>
","python, text-mining","<p>Use <code>join()</code>:</p>
<pre><code>def untokenize(data):
    for tokens in data:
        yield ' '.join(tokens)


data = [['this', 'is', 'a', 'sentence'], ['this', 'is', 'a', 'sentence', '2']]
untokenized_data = list(untokenize(data))
</code></pre>
",4,0,675,2021-04-13 21:23:02,https://stackoverflow.com/questions/67082571/undo-the-tokenization-in-python
turning json type column into R dataframe,"<p>Here I have a dataframe <code>df1</code> that I would like to turn into a dataframe <code>df2</code>. Does anybody have any suggestions/ideas?</p>
<pre><code>df1 &lt;- data.frame (ID  = c(&quot;UniqueValue1&quot;, &quot;UniqueValue2&quot;, &quot;UniqueValue3&quot;,
                          &quot;UniqueValue4&quot;, &quot;UniqueValue5&quot;, &quot;UniqueValue6&quot;),
                  stringtext = c(&quot;Factor1:1.0, Factor2:2.0, Factor3:3.0&quot;))
</code></pre>
<p>into...</p>
<pre><code>df2 &lt;- data.frame (ID  = c(&quot;UniqueValue1&quot;, &quot;UniqueValue2&quot;, &quot;UniqueValue3&quot;,
                          &quot;UniqueValue4&quot;, &quot;UniqueValue5&quot;, &quot;UniqueValue6&quot;),
                  Factor1 = c(&quot;1.0&quot;, &quot;1.0&quot;, &quot;1.0&quot;, &quot;1.0&quot;, &quot;1.0&quot;, &quot;1.0&quot;),
                  Factor2 = c(&quot;2.0&quot;, &quot;2.0&quot;, &quot;2.0&quot;, &quot;2.0&quot;, &quot;2.0&quot;, &quot;2.0&quot;),
                  Factor3 = c(&quot;3.0&quot;, &quot;3.0&quot;, &quot;3.0&quot;, &quot;3.0&quot;, &quot;3.0&quot;, &quot;3.0&quot;))
</code></pre>
<p>Is there a way where you don't have to manually specify 'Factor1', 'Factor2', and 'Factor3'? Like if whatever is behind the colon, turn that into a column, and what ever is AFTER the colon, turn that into its specific value. If we can get that, that would be awesome. Any help or feedback regarding this would be highly appreciated! Thanks!</p>
","r, json, dataframe, text-mining","<p>You can try something like this, it may not be very elegant but gets the outcome you want.</p>
<pre><code>column_names &lt;- as.data.frame(unlist(str_split(df1$stringtext,&quot;,&quot;))) %&gt;%
setNames(c(&quot;factor&quot;)) %&gt;%
distinct() %&gt;%
mutate(factor = trimws(factor)) %&gt;%
mutate(factor = gsub(&quot;:.+$&quot;,&quot;&quot;,factor))

df1 &lt;- df1 %&gt;%
tidyr::separate(stringtext, column_names$factor ,sep=&quot;,&quot;)

df &lt;- as.data.frame((sapply(df1,function(x) gsub(&quot;^F.+:&quot;,&quot;&quot;,trimws(x)))))
</code></pre>
",0,0,96,2021-04-14 13:44:31,https://stackoverflow.com/questions/67093011/turning-json-type-column-into-r-dataframe
text mining preprocessing must be applied to test or to train set?,"<p>I'm doing some text-mining tasks and I have such a simple question and I still can't reach a conclusion.</p>
<p>I am applying pre-processing, such as tokenization and stemming to my training set so i can train my model.</p>
<p>Should I also apply this pre-processing to my test set?</p>
","python, nlp, text-mining, sentiment-analysis","<p>Yes, you should apply same things to your test set. Because you test set must represent your train set, that's why they should be from same distribution. Let's think intuitively:</p>
<p>You will enter an exam. In order you to prepare for exam and get a normal result, lecturer should ask from same subjects in the lectures. But if the lecturer ask questions from a totally different subjects that no one has seen, it is not possible to get a normal result.</p>
",1,0,245,2021-04-17 20:34:59,https://stackoverflow.com/questions/67142717/text-mining-preprocessing-must-be-applied-to-test-or-to-train-set
Entities extraction based on customized list in R,"<p>I have list of texts and I also have a list of entities.</p>
<p>The list of texts is typically in vectorized string.</p>
<p>The list of entities is a bit more complexed.
Some entities, can be listed out exhaustively such as the list of main cities of the world.
Some entities, while impossible to be listed out exhaustively, can be captured by regex pattern.</p>
<pre><code>
list_of_text &lt;- c('Lorem ipsum 12-01-2021 eat, Copenhagen 133.001.00.00 ...', 'Lorem ipsum 12-01-2021, Copenhagen www.stackoverflow.com swimming', ...)

entity_city &lt;- c('Copenhagen', 'Paris', 'New York', ...)

entity_IP_address &lt;- c('regex code for IP address')

entity_IP_address &lt;- c('regex code for URL')

entity_verb &lt;- c('verbs')

</code></pre>
<p>Given the <code>list_of_text</code> and the list of <code>entities</code>, I want to find matching entities for each text.</p>
<p>For example <code>c('Lorem ipsum 12-01-2021 eat drink sleep, Copenhagen 133.001.00.00 ...')</code>, it has <code>c(eat, drink, sleep)</code> for <code>entity_verb</code>, <code>c(133.001.00.00)</code> for <code>entity_IP</code>, etc.</p>
<pre><code>
res &lt;- extract_entity(text = c('Lorem ipsum 12-01-2021 eat drink sleep, Copenhagen 133.001.00.00 ...')
                      ,entities &lt;- c(entity_verb, entity_IP_address, entity_city))

res[['verb']]
c('eat', 'drink', 'sleep')

res[['IP']]
c('133.001.00.00')

res[['city']]
c('Copenhagen')

</code></pre>
<p>Is there a <code>R package</code> I can leverage on?</p>
","r, nlp, text-mining, r-package, named-entity-recognition","<p>Please take a look at maps and qdapDictionaries.  For world cities, I subset for cities with greater than a population of 1M.  Otherwise, it error with 'regular expression is too large'.</p>
<pre><code>library(maps)
library(qdapDictionaries)

list_of_text  &lt;- c('Lorem ipsum 12-01-2021 eat, Copenhagen 192.41.196.888','192.41.199.888','Lorem ipsum 12-01-2021, Copenhagen www.stackoverflow.com swimming')
#regex needs adjusted. Not extracting the first IP Address
ipRegex   &lt;- &quot;(?(?=.*?(\\d+\\.\\d+\\.\\d+\\.\\d+).*?)(\\1|))&quot;

regmatches(x = list_of_text , m = regexpr(ipRegex ,list_of_text ,perl = TRUE))[
  regmatches(x = list_of_text , m = regexpr(ipRegex ,list_of_text ,perl = TRUE)) != '']

verbRegex &lt;- substr(paste0((unlist(action.verbs)),'|',collapse = &quot;&quot;),
                     start = 1,nchar(paste0((unlist(action.verbs)),'|',collapse = &quot;&quot;))-1)

unlist(regmatches(x = list_of_text , m = gregexpr(verbRegex,list_of_text ,perl = TRUE))[
  regmatches(x = list_of_text , m = gregexpr(verbRegex,list_of_text ,perl = TRUE)) != ''])

citiesRegex &lt;- substr(paste0((unlist(world.cities[world.cities$pop &gt;1000000,'name'])),'|',collapse = &quot;&quot;),
                    start = 1,nchar(paste0((unlist(world.cities[world.cities$pop &gt;1000000,'name'])),'|',collapse = &quot;&quot;))-1)

unlist(regmatches(x = list_of_text , m = gregexpr(citiesRegex,list_of_text ,perl = TRUE))[
  regmatches(x = list_of_text , m = gregexpr(citiesRegex,list_of_text ,perl = TRUE)) != ''])
</code></pre>
",1,0,118,2021-04-22 07:55:42,https://stackoverflow.com/questions/67209062/entities-extraction-based-on-customized-list-in-r
Using Anti Join in R,"<p>I am a noob in R, and I been trying to compare two data frames which is derived using Text mining and it has two columns, one with words and other with count.
Assume they are dataframe1 and dataframe2.</p>
<p>I am trying to find out how to write the code which will select those words are present in dataframe2 but not present in dataframe1.</p>
<p>If we had to use it in excel, we would just use word as reference in dataframe2 and VLOOKUP the same list of words from dataframe1 and select the #N/A which are there and then sort the #N/A based on the highest count.</p>
<p>Below is the picture to explain in detail:
dataframe1</p>
<p><a href=""https://i.sstatic.net/vG9V6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vG9V6.png"" alt=""enter image description here"" /></a></p>
<p>dataframe2:</p>
<p><a href=""https://i.sstatic.net/o6twy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/o6twy.png"" alt=""enter image description here"" /></a></p>
<p>As you can see the word C &amp; F are in dataframe1 and also in dataframe2. So we have to exclude this and it should look like this.</p>
<p>Expected Output:</p>
<p><a href=""https://i.sstatic.net/MMAmk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MMAmk.png"" alt=""enter image description here"" /></a></p>
<p>Can someone help me? I been trying for hours now. Thanks in advance.</p>
",text-mining,"<p>There's a dplyr function to do this called <code>anti_join</code>:</p>
<pre><code>library(dplyr)
anti_join(df1, df2, by = c('Check'))
</code></pre>
<p>To sort it in descending order of Count (thanks to Ben Bolker for pointing out that part of the question) you can use <code>arrange</code>.</p>
<pre><code>library(dplyr)
df1 %&gt;% 
anti_join(df2, by = c('Check')) %&gt;%
arrange(desc(Count))
</code></pre>
",2,0,1512,2021-04-23 00:50:04,https://stackoverflow.com/questions/67222581/using-anti-join-in-r
Dealing with several text columns in a labeled data set while running NLP in R,"<p>Hope all of you guys are healthy and well.
I am new to the world of NLP and my question may sound stupid, so I apologize in advance.I would like to perform NLP on some text data which is labeled and run a text mining predictive model. I have four text columns that can be used as predictors and my labeled column is my class variable. Perhaps, the following can give you a glimpse of the data set</p>
<pre><code> var1    var2  var3    var4      class_var
  NA     text  text     NA          0
  text   text   NA     text         1
  text    NA    NA     text         1
  NA      NA    NA     text         0
  NA     text  text    text         1  
</code></pre>
<p>As shown, in some columns there are no texts ( <code>I put NAs</code>) I have texts in other columns.
That being said, my question whether I should combine all text columns into one?
if so, what would be an appropriate method for dealing with this issue?</p>
<p>I truly appreciated your help guys.</p>
<p>Many thanks!</p>
","r, nlp, text-mining, data-cleaning, tm","<p>There are way too many options here but seeing as your data is already split into four columns, maybe you can first just replace the texts with a 1 if text is present or 0 for NA and see how well you can predict the class_var with a simple logistic regression as a start. From there, you could go into tokenizers etc.</p>
",0,0,190,2021-04-29 21:11:18,https://stackoverflow.com/questions/67325115/dealing-with-several-text-columns-in-a-labeled-data-set-while-running-nlp-in-r
R language : Check if two columns containing text are highly correlated,"<p>In R, we can use the <code>cor</code> function to get the correlation between two columns but it doesn't work for non-numerical values.</p>
<p>I ask this because I need to preprocess some data, and I suspect 2 columns being very similar because by looking, I found that when the first column says &quot;A&quot;, the second column always says &quot;B&quot; but I want to be sure that, indeed, if I know the value in the first column, I can deduce the value in the second.</p>
<p>If i'm not clear here's an exemple to illustrate.</p>
<pre><code>dataframe &lt;- read.csv(file = 'data/company_product.csv')
</code></pre>
<p>Where data/company_product.csv is a table like so</p>
<pre><code>Company Name     Main Product       rest of the data    ...
By Apple         A phone            some_other_data     ...
By Apple         A phone            some_other_data     ...
By Microsoft     A computer         some_other_data     ...
By Nokia         A tablet           some_other_data     ...
By Nokia         A tablet           some_other_data     ...
By Nokia         A tablet           some_other_data     ...
...              ...                ...
</code></pre>
<p>As you can see in this file, the column Main Product is useless because if I know the column Company Name is &quot;By Apple&quot;, the Main Product will always be &quot;A phone&quot;.</p>
<p>This means the column Company Name is highly correlated to the column Main Product, but I do not find a simple way in R to show that</p>
<p>I'm not sure if the answer will be extremely trivial, or if it is a key problem in text mining, but I do not need precise correlation, all I want is a Yes/No for &quot;Every time a value appear in first column, it will always be the same value in the second column&quot;</p>
<p>Thanks</p>
","r, text-mining","<p>Use table to assess this:</p>
<pre><code>table(df[, 1:2])
</code></pre>
<p>giving the following which shows only one non-zero in each row and in each column showing By Apple is associated with A phone, By Microsoft is associated with A computer and By Nokia is associated with A tablet.</p>
<pre><code>              second
first          A computer A phone A tablet
  By Apple              0       2        0
  By Microsoft          1       0        0
  By Nokia              0       0        2
</code></pre>
<p>or simply count the number of times each unique row appears:</p>
<pre><code>aggregate(list(count = df[[1]]), df, length)
##          first     second count
## 1 By Microsoft A computer     1
## 2     By Apple    A phone     2
## 3     By Nokia   A tablet     2
</code></pre>
<p>or</p>
<pre><code>library(dplyr)
count(df, first, second)
##          first     second n
## 1     By Apple    A phone 2
## 2 By Microsoft A computer 1
## 3     By Nokia   A tablet 2
</code></pre>
<p>or if you don't care about the count just look at the unique rows:</p>
<pre><code>unique(df[, 1:2])
##          first     second
## 1     By Apple    A phone
## 2 By Microsoft A computer
## 4     By Nokia   A tablet
</code></pre>
<p>Visualize this as follows:</p>
<pre><code>library(igraph)
g &lt;- graph_from_incidence_matrix(table(df[, 1:2]))
plot(g, layout = layout.bipartite)
</code></pre>
<p><a href=""https://i.sstatic.net/rRSOF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rRSOF.png"" alt=""screenshot"" /></a></p>
",1,1,275,2021-04-30 14:15:49,https://stackoverflow.com/questions/67335480/r-language-check-if-two-columns-containing-text-are-highly-correlated
If a condition is true how to extract specific rows addressing that condition from a csv file as print result,"<p>If I have 2 conditions: 1. a document is health relate 2. a document is not health-related
and my document satisfies one of the above-mentioned conditions. As an output, it must pull out data from a CSV file from rows associated with this condition say row 1, 5, and 8, and print it.</p>
","python, pandas, conditional-statements, text-mining","<pre><code>import pandas as pd

df = pd.read_csv('path/to/csv')

print(df.iloc[1, 5, 8])
</code></pre>
",-1,-2,156,2021-05-04 19:56:00,https://stackoverflow.com/questions/67391381/if-a-condition-is-true-how-to-extract-specific-rows-addressing-that-condition-fr
Counting specific word occurrences between 2 data frames in R with a group_by needed,"<p>I have two data frames in R, the first one (named Words) is composed by a single columns of words :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Words</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hello</td>
</tr>
<tr>
<td>Building</td>
</tr>
<tr>
<td>School</td>
</tr>
<tr>
<td>Hospital</td>
</tr>
<tr>
<td>Doctors</td>
</tr>
</tbody>
</table>
</div>
<p>The second is a big dataset presented like this :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">id</th>
<th style=""text-align: center;"">description</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">382</td>
<td style=""text-align: center;"">Building a school</td>
</tr>
<tr>
<td style=""text-align: left;"">787</td>
<td style=""text-align: center;"">Hiring doctors for the new hospital and teachers for the school</td>
</tr>
</tbody>
</table>
</div>
<p>Then, i'd like to group by ID and to obtain the following results</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">id</th>
<th style=""text-align: center;"">description</th>
<th style=""text-align: left;"">Match</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">382</td>
<td style=""text-align: center;"">Building a school</td>
<td style=""text-align: left;"">2</td>
</tr>
<tr>
<td style=""text-align: left;"">787</td>
<td style=""text-align: center;"">Hiring doctors for the new hospital and teachers for the school</td>
<td style=""text-align: left;"">3</td>
</tr>
</tbody>
</table>
</div>
<p>Here is what i tried</p>
<pre><code>library(stringr)

df &lt;- df %&gt;% group_by(df$id)

getCount &lt;- function(data,keyword)
{
  wcount &lt;- str_count(df$description, keyword)
  return(data.frame(data,wcount))
}

gCount(df$description,Words)

</code></pre>
<p>(I also tried by converting the Words dataset to a list)</p>
<p>As well as :</p>
<pre><code>df &lt;- df %&gt;% group_by(df$id)
table(df$description)

df$match &lt;- df[df$description %in% Words$Words,]
table(df$match)
</code></pre>
<p>And finally</p>
<pre><code>
Words.list &lt;- setNames(split(Words, seq(nrow(Words))), rownames(Words))
description &lt;- subset(df, select = c(&quot;description&quot;,&quot;id&quot;))
description &lt;- description %&gt;% group_by(description$id)
description.list &lt;- setNames(split(description, seq(nrow(description))), rownames(description))

str_to_search = Words.list
str_to_count = description.list

lengths(regmatches(str_to_search, gregexpr(str_to_count, str_to_search, fixed = TRUE)))
</code></pre>
<p>However i only have weird error messages that i don't understand.</p>
","r, dataframe, text-mining, matching","<pre><code>library(stringr)
library(purrr)

words &lt;- c(&quot;Hello&quot;, &quot;Building&quot;, &quot;School&quot;, &quot;Hospital&quot;, &quot;Doctors&quot;) %&gt;%
  str_to_lower()
descriptions &lt;- c(&quot;Building a school&quot;, &quot;Hiring doctors for the new hospital and teachers for the school&quot;) 

df_descriptions &lt;- data.frame(description = descriptions) %&gt;%
    mutate(Match = map_int(str_to_lower(description), ~str_count(.x, words) %&gt;% sum()))
</code></pre>
<p><strong>edit</strong></p>
<pre><code>df_descriptions &lt;- data.frame(description = descriptions) %&gt;%
  mutate(
    Match = str_to_lower(description) %&gt;%
      str_split(&quot; &quot;) %&gt;%
      map_int(~sum(.x %in% words))
  )
</code></pre>
",0,0,25,2021-05-05 12:46:53,https://stackoverflow.com/questions/67401647/counting-specific-word-occurrences-between-2-data-frames-in-r-with-a-group-by-ne
tokenizing on a pdf for quantitative analysis,"<p>I ran into an issue using the unnest_tokens function on a data_frame. I am working with pdf files I want to compare.</p>
<pre><code>text_path &lt;- &quot;c:/.../text1.pdf&quot;
text_raw &lt;- pdf_text(&quot;c:/.../text1.pdf&quot;)
text1df&lt;- data_frame(Zeile = 1:25, 
                      text_raw)
</code></pre>
<p>So far so good. But here comes my problemo:</p>
<pre><code>  unnest_tokens(output = token, input = content) -&gt; text1_long
</code></pre>
<p>Error: Must extract column with a single valid subscript.
x Subscript <code>var</code> has the wrong type <code>function</code>.
i It must be numeric or character.</p>
<p>I want to tokenize my pdf files so I can analyse the word frequencies and maybe compare multiple pdf files on wordclouds.</p>
","r, nlp, text-mining, quanteda","<p>Here is a piece of simple code. I kept your German words so you can copy paste everything.</p>
<pre><code>library(pdftools)
library(dplyr)
library(stringr)
library(tidytext)

file_location &lt;- &quot;d:/.../my_doc.pdf&quot;
text_raw &lt;- pdf_text(file_location)
# Zeile 12 because I only have 12 pages
text1df &lt;- data_frame(Zeile = 1:12, 
                     text_raw) 

text1df_long &lt;- unnest_tokens(text1df , output = wort, input = text_raw ) %&gt;% 
  filter(str_detect(wort, &quot;[a-z]&quot;))

text1df_long
# A tibble: 4,134 x 2
   Zeile wort       
   &lt;int&gt; &lt;chr&gt;      
 1     1 training   
 2     1 and        
 3     1 development
 4     1 policy     
 5     1 contents   
 6     1 policy     
 7     1 statement  
 8     1 scope      
 9     1 induction  
10     1 training   
# ... with 4,124 more rows
</code></pre>
",0,0,565,2021-05-05 18:30:56,https://stackoverflow.com/questions/67406898/tokenizing-on-a-pdf-for-quantitative-analysis
Text mining between a data frame column and 2 lists in R,"<p>So i created two lists composed of words :</p>
<pre><code>fruits &lt;- c(&quot;banana&quot;,&quot;apple&quot;,&quot;strawberry&quot;)
homemade &lt;- c(&quot;kitchen&quot;,&quot;homemade&quot;,&quot;mom&quot;,&quot;dad&quot;,&quot;sister&quot;)
</code></pre>
<p>And here is my dataset</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>description</th>
<th>isCake</th>
</tr>
</thead>
<tbody>
<tr>
<td>apple cake cooked by mom</td>
<td>YES</td>
</tr>
<tr>
<td>pie from the bakery</td>
<td>NO</td>
</tr>
<tr>
<td>strawberry dessert by dad</td>
<td>NO</td>
</tr>
</tbody>
</table>
</div>
<p>I want to create a text mining code so that when df$description contains one or multiple words from &quot;fruits&quot; AND one or multiple words from homemade, df$isCake become &quot;OK&quot;</p>
<p><strong>Expected output</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>description</th>
<th>isCake</th>
</tr>
</thead>
<tbody>
<tr>
<td>apple cake cooked by mom</td>
<td>YES</td>
</tr>
<tr>
<td>pie from the bakery</td>
<td>NO</td>
</tr>
<tr>
<td>strawberry dessert by dad</td>
<td>OK</td>
</tr>
</tbody>
</table>
</div>
<pre><code>df &lt;- df %&gt;% mutate(isCake=ifelse(description %in% fruits &amp; description %in% homemade, &quot;OK&quot;, isCake))
</code></pre>
<p>I have no error messages but apparently is doesn't work because when i subset if isCake==&quot;OK&quot; i always have 0 obs.</p>
","r, list, dataframe, text-mining, rdata","<p>You can create a pattern from <code>fruits</code> and <code>homemade</code> vector and use it in <code>grepl</code> :</p>
<pre><code>df$isCake[grepl(paste0(fruits, collapse = '|'), df$description) &amp; 
          grepl(paste0(homemade, collapse = '|'), df$description)] &lt;- 'OK'

df
#                description isCake
#1  apple cake cooked by mom     OK
#2       pie from the bakery     NO
#3 strawberry dessert by dad     OK
</code></pre>
",0,0,112,2021-05-13 11:44:52,https://stackoverflow.com/questions/67518675/text-mining-between-a-data-frame-column-and-2-lists-in-r
Edit XML files with python,"<p>I'm trying to edit several XML files at the same time using python.
In the original XML files I have Spekers and what they say but not as parent-child tag. Like this:</p>
<pre><code>     &lt;p Speaker&gt;John&lt;/p&gt;
     &lt;p Text&gt;Speech he's giving&lt;/p&gt;
     &lt;p Text&gt;Speech he's giving&lt;/p&gt;
     &lt;p Speaker&gt;Laura&lt;/p&gt;
     &lt;p Text&gt;Speech she's giving&lt;/p&gt;
     &lt;p Text&gt;Speech she's giving&lt;/p&gt;
</code></pre>
<p>But I want to make a parent-child relation between speaker and text. Besides in a database that I already have with the information of the speakers, I would like to add their information like their speakerid, their role, and create a count of the times they speak. just like this:</p>
<pre><code>    &lt;u xml:id=&quot;speakercount.u1&quot;
           who=&quot;#speakerid&quot;
           ana=&quot;#role&quot;&gt;
           &lt;seg xml:id=&quot;speechcount.u1.1&quot;&gt;text&lt;/seg&gt;
           &lt;seg xml:id=&quot;speechcount.u.1.2&quot;&gt;text&lt;/seg&gt;
       &lt;/u&gt;
    &lt;u xml:id=&quot;speakercount.u2&quot;
           who=&quot;#speakerid&quot;
           ana=&quot;#role&quot;&gt;
           &lt;seg xml:id=&quot;speechcount.u.2.1&quot;&gt;text&lt;/seg&gt;
           &lt;seg xml:id=&quot;speechcount.u.2.2&quot;&gt;text&lt;/seg&gt;
    &lt;/u&gt;
</code></pre>
<p>Is this possible to do? and to do it to several XML at once? What python modules I would need to do it? Because I can't seem to find the necessary information to do so...</p>
","python, xml, xslt, text-mining","<p>You could use xslt for this.</p>
<p>To give you some direction, i.e. build a singel xml file containing the db-info combined combined with the content of hose separate xml-files like this:</p>
<pre><code>&lt;root&gt;
  &lt;speakers&gt;
    &lt;speaker id=&quot;1&quot; role=&quot;A&quot; name=&quot;John&quot;/&gt;
    &lt;speaker id=&quot;2&quot; role=&quot;B&quot; name=&quot;Laura&quot;/&gt;
  &lt;/speakers&gt;
  &lt;ps&gt;
    &lt;p type=&quot;Speaker&quot;&gt;John&lt;/p&gt;
    &lt;p type=&quot;Text&quot;&gt;Speech he's giving&lt;/p&gt;
    &lt;p type=&quot;Text&quot;&gt;Speech he's giving&lt;/p&gt;
    &lt;p type=&quot;Speaker&quot;&gt;Laura&lt;/p&gt;
    &lt;p type=&quot;Text&quot;&gt;Speech she's giving&lt;/p&gt;
    &lt;p type=&quot;Text&quot;&gt;Speech she's giving&lt;/p&gt;    
  &lt;/ps&gt;
&lt;/root&gt;
</code></pre>
<p>And then use a xslt like this:</p>
<pre><code>&lt;?xml version='1.0' encoding='UTF-8'?&gt;
&lt;xsl:stylesheet 
  version=&quot;2.0&quot; 
  xmlns:xsl=&quot;http://www.w3.org/1999/XSL/Transform&quot; &gt;
  &lt;xsl:output indent=&quot;yes&quot;/&gt;
  
  &lt;xsl:variable name=&quot;speakers&quot; as=&quot;element()*&quot; select=&quot;/*/speakers/speaker&quot;/&gt;

  &lt;xsl:template match=&quot;*[p]&quot;&gt;
    &lt;us&gt;
      &lt;xsl:for-each-group select=&quot;p&quot; group-starting-with=&quot;p[@type='Speaker']&quot;&gt;
        &lt;xsl:variable name=&quot;speakerName&quot; select=&quot;current-group()[1]/text()&quot;/&gt;
        &lt;xsl:variable name=&quot;speakerDbRecord&quot; select=&quot;$speakers[@name=$speakerName]&quot;/&gt;
        &lt;xsl:variable name=&quot;posSpeaker&quot; select=&quot;position()&quot;/&gt;
        &lt;u xml:id=&quot;speakercount.u{$posSpeaker}&quot;
          who=&quot;#{$speakerDbRecord/@id}&quot;
          ana=&quot;#{$speakerDbRecord/@role}&quot;&gt;
          &lt;xsl:for-each select=&quot;current-group()[ position() gt 1]&quot;&gt;
            &lt;xsl:variable name=&quot;posText&quot; select=&quot;position() - 1&quot;/&gt;
            &lt;seg xml:id=&quot;speechcount.u.{$posSpeaker}.{$posText}&quot;&gt;&lt;xsl:value-of select=&quot;.&quot;/&gt;&lt;/seg&gt;
          &lt;/xsl:for-each&gt;
        &lt;/u&gt;
      &lt;/xsl:for-each-group&gt;
    &lt;/us&gt;
  &lt;/xsl:template&gt;
  
&lt;/xsl:stylesheet&gt;
</code></pre>
<p>Will give you this:</p>
<pre><code>&lt;us&gt;
   &lt;u xml:id=&quot;speakercount.u1&quot; who=&quot;#1&quot; ana=&quot;#A&quot;&gt;
      &lt;seg xml:id=&quot;speechcount.u.1.0&quot;&gt;Speech he's giving&lt;/seg&gt;
      &lt;seg xml:id=&quot;speechcount.u.1.1&quot;&gt;Speech he's giving&lt;/seg&gt;
   &lt;/u&gt;
   &lt;u xml:id=&quot;speakercount.u2&quot; who=&quot;#2&quot; ana=&quot;#B&quot;&gt;
      &lt;seg xml:id=&quot;speechcount.u.2.0&quot;&gt;Speech she's giving&lt;/seg&gt;
      &lt;seg xml:id=&quot;speechcount.u.2.1&quot;&gt;Speech she's giving&lt;/seg&gt;
   &lt;/u&gt;
&lt;/us&gt;
</code></pre>
",1,1,69,2021-05-20 15:36:23,https://stackoverflow.com/questions/67623616/edit-xml-files-with-python
Rounding to three decimals in Python,"<p>I have an answer to my code question that asks me to round</p>
<p>Part 1: Write a line of code in the cell below that will display the lexicon-based sentiment polarity score for the following sentence: 'Hiking in the mountains is fun and very relaxing.'</p>
<p>Part 2: What is the lexicon-based sentiment polarity score for the sentence 'Hiking in the mountains is fun and very relaxing.' ? Report your answer <strong>using three decimals</strong> of precision (e.g., 0.321).**</p>
<p>How do I add a rounding function to this in the snippet of code?</p>
<pre><code>get_lexicon_polarity('Hiking in the mountains is fun and very relaxing')

</code></pre>
<p>I know that I can always do this to the answer:</p>
<pre><code>np.round(0.2310364009813431, 3)

</code></pre>
<p>BUT - I would like to know how to call it in the original code.</p>
","python, text-mining","<p>2 ways here:</p>
<p>1: assign the result of get_lexicon_polarity to a variable, then pass the variable to np.round</p>
<pre><code>lexicon_polarity = get_lexicon_polarity('Hiking in the mountains is fun and very relaxing')
np.round(lexicon_polarity, 3)
</code></pre>
<p>2: pass the function with parameter as an input parameter to np.round</p>
<pre><code>np.round(get_lexicon_polarity('Hiking in the mountains is fun and very relaxing'), 3)
</code></pre>
",1,-1,53,2021-05-29 03:44:57,https://stackoverflow.com/questions/67747545/rounding-to-three-decimals-in-python
Memory problems when using lapply for corpus creation,"<p>My eventual goal is to transform thousands of pdfs into a corpus / document term matrix to conduct some topic modeling. I am using the pdftools package to import my pdfs and work with the tm package for preparing my data for text mining. I managed to import and transform one individual pdf, like this:</p>
<pre><code>txt &lt;- pdf_text(&quot;pdfexample.pdf&quot;)

#create corpus
txt_corpus &lt;- Corpus(VectorSource(txt))

# Some basic text prep, with tm_map(), like:
txt_corpus &lt;- tm_map(txt_corpus, tolower)

# create document term matrix
dtm &lt;- DocumentTermMatrix(txt_corpus)
</code></pre>
<p>However, I am completely stuck with automating this process and I have only limited experience with either loops or apply functions. My approach has run into memory problems, when converting the raw pdf_text() output into a corpus, even though I tested my code only with 5 pdf files (total: 1.5MB). R tried to allocate a vector of more than half GB. Which seems absolutely not right to me. My attempt looks like this:</p>
<pre><code># Create a list of all pdf paths
file_list &lt;- list.files(path = &quot;mydirectory&quot;,
                 full.names = TRUE,
                 pattern = &quot;name*&quot;, # to import only specific pdfs
                 ignore.case = FALSE)

# Run a function that reads the pdf of each of those files:
all_files &lt;- lapply(file_list, FUN = function(files) {
             pdf_text(files)
             })

all_files_corpus = lapply(all_files,
                          FUN = Corpus(DirSource())) # That's where I run into memory issues
</code></pre>
<p>Am I doing something fundamentally wrong? Not sure if it is just a mere memory issue or whether there are easier approaches to my problem. At least, from what I gathered, lapply should be a lot more memory efficient then looping. But maybe there is more to it. I've tried to solve it by my own for days now, but nothing worked.</p>
<p>Grateful for any advice/hint on how to proceed!</p>
<p>Edit: I tried to execute the lapply with only one pdf and my R crashed again, even though I have no capacity problems at all, when using the code mentioned first.</p>
","r, memory, lapply, text-mining, corpus","<p>You can write a function which has series of steps that you want to execute on each pdf.</p>
<pre><code>pdf_to_dtm &lt;- function(file) {
  txt &lt;- pdf_text(file)
  #create corpus
  txt_corpus &lt;- Corpus(VectorSource(txt))
  # Some basic text prep, with tm_map(), like:
  txt_corpus &lt;- tm_map(txt_corpus, tolower)
  # create document term matrix
  dtm &lt;- DocumentTermMatrix(txt_corpus)
  dtm
}
</code></pre>
<p>Using <code>lapply</code> apply the function on each file</p>
<pre><code>file_list &lt;- list.files(path = &quot;mydirectory&quot;,
                 full.names = TRUE,
                 pattern = &quot;name*&quot;, # to import only specific pdfs
                 ignore.case = FALSE)

all_files_corpus &lt;- lapply(file_list, pdf_to_dtm)
</code></pre>
",0,0,184,2021-06-03 15:03:51,https://stackoverflow.com/questions/67823934/memory-problems-when-using-lapply-for-corpus-creation
"Error in LDA(cdes, k = K, method = &quot;Gibbs&quot;, control = list(verbose = 25L, : Each row of the input matrix needs to contain at least one non-zero entry","<p>I have a big dataset of almost 90 columns and about 200k observations. One of the column contains descriptions, so it's only text. However, i have like 100 descriptions that are NAs.</p>
<p>I tried the code of Pablo Barbera from GitHub concerning Topic Models because i need it.</p>
<p><strong>OUTPUT</strong></p>
<pre><code>library(topicmodels)
library(quanteda)

des &lt;- subset(finalMSI, !is.na(description), select=c(description))
corpus_des &lt;- corpus(des$description)
df_des &lt;- dfm(corpus_des, remove=stopwords(&quot;spanish&quot;), verbose=TRUE,
              remove_punct=TRUE, remove_numbers=TRUE)
cdes &lt;- dfm_trim(df_des, min_docfreq = 2)

# estimate LDA with K topics
K &lt;- 20
lda &lt;- LDA(cdes, k = K, method = &quot;Gibbs&quot;, 
           control = list(verbose=25L, seed = 123, burnin = 100, iter = 500))

</code></pre>
<blockquote>
<p>Error in LDA(cdes, k = K, method = &quot;Gibbs&quot;, control = list(verbose = 25L,  : Each row of the input matrix needs to contain at least one non-zero entry</p>
</blockquote>
<p>As i don't have any NA in my subset, i don't understand this error message (it's my first time using this package)</p>
","r, dataframe, text-mining, quanteda, topicmodels","<p>It looks like some of your documents are empty, in the sense that they contain no counts of any feature.</p>
<p>You can remove them with:</p>
<pre><code>cdes &lt;- dfm_trim(df_des, min_docfreq = 2) %&gt;%
   dfm_subset(ntoken(cdes) &gt; 0)
</code></pre>
",1,0,211,2021-06-03 16:43:56,https://stackoverflow.com/questions/67825501/error-in-ldacdes-k-k-method-gibbs-control-listverbose-25l-each
Can&#39;t import spacy,"<p>i've been trying to import <strong>spacy</strong> but everytime an error appears  as a result.
I used this line to install the package :</p>
<pre><code>conda install -c conda-forge spacy
</code></pre>
<p>then i tried to <strong>import spacy</strong> and it gives me this error:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-11-76a01d9c502b&gt; in &lt;module&gt;
----&gt; 1 import spacy

~\Python\Text\spacy.py in &lt;module&gt;
      9 import spacy
     10 # Load English tokenizer, tagger, parser, and NER
---&gt; 11 nlp = spacy.load('en_core_web_sm')
     12 # Process whole documents
     13 text = (&quot;When Sebastian Thrun started working on self-driving cars at &quot;

AttributeError: partially initialized module 'spacy' has no attribute 'load' (most likely due to a circular import)
</code></pre>
<p>Can anybody help me.</p>
","python, spacy, text-mining","<p>The problem is that the file you are working in is named <code>spacy.py</code>, which is interfering with the spacy module. So you should rename your file to something other than &quot;spacy&quot;.</p>
",2,0,1604,2021-06-08 16:06:14,https://stackoverflow.com/questions/67890652/cant-import-spacy
Remove 2 stopwords lists with Quanteda package R,"<p>I'm working with quanteda package on a corpus dataframe, and here is the basic code i use :</p>
<pre><code>library(quanteda)

fmsi_des &lt;- dfm(corpus_des, remove=stopwords(&quot;spanish&quot;), verbose=TRUE,
                remove_punct=TRUE, remove_numbers=TRUE)
</code></pre>
<p>However, i have another stopowords list as a data frame, called stpw, that i'd like to take into account.</p>
<p>I tried :</p>
<pre><code>fmsi_des &lt;- dfm(corpus_des, remove=stopwords(&quot;spanish&quot;,&quot;stpw&quot;), verbose=TRUE,
                remove_punct=TRUE, remove_numbers=TRUE)
</code></pre>
<blockquote>
<p>Error in stopwords(&quot;spanish&quot;, &quot;stpw&quot;) : unused argument (&quot;stpw&quot;)</p>
</blockquote>
<p>Then i created a list with the stopwords of &quot;spanish&quot; + the stopwords of stpw :</p>
<pre><code>all_stops &lt;- c(&quot;bogota&quot;,&quot;vias&quot;,&quot;medellin&quot;,&quot;valle&quot;,&quot;departamento&quot;,stopwords(&quot;spanish&quot;))

fmsi_des &lt;- dfm(corpus_des, remove=stopwords(&quot;all_stops&quot;), verbose=TRUE,
                remove_punct=TRUE, remove_numbers=TRUE)

</code></pre>
<blockquote>
<p>Error in stopwords(&quot;all_stops&quot;) : no stopwords available for 'all_stops'</p>
</blockquote>
<p>I also created a txt file with my stopwords, in order to try that :</p>
<pre><code>library(tm)

stopwords = readLines('stpw.txt') 
x  = fd$contract_description        
x  =  removeWords(x,stopwords)

des &lt;- subset(x, !is.na(x))
corpus_des &lt;- corpus(des$fd.contract_description)
fmsi_des &lt;- dfm(corpus_des, remove=stopwords(&quot;spanish&quot;), verbose=TRUE,
                remove_punct=TRUE, remove_numbers=TRUE)
     
</code></pre>
<blockquote>
<p>Warning message:
In readLines(&quot;stp.txt&quot;) : Incomplete final line found in 'stpw.txt'</p>
</blockquote>
<blockquote>
<p>Error in gsub(sprintf(&quot;(*UCP)\b(%s)\b&quot;, paste(sort(words, decreasing = TRUE),  :
incorrect regular expression '(*UCP)\b(bogota|vias|medellin|valle|departamento|+)\b'
In addition : Warning message:
In gsub(sprintf(&quot;(*UCP)\b(%s)\b&quot;, paste(sort(words, decreasing = TRUE),  :
PCRE pattern compilation error
'nothing to repeat'
at '+)\b'</p>
</blockquote>
","r, text-mining, corpus, stop-words, quanteda","<p>This is a case where knowing the value of return objects in R is the key to obtaining the result you want.  Specifically, you need to know what <code>stopwords()</code> returns, as well as what it is expected as its first argument.</p>
<p><code>stopwords(language = &quot;sp&quot;)</code> returns a character vector of Spanish stopwords, using the default <code>source = &quot;snowball&quot;</code> list.  (See <code>?stopwords</code> for full details.)</p>
<p>So if you want to remove the default Spanish list <em>plus</em> your own words, you concatenate the returned character vector with additional elements.  This is what you have done in creating <code>all_stops</code>.</p>
<p>So to remove <code>all_stops</code> -- and here, using the quanteda v3 suggested usage -- you simply do the following:</p>
<pre><code>fmsi_des &lt;- corpus_des %&gt;%
    tokens(remove_punct = TRUE, remove_numbers = TRUE) %&gt;%
    tokens_remove(pattern = all_stops) %&gt;%
    dfm()
</code></pre>
",1,0,466,2021-06-09 10:09:11,https://stackoverflow.com/questions/67902006/remove-2-stopwords-lists-with-quanteda-package-r
PDF Text extraction and storing them as key-value pair,"<p>I want to extract text from a PDF. The output I am getting from text extraction is not that much organised.</p>
<p>PDF Link (Only 1st Page): <a href=""https://microprecision.com/wp-content/uploads/2020/08/Sample-Cert_rev-7-1.pdf"" rel=""nofollow noreferrer"">https://microprecision.com/wp-content/uploads/2020/08/Sample-Cert_rev-7-1.pdf</a></p>
<p>I want to extract parameters like MPC Control No #, Serial No., Model Number, etc. and may store them as key-value pair in dictionary.</p>
<p>I am trying this with the below code but not getting the desired output.</p>
<pre><code>import io
from pdfminer.layout import LAParams, LTTextBox
from pdfminer3.pdfpage import PDFPage
from pdfminer3.pdfinterp import PDFResourceManager,PDFPageInterpreter
from pdfminer3.converter import PDFPageAggregator,TextConverter

def pdftotext(path):
   resource_manager = PDFResourceManager()
   file_handle = io.StringIO()
laprams = LAParams(word_margin=1.0,boxes_flow=0.5,char_margin=2.0,line_overlap=0.5,line_margin=0.5)
   converter = TextConverter(resource_manager,file_handle, laparams=laprams)
   page_interpreter = PDFPageInterpreter(resource_manager,converter)
   i = 1
   with open(path,'rb') as fh:
      for page in PDFPage.get_pages(fh,caching=False,check_extractable=True):
        page_interpreter.process_page(page)
        
      text = file_handle.getvalue()

   converter.close()
   file_handle.close()

   return text

 raw = pdftotext('Sample-Certificate.pdf')
 print(raw)
</code></pre>
","python, regex, text-mining, pdftotext, pdf-extraction","<p>When working with PDF files I prefer to work with PyMuPDF library <a href=""https://pypi.org/project/PyMuPDF/"" rel=""nofollow noreferrer"">https://pypi.org/project/PyMuPDF/</a></p>
<pre><code>import fitz

txt = []
doc = fitz.open(&quot;Sample-Cert_rev-7-1.pdf&quot;)            # some existing PDF
page = doc[0]
text = page.getText(&quot;text&quot;)
txt = list(text)
print(text)
text = text.split('\n')
txt = list(text)
print(txt)
ix = text.index('MPC Control #:')
print(ix)
print(text[ix+18])
</code></pre>
<p>Pay attention on how to install the library correctly<br />
Here is the output:</p>
<pre><code>&quot;C:\Program Files\Python38\python.exe&quot; C:/Python/stackoverflow extract_pdf_text1.py
MICRO PRECISION CALIBRATION, INC.
22835 INDUSTRIAL PLACE
GRASS VALLEY CA 95949
530-268-1860
Cert No.
551220083746791
Date: Aug 3, 2020
Certificate of Calibration
AC-1969.00
N/A
July 01, 2021
N/A
Customer:
MPC Control #:
Asset ID:
Gage Type:
Manufacturer:
Model Number:
Size:
Temp/RH:
Serial Number:
Department:
Performed By:
Received Condition:
Returned Condition:
Cal. Date:
Cal. Interval:
Cal. Due Date:
Work Order #:
DIGITAL MULTIMETER
DANNY BOY B. BUTIAL
0258964
0258964
NONE
AGILENT
34401A
10MHZ
SAMPLE
N/A
IN TOLERANCE
IN TOLERANCE
 July 01, 2020
N/A
12 MONTHS
Calibration Notes:
SAMPLE COMPANY
23.0°C / 40.0%
Location:
Calibration performed at MPC facility
Standards Used to Calibrate Equipment
I.D.
Description.
Model
Serial
Manufacturer
Cal. Due Date
Traceability #
PH1405
MULTI-PRODUCT CALIBRATOR
5520A
7575006
FLUKE
Sep 10, 2020
551220083204793
AL4394
DIGITAL MULTIMETER
3458A
2823A09832
AGILENT
Aug 1, 2020
551220083719099
Procedures Used in this Event
Procedure Name
Description
MPC Automated Procedure
MPCCAL Rev. 00
STATEMENTS OF PASS OR FAIL CONFORMANCE: The uncertainty of measurement has been taken into account when determining compliance with specification. All measurements and test results guard banded to ensure the
probability of false-accept does not exceed 2% in compliance with ANSI/NCSL Z540.3-2006 and in case without guard banded the probability of false-accept depending on test uncertainty ratio.
THE CALIBRATION REPORT STATUS:
PASS- Term used when compliance statement is given, and the measurement result is PASS.
PASSz- Term used when compliance statement is given, and the measurement result is conditional passed or PASSz.
FAIL- Term used when compliance statement is given, and the measurement result is FAIL.
FAILz- Term used when compliance statement is given, and the measurement result is conditional failed or FAILz.
REPORT OF VALUE - Term used when reported measurement is not requiring compliance statement in report.
ADJUSTED- When adjustments are made to an instrument which changes the value of measurement from what was measured as found to new value as left.
LIMITED - When an instrument fails calibration but is still functional in a limited manner.
The expanded uncertainty of measurement is stated as the standard uncertainty of measurement multiplied by the coverage factor k=2, which for a normal distribution corresponds to a coverage probability of approximately 95%, unless otherwise stated. This
calibration report complies with ISO/IEC 17025:2017 and ANSI/NCSL Z540.3. Calibration cycles and resulting due dates were submitted/approved by the customer. Any number of factors may cause an instrument to drift out of tolerance before the next
scheduled calibration. Recalibration cycles should be based on frequency of use, environmental conditions and customer's established systematic accuracy. All standards are traceable to SI through the National Institute of Standards and Technology (NIST)
and/or recognized national or international standards laboratories. Services rendered include proper manufacturer’s service instruction and are warranted for no less than thirty (30) days. The information on this report pertains only to the instrument identified,
this may not be reproduced in part or in a whole without the prior written approval of the issuing MP Calibration Laboratory.
Rick Hernandez
Calibrating Technician:
QC Approval:
DANNY BOY B. BUTIAL
(CERT, Rev 7)
Page 1 of 1

['MICRO PRECISION CALIBRATION, INC.', '22835 INDUSTRIAL PLACE', 'GRASS VALLEY CA 95949', '530-268-1860', 'Cert No.', '551220083746791', 'Date: Aug 3, 2020', 'Certificate of Calibration', 'AC-1969.00', 'N/A', 'July 01, 2021', 'N/A', 'Customer:', 'MPC Control #:', 'Asset ID:', 'Gage Type:', 'Manufacturer:', 'Model Number:', 'Size:', 'Temp/RH:', 'Serial Number:', 'Department:', 'Performed By:', 'Received Condition:', 'Returned Condition:', 'Cal. Date:', 'Cal. Interval:', 'Cal. Due Date:', 'Work Order #:', 'DIGITAL MULTIMETER', 'DANNY BOY B. BUTIAL', '0258964', '0258964', 'NONE', 'AGILENT', '34401A', '10MHZ', 'SAMPLE', 'N/A', 'IN TOLERANCE', 'IN TOLERANCE', ' July 01, 2020', 'N/A', '12 MONTHS', 'Calibration Notes:', 'SAMPLE COMPANY', '23.0°C / 40.0%', 'Location:', 'Calibration performed at MPC facility', 'Standards Used to Calibrate Equipment', 'I.D.', 'Description.', 'Model', 'Serial', 'Manufacturer', 'Cal. Due Date', 'Traceability #', 'PH1405', 'MULTI-PRODUCT CALIBRATOR', '5520A', '7575006', 'FLUKE', 'Sep 10, 2020', '551220083204793', 'AL4394', 'DIGITAL MULTIMETER', '3458A', '2823A09832', 'AGILENT', 'Aug 1, 2020', '551220083719099', 'Procedures Used in this Event', 'Procedure Name', 'Description', 'MPC Automated Procedure', 'MPCCAL Rev. 00', 'STATEMENTS OF PASS OR FAIL CONFORMANCE: The uncertainty of measurement has been taken into account when determining compliance with specification. All measurements and test results guard banded to ensure the', 'probability of false-accept does not exceed 2% in compliance with ANSI/NCSL Z540.3-2006 and in case without guard banded the probability of false-accept depending on test uncertainty ratio.', 'THE CALIBRATION REPORT STATUS:', 'PASS- Term used when compliance statement is given, and the measurement result is PASS.', 'PASSz- Term used when compliance statement is given, and the measurement result is conditional passed or PASSz.', 'FAIL- Term used when compliance statement is given, and the measurement result is FAIL.', 'FAILz- Term used when compliance statement is given, and the measurement result is conditional failed or FAILz.', 'REPORT OF VALUE - Term used when reported measurement is not requiring compliance statement in report.', 'ADJUSTED- When adjustments are made to an instrument which changes the value of measurement from what was measured as found to new value as left.', 'LIMITED - When an instrument fails calibration but is still functional in a limited manner.', 'The expanded uncertainty of measurement is stated as the standard uncertainty of measurement multiplied by the coverage factor k=2, which for a normal distribution corresponds to a coverage probability of approximately 95%, unless otherwise stated. This', 'calibration report complies with ISO/IEC 17025:2017 and ANSI/NCSL Z540.3. Calibration cycles and resulting due dates were submitted/approved by the customer. Any number of factors may cause an instrument to drift out of tolerance before the next', &quot;scheduled calibration. Recalibration cycles should be based on frequency of use, environmental conditions and customer's established systematic accuracy. All standards are traceable to SI through the National Institute of Standards and Technology (NIST)&quot;, 'and/or recognized national or international standards laboratories. Services rendered include proper manufacturer’s service instruction and are warranted for no less than thirty (30) days. The information on this report pertains only to the instrument identified,', 'this may not be reproduced in part or in a whole without the prior written approval of the issuing MP Calibration Laboratory.', 'Rick Hernandez', 'Calibrating Technician:', 'QC Approval:', 'DANNY BOY B. BUTIAL', '(CERT, Rev 7)', 'Page 1 of 1', '']
13
0258964

Process finished with exit code 0

</code></pre>
",2,0,6679,2021-06-16 08:45:12,https://stackoverflow.com/questions/67999262/pdf-text-extraction-and-storing-them-as-key-value-pair
access elements in list of lists,"<p>I am new at text mining, I am using Python. I have a list of lists, each list contains clusters of synonyms and each word in the cluster has a list which contains the number of sentences which it appears.
The list i have is like this</p>
<pre><code>syn_cluster = [[['Jack', [1]]], [['small', [1, 2]], ['modest', [1, 3]], ['little', [2]]], [['big', [1]], ['large', [2]]]]
</code></pre>
<p>I want to assign for each cluster the <code>min</code> and the <code>max</code> from the appearance list, so i want the result to be like this</p>
<pre><code>[[['Jack', [1]]], [['small', 'modest, 'little'], [1, 3]], [['big', large], [1, 2]]]
</code></pre>
","python, list, nlp, text-mining","<p>I am not sure if you are using the best data structure for your problem. But if you do this with lists of lists you could do:</p>
<pre><code>from itertools import chain

serialized = []
for syn_words in syn_cluster:
    words = [w[0] for w in syn_words]
    freqs = list(chain.from_iterable([f[1] for f in syn_words]))
    min_max = [min(freqs), max(freqs)]
    # or:  min_max = list({min(freqs), max(freqs)}) if you want [1] instead of [1, 1]
    serialized.append([words, min_max])

serialized
&gt;&gt;&gt; [[['Jack'], [1, 1]],
    [['small', 'modest', 'little'], [1, 3]],
    [['big', 'large'], [1, 2]]]
</code></pre>
",2,2,1160,2021-06-18 16:59:06,https://stackoverflow.com/questions/68038955/access-elements-in-list-of-lists
Find city names within affiliations and add them with their corresponding countries in new columns of a dataframe,"<p>I have a dataframe ‘dfa’ of affiliations that contains city names, for which the country is sometimes missing, e.g. like rows 4 (BAGHDAD) and 7 (BERLIN):</p>
<pre><code>dfa &lt;- data.frame(affiliation=c(&quot;DEPARTMENT OF PHARMACY, AMSTERDAM UNIVERSITY, AMSTERDAM, THE NETHERLANDS&quot;,
                                &quot;DEPARTMENT OF BIOCHEMISTRY, LADY HARDINGE MEDICAL COLLEGE, NEW DELHI, INDIA.&quot;,
                                &quot;DEPARTMENT OF PATHOLOGY, CHILDREN'S HOSPITAL, LOS ANGELES, UNITED STATES&quot;,
                                &quot;COLLEGE OF EDUCATION FOR PURE SCIENCE, UNIVERSITY OF BAGHDAD.&quot;,
                                &quot;DEPARTMENT OF CLINICAL LABORATORY, BEIJING GENERAL HOSPITAL, BEIJING, CHINA.&quot;,
                                &quot;LABORATORY OF MOLECULAR BIOLOGY, ISTITUTO ORTOPEDICO, MILAN, ITALY.&quot;,
                                &quot;DEPARTMENT OF AGRICULTURE, BERLIN INSTITUTE OF HEALTH, BERLIN&quot;,
                                &quot;INSTITUTE OF LABORATORY MEDICINE, UNIVERSITY HOSPITAL, MUNICH, GERMANY.&quot;,
                                &quot;DEPARTMENT OF CLINICAL PATHOLOGY, MAHIDOL UNIVERSITY, BANGKOK, THAILAND.&quot;,
                                &quot;DEPARTMENT OF BIOLOGY, WASEDA UNIVERSITY, TOKYO, JAPAN&quot;,
                                &quot;DEPARTMENT OF MOLECULAR BIOLOGY, MINISTRY OF HEALTH, TEHRAN, IRAN.&quot;,
                                &quot;LABORATORY OF CARDIOVASCULAR DISEASE, FUWAI HOSPITAL, BEIJING, CHINA.&quot;))
</code></pre>
<p>I have now a second dataframe ‘dfb’ that contains a list of cities and corresponding country, some of which are present in 'dfa':</p>
<pre><code>dfb &lt;- data.frame(city=c(&quot;AGRI&quot;,&quot;AMSTERDAM&quot;,&quot;ATHENS&quot;,&quot;AUCKLAND&quot;,&quot;BUENOS AIRES&quot;,&quot;BEIJING&quot;,&quot;BAGHDAD&quot;,&quot;BANGKOK&quot;,&quot;BERLIN&quot;,&quot;BUDAPEST&quot;),
                  country=c(&quot;TURKEY&quot;,&quot;NETHERLANDS&quot;,&quot;GREECE&quot;,&quot;NEW ZEALAND&quot;,&quot;ARGENTINA&quot;,&quot;CHINA&quot;,&quot;IRAQ&quot;,&quot;THAILAND&quot;,&quot;GERMANY&quot;,&quot;HUNGARY&quot;))
</code></pre>
<p>How can I add cities and corresponding countries in two new columns only for cities that are present in both ‘dfa’ and ‘dfb’ (and even when the country is missing, as for BAGHDAD and BERLIN)?</p>
<p>NB: the goal is to add <strong>full</strong> city names but <strong>not part</strong> of them. Below in row 7, an example of what is not wanted: the AGRI city of TURKEY is inappropriately associated with BERLIN because this row includes the 'AGRICULTURE' word.</p>
<p>Is there a simple way to do that, ideally using dplyr?</p>
<pre><code>    affiliation      city     country
1      DEPARTMENT OF PHARMACY, AMSTERDAM UNIVERSITY, AMSTERDAM, THE NETHERLANDS AMSTERDAM NETHERLANDS
2  DEPARTMENT OF BIOCHEMISTRY, LADY HARDINGE MEDICAL COLLEGE, NEW DELHI, INDIA.      &lt;NA&gt;        &lt;NA&gt;
3      DEPARTMENT OF PATHOLOGY, CHILDREN'S HOSPITAL, LOS ANGELES, UNITED STATES      &lt;NA&gt;        &lt;NA&gt;
4                 COLLEGE OF EDUCATION FOR PURE SCIENCE, UNIVERSITY OF BAGHDAD.   BAGHDAD        IRAQ
5  DEPARTMENT OF CLINICAL LABORATORY, BEIJING GENERAL HOSPITAL, BEIJING, CHINA.   BEIJING       CHINA
6           LABORATORY OF MOLECULAR BIOLOGY, ISTITUTO ORTOPEDICO, MILAN, ITALY.      &lt;NA&gt;        &lt;NA&gt;
7                 DEPARTMENT OF AGRICULTURE, BERLIN INSTITUTE OF HEALTH, BERLIN      AGRI      TURKEY
8       INSTITUTE OF LABORATORY MEDICINE, UNIVERSITY HOSPITAL, MUNICH, GERMANY.      &lt;NA&gt;        &lt;NA&gt;
9      DEPARTMENT OF CLINICAL PATHOLOGY, MAHIDOL UNIVERSITY, BANGKOK, THAILAND.   BANGKOK    THAILAND
10                       DEPARTMENT OF BIOLOGY, WASEDA UNIVERSITY, TOKYO, JAPAN      &lt;NA&gt;        &lt;NA&gt;
11           DEPARTMENT OF MOLECULAR BIOLOGY, MINISTRY OF HEALTH, TEHRAN, IRAN.      &lt;NA&gt;        &lt;NA&gt;
12        LABORATORY OF CARDIOVASCULAR DISEASE, FUWAI HOSPITAL, BEIJING, CHINA.   BEIJING       CHINA
</code></pre>
","r, dplyr, text-mining, country, city","<p>This approach accounts for the possibility that an affiliation could match more than one city name.</p>
<pre><code>library(tidyverse)

dfa %&gt;% 
  mutate(city = map(affiliation, ~ str_extract(.x, dfb$city))) %&gt;% 
  unnest(cols = c(city)) %&gt;% 
  group_by(affiliation) %&gt;% 
  mutate(nmatches = sum(!is.na(city))) %&gt;% 
  filter((nmatches &gt; 0 &amp; !is.na(city)) | (nmatches == 0 &amp; row_number() == 1)) %&gt;%
  ungroup() %&gt;% 
  left_join(dfb, by = &quot;city&quot;) %&gt;% 
  mutate(country_match = str_detect(affiliation, country))

# A tibble: 12 x 5
   affiliation              city   nmatches country country_match
   &lt;chr&gt;                    &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;lgl&gt;        
 1 DEPARTMENT OF PHARMACY,… AMSTE…        1 NETHER… TRUE         
 2 DEPARTMENT OF BIOCHEMIS… NA            0 NA      NA           
 3 DEPARTMENT OF PATHOLOGY… NA            0 NA      NA           
 4 COLLEGE OF EDUCATION FO… BAGHD…        1 IRAQ    FALSE        
 5 DEPARTMENT OF CLINICAL … BEIJI…        1 CHINA   TRUE         
 6 LABORATORY OF MOLECULAR… NA            0 NA      NA           
 7 BERLIN INSTITUTE OF HEA… BERLIN        1 GERMANY FALSE        
 8 INSTITUTE OF LABORATORY… NA            0 NA      NA           
 9 DEPARTMENT OF CLINICAL … BANGK…        1 THAILA… TRUE         
10 DEPARTMENT OF BIOLOGY, … NA            0 NA      NA           
11 DEPARTMENT OF MOLECULAR… NA            0 NA      NA           
12 LABORATORY OF CARDIOVAS… BEIJI…        1 CHINA   TRUE   
</code></pre>
<p>You could then double-check cases with 1 <code>nmatches</code> but <code>country_match == F</code>, and when there are 2 or more <code>nmatches</code> you can keep the one with <code>country_match == T</code>.</p>
",1,0,352,2021-06-22 22:23:02,https://stackoverflow.com/questions/68091342/find-city-names-within-affiliations-and-add-them-with-their-corresponding-countr
How to match terms except those after specific word using spacy matcher?,"<p>I have multiple research papers containing the terms: &quot;social media platform&quot; and &quot;media platform&quot;.</p>
<p>I want to match all &quot;media platform&quot; terms without touching the &quot;social media platform&quot; terms.</p>
<p>Here is my example text:</p>
<p>&quot;The social media platform is great. It is a great media platform.&quot;</p>
<p>I just want to match &quot;media platform in the second sentence leaving out the one in the first:</p>
<p>&quot;The social media platform is great. it is a great <strong>media platform</strong>.&quot;</p>
<p>The pattern I use so far is not quite working because it also matches the word &quot;great&quot;..</p>
<p>&quot;The social media platform is great. it is a <strong>great media platform</strong>.&quot;</p>
<p>Here is my pattern:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>pattern = [{'LOWER': 'social', 'OP': '!'},
           {'LOWER': 'media'},
           {'LOWER': 'platform'}]</code></pre>
</div>
</div>
</p>
<p>Is it even possible to solve this task with spacy matcher? Or is there a possibility to use regex?</p>
","python, regex, spacy, text-mining","<p>You can't get exactly what you want with the spaCy matcher because of the way the negation op works. You should just use a function to filter matches, something like this:</p>
<pre><code>matches = ... matcher output ...
final = [mm for mm in matches if mm.start == 0 or mm.doc[mm.start-1].text != &quot;social&quot;]
</code></pre>
<p>There is no reason to use regex for this problem.</p>
",1,0,772,2021-06-24 17:06:05,https://stackoverflow.com/questions/68120015/how-to-match-terms-except-those-after-specific-word-using-spacy-matcher
How to extract multi-word units from a column using R?,"<p>my data looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Topic</th>
<th>Measure</th>
</tr>
</thead>
<tbody>
<tr>
<td>climate change</td>
<td>reduce emissions</td>
</tr>
<tr>
<td>pandemic</td>
<td>vaccination</td>
</tr>
<tr>
<td>charity</td>
<td>call for donations</td>
</tr>
</tbody>
</table>
</div>
<p>Now I would like to extract all multi-word units (MWU) within one column, i.e.:</p>
<p>topic_mwu&lt;-c(&quot;climate change&quot;)</p>
<p>measure_mwu&lt;-c(&quot;reduce emission&quot;,&quot;call for donations&quot;)</p>
<p>Is there a function in R to extract these MWU automatically? Basically I only have to identify those entries including at least one whitespace, so I am thinking of an RegEx - hack..</p>
<p>I would very much appreciate your help!</p>
","r, regex, text-mining","<p>The code below should work:</p>
<pre><code>#your dataframe
dt &lt;- matrix(c(&quot;reduce emission&quot;, &quot;call for donations&quot;, &quot;pandemic&quot;, &quot;climate change&quot;, &quot;donations&quot;, &quot;charity&quot;), ncol =2)

#make it a vector
dt &lt;- as.vector(dt)

#if the table is very big, you can do unique() to remove duplicates
dt &lt;- unique(dt)

#get the MWU
dt[unlist(lapply(strsplit(dt,split = &quot; &quot;), length)) &gt; 1]
</code></pre>
<p>Is this what you were looking for?</p>
",0,0,33,2021-07-05 09:29:10,https://stackoverflow.com/questions/68253706/how-to-extract-multi-word-units-from-a-column-using-r
Extracting first word after a specific expression in R,"<p>I have a column that contains thousands of descriptions like this (example) :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Building a hospital in the city of LA, USA</td>
</tr>
<tr>
<td>Building a school in the city of NYC, USA</td>
</tr>
<tr>
<td>Building shops in the city of Chicago, USA</td>
</tr>
</tbody>
</table>
</div>
<p>I'd like to create a column with the first word after &quot;city of&quot;, like that :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Description</th>
<th>City</th>
</tr>
</thead>
<tbody>
<tr>
<td>Building a hospital in the city of LA, USA</td>
<td>LA</td>
</tr>
<tr>
<td>Building a school in the city of NYC, USA</td>
<td>NYC</td>
</tr>
<tr>
<td>Building shops in the city of Chicago, USA</td>
<td>Chicago</td>
</tr>
</tbody>
</table>
</div>
<p>I tried with the following code after seeing this topic <a href=""https://stackoverflow.com/questions/66141475/extracting-string-after-specific-word"">Extracting string after specific word</a>, but my column is only filled with missing values</p>
<pre><code>library(stringr)

df$city &lt;- data.frame(str_extract(df$Description, &quot;(?&lt;=city of:\\s)[^;]+&quot;))

df$city &lt;- data.frame(str_extract(df$Description, &quot;(?&lt;=of:\\s)[^;]+&quot;))

</code></pre>
<p>I took a look at the dput() and the output is the same than the descriptions i see in the dataframe directly.</p>
","r, dataframe, text-mining, stringr","<h2 id=""solution-esvk"">Solution</h2>
<p>This should make the trick for the data you showed:</p>
<pre class=""lang-r prettyprint-override""><code>df$city &lt;- str_extract(df$Description, &quot;(?&lt;=city of )(\\w+)&quot;)

df
#&gt;                                  Description    city
#&gt; 1 Building a hospital in the city of LA, USA      LA
#&gt; 2  Building a school in the city of NYC, USA     NYC
#&gt; 3 Building shops in the city of Chicago, USA Chicago
</code></pre>
<hr />
<h2 id=""alternative-mk73"">Alternative</h2>
<p>However, in case you want the whole string till the first comma (for example in case of cities with a blank in the name), you can go with:</p>
<pre class=""lang-r prettyprint-override""><code>df$city &lt;- str_extract(df$Description, &quot;(?&lt;=city of )(.+)(?=,)&quot;)
</code></pre>
<p>Check out the following example:</p>
<pre class=""lang-r prettyprint-override""><code>df &lt;- data.frame(Description = c(&quot;Building a hospital in the city of LA, USA&quot;,
                                 &quot;Building a school in the city of NYC, USA&quot;,
                                 &quot;Building shops in the city of Chicago, USA&quot;,
                                 &quot;Building a church in the city of Salt Lake City, USA&quot;))

str_extract(df$Description, &quot;(?&lt;=the city of )(\\w+)&quot;)
#&gt; [1] &quot;LA&quot;      &quot;NYC&quot;     &quot;Chicago&quot; &quot;Salt&quot;   

str_extract(df$Description, &quot;(?&lt;=the city of )(.+)(?=,)&quot;)
#&gt; [1] &quot;LA&quot;             &quot;NYC&quot;            &quot;Chicago&quot;        &quot;Salt Lake City&quot;
</code></pre>
<hr />
<h2 id=""documentation-ulsa"">Documentation</h2>
<p>Check out <code>?regex</code>:</p>
<blockquote>
<p>Patterns (?=...) and (?!...) are zero-width positive and negative
lookahead assertions: they match if an attempt to match the ...
forward from the current position would succeed (or not), but use up
no characters in the string being processed. Patterns (?&lt;=...) and
(?&lt;!...) are the lookbehind equivalents: they do not allow repetition
quantifiers nor \C in ....</p>
</blockquote>
",3,1,1131,2021-07-16 15:27:36,https://stackoverflow.com/questions/68411606/extracting-first-word-after-a-specific-expression-in-r
Partial string merge between 2 large datasets on R,"<p>I have two dataframes, the 1st one contains about 900K observations and 2 columns :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>COMPANY</th>
</tr>
</thead>
<tbody>
<tr>
<td>AD8.OSZ.23490</td>
<td>Company1</td>
</tr>
<tr>
<td>AD8.OSZ.18903</td>
<td>Company2</td>
</tr>
<tr>
<td>AD8.OSZ.90126</td>
<td>Company3</td>
</tr>
</tbody>
</table>
</div>
<p>The second ones contains about 130k observations and also 2 columns, but the ID format is different (but not all observations are in the same form, for some there is no AD8.OSZ. for instance).</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Client_Since</th>
</tr>
</thead>
<tbody>
<tr>
<td>desr-j50q02-AD8.OSZ.23490</td>
<td>1981</td>
</tr>
<tr>
<td>desr-j50q02-AD8.OSZ.18903</td>
<td>2003</td>
</tr>
<tr>
<td>desr-j50q02-AD8.OSZ.90126</td>
<td>2018</td>
</tr>
</tbody>
</table>
</div>
<p><strong>DESIRED OUTPUT</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Full_ID</th>
<th>Client_Since</th>
<th>Company</th>
</tr>
</thead>
<tbody>
<tr>
<td>desr-j50q02-AD8.OSZ.23490</td>
<td>1981</td>
<td>Company1</td>
</tr>
<tr>
<td>desr-j50q02-AD8.OSZ.18903</td>
<td>2003</td>
<td>Company2</td>
</tr>
<tr>
<td>desr-j50q02-AD8.OSZ.90126</td>
<td>2018</td>
<td>Company3</td>
</tr>
</tbody>
</table>
</div>
<p>I tried 2 codes for my left join (i want to keep all the 130k obs) :</p>
<pre><code>#1st 

library(fuzzyjoin)

df3 &lt;- df %&gt;% regex_left_join(df2, by = c(Full_ID = &quot;ID&quot;))

#2nd code

library(stringr)

df3 &lt;- df %&gt;% fuzzy_left_join(df2, by = c(&quot;Full_ID&quot; = &quot;ID&quot;), match_fun = str_detect)

</code></pre>
<blockquote>
<p>Error : memory vectors exhausted (limit reached ?)</p>
</blockquote>
<p>I think that this code is too weak for the datasets i have / not appropriate for my MacbookAir. I did the manipulation found here : <a href=""https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached"">R on MacOS Error: vector memory exhausted (limit reached?)</a> but it didn't change anything.</p>
<p>I read about &quot;parallelizing&quot; the use of R (<a href=""https://datasquad.at.sites.carleton.edu/data/storage-design/dealing-with-a-vector-memory-exhausted-error-in-r/"" rel=""nofollow noreferrer"">https://datasquad.at.sites.carleton.edu/data/storage-design/dealing-with-a-vector-memory-exhausted-error-in-r/</a>) but i really don't understand how to use mclapply with my join command.</p>
<p>I also looked at that topic : <a href=""https://stackoverflow.com/questions/28975268/partial-string-merge-r-large-dataset"">Partial string merge R large dataset</a> but is not exactly the same case as me.</p>
","r, string, dataframe, merge, text-mining","<p>There is another way to think of it where from the sample you have shared you always seek IDs after the last period, hence you can create a new column with text after last period and join using it.</p>
<p>Below is example of how you can do that;</p>
<pre><code># Reading required libraries
library(dplyr)
library(stringr)

# Create sample dataframes
df1 &lt;-
  data.frame(ID = c(&quot;AD8.OSZ.23490&quot;, &quot;AD8.OSZ.18903&quot;, &quot;AD8.OSZ.90126&quot;),
             COMPANY = c(&quot;Company1&quot;, &quot;Company2&quot;, &quot;Company3&quot;))

df2 &lt;-
  data.frame(ID = c(&quot;desr-j50q02-AD8.OSZ.23490&quot;, &quot;desr-j50q02-AD8.OSZ.18903&quot;, &quot;desr-j50q02-AD8.OSZ.90126&quot;),
             Client_Since = c(&quot;1981&quot;, &quot;2003&quot;, &quot;2018&quot;))

# Modify first dataframe
mod_df1 &lt;-
  df1 %&gt;%
  # Get characters after last period
  mutate(MOD_ID = sub('.*\\.', '', ID))

# Modify second dataframe
mod_df2 &lt;-
  df2 %&gt;%
  # Get characters after last period
  mutate(MOD_ID = sub('.*\\.', '', ID))

# Join tables
mod_df1 %&gt;%
  left_join(mod_df2, by = c(&quot;MOD_ID&quot;))
</code></pre>
",2,0,71,2021-07-19 20:44:18,https://stackoverflow.com/questions/68446820/partial-string-merge-between-2-large-datasets-on-r
"R: Text Mining, create list of words per document","<p>I am reading in the text from a number of PDFs in a directory.
Then, I split these texts into single words (tokens) using the <code>tidytext::unnest_tokens()</code>-function.
Can someone please tell me, how I can add an additional column to the <code>test</code>-tibble with the name of the file each word comes from?</p>
<pre><code>library(pdftools)
library(tidyverse)
library(tidytext)

files &lt;- list.files(pattern = &quot;pdf$&quot;)
content &lt;- lapply(files, pdf_text)
list &lt;- unlist(content, recursive = TRUE, use.names = TRUE)
df = data.frame(text = list)

test &lt;- df %&gt;% tidytext::unnest_tokens(word, text)
</code></pre>
","r, tidyverse, text-mining, tidytext","<p>You can try the following. Instead of using <code>unlist</code> with all the files, instead pass the entire list of files to <code>map_df</code> from <code>purrr</code>. Then, you can add a column with <code>filename</code> along with the <code>word</code> column.</p>
<pre><code>library(pdftools)
library(tidyverse)
library(tidytext)

files &lt;- list.files(pattern = &quot;pdf$&quot;)

map_df(files, ~ data.frame(txt = pdf_text(.x)) %&gt;%
         mutate(filename = .x) %&gt;%
         unnest_tokens(word, txt))
</code></pre>
",2,1,460,2021-08-05 23:37:47,https://stackoverflow.com/questions/68674581/r-text-mining-create-list-of-words-per-document
Some words won&#39;t be stemmed using tm (&quot;easier&quot; or &quot;easiest&quot;),"<p>I have large questionaire dataset where some of the features need to be stemmed, with the goal being to assign a topic to each response. However, I'm having trouble stemming some words using the package <code>tm</code>.</p>
<p>Here is a reproducible (simplified) example:</p>
<pre><code>library(tm)

# Words that need to be stemmed
test_vec &lt;- c(&quot;easier&quot;,&quot;easy&quot;,&quot;easiest&quot;,&quot;closest&quot;,&quot;close&quot;,&quot;closer&quot;,&quot;near&quot;,&quot;nearest&quot;)

# Preprocessing function to clean corpus
# Note that, this is my full pipeline, but only the last command will be used in this case example
clean_corpus&lt;- function(corpus){
  corpus &lt;- tm_map(corpus, stripWhitespace)
  corpus &lt;- tm_map(corpus, removePunctuation)
  corpus &lt;- tm_map(corpus, removeNumbers)
  corpus &lt;- tm_map(corpus, content_transformer(tolower))
  corpus &lt;- tm_map(corpus, removeWords, stopwords(&quot;en&quot;))
  corpus &lt;- tm_map(corpus,stemDocument)
    return(corpus)
}

# Create corpus with test_vec
test_corpus &lt;- VCorpus(VectorSource(test_vec))
# Apply cleaning
test_corpus &lt;- clean_corpus(test_corpus)

# Print out stemmed values
for(i in 1:length(test_corpus)){
  print(test_corpus[[i]]$content)
}

[1] &quot;easier&quot;
[2] &quot;easi&quot;
[3] &quot;easiest&quot;
[4] &quot;closest&quot;
[5] &quot;close&quot;
[6] &quot;closer&quot;
[7] &quot;near&quot;
[8] &quot;nearest&quot;
</code></pre>
<p><strong>Question 1</strong>
Why isn't <code> [1] &quot;easier&quot;</code> and <code>[3] &quot;easiest&quot; </code> stemmed to be <code>&quot;easi&quot;</code> (like <code>&quot;easy&quot;</code> has been). Similarly, why isn't <code>&quot;close&quot;</code> or <code>&quot;near&quot;</code> stemmed. Am I missing something?</p>
<p><strong>Question 2</strong>
This is a side question, but is there a way to relate words like <code>&quot;close&quot;</code> and <code>&quot;near&quot;</code> from a dictionary that would be able to verify that these are synonyms. If they are synonyms, all instances of <code>&quot;near&quot;</code> will then be changed to <code>&quot;close&quot;</code>, for example.</p>
","r, text-mining, tm","<p>There are multiple stemmers (quick overview <a href=""https://www.geeksforgeeks.org/introduction-to-stemming/"" rel=""nofollow noreferrer"">here</a>), but porter is used the most. Python also has Lancaster stemming which would return the following based on your <code>test_vec</code>:</p>
<pre><code>easier easy
easy easy
easiest easiest
closest closest
close clos
closer clos
near near
nearest nearest
</code></pre>
<p>But still there are issues as <code>iest</code>, is not shortened.</p>
<p>But you could also use lemmatization which would return the following:</p>
<pre><code>library(textstem)
lemmatize_words(test_vec)
&quot;easy&quot;  &quot;easy&quot;  &quot;easy&quot;  &quot;close&quot; &quot;close&quot; &quot;close&quot; &quot;near&quot;  &quot;near&quot; 
</code></pre>
<p>For topic assignment lemmatising might be preferred to stemming because it groups better. But you need to be aware of the differences between both.</p>
<p>Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.
<a href=""https://en.wikipedia.org/wiki/Lemmatisation"" rel=""nofollow noreferrer"">Wikipedia: lemmatisation</a></p>
<p>Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.
<a href=""https://en.wikipedia.org/wiki/Stemming"" rel=""nofollow noreferrer"">Wikipedia: stemming</a></p>
<p>As for your second question, there is a package called <code>syn</code> (only for English), which contains all the synonyms, but it will create a list of all of them and for &quot;close&quot; or &quot;near&quot; that is a very long list.
Or package <code>qdap</code>, that also has a synonym function.</p>
",3,2,79,2021-08-07 17:43:04,https://stackoverflow.com/questions/68694785/some-words-wont-be-stemmed-using-tm-easier-or-easiest
finding associations in dataset with list of string data in each cell in R,"<p>I am looking for finding a method to find the association between words in the table (or list). In each cell of the table, I have several words separated by &quot;;&quot;.</p>
<p>lets say I have a table as below; some words are 'af' or 'aa' belong to one cell.</p>
<pre><code>df&lt;-read.table(text=&quot;
A           B            C           D
af;aa;az    bf;bb        c;cc       df;dd
aa;az       bf;bc        c          dc;dd
ah;al;aa    bb           c;cd       dd
af;aa       bf           cc         dd&quot;,header=T,stringsAsFactors = F)
</code></pre>
<p>I want to find associations between all words in the entire dataset, between cells(not interested in within cell association). for example, how many times <code>aa</code> and <code>dd</code> appear in one row, or show me which words have the highest association (e.g. aa with bb, aa with dd,....).</p>
<p>expected output: (the numbers can be inaccurate and association rep does not have be shown with '--')</p>
<pre><code>2 pairs association (numbers can be counts, probability or normalized association)
association    number of associations 
aa--dd          3
aa--c           3
bb--dd          2
...
3 pairs association
aa--bb--dd      3
aa--bb--c       3
...

4 pairs association
aa--bb--c--dd   2
aa--bf--c--dd   2
...
</code></pre>
<p>can you help me to implement it in R?
Tx</p>
","r, associations, text-mining","<p>I am not sure if you have something like the approach below in mind. It is basically a custom function which we use in a nested <code>purrr::map</code> call. The outer call loops over the number of pairs: <code>2</code>,<code>3</code>, <code>4</code> and the inner call uses <code>combn</code> to create all possible combinations as input and uses the custom function to create the desired output.</p>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)

count_pairs &lt;- function(x) {
s &lt;- seq(x)
 df[, x] %&gt;% 
    reduce(s, separate_rows, .init = ., sep = &quot;;&quot;)  
    group_by(across()) %&gt;% 
    count() %&gt;% 
    rename(set_names(s))
}

map(2:4,
    ~ map_dfr(combn(1:4, .x, simplify = FALSE),
                    count_pairs) %&gt;% arrange(-n))
#&gt; [[1]]
#&gt; # A tibble: 50 x 3
#&gt; # Groups:   1, 2 [50]
#&gt;    `1`   `2`       n
#&gt;    &lt;chr&gt; &lt;chr&gt; &lt;int&gt;
#&gt;  1 aa    dd        4
#&gt;  2 aa    bf        3
#&gt;  3 aa    c         3
#&gt;  4 bf    dd        3
#&gt;  5 c     dd        3
#&gt;  6 aa    bb        2
#&gt;  7 af    bf        2
#&gt;  8 az    bf        2
#&gt;  9 aa    cc        2
#&gt; 10 af    cc        2
#&gt; # ... with 40 more rows
#&gt; 
#&gt; [[2]]
#&gt; # A tibble: 70 x 4
#&gt; # Groups:   1, 2, 3 [70]
#&gt;    `1`   `2`   `3`       n
#&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;
#&gt;  1 aa    bf    dd        3
#&gt;  2 aa    c     dd        3
#&gt;  3 aa    bb    c         2
#&gt;  4 aa    bf    c         2
#&gt;  5 aa    bf    cc        2
#&gt;  6 af    bf    cc        2
#&gt;  7 az    bf    c         2
#&gt;  8 aa    bb    dd        2
#&gt;  9 af    bf    dd        2
#&gt; 10 az    bf    dd        2
#&gt; # ... with 60 more rows
#&gt; 
#&gt; [[3]]
#&gt; # A tibble: 35 x 5
#&gt; # Groups:   1, 2, 3, 4 [35]
#&gt;    `1`   `2`   `3`   `4`       n
#&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;
#&gt;  1 aa    bb    c     dd        2
#&gt;  2 aa    bf    c     dd        2
#&gt;  3 aa    bf    cc    dd        2
#&gt;  4 af    bf    cc    dd        2
#&gt;  5 az    bf    c     dd        2
#&gt;  6 aa    bb    c     df        1
#&gt;  7 aa    bb    cc    dd        1
#&gt;  8 aa    bb    cc    df        1
#&gt;  9 aa    bb    cd    dd        1
#&gt; 10 aa    bc    c     dc        1
#&gt; # ... with 25 more rows

# the data
df&lt;-read.table(text=&quot;
A           B            C           D
af;aa;az    bf;bb        c;cc       df;dd
aa;az       bf;bc        c          dc;dd
ah;al;aa    bb           c;cd       dd
af;aa       bf           cc         dd&quot;,header=T,stringsAsFactors = F)

</code></pre>
<p><sup>Created on 2021-08-11 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v2.0.1)</sup></p>
",2,3,194,2021-08-08 20:27:18,https://stackoverflow.com/questions/68704499/finding-associations-in-dataset-with-list-of-string-data-in-each-cell-in-r
Grab text between two lines,"<p>I was just learning and had a problem working with files.
I have a method that has two inputs, one at the beginning of the line (lineStart) I want and the other at the end of the line (lineEnd)
I need  method that extract between these two numbers for me and write on file .</p>
<p><strong>ex )</strong> <strong>lineStart = 20 , lineEnd = 90, in output Must be = 21-89 line of txt file.</strong></p>
<pre><code>          string[] lines = File.ReadAllLines(@&quot;&quot;);

        int lineStart = 0;
        foreach (string line0 in lines)
        {
            lineStart++;
            if (line0.IndexOf(&quot;target1&quot;) &gt; -1)
            {
                Console.Write(lineStart + &quot;\n&quot;);
            }
        }
           int lineEnd = 0;

         foreach (string line1 in lines)
            {
                lineEnd++;
                if (line1.IndexOf(&quot;target2&quot;) &gt; -1)
                {
                    Console.Write(lineEnd);
                }
            }

        // method grabText(lineStart,lineEnd){}

enter code here
</code></pre>
","c#, text-mining","<p>If your text file is huge, don't read it into memory. Don't look for indexes either, just process it line by line:</p>
<pre><code>bool writing = false;

using var sw = File.CreateText(@&quot;C:\some\path\to.txt&quot;);

foreach(var line in File.ReadLines(...)){ //don't use ReadAllInes, use ReadLines - it's incremental and burns little memory

  if(!writing &amp;&amp; line.Contains(&quot;target1&quot;)){
    writing = true; //start writing
    continue; //don't write this line
  }

  if(writing){
    if(line.Contains(&quot;target2&quot;))
      break; //exit loop without writing this line
    sw.WriteLine(line);
  }
}
</code></pre>
",0,0,765,2021-08-10 07:09:42,https://stackoverflow.com/questions/68722390/grab-text-between-two-lines
quanteda collocations and lemmatization,"<p>I am using the <strong>Quanteda suite of packages</strong> to preprocess some text data. I want to incorporate collocations as features and decided to use the <strong>textstat_collocations</strong> function. According to the documentation and I quote:</p>
<p>&quot;<em>The tokens object .  . . . <strong>While identifying collocations for tokens objects is supported, you will get better results with character or corpus objects</strong> due to relatively imperfect detection of sentence boundaries from texts already tokenized.</em>&quot;</p>
<p>This makes perfect sense, so here goes:</p>
<pre><code>library(dplyr)
library(tibble)
library(quanteda)
library(quanteda.textstats)

# Some sample data and lemmas
df= c(&quot;this column has a lot of missing data, 50% almost!&quot;,
                   &quot;I am interested in missing data problems&quot;,
                   &quot;missing data is a headache&quot;,
                   &quot;how do you handle missing data?&quot;)

lemmas &lt;- data.frame() %&gt;%
    rbind(c(&quot;missing&quot;, &quot;miss&quot;)) %&gt;%
    rbind(c(&quot;data&quot;, &quot;datum&quot;)) %&gt;%
    `colnames&lt;-`(c(&quot;inflected_form&quot;, &quot;lemma&quot;))
</code></pre>
<p>(1) Generate collocations using the corpus object:</p>
<pre><code>txtCorpus = corpus(df)
docvars(txtCorpus)$text &lt;- as.character(txtCorpus)
myPhrases = textstat_collocations(txtCorpus, tolower = FALSE)
</code></pre>
<p>(2) preprocess text and identify collocations and lemmatize for downstream tasks.</p>
<pre><code># I used a blank space as concatenator and the phrase function as explained in the documentation and I followed the multi multi substitution example in the documentation
# https://quanteda.io/reference/tokens_replace.html
txtTokens = tokens(txtCorpus, remove_numbers = TRUE, remove_punct = TRUE, 
                               remove_symbols = TRUE, remove_separators = TRUE) %&gt;%
    tokens_tolower() %&gt;%
    tokens_compound(pattern = phrase(myPhrases$collocation), concatenator = &quot; &quot;) %&gt;%
    tokens_replace(pattern=phrase(c(lemmas$inflected_form)), replacement=phrase(c(lemmas$lemma)))
</code></pre>
<p>(3) test results</p>
<pre><code># Create dtm
dtm = dfm(txtTokens, remove_padding = TRUE)

# pull features
dfm_feat = as.data.frame(featfreq(dtm)) %&gt;%
    rownames_to_column(var=&quot;feature&quot;) %&gt;%
    `colnames&lt;-`(c(&quot;feature&quot;, &quot;count&quot;))

dfm_feat
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>feature</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>this</td>
<td>1</td>
</tr>
<tr>
<td>column</td>
<td>1</td>
</tr>
<tr>
<td>has</td>
<td>1</td>
</tr>
<tr>
<td>a</td>
<td>2</td>
</tr>
<tr>
<td>lot</td>
<td>1</td>
</tr>
<tr>
<td>of</td>
<td>1</td>
</tr>
<tr>
<td>almost</td>
<td>1</td>
</tr>
<tr>
<td>i</td>
<td>2</td>
</tr>
<tr>
<td>am</td>
<td>1</td>
</tr>
<tr>
<td>interested</td>
<td>1</td>
</tr>
<tr>
<td>in</td>
<td>1</td>
</tr>
<tr>
<td>problems</td>
<td>1</td>
</tr>
<tr>
<td>is</td>
<td>1</td>
</tr>
<tr>
<td>headache</td>
<td>1</td>
</tr>
<tr>
<td>how</td>
<td>1</td>
</tr>
<tr>
<td>do</td>
<td>1</td>
</tr>
<tr>
<td>you</td>
<td>1</td>
</tr>
<tr>
<td>handle</td>
<td>1</td>
</tr>
<tr>
<td><strong>missing data</strong></td>
<td>4</td>
</tr>
</tbody>
</table>
</div>
<p>&quot;<em>missing data</em>&quot; should be &quot;<em>miss datum</em>&quot;.</p>
<p>This is only works if each document in df is a single word. I can make the process work if I generate my collocations using a token object from the get-go but that's not what I want.</p>
","r, text-mining, quanteda, collocation","<p>The problem is that you have already compounded the elements of the collocations into a single &quot;token&quot; containing a space, but by supplying the <code>phrase()</code> wrapper in <code>tokens_compound()</code>, you are telling <code>tokens_replace()</code> to look for two sequential tokens, not the one with a space.</p>
<p>The way to get what you want is by making the lemmatised replacement match the collocation.</p>
<pre class=""lang-r prettyprint-override""><code>phrase_lemmas &lt;- data.frame(
  inflected_form = &quot;missing data&quot;,
  lemma = &quot;miss datum&quot;
)
tokens_replace(txtTokens, phrase_lemmas$inflected_form, phrase_lemmas$lemma)
## Tokens consisting of 4 documents and 1 docvar.
## text1 :
## [1] &quot;this&quot;       &quot;column&quot;     &quot;has&quot;        &quot;a&quot;          &quot;lot&quot;       
## [6] &quot;of&quot;         &quot;miss datum&quot; &quot;almost&quot;    
## 
## text2 :
## [1] &quot;i&quot;          &quot;am&quot;         &quot;interested&quot; &quot;in&quot;         &quot;miss datum&quot;
## [6] &quot;problems&quot;  
## 
## text3 :
## [1] &quot;miss datum&quot; &quot;is&quot;         &quot;a&quot;          &quot;headache&quot;  
## 
## text4 :
## [1] &quot;how&quot;        &quot;do&quot;         &quot;you&quot;        &quot;handle&quot;     &quot;miss datum&quot;
</code></pre>
<p>Alternatives would be to use <code>tokens_lookup()</code> on uncompounded tokens directly, if you have a fixed listing of sequences you want to match to lemmatised sequences.  E.g.,</p>
<pre class=""lang-r prettyprint-override""><code>tokens(txtCorpus) %&gt;%
  tokens_lookup(dictionary(list(&quot;miss datum&quot; = &quot;missing data&quot;)),
    exclusive = FALSE, capkeys = FALSE
  )
## Tokens consisting of 4 documents and 1 docvar.
## text1 :
##  [1] &quot;this&quot;       &quot;column&quot;     &quot;has&quot;        &quot;a&quot;          &quot;lot&quot;       
##  [6] &quot;of&quot;         &quot;miss datum&quot; &quot;,&quot;          &quot;50&quot;         &quot;%&quot;         
## [11] &quot;almost&quot;     &quot;!&quot;         
## 
## text2 :
## [1] &quot;I&quot;          &quot;am&quot;         &quot;interested&quot; &quot;in&quot;         &quot;miss datum&quot;
## [6] &quot;problems&quot;  
## 
## text3 :
## [1] &quot;miss datum&quot; &quot;is&quot;         &quot;a&quot;          &quot;headache&quot;  
## 
## text4 :
## [1] &quot;how&quot;        &quot;do&quot;         &quot;you&quot;        &quot;handle&quot;     &quot;miss datum&quot;
## [6] &quot;?&quot;
</code></pre>
",2,1,431,2021-09-03 23:49:32,https://stackoverflow.com/questions/69051478/quanteda-collocations-and-lemmatization
Get length statistics over very large textual file,"<p>I have a very large file (1.5M rows), containing json dictionaries in each row.
Each row contains a parsed Wikipedia article.
For example</p>
<pre><code>{&quot;title&quot;: &quot;article title&quot;, &quot;summary&quot;: &quot;this is a summary of around 500 words&quot;, &quot;article&quot;: &quot;This is the whole article with more than 3K words&quot;}
{&quot;title&quot;: &quot;article2 title&quot;, &quot;summary2&quot;: &quot;this is another summary of around 500 words&quot;, &quot;article&quot;: &quot;This is another whole article with more than 3K words&quot;}
</code></pre>
<p>Note that the file is not itself a json.</p>
<p>I want to compute some statistics on these texts, e.g mean number of sentences, mean number of words, compression ratio etc. However, everything I try takes ages.
What is the fastest way to go with this? For reference, at the moment I am using spacy for word and sentence tokenization, but I am open to more approximate solutions e.g. using regex, if they are the only way.</p>
","performance, text, text-mining","<p>If you want to achieve high performance, then you should probably compute the lines in parallel using <strong>multiple threads</strong> and each line should extract the target metric using a <strong>SIMD-friendly code</strong>. It is also probably a good idea to simplify the parsing by using a <strong>specialized code</strong> working only on this problem and not general parsing tool (like regular expression, unless the target engine is capable of producing a very fast linear-time JIT-compiled efficient code).</p>
<p>For the multithreading part, this is certainly the easiest part since the computation appear to be mostly <em>embarrassingly parallel</em>. Each thread compute the target metrics on a chunks of lines and can then perform a parallel reduction (ie. sum) of the target metrics.</p>
<p>Each line can be parsed relatively quickly using <a href=""https://simdjson.org/"" rel=""nofollow noreferrer"">SimdJson</a>. Since the JSON documents are small and the structure appear to be simple and always the same, you can use a regular expression to search for <code>&quot;article&quot; *: *&quot;((?:[^&quot;\]+|\&quot;)*)&quot;</code> (note that you may need to escape the backslash regarding the language used). However, the best strategy is probably to parse yourself the JSON document to extract the wanted string much more efficiently for example by searching for some very specific key pattern/string like <code>&quot;</code> (with a SIMD-friendly loop) followed by <code>article</code> and then parse the rest using a more robust (but slower) method.</p>
<p>Similar strategies apply to count words. A fast over-approximation is to count the number of space directly followed by a character. The string encoding matters to speed up parsing as decoding UTF-8 string is generally pretty slow. One fast solution is to just discard non-ASCII character if the target language is English or mostly use ASCII characters. If this is not the case, then you can use some SIMD-aware UTF-8 decoding library (or hand written algorithm in the worst case). Working on small <strong>chunks</strong> of about 1KB can help to use the CPU cache more efficiently (and to auto-vectorize your code if you use a compiled native language like C or C++).</p>
<p>If you are not very familiar with <a href=""https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/"">SIMD instructions</a>, or low-level parsing strategies/algorithms (like <a href=""https://fr.wikipedia.org/wiki/Algorithme_de_Boyer-Moore"" rel=""nofollow noreferrer"">this</a> one), note that there are some <strong>fast parsing libraries</strong> to do basic operation efficiently like <a href=""https://github.com/intel/hyperscan"" rel=""nofollow noreferrer"">Hyperscan</a>.</p>
",0,0,27,2021-09-20 08:04:30,https://stackoverflow.com/questions/69250977/get-length-statistics-over-very-large-textual-file
pdftotext cannot read certain documents,"<p>I am currently using <code>pdftotext</code> to read PDF files into python using the following code</p>
<pre><code>import pdftotext
bill_full = []

with open('sample.pdf', &quot;rb&quot;) as f:
    pdf = pdftotext.PDF(f)
    bill = ''
    for page in pdf:
        bill = bill + page
    bill_full.append(bill)
</code></pre>
<p>The previous code seems to mostly work for my complete dataset, however I seem to encounter seemingly random errors. The previous code applied to the following PDF <a href=""https://legiscan.com/WI/text/AB649/id/456434/Wisconsin-2009-AB649-Introduced.pdf"" rel=""nofollow noreferrer"">https://legiscan.com/WI/text/AB649/id/456434/Wisconsin-2009-AB649-Introduced.pdf</a> results in</p>
<pre><code>2011 − 2012 LEGISLATURE LRB−1478/1 2011 SENATE BILL 27\r\n\r\n\r\n\r\n\r\n    March 1, 2011 − Introduced by JOINT COMMITTEE             ON   FINANCE. Referred to Joint\r\n        Committee on Finance.\r\n\r\n\r\n\r\n\r\n1   AN ACT         relating to: state finances and appropriations, constituting the\r\n\r\n2        executive budget act of the 2011 legislature.\r\n\r\n\r\n                      Analysis by the Legislative Reference Bureau\r\n                                        INTRODUCTION\r\n          
</code></pre>
<p>However when applied to others (eg. <a href=""https://legiscan.com/WI/text/AB408/id/423828/Wisconsin-2009-AB408-Introduced.pdf"" rel=""nofollow noreferrer"">https://legiscan.com/WI/text/AB408/id/423828/Wisconsin-2009-AB408-Introduced.pdf</a>) I get the following sequence of characters:</p>
<pre><code> \x08\x08\x11 \x06 \x08 \x08 \x1c\x18\x1a\x1b&quot;\x1c\x14#$!\x18 
</code></pre>
<p>What is different in these two PDFs? Ideally I would like to detect &quot;unreadable&quot; PDFs and drop them from my analysis.</p>
","python, text-mining, pdftotext","<p>To answer the direct question what is different is the CID data so lets just look at one object on each page 1.
here I pick the subject of your question, the first text that includes the numbers 1 2 9 0, letters L E G I S A T U R and the others in title
<a href=""https://i.sstatic.net/ryIQu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ryIQu.png"" alt=""enter image description here"" /></a></p>
<p>Here we see good or bad they are all stored as the same font type ??????+PSOwstnewcspsb, unclear to me but seems to be named along the lines PSO WeSTern NEW Courier ??? Bold</p>
<p>So why would there then be some working as mapped correctly by say OCR and some not ? That is an unknown to me and there is often no clear rhyme or reason, but we can see a difference in outcomes as the good one starts with printable space (/FirstChar 32/LastChar 116) whilst both of the non working ones start (/FirstChar 0/LastChar ## of approx 66) i.e. include a non standard printing range. That however is not an indicator of a bad font and in other bad examples I have seen /FirstChar 2 as giving a hint to a poorly defined font. the problem with searching /FirstChar is it may be encrypted or encode thus not possible to look for in many pdfs until disassembled.</p>
<p>The only good indication of bad characters is good plain text extraction contains invalid print characters.</p>
<p>You say you wish to avoid files with bad construct but many files may only have bad parts of pages, for a wider example of this issue see <a href=""https://stackoverflow.com/questions/68329240/how-to-identify-likely-broken-pdf-pages-before-extracting-its-text/68627207#68627207"">How to identify likely broken pdf pages before extracting its text?</a></p>
",1,1,918,2021-10-18 15:38:41,https://stackoverflow.com/questions/69618856/pdftotext-cannot-read-certain-documents
How to list the outer-most fields of a large document usign PyMongo?,"<p>I am just having difficulties to understand this, how can I list outer-most fields when I am working with a huge text datasets? I am trying to implement it in Mongodb and pymongo? any suggestions?</p>
","python, mongodb, pymongo, data-mining, text-mining","<p>I am not sure what you need, but maybe the bellow can help.</p>
<p>Query</p>
<ul>
<li>this query returns an array with all the names of the outer fields</li>
<li>objectToArray to convert ROOT document to array</li>
<li>get the first member that is the field name</li>
</ul>
<p><a href=""https://cmql.org/playmongo/?q=6228b275c2de7b441c4b49dc"" rel=""nofollow noreferrer"">PlayMongo</a></p>
<pre class=""lang-js prettyprint-override""><code>aggregate(
[{&quot;$project&quot;: 
    {&quot;_id&quot;: 0,
      &quot;outer-fields&quot;: 
      {&quot;$map&quot;: 
        {&quot;input&quot;: 
          {&quot;$map&quot;: 
            {&quot;input&quot;: {&quot;$objectToArray&quot;: &quot;$$ROOT&quot;},
              &quot;in&quot;: [&quot;$$m.k&quot;, &quot;$$m.v&quot;],
              &quot;as&quot;: &quot;m&quot;}},
          &quot;in&quot;: {&quot;$arrayElemAt&quot;: [&quot;$$this&quot;, 0]}}}}}])
</code></pre>
",0,0,45,2021-11-10 04:09:32,https://stackoverflow.com/questions/69907899/how-to-list-the-outer-most-fields-of-a-large-document-usign-pymongo
How do I add the result of a print function ito a list,"<p>I have the follwing def what ends with a print function:</p>
<pre><code>from nltk.corpus import words
nltk.download('words')
correct_spellings = words.words()
from nltk.metrics.distance import jaccard_distance
from nltk.util import ngrams
from nltk.metrics.distance  import edit_distance    
        
def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):
    for entry in entries:
        temp = [(jaccard_distance(set(ngrams(entry, 2)), set(ngrams(w, 2))),w) for w in correct_spellings if w[0]==entry[0]]
        result = print(sorted(temp, key = lambda val:val[0])[0][1])
    return  result 
answer_nine()
</code></pre>
<p>I have the three results correctly printed out, but I would like to have them in a list. I tried to assign them into a list in many different ways but I always receive the following error message: <em>AttributeError: 'NoneType' object has no attribute 'append'.</em> I do not understand why does my result has a NoneType if it has values, what do I missing here?</p>
<p>ps.: if I remove the print function like this: <code>result = sorted(temp, key = lambda val:val[0])[0][1]</code> I receive back only the third word but at least it has string as a type.</p>
","python, nltk, text-mining","<pre class=""lang-py prettyprint-override""><code>def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):
    result = []
    for entry in entries:
        temp = [(jaccard_distance(set(ngrams(entry, 2)), set(ngrams(w, 2))),w) for w in correct_spellings if w[0]==entry[0]]
        result.append(sorted(temp, key = lambda val:val[0])[0][1])
    return result
</code></pre>
<p>returns <code>['corpulent', 'indecence', 'validate']</code></p>
",0,-1,42,2021-12-03 09:19:59,https://stackoverflow.com/questions/70211952/how-do-i-add-the-result-of-a-print-function-ito-a-list
searching for texting and storing results in new columns within the dataframe,"<p>I have a data frame (df1) with one column, with each entry/row/observation consisting of a long string of text (df1$text). In a separate data frame (df2) I have one column, with each entry/row/observation consisting of a single name (df2$name).</p>
<p>I would like to note for each row in df1 which of the names in df2$name appear in the text. Ideally, I'd like to store whether a name appears in df1$text as a 1/0 value that is stored in a new column in df1 (i.e. dummy variables), that is named for that name:</p>
<pre><code>&gt; df1
  text
1 ...
2 ...
3 ...
4 ...

&gt; df2
   name
1  John
2  James
3  Jerry
4  Jackson
</code></pre>
<p>After code is executed:</p>
<pre><code>&gt; df1
  text John James Jerry Jackson 
1 ...   1    1     0        1
2 ...   0    0     0        1 
3 ...   1    1     0        1
4 ...   1    0     0        1
</code></pre>
<p>Is there a way to do this without using a for loop? my text fields are long and I have many observations in both df1 and df2.</p>
","r, dataframe, for-loop, text-mining, grepl","<p>A base R option using <code>lapply</code> -</p>
<pre><code>df1[df2$name] &lt;- lapply(df2$name, function(x) +(grepl(x, df1$text)))
</code></pre>
<p>If you want the match to be case insensitive then add <code>ignore.case = TRUE</code> in <code>grepl</code>.</p>
",2,1,50,2021-12-12 23:48:07,https://stackoverflow.com/questions/70328628/searching-for-texting-and-storing-results-in-new-columns-within-the-dataframe
How do I convert this corpus of words from an online book into a term document matrix?,"<p>Here is a snippet of my code:</p>
<pre><code>library(gutenbergr)
library(tm)
Alice &lt;- gutenberg_download(c(11))
Alice &lt;- Corpus(VectorSource(Alice))
cleanAlice &lt;- tm_map(Alice, removeWords, stopwords('english'))
cleanAlice &lt;- tm_map(cleanAlice, removeWords, c('Alice'))
cleanAlice &lt;- tm_map(cleanAlice, tolower)
cleanAlice &lt;- tm_map(cleanAlice, removePunctuation)
cleanAlice &lt;- tm_map(cleanAlice, stripWhitespace)
dtm1 &lt;- TermDocumentMatrix(cleanAlice)
dtm1
</code></pre>
<p>But then I receive the following error:</p>
<pre><code>&lt;&lt;TermDocumentMatrix (terms: 3271, documents: 2)&gt;&gt;
Non-/sparse entries: 3271/3271
Sparsity           : 50%
Error in nchar(Terms(x), type = &quot;chars&quot;) : 
  invalid multibyte string, element 12
</code></pre>
<p>How should I deal with this? Should I convert the corpus into a plain text document first? Is there something wrong with the text format of the book?</p>
","r, matrix, text-mining","<p>Gutenbergr returns a data.frame, not a text vector. You just need to slightly adjust your code and it should work fine. Instead of <code>VectorSource(Alice)</code> you need <code>VectorSource(Alice$text)</code></p>
<pre><code>library(gutenbergr)
library(tm)

# don't overwrite your download when you are testing
Alice &lt;- gutenberg_download(c(11))

# specify the column in the data.frame
Alice_corpus &lt;- Corpus(VectorSource(Alice$text))
cleanAlice &lt;- tm_map(Alice_corpus, removeWords, stopwords('english'))
cleanAlice &lt;- tm_map(cleanAlice, removeWords, c('Alice'))
cleanAlice &lt;- tm_map(cleanAlice, tolower)
cleanAlice &lt;- tm_map(cleanAlice, removePunctuation)
cleanAlice &lt;- tm_map(cleanAlice, stripWhitespace)
dtm1 &lt;- TermDocumentMatrix(cleanAlice)
dtm1

&lt;&lt;TermDocumentMatrix (terms: 3293, documents: 3380)&gt;&gt;
Non-/sparse entries: 13649/11116691
Sparsity           : 100%
Maximal term length: 46
Weighting          : term frequency (tf)
</code></pre>
<p>P.S. you can ignore the warning messages in the code.</p>
",0,1,85,2021-12-19 15:26:33,https://stackoverflow.com/questions/70412760/how-do-i-convert-this-corpus-of-words-from-an-online-book-into-a-term-document-m
Is there an easy way to read a .md (markdown) file as a character vector into R,"<p>I have a raw markdown file that I have taken from online, and I want to read the text into R as a character vector. Is there a good way of doing this?</p>
<p>Thanks.</p>
","r, nlp, r-markdown, markdown, text-mining","<p>You can read (most) files line by line using <code>readLines</code>, which should also work for markdown. Take a look at the <a href=""https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/readLines"" rel=""nofollow noreferrer"">documentation.</a></p>
<p>You specify the file to read using the <code>con</code> argument, e.g.</p>
<pre class=""lang-r prettyprint-override""><code>readLines(con = &quot;markdown.md&quot;)
</code></pre>
",1,0,1021,2022-01-07 14:33:44,https://stackoverflow.com/questions/70622756/is-there-an-easy-way-to-read-a-md-markdown-file-as-a-character-vector-into-r
How can I extract bigrams from text without removing the hash symbol?,"<p>I am using the following function (based on <a href=""https://rpubs.com/sprishi/twitterIBM"" rel=""nofollow noreferrer"">https://rpubs.com/sprishi/twitterIBM</a>) to extract bigrams from text. However, I want to keep the hash symbol for analysis purposes. The function to clean text works fine, but the unnest tokens function removes special characters. Is there any way to run unnest tokens without removing special characters?</p>
<pre><code>x &lt;- (c(&quot;I went to afternoon tea with her majesty and #queen @Victoria in the palace.&quot;, &quot;Does tea have extra caffeine?&quot;))

clean_Twitter_Corpus &lt;- function(x) {
x  =  tolower(x)                          # convert to lower case characters
x  =  stripWhitespace(x)                  # removing white space
x  =  gsub(&quot;^\\s+|\\s+$&quot;, &quot;&quot;, x)          # remove leading and trailing white space
x  = removeWords(x,stopwords(&quot;english&quot;))  # remove stopwords
return(x)
}    

# clean the twitter texts. call the clean_Twitter_Corpus function
tweets &lt;- clean_Twitter_Corpus(x)
tweets
text &lt;- as.character(tweets)
text &lt;- as.data.frame(text)

tidy_descr_ngrams &lt;- text %&gt;%
  unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)
tidy_descr_ngrams

bigram_counts &lt;- tidy_descr_ngrams %&gt;%
  count(word1, word2, sort = TRUE)

bigram_counts   
</code></pre>
","text, tidyverse, text-mining, tm, unnest","<p>Here is a solution that involving create a custom n-grams function</p>
<h4>Setup</h4>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)
library(tidytext)
library(tm)
library(purrr)

x &lt;- (c(&quot;I went to afternoon tea with her majesty and #queen @Victoria in the palace.&quot;, &quot;Does tea have extra caffeine?&quot;))

clean_Twitter_Corpus &lt;- function(x) {
  x  =  tolower(x)                          # convert to lower case characters
  x  =  stripWhitespace(x)                  # removing white space
  x  =  gsub(&quot;^\\s+|\\s+$&quot;, &quot;&quot;, x)          # remove leading and trailing white space
  x  = removeWords(x,stopwords(&quot;english&quot;))  # remove stopwords
  return(x)
}
</code></pre>
<h4>The custom function that create n grams without removing special character</h4>
<pre class=""lang-r prettyprint-override""><code># A custom build function that will take in a sentence and create
# a tibble of ngrams
ngrams_build = function(sentence, column_name, n = 2) {
  words &lt;- sentence %&gt;% str_split(pattern = &quot; &quot;, simplify = TRUE) 
  words &lt;- words[words != &quot;&quot;]
  ngrams &lt;- map_chr(1:(length(words) - n + 1),
                    .f = function(x, words, n) {
                      paste(words[x:(x + n - 1)], collapse = &quot; &quot;)
                    }, words = words, n = n)
  tibble(!!column_name := ngrams)
}
</code></pre>
<h4>Your code again</h4>
<pre class=""lang-r prettyprint-override""><code># clean the twitter texts. call the clean_Twitter_Corpus function
tweets &lt;- clean_Twitter_Corpus(x)
tweets
#&gt; [1] &quot; went  afternoon tea   majesty  #queen @victoria   palace.&quot;
#&gt; [2] &quot; tea  extra caffeine?&quot;
text &lt;- as.character(tweets)
text &lt;- as.data.frame(text)

tidy_descr_ngrams &lt;- 
  # here I use purrr function with the custom function
  map_dfr(text$text, ngrams_build, column_name = &quot;bigram&quot;, n = 2) %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)

# Here is the output which is similar to unnest_tokens but has special
# character included
tidy_descr_ngrams
#&gt; # A tibble: 8 x 2
#&gt;   word1     word2    
#&gt;   &lt;chr&gt;     &lt;chr&gt;    
#&gt; 1 went      afternoon
#&gt; 2 afternoon tea      
#&gt; 3 tea       majesty  
#&gt; 4 majesty   #queen   
#&gt; 5 #queen    @victoria
#&gt; 6 @victoria palace.  
#&gt; 7 tea       extra    
#&gt; 8 extra     caffeine?

</code></pre>
<h4>Final results</h4>
<pre class=""lang-r prettyprint-override""><code>bigram_counts &lt;- tidy_descr_ngrams %&gt;%
  count(word1, word2, sort = TRUE)

bigram_counts
#&gt; # A tibble: 8 x 3
#&gt;   word1     word2         n
#&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt;
#&gt; 1 #queen    @victoria     1
#&gt; 2 @victoria palace.       1
#&gt; 3 afternoon tea           1
#&gt; 4 extra     caffeine?     1
#&gt; 5 majesty   #queen        1
#&gt; 6 tea       extra         1
#&gt; 7 tea       majesty       1
#&gt; 8 went      afternoon     1
</code></pre>
<p><sup>Created on 2022-01-09 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v2.0.1)</sup></p>
",1,1,268,2022-01-08 17:18:31,https://stackoverflow.com/questions/70634673/how-can-i-extract-bigrams-from-text-without-removing-the-hash-symbol
Can a stemming dictionary be used as rejection criteria in R?,"<p>I am struggling through some text analysis, and I'm not sure I'm doing the stemming correctly. Right now, my command for single-term stemming is</p>
<pre><code>text_stem &lt;- text_clean %&gt;% mutate(stem = wordStem(word, language = &quot;english&quot;))
</code></pre>
<p>Is it possible to use this not only as a stemmer, but as a filter? For example, if &quot;text_clean&quot; contains the word aksdjhgla and that word is not in whatever SnowballC uses as a dictionary, the stemmed text would reject it? Maybe there's another command that does this kind of filtering?</p>
","r, nlp, text-mining","<p><code>wordStem</code> does not employ a dictionary but uses grammatical rules to do stemming (which is a rather crude approximation to lemmatisation btw). Here is an example:</p>
<pre class=""lang-r prettyprint-override""><code>words &lt;- c(&quot;win&quot;, &quot;winning&quot;)
words2 &lt;- c(&quot;aksdjhglain&quot;, &quot;aksdjhglainning&quot;)

SnowballC::wordStem(words, language = &quot;english&quot;)
#&gt; [1] &quot;win&quot;    &quot;win&quot;
SnowballC::wordStem(words2, language = &quot;english&quot;)
#&gt; [1] &quot;aksdjhglain&quot;  &quot;aksdjhglain&quot;
</code></pre>
<p>As you can see, <code>wordStem</code> does exactly the same, no matter if the words actually exist or are complete rubbish. All that matters are the word endings (ie stems). As @Kat suggested, you probably want to look at the <code>hunspell</code> package which actually uses dictionaries. To find out which words exist in the dictionary, use <code>hunspell_check</code>:</p>
<pre><code>hunspell::hunspell_check(c(words, words2))
#&gt; [1]  TRUE  TRUE  FALSE FALSE
</code></pre>
<p>Inside your existing code, you could use this to remove misspelled words:</p>
<pre><code>text_stem &lt;- text_clean %&gt;% 
  mutate(stem = wordStem(word, language = &quot;english&quot;)) %&gt;% 
  filter(hunspell::hunspell_check(word), dict = dictionary(&quot;en_US&quot;))
</code></pre>
",1,0,41,2022-01-09 21:49:27,https://stackoverflow.com/questions/70645817/can-a-stemming-dictionary-be-used-as-rejection-criteria-in-r
Automated tagging/text mining in excel,"<p>I have a monthly excel spreadsheet with the following:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Category</th>
<th style=""text-align: center;"">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">A</td>
<td style=""text-align: center;"">free text in paragraph form</td>
</tr>
<tr>
<td style=""text-align: center;"">B</td>
<td style=""text-align: center;"">free text in paragraph form</td>
</tr>
<tr>
<td style=""text-align: center;"">C</td>
<td style=""text-align: center;"">free text in paragraph form</td>
</tr>
<tr>
<td style=""text-align: center;"">B</td>
<td style=""text-align: center;"">free text in paragraph form</td>
</tr>
<tr>
<td style=""text-align: center;"">B</td>
<td style=""text-align: center;"">free text in paragraph form</td>
</tr>
<tr>
<td style=""text-align: center;"">A</td>
<td style=""text-align: center;"">free text in paragraph form</td>
</tr>
</tbody>
</table>
</div>
<p>I would like to add a third column that adds tags or keywords from a predetermined list that searches the free text and then pre-populates it based on whether one or more of the terms is found there or not.</p>
<p>So for example a list of tags could be price, distance, availability, location, and so on with the Keywords or Tags column populated based on the free text in the second column as below</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Category</th>
<th style=""text-align: center;"">Description</th>
<th style=""text-align: center;"">Keywords or Tags</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">A</td>
<td style=""text-align: center;"">Really doesn't like the price and location is too far</td>
<td style=""text-align: center;"">price, location</td>
</tr>
<tr>
<td style=""text-align: center;"">B</td>
<td style=""text-align: center;"">The distance is an issue and not too much availability</td>
<td style=""text-align: center;"">Distance, availability</td>
</tr>
<tr>
<td style=""text-align: center;"">C</td>
<td style=""text-align: center;"">Location is close so I like the convenience</td>
<td style=""text-align: center;"">location, convenience</td>
</tr>
<tr>
<td style=""text-align: center;"">B</td>
<td style=""text-align: center;"">The distance is near and there is a lot of availability</td>
<td style=""text-align: center;"">availability, distance</td>
</tr>
</tbody>
</table>
</div>
<p>As shown above, the tags would be separated by commas.</p>
<p>The issue is that the list of predetermined keywords is large (around 20 to 30 tags).</p>
<p><strong>My Questions:</strong></p>
<p>What would be the most efficient way to create this list without removing any tags?</p>
<p>Also, is there a way to do this in RStudio?</p>
","r, excel, text-mining","<p>We can use regular expressions here to extract the keywords from the strings.</p>
<p>If we put the keywords in a vector <code>keywords</code>, we can use the <code>str_extract_all</code> from the <code>stringr</code> package to extract all matching words in the string. I've made it into a simple function which we apply to the <code>Description</code> column of your data.frame, inserting the results into a new variable <code>Keys</code></p>
<pre><code>library(stringr)

get_tags &lt;- function(str, tags) {
    res = str_extract_all(str,
                          regex(tags, ignore_case = T), # Search case insensitive
                          simplify = T)[,1] # Get result as vector, not matrix
    return(res[nchar(res) &gt; 0])  # Drop empty strings from non-matched keywords
}

df$Keys &lt;- sapply(df$Description,
                  function(x) paste0(get_tags(x, keywords),
                                     collapse=', ')) # Collapse matches w/ commas

df

  Category                                             Description                   Keys
1        A   Really doesn't like the price and location is too far        price, location
2        B  The distance is an issue and not too much availability distance, availability
3        C             Location is close so I like the convenience  Location, convenience
4        D The distance is near and there is a lot of availability distance, availability

</code></pre>
<p>Since you want the matches to be case insensitive, putting the regex pattern (<code>tags</code>) in the <code>regex</code> function allows us to specify that it should ignore case.</p>
",0,0,310,2022-01-31 15:25:47,https://stackoverflow.com/questions/70928451/automated-tagging-text-mining-in-excel
How skip some line in R,"<p>I have many URLs which I import their text in R.
I use this code:</p>
<pre><code>setNames(lapply(1:1000, function(x) gettxt(get(paste0(&quot;url&quot;, x)))), paste0(&quot;url&quot;, 1:1000, &quot;_txt&quot;)) %&gt;% 
  list2env(envir = globalenv())
</code></pre>
<p>However, some URLs can not import and show this error:</p>
<blockquote>
<p>Error in file(con, &quot;r&quot;) : cannot open the connection In addition:
Warning message: In file(con, &quot;r&quot;) :   InternetOpenUrl failed: 'A
connection with the server could not be established'</p>
</blockquote>
<p>So, my code doesn't run and doesn't import any text from any URL.
How can I recognize wrong URLs and skip them in other to import correct URLs?</p>
","r, web-scraping, text-mining","<p>one possible aproach besides <code>trycatch</code> mentioned by @tester can be the <a href=""https://purrr.tidyverse.org/"" rel=""nofollow noreferrer""><code>purrr</code>-package</a>:</p>
<pre><code>library(purrr)
# declare function
my_gettxt &lt;- function(x) {
    gettxt(get(paste0(&quot;url&quot;, x)))
}
# make function error prone by defining the otherwise value (could be empty df with column defintion, etc.) used as output if function fails
my_gettxt &lt;- purrr::possibly(my_gettxt , otherwise = NA)
# use map from purrr instead of apply function
my_data &lt;- purrr::map(1:1000, ~my_gettxt(.x))
</code></pre>
",2,1,92,2022-02-13 20:01:48,https://stackoverflow.com/questions/71104378/how-skip-some-line-in-r
Hot to remove one letter token with TF-IDF Vectorizer,"<p>I'm working on a small project to calculate the tf_idf in this document which basically contains book titles and their abstracts. So far I only managed to remove stopwords and numbers, now my goal is to select words that contain at least three letters and up and do a lemmatization of the words.
This is the code I have written:</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tf_idf = TfidfVectorizer(stop_words='english', token_pattern=r'(?u)\b[A-Za-z]+\b')
tfidf_matrix = tf_idf.fit_transform(doc)
print(tfidf_matrix)
</code></pre>
<p>If I print &quot;tf_idf.vocabulary_&quot; I get all words that occur in the document as well as letters such as r,s,t,m etc. As far as lemmatization is concerned, I don't know how to go about it and I still don't understand how it works, if someone can give me a hand I thank you in advance.</p>
","python, regex, text-mining, tf-idf, stop-words","<blockquote>
<p><strong>token_patternstr, default=r”(?u)\b\w\w+\b”</strong>
Regular expression denoting what constitutes a “token”, only used if <strong>analyzer == 'word'</strong>. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).</p>
</blockquote>
<p>To select words that contain at least three letters change your regex:</p>
<pre class=""lang-py prettyprint-override""><code>tf_idf = TfidfVectorizer(stop_words='english', token_pattern=r'(?u)\b[A-Za-z]+\b')
</code></pre>
<p>to regex quantifer <code>{3,}</code>, which match its preceding element at least <em>n</em> times.</p>
<pre class=""lang-py prettyprint-override""><code>tf_idf = TfidfVectorizer(stop_words='english', analyzer='word', token_pattern=r'(?u)\b[A-Za-z]{3,}\b')
</code></pre>
<pre class=""lang-py prettyprint-override""><code># doc used as sample text.
doc = &quot;&quot;&quot;Hi Lucia. How are you? It was so nice to meet you last week in Sydney at the sales meeting. How was the rest of your trip? Did you see any kangaroos? I hope you got home to Mexico City OK.
Anyway, I have the documents about the new Berlin offices. We're going to be open in three months. I moved here from London just last week. They are very nice offices, and the location is perfect.
There are lots of restaurants, cafés and banks in the area. There's also public transport; we are next to an U-Bahn (that is the name for the metro here). Maybe you can come and see them one day? I would love to show you Berlin, especially in the winter. You said you have never seen snow – you will see lots here! Here's a photo of you and me at the restaurant in Sydney. That was a very fun night! Remember the singing Englishman? Crazy! Please send me any other photos you have of that night. Good memories.
Please give me your email address and I will send you the documents. Bye for now. Mikel&quot;&quot;&quot;

print(tf_idf.vocabulary_)
{
   &quot;lucia&quot;: 27,
   &quot;nice&quot;: 38,
   &quot;meet&quot;: 29,
   &quot;week&quot;: 59,
   &quot;sydney&quot;: 56,
   &quot;sales&quot;: 51,
   &quot;meeting&quot;: 30,
   &quot;rest&quot;: 47,
   &quot;trip&quot;: 58,
   &quot;did&quot;: 10,
   &quot;kangaroos&quot;: 22,
   &quot;hope&quot;: 20,
   &quot;got&quot;: 18,
   ...
   ...
</code></pre>
",1,0,769,2022-02-20 11:09:48,https://stackoverflow.com/questions/71193790/hot-to-remove-one-letter-token-with-tf-idf-vectorizer
Tibble error (Tibble columns must have compatible sizes.),"<p>I have been trying to run the updated code provided at <a href=""https://github.com/dgrtwo/tidy-text-mining/blob/master/05-document-term-matrices.Rmd"" rel=""nofollow noreferrer"">GitHub</a> by Dr. Silge in &quot;Text Mining with R: A tidy approach&quot; for the example &quot;Mining Financial Articles&quot;.</p>
<pre><code>library(tm.plugin.webmining)
library(purrr)
company &lt;- c(&quot;Microsoft&quot;, &quot;Apple&quot;, &quot;Google&quot;, &quot;Amazon&quot;, &quot;Facebook&quot;,
             &quot;Twitter&quot;, &quot;IBM&quot;, &quot;Yahoo&quot;, &quot;Netflix&quot;)
symbol  &lt;- c(&quot;MSFT&quot;, &quot;AAPL&quot;, &quot;GOOG&quot;, &quot;AMZN&quot;, &quot;FB&quot;, 
             &quot;TWTR&quot;, &quot;IBM&quot;, &quot;YHOO&quot;, &quot;NFLX&quot;)
download_articles &lt;- function(symbol) {
  WebCorpus(GoogleFinanceSource(paste0(&quot;NASDAQ:&quot;, symbol)))
}
stock_articles &lt;- tibble(company = company,
                         symbol = symbol) %&gt;%
  mutate(corpus = map(symbol, download_articles))
</code></pre>
<p>However, after running the tibble function, it gives me the following error:</p>
<blockquote>
<p>Tibble columns must have compatible sizes.
Size 8: Existing data.
• Size 9: Column <code>symbol</code>.
ℹ Only values of size one are recycled.</p>
</blockquote>
<p>Does anyone know how to fix this?</p>
<p>Thanks!</p>
","text-mining, tidymodels","<p>The tm.plugin.webmining package has not been updated in a long time and no longer connected to the resources it needs, unfortunately. The code in the book still runs because we recorded what this package <em>did</em> return at the time that we wrote the book; the book uses that older result. We are tracking <a href=""https://github.com/dgrtwo/tidy-text-mining/issues/62"" rel=""nofollow noreferrer"">this needed update on GitHub</a> and thinking about a good replacement.</p>
",0,0,834,2022-03-21 12:01:16,https://stackoverflow.com/questions/71557300/tibble-error-tibble-columns-must-have-compatible-sizes
How to Extract Words Following a Key Word,"<p>I'm currently trying to extract 4 words after &quot;our&quot;, but keep getting words after &quot;hour&quot; and &quot;your&quot; as well.</p>
<p>i.e.) &quot;my family will send an email in 2 hours when we arrive at.&quot; (text in the column)</p>
<p>What I want: nan (since there is no &quot;our&quot;)</p>
<p>What I get: when we arrive at (because hour as &quot;our&quot; in it)</p>
<p>I tried the following code and still have no luck.</p>
<pre><code>our = 'our\W+(?P&lt;after&gt;(?:\w+\W+){,4})' 
Reviews_C['Review_for_Fam'] =Reviews_C.ReviewText2.str.extract(our, expand=True)
</code></pre>
<p>Can you please help?</p>
<p>Thank you!</p>
","python, regex, text, nlp, text-mining","<p>You need to make sure &quot;our&quot; is with space boundaries, like this:</p>
<pre><code>our = '(^|\s+)our(\s+)?\W+(?P&lt;after&gt;(?:\w+\W+){,4})'
</code></pre>
<p>specifically <code>(^|\s+)our(\s+)?</code> is where you need to play, the example only handles spaces and start of sentence, but you might need to extend this to have quotes or other special characters.</p>
",0,0,579,2022-04-05 23:02:03,https://stackoverflow.com/questions/71759331/how-to-extract-words-following-a-key-word
Extract individual speech acts from call transcript,"<p>I have call transcript data as follow:</p>
<pre><code>'[0:00:00] spk1 : Hi how are you [0:00:02] spk2 : I am good, need help on my phone. 
[0:00:10] spk1 : sure, let me know the issue'
</code></pre>
<p>I want the text data for <code>spk1</code> separated from <code>spk2</code>.</p>
<p>I tried this</p>
<pre><code>import re

text = &quot;[0:00:00] spk1 : Hi how are you [0:00:02] spk2 : I am good, need help on my phone. [0:00:10] spk1 : sure, let me know the issue&quot;

m = re.search('\](.+?)\[', text)
if m:
    found = m.group
found
</code></pre>
<p>But I am not getting the answer.</p>
","python, nlp, text-mining","<p>Assuming you want to keep order, time, speaker information and allow for some relatively dynamic orders (flexible number of speakers, same speaker is allowed to speak in two timestamps or more in a row):</p>
<pre><code>import re

text = &quot;[0:00:00] spk1 : Hi how are you [0:00:02] spk2 : I am good, need help on my phone. [0:00:10] spk1 : sure, let me know the issue&quot;

conversation_dict_list = []
# iterate over tokens split by whitespaces
for token in text.split(): 
    # timestamp: add new dict to list, add time and empty speaker and empty text 
    if re.fullmatch(&quot;\[\d+:\d\d:\d\d\]&quot;, token):
        conversation_dict_list.append({&quot;time&quot;: token[1:-1], &quot;speaker&quot;: None, &quot;text&quot;: &quot;&quot;})
    # speaker: fill speaker field
    elif re.fullmatch(&quot;spk\d+&quot;, token):
        conversation_dict_list[-1][&quot;speaker&quot;] = token
    # text: keep concatenating to text field (plus whitespace)
    else:  
        conversation_dict_list[-1][&quot;text&quot;] += &quot; &quot; + token

# remove leading &quot; : &quot; from each text
conversation_dict_list = [{k_:(v_ if k_ != &quot;text&quot; else v_[3:]) for k_,v_ in d.items()} for d in conversation_dict_list]

print(conversation_dict_list)
</code></pre>
<p>Which returns:</p>
<pre><code>&gt; [{'time': '0:00:00', 'speaker': 'spk1', 'text': 'Hi how are you'}, {'time': '0:00:02', 'speaker': 'spk2', 'text': 'I am good, need help on my phone.'}, {'time': '0:00:10', 'speaker': 'spk1', 'text': 'sure, let me know the issue'}]
</code></pre>
<p>Obviously this will only work if you always have the exact pattern <code>[h:mm:ss] spkX</code> because if you have e.g. multiple speakers within the same timestamp the speaker would be overwritten with the last one.</p>
",0,0,341,2022-04-06 11:36:29,https://stackoverflow.com/questions/71766046/extract-individual-speech-acts-from-call-transcript
How to count occurrences of a word/token in a one-token-per-document-per-row tibble,"<p>Hello I have a tibble through a pipe from <code>tidytext::unnest_tokens()</code> and <code>count(category, word, name = &quot;count&quot;)</code>. It looks like this example.</p>
<pre><code>owl &lt;- tibble(category = c(0, 1, 2, -1, 0, 1, 2),
              word = c(rep(&quot;hello&quot;, 3), rep(&quot;world&quot;, 4)),
              count = sample(1:100, 7))
</code></pre>
<p>and I would like to get this tibble with an additional column that gives the number of categories the word appears in, i.e. the same number for each time the word appears.</p>
<p>I tried the following code that works in principal. The result is what I want.</p>
<pre><code>owl %&gt;% mutate(sum_t = sapply(1:nrow(.), function(x) {filter(., word == .$word[[x]]) %&gt;% nrow()}))
</code></pre>
<p>However, seeing that my data has 10s of thousands of rows this takes a rather long time. Is there a more efficient way to achieve this?</p>
","r, text-mining","<p>We could use <code>add_count</code>:</p>
<pre><code>library(dplyr)

 owl %&gt;% 
   add_count(word)
</code></pre>
<p>output:</p>
<pre><code>  category word  count     n
     &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;
1        0 hello    98     3
2        1 hello    30     3
3        2 hello    37     3
4       -1 world    22     4
5        0 world    80     4
6        1 world    18     4
7        2 world    19     4
</code></pre>
",2,2,350,2022-04-10 06:33:10,https://stackoverflow.com/questions/71814291/how-to-count-occurrences-of-a-word-token-in-a-one-token-per-document-per-row-tib
"Remove Numbers, Punctuations, White Spaces before Tokenization","<p>I have the following data frame</p>
<pre><code>report &lt;- data.frame(Text = c(&quot;unit 1 crosses the street&quot;, 
       &quot;driver 2 was speeding and saw driver# 1&quot;, 
        &quot;year 2019 was the year before the pandemic&quot;,
        &quot;hey saw       hei hei in        the    wood&quot;,
        &quot;hello: my kityy! you are the best&quot;), id = 1:5)
report 
                                         Text id
1                   unit 1 crosses the street  1
2     driver 2 was speeding and saw driver# 1  2
3  year 2019 was the year before the pandemic  3
4 hey saw       hei hei in        the    wood  4
5           hello: my kityy! you are the best  5
</code></pre>
<p>From a previous coding help, we can remove stop words using the following code.</p>
<pre><code>report$Text &lt;- gsub(paste0('\\b',tm::stopwords(&quot;english&quot;), '\\b', 
                          collapse = '|'), '', report$Text)
report
                                    Text id
1                 unit 1 crosses  street  1
2      driver 2  speeding  saw driver# 1  2
3            year 2019   year   pandemic  3
4 hey saw       hei hei             wood  4
5                 hello:  kityy!    best  5
</code></pre>
<p>The above data still has noises (numbers, punctuations, and white space). Need to get the data in the following format by removing these noises before tokenization. Additionally, I want to remove selected stop words (for example, <code>saw</code> and <code>kitty</code>).</p>
<pre><code>                                    Text id
1                   unit crosses  street  1
2                driver speeding  driver  2
3                     year year pandemic  3
4                       hey hei hei wood  4
5                             hello best  5
</code></pre>
","r, text-mining, tm, stop-words, tidytext","<p>We may get the <code>union</code> of <code>tm::stopwords</code> and the new entries, <code>paste</code> them with <code>collapse = &quot;|&quot;</code>, remove those with replacement as <code>&quot;&quot;</code> in <code>gsub</code>, along with removing the punctuations and digits and extra spaces (<code>\\s+</code> - one or more spaces)</p>
<pre><code>trimws(gsub(&quot;\\s+&quot;, &quot; &quot;, 
 gsub(paste0(&quot;\\b(&quot;, paste(union(c(&quot;saw&quot;, &quot;kityy&quot;), 
   tm::stopwords(&quot;english&quot;)), collapse=&quot;|&quot;), &quot;)\\b&quot;), &quot;&quot;, 
     gsub(&quot;[[:punct:]0-9]+&quot;, &quot;&quot;, report$Text))
))
</code></pre>
<p>-output</p>
<pre><code>[1] &quot;unit crosses street&quot; 
[2  &quot;driver speeding driver&quot; 
[3] &quot;year year pandemic&quot;   
[4] &quot;hey hei hei wood&quot;   
[5] &quot;hello best&quot;                    
</code></pre>
",4,2,245,2022-04-22 15:20:44,https://stackoverflow.com/questions/71971099/remove-numbers-punctuations-white-spaces-before-tokenization
Remove Words with less than Certain Character Lengths plus Noise Reduction before Tokenization,"<p>I have the following data frame</p>
<pre><code>report &lt;- data.frame(Text = c(&quot;unit 1 crosses the street&quot;, 
       &quot;driver 2 was speeding and saw driver# 1&quot;, 
        &quot;year 2019 was the year before the pandemic&quot;,
        &quot;hey saw       hei hei in        the    wood&quot;,
        &quot;hello: my kityy! you are the best&quot;), id = 1:5)
report 
                                         Text id
1                   unit 1 crosses the street  1
2     driver 2 was speeding and saw driver# 1  2
3  year 2019 was the year before the pandemic  3
4 hey saw       hei hei in        the    wood  4
5           hello: my kityy! you are the best  5
</code></pre>
<p>From a previous coding help, we can remove stop words using the following code.</p>
<pre><code>report$Text &lt;- gsub(paste0('\\b',tm::stopwords(&quot;english&quot;), '\\b', 
                          collapse = '|'), '', report$Text)
report
                                    Text id
1                 unit 1 crosses  street  1
2      driver 2  speeding  saw driver# 1  2
3            year 2019   year   pandemic  3
4 hey saw       hei hei             wood  4
5                 hello:  kityy!    best  5
</code></pre>
<p>I want to remove words less than certain character length (for example, want to remove words less than 4 characters such as <code>hei</code> and <code>hey</code>). Plus need to remove manual stop words (for example, <code>saw</code> and <code>kitty</code>) and common noises (whitespaces, numbers, and punctuations) before tokenization. The final outcome would be:</p>
<pre><code>                                    Text id
1                   unit crosses  street  1
2                driver speeding  driver  2
3                     year year pandemic  3
4                                   wood  4
5                             hello best  5
</code></pre>
<p>A similar question regarding noise and manual stop words is posted <a href=""https://stackoverflow.com/questions/71971099/remove-numbers-punctuations-white-spaces-before-tokenization/71971183"">here</a>.</p>
","r, nlp, text-mining, tm, stop-words","<p>With the previous code, if we start with removal of words that have <code>nchar</code> less than or equal to 3 (with <code>gsubfn</code>) it should work</p>
<pre><code>trimws(gsub(paste0(&quot;\\b(&quot;, paste(union(c(&quot;saw&quot;, &quot;kityy&quot;), 
   tm::stopwords(&quot;english&quot;)), collapse=&quot;|&quot;), &quot;)\\b&quot;), &quot;&quot;, 
     gsub(&quot;[[:punct:]0-9]+&quot;, &quot;&quot;,gsubfn(&quot;\\w+&quot;, function(x) 
     if(nchar(x) &gt; 3) x else '', report$Text))))))
</code></pre>
<p>-output</p>
<pre><code>[1] &quot;unit crosses street&quot;    &quot;driver speeding driver&quot; 
[3] &quot;year year pandemic&quot;     &quot;wood&quot;                   &quot;hello best&quot;       
</code></pre>
",3,1,182,2022-04-22 15:46:25,https://stackoverflow.com/questions/71971408/remove-words-with-less-than-certain-character-lengths-plus-noise-reduction-befor
R Count Frequency of Custom Dictionary in a Dataframe Column but Group them,"<p>I have a task, which is too complex for my R-knowledge.
I have a dataframe with Tweets-data, including a column that consists of the usernames, data of the Tweet and the content of the Tweet.
It looks like this:
<a href=""https://i.sstatic.net/bbTGd.png"" rel=""nofollow noreferrer"">Datastructure</a></p>
<p>I have dictionaries of words like:</p>
<pre><code>dict &lt;- c(&quot;one&quot;, &quot;two&quot;, &quot;eleven&quot;)
</code></pre>
<p>I want to count the frequency of the words used within their tweets, but I want to group them by year and name.</p>
<p>I count the frequency by using this:</p>
<pre><code>freq_auth &lt;- tweetsanalysis1 %&gt;% mutate(authority_dic = str_c(str_extract(text, str_c(authority_dic, collapse = '|')))) %&gt;% count(authority_dic, name = 'freq_word') %&gt;% arrange(desc(freq_word))
</code></pre>
<p>It works just like it should:</p>
<p><a href=""https://i.sstatic.net/2i1Pl.png"" rel=""nofollow noreferrer"">Output</a></p>
<p>But it counts for all names and dates. How do I count the frequency for every different name on their own and split it by year? I want to analyse each individual name on its word frequency and afterwards add the name and date of the tweet to the output.</p>
<p>Maybe cut the dataframe into tiny pieces by each name within a year and then run the analysis on each name? My dataset contains 30k observations and over 200 individual names, so that would take a lot of time.</p>
<p>I hope I was able to get my point across. If not, just ask me. :)
It would be greatly welcomed if someone would help me!
Thanks in advance.</p>
","r, string, dataframe, text-mining, tweets","<p>Try <code>group_by()</code> and <code>summarise()</code> and you can <code>spread()</code> after to create a column for each year.</p>
<p>See if this works for your:</p>
<pre><code>freq_auth &lt;- tweetsanalysis1 %&gt;%
        mutate(authority_dic =str_c(str_extract(text, str_c(authority_dic, collapse = '|')))) %&gt;%
        group_by(authority_dic, year, user_username) %&gt;%
        summarise(freq_word = n()) %&gt;% 
        arrange(desc(freq_word)) %&gt;%
        spread(year, freq_word)
</code></pre>
",0,0,132,2022-05-14 16:32:07,https://stackoverflow.com/questions/72242136/r-count-frequency-of-custom-dictionary-in-a-dataframe-column-but-group-them
Split Player and Chat from Chat Log (text-mining),"<p>I have a chat log which includes 4 players (A, B, C, D) and their chats in one row in my data frame (across many groups). I want to split each phrase into its own row and identify the speaker of that phrase in a separate column.</p>
<p>I have attempted many things using the following packages but haven't been able to succeed.
psych
dplyr
splitstackshape
tidytext
stringr
tidyr</p>
<p>The data frame is not a txt.document, but I'm thinking it needs to be?</p>
<p>For example this is what the chat log looks like. This is all in one row in my dataset.</p>
<pre><code>[1] &quot; *** D has joined the chat ***&quot;                                                                                                                                         
  [2] &quot; *** B has joined the chat ***&quot;                                                                                                                                         
  [3] &quot; *** A has joined the chat ***&quot;                                                                                                                                         
  [4] &quot;D: hi&quot;                                                                                                                                                                  
  [5] &quot;B: hello!&quot;                                                                                                                                                              
  [6] &quot;A: Hi!&quot;                                                                                                                                                                 
  [7] &quot;D: i think oxygen is most important&quot;                                                                                                                                    
  [8] &quot;A: I do too&quot;                                                                                                                                                            
  [9] &quot; *** C has joined the chat ***&quot;                                                                                                                                         
 [10] &quot;B: agreed, that was my #1&quot;                                                                                                                                              
 [11] &quot;A: I didnt at first but then on second guess&quot;                                                                                                                           
 [12] &quot;A: oxygen then water&quot;                                                                                                                                                   
 [13] &quot;C: hi hi&quot;                                                              
</code></pre>
<p>I want the following (to have these columns where each row is a new phrase)</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Player ID</th>
<th>Phrase</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>hi!</td>
</tr>
<tr>
<td>B</td>
<td>hello!</td>
</tr>
</tbody>
</table>
</div>
<p>I want to eventually use this to count # of words/characters per player</p>
","r, text-mining","<pre><code>library(dplyr)
library(tidyr)

d %&gt;%
  t() %&gt;%
  as.data.frame(&quot;V1&quot;) %&gt;%
  filter(!grepl(&quot;***&quot;, V1, fixed = TRUE)) %&gt;%
  separate(V1, into = c(&quot;PlayerID&quot;, &quot;Phrase&quot;), sep = &quot;: &quot;) %&gt;%
  mutate(Count = nchar(Phrase))
</code></pre>
<p>result:</p>
<pre><code>#&gt;   PlayerID                                    Phrase Count
#&gt; 1        D                                        hi     2
#&gt; 2        B                                    hello!     6
#&gt; 3        A                                       Hi!     3
#&gt; 4        D          i think oxygen is most important    32
#&gt; 5        A                                  I do too     8
#&gt; 6        B                    agreed, that was my #1    22
#&gt; 7        A I didnt at first but then on second guess    41
#&gt; 8        A                         oxygen then water    17
#&gt; 9        C                                     hi hi     5
</code></pre>
<p>You could use add this to the dplyr chain to count the number of characters per player:</p>
<pre><code>group_by(PlayerID) %&gt;%
summarize(Total = sum(Count))

#&gt;   PlayerID Total
#&gt;   &lt;chr&gt;    &lt;int&gt;
#&gt; 1 A           69
#&gt; 2 B           28
#&gt; 3 C            5
#&gt; 4 D           34
</code></pre>
<p>data:</p>
<pre><code>d &lt;- structure(c(&quot; *** D has joined the chat ***&quot;, &quot; *** B has joined the chat ***&quot;, 
                 &quot; *** A has joined the chat ***&quot;, &quot;D: hi&quot;, &quot;B: hello!&quot;, &quot;A: Hi!&quot;, 
                 &quot;D: i think oxygen is most important&quot;, &quot;A: I do too&quot;, &quot; *** C has joined the chat ***&quot;, 
                 &quot;B: agreed, that was my #1&quot;, &quot;A: I didnt at first but then on second guess&quot;, 
                 &quot;A: oxygen then water&quot;, &quot;C: hi hi&quot;), .Dim = c(1L, 13L))

Created on 2022-05-25 by the reprex package (v2.0.1)
</code></pre>
",0,0,76,2022-05-25 15:19:58,https://stackoverflow.com/questions/72380308/split-player-and-chat-from-chat-log-text-mining
"In R, how to find the location of a word in a string?","<p>How can I find the first location of specific words in a dataframe cell, and save the output in a new column in the same dataframe?</p>
<p>Ideally I want the first match for each of the words in dictionary.</p>
<pre><code>df &lt;- data.frame(text = c(&quot;omg coke is so awsme&quot;,&quot;i always preferred pepsi&quot;, &quot;mozart is so overrated by yeah fanta makes my day, always&quot;))

dict &lt;- c(&quot;coke&quot;, &quot;pepsi&quot;, &quot;fanta&quot;)
</code></pre>
<p>Location can be N of characters or words preceding the dictionary word.</p>
<p>I've been playing around with the code found <a href=""https://www.statology.org/r-find-character-in-string/"" rel=""nofollow noreferrer"">here</a>, but I can't make it work.</p>
<p>For example, this code does the job, but only for one word, and for one string (rather than a df and dictionary)</p>
<pre><code>my_string = &quot;omg coke is so awsme&quot;
unlist(gregexpr(&quot;coke&quot;, my_string))[1]
</code></pre>
<p>Desired output:</p>
<pre><code>                                                       text  location
1                                      omg coke is so awsme         2
2                                  i always preferred pepsi         4
3 mozart is so overrated by yeah fanta makes my day, always         7
</code></pre>
<p>Like I said, the location can also be string rather than word, if that is easier.</p>
","r, text, nlp, text-mining, quanteda","<p>Here's a simple for loop:</p>
<pre><code>for(i in dict) {
  df[[i]] = stringi::stri_locate_first_fixed(df$text, i)[, 1]
}
df
#                                                        text coke pepsi fanta
# 1                                      omg coke is so awsme    5    NA    NA
# 2                                  i always preferred pepsi   NA    20    NA
# 3 mozart is so overrated by yeah fanta makes my day, always   NA    NA    32
</code></pre>
<p>Or with <code>regexpr</code> (part of base, so no dependencies):</p>
<pre><code>for(i in dict) {
  df[[i]] = regexpr(i, df$text, fixed = TRUE)
}
df
#                                                        text coke pepsi fanta
# 1                                      omg coke is so awsme    5    -1    -1
# 2                                  i always preferred pepsi   -1    20    -1
# 3 mozart is so overrated by yeah fanta makes my day, always   -1    -1    32
</code></pre>
<p>And here's a solution for word number, though I would recommend deleting all the punctuation before using this:</p>
<pre><code>df$words = strsplit(df$text, split = &quot; &quot;)
for(i in dict) {
  df[[i]] = sapply(df$words, \(x) match(i, unlist(x)))
}
df
#                                                        text coke pepsi fanta
# 1                                      omg coke is so awsme    2    NA    NA
# 2                                  i always preferred pepsi   NA     4    NA
# 3 mozart is so overrated by yeah fanta makes my day, always   NA    NA     7
#                                                                 words
# 1                                            omg, coke, is, so, awsme
# 2                                         i, always, preferred, pepsi
# 3 mozart, is, so, overrated, by, yeah, fanta, makes, my, day,, always
</code></pre>
",1,1,686,2022-05-26 13:14:33,https://stackoverflow.com/questions/72392438/in-r-how-to-find-the-location-of-a-word-in-a-string
How can I count comma-separated values in my Dataframe?,"<p>I am trying to figure out how to get value_counts from how many times a specific text value is listed in the column.</p>
<p>Example data:</p>
<pre><code>d = {'Title': ['Crash Landing on You', 'Memories of the Alhambra', 'The Heirs', 'While You Were Sleeping', 
'Something in the Rain', 'Uncontrollably Fond'], 
'Cast' : ['Hyun Bin,Son Ye Jin,Seo Ji Hye', 'Hyun Bin,Park Shin Hye,Park Hoon', 'Lee Min Ho,Park Shin Hye,Kim Woo Bin', 
'Bae Suzy,Lee Jong Suk,Jung Hae In', 'Son Ye Jin,Jung Hae In,Jang So Yeon', 'Kim Woo Bin,Bae Suzy,Im Joo Hwan']}

Title   Cast
0   Crash Landing on You    Hyun Bin,Son Ye Jin,Seo Ji Hye
1   Memories of the Alhambra    Hyun Bin,Park Shin Hye,Park Hoon
2   The Heirs   Lee Min Ho,Park Shin Hye,Kim Woo Bin
3   While You Were Sleeping Bae Suzy,Lee Jong Suk,Jung Hae In
4   Something in the Rain   Son Ye Jin,Jung Hae In,Jang So Yeon
5   Uncontrollably Fond Kim Woo Bin,Bae Suzy,Im Joo Hwan
</code></pre>
<p>When I split the text and do value counts:</p>
<pre><code>df['Cast'] = df['Cast'].str.split(',')
df['Cast'].value_counts()

[Hyun Bin, Son Ye Jin, Seo Ji Hye]          1
[Hyun Bin, Park Shin Hye, Park Hoon]        1
[Lee Min Ho, Park Shin Hye, Kim Woo Bin]    1
[Bae Suzy, Lee Jong Suk, Jung Hae In]       1
[Son Ye Jin, Jung Hae In, Jang So Yeon]     1
[Kim Woo Bin, Bae Suzy, Im Joo Hwan]        1
Name: Cast, dtype: int64
</code></pre>
<p>How do I get the amount of times a specific text is shown in the 'Cast' column? ie:</p>
<pre><code>[Park Shin Hye] 2
[Hyun Bin] 2
[Bae Suzy] 1 
etc 
</code></pre>
","python, pandas, text-mining","<p>You should use the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html"" rel=""noreferrer""><code>.explode</code></a> method to &quot;unpack&quot; each list in different rows. Then <code>.value_counts</code> will work as intended in the original code:</p>
<pre><code>import pandas as pd

d = {'Title': ['Crash Landing on You', 'Memories of the Alhambra', 'The Heirs', 'While You Were Sleeping', 
'Something in the Rain', 'Uncontrollably Fond'], 
'Cast' : ['Hyun Bin,Son Ye Jin,Seo Ji Hye', 'Hyun Bin,Park Shin Hye,Park Hoon', 'Lee Min Ho,Park Shin Hye,Kim Woo Bin', 
'Bae Suzy,Lee Jong Suk,Jung Hae In', 'Son Ye Jin,Jung Hae In,Jang So Yeon', 'Kim Woo Bin,Bae Suzy,Im Joo Hwan']}

df = pd.DataFrame(d)
df['Cast'].str.split(',').explode('Cast').value_counts()
</code></pre>
",10,4,2566,2022-05-28 00:16:24,https://stackoverflow.com/questions/72412077/how-can-i-count-comma-separated-values-in-my-dataframe
Is there a way in R to find a combination of words (or sentences) within a certain range in a string,"<p>I'm trying to find all strings with a combination of words/sentences with other words separating them but with a fixed limit.</p>
<p>Example : I want the combination of &quot;bought&quot; and &quot;watch&quot; but with, at maximum, 2 words separating them.</p>
<ul>
<li>I bought a beautiful and shiny watch
-&gt; not ok because there is 4 words between &quot;bought&quot; and &quot;watch&quot; (&quot;a beautiful and shiny&quot;)</li>
<li>I bought a shiny watch -&gt; ok because there is 2 words between &quot;bought&quot; and &quot;watch&quot; (&quot;a shiny&quot;)</li>
</ul>
<p>I haven't found anything close to what I wanted on R.</p>
<p>To find simple words/sentences in strings I'm using <code>str_extract_all</code> from <code>stringr</code> as here :</p>
<pre><code>my_analysis &lt;- str_c(&quot;\\b(&quot;, str_c(my_list_of_words_and_sentences, collapse=&quot;|&quot;), &quot;)\\b&quot;)
df$words_and_sentences_found &lt;- str_extract_all(df$my_strings, my_analysis)
</code></pre>
","r, text-mining, stringr, tidytext","<p>You can use <a href=""https://en.wikipedia.org/wiki/N-gram#Skip-gram"" rel=""nofollow noreferrer"">skip-grams</a> for this:</p>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)
library(tidytext)

df &lt;- tibble(id = 1:3,
             txt = c(&quot;I bought a beautiful and shiny watch&quot;, 
                     &quot;I bought a shiny watch&quot;, 
                     &quot;The watch is very shiny&quot;))

tidy_ngrams &lt;- df %&gt;%
  ## use k for the skip, and n for what degree of n-gram:
  unnest_tokens(ngram, txt, token = &quot;skip_ngrams&quot;, n_min = 2, n = 2, k = 2) 

tidy_ngrams
#&gt; # A tibble: 33 × 2
#&gt;       id ngram           
#&gt;    &lt;int&gt; &lt;chr&gt;           
#&gt;  1     1 i bought        
#&gt;  2     1 i a             
#&gt;  3     1 i beautiful     
#&gt;  4     1 bought a        
#&gt;  5     1 bought beautiful
#&gt;  6     1 bought and      
#&gt;  7     1 a beautiful     
#&gt;  8     1 a and           
#&gt;  9     1 a shiny         
#&gt; 10     1 beautiful and   
#&gt; # … with 23 more rows

tidy_ngrams %&gt;%
  filter(ngram == &quot;bought watch&quot;)
#&gt; # A tibble: 1 × 2
#&gt;      id ngram       
#&gt;   &lt;int&gt; &lt;chr&gt;       
#&gt; 1     2 bought watch
</code></pre>
<p><sup>Created on 2022-06-03 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v2.0.1)</sup></p>
",0,0,446,2022-06-01 14:38:25,https://stackoverflow.com/questions/72463717/is-there-a-way-in-r-to-find-a-combination-of-words-or-sentences-within-a-certa
How to get single column answers in `texplot_xray()`?,"<p>I want to do a scan of a multilingual parallel corpus to evaluate possible equivalences. For that I need <code>texplot_xray()</code> to return multiple answers in a single column.</p>
<p>In the first search, where the word of Latin origin is used equally in English, Italian and Spanish, some degree of equivalence seems to be interpreted, which is not the case for French <em>human</em> =&gt; <em>l'homme</em>.</p>
<pre class=""lang-r prettyprint-override""><code># require(quanteda)
# require(quanteda.corpora)
# require(quanteda.texplots)
corpusa &lt;- data_corpus_udhr[c('ita', 'eng', 'eus', 'spa', 'fra')]
quanteda.textplots::textplot_xray(kwic(x = corpusa, pattern = '*uman*'))
</code></pre>
<p><a href=""https://i.sstatic.net/L0Y47.png"" rel=""nofollow noreferrer"">Results of a search in four languages (five, one no result)</a></p>
<p>When searching more closely, I would like to summarise the equivalents in the one relevant column.</p>
<pre class=""lang-r prettyprint-override""><code>bilaketa &lt;- c('umani', 'human', 'giza', 'humanos', &quot;l'homme&quot;)
quanteda.textplots::textplot_xray(kwic(corpusa, pattern = phrase(bilaketa)))
</code></pre>
<p><a href=""https://i.sstatic.net/lvj4r.png"" rel=""nofollow noreferrer"">Results reducible to a single relevant column</a></p>
<p>Is there a way to resolve such queries?</p>
","r, text-mining, quanteda","<p>You can use a dictionary as a pattern in the <code>kwic()</code>, although you will get the dictionary key as the column total rather than the individual (pattern) value, as in the case with the five columns.</p>
<pre class=""lang-r prettyprint-override""><code>library(&quot;quanteda&quot;)
## Package version: 3.2.1
## Unicode version: 14.0
## ICU version: 70.1
## Parallel computing: 8 of 8 threads used.
## See https://quanteda.io for tutorials and examples.
library(&quot;quanteda.textplots&quot;)

data(data_corpus_udhr, package = &quot;quanteda.corpora&quot;)
corpusa &lt;- data_corpus_udhr[c(&quot;ita&quot;, &quot;eng&quot;, &quot;eus&quot;, &quot;spa&quot;, &quot;fra&quot;)]

bilaketa &lt;- c(&quot;umani&quot;, &quot;human&quot;, &quot;giza&quot;, &quot;humanos&quot;, &quot;l'homme&quot;)

corpusa %&gt;%
  tokens() %&gt;%
  kwic(pattern = dictionary(list(human = bilaketa))) %&gt;%
  textplot_xray()
</code></pre>
<p><img src=""https://i.sstatic.net/YMADw.png"" alt="""" /></p>
",0,1,27,2022-06-05 15:51:27,https://stackoverflow.com/questions/72508846/how-to-get-single-column-answers-in-texplot-xray
Organizing text blocks as rows in R,"<p>I have a text file containing details of about 1000 articles, each containing the same items (PMC, PMID..... AID, SO). I need to have every article details as a single row such that the item names are read as column names, and the respective content as row values:</p>
<pre><code>PMC              PMID        ........      SO

PMC8882595       35237547    ........      Front ....69001
PMC8967719       35296856    ........      Child Yo... 040
</code></pre>
<p>The author names and affiliations columns (FAU, AU, AD) in between are repeated, and I just need to extract the country name from that content, if possible. Or just merge them otherwise.</p>
<p>I am really sorry about the poor phrasing of the question. Here is a glimpse of first two articles to describe it better.</p>
<pre><code>PMC - PMC8882595
PMID- 35237547
IS  - 2296-2565 (Electronic)
VI  - 10
DP  - 2022
TI  - Poverty Vulnerability and Health Risk Action Path of Families of Rural Elderly
      With Chronic Diseases: Empirical Analysis of 1,852 Families in Central and
      Western China.
LID - 776901
AB  - Health poverty has become the most important cause of poverty and return to
      poverty. Understanding the health risk factors and action paths of poverty in
      families of rural elderly with chronic diseases is important to alleviate return 
      to poverty because of illness. This study selected families with at least one
      elderly member (over 60 years old) with chronic diseases (sample size was 1,852
      people and localities, the causes of poverty and returning to poverty, and the
      types of poverty vulnerabilities. The use efficiency of medical insurance should 
      be further improved, and the responsibility of medical insurance targeted poverty
      alleviation must be clarified.
FAU - Ma, Ying
AU  - Ma Y
AD  - Department of Health Management, School of Medicine and Health Management, Tongji
      Medical College, Huazhong University of Science and Technology, Wuhan, China
FAU - Xiang, Qin
AU  - Xiang Q
AD  - Department of Health Management, School of Medicine and Health Management, Tongji
      Medical College, Huazhong University of Science and Technology, Wuhan, China
FAU - Yan, Chaoyang
AU  - Yan C
AD  - Department of Health Management, School of Medicine and Health Management, Tongji
      Medical College, Huazhong University of Science and Technology, Wuhan, China
FAU - Liao, Hui
AU  - Liao H
AD  - Department of Health Management, School of Medicine and Health Management, Tongji
      Medical College, Huazhong University of Science and Technology, Wuhan, China
FAU - Wang, Jing
AU  - Wang J
AD  - Department of Health Management, School of Medicine and Health Management, Tongji
      Medical College, Huazhong University of Science and Technology, Wuhan, China
LA  - eng
PT  - Journal Article
DEP - 20220214
PHST- 2021/09/14 [received]
PHST- 2022/01/20 [accepted]
TA  - Front Public Health
JT  - Frontiers in Public Health
AID - 10.3389/fpubh.2022.776901 [doi]
SO  - Front Public Health. 2022 Feb 14;10:. doi:10.3389/fpubh.2022.776901.


    
    
PMC - PMC6107082
PMID- 30147212
IS  - 0190-7409 (Print)
VI  - 79
DP  - 2017 Aug
TI  - Family poverty and neighborhood poverty: Links with children’s school readiness
      before and after the Great Recession.
PG  - 368-84
AB  - This paper examines how neighborhood and family poverty predict children’s
      academic skills and classroom behavior at school entry, and whether associations 
      additional support.
FAU - Wolf, Sharon
AU  - Wolf S
AD  - Graduate School of Education, University of Pennsylvania, 3700 Walnut Street,
      Room 340, Philadelphia, PA 19104, United States
FAU - Magnuson, Katherine A.
AU  - Magnuson KA
AD  - Institute for Research on Poverty, University of Wisconsin – Madison, 1180
      Observatory Drive, Madison, WI 53706, United States
FAU - Kimbro, Rachel T.
AU  - Kimbro RT
AD  - Rice University, 6100 Main St., MS-28, Houston, TX 77005, United States
LA  - eng
PT  - Journal Article
DEP - 20170623
TA  - Child Youth Serv Rev
JT  - Children and youth services review
AID - 10.1016/j.childyouth.2017.06.040 [doi]
MID - NIHMS984695
SO  - Child Youth Serv Rev. 2017 Aug;79:368-84. Epub 2017 Jun 23
      doi:10.1016/j.childyouth.2017.06.040.
</code></pre>
","r, text-mining, text-manipulation","<p>Here's an admittedly brute-force start for you.</p>
<p>Up front, though, you will need to put some thought onto how you want to aggregate your fields. In general, all of the fields (e.g., <code>&quot;PMC&quot;</code>, <code>&quot;PHST&quot;</code>, <code>&quot;FAU&quot;</code>) must recycling safely, with a length of exactly 1 or the same non-1 length from each group. In this data, in the first batch of data, you have <code>PHST</code> with length 2 and <code>FAU</code> with length 5. I know this is known based on the variable data, and that's fine, but that means you need to come up with a clean way to aggregate them. Some options might include list-columns or text-reduction using tokenization, pattern-matching, or whatever. You said you want to extract just the country from the multiple <code>AU</code> fields, that's fine, but ... that's not always easy given the free-text flow of those fields.</p>
<p>I'm addressing everything except how to find country names and reduce those fields. I strongly encourage looking into list-columns (as opposed to my naive string-concatenation) and some other functions to do the aggregation. (Perhaps another question?)</p>
<p>For now, my naive aggregator:</p>
<pre class=""lang-r prettyprint-override""><code>agg_fun &lt;- function(x, fld) {
  # do something different based on fld[1] such as extract a country
  paste(x, collapse = &quot; &quot;)
}
</code></pre>
<p>First, to read in that data, I'll use some bespoke code to turn it into a frame with which we can work a little more easily:</p>
<pre class=""lang-r prettyprint-override""><code>txt &lt;- readLines(&quot;~/StackOverflow/12121693/72706285.txt&quot;)
tmp1 &lt;- tmp2 &lt;- character(0)
leadsp &lt;- grepl(&quot;^\\s&quot;, txt)
tmp1[!leadsp] &lt;- sub(&quot;^(\\S+)\\s*-.*&quot;, &quot;\\1&quot;, txt[!leadsp])
tmp2[!leadsp] &lt;- sub(&quot;^\\S+\\s*-\\s*(.*)&quot;, &quot;\\1&quot;, txt[!leadsp])
tmp2[leadsp] &lt;- sub(&quot;^ *&quot;, &quot;&quot;, txt[leadsp])

tmpdat &lt;- tibble(field = tmp1, value = tmp2)
tmpdat
# # A tibble: 82 x 2
#    field value                                                                         
#    &lt;chr&gt; &lt;chr&gt;                                                                         
#  1 PMC   PMC8882595                                                                    
#  2 PMID  35237547                                                                      
#  3 IS    2296-2565 (Electronic)                                                        
#  4 VI    10                                                                            
#  5 DP    2022                                                                          
#  6 TI    Poverty Vulnerability and Health Risk Action Path of Families of Rural Elderly
#  7 NA    With Chronic Diseases: Empirical Analysis of 1,852 Families in Central and    
#  8 NA    Western China.                                                                
#  9 LID   776901                                                                        
# 10 AB    Health poverty has become the most important cause of poverty and return to   
# # ... with 72 more rows
</code></pre>
<p>With that, here's a dplyr/tidyr flow using that data:</p>
<pre class=""lang-r prettyprint-override""><code>library(dplyr)
library(tidyr) # pivot_wider
tibble(field = tmp1, value = tmp2) %&gt;%
  filter(nzchar(trimws(field)) | nzchar(trimws(value))) %&gt;%
  group_by(grp = cumsum(!is.na(field))) %&gt;%
  summarize(field = field[1], value = paste(value, collapse = &quot; &quot;)) %&gt;%
  ungroup() %&gt;%
  group_by(grp = cumsum(field == &quot;PMC&quot;), field) %&gt;%
  summarize(value = agg_fun(value, field[1])) %&gt;%
  pivot_wider(grp, names_from = &quot;field&quot;, values_from = &quot;value&quot;) %&gt;%
  ungroup()
# # A tibble: 2 x 22
#     grp AB                                      AD                                      AID        AU        DEP    DP     FAU          IS       JT        LA    LID    PHST      PMC   PMID  PT     SO                TA     TI                         VI    MID   PG   
#   &lt;int&gt; &lt;chr&gt;                                   &lt;chr&gt;                                   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;                      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;
# 1     1 Health poverty has become the most imp~ Department of Health Management, Schoo~ 10.3389/f~ Ma Y Xia~ 20220~ 2022   Ma, Ying Xi~ 2296-25~ Frontier~ eng   776901 2021/09/~ PMC8~ 3523~ Journ~ &quot;Front Public He~ Front~ Poverty Vulnerability and~ 10    NA    NA   
# 2     2 This paper examines how neighborhood a~ Graduate School of Education, Universi~ 10.1016/j~ Wolf S M~ 20170~ 2017 ~ Wolf, Sharo~ 0190-74~ Children~ eng   NA     NA        PMC6~ 3014~ Journ~ &quot;Child Youth Ser~ Child~ Family poverty and neighb~ 79    NIHM~ 368-~
</code></pre>
<p>You can likely now discard <code>grp</code> (it was artifical), and if you look deeper into many of these strings, you'll realize some of them are quite long.</p>
",1,0,75,2022-06-21 19:39:29,https://stackoverflow.com/questions/72706285/organizing-text-blocks-as-rows-in-r
Pandas find multiple words from a list and assign Boolean value if found,"<p>So, I have dataframe like this,</p>
<pre><code>data = {
  &quot;properties&quot;: [&quot;FinancialOffice&quot;,&quot;Gas Station&quot;, &quot;Office&quot;, &quot;K-12 School&quot;, &quot;Commercial, Office&quot;],
}
df = pd.DataFrame(data)
</code></pre>
<p>This is my list,</p>
<pre><code>proplist = [&quot;Office&quot;,&quot;Other - Mall&quot;,&quot;Gym&quot;]
</code></pre>
<p>what I am trying to do is using the list I am trying to find out which words exactly matches with the dataframe column and for each word from the dataframe I need to assign a Boolean true/false value or 0/1. It has to be a exact match.</p>
<p>Output like this,</p>
<pre><code>properties         flag
FinancialOffice    FALSE
Gas Station        FALSE
Office             TRUE
K-12 School        FALSE
Commercial, Office TRUE
</code></pre>
<p>So, It returns TRUE for only &quot;Office&quot; because it is the exact match from the list. FinancialOffice is not because it is not in the list. Also, For the last one <strong>Commercial, Office</strong> it is TRUE because <strong>Office</strong> is found in the list even though <strong>Commercial</strong> not. So, even one of them is present it will be TRUE.</p>
<pre><code>df[&quot;flag&quot;] = df[&quot;properties&quot;].isin(proplist)
</code></pre>
<p>Above code works fine to assign a boolean true/false but It returns <strong>FALSE</strong> for the last one(Commercial,Office) as it tries to find the exact match.</p>
<p>Any help is appreciated.</p>
","python, pandas, regex, dataframe, text-mining","<p>Use a crafted regex with word delimiter:</p>
<pre><code>import re

regex = r'\b(?:%s)\b' % '|'.join(map(re.escape, proplist))
# '\\b(?:Office|Other\\ \\-\\ Mall|Gym)\\b'

df['flag'] = df['properties'].str.contains(regex, regex=True)
# for a case insensitive match add the case=False parameter
</code></pre>
<p>output:</p>
<pre><code>           properties   flag
0     FinancialOffice  False
1         Gas Station  False
2              Office   True
3         K-12 School  False
4  Commercial, Office   True
</code></pre>
",2,-2,426,2022-07-06 15:03:54,https://stackoverflow.com/questions/72885859/pandas-find-multiple-words-from-a-list-and-assign-boolean-value-if-found
Most frequent phrases from text data in R,"<p>Does anyone here have experience in identifying the most common phrases (3 ~ 7 consecutive words)? Understand that most analysis on frequency focuses on the most frequent/common word (along with plotting a WordCloud) rather than phrases.</p>
<pre><code># Assuming a particular column in a data frame (df) with n rows that is all text data
# as I'm not able to provide a sample data as using dput() on a large text file won't # be feasible here 

Text = df$Text_Column
docs = Corpus(VectorSource(Text))
...

</code></pre>
<p>Thanks in advance!</p>
","r, text, nlp, text-mining, corpus","<p>You have several options to do this in <code>R</code>. Let's grab some data first. I use the books by Jane Austen from the <code>janeaustenr</code> and do some cleaning to have each paragrah in a separate row:</p>
<pre class=""lang-r prettyprint-override""><code>library(janeaustenr)
library(tidyverse)
books &lt;- austen_books() %&gt;% 
  mutate(paragraph = cumsum(text == &quot;&quot; &amp; lag(text) != &quot;&quot;)) %&gt;% 
  group_by(paragraph) %&gt;% 
  summarise(book = head(book, 1),
            text = trimws(paste(text, collapse = &quot; &quot;)),
            .groups = &quot;drop&quot;)
</code></pre>
<p>With <strong>tidytext</strong>:</p>
<pre class=""lang-r prettyprint-override""><code>library(tidytext)
map_df(3L:7L, ~unnest_tokens(books, ngram, text, token = &quot;ngrams&quot;, n = .x)) %&gt;% # using multiple values for n is not directly implemented in tidytext
  count(ngram) %&gt;%
  filter(!is.na(ngram)) %&gt;% 
  slice_max(n, n = 10)
#&gt; # A tibble: 10 × 2
#&gt;    ngram               n
#&gt;    &lt;chr&gt;           &lt;int&gt;
#&gt;  1 i am sure         415
#&gt;  2 i do not          412
#&gt;  3 she could not     328
#&gt;  4 it would be       258
#&gt;  5 in the world      247
#&gt;  6 as soon as        236
#&gt;  7 a great deal      214
#&gt;  8 would have been   211
#&gt;  9 she had been      203
#&gt; 10 it was a          202

</code></pre>
<p>With <strong>quanteda</strong>:</p>
<pre class=""lang-r prettyprint-override""><code>library(quanteda)
books %&gt;% 
  corpus(docid_field = &quot;paragraph&quot;,
         text_field = &quot;text&quot;) %&gt;% 
  tokens(remove_punct = TRUE,
         remove_symbols = TRUE) %&gt;% 
  tokens_ngrams(n = 3L:7L) %&gt;%
  dfm() %&gt;% 
  topfeatures(n = 10) %&gt;% 
  enframe()
#&gt; # A tibble: 10 × 2
#&gt;    name            value
#&gt;    &lt;chr&gt;           &lt;dbl&gt;
#&gt;  1 i_am_sure         415
#&gt;  2 i_do_not          412
#&gt;  3 she_could_not     328
#&gt;  4 it_would_be       258
#&gt;  5 in_the_world      247
#&gt;  6 as_soon_as        236
#&gt;  7 a_great_deal      214
#&gt;  8 would_have_been   211
#&gt;  9 she_had_been      203
#&gt; 10 it_was_a          202
</code></pre>
<p>With <strong>text2vec</strong>:</p>
<pre class=""lang-r prettyprint-override""><code>library(text2vec)
library(janeaustenr)
library(tidyverse)
books &lt;- austen_books() %&gt;% 
  mutate(paragraph = cumsum(text == &quot;&quot; &amp; lag(text) != &quot;&quot;)) %&gt;% 
  group_by(paragraph) %&gt;% 
  summarise(book = head(book, 1),
            text = trimws(paste(text, collapse = &quot; &quot;)),
            .groups = &quot;drop&quot;)

library(text2vec)
itoken(books$text, tolower, word_tokenizer) %&gt;% 
  create_vocabulary(ngram = c(3L, 7L), sep_ngram = &quot; &quot;) %&gt;% 
  filter(str_detect(term, &quot;[[:alpha:]]&quot;)) %&gt;% # keep terms with at tleas one alphabetic character
  slice_max(term_count, n = 10)
#&gt; Number of docs: 10293 
#&gt; 0 stopwords:  ... 
#&gt; ngram_min = 3; ngram_max = 7 
#&gt; Vocabulary: 
#&gt;                term term_count doc_count
#&gt;  1:       i am sure        415       384
#&gt;  2:        i do not        412       363
#&gt;  3:   she could not        328       288
#&gt;  4:     it would be        258       233
#&gt;  5:    in the world        247       234
#&gt;  6:      as soon as        236       233
#&gt;  7:    a great deal        214       209
#&gt;  8: would have been        211       192
#&gt;  9:    she had been        203       179
#&gt; 10:        it was a        202       194
</code></pre>
<p><sup>Created on 2022-08-03 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v2.0.1)</sup></p>
",5,2,790,2022-07-27 15:32:48,https://stackoverflow.com/questions/73140779/most-frequent-phrases-from-text-data-in-r
Usng R - gsub using code in replacement - Replace comma with full stop after pattern,"<p>I would like to manually correct a record by using R. Last name and first name should always be separated by a comma.</p>
<pre><code>names &lt;- c(&quot;ADAM, Smith J.&quot;, &quot;JOHNSON. Richard&quot;, &quot;BROWN, Wilhelm K.&quot;, &quot;DAVIS, Daniel&quot;)
</code></pre>
<p>Sometimes, however, a full stop has crept in as a separator, as in the case of &quot;JOHNSON. Richard&quot;. I would like to do this automatically. Since the last name is always at the beginning of the line, I can simply access it via <code>sub</code>:</p>
<pre><code>sub(&quot;^[[:upper:]]+\\.&quot;,&quot;^[[:upper:]]+\\,&quot;,names)
</code></pre>
<p>However, I cannot use a function for the replacement that specifically replaces the full stop with a comma.</p>
<p>Is there a way to insert a function into the replacement that does this for me?</p>
","r, string, replace, text-mining, gsub","<p>Your <code>sub</code> is mostly correct, but you'll need a capture group (the brackets and backreference <code>\\1</code>) for the replacement.</p>
<p>Because we are &quot;capturing&quot; the upper case letters, therefore <code>\\1</code> here represents the original upper case letters in your original strings. The only replacement here is <code>\\.</code> to <code>\\,</code>. In other words, we are replacing upper case letters <code>^(([[:upper:]]+)</code> <strong>AND</strong> full stop <code>\\.</code> with it's original content <code>\\1</code> <strong>AND</strong> comma <code>\\,</code>.</p>
<p>For more details you can visit this <a href=""https://www.regular-expressions.info/refcapture.html"" rel=""nofollow noreferrer"">page</a>.</p>
<pre><code>test_names &lt;- c(&quot;ADAM, Smith J.&quot;, &quot;JOHNSON. Richard&quot;, &quot;BROWN, Wilhelm K.&quot;, &quot;DAVIS, Daniel&quot;)

sub(&quot;^([[:upper:]]+)\\.&quot;,&quot;\\1\\,&quot;,test_names)
[1] &quot;ADAM, Smith J.&quot;    &quot;JOHNSON, Richard&quot;  &quot;BROWN, Wilhelm K.&quot;
[4] &quot;DAVIS, Daniel&quot;   
</code></pre>
",3,0,225,2022-08-02 08:35:39,https://stackoverflow.com/questions/73204377/usng-r-gsub-using-code-in-replacement-replace-comma-with-full-stop-after-pat
is package tm suitable for extracting scores from text data?,"<p>I have many cognitive assessment data stored as txt files. Each file looks like this:</p>
<pre><code>patient number xxxxxx
score A        (98) (95)ile%
score B        (100) (97)ile%
test C
   score D     (76)
   score E     (80)
(the real report is longer and more orderless than this)
</code></pre>
<p>As the example data showed, the format of each score is not well ordered. It's easy to read, but hard to analyze. I want to extract scores of each test for each patient and create a table for further analysis. Because I've never use text mining function or package in R before. I'm wondering if it's more appropriate to do it with text mining package in R, or is it ok if I just treat the whole report as a very long string? What are the difference? Thanks!</p>
<p>the actual report I'm dealing with looks like this (I've converted all actual number to &quot;X&quot;)</p>
<pre><code> ORIENTATION                                   R.S.      %ile      N/D/B
     Temporal orientation-Error score          ( xx)  ( xx  )%ile ( x )
     Orientation to Personal Information       ( x )/8   ( xx  )%ile ( x )
     Orientation to Place                      ( x )/4   ( xx  )%ile ( x )

   WMS-III - Verbal Memory                       R.S.     %ile       N/D/B
     Verbal Paired
       Associates-I       Scale score ( -  )   ( -  )/32 ( -   )%ile ( - )
     Verbal Paired
       Associates-II      Scale score ( -  )   ( -  )/8  ( -   )%ile ( - )
     Word List Memory-I   Scale score ( x  )   ( xx )/48 ( x   )%ile ( x )
     Word List Memory-II  Scale score ( x  )   ( xx  )/12 ( x   )%ile ( x )
     Logical Memory-I     Scale score ( x  )   ( xx )/75 ( x  )%ile ( x )
     Logical Memory-II    Scale score ( x  )   ( xx )/50 ( x   )%ile ( x )
     Faces Memory-I      Scale score ( -  )  ( -  )/48   ( -   )%ile ( - )
     Faces Memory -II    Scale score ( -  )  ( -  )/48   ( -   )%ile ( - )
     Visual
       Reproduction-I    Scale score ( x  )  ( xx  )/104 ( x  )%ile ( x )
     Visual
       Reproduction-II   Scale score ( x  )  ( xx  )/104 ( x   )%ile ( x )
     Spatial Memory
      F:( x ) B:( x )    Scale score ( xx )  ( xx )/32   ( xx  )%ile ( N )
   LANGUAGE                                      R.S.      %ile      N/D/B
     Visual Naming                             ( xx )/60 ( xx  )%ile ( x )
     Object Naming  A+B                        ( -  )/16 ( -   )%ile ( - )
     Aural Comprehension                       ( xx )/18 ( xx  )%ile ( x )
     Semantic Association of Verbal Fluency    ( xx  )   ( xx  )%ile ( x )
     (                                         (       ) (     )%ile (   )
     (                                         (       ) (     )%ile (   )
    WCST-S    Number cards used               ( xx  )               (   )
              Number complete categories      ( x/x )   ( x   )%ile ( x )
              Number perseverative errors     ( xx  )   ( x  )%ile ( x )
              Number non-perseverative errors ( xx  )   ( xx  )%ile ( x )
    Trails Making Test-Part A           Time  ( xx  )   ( xx  )%ile ( x )
    Trails Making Test-Part B           Time  ( N/A )   (     )%ile (   )
    (                                         (       ) (     )%ile (   )
    (                                         (       ) (     )%ile (   )

  SPATIAL PERCEPTUAL FUNCTION                   R.S.      %ile      N/D/B
    Judgment of Line Orientation  Form( x )   ( x )/30 ( xx  )%ile ( x )
    3-D Block Construction-Model  Form(   )
                                       score (     )/29 (     )%ile (   )
                                       Time  (     )s   (     )%ile (   )
    MANUAL DEXTERITY                              R.S.      %ile      N/D/B
      Purdue Pegboard                 RH         (    )   (     )%ile (   )
                                      LH         (    )   (     )%ile (   )
                                      Both Hands (    )   (     )%ile (   )
IMPRESSION：
&lt; xxxxxxxxxxxxxxxxxxxxxxxxxxx  &gt;
    Age : ( xx  ) y/o
    Edu : ( xx  ) yrs
    Handedness : ( xx  )

  MINI-MENTAL EXAMINATION                      R.S.      %ile      N/D/B
    MMSE                                     ( xx )/30 (     )%ile ( x )
    
    
PERSONALITY ASSESSMENT                        R.S.      %ile      N/D/B  SCL-90R                                  ( x.xx )   (     )%ile ( N )  Frontal Behavioral Inventory                 Negative behavior score   ( xx )/36  (     )%ile ( N )                     Disinhibition score   ( x  )/36  (     )%ile ( x )                             Total score   ( xx )/72  (     )%ile ( x )  BDI-II                                   (    )/63  (     )%ile (   )  BAI                                      (    )/6   (     )%ile (   )


</code></pre>
","r, text-mining, stringr, tm","<p>It's hard to say without seeing the actual file (or a similar example file), but my guess is that you could use regular expressions to pull out what you need. If you do convert that whole thing into one long string, you'll probably get something like this:</p>
<pre><code>library(stringr)

string &lt;- &quot;patient number xxxxxx\nscore A        (98) (95)ile%\nscore B        (100) (97)ile%\ntest C\n   score D     (76)\n   score E     (80)&quot;

</code></pre>
<p>You could then pull out patient numbers with something like this:</p>
<pre><code>patient_number &lt;- str_extract(string, &quot;(?&lt;=patient number).*&quot;)

</code></pre>
<p>And then score names like this</p>
<pre><code>score_name &lt;- str_extract_all(string, &quot;score [A-Z]&quot;) %&gt;% unlist()


</code></pre>
<p>Then the actual scores</p>
<pre><code>score &lt;- str_extract_all(string, &quot; \\([0-9]{1,3}\\)( |(?=\\\n)|)&quot;) %&gt;% 
unlist() %&gt;% 
str_squish() %&gt;% 
str_replace(&quot;\\(&quot;,&quot;&quot;) %&gt;% 
str_replace(&quot;\\)&quot;,&quot;&quot;)

</code></pre>
<p>Then put it all together into a dataframe</p>
<pre><code>scores_df &lt;- data.frame(patient_number,score_name, score)

</code></pre>
<pre><code>scores_df
</code></pre>
<pre><code>&gt; scores_df
  patient_number score_name score
1         xxxxxx    score A    98
2         xxxxxx    score B   100
3         xxxxxx    score D    76
4         xxxxxx    score E    80
</code></pre>
<p>Please consider editing your question to include a sample of your actual data and a more specific description of what you want to pull out. If you do that we'll be able to give you much better help than this random example :)</p>
",1,0,160,2022-08-13 13:54:41,https://stackoverflow.com/questions/73344857/is-package-tm-suitable-for-extracting-scores-from-text-data
finding the index of word and its distance,"<p>hello there ive been trying to code a script that finds the distance the word <code>one</code> to the other <code>one</code> in a text but the code somehow doesnt work well...</p>
<pre><code>import re

#dummy text
words_list = ['one']
long_string = &quot;are marked by one  the ()meta-characters. two They group together the expressions contained one inside them, and you can one repeat the contents of a group with a repeating qualifier, such as there one&quot;

striped_long_text = re.sub(' +', ' ',(long_string.replace('\n',' '))).strip()
length = []

index_items = {}

for item in words_list :
    text_split = striped_long_text.split('{}'.format(item))[:-1]

    for space in text_split:
        if space:
            length.append(space.count(' ')-1)

print(length)
</code></pre>
<p><strong>dummy Text :</strong></p>
<blockquote>
<p>are marked by one  the ()meta-characters. two They group together the expressions contained one inside them, and you can one repeat the contents of a group with a repeating qualifier, such as there one</p>
</blockquote>
<p>so what im trying to do is that find the exact distance the word <code>one</code> has to its next similar <code>one</code> in text as you see , this code works well with in some exceptions... if the first text has the word <code>one</code> after 2-3 words the index starts to the counting from the start of the text and this will fail the output but for example if add the word <code>one</code> at the start of the text the code will work fine...</p>
<p><strong>Output of the code vs what it should be :</strong></p>
<br>
<p><code>result = [2, 9, 5, 13]</code>
<code>expected result = [ 9, 5, 13] </code></p>
","python, python-3.x, list, text-mining","<p>Another solution, using <code>itertools.groupby</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from itertools import groupby

words_list = [&quot;one&quot;]
long_string = &quot;are marked by one  the ()meta-characters. two They group together the expressions contained one inside them, and you can one repeat the contents of a group with a repeating qualifier, such as there one&quot;

for w in words_list:
    out = [
        (v, sum(1 for _ in g))
        for v, g in groupby(long_string.split(), lambda k: k == w)
    ]

    # check first and last occurence:
    if out and out[0][0] is False:
        out = out[1:]

    if out and out[-1][0] is False:
        out = out[:-1]

    out = [length for is_word, length in out if not is_word]

print(out)
</code></pre>
<p>Prints:</p>
<pre class=""lang-py prettyprint-override""><code>[9, 5, 13]
</code></pre>
",0,1,57,2022-08-22 09:51:43,https://stackoverflow.com/questions/73443199/finding-the-index-of-word-and-its-distance
Is there a way to create a network of word associations using a bi-partite network analysis in R?,"<p>I have a text file with words from historical accounts and I want to visualise the species and frequency of words associated with them.</p>
<p>So far I have tried using the following code with a txt file of all the historical documents in one doc but want to ask if there is specific formatting of a csv to then input into R for a bipartite network graph:</p>
<pre><code>&quot;&quot;&quot;library(ggraph)
library(ggplot2)
library(dplyr)
library(pdftools)
library(tm)
library(readtext)
library(tidytext)
library(igraph)
library(tidyr)
library(FactoMineR)
library(factoextra)
library(flextable)
library(GGally)
library(ggdendro)
library(network)
library(Matrix)
library(quanteda)
library(stringr)
library(quanteda.textstats)

options(stringsAsFactors = F)
options(scipen = 999)
options(max.print=1000)

# Read in text--------
wordbase &lt;- readtext(&quot;mq_bird_stories.txt&quot;)

# List of extra words to remove---------
extrawords &lt;- c(&quot;the&quot;, &quot;can&quot;, &quot;get&quot;, &quot;Ccchants&quot;, &quot;make&quot;, &quot;making&quot;, &quot;house&quot;, &quot;torn&quot;, &quot;tree&quot;, &quot;man&quot;, &quot;however&quot;, &quot;upon&quot;, &quot;instructs&quot;, &quot;wife&quot;, &quot;coming&quot;,&quot;without&quot;, &quot;mother&quot;, &quot;versions&quot;,&quot;variant&quot;, &quot;version&quot;, &quot;thus&quot;, &quot;got&quot;,&quot;throws&quot;, &quot;are&quot;, &quot;has&quot;, &quot;already&quot;, &quot;asks&quot;, &quot;sacra&quot;, &quot;can&quot;, &quot;brings&quot;, &quot;one&quot;, &quot;look&quot;, &quot;sees&quot;, &quot;tonaheiee&quot;, &quot;wants&quot;, &quot;later&quot;,
                &quot;dont&quot;, &quot;even&quot;, &quot;may&quot;, &quot;but&quot;, &quot;will&quot;, &quot;turn&quot;, &quot;sing&quot;, &quot;swallows&quot;, &quot;alba&quot;,  &quot;gives&quot;, &quot;find&quot;, &quot;other&quot;,&quot;tonaheieee&quot;, &quot;away&quot;,&quot;day&quot;,&quot;comes&quot;,&quot;another&quot;,
                &quot;much&quot;, &quot;first&quot;, &quot;but&quot;, &quot;see&quot;, &quot;new&quot;, &quot;back&quot;,&quot;goes&quot;, &quot;go&quot;,&quot;songs&quot;,  &quot;returns&quot;, &quot;take&quot;,&quot;takes&quot;,&quot;come&quot;,
                &quot;many&quot;, &quot;less&quot;, &quot;now&quot;, &quot;well&quot;, &quot;taught&quot;, &quot;like&quot;, &quot;puts&quot;, &quot;slits&quot;, &quot;sends&quot;, &quot;tell&quot;,&quot;tells&quot;,&quot;open&quot;,&quot;mentions&quot;,
                &quot;often&quot;, &quot;every&quot;, &quot;said&quot;, &quot;two&quot;, &quot;and&quot;, &quot;handsome&quot;, &quot;husband&quot;, &quot;bring&quot;, &quot;lives&quot;,&quot;gets&quot;, &quot;von&quot;, &quot;den&quot;, &quot;steinen&quot;, &quot;handy&quot;)

# Clean the data-------
darwin &lt;- wordbase %&gt;%
  paste0(collapse = &quot; &quot;) %&gt;%
  stringr::str_squish() %&gt;%
  stringr::str_remove_all(&quot;\\(&quot;) %&gt;% 
  stringr::str_remove_all(&quot;\\)&quot;) %&gt;% 
  stringr::str_remove_all(&quot;!&quot;) %&gt;% 
  stringr::str_remove_all(&quot;,&quot;) %&gt;% 
  stringr::str_remove_all(&quot;;&quot;) %&gt;% 
  stringr::str_remove_all(&quot;\\?&quot;) %&gt;% 
  stringr::str_split(fixed(&quot;.&quot;)) %&gt;% 
  unlist() %&gt;% 
  tm :: removeWords(extrawords) %&gt;% 
  paste0(collapse = &quot; &quot;)




# One method for calculating frequencies of bigrams------
# Process into a table of words
darwin_split &lt;- darwin %&gt;% 
  as_tibble() %&gt;%
  tidytext::unnest_tokens(words, value)

# Create data frame of bigrams-------
darwin_words &lt;- darwin_split %&gt;%
  dplyr::rename(word1 = words) %&gt;%
  dplyr::mutate(word2 = c(word1[2:length(word1)], NA)) %&gt;%
  na.omit()

# Calculate frequency of bigrams-----
darwin2grams &lt;- darwin_words %&gt;%
  dplyr::mutate(bigram = paste(word1, word2, sep = &quot; &quot;)) %&gt;%
  dplyr::group_by(bigram) %&gt;%
  dplyr::summarise(frequency = n()) %&gt;%
  dplyr::arrange(-frequency)

# Define stopwords
stps &lt;- paste0(tm::stopwords(kind = &quot;en&quot;), collapse = &quot;\\b|\\b&quot;)

# Remove stopwords from bigram table
darwin2grams_clean &lt;- darwin2grams %&gt;%
  dplyr::filter(!str_detect(bigram, stps))




# Another method for calculating frequencies of bigrams
# Clean corpus
darwin_clean &lt;- darwin %&gt;%
  stringr::str_to_title()

# Tokenize corpus----
darwin_tokzd &lt;- quanteda::tokens(darwin_clean)

# Extract bigrams------
BiGrams &lt;- darwin_tokzd %&gt;% 
  quanteda::tokens_remove(stopwords(&quot;en&quot;)) %&gt;% 
  quanteda::tokens_select(pattern = &quot;^[A-Z]&quot;, 
                          valuetype = &quot;regex&quot;,
                          case_insensitive = FALSE, 
                          padding = TRUE) %&gt;% 
  quanteda.textstats::textstat_collocations(min_count = 1, tolower = FALSE)

# read in and process text
darwinsentences &lt;- darwin %&gt;%
  stringr::str_squish() %&gt;%
  tokenizers::tokenize_sentences(.) %&gt;%
  unlist() %&gt;%
  stringr::str_remove_all(&quot;- &quot;) %&gt;%
  stringr::str_replace_all(&quot;\\W&quot;, &quot; &quot;) %&gt;%
  stringr::str_squish()
# inspect data
head(darwinsentences)

darwincorpus &lt;- Corpus(VectorSource(darwinsentences))

# clean corpus-----
darwincorpusclean &lt;- darwincorpus %&gt;%
  tm::tm_map(removeNumbers) %&gt;%
  tm::tm_map(tolower) %&gt;%
  tm::tm_map(removeWords, stopwords()) %&gt;%
  tm::tm_map(removeWords, extrawords)
# create document term matrix
darwindtm &lt;- DocumentTermMatrix(darwincorpusclean, control=list(bounds = list(global=c(1, Inf)), weighting = weightBin))
# convert dtm into sparse matrix
darwinsdtm &lt;- Matrix::sparseMatrix(i = darwindtm$i, j = darwindtm$j, 
                                   x = darwindtm$v, 
                                   dims = c(darwindtm$nrow, darwindtm$ncol),
                                   dimnames = dimnames(darwindtm))
# calculate co-occurrence counts
coocurrences &lt;- t(darwinsdtm) %*% darwinsdtm
# convert into matrix
collocates &lt;- as.matrix(coocurrences)
# inspect size of matrix
ncol(collocates)
#provide some summary stats
summary(rowSums(collocates))

#visualising collocations

# load function for co-occurrence calculation
source(&quot;https://slcladal.github.io/rscripts/calculateCoocStatistics.R&quot;)
# define term
coocTerm &lt;- &quot;pigeon&quot;
# calculate co-occurrence statistics
coocs &lt;- calculateCoocStatistics(coocTerm, darwinsdtm, measure=&quot;LOGLIK&quot;)
# inspect results
coocs[1:50]

coocdf &lt;- coocs %&gt;%
  as.data.frame() %&gt;%
  dplyr::mutate(CollStrength = coocs,
                Term = names(coocs)) %&gt;%
  dplyr::filter(CollStrength &gt; 0)

###Make graph - visualize association strengths------

ggplot(coocdf, aes(x = reorder(Term, CollStrength, mean), y = CollStrength)) +
  geom_point() +
  coord_flip() +
  theme_bw() +
  labs(y = &quot;&quot;)


##network
net = network::network(collocates_redux, 
                       directed = FALSE,
                       ignore.eval = FALSE,
                       names.eval = &quot;weights&quot;)
# vertex names
network.vertex.names(net) = rownames(collocates_redux)
# inspect object
net

ggnet2(net,label = TRUE, 
       label.size = 4,
       alpha = 0.2,
       size.cut = 3,
       edge.alpha = 0.3) +
  guides(color = FALSE, size = FALSE)&quot;&quot;&quot;
</code></pre>
","r, network-programming, text-mining","<p>I'd suggest taking a look at the netCoin package. If you can transform your data into nodes and links data frames, then you can easily get a high quality network visualization:</p>
<pre><code>#Example of links data frame
links &lt;-
  data.frame(
    matrix(
      c(
        &quot;Person A&quot;,&quot;Account 1&quot;, &quot;not link&quot;,
        &quot;Person A&quot;,&quot;Account 2&quot;, &quot;link&quot;,
        &quot;Person B&quot;,&quot;Account 2&quot;, &quot;link&quot;,
        &quot;Person B&quot;,&quot;Account 3&quot;, &quot;not link&quot;,
        &quot;Person B&quot;,&quot;Account 4&quot;, &quot;link&quot;,
        &quot;Person C&quot;,&quot;Account 4&quot;, &quot;link&quot;
      ),
      nrow = 6,
      ncol = 3,
      byrow = TRUE,
      dimnames = list(NULL,
                      c(&quot;Source&quot;, &quot;Target&quot;, &quot;other_links_column&quot;))
    ),
    stringsAsFactors = FALSE
  )

#Example of nodes data frame
nodes &lt;-
  data.frame(
    matrix(
      c(
        &quot;Person A&quot;,&quot;person&quot;,
        &quot;Person B&quot;,&quot;person&quot;,
        &quot;Person C&quot;,&quot;person&quot;,
        &quot;Account 1&quot;, &quot;account&quot;,
        &quot;Account 2&quot;, &quot;account&quot;,
        &quot;Account 3&quot;, &quot;account&quot;,
        &quot;Account 4&quot;, &quot;account&quot;
      ),
      nrow = 7,
      ncol = 2,
      byrow = TRUE,
      dimnames = list(NULL,
                      c(&quot;name&quot;, &quot;other_nodes_column&quot;))
    ),
    stringsAsFactors = FALSE
  )

install.packages(&quot;netCoin&quot;) #may need to install the netCoin package
library(netCoin)
?netCoin #displays netCoin Help to see all the function options


graph_df &lt;- netCoin(nodes =  nodes, #Data frame of unique nodes and their attributes #Must contain name column
                     links = links, #Data frame of links and their attributes #Must contain Source and Target columns
                     cex = 1.25, #Font size
                     color = &quot;other_nodes_column&quot;, #Column in node data frame to determine node color 
                     shape = &quot;other_nodes_column&quot;, #Column in node data frame to determine node shape                    
                     main = &quot;This is the title of my visualization&quot;, #Visualization title
                     controls = 1:5, #Controls that will be shown in the visualization (maximum of 5)
                     dir = &quot;folder-with-viz-output&quot;) #Output folder for the visualization #Entire folder should be exported as a zip file

plot(graph_df) #Command to display the visualization
</code></pre>
",0,0,208,2022-09-02 08:30:22,https://stackoverflow.com/questions/73579816/is-there-a-way-to-create-a-network-of-word-associations-using-a-bi-partite-netwo
Python get multiple docx file names and extract specific words from the files to generate a dataframe or table,"<p>I hope to read multiple word documents (docx files) in a folder and then search a specific word e.g. &quot;laptop&quot; from each of docx file to generate a table or a dataframe.
For instance: in my folder I have file_1.docx, file_2.docx ... file_n.docx, each file may or may not contain work &quot;Laptop&quot;. In the end I hope to generate a table like:</p>
<pre class=""lang-none prettyprint-override""><code>FileName          Keyword
file_1.docx       &quot;laptop&quot;
file_2.docx       &quot;laptop&quot;
...
</code></pre>
","python, text-mining, python-docx, docx2txt","<p>If you are using Python3.X you will need to do</p>
<blockquote>
<p>pip install python-docx</p>
</blockquote>
<p>Not to be confuse with docx as I had some issues using this.</p>
<pre><code>import os
from docx import Document
import pandas as pd

match_word = &quot;laptop&quot;
match_items = []
folder = 'C:\\Dev\\Docs'
file_names = os.listdir(folder)
file_names = [file for file in file_names if file.endswith('.docx')]
file_names = [os.path.join(folder, file) for file in file_names]

For file in file_names:
    document = Document(file)
    for paragraph in document.paragraphs:
        if match_word in paragraph.text:
            match_items.append([file, match_word])

the_df = pd.DataFrame(
    match_items,
    columns=['file_name', 'word_match'],
    index=[i[0] for i in match_items]
)

print(the_df)
</code></pre>
<p>Output:</p>
<pre><code>file_name              word_match
C:\Dev\Docs\c.docx     laptop
</code></pre>
",2,0,1326,2022-09-08 05:02:47,https://stackoverflow.com/questions/73643948/python-get-multiple-docx-file-names-and-extract-specific-words-from-the-files-to
Facet plot word proportions from 3 groups against each other in R (tidytext + ggplot2),"<p>In the first chapter of <em>Text Mining with R</em> Silge &amp; Robinson propose the following code to produce the plots shown in <a href=""https://www.tidytextmining.com/tidytext.html#word-frequencies"" rel=""nofollow noreferrer"">https://www.tidytextmining.com/tidytext.html#word-frequencies</a>, comparing word frequencies (proportions) from Austen's books, to Wells' and the Brontë Sisters'.</p>
<pre><code>library(gutenbergr)
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidytext)
library(tidyr)
library(scales)

original_books &lt;- austen_books() %&gt;%
  group_by(book) %&gt;%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                                     regex(&quot;^chapter [\\divxlc]&quot;,
                                           ignore_case = TRUE)))) %&gt;%
  ungroup()

tidy_books &lt;- original_books %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words)

hgwells &lt;- gutenberg_download(c(35, 36, 5230, 159))
tidy_hgwells &lt;- hgwells %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words)

bronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 767))
tidy_bronte &lt;- bronte %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words)

frequency &lt;- bind_rows(mutate(tidy_bronte, author = &quot;Brontë Sisters&quot;),
                       mutate(tidy_hgwells, author = &quot;H.G. Wells&quot;), 
                       mutate(tidy_books, author = &quot;Jane Austen&quot;)) %&gt;% 
  mutate(word = str_extract(word, &quot;[a-z']+&quot;)) %&gt;%
  count(author, word) %&gt;%
  group_by(author) %&gt;%
  mutate(proportion = n / sum(n)) %&gt;% 
  select(-n) %&gt;% 
  pivot_wider(names_from = author, values_from = proportion) %&gt;%
  pivot_longer(`Brontë Sisters`:`H.G. Wells`,
               names_to = &quot;author&quot;, values_to = &quot;proportion&quot;)

ggplot(frequency, aes(x = proportion, y = `Jane Austen`, 
                      color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = &quot;gray40&quot;, lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position=&quot;none&quot;) +
  labs(y = &quot;Jane Austen&quot;, x = NULL)
</code></pre>
<p>This effectively produce 2 plots in a single facet (Austen against Wells, and Austen against the Brontës). But a plot is missing there: that of Wells vs. the Brontë sisters, and though it's fairly trivial to plot that as a single graph on its own, I'm having trouble plotting the <em>3 graphs together</em>, ie., as a 3-column facet depicting each author's word frequencies vs. the others.</p>
<p>Any advice on achieving this visualization?
Thank you!</p>
","r, ggplot2, nlp, tidyverse, text-mining","<p>Here's a modified approach where the <code>pivot_longer</code> treats the authors on equal footing, then we join that <code>frequency</code> table to itself and just keep the inter-author comparisons. (You could keep the Austen vs. Austen but it would just be a line.)</p>
<pre><code>  ...
  pivot_wider(names_from = author, values_from = proportion) %&gt;%
  pivot_longer(-word, names_to = &quot;author&quot;, values_to = &quot;proportion&quot;)

frequency %&gt;%
  left_join(frequency, by = &quot;word&quot;) %&gt;%
  filter(author.x != author.y) %&gt;%
  replace_na(list(proportion.x = 0, proportion.y = 0)) %&gt;%

ggplot(aes(x = proportion.x, y = proportion.y,
                      color = abs(proportion.y - proportion.x))) +
  geom_abline(color = &quot;gray40&quot;, lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) +
  facet_grid(author.x ~ author.y) +
  theme(legend.position=&quot;none&quot;) +
  labs(y = &quot;Jane Austen&quot;, x = NULL)
</code></pre>
<p><a href=""https://i.sstatic.net/ils14.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ils14.png"" alt=""enter image description here"" /></a></p>
<p>You might consider adding a <code>slice_sample(n = 10000) %&gt;%</code> or similar in between the prep and the ggplot in case it's too slow for your taste. In my experience ggplot2 starts to get too sluggish for my taste when n &gt; 10k; in this case the combined data has 171k points so it's real slow.</p>
",1,0,49,2022-09-08 19:51:27,https://stackoverflow.com/questions/73654403/facet-plot-word-proportions-from-3-groups-against-each-other-in-r-tidytext-gg
Python regex - Extract all the matching text between two patterns,"<p>I want to extract all the text in the bullet points numbered as 1.1, 1.2, 1.3 etc. Sometimes the bullet points can have space like 1. 1, 1. 2, 1 .3, 1 . 4</p>
<p>Sample text</p>
<pre><code>    text = &quot;some text before pattern 1.1 text_1_here  1.2 text_2_here  1 . 3 text_3_here  1. 4 text_4_here  1 .5 text_5_here 1.10 last_text_here 1.23 text after pattern&quot;
</code></pre>
<p>For the text above, the output should be
[' text_1_here  ', ' text_2_here  ', ' text_3_here  ', ' text_4_here  ', ' text_5_here ', ' last_text_here  ']</p>
<p>I tried regex findall but not getting the required output. It is able to identify and extract 1.1 &amp; 1.2 and then 1.3 &amp; 1.4. It is skipping text between 1.2 &amp; 1.3.</p>
<pre><code>    import re
    re.findall(r'[0-9].\s?[0-9]+(.*?)[0-9].\s?[0-9]+', text)
</code></pre>
","python, regex, text, text-mining","<p>I'm unsure about the exact rule why you'd want to exclude the last bit of text but based on your comments it seems we could also just split the entire text on the bullits and simply exclude the 1st and last element from the resulting array:</p>
<pre><code>re.split(r'\s+\d(?:\s*\.\s*\d+)+\s+', text)[1:-1]
</code></pre>
<p>Which would output:</p>
<pre><code>['text_1_here', 'text_2_here', 'text_3_here', 'text_4_here', 'text_5_here', 'last_text_here']
</code></pre>
",4,0,495,2022-09-28 12:07:27,https://stackoverflow.com/questions/73881130/python-regex-extract-all-the-matching-text-between-two-patterns
How to read PDFs line by line in R?,"<p>I was using read_pdf() function from pdftools package to read PDF files line by line, but suddenly without changing anything in the script, any argument or line, it started reading the whole page instead of separating the elements by line. How do I get it to go back to line by line separation? This is the only way I can use text mining to build the database I need.</p>
","r, pdf, text-mining","<p>With the following code, you can text line by line by reading the PDF file directly</p>
<pre><code>library(pdftools)
library(pagedown)

chrome_print(input = &quot;https://en.wikipedia.org/wiki/Cat&quot;, 
             output = &quot;D:\\Text_PDF_Cat.pdf&quot;)

text &lt;- pdf_text(&quot;D:\\Text_PDF_Cat.pdf&quot;)
text &lt;- lapply(X = text, FUN = function(x) strsplit(x, &quot;\n&quot;))
text &lt;- unlist(text)
</code></pre>
",0,0,1278,2022-09-30 12:03:44,https://stackoverflow.com/questions/73908479/how-to-read-pdfs-line-by-line-in-r
How to aggregate group of rows by a irregular interval in R?,"<p>I have a data frame with lines of a transcription of a conversation, in which what was said by each person is separated by an empty line. I now need to aggregate the lines so that each one is a row, but the line ranges are irregular. How can I aggregate this data?</p>
<p>The data are like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Speech</th>
<th>Sep line</th>
</tr>
</thead>
<tbody>
<tr>
<td>Was in Augoust</td>
<td>0</td>
</tr>
<tr>
<td>Don't you remember?</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>1</td>
</tr>
<tr>
<td>Yes, i did</td>
<td>0</td>
</tr>
<tr>
<td>It was a hot Saturday</td>
<td>0</td>
</tr>
<tr>
<td>we were in the park</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>1</td>
</tr>
<tr>
<td>That's right</td>
<td>0</td>
</tr>
<tr>
<td>it was a fun day</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>I want the date to be like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>speech</th>
</tr>
</thead>
<tbody>
<tr>
<td>Was in Augoust, Don't you remember?</td>
</tr>
<tr>
<td>Yes, i did. It was a hot Saturday, we were in the park</td>
</tr>
<tr>
<td>That's right,it was a fun day</td>
</tr>
</tbody>
</table>
</div>","r, aggregate, character, text-mining","<p>Here's a way with <code>dplyr</code> -</p>
<pre><code>df %&gt;% 
  mutate(group = cumsum(sep_line)) %&gt;% 
  filter(sep_line == 0) %&gt;% 
  group_by(group) %&gt;% 
  summarise(
    speech = paste(speech, collapse = &quot; &quot;)
  ) %&gt;% 
  select(speech)
</code></pre>
",2,0,51,2022-10-11 04:36:04,https://stackoverflow.com/questions/74023033/how-to-aggregate-group-of-rows-by-a-irregular-interval-in-r
"Given an R dataframe with two columns with strings of words in each, remove words repeating in between the first and second column in each row","<p>I have an R data frame like the example below (but with ten of thousands of rows).</p>
<pre><code>A1 &lt;- c(&quot;AB AC AD AE AF AG&quot;,&quot;AB AD AH AI AJ&quot;)
q1 &lt;- c(&quot;AB AC AE&quot;,&quot;AD AJ AI&quot;)
id &lt;- 1:2
df &lt;- data.frame(id,A1,q1)
</code></pre>
<p>I would like the result to look like this:</p>
<pre><code>df$A_clean &lt;- c(&quot;AD AF AG&quot;, &quot;AB AH&quot;)
</code></pre>
<p>I have tried using the &quot;str_split&quot; and &quot;exclude&quot; function that is part of the qdap package but this seems to work on the whole column at once rather than on a row-by-row basis in the data frame and just gives me the unique words in each row of A1 excluding all the words in the q1 column.</p>
","r, string, data-cleaning, text-mining","<p>In base R, with <code>intersect</code>:</p>
<pre class=""lang-r prettyprint-override""><code>mapply(\(x, y) paste(setdiff(x, y), collapse = &quot; &quot;), 
       strsplit(df$A1, &quot; &quot;), strsplit(df$q1, &quot; &quot;))
#[1] &quot;AD AF AG&quot; &quot;AB AH&quot; 
</code></pre>
",2,1,51,2022-10-11 13:40:49,https://stackoverflow.com/questions/74029113/given-an-r-dataframe-with-two-columns-with-strings-of-words-in-each-remove-word
How to convert raw lines to df,"<p>I need to read a df from a pdf file and here is an example table</p>
<p><a href=""https://i.sstatic.net/sDDi9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sDDi9.png"" alt=""enter image description here"" /></a></p>
<p>So far I was able to read the data as raw lines with the following chunk</p>
<pre><code>library(pdftools)
library(tidyverse)

pdf_file &lt;- pdf_text(&quot;exm.pdf&quot;)

raw_df &lt;- pdf_file %&gt;%
  read_lines() %&gt;%
  data.frame() %&gt;% 
  rename(rawline = 1)

raw_df &lt;- raw_df %&gt;% 
  mutate(
    rawline = str_replace(string = rawline,
                          pattern = &quot;^ \\s*&quot;,
                          replacement = &quot;&quot;)
    )
</code></pre>
<p>here is the structure of raw df</p>
<pre><code>&gt; raw_df
                                     rawline
1      Id   Name    Address           Mobile
2 1    Kiran   Bengaluru,        99999 99999
3                                Mysore Road
4                                   6th Lane
5 2    John    Mandya            77777 77777
6                            Taluka Junction
7 3    Ravi    Mysore            88888 88888
</code></pre>
<p>How can i convert this into a proper df? I tried filtering out the lines that start with a digit by using regex but after that I got stuck. I need to gather address lines (that have no number at the beginning) and attach them to the previous address text and then split lines into columns. I tried splitting based the space between id, name, address and Mobile but it is not constant across all lines. How can I resolve this issue? Thanks in advance.</p>
<p><strong>Edit</strong></p>
<p>As suggested, I tried <strong>pdf_data</strong> and i got a table (head(15)) like this with x and y positions of the text</p>
<pre><code># A tibble: 15 x 6
   width height     x     y space text      
   &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; &lt;chr&gt;     
 1     8     11    77    74 FALSE Id        
 2     5     11    77    88 FALSE 1         
 3    26     11   181    74 FALSE Name      
 4    23     11   181    88 FALSE Kiran     
 5     5     11    77   129 FALSE 2         
 6    20     11   181   129 FALSE John      
 7     5     11    77   156 FALSE 3         
 8    18     11   181   156 FALSE Ravi      
 9    35     11   294    74 FALSE Address   
10    48     11   294    88 FALSE Bengaluru,
11    33     11   294   102 TRUE  Mysore    
12    22     11   330   102 FALSE Road      
13     5     11   294   115 FALSE 6         
14     5      6   299   114 TRUE  th        
15    21     11   308   115 FALSE Lane
</code></pre>
<p>based on this table I can filter out the x values and get the columns as vectors. but if there are spaces in the values( like address) this filtering will not work. Is there a way to gather address column based on x and y values?</p>
<p>Basically I need to gather rows based on a value (ex: x == 294) until the same value appears then I can use <strong>str_c</strong> to merge those cells to a single string.</p>
","r, pdf, text-mining, pdftools","<p>based on your first method, try this function after getting row_df :</p>
<pre><code>library(dplyr)
parse_pdfs_lines_ById&lt;- function(raw_df){
 
# ----delete rownames : the first character and space
raw_df=raw_df%&gt;%
 mutate(rawline=sub('.', '', rawline))%&gt;%
 # ----remove the first space to keep Id as a first word
 mutate(rawline=gsub('^ ', '', rawline))  

# ------ now ignore the raw of colnames
raw_df=data.frame(rawline=raw_df[-1,])


# ---------assign  the correct id to  correct line 
# id=&quot;&quot;
# initialize index of line
i=1
while (i&lt;nrow(raw_df))
{
 if(grepl(&quot;^[0-9]&quot;,raw_df$rawline[i]))
 {
   # get the id , first word of line ./!\ not the first character! e.g : id == 22 )
   id=stringr::word(raw_df$rawline[i],1)
 }else{ 
   raw_df$rawline[i]=paste0(id,raw_df$rawline[i])
 }   
 i=i+1
}
# &gt; raw_df
#                                    rawline
# 1    Kiran   Bengaluru,        99999 99999
# 1                               Mysore Road
# 1                                  6th Lane
# 2    John    Mandya            77777 77777
# 2                           Taluka Junction
# 3    Ravi    Mysore            88888 88888



# ------build the dataframe

col_df= list(&quot;Id&quot;,&quot;Name&quot;, &quot;Address&quot;, &quot;Mobile&quot;)
raw_df2 =setNames(data.frame(matrix(ncol = 4, nrow = 0),stringsAsFactors = F),col_df)

for (j in 1:nrow(raw_df))
{
 # split the line of dataframe by  double space or more
 line= unlist(strsplit(raw_df$rawline[j],&quot;   +&quot;))
 df_line= data.frame(t(line),stringsAsFactors = F)
 # if all 4 column exist , affect column names else these is just Id and Part2 of adress ==&gt;column Adress2
 names(df_line) = unlist(ifelse(length(line)==4,
                                list(col_df),
                                list(c(&quot;Id&quot;,&quot;Adress2&quot;)))
 )
 # rbind even the number of column is not the same
 raw_df2=plyr::rbind.fill(raw_df2,df_line )
}

# ----- clean final dataframe

final_df = raw_df2%&gt;%
 # replace Na with emty value
 mutate_all(~ifelse(is.na(.), &quot;&quot;, .))%&gt;%
 group_by(Id)%&gt;%
 mutate(Address= paste(Address,Adress2,collapse = &quot; &quot;))%&gt;% #put collapse =&quot;\r\n&quot; to display the exact format
 # keep just the first line by Id 
 slice(1)%&gt;%
 # remove adress2 column 
 select(-Adress2)%&gt;%
 ungroup()
return(final_df)
}
</code></pre>
<p>apply function on your first example and the result is :</p>
<pre><code>raw_df  = data.frame(rawline=
                       c(&quot;1      Id   Name    Address           Mobile&quot;,
                         &quot;2 1    Kiran   Bengaluru,        99999 99999&quot;,
                         &quot;3                                Mysore Road&quot;,
                         &quot;4                                   6th Lane&quot;,
                         &quot;5 2    John    Mandya            77777 77777&quot;,
                         &quot;6                            Taluka Junction&quot;,
                         &quot;7 3    Ravi    Mysore            88888 88888&quot;)
)
final_df=parse_pdfs_lines_ById(raw_df)
final_df
# final_df
# A tibble: 3 x 4
# Id    Name  Address                              Mobile     
# &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                                &lt;chr&gt;      
# 1     Kiran &quot;Bengaluru,   Mysore Road  6th Lane&quot; 99999 99999
# 2     John  &quot;Mandya   Taluka Junction&quot;           77777 77777
# 3     Ravi  &quot;Mysore &quot;                            88888 88888
</code></pre>
<p>hope this will help!, please let me know if something does not work or is not clear enough.(update response format).</p>
",1,1,361,2022-10-19 09:20:56,https://stackoverflow.com/questions/74122881/how-to-convert-raw-lines-to-df
"Is there a way in python to extract only the CORE TEXT (without boxes, footer etc.) from a pdf?","<p>I am trying to extract only the core text from a &quot;rich&quot; pdf document, meaning that it has a lot of tables, graphs, boxes, footers etc.  in which I am not interested in.</p>
<p>I tried with some common python packages like PyPDF2, pdfplumber or pdfreader.The problem is that apparently they extract all the text present in the pdf, including those parts listed above in which I am not interested.</p>
<p>As an example:</p>
<pre><code>from PyPDF2 import PdfReader
file = PdfReader(file)
page = file.pages[10] 
text = page.extract_text()
</code></pre>
<p>This code will get me the whole text from page 11, including footers, box, text from a table and the number of the page, while what I would like is only the core text.</p>
<p>Unluckily the only solution I found up to now is to copy paste in another file the core text.</p>
<p>Is there any method/package which can automatically recognize the main text from the other parts of the pdf and return me only that?</p>
<p>Thank you for your help!!!</p>
","python, text, text-mining, text-extraction, pdfplumber","<p>per  <a href=""https://stackoverflow.com/users/7318120/d-l"">D.L</a>'s comment, please add some reproducible code and, preferably, a pdf to work with.</p>
<p>However, I think I can answer at least <em>part</em> of your question. <a href=""https://github.com/jsvine"" rel=""nofollow noreferrer"">jsvine</a>'s <a href=""https://github.com/jsvine/pdfplumber"" rel=""nofollow noreferrer"">pdfplumber</a> is an incredibly robust python pdf processing package. <a href=""https://github.com/jsvine/pdfplumber"" rel=""nofollow noreferrer"">pdfplumber</a> contains a <a href=""https://github.com/jsvine/pdfplumber#extracting-tables"" rel=""nofollow noreferrer"">bounding box</a> functionality that lets you extract text from within (<code>.within_bbox(...)</code>) or from outside (<code>.outside_bbox</code>) the 'bounding box' -- or geographical area -- delineated on the <code>Page</code> object. Every <a href=""https://github.com/jsvine/pdfplumber#objects"" rel=""nofollow noreferrer"">character object</a> extracted from the page contains location information such as <code>y1 - Distance of top of character from bottom of page</code> and <code>Distance of left side of character from left side of page</code>. If the majority of pages within the <code>.pdf</code> you are trying to extract text from contain footnotes, I would recommend only extracting text above the <code>y1</code> value. Given that footnotes are typically well below the end of a page, except for academic papers using <a href=""https://owl.purdue.edu/owl/research_and_citation/chicago_manual_17th_edition/cmos_formatting_and_style_guide/chicago_manual_of_style_17th_edition.html"" rel=""nofollow noreferrer"">Chicago Style</a> citations, you should still be able to set a standard <code>.bbox</code> for where you want to extract text (within a set <code>.bbox</code> that does not include footnotes or out of a set <code>.bbox</code> that does not include footnotes).</p>
<p>To your question about tables, that poses a trickier question. Tables are by far the trickiest thing to detect and/or extract from. <a href=""https://github.com/jsvine/pdfplumber"" rel=""nofollow noreferrer"">pdfplumber</a> offers, to my knowledge, the most robust open source <a href=""https://github.com/jsvine/pdfplumber#extracting-tables"" rel=""nofollow noreferrer"">table detection/extraction</a> capabilities out there. To extract the area <em>outside</em> a table, I would call the <code>.find_tables(...)</code> function on each <code>Page</code> object to return a <code>.bbox</code> of the table and extract <em>around</em> that. <strong>However -- this is not perfect.</strong> It is not always able to detect tables.</p>
<p>Regarding your 3rd question, how to exclude boxes, are you referring to text boxes? Please provide further clarification!</p>
<p>Finally -- to reiterate my first point -- <a href=""https://github.com/jsvine/pdfplumber"" rel=""nofollow noreferrer"">pdfplumber</a> is an incredibly robust package. That being said, extracting text from <code>.pdf</code> files is really tough. Good luck -- please provide more information and I will be happy to help as best I can.</p>
",4,3,6342,2022-11-07 09:48:49,https://stackoverflow.com/questions/74344614/is-there-a-way-in-python-to-extract-only-the-core-text-without-boxes-footer-et
Inner Join not working for R sentiment analysis,"<p>I'm trying to conduct sentiment analysis on a Simpsons' episode using the afinn library but for some reason when I inner join sentiment with my tidy_text and filter for words labelled -5, it returns words in the afinn library that are not in my dataframe.</p>
<p>I double checked my df ('tidy_text') and it definitely is a subset of words from the episode.</p>
<p>Any idea what I'm doing wrong here?</p>
<pre><code>afinn &lt;- tidy_text %&gt;%
  inner_join(get_sentiments(&quot;afinn&quot;)) %&gt;%
  filter(value == -5) %&gt;%
  count(word, value, sort = TRUE)


</code></pre>
","r, text-mining, sentiment-analysis","<p>Sounds like an interesting project.
Try adding , by = c(&quot;word&quot; = &quot;word&quot;)) %&gt;%</p>
<pre><code>afinn &lt;- tidy_text %&gt;%
  inner_join(get_sentiments(&quot;afinn&quot;), by = c(&quot;word&quot; = &quot;word&quot;)) %&gt;%
  filter(value == -5) %&gt;%
  count(word, value, sort = TRUE)
</code></pre>
",2,-1,60,2022-12-02 17:51:42,https://stackoverflow.com/questions/74659657/inner-join-not-working-for-r-sentiment-analysis
Extract First Page of All PDF Documents in a Library,"<p>I am new to PDF Handling in Python. I have a document library which contains a large volume of PDF Documents. I am trying to extract the First Page of each document. I have produced the below code.</p>
<p>My initial for loop &quot;for entry in entries&quot; returns the name of all documents in the library. I verify this by successfully printing all document names in the library.</p>
<p>I am using the pdfReader.getPage to specify the page number of each document whilst also using the extractText function to extract the text from the page. However, when i run this entire script, I am being thrown an error which states that one of the documents cannot be located. However, the document does exist in the library. This is shown in the screenshot from the library below. Whilst also verified by the fact that it prints in the list of documents in the repository.</p>
<p>I believe the issue is with how the extractText is iterating through all documents but I am unclear on how to resolve. Would anyone have any suggestions?</p>
<pre><code>import os
import PyPDF2
from PyPDF2 import PdfFileWriter, PdfFileReader

# get the file names in the directory
directory = 'Fund Docs'
entries = os.listdir(directory)


for entry in entries:
    print(entry)
    # create a PDF reader object
    pdfFileObj = open(entry, 'rb')
    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)
    print(pdfReader.numPages)

    # creating a page object
    pageObj = pdfReader.getPage(0)

    # extracting text from page
    print(pageObj.extractText())

    # closing the pdf file object
    pdfFileObj.close()


</code></pre>
<p><a href=""https://i.sstatic.net/KawgN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KawgN.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/mMGuh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mMGuh.png"" alt=""enter image description here"" /></a></p>
","python, pdf, text-mining, pypdf","<p>You need to specify the full path:</p>
<pre class=""lang-py prettyprint-override""><code>pdfFileObj = open(directory + '/' + entry, 'rb')
</code></pre>
<p>This will open the file at <code>Fund Docs/FILE_NAME.pdf</code>. By only specifying <code>entry</code>, it will look for the file in the current folder, which it won't find. By adding the folder at the start, you're saying to find the entry inside that folder.</p>
",0,1,780,2022-12-20 12:35:25,https://stackoverflow.com/questions/74863185/extract-first-page-of-all-pdf-documents-in-a-library
How do I extract the text of a single page with PyPDF2?,"<p>I have a document library which consists of several hundred PDF Documents. I am attempting to export the first page of each PDF document. Below is my script which extracts the page. It saves each page as an individual PDF. However, the files which are exported seem to be exporting in unreadable or damaged format.</p>
<p>Is there something missing from my script?</p>
<pre class=""lang-py prettyprint-override""><code>import os
from PyPDF2 import PdfReader, PdfWriter

# get the file names in the directory
input_directory = &quot;Fund_Docs_Sample&quot;
entries = os.listdir(input_directory)
output_directory = &quot;First Pages&quot;
outputs = os.listdir(output_directory)

for output_file_name in entries:
    reader = PdfReader(input_directory + &quot;/&quot; + output_file_name)
    page = reader.pages[0]
    first_page = &quot;\n&quot; + page.extract_text() + &quot;\n&quot;

    with open(output_file_name, &quot;wb&quot;) as outputStream:
        pdf_writer = PdfWriter(output_file_name + first_page)
</code></pre>
<p><a href=""https://i.sstatic.net/i13ix.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/i13ix.png"" alt=""enter image description here"" /></a></p>
","python, data-science, text-mining, pypdf","<ol>
<li>You're missing <code>pdf_writer.write(outputStream)</code></li>
<li>Do you want to write a text file (containing the extracted text) or a PDF file (containing the first page of the PDF)?</li>
<li>You seem to overwrite the files of the input</li>
<li><code>output_directory</code> is not used at all</li>
</ol>
<p>After reading the comments, you likely want this:</p>
<pre class=""lang-py prettyprint-override""><code>from pathlib import Path
from PyPDF2 import PdfReader

# get the file names in the directory
input_directory = Path(&quot;Fund_Docs_Sample&quot;)
output_directory = Path(&quot;First Pages&quot;)

for input_file_path in input_directory.glob(&quot;*.pdf&quot;):
    print(input_file_path)
    reader = PdfReader(input_file_path)
    page = reader.pages[0]
    first_page_text = &quot;\n&quot; + page.extract_text() + &quot;\n&quot;
    
    # create the output text file path
    output_file_path = output_directory / f&quot;{input_file_path.name}.txt&quot;
    
    # write the text to the output file
    with open(output_file_path, &quot;w&quot;) as output_file:
        output_file.write(first_page_text)
</code></pre>
",0,0,1425,2022-12-21 11:29:16,https://stackoverflow.com/questions/74875388/how-do-i-extract-the-text-of-a-single-page-with-pypdf2
Creating a token count by date and co-occurence term proportion by date using quanteda,"<p>I have a quite massive dataset that contains reviews of utilities services from customers all over the UK, this is a small sample of what the data looks like:</p>
<pre><code>df &lt;- data.frame (text  = c(&quot;The investors and their supporters shall invest and do something mostly invest&quot;,
         &quot; Shall we tell the investors to invest ?&quot;,  &quot;Investors shall invest.&quot;,
         &quot;Investors may sometimes invest&quot;,&quot;spend what Investor Do&quot;),
                  date = c(&quot;10/12/2022&quot;, &quot;10/12/2022&quot;, &quot;10/12/2022&quot;,&quot;11/12/2022&quot;,&quot;12/12/2022&quot;))
</code></pre>
<p>What I want is to be able to count the frequency of terms/words/tokens by <code>date</code>.</p>
<p>For instance, the word <code>invest</code> appears 6 times in total, so for the date 10/12/2022 its word count is 4 I want to be able to use the quanteda library (since it is so powerful) to achieve the count and plot the viz over <code>date</code>.</p>
<p>I also want to plot the association or co-occurence of the word investor &amp; invest over date</p>
<p>For instance, we have in this example 5 reviews in those reviews 4/5 times the word invest and investor were present and I'd like to plot that percentage over <code>date</code> as well. Is that is possible? What amazing options does the quantada lib has that can perform this task? Will it be possible to also find lets say a min percentage of the 0.25 most frequent words that appear when &quot;invest&quot; appears?</p>
<p>To achieve the first point I started with the following code:</p>
<pre><code>df %&gt;% 
  corpus(text_field=&quot;text&quot;) %&gt;% 
  dfm() %&gt;%
  textstat_frequency(10)
</code></pre>
<p>which gives:</p>
<pre><code>      feature frequency rank docfreq group
1      invest         6    1       5   all
2   investors         4    2       4   all
3       shall         3    3       3   all
4         the         2    4       2   all
5         and         2    4       1   all
6          do         2    4       2   all
7       their         1    7       1   all
8  supporters         1    7       1   all
9   something         1    7       1   all
10         we         1    7       1   all
Warning message:
'dfm.corpus()' is deprecated. Use 'tokens()' first. 
</code></pre>
<p>How would I go about plotting the frequency of this words over the date column? I read in the documentation that one can group but I had have no luck in doing so.</p>
<p>And for the second question I don't know for sure if what function of the quenteda lib to use but I am trying to mirror the  <code>tm::findAssocs()</code> fun from the tm library.</p>
","r, nlp, text-mining, data-wrangling, quanteda","<p>Answer to your first question:</p>
<p>The dates are put into the <code>docvars</code> part of your corpus. This can be used within the <code>textstat_frequency</code> with the <code>group</code> option.</p>
<pre><code>dat &lt;- data.frame (text  = c(&quot;The investors and their supporters shall invest and do something mostly invest&quot;,
                            &quot; Shall we tell the investors to invest ?&quot;,  &quot;Investors shall invest.&quot;,
                            &quot;Investors may sometimes invest&quot;,&quot;spend what Investor Do&quot;),
                  date = c(&quot;10/12/2022&quot;, &quot;10/12/2022&quot;, &quot;10/12/2022&quot;,&quot;11/12/2022&quot;,&quot;12/12/2022&quot;))


library(dplyr)
library(quanteda)
library(quanteda.textstats)

dat %&gt;% 
  corpus(text_field=&quot;text&quot;) %&gt;% 
  tokens() %&gt;%
  dfm() %&gt;% 
  textstat_frequency(groups = date)

      feature frequency rank docfreq      group
1      invest         4    1       3 10/12/2022
2   investors         3    2       3 10/12/2022
3       shall         3    2       3 10/12/2022
4         the         2    4       2 10/12/2022
5         and         2    4       1 10/12/2022
6       their         1    6       1 10/12/2022
7  supporters         1    6       1 10/12/2022
8          do         1    6       1 10/12/2022
9   something         1    6       1 10/12/2022
10     mostly         1    6       1 10/12/2022
11         we         1    6       1 10/12/2022
12       tell         1    6       1 10/12/2022
13         to         1    6       1 10/12/2022
14          ?         1    6       1 10/12/2022
15          .         1    6       1 10/12/2022
16  investors         1    1       1 11/12/2022
17     invest         1    1       1 11/12/2022
18        may         1    1       1 11/12/2022
19  sometimes         1    1       1 11/12/2022
20         do         1    1       1 12/12/2022
21      spend         1    1       1 12/12/2022
22       what         1    1       1 12/12/2022
23   investor         1    1       1 12/12/2022
</code></pre>
<p>You now have access to the frequency per day.</p>
<p>As for question 2, I think you can use <code>textstat_simil</code>. Something like below. It does give some different answers as using <code>tm::findAssoc</code>, usually more features. So I'm not completely sure if this is the correct answer. Maybe someone from the quanteda team can confirm or deny this.</p>
<pre><code>my_dfm &lt;- dat %&gt;% 
  corpus(text_field=&quot;text&quot;) %&gt;% 
  tokens() %&gt;%
  dfm()

textstat_simil(my_dfm, 
               my_dfm[, c(&quot;investor&quot;)], 
               method = &quot;correlation&quot;, 
               margin = &quot;features&quot;,
               min_simil = 0.7)

textstat_simil object; method = &quot;correlation&quot;
           investor
the               .
investors         .
and               .
their             .
supporters        .
shall             .
invest            .
do                .
something         .
mostly            .
we                .
tell              .
to                .
?                 .
.                 .
may               .
sometimes         .
spend             1
what              1
investor          1
</code></pre>
<p>You can save the outcome of textstat_simil as a data.frame or list if you want to with <code>as.data.frame</code> or <code>as.list</code>.</p>
",1,0,99,2022-12-27 23:58:34,https://stackoverflow.com/questions/74935089/creating-a-token-count-by-date-and-co-occurence-term-proportion-by-date-using-qu
Creating and computing percentage of co-occurrence based on keywords,"<p>I have the following data set:</p>
<pre><code>df &lt;- data.frame (text  = c(&quot;House Sky Blue&quot;,
                            &quot;House Sky Green&quot;,
                            &quot;House Sky Red&quot;,
                            &quot;House Sky Yellow&quot;,
                            &quot;House Sky Green&quot;,
                            &quot;House Sky Glue&quot;,
                            &quot;House Sky Green&quot;))
</code></pre>
<p>I'd like to find the percentage of co-occurrence of some terms of tokens. For example, out of all documents, where can I find the token &quot;House&quot; and at the same time how many of them also include the term &quot;Green&quot;?</p>
<p>In out data we have 7 documents that have the term House and 3 out of those 7 p=(100*3/7) also include the term Green, It would be so nice to see also what terms or tokens appear within some p threshold along side the token &quot;House&quot;.</p>
<p>I have used these two functions:</p>
<pre><code>textstat_collocations(tokens)

&gt; textstat_collocations(tokens)
  collocation count count_nested length   lambda        z
1   house sky     7            0      2 5.416100 2.622058
2   sky green     3            0      2 2.456736 1.511653

Fun textstat_simil

textstat_simil(dfm(tokens),margin=&quot;features&quot;)

textstat_simil object; method = &quot;correlation&quot;
       house sky   blue  green    red yellow   glue
house    NaN NaN    NaN    NaN    NaN    NaN    NaN
sky      NaN NaN    NaN    NaN    NaN    NaN    NaN
blue     NaN NaN  1.000 -0.354 -0.167 -0.167 -0.167
green    NaN NaN -0.354  1.000 -0.354 -0.354 -0.354
red      NaN NaN -0.167 -0.354  1.000 -0.167 -0.167
yellow   NaN NaN -0.167 -0.354 -0.167  1.000 -0.167
glue     NaN NaN -0.167 -0.354 -0.167 -0.167  1.000
</code></pre>
<p>but they do not seem to give my desired output also I wonder why the correlation btw green and house is NaN for the <code>textsats_simil</code> fun</p>
<p>My desired output would show the following info:</p>
<pre><code>feature=&quot;House&quot;
 percentage of co-occurrence 

Green = 3/7
Blue= 1/7
Red = 1/7
Yellow = 1/7
Glue = 1/7
</code></pre>
<p>In the quetda docs I can't seem to find a function that can give me my desired output, although I know there must be a way around since I find this library to be so fast and complete.</p>
","r, text-mining, quanteda","<p>One way to do this is using the <code>fcm()</code> to get document-level co-occurrences for a target feature.  Below, I show how to do this using <code>fcm()</code>, <code>fcm_remove()</code> to remove the target feature, then a loop to get the desired printed output.</p>
<pre class=""lang-r prettyprint-override""><code>library(&quot;quanteda&quot;)
#&gt; Package version: 3.2.4
#&gt; Unicode version: 14.0
#&gt; ICU version: 70.1
#&gt; Parallel computing: 10 of 10 threads used.
#&gt; See https://quanteda.io for tutorials and examples.

df &lt;- data.frame(text = c(&quot;House Sky Blue&quot;,
                          &quot;House Sky Green&quot;,
                          &quot;House Sky Red&quot;,
                          &quot;House Sky Yellow&quot;,
                          &quot;House Sky Green&quot;,
                          &quot;House Sky Glue&quot;,
                          &quot;House Sky Green&quot;))
corp &lt;- corpus(df)

coocc_fract &lt;- function(corp, feature) {
   # create a document-level co-occurrence matrix
   fcmat &lt;- fcm(dfm(tokens(corp), tolower = FALSE), context = &quot;document&quot;)
   # select for the given feature
   fcmat &lt;- fcm_remove(fcmat, feature)
   cat(&quot;feature=\&quot;&quot;, feature, &quot;\&quot;\n&quot;, sep = &quot;&quot;)
   cat(&quot; percentage of co-occurrence\n\n&quot;)
   for (f in featnames(fcmat)) {
       # skip zeroes
       freq &lt;- as.character(fcmat[1, f])
       if (freq != &quot;0&quot;) {
           cat(f, &quot; = &quot;, as.character(fcmat[1, f]), &quot;/&quot;, ndoc(corp), 
               &quot;\n&quot;, sep = &quot;&quot;)
       }
   }
}
</code></pre>
<p>This produces this output:</p>
<pre class=""lang-r prettyprint-override""><code>coocc_fract(corp, feature = &quot;House&quot;)
#&gt; feature=&quot;House&quot;
#&gt;  percentage of co-occurrence
#&gt; 
#&gt; Blue = 1/7
#&gt; Green = 3/7
#&gt; Red = 1/7
#&gt; Yellow = 1/7
#&gt; Glue = 1/7
</code></pre>
<p><sup>Created on 2023-01-02 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.0.2</a></sup></p>
",2,0,154,2022-12-29 16:06:50,https://stackoverflow.com/questions/74953426/creating-and-computing-percentage-of-co-occurrence-based-on-keywords
Quanteda calculating tokens frequency in dfm including also a customized list of phrases,"<p>I have been wondering if it is possible to perform the <code>feauture_frequency</code> of the powerful <code>quanteda</code> library in R including also a list of phrases or &quot;words&quot; to be accounted for, for instance I have the following data set:</p>
<pre><code>library(quanteda)
library(quanteda.textstats)

df_sample&lt;-c(&quot;Word Record&quot;,
             &quot;be able to count by word&quot;,
             &quot;But also include some phrases such as&quot;,
             &quot;World Record Super Bass Mr. President Mr. President&quot;)
</code></pre>
<p>When I calculate the <code>textstat_frequency</code> of the df_sample I get something like this:</p>
<pre><code>&gt; tokens&lt;-corpus(df_sample) %&gt;% tokens(remove_punct = TRUE)
&gt; dfm&lt;-dfm(tokens)
&gt; 
&gt; quanteda.textstats::textstat_frequency(dfm)
     feature frequency rank docfreq group
1       word         2    1       2   all
2     record         2    1       2   all
3         mr         2    1       1   all
4  president         2    1       1   all
5         be         1    5       1   all
6       able         1    5       1   all
7         to         1    5       1   all
8      count         1    5       1   all
9         by         1    5       1   all
10       but         1    5       1   all
11      also         1    5       1   all
12   include         1    5       1   all
13      some         1    5       1   all
14   phrases         1    5       1   all
15      such         1    5       1   all
16        as         1    5       1   all
17     world         1    5       1   all
18     super         1    5       1   all
19      bass         1    5       1   all
&gt; 
</code></pre>
<p>which is correct but I also want to change my code in other to take into account and print in the output the words or phrases &quot;Mr. President&quot;, &quot;World Record&quot;, &quot;Super Bass&quot;</p>
<pre><code>key_lookups&lt;-c(&quot;Mr. President&quot;, &quot;World Record&quot;, &quot;Super Bass&quot; )
</code></pre>
<p>How can I use <code>quanteda</code> funs to have in my output <em><strong>along with the previous</strong></em> counts also the frequency of the previous phrases,for example</p>
<blockquote>
<p>&quot;Mr. President&quot;  2 &quot;World Record&quot;   2 &quot;Super Bass&quot;     1</p>
</blockquote>
","r, regex, text-mining, quanteda","<p>First: a warning about your example code: do not create objects that have the same name as functions (like tokens and dfm) this will (eventually) lead to errors and is difficult to debug.</p>
<p>There are probably a few ways of doing this. I created a &quot;normal&quot; tokens object and one ngrams tokens object. both turned into dfm's and from the ngrams dfm, I kept the phrases you wanted. Then combined the dfm's and you can use <code>textstat_frequency</code> as normal.</p>
<blockquote>
<p>Note: you can't combine tokens objects like you can combine dfm objects.</p>
</blockquote>
<pre><code>library(quanteda)
library(quanteda.textstats)

df_sample&lt;-c(&quot;Word Record&quot;,
             &quot;be able to count by word&quot;,
             &quot;But also include some phrases such as&quot;,
             &quot;World Record Super Bass Mr. President Mr. President&quot;)



my_tokens &lt;- corpus(df_sample) %&gt;% tokens(remove_punct = TRUE)
my_dfm &lt;- dfm(my_tokens)

# No points as they are removed in the dfm
key_lookups&lt;-c(&quot;Mr President&quot;, &quot;World Record&quot;, &quot;Super Bass&quot; )


my_tokens_ngram &lt;- tokens_ngrams(my_tokens, n = 2, concatenator = &quot; &quot;)

my_dfm_ngrams &lt;- dfm(my_tokens_ngram)

# Only keep the lookups
my_dfm_ngrams &lt;- dfm_keep(my_dfm_ngrams, key_lookups)

# Combine both dfms
my_dfms &lt;- rbind(my_dfm, my_dfm_ngrams)

# if necessary uncomment next part
# my_dfms &lt;- dfm_compress(my_dfms) 
</code></pre>
<p>outcome:</p>
<pre><code>head(textstat_frequency(my_dfms), 5)
       feature frequency rank docfreq group
1         word         2    1       2   all
2       record         2    1       2   all
3           mr         2    1       1   all
4    president         2    1       1   all
5 mr president         2    1       1   all

tail(textstat_frequency(my_dfms), 5)
        feature frequency rank docfreq group
18        world         1    6       1   all
19        super         1    6       1   all
20         bass         1    6       1   all
21 world record         1    6       1   all
22   super bass         1    6       1   all
</code></pre>
<p>Do note that using rbind on dfms, creates a new document name like &quot;text1.1&quot;. If you  want this merged back to the original documents, you can call <code>dfm_compress(my_dfms) </code> first and then call <code>textstat_frequency</code>.</p>
",0,0,240,2023-01-01 22:33:14,https://stackoverflow.com/questions/74977785/quanteda-calculating-tokens-frequency-in-dfm-including-also-a-customized-list-of
"R Quanteda Filtering, counting and grouping features from a Customized dictionary","<p>I have the following data set:</p>
<pre><code>library(quanteda)
library(quanteda.textstats)

df_test&lt;-c(&quot;I find water to be so healthy and refreshing&quot;,
           &quot;Nothing like a freshly made burguer to make me feel good&quot;,
           &quot;I dislike sugar in the morning it tastes horrible&quot;,
           &quot;A nice burguer is always crispy and spicy&quot;,
           &quot;It is beyond me to dare to drink soda it's just gross too much sugar&quot;,
           &quot;Yes I will have a hot burguer anytime is so cheap and tasty&quot;)
</code></pre>
<p>I want to be able to built a Customized dictionary so that I can
classify words/tokens into two categories &quot;Negative&quot; and &quot;Positive&quot;
after that I want to filter by the most frequent words/tokens and plot
the positive and negative words associated with them</p>
<p>This is my dictionary</p>
<pre><code>dict_custom &lt;- dictionary(list(positive = c(&quot;healthy&quot;, &quot;refreshing&quot;, &quot;good&quot;, &quot;crispy&quot;, 
                                      &quot;spicy&quot;, &quot;cheap&quot;, &quot;tasty&quot;),
                               negative=c(&quot;horrible&quot;,&quot;gross&quot;)))
</code></pre>
<p>What are some of the most frequent tokens?</p>
<pre><code>&gt; tok_df&lt;-corpus(df_test) %&gt;% tokens(remove_punct=TRUE) %&gt;% tokens_remove(stopwords(&quot;en&quot;))
&gt; 
&gt; tok_df %&gt;% dfm() %&gt;% 
+   textstat_frequency(5)  
  feature frequency rank docfreq group
1 burguer         3    1       3   all
2   sugar         2    2       2   all
3    find         1    3       1   all
4   water         1    3       1   all
5 healthy         1    3       1   all
</code></pre>
<p>I want to choose burger and get all the positive and negative words (after using my dictionary) and count the number of times they appear also perhaps create a word_cloud</p>
<p>I'm using this code:</p>
<pre><code>&gt; tokens_lookup(tok_df,dictionary = dict_custom) %&gt;% 
+   dfm()
Document-feature matrix of: 6 documents, 2 features (50.00% sparse) and 0 docvars.
       features
docs    positive negative
  text1        2        0
  text2        1        0
  text3        0        1
  text4        2        0
  text5        0        1
  text6        2        0
</code></pre>
<p>but instead of words I get the count of positive and negative tokens per document.</p>
<p>My desired output will contain a matrix/dfm like object filter by burger with all of the negative and positive tokens (crispy, healthy, gross, ect) instead of the count of neg and pos tokens by document (that I do not want).</p>
<p>By the way, what if I want to instead of creating a neg and positive words, rather assign a numeric value lets say gross=-5 and crispy=5 how can I join and merge my tokens with this kind of dictionary so that I afterwards can summarize the numeric output?</p>
","r, text-mining, quanteda","<p>The best way to do this is using the ability of <code>tokens_select()</code> to filter on dictionaries. By indexing each key separately - below, using <code>lapply</code> - then you can create a list of dfm objects whose features are the value matches for each key.</p>
<pre class=""lang-r prettyprint-override""><code>library(&quot;quanteda&quot;)
#&gt; Package version: 3.2.4
#&gt; Unicode version: 14.0
#&gt; ICU version: 70.1
#&gt; Parallel computing: 10 of 10 threads used.
#&gt; See https://quanteda.io for tutorials and examples.
library(&quot;quanteda.textstats&quot;)

df_test &lt;- c(&quot;I find water to be so healthy and refreshing&quot;,
             &quot;Nothing like a freshly made burguer to make me feel good&quot;,
             &quot;I dislike sugar in the morning it tastes horrible&quot;,
             &quot;A nice burguer is always crispy and spicy&quot;,
             &quot;It is beyond me to dare to drink soda it's just gross too much sugar&quot;,
             &quot;Yes I will have a hot burguer anytime is so cheap and tasty&quot;)

dict_custom &lt;- dictionary(list(positive = c(&quot;healthy&quot;, &quot;refreshing&quot;, &quot;good&quot;, &quot;crispy&quot;, 
                                            &quot;spicy&quot;, &quot;cheap&quot;, &quot;tasty&quot;),
                               negative = c(&quot;horrible&quot;,&quot;gross&quot;)))

toks &lt;- tokens(df_test)

dfm_list &lt;- lapply(
    names(dict_custom), 
    function(x) {
        tokens_select(toks, dict_custom[x]) |&gt;
            dfm()
    }
)
names(dfm_list) &lt;- names(dict_custom)
</code></pre>
<p>Now you have a list of dfm objects, named by your dictionary keys, which you can then get frequencies for, or wordclouds, etc.</p>
<pre class=""lang-r prettyprint-override""><code>dfm_list
#&gt; $positive
#&gt; Document-feature matrix of: 6 documents, 7 features (83.33% sparse) and 0 docvars.
#&gt;        features
#&gt; docs    healthy refreshing good crispy spicy cheap tasty
#&gt;   text1       1          1    0      0     0     0     0
#&gt;   text2       0          0    1      0     0     0     0
#&gt;   text3       0          0    0      0     0     0     0
#&gt;   text4       0          0    0      1     1     0     0
#&gt;   text5       0          0    0      0     0     0     0
#&gt;   text6       0          0    0      0     0     1     1
#&gt; 
#&gt; $negative
#&gt; Document-feature matrix of: 6 documents, 2 features (83.33% sparse) and 0 docvars.
#&gt;        features
#&gt; docs    horrible gross
#&gt;   text1        0     0
#&gt;   text2        0     0
#&gt;   text3        1     0
#&gt;   text4        0     0
#&gt;   text5        0     1
#&gt;   text6        0     0
</code></pre>
<p>Frequencies:</p>
<pre class=""lang-r prettyprint-override""><code>lapply(dfm_list, textstat_frequency)
#&gt; $positive
#&gt;      feature frequency rank docfreq group
#&gt; 1    healthy         1    1       1   all
#&gt; 2 refreshing         1    1       1   all
#&gt; 3       good         1    1       1   all
#&gt; 4     crispy         1    1       1   all
#&gt; 5      spicy         1    1       1   all
#&gt; 6      cheap         1    1       1   all
#&gt; 7      tasty         1    1       1   all
#&gt; 
#&gt; $negative
#&gt;    feature frequency rank docfreq group
#&gt; 1 horrible         1    1       1   all
#&gt; 2    gross         1    1       1   all
</code></pre>
<p><sup>Created on 2023-01-04 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.0.2</a></sup></p>
",2,1,266,2023-01-02 22:31:59,https://stackoverflow.com/questions/74987587/r-quanteda-filtering-counting-and-grouping-features-from-a-customized-dictionar
How to compute a numeric sentiment score using quanteda from a custom dictionary,"<p>I have been using the AWESOME quanteda library for text analysis lately and it has been quite a joy, recently I have stumbled with a task and that is to use a dictionary relating words to a numeric sentiment score to summarize a measure per document called: NetSentScore which is calculating in the following manner:</p>
<p>NetSentScore per document= sum(Positive_word<em>score)+sum(Negative_word</em>score)</p>
<p>I have the following dictionary:</p>
<pre><code>ScoreDict&lt;- tibble::tibble(
  score= c(-5,-9,1,8,9,-10),
  word = c(&quot;bad&quot;, &quot;horrible&quot;, &quot;open&quot;,&quot;awesome&quot;,&quot;gorgeous&quot;,&quot;trash&quot;)
)
</code></pre>
<p>My corpus</p>
<pre><code>text&lt;-c(&quot;this is a bad movie very bad&quot;,&quot;horrible movie, just awful&quot;,&quot;im open to new dreams&quot;,
         &quot;awesome place i loved it&quot;,&quot;she is gorgeous&quot;,&quot;that is trash&quot;)
</code></pre>
<p>by definition quanteda will not allow to have numeric data in a dictionary, but I can have this:</p>
<pre><code>&gt; text %&gt;% 
+   corpus() %&gt;% 
+   tokens(remove_punct = TRUE) %&gt;% 
+   tokens_remove(stopwords(&quot;en&quot;)) %&gt;% 
+   dfm() 
Document-feature matrix of: 6 documents, 14 features (82.14% sparse) and 0 docvars.
       features
docs    bad movie horrible just awful im open new dreams awesome
  text1   2     1        0    0     0  0    0   0      0       0
  text2   0     1        1    1     1  0    0   0      0       0
  text3   0     0        0    0     0  1    1   1      1       0
  text4   0     0        0    0     0  0    0   0      0       1
  text5   0     0        0    0     0  0    0   0      0       0
  text6   0     0        0    0     0  0    0   0      0       0
[ reached max_nfeat ... 4 more features ]
</code></pre>
<p>which gives me the number or times a word was found in a document, I will only need to &quot;join&quot; or &quot;merge&quot; with my dictionary so I have have the score by each word and then compute the NetSentScore, is there a way to do this in quanteda?</p>
<p>Please keep in mind that I do have a quite massive large corpus so converting my dfm to a dataframe will make the RAM die as I have over 500k documents and approx 800 features.</p>
<pre><code>to illustrate the NetSentScore of text1 will be:
2*-5+0=-10, this is because the word bad appears two times and according to the dictionary it has a score of -5
</code></pre>
","r, text-mining, quanteda","<p>As @stomper suggests, you can do this with the quanteda.sentiment package, by setting the numeric values as &quot;valences&quot; for the dictionary. Here's how to do it.</p>
<p>This ought to work on 500k documents but of course this will depend on your machine's capacity.</p>
<pre class=""lang-r prettyprint-override""><code>library(&quot;quanteda&quot;)
#&gt; Package version: 3.2.4
#&gt; Unicode version: 14.0
#&gt; ICU version: 70.1
#&gt; Parallel computing: 10 of 10 threads used.
#&gt; See https://quanteda.io for tutorials and examples.
library(&quot;quanteda.sentiment&quot;)
#&gt; 
#&gt; Attaching package: 'quanteda.sentiment'
#&gt; The following object is masked from 'package:quanteda':
#&gt; 
#&gt;     data_dictionary_LSD2015

dict &lt;- dictionary(list(
  sentiment = c(&quot;bad&quot;, &quot;horrible&quot;, &quot;open&quot;, &quot;awesome&quot;, &quot;gorgeous&quot;, &quot;trash&quot;)
))

valence(dict) &lt;- list(
  sentiment = c(bad = -5, 
                horrible = -9, 
                open = 1, 
                awesome = 8, gorgeous = 9, 
                trash = -10)
)

print(dict)
#&gt; Dictionary object with 1 key entry.
#&gt; Valences set for keys: sentiment 
#&gt; - [sentiment]:
#&gt;   - bad, horrible, open, awesome, gorgeous, trash

text &lt;- c(&quot;this is a bad movie very bad&quot;,
          &quot;horrible movie, just awful&quot;,
          &quot;im open to new dreams&quot;,
          &quot;awesome place i loved it&quot;,
          &quot;she is gorgeous&quot;,
          &quot;that is trash&quot;)
</code></pre>
<p>Now to compute the document scores, you use <code>textstat_valence()</code> but you sent the normalisation to &quot;none&quot; in order to sum the valences rather than average them.  Normalisation is the default because raw sums are affected by documents having different lengths, but as this package is still in a developmental stage, it's easy to imagine that other choices might be preferable to the default.</p>
<pre class=""lang-r prettyprint-override""><code>textstat_valence(tokens(text), dictionary = dict, normalization = &quot;none&quot;)
#&gt;   doc_id sentiment
#&gt; 1  text1       -10
#&gt; 2  text2        -9
#&gt; 3  text3         1
#&gt; 4  text4         8
#&gt; 5  text5         9
#&gt; 6  text6       -10
</code></pre>
<p><sup>Created on 2023-01-11 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.0.2</a></sup></p>
",1,0,191,2023-01-11 01:24:29,https://stackoverflow.com/questions/75077458/how-to-compute-a-numeric-sentiment-score-using-quanteda-from-a-custom-dictionary
Grouping text data in a corpus by a data frame variable,"<p>I have a data frame in R with a column that I need to do basic text analysis on. I am able to do this modifying the code as needed from <a href=""https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a"" rel=""nofollow noreferrer"">this source</a>. However, I now need to do this same analysis but for groups of data. I've included the <code>dput</code> of a small sample here.</p>
<pre><code>structure(list(Pad.Name = c(&quot;MISSOURI W&quot;, &quot;MISSOURI W&quot;, &quot;MISSOURI W&quot;, 
&quot;LEE&quot;, &quot;LEE&quot;, &quot;LEE&quot;), Message = c(&quot;pump maint&quot;, &quot;PUMP MAINT&quot;, &quot;Pump Maintenance&quot;, 
&quot;waiting on wireline&quot;, 
&quot;seating the ball&quot;, &quot;Waiting on wireline&quot;)), row.names = 11:16, class = &quot;data.frame&quot;)
</code></pre>
<p>I want to group by the variable Pad.Name. I've tried using <code>corpus_group</code> function from the <code>quanteda</code> as well as the <code>corpus</code> function from the same package, setting the parameters as follows: <code>docid_field = dat$Pad.Name</code> and <code>text_field = dat$Message</code>. Yet none of these seem to work.</p>
<p>My desired output are the most frequent words, say the top 10 most frequent, and a count of those words, for each unique Pad.Name. Similar something to as follows, however the true counts would work out, obviously:</p>
<p>edit: the table option never seems to work here, so here is a dput and data frame of my desired output</p>
<pre><code>structure(list(Pad.Name = c(&quot;MISSOURI W&quot;, &quot;MISSOURI W&quot;, &quot;LEE&quot;, 
&quot;LEE&quot;), Word = c(&quot;pump&quot;, &quot;maint&quot;, &quot;waiting&quot;, &quot;wireline&quot;), Count = c(3, 
2, 2, 2)), class = &quot;data.frame&quot;, row.names = c(NA, -4L))

output &lt;- data.frame(Pad.Name = c(&quot;MISSOURI W&quot;, &quot;MISSOURI W&quot;, &quot;LEE&quot;, &quot;LEE&quot;), Word = c(&quot;pump&quot;, &quot;maint&quot;, &quot;waiting&quot;, &quot;wireline&quot;), Count = c(3,2,2,2))
</code></pre>
","r, dataframe, grouping, text-mining, corpus","<p>Would <em>dplyr</em> and <em>tidytext</em> do?</p>
<pre class=""lang-r prettyprint-override""><code>library(tidytext)
library(dplyr)

as_tibble(data) %&gt;% 
  # split to words
  unnest_tokens(word,Message) %&gt;% 
  # filter out stopwords
  anti_join(get_stopwords()) %&gt;% 
  # count by (Pad.Name, word) groups 
  count(Pad.Name, word, name = &quot;Count&quot;, sort = T) %&gt;%
  # output is sorted by Count, no grouping, keep top-4
  slice_head(n = 4) %&gt;% 
  arrange(Pad.Name, desc(Count))
#&gt; Joining, by = &quot;word&quot;
#&gt; # A tibble: 4 × 3
#&gt;   Pad.Name   word     Count
#&gt;   &lt;chr&gt;      &lt;chr&gt;    &lt;int&gt;
#&gt; 1 LEE        waiting      2
#&gt; 2 LEE        wireline     2
#&gt; 3 MISSOURI W pump         3
#&gt; 4 MISSOURI W maint        2
</code></pre>
<p>Input:</p>
<pre><code>data &lt;- structure(list(Pad.Name = c(
  &quot;MISSOURI W&quot;, &quot;MISSOURI W&quot;, &quot;MISSOURI W&quot;,
  &quot;LEE&quot;, &quot;LEE&quot;, &quot;LEE&quot;
), Message = c(
  &quot;pump maint&quot;, &quot;PUMP MAINT&quot;, &quot;Pump Maintenance&quot;,
  &quot;waiting on wireline&quot;,
  &quot;seating the ball&quot;, &quot;Waiting on wireline&quot;
)), row.names = 11:16, class = &quot;data.frame&quot;)

</code></pre>
<p><sup>Created on 2023-01-26 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.0.2</a></sup></p>
",1,1,355,2023-01-25 23:13:27,https://stackoverflow.com/questions/75240863/grouping-text-data-in-a-corpus-by-a-data-frame-variable
Apply a text mining function to every data frame within a list of data frames by using a for loop?,"<p>I have a list of dataframes and I would like to apply a text mining command (namely removing punctuation) to every dataframe within my list of dataframes. Since I have more than 1k dataframes/documents within my list I should do it with a for loop/apply function.</p>
<pre><code>   library(&quot;tm&quot;)
    
    corpus$df_19970207 &lt;- tm_map(corpus$df_19970207, removePunctuation)
    corpus$df_19970310 &lt;- tm_map(corpus$df_19970310, removePunctuation)
</code></pre>
<p>... and so on.</p>
<p>corpus would be my list of data frames and df_xxx the respective data frame.</p>
<p>What would be the easiest way to do this?
I know I did not provide a repex but I feel like my question is so trivial..</p>
<p>Many thanks in advance!</p>
","r, for-loop, apply, text-mining","<p>We may use <code>lapply</code></p>
<pre><code>corpus2 &lt;- lapply(corpus, function(x) tmp_map(x, removePunctuation))
</code></pre>
",1,1,34,2023-01-29 21:28:20,https://stackoverflow.com/questions/75278519/apply-a-text-mining-function-to-every-data-frame-within-a-list-of-data-frames-by
Search survey comments (strings) for person names within large vector of names,"<p>I have data from a survey with many open-text comments (circa 40K comments). I need to create a new column which indicates whether the comment on each row contains a person name so I can then find and remove the name before releasing data to others.  I have a full list of person names about (80K names).  I tried using grepl and apply but my solution took a very long time and did not give me accurate results.</p>
<p>I have below some code with example data and the result I'm hoping for.</p>
<pre><code>SurveyID &lt;- 1:4
Comments &lt;- c(&quot;I'm very dissatisfied with Ian Smith as he is not very inclusive&quot;, 
              &quot;May Horner is a great person and I recommend her&quot;,
              &quot;Robbing is at an all-time high&quot;, 
              &quot;The person in charge may be clear but I'm not&quot;)
CommentsData &lt;- data.frame(SurveyID = SurveyID, Comments=Comments)

names &lt;- c(&quot;Ian&quot;, &quot;May&quot;, &quot;John&quot;, &quot;Rob&quot;, &quot;Emily&quot;, &quot;Todd&quot;)

Result &lt;- c(&quot;Name&quot;, &quot;Name&quot;, &quot;Clear&quot;, &quot;Clear&quot;)

</code></pre>
<p>I had tried using grepl and lapply but it was very slow and doesn't fully work as I expected. For example, it finds &quot;Rob&quot; in Robbing.</p>
<pre><code>CommentsData$flag &lt;- ifelse(colSums(do.call(rbind,lapply(names,grepl,CommentsData$Comments,ignore.case=F)))&gt;0,&quot;Name&quot;,&quot;Clear&quot;)
</code></pre>
<p>Would appreciate any suggestions to solve this and speed up the performance.</p>
","r, text-mining, string-matching, survey","<p><code>paste</code> the <code>names</code> and use <code>|</code> (or) to collapse them, put brackets around and place <code>\\b</code> (word boundary) around the regex.</p>
<pre><code>c(&quot;Clear&quot;, &quot;Name&quot;)[1L +
                   grepl(paste0(&quot;\\b(&quot;, paste(names, collapse=&quot;|&quot;), &quot;)\\b&quot;),
                         CommentsData$Comments)]
#[1] &quot;Name&quot;  &quot;Name&quot;  &quot;Clear&quot; &quot;Clear&quot;
</code></pre>
<p>Some other variants using <code>Reduce</code></p>
<pre><code>c(&quot;Clear&quot;, &quot;Name&quot;)[1L +
    Reduce(function(a,b) a | grepl(b, CommentsData$Comments),
       paste0(&quot;\\b&quot;, names, &quot;\\b&quot;), FALSE)]
#[1] &quot;Name&quot;  &quot;Name&quot;  &quot;Clear&quot; &quot;Clear&quot;

c(&quot;Clear&quot;, &quot;Name&quot;)[1L +
     Reduce(function(a,b) `[&lt;-`(a, !a, grepl(b, CommentsData$Comments[!a])),
       paste0(&quot;\\b&quot;, names, &quot;\\b&quot;), logical(nrow(CommentsData)))]
#[1] &quot;Name&quot;  &quot;Name&quot;  &quot;Clear&quot; &quot;Clear&quot;

`[&lt;-`(rep(&quot;Name&quot;, nrow(CommentsData)),
          Reduce(function(a,b) a[!grepl(b, CommentsData$Comments[a])],
       paste0(&quot;\\b&quot;, names, &quot;\\b&quot;), seq_len(nrow(CommentsData))), &quot;Clear&quot;)
#[1] &quot;Name&quot;  &quot;Name&quot;  &quot;Clear&quot; &quot;Clear&quot;
</code></pre>
<p>Benchmark</p>
<pre><code>bench::mark(
&quot;A&quot; = ifelse(colSums(do.call(rbind,lapply(paste0(&quot;\\b&quot;, names, &quot;\\b&quot;),grepl,CommentsData$Comments,ignore.case=F)))&gt;0,&quot;Name&quot;,&quot;Clear&quot;),
&quot;B&quot; = c(&quot;Clear&quot;, &quot;Name&quot;)[1L +
                   grepl(paste0(&quot;\\b(&quot;, paste(names, collapse=&quot;|&quot;), &quot;)\\b&quot;),
                         CommentsData$Comments)],
&quot;C&quot; = c(&quot;Clear&quot;, &quot;Name&quot;)[1L +
                   grepl(paste0(&quot;\\b(&quot;, paste(names, collapse=&quot;|&quot;), &quot;)\\b&quot;),
                         CommentsData$Comments, perl=TRUE)],
&quot;D&quot; = c(&quot;Clear&quot;, &quot;Name&quot;)[1L +
    Reduce(function(a,b) a | grepl(b, CommentsData$Comments),
       paste0(&quot;\\b&quot;, names, &quot;\\b&quot;), FALSE)],
&quot;E&quot; = c(&quot;Clear&quot;, &quot;Name&quot;)[1L +
     Reduce(function(a,b) `[&lt;-`(a, !a, grepl(b, CommentsData$Comments[!a])),
       paste0(&quot;\\b&quot;, names, &quot;\\b&quot;), logical(nrow(CommentsData)))],
&quot;F&quot; = `[&lt;-`(rep(&quot;Name&quot;, nrow(CommentsData)),
          Reduce(function(a,b) a[!grepl(b, CommentsData$Comments[a])],
       paste0(&quot;\\b&quot;, names, &quot;\\b&quot;), seq_len(nrow(CommentsData))), &quot;Clear&quot;) )
</code></pre>
<p>Result</p>
<pre><code>  expression      min  median itr/s…¹ mem_a…² gc/se…³ n_itr  n_gc total…⁴ result
  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:t&gt;   &lt;dbl&gt; &lt;bch:b&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;bch:t&gt; &lt;list&gt;
1 A            97.7µs 105.1µs   8819.      0B    6.07  4358     3   494ms &lt;chr&gt; 
2 B            32.5µs  35.2µs  27940.      0B    2.79  9999     1   358ms &lt;chr&gt; 
3 C            21.7µs  23.5µs  41622.      0B    8.33  9998     2   240ms &lt;chr&gt; 
4 D            88.3µs  93.8µs  10455.  8.18KB    6.07  5166     3   494ms &lt;chr&gt; 
5 E            93.1µs 100.5µs   9732.  4.13KB    8.12  4794     4   493ms &lt;chr&gt; 
6 F              92µs  98.3µs   9944. 12.01KB    8.12  4900     4   493ms &lt;chr&gt; 
</code></pre>
",1,1,39,2023-04-01 18:48:47,https://stackoverflow.com/questions/75908217/search-survey-comments-strings-for-person-names-within-large-vector-of-names
Randomly reshuffle words order in string,"<p>I have a larger data frame consisting of texts where I want to reshuffle the order of words in each string randomly.</p>
<p>To give you a concrete exampleMy data looks somehow like the data below:</p>
<pre><code>library(stringi)
require(tidyverse)

set.seed(123)

n &lt;- 100
df &lt;- data.frame(id = 1:n,
                 text = rep(stri_rand_lipsum(n)))

# Some preprocessing
df &lt;- df %&gt;%
  mutate(text = tolower(text),
         text = gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, text))
</code></pre>
<p>I want to reshuffle word order at random in each string found in the variable <code>text</code>.</p>
<p>I found several ways how to reshuffle each letter, but not any ways of how to reshuffle word's order randomly. Does anybody know how to do it? An important factor is that my data consists of millions of rows, thus, the approach need to be suitable for larger data sets as well.</p>
<p>Thanks!</p>
","r, text, text-mining","<p>We can <code>strsplit</code> the whole string with space <code>&quot; &quot;</code> as the delimiter. Then use <code>sample</code> on these individual words to generate random order, and <code>paste</code> them back into one string. I guess we should directly assign the result into a new column instead of using <code>mutate</code> if we are aiming for efficiency. However, I'm not sure how efficient my code is.</p>
<pre class=""lang-r prettyprint-override""><code>df$random_text &lt;- sapply(strsplit(df$text, &quot; &quot;), \(x) paste(sample(x), collapse = &quot; &quot;))
</code></pre>
",3,0,55,2023-04-18 15:31:38,https://stackoverflow.com/questions/76046453/randomly-reshuffle-words-order-in-string
Indicate in df columns occurrence of keywords of another column in R,"<p>Assuming a data frame <code>df</code> with several columns, of which a &quot;description&quot; field,</p>
<p>and assuming a set of keywords  stored within a separate vector <code>keywords</code>, what is the best practice to:</p>
<ul>
<li>create columns in <code>df</code> for each of these keywords, named accordingly ;</li>
<li>and storing the count of its own number of occurrences within the &quot;description&quot; field of <code>df</code>?</li>
</ul>
<p>e.g.</p>
<pre><code>(df &lt;- data.frame(
  ID = letters[1:10],
  DESCRIPTION = c(&quot;blue&quot;, &quot;red&quot;, &quot;this was green&quot;, &quot;this was red&quot;, &quot;blue and red&quot;, &quot;green&quot;, NA, &quot;green&quot;, &quot;green, blue, and red&quot;, NA)
))
   ID          DESCRIPTION
1   a                 blue
2   b                  red
3   c       this was green
4   d         this was red
5   e         blue and red
6   f                green
7   g                 &lt;NA&gt;
8   h                green
9   i green, blue, and red
10  j                 &lt;NA&gt;
keywords &lt;- c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;)
</code></pre>
<p>would return</p>
<pre><code>   ID          DESCRIPTION blue red green
1   a                 blue    1   0     0
2   b                  red    0   1     0
3   c       this was green    0   0     1
4   d         this was red    0   1     0
5   e         blue and red    1   1     0
6   f                green    0   0     1
7   g                 &lt;NA&gt;    0   0     0
8   h                green    0   0     1
9   i green, blue, and red    1   1     1
10  j                 &lt;NA&gt;    0   0     0
</code></pre>
<p>Prefereable using <code>base</code> R or <code>dplyr</code> (eg. avoiding <code>data.table</code>).</p>
<p>Note: the answer needs to be scalable (many possible keywords).</p>
","r, text-mining","<p>An approach using <code>unnest</code>, <code>str_count</code> and <code>pivot_wider</code>, assuming all occurrences of the keywords within all strings per row have to be counted. Using a slightly modified data set to show multiple counts</p>
<pre><code>library(dplyr)
library(tidyr)
library(stringr)

df %&gt;% 
  mutate(nms = list(!!keywords)) %&gt;% 
  unnest(nms) %&gt;% 
  rowwise() %&gt;% 
  mutate(values = str_count(DESCRIPTION, nms), 
         values = replace(values, is.na(values), 0)) %&gt;% 
  ungroup() %&gt;% 
  pivot_wider(names_from=nms, values_from=values)
# A tibble: 10 × 5
   ID    DESCRIPTION           blue   red green
   &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
 1 a     blue blue                2     0     0
 2 b     red                      0     1     0
 3 c     this was green           0     0     1
 4 d     this was red             0     1     0
 5 e     blue and red             1     1     0
 6 f     green                    0     0     1
 7 g     NA                       0     0     0
 8 h     green                    0     0     1
 9 i     green, blue, and red     1     1     1
10 j     NA                       0     0     0
</code></pre>
<p>or with <strong>base R</strong> <code>sapply</code> and <code>str_count</code></p>
<pre><code>library(stringr)

cbind(df, sapply(keywords, function(x){
  res &lt;- str_count(df$DESCRIPTION, x)
  replace(res, is.na(res), 0)}))
   ID          DESCRIPTION blue red green
1   a            blue blue    2   0     0
2   b                  red    0   1     0
3   c       this was green    0   0     1
4   d         this was red    0   1     0
5   e         blue and red    1   1     0
6   f                green    0   0     1
7   g                 &lt;NA&gt;    0   0     0
8   h                green    0   0     1
9   i green, blue, and red    1   1     1
10  j                 &lt;NA&gt;    0   0     0
</code></pre>
<h4>mod. data</h4>
<pre><code>df &lt;- structure(list(ID = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;, 
&quot;i&quot;, &quot;j&quot;), DESCRIPTION = c(&quot;blue blue&quot;, &quot;red&quot;, &quot;this was green&quot;, 
&quot;this was red&quot;, &quot;blue and red&quot;, &quot;green&quot;, NA, &quot;green&quot;, &quot;green, blue, and red&quot;, 
NA)), row.names = c(NA, -10L), class = &quot;data.frame&quot;)
</code></pre>
",2,0,34,2023-05-04 15:45:15,https://stackoverflow.com/questions/76175011/indicate-in-df-columns-occurrence-of-keywords-of-another-column-in-r
How to extract only those rows of the DataFrame where the values of two columns of the DataFrame are in English Language?,"<p>I have a dataframe which has 27 columns including columns <code>FonctionsStagiaire</code> and <code>ExigencesParticulieres</code>. The dataframe has 13774 rows which are either entirely in English or French. The csv file can be found here: <a href=""https://drive.google.com/file/d/14-ge2hWvF4CyBDaFH_8snQKgKPiIcl3q/view?usp=sharing"" rel=""nofollow noreferrer"">GDrive link</a></p>
<p>I am trying to keep <strong>only</strong> those rows of the dataframe where the values or contents of <code>FonctionsStagiaire</code> or <code>ExigencesParticulieres</code> are in English. I want to drop the <strong>entire</strong> rows where these columns contain values in French language.</p>
<p>I am using <code>langdetect</code> but is getting the error <code>langdetect.lang_detect_exception.LangDetectException: No features in text.</code> I checked out all the solutions on SO to resolve this error but nothing is working.</p>
<p>I also want to check for if these 2 columns contain only <code>NaN</code> values, only numeric or whitespace characters or only punctuation marks, etc. which can't be detected whether it is in English or French and might cause error while using <code>langdetect</code>.</p>
<p>When I am trying to see which row is throwing the error based on this <a href=""https://stackoverflow.com/questions/40783383/error-using-langdetect-in-python-no-features-in-text"">SO question</a> it is showing that <code>This row throws error</code> for every row.</p>
<p>I intend to use <code>langdetect</code> but any other solution would also be helpful as long as it works properly.</p>
<p>Any help is much appreciated.</p>
<p>I am trying with this code:</p>
<pre><code>from langdetect import detect
import pandas as pd
import re
import string

def filter_nonenglish(df):
     
    list = ['FonctionsStagiaire', 'ExigencesParticulieres']

    for col in list:       
        #to check if values are only whitespaces or only numeric characters or only punctuation marks
        #something like this -&gt; if (df[col].apply(lambda x: x.isnumeric()) == True) | (df[col].apply(lambda x: x.isspace()) == True) | (all(i in string.punctuation for i in df[col]) == True):
            return False     
        else:
            #trying to use apply() to apply detect() to each &lt;str&gt; values of the columns and not to the entire pd.Series object
            new_df = df[(df[col].apply(detect).eq('en'))]

    return new_df
   
    #df['FonctionsStagiaire'] = df['FonctionsStagiaire'].apply(detect)
    #df['ExigencesParticulieres'] = df['ExigencesParticulieres'].apply(detect)
    
    #df = df[df['FonctionsStagiaire'] == 'en']
    #df = df[df['ExigencesParticulieres'] == 'en']
    
    #new_df = df[(df.FonctionsStagiaire.apply(detect).eq('en')) &amp; (df.ExigencesParticulieres.apply(detect).eq('en'))]
    
df = pd.read_csv('emplois_df_parsed.csv')

df = df[df['FonctionsStagiaire'].notna() &amp; df['ExigencesParticulieres'].notna()]   #to remove empty values

#to check whether all empty values are removed or not in the column 'FonctionsStagiaire'
#df['FonctionsStagiaire'] = df['FonctionsStagiaire'].str.lower()  
#df = df[df['FonctionsStagiaire'].str.islower()]

#to check whether all empty values are removed or not in the column 'ExigencesParticulieres'
#df['ExigencesParticulieres'] = df['ExigencesParticulieres'].str.lower()
#df = df[df['ExigencesParticulieres'].str.islower()]

#to make sure that values of both the columns are of &lt;str&gt; datatype
df['FonctionsStagiaire'] = pd.Series(df['FonctionsStagiaire'], dtype = &quot;string&quot;)
df['ExigencesParticulieres'] = pd.Series(df['ExigencesParticulieres'], dtype = &quot;string&quot;)

#bool(re.match('^(?=.*[a-zA-Z])', df.loc[:, 'FonctionsStagiaire']))
#bool(re.match('^(?=.*[a-zA-Z])', df.loc[:, 'ExigencesParticulieres']))

#   df[df[column].map(lambda x: x.isascii())]

df_new = filter_nonenglish(df)

df_new.to_csv('emplois_df_filtered.csv', index= False)

</code></pre>
","python, dataframe, nlp, text-mining, language-detection","<p>I'm not sure but it seems like <a href=""https://github.com/Mimino666/langdetect"" rel=""nofollow noreferrer""><code>langdetect</code></a> can't handle <em>urls</em> :</p>
<pre><code>tmp = df.loc[[24], [&quot;FonctionsStagiaire&quot;, &quot;ExigencesParticulieres&quot;]].T
​
                                                                       24
FonctionsStagiaire      https://www2.csrdn.qc.ca//files/jobs/P-22-875-...
ExigencesParticulieres  https://www2.csrdn.qc.ca//files/jobs/P-22-875-...
</code></pre>
<p>Using <code>tmp[24].apply(detect)</code> (<em>row 25</em>) throws a <code>LangDetectException: No features in text.</code></p>
<p>An alternative would be to use <a href=""https://github.com/saffsd/langid.py"" rel=""nofollow noreferrer""><code>langid</code></a> :</p>
<pre><code>#pip install langid
from langid import classify

use_cols = [&quot;FonctionsStagiaire&quot;, &quot;ExigencesParticulieres&quot;]

# checking english content
is_en = [
    classify(r)[0] == &quot;en&quot;
    for r in df[use_cols].fillna(&quot;&quot;).agg(&quot; &quot;.join, axis=1)
]

# not a null content
not_na = df[use_cols].notna().all(axis=1)

# not a random content (optional!)
not_arc = df[use_cols].apply(lambda x: x.str.fullmatch(&quot;\w+\s?\d?&quot;)).any(axis=1)

out = df.loc[is_en &amp; not_na &amp; ~not_arc]
</code></pre>
<p>Output :</p>
<pre><code>print(out.loc[:, use_cols])

                            FonctionsStagiaire                   ExigencesParticulieres
11     As a Level I Technical Customer Serv...  Qualifications           Your contri...
106    What you'll create and do    Today's...  What you'll bring to this role:    A...
140    The Training Department plays a cruc...  REQUIREMENTS:            \t     Coll...
...                                        ...                                      ...
13608  CCHS Facility: Cleveland Clinic Cana...  MINIMUM QUALIFICATIONS:  · Registere...
13697                               Calculate!                          Love numbers...
13698                               Calculate!                          Love numbers...

[311 rows x 2 columns]
</code></pre>
",1,2,73,2023-06-06 17:58:21,https://stackoverflow.com/questions/76417261/how-to-extract-only-those-rows-of-the-dataframe-where-the-values-of-two-columns
Extracting Text and Tables in Semi-Structured .txt,"<p>I have a .txt file that serves as the codebook for a large dataset that looks similar to this</p>
<pre><code>==============================                                                
VAR V960922                                                                    
              NUMERIC                                                         
                                                                              
         Admin.48                                                             
                                                                              
         SUMMARY - POST MODE ASSIGNMENT AND ADMINISTRATION                    
         -----------------------------------------------------------          
                                                                              
              Post mode in this variable refers to beginning mode             
              (question Admin.47).                                            
                                                                              
        749      1.   Assigned to personal, administered as                   
                      personal IW                                             
          7      2.   Assigned to personal, administered as                   
                      telephone IW                                            
         28      3.   Assigned to telephone, administered as                  
                      personal IW                                             
        750      4.   Assigned to telephone, administered as                  
                      telephone IW                                            
                                                                              
                 0.   Inap, no Post IW                                        
                                                                              
============================== 
</code></pre>
<p>I would like to be able to convert this structure into a data frame to help with cleaning and labeling the dataset for use later. My ideal end result would be a table like this</p>
<pre><code>
| Var Name | Freqeuncies | Value Labels
| -------- | --------    | ---------------------------------------------------
| V960922  |        749  | 1. Assigned to personal, administered as personal IW
| V960922  |          7  | 2. Assigned to personal, administered as telephone IW
| V960922  |         28  | 3. Assigned to telephone, administered as personal IW
| V960922  |        750  | 4. Assigned to telephone, administered as telephone IW
| V960922  |         NA  | 0. Inap, no Post IW
     
</code></pre>
<p>Repeating for each of the variables included in the txt file. Each variable in the file follows a similar structure but has variations in the number of values or length of the summary for instance.</p>
<p>My main strategy so far has been to read in the txt file with readLines and then use str_subset to break off lines of the text that meet the criteria I need with the goal of then appending these together to create a data frame.</p>
<pre><code>nes &lt;- readLines(&quot;nes1996var.txt&quot;)
 
vars &lt;- str_subset(nes, &quot;^VAR&quot;, )
vars


numbers &lt;- str_subset(nes,&quot;\\d?\\.&quot;)
numbers
</code></pre>
<p>The first instance of just grabbing variable names worked okay since I ended up with a vector of all the variables like I wanted.</p>
<p>However, trying to pull the tables has been trickier. I've seen other threads on StackOverflow suggest to filter off of the rows that start with numbers, but in the text file there's a lot of deadspace before the numbers so I can't pull just the rows that begin with numbers because technically there aren't any.</p>
<p>So instead I've pulled all the rows that have any numbers at all that are then followed by a period, hoping to catch on the value labels formatting. This was better but not perfect, both because it captured a lot of rows from summaries that included years or other numbers and the fact that some of the rows in the tables actually go over and fill in the second row, meaning sometimes the necessary text got cut off.</p>
<p>Even after that I couldn't find a way to separate the frequency number from the value label strings since they were placed on the same row.</p>
<p>Is there a more efficient/effective method of achieving what I want? I'm somewhat experienced with R but I am also still learning a lot if that helps also.</p>
<p>Edit: The solution provided by Dave did what I needed once I made a few tweaks. Here is the code that worked for me in case anyone happens to be in a similar situation.</p>
<pre><code>rl &lt;- readLines(.txt file path)


## trim the white space from the front and back of each string 
## this will put the frequencies as the first characters in their lines. 
rl &lt;- trimws(rl)

## find the variable delimiters
delims &lt;- grep(&quot;==============================&quot;, rl)

## initialize the output as a list
out &lt;- vector(mode=&quot;list&quot;, length=length(delims)-1)
    ## loop over the delimiters
for (i in 1:(length(delims) - 1)) {
  ## find the text between adjacent delimiters and call that vbl
  vbl &lt;- rl[(delims[i] + 1):(delims[(i + 1)] - 1)]
  ## capture the varname as the stuff after &quot;VAR &quot; in the first row of vbl
  varname &lt;- gsub(&quot;VAR (.*)&quot;, &quot;\\1&quot;, vbl[1])
  ## identify the lines that start with a number
  resps &lt;- grep(&quot;^\\d&quot;, vbl)
  
  if (length(resps) &gt; 0) {
    ## identify the closest blank line to the last last response value and treat 
    ## that as the delimiter for the end of the last response category
    blanks &lt;- which(vbl == &quot;&quot;)
    resps &lt;- c(resps, blanks[min(which(blanks &gt; max(resps)))])
    ## grab the frequencies and remove the last one because the last one should be blank
    freqs &lt;- gsub(&quot;^(\\d+).*&quot;, &quot;\\1&quot;, vbl[resps])
    ## thanks to use padding out resps with the blank line after the last response category
    freqs &lt;- freqs[-length(freqs)]
    ## for each identified response, paste together the text between the identified response row 
    ## and everything that comes before the next identifies response row.
    vlabs &lt;- sapply(1:(length(resps) - 1), function(j) {
      paste(vbl[resps[j]:(resps[(j + 1)] - 1)], collapse = &quot; &quot;)
    })
    ## remove the frequencies and white space from the start of the variable labels
    ## trim the white space around variable labels as well
    vlabs &lt;- trimws(gsub(&quot;^\\d+\\s+(.*)&quot;, &quot;\\1&quot;, vlabs))
    ## collect all the information in one place
    out[[i]] &lt;- data.frame(`Var Name` = varname, Frequencies = freqs, `Value Labels` = vlabs)
  } else {
    out[[i]] &lt;- data.frame(`Var Name` = character(0), Frequencies = character(0), `Value Labels` = character(0))
  }
}
</code></pre>
","r, text-mining, txt","<p>Here's an example.  Comments through identify what each piece of code does.  My assumption is that the delisting rows of equals signs separate each variable.</p>
<pre class=""lang-r prettyprint-override""><code>rl &lt;- readLines(textConnection(&quot;==============================                                                
VAR V960922                                                                    
              NUMERIC                                                         
                                                                              
         Admin.48                                                             
                                                                              
         SUMMARY - POST MODE ASSIGNMENT AND ADMINISTRATION                    
         -----------------------------------------------------------          
                                                                              
              Post mode in this variable refers to beginning mode             
              (question Admin.47).                                            
                                                                              
        749      1.   Assigned to personal, administered as                   
                      personal IW                                             
          7      2.   Assigned to personal, administered as                   
                      telephone IW                                            
         28      3.   Assigned to telephone, administered as                  
                      personal IW                                             
        750      4.   Assigned to telephone, administered as                  
                      telephone IW                                            
                                                                              
                 0.   Inap, no Post IW                                        
                                                                              
============================== &quot;))

## trim the white space from the front and back of each string 
## this will put the frequencies as the first characters in their lines. 
rl &lt;- trimws(rl)

## find the variable delimiters
delims &lt;- grep(&quot;==============================&quot;, rl)

## initialize the output as a list
out &lt;- vector(mode=&quot;list&quot;, length=length(delims)-1)

## loop over the delimiters
for(i in 1:(length(delims)-1)){
  ## find the text between adjacent delimiters and call that vbl
  vbl &lt;- rl[(delims[i]+1):(delims[(i+1)]-1)]
  ## capture the varname as the stuff after &quot;VAR &quot; in the first row of vbl
  varname &lt;- gsub(&quot;VAR (.*)&quot;, &quot;\\1&quot;, vbl[1])
  ## identify the lines that start with a number 
  resps &lt;- grep(&quot;^\\d&quot;, vbl)
  ## identify the closest blank line to the last last response value and treat 
  ## that as the delimiter for the end of the last response category
  blanks &lt;- which(vbl == &quot;&quot;)
  resps &lt;- c(resps, blanks[min(which(blanks &gt; max(resps)))])
  ## grab the frequencies and remove the last one because the last one should be blank 
  freqs &lt;- gsub(&quot;^(\\d+).*&quot;, &quot;\\1&quot;, vbl[resps])
  ## thanks to use padding out resps with the blank line after the last response category
  freqs &lt;- freqs[-length(freqs)]
  ## for each identified response, paste together the text between the identified response row 
  ## and everything that comes before the next identifies response row. 
  vlabs &lt;- sapply(1:(length(resps)-1), function(i){
    paste(vbl[resps[i]:(resps[(i+1)]-1)], collapse=&quot; &quot;)
  })
  ## remove the frequencies and white space from the start of the variable labels
  ## trim the white space around variable labels as well
  vlabs &lt;- trimws(gsub(&quot;^\\d+\\s+(.*)&quot;, &quot;\\1&quot;, vlabs))
  ## collect all the information in one place
  out[[i]] &lt;- data.frame(`Var Name` = varname, 
                    Frequencies = freqs, 
                    `Value Labels` = vlabs)  
  
}
## make all the variables into a markdown table
lapply(out, knitr::kable)
#&gt; [[1]]
#&gt; 
#&gt; 
#&gt; |Var.Name |Frequencies |Value.Labels                                             |
#&gt; |:--------|:-----------|:--------------------------------------------------------|
#&gt; |V960922  |749         |1.   Assigned to personal, administered as personal IW   |
#&gt; |V960922  |7           |2.   Assigned to personal, administered as telephone IW  |
#&gt; |V960922  |28          |3.   Assigned to telephone, administered as personal IW  |
#&gt; |V960922  |750         |4.   Assigned to telephone, administered as telephone IW |
#&gt; |V960922  |0           |0.   Inap, no Post IW                                    |
</code></pre>
<p><sup>Created on 2023-06-08 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.0.2</a></sup></p>
",1,0,154,2023-06-08 20:31:03,https://stackoverflow.com/questions/76435499/extracting-text-and-tables-in-semi-structured-txt
How to detect and tabulate data from an excel file?,"<p>I have an Excel file (Available at <a href=""https://docs.google.com/spreadsheets/d/1c8bSje2wP9fffaoOSDAMYO34RwKff3os/edit?usp=sharing&amp;ouid=110947871950285579478&amp;rtpof=true&amp;sd=true"" rel=""nofollow noreferrer"">Google Drive</a>) with data which was saved with a very strange format in order to get printed easily:</p>
<p><a href=""https://i.sstatic.net/Lbzba.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Lbzba.png"" alt=""Non-relational data, available at Google Drive "" /></a></p>
<p>And every table repeats daily for over 5 years. I need to analyze this data and tried to get a relational format in order to load it in R/Python-like tools and get only 5 columns:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>DATE</th>
<th>CLIENT NAME</th>
<th>TEST</th>
<th>MEASUREMENT</th>
<th>TESTER</th>
</tr>
</thead>
<tbody>
<tr>
<td>01-01-2023</td>
<td>JOHN SMITH</td>
<td>METABOLYTE A</td>
<td>0.01</td>
<td>PhD. IMA GU</td>
</tr>
<tr>
<td>01-01-2023</td>
<td>JOHN SMITH</td>
<td>METABOLYTE B</td>
<td>10</td>
<td>PhD. IMA GU</td>
</tr>
<tr>
<td>01-01-2023</td>
<td>JOHN SMITH</td>
<td>PCR</td>
<td>NEGATIVE</td>
<td>PhD. IMA GU</td>
</tr>
<tr>
<td>01-01-2023</td>
<td>JOHN SMITH</td>
<td>MUTATION</td>
<td>+++</td>
<td>PhD. IMA GU</td>
</tr>
<tr>
<td>01-01-2023</td>
<td>ALBUS DUMBLE</td>
<td>PREGNANT</td>
<td>NEGATIVE</td>
<td>TECH. GUIVER</td>
</tr>
<tr>
<td>01-01-2023</td>
<td>ALBUS DUMBLE</td>
<td>GLUCOSE</td>
<td>121</td>
<td>TECH. GUIVER</td>
</tr>
<tr>
<td>02-01-2023</td>
<td>MAYDAY JUNE</td>
<td>METABOLYTE A</td>
<td>0.01</td>
<td>PhD. IMA GU</td>
</tr>
<tr>
<td>02-01-2023</td>
<td>JOHN SMITH</td>
<td>METABOLYTE A</td>
<td>0.01</td>
<td>TECH. GUIVER</td>
</tr>
<tr>
<td>02-01-2023</td>
<td>JOHN SMITH</td>
<td>METABOLYTE B</td>
<td>10</td>
<td>TECH. GUIVER</td>
</tr>
<tr>
<td>02-01-2023</td>
<td>JOHN SMITH</td>
<td>PCR</td>
<td>NEGATIVE</td>
<td>TECH. GUIVER</td>
</tr>
<tr>
<td>02-01-2023</td>
<td>JOHN SMITH</td>
<td>MUTATION</td>
<td>+++</td>
<td>TECH. GUIVER</td>
</tr>
</tbody>
</table>
</div>
<p>So, in order to get a conversion from non-relational data to relational table I have applied text-mining techniques available at this <a href=""https://github.com/silvertaqman/kunu-kutu-data/tree/master"" rel=""nofollow noreferrer"">GitHub</a> repo. But, basically, have converted everything into one column with <code>tidyr::pivot_longer()</code>. Is there any optimal function or method to detect and tabulate this kind of data, or should i try to do it with a loop (+843 files)?</p>
","python, r, text-mining, non-relational-database","<p>My attempt is based on fact, that the entries are formatted identically, so we can use kind of 'moving window'.</p>
<pre class=""lang-r prettyprint-override""><code>a &lt;- openxlsx::read.xlsx(xlsxFile = &quot;/home/sapi/Downloads/ENERO_2023_prueba.xlsx&quot;,
                    colNames = FALSE
                    )
</code></pre>
<p>Now we have to define data frame for data storage. Comments like <code># [2,2] +0, +1</code> corresponds to row and column of <code>a</code> (loaded excel).</p>
<pre class=""lang-r prettyprint-override""><code>entry &lt;- data.frame(
  NOMBRE = character(),    # [2,2] +0, +1
  FECHA = character(),     # [2,6] +0, +5
  MUESTRA = character(),   # [3,3] +1, +2
  place = character(),     # [3,5] +1, +4
  COLOR = character(),     # [6,3] +4, +2
  ASPECTO = character(),   # [7,3] +5&lt; +2
  DENSIDAD = double(),     # [8,3] +6, +2
  PH = character(),        # [9,3] +7, +2
# ...
  LEUCOCITOS = character(),#[19,3] +17, +2
  BACTERIAS = character(), # [6,7] +4, +6
  PIOCITOS = character()   # [7,7] +5, +6
# ...
)
</code></pre>
<p>Now we have to find all rows with <code>NOMBRE</code></p>
<pre class=""lang-r prettyprint-override""><code>nombre_rows &lt;- which(a[,&quot;X1&quot;] == &quot;NOMBRE&quot;)
</code></pre>
<p>and use it in loop like:</p>
<pre class=""lang-r prettyprint-override""><code>for (i in 1:length(nombre_rows)) {
  x &lt;- nombre_rows[i]
  nombre_cols &lt;- which(a[x,] == &quot;NOMBRE&quot;) # the same for columns
  for (j in 1:length(nombre_cols)) {
    y &lt;- nombre_cols[j]

    entry &lt;- data.frame(
      NOMBRE = a[x, y+1],
      FECHA = a[x, y+5],
      MUESTRA = a[x+1, y+2],
      place = a[x+1, y+4],
      COLOR = a[x+4, y+2],
      ASPECTO = a[x+5, y+2],
      DENSIDAD = a[x+6, y+2],
      PH = a[x+7, y+2],
      # ...
      LEUCOCITOS = a[x+17, y+2],
      BACTERIAS = a[x+4, y+6],
      PIOCITOS = a[x+5, y+6]
      # ...
    ) |&gt; rbind(entry)
  }
}
</code></pre>
<p>And finally the data:</p>
<pre class=""lang-r prettyprint-override""><code>head(entry)
#&gt;             NOMBRE   FECHA MUESTRA            place      COLOR      ASPECTO
#&gt; 1      RUANO EDITH 44957.0   ORINA         CEXTERNA  AMARILLO   LIG.TURBIO 
#&gt; 2    CUNIN ELVIRA  44957.0   ORINA HOSPÌTALIZACION   AMARILLO       TURBIO 
#&gt; 3 LOACHAMIN MARIA  44957.0   ORINA         CEXTERNA  AMARILLO  TRANSPARENTE
#&gt; 4    MANZANO RAUL  44957.0   ORINA         CEXTERNA  AMARILLO   LIG.TURBIO 
#&gt; 5    MERCHAN IVAN  44957.0   ORINA      HIDRATACION ANARANJADO      TURBIO 
#&gt; 6   ACERO ANTHONY  44957.0   ORINA         CEXTERNA   AMARILLO  LIG.TURBIO 
#&gt;   DENSIDAD  PH LEUCOCITOS    BACTERIAS     PIOCITOS
#&gt; 1   1005.0 8.0   NEGATIVO            +    1-2/CAMPO
#&gt; 2   1020.0 5.0        +++           ++ CAMPO LLENO 
#&gt; 3   1005.0 6.0   NEGATIVO OCASIONALES    1-2/CAMPO 
#&gt; 4   1010.0 7.0   NEGATIVO            +   3-7/CAMPO 
#&gt; 5   1015.0 6.0         ++           ++ 50-60/CAMPO 
#&gt; 6   1010.0 5.0   NEGATIVO            +   0-2/CAMPO
</code></pre>
<p><sup>Created on 2023-06-18 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.0.2</a></sup></p>
<p>You should extend the <code>entry</code> data frame to grab all variables from your data. And then loop it through all excels you have.</p>
",1,1,311,2023-06-16 21:01:02,https://stackoverflow.com/questions/76493551/how-to-detect-and-tabulate-data-from-an-excel-file
Deleting certain segments of strings in R?,"<p>In R, suppose I have the following string:</p>
<pre><code>x&lt;-&quot;The bank is going after bank one and pizza and corn.&quot;
</code></pre>
<p>I would like to delete all segments of the string before the <strong>FINAL</strong> time <strong>bank</strong> appears in the sentence, thus obtaining the string &quot;one and pizza and corn.&quot; More generally, if I want to delete all text before the <strong>final time a specific word appears</strong> in a string, if there a way to do this?</p>
","r, string, text, text-mining, sentiment-analysis","<p>You can use group cature regex as shown below:</p>
<pre><code>words &lt;- &quot;bank&quot;
pat &lt;- sprintf(&quot;^.*?(\\b%s\\b).*\\1 ?&quot;, paste0(words, collapse = &quot;|&quot;))
sub(pat, &quot;&quot;, x, perl = TRUE)
</code></pre>
<p>Explanation <code>^</code> from begining the sentence, match <code>.*?</code> anything 0 or many times lazily until the first appearance of the bounded <code>word</code>. From here match greedily everything until you meet the word the last time. Replace everything with a empty string <code>''</code></p>
",2,0,35,2023-07-12 21:05:19,https://stackoverflow.com/questions/76674439/deleting-certain-segments-of-strings-in-r
Web scrape hyperlinked text in R?,"<p><a href=""https://www.nber.org/papers?page=1&amp;perPage=50&amp;sortBy=public_date"" rel=""nofollow noreferrer"">https://www.nber.org/papers?page=1&amp;perPage=50&amp;sortBy=public_date</a></p>
<p>The above webpage consists of a series of academic papers. The titles of these papers (e.g, <em>Sparse Modeling Under Grouped Heterogeneity with an Application to Asset Pricing</em>) are hyperlinked to pages with more detail on them; so, if you click on these titles (hyperlinked text) it directs you to pages with more detail.</p>
<p>Is there any way to scrape all these links in R? I would like all the <strong>links attached to the titles of the academic papers</strong>, not hyperlinks related to other things like people's names. I do not want the titles themselves, just the links they are attached to.</p>
","html, r, web-scraping, text-mining, sentiment-analysis","<p>The abstracts and links are loaded dynamically onto the page using an xhr call which fetches a JSON file to populate the html. If you want to get the links quickly and efficiently, you can download the json directly and parse it. You will find the json url using your browser's console.</p>
<p>Here's a full reprex:</p>
<pre class=""lang-r prettyprint-override""><code>urls &lt;- &quot;https://www.nber.org/api/v1/working_page_listing/contentType/&quot; |&gt;
  paste0(&quot;working_paper/_/_/search?page=1&amp;perPage=50&amp;sortBy=public_date&quot;) |&gt;
  httr::GET() |&gt;
  httr::content(&quot;parsed&quot;) |&gt;
  getElement(&quot;results&quot;) |&gt;
  sapply(function(x) x$url)
</code></pre>
<p>If you want the complete urls, rather than relative ones, simply paste the domain on in front.</p>
<pre class=""lang-r prettyprint-override""><code>paste0(&quot;https://www.nber.org&quot;, urls)
#&gt;  [1] &quot;https://www.nber.org/papers/w31388&quot; &quot;https://www.nber.org/papers/w31424&quot;
#&gt;  [3] &quot;https://www.nber.org/papers/w31482&quot; &quot;https://www.nber.org/papers/w31477&quot;
#&gt;  [5] &quot;https://www.nber.org/papers/w31478&quot; &quot;https://www.nber.org/papers/w31479&quot;
#&gt;  [7] &quot;https://www.nber.org/papers/w31480&quot; &quot;https://www.nber.org/papers/w31481&quot;
#&gt;  [9] &quot;https://www.nber.org/papers/w31490&quot; &quot;https://www.nber.org/papers/w31502&quot;
#&gt; [11] &quot;https://www.nber.org/papers/w31486&quot; &quot;https://www.nber.org/papers/w31483&quot;
#&gt; [13] &quot;https://www.nber.org/papers/w31484&quot; &quot;https://www.nber.org/papers/w31485&quot;
#&gt; [15] &quot;https://www.nber.org/papers/w31494&quot; &quot;https://www.nber.org/papers/w31489&quot;
#&gt; [17] &quot;https://www.nber.org/papers/w31496&quot; &quot;https://www.nber.org/papers/w31491&quot;
#&gt; [19] &quot;https://www.nber.org/papers/w31493&quot; &quot;https://www.nber.org/papers/w31488&quot;
#&gt; [21] &quot;https://www.nber.org/papers/w31495&quot; &quot;https://www.nber.org/papers/w31497&quot;
#&gt; [23] &quot;https://www.nber.org/papers/w31498&quot; &quot;https://www.nber.org/papers/w31499&quot;
#&gt; [25] &quot;https://www.nber.org/papers/w31500&quot; &quot;https://www.nber.org/papers/w31501&quot;
#&gt; [27] &quot;https://www.nber.org/papers/w31487&quot; &quot;https://www.nber.org/papers/w31503&quot;
#&gt; [29] &quot;https://www.nber.org/papers/w31476&quot; &quot;https://www.nber.org/papers/w31492&quot;
#&gt; [31] &quot;https://www.nber.org/papers/w31450&quot; &quot;https://www.nber.org/papers/w31449&quot;
#&gt; [33] &quot;https://www.nber.org/papers/w31448&quot; &quot;https://www.nber.org/papers/w31453&quot;
#&gt; [35] &quot;https://www.nber.org/papers/w31451&quot; &quot;https://www.nber.org/papers/w31452&quot;
#&gt; [37] &quot;https://www.nber.org/papers/w31454&quot; &quot;https://www.nber.org/papers/w31455&quot;
#&gt; [39] &quot;https://www.nber.org/papers/w31465&quot; &quot;https://www.nber.org/papers/w31458&quot;
#&gt; [41] &quot;https://www.nber.org/papers/w31459&quot; &quot;https://www.nber.org/papers/w31460&quot;
#&gt; [43] &quot;https://www.nber.org/papers/w31461&quot; &quot;https://www.nber.org/papers/w31472&quot;
#&gt; [45] &quot;https://www.nber.org/papers/w31473&quot; &quot;https://www.nber.org/papers/w31475&quot;
#&gt; [47] &quot;https://www.nber.org/papers/w31474&quot; &quot;https://www.nber.org/papers/w31470&quot;
#&gt; [49] &quot;https://www.nber.org/papers/w31462&quot; &quot;https://www.nber.org/papers/w31471&quot;
</code></pre>
<p>These are all the complete links to the articles on the first page. They are not in the order they appear on the page; I'm unsure whether these are just randomized.</p>
<p><sup>Created on 2023-07-24 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.0.2</a></sup></p>
",3,0,58,2023-07-24 20:25:36,https://stackoverflow.com/questions/76757814/web-scrape-hyperlinked-text-in-r
Text mining in R: delete first sentence of each document,"<p>I have several documents and do not need the first sentence of each document.
I could not find a solution so far.</p>
<p>Here is an example. The structure of the data looks like this</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>case_number</th>
<th>text</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Today is a good day. It is sunny.</td>
</tr>
<tr>
<td>2</td>
<td>Today is a bad day. It is rainy.</td>
</tr>
</tbody>
</table>
</div>
<p>So the results should look like this</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>case_number</th>
<th>text</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>It is sunny.</td>
</tr>
<tr>
<td>2</td>
<td>It is rainy.</td>
</tr>
</tbody>
</table>
</div>
<p>Here is the example dataset:</p>
<pre class=""lang-r prettyprint-override""><code>case_number &lt;- c(1, 2)

text &lt;- c(&quot;Today is a good day. It is sunny.&quot;,
          &quot;Today is a bad day. It is rainy.&quot;)

data &lt;- data.frame(case_number, text)
</code></pre>
","r, text-mining","<p>If there's a chance that sentences might include some punctuation (e.g. abbreviations or numerics), and you are using some text mining library anyway, it makes perfect sense to let it handle tokenization.</p>
<p>With <code>{tidytext}</code> :</p>
<pre class=""lang-r prettyprint-override""><code>library(dplyr)
library(tidytext)

# exmple with punctuation in 1st sentence
data &lt;- data.frame(case_number = c(1, 2),
                   text = c(&quot;Today is a good day, above avg. for sure, by 5.1 points. It is sunny.&quot;,
                            &quot;Today is a bad day. It is rainy.&quot;))
# tokenize to sentences, converting tokens to lowercase is optional
data %&gt;% 
  unnest_sentences(s, text)
#&gt;   case_number                                                        s
#&gt; 1           1 today is a good day, above avg. for sure, by 5.1 points.
#&gt; 2           1                                             it is sunny.
#&gt; 3           2                                      today is a bad day.
#&gt; 4           2                                             it is rainy.

# drop 1st record of every case_number group
data %&gt;% 
  unnest_sentences(s, text) %&gt;% 
  filter(row_number() &gt; 1, .by = case_number)
#&gt;   case_number            s
#&gt; 1           1 it is sunny.
#&gt; 2           2 it is rainy.
</code></pre>
<p><sup>Created on 2023-08-10 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.0.2</a></sup></p>
",1,0,54,2023-08-10 07:05:52,https://stackoverflow.com/questions/76873337/text-mining-in-r-delete-first-sentence-of-each-document
problem with text find and replacement in python,"<p>i have very specific function. I have 2 strings, one that is backup of input of the code, and second one, that is modified by steps like replacing spaces, extract of information etc (not important for this case).</p>
<p>I need to find a match in those strings, even when the first one is modified. After the match is found, i need to store the match from original string (without modification), and remove it from &quot;sub_str&quot;/&quot;modified_sub_str&quot;.</p>
<pre class=""lang-py prettyprint-override""><code>def find_and_save(sub_str, main_str):
    # Convert both strings to lowercase and remove spaces, commas, and hyphens for case-insensitive matching
    sub_str_mod = re.escape(sub_str.lower().replace(&quot; &quot;, &quot;&quot;).replace(&quot;,&quot;, &quot;&quot;).replace(&quot;-&quot;, &quot;&quot;))
    main_str_mod = main_str.lower().replace(&quot; &quot;, &quot;&quot;).replace(&quot;,&quot;, &quot;&quot;).replace(&quot;-&quot;, &quot;&quot;)

    # Use re.search() to find the substring in the modified main string
    match = re.search(sub_str_mod, main_str_mod)

    if match:
        start = match.start()
        end = match.end()

        count = 0
        original_start = 0
        original_end = 0

        for i, c in enumerate(main_str):
            if c not in [' ', ',', '-']:
                count += 1
            if count == start + 1:
                original_start = i
            if count == end:
                original_end = i + 1
                break

        original_sub_str = main_str[original_start:original_end]

        # If the whole sub_str is matching with some part of main_str, return an empty string as modified_sub_str
        if original_sub_str.lower().replace(&quot; &quot;, &quot;&quot;).replace(&quot;,&quot;, &quot;&quot;).replace(&quot;-&quot;, &quot;&quot;) == sub_str_mod:
            modified_sub_str = &quot;&quot;
        else:
            # Remove the matching part from sub_str in a case-insensitive manner
            modified_sub_str = re.sub(re.escape(original_sub_str), '', sub_str, flags=re.IGNORECASE)

        return modified_sub_str, original_sub_str  # Returns the modified sub_str and the matched string in its original form
    else:
        return sub_str, None  # Returns sub_str as it was and None if no match is found
</code></pre>
<p>But i have a specific problems with this code. For example if i have inputs like</p>
<pre class=""lang-py prettyprint-override""><code>sub_str = &quot;internationalworkshopongraphene/ceramiccomposites2016,wgcc2016&quot;
</code></pre>
<p>and</p>
<pre class=""lang-py prettyprint-override""><code>main_str = &quot;Roč. 37, č. 12, International Workshop on Graphene/Ceramic Composites 2016, WGCC 2016 (2017), s. 3773-3780 [print, online]&quot; 
</code></pre>
<p>This code can find match, can return &quot;original_sub_str&quot;, but cannot remove the match from &quot;modified_sub_str&quot;.</p>
<p>The same problem for those inputs:
&quot;sub_str&quot; - &quot;main_str&quot;</p>
<pre class=""lang-none prettyprint-override""><code>&quot;isnnm-2016,internationalsymposiumon&quot;
&quot;Roč. 2017, č. 65, ISNNM-2016, International Symposium on Novel and Nano Materials (2017), s. 76-82 [print, online]&quot;

&quot;fractographyofadvancedceramics5“fractographyfrommacro-tonano-scale”&quot; 
&quot;Roč. 37, č. 14, Fractography of Advanced Ceramics 5 “Fractography from MACRO- to NANO-scale” (2017), s. 4315-4322 [print, online]&quot;

&quot;73.zjazdchemikov,zborníkabstraktov&quot;
&quot;Roč. 17, č. 1, 73. Zjazd chemikov, zborník abstraktov (2021), s. 246-246 [print, online]&quot; 
</code></pre>
<p>I cant find a solution even with use of AI, but i know theres a problem with replace function, unique symbols, case sensitivity.</p>
","python, replace, extract, text-mining","<p>Your <code>sub_str_mod</code> was a regex escaped string. <code>.</code> is converted to <code>\.</code>, now <code>original_sub_str</code> can not be found because <code>original_sub_str</code> has no backslash. (Next time use a debugger)</p>
<p>Removed <code>re</code> and do all with literal string find.</p>
<p>Removed the <code>else</code> because the <code>if</code> test is always <code>True</code></p>
<pre class=""lang-py prettyprint-override""><code>def clean_str(s) -&gt; str:
    return s.lower().replace(&quot; &quot;, &quot;&quot;).replace(&quot;,&quot;, &quot;&quot;).replace(&quot;-&quot;, &quot;&quot;)

def find_and_save(sub_str, main_str):
    # Convert both strings to lowercase and remove spaces, commas, and hyphens for case-insensitive matching
    sub_str_mod = clean_str(sub_str)
    main_str_mod = clean_str(main_str)

    # find the substring in the modified main string
    start = main_str_mod.find(sub_str_mod)
    if start == -1:
        return sub_str, None  # Returns sub_str as it was and None if no match is found

    end = start + len(sub_str_mod)

    count = 0
    original_start = 0
    original_end = 0

    for i, c in enumerate(main_str):
        if c not in [' ', ',', '-']:
            count += 1
        if count == start + 1:
            original_start = i
        if count == end:
            original_end = i + 1
            break

    original_sub_str = main_str[original_start:original_end]

    # If the whole sub_str is matching with some part of main_str, return an empty string as modified_sub_str
    modified_sub_str = &quot;&quot;
    if clean_str(original_sub_str) == sub_str_mod:  # always True
        modified_sub_str = &quot;&quot;
    return modified_sub_str, original_sub_str  # Returns the modified sub_str and the matched string in its original form
</code></pre>
<p>Output of the 4 cases:</p>
<pre class=""lang-none prettyprint-override""><code>('', 'International Workshop on Graphene/Ceramic Composites 2016, WGCC 2016')
('', 'ISNNM-2016, International Symposium on')
('', 'Fractography of Advanced Ceramics 5 “Fractography from MACRO- to NANO-scale”')
('', '73. Zjazd chemikov, zborník abstraktov')
</code></pre>
",1,1,72,2023-09-29 10:55:47,https://stackoverflow.com/questions/77201376/problem-with-text-find-and-replacement-in-python
Python NLTK text dispersion plot has y vertical axis is in backwards / reversed order,"<p>Since last month NLTK dispersion_plot seems to have y (vertical) axis in reversed order on my machine. This is likely something about my versions of software (I am on a school virtual machine).</p>
<p>versions:
nltk 3.8.1
matplotlib 3.7.2
Python 3.9.13</p>
<p>code:</p>
<pre><code>from nltk.draw.dispersion import dispersion_plot
words=['aa','aa','aa','bbb','cccc','aa','bbb','aa','aa','aa','cccc','cccc','cccc','cccc']
targets=['aa','bbb', 'f', 'cccc']
dispersion_plot(words, targets)
</code></pre>
<p><a href=""https://i.sstatic.net/ciYPl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ciYPl.png"" alt=""enter image description here"" /></a></p>
<p>expected: aaa is present at the beginning, and cccc at the end.
actual: it's backwards! also notice f should be completely absent - instead bbb is absent.</p>
<p>conclusion:
Y axis is backwards.</p>
","python, nltk, text-mining","<p>I found source code for <a href=""https://www.nltk.org/_modules/nltk/draw/dispersion.html#dispersion_plot"" rel=""nofollow noreferrer"">nltk.draw.dispersion</a> and it seems there is mistake.</p>
<pre><code>def dispersion_plot(text, words, ignore_case=False, title=&quot;Lexical Dispersion Plot&quot;):
    &quot;&quot;&quot;
    Generate a lexical dispersion plot.

    :param text: The source text
    :type text: list(str) or iter(str)
    :param words: The target words
    :type words: list of str
    :param ignore_case: flag to set if case should be ignored when searching text
    :type ignore_case: bool
    :return: a matplotlib Axes object that may still be modified before plotting
    :rtype: Axes
    &quot;&quot;&quot;

    try:
        import matplotlib.pyplot as plt
    except ImportError as e:
        raise ImportError(
            &quot;The plot function requires matplotlib to be installed. &quot;
            &quot;See https://matplotlib.org/&quot;
        ) from e

    word2y = {
        word.casefold() if ignore_case else word: y
        for y, word in enumerate(reversed(words))  # &lt;--- HERE
    }
    xs, ys = [], []
    for x, token in enumerate(text):
        token = token.casefold() if ignore_case else token
        y = word2y.get(token)
        if y is not None:
            xs.append(x)
            ys.append(y)

    _, ax = plt.subplots()
    ax.plot(xs, ys, &quot;|&quot;)
    ax.set_yticks(list(range(len(words))), words, color=&quot;C0&quot;)  # &lt;--- HERE
    ax.set_ylim(-1, len(words))
    ax.set_title(title)
    ax.set_xlabel(&quot;Word Offset&quot;)
    return ax



if __name__ == &quot;__main__&quot;:
    import matplotlib.pyplot as plt

    from nltk.corpus import gutenberg

    words = [&quot;Elinor&quot;, &quot;Marianne&quot;, &quot;Edward&quot;, &quot;Willoughby&quot;]
    dispersion_plot(gutenberg.words(&quot;austen-sense.txt&quot;), words)
    plt.show()
</code></pre>
<p>It calculates <code>word2y</code> using <code>reversed(words)</code></p>
<pre><code>for y, word in enumerate(reversed(words))
</code></pre>
<p>but later it uses <code>ax.set_yticks()</code> using <code>words</code> but it should use <code>reversed(words)</code></p>
<pre><code>ax.set_yticks(list(range(len(words))), words, color=&quot;C0&quot;)
</code></pre>
<p>(or it should calculate <code>word2y</code> without using <code>reversed()</code>).</p>
<p>I added <code># &lt;--- HERE</code> in code above to show these places.</p>
<p>It may need to report it as a issue.</p>
<p>At this moment you can get <code>ax</code> and use <code>set_yticks</code> with <code>reversed</code> to correct it.<br />
In your code it will be <code>targets</code> instead of <code>words</code></p>
<pre><code>ax = dispersion_plot(words, targets)

ax.set_yticks(list(range(len(targets))), reversed(targets), color=&quot;C0&quot;)
</code></pre>
<hr />
<p>Full working code</p>
<pre><code>import matplotlib.pyplot as plt
from nltk.draw.dispersion import dispersion_plot

words = ['aa','aa','aa','bbb','cccc','aa','bbb','aa','aa','aa','cccc','cccc','cccc','cccc']
targets = ['aa','bbb', 'f', 'cccc']

ax = dispersion_plot(words, targets)
ax.set_yticks(list(range(len(targets))), reversed(targets), color=&quot;C0&quot;)

plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/XDoYC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XDoYC.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>EDIT:</strong> I seems this problem was reported few months ago and they add <code>reversed()</code> in code on GitHub - and probably it will work in next version</p>
<p><a href=""https://github.com/nltk/nltk/issues/3133"" rel=""nofollow noreferrer"">dispersion plot not working properly · Issue #3133 · nltk/nltk</a></p>
<p><a href=""https://github.com/nltk/nltk/pull/3134"" rel=""nofollow noreferrer"">dispersion plot not working properly by Apros7 · Pull Request #3134 · nltk/nltk</a></p>
",2,0,432,2023-10-10 00:05:07,https://stackoverflow.com/questions/77262318/python-nltk-text-dispersion-plot-has-y-vertical-axis-is-in-backwards-reversed
catelog sentences into 5 words that represent them,"<p>I have dataframe with 1000 text rows. <code>df['text']</code></p>
<p>I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)</p>
<p>every score will be in <code>df[&quot;word1&quot;]</code> ,<code>df[&quot;word2&quot;]</code> and etc</p>
<p>I will glad for recomendations how to do that</p>
<p><strong>edit</strong></p>
<p>represnt = the semantic distance between the word to the text.</p>
<p>for example -
lets say in row 1 the text is &quot;i want to eat&quot;
and I have 2 words : food and house.</p>
<p>so in <code>df[&quot;food &quot;]</code> it would be higher score than in <code>df[&quot;house&quot;]</code></p>
","python, pandas, nlp, text-mining, similarity","<p>You could use a pre-trained sentence transformer model from <a href=""https://pypi.org/project/sentence-transformers/"" rel=""nofollow noreferrer""><code>sentence_transformers</code></a>:</p>
<pre><code>import pandas as pd
from sentence_transformers import SentenceTransformer, util


class SemanticSimilarityCalculator:
  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -&gt; None:
    self.model = SentenceTransformer(model_name)
    self.word_embeddings = None

  def encode_words(self, words: list[str]) -&gt; None:
    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)
    self.words = words

  def calculate_similarity(self, text: str) -&gt; list[float]:
    if self.word_embeddings is None:
      raise ValueError('Words must be encoded before calculating similarity.')
    text_embedding = self.model.encode(text, convert_to_tensor=True)
    similarities = util.cos_sim(text_embedding, self.word_embeddings)[
      0
    ].tolist()
    return similarities

  def add_similarity_scores_to_df(
    self, df: pd.DataFrame, text_column: str
  ) -&gt; pd.DataFrame:
    if self.words is None:
      raise ValueError(
        'Words must be encoded before adding scores to the DataFrame.'
      )
    similarity_columns = ['word_' + word for word in self.words]
    df[similarity_columns] = df[text_column].apply(
      lambda text: pd.Series(self.calculate_similarity(text))
    )
    return df


def main():
  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}
  df = pd.DataFrame(data)
  words = ['food', 'house', 'sleep', 'drink', 'run']
  calculator = SemanticSimilarityCalculator()
  calculator.encode_words(words)
  df_with_scores = calculator.add_similarity_scores_to_df(
    df, text_column='text'
  )
  print(df_with_scores)


if __name__ == '__main__':
  main()
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>               text  word_food  word_house  word_sleep  word_drink  word_run
0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350
1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716
2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838
</code></pre>
",0,0,54,2024-12-19 10:16:47,https://stackoverflow.com/questions/79293889/catelog-sentences-into-5-words-that-represent-them
