Sentiment analysis for Twitter in Python,"<p>I'm looking for an open source implementation, preferably in python, of <strong>Textual Sentiment Analysis</strong> (<a href=""http://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Sentiment_analysis</a>). Is anyone familiar with such open source implementation I can use?</p>

<p>I'm writing an application that searches twitter for some search term, say ""youtube"", and counts ""happy"" tweets vs. ""sad"" tweets. 
I'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.
I haven't been able to find such sentiment analyzer so far, specifically not in python. 
Are you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python.</p>

<p>Note, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts.</p>

<p>BTW, twitter does support the "":)"" and "":("" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself.</p>

<p>Thanks!</p>

<p>BTW, an early demo is <a href=""http://twitgraph.appspot.com/?show_inputs=1&amp;duration=30&amp;q=youtube+annotations"" rel=""noreferrer"">here</a> and the code I have so far is <a href=""http://code.google.com/p/twitgraph/"" rel=""noreferrer"">here</a> and I'd love to opensource it with any interested developer.</p>
","python, machine-learning, nlp, open-source, sentiment-analysis","<p>With most of these kinds of applications, you'll have to roll much of your own code for a statistical classification task. As Lucka suggested, NLTK is the perfect tool for natural language manipulation in Python, so long as your goal doesn't interfere with the non commercial nature of its license.  However, I would suggest other software packages for modeling.  I haven't found many strong advanced machine learning models available for Python, so I'm going to suggest some standalone binaries that easily cooperate with it.</p>

<p>You may be interested in <a href=""http://tadm.sf.net"" rel=""noreferrer"">The Toolkit for Advanced Discriminative Modeling</a>, which can be easily interfaced with Python.  This has been used for classification tasks in various areas of natural language processing.  You also have a pick of a number of different models.  I'd suggest starting with Maximum Entropy classification so long as you're already familiar with implementing a Naive Bayes classifier.  If not, you may want to look into it and code one up to really get a decent understanding of statistical classification as a machine learning task.</p>

<p>The University of Texas at Austin computational linguistics groups have held classes where most of the projects coming out of them have used this great tool.  You can look at the course page for <a href=""http://comp.ling.utexas.edu/jbaldrid/courses/2006/cl2/"" rel=""noreferrer"">Computational Linguistics II</a> to get an idea of how to make it work and what previous applications it has served.</p>

<p>Another great tool which works in the same vein is <a href=""http://mallet.cs.umass.edu/"" rel=""noreferrer"">Mallet</a>.  The difference between Mallet is that there's a bit more documentation and some more models available, such as decision trees, and it's in Java, which, in my opinion, makes it a little slower.  <a href=""http://www.cs.waikato.ac.nz/ml/weka/"" rel=""noreferrer"">Weka</a> is a whole suite of different machine learning models in one big package that includes some graphical stuff, but it's really mostly meant for pedagogical purposes, and isn't really something I'd put into production.</p>

<p>Good luck with your task.  The real difficult part will probably be the amount of knowledge engineering required up front for you to classify the 'seed set' off of which your model will learn.  It needs to be pretty sizeable, depending on whether you're doing binary classification (happy vs sad) or a whole range of emotions (which will require even more).  Make sure to hold out some of this engineered data for testing, or run some tenfold or remove-one tests to make sure you're actually doing a good job predicting before you put it out there. And most of all, have fun!  This is the best part of NLP and AI, in my opinion.</p>
",44,88,52226,2009-02-21 21:20:24,https://stackoverflow.com/questions/573768/sentiment-analysis-for-twitter-in-python
Seed data for sentiment analysis,"<p>I'm playing around with sentiment analysis, and I'm looking for some seed data.  Is there a free dictionary around?</p>

<p>It can be really simple: 3 sets of texts/sentences, for ""positive"", ""negative"", ""neutral"".  It doesn't have to be huge.</p>

<p>Eventually I'll probably generate my own seed data for my specific use case, but it would be great to have something to play with now while I'm building the thing.</p>
","dictionary, sentiment-analysis","<p><a href=""http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"" rel=""nofollow noreferrer"">Bing Liu and Minqing Hu</a> from UIC have a number of datasets:</p>

<ul>
<li><a href=""http://www.cs.uic.edu/~liub/FBS/CustomerReviewData.zip"" rel=""nofollow noreferrer"">http://www.cs.uic.edu/~liub/FBS/CustomerReviewData.zip</a></li>
<li><a href=""http://www.cs.uic.edu/~liub/FBS/Reviews-9-products.rar"" rel=""nofollow noreferrer"">http://www.cs.uic.edu/~liub/FBS/Reviews-9-products.rar</a></li>
</ul>

<p><a href=""http://www.cs.cornell.edu/people/pabo/movie-review-data/"" rel=""nofollow noreferrer"">Bo Pang</a> from Cornell has some more.</p>
",4,10,5237,2009-07-28 19:07:34,https://stackoverflow.com/questions/1196133/seed-data-for-sentiment-analysis
Unsupervised Sentiment Analysis,"<p>I've been reading a lot of articles that explain the need for an initial set of texts that are classified as either 'positive' or 'negative' before a sentiment analysis system will really work.</p>

<p>My question is: Has anyone attempted just doing a rudimentary check of 'positive' adjectives vs 'negative' adjectives, taking into account any simple negators to avoid classing 'not happy' as positive? If so, are there any articles that discuss just why this strategy isn't realistic?</p>
","machine-learning, nlp, sentiment-analysis","<p>A <a href=""https://www.aclweb.org/anthology/P02-1053.pdf"" rel=""nofollow noreferrer"">classic paper by Peter Turney</a> (2002) explains a method to do unsupervised sentiment analysis (positive/negative classification) using only the words <em>excellent</em> and <em>poor</em> as a seed set. Turney uses the <a href=""http://en.wikipedia.org/wiki/Pointwise_mutual_information"" rel=""nofollow noreferrer"">mutual information</a> of other words with these two adjectives to achieve an accuracy of 74%.</p>
",71,48,33349,2010-10-13 04:25:42,https://stackoverflow.com/questions/3920759/unsupervised-sentiment-analysis
Perl or Java Sentiment Analysis,"<p>I was wondering if anybody knew of any good Perl modules and/or Java classes for sentiment analysis.  I have read about LingPipe, but the program would eventually need to be used for commercial use so something open-source would be better.  I also looked into GATE, but their documentation on sentiment analysis is sparse at best.</p>
","java, perl, nlp, sentiment-analysis","<p>Have a look at <a href=""http://search.cpan.org/perldoc?Rate_Sentiment"" rel=""noreferrer"">Rate_Sentiment</a> in the <a href=""http://search.cpan.org/dist/WebService-GoogleHack/"" rel=""noreferrer"">WebService::GoogleHack</a> module at <a href=""http://search.cpan.org/"" rel=""noreferrer"">CPAN</a>. There's more information about the project at <a href=""http://google-hack.sourceforge.net/"" rel=""noreferrer"">SourceForge</a>.</p>
",7,13,3949,2010-11-15 14:20:24,https://stackoverflow.com/questions/4185199/perl-or-java-sentiment-analysis
Best Algorithmic Approach to Sentiment Analysis,"<p>My requirement is taking in news articles and determining if they are positive or negative about a subject.  I am taking the approach outlined below, but I keep reading NLP may be of use here.  All that I have read has pointed at NLP detecting opinion from fact, which I don't think would matter much in my case. I'm wondering two things:</p>

<p>1)  Why wouldn't my algorithm work and/or how can I improve it? ( I know sarcasm would probably be a pitfall, but again I don't see that occurring much in the type of news we will be getting)</p>

<p>2)  How would NLP help, why should I use it?</p>

<p>My algorithmic approach (I have dictionaries of positive, negative, and negation words):</p>

<p>1) Count number of positive and negative words in article</p>

<p>2) If a negation word is found with 2 or 3 words of the positive or negative word, (ie: NOT the best) negate the score.</p>

<p>3) Multiply the scores by weights that have been manually assigned to each word. (1.0 to start)</p>

<p>4) Add up the totals for positive and negative to get the sentiment score.</p>
","nlp, sentiment-analysis","<p>I don't think there's anything particularly <strong><em>wrong</em></strong> with your algorithm, it's a fairly straightforward and practical way to go, but there are a lot of situations where it will get make mistakes.</p>

<ol>
<li><p><strong>Ambiguous sentiment words</strong> - ""This product works terribly"" vs. ""This product is terribly good""</p></li>
<li><p><strong>Missed negations</strong> - ""I would never in a millions years say that this product is worth buying""</p></li>
<li><p><strong>Quoted/Indirect text</strong> - ""My dad says this product is terrible, but I disagree""</p></li>
<li><p><strong>Comparisons</strong> - ""This product is about as useful as a hole in the head""</p></li>
<li><p><strong>Anything subtle</strong> - ""This product is ugly, slow and uninspiring, but it's the only thing on the market that does the job""</p></li>
</ol>

<p>I'm using product reviews for examples instead of news stories, but you get the idea. In fact, news articles are probably harder because they will often try to show both sides of an argument and tend to use a certain style to convey a point. The final example is quite common in opinion pieces, for example.</p>

<p>As far as NLP helping you with any of this, <a href=""http://en.wikipedia.org/wiki/Word_sense_disambiguation"" rel=""noreferrer"">word sense disambiguation</a> (or even just <a href=""http://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""noreferrer"">part-of-speech tagging</a>) may help with (1), <a href=""http://en.wikipedia.org/wiki/Statistical_parsing"" rel=""noreferrer"">syntactic parsing</a> might help with the long range dependencies in (2), some kind of <a href=""http://en.wikipedia.org/wiki/Shallow_parsing"" rel=""noreferrer"">chunking</a> might help with (3). It's all research level work though, there's nothing that I know of that you can directly use. Issues (4) and (5) are a lot harder, I throw up my hands and give up at this point.</p>

<p>I'd stick with the approach you have and look at the output carefully to see if it is doing what you want. Of course that then raises the issue of what you want you understand the definition of ""sentiment"" to be in the first place...</p>
",33,26,18113,2010-11-16 21:57:18,https://stackoverflow.com/questions/4199441/best-algorithmic-approach-to-sentiment-analysis
1 million sentences to save in DB - removing non-relevant English words,"<p>I am trying to train a Naive Bayes classifier with positive/negative words extracting from a sentiment. example:  </p>

<p>I love this movie :))  </p>

<p>I hate when it rains :(  </p>

<p>The idea is I extract positive or negative sentences based on the emoctions used, but in order to train a classifier and persist it into database.  </p>

<p>The problem is that I have more than 1 million such sentences, so if I train it word by word, the database will go for a toss. I want to remove all non-relevant word example 'I','this', 'when', 'it' so that number of times I have to make a database query is less.  </p>

<p>Please help me in resolving this issue to suggest me better ways of doing it  </p>

<p>Thank you</p>
","database, hadoop, nlp, classification, sentiment-analysis","<p>You might want to check this out
<a href=""http://books.google.com/books?id=CE1QzecoVf4C&amp;lpg=PA390&amp;ots=OHuYwLRhag&amp;dq=sentiment%20%20mining%20for%20fortune%20500&amp;pg=PA379#v=onepage&amp;q=sentiment%20%20mining%20for%20fortune%20500&amp;f=false"" rel=""nofollow"">http://books.google.com/books?id=CE1QzecoVf4C&amp;lpg=PA390&amp;ots=OHuYwLRhag&amp;dq=sentiment%20%20mining%20for%20fortune%20500&amp;pg=PA379#v=onepage&amp;q=sentiment%20%20mining%20for%20fortune%20500&amp;f=false</a></p>
",4,6,944,2010-11-23 17:39:32,https://stackoverflow.com/questions/4259044/1-million-sentences-to-save-in-db-removing-non-relevant-english-words
A Social Network with Artificial Intelligence,"<p>I'm planning a Social Network with an Artificial Intelligence . that means that the SN will take the conversations (in english) .. and analyse them in order to extract the general opinion about a subject. This helps to collect information and build statistics, which will be sent then to the appropriate user.
My question is : how to organize words and grammatical rules in a database, in order to help the social network extracting a general opinion from a conversation (agree, disagree..etc)! 
thank you.</p>
","social-networking, sentiment-analysis","<p>its very challengning task to extract text that reflects real opnions of people because machine learning is still not advanced in this area, take a look at the following example:
your program finds this sentence on some SN website:
""I made her duck""</p>

<pre><code>what are possible options that your program will infer:
1-i cooked a duck for her
2-i ""magically"" transformed her into a duck""
3-i took her duck and created it.
and many other
</code></pre>

<p>so its really challenge. I suggest to take a look at NLP(natural language processing) which covers this area you're interested in</p>
",1,1,998,2010-11-24 23:54:42,https://stackoverflow.com/questions/4272625/a-social-network-with-artificial-intelligence
What are the most challenging issues in Sentiment Analysis(opinion mining)?,"<p>Opinion Mining/Sentiment Analysis is a somewhat recent subtask of Natural Language processing.Some compare it to text classification,some take a more deep stance towards it. What do you think about the most challenging issues in Sentiment Analysis(opinion mining)? Can you name a few?</p>
","nlp, sentiment-analysis","<p>The key challenges for sentiment analysis are:-</p>

<p>1) Named Entity Recognition - What is the person actually talking about, e.g. is 300 Spartans a group of Greeks or a movie?</p>

<p>2) Anaphora Resolution - the problem of resolving what a pronoun, or a noun phrase refers to.  ""We watched the movie and went to dinner; it was awful.""  What does ""It"" refer to?</p>

<p>3) Parsing - What is the subject and object of the sentence, which one does the verb and/or adjective actually refer to?</p>

<p>4) Sarcasm - If you don't know the author you have no idea whether 'bad' means bad or good.</p>

<p>5) Twitter - abbreviations, lack of capitals, poor spelling, poor punctuation, poor grammar, ... </p>
",19,1,10285,2011-01-26 15:15:34,https://stackoverflow.com/questions/4806176/what-are-the-most-challenging-issues-in-sentiment-analysisopinion-mining
Is there a Sentiment Analysis Script available in open source?,"<p>I am looking for a sentiment analysis script / soyurce code preferably in PHP. Do you know of any such script?
Thanks, Sameer</p>
","php, sentiment-analysis","<p>PHP/ir is a great blog for this kind of stuff, and has a post on a <a href=""http://www.phpir.com/bayesian-opinion-mining"" rel=""noreferrer"">basic bayesian sentiment classifier</a>.</p>
",7,4,8104,2011-03-03 06:01:37,https://stackoverflow.com/questions/5177246/is-there-a-sentiment-analysis-script-available-in-open-source
question on sentiment analysis,"<p>I have a question regarding sentiment analysis that i need help with.</p>

<p>Right now, I have a bunch of tweets I've gathered through the twitter search api. Because I used my search terms, I know what are the subjects or entities (Person names) that I want to look at. I want to know how others feel about these people.</p>

<p>For starters, I downloaded a list of english words with known valence/sentiment score and calculate the sentiments (+/-) based on availability of these words in the tweet. The problem is that sentiments calculated this way - I'm actually looking more at the tone of the tweet rather than ABOUT the person.</p>

<p>For instance, I have this tweet:</p>

<blockquote>
<pre><code>""lol... Person A is a joke. lmao!""
</code></pre>
</blockquote>

<p>The message is obviously in a positive tone, but person A should get a negative. </p>

<p>To improve my sentiment analysis, I can probably take into account negation and modifiers from my word list. But how exactly can I get my sentiments analysis to look at the subject of the message (and possibly sarcasm) instead? </p>

<p>It would be great if someone can direct me towards some resources....</p>
","python, twitter, machine-learning, sentiment-analysis","<p>While awaiting for answers from researchers in AI field I will give you some clues on what you can do quickly. </p>

<p>Even though this topic requires knowledge from natural language processing, machine learning and even psychology, you don't have to start from scratch unless you're desperate or have no trust in the quality of research going on in the field.</p>

<p>One possible approach to sentiment analysis would be to treat it as a supervised learning problem, where you have some small training corpus that includes human made annotations  (later about that) and a testing corpus on which you test how well you approach/system is performing. For training you will need some classifiers, like SVM, HMM or some others, but keep it simple. I would start from binary classification: good, bad. You could do the same for a continuous spectrum of opinion ranges, from positive to negative, that is to get a ranking, like google, where the most valuable results come on top.</p>

<p>For a start check <a href=""http://www.csie.ntu.edu.tw/~cjlin/libsvm/"" rel=""nofollow"">libsvm classifier</a>, it is capable of doing both classification {good, bad} and regression (ranking).
The quality of annotations will have a massive influence on the results you get, but where to get it from?</p>

<p>I found <a href=""http://people.csail.mit.edu/bsnyder/naacl07/"" rel=""nofollow"">one project about sentiment analysis</a> that deals with restaurants. There is both data and code, so you can see how they extracted features from natural language and which features scored high in the classification or regression.
The corpus consists of opinions of customers about restaurants they recently visited and gave some feedback about the food, service or atmosphere. 
The connection about their opinions and numerical world is expressed in terms of numbers of stars they gave to the restaurant. You have natural language on one site and restaurant's rate on another.</p>

<p>Looking at this example you can devise your own approach for the problem stated. 
Take a look at <a href=""http://www.nltk.org/"" rel=""nofollow"">nltk</a> as well. With nltk you can do part of speech tagging and with some luck get names as well. Having done that you can add a feature to your classifier that will assign a score to a name if within n words (skip n-gram) there are words expressing opinions (look at the restaurant corpus) or use weights you already have, but it's best to rely on a classfier to learn weights, that's his job.</p>
",5,3,2102,2011-04-21 07:52:13,https://stackoverflow.com/questions/5741135/question-on-sentiment-analysis
"Sentiment analysis api/tool, for Java","<p>I'm writing a Java program and need to analyze small chunks of text (3-4 sentences, news articles paraphrased) for their sentiment. I just need to know whether the article is generally positive, negative or neutral.</p>

<p>For example, the following would ideally be classed as positive:</p>

<blockquote>
  <p>Kindle e-book sales soar for Amazon.
  Amazon.com says it is selling more
  e-books for its Kindle electronic
  reading device than paperback and
  hardback print editions combined</p>
</blockquote>

<p>All I need is a very simple and quick to implement third party solution, that I can use in my program. It does not have to be totally accurate all the time. Licenses etc. are not an issue, so long as it is possible to trail the solution.</p>

<p>So far I have found a potential good solution, <a href=""http://www.alchemyapi.com/api/sentiment/"" rel=""noreferrer"">AlchemyAPI</a>, but am struggling to actually use it.</p>

<p><br /></p>

<p>If anyone has encountered this problem before and knows of a particularly good/easy solution, or of a really good tutorial, I would be very grateful :-)</p>

<p><br /></p>

<p>(Also I apologize for the lack of code in this question.)</p>
","java, sentiment-analysis","<p>i just tested AlchemyAPI. it's not 100% accurate but i guess this sort of technology is still in its infancy.</p>

<p>you will need to register (free) to get an api key.</p>

<p>here's a sample usage: <code>http://access.alchemyapi.com/calls/text/TextGetTextSentiment?apikey=&lt;insert your api key&gt;&amp;sentiment=1&amp;showSourceText=1&amp;text=Kindle%20e-book%20sales%20soar%20for%20Amazon.%20Amazon.com%20says%20it%20is%20selling%20more%20e-books%20for%20its%20Kindle%20electronic%20reading%20device%20than%20paperback%20and%20hardback%20print%20editions%20combined</code></p>

<p>the inputs are:</p>

<ol>
<li>sentiment=1</li>
<li>showSourceText=1</li>
<li>text (i used your sample text, uri encoded)</li>
</ol>

<p>i got the following output (neutral sentiment, instead of the expected positive sentiment):</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;results&gt;    
    &lt;status&gt;OK&lt;/status&gt;    
    &lt;usage&gt;By accessing AlchemyAPI or using information generated by AlchemyAPI, you are agreeing to be bound by the AlchemyAPI Terms of Use: http://www.alchemyapi.com/company/terms.html&lt;/usage&gt;    
    &lt;url&gt;&lt;/url&gt;    
    &lt;language&gt;english&lt;/language&gt;    
    &lt;text&gt;Kindle e-book sales soar for Amazon. Amazon.com says it is selling more e-books for its Kindle electronic reading device than paperback and hardback print editions combined&lt;/text&gt;    
    &lt;docSentiment&gt;    
        &lt;type&gt;neutral&lt;/type&gt;    
    &lt;/docSentiment&gt;    
&lt;/results&gt;
</code></pre>

<p><br>
another sample usage: <code>http://access.alchemyapi.com/calls/text/TextGetTextSentiment?apikey=&lt;insert your api key&gt;&amp;sentiment=1&amp;showSourceText=1&amp;text=kindle%20is%20amazing</code></p>

<p>and the output:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;    
&lt;results&gt;    
    &lt;status&gt;OK&lt;/status&gt;    
    &lt;usage&gt;By accessing AlchemyAPI or using information generated by AlchemyAPI, you are agreeing to be bound by the AlchemyAPI Terms of Use: http://www.alchemyapi.com/company/terms.html&lt;/usage&gt;    
    &lt;url&gt;&lt;/url&gt;    
    &lt;language&gt;english&lt;/language&gt;    
    &lt;text&gt;kindle is amazing&lt;/text&gt;    
    &lt;docSentiment&gt;    
        &lt;type&gt;positive&lt;/type&gt;    
        &lt;score&gt;0.283568&lt;/score&gt;    
    &lt;/docSentiment&gt;    
&lt;/results&gt;
</code></pre>
",15,7,9128,2011-05-20 14:05:43,https://stackoverflow.com/questions/6073109/sentiment-analysis-api-tool-for-java
Looking up positivity or negativity in a sentence using WordNet,"<p>I have to ask some questions in an interactive system and user is free to type just anything as response. I have to conclude from response whether it is positive (yes) or negative (no). In some presentation that I attended few months ago I understood that a specific library of wordnet (not wordnet itself) exists that does the exact same job but I have forgot what it was. Does anyone know about it? If not can you suggest some alternative to attain the same result?</p>

<p>Thanks in advance</p>
","c#, nlp, wordnet, sentiment-analysis","<p>Nevermind I recalled it now. It was <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""nofollow"">SentiWordnet</a>.</p>
",3,0,1196,2011-05-25 13:54:20,https://stackoverflow.com/questions/6125584/looking-up-positivity-or-negativity-in-a-sentence-using-wordnet
Simple toolkits for emotion (sentiment) analysis (not using machine learning),"<p>I am looking for a tool that can analyze the emotion of short texts. I searched for a week and I couldn't find a good one that is publicly available. The ideal tool is one that takes a short text as input and guesses the emotion. It is preferably a standalone application or library.</p>

<p>I don't need tools that is trained by texts. And although similar questions are asked before no satisfactory answers are got.</p>

<p>I searched the Internet and read some papers but I can't find a good tool I want. Currently I found SentiStrength, but the accuracy is not good. I am using emotional dictionaries right now. I felt that some syntax parsing may be necessary but it's too complex for me to build one. Furthermore, it's researched by some people and I don't want to reinvent the wheels. Does anyone know such publicly/research available software? I need a tool that doesn't need training before using.
Thanks in advance.</p>
","nlp, sentiment-analysis","<p>Maybe <a href=""http://dtminredis.housing.salle.url.edu:8080/EmoLib/"" rel=""nofollow"">EmoLib</a> could be of help.</p>
",-1,4,1913,2011-06-27 18:18:06,https://stackoverflow.com/questions/6497152/simple-toolkits-for-emotion-sentiment-analysis-not-using-machine-learning
How do I make an API call to viralheat on submit and then parse and save the JSON response?,"<p>I want to send a request via Viralheat's API in my controller's update method so that when a user hits the submit button, an action is completed and the API call is made. I want to post to http://www.viralheat.com/api/sentiment/review.json?text=i&amp;do&amp;not&amp;like&amp;this&amp;api_key=[<strong>* your api key *</strong>]</p>

<p>This will return some JSON in the format:</p>

<pre><code>{""mood"":""negative"",""prob"":0.773171679917001,""text"":""i do not like this""}
</code></pre>

<p>Is it possible to make that API call simultaneously while executing the controller method and how would I handle the JSON response? Which controller method would I put it in?</p>

<p>Ultimately I'd like to save the response mood to my sentiment column in a BrandUsers table. Submit is in main.html.erb which then uses the update method.</p>

<p><strong>Controller</strong></p>

<pre><code>def update
  @brand = Brand.find(params[:id])
  current_user.tag(@brand, :with =&gt; params[:brand][:tag_list], :on =&gt; :tags)
  if @brand.update_attributes(params[:brand])
    redirect_to :root, :notice  =&gt; ""Brand tagged.""
  else
    render :action =&gt; 'edit'
  end
end

def main
  @brands = Brand.all
  if current_user
    @brand = current_user.brands.not_tagged_by_user(current_user).order(""RANDOM()"").first
end
</code></pre>
","ruby-on-rails, ruby-on-rails-3, json, api, sentiment-analysis","<p>With the <code>wrest</code> gem installed, you could do something like</p>

<pre><code>params[:api_key] = ""your key""

url = ""http://www.viralheat.com/api/sentiment/review.json""

response = url.to_uri.get(params).deserialize
</code></pre>

<p><code>response</code> would contain the json already turned into a hash. So you can access the mood with</p>

<pre><code>response[:mood]
</code></pre>
",2,1,657,2011-08-31 12:06:12,https://stackoverflow.com/questions/7256686/how-do-i-make-an-api-call-to-viralheat-on-submit-and-then-parse-and-save-the-jso
Sentiment Analysis with ruby,"<p>Any one with sentient analysis experience with <code>liblinear</code> algorithm. Any one have used <a href=""https://github.com/tomz/liblinear-ruby-swig"" rel=""nofollow"">liblinear-ruby-swig</a> gem?</p>

<p>Please suggest me something to start with.</p>
","ruby-on-rails, ruby, sentiment-analysis","<p>I used lib linear a lot for other classification not for sentiment analysis 
Are you interested in using lib linear or to do sentiment analysis? 
For simple sentiment analysis look at
<a href=""https://chrismaclellan.com/blog/sentiment-analysis-of-tweets-using-ruby"" rel=""nofollow noreferrer"">https://chrismaclellan.com/blog/sentiment-analysis-of-tweets-using-ruby</a></p>
",4,3,3793,2011-09-13 10:26:30,https://stackoverflow.com/questions/7400333/sentiment-analysis-with-ruby
Training data for sentiment analysis,"<p>Where can I get a corpus of documents that have already been classified as positive/negative for sentiment in the corporate domain? I want a large corpus of documents that provide reviews for companies, like reviews of companies provided by analysts and media.</p>

<p>I find corpora that have reviews of products and movies. Is there a corpus for the business domain including reviews of companies, that match the language of business?</p>
","nlp, machine-learning, text-analysis, sentiment-analysis, training-data","<p><a href=""http://www.cs.cornell.edu/home/llee/data/"" rel=""noreferrer"">http://www.cs.cornell.edu/home/llee/data/</a></p>

<p><a href=""http://mpqa.cs.pitt.edu/corpora/mpqa_corpus"" rel=""noreferrer"">http://mpqa.cs.pitt.edu/corpora/mpqa_corpus</a></p>

<p>You can use twitter, with its smileys, like this: <a href=""http://web.archive.org/web/20111119181304/http://deepthoughtinc.com/wp-content/uploads/2011/01/Twitter-as-a-Corpus-for-Sentiment-Analysis-and-Opinion-Mining.pdf"" rel=""noreferrer"">http://web.archive.org/web/20111119181304/http://deepthoughtinc.com/wp-content/uploads/2011/01/Twitter-as-a-Corpus-for-Sentiment-Analysis-and-Opinion-Mining.pdf</a></p>

<p>Hope that gets you started.  There's more in the literature, if you're interested in specific subtasks like negation, sentiment scope, etc.</p>

<p>To get a focus on companies, you might pair a method with topic detection, or cheaply just a lot of mentions of a given company.  Or you could get your data annotated by Mechanical Turkers.</p>
",37,57,42907,2011-09-26 06:18:54,https://stackoverflow.com/questions/7551262/training-data-for-sentiment-analysis
Sentimental analysis using apache mahout,"<p>I am planning to develop a system that would predict the mood of a given text(sentiment analysis in short).</p>

<p>I would also prefer apache mahout because, it is seriously huge data and the system should be scalable realtime. Kindly suggest me algorithms that apache mahout provides, which will be suitable for sentiment analysis.</p>
","machine-learning, classification, mahout, sentiment-analysis","<p>If you have labeled training data then you could try <a href=""http://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""nofollow"">Naive Bayes classifier</a> which is one of the simplest supervised learning algorithms out there (and is supported by Mahout). If that is not sufficient for some reason then you could try more involved algorithms such as logistic regression etc.</p>

<p>If you don't have labeled data then you are out of luck - you will need to get some for this to work (e.g. by hiring someone to label your data for you via <a href=""https://www.mturk.com/mturk/welcome"" rel=""nofollow"">Amazon's Mechanical Turk</a>)</p>

<p>By the way, what size of the data are we talking about? (if it is is up to a few hundred of gigabytes then you don't need hadoop/mahout to train this type of models - unless you have that data sitting in hadoop already of course..)</p>
",3,3,2780,2011-12-09 13:12:06,https://stackoverflow.com/questions/8445956/sentimental-analysis-using-apache-mahout
Customer support data sets for e-mail sentiment analysis,"<p>I am looking for an annotated data set in the customer support domain for a sentiment analysis, to train my Naive Bayes Classifier. Are there any such data sets available on the internet? I am unable to find any so far.</p>

<p>How do I go about this.</p>
","machine-learning, sentiment-analysis","<p>With this being a 6-month old question, this response is probably only helpful to people who stumble upon this question.  You may want to consider using the data sets from the following page:</p>

<p><a href=""http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"" rel=""noreferrer"">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a></p>

<p>The polarity 2.0 data set from the following page is also frequently used:</p>

<p><a href=""http://www.cs.cornell.edu/people/pabo/movie-review-data/"" rel=""noreferrer"">http://www.cs.cornell.edu/people/pabo/movie-review-data/</a></p>
",5,3,3483,2011-12-13 13:21:15,https://stackoverflow.com/questions/8489956/customer-support-data-sets-for-e-mail-sentiment-analysis
Weighted Naive Bayes Classifier in Apache Mahout,"<p>I am using Naive Bayes classifier for my sentiment analysis on customer support. But unfortunately I don't have huge annotated data sets in the customer support domain. But I have a little amount of annotated data in the same domain(around 100 positive and 100 negative). I have the amazon product review data set as well.</p>

<p>Is there anyway can I implement a weighted naive bayes classifier using mahout, so that I can give more weight to the small set of customer support data and small weight to the amazon product review data. A training on the above weighted data set would drastically improve accuracy I guess. Kindly help me with the same.</p>
","machine-learning, sentiment-analysis, mahout, naivebayes","<p>One really simple approach is oversampling. Ie just repeat the customer support examples in your training data multiple times.</p>

<p>Though it's not the same problem you might get some further ideas by looking into the approaches used for class imbalance; in particular oversampling (as mentioned) and undersampling.</p>
",1,2,1016,2011-12-20 13:33:01,https://stackoverflow.com/questions/8576267/weighted-naive-bayes-classifier-in-apache-mahout
How I can start building wordnet for Turkish language to use in sentiment analysis,"<p>Although I hold EE background, I didn't get chance to attend Natural Language processing classes. </p>

<p>I would like to build sentiment analysis tool for Turkish language. I think it is best to create a Turkish wordnet database rather than translating the text to English and analyze it with buggy translated text with provided tools. (<strong>is it?</strong>)</p>

<p>So what do you guys recommend me to do ? First of all taking NLP classes from an open class website? I really don't know where to start. Could you help me and maybe provide me step by  step guide? I know this is an academic project but I am interested to build skills as a hobby in that area.</p>

<p>Thanks in advance.</p>
","wordnet, sentiment-analysis","<p>Here is the process I have used before (making Japanese, Chinese, German and Arabic semantic networks):</p>

<ol>
<li>Gather at least two English/Turkish dictionaries. They must be independent, not derived from each other. You can use Wikipedia to auto-generate one of your dictionaries. If you need to publish your network, then you may need open source dictionaries, or license fees, or a lawyer.</li>
<li>Use those dictionaries to translate English Wordnet, producing a confidence rating for each synset.</li>
<li>Keep those with strong confidence, manually approving or fixing through those with medium or low confidence.</li>
<li>Finish it off manually</li>
</ol>

<p>I expanded on this in the ""Automatic Translation Of WordNet"" section of my 2008 paper: <a href=""http://dcook.org/mlsn/about/papers/nlp2008.MLSN_A_Multilingual_Semantic_Network.pdf"" rel=""noreferrer"">http://dcook.org/mlsn/about/papers/nlp2008.MLSN_A_Multilingual_Semantic_Network.pdf</a></p>

<p>(For your stated goal of a Turkish sentiment dictionary, there are other approaches, not involving a semantic network. E.g. ""Semantic Analysis and Opinion Mining"", by Bing Liu, is a good round-up of research. But a semantic network approach will, IMHO, always give better results in the long run, and has so many other uses.)</p>
",6,8,2297,2011-12-27 05:33:34,https://stackoverflow.com/questions/8641503/how-i-can-start-building-wordnet-for-turkish-language-to-use-in-sentiment-analys
Naive Bayes Identifying Neutrals PHP,"<p>I am using Ian Barbers Naive Bayes analysis class to analyze the sentiment of sentences for a school project. I have created my own datasets of positive neutral and negatives. My problem is I have no clue on how to implement the neutrals and get the class to find them. The link below is for the php class I am using</p>

<p><a href=""http://phpir.com/bayesian-opinion-mining"" rel=""nofollow"">http://phpir.com/bayesian-opinion-mining</a></p>
","php, sentiment-analysis","<p>Well the <code>Opinion</code> class is already pretty flexible for adding new ""sentiment classes"". Just the <code>classify</code> method has implemented the calculation of ""prior"" static. But it can be easily replaced with a <code>foreach</code>:</p>

<pre><code>private $classes = array('pos', 'neg', 'neutr');
private $classTokCounts = array('pos' =&gt; 0, 'neg' =&gt; 0, 'neutr' =&gt; 0);
private $classDocCounts = array('pos' =&gt; 0, 'neg' =&gt; 0, 'neutr' =&gt; 0);
private $prior = array('pos' =&gt; 1.0/3.0, 'neg' =&gt; 1.0/3.0, 'neutr' =&gt; 1.0/3.0);

public function classify($document) {
    // remove those:
    //$this-&gt;prior['pos'] = $this-&gt;classDocCounts['pos'] / $this-&gt;docCount;
    //$this-&gt;prior['neg'] = $this-&gt;classDocCounts['neg'] / $this-&gt;docCount;
    // add this:
    foreach($this-&gt;classes as $class) {
        $this-&gt;prior[$class] = $this-&gt;classDocCounts[$class] / $this-&gt;docCount;
    }

    // the rest is fine
</code></pre>
",2,0,1160,2011-12-30 05:51:24,https://stackoverflow.com/questions/8676589/naive-bayes-identifying-neutrals-php
Training data size for a bayesian classifier,"<p>I am using apache mahout for performing sentiment analysis in the customer support domain. Since I am not able to get a proper training data set, I made my own. Now I have 100 support mails for positive sentiment and 100 for negative.</p>

<p>But the problem is, I am not able to achieve accuracy. It stays somewhere around 55%, which is pathetic. Some 70% and around accuracy will be satisfactory. And also note that I am using a complimentary naive bayes classifier of apache mahout.</p>

<p>Coming to the question precisely, is it the smaller data set size that is bringing down the accuracy? If not, where should I tweak?</p>
","mahout, bayesian, sentiment-analysis","<p>Only for the benefit of those looking into this question in future, I will share the ways in which I tweaked the accuracy of my classifier from 50 to around 78%</p>

<ul>
<li>Perform stemming on training and input data</li>
<li>Perform stop word removal on training and input data</li>
<li>Convert training and input data to lower case (or uppercase)</li>
<li>Have near equal amount of samples in each category of the training data</li>
<li>Fine tune the ngram level according to your domain.</li>
</ul>

<p>This should dramatically raise your accuracy.</p>
",3,1,1253,2012-01-25 03:50:51,https://stackoverflow.com/questions/8997597/training-data-size-for-a-bayesian-classifier
bag of words algorithm in php,"<p>Im making my final project in my studies.</p>

<p>and I'm trying to create sentiment analysis of Twitter messages.</p>

<p>I'm using Bayesian algorithm, and bag of words.</p>

<p>Do you have an example of bag of words algorithm in PHP?</p>

<p>I can't find anything, maybe list of positive and negative words or something</p>
","php, algorithm, sentiment-analysis","<p>I haven't implemented Bag of Words in PHP but I've done it in java. A simple way to implement it would be by taking the training data and tokenizing it (example Stanford Tokenizer). Once you have tokenized all your training data, you can then extract 1-grams from it. I use this <a href=""http://homepages.inf.ed.ac.uk/lzhang10/ngram.html"" rel=""nofollow"">http://homepages.inf.ed.ac.uk/lzhang10/ngram.html</a> to extract the grams and then remove the count of words from the output and just use the words. This becomes your Bag of Words corpus which can be used during training and classification. Make sure, you use the same tokenizer during training and testing or classification and also use the same corpus while training the models.</p>

<p>Now implementing it is pretty easy, just take a string of data and tokenize it using the same tokenizer used to create the bag of words corpus. Now take each token and then find whether that token is available in your corpus and at what position. For example, you have a corpus which has words as follows :-</p>

<p>a</p>

<p>name</p>

<p>the</p>

<p>hello</p>

<p>world</p>

<p>,</p>

<p>And you have a string ""hello, my name is Jas"". Tokenizing it would give the following tokens {hello,,,my,name,is,Jas} and when you try to match these tokens with the corpus your result would be :-</p>

<p>2:1 4:1 6:1</p>

<p>This means, the words name, hello and comma which are present in the location 2, 4 and 6 in your corpus are present in the incoming test string.</p>
",0,0,2298,2012-01-28 13:51:33,https://stackoverflow.com/questions/9045741/bag-of-words-algorithm-in-php
Automatic negation of words,"<p>Consider the following statements</p>

<pre><code>We are not talking about a well established company in the NASDAQ
I will not initiate any trades until those clowns hammer out a deal
</code></pre>

<p>I am writing a simple Naive Bayes classifier, basically marking a training set of statements by hand (as either positive or negative sentiment) and storing the words that make up the statement accordingly.</p>

<p>Problem: if I mark both of these statements as having a negative sentiment, the words ""well"", ""established"" (statement 1) and ""any"", ""until"" (statement 2) would be indivudually marked as negatives. Whereas in another case (i.e., ""This company is performing well""), the same words (""well"" in this case) would be marked as a positive, making the sum of sentiment for ""well"" -1 + 1 = 0. I would overcome this by tagging these words as negated words, for example:</p>

<pre><code>We are talking about a not-well not-established company in the NASDAY.
I will initiate not-anymore trades not-until those clowns hammer out a deal
</code></pre>

<p>Is there a standard or best way of tagging these kinds of words (I don't even know if they are of a same group of words)? Obviously, tagging ""company"" wouldn't make sense ""not-company"" doesn't hold any sentimental value. I have (in PHP) made a function that would tag all words after the negation word (not, no, couldn't, etc) but many of them didn't make real sense afterwards (such as ""not-company"", ""not-NASDAQ"", ""not-clowns"").</p>

<p>Since English is not my mother language, I'm asking you if there's a common name for the words I have marked here and if what I want is (rudimentary) possible. I am aware that there are a lot of exceptions possible (double negations etc.) but I do not want to go into that; I believe if this would be possible, it would cover a lot of ground.</p>
","php, nlp, sentiment-analysis","<p>Taking from your example,</p>

<pre><code>We are talking about a not-well not-established company in the NASDAY.
I will initiate not-anymore trades not-until those clowns hammer out a deal
</code></pre>

<p>I think you want to tag <strong>adjectives</strong> (and their variants) so they would be negated, right? It is called ""part of speech tagging"". There is a good tutorial with PHP <a href=""http://phpir.com/part-of-speech-tagging"" rel=""nofollow"">here</a>.</p>

<p>You need a dictionary (or list of word) of common English adjectives, however.</p>
",4,2,255,2012-02-11 22:43:50,https://stackoverflow.com/questions/9244692/automatic-negation-of-words
Logical fallacy detection and/or identification with natural-language-processing,"<p>Is there a package or methodology in existence for the detection of flawed logical arguments in text? </p>

<p>I was hoping for something that would work for text that is not written in an academic setting (such as a logic class). It might be a stretch but I would like something that can identify where logic is trying to be used and identify the logical error. A possible use for this would be marking errors in editorial articles.</p>

<p>I don't need anything that is polished. I wouldn't mind working to develop something either so I'm really looking for what's out there in the wild now.</p>
","nlp, logic, machine-learning, sentiment-analysis","<p>That's a difficult problem, because you'll have to map natural language to some logical representation, and deal with ambiguity in the process.</p>

<p><a href=""http://attempto.ifi.uzh.ch/site/"">Attempto Project</a> may be interesting for you. It has several <a href=""http://attempto.ifi.uzh.ch/site/tools/"">tools</a> that you can try online. In particular, <a href=""http://attempto.ifi.uzh.ch/race/"">RACE</a> may be doing something you wanted to do. It checks for consistency on the given assertions. But the bigger issue here is in transforming them to logical forms.</p>
",9,15,3827,2012-04-06 16:39:57,https://stackoverflow.com/questions/10046407/logical-fallacy-detection-and-or-identification-with-natural-language-processing
Using Sentiwordnet 3.0,"<p>I plan on using Sentiwordnet 3.0 for Sentiment classification. Could someone clarify as to what the numbers associated with words in Sentiwordnet represent? For e.g. what does 5 in rank#5 mean? Also for POS what is the letter used to represent adverbs? Im assuming 'a' is adjectives. I could not find an explanation either on their site or on other sites.</p>
","machine-learning, nlp, wordnet, sentiment-analysis, senti-wordnet","<p>I found the answer. Seems like the number notation comes form Wordnet. It represents the rank in which the given word is commonly used. So rank#5 refers to the context in which rank is used 5th most commonly. Similarly rank#1 refers to the meaning of rank most commonly used. The following are the POS notations: </p>

<p>n  -  NOUN <br/>
v  -  VERB <br/>
a   - ADJECTIVE <br/>
s    - ADJECTIVE SATELLITE <br/>
r    - ADVERB <br/></p>
",13,6,6936,2012-04-19 07:17:34,https://stackoverflow.com/questions/10223314/using-sentiwordnet-3-0
Javaee mbean vs singleton,"<p>I have created a javaee application that among other things must perform sentiment analysis using naive bayes,. In order for the sentiment algorithm to work, we have to first train it,so I would like to create an object that would handle the trainning whenever the server is started to avoid training over and over again. I thought of using a singleton ejb to do this but I don't know if this is the correct way to go, also a friend suggested the use of managed beans. What are the pros and cons of those approaches for my problem? Am I looking in the correct direction or I am just barking in the wrong tree?</p>
","java, jakarta-ee, sentiment-analysis","<p>MBeans are great for modifying the state of your application at runtime. If you want to alter the training of the algorithm at runtime and use MBeans, it could make sense to use them to initialize when you start up as well.</p>

<p>Also, I'd recommend in the design of the class containing your algorithm that you externalize the coefficients that you are calculating during training. You can then persist those coefficients and not have to re-run the training. On start your app would load the coefficients from persistence.</p>

<p>Combining loadable coefficients with MBeans, you could use the latter to retrieve or reload the set of coefficients at runtime. You would want to ensure reloading them is atomic. This would enable you to arbitrarily tune your analysis on the fly.</p>
",1,0,756,2012-04-19 07:51:32,https://stackoverflow.com/questions/10223786/javaee-mbean-vs-singleton
Sentiment analysis using R,"<p>Are there any R packages that focus on sentiment analysis? I have a small survey where users can write a comment about their experience of using a web-tool. I ask for a numerical ranking, and there is the option of including a comment. </p>

<p>I am wondering what the best way of assessing the positiveness or negativeness of the comment is. I would like to be able to compare it to the numerical ranking that the user provides, using R.</p>
","r, sentiment-analysis","<p>And there is <a href=""http://cran.r-project.org/web/packages/sentiment/index.html"" rel=""noreferrer"">this package</a>:</p>

<p><code>sentiment: Tools for Sentiment Analysis</code></p>

<p>sentiment is an R package with tools for sentiment analysis including bayesian classifiers for positivity/negativity and emotion classification.</p>

<p>Update 14 Dec 2012: it has been removed  to the <a href=""http://cran.r-project.org/src/contrib/Archive/sentiment/"" rel=""noreferrer"">archive</a>...</p>

<p>Update 15 Mar 2013: the <a href=""http://cran.us.r-project.org/web/packages/qdap/index.html"" rel=""noreferrer"">qdap</a> package has a <code>polarity</code> function, based on Jeffery Breen's work</p>
",26,28,43652,2012-04-19 17:01:05,https://stackoverflow.com/questions/10233087/sentiment-analysis-using-r
How to tackle twitter sentiment analysis?,"<p>I'd like you  to give me some advice in order to tackle this problem. At college I've been solving opinion mining tasks but with Twitter the approach is quite different. For example, I used an ensemble learning approach to classify users opinions about a certain Hotel in Spain. Of course, I was given a training set with positive and negative opinions and then I tested with the test set. But now, with twitter, I've found this kind of categorization very difficult. </p>

<ol>
<li><p>Do I need to have a training set? and if the answer to this question is positive, don't you think twitter is so temporal so if I have that set, my performance on future topics will be very poor?</p></li>
<li><p>I was thinking in getting a dictionary  (mainly adjectives) and cross my tweets with it and obtain a term-document matrix but I have no class assigned to any twitter. Also, positive adjectives and negative adjectives could vary depending on the topic and time. So, how to deal with this?</p></li>
<li><p>How to deal with the problem of languages? For instance, I'd like to study tweets written in English and those in Spanish, but separately.</p></li>
<li><p>Which programming languages do you suggest to do something like this? I've been trying with R packages like tm, twitteR.</p></li>
</ol>
",sentiment-analysis,"<ol>
<li>Sure, I think the way sentiment is used will stay constant for a few months. worst case you relabel and retrain. Unsupervised learning has a shitty track record for industrial applications in my experience.</li>
<li>You'll need some emotion/adj dictionary for sentiment stuff- there are some datasets out there but I forget where they are. I may have answered previous questions with better info.</li>
<li>Just do English tweets, it's fairly easy to build a language classifier, but you want to start small, so take it easy on yourself</li>
<li>Python (NLTK) if you want to do it easily in a small amount of code. Java has good NLP stuff, but Python and it's libraries are way more user friendly</li>
</ol>
",1,2,1589,2012-05-02 14:59:05,https://stackoverflow.com/questions/10416343/how-to-tackle-twitter-sentiment-analysis
NLP Library (Subject Extraction+Sentiment Analysis) for a Java-based Web Application,"<p>I'm a college student looking for a NLP library to perform subject extraction and sentiment analysis in a Java-based web application for a summer-hobby project.</p>

<p>To give you a little context on what I'm trying to do... I want to build a Java-based web application that will extract subjects out of a Reddit submission's headlines, as well as identify the OP's sentiment for the headline (when possible). </p>

<p>Example Inputs:</p>

<ul>
<li>Reddit, we took the anti-SOPA petition from 943,702 signatures to
3,460,313. The anti-CISPA petition is at 691,768, a bill expansively
worse than SOPA. Please bump it, then let us discuss further measures
or our past efforts are in vain. We did it before, I'm afraid we are
called on to do it again.</li>
<li>My friend calls him ""Mr Ridiculously Photogenic Guy""</li>
<li>Insanity: CISPA Just Got Way Worse, And Then Passed On Rushed Vote</li>
</ul>

<p>I'm currently trying out AlchemyAPI, but it sounds like better NLP libraries exist out there. Preferablly, I wouldn't be restricted to a limited number of API requests in a given time period (AlchemyAPI has a quota). I've heard the names of GATE, LingPipe, and OpenNLP - however, I'm unsure whether they fit my needs.</p>

<p>I'm looking for framework/library/api recommendations, or even better, comparisons from experienced users. My experience with NLP is extremely limited, which is why I'm asking for help here (ps: if anyone has any resources for learning more, outside of www.nlp-class.org, please let me know!) :)</p>
","java, nlp, sentiment-analysis","<p>First, I'd highly recommend using python, as the NLP libraries are a bit more user friendly than java, and it'd be a lot less code to maintain for a one-man project.</p>

<p>I can't think of anything off the top of my head to do either classification, so my recommendation would be to train two classifiers, one for subject, and one for sentiment. You'll have to label data and define features, but I think that wouldn't be too hard, especially with sentiment where you build up a dictionary of 'emotion' words. Labeling data is a pain in the ass, but that and good features are how you get good classification.</p>

<p><strong>Subject Classifier:</strong></p>

<p>Use NLTK with a Naive Bayes classifier, and define features as the word (lowercased), and word bigrams and trigrams.</p>

<p><strong>Sentiment Classifier:</strong></p>

<p>Same features as subject classifier, but also have a feature that says word w is in emotion dictionary with connection c. So, word 'bad' means 'bad sentiment'.</p>

<p>Once you've amassed sufficient training/testing data, you train your classifiers and optimize features, if necessary, and then you can run the classifiers against whatever other data you want.</p>

<p>General Purpose Libraries (Java):</p>

<ul>
<li>OpenNLP </li>
<li>LingPipe</li>
<li>Weka</li>
<li>Stanford stuff</li>
</ul>

<p>Libraries (Python):</p>

<ul>
<li>NLTK</li>
<li>Scipy</li>
</ul>
",4,4,2252,2012-05-02 18:19:21,https://stackoverflow.com/questions/10419437/nlp-library-subject-extractionsentiment-analysis-for-a-java-based-web-applica
What are the existent Sentiment Analysis Algorithm?,"<p>I and a group of people are developing a Sentiment Analysis Algorithm. I would like to know what are the existent ones, because I want to compare them. Is there any article that have the main algorithms in this area? </p>

<p>Thanks in advance</p>

<p>Thiago</p>
",sentiment-analysis,"<p>Some of the papers on sentiment analysis may help you - </p>

<ol>
<li>One of the earlier works by Bo Pang, Lillian Lee <a href=""http://acl.ldc.upenn.edu/acl2002/EMNLP/pdfs/EMNLP219.pdf"">http://acl.ldc.upenn.edu/acl2002/EMNLP/pdfs/EMNLP219.pdf</a> </li>
<li>A comprehensive survey of sentiment analysis techniques <a href=""http://www.cse.iitb.ac.in/~pb/cs626-449-2009/prev-years-other-things-nlp/sentiment-analysis-opinion-mining-pang-lee-omsa-published.pdf"">http://www.cse.iitb.ac.in/~pb/cs626-449-2009/prev-years-other-things-nlp/sentiment-analysis-opinion-mining-pang-lee-omsa-published.pdf</a> </li>
<li>Study by Hang Cui, V Mittal, M Datar using 6-grams <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.5942&amp;rep=rep1&amp;type=pdf"">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.5942&amp;rep=rep1&amp;type=pdf</a></li>
</ol>

<p>For quick implementation naive bayes is recommended. You can find an example here <a href=""http://nlp.stanford.edu/IR-book/"">http://nlp.stanford.edu/IR-book/</a></p>

<p>We did a statistical comparision of various classifiers and found SVM to be most accurate, though for a dataset consisting of large contents 
( <a href=""http://ai.stanford.edu/~amaas/data/sentiment/"">http://ai.stanford.edu/~amaas/data/sentiment/</a> ) none of the methods worked well.Our study may not be accurate though. Also instead of treating sentiment analysis as a text classification problem, you can look at extraction of meaning from text, though I do not know how successful it might be.</p>
",11,7,8326,2012-05-21 20:49:55,https://stackoverflow.com/questions/10692428/what-are-the-existent-sentiment-analysis-algorithm
training libsvm for text classification(sentiment),"<p>From following links I came with some idea. I want to ask whether I am doing it right or I am in the wrong way. If I am in the wrong way, please guide me.</p>

<p>Links<br/>
<a href=""https://stackoverflow.com/questions/9021658/using-libsvm-for-text-classification-c-sharp"">Using libsvm for text classification c#</a><br/>
<a href=""https://stackoverflow.com/questions/6172159/how-to-use-libsvm-for-text-classification"">How to use libsvm for text classification?</a></p>

<p>My way</p>

<p>First calculate the word count in each training set<br>
Create a maping list for each word</p>

<p>eg</p>

<pre><code>sample word count form training set
|-----|-----------|
|     |   counts  |
|-----|-----|-----|
|text | +ve | -ve |
|-----|-----|-----|
|this | 3   | 3   |
|forum| 1   | 0   |
|is   | 10  | 12  |
|good | 10  | 5   |
|-----|-----|-----|
</code></pre>

<p>positive training data</p>

<pre><code>this forum is good
</code></pre>

<p>so will the training set be</p>

<pre><code>+1 1:3 2:1 3:10 4:10
</code></pre>

<p>this all is just what I received from above links.<br>
Please help me.</p>
","svm, libsvm, sentiment-analysis","<p>You're doing it right.</p>

<p>I don't know why your laben is called ""+1"" - should be a simple integer (refering to the document ""+ve""), but all in all it's the way to go. </p>

<p>For document classification you may want to take a look at liblinear which is specially designed for handling a lot of features.</p>
",4,3,2905,2012-05-24 09:29:32,https://stackoverflow.com/questions/10734728/training-libsvm-for-text-classificationsentiment
Sentiment Analysis on Twitter Data?,"<p>I am working on this project where I wish to classify the general mood of a Twitter user from his recent tweets. Since the tweets can belong to a huge variety of domains how should I go about it ?</p>

<p>I could use the Naive Bayes algorithm (like here: <a href=""http://phpir.com/bayesian-opinion-mining"" rel=""nofollow"">http://phpir.com/bayesian-opinion-mining</a>) but since the tweets can belong to a large variety of domains, I am not sure if this will be very accurate.</p>

<p>The other option is using maybe sentiment dictionaries like <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""nofollow"">SentiWordNet</a> or <a href=""http://alexdavies.net/2011/10/word-lists-for-sentiment-analysis-of-twitter/"" rel=""nofollow"">here</a>. Would this be a better approach, I don't know.</p>

<p>Also where can I get data to train my classifier if I plan to use the Naive Bayes or some other algorithm ?</p>

<p>Just to add here, I am primarily coding in PHP.</p>
","twitter, dataset, sentiment-analysis","<p>It appears you could use <code>SentiWordNet</code> as the <em>classifier</em> data if you are focused on a word-by-word approach.  It is how simple <code>Bayesian spam filters</code> works; it focuses on each word.</p>

<p>The advantage here is that while many of the words in <code>SentiWordNet</code> have multiple meanings, each with different <code>positive/objective/negative</code> scores, you could experiment with using the scores of the other words in the tweet to narrow in on the most appropriate meaning for each multi-meaning word, which could give you a more accurate score for each word and for the overall tweet.</p>
",2,1,2300,2012-06-21 19:09:01,https://stackoverflow.com/questions/11145094/sentiment-analysis-on-twitter-data
Is there a set of adjective word list for positive or negative polarity,"<p>I am working on sentiment analysis. I thought if there is any available set of adjectives indicating positive/negative(like for positive: good,awesome,amazing,) meaning? and the second thing is a set of data from which i can use as a test case.</p>
","nlp, stanford-nlp, sentiment-analysis","<p>Resources related to polarity:</p>

<p>SentiWordNet: <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""noreferrer"">http://sentiwordnet.isti.cnr.it/</a></p>

<p>Inquirer Dictionary: <a href=""http://www.wjh.harvard.edu/~inquirer/homecat.htm"" rel=""noreferrer"">http://www.wjh.harvard.edu/~inquirer/homecat.htm</a></p>

<p>Possible test data: <a href=""http://www.cs.pitt.edu/mpqa/"" rel=""noreferrer"">http://www.cs.pitt.edu/mpqa/</a></p>
",13,5,11215,2012-08-03 16:49:11,https://stackoverflow.com/questions/11799971/is-there-a-set-of-adjective-word-list-for-positive-or-negative-polarity
List of Natural Language Processing Tools in Regards to Sentiment Analysis - Which one do you recommend,"<p>first up sorry for my not so perfect English... I am from Germany ;) </p>

<p>So, for a research project of mine (Bachelor thesis) I need to analyze the sentiment of tweets about certain companies and brands. For this purpose I will need to script my own program / use some sort of modified open source code (no APIs' - I need to understand what is happening). </p>

<p>Below you will find a list of some of the NLP Applications I found. My Question now is which one and which approach would you recommend? And which one does not require long nights adjusting the code?</p>

<p>For example: When I screen twitter for the music player >iPod&lt; and someone writes: ""It's a terrible day but at least my iPod makes me happy"" or even harder: ""It's a terrible day but at least my iPod makes up for it"" </p>

<p>Which software is smart enough to understand that the focused is on iPod and not the weather? </p>

<p>Also which software is scalable / resource efficient (I want to analyze several tweets and don't want to spend thousands of dollars)? </p>

<p><strong>Machine learning and data mining</strong></p>

<p><em>Weka</em> - is a collection of machine learning algorithms for data mining. It is one of the most popular text classification frameworks. It contains implementations of a wide variety of algorithms including Naive Bayes and Support Vector Machines (SVM, listed under SMO) [Note: Other commonly used non-Java SVM implementations are SVM-Light, LibSVM, and SVMTorch]. A related project is Kea (Keyphrase Extraction Algorithm) an algorithm for extracting keyphrases from text documents.</p>

<p><em>Apache Lucene Mahout</em> - An incubator project to created highly scalable distributed implementations of common machine learning algorithms on top of the Hadoop map-reduce framework.</p>

<p><strong>NLP Tools</strong></p>

<p><em>LingPipe</em> - (not technically 'open-source, see below) Alias-I's Lingpipe is a suite of java tools for linguistic processing of text including entity extraction, speech tagging (pos) , clustering, classification, etc... It is one of the most mature and widely used open source NLP toolkits in industry. It is known for it's speed, stability, and scalability. One of its best features is the extensive collection of well-written tutorials to help you get started. They have a list of links to competition, both academic and industrial tools. Be sure to check out their blog. LingPipe is released under a royalty-free commercial license that includes the source code, but it's not technically 'open-source'.</p>

<p><em>OpenNLP</em> - hosts a variety of java-based NLP tools which perform sentence detection, tokenization, part-of-speech tagging, chunking and parsing, named-entity detection, and co-reference analysis using the Maxent machine learning package.</p>

<p><em>Stanford Parser and Part-of-Speech (POS) Tagger</em> - Java packages for sentence parsing and part of speech tagging from the Stanford NLP group. It has implementations of probabilistic natural language parsers, both highly optimized PCFG and lexicalized dependency parsers, and a lexicalized PCFG parser. It's has a full GNU GPL license.</p>

<p><em>OpenFST</em> - A package for manipulating weighted finite state automata. These are often used to represented a probablistic model. They are used to model text for speech recognition, OCR error correction, machine translation, and a variety of other tasks. The library was developed by contributors from Google Research and NYU. It is a C++ library that is meant to be fast and scalable.</p>

<p><em>NTLK</em> - The natural language toolkit is a tool for teaching and researching classification, clustering, speech tagging and parsing, and more. It contains a set of tutorials and data sets for experimentation. It is written by Steven Bird, from the University of Melbourne.</p>

<p><em>Opinion Finder</em> - A system that performs subjectivity analysis, automatically identifying when opinions, sentiments, speculations and other private states are present in text. Specifically, OpinionFinder aims to identify subjective sentences and to mark various aspects of the subjectivity in these sentences, including the source (holder) of the subjectivity and words that are included in phrases expressing positive or negative sentiments.</p>

<p><em>Tawlk/osae</em> - A python library for sentiment classification on social text. The end-goal is to have a simple library that ""just works"". It should have an easy barrier to entry and be thoroughly documented. We have acheived best accuracy using stopwords filtering with tweets collected on negwords.txt and poswords.txt</p>

<p><em>GATE</em> - GATE is over 15 years old and is in active use for all types of computational task involving human language. GATE excels at text analysis of all shapes and sizes. From large corporations to small startups, from €multi-million research consortia to undergraduate projects, our user community is the largest and most diverse of any system of this type, and is spread across all but one of the continents1.</p>

<p><em>textir</em> - A suite of tools for text and sentiment mining. This includes the ‘mnlm’ function, for sparse multinomial logistic regression, ‘pls’, a concise partial least squares routine, and the ‘topics’ function, for efficient estimation and dimension selection in latent topic models.</p>

<p>NLP Toolsuite - The JULIE Lab here offers a comprehensive NLP tool suite for the application purposes of semantic search, information extraction and text mining. Most of our continuously expanding tool suite is based on machine learning methods and thus is domain- and language independent.</p>

<p>...</p>

<p>On a side note: Would you recommend the twitter streaming or the get API? </p>

<p>As to me, I am a fan of python and java ;)</p>

<p>Thanks a lot for your help!!!</p>
","twitter, nlp, nltk, sentiment-analysis","<p>I'm not sure how much I can help, but I have worked with hand-rolled NLP before. A couple of issues come to mind - not all products are language agnostic (human language that is, not computer language). If you're planning on analysing German tweets, it's going to be important that your selected product is able to handle the German language. Obvious I know, but easy to forget. Then there's the fact that it's twitter where contractions and acronyms abound, and the language structure is constrained by the character limit which means that the grammar won't always match the expected structure of the language.</p>

<p>In English, pulling nouns from a sentence can be simplified somewhat if you ever have to write code of your own. Proper nouns have initial capitals and a string of such words (possibly including ""of"") is an example of a noun phrase. A word preceeded by ""a/an/my/his/hers/the/this/these/those"" is going to be either an adjective or a noun. It gets harder after that unfortunately.</p>

<p>There are rules which help identify plurals, but there are also lots of exceptions. I'm talking about English here of course, my very poor spoken German doesn't help me understand that grammar I'm afraid.</p>
",4,15,7268,2012-09-06 12:05:15,https://stackoverflow.com/questions/12299724/list-of-natural-language-processing-tools-in-regards-to-sentiment-analysis-whi
Hive: How to have a derived column that has stores the sentiment value from the sentiment analysis API,"<p>Here's the scenario:</p>

<p>Say you have a Hive Table that stores twitter data. </p>

<p>Say it has 5 columns. One column being the Text Data. </p>

<p>Now How do you add a 6th column that stores the sentiment value from the Sentiment Analysis of the twitter Text data. I plan to use the Sentiment Analysis API like Sentiment140 or viralheat. </p>

<p>I would appreciate any tips on how to implement the ""derived"" column in Hive.</p>

<p>Thanks. </p>
","hadoop, hive, sentiment-analysis","<p>Unfortunately, while the Hive API lets you add a new column to your table (using ALTER TABLE foo ADD COLUMNS (bar binary)), those new columns will be NULL and cannot be populated. The only way to add data to these columns is to clear the table's rows and load data from a new file, this new file having that new column's data.</p>

<p>To answer your question: You can't, in Hive. To do what you propose, you would have to have a file with 6 columns, the 6th already containing the sentiment analysis data. This could then be loaded into your HDFS, and queried using Hive.</p>

<p>EDIT: Just tried an example where I exported the table as a .csv after adding the new column (see above), and popped that into M$ Excel where I was able to perform functions on the table values. After adding functions, I just saved and uploaded the .csv, and rebuilt the table from it. Not sure if this is helpful to you specifically (since it's not likely that sentiment analysis can be done in Excel), but may be of use to anyone else just wanting to have computed columns in Hive.</p>

<p>References:</p>

<p><a href=""https://cwiki.apache.org/Hive/gettingstarted.html#GettingStarted-DDLOperations"" rel=""nofollow"">https://cwiki.apache.org/Hive/gettingstarted.html#GettingStarted-DDLOperations</a></p>

<p><a href=""http://comments.gmane.org/gmane.comp.java.hadoop.hive.user/6665"" rel=""nofollow"">http://comments.gmane.org/gmane.comp.java.hadoop.hive.user/6665</a></p>
",1,2,3166,2012-11-16 22:52:23,https://stackoverflow.com/questions/13425623/hive-how-to-have-a-derived-column-that-has-stores-the-sentiment-value-from-the
Add new words to the lexicon for R sentiment package,"<p>I'm currently doing sentiment and emotion analysis of Twitter's data using R sentiment package and need to add new words to the subjectivity and emotion lexicons used by the package as there are some words that carry specific sentiment and emotion in the topic that I analyze.</p>

<p>Does anyone know how to add words to the lexicon using the R sentiment package itself or any other R command? I have searched in the documentation but cannot find any means to do so.  </p>
","r, sentiment-analysis","<p>Both the subjectivity &amp; emotion lexicon when read in (say, as csv) construct a data frame for you. Adding entries to a data frame can be done using the rbind() function.</p>

<p>> patientID &lt;- c(1, 2, 3, 4)<br>
> age &lt;- c(25, 34, 28, 52)<br>
> diabetes &lt;- c(""Type1"", ""Type2"", ""Type1"", ""Type1"")<br>
> status &lt;- c(""Poor"", ""Improved"", ""Excellent"", ""Poor"")<br>
> patientdata &lt;- data.frame(patientID, age, diabetes, status)<br>
> patientdata<br>
  patientID age diabetes    status<br>
1         1  25    Type1      Poor<br>
2         2  34    Type2  Improved<br>
3         3  28    Type1 Excellent<br>
4         4  52    Type1      Poor<br>
> patientID &lt;- c(10, 20, 30, 40)<br>
> age &lt;- c(50, 68, 56, 104)<br>
> diabetes &lt;- c(""Type4"", ""Type5"", ""Type6"", ""Type7"")<br>
> status &lt;- c(""Poorish"", ""Improving"", ""Excellento"", ""Poorish"")<br>
> patientdata1 &lt;- data.frame(patientID, age, diabetes, status)<br>
> patientdata1<br>
  patientID age diabetes     status<br>
1        10  50    Type4    Poorish<br>
2        20  68    Type5  Improving<br>
3        30  56    Type6 Excellento<br>
4        40 104    Type7    Poorish<br>
> concatPD &lt;- rbind(patientdata,patientdata1)<br>
> concatPD<br>
  patientID age diabetes     status<br>
1         1  25    Type1       Poor<br>
2         2  34    Type2   Improved<br>
3         3  28    Type1  Excellent<br>
4         4  52    Type1       Poor<br>
5        10  50    Type4    Poorish<br>
6        20  68    Type5  Improving<br>
7        30  56    Type6 Excellento<br>
8        40 104    Type7    Poorish<br>
> </p>

<p>I simply added the weird types of diabetes to ensure that the 2 frames are distinguished. :-)
In other words, you can create our own csv, read it (thereby creating another data frame) &amp; rbind them. Ensure that the columns of both data frames are in sync.</p>
",0,1,1181,2012-11-18 08:24:27,https://stackoverflow.com/questions/13438579/add-new-words-to-the-lexicon-for-r-sentiment-package
Feature Selection and Reduction for Text Classification,"<p>I am currently working on a project, a <strong>simple sentiment analyzer</strong> such that there will be <strong>2 and 3 classes</strong> in <strong>separate cases</strong>. I am using a <strong>corpus</strong> that is pretty <strong>rich</strong> in the means of <strong>unique words</strong> (around 200.000). I used <strong>bag-of-words</strong> method for <strong>feature selection</strong> and to reduce the number of <strong>unique features</strong>, an elimination is done due to a <strong>threshold value</strong> of <strong>frequency of occurrence</strong>. The <strong>final set of features</strong> includes around 20.000 features, which is actually a <strong>90% decrease</strong>, but <strong>not enough</strong> for intended <strong>accuracy</strong> of test-prediction. I am using <strong>LibSVM</strong> and <strong>SVM-light</strong> in turn for training and prediction (both <strong>linear</strong> and <strong>RBF kernel</strong>) and also <strong>Python</strong> and <strong>Bash</strong> in general.</p>

<p>The <strong>highest accuracy</strong> observed so far <strong>is around 75%</strong> and I <strong>need at least 90%</strong>. This is the case for <strong>binary classification</strong>. For <strong>multi-class training</strong>, the accuracy falls to <strong>~60%</strong>. I <strong>need at least 90%</strong> at both cases and can not figure how to increase it: via <strong>optimizing training parameters</strong> or <strong>via optimizing feature selection</strong>?</p>

<p>I have read articles about <strong>feature selection</strong> in text classification and what I found is that three different methods are used, which have actually a clear correlation among each other. These methods are as follows:</p>

<ul>
<li>Frequency approach of <strong>bag-of-words</strong> (BOW)</li>
<li><strong>Information Gain</strong> (IG)</li>
<li><strong>X^2 Statistic</strong> (CHI)</li>
</ul>

<p>The first method is already the one I use, but I use it very simply and need guidance for a better use of it in order to obtain high enough accuracy. I am also lacking knowledge about practical implementations of <strong>IG</strong> and <strong>CHI</strong> and looking for any help to guide me in that way.</p>

<p>Thanks a lot, and if you need any additional info for help, just let me know.</p>

<hr>

<ul>
<li><p>@larsmans: <strong>Frequency Threshold</strong>: I am looking for the occurrences of unique words in examples, such that if a word is occurring in different examples frequently enough, it is included in the feature set as a unique feature.   </p></li>
<li><p>@TheManWithNoName: First of all thanks for your effort in explaining the general concerns of document classification. I examined and experimented all the methods you bring forward and others. I found <strong>Proportional Difference</strong> (PD) method the best for feature selection, where features are uni-grams and <strong>Term Presence</strong> (TP) for the weighting (I didn't understand why you tagged <strong>Term-Frequency-Inverse-Document-Frequency</strong> (TF-IDF) as an indexing method, I rather consider it as a <strong>feature weighting</strong> approach).  <strong>Pre-processing</strong> is also an important aspect for this task as you mentioned. I used certain types of string elimination for refining the data as well as <strong>morphological parsing</strong> and <strong>stemming</strong>. Also note that I am working on <strong>Turkish</strong>, which has <strong>different characteristics</strong> compared to English. Finally, I managed to reach <strong>~88% accuracy</strong> (f-measure) for <strong>binary</strong> classification and <strong>~84%</strong> for <strong>multi-class</strong>. These values are solid proofs of the success of the model I used. This is what I have done so far. Now working on clustering and reduction models, have tried <strong>LDA</strong> and <strong>LSI</strong> and moving on to <strong>moVMF</strong> and maybe <strong>spherical models</strong> (LDA + moVMF), which seems to work better on corpus those have objective nature, like news corpus. If you have any information and guidance on these issues, I will appreciate. I need info especially to setup an interface (python oriented, open-source) between <strong>feature space dimension reduction</strong> methods (LDA, LSI, moVMF etc.) and <strong>clustering methods</strong> (k-means, hierarchical etc.).</p></li>
</ul>
","python, nlp, svm, sentiment-analysis, feature-extraction","<p>This is probably a bit late to the table, but...</p>

<p>As Bee points out and you are already aware, the use of SVM as a classifier is wasted if you have already lost the information in the stages prior to classification. However, the process of text classification requires much more that just a couple of stages and each stage has significant effects on the result. Therefore, before looking into more complicated feature selection measures there are a number of much simpler possibilities that will typically require much lower resource consumption.</p>

<p>Do you pre-process the documents before performing tokensiation/representation into the bag-of-words format? Simply removing stop words or punctuation may improve accuracy considerably.</p>

<p>Have you considered altering your bag-of-words representation to use, for example, word pairs or n-grams instead? You may find that you have more dimensions to begin with but that they condense down a lot further and contain more useful information.</p>

<p>Its also worth noting that dimension reduction <strong>is</strong> feature selection/feature extraction. The difference is that feature selection reduces the dimensions in a univariate manner, i.e. it removes terms on an individual basis as they currently appear without altering them, whereas feature extraction (which I think Ben Allison is referring to) is multivaritate, combining one or more single terms together to produce higher orthangonal terms that (hopefully) contain more information and reduce the feature space.</p>

<p>Regarding your use of document frequency, are you merely using the probability/percentage of documents that contain a term or are you using the term densities found within the documents? If category one has only 10 douments and they each contain a term once, then category one is indeed associated with the document. However, if category two has only 10 documents that each contain the same term a hundred times each, then obviously category two has a much higher relation to that term than category one. If term densities are not taken into account this information is lost and the fewer categories you have the more impact this loss with have. On a similar note, it is not always prudent to only retain terms that have high frequencies, as they may not actually be providing any useful information. For example if a term appears a hundred times in every document, then it is considered a noise term and, while it looks important, there is no practical value in keeping it in your feature set.</p>

<p>Also how do you index the data, are you using the Vector Space Model with simple boolean indexing or a more complicated measure such as TF-IDF? Considering the low number of categories in your scenario a more complex measure will be beneficial as they can account for term importance for each category in relation to its importance throughout the entire dataset.</p>

<p>Personally I would experiment with some of the above possibilities first and then consider tweaking the feature selection/extraction with a (or a combination of) complex equations if you need an additional performance boost.</p>

<hr>

<p><strong>Additional</strong></p>

<p>Based on the new information, it sounds as though you are on the right track and 84%+ accuracy (F1 or BEP - precision and recall based for multi-class problems) is generally considered very good for most datasets. It might be that you have successfully acquired all information rich features from the data already, or that a few are still being pruned.</p>

<p>Having said that, something that can be used as a predictor of how good aggressive dimension reduction may be for a particular dataset is 'Outlier Count' analysis, which uses the decline of Information Gain in outlying features to determine how likely it is that information will be lost during feature selection. You can use it on the raw and/or processed data to give an estimate of how aggressively you should aim to prune features (or unprune them as the case may be). A paper describing it can be found here:</p>

<p><a href=""http://www.cs.technion.ac.il/~gabr/papers/fs-svm.pdf"" rel=""noreferrer"" title=""Text Categorization with Many Redundant Features: Using Aggressive Feature Selection to Make SVMs Competitive with C4.5"">Paper with Outlier Count information</a></p>

<p>With regards to describing TF-IDF as an indexing method, you are correct in it being a feature weighting measure, but I consider it to be used mostly as part of the indexing process (though it can also be used for dimension reduction). The reasoning for this is that some measures are better aimed toward feature selection/extraction, while others are preferable for feature weighting specifically in your document vectors (i.e. the indexed data). This is generally due to dimension reduction measures being determined on a per category basis, whereas index weighting measures tend to be more document orientated to give superior vector representation.</p>

<p>In respect to LDA, LSI and moVMF, I'm afraid I have too little experience of them to provide any guidance. Unfortunately I've also not worked with Turkish datasets or the python language. </p>
",40,53,33031,2012-11-28 11:21:59,https://stackoverflow.com/questions/13603882/feature-selection-and-reduction-for-text-classification
Find &quot;near duplicates&quot; strings in R,"<p>I am using R to build a sentiment analysis tool and I am having some problems with duplicates. The main source of data is Twitter, and it looks like many are bypassing twitter own spam filter by adding some random text at the end of each tweet. For example</p>

<pre><code>Click xxxxx to buy the amazing xxxxx for FREE ugjh
</code></pre>

<p>I get tons of those exact tweets with a different random string at the end. They are either from the same user or from different.</p>

<p>Is there any function like <code>duplicated</code> or <code>unique</code> which returns how close 2 strings are and if they are above a certain % dismiss them?</p>

<p>I know doing that will eventually delete real tweets from people saying exactly the same, like</p>

<pre><code>I love xxxx !
</code></pre>

<p>but I will deal with that in the future.</p>

<p>Any tip in the right direction will be much appreciated!</p>
","r, nlp, sentiment-analysis, text-analysis","<p>I mentioned <code>agrep</code> above.  Here's an example with what you've explained.  By varying the <code>max.distance</code> we can adjust what gets kicked:</p>

<pre><code>comp &lt;- ""Click xxxxx to buy the amazing xxxxx for FREE ugjh""
w &lt;- ""I love xxxx !""
x &lt;- ""Click xxxxx to purchase the awesome xxxxx for FREE bmf""

agrep(comp, c(x, w), max.distance =.4, value = TRUE)
agrep(comp, c(x, w), max.distance =.9, value = TRUE)
</code></pre>
",6,3,1533,2012-12-05 01:23:58,https://stackoverflow.com/questions/13714893/find-near-duplicates-strings-in-r
semantic analysis opensource tool - suggestions needed,"<p>I've a book review site, where readers can write reviews about books, other users can post comments. I wanted to know following things automatically whenever new review publish or new comment published.</p>

<p>(1) whether book review is positive or not? How much % positive / negative?</p>

<p>(2) whether comment made by particular user is positive or not? How much % positive / negative?</p>

<p>(3) I want to read Tweets about particular book and wanted to check whether the tweet is positive or not?</p>

<p>bottom line, I want some tool suggestions (opensource), which I can use for my website. Website is written in PHP and I'm looking for some semantic analysis tool which I can customize to meet my need or which best fit my need.</p>

<p>if not, I want to know if its easy to build one with minimal requirements. I know PHP, Perl, Shell Script. I can learn Python. I know C++, Java may be right language to start from scratch; but don't have much experience.</p>
","sentiment-analysis, semantic-analysis","<p>There is an open source semantic analyses engine incubated in the Apache Software Foundation, currently, called <a href=""http://stanbol.apache.org/"" rel=""nofollow"">Stanbol</a>. It provides APIs to interface with it over HTTP as well as through a Java API if needed. It's pretty advanced, but generally speaking if your needs are simpler you can always try some SaS solution like <a href=""http://www.uclassify.com/browse/uclassify/Sentiment"" rel=""nofollow"">uClassify</a>.</p>
",1,3,2785,2012-12-17 17:50:25,https://stackoverflow.com/questions/13919351/semantic-analysis-opensource-tool-suggestions-needed
Sentiment analysis/linear regression (Django),"<p>I need a suggestion on how to do analyze this type of data. I want to perform a sentiment analysis or linear regression on it as a machine learning tool. The predictor is score.</p>

<pre><code>color   type    make    new score

red     truck   ford    y   2
black   sedan   chevy   n   4
silver  sedan   nissan  y   5
silver  truck   nissan  n   2
black   coupe   toyota  y   1
blue    van     honda   y   1
red     truck   toyota  n   4
red     coupe   ford    n   2
black   sedan   ford    y   1
blue    truck   toyota  y   4
white   coupe   chevy   y   3
white   van     toyota  n   5
red     van     ford    y   2
silver  truck   nissan  n   3
black   sedan   honda   n   1
silver  truck   chevy   y   4
red     truck   chevy   y   5
white   coupe   honda   n   5
blue    sedan   chevy   n   2
blue    van     nissan  y   3
</code></pre>

<p>I can run a LinearRegression classifier in WEKA which yields:</p>

<pre><code>score =  1.6 ( color=red,silver,white) + 1.8 (make=honda,nissan,toyota,chevy) + 0.55
</code></pre>

<p>However, I would like to implement this in Django for a web app. Is there another way to process this data and yield a linear regression equation not using WEKA. Any other suggestions on how to analyze it other than linear regression? I've already implemented a decision tree.</p>
","python, django, machine-learning, weka, sentiment-analysis","<p>You can use <a href=""http://scikit-learn.org/stable/"" rel=""nofollow"">scikit-learn</a> as your machine learning library, and particularly its <a href=""http://scikit-learn.org/0.12/modules/linear_model.html"" rel=""nofollow"">linear regression capability</a>. <a href=""http://scikit-learn.org/0.12/auto_examples/linear_model/plot_ols.html#example-linear-model-plot-ols-py"" rel=""nofollow"">This example</a> might also be useful.</p>

<p>Also, you can always bind the Weka java API to your application, or alternatively implement linear regression on your own, it is fairly easy algorithm to implement given a matrix algebra library.</p>
",4,4,2045,2012-12-19 16:37:25,https://stackoverflow.com/questions/13956943/sentiment-analysis-linear-regression-django
use sentiment dictionary value as features in SVM,"<p>I have a sentiment dictionary of positive and negative words with their sentiment strength value. My main work is to check whether this strength value have effect on final classification or not. It means I want to check if the text with word ""good"" (strength=6) and word with outstanding(strength=9) have different final sentiment score or not.</p>

<p>I am confused in creating feature vector for <code>SVM</code>. If i use <code>TF-IDF</code> measure or <code>POS tagging</code> it doesn't check strength value. So my main problem is how to use this strength value in SVM and how to generate feature vector containing strength value of word?</p>

<p>For example, </p>

<pre><code>""This book is good."" 
</code></pre>

<p>For this sentence how can I generate feature vector considering strength value? </p>

<ul>
<li><p>First I thought to multiply strength value with term frequency and use this weighted score as feature input, but it will just increase the frequency of word. For example ""good"" occurs 2 times and then I multiply it with its strength value 6 then its value became 12, so it will just increase the occurrence of word ""good"", am I right?</p></li>
<li><p>So please can anyone tell me if i can use sentiment strength value for <code>SVM</code> and how can i use it? </p></li>
<li><p>How can I generate feature vector with their values?</p></li>
</ul>
","machine-learning, classification, svm, information-retrieval, sentiment-analysis","<p>Just some suggestions:</p>

<h2>Construct a vocabulary</h2>

<p>This vocabulary serves as a dictionary. You will not include any word that does not present in the dictionary into your feature vector. Suppose your dictionary contains 5000 words. </p>

<h2>Prepare the sentiment strength for each word in the vocabulary</h2>

<p>Of course you can setup some default for those words that you have no idea about their sentiment strength.  </p>

<h2>Construct feature vector for each text you want to do classification</h2>

<p>For any given text, e.g., </p>

<pre><code>This book is good.
</code></pre>

<p>construct a feature vector with 5000 dimensions. Each dimension corresponds to its Tf-Idf score or just the number of occurrences of a word in the dictionary. Suppose in your dictionary, you have </p>

<pre><code>strength(book) = 0.01
strength(good) = 6.0, 
</code></pre>

<p>and you don't have entries for <code>this</code> or <code>is</code>. Then you will end up with a vector with 5000 elements (I am using the number of occurrences instead of Tf-Idf in my following example. Feel free to try Tf-Idf in a similar way). </p>

<pre><code>          book,good
[0,0,0, ..., 1,1,0,0,....,0]
</code></pre>

<p>All elements are zeros except the two elements that correspond to <code>book</code> and <code>good</code>. Plug in your sentiment strength, you get:</p>

<pre><code>           book,good
[0,0,0, ...,0.01,6.0,0,0,....,0]
</code></pre>

<p>Multiplying the strength value with the number of occurrences will probably increase or decrease the value of the corresponding element. This is fine because you do want to boost or weaken the contribution of the component by its sentiment strength. </p>

<h2>Training the SVM</h2>

<p>When supplying each feature vector with a target value or class label, you can train your SVM now. </p>

<p>Hope they help. </p>
",0,0,1320,2013-01-23 18:23:37,https://stackoverflow.com/questions/14486665/use-sentiment-dictionary-value-as-features-in-svm
Sentiment analysis,"<p>while performing sentiment analysis, how can I make the machine understand that I'm referring apple (the iphone), instead of apple (the fruit)? </p>

<p>Thanks for the advise ! </p>
",sentiment-analysis,"<p>Well, there are several methods,</p>

<p>I would start with checking Capital letter, usually, when referring to a name, first letter is capitalized.</p>

<p>Before doing sentiment analysis, I would use some Part-of-speech and Named Entity Recognition to tag the relevant words.</p>

<p><a href=""http://nlp.stanford.edu/software/CRF-NER.shtml#Download"" rel=""nofollow noreferrer"">Stanford CoreNLP</a> is a good text analysis project to start with, it will teach
you the basic concepts.</p>

<p>Example from CoreNLP:</p>

<p><img src=""https://i.sstatic.net/r7ZFP.png"" alt=""enter image description here""></p>

<p>You can see how the tags can help you.</p>

<p>And check out <a href=""https://stackoverflow.com/questions/6893858/how-i-classify-an-word-of-a-text-in-things-like-names-number-money-date-etc"">more info</a></p>
",4,7,2484,2013-02-17 18:54:11,https://stackoverflow.com/questions/14924772/sentiment-analysis
LingPipe and Sentiment Analysis,"<p>I am following this document: </p>

<p><a href=""http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html"" rel=""nofollow"">http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html</a> to perform sentiment analysis using lingpipe. </p>

<p>One of the steps says that ""Assuming the data is in the directory POLARITY_DIR and the sentimentDemo.jar file exists (if the jar doesn't exist, compile it), the demo may be run from the command line""</p>

<p>I am not able to find the SentimentDemo.jar file. Tried windows search. 
So how do we compile it? I am not familiar with using Ant on windows and how to compile lingpipe jar files w/ it - any guidance is appreciated!</p>

<p>Also When I try to run the code: </p>

<pre><code>java
-cp ""sentimentDemo.jar;
     ../../../lingpipe-4.1.0.jar""
PolarityBasic POLARITY_DIR
</code></pre>

<p>I get the error saying that:
""Error: Could not find or load main class PolarityBasic""</p>

<p>I've configured polarity_dir so I am guessing that this isn't an issue.</p>

<p>Additional Info:</p>

<blockquote>
  <p>I am on windows 7
  using lingpipe 4.1.0</p>
</blockquote>

<p>Any guidance in completing the tutorial is appreciated, Thanks!</p>
","nlp, sentiment-analysis","<p>You should be able to build the jar file with ""ant jar"". </p>

<p>That command is suggested a little later in the tutorial (<a href=""http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html"" rel=""nofollow"">LingPipe: Sentiment Analysis Tutorial</a>) </p>

<blockquote>
  <p>...(if it doesn't, run ant jar to create it)...</p>
</blockquote>

<p>You'll need to install Ant (from <a href=""http://ant.apache.org/"" rel=""nofollow"">ant.apache.org</a>), and add it to your PATH. And you may have to set the ANT_HOME and/or JAVA_HOME environment variables prior to running ant.</p>
",1,0,1366,2013-02-21 18:58:48,https://stackoverflow.com/questions/15010184/lingpipe-and-sentiment-analysis
Mahout for sentiment analysis,"<p>Using mahout I am able to classify sentiment of data . But I am stuck with a confusion matrix. </p>

<p>I am using mahout 0.7 naive bayes algorithms to classify sentiment of tweets.
I  use <code>trainnb</code> and <code>testnb</code> naive bayes classifiers to train the classifier and classify sentiment of tweets as 'positive' ,'negative' or 'neutral'.</p>

<p>Sample positive training set </p>

<pre><code>      'positive','i love my i phone'
      'positive' , it's pleasure to have i phone'  
</code></pre>

<p>Similarly I have prepared training samples of negative 
and neutral, it is a huge data set.</p>

<p>The sample test data tweets I am providing is without including sentiments.</p>

<pre><code>  'it is nice model'
  'simply fantastic ' 
</code></pre>

<p>I am able to run the mahout classification algorithm, and it gives output of classified instances as confusion matrix .</p>

<p>Next step I need to find out which tweets are showing positive sentiment and which are negative.
expected output using classification:  to tag text with the sentiment.</p>

<pre><code>       'negative','very bad btr life time'
      'positive' , 'i phone has excellent design features' 
</code></pre>

<p>In mahout  which algorithm do I need to implement to get output in the above format. or any custom source implementation is required. </p>

<p>To display data 'kindly' suggest me algorithms that apache mahout provides, which will be suitable for my twitter data sentiment analysis.</p>
","machine-learning, mahout, sentiment-analysis","<p>In general to classify some text you need to run Naive Bayes with different priors (positive and negative in your case) and then just chose the one that results in greater value.</p>

<p><a href=""http://www.oracle.com/technetwork/community/join/member-discounts/523387"" rel=""nofollow"">This excerpt</a> from the Mahout book has some examples. See Listing 2:</p>

<pre><code>Parameters p = new Parameters();
p.set(""basePath"", modelDir.getCanonicalPath());9
Datastore ds = new InMemoryBayesDatastore(p);
Algorithm a = new BayesAlgorithm();
ClassifierContext ctx = new ClassifierContext(a,ds);
ctx.initialize();

....

ClassifierResult result = ctx.classifyDocument(tokens, defaultCategory);
</code></pre>

<p>Here result should hold either ""positive"" or ""negative"" label.</p>
",3,6,4772,2013-03-07 11:36:29,https://stackoverflow.com/questions/15270145/mahout-for-sentiment-analysis
Sentiment Analysis on LARGE collection of online conversation text,"<p>The title says it all; I have an SQL database bursting at the seams with online conversation text. I've already done most of this project in Python, so I would like to do this using Python's NLTK library (unless there's a <strong>strong</strong> reason not to).</p>

<p>The data is organized by <strong>Thread</strong>, <strong>Username</strong>, and <strong>Post</strong>. Each thread more or less focuses on discussing one ""product"" of the Category that I am interested in analyzing. Ultimately,  when this is finished, I would like to have an estimated opinion (like/dislike sort of deal) from each user for any of the products they had discussed at some point.</p>

<p>So, what I would like to know:</p>

<p>1) <strong>How can I go about determining what product each thread is about?</strong> I was reading about keyword extraction... is that the correct method?</p>

<p>2) <strong>How do I determine a specific users sentiment based on their posts?</strong> From my limited understanding, I must first ""train"" NLTK to recognize certain indicators of opinion, and then do I simply determine the context of those words when they appear in the text?</p>

<p>As you may have guessed by now, I have no prior experience with NLP. From my reading so far, I think I can handle learning it though. Even just a basic and crude working model for now would be great if someone can point me in the right direction. Google was not very helpful to me.</p>

<p><strong>P.S.</strong> I have permission to analyze this data (in case it matters)</p>
","python, nlp, nltk, text-mining, sentiment-analysis","<p>Training any classifier requires a <strong>training set</strong> of <strong>labeled data</strong> and a feature extractor to obtain <strong>feature sets</strong> for each text. After you have a trained classifier, you can apply it to previously unseen text (unlabeled) and obtain a classification based on the machine learning algorithm used. NLTK <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html"" rel=""nofollow"">gives a good explanation and some samples to play around with</a>.</p>

<p>If you are interested in building a classifier for positive/negative sentiment, using your own training dataset, I would avoid simple keyword counts, as <a href=""http://sentiment.christopherpotts.net/lingcog.html#commitment"" rel=""nofollow"">they aren't accurate for a number of reasons</a> (eg. negation of positive words: ""not happy""). An alternative, where you can still use a large training set without having to manually label anything, is <strong>distant supervision</strong>. Basically, this approach uses <em>emoticons</em> or other specific text elements as <strong>noisy labels</strong>. You still have to choose which features are relevant but many studies have had good results with simply using <em>unigrams</em> or <em>bigrams</em> (individual words or pairs of words, respectively).</p>

<p>All of this can be done relatively easily with Python and NLTK. You can also choose to use a tool like <a href=""https://github.com/japerk/nltk-trainer"" rel=""nofollow"">NLTK-trainer</a>, which is a wrapper for NLTK and requires less code.</p>

<p>I think <a href=""http://cs.wmich.edu/~tllake/fileshare/TwitterDistantSupervision09.pdf"" rel=""nofollow"">this study</a> by Go et al. is one of the easiest to understand. You can also read other studies for <a href=""http://scholar.google.com/scholar?q=distant%20supervision"" rel=""nofollow"">distant supervision</a>, <a href=""http://scholar.google.com/scholar?q=distant%20supervision%20sentiment%20analysis"" rel=""nofollow"">distant supervision sentiment analysis</a>, and <a href=""http://scholar.google.com/scholar?q=twitter%20sentiment%20analysis"" rel=""nofollow"">sentiment analysis</a>. </p>

<p>There are a few built-in classifiers in NLTK with both training and classification methods (<a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.classify.naivebayes.NaiveBayesClassifier-class.html"" rel=""nofollow"">Naive Bayes</a>, <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.classify.maxent.MaxentClassifier-class.html"" rel=""nofollow"">MaxEnt</a>, etc.) but if you are interested in using Support Vector Machines (SVM) then you should look elsewhere. Technically NLTK provides you with an <a href=""http://nltk.org/_modules/nltk/classify/svm.html"" rel=""nofollow"">SVM class</a> but its really just a wrapper for <a href=""https://bitbucket.org/wcauchois/pysvmlight"" rel=""nofollow"">PySVMLight</a>, which itself is a wrapper for <a href=""http://svmlight.joachims.org"" rel=""nofollow"">SVMLight</a>, written in C. I had numerous problems with this approach though, and would instead recommend <a href=""http://www.csie.ntu.edu.tw/~cjlin/libsvm/"" rel=""nofollow"">LIBSVM</a>.</p>

<p>For determining the topic, many have used simple keywords but there are some more complex methods available.</p>
",5,10,5839,2013-03-10 19:44:20,https://stackoverflow.com/questions/15326694/sentiment-analysis-on-large-collection-of-online-conversation-text
How to use SentiWordNet,"<p>I need to do sentiment analysis on some csv files containing tweets. I'm using <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""nofollow"">SentiWordNet</a> to do the sentiment analysis.</p>

<p>I got the following piece of sample java code they provided on their site. I'm not sure how to use it. The path of the csv file that I want to analyze is <code>C:\Users\MyName\Desktop\tweets.csv</code> . The path of the <code>SentiWordNet_3.0.0.txt</code> is <code>C:\Users\MyName\Desktop\SentiWordNet_3.0.0\home\swn\www\admin\dump\SentiWordNet_3.0.0_20130122.txt</code> . I'm new to java, pls help, thanks! The link to the sample java code below is <a href=""http://sentiwordnet.isti.cnr.it/code/SWN3.java"" rel=""nofollow"">this</a>.</p>

<pre><code>import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Set;
import java.util.Vector;

public class SWN3 {
    private String pathToSWN = ""data""+File.separator+""SentiWordNet_3.0.0.txt"";
    private HashMap&lt;String, String&gt; _dict;

    public SWN3(){

        _dict = new HashMap&lt;String, String&gt;();
        HashMap&lt;String, Vector&lt;Double&gt;&gt; _temp = new HashMap&lt;String, Vector&lt;Double&gt;&gt;();
        try{
            BufferedReader csv =  new BufferedReader(new FileReader(pathToSWN));
            String line = """";           
            while((line = csv.readLine()) != null)
            {
                String[] data = line.split(""\t"");
                Double score = Double.parseDouble(data[2])-Double.parseDouble(data[3]);
                String[] words = data[4].split("" "");
                for(String w:words)
                {
                    String[] w_n = w.split(""#"");
                    w_n[0] += ""#""+data[0];
                    int index = Integer.parseInt(w_n[1])-1;
                    if(_temp.containsKey(w_n[0]))
                    {
                        Vector&lt;Double&gt; v = _temp.get(w_n[0]);
                        if(index&gt;v.size())
                            for(int i = v.size();i&lt;index; i++)
                                v.add(0.0);
                        v.add(index, score);
                        _temp.put(w_n[0], v);
                    }
                    else
                    {
                        Vector&lt;Double&gt; v = new Vector&lt;Double&gt;();
                        for(int i = 0;i&lt;index; i++)
                            v.add(0.0);
                        v.add(index, score);
                        _temp.put(w_n[0], v);
                    }
                }
            }
            Set&lt;String&gt; temp = _temp.keySet();
            for (Iterator&lt;String&gt; iterator = temp.iterator(); iterator.hasNext();) {
                String word = (String) iterator.next();
                Vector&lt;Double&gt; v = _temp.get(word);
                double score = 0.0;
                double sum = 0.0;
                for(int i = 0; i &lt; v.size(); i++)
                    score += ((double)1/(double)(i+1))*v.get(i);
                for(int i = 1; i&lt;=v.size(); i++)
                    sum += (double)1/(double)i;
                score /= sum;
                String sent = """";               
                if(score&gt;=0.75)
                    sent = ""strong_positive"";
                else
                if(score &gt; 0.25 &amp;&amp; score&lt;=0.5)
                    sent = ""positive"";
                else
                if(score &gt; 0 &amp;&amp; score&gt;=0.25)
                    sent = ""weak_positive"";
                else
                if(score &lt; 0 &amp;&amp; score&gt;=-0.25)
                    sent = ""weak_negative"";
                else
                if(score &lt; -0.25 &amp;&amp; score&gt;=-0.5)
                    sent = ""negative"";
                else
                if(score&lt;=-0.75)
                    sent = ""strong_negative"";
                _dict.put(word, sent);
            }
        }
        catch(Exception e){e.printStackTrace();}        
    }

    public String extract(String word, String pos)
    {
        return _dict.get(word+""#""+pos);
    }
}
</code></pre>

<p>Newcode:</p>

<pre><code>public class SWN3 {
        private String pathToSWN = ""C:\\Users\\MyName\\Desktop\\SentiWordNet_3.0.0\\home\\swn\\www\\admin\\dump\\SentiWordNet_3.0.0.txt"";
    private HashMap&lt;String, String&gt; _dict;

    public SWN3(){

        _dict = new HashMap&lt;String, String&gt;();
        HashMap&lt;String, Vector&lt;Double&gt;&gt; _temp = new HashMap&lt;String, Vector&lt;Double&gt;&gt;();
        try{
            BufferedReader csv =  new BufferedReader(new FileReader(pathToSWN));
            String line = """";           
            while((line = csv.readLine()) != null)
            {
                String[] data = line.split(""\t"");
                Double score = Double.parseDouble(data[2])-Double.parseDouble(data[3]);
                String[] words = data[4].split("" "");
                for(String w:words)
                {
                    String[] w_n = w.split(""#"");
                    w_n[0] += ""#""+data[0];
                    int index = Integer.parseInt(w_n[1])-1;
                    if(_temp.containsKey(w_n[0]))
                    {
                        Vector&lt;Double&gt; v = _temp.get(w_n[0]);
                        if(index&gt;v.size())
                            for(int i = v.size();i&lt;index; i++)
                                v.add(0.0);
                        v.add(index, score);
                        _temp.put(w_n[0], v);
                    }
                    else
                    {
                        Vector&lt;Double&gt; v = new Vector&lt;Double&gt;();
                        for(int i = 0;i&lt;index; i++)
                            v.add(0.0);
                        v.add(index, score);
                        _temp.put(w_n[0], v);
                    }
                }
            }
            Set&lt;String&gt; temp = _temp.keySet();
            for (Iterator&lt;String&gt; iterator = temp.iterator(); iterator.hasNext();) {
                String word = (String) iterator.next();
                Vector&lt;Double&gt; v = _temp.get(word);
                double score = 0.0;
                double sum = 0.0;
                for(int i = 0; i &lt; v.size(); i++)
                    score += ((double)1/(double)(i+1))*v.get(i);
                for(int i = 1; i&lt;=v.size(); i++)
                    sum += (double)1/(double)i;
                score /= sum;
                String sent = """";               
                if(score&gt;=0.75)
                    sent = ""strong_positive"";
                else
                if(score &gt; 0.25 &amp;&amp; score&lt;=0.5)
                    sent = ""positive"";
                else
                if(score &gt; 0 &amp;&amp; score&gt;=0.25)
                    sent = ""weak_positive"";
                else
                if(score &lt; 0 &amp;&amp; score&gt;=-0.25)
                    sent = ""weak_negative"";
                else
                if(score &lt; -0.25 &amp;&amp; score&gt;=-0.5)
                    sent = ""negative"";
                else
                if(score&lt;=-0.75)
                    sent = ""strong_negative"";
                _dict.put(word, sent);
            }
        }
        catch(Exception e){e.printStackTrace();}        
    }

    public Double extract(String word)
    {
        Double total = new Double(0);
        if(_dict.get(word+""#n"") != null)
             total = _dict.get(word+""#n"") + total;
        if(_dict.get(word+""#a"") != null)
            total = _dict.get(word+""#a"") + total;
        if(_dict.get(word+""#r"") != null)
            total = _dict.get(word+""#r"") + total;
        if(_dict.get(word+""#v"") != null)
            total = _dict.get(word+""#v"") + total;
        return total;
    }

    public String classifytweet(){
        String[] words = twit.split(""\\s+""); 
        double totalScore = 0, averageScore;
        for(String word : words) {
            word = word.replaceAll(""([^a-zA-Z\\s])"", """");
            if (_sw.extract(word) == null)
                continue;
            totalScore += _sw.extract(word);
        }
        Double AverageScore = totalScore;

        if(averageScore&gt;=0.75)
            return ""very positive"";
        else if(averageScore &gt; 0.25 &amp;&amp; averageScore&lt;0.5)
            return  ""positive"";
        else if(averageScore&gt;=0.5)
            return  ""positive"";
        else if(averageScore &lt; 0 &amp;&amp; averageScore&gt;=-0.25)
            return ""negative"";
        else if(averageScore &lt; -0.25 &amp;&amp; averageScore&gt;=-0.5)
            return ""negative"";
        else if(averageScore&lt;=-0.75)
            return ""very negative"";
        return ""neutral"";
    }

    public static void main(String[] args) {
        // TODO Auto-generated method stub
    }
</code></pre>
","java, twitter, sentiment-analysis","<p>First of all start by deleting all the ""garbage"" at the first of the file (which includes description, instruction etc..)</p>

<p>One possible usage is to change <code>SWN3</code> an make the method <code>extract</code> in it return a <code>Double</code>:</p>

<pre><code>public Double extract(String word)
{
    Double total = new Double(0);
    if(_dict.get(word+""#n"") != null)
         total = _dict.get(word+""#n"") + total;
    if(_dict.get(word+""#a"") != null)
        total = _dict.get(word+""#a"") + total;
    if(_dict.get(word+""#r"") != null)
        total = _dict.get(word+""#r"") + total;
    if(_dict.get(word+""#v"") != null)
        total = _dict.get(word+""#v"") + total;
    return total;
}
</code></pre>

<p>Then, giving a String that you want to tag, you can split it so it'll have only words (with no signs and unknown chars) and using the result returned from <code>extract</code> method on each word, you can decide what is the average weight of the String:</p>

<pre><code>String[] words = twit.split(""\\s+""); 
double totalScore = 0, averageScore;
for(String word : words) {
    word = word.replaceAll(""([^a-zA-Z\\s])"", """");
    if (_sw.extract(word) == null)
        continue;
    totalScore += _sw.extract(word);
}
verageScore = totalScore;

if(averageScore&gt;=0.75)
    return ""very positive"";
else if(averageScore &gt; 0.25 &amp;&amp; averageScore&lt;0.5)
    return  ""positive"";
else if(averageScore&gt;=0.5)
    return  ""positive"";
else if(averageScore &lt; 0 &amp;&amp; averageScore&gt;=-0.25)
    return ""negative"";
else if(averageScore &lt; -0.25 &amp;&amp; averageScore&gt;=-0.5)
    return ""negative"";
else if(averageScore&lt;=-0.75)
    return ""very negative"";
return ""neutral"";
</code></pre>

<p>I found this way easier and it works fine for me.</p>

<hr>

<p><strong>UPDATE:</strong></p>

<p>I changed <code>_dict</code> to <code>_dict = new HashMap&lt;String, Double&gt;();</code> So it will have a <code>String</code> key and a <code>Double</code> value.</p>

<p>So I replaced <code>_dict.put(word, sent);</code> wish <code>_dict.put(word, score);</code></p>
",7,5,17822,2013-03-27 06:33:30,https://stackoverflow.com/questions/15653091/how-to-use-sentiwordnet
Emoticons in Twitter Sentiment Analysis in r,"<p>How do I handle/get rid of emoticons so that I can sort tweets for sentiment analysis?</p>

<p>Getting:
Error in sort.list(y) : 
  invalid input </p>

<p>Thanks</p>

<p>and this is how the emoticons come out looking from twitter and into r:</p>

<pre><code>\xed��\xed�\u0083\xed��\xed��
\xed��\xed�\u008d\xed��\xed�\u0089 
</code></pre>
","r, text-mining, iconv, sentiment-analysis","<p>This should get rid of the emoticons, using <code>iconv</code> as suggested by ndoogan.</p>

<p>Some reproducible data:</p>

<pre><code>require(twitteR) 
# note that I had to register my twitter credentials first
# here's the method: http://stackoverflow.com/q/9916283/1036500
s &lt;- searchTwitter('#emoticons', cainfo=""cacert.pem"") 

# convert to data frame
df &lt;- do.call(""rbind"", lapply(s, as.data.frame))

# inspect, yes there are some odd characters in row five
head(df)

                                                                                                                                                text
1                                                                      ROFLOL: echte #emoticons [humor] http://t.co/0d6fA7RJsY via @tweetsmania  ;-)
2 “@teeLARGE: when tmobile get the iphone in 2 wks im killin everybody w/ emoticons &amp;amp; \nall the other stuff i cant see on android!"" \n#Emoticons
3                      E poi ricevi dei messaggi del genere da tua mamma xD #crazymum #iloveyou #emoticons #aiutooo #bestlike http://t.co/Yee1LB9ZQa
4                                                #emoticons I want to change my name to an #emoticon. Is it too soon? #prince http://t.co/AgmR5Lnhrk
5  I use emoticons too much. #addicted #admittingit #emoticons &lt;ed&gt;&lt;U+00A0&gt;&lt;U+00BD&gt;&lt;ed&gt;&lt;U+00B8&gt;&lt;U+00AC&gt;&lt;ed&gt;&lt;U+00A0&gt;&lt;U+00BD&gt;&lt;ed&gt;&lt;U+00B8&gt;&lt;U+0081&gt; haha
6                                                                                         What you text What I see #Emoticons http://t.co/BKowBSLJ0s
</code></pre>

<p><strong>Here's the key line that will remove the emoticons:</strong></p>

<pre><code># Clean text to remove odd characters
df$text &lt;- sapply(df$text,function(row) iconv(row, ""latin1"", ""ASCII"", sub=""""))
</code></pre>

<p>Now inspect again, to see if the odd characters are gone (see row 5)</p>

<pre><code>head(df)    
                                                                                                                               text
1                                                                     ROFLOL: echte #emoticons [humor] http://t.co/0d6fA7RJsY via @tweetsmania  ;-)
2 @teeLARGE: when tmobile get the iphone in 2 wks im killin everybody w/ emoticons &amp;amp; \nall the other stuff i cant see on android!"" \n#Emoticons
3                     E poi ricevi dei messaggi del genere da tua mamma xD #crazymum #iloveyou #emoticons #aiutooo #bestlike http://t.co/Yee1LB9ZQa
4                                               #emoticons I want to change my name to an #emoticon. Is it too soon? #prince http://t.co/AgmR5Lnhrk
5                                                                                 I use emoticons too much. #addicted #admittingit #emoticons  haha
6                                                                                        What you text What I see #Emoticons http://t.co/BKowBSLJ0s
</code></pre>
",22,19,14892,2013-04-01 17:25:32,https://stackoverflow.com/questions/15748190/emoticons-in-twitter-sentiment-analysis-in-r
Getting incorrect Score using SentiWordNet,"<p>I'm doing some sentiment analysis using SentiWordNet and I referred to the post here <a href=""https://stackoverflow.com/questions/15653091/how-to-use-sentiwordnet"">How to use SentiWordNet</a> . However, I'm getting a score of 0.0 despite trying out various inputs. Is there anything I'm doing wrong here? Thanks!</p>

<pre><code>    import java.io.BufferedReader;
    import java.io.File;
    import java.io.FileReader;
    import java.util.HashMap;
    import java.util.Iterator;
    import java.util.Set;
    import java.util.Vector;

    public class SWN3 {
        private String pathToSWN = ""C:\\Users\\Malcolm\\Desktop\\SentiWordNet_3.0.0\\home\\swn\\www\\admin\\dump\\SentiWordNet_3.0.0.txt"";
        private HashMap&lt;String, Double&gt; _dict;

        public SWN3(){

            _dict = new HashMap&lt;String, Double&gt;();
            HashMap&lt;String, Vector&lt;Double&gt;&gt; _temp = new HashMap&lt;String, Vector&lt;Double&gt;&gt;();
            try{
                BufferedReader csv =  new BufferedReader(new FileReader(pathToSWN));
                String line = """";           
                while((line = csv.readLine()) != null)
                {
                    String[] data = line.split(""\t"");
                    Double score = Double.parseDouble(data[2])-Double.parseDouble(data[3]);
                    String[] words = data[4].split("" "");
                    for(String w:words)
                    {
                        String[] w_n = w.split(""#"");
                        w_n[0] += ""#""+data[0];
                        int index = Integer.parseInt(w_n[1])-1;
                        if(_temp.containsKey(w_n[0]))
                        {
                            Vector&lt;Double&gt; v = _temp.get(w_n[0]);
                            if(index&gt;v.size())
                                for(int i = v.size();i&lt;index; i++)
                                    v.add(0.0);
                            v.add(index, score);
                            _temp.put(w_n[0], v);
                        }
                        else
                        {
                            Vector&lt;Double&gt; v = new Vector&lt;Double&gt;();
                            for(int i = 0;i&lt;index; i++)
                                v.add(0.0);
                            v.add(index, score);
                            _temp.put(w_n[0], v);
                        }
                    }
                }
                Set&lt;String&gt; temp = _temp.keySet();
                for (Iterator&lt;String&gt; iterator = temp.iterator(); iterator.hasNext();) {
                    String word = (String) iterator.next();
                    Vector&lt;Double&gt; v = _temp.get(word);
                    double score = 0.0;
                    double sum = 0.0;
                    for(int i = 0; i &lt; v.size(); i++)
                        score += ((double)1/(double)(i+1))*v.get(i);
                    for(int i = 1; i&lt;=v.size(); i++)
                        sum += (double)1/(double)i;
                    score /= sum;
                    String sent = """";               
                    if(score&gt;=0.75)
                        sent = ""strong_positive"";
                    else
                    if(score &gt; 0.25 &amp;&amp; score&lt;=0.5)
                        sent = ""positive"";
                    else
                    if(score &gt; 0 &amp;&amp; score&gt;=0.25)
                        sent = ""weak_positive"";
                    else
                    if(score &lt; 0 &amp;&amp; score&gt;=-0.25)
                        sent = ""weak_negative"";
                    else
                    if(score &lt; -0.25 &amp;&amp; score&gt;=-0.5)
                        sent = ""negative"";
                    else
                    if(score&lt;=-0.75)
                        sent = ""strong_negative"";
                    _dict.put(word, score);
                }
            }
            catch(Exception e){e.printStackTrace();}        
        }

public Double extract(String word)
{
    Double total = new Double(0);
    if(_dict.get(word+""#n"") != null)
         total = _dict.get(word+""#n"") + total;
    if(_dict.get(word+""#a"") != null)
        total = _dict.get(word+""#a"") + total;
    if(_dict.get(word+""#r"") != null)
        total = _dict.get(word+""#r"") + total;
    if(_dict.get(word+""#v"") != null)
        total = _dict.get(word+""#v"") + total;
    return total;
}

public static void main(String[] args) {
    SWN3 test = new SWN3();
    String sentence=""Hello have a Super awesome great day"";
    String[] words = sentence.split(""\\s+""); 
    double totalScore = 0;
    for(String word : words) {
        word = word.replaceAll(""([^a-zA-Z\\s])"", """");
        if (test.extract(word) == null)
            continue;
        totalScore += test.extract(word);
    }
    System.out.println(totalScore);
}

}
</code></pre>

<p>Here's the first 10 lines of SentiWordNet.txt</p>

<pre><code>a   00001740    0.125   0   able#1  (usually followed by `to') having the necessary means or skill or know-how or authority to do something; ""able to swim""; ""she was able to program her computer""; ""we were at last able to buy a car""; ""able to get a grant for the project""
a   00002098    0   0.75    unable#1    (usually followed by `to') not having the necessary means or skill or know-how; ""unable to get to town without a car""; ""unable to obtain funds""
a   00002312    0   0   dorsal#2 abaxial#1  facing away from the axis of an organ or organism; ""the abaxial surface of a leaf is the underside or side facing away from the stem""
a   00002527    0   0   ventral#2 adaxial#1 nearest to or facing toward the axis of an organ or organism; ""the upper side of a leaf is known as the adaxial surface""
a   00002730    0   0   acroscopic#1    facing or on the side toward the apex
a   00002843    0   0   basiscopic#1    facing or on the side toward the base
a   00002956    0   0   abducting#1 abducent#1  especially of muscles; drawing away from the midline of the body or from an adjacent part
a   00003131    0   0   adductive#1 adducting#1 adducent#1  especially of muscles; bringing together or drawing toward the midline of the body or toward an adjacent part
a   00003356    0   0   nascent#1   being born or beginning; ""the nascent chicks""; ""a nascent insurgency""
a   00003553    0   0   emerging#2 emergent#2   coming into existence; ""an emergent republic""
</code></pre>
","java, twitter, sentiment-analysis","<p>Usually the <code>SentiWord.txt</code> file comes with a weird format.</p>

<p>You need to remove the first part of it (which includes comments and instructions) and the last two lines:</p>

<pre><code>#
EMPTY LINE
</code></pre>

<p>The parser doesn't know how to handle these situations, if you delete these extra two lines you'll be fine.</p>
",5,3,2410,2013-04-06 04:31:45,https://stackoverflow.com/questions/15847025/getting-incorrect-score-using-sentiwordnet
KNN classifier sentiment analysis vs category analysis precision,"<p>I have implemented the KNN classifier in java and I got a strange result. If I do a sentiment analysis on a dataset example amazon books review I got 55% precision. From 100 test document 55 correctly classified as negative or positive review and 45 incorrectly. But If I use the KNN for category classification example camera or books then I got 95% precision. </p>

<p>There are some explanation my code is wrong?  Any idea?</p>
","machine-learning, sentiment-analysis, document-classification","<p>@Christopher Pfohl is right. They are different approaches with one key difference for you. Sentiment analysis (based on simple Bag of Words) is much more complicated, in general, than category classification in your case.</p>

<p>Btw, just one clarification, 55% is not precision, that is accuracy.
(More info: <a href=""http://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification"" rel=""nofollow"">http://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification</a>)</p>
",3,0,1344,2013-04-23 16:35:03,https://stackoverflow.com/questions/16174656/knn-classifier-sentiment-analysis-vs-category-analysis-precision
Sentiment Analysis in Spanish - Dictionary,"<p>I'm interested in doing sentiment analysis in spanish. I've looked up in the web for an opinions dictionary and it seems imposible to find something similar as the existing ones in english. For example, <a href=""http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon"" rel=""nofollow"">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon</a></p>

<p>I'm new to the field and would be interested in some guidance, so I'm open to suggestions. How useful, for instance, would a standard Spanish dictionary be?</p>

<p>The object of study would be twitter posts related to politics.</p>

<p>Thanks for your help!</p>
",sentiment-analysis,"<p>I don't know if something like this already exists, but making one seems like it would definitely be a valuable thing to do for the community--though it might be a fair amount of work.</p>

<p>A standard Spanish dictionary probably won't really help you create a sentiment dictionary from scratch unless you're planning to manually assign sentiment values to a very large set of Spanish words. An English-to-Spanish dictionary might help you translate an English sentiment dictionary into a Spanish one, which, if nothing else, would probably be a good start, though it would be woefully lacking in its lack of common idioms, misspellings, and so forth.</p>

<p>One way that you <em>could</em> try using a standard Spanish dictionary would be to take the ""starting point"" you get from the above translation process and apply it to the <em>definitions</em> of Spanish words and phrases for which you haven't yet assigned a sentiment value; this would give you an easy way to extend your sentiment dictionary, though it might not be very accurate.</p>

<p>Good luck!</p>
",6,3,3087,2013-05-04 02:54:54,https://stackoverflow.com/questions/16370121/sentiment-analysis-in-spanish-dictionary
Sentiment analysis for Twitter in Python,"<p>I'm looking for an open source implementation, preferably in python, of <strong>Textual Sentiment Analysis</strong> (<a href=""http://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Sentiment_analysis</a>). Is anyone familiar with such open source implementation I can use?</p>

<p>I'm writing an application that searches twitter for some search term, say ""youtube"", and counts ""happy"" tweets vs. ""sad"" tweets. 
I'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.
I haven't been able to find such sentiment analyzer so far, specifically not in python. 
Are you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python.</p>

<p>Note, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts.</p>

<p>BTW, twitter does support the "":)"" and "":("" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself.</p>

<p>Thanks!</p>

<p>BTW, an early demo is <a href=""http://twitgraph.appspot.com/?show_inputs=1&amp;duration=30&amp;q=youtube+annotations"" rel=""noreferrer"">here</a> and the code I have so far is <a href=""http://code.google.com/p/twitgraph/"" rel=""noreferrer"">here</a> and I'd love to opensource it with any interested developer.</p>
","python, machine-learning, nlp, open-source, sentiment-analysis","<p>With most of these kinds of applications, you'll have to roll much of your own code for a statistical classification task. As Lucka suggested, NLTK is the perfect tool for natural language manipulation in Python, so long as your goal doesn't interfere with the non commercial nature of its license.  However, I would suggest other software packages for modeling.  I haven't found many strong advanced machine learning models available for Python, so I'm going to suggest some standalone binaries that easily cooperate with it.</p>

<p>You may be interested in <a href=""http://tadm.sf.net"" rel=""noreferrer"">The Toolkit for Advanced Discriminative Modeling</a>, which can be easily interfaced with Python.  This has been used for classification tasks in various areas of natural language processing.  You also have a pick of a number of different models.  I'd suggest starting with Maximum Entropy classification so long as you're already familiar with implementing a Naive Bayes classifier.  If not, you may want to look into it and code one up to really get a decent understanding of statistical classification as a machine learning task.</p>

<p>The University of Texas at Austin computational linguistics groups have held classes where most of the projects coming out of them have used this great tool.  You can look at the course page for <a href=""http://comp.ling.utexas.edu/jbaldrid/courses/2006/cl2/"" rel=""noreferrer"">Computational Linguistics II</a> to get an idea of how to make it work and what previous applications it has served.</p>

<p>Another great tool which works in the same vein is <a href=""http://mallet.cs.umass.edu/"" rel=""noreferrer"">Mallet</a>.  The difference between Mallet is that there's a bit more documentation and some more models available, such as decision trees, and it's in Java, which, in my opinion, makes it a little slower.  <a href=""http://www.cs.waikato.ac.nz/ml/weka/"" rel=""noreferrer"">Weka</a> is a whole suite of different machine learning models in one big package that includes some graphical stuff, but it's really mostly meant for pedagogical purposes, and isn't really something I'd put into production.</p>

<p>Good luck with your task.  The real difficult part will probably be the amount of knowledge engineering required up front for you to classify the 'seed set' off of which your model will learn.  It needs to be pretty sizeable, depending on whether you're doing binary classification (happy vs sad) or a whole range of emotions (which will require even more).  Make sure to hold out some of this engineered data for testing, or run some tenfold or remove-one tests to make sure you're actually doing a good job predicting before you put it out there. And most of all, have fun!  This is the best part of NLP and AI, in my opinion.</p>
",44,88,52227,2009-02-21 21:20:24,https://stackoverflow.com/questions/573768/sentiment-analysis-for-twitter-in-python
Seed data for sentiment analysis,"<p>I'm playing around with sentiment analysis, and I'm looking for some seed data.  Is there a free dictionary around?</p>

<p>It can be really simple: 3 sets of texts/sentences, for ""positive"", ""negative"", ""neutral"".  It doesn't have to be huge.</p>

<p>Eventually I'll probably generate my own seed data for my specific use case, but it would be great to have something to play with now while I'm building the thing.</p>
","dictionary, sentiment-analysis","<p><a href=""http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"" rel=""nofollow noreferrer"">Bing Liu and Minqing Hu</a> from UIC have a number of datasets:</p>

<ul>
<li><a href=""http://www.cs.uic.edu/~liub/FBS/CustomerReviewData.zip"" rel=""nofollow noreferrer"">http://www.cs.uic.edu/~liub/FBS/CustomerReviewData.zip</a></li>
<li><a href=""http://www.cs.uic.edu/~liub/FBS/Reviews-9-products.rar"" rel=""nofollow noreferrer"">http://www.cs.uic.edu/~liub/FBS/Reviews-9-products.rar</a></li>
</ul>

<p><a href=""http://www.cs.cornell.edu/people/pabo/movie-review-data/"" rel=""nofollow noreferrer"">Bo Pang</a> from Cornell has some more.</p>
",4,10,5237,2009-07-28 19:07:34,https://stackoverflow.com/questions/1196133/seed-data-for-sentiment-analysis
Unsupervised Sentiment Analysis,"<p>I've been reading a lot of articles that explain the need for an initial set of texts that are classified as either 'positive' or 'negative' before a sentiment analysis system will really work.</p>

<p>My question is: Has anyone attempted just doing a rudimentary check of 'positive' adjectives vs 'negative' adjectives, taking into account any simple negators to avoid classing 'not happy' as positive? If so, are there any articles that discuss just why this strategy isn't realistic?</p>
","machine-learning, nlp, sentiment-analysis","<p>A <a href=""https://www.aclweb.org/anthology/P02-1053.pdf"" rel=""nofollow noreferrer"">classic paper by Peter Turney</a> (2002) explains a method to do unsupervised sentiment analysis (positive/negative classification) using only the words <em>excellent</em> and <em>poor</em> as a seed set. Turney uses the <a href=""http://en.wikipedia.org/wiki/Pointwise_mutual_information"" rel=""nofollow noreferrer"">mutual information</a> of other words with these two adjectives to achieve an accuracy of 74%.</p>
",71,48,33355,2010-10-13 04:25:42,https://stackoverflow.com/questions/3920759/unsupervised-sentiment-analysis
Perl or Java Sentiment Analysis,"<p>I was wondering if anybody knew of any good Perl modules and/or Java classes for sentiment analysis.  I have read about LingPipe, but the program would eventually need to be used for commercial use so something open-source would be better.  I also looked into GATE, but their documentation on sentiment analysis is sparse at best.</p>
","java, perl, nlp, sentiment-analysis","<p>Have a look at <a href=""http://search.cpan.org/perldoc?Rate_Sentiment"" rel=""noreferrer"">Rate_Sentiment</a> in the <a href=""http://search.cpan.org/dist/WebService-GoogleHack/"" rel=""noreferrer"">WebService::GoogleHack</a> module at <a href=""http://search.cpan.org/"" rel=""noreferrer"">CPAN</a>. There's more information about the project at <a href=""http://google-hack.sourceforge.net/"" rel=""noreferrer"">SourceForge</a>.</p>
",7,13,3950,2010-11-15 14:20:24,https://stackoverflow.com/questions/4185199/perl-or-java-sentiment-analysis
Best Algorithmic Approach to Sentiment Analysis,"<p>My requirement is taking in news articles and determining if they are positive or negative about a subject.  I am taking the approach outlined below, but I keep reading NLP may be of use here.  All that I have read has pointed at NLP detecting opinion from fact, which I don't think would matter much in my case. I'm wondering two things:</p>

<p>1)  Why wouldn't my algorithm work and/or how can I improve it? ( I know sarcasm would probably be a pitfall, but again I don't see that occurring much in the type of news we will be getting)</p>

<p>2)  How would NLP help, why should I use it?</p>

<p>My algorithmic approach (I have dictionaries of positive, negative, and negation words):</p>

<p>1) Count number of positive and negative words in article</p>

<p>2) If a negation word is found with 2 or 3 words of the positive or negative word, (ie: NOT the best) negate the score.</p>

<p>3) Multiply the scores by weights that have been manually assigned to each word. (1.0 to start)</p>

<p>4) Add up the totals for positive and negative to get the sentiment score.</p>
","nlp, sentiment-analysis","<p>I don't think there's anything particularly <strong><em>wrong</em></strong> with your algorithm, it's a fairly straightforward and practical way to go, but there are a lot of situations where it will get make mistakes.</p>

<ol>
<li><p><strong>Ambiguous sentiment words</strong> - ""This product works terribly"" vs. ""This product is terribly good""</p></li>
<li><p><strong>Missed negations</strong> - ""I would never in a millions years say that this product is worth buying""</p></li>
<li><p><strong>Quoted/Indirect text</strong> - ""My dad says this product is terrible, but I disagree""</p></li>
<li><p><strong>Comparisons</strong> - ""This product is about as useful as a hole in the head""</p></li>
<li><p><strong>Anything subtle</strong> - ""This product is ugly, slow and uninspiring, but it's the only thing on the market that does the job""</p></li>
</ol>

<p>I'm using product reviews for examples instead of news stories, but you get the idea. In fact, news articles are probably harder because they will often try to show both sides of an argument and tend to use a certain style to convey a point. The final example is quite common in opinion pieces, for example.</p>

<p>As far as NLP helping you with any of this, <a href=""http://en.wikipedia.org/wiki/Word_sense_disambiguation"" rel=""noreferrer"">word sense disambiguation</a> (or even just <a href=""http://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""noreferrer"">part-of-speech tagging</a>) may help with (1), <a href=""http://en.wikipedia.org/wiki/Statistical_parsing"" rel=""noreferrer"">syntactic parsing</a> might help with the long range dependencies in (2), some kind of <a href=""http://en.wikipedia.org/wiki/Shallow_parsing"" rel=""noreferrer"">chunking</a> might help with (3). It's all research level work though, there's nothing that I know of that you can directly use. Issues (4) and (5) are a lot harder, I throw up my hands and give up at this point.</p>

<p>I'd stick with the approach you have and look at the output carefully to see if it is doing what you want. Of course that then raises the issue of what you want you understand the definition of ""sentiment"" to be in the first place...</p>
",33,26,18114,2010-11-16 21:57:18,https://stackoverflow.com/questions/4199441/best-algorithmic-approach-to-sentiment-analysis
1 million sentences to save in DB - removing non-relevant English words,"<p>I am trying to train a Naive Bayes classifier with positive/negative words extracting from a sentiment. example:  </p>

<p>I love this movie :))  </p>

<p>I hate when it rains :(  </p>

<p>The idea is I extract positive or negative sentences based on the emoctions used, but in order to train a classifier and persist it into database.  </p>

<p>The problem is that I have more than 1 million such sentences, so if I train it word by word, the database will go for a toss. I want to remove all non-relevant word example 'I','this', 'when', 'it' so that number of times I have to make a database query is less.  </p>

<p>Please help me in resolving this issue to suggest me better ways of doing it  </p>

<p>Thank you</p>
","database, hadoop, nlp, classification, sentiment-analysis","<p>You might want to check this out
<a href=""http://books.google.com/books?id=CE1QzecoVf4C&amp;lpg=PA390&amp;ots=OHuYwLRhag&amp;dq=sentiment%20%20mining%20for%20fortune%20500&amp;pg=PA379#v=onepage&amp;q=sentiment%20%20mining%20for%20fortune%20500&amp;f=false"" rel=""nofollow"">http://books.google.com/books?id=CE1QzecoVf4C&amp;lpg=PA390&amp;ots=OHuYwLRhag&amp;dq=sentiment%20%20mining%20for%20fortune%20500&amp;pg=PA379#v=onepage&amp;q=sentiment%20%20mining%20for%20fortune%20500&amp;f=false</a></p>
",4,6,944,2010-11-23 17:39:32,https://stackoverflow.com/questions/4259044/1-million-sentences-to-save-in-db-removing-non-relevant-english-words
A Social Network with Artificial Intelligence,"<p>I'm planning a Social Network with an Artificial Intelligence . that means that the SN will take the conversations (in english) .. and analyse them in order to extract the general opinion about a subject. This helps to collect information and build statistics, which will be sent then to the appropriate user.
My question is : how to organize words and grammatical rules in a database, in order to help the social network extracting a general opinion from a conversation (agree, disagree..etc)! 
thank you.</p>
","social-networking, sentiment-analysis","<p>its very challengning task to extract text that reflects real opnions of people because machine learning is still not advanced in this area, take a look at the following example:
your program finds this sentence on some SN website:
""I made her duck""</p>

<pre><code>what are possible options that your program will infer:
1-i cooked a duck for her
2-i ""magically"" transformed her into a duck""
3-i took her duck and created it.
and many other
</code></pre>

<p>so its really challenge. I suggest to take a look at NLP(natural language processing) which covers this area you're interested in</p>
",1,1,998,2010-11-24 23:54:42,https://stackoverflow.com/questions/4272625/a-social-network-with-artificial-intelligence
What are the most challenging issues in Sentiment Analysis(opinion mining)?,"<p>Opinion Mining/Sentiment Analysis is a somewhat recent subtask of Natural Language processing.Some compare it to text classification,some take a more deep stance towards it. What do you think about the most challenging issues in Sentiment Analysis(opinion mining)? Can you name a few?</p>
","nlp, sentiment-analysis","<p>The key challenges for sentiment analysis are:-</p>

<p>1) Named Entity Recognition - What is the person actually talking about, e.g. is 300 Spartans a group of Greeks or a movie?</p>

<p>2) Anaphora Resolution - the problem of resolving what a pronoun, or a noun phrase refers to.  ""We watched the movie and went to dinner; it was awful.""  What does ""It"" refer to?</p>

<p>3) Parsing - What is the subject and object of the sentence, which one does the verb and/or adjective actually refer to?</p>

<p>4) Sarcasm - If you don't know the author you have no idea whether 'bad' means bad or good.</p>

<p>5) Twitter - abbreviations, lack of capitals, poor spelling, poor punctuation, poor grammar, ... </p>
",19,1,10285,2011-01-26 15:15:34,https://stackoverflow.com/questions/4806176/what-are-the-most-challenging-issues-in-sentiment-analysisopinion-mining
Is there a Sentiment Analysis Script available in open source?,"<p>I am looking for a sentiment analysis script / soyurce code preferably in PHP. Do you know of any such script?
Thanks, Sameer</p>
","php, sentiment-analysis","<p>PHP/ir is a great blog for this kind of stuff, and has a post on a <a href=""http://www.phpir.com/bayesian-opinion-mining"" rel=""noreferrer"">basic bayesian sentiment classifier</a>.</p>
",7,4,8104,2011-03-03 06:01:37,https://stackoverflow.com/questions/5177246/is-there-a-sentiment-analysis-script-available-in-open-source
question on sentiment analysis,"<p>I have a question regarding sentiment analysis that i need help with.</p>

<p>Right now, I have a bunch of tweets I've gathered through the twitter search api. Because I used my search terms, I know what are the subjects or entities (Person names) that I want to look at. I want to know how others feel about these people.</p>

<p>For starters, I downloaded a list of english words with known valence/sentiment score and calculate the sentiments (+/-) based on availability of these words in the tweet. The problem is that sentiments calculated this way - I'm actually looking more at the tone of the tweet rather than ABOUT the person.</p>

<p>For instance, I have this tweet:</p>

<blockquote>
<pre><code>""lol... Person A is a joke. lmao!""
</code></pre>
</blockquote>

<p>The message is obviously in a positive tone, but person A should get a negative. </p>

<p>To improve my sentiment analysis, I can probably take into account negation and modifiers from my word list. But how exactly can I get my sentiments analysis to look at the subject of the message (and possibly sarcasm) instead? </p>

<p>It would be great if someone can direct me towards some resources....</p>
","python, twitter, machine-learning, sentiment-analysis","<p>While awaiting for answers from researchers in AI field I will give you some clues on what you can do quickly. </p>

<p>Even though this topic requires knowledge from natural language processing, machine learning and even psychology, you don't have to start from scratch unless you're desperate or have no trust in the quality of research going on in the field.</p>

<p>One possible approach to sentiment analysis would be to treat it as a supervised learning problem, where you have some small training corpus that includes human made annotations  (later about that) and a testing corpus on which you test how well you approach/system is performing. For training you will need some classifiers, like SVM, HMM or some others, but keep it simple. I would start from binary classification: good, bad. You could do the same for a continuous spectrum of opinion ranges, from positive to negative, that is to get a ranking, like google, where the most valuable results come on top.</p>

<p>For a start check <a href=""http://www.csie.ntu.edu.tw/~cjlin/libsvm/"" rel=""nofollow"">libsvm classifier</a>, it is capable of doing both classification {good, bad} and regression (ranking).
The quality of annotations will have a massive influence on the results you get, but where to get it from?</p>

<p>I found <a href=""http://people.csail.mit.edu/bsnyder/naacl07/"" rel=""nofollow"">one project about sentiment analysis</a> that deals with restaurants. There is both data and code, so you can see how they extracted features from natural language and which features scored high in the classification or regression.
The corpus consists of opinions of customers about restaurants they recently visited and gave some feedback about the food, service or atmosphere. 
The connection about their opinions and numerical world is expressed in terms of numbers of stars they gave to the restaurant. You have natural language on one site and restaurant's rate on another.</p>

<p>Looking at this example you can devise your own approach for the problem stated. 
Take a look at <a href=""http://www.nltk.org/"" rel=""nofollow"">nltk</a> as well. With nltk you can do part of speech tagging and with some luck get names as well. Having done that you can add a feature to your classifier that will assign a score to a name if within n words (skip n-gram) there are words expressing opinions (look at the restaurant corpus) or use weights you already have, but it's best to rely on a classfier to learn weights, that's his job.</p>
",5,3,2102,2011-04-21 07:52:13,https://stackoverflow.com/questions/5741135/question-on-sentiment-analysis
"Sentiment analysis api/tool, for Java","<p>I'm writing a Java program and need to analyze small chunks of text (3-4 sentences, news articles paraphrased) for their sentiment. I just need to know whether the article is generally positive, negative or neutral.</p>

<p>For example, the following would ideally be classed as positive:</p>

<blockquote>
  <p>Kindle e-book sales soar for Amazon.
  Amazon.com says it is selling more
  e-books for its Kindle electronic
  reading device than paperback and
  hardback print editions combined</p>
</blockquote>

<p>All I need is a very simple and quick to implement third party solution, that I can use in my program. It does not have to be totally accurate all the time. Licenses etc. are not an issue, so long as it is possible to trail the solution.</p>

<p>So far I have found a potential good solution, <a href=""http://www.alchemyapi.com/api/sentiment/"" rel=""noreferrer"">AlchemyAPI</a>, but am struggling to actually use it.</p>

<p><br /></p>

<p>If anyone has encountered this problem before and knows of a particularly good/easy solution, or of a really good tutorial, I would be very grateful :-)</p>

<p><br /></p>

<p>(Also I apologize for the lack of code in this question.)</p>
","java, sentiment-analysis","<p>i just tested AlchemyAPI. it's not 100% accurate but i guess this sort of technology is still in its infancy.</p>

<p>you will need to register (free) to get an api key.</p>

<p>here's a sample usage: <code>http://access.alchemyapi.com/calls/text/TextGetTextSentiment?apikey=&lt;insert your api key&gt;&amp;sentiment=1&amp;showSourceText=1&amp;text=Kindle%20e-book%20sales%20soar%20for%20Amazon.%20Amazon.com%20says%20it%20is%20selling%20more%20e-books%20for%20its%20Kindle%20electronic%20reading%20device%20than%20paperback%20and%20hardback%20print%20editions%20combined</code></p>

<p>the inputs are:</p>

<ol>
<li>sentiment=1</li>
<li>showSourceText=1</li>
<li>text (i used your sample text, uri encoded)</li>
</ol>

<p>i got the following output (neutral sentiment, instead of the expected positive sentiment):</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;results&gt;    
    &lt;status&gt;OK&lt;/status&gt;    
    &lt;usage&gt;By accessing AlchemyAPI or using information generated by AlchemyAPI, you are agreeing to be bound by the AlchemyAPI Terms of Use: http://www.alchemyapi.com/company/terms.html&lt;/usage&gt;    
    &lt;url&gt;&lt;/url&gt;    
    &lt;language&gt;english&lt;/language&gt;    
    &lt;text&gt;Kindle e-book sales soar for Amazon. Amazon.com says it is selling more e-books for its Kindle electronic reading device than paperback and hardback print editions combined&lt;/text&gt;    
    &lt;docSentiment&gt;    
        &lt;type&gt;neutral&lt;/type&gt;    
    &lt;/docSentiment&gt;    
&lt;/results&gt;
</code></pre>

<p><br>
another sample usage: <code>http://access.alchemyapi.com/calls/text/TextGetTextSentiment?apikey=&lt;insert your api key&gt;&amp;sentiment=1&amp;showSourceText=1&amp;text=kindle%20is%20amazing</code></p>

<p>and the output:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;    
&lt;results&gt;    
    &lt;status&gt;OK&lt;/status&gt;    
    &lt;usage&gt;By accessing AlchemyAPI or using information generated by AlchemyAPI, you are agreeing to be bound by the AlchemyAPI Terms of Use: http://www.alchemyapi.com/company/terms.html&lt;/usage&gt;    
    &lt;url&gt;&lt;/url&gt;    
    &lt;language&gt;english&lt;/language&gt;    
    &lt;text&gt;kindle is amazing&lt;/text&gt;    
    &lt;docSentiment&gt;    
        &lt;type&gt;positive&lt;/type&gt;    
        &lt;score&gt;0.283568&lt;/score&gt;    
    &lt;/docSentiment&gt;    
&lt;/results&gt;
</code></pre>
",15,7,9128,2011-05-20 14:05:43,https://stackoverflow.com/questions/6073109/sentiment-analysis-api-tool-for-java
Looking up positivity or negativity in a sentence using WordNet,"<p>I have to ask some questions in an interactive system and user is free to type just anything as response. I have to conclude from response whether it is positive (yes) or negative (no). In some presentation that I attended few months ago I understood that a specific library of wordnet (not wordnet itself) exists that does the exact same job but I have forgot what it was. Does anyone know about it? If not can you suggest some alternative to attain the same result?</p>

<p>Thanks in advance</p>
","c#, nlp, wordnet, sentiment-analysis","<p>Nevermind I recalled it now. It was <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""nofollow"">SentiWordnet</a>.</p>
",3,0,1196,2011-05-25 13:54:20,https://stackoverflow.com/questions/6125584/looking-up-positivity-or-negativity-in-a-sentence-using-wordnet
Simple toolkits for emotion (sentiment) analysis (not using machine learning),"<p>I am looking for a tool that can analyze the emotion of short texts. I searched for a week and I couldn't find a good one that is publicly available. The ideal tool is one that takes a short text as input and guesses the emotion. It is preferably a standalone application or library.</p>

<p>I don't need tools that is trained by texts. And although similar questions are asked before no satisfactory answers are got.</p>

<p>I searched the Internet and read some papers but I can't find a good tool I want. Currently I found SentiStrength, but the accuracy is not good. I am using emotional dictionaries right now. I felt that some syntax parsing may be necessary but it's too complex for me to build one. Furthermore, it's researched by some people and I don't want to reinvent the wheels. Does anyone know such publicly/research available software? I need a tool that doesn't need training before using.
Thanks in advance.</p>
","nlp, sentiment-analysis","<p>Maybe <a href=""http://dtminredis.housing.salle.url.edu:8080/EmoLib/"" rel=""nofollow"">EmoLib</a> could be of help.</p>
",-1,4,1913,2011-06-27 18:18:06,https://stackoverflow.com/questions/6497152/simple-toolkits-for-emotion-sentiment-analysis-not-using-machine-learning
How do I make an API call to viralheat on submit and then parse and save the JSON response?,"<p>I want to send a request via Viralheat's API in my controller's update method so that when a user hits the submit button, an action is completed and the API call is made. I want to post to http://www.viralheat.com/api/sentiment/review.json?text=i&amp;do&amp;not&amp;like&amp;this&amp;api_key=[<strong>* your api key *</strong>]</p>

<p>This will return some JSON in the format:</p>

<pre><code>{""mood"":""negative"",""prob"":0.773171679917001,""text"":""i do not like this""}
</code></pre>

<p>Is it possible to make that API call simultaneously while executing the controller method and how would I handle the JSON response? Which controller method would I put it in?</p>

<p>Ultimately I'd like to save the response mood to my sentiment column in a BrandUsers table. Submit is in main.html.erb which then uses the update method.</p>

<p><strong>Controller</strong></p>

<pre><code>def update
  @brand = Brand.find(params[:id])
  current_user.tag(@brand, :with =&gt; params[:brand][:tag_list], :on =&gt; :tags)
  if @brand.update_attributes(params[:brand])
    redirect_to :root, :notice  =&gt; ""Brand tagged.""
  else
    render :action =&gt; 'edit'
  end
end

def main
  @brands = Brand.all
  if current_user
    @brand = current_user.brands.not_tagged_by_user(current_user).order(""RANDOM()"").first
end
</code></pre>
","ruby-on-rails, ruby-on-rails-3, json, api, sentiment-analysis","<p>With the <code>wrest</code> gem installed, you could do something like</p>

<pre><code>params[:api_key] = ""your key""

url = ""http://www.viralheat.com/api/sentiment/review.json""

response = url.to_uri.get(params).deserialize
</code></pre>

<p><code>response</code> would contain the json already turned into a hash. So you can access the mood with</p>

<pre><code>response[:mood]
</code></pre>
",2,1,657,2011-08-31 12:06:12,https://stackoverflow.com/questions/7256686/how-do-i-make-an-api-call-to-viralheat-on-submit-and-then-parse-and-save-the-jso
Sentiment Analysis with ruby,"<p>Any one with sentient analysis experience with <code>liblinear</code> algorithm. Any one have used <a href=""https://github.com/tomz/liblinear-ruby-swig"" rel=""nofollow"">liblinear-ruby-swig</a> gem?</p>

<p>Please suggest me something to start with.</p>
","ruby-on-rails, ruby, sentiment-analysis","<p>I used lib linear a lot for other classification not for sentiment analysis 
Are you interested in using lib linear or to do sentiment analysis? 
For simple sentiment analysis look at
<a href=""https://chrismaclellan.com/blog/sentiment-analysis-of-tweets-using-ruby"" rel=""nofollow noreferrer"">https://chrismaclellan.com/blog/sentiment-analysis-of-tweets-using-ruby</a></p>
",4,3,3793,2011-09-13 10:26:30,https://stackoverflow.com/questions/7400333/sentiment-analysis-with-ruby
Training data for sentiment analysis,"<p>Where can I get a corpus of documents that have already been classified as positive/negative for sentiment in the corporate domain? I want a large corpus of documents that provide reviews for companies, like reviews of companies provided by analysts and media.</p>

<p>I find corpora that have reviews of products and movies. Is there a corpus for the business domain including reviews of companies, that match the language of business?</p>
","nlp, machine-learning, text-analysis, sentiment-analysis, training-data","<p><a href=""http://www.cs.cornell.edu/home/llee/data/"" rel=""noreferrer"">http://www.cs.cornell.edu/home/llee/data/</a></p>

<p><a href=""http://mpqa.cs.pitt.edu/corpora/mpqa_corpus"" rel=""noreferrer"">http://mpqa.cs.pitt.edu/corpora/mpqa_corpus</a></p>

<p>You can use twitter, with its smileys, like this: <a href=""http://web.archive.org/web/20111119181304/http://deepthoughtinc.com/wp-content/uploads/2011/01/Twitter-as-a-Corpus-for-Sentiment-Analysis-and-Opinion-Mining.pdf"" rel=""noreferrer"">http://web.archive.org/web/20111119181304/http://deepthoughtinc.com/wp-content/uploads/2011/01/Twitter-as-a-Corpus-for-Sentiment-Analysis-and-Opinion-Mining.pdf</a></p>

<p>Hope that gets you started.  There's more in the literature, if you're interested in specific subtasks like negation, sentiment scope, etc.</p>

<p>To get a focus on companies, you might pair a method with topic detection, or cheaply just a lot of mentions of a given company.  Or you could get your data annotated by Mechanical Turkers.</p>
",37,57,42908,2011-09-26 06:18:54,https://stackoverflow.com/questions/7551262/training-data-for-sentiment-analysis
Sentimental analysis using apache mahout,"<p>I am planning to develop a system that would predict the mood of a given text(sentiment analysis in short).</p>

<p>I would also prefer apache mahout because, it is seriously huge data and the system should be scalable realtime. Kindly suggest me algorithms that apache mahout provides, which will be suitable for sentiment analysis.</p>
","machine-learning, classification, mahout, sentiment-analysis","<p>If you have labeled training data then you could try <a href=""http://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""nofollow"">Naive Bayes classifier</a> which is one of the simplest supervised learning algorithms out there (and is supported by Mahout). If that is not sufficient for some reason then you could try more involved algorithms such as logistic regression etc.</p>

<p>If you don't have labeled data then you are out of luck - you will need to get some for this to work (e.g. by hiring someone to label your data for you via <a href=""https://www.mturk.com/mturk/welcome"" rel=""nofollow"">Amazon's Mechanical Turk</a>)</p>

<p>By the way, what size of the data are we talking about? (if it is is up to a few hundred of gigabytes then you don't need hadoop/mahout to train this type of models - unless you have that data sitting in hadoop already of course..)</p>
",3,3,2780,2011-12-09 13:12:06,https://stackoverflow.com/questions/8445956/sentimental-analysis-using-apache-mahout
Customer support data sets for e-mail sentiment analysis,"<p>I am looking for an annotated data set in the customer support domain for a sentiment analysis, to train my Naive Bayes Classifier. Are there any such data sets available on the internet? I am unable to find any so far.</p>

<p>How do I go about this.</p>
","machine-learning, sentiment-analysis","<p>With this being a 6-month old question, this response is probably only helpful to people who stumble upon this question.  You may want to consider using the data sets from the following page:</p>

<p><a href=""http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"" rel=""noreferrer"">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a></p>

<p>The polarity 2.0 data set from the following page is also frequently used:</p>

<p><a href=""http://www.cs.cornell.edu/people/pabo/movie-review-data/"" rel=""noreferrer"">http://www.cs.cornell.edu/people/pabo/movie-review-data/</a></p>
",5,3,3485,2011-12-13 13:21:15,https://stackoverflow.com/questions/8489956/customer-support-data-sets-for-e-mail-sentiment-analysis
Weighted Naive Bayes Classifier in Apache Mahout,"<p>I am using Naive Bayes classifier for my sentiment analysis on customer support. But unfortunately I don't have huge annotated data sets in the customer support domain. But I have a little amount of annotated data in the same domain(around 100 positive and 100 negative). I have the amazon product review data set as well.</p>

<p>Is there anyway can I implement a weighted naive bayes classifier using mahout, so that I can give more weight to the small set of customer support data and small weight to the amazon product review data. A training on the above weighted data set would drastically improve accuracy I guess. Kindly help me with the same.</p>
","machine-learning, sentiment-analysis, mahout, naivebayes","<p>One really simple approach is oversampling. Ie just repeat the customer support examples in your training data multiple times.</p>

<p>Though it's not the same problem you might get some further ideas by looking into the approaches used for class imbalance; in particular oversampling (as mentioned) and undersampling.</p>
",1,2,1016,2011-12-20 13:33:01,https://stackoverflow.com/questions/8576267/weighted-naive-bayes-classifier-in-apache-mahout
How I can start building wordnet for Turkish language to use in sentiment analysis,"<p>Although I hold EE background, I didn't get chance to attend Natural Language processing classes. </p>

<p>I would like to build sentiment analysis tool for Turkish language. I think it is best to create a Turkish wordnet database rather than translating the text to English and analyze it with buggy translated text with provided tools. (<strong>is it?</strong>)</p>

<p>So what do you guys recommend me to do ? First of all taking NLP classes from an open class website? I really don't know where to start. Could you help me and maybe provide me step by  step guide? I know this is an academic project but I am interested to build skills as a hobby in that area.</p>

<p>Thanks in advance.</p>
","wordnet, sentiment-analysis","<p>Here is the process I have used before (making Japanese, Chinese, German and Arabic semantic networks):</p>

<ol>
<li>Gather at least two English/Turkish dictionaries. They must be independent, not derived from each other. You can use Wikipedia to auto-generate one of your dictionaries. If you need to publish your network, then you may need open source dictionaries, or license fees, or a lawyer.</li>
<li>Use those dictionaries to translate English Wordnet, producing a confidence rating for each synset.</li>
<li>Keep those with strong confidence, manually approving or fixing through those with medium or low confidence.</li>
<li>Finish it off manually</li>
</ol>

<p>I expanded on this in the ""Automatic Translation Of WordNet"" section of my 2008 paper: <a href=""http://dcook.org/mlsn/about/papers/nlp2008.MLSN_A_Multilingual_Semantic_Network.pdf"" rel=""noreferrer"">http://dcook.org/mlsn/about/papers/nlp2008.MLSN_A_Multilingual_Semantic_Network.pdf</a></p>

<p>(For your stated goal of a Turkish sentiment dictionary, there are other approaches, not involving a semantic network. E.g. ""Semantic Analysis and Opinion Mining"", by Bing Liu, is a good round-up of research. But a semantic network approach will, IMHO, always give better results in the long run, and has so many other uses.)</p>
",6,8,2297,2011-12-27 05:33:34,https://stackoverflow.com/questions/8641503/how-i-can-start-building-wordnet-for-turkish-language-to-use-in-sentiment-analys
Naive Bayes Identifying Neutrals PHP,"<p>I am using Ian Barbers Naive Bayes analysis class to analyze the sentiment of sentences for a school project. I have created my own datasets of positive neutral and negatives. My problem is I have no clue on how to implement the neutrals and get the class to find them. The link below is for the php class I am using</p>

<p><a href=""http://phpir.com/bayesian-opinion-mining"" rel=""nofollow"">http://phpir.com/bayesian-opinion-mining</a></p>
","php, sentiment-analysis","<p>Well the <code>Opinion</code> class is already pretty flexible for adding new ""sentiment classes"". Just the <code>classify</code> method has implemented the calculation of ""prior"" static. But it can be easily replaced with a <code>foreach</code>:</p>

<pre><code>private $classes = array('pos', 'neg', 'neutr');
private $classTokCounts = array('pos' =&gt; 0, 'neg' =&gt; 0, 'neutr' =&gt; 0);
private $classDocCounts = array('pos' =&gt; 0, 'neg' =&gt; 0, 'neutr' =&gt; 0);
private $prior = array('pos' =&gt; 1.0/3.0, 'neg' =&gt; 1.0/3.0, 'neutr' =&gt; 1.0/3.0);

public function classify($document) {
    // remove those:
    //$this-&gt;prior['pos'] = $this-&gt;classDocCounts['pos'] / $this-&gt;docCount;
    //$this-&gt;prior['neg'] = $this-&gt;classDocCounts['neg'] / $this-&gt;docCount;
    // add this:
    foreach($this-&gt;classes as $class) {
        $this-&gt;prior[$class] = $this-&gt;classDocCounts[$class] / $this-&gt;docCount;
    }

    // the rest is fine
</code></pre>
",2,0,1160,2011-12-30 05:51:24,https://stackoverflow.com/questions/8676589/naive-bayes-identifying-neutrals-php
Training data size for a bayesian classifier,"<p>I am using apache mahout for performing sentiment analysis in the customer support domain. Since I am not able to get a proper training data set, I made my own. Now I have 100 support mails for positive sentiment and 100 for negative.</p>

<p>But the problem is, I am not able to achieve accuracy. It stays somewhere around 55%, which is pathetic. Some 70% and around accuracy will be satisfactory. And also note that I am using a complimentary naive bayes classifier of apache mahout.</p>

<p>Coming to the question precisely, is it the smaller data set size that is bringing down the accuracy? If not, where should I tweak?</p>
","mahout, bayesian, sentiment-analysis","<p>Only for the benefit of those looking into this question in future, I will share the ways in which I tweaked the accuracy of my classifier from 50 to around 78%</p>

<ul>
<li>Perform stemming on training and input data</li>
<li>Perform stop word removal on training and input data</li>
<li>Convert training and input data to lower case (or uppercase)</li>
<li>Have near equal amount of samples in each category of the training data</li>
<li>Fine tune the ngram level according to your domain.</li>
</ul>

<p>This should dramatically raise your accuracy.</p>
",3,1,1253,2012-01-25 03:50:51,https://stackoverflow.com/questions/8997597/training-data-size-for-a-bayesian-classifier
bag of words algorithm in php,"<p>Im making my final project in my studies.</p>

<p>and I'm trying to create sentiment analysis of Twitter messages.</p>

<p>I'm using Bayesian algorithm, and bag of words.</p>

<p>Do you have an example of bag of words algorithm in PHP?</p>

<p>I can't find anything, maybe list of positive and negative words or something</p>
","php, algorithm, sentiment-analysis","<p>I haven't implemented Bag of Words in PHP but I've done it in java. A simple way to implement it would be by taking the training data and tokenizing it (example Stanford Tokenizer). Once you have tokenized all your training data, you can then extract 1-grams from it. I use this <a href=""http://homepages.inf.ed.ac.uk/lzhang10/ngram.html"" rel=""nofollow"">http://homepages.inf.ed.ac.uk/lzhang10/ngram.html</a> to extract the grams and then remove the count of words from the output and just use the words. This becomes your Bag of Words corpus which can be used during training and classification. Make sure, you use the same tokenizer during training and testing or classification and also use the same corpus while training the models.</p>

<p>Now implementing it is pretty easy, just take a string of data and tokenize it using the same tokenizer used to create the bag of words corpus. Now take each token and then find whether that token is available in your corpus and at what position. For example, you have a corpus which has words as follows :-</p>

<p>a</p>

<p>name</p>

<p>the</p>

<p>hello</p>

<p>world</p>

<p>,</p>

<p>And you have a string ""hello, my name is Jas"". Tokenizing it would give the following tokens {hello,,,my,name,is,Jas} and when you try to match these tokens with the corpus your result would be :-</p>

<p>2:1 4:1 6:1</p>

<p>This means, the words name, hello and comma which are present in the location 2, 4 and 6 in your corpus are present in the incoming test string.</p>
",0,0,2298,2012-01-28 13:51:33,https://stackoverflow.com/questions/9045741/bag-of-words-algorithm-in-php
Automatic negation of words,"<p>Consider the following statements</p>

<pre><code>We are not talking about a well established company in the NASDAQ
I will not initiate any trades until those clowns hammer out a deal
</code></pre>

<p>I am writing a simple Naive Bayes classifier, basically marking a training set of statements by hand (as either positive or negative sentiment) and storing the words that make up the statement accordingly.</p>

<p>Problem: if I mark both of these statements as having a negative sentiment, the words ""well"", ""established"" (statement 1) and ""any"", ""until"" (statement 2) would be indivudually marked as negatives. Whereas in another case (i.e., ""This company is performing well""), the same words (""well"" in this case) would be marked as a positive, making the sum of sentiment for ""well"" -1 + 1 = 0. I would overcome this by tagging these words as negated words, for example:</p>

<pre><code>We are talking about a not-well not-established company in the NASDAY.
I will initiate not-anymore trades not-until those clowns hammer out a deal
</code></pre>

<p>Is there a standard or best way of tagging these kinds of words (I don't even know if they are of a same group of words)? Obviously, tagging ""company"" wouldn't make sense ""not-company"" doesn't hold any sentimental value. I have (in PHP) made a function that would tag all words after the negation word (not, no, couldn't, etc) but many of them didn't make real sense afterwards (such as ""not-company"", ""not-NASDAQ"", ""not-clowns"").</p>

<p>Since English is not my mother language, I'm asking you if there's a common name for the words I have marked here and if what I want is (rudimentary) possible. I am aware that there are a lot of exceptions possible (double negations etc.) but I do not want to go into that; I believe if this would be possible, it would cover a lot of ground.</p>
","php, nlp, sentiment-analysis","<p>Taking from your example,</p>

<pre><code>We are talking about a not-well not-established company in the NASDAY.
I will initiate not-anymore trades not-until those clowns hammer out a deal
</code></pre>

<p>I think you want to tag <strong>adjectives</strong> (and their variants) so they would be negated, right? It is called ""part of speech tagging"". There is a good tutorial with PHP <a href=""http://phpir.com/part-of-speech-tagging"" rel=""nofollow"">here</a>.</p>

<p>You need a dictionary (or list of word) of common English adjectives, however.</p>
",4,2,255,2012-02-11 22:43:50,https://stackoverflow.com/questions/9244692/automatic-negation-of-words
Logical fallacy detection and/or identification with natural-language-processing,"<p>Is there a package or methodology in existence for the detection of flawed logical arguments in text? </p>

<p>I was hoping for something that would work for text that is not written in an academic setting (such as a logic class). It might be a stretch but I would like something that can identify where logic is trying to be used and identify the logical error. A possible use for this would be marking errors in editorial articles.</p>

<p>I don't need anything that is polished. I wouldn't mind working to develop something either so I'm really looking for what's out there in the wild now.</p>
","nlp, logic, machine-learning, sentiment-analysis","<p>That's a difficult problem, because you'll have to map natural language to some logical representation, and deal with ambiguity in the process.</p>

<p><a href=""http://attempto.ifi.uzh.ch/site/"">Attempto Project</a> may be interesting for you. It has several <a href=""http://attempto.ifi.uzh.ch/site/tools/"">tools</a> that you can try online. In particular, <a href=""http://attempto.ifi.uzh.ch/race/"">RACE</a> may be doing something you wanted to do. It checks for consistency on the given assertions. But the bigger issue here is in transforming them to logical forms.</p>
",9,15,3828,2012-04-06 16:39:57,https://stackoverflow.com/questions/10046407/logical-fallacy-detection-and-or-identification-with-natural-language-processing
Using Sentiwordnet 3.0,"<p>I plan on using Sentiwordnet 3.0 for Sentiment classification. Could someone clarify as to what the numbers associated with words in Sentiwordnet represent? For e.g. what does 5 in rank#5 mean? Also for POS what is the letter used to represent adverbs? Im assuming 'a' is adjectives. I could not find an explanation either on their site or on other sites.</p>
","machine-learning, nlp, wordnet, sentiment-analysis, senti-wordnet","<p>I found the answer. Seems like the number notation comes form Wordnet. It represents the rank in which the given word is commonly used. So rank#5 refers to the context in which rank is used 5th most commonly. Similarly rank#1 refers to the meaning of rank most commonly used. The following are the POS notations: </p>

<p>n  -  NOUN <br/>
v  -  VERB <br/>
a   - ADJECTIVE <br/>
s    - ADJECTIVE SATELLITE <br/>
r    - ADVERB <br/></p>
",13,6,6936,2012-04-19 07:17:34,https://stackoverflow.com/questions/10223314/using-sentiwordnet-3-0
Javaee mbean vs singleton,"<p>I have created a javaee application that among other things must perform sentiment analysis using naive bayes,. In order for the sentiment algorithm to work, we have to first train it,so I would like to create an object that would handle the trainning whenever the server is started to avoid training over and over again. I thought of using a singleton ejb to do this but I don't know if this is the correct way to go, also a friend suggested the use of managed beans. What are the pros and cons of those approaches for my problem? Am I looking in the correct direction or I am just barking in the wrong tree?</p>
","java, jakarta-ee, sentiment-analysis","<p>MBeans are great for modifying the state of your application at runtime. If you want to alter the training of the algorithm at runtime and use MBeans, it could make sense to use them to initialize when you start up as well.</p>

<p>Also, I'd recommend in the design of the class containing your algorithm that you externalize the coefficients that you are calculating during training. You can then persist those coefficients and not have to re-run the training. On start your app would load the coefficients from persistence.</p>

<p>Combining loadable coefficients with MBeans, you could use the latter to retrieve or reload the set of coefficients at runtime. You would want to ensure reloading them is atomic. This would enable you to arbitrarily tune your analysis on the fly.</p>
",1,0,756,2012-04-19 07:51:32,https://stackoverflow.com/questions/10223786/javaee-mbean-vs-singleton
Sentiment analysis using R,"<p>Are there any R packages that focus on sentiment analysis? I have a small survey where users can write a comment about their experience of using a web-tool. I ask for a numerical ranking, and there is the option of including a comment. </p>

<p>I am wondering what the best way of assessing the positiveness or negativeness of the comment is. I would like to be able to compare it to the numerical ranking that the user provides, using R.</p>
","r, sentiment-analysis","<p>And there is <a href=""http://cran.r-project.org/web/packages/sentiment/index.html"" rel=""noreferrer"">this package</a>:</p>

<p><code>sentiment: Tools for Sentiment Analysis</code></p>

<p>sentiment is an R package with tools for sentiment analysis including bayesian classifiers for positivity/negativity and emotion classification.</p>

<p>Update 14 Dec 2012: it has been removed  to the <a href=""http://cran.r-project.org/src/contrib/Archive/sentiment/"" rel=""noreferrer"">archive</a>...</p>

<p>Update 15 Mar 2013: the <a href=""http://cran.us.r-project.org/web/packages/qdap/index.html"" rel=""noreferrer"">qdap</a> package has a <code>polarity</code> function, based on Jeffery Breen's work</p>
",26,28,43652,2012-04-19 17:01:05,https://stackoverflow.com/questions/10233087/sentiment-analysis-using-r
How to tackle twitter sentiment analysis?,"<p>I'd like you  to give me some advice in order to tackle this problem. At college I've been solving opinion mining tasks but with Twitter the approach is quite different. For example, I used an ensemble learning approach to classify users opinions about a certain Hotel in Spain. Of course, I was given a training set with positive and negative opinions and then I tested with the test set. But now, with twitter, I've found this kind of categorization very difficult. </p>

<ol>
<li><p>Do I need to have a training set? and if the answer to this question is positive, don't you think twitter is so temporal so if I have that set, my performance on future topics will be very poor?</p></li>
<li><p>I was thinking in getting a dictionary  (mainly adjectives) and cross my tweets with it and obtain a term-document matrix but I have no class assigned to any twitter. Also, positive adjectives and negative adjectives could vary depending on the topic and time. So, how to deal with this?</p></li>
<li><p>How to deal with the problem of languages? For instance, I'd like to study tweets written in English and those in Spanish, but separately.</p></li>
<li><p>Which programming languages do you suggest to do something like this? I've been trying with R packages like tm, twitteR.</p></li>
</ol>
",sentiment-analysis,"<ol>
<li>Sure, I think the way sentiment is used will stay constant for a few months. worst case you relabel and retrain. Unsupervised learning has a shitty track record for industrial applications in my experience.</li>
<li>You'll need some emotion/adj dictionary for sentiment stuff- there are some datasets out there but I forget where they are. I may have answered previous questions with better info.</li>
<li>Just do English tweets, it's fairly easy to build a language classifier, but you want to start small, so take it easy on yourself</li>
<li>Python (NLTK) if you want to do it easily in a small amount of code. Java has good NLP stuff, but Python and it's libraries are way more user friendly</li>
</ol>
",1,2,1589,2012-05-02 14:59:05,https://stackoverflow.com/questions/10416343/how-to-tackle-twitter-sentiment-analysis
NLP Library (Subject Extraction+Sentiment Analysis) for a Java-based Web Application,"<p>I'm a college student looking for a NLP library to perform subject extraction and sentiment analysis in a Java-based web application for a summer-hobby project.</p>

<p>To give you a little context on what I'm trying to do... I want to build a Java-based web application that will extract subjects out of a Reddit submission's headlines, as well as identify the OP's sentiment for the headline (when possible). </p>

<p>Example Inputs:</p>

<ul>
<li>Reddit, we took the anti-SOPA petition from 943,702 signatures to
3,460,313. The anti-CISPA petition is at 691,768, a bill expansively
worse than SOPA. Please bump it, then let us discuss further measures
or our past efforts are in vain. We did it before, I'm afraid we are
called on to do it again.</li>
<li>My friend calls him ""Mr Ridiculously Photogenic Guy""</li>
<li>Insanity: CISPA Just Got Way Worse, And Then Passed On Rushed Vote</li>
</ul>

<p>I'm currently trying out AlchemyAPI, but it sounds like better NLP libraries exist out there. Preferablly, I wouldn't be restricted to a limited number of API requests in a given time period (AlchemyAPI has a quota). I've heard the names of GATE, LingPipe, and OpenNLP - however, I'm unsure whether they fit my needs.</p>

<p>I'm looking for framework/library/api recommendations, or even better, comparisons from experienced users. My experience with NLP is extremely limited, which is why I'm asking for help here (ps: if anyone has any resources for learning more, outside of www.nlp-class.org, please let me know!) :)</p>
","java, nlp, sentiment-analysis","<p>First, I'd highly recommend using python, as the NLP libraries are a bit more user friendly than java, and it'd be a lot less code to maintain for a one-man project.</p>

<p>I can't think of anything off the top of my head to do either classification, so my recommendation would be to train two classifiers, one for subject, and one for sentiment. You'll have to label data and define features, but I think that wouldn't be too hard, especially with sentiment where you build up a dictionary of 'emotion' words. Labeling data is a pain in the ass, but that and good features are how you get good classification.</p>

<p><strong>Subject Classifier:</strong></p>

<p>Use NLTK with a Naive Bayes classifier, and define features as the word (lowercased), and word bigrams and trigrams.</p>

<p><strong>Sentiment Classifier:</strong></p>

<p>Same features as subject classifier, but also have a feature that says word w is in emotion dictionary with connection c. So, word 'bad' means 'bad sentiment'.</p>

<p>Once you've amassed sufficient training/testing data, you train your classifiers and optimize features, if necessary, and then you can run the classifiers against whatever other data you want.</p>

<p>General Purpose Libraries (Java):</p>

<ul>
<li>OpenNLP </li>
<li>LingPipe</li>
<li>Weka</li>
<li>Stanford stuff</li>
</ul>

<p>Libraries (Python):</p>

<ul>
<li>NLTK</li>
<li>Scipy</li>
</ul>
",4,4,2252,2012-05-02 18:19:21,https://stackoverflow.com/questions/10419437/nlp-library-subject-extractionsentiment-analysis-for-a-java-based-web-applica
What are the existent Sentiment Analysis Algorithm?,"<p>I and a group of people are developing a Sentiment Analysis Algorithm. I would like to know what are the existent ones, because I want to compare them. Is there any article that have the main algorithms in this area? </p>

<p>Thanks in advance</p>

<p>Thiago</p>
",sentiment-analysis,"<p>Some of the papers on sentiment analysis may help you - </p>

<ol>
<li>One of the earlier works by Bo Pang, Lillian Lee <a href=""http://acl.ldc.upenn.edu/acl2002/EMNLP/pdfs/EMNLP219.pdf"">http://acl.ldc.upenn.edu/acl2002/EMNLP/pdfs/EMNLP219.pdf</a> </li>
<li>A comprehensive survey of sentiment analysis techniques <a href=""http://www.cse.iitb.ac.in/~pb/cs626-449-2009/prev-years-other-things-nlp/sentiment-analysis-opinion-mining-pang-lee-omsa-published.pdf"">http://www.cse.iitb.ac.in/~pb/cs626-449-2009/prev-years-other-things-nlp/sentiment-analysis-opinion-mining-pang-lee-omsa-published.pdf</a> </li>
<li>Study by Hang Cui, V Mittal, M Datar using 6-grams <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.5942&amp;rep=rep1&amp;type=pdf"">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.5942&amp;rep=rep1&amp;type=pdf</a></li>
</ol>

<p>For quick implementation naive bayes is recommended. You can find an example here <a href=""http://nlp.stanford.edu/IR-book/"">http://nlp.stanford.edu/IR-book/</a></p>

<p>We did a statistical comparision of various classifiers and found SVM to be most accurate, though for a dataset consisting of large contents 
( <a href=""http://ai.stanford.edu/~amaas/data/sentiment/"">http://ai.stanford.edu/~amaas/data/sentiment/</a> ) none of the methods worked well.Our study may not be accurate though. Also instead of treating sentiment analysis as a text classification problem, you can look at extraction of meaning from text, though I do not know how successful it might be.</p>
",11,7,8327,2012-05-21 20:49:55,https://stackoverflow.com/questions/10692428/what-are-the-existent-sentiment-analysis-algorithm
training libsvm for text classification(sentiment),"<p>From following links I came with some idea. I want to ask whether I am doing it right or I am in the wrong way. If I am in the wrong way, please guide me.</p>

<p>Links<br/>
<a href=""https://stackoverflow.com/questions/9021658/using-libsvm-for-text-classification-c-sharp"">Using libsvm for text classification c#</a><br/>
<a href=""https://stackoverflow.com/questions/6172159/how-to-use-libsvm-for-text-classification"">How to use libsvm for text classification?</a></p>

<p>My way</p>

<p>First calculate the word count in each training set<br>
Create a maping list for each word</p>

<p>eg</p>

<pre><code>sample word count form training set
|-----|-----------|
|     |   counts  |
|-----|-----|-----|
|text | +ve | -ve |
|-----|-----|-----|
|this | 3   | 3   |
|forum| 1   | 0   |
|is   | 10  | 12  |
|good | 10  | 5   |
|-----|-----|-----|
</code></pre>

<p>positive training data</p>

<pre><code>this forum is good
</code></pre>

<p>so will the training set be</p>

<pre><code>+1 1:3 2:1 3:10 4:10
</code></pre>

<p>this all is just what I received from above links.<br>
Please help me.</p>
","svm, libsvm, sentiment-analysis","<p>You're doing it right.</p>

<p>I don't know why your laben is called ""+1"" - should be a simple integer (refering to the document ""+ve""), but all in all it's the way to go. </p>

<p>For document classification you may want to take a look at liblinear which is specially designed for handling a lot of features.</p>
",4,3,2905,2012-05-24 09:29:32,https://stackoverflow.com/questions/10734728/training-libsvm-for-text-classificationsentiment
Sentiment Analysis on Twitter Data?,"<p>I am working on this project where I wish to classify the general mood of a Twitter user from his recent tweets. Since the tweets can belong to a huge variety of domains how should I go about it ?</p>

<p>I could use the Naive Bayes algorithm (like here: <a href=""http://phpir.com/bayesian-opinion-mining"" rel=""nofollow"">http://phpir.com/bayesian-opinion-mining</a>) but since the tweets can belong to a large variety of domains, I am not sure if this will be very accurate.</p>

<p>The other option is using maybe sentiment dictionaries like <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""nofollow"">SentiWordNet</a> or <a href=""http://alexdavies.net/2011/10/word-lists-for-sentiment-analysis-of-twitter/"" rel=""nofollow"">here</a>. Would this be a better approach, I don't know.</p>

<p>Also where can I get data to train my classifier if I plan to use the Naive Bayes or some other algorithm ?</p>

<p>Just to add here, I am primarily coding in PHP.</p>
","twitter, dataset, sentiment-analysis","<p>It appears you could use <code>SentiWordNet</code> as the <em>classifier</em> data if you are focused on a word-by-word approach.  It is how simple <code>Bayesian spam filters</code> works; it focuses on each word.</p>

<p>The advantage here is that while many of the words in <code>SentiWordNet</code> have multiple meanings, each with different <code>positive/objective/negative</code> scores, you could experiment with using the scores of the other words in the tweet to narrow in on the most appropriate meaning for each multi-meaning word, which could give you a more accurate score for each word and for the overall tweet.</p>
",2,1,2300,2012-06-21 19:09:01,https://stackoverflow.com/questions/11145094/sentiment-analysis-on-twitter-data
Is there a set of adjective word list for positive or negative polarity,"<p>I am working on sentiment analysis. I thought if there is any available set of adjectives indicating positive/negative(like for positive: good,awesome,amazing,) meaning? and the second thing is a set of data from which i can use as a test case.</p>
","nlp, stanford-nlp, sentiment-analysis","<p>Resources related to polarity:</p>

<p>SentiWordNet: <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""noreferrer"">http://sentiwordnet.isti.cnr.it/</a></p>

<p>Inquirer Dictionary: <a href=""http://www.wjh.harvard.edu/~inquirer/homecat.htm"" rel=""noreferrer"">http://www.wjh.harvard.edu/~inquirer/homecat.htm</a></p>

<p>Possible test data: <a href=""http://www.cs.pitt.edu/mpqa/"" rel=""noreferrer"">http://www.cs.pitt.edu/mpqa/</a></p>
",13,5,11215,2012-08-03 16:49:11,https://stackoverflow.com/questions/11799971/is-there-a-set-of-adjective-word-list-for-positive-or-negative-polarity
List of Natural Language Processing Tools in Regards to Sentiment Analysis - Which one do you recommend,"<p>first up sorry for my not so perfect English... I am from Germany ;) </p>

<p>So, for a research project of mine (Bachelor thesis) I need to analyze the sentiment of tweets about certain companies and brands. For this purpose I will need to script my own program / use some sort of modified open source code (no APIs' - I need to understand what is happening). </p>

<p>Below you will find a list of some of the NLP Applications I found. My Question now is which one and which approach would you recommend? And which one does not require long nights adjusting the code?</p>

<p>For example: When I screen twitter for the music player >iPod&lt; and someone writes: ""It's a terrible day but at least my iPod makes me happy"" or even harder: ""It's a terrible day but at least my iPod makes up for it"" </p>

<p>Which software is smart enough to understand that the focused is on iPod and not the weather? </p>

<p>Also which software is scalable / resource efficient (I want to analyze several tweets and don't want to spend thousands of dollars)? </p>

<p><strong>Machine learning and data mining</strong></p>

<p><em>Weka</em> - is a collection of machine learning algorithms for data mining. It is one of the most popular text classification frameworks. It contains implementations of a wide variety of algorithms including Naive Bayes and Support Vector Machines (SVM, listed under SMO) [Note: Other commonly used non-Java SVM implementations are SVM-Light, LibSVM, and SVMTorch]. A related project is Kea (Keyphrase Extraction Algorithm) an algorithm for extracting keyphrases from text documents.</p>

<p><em>Apache Lucene Mahout</em> - An incubator project to created highly scalable distributed implementations of common machine learning algorithms on top of the Hadoop map-reduce framework.</p>

<p><strong>NLP Tools</strong></p>

<p><em>LingPipe</em> - (not technically 'open-source, see below) Alias-I's Lingpipe is a suite of java tools for linguistic processing of text including entity extraction, speech tagging (pos) , clustering, classification, etc... It is one of the most mature and widely used open source NLP toolkits in industry. It is known for it's speed, stability, and scalability. One of its best features is the extensive collection of well-written tutorials to help you get started. They have a list of links to competition, both academic and industrial tools. Be sure to check out their blog. LingPipe is released under a royalty-free commercial license that includes the source code, but it's not technically 'open-source'.</p>

<p><em>OpenNLP</em> - hosts a variety of java-based NLP tools which perform sentence detection, tokenization, part-of-speech tagging, chunking and parsing, named-entity detection, and co-reference analysis using the Maxent machine learning package.</p>

<p><em>Stanford Parser and Part-of-Speech (POS) Tagger</em> - Java packages for sentence parsing and part of speech tagging from the Stanford NLP group. It has implementations of probabilistic natural language parsers, both highly optimized PCFG and lexicalized dependency parsers, and a lexicalized PCFG parser. It's has a full GNU GPL license.</p>

<p><em>OpenFST</em> - A package for manipulating weighted finite state automata. These are often used to represented a probablistic model. They are used to model text for speech recognition, OCR error correction, machine translation, and a variety of other tasks. The library was developed by contributors from Google Research and NYU. It is a C++ library that is meant to be fast and scalable.</p>

<p><em>NTLK</em> - The natural language toolkit is a tool for teaching and researching classification, clustering, speech tagging and parsing, and more. It contains a set of tutorials and data sets for experimentation. It is written by Steven Bird, from the University of Melbourne.</p>

<p><em>Opinion Finder</em> - A system that performs subjectivity analysis, automatically identifying when opinions, sentiments, speculations and other private states are present in text. Specifically, OpinionFinder aims to identify subjective sentences and to mark various aspects of the subjectivity in these sentences, including the source (holder) of the subjectivity and words that are included in phrases expressing positive or negative sentiments.</p>

<p><em>Tawlk/osae</em> - A python library for sentiment classification on social text. The end-goal is to have a simple library that ""just works"". It should have an easy barrier to entry and be thoroughly documented. We have acheived best accuracy using stopwords filtering with tweets collected on negwords.txt and poswords.txt</p>

<p><em>GATE</em> - GATE is over 15 years old and is in active use for all types of computational task involving human language. GATE excels at text analysis of all shapes and sizes. From large corporations to small startups, from €multi-million research consortia to undergraduate projects, our user community is the largest and most diverse of any system of this type, and is spread across all but one of the continents1.</p>

<p><em>textir</em> - A suite of tools for text and sentiment mining. This includes the ‘mnlm’ function, for sparse multinomial logistic regression, ‘pls’, a concise partial least squares routine, and the ‘topics’ function, for efficient estimation and dimension selection in latent topic models.</p>

<p>NLP Toolsuite - The JULIE Lab here offers a comprehensive NLP tool suite for the application purposes of semantic search, information extraction and text mining. Most of our continuously expanding tool suite is based on machine learning methods and thus is domain- and language independent.</p>

<p>...</p>

<p>On a side note: Would you recommend the twitter streaming or the get API? </p>

<p>As to me, I am a fan of python and java ;)</p>

<p>Thanks a lot for your help!!!</p>
","twitter, nlp, nltk, sentiment-analysis","<p>I'm not sure how much I can help, but I have worked with hand-rolled NLP before. A couple of issues come to mind - not all products are language agnostic (human language that is, not computer language). If you're planning on analysing German tweets, it's going to be important that your selected product is able to handle the German language. Obvious I know, but easy to forget. Then there's the fact that it's twitter where contractions and acronyms abound, and the language structure is constrained by the character limit which means that the grammar won't always match the expected structure of the language.</p>

<p>In English, pulling nouns from a sentence can be simplified somewhat if you ever have to write code of your own. Proper nouns have initial capitals and a string of such words (possibly including ""of"") is an example of a noun phrase. A word preceeded by ""a/an/my/his/hers/the/this/these/those"" is going to be either an adjective or a noun. It gets harder after that unfortunately.</p>

<p>There are rules which help identify plurals, but there are also lots of exceptions. I'm talking about English here of course, my very poor spoken German doesn't help me understand that grammar I'm afraid.</p>
",4,15,7268,2012-09-06 12:05:15,https://stackoverflow.com/questions/12299724/list-of-natural-language-processing-tools-in-regards-to-sentiment-analysis-whi
Hive: How to have a derived column that has stores the sentiment value from the sentiment analysis API,"<p>Here's the scenario:</p>

<p>Say you have a Hive Table that stores twitter data. </p>

<p>Say it has 5 columns. One column being the Text Data. </p>

<p>Now How do you add a 6th column that stores the sentiment value from the Sentiment Analysis of the twitter Text data. I plan to use the Sentiment Analysis API like Sentiment140 or viralheat. </p>

<p>I would appreciate any tips on how to implement the ""derived"" column in Hive.</p>

<p>Thanks. </p>
","hadoop, hive, sentiment-analysis","<p>Unfortunately, while the Hive API lets you add a new column to your table (using ALTER TABLE foo ADD COLUMNS (bar binary)), those new columns will be NULL and cannot be populated. The only way to add data to these columns is to clear the table's rows and load data from a new file, this new file having that new column's data.</p>

<p>To answer your question: You can't, in Hive. To do what you propose, you would have to have a file with 6 columns, the 6th already containing the sentiment analysis data. This could then be loaded into your HDFS, and queried using Hive.</p>

<p>EDIT: Just tried an example where I exported the table as a .csv after adding the new column (see above), and popped that into M$ Excel where I was able to perform functions on the table values. After adding functions, I just saved and uploaded the .csv, and rebuilt the table from it. Not sure if this is helpful to you specifically (since it's not likely that sentiment analysis can be done in Excel), but may be of use to anyone else just wanting to have computed columns in Hive.</p>

<p>References:</p>

<p><a href=""https://cwiki.apache.org/Hive/gettingstarted.html#GettingStarted-DDLOperations"" rel=""nofollow"">https://cwiki.apache.org/Hive/gettingstarted.html#GettingStarted-DDLOperations</a></p>

<p><a href=""http://comments.gmane.org/gmane.comp.java.hadoop.hive.user/6665"" rel=""nofollow"">http://comments.gmane.org/gmane.comp.java.hadoop.hive.user/6665</a></p>
",1,2,3166,2012-11-16 22:52:23,https://stackoverflow.com/questions/13425623/hive-how-to-have-a-derived-column-that-has-stores-the-sentiment-value-from-the
Add new words to the lexicon for R sentiment package,"<p>I'm currently doing sentiment and emotion analysis of Twitter's data using R sentiment package and need to add new words to the subjectivity and emotion lexicons used by the package as there are some words that carry specific sentiment and emotion in the topic that I analyze.</p>

<p>Does anyone know how to add words to the lexicon using the R sentiment package itself or any other R command? I have searched in the documentation but cannot find any means to do so.  </p>
","r, sentiment-analysis","<p>Both the subjectivity &amp; emotion lexicon when read in (say, as csv) construct a data frame for you. Adding entries to a data frame can be done using the rbind() function.</p>

<p>> patientID &lt;- c(1, 2, 3, 4)<br>
> age &lt;- c(25, 34, 28, 52)<br>
> diabetes &lt;- c(""Type1"", ""Type2"", ""Type1"", ""Type1"")<br>
> status &lt;- c(""Poor"", ""Improved"", ""Excellent"", ""Poor"")<br>
> patientdata &lt;- data.frame(patientID, age, diabetes, status)<br>
> patientdata<br>
  patientID age diabetes    status<br>
1         1  25    Type1      Poor<br>
2         2  34    Type2  Improved<br>
3         3  28    Type1 Excellent<br>
4         4  52    Type1      Poor<br>
> patientID &lt;- c(10, 20, 30, 40)<br>
> age &lt;- c(50, 68, 56, 104)<br>
> diabetes &lt;- c(""Type4"", ""Type5"", ""Type6"", ""Type7"")<br>
> status &lt;- c(""Poorish"", ""Improving"", ""Excellento"", ""Poorish"")<br>
> patientdata1 &lt;- data.frame(patientID, age, diabetes, status)<br>
> patientdata1<br>
  patientID age diabetes     status<br>
1        10  50    Type4    Poorish<br>
2        20  68    Type5  Improving<br>
3        30  56    Type6 Excellento<br>
4        40 104    Type7    Poorish<br>
> concatPD &lt;- rbind(patientdata,patientdata1)<br>
> concatPD<br>
  patientID age diabetes     status<br>
1         1  25    Type1       Poor<br>
2         2  34    Type2   Improved<br>
3         3  28    Type1  Excellent<br>
4         4  52    Type1       Poor<br>
5        10  50    Type4    Poorish<br>
6        20  68    Type5  Improving<br>
7        30  56    Type6 Excellento<br>
8        40 104    Type7    Poorish<br>
> </p>

<p>I simply added the weird types of diabetes to ensure that the 2 frames are distinguished. :-)
In other words, you can create our own csv, read it (thereby creating another data frame) &amp; rbind them. Ensure that the columns of both data frames are in sync.</p>
",0,1,1181,2012-11-18 08:24:27,https://stackoverflow.com/questions/13438579/add-new-words-to-the-lexicon-for-r-sentiment-package
Feature Selection and Reduction for Text Classification,"<p>I am currently working on a project, a <strong>simple sentiment analyzer</strong> such that there will be <strong>2 and 3 classes</strong> in <strong>separate cases</strong>. I am using a <strong>corpus</strong> that is pretty <strong>rich</strong> in the means of <strong>unique words</strong> (around 200.000). I used <strong>bag-of-words</strong> method for <strong>feature selection</strong> and to reduce the number of <strong>unique features</strong>, an elimination is done due to a <strong>threshold value</strong> of <strong>frequency of occurrence</strong>. The <strong>final set of features</strong> includes around 20.000 features, which is actually a <strong>90% decrease</strong>, but <strong>not enough</strong> for intended <strong>accuracy</strong> of test-prediction. I am using <strong>LibSVM</strong> and <strong>SVM-light</strong> in turn for training and prediction (both <strong>linear</strong> and <strong>RBF kernel</strong>) and also <strong>Python</strong> and <strong>Bash</strong> in general.</p>

<p>The <strong>highest accuracy</strong> observed so far <strong>is around 75%</strong> and I <strong>need at least 90%</strong>. This is the case for <strong>binary classification</strong>. For <strong>multi-class training</strong>, the accuracy falls to <strong>~60%</strong>. I <strong>need at least 90%</strong> at both cases and can not figure how to increase it: via <strong>optimizing training parameters</strong> or <strong>via optimizing feature selection</strong>?</p>

<p>I have read articles about <strong>feature selection</strong> in text classification and what I found is that three different methods are used, which have actually a clear correlation among each other. These methods are as follows:</p>

<ul>
<li>Frequency approach of <strong>bag-of-words</strong> (BOW)</li>
<li><strong>Information Gain</strong> (IG)</li>
<li><strong>X^2 Statistic</strong> (CHI)</li>
</ul>

<p>The first method is already the one I use, but I use it very simply and need guidance for a better use of it in order to obtain high enough accuracy. I am also lacking knowledge about practical implementations of <strong>IG</strong> and <strong>CHI</strong> and looking for any help to guide me in that way.</p>

<p>Thanks a lot, and if you need any additional info for help, just let me know.</p>

<hr>

<ul>
<li><p>@larsmans: <strong>Frequency Threshold</strong>: I am looking for the occurrences of unique words in examples, such that if a word is occurring in different examples frequently enough, it is included in the feature set as a unique feature.   </p></li>
<li><p>@TheManWithNoName: First of all thanks for your effort in explaining the general concerns of document classification. I examined and experimented all the methods you bring forward and others. I found <strong>Proportional Difference</strong> (PD) method the best for feature selection, where features are uni-grams and <strong>Term Presence</strong> (TP) for the weighting (I didn't understand why you tagged <strong>Term-Frequency-Inverse-Document-Frequency</strong> (TF-IDF) as an indexing method, I rather consider it as a <strong>feature weighting</strong> approach).  <strong>Pre-processing</strong> is also an important aspect for this task as you mentioned. I used certain types of string elimination for refining the data as well as <strong>morphological parsing</strong> and <strong>stemming</strong>. Also note that I am working on <strong>Turkish</strong>, which has <strong>different characteristics</strong> compared to English. Finally, I managed to reach <strong>~88% accuracy</strong> (f-measure) for <strong>binary</strong> classification and <strong>~84%</strong> for <strong>multi-class</strong>. These values are solid proofs of the success of the model I used. This is what I have done so far. Now working on clustering and reduction models, have tried <strong>LDA</strong> and <strong>LSI</strong> and moving on to <strong>moVMF</strong> and maybe <strong>spherical models</strong> (LDA + moVMF), which seems to work better on corpus those have objective nature, like news corpus. If you have any information and guidance on these issues, I will appreciate. I need info especially to setup an interface (python oriented, open-source) between <strong>feature space dimension reduction</strong> methods (LDA, LSI, moVMF etc.) and <strong>clustering methods</strong> (k-means, hierarchical etc.).</p></li>
</ul>
","python, nlp, svm, sentiment-analysis, feature-extraction","<p>This is probably a bit late to the table, but...</p>

<p>As Bee points out and you are already aware, the use of SVM as a classifier is wasted if you have already lost the information in the stages prior to classification. However, the process of text classification requires much more that just a couple of stages and each stage has significant effects on the result. Therefore, before looking into more complicated feature selection measures there are a number of much simpler possibilities that will typically require much lower resource consumption.</p>

<p>Do you pre-process the documents before performing tokensiation/representation into the bag-of-words format? Simply removing stop words or punctuation may improve accuracy considerably.</p>

<p>Have you considered altering your bag-of-words representation to use, for example, word pairs or n-grams instead? You may find that you have more dimensions to begin with but that they condense down a lot further and contain more useful information.</p>

<p>Its also worth noting that dimension reduction <strong>is</strong> feature selection/feature extraction. The difference is that feature selection reduces the dimensions in a univariate manner, i.e. it removes terms on an individual basis as they currently appear without altering them, whereas feature extraction (which I think Ben Allison is referring to) is multivaritate, combining one or more single terms together to produce higher orthangonal terms that (hopefully) contain more information and reduce the feature space.</p>

<p>Regarding your use of document frequency, are you merely using the probability/percentage of documents that contain a term or are you using the term densities found within the documents? If category one has only 10 douments and they each contain a term once, then category one is indeed associated with the document. However, if category two has only 10 documents that each contain the same term a hundred times each, then obviously category two has a much higher relation to that term than category one. If term densities are not taken into account this information is lost and the fewer categories you have the more impact this loss with have. On a similar note, it is not always prudent to only retain terms that have high frequencies, as they may not actually be providing any useful information. For example if a term appears a hundred times in every document, then it is considered a noise term and, while it looks important, there is no practical value in keeping it in your feature set.</p>

<p>Also how do you index the data, are you using the Vector Space Model with simple boolean indexing or a more complicated measure such as TF-IDF? Considering the low number of categories in your scenario a more complex measure will be beneficial as they can account for term importance for each category in relation to its importance throughout the entire dataset.</p>

<p>Personally I would experiment with some of the above possibilities first and then consider tweaking the feature selection/extraction with a (or a combination of) complex equations if you need an additional performance boost.</p>

<hr>

<p><strong>Additional</strong></p>

<p>Based on the new information, it sounds as though you are on the right track and 84%+ accuracy (F1 or BEP - precision and recall based for multi-class problems) is generally considered very good for most datasets. It might be that you have successfully acquired all information rich features from the data already, or that a few are still being pruned.</p>

<p>Having said that, something that can be used as a predictor of how good aggressive dimension reduction may be for a particular dataset is 'Outlier Count' analysis, which uses the decline of Information Gain in outlying features to determine how likely it is that information will be lost during feature selection. You can use it on the raw and/or processed data to give an estimate of how aggressively you should aim to prune features (or unprune them as the case may be). A paper describing it can be found here:</p>

<p><a href=""http://www.cs.technion.ac.il/~gabr/papers/fs-svm.pdf"" rel=""noreferrer"" title=""Text Categorization with Many Redundant Features: Using Aggressive Feature Selection to Make SVMs Competitive with C4.5"">Paper with Outlier Count information</a></p>

<p>With regards to describing TF-IDF as an indexing method, you are correct in it being a feature weighting measure, but I consider it to be used mostly as part of the indexing process (though it can also be used for dimension reduction). The reasoning for this is that some measures are better aimed toward feature selection/extraction, while others are preferable for feature weighting specifically in your document vectors (i.e. the indexed data). This is generally due to dimension reduction measures being determined on a per category basis, whereas index weighting measures tend to be more document orientated to give superior vector representation.</p>

<p>In respect to LDA, LSI and moVMF, I'm afraid I have too little experience of them to provide any guidance. Unfortunately I've also not worked with Turkish datasets or the python language. </p>
",40,53,33034,2012-11-28 11:21:59,https://stackoverflow.com/questions/13603882/feature-selection-and-reduction-for-text-classification
Find &quot;near duplicates&quot; strings in R,"<p>I am using R to build a sentiment analysis tool and I am having some problems with duplicates. The main source of data is Twitter, and it looks like many are bypassing twitter own spam filter by adding some random text at the end of each tweet. For example</p>

<pre><code>Click xxxxx to buy the amazing xxxxx for FREE ugjh
</code></pre>

<p>I get tons of those exact tweets with a different random string at the end. They are either from the same user or from different.</p>

<p>Is there any function like <code>duplicated</code> or <code>unique</code> which returns how close 2 strings are and if they are above a certain % dismiss them?</p>

<p>I know doing that will eventually delete real tweets from people saying exactly the same, like</p>

<pre><code>I love xxxx !
</code></pre>

<p>but I will deal with that in the future.</p>

<p>Any tip in the right direction will be much appreciated!</p>
","r, nlp, sentiment-analysis, text-analysis","<p>I mentioned <code>agrep</code> above.  Here's an example with what you've explained.  By varying the <code>max.distance</code> we can adjust what gets kicked:</p>

<pre><code>comp &lt;- ""Click xxxxx to buy the amazing xxxxx for FREE ugjh""
w &lt;- ""I love xxxx !""
x &lt;- ""Click xxxxx to purchase the awesome xxxxx for FREE bmf""

agrep(comp, c(x, w), max.distance =.4, value = TRUE)
agrep(comp, c(x, w), max.distance =.9, value = TRUE)
</code></pre>
",6,3,1533,2012-12-05 01:23:58,https://stackoverflow.com/questions/13714893/find-near-duplicates-strings-in-r
semantic analysis opensource tool - suggestions needed,"<p>I've a book review site, where readers can write reviews about books, other users can post comments. I wanted to know following things automatically whenever new review publish or new comment published.</p>

<p>(1) whether book review is positive or not? How much % positive / negative?</p>

<p>(2) whether comment made by particular user is positive or not? How much % positive / negative?</p>

<p>(3) I want to read Tweets about particular book and wanted to check whether the tweet is positive or not?</p>

<p>bottom line, I want some tool suggestions (opensource), which I can use for my website. Website is written in PHP and I'm looking for some semantic analysis tool which I can customize to meet my need or which best fit my need.</p>

<p>if not, I want to know if its easy to build one with minimal requirements. I know PHP, Perl, Shell Script. I can learn Python. I know C++, Java may be right language to start from scratch; but don't have much experience.</p>
","sentiment-analysis, semantic-analysis","<p>There is an open source semantic analyses engine incubated in the Apache Software Foundation, currently, called <a href=""http://stanbol.apache.org/"" rel=""nofollow"">Stanbol</a>. It provides APIs to interface with it over HTTP as well as through a Java API if needed. It's pretty advanced, but generally speaking if your needs are simpler you can always try some SaS solution like <a href=""http://www.uclassify.com/browse/uclassify/Sentiment"" rel=""nofollow"">uClassify</a>.</p>
",1,3,2785,2012-12-17 17:50:25,https://stackoverflow.com/questions/13919351/semantic-analysis-opensource-tool-suggestions-needed
Sentiment analysis/linear regression (Django),"<p>I need a suggestion on how to do analyze this type of data. I want to perform a sentiment analysis or linear regression on it as a machine learning tool. The predictor is score.</p>

<pre><code>color   type    make    new score

red     truck   ford    y   2
black   sedan   chevy   n   4
silver  sedan   nissan  y   5
silver  truck   nissan  n   2
black   coupe   toyota  y   1
blue    van     honda   y   1
red     truck   toyota  n   4
red     coupe   ford    n   2
black   sedan   ford    y   1
blue    truck   toyota  y   4
white   coupe   chevy   y   3
white   van     toyota  n   5
red     van     ford    y   2
silver  truck   nissan  n   3
black   sedan   honda   n   1
silver  truck   chevy   y   4
red     truck   chevy   y   5
white   coupe   honda   n   5
blue    sedan   chevy   n   2
blue    van     nissan  y   3
</code></pre>

<p>I can run a LinearRegression classifier in WEKA which yields:</p>

<pre><code>score =  1.6 ( color=red,silver,white) + 1.8 (make=honda,nissan,toyota,chevy) + 0.55
</code></pre>

<p>However, I would like to implement this in Django for a web app. Is there another way to process this data and yield a linear regression equation not using WEKA. Any other suggestions on how to analyze it other than linear regression? I've already implemented a decision tree.</p>
","python, django, machine-learning, weka, sentiment-analysis","<p>You can use <a href=""http://scikit-learn.org/stable/"" rel=""nofollow"">scikit-learn</a> as your machine learning library, and particularly its <a href=""http://scikit-learn.org/0.12/modules/linear_model.html"" rel=""nofollow"">linear regression capability</a>. <a href=""http://scikit-learn.org/0.12/auto_examples/linear_model/plot_ols.html#example-linear-model-plot-ols-py"" rel=""nofollow"">This example</a> might also be useful.</p>

<p>Also, you can always bind the Weka java API to your application, or alternatively implement linear regression on your own, it is fairly easy algorithm to implement given a matrix algebra library.</p>
",4,4,2045,2012-12-19 16:37:25,https://stackoverflow.com/questions/13956943/sentiment-analysis-linear-regression-django
use sentiment dictionary value as features in SVM,"<p>I have a sentiment dictionary of positive and negative words with their sentiment strength value. My main work is to check whether this strength value have effect on final classification or not. It means I want to check if the text with word ""good"" (strength=6) and word with outstanding(strength=9) have different final sentiment score or not.</p>

<p>I am confused in creating feature vector for <code>SVM</code>. If i use <code>TF-IDF</code> measure or <code>POS tagging</code> it doesn't check strength value. So my main problem is how to use this strength value in SVM and how to generate feature vector containing strength value of word?</p>

<p>For example, </p>

<pre><code>""This book is good."" 
</code></pre>

<p>For this sentence how can I generate feature vector considering strength value? </p>

<ul>
<li><p>First I thought to multiply strength value with term frequency and use this weighted score as feature input, but it will just increase the frequency of word. For example ""good"" occurs 2 times and then I multiply it with its strength value 6 then its value became 12, so it will just increase the occurrence of word ""good"", am I right?</p></li>
<li><p>So please can anyone tell me if i can use sentiment strength value for <code>SVM</code> and how can i use it? </p></li>
<li><p>How can I generate feature vector with their values?</p></li>
</ul>
","machine-learning, classification, svm, information-retrieval, sentiment-analysis","<p>Just some suggestions:</p>

<h2>Construct a vocabulary</h2>

<p>This vocabulary serves as a dictionary. You will not include any word that does not present in the dictionary into your feature vector. Suppose your dictionary contains 5000 words. </p>

<h2>Prepare the sentiment strength for each word in the vocabulary</h2>

<p>Of course you can setup some default for those words that you have no idea about their sentiment strength.  </p>

<h2>Construct feature vector for each text you want to do classification</h2>

<p>For any given text, e.g., </p>

<pre><code>This book is good.
</code></pre>

<p>construct a feature vector with 5000 dimensions. Each dimension corresponds to its Tf-Idf score or just the number of occurrences of a word in the dictionary. Suppose in your dictionary, you have </p>

<pre><code>strength(book) = 0.01
strength(good) = 6.0, 
</code></pre>

<p>and you don't have entries for <code>this</code> or <code>is</code>. Then you will end up with a vector with 5000 elements (I am using the number of occurrences instead of Tf-Idf in my following example. Feel free to try Tf-Idf in a similar way). </p>

<pre><code>          book,good
[0,0,0, ..., 1,1,0,0,....,0]
</code></pre>

<p>All elements are zeros except the two elements that correspond to <code>book</code> and <code>good</code>. Plug in your sentiment strength, you get:</p>

<pre><code>           book,good
[0,0,0, ...,0.01,6.0,0,0,....,0]
</code></pre>

<p>Multiplying the strength value with the number of occurrences will probably increase or decrease the value of the corresponding element. This is fine because you do want to boost or weaken the contribution of the component by its sentiment strength. </p>

<h2>Training the SVM</h2>

<p>When supplying each feature vector with a target value or class label, you can train your SVM now. </p>

<p>Hope they help. </p>
",0,0,1320,2013-01-23 18:23:37,https://stackoverflow.com/questions/14486665/use-sentiment-dictionary-value-as-features-in-svm
Sentiment analysis,"<p>while performing sentiment analysis, how can I make the machine understand that I'm referring apple (the iphone), instead of apple (the fruit)? </p>

<p>Thanks for the advise ! </p>
",sentiment-analysis,"<p>Well, there are several methods,</p>

<p>I would start with checking Capital letter, usually, when referring to a name, first letter is capitalized.</p>

<p>Before doing sentiment analysis, I would use some Part-of-speech and Named Entity Recognition to tag the relevant words.</p>

<p><a href=""http://nlp.stanford.edu/software/CRF-NER.shtml#Download"" rel=""nofollow noreferrer"">Stanford CoreNLP</a> is a good text analysis project to start with, it will teach
you the basic concepts.</p>

<p>Example from CoreNLP:</p>

<p><img src=""https://i.sstatic.net/r7ZFP.png"" alt=""enter image description here""></p>

<p>You can see how the tags can help you.</p>

<p>And check out <a href=""https://stackoverflow.com/questions/6893858/how-i-classify-an-word-of-a-text-in-things-like-names-number-money-date-etc"">more info</a></p>
",4,7,2484,2013-02-17 18:54:11,https://stackoverflow.com/questions/14924772/sentiment-analysis
LingPipe and Sentiment Analysis,"<p>I am following this document: </p>

<p><a href=""http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html"" rel=""nofollow"">http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html</a> to perform sentiment analysis using lingpipe. </p>

<p>One of the steps says that ""Assuming the data is in the directory POLARITY_DIR and the sentimentDemo.jar file exists (if the jar doesn't exist, compile it), the demo may be run from the command line""</p>

<p>I am not able to find the SentimentDemo.jar file. Tried windows search. 
So how do we compile it? I am not familiar with using Ant on windows and how to compile lingpipe jar files w/ it - any guidance is appreciated!</p>

<p>Also When I try to run the code: </p>

<pre><code>java
-cp ""sentimentDemo.jar;
     ../../../lingpipe-4.1.0.jar""
PolarityBasic POLARITY_DIR
</code></pre>

<p>I get the error saying that:
""Error: Could not find or load main class PolarityBasic""</p>

<p>I've configured polarity_dir so I am guessing that this isn't an issue.</p>

<p>Additional Info:</p>

<blockquote>
  <p>I am on windows 7
  using lingpipe 4.1.0</p>
</blockquote>

<p>Any guidance in completing the tutorial is appreciated, Thanks!</p>
","nlp, sentiment-analysis","<p>You should be able to build the jar file with ""ant jar"". </p>

<p>That command is suggested a little later in the tutorial (<a href=""http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html"" rel=""nofollow"">LingPipe: Sentiment Analysis Tutorial</a>) </p>

<blockquote>
  <p>...(if it doesn't, run ant jar to create it)...</p>
</blockquote>

<p>You'll need to install Ant (from <a href=""http://ant.apache.org/"" rel=""nofollow"">ant.apache.org</a>), and add it to your PATH. And you may have to set the ANT_HOME and/or JAVA_HOME environment variables prior to running ant.</p>
",1,0,1366,2013-02-21 18:58:48,https://stackoverflow.com/questions/15010184/lingpipe-and-sentiment-analysis
Mahout for sentiment analysis,"<p>Using mahout I am able to classify sentiment of data . But I am stuck with a confusion matrix. </p>

<p>I am using mahout 0.7 naive bayes algorithms to classify sentiment of tweets.
I  use <code>trainnb</code> and <code>testnb</code> naive bayes classifiers to train the classifier and classify sentiment of tweets as 'positive' ,'negative' or 'neutral'.</p>

<p>Sample positive training set </p>

<pre><code>      'positive','i love my i phone'
      'positive' , it's pleasure to have i phone'  
</code></pre>

<p>Similarly I have prepared training samples of negative 
and neutral, it is a huge data set.</p>

<p>The sample test data tweets I am providing is without including sentiments.</p>

<pre><code>  'it is nice model'
  'simply fantastic ' 
</code></pre>

<p>I am able to run the mahout classification algorithm, and it gives output of classified instances as confusion matrix .</p>

<p>Next step I need to find out which tweets are showing positive sentiment and which are negative.
expected output using classification:  to tag text with the sentiment.</p>

<pre><code>       'negative','very bad btr life time'
      'positive' , 'i phone has excellent design features' 
</code></pre>

<p>In mahout  which algorithm do I need to implement to get output in the above format. or any custom source implementation is required. </p>

<p>To display data 'kindly' suggest me algorithms that apache mahout provides, which will be suitable for my twitter data sentiment analysis.</p>
","machine-learning, mahout, sentiment-analysis","<p>In general to classify some text you need to run Naive Bayes with different priors (positive and negative in your case) and then just chose the one that results in greater value.</p>

<p><a href=""http://www.oracle.com/technetwork/community/join/member-discounts/523387"" rel=""nofollow"">This excerpt</a> from the Mahout book has some examples. See Listing 2:</p>

<pre><code>Parameters p = new Parameters();
p.set(""basePath"", modelDir.getCanonicalPath());9
Datastore ds = new InMemoryBayesDatastore(p);
Algorithm a = new BayesAlgorithm();
ClassifierContext ctx = new ClassifierContext(a,ds);
ctx.initialize();

....

ClassifierResult result = ctx.classifyDocument(tokens, defaultCategory);
</code></pre>

<p>Here result should hold either ""positive"" or ""negative"" label.</p>
",3,6,4773,2013-03-07 11:36:29,https://stackoverflow.com/questions/15270145/mahout-for-sentiment-analysis
Sentiment Analysis on LARGE collection of online conversation text,"<p>The title says it all; I have an SQL database bursting at the seams with online conversation text. I've already done most of this project in Python, so I would like to do this using Python's NLTK library (unless there's a <strong>strong</strong> reason not to).</p>

<p>The data is organized by <strong>Thread</strong>, <strong>Username</strong>, and <strong>Post</strong>. Each thread more or less focuses on discussing one ""product"" of the Category that I am interested in analyzing. Ultimately,  when this is finished, I would like to have an estimated opinion (like/dislike sort of deal) from each user for any of the products they had discussed at some point.</p>

<p>So, what I would like to know:</p>

<p>1) <strong>How can I go about determining what product each thread is about?</strong> I was reading about keyword extraction... is that the correct method?</p>

<p>2) <strong>How do I determine a specific users sentiment based on their posts?</strong> From my limited understanding, I must first ""train"" NLTK to recognize certain indicators of opinion, and then do I simply determine the context of those words when they appear in the text?</p>

<p>As you may have guessed by now, I have no prior experience with NLP. From my reading so far, I think I can handle learning it though. Even just a basic and crude working model for now would be great if someone can point me in the right direction. Google was not very helpful to me.</p>

<p><strong>P.S.</strong> I have permission to analyze this data (in case it matters)</p>
","python, nlp, nltk, text-mining, sentiment-analysis","<p>Training any classifier requires a <strong>training set</strong> of <strong>labeled data</strong> and a feature extractor to obtain <strong>feature sets</strong> for each text. After you have a trained classifier, you can apply it to previously unseen text (unlabeled) and obtain a classification based on the machine learning algorithm used. NLTK <a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html"" rel=""nofollow"">gives a good explanation and some samples to play around with</a>.</p>

<p>If you are interested in building a classifier for positive/negative sentiment, using your own training dataset, I would avoid simple keyword counts, as <a href=""http://sentiment.christopherpotts.net/lingcog.html#commitment"" rel=""nofollow"">they aren't accurate for a number of reasons</a> (eg. negation of positive words: ""not happy""). An alternative, where you can still use a large training set without having to manually label anything, is <strong>distant supervision</strong>. Basically, this approach uses <em>emoticons</em> or other specific text elements as <strong>noisy labels</strong>. You still have to choose which features are relevant but many studies have had good results with simply using <em>unigrams</em> or <em>bigrams</em> (individual words or pairs of words, respectively).</p>

<p>All of this can be done relatively easily with Python and NLTK. You can also choose to use a tool like <a href=""https://github.com/japerk/nltk-trainer"" rel=""nofollow"">NLTK-trainer</a>, which is a wrapper for NLTK and requires less code.</p>

<p>I think <a href=""http://cs.wmich.edu/~tllake/fileshare/TwitterDistantSupervision09.pdf"" rel=""nofollow"">this study</a> by Go et al. is one of the easiest to understand. You can also read other studies for <a href=""http://scholar.google.com/scholar?q=distant%20supervision"" rel=""nofollow"">distant supervision</a>, <a href=""http://scholar.google.com/scholar?q=distant%20supervision%20sentiment%20analysis"" rel=""nofollow"">distant supervision sentiment analysis</a>, and <a href=""http://scholar.google.com/scholar?q=twitter%20sentiment%20analysis"" rel=""nofollow"">sentiment analysis</a>. </p>

<p>There are a few built-in classifiers in NLTK with both training and classification methods (<a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.classify.naivebayes.NaiveBayesClassifier-class.html"" rel=""nofollow"">Naive Bayes</a>, <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.classify.maxent.MaxentClassifier-class.html"" rel=""nofollow"">MaxEnt</a>, etc.) but if you are interested in using Support Vector Machines (SVM) then you should look elsewhere. Technically NLTK provides you with an <a href=""http://nltk.org/_modules/nltk/classify/svm.html"" rel=""nofollow"">SVM class</a> but its really just a wrapper for <a href=""https://bitbucket.org/wcauchois/pysvmlight"" rel=""nofollow"">PySVMLight</a>, which itself is a wrapper for <a href=""http://svmlight.joachims.org"" rel=""nofollow"">SVMLight</a>, written in C. I had numerous problems with this approach though, and would instead recommend <a href=""http://www.csie.ntu.edu.tw/~cjlin/libsvm/"" rel=""nofollow"">LIBSVM</a>.</p>

<p>For determining the topic, many have used simple keywords but there are some more complex methods available.</p>
",5,10,5841,2013-03-10 19:44:20,https://stackoverflow.com/questions/15326694/sentiment-analysis-on-large-collection-of-online-conversation-text
How to use SentiWordNet,"<p>I need to do sentiment analysis on some csv files containing tweets. I'm using <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""nofollow"">SentiWordNet</a> to do the sentiment analysis.</p>

<p>I got the following piece of sample java code they provided on their site. I'm not sure how to use it. The path of the csv file that I want to analyze is <code>C:\Users\MyName\Desktop\tweets.csv</code> . The path of the <code>SentiWordNet_3.0.0.txt</code> is <code>C:\Users\MyName\Desktop\SentiWordNet_3.0.0\home\swn\www\admin\dump\SentiWordNet_3.0.0_20130122.txt</code> . I'm new to java, pls help, thanks! The link to the sample java code below is <a href=""http://sentiwordnet.isti.cnr.it/code/SWN3.java"" rel=""nofollow"">this</a>.</p>

<pre><code>import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Set;
import java.util.Vector;

public class SWN3 {
    private String pathToSWN = ""data""+File.separator+""SentiWordNet_3.0.0.txt"";
    private HashMap&lt;String, String&gt; _dict;

    public SWN3(){

        _dict = new HashMap&lt;String, String&gt;();
        HashMap&lt;String, Vector&lt;Double&gt;&gt; _temp = new HashMap&lt;String, Vector&lt;Double&gt;&gt;();
        try{
            BufferedReader csv =  new BufferedReader(new FileReader(pathToSWN));
            String line = """";           
            while((line = csv.readLine()) != null)
            {
                String[] data = line.split(""\t"");
                Double score = Double.parseDouble(data[2])-Double.parseDouble(data[3]);
                String[] words = data[4].split("" "");
                for(String w:words)
                {
                    String[] w_n = w.split(""#"");
                    w_n[0] += ""#""+data[0];
                    int index = Integer.parseInt(w_n[1])-1;
                    if(_temp.containsKey(w_n[0]))
                    {
                        Vector&lt;Double&gt; v = _temp.get(w_n[0]);
                        if(index&gt;v.size())
                            for(int i = v.size();i&lt;index; i++)
                                v.add(0.0);
                        v.add(index, score);
                        _temp.put(w_n[0], v);
                    }
                    else
                    {
                        Vector&lt;Double&gt; v = new Vector&lt;Double&gt;();
                        for(int i = 0;i&lt;index; i++)
                            v.add(0.0);
                        v.add(index, score);
                        _temp.put(w_n[0], v);
                    }
                }
            }
            Set&lt;String&gt; temp = _temp.keySet();
            for (Iterator&lt;String&gt; iterator = temp.iterator(); iterator.hasNext();) {
                String word = (String) iterator.next();
                Vector&lt;Double&gt; v = _temp.get(word);
                double score = 0.0;
                double sum = 0.0;
                for(int i = 0; i &lt; v.size(); i++)
                    score += ((double)1/(double)(i+1))*v.get(i);
                for(int i = 1; i&lt;=v.size(); i++)
                    sum += (double)1/(double)i;
                score /= sum;
                String sent = """";               
                if(score&gt;=0.75)
                    sent = ""strong_positive"";
                else
                if(score &gt; 0.25 &amp;&amp; score&lt;=0.5)
                    sent = ""positive"";
                else
                if(score &gt; 0 &amp;&amp; score&gt;=0.25)
                    sent = ""weak_positive"";
                else
                if(score &lt; 0 &amp;&amp; score&gt;=-0.25)
                    sent = ""weak_negative"";
                else
                if(score &lt; -0.25 &amp;&amp; score&gt;=-0.5)
                    sent = ""negative"";
                else
                if(score&lt;=-0.75)
                    sent = ""strong_negative"";
                _dict.put(word, sent);
            }
        }
        catch(Exception e){e.printStackTrace();}        
    }

    public String extract(String word, String pos)
    {
        return _dict.get(word+""#""+pos);
    }
}
</code></pre>

<p>Newcode:</p>

<pre><code>public class SWN3 {
        private String pathToSWN = ""C:\\Users\\MyName\\Desktop\\SentiWordNet_3.0.0\\home\\swn\\www\\admin\\dump\\SentiWordNet_3.0.0.txt"";
    private HashMap&lt;String, String&gt; _dict;

    public SWN3(){

        _dict = new HashMap&lt;String, String&gt;();
        HashMap&lt;String, Vector&lt;Double&gt;&gt; _temp = new HashMap&lt;String, Vector&lt;Double&gt;&gt;();
        try{
            BufferedReader csv =  new BufferedReader(new FileReader(pathToSWN));
            String line = """";           
            while((line = csv.readLine()) != null)
            {
                String[] data = line.split(""\t"");
                Double score = Double.parseDouble(data[2])-Double.parseDouble(data[3]);
                String[] words = data[4].split("" "");
                for(String w:words)
                {
                    String[] w_n = w.split(""#"");
                    w_n[0] += ""#""+data[0];
                    int index = Integer.parseInt(w_n[1])-1;
                    if(_temp.containsKey(w_n[0]))
                    {
                        Vector&lt;Double&gt; v = _temp.get(w_n[0]);
                        if(index&gt;v.size())
                            for(int i = v.size();i&lt;index; i++)
                                v.add(0.0);
                        v.add(index, score);
                        _temp.put(w_n[0], v);
                    }
                    else
                    {
                        Vector&lt;Double&gt; v = new Vector&lt;Double&gt;();
                        for(int i = 0;i&lt;index; i++)
                            v.add(0.0);
                        v.add(index, score);
                        _temp.put(w_n[0], v);
                    }
                }
            }
            Set&lt;String&gt; temp = _temp.keySet();
            for (Iterator&lt;String&gt; iterator = temp.iterator(); iterator.hasNext();) {
                String word = (String) iterator.next();
                Vector&lt;Double&gt; v = _temp.get(word);
                double score = 0.0;
                double sum = 0.0;
                for(int i = 0; i &lt; v.size(); i++)
                    score += ((double)1/(double)(i+1))*v.get(i);
                for(int i = 1; i&lt;=v.size(); i++)
                    sum += (double)1/(double)i;
                score /= sum;
                String sent = """";               
                if(score&gt;=0.75)
                    sent = ""strong_positive"";
                else
                if(score &gt; 0.25 &amp;&amp; score&lt;=0.5)
                    sent = ""positive"";
                else
                if(score &gt; 0 &amp;&amp; score&gt;=0.25)
                    sent = ""weak_positive"";
                else
                if(score &lt; 0 &amp;&amp; score&gt;=-0.25)
                    sent = ""weak_negative"";
                else
                if(score &lt; -0.25 &amp;&amp; score&gt;=-0.5)
                    sent = ""negative"";
                else
                if(score&lt;=-0.75)
                    sent = ""strong_negative"";
                _dict.put(word, sent);
            }
        }
        catch(Exception e){e.printStackTrace();}        
    }

    public Double extract(String word)
    {
        Double total = new Double(0);
        if(_dict.get(word+""#n"") != null)
             total = _dict.get(word+""#n"") + total;
        if(_dict.get(word+""#a"") != null)
            total = _dict.get(word+""#a"") + total;
        if(_dict.get(word+""#r"") != null)
            total = _dict.get(word+""#r"") + total;
        if(_dict.get(word+""#v"") != null)
            total = _dict.get(word+""#v"") + total;
        return total;
    }

    public String classifytweet(){
        String[] words = twit.split(""\\s+""); 
        double totalScore = 0, averageScore;
        for(String word : words) {
            word = word.replaceAll(""([^a-zA-Z\\s])"", """");
            if (_sw.extract(word) == null)
                continue;
            totalScore += _sw.extract(word);
        }
        Double AverageScore = totalScore;

        if(averageScore&gt;=0.75)
            return ""very positive"";
        else if(averageScore &gt; 0.25 &amp;&amp; averageScore&lt;0.5)
            return  ""positive"";
        else if(averageScore&gt;=0.5)
            return  ""positive"";
        else if(averageScore &lt; 0 &amp;&amp; averageScore&gt;=-0.25)
            return ""negative"";
        else if(averageScore &lt; -0.25 &amp;&amp; averageScore&gt;=-0.5)
            return ""negative"";
        else if(averageScore&lt;=-0.75)
            return ""very negative"";
        return ""neutral"";
    }

    public static void main(String[] args) {
        // TODO Auto-generated method stub
    }
</code></pre>
","java, twitter, sentiment-analysis","<p>First of all start by deleting all the ""garbage"" at the first of the file (which includes description, instruction etc..)</p>

<p>One possible usage is to change <code>SWN3</code> an make the method <code>extract</code> in it return a <code>Double</code>:</p>

<pre><code>public Double extract(String word)
{
    Double total = new Double(0);
    if(_dict.get(word+""#n"") != null)
         total = _dict.get(word+""#n"") + total;
    if(_dict.get(word+""#a"") != null)
        total = _dict.get(word+""#a"") + total;
    if(_dict.get(word+""#r"") != null)
        total = _dict.get(word+""#r"") + total;
    if(_dict.get(word+""#v"") != null)
        total = _dict.get(word+""#v"") + total;
    return total;
}
</code></pre>

<p>Then, giving a String that you want to tag, you can split it so it'll have only words (with no signs and unknown chars) and using the result returned from <code>extract</code> method on each word, you can decide what is the average weight of the String:</p>

<pre><code>String[] words = twit.split(""\\s+""); 
double totalScore = 0, averageScore;
for(String word : words) {
    word = word.replaceAll(""([^a-zA-Z\\s])"", """");
    if (_sw.extract(word) == null)
        continue;
    totalScore += _sw.extract(word);
}
verageScore = totalScore;

if(averageScore&gt;=0.75)
    return ""very positive"";
else if(averageScore &gt; 0.25 &amp;&amp; averageScore&lt;0.5)
    return  ""positive"";
else if(averageScore&gt;=0.5)
    return  ""positive"";
else if(averageScore &lt; 0 &amp;&amp; averageScore&gt;=-0.25)
    return ""negative"";
else if(averageScore &lt; -0.25 &amp;&amp; averageScore&gt;=-0.5)
    return ""negative"";
else if(averageScore&lt;=-0.75)
    return ""very negative"";
return ""neutral"";
</code></pre>

<p>I found this way easier and it works fine for me.</p>

<hr>

<p><strong>UPDATE:</strong></p>

<p>I changed <code>_dict</code> to <code>_dict = new HashMap&lt;String, Double&gt;();</code> So it will have a <code>String</code> key and a <code>Double</code> value.</p>

<p>So I replaced <code>_dict.put(word, sent);</code> wish <code>_dict.put(word, score);</code></p>
",7,5,17822,2013-03-27 06:33:30,https://stackoverflow.com/questions/15653091/how-to-use-sentiwordnet
Emoticons in Twitter Sentiment Analysis in r,"<p>How do I handle/get rid of emoticons so that I can sort tweets for sentiment analysis?</p>

<p>Getting:
Error in sort.list(y) : 
  invalid input </p>

<p>Thanks</p>

<p>and this is how the emoticons come out looking from twitter and into r:</p>

<pre><code>\xed��\xed�\u0083\xed��\xed��
\xed��\xed�\u008d\xed��\xed�\u0089 
</code></pre>
","r, text-mining, iconv, sentiment-analysis","<p>This should get rid of the emoticons, using <code>iconv</code> as suggested by ndoogan.</p>

<p>Some reproducible data:</p>

<pre><code>require(twitteR) 
# note that I had to register my twitter credentials first
# here's the method: http://stackoverflow.com/q/9916283/1036500
s &lt;- searchTwitter('#emoticons', cainfo=""cacert.pem"") 

# convert to data frame
df &lt;- do.call(""rbind"", lapply(s, as.data.frame))

# inspect, yes there are some odd characters in row five
head(df)

                                                                                                                                                text
1                                                                      ROFLOL: echte #emoticons [humor] http://t.co/0d6fA7RJsY via @tweetsmania  ;-)
2 “@teeLARGE: when tmobile get the iphone in 2 wks im killin everybody w/ emoticons &amp;amp; \nall the other stuff i cant see on android!"" \n#Emoticons
3                      E poi ricevi dei messaggi del genere da tua mamma xD #crazymum #iloveyou #emoticons #aiutooo #bestlike http://t.co/Yee1LB9ZQa
4                                                #emoticons I want to change my name to an #emoticon. Is it too soon? #prince http://t.co/AgmR5Lnhrk
5  I use emoticons too much. #addicted #admittingit #emoticons &lt;ed&gt;&lt;U+00A0&gt;&lt;U+00BD&gt;&lt;ed&gt;&lt;U+00B8&gt;&lt;U+00AC&gt;&lt;ed&gt;&lt;U+00A0&gt;&lt;U+00BD&gt;&lt;ed&gt;&lt;U+00B8&gt;&lt;U+0081&gt; haha
6                                                                                         What you text What I see #Emoticons http://t.co/BKowBSLJ0s
</code></pre>

<p><strong>Here's the key line that will remove the emoticons:</strong></p>

<pre><code># Clean text to remove odd characters
df$text &lt;- sapply(df$text,function(row) iconv(row, ""latin1"", ""ASCII"", sub=""""))
</code></pre>

<p>Now inspect again, to see if the odd characters are gone (see row 5)</p>

<pre><code>head(df)    
                                                                                                                               text
1                                                                     ROFLOL: echte #emoticons [humor] http://t.co/0d6fA7RJsY via @tweetsmania  ;-)
2 @teeLARGE: when tmobile get the iphone in 2 wks im killin everybody w/ emoticons &amp;amp; \nall the other stuff i cant see on android!"" \n#Emoticons
3                     E poi ricevi dei messaggi del genere da tua mamma xD #crazymum #iloveyou #emoticons #aiutooo #bestlike http://t.co/Yee1LB9ZQa
4                                               #emoticons I want to change my name to an #emoticon. Is it too soon? #prince http://t.co/AgmR5Lnhrk
5                                                                                 I use emoticons too much. #addicted #admittingit #emoticons  haha
6                                                                                        What you text What I see #Emoticons http://t.co/BKowBSLJ0s
</code></pre>
",22,19,14894,2013-04-01 17:25:32,https://stackoverflow.com/questions/15748190/emoticons-in-twitter-sentiment-analysis-in-r
Getting incorrect Score using SentiWordNet,"<p>I'm doing some sentiment analysis using SentiWordNet and I referred to the post here <a href=""https://stackoverflow.com/questions/15653091/how-to-use-sentiwordnet"">How to use SentiWordNet</a> . However, I'm getting a score of 0.0 despite trying out various inputs. Is there anything I'm doing wrong here? Thanks!</p>

<pre><code>    import java.io.BufferedReader;
    import java.io.File;
    import java.io.FileReader;
    import java.util.HashMap;
    import java.util.Iterator;
    import java.util.Set;
    import java.util.Vector;

    public class SWN3 {
        private String pathToSWN = ""C:\\Users\\Malcolm\\Desktop\\SentiWordNet_3.0.0\\home\\swn\\www\\admin\\dump\\SentiWordNet_3.0.0.txt"";
        private HashMap&lt;String, Double&gt; _dict;

        public SWN3(){

            _dict = new HashMap&lt;String, Double&gt;();
            HashMap&lt;String, Vector&lt;Double&gt;&gt; _temp = new HashMap&lt;String, Vector&lt;Double&gt;&gt;();
            try{
                BufferedReader csv =  new BufferedReader(new FileReader(pathToSWN));
                String line = """";           
                while((line = csv.readLine()) != null)
                {
                    String[] data = line.split(""\t"");
                    Double score = Double.parseDouble(data[2])-Double.parseDouble(data[3]);
                    String[] words = data[4].split("" "");
                    for(String w:words)
                    {
                        String[] w_n = w.split(""#"");
                        w_n[0] += ""#""+data[0];
                        int index = Integer.parseInt(w_n[1])-1;
                        if(_temp.containsKey(w_n[0]))
                        {
                            Vector&lt;Double&gt; v = _temp.get(w_n[0]);
                            if(index&gt;v.size())
                                for(int i = v.size();i&lt;index; i++)
                                    v.add(0.0);
                            v.add(index, score);
                            _temp.put(w_n[0], v);
                        }
                        else
                        {
                            Vector&lt;Double&gt; v = new Vector&lt;Double&gt;();
                            for(int i = 0;i&lt;index; i++)
                                v.add(0.0);
                            v.add(index, score);
                            _temp.put(w_n[0], v);
                        }
                    }
                }
                Set&lt;String&gt; temp = _temp.keySet();
                for (Iterator&lt;String&gt; iterator = temp.iterator(); iterator.hasNext();) {
                    String word = (String) iterator.next();
                    Vector&lt;Double&gt; v = _temp.get(word);
                    double score = 0.0;
                    double sum = 0.0;
                    for(int i = 0; i &lt; v.size(); i++)
                        score += ((double)1/(double)(i+1))*v.get(i);
                    for(int i = 1; i&lt;=v.size(); i++)
                        sum += (double)1/(double)i;
                    score /= sum;
                    String sent = """";               
                    if(score&gt;=0.75)
                        sent = ""strong_positive"";
                    else
                    if(score &gt; 0.25 &amp;&amp; score&lt;=0.5)
                        sent = ""positive"";
                    else
                    if(score &gt; 0 &amp;&amp; score&gt;=0.25)
                        sent = ""weak_positive"";
                    else
                    if(score &lt; 0 &amp;&amp; score&gt;=-0.25)
                        sent = ""weak_negative"";
                    else
                    if(score &lt; -0.25 &amp;&amp; score&gt;=-0.5)
                        sent = ""negative"";
                    else
                    if(score&lt;=-0.75)
                        sent = ""strong_negative"";
                    _dict.put(word, score);
                }
            }
            catch(Exception e){e.printStackTrace();}        
        }

public Double extract(String word)
{
    Double total = new Double(0);
    if(_dict.get(word+""#n"") != null)
         total = _dict.get(word+""#n"") + total;
    if(_dict.get(word+""#a"") != null)
        total = _dict.get(word+""#a"") + total;
    if(_dict.get(word+""#r"") != null)
        total = _dict.get(word+""#r"") + total;
    if(_dict.get(word+""#v"") != null)
        total = _dict.get(word+""#v"") + total;
    return total;
}

public static void main(String[] args) {
    SWN3 test = new SWN3();
    String sentence=""Hello have a Super awesome great day"";
    String[] words = sentence.split(""\\s+""); 
    double totalScore = 0;
    for(String word : words) {
        word = word.replaceAll(""([^a-zA-Z\\s])"", """");
        if (test.extract(word) == null)
            continue;
        totalScore += test.extract(word);
    }
    System.out.println(totalScore);
}

}
</code></pre>

<p>Here's the first 10 lines of SentiWordNet.txt</p>

<pre><code>a   00001740    0.125   0   able#1  (usually followed by `to') having the necessary means or skill or know-how or authority to do something; ""able to swim""; ""she was able to program her computer""; ""we were at last able to buy a car""; ""able to get a grant for the project""
a   00002098    0   0.75    unable#1    (usually followed by `to') not having the necessary means or skill or know-how; ""unable to get to town without a car""; ""unable to obtain funds""
a   00002312    0   0   dorsal#2 abaxial#1  facing away from the axis of an organ or organism; ""the abaxial surface of a leaf is the underside or side facing away from the stem""
a   00002527    0   0   ventral#2 adaxial#1 nearest to or facing toward the axis of an organ or organism; ""the upper side of a leaf is known as the adaxial surface""
a   00002730    0   0   acroscopic#1    facing or on the side toward the apex
a   00002843    0   0   basiscopic#1    facing or on the side toward the base
a   00002956    0   0   abducting#1 abducent#1  especially of muscles; drawing away from the midline of the body or from an adjacent part
a   00003131    0   0   adductive#1 adducting#1 adducent#1  especially of muscles; bringing together or drawing toward the midline of the body or toward an adjacent part
a   00003356    0   0   nascent#1   being born or beginning; ""the nascent chicks""; ""a nascent insurgency""
a   00003553    0   0   emerging#2 emergent#2   coming into existence; ""an emergent republic""
</code></pre>
","java, twitter, sentiment-analysis","<p>Usually the <code>SentiWord.txt</code> file comes with a weird format.</p>

<p>You need to remove the first part of it (which includes comments and instructions) and the last two lines:</p>

<pre><code>#
EMPTY LINE
</code></pre>

<p>The parser doesn't know how to handle these situations, if you delete these extra two lines you'll be fine.</p>
",5,3,2410,2013-04-06 04:31:45,https://stackoverflow.com/questions/15847025/getting-incorrect-score-using-sentiwordnet
KNN classifier sentiment analysis vs category analysis precision,"<p>I have implemented the KNN classifier in java and I got a strange result. If I do a sentiment analysis on a dataset example amazon books review I got 55% precision. From 100 test document 55 correctly classified as negative or positive review and 45 incorrectly. But If I use the KNN for category classification example camera or books then I got 95% precision. </p>

<p>There are some explanation my code is wrong?  Any idea?</p>
","machine-learning, sentiment-analysis, document-classification","<p>@Christopher Pfohl is right. They are different approaches with one key difference for you. Sentiment analysis (based on simple Bag of Words) is much more complicated, in general, than category classification in your case.</p>

<p>Btw, just one clarification, 55% is not precision, that is accuracy.
(More info: <a href=""http://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification"" rel=""nofollow"">http://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification</a>)</p>
",3,0,1344,2013-04-23 16:35:03,https://stackoverflow.com/questions/16174656/knn-classifier-sentiment-analysis-vs-category-analysis-precision
Sentiment Analysis in Spanish - Dictionary,"<p>I'm interested in doing sentiment analysis in spanish. I've looked up in the web for an opinions dictionary and it seems imposible to find something similar as the existing ones in english. For example, <a href=""http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon"" rel=""nofollow"">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon</a></p>

<p>I'm new to the field and would be interested in some guidance, so I'm open to suggestions. How useful, for instance, would a standard Spanish dictionary be?</p>

<p>The object of study would be twitter posts related to politics.</p>

<p>Thanks for your help!</p>
",sentiment-analysis,"<p>I don't know if something like this already exists, but making one seems like it would definitely be a valuable thing to do for the community--though it might be a fair amount of work.</p>

<p>A standard Spanish dictionary probably won't really help you create a sentiment dictionary from scratch unless you're planning to manually assign sentiment values to a very large set of Spanish words. An English-to-Spanish dictionary might help you translate an English sentiment dictionary into a Spanish one, which, if nothing else, would probably be a good start, though it would be woefully lacking in its lack of common idioms, misspellings, and so forth.</p>

<p>One way that you <em>could</em> try using a standard Spanish dictionary would be to take the ""starting point"" you get from the above translation process and apply it to the <em>definitions</em> of Spanish words and phrases for which you haven't yet assigned a sentiment value; this would give you an easy way to extend your sentiment dictionary, though it might not be very accurate.</p>

<p>Good luck!</p>
",6,3,3087,2013-05-04 02:54:54,https://stackoverflow.com/questions/16370121/sentiment-analysis-in-spanish-dictionary
Sentimental analysis of tweets in python using a machine learning algorithm,"<p>say I download 'n' number of tweets and remove words with length &lt;= 2 from them and then label each tweet as 'Negative' or 'Non negative', so that this forms my training set.</p>

<p>but instead of having well defined attributes like how an Iris data-set has Sepal Length, Sepal Width, Petal Length and Petal Width, in my data-set simply every word becomes an attribute and different example tweets will have different number of attributes.</p>

<p>Can I use this data-set and consider my problem as a classification problem ? and try to predict whether a new tweet is Negative or Non-Negative?  </p>

<p>or what would you suggest as the best way to predict whether a tweet is negative or not ?</p>
","data-analysis, sentiment-analysis","<p>You are describing a standar text classification problem. In this setting, the set of features  is a (finite) set of words instead of the Sepal length, width, ... </p>

<p>As a result, each document is represented with respect to all such features (all documents have the same number of features) but most of the values will be zero, creating a very sparse vector.</p>

<p>This is the best way to predict polarity/sentiment but you should improve your knowledge of the topic a bit more. I would suggest a read of <a href=""http://nmis.isti.cnr.it/sebastiani/Publications/ACMCS02.pdf"" rel=""nofollow"">Sebastiani's survey on Text Classification</a>.</p>

<p>Regards,</p>
",4,3,752,2013-06-13 09:37:17,https://stackoverflow.com/questions/17083821/sentimental-analysis-of-tweets-in-python-using-a-machine-learning-algorithm
Transforming Curl into Python using Urllib with Sentiment140 API,"<p>I am trying to use Python to request data from the Sentiment140 API.
The API is using a Bulk Classification Service (JSON). Within terminal it is working fine</p>

<pre><code>curl -d ""{'data': [{'text': 'I love Titanic.'}, {'text': 'I hate Titanic.'}]}"" http://www.sentiment140.com/api/bulkClassifyJson
</code></pre>

<p>leading to the following response:</p>

<pre><code>{""data"":[{""text"":""I love Titanic."",""polarity"":4,""meta"":{""language"":""en""}},{""text"":""I hate Titanic."",""polarity"":0,""meta"":{""language"":""en""}}]}
</code></pre>

<p>I thought I could just use urllib to get the same response from my python code. I tried:</p>

<pre><code>import urllib
import urllib2

url = 'http://www.sentiment140.com/api/bulkClassifyJson'
values = {'data': [{'text': 'I love Titanic.'}, {'text': 'I hate Titanic.'}]}

data = urllib.urlencode(values)
response = urllib2.urlopen(url, data)
page = response.read()
</code></pre>

<p>The code works but it does not give me any results.
Am I missing something?</p>
","python, json, curl, urllib2, sentiment-analysis","<p>I think you need to use json here.</p>

<p>Try to do:</p>

<pre><code>data = json.dumps(values) # instead of urllib.urlencode(values)
response = urllib2.urlopen(url, data)
page = response.read()
</code></pre>

<p>and on the top</p>

<pre><code>import json 
</code></pre>
",4,1,647,2013-07-09 09:43:20,https://stackoverflow.com/questions/17545038/transforming-curl-into-python-using-urllib-with-sentiment140-api
Creating a sentiment analysis tool,"<p>I'm trying to create a sentiment analysis tool to analyse tweets over a three day period about Manchester United football club and determine whether people view them positively or negatively.
I am currently using this guide for guidance (with Java being my coding language)</p>

<p><a href=""http://cavajohn.blogspot.co.uk/2013/05/how-to-sentiment-analysis-of-tweets.html"" rel=""nofollow"">http://cavajohn.blogspot.co.uk/2013/05/how-to-sentiment-analysis-of-tweets.html</a></p>

<p>I am using Apache Flume to download my tweets into Apache Hadoop and then am intending to use Apache Hive to query the tweets. I may also use Apache Oozie to partition the tweets effectively.</p>

<p>In the link I posted above, it is mentioned that I need to have a training dataset to train the classifier I will create to analyse the tweets. The sample classifier provided has some 5000 tweets. As I am doing this for a summer project for uni, I feel I should probably create my own dataset.</p>

<p>What is the minimum amount of tweets I should use to make this classifier effective? Is there a recommended number? For example, if I manually analysed a hundred tweets, or five hundred, or a thousand, would it be effective?</p>
","java, hadoop, twitter4j, sentiment-analysis","<p>There is not a exact number to train a classifier. You can have a large dataset where all the data has the same attributes so you classifier will memorize a pattern, or, you can have a no so big dataset with good instances so you classifier will have better results.</p>

<p>You can train the classifier using the sample dataset that they give you in the post and use the <a href=""http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29"" rel=""nofollow"">cross validation</a> in order to get the best classifier. </p>

<p>After you got the best classifier, you can compare your classifier with the classifier provided in the post and choose the better.</p>
",1,3,2753,2013-07-24 12:03:35,https://stackoverflow.com/questions/17833489/creating-a-sentiment-analysis-tool
Sentiment analysis on JSON tweets in Hadoop HDFS,"<p>I've used Apache Flume to pipe a large amount of tweets into the HDFS of Hadoop. I was trying to do sentiment analysis on this data - just something simple to begin with, like positive v negative word comparison.</p>

<p>My problem is that all the guides I find showing me how to do it have a text file of positive and negative words and then a huge text file with every tweet.</p>

<p>As I used Flume, all my data is already in Hadoop. When I access it using localhost:50070 I can see the data, in separate files according to month/day/hour, with each file containing three or four tweets. I have maybe 50 of these files for every hour. Although it doesn't say anywhere, I'm assuming they are in JSON format.</p>

<p>Bearing this in mind, how can I perform my analysis on them? In all the examples I've seen where the Mapper and Reducer have been written, there has been a single file this has been performed on, not a large collection of small JSON files. What should my next step be?</p>
","java, hadoop, sentiment-analysis","<p>This example should get you started
<a href=""https://github.com/cloudera/cdh-twitter-example"" rel=""nofollow"">https://github.com/cloudera/cdh-twitter-example</a></p>

<p>Basically use hive external table to map your json data and query using hiveql</p>
",1,2,4292,2013-08-04 00:48:43,https://stackoverflow.com/questions/18038735/sentiment-analysis-on-json-tweets-in-hadoop-hdfs
What is this dictionary assignment doing?,"<p>I am learning Python and am trying to use it to perform sentiment analysis. I am following an online tutorial from this link: <a href=""http://www.alex-hanna.com/tworkshops/lesson-6-basic-sentiment-analysis/"" rel=""nofollow"">http://www.alex-hanna.com/tworkshops/lesson-6-basic-sentiment-analysis/</a>. I have taken a piece of code as a mapper class, an excerpt of which looks like this:</p>

<pre><code>sentimentDict = {
    'positive': {},
    'negative': {}
}

def loadSentiment():
    with open('Sentiment/positive_words.txt', 'r') as f:
        for line in f:
            sentimentDict['positive'][line.strip()] = 1

    with open('Sentiment/negative_words.txt', 'r') as f:
        for line in f:
            sentimentDict['negative'][line.strip()] = 1
</code></pre>

<p>Here, I can see that a new dictionary is created with two keys, positive and negative, but no values.    </p>

<p>Following this, two text files are opened and each line is stripped and mapped to the dictionary.  </p>

<p>However, what is the = 1 part for? Why is this required (and if it isn't how could it be removed?)</p>
","python, mapreduce, nltk, sentiment-analysis","<p>The loop creates a nested dictionary, and sets all values to 1, presumably to then just use the keys as a way to weed out duplicate values.</p>

<p>You could use sets instead and avoid the <code>= 1</code> value:</p>

<pre><code>sentimentDict = {}

def loadSentiment():
    with open('Sentiment/positive_words.txt', 'r') as f:
        sentimentDict['positive'] = {line.strip() for line in f}

    with open('Sentiment/negative_words.txt', 'r') as f:
        sentimentDict['negative'] = {line.strip() for line in f}
</code></pre>

<p>Note that you don't even need to create the initial dictionaries; you can create the whole set with one statement, a set comprehension.</p>

<p>If other code <em>does</em> rely on dictionaries with the values being set to <code>1</code> (perhaps to update counts at a later stage), it'd be more performant to use the <code>dict.fromkeys()</code> class method instead:</p>

<pre><code>sentimentDict = {}

def loadSentiment():
    with open('Sentiment/positive_words.txt', 'r') as f:
        sentimentDict['positive'] = dict.fromkeys((line.strip() for line in f), 1)

    with open('Sentiment/negative_words.txt', 'r') as f:
        sentimentDict['negative'] = dict.fromkeys((line.strip() for line in f), 1)
</code></pre>

<p>Looking at your <a href=""http://www.alex-hanna.com/tworkshops/lesson-6-basic-sentiment-analysis/"" rel=""nofollow"">source blog article</a> however shows that the dictionaries are only used to do membership testing against the keys, so using sets here is much better and transparent to the rest of the code to boot.</p>
",7,2,180,2013-08-07 15:10:37,https://stackoverflow.com/questions/18107104/what-is-this-dictionary-assignment-doing
What exactly is an n Gram?,"<p>I found this previous question on SO: <a href=""https://stackoverflow.com/questions/1032288/n-grams-explanation-2-applications"">N-grams: Explanation + 2 applications</a>. The OP gave this example and asked if it was correct:</p>

<pre><code>Sentence: ""I live in NY.""

word level bigrams (2 for n): ""# I', ""I live"", ""live in"", ""in NY"", 'NY #'
character level bigrams (2 for n): ""#I"", ""I#"", ""#l"", ""li"", ""iv"", ""ve"", ""e#"", ""#i"", ""in"", ""n#"", ""#N"", ""NY"", ""Y#""

When you have this array of n-gram-parts, you drop the duplicate ones and add a counter for each part giving the frequency:

word level bigrams: [1, 1, 1, 1, 1]
character level bigrams: [2, 1, 1, ...]
</code></pre>

<p>Someone in the answer section confirmed this was correct, but unfortunately I'm a bit lost beyond that as I didn't fully understand everything else that was said! I'm using LingPipe and following a tutorial which stated I should choose a value between 7 and 12 - but without stating why.</p>

<p>What is a good nGram value and how should I take it into account when using a tool like LingPipe?</p>

<p>Edit: This was the tutorial: <a href=""http://cavajohn.blogspot.co.uk/2013/05/how-to-sentiment-analysis-of-tweets.html"" rel=""noreferrer"">http://cavajohn.blogspot.co.uk/2013/05/how-to-sentiment-analysis-of-tweets.html</a></p>
",sentiment-analysis,"<p>N-grams are simply all combinations of adjacent words or letters of length <em>n</em> that you can find in your source text. For example, given the word <code>fox</code>, all 2-grams (or “bigrams”) are <code>fo</code> and <code>ox</code>. You may also count the word boundary – that would expand the list of 2-grams to <code>#f</code>, <code>fo</code>, <code>ox</code>, and <code>x#</code>, where <code>#</code> denotes a word boundary.</p>

<p>You can do the same on the word level. As an example, the <code>hello, world!</code> text contains the following word-level bigrams: <code># hello</code>, <code>hello world</code>, <code>world #</code>.</p>

<p>The basic point of n-grams is that they capture the language structure from the statistical point of view, like what letter or word is likely to follow the given one. The longer the n-gram (the higher the <em>n</em>), the more context you have to work with. Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases.</p>
",47,30,44618,2013-08-12 17:40:17,https://stackoverflow.com/questions/18193253/what-exactly-is-an-n-gram
Implementing scikit-learn machine learning algorithm,"<p>Linked: <a href=""https://stackoverflow.com/questions/18154278/is-there-a-maximum-size-for-the-nltk-naive-bayes-classifer"">https://stackoverflow.com/questions/18154278/is-there-a-maximum-size-for-the-nltk-naive-bayes-classifer</a></p>

<p>I'm having trouble implementing a scikit-learn machine learning algorithm in my code. One of the authors of the scikit-learn kindly helped me in the question I linked above, but I can't quite get it working and as my original question was about a different matter, I thought it would be best to open a new one.</p>

<p>This code is taking an input of tweets and reading their text and sentiment into a dictionary. It then parses each line of text and adds the text to one list and its sentiment to another (at the advice of the author in the linked question above).</p>

<p>However, despite using the code in the link and looking up the API as best I can, I think I am missing something. Running the code below gives me first a bunch of output separated by a colon, like this:</p>

<pre><code>  (0, 299)  0.270522159585
  (0, 271)  0.32340892262
  (0, 266)  0.361182814311
  : :
  (48, 123) 0.240644787937
</code></pre>

<p>followed by: </p>

<pre><code>['negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', etc]
</code></pre>

<p>and then:</p>

<pre><code>ValueError: empty vocabulary; perhaps the documents only contain stop words
</code></pre>

<p>Am I assigning the classifier in the wrong way? This is my code:</p>

<pre><code>test_file = 'RawTweetDataset/SmallSample.csv'
#test_file = 'RawTweetDataset/Dataset.csv'
sample_tweets = 'SampleTweets/FlumeData2.txt'
csv_file = csv.DictReader(open(test_file, 'rb'), delimiter=',', quotechar='""')

tweetsDict = {}

for line in csv_file:
    tweetsDict.update({(line['SentimentText'],line['Sentiment'])})

tweets = []
labels = []
shortenedText = """"
for (text, sentiment) in tweetsDict.items():
    text = HTMLParser.HTMLParser().unescape(text.decode(""cp1252"", ""ignore""))
    exclude = set(string.punctuation)
    for punct in string.punctuation:
        text = text.replace(punct,"""")
    cleanedText = [e.lower() for e in text.split() if not e.startswith(('http', '@'))]
    shortenedText = [e.strip() for e in cleanedText if e not in exclude]

    text = ' '.join(ch for ch in shortenedText if ch not in exclude)
    tweets.append(text.encode(""utf-8"", ""ignore""))
    labels.append(sentiment)

vectorizer = TfidfVectorizer(input='content')
X = vectorizer.fit_transform(tweets)
y = labels
classifier = MultinomialNB().fit(X, y)

X_test = vectorizer.fit_transform(sample_tweets)
y_pred = classifier.predict(X_test)
</code></pre>

<p>Update: Current code:</p>

<pre><code>all_files = glob.glob (tweet location)
for filename in all_files:
    with open(filename, 'r') as file:
        for line file.readlines():
            X_test = vectorizer.transform([line])
            y_pred = classifier.predict(X_test)
            print line
            print y_pred
</code></pre>

<p>This always produces something like:</p>

<pre><code>happy bday trish
['negative'] &lt;&lt; Never changes, always negative
</code></pre>
","python, scikit-learn, sentiment-analysis","<p>The problem is here:</p>

<pre><code>X_test = vectorizer.fit_transform(sample_tweets)
</code></pre>

<p><code>fit_transform</code> is intended to be called on the training set, not the test set. On the test set, call <code>transform</code>.</p>

<p>Also, <code>sample_tweets</code> is a filename. You should open it and read the tweets from it before passing it to a vectorizer. If you do that, then you should finally be able to do something like</p>

<pre><code>for tweet, sentiment in zip(list_of_sample_tweets, y_pred):
    print(""Tweet: %s"" % tweet)
    print(""Sentiment: %s"" % sentiment)
</code></pre>
",6,1,1474,2013-08-13 22:05:05,https://stackoverflow.com/questions/18220015/implementing-scikit-learn-machine-learning-algorithm
Where can I find sentiment based categorical dictionary?,"<p>I want sentiment based category wise dictionary which categories words like fruits, vehicles, conjunctions, articles etc. </p>

<p>Dictionary which categorize negative and positive words are available easily but this I could not find out.</p>
","dictionary, data-mining, sentiment-analysis","<p><a href=""http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010"" rel=""nofollow"">AFINN-111</a> is the basic sentiment index but good luck trying to make a fruit happy. You can see the list <a href=""https://github.com/uwescience/datasci_course_materials/blob/master/assignment1/AFINN-111.txt"" rel=""nofollow"">here</a>.</p>

<p>One thing you can do is to append to AFINN-111 by giving banana +1 every time it appears in a sentence with a positive word, and -1 when it appears with a negative word. This way you can analyze if banana is showing up in more positive or negative context.</p>
",2,-1,906,2013-08-15 05:41:58,https://stackoverflow.com/questions/18246964/where-can-i-find-sentiment-based-categorical-dictionary
Understanding this application of a Naive Bayes Classifier,"<p>I'm a little confused with this example I've been following online. Please correct me if anything is wrong before I get to my question! I know Bayes theorem is this:</p>

<pre><code>P(A│B)= P(B│A) * P(A)  
         ----------             
            P(B)
</code></pre>

<p>In the example I'm looking at, classifying is being done on text documents. The text documents are all either ""terrorism"" or ""entertainment"", so:</p>

<pre><code>Prior probability for either, i.e. P(A) = 0.5
</code></pre>

<p>There are six documents with word frequencies like so:</p>

<p><img src=""https://i.sstatic.net/e8pgW.png"" alt=""enter image description here""></p>

<p>The example goes on to break down the frequency of these words in relation to each class, applying Laplace estimation:</p>

<p><img src=""https://i.sstatic.net/aK2uM.png"" alt=""enter image description here""></p>

<p>So to my understanding each of these numbers represents the P(B|A), i.e. the probability of that word appearing given a particular class (either terrorism or entertainment).</p>

<p>Now a new document arrives, with this breakdown:</p>

<p><img src=""https://i.sstatic.net/HcRlj.png"" alt=""enter image description here""></p>

<p>The example calculates the probability of this new text document relating to terrorism by doing this:</p>

<pre><code>P(Terrorism | W) = P(Terrorism) x P(kill | Terrorism) x P(bomb | Terrorism) x P(kidnap | Terrorism) x P(music | Terrorism) x P(movie | Terrorism) x P(TV | Terrorism)
</code></pre>

<p>which works out as:</p>

<pre><code>0.5 x 0.2380 x 0.1904 x 0.3333 x 0.0476 x 0.0952 x 0.0952
</code></pre>

<p>Again, up to now I think I'm following. The P(Terrorism | W) is P (A|B), P(Terrorism) = P(A) = 0.5 and P(B|A) = all the results for ""terrorism"" in the above table multiplied together.</p>

<p>But to apply it to this new document, the example calculates each of the P(B|A) above to the power of the new frequency. So the above calculation becomes:</p>

<pre><code>0.5 x 0.2380^2 x 0.1904^1 x 0.3333^2 x 0.0476^0 x 0.0952^0 x 0.0952^1
</code></pre>

<p>From there they do a few sums which I get and find the answer. My question is:</p>

<p><strong>Where in the formula does it say to apply the new frequency as a power to the current P(B|A)?</strong></p>

<p>Is this just something statistical I don't know about? Is this universal or just a particular example of how to do it? I'm asking because all the examples I find are slightly different, using slightly different keywords and terms and I'm finding it just a tad confusing!</p>
","bayesian, sentiment-analysis","<p>First of all, the formula</p>

<pre><code>P(Terrorism | W) = P(Terrorism) x P(kill | Terrorism) x P(bomb | Terrorism) x P(kidnap | Terrorism) x P(music | Terrorism) x P(movie | Terrorism) x P(TV | Terrorism)
</code></pre>

<p>isn't quite right. You need to divide that by <code>P(W)</code>. But you hint that this is taken care of later when it says that ""they do a few sums"", so we can move on to your main question.</p>

<hr>

<p>Traditionally when doing Naive Bayes on text classification, you only look at the <em>existence</em> of words, not their counts. Of course you need the counts to estimate <code>P(word | class)</code> at train time, but at test time <code>P(""music"" | Terrorism)</code> typically means the probability that the word ""music"" is present at least once in a Terrorism document.</p>

<p>It looks like what the implementation you are dealing with is doing is it's trying to take into account <code>P(""occurrences of kill"" = 2 | Terrorism)</code> which is different from <code>P(""at least 1 occurrence of kill"" | Terrorism)</code>. So why do they end up raising probabilities to powers? It looks like their reasoning is that <code>P(""kill"" | Terrorism)</code> (which they estimated at train time) represents the probability of an arbitrary word in a Terrorism document to be ""kill"". So by simplifying assumption, the probability of a <em>second</em> arbitrary word in a Terrorism document to be ""kill"" is also <code>P(""kill"" | Terrorism)</code>.</p>

<p>This leaves a slight problem for the case that a word does not occur in a document. With this scheme, the corresponding probability is raised to the 0th power, in other words it goes away. In other words, it is approximating that <code>P(""occurrences of music"" = 0 | Terrorism) = 1</code>. It should be clear that in general, this is strictly speaking false since it would imply that <code>P(occurrences of music"" &gt; 0 | Terrorism) = 0</code>. But for real world examples where you have long documents and thousands or tens of thousands of words, <em>most words don't occur in most documents</em>. So instead of bothering with accurately calculating all those probabilities (which would be computationally expensive), they are basically swept under the rug because for the vast majority of cases, it wouldn't change the classification outcome anyway. Also note that on top of it being computationally intensive, it is numerically unstable because if you are multiplying thousands or tens of thousands of numbers less than 1 together, you will underflow and it will spit out 0; if you do it in log space, you are still adding tens of thousands of numbers together which would have to be handled delicately from a numerical stability point of view. So the ""raising it to a power"" scheme inherently removes unnecessary fluff, decreasing computational intensity, increasing numerical stability, and still yields nearly identical results.</p>

<hr>

<p>I hope the NSA doesn't think I'm a terrorist for having used the word Terrorism so much in this answer :S</p>
",1,0,570,2013-08-29 14:15:05,https://stackoverflow.com/questions/18513413/understanding-this-application-of-a-naive-bayes-classifier
Why are NLP processes considered language-dependent?,"<p>Why are NLP Processes considered language-dependent? </p>

<p>For example, here:
<a href=""http://www.slideshare.net/saschanarr/languageindependent-twitter-sentiment-analysis"" rel=""nofollow"">http://www.slideshare.net/saschanarr/languageindependent-twitter-sentiment-analysis</a>
on slide 6, its says that: ""Natural Language Processing methods are often designed specifically for one language"".</p>

<p>Why is it so? I would think that once the method is implemented using machine learning, the algorithm is the same and all you need different is the training set...</p>
","machine-learning, nlp, artificial-intelligence, sentiment-analysis","<p>In the case of heuristics, those are usually problem- and language-dependent. In the case of machine learning, yes, in an abstract, theoretical sense, the ""only"" difference is the training set. The availability of training sets for various languages is the first problem. Then comes the number of useful features that can be pruned from the training set, the availability of heuristics and knowledge sources to improve the machine learning, the hyperparameters required to make the learning successful, etc.</p>

<p>As an example, consider the problem of named-entity recognition (NER). On English data, the ""word is capitalized"" feature is almost a giveaway for spotting the names, but in German, every noun is capitalized. The result is that NER for German is quite a different problem than it is for English.</p>
",2,-4,195,2013-09-17 17:38:31,https://stackoverflow.com/questions/18856369/why-are-nlp-processes-considered-language-dependent
Python NLTK not sentiment calculate correct,"<p>I do have some positive and negative sentence. I want very simple to use Python NLTK to train a NaiveBayesClassifier for investigate sentiment for other sentence.</p>

<p>I try to use this code, but my result is always positive.
<a href=""http://www.sjwhitworth.com/sentiment-analysis-in-python-using-nltk/"" rel=""nofollow"">http://www.sjwhitworth.com/sentiment-analysis-in-python-using-nltk/</a></p>

<p>I am very new at python so there my be a mistake in the code when i copy it.</p>

<pre><code>import nltk
import math
import re
import sys
import os
import codecs
reload(sys)
sys.setdefaultencoding('utf-8')

from nltk.corpus import stopwords

__location__ = os.path.realpath(
    os.path.join(os.getcwd(), os.path.dirname(__file__)))

postweet = __location__ + ""/postweet.txt""
negtweet = __location__ + ""/negtweet.txt""


customstopwords = ['band', 'they', 'them']

#Load positive tweets into a list
p = open(postweet, 'r')
postxt = p.readlines()

#Load negative tweets into a list
n = open(negtweet, 'r')
negtxt = n.readlines()

neglist = []
poslist = []

#Create a list of 'negatives' with the exact length of our negative tweet list.
for i in range(0,len(negtxt)):
    neglist.append('negative')

#Likewise for positive.
for i in range(0,len(postxt)):
    poslist.append('positive')

#Creates a list of tuples, with sentiment tagged.
postagged = zip(postxt, poslist)
negtagged = zip(negtxt, neglist)

#Combines all of the tagged tweets to one large list.
taggedtweets = postagged + negtagged

tweets = []

#Create a list of words in the tweet, within a tuple.
for (word, sentiment) in taggedtweets:
    word_filter = [i.lower() for i in word.split()]
    tweets.append((word_filter, sentiment))

#Pull out all of the words in a list of tagged tweets, formatted in tuples.
def getwords(tweets):
    allwords = []
    for (words, sentiment) in tweets:
        allwords.extend(words)
    return allwords

#Order a list of tweets by their frequency.
def getwordfeatures(listoftweets):
    #Print out wordfreq if you want to have a look at the individual counts of words.
    wordfreq = nltk.FreqDist(listoftweets)
    words = wordfreq.keys()
    return words

#Calls above functions - gives us list of the words in the tweets, ordered by freq.
print getwordfeatures(getwords(tweets))

wordlist = [] 
wordlist = [i for i in wordlist if not i in stopwords.words('english')]
wordlist = [i for i in wordlist if not i in customstopwords]

def feature_extractor(doc):
    docwords = set(doc)
    features = {}
    for i in wordlist:
        features['contains(%s)' % i] = (i in docwords)
    return features

#Creates a training set - classifier learns distribution of true/falses in the input.
training_set = nltk.classify.apply_features(feature_extractor, tweets)
classifier = nltk.NaiveBayesClassifier.train(training_set)

print classifier.show_most_informative_features(n=30)

while True:
    input = raw_input('ads')
    if input == 'exit':
        break
    elif input == 'informfeatures':
        print classifier.show_most_informative_features(n=30)
        continue
    else:
        input = input.lower()
        input = input.split()
        print '\nWe think that the sentiment was ' + classifier.classify(feature_extractor(input)) + ' in that sentence.\n'

p.close()
n.close()
</code></pre>

<p>Are this just a code-error? Or what is the problem.
When the problem start it should it should print out print <code>classifier.show_most_informative_features(n=30)</code> but the result i get is <em>Most Informative Features
None</em></p>

<p>Don't if this can give a hint.</p>

<p>Thanks</p>
","python, nltk, bayesian, sentiment-analysis","<p>wordList is empty. It should be assigned to getwordfeatures(getwords(tweets)).</p>

<p>The following two lines:</p>

<p>wordlist = [i for i in wordlist if not i in stopwords.words('english')]</p>

<p>and </p>

<p>wordlist = [i for i in wordlist if not i in customstopwords]</p>

<p>are an ""either-or""; You can try which stopword list works better. </p>
",2,1,1484,2013-10-27 19:35:52,https://stackoverflow.com/questions/19622538/python-nltk-not-sentiment-calculate-correct
Sentiment Analysis using perceptron,"<p>I am trying to implement Sentiment analysis using perceptron to get a better accuracy in python. I am lost in  the maths that sorounds it and need easy explanation on how to port it to be used for sentiment analysis. There is already  a paper published on the same : <a href=""http://aclweb.org/anthology/P/P11/P11-1015.pdf"" rel=""nofollow"">http://aclweb.org/anthology/P/P11/P11-1015.pdf</a></p>

<p>Would anyone here be able to explain in detail and clarity ? I have a training datatset and test dataset of 5000 reviews each and am getting an accuracy of 78 percent with bag of words. I have been told perceptron will give me an accuracy of 88% and am curious to implement it. </p>
","machine-learning, nlp, neural-network, sentiment-analysis, perceptron","<p>Perceptron is just a simple binary classifier, that works on fixed size vectors from R^n as input data. So in order to use it you have to encode each of your documents in such a real-valued vector. It could be for example a bag-of-words representation (where each dimension corresponds to one wor, and the value to number of occurences), or any ""more complex"" representation (one of which is described in the attached paper). </p>

<p>So in order to ""port"" perceptron to sentiment analysis, you have to figure out some function f, that feeded with document returns real-valued vector, and then train you perceptron on pairs</p>

<p>(f(x),0) for negative reviews</p>

<p>(f(x),1) for positive reviews</p>
",1,2,1547,2013-11-05 03:49:55,https://stackoverflow.com/questions/19781644/sentiment-analysis-using-perceptron
tweet classification how to identify type of conversation,"<p>I've been reading papers on tweet classification and sentiment analysis. So far all of it was about positive and negative classification. What about if you want to identify the kind of communication or tweet it is (inquiry, suggestion, news, etc.)? Also, would naive Bayes be possible to use for it?</p>
","twitter, classification, bayesian, tweets, sentiment-analysis","<p>In Sentiment Analysis and Opinion Mining  (<a href=""https://rads.stackoverflow.com/amzn/click/com/B009KET3PU"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">http://www.amazon.com/gp/product/B009KET3PU</a>) there are some chapters on spam detection in online reviews. I imagine some of those techniques would apply very well.</p>

<p>For learning features, beyond just the words and punctuation and sentence patterns, the number of retweets might be useful (questions don't get retweeted as much as news, for instance). (Also what people write with the retweet: ""This is great: ..."" is more likely to be news, whereas ""Anyone know?"" is likely to be a question, whereas ""What do you think of this?"" or ""I'd never thought of this!"" might introduce a tip or suggestion.)</p>
",1,1,218,2013-11-16 10:39:35,https://stackoverflow.com/questions/20017419/tweet-classification-how-to-identify-type-of-conversation
Dependency parser for spanish?,"<p>Somebody knows if there is any dependency parser for <strong>spanish</strong>? I need to analyze dependency relationships between opinion words. Some relations that I know exist are mod, pnmod, subj, s, obj, obj2. </p>

<p>Thank you.</p>
","parsing, dependencies, sentiment-analysis","<p>Have a look <a href=""http://code.google.com/p/mate-tools/downloads/list"" rel=""nofollow"">mate-tools</a>. It includes a dependency parser for Spanish. <a href=""http://aclweb.org/anthology//W/W09/W09-1206.pdf"" rel=""nofollow"">Here</a> is a research paper where the parser was evaluated.</p>
",0,1,405,2013-11-20 19:56:44,https://stackoverflow.com/questions/20105676/dependency-parser-for-spanish
Twitter sentiment - free existing tool?,"<p>I need a tool to analyze tweets sentiment (positive/neutral/negative). I would like it to be free for students. </p>

<p>So far I found <strong>Sentiment140</strong> (<a href=""http://www.sentiment140.com/"" rel=""nofollow"">http://www.sentiment140.com/</a>) and <strong>Sanders-Twitter Sentiment Corpus</strong> (<a href=""http://www.sananalytics.com/lab/twitter-sentiment/"" rel=""nofollow"">http://www.sananalytics.com/lab/twitter-sentiment/</a>). Although both tools provides only training data (I know, I can use them with NLTK (<a href=""http://nltk.org/"" rel=""nofollow"">http://nltk.org/</a>)).</p>

<p>But I would like to know if there is any python script or a Java program, that is free distributed and can be download and run to analyze tweets sentiment. I need to analyze huge set of data, so using Sentiment140 REST API is also unsatisfying.</p>

<p>Any ideas?</p>

<p>PS. I'm interested in english sentiment analyse.</p>
","twitter, sentiment-analysis","<p>You can use <a href=""https://textblob.readthedocs.org/en/latest/"" rel=""nofollow"">TextBlob</a>, a python package to analyze short texts for sentiment offline. There is a trained model included with the package, so you won't need to train one. It should be fine for some projects, but if you want to achieve greater accuracy, consider training your own model on your specific data.</p>
",1,1,1394,2013-11-25 09:25:35,https://stackoverflow.com/questions/20188511/twitter-sentiment-free-existing-tool
How to compute custom metrics using Elasticsearch + Kibana?,"<p>It seems like a pretty easy question, but for some reason I still can't understand how to solve the same. I have an elastic search cluster which is using twitter river to download tweets. I would like to implement a sentiment analysis module which takes each tweet and computes a score (+ve/-ve) etc. I would like the score to be computed for each of the existing tweets as well as for new tweets and then visualize using Kibana.</p>

<p>However, I am not sure where should I place the call to this sentiment analysis module in the elastic search pipeline.</p>

<p>I have considered the option of modifying twitter river plugin but that will not work retrospectively.</p>

<p>Essentially, I need to answer two questions :-
1) how to call python/java code while indexing a document so that I can modify the json accordingly.
2) how to use the same code to modify all the existing documents in ES.</p>
","twitter, elasticsearch, sentiment-analysis","<p>If you don't want an external application to do the analysis before indexing the documents in Elasticsearch, the best way I guess is to write a plugin that does it. You can write a plugin that implements a custom analyzer that does the sentiment analysis. Then in the mapping define the fields you want to run your analyzer on.</p>

<p>See examples of analysis plugins - 
<a href=""https://github.com/barminator/elasticsearch-analysis-annotation"" rel=""nofollow"">https://github.com/barminator/elasticsearch-analysis-annotation</a>
<a href=""https://github.com/yakaz/elasticsearch-analysis-combo/"" rel=""nofollow"">https://github.com/yakaz/elasticsearch-analysis-combo/</a></p>

<p>To run the analysis on all existing documents you will need to reindex them after defining the correct mapping.</p>
",3,3,3359,2013-12-07 13:50:10,https://stackoverflow.com/questions/20442124/how-to-compute-custom-metrics-using-elasticsearch-kibana
What data structure to use to store the sentiment count of corresponding word during sentiment analysis in python?,"<p>We are doing a project on twitter sentiment analyser in python. In order to increase the efficiency of the system ,during training we wish to store the occurrence of particular words in positive,negative and neutral tweets. Finally we will take the sentiment of the word as the one with maximum occurrence. Which data structure is suitable for storing words and their sentiments(positive,negative and neutral) dynamically?
example:</p>

<pre><code>            positive  negative   neutral
 market       45        12         2
 quite        35         67        5
 good         98         2         7
</code></pre>

<p>we require to add words to the structure dynamically.</p>
","python, twitter, sentiment-analysis","<p>Something like this might do the trick for you:</p>

<pre><code>sentiment_words = {}  # this will be a dict of 3-member lists, with word as key

for word in words:
    if not word in sentiment_words:  # initialize the word if it's not present yet
        sentiment_words[word] = [0, 0, 0]
    if ispositive(word):  # increment the right sentiment item in the list
        sentiment_words[word][0] += 1
    elif isnegative(word):
        sentiment_words[word][1] += 1
    elif isneutral(word):
        sentiment_words[word][2] += 1
</code></pre>

<p>If you can say more about the specifics I might be able to tune it in a bit for you.</p>
",2,1,829,2013-12-18 04:25:15,https://stackoverflow.com/questions/20649595/what-data-structure-to-use-to-store-the-sentiment-count-of-corresponding-word-du
nltk NaiveBayesClassifier training for sentiment analysis,"<p>I am training the <code>NaiveBayesClassifier</code> in Python using sentences, and it gives me the error below. I do not understand what the error might be, and any help would be good. </p>

<p>I have tried many other input formats, but the error remains. The code given below:</p>

<pre><code>from text.classifiers import NaiveBayesClassifier
from text.blob import TextBlob
train = [('I love this sandwich.', 'pos'),
         ('This is an amazing place!', 'pos'),
         ('I feel very good about these beers.', 'pos'),
         ('This is my best work.', 'pos'),
         (""What an awesome view"", 'pos'),
         ('I do not like this restaurant', 'neg'),
         ('I am tired of this stuff.', 'neg'),
         (""I can't deal with this"", 'neg'),
         ('He is my sworn enemy!', 'neg'),
         ('My boss is horrible.', 'neg') ]

test = [('The beer was good.', 'pos'),
        ('I do not enjoy my job', 'neg'),
        (""I ain't feeling dandy today."", 'neg'),
        (""I feel amazing!"", 'pos'),
        ('Gary is a friend of mine.', 'pos'),
        (""I can't believe I'm doing this."", 'neg') ]
classifier = nltk.NaiveBayesClassifier.train(train)
</code></pre>

<p>I am including the traceback below.</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\5460\Desktop\train01.py"", line 15, in &lt;module&gt;
    all_words = set(word.lower() for passage in train for word in word_tokenize(passage[0]))
  File ""C:\Users\5460\Desktop\train01.py"", line 15, in &lt;genexpr&gt;
    all_words = set(word.lower() for passage in train for word in word_tokenize(passage[0]))
  File ""C:\Python27\lib\site-packages\nltk\tokenize\__init__.py"", line 87, in word_tokenize
    return _word_tokenize(text)
  File ""C:\Python27\lib\site-packages\nltk\tokenize\treebank.py"", line 67, in tokenize
    text = re.sub(r'^\""', r'``', text)
  File ""C:\Python27\lib\re.py"", line 151, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or buffer
</code></pre>
","python, nlp, nltk, sentiment-analysis, textblob","<p>You need to change your data structure. Here is your <code>train</code> list as it currently stands:</p>

<pre><code>&gt;&gt;&gt; train = [('I love this sandwich.', 'pos'),
('This is an amazing place!', 'pos'),
('I feel very good about these beers.', 'pos'),
('This is my best work.', 'pos'),
(""What an awesome view"", 'pos'),
('I do not like this restaurant', 'neg'),
('I am tired of this stuff.', 'neg'),
(""I can't deal with this"", 'neg'),
('He is my sworn enemy!', 'neg'),
('My boss is horrible.', 'neg')]
</code></pre>

<p>The problem is, though, that the first element of each tuple should be a dictionary of features. So I will change your list into a data structure that the classifier can work with:</p>

<pre><code>&gt;&gt;&gt; from nltk.tokenize import word_tokenize # or use some other tokenizer
&gt;&gt;&gt; all_words = set(word.lower() for passage in train for word in word_tokenize(passage[0]))
&gt;&gt;&gt; t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in train]
</code></pre>

<p>Your data should now be structured like this:</p>

<pre><code>&gt;&gt;&gt; t
[({'this': True, 'love': True, 'deal': False, 'tired': False, 'feel': False, 'is': False, 'am': False, 'an': False, 'sandwich': True, 'ca': False, 'best': False, '!': False, 'what': False, '.': True, 'amazing': False, 'horrible': False, 'sworn': False, 'awesome': False, 'do': False, 'good': False, 'very': False, 'boss': False, 'beers': False, 'not': False, 'with': False, 'he': False, 'enemy': False, 'about': False, 'like': False, 'restaurant': False, 'these': False, 'of': False, 'work': False, ""n't"": False, 'i': False, 'stuff': False, 'place': False, 'my': False, 'view': False}, 'pos'), . . .]
</code></pre>

<p>Note that the first element of each tuple is now a dictionary. Now that your data is in place and the first element of each tuple is a dictionary, you can train the classifier like so:</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(t)
&gt;&gt;&gt; classifier.show_most_informative_features()
Most Informative Features
                    this = True              neg : pos    =      2.3 : 1.0
                    this = False             pos : neg    =      1.8 : 1.0
                      an = False             neg : pos    =      1.6 : 1.0
                       . = True              pos : neg    =      1.4 : 1.0
                       . = False             neg : pos    =      1.4 : 1.0
                 awesome = False             neg : pos    =      1.2 : 1.0
                      of = False             pos : neg    =      1.2 : 1.0
                    feel = False             neg : pos    =      1.2 : 1.0
                   place = False             neg : pos    =      1.2 : 1.0
                horrible = False             pos : neg    =      1.2 : 1.0
</code></pre>

<p>If you want to use the classifier, you can do it like this. First, you begin with a test sentence:</p>

<pre><code>&gt;&gt;&gt; test_sentence = ""This is the best band I've ever heard!""
</code></pre>

<p>Then, you tokenize the sentence and figure out which words the sentence shares with all_words. These constitute the sentence's features.</p>

<pre><code>&gt;&gt;&gt; test_sent_features = {word: (word in word_tokenize(test_sentence.lower())) for word in all_words}
</code></pre>

<p>Your features will now look like this:</p>

<pre><code>&gt;&gt;&gt; test_sent_features
{'love': False, 'deal': False, 'tired': False, 'feel': False, 'is': True, 'am': False, 'an': False, 'sandwich': False, 'ca': False, 'best': True, '!': True, 'what': False, 'i': True, '.': False, 'amazing': False, 'horrible': False, 'sworn': False, 'awesome': False, 'do': False, 'good': False, 'very': False, 'boss': False, 'beers': False, 'not': False, 'with': False, 'he': False, 'enemy': False, 'about': False, 'like': False, 'restaurant': False, 'this': True, 'of': False, 'work': False, ""n't"": False, 'these': False, 'stuff': False, 'place': False, 'my': False, 'view': False}
</code></pre>

<p>Then you simply classify those features:</p>

<pre><code>&gt;&gt;&gt; classifier.classify(test_sent_features)
'pos' # note 'best' == True in the sentence features above
</code></pre>

<p>This test sentence appears to be positive.</p>
",47,23,41780,2013-12-29 17:00:44,https://stackoverflow.com/questions/20827741/nltk-naivebayesclassifier-training-for-sentiment-analysis
Text Classification - how to find the features that most affected the decision,"<p>When using <code>SVMlight</code> or <code>LIBSVM</code> in order to classify phrases as positive or negative (Sentiment Analysis), is there a way to determine which are the most influential words that affected the algorithms decision? For example, finding that the word <code>""good""</code> helped determine a phrase as positive, etc. </p>
","machine-learning, nlp, svm, sentiment-analysis, text-classification","<p>If you use the linear kernel then yes - simply compute the weights vector:</p>

<pre><code>w = SUM_i y_i alpha_i sv_i
</code></pre>

<p>Where:</p>

<ul>
<li><code>sv</code> - support vector</li>
<li><code>alpha</code> - coefficient found with SVMlight</li>
<li><code>y</code> - corresponding class (+1 or -1)</li>
</ul>

<p>(in some implementations <code>alpha</code>'s are already multiplied by <code>y_i</code> and so they are positive/negative)</p>

<p>Once you have <code>w</code>, which is of dimensions <code>1 x d</code> where <code>d</code> is your data dimension (number of words in the bag of words/tfidf representation) simply select the dimensions with high absolute value (no matter positive or negative) in order to find the most important features (words).</p>

<p>If you use some kernel (like RBF) then the answer is no, there is no direct method of taking out the most important features, as the classification process is performed in completely different way.</p>
",5,2,777,2013-12-29 22:47:34,https://stackoverflow.com/questions/20830964/text-classification-how-to-find-the-features-that-most-affected-the-decision
Weka: Text sentiment analysis on multiple text attributes,"<p>I am looking to do text sentiment analysis on multiple text attributes. I followed this great  <a href=""https://www.youtube.com/watch?v=IY29uC4uem8"" rel=""nofollow noreferrer"" title=""link"">beginners video tutorial</a> which could be used for a single text attribute and its class - positive or negative. I want to extend the idea to multiple attributes simultaneously.</p>
<p>To make clear, here's an example of what I am trying to do:</p>
<p>Attributes collected from customers about a retail store:</p>
<ol>
<li>Store Experience review - String</li>
<li>Collection review - String</li>
<li>Assistance provided review - String</li>
<li>Overall ranking - Integer (1 to 5) - <strong>class</strong></li>
</ol>
<p>I want the analysis based on all the attributes (1 - 3) for the class attribute (4).</p>
<p>If I tried using filter &gt; unsupervised &gt; attribute &gt; StringToWordVector individually for each of these attributes then observed the results have lower correctly classified %.</p>
<p>Is this the correct way to proceed here to perform the text sentiment analysis?</p>
","machine-learning, data-mining, weka, sentiment-analysis","<p>You are approaching <a href=""http://times.cs.uiuc.edu/czhai/pub/www07-sent.pdf"" rel=""nofollow"">multi-faceted sentiment analysis</a>, as you are keeping information about different facets (attributes) of the retail store. For getting an overall analysis of the store, it is not wrong to mix all attributes in the analysis; just apply <code>StringToWordVector</code> to all String attributes and that's it.</p>

<p>On one side, you may increase accuracy because you will be getting better statistics and more features tyhan when using only one of the attributes. On the other side, you may decrease acuracy because one review may say positive things about the Store Experience but being negative overall, so mixing the attributes may put some noise in the model - however this is unlikely because such a review would be a bad example when learning only from the Store Experience attribute.</p>

<p>If you follow the tutorial, you will see that there are plenty of options in the <code>StringToWordVector</code> filter, and you can add <code>AttributeSelection</code> as well. I suggest to test both per attribute and combining all attributes, using binary/TF/TF.IDF weights in the <code>StringToWordVector</code> filter, using the <code>NGramTokenizer</code> (for identifying positive/negative multiwords -- e.g. ""very very good""), using <code>AttributeSelection</code> with <code>Ranker</code> and <code>InfoGainAttributeEval</code>, and of course, testing as many learning algorithms as you can.</p>

<p>You have an additional tutorial <a href=""http://jmgomezhidalgo.blogspot.com.es/2013/06/baseline-sentiment-analysis-with-weka.html"" rel=""nofollow"">here</a>.</p>
",0,0,5764,2014-01-02 23:54:47,https://stackoverflow.com/questions/20894165/weka-text-sentiment-analysis-on-multiple-text-attributes
Data Structure required to store extracted POS tag text in Java,"<p>Friends I am doing Sentiment analysis using AANV(adjective-adverb-noun-verb) approach as a my BE final year project. In this project I have done upto the POS tagging, I am using stanford POS Tagger for the same It gives me appropriate result.
for example suppose for the following sentences it gives me output as follows:</p>

<p><strong>Input Sentences:</strong></p>

<p>The camera is worked well.</p>

<p>Camera is very good.</p>

<p>Camera captures photo so slowly.</p>

<p><strong>POS Tagging Output sentences:</strong></p>

<p>The/DT camera/NN is/VBZ worked/VBN well/RB ./. </p>

<p>Camera/NN is/VBZ very/RB good/JJ ./. </p>

<p>Camera/NN captures/VBZ photo/NN so/RB slowly/RB ./.</p>

<p>As above pos tagged output sentences, among that I will required adjective, adverb,noun,verb to be extracted only, with its POS category.
For getting AANV I am using regular expression and write down the following code :</p>

<pre><code>private void btnShowTagActionPerformed(java.awt.event.ActionEvent evt) {                                           
    Pattern NounPat=Pattern.compile(""[A-Za-z]+/NN"");
    Pattern AdvPat=Pattern.compile(""[A-Za-z]+/RB"");
    Pattern AdjPat=Pattern.compile(""[A-Za-z]+/JJ"");
    Pattern VerbPat=Pattern.compile(""[A-Za-z]+/VB."");
    String StrToken;
    Matcher mat;
    StringTokenizer PosToken;
    String TempStr;  
    int j;
    for(int line=0;line&lt;SAPosTagging.tagedReview.length;line++)
    {
       try{

       PosToken=new StringTokenizer(SAPosTagging.tagedReview[line]);
       while(PosToken.hasMoreTokens())
       {
           StrToken=PosToken.nextToken();
           mat=NounPat.matcher(StrToken);
           if(mat.matches())
           {
               TempStr=StrToken;
               txtareaExTagText.append(""Noun=&gt;""+StrToken);   //textarea to be appended
               j=TempStr.indexOf(""/"");
               TempStr=TempStr.substring(0,j);
               System.out.print(""\tNoun=&gt;""+TempStr);
           }
           mat=VerbPat.matcher(StrToken);
           if(mat.matches())
           {

               txtareaExTagText.append(""\tVerb=&gt;""+StrToken);
               TempStr=StrToken;
               j=TempStr.indexOf(""/"");
               TempStr=TempStr.substring(0,j);
               System.out.print(""\tVerb=&gt;""+TempStr);

           }
           mat=AdvPat.matcher(StrToken);
           if(mat.matches())
           {

               txtareaExTagText.append(""\tAdverb=&gt;""+StrToken);
               TempStr=StrToken;
               j=TempStr.indexOf(""/"");
               TempStr=TempStr.substring(0,j);
               System.out.print(""\tAdVerb=&gt;""+TempStr);

           }
           mat=AdjPat.matcher(StrToken);
           if(mat.matches())
           {

              txtareaExTagText.append(""\tAdjective=&gt;""+StrToken);
               TempStr=StrToken;
               j=TempStr.indexOf(""/"");
               TempStr=TempStr.substring(0,j);
               System.out.print(""\tAdjective=&gt;""+TempStr);

           }  
       }
       System.out.println();
       txtareaExTagText.append(""\n\n"");
      }catch(Exception e){}
    }
}                     
</code></pre>

<p>with the help of above code I am getting the as below output in my required textarea as follows (i.e. after extracting required tag)</p>

<p><strong>Noun=>camera/NN   Verb=>is/VBZ    Verb=>worked/VBN    Adverb=>well/RB</strong></p>

<p><strong>Noun=>Camera/NN   Verb=>is/VBZ    Adverb=>very/RB Adjective=>good/JJ</strong></p>

<p><strong>Noun=>Camera/NN   Verb=>captures/VBZNoun=>photo/NN    Adverb=>so/RB   Adverb=>slowly/RB**</strong></p>

<p>Now I want to form the pair as (posword,poscategory) for example (camera,n) so that this pair will be passed to sentiwordnet in order to retrive the score from the Sentiwordnet. Please give me code for storing this pair structure without disurbing sentences link or structure so that i will passed it to sentiwordnet.
While forming pair the sentences structure  should be maintained. It may happened that one sentence contains multiple verbs, nouns, adverbs or adjective.</p>
","java, regex, sentiment-analysis, pos-tagger","<p>I advice you to forget about 'data structure' and model it thinking about OO classes. Think about a Sentence class and what do you want to store about a sentence and how to store Sentences.</p>

<p>If you insist on using 'general' data structures you may use a List where every element represents a sentence with type Guava's Multimap.</p>

<p>The key would be Noun/Verb/Etc and the value would be the word. It allows several values per key.
Reference <a href=""http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/collect/Multimap.html"" rel=""nofollow"">here</a>.</p>

<p>Guava example (not tested):</p>

<pre><code>List&lt;Multimap&lt;String, String&gt;&gt; sentenceList = new ArrayList&lt;&gt;();
for (String line: lines) {
   Multimap&lt;String, String&gt; aux = ArrayListMultimap.create();
   PosToken=new StringTokenizer(SAPosTagging.tagedReview[line]);
   while(PosToken.hasMoreTokens()) {
       // TODO ...
       strToken=PosToken.nextToken();
       // TODO, lets assume it is a noun
       aux.put(""noun"", strToken);
       // TODO, etc.
   }
  sentenceList.add(aux);
}
</code></pre>

<p>OO example (not tested):</p>

<pre><code>public class Sentence {
    private List&lt;String&gt; nouns = new ArrayList&lt;&gt;;
    private List&lt;String&gt; verbs = new ArrayList&lt;&gt;;
    // TODO Adverbs, etc.
    public List&lt;String&gt; getNons() { return nouns; };
    // TODO Other getters, etc.
}

List&lt;Sentence&gt; sentenceList = new ArrayList&lt;&gt;();
for (String line: lines) {
   Sentence aux = new Sentence();
   PosToken=new StringTokenizer(SAPosTagging.tagedReview[line]);
   while(PosToken.hasMoreTokens()) {
       // TODO ...
       strToken=PosToken.nextToken();
       // TODO, lets assume it is a noun
       aux.getNouns().add(strToken);
       // TODO, etc.
   }
  sentenceList.add(aux);
}
</code></pre>
",2,1,1099,2014-01-13 12:52:34,https://stackoverflow.com/questions/21091808/data-structure-required-to-store-extracted-pos-tag-text-in-java
Looking up Bigrams in Excel,"<p>Suppose I have a list of two-word pairs in a column in Excel.  These words are delimited by a space so that a typical pair might look like ""extreme happiness"".  The goal is to search for these 'bigrams' in a larger string located in another column.  The issue is that the bigram will only be found if the two words are together and separated by a space.  What would be preferable is if Excel could look for both words anywhere in a given larger string.  It is crucial that the bigrams occupy one cell each since a score is assigned to each bigram and in fact the function used VLOOKUPs this value based on the bigram cell value.  Would it make sense to change the space between any two words to a - or some other character?  Is there a way to have Excel look up each value one at a time (perhaps by recognizing this character and passing through the larger string twice, that is, once for each word)? </p>

<p><strong>Example</strong>:  ""The weather last night was extremely cold, but the warm fire gave me some happiness.""</p>

<p>Here we would like to find both the word 'extreme' within the word extremely and the word happiness.  Currently Excel would not be successful in doing this since it would just look for ""extreme happiness"" and determine that no such string exists.  </p>

<p>If the bigram in the row below ""extreme happiness"" reads ""weather gave"" (for some reason) Excel will go check whether that bigram exists in the larger string and return a second score.  This is done so that at the end every score can be added together.</p>
","excel, sentiment-analysis, vba","<p>This is pretty easy with a couple of formulas. See screenshot below:</p>

<p><img src=""https://i.sstatic.net/fPss5.png"" alt=""enter image description here""></p>

<p>The logic is simple. Assuming your bigram is in <code>B1</code>, we can input the following in <code>C1</code>. This will replace the spaces with <code>*</code>, which is Excel's wildcard character.</p>

<pre><code>=SUBSTITUTE(B2,"" "",""*"")
</code></pre>

<p>Then we concatenate it to give us a wildcarded beginning and end.</p>

<pre><code>=CONCATENATE(""*"",SUBSTITUTE(B2,"" "",""*""),""*"")
</code></pre>

<p>We then use a simple COUNTIF against the statement (here in <code>A1</code>) to return to us a count of occurence.</p>

<pre><code>=COUNTIF(A2,CONCATENATE(""*"",SUBSTITUTE(B2,"" "",""*""),""*""))
</code></pre>

<p>A simple <code>IF</code> check enclosing the above, with condition <code>&gt;0</code>, can be used to give us either <code>Yes</code> or <code>No</code>.</p>

<pre><code>=IF(COUNTIF(A2,CONCATENATE(""*"",SUBSTITUTE(B2,"" "",""*""),""*""))&gt;0,""Yes"",""No"")
</code></pre>

<p>Let us know if this helps.</p>
",1,-1,1819,2014-01-21 17:24:10,https://stackoverflow.com/questions/21264997/looking-up-bigrams-in-excel
Text Classification - using stemmer degrades results?,"<p>There's <a href=""http://www.cs.indiana.edu/~mkorayem/paper/survey_Arabic.pdf"" rel=""nofollow"">this</a> article about sentiment analysis of Arabic. </p>

<p>In the beginning of page 5 it says that:</p>

<blockquote>
  <p>""Experiments also show that stemming words before feature extraction and classification nearly always degrades the results"".</p>
</blockquote>

<p>Later on in the same page, they state that:</p>

<blockquote>
  <p>""...and an Arabic light stemmer is used for stemming the words""</p>
</blockquote>

<p>Um I thought that a stemmer/lemmatizer was <em>always</em> used before text classifications, why does he say that it degrades the results?</p>

<p>Thanks :)</p>
","nlp, sentiment-analysis, stemming, text-classification","<p>I do not know the arabic language, it may be specific in many aspects, my answer regards english.</p>

<blockquote>
  <p>Um I thought that a stemmer/lemmatizer was always used before text classifications, why does he say that it degrades the results?</p>
</blockquote>

<p>No it is not, in entirely depends on the <strong>task</strong>. If you want to extract some general concept of the text, then stemming/lematization is a good step. But in analysis of short chunks, where each word is valuable, stemming simply destroys its meaning. In particular - in sentiment analysis stemming may destroy the sentiment of the word. </p>
",5,3,822,2014-01-22 21:47:38,https://stackoverflow.com/questions/21294694/text-classification-using-stemmer-degrades-results
Loading a classifier using Pickle?,"<p>I am trying run a sentiment analysis.  I have managed to use Naive Bayes through nltk to classify a corpus of negative and positive tweets.  However I do not want to go through the process of running this classifier every time I run this program so I tried to use pickle to save, and then load into a different script the classifier.  However when I try to run the script it returns the error NameError: name classifier is not defined, although I thought it was defined through the def load_classifier():</p>

<p>The code I have atm is below:</p>

<pre><code>import nltk, pickle
from nltk.corpus import stopwords

customstopwords = ['']

p = open('xxx', 'r')
postxt = p.readlines()

n = open('xxx', 'r')
negtxt = n.readlines()

neglist = []
poslist = []

for i in range(0,len(negtxt)):
    neglist.append('negative')

for i in range(0,len(postxt)):
    poslist.append('positive')

postagged = zip(postxt, poslist)
negtagged = zip(negtxt, neglist)


taggedtweets = postagged + negtagged

tweets = []

for (word, sentiment) in taggedtweets:
    word_filter = [i.lower() for i in word.split()]
    tweets.append((word_filter, sentiment))

def getwords(tweets):
    allwords = []
    for (words, sentiment) in tweets:
            allwords.extend(words)
    return allwords

def getwordfeatures(listoftweets):
    wordfreq = nltk.FreqDist(listoftweets)
    words = wordfreq.keys()
    return words

wordlist = [i for i in getwordfeatures(getwords(tweets)) if not i in                  stopwords.words('english')]
wordlist = [i for i in getwordfeatures(getwords(tweets)) if not i in customstopwords]


def feature_extractor(doc):
    docwords = set(doc)
    features = {}
    for i in wordlist:
        features['contains(%s)' % i] = (i in docwords)
    return features


training_set = nltk.classify.apply_features(feature_extractor, tweets)

def load_classifier():
   f = open('my_classifier.pickle', 'rb')
   classifier = pickle.load(f)
   f.close
   return classifier

while True:
    input = raw_input('I hate this film')
    if input == 'exit':
        break
    elif input == 'informfeatures':
        print classifier.show_most_informative_features(n=30)
        continue
    else:
        input = input.lower()
        input = input.split()
        print '\nSentiment is ' + classifier.classify(feature_extractor(input)) + ' in that sentence.\n'

p.close()
n.close()
</code></pre>

<p>Any help would be great, the script seems to make it to the print '\nSentiment is ' + classifier.classify(feature_extractor(input)) + ' in that sentence.\n'"" before returning the error...</p>
","python, classification, pickle, sentiment-analysis","<p>Well, you have <em>declared and defined</em> the <code>load_classifier()</code> method but never called it and <em>assigned</em> a variable using it. That means, by the time, the execution reaches the <code>print '\nSentiment is... '</code> line, there is no variable names <code>classifier</code>. Naturally, the execution throws an exception.</p>

<p>Add the line <code>classifier = load_classifier()</code> just before while loop. (without any indentation)</p>
",1,0,1296,2014-02-11 14:35:02,https://stackoverflow.com/questions/21704760/loading-a-classifier-using-pickle
How to get overall sentiment for multiple sentences,"<p>How do you find the aggregated sentiment of multiple sentences/a paragraph/large passage of text.</p>

<p>I have the following code below which I have based on the github Stanford CoreNLP tests and various examples, but <em>everything</em> I've found has completed sentiment analysis only computes the sentiment for individual sentences. But I want the overall tweet's sentiment regardless of how many sentences are in it. </p>

<p>The only other way I can think of doing this is creating a separate thread for a <code>SentimentPipeline.main(String[])</code> and feeding the text to <code>stdin</code> and collecting the overall sentiment in <code>sdout</code>.  I would prefer just being able to use my code to make it simpler/more efficient, but I haven't found anything.</p>

<p>Also, I don't want to do a system call to a jar like most people do as I will be doing millions of tweets per day. The overhead would be too great loading the resources each time.</p>

<pre><code>Annotation document = new Annotation(text);
pipeline.annotate(document);

List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
        String output;
        for (CoreMap sentence : sentences) {
            // traversing the words in the current sentence a CoreLabel is a CoreMap with additional token-specific methods
             output = """";
            for (CoreLabel token : sentence.get(TokensAnnotation.class)) {

                // this is the text of the token
                String word = token.get(TextAnnotation.class);

                // this is the Parts Of Speech tag of the token (noun, verb, adjective etc)
                // String pos = token.get(PartOfSpeechAnnotation.class);

                // this is the NER label of the token
                String ne = token.get(NamedEntityTagAnnotation.class);
                if (!ne.contentEquals(""O"")) {
                    output = output + (ne + "" "" + word + "" "");
                }
            }

            //**************Sentiment Analysis 
            Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
             String sentiment = RNNCoreAnnotations.getPredictedClass(tree);
</code></pre>
","java, jar, stanford-nlp, sentiment-analysis","<p>The sentiment analysis toolkit in stanford corenlp is trained on a sentence-level data set. If you need a document-level sentiment engine, I think training a new model on documents is a better choice. You can also try to process the sentences one by one, and use some tricky methods (such as average, max) as your baselines to test how it works.</p>
",3,3,3098,2014-02-24 21:10:16,https://stackoverflow.com/questions/21999067/how-to-get-overall-sentiment-for-multiple-sentences
get the number of character vector elements in a corpus,"<p>my goal is to use R for lexicon based sentiment analysis!</p>

<p>i have two character vectors. one with positive words and one with negative words.
e.g. </p>

<pre><code>pos &lt;- c(""good"", ""accomplished"", ""won"", ""happy"")
neg &lt;- c(""bad"", ""loss"", ""damaged"", ""sued"", ""disaster"")
</code></pre>

<p>i now have a corpus of thousands of news articles and i want to know for each article, how
many elements of my vectors pos and neg are in the article.</p>

<p>e.g. (not sure about how the corpus function works here but you get the idea: there are two articles in my corpus)</p>

<pre><code>mycorpus &lt;- Corpus(""The CEO is happy that they finally won the case."", ""The disaster caused a huge loss."")
</code></pre>

<p>i want to get something like this:</p>

<pre><code>article 1: 2 element of pos and 0 element of neg
article 2: 0 elements of pos, 2 elements of neg
</code></pre>

<p>another good thing would be, if i can get the following for each article:</p>

<p>(number of pos words - number of neg words)/(number of total words in article)</p>

<p>thank you very much!!</p>

<p>EDIT:</p>

<p>@ Victorp: this doesn't seem to work</p>

<p>the matrix i get looks good:</p>

<pre><code>mytdm[1:6,1:10]
               Docs
Terms          1 2 3 4 5 6 7 8 9 10
aaron          0 0 0 0 0 1 0 0 0  0
abandon        1 1 0 0 0 0 0 0 0  0
abandoned      0 0 0 3 0 0 0 0 0  0
abbey          0 0 0 0 0 0 0 0 0  0
abbott         0 0 0 0 0 0 0 0 0  0
abbotts        0 0 1 0 0 0 0 0 0  0
</code></pre>

<p>but when i do your command i get zero for every document!</p>

<pre><code>colSums(mytdm[rownames(mytdm) %in% pos, ])
   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 
   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 
  16   17   18   19   20   21   22   23   24   25   26   27   28   29   30 
   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0
</code></pre>

<p>why is that??</p>
","r, word-count, sentiment-analysis, word-frequency, lexicon","<p>Hello you can use the TermDocumentMatrix for doing that :</p>

<pre><code>mycorpus &lt;- Corpus(VectorSource(c(""The CEO is happy that they finally won the case."", ""The disaster caused a huge loss."")))
mytdm &lt;- TermDocumentMatrix(mycorpus, control=list(removePunctuation=TRUE))
mytdm &lt;- as.matrix(mytdm)

# Positive words
colSums(mytdm[rownames(mytdm) %in% pos, ])
1 2 
2 0 

# Negative words
colSums(mytdm[rownames(mytdm) %in% neg, ])
1 2 
0 2 

# Total number of words per documents
colSums(mytdm)
1 2 
9 5
</code></pre>
",1,0,1158,2014-02-25 14:43:27,https://stackoverflow.com/questions/22017512/get-the-number-of-character-vector-elements-in-a-corpus
Twitter Sentiment Analysis w R using german language set SentiWS,"<p>I want to do a sentiment analysis of German tweets. The code I use works fine with English, but when I load the German word list, all scores just result zero. As far as I can guess, it must have to do with the different structures of the word lists. So what I need to know is, how to adapt my code to the structure of the German word-list. Someone could take a look at both of the lists ?</p>

<p><a href=""http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar"" rel=""nofollow noreferrer"">English Wordlist</a><br>
<a href=""http://pcai056.informatik.uni-leipzig.de/downloads/etc/SentiWS/SentiWS_v1.8c.zip"" rel=""nofollow noreferrer"">German Wordlist</a></p>

<pre><code>    # load the wordlists
    pos.words = scan(""~/positive-words.txt"",what='character', comment.char=';')
    neg.words = scan(""~/negative-words.txt"",what='character', comment.char=';')

        # bring in the sentiment analysis algorithm
        # we got a vector of sentences. plyr will handle a list or a vector as an ""l"" 
        # we want a simple array of scores back, so we use ""l"" + ""a"" + ""ply"" = laply:
        score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
         { 
          require(plyr)
          require(stringr)
            scores = laply(sentences, function(sentence, pos.words, neg.words) 
            {
             # clean up sentences with R's regex-driven global substitute, gsub():
             sentence = gsub('[[:punct:]]', '', sentence)
             sentence = gsub('[[:cntrl:]]', '', sentence)
             sentence = gsub('\\d+', '', sentence)
             # and convert to lower case:
             sentence = tolower(sentence)
             # split into words. str_split is in the stringr package
             word.list = str_split(sentence, '\\s+')
             # sometimes a list() is one level of hierarchy too much
             words = unlist(word.list)
             # compare our words to the dictionaries of positive &amp; negative terms
             pos.matches = match(words, pos.words)
             neg.matches = match(words, neg.words)
             # match() returns the position of the matched term or NA
             # we just want a TRUE/FALSE:
             pos.matches = !is.na(pos.matches)
             neg.matches = !is.na(neg.matches)
             # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
             score = sum(pos.matches) - sum(neg.matches)
             return(score)
            }, 
          pos.words, neg.words, .progress=.progress )
          scores.df = data.frame(score=scores, text=sentences)
          return(scores.df)
         }

    # and to see if it works, there should be a score...either in German or in English
    sample = c(""ich liebe dich. du bist wunderbar"",""I hate you. Die!"");sample
    test.sample = score.sentiment(sample, pos.words, neg.words);test.sample
</code></pre>
","r, tweets, sentiment-analysis","<p>This may work for you:</p>

<pre><code>readAndflattenSentiWS &lt;- function(filename) { 
  words = readLines(filename, encoding=""UTF-8"")
  words &lt;- sub(""\\|[A-Z]+\t[0-9.-]+\t?"", "","", words)
  words &lt;- unlist(strsplit(words, "",""))
  words &lt;- tolower(words)
  return(words)
}
pos.words &lt;- c(scan(""positive-words.txt"",what='character', comment.char=';', quiet=T), 
               readAndflattenSentiWS(""SentiWS_v1.8c_Positive.txt""))
neg.words &lt;- c(scan(""negative-words.txt"",what='character', comment.char=';', quiet=T), 
              readAndflattenSentiWS(""SentiWS_v1.8c_Negative.txt""))

score.sentiment = function(sentences, pos.words, neg.words, .progress='none') {
  # ... see OP ...
}

sample &lt;- c(""ich liebe dich. du bist wunderbar"",
            ""Ich hasse dich, geh sterben!"", 
            ""i love you. you are wonderful."",
            ""i hate you, die."")
(test.sample &lt;- score.sentiment(sample, 
                                pos.words, 
                                neg.words))
#   score                              text
# 1     2 ich liebe dich. du bist wunderbar
# 2    -2      ich hasse dich, geh sterben!
# 3     2    i love you. you are wonderful.
# 4    -2                  i hate you, die.
</code></pre>
",3,4,5593,2014-03-01 16:24:29,https://stackoverflow.com/questions/22116938/twitter-sentiment-analysis-w-r-using-german-language-set-sentiws
Sentiments Scores Stanford Core NLP,"<p>How can we get sentiment score of complete sentence using Stanford core NLP?</p>

<p>It classifies complete sentence into positive and negative sentiment, but can we get total sentiment score out Stanford NLP tool?</p>
","java, stanford-nlp, sentiment-analysis","<p>What I did was to average out the score of each sentence based on sentence length. Logic behind it is that longer sentences should carry more weight than shorter ones. </p>

<p>The code looks like this:</p>

<pre><code>String line = ""Great item! HDMI and decent wifi required as with all streaming devices.\n"" +
            ""The flow on the homepage is very good and responsive. Watching a series is a doddle, flow is great, no action required.\n"" +
            ""The remote and controller app both work a treat.\n"" +
            ""I really like this device.\n"" +
            ""I'd like to see an Amazon-written mirroring app available for non-Amazon products but no-one likes talking to each other in this field!"";

    Long textLength = 0L;
    int sumOfValues = 0;

    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    int mainSentiment = 0;
    if (line != null &amp;&amp; line.length() &gt; 0) {
        int longest = 0;
        Annotation annotation = pipeline.process(line);
        for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
            Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
            int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
            String partText = sentence.toString();
            if (partText.length() &gt; longest) {
                textLength += partText.length();
                sumOfValues = sumOfValues + sentiment * partText.length();

                System.out.println(sentiment + "" "" + partText);
            }
        }
    }

    System.out.println(""Overall: "" + (double)sumOfValues/textLength);
</code></pre>

<p>Download the entire project <a href=""http://www.project2ist.com/2015/04/sentiment-analysis-using-stanford-core.html"" rel=""noreferrer"">here</a></p>
",5,2,3718,2014-03-16 07:14:25,https://stackoverflow.com/questions/22434081/sentiments-scores-stanford-core-nlp
Looking for product reviews dataset,"<p>I'm working on a school project on product analysis which is based on sentimental analysis. I've been looking for a training dataset for quite a some time now and what I've been able to find so far is a dataset for movie reviews. My question is, can I use this dataset for training the classifier, i.e. will it have an effect on the accuracy of classification? If so, does anyone here know where I can get a free dataset for product reviews?</p>
","algorithm, machine-learning, dataset, nlp, sentiment-analysis","<p>I am assuming you are using some textual model like the bag of words model.</p>

<p>From my experiments, <strong>you usually don't get good results when changing from one domain to another</strong> (even if the train data set and the test are all products, but of different categories!). 
<br>Think of it logically, an oven that gets hot quickly usually indicate a good product. Is it also the same for laptops?</p>

<p>When I experimented with it a few years ago I used <strong>amazon comments</strong> as both train set and also to test my algorithms.
<br>The comments are short and informative and were enough to get ~80% accuracy. The 'ground' truth was the stars system, where 1-2 stars were 'negative', 3 stars - 'neutral', and 4-5 stars 'positive'.
<br>I used a pearl <a href=""http://www.esuli.it/fossil/repo/amazonReviewsDownloader/index"" rel=""nofollow"">script from esuli.it</a> to crawl amazon's comments.</p>
",1,1,796,2014-03-17 10:30:24,https://stackoverflow.com/questions/22451961/looking-for-product-reviews-dataset
Theano Classification Task always gives 50% validation error and test error?,"<p>I am doing a text classification experiment with Theano's DBN (Deep Belief Network) and SDA (Stacked Denoising Autoencoder) examples.
I have produced a feature/label dataset just as Theano's MINST dataset is produced and changed the feature length and output values of those examples to adopt to my dataset (2 outputs instead of 10 outputs, and the number of features is adopted to my dataset).
Every time i run the experiments (both DBN and SDA) i get an exact 50% validation error and test error.
Do you have any ideas what i'm doing wrong? because i have just produced a dataset out of Movie Review Dataset as MINST dataset format and pickled it.</p>
<p>my code is the same code you can find in <a href=""http://www.deeplearning.net/tutorial/DBN.html"" rel=""nofollow noreferrer"">http://www.deeplearning.net/tutorial/DBN.html</a>
and my SDA code is the same code you can find in
<a href=""http://www.deeplearning.net/tutorial/SdA.html"" rel=""nofollow noreferrer"">http://www.deeplearning.net/tutorial/SdA.html</a></p>
<p>The only difference is that i have made my own dataset instead of MINST digit recognition dataset. My dataset is Bag of Words features from Movie Review Dataset which of course has different number of features and output classes so i just have made tiny modifications in function parameters number of inputs and output classes.
The code runs beautifully but the results are always 50%.
This is a sample output:</p>
<pre><code>Pre-training layer 2, epoch 77, cost  -11.8415031463
Pre-training layer 2, epoch 78, cost  -11.8225591118
Pre-training layer 2, epoch 79, cost  -11.8309999005
Pre-training layer 2, epoch 80, cost  -11.8362189546
Pre-training layer 2, epoch 81, cost  -11.8251214285
Pre-training layer 2, epoch 82, cost  -11.8333494168
Pre-training layer 2, epoch 83, cost  -11.8564580976
Pre-training layer 2, epoch 84, cost  -11.8243052414
Pre-training layer 2, epoch 85, cost  -11.8373403275
Pre-training layer 2, epoch 86, cost  -11.8341470443
Pre-training layer 2, epoch 87, cost  -11.8272021013
Pre-training layer 2, epoch 88, cost  -11.8403720434
Pre-training layer 2, epoch 89, cost  -11.8393612003
Pre-training layer 2, epoch 90, cost  -11.828745041
Pre-training layer 2, epoch 91, cost  -11.8300890796
Pre-training layer 2, epoch 92, cost  -11.8209189065
Pre-training layer 2, epoch 93, cost  -11.8263340225
Pre-training layer 2, epoch 94, cost  -11.8348454378
Pre-training layer 2, epoch 95, cost  -11.8288419285
Pre-training layer 2, epoch 96, cost  -11.8366522357
Pre-training layer 2, epoch 97, cost  -11.840142131
Pre-training layer 2, epoch 98, cost  -11.8334445128
Pre-training layer 2, epoch 99, cost  -11.8523094141
</code></pre>
<p>The pretraining code for file DBN_MovieReview.py ran for 430.33m</p>
<pre><code>... getting the finetuning functions
... finetuning the model
epoch 1, minibatch 140/140, validation error 50.000000 %
     epoch 1, minibatch 140/140, test error of best model 50.000000 %
epoch 2, minibatch 140/140, validation error 50.000000 %
epoch 3, minibatch 140/140, validation error 50.000000 %
epoch 4, minibatch 140/140, validation error 50.000000 %
Optimization complete with best validation score of 50.000000 %,with test performance 50.000000 %
</code></pre>
<p>The fine tuning code for file DBN_MovieReview.py ran for 5.48m</p>
<p>I ran both SDA and DBN with two different feature sets. So i got this exact 50% accuracy on all these 4 experiments.</p>
","sentiment-analysis, text-classification, theano, deep-learning","<p>I asked the same question in Theano's user groups and they answered that feature values should be between 0 and 1.</p>

<p>So i used a normalizer to normalize feature values and it solved the problem.</p>
",4,3,1240,2014-03-24 15:15:03,https://stackoverflow.com/questions/22613364/theano-classification-task-always-gives-50-validation-error-and-test-error
incremental training SGD Classifier of Sklearn with sentences,"<p>How to incrementally train SGDClassifier available in Sklearn linear models for sentences.
It is usually trains with docs.But i want to train it with sentences one by one.I want to incrementally train it for tweets for sentiment analysis.Any explanation with example in python will be great help.</p>

<p>Task: Sentiment Analysis of Tweets
Doubt: Incrementally  training on labelled Tweets</p>

<p>Thanks in advance.</p>
","python, machine-learning, scikit-learn, sentiment-analysis, text-classification","<p>Just make list of sentences 
for example </p>

<pre><code>sents=[""I am x"",""I am Y""]
</code></pre>

<p>then transform it using HashingVectorizer and then use partial_fit to train it incrementally.
This worked for me.</p>

<p>Thanks</p>
",1,1,875,2014-03-27 13:11:55,https://stackoverflow.com/questions/22688728/incremental-training-sgd-classifier-of-sklearn-with-sentences
How to output resultant documents from Weka text-classification,"<p>So we are running a multinomial naive bayes classification algorithm on a set of 15k tweets. We first break up each tweet into a vector of word features based on Weka's StringToWordVector function. We then save the results to a new arff file to user as our training set. We repeat this process with another set of 5k tweets and re-evaluate the test set using the same model derived from our training set. </p>

<p>What we would like to do is to output each sentence that weka classified in the test set along with its classification... We can see the general information (Precision, recall, f-score) of the performance and accuracy of the algorithm but we cannot see the individual sentences that were classified by weka, based on our classifier... Is there anyway to do this?</p>

<p>Another problem is that ultimately our professor will give us 20k more tweets and expect us to classify this new document. We are not sure how to do this however as:</p>

<pre><code>All of the data we have been working with has been classified manually, both the training and test sets...
however the data we will be getting from the professor will be UNclassified... How can we 
reevaluate our model on the unclassified data if Weka requires that the attribute information must
be the same as the set used to form the model and the test set we are evaluating against?
</code></pre>

<p>Thanks for any help!</p>
","machine-learning, weka, sentiment-analysis, text-classification","<p>The easiest way to acomplish these tasks is using a <code>FilteredClassifier</code>. This kind of classifier integrates a <code>Filter</code> and a <code>Classifier</code>, so you can connect a <code>StringToWordVector</code> filter with the classifier you prefer (<code>J48</code>, <code>NaiveBayes</code>, whatever), and you will be always keeping the original training set (unprocessed text), and applying the classifier to new tweets (unprocessed) by using the vocabular derived by the <code>StringToWordVector</code> filter.</p>

<p>You can see how to do this in the command line in ""<a href=""http://jmgomezhidalgo.blogspot.com.es/2013/04/command-line-functions-for-text-mining.html"" rel=""nofollow"">Command Line Functions for Text Mining in WEKA</a>"" and via a program in ""<a href=""http://jmgomezhidalgo.blogspot.com.es/2013/04/a-simple-text-classifier-in-java-with.html"" rel=""nofollow"">A Simple Text Classifier in Java with WEKA</a>"".</p>
",1,0,485,2014-04-22 00:01:52,https://stackoverflow.com/questions/23208044/how-to-output-resultant-documents-from-weka-text-classification
How to use StanfordCoreNLP jar to conduct semantic analysis in a Java Map/Reduce Job?,"<p>I am currently working on Capital Markets Modelling. I am storing the data from various data sources  i.e. Financial news using Impala. </p>

<p>I have stored this data using Impala in a table of the form <code>articleID,ArticleDATA</code>. </p>

<p>My next step is to conduct semantic analysis row-by-row, on all records. </p>

<p>I have been reading through these resources </p>

<p><a href=""https://stackoverflow.com/questions/20359346/executing-and-testing-stanford-core-nlp-example"">Executing and testing stanford core nlp example</a></p>

<p><a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/corenlp.shtml</a></p>

<p><a href=""https://www.openshift.com/blogs/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java"" rel=""nofollow noreferrer"">https://www.openshift.com/blogs/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java</a></p>

<p><a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/sentiment/code.html</a></p>

<p>I am making a Map/Reduce job in Eclipse using Java and couldn't figure out how do I use the JAR to help me with the analysis.</p>

<p>P.S: I ran the command
 <code>java -cp ""*"" -mx1g edu.stanford.nlp.sentiment.SentimentPipeline -file input.txt</code>
on sample data and it worked but via command line. I want to know how to do it in eclipse, map/reduce.</p>

<p>The current output sentiment is in terms of Positive, Neutral, Negative but I need it as 1,0,-1. I guess that's easy to figure out, so not a priority here.!</p>

<p>I Am confused as how to go about it. </p>

<p>Thanks</p>
","java, eclipse, mapreduce, stanford-nlp, sentiment-analysis","<p>Well I figured it out.. So closing the question. Had to work with the Sentiment files in the JAR. Trial and Error. Then made a MApReduce job.</p>
",0,0,1326,2014-04-23 04:41:24,https://stackoverflow.com/questions/23234886/how-to-use-stanfordcorenlp-jar-to-conduct-semantic-analysis-in-a-java-map-reduce
"How to add tags to negated words in strings that follow &quot;not&quot;, &quot;no&quot; and &quot;never&quot;","<p>How do I add the tag <code>NEG_</code> to all words that follow <code>not</code>, <code>no</code> and <code>never</code> until the next punctuation mark in a string(used for sentiment analysis)? I assume that regular expressions could be used, but I'm not sure how.</p>

<p><strong>Input:</strong><br><code>It was never going to work, he thought. He did not play so well, so he had to practice some more.</code></p>

<p><strong>Desired output:</strong><br><code>It was never NEG_going NEG_to NEG_work, he thought. He did not NEG_play NEG_so NEG_well, so he had to practice some more.</code></p>

<p>Any idea how to solve this?</p>
","python, regex, python-2.7, sentiment-analysis","<p>To make up for Python's <code>re</code> regex engine's lack of some Perl abilities, you can use a lambda expression in a <code>re.sub</code> function to create a dynamic replacement:</p>

<pre><code>import re
string = ""It was never going to work, he thought. He did not play so well, so he had to practice some more. Not foobar !""
transformed = re.sub(r'\b(?:not|never|no)\b[\w\s]+[^\w\s]', 
       lambda match: re.sub(r'(\s+)(\w+)', r'\1NEG_\2', match.group(0)), 
       string,
       flags=re.IGNORECASE)
</code></pre>

<p>Will print (<a href=""http://ideone.com/ljZWHS"" rel=""noreferrer"">demo here</a>)</p>

<pre class=""lang-none prettyprint-override""><code>It was never NEG_going NEG_to NEG_work, he thought. He did not NEG_play NEG_so NEG_well, so he had to practice some more. Not NEG_foobar !
</code></pre>

<hr>

<p><strong>Explanation</strong></p>

<ul>
<li><p>The first step is to select the parts of your string you're interested in. This is done with</p>

<pre class=""lang-none prettyprint-override""><code>\b(?:not|never|no)\b[\w\s]+[^\w\s]
</code></pre>

<p>Your negative keyword (<code>\b</code> is a word boundary, <code>(?:...)</code> a non capturing group), followed by alpahnum and spaces (<code>\w</code> is <code>[0-9a-zA-Z_]</code>, <code>\s</code> is all kind of whitespaces), up until something that's neither an alphanum nor a space (acting as punctuation).</p>

<p>Note that the punctuation is mandatory here, but you could safely remove <code>[^\w\s]</code> to match end of string as well.</p></li>
<li><p>Now you're dealing with <code>never going to work,</code> kind of strings. Just select the words preceded by spaces with</p>

<pre><code>(\s+)(\w+)
</code></pre>

<p>And replace them with what you want</p>

<pre><code>\1NEG_\2
</code></pre></li>
</ul>
",9,3,1901,2014-04-30 09:25:44,https://stackoverflow.com/questions/23384351/how-to-add-tags-to-negated-words-in-strings-that-follow-not-no-and-never
combine any word which comes after specific word,"<p>I want to combine word which comes after a specific word ,I have try <strong>bigram</strong> approach which is too slow and also tried with <strong>gregexpr</strong> but didnt get any good solution. for ex</p>

<pre><code>text=""This approach isnt good enough.""
 BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
 BigramTokenizer(text)
[1] ""This approach"" ""approach isnt"" ""isnt good""     ""good enough""
</code></pre>

<p>what i really want is <strong>isnt_good</strong> as single word in a text ,combine next word which comes after <strong>isnt</strong>.</p>

<pre><code>text
""This approach isnt_good enough.""
</code></pre>

<p>Any efficient approach to convert into unigram.Thanks.</p>
","r, sentiment-analysis","<p>To extract all occurrences of the word ""isn't"" and the following word you can do this:</p>

<pre><code>library(stringr)
pattern &lt;- ""isnt \\w+""
str_extract_all(text, pattern)

[[1]]
[1] ""isnt good""
</code></pre>

<p>It essentially does the same thing as the example below (from the <code>base</code> package) but I find the <code>stringr</code> solution more elegant and readable.</p>

<pre><code>&gt; regmatches(text, regexpr(pattern, text))
[1] ""isnt good""
</code></pre>

<h3>Update</h3>

<p>To replace the occurrences of <code>isnt x</code> with <code>isnt_x</code> you just need <code>gsub</code> of the base package.</p>

<pre><code>gsub(""isnt (\\w+)"", ""isnt_\\1"", text)
[1] ""This approach isnt_good enough.""
</code></pre>

<p>What you do is to use a <em>capturing group</em> that copies whatever is found inside the parentheses to the <code>\\1</code>. See this page for a good introduction: <a href=""http://www.regular-expressions.info/brackets.html"" rel=""nofollow"">http://www.regular-expressions.info/brackets.html</a></p>
",1,0,116,2014-05-09 07:15:11,https://stackoverflow.com/questions/23558684/combine-any-word-which-comes-after-specific-word
How to tweak the NLTK Python code in such a way that I train the classifier only once,"<p>I have tried performing Sentiment Analysis on a huge data set which is about 10000 sentences. Now, when I use the NLTK Python code for performing training and testing using Naive Bayes, I will have train the classifier each time when I need to classify a set of new sentences. This is taking a lot of time.Is there a way I can take the output of the training part and then use it for classification which would save a lot of time.This is the NLTK code that I have used.</p>

<pre><code>import nltk
import re
import csv
#Read the tweets one by one and process it



def processTweet(tweet):
    # process the tweets
    #convert to lower case
    tweet = tweet.lower()
    #Convert www.* or https?://* to URL
    tweet = re.sub('((www\.[\s]+)|(https?://[^\s]+))','URL',tweet)
    #Convert @username to AT_USER
    tweet = re.sub('@[^\s]+','AT_USER',tweet)
    #Remove additional white spaces
    tweet = re.sub('[\s]+', ' ', tweet)
    #Replace #word with word
    tweet = re.sub(r'#([^\s]+)', r'\1', tweet)
    #trim
    tweet = tweet.strip('\'""')
    return tweet

def replaceTwoOrMore(s):
    #look for 2 or more repetitions of character and replace with the character itself
    pattern = re.compile(r""(.)\1{1,}"", re.DOTALL)
    return pattern.sub(r""\1\1"", s)
#end

#start getStopWordList
def getStopWordList(stopWordListFileName):
    #read the stopwords file and build a list
    stopWords = []
    stopWords.append('AT_USER')
    stopWords.append('url')
    stopWords.append('URL')
    stopWords.append('rt')

    fp = open(stopWordListFileName)
    line = fp.readline()
    while line:
        word = line.strip()
        stopWords.append(word)
        line = fp.readline()
    fp.close()
    return stopWords
#end

#start getfeatureVector
def getFeatureVector(tweet):
    featureVector = []
    #split tweet into words
    words = tweet.split()
    for w in words:
        #replace two or more with two occurrences
        w = replaceTwoOrMore(w)
        #strip punctuation
        w = w.strip('\'""?,.')
        #check if the word starts with an alphabet
        val = re.search(r""^[a-zA-Z][a-zA-Z0-9]*$"", w)

        #ignore if it is a stop word
        if(w in stopWords or val is None):
            continue
        else:
            featureVector.append(w.lower())
    return featureVector
#end

def extract_features(tweet):
    tweet_words = set(tweet)
    features = {}
    for word in featureList:
        features['contains(%s)' % word] = (word in tweet_words)
    return features

inpTweets = csv.reader(open('sheet3.csv', 'rb'), delimiter=',')
stopWords = getStopWordList('stopwords.txt')
featureList = []



# Get tweet words
tweets = []
for row in inpTweets:
    sentiment = row[0]
    tweet = row[1]
    processedTweet = processTweet(tweet)
    featureVector = getFeatureVector(processedTweet)
    featureList.extend(featureVector)
    tweets.append((featureVector, sentiment));
#end loop

# Remove featureList duplicates
featureList = list(set(featureList))

# Extract feature vector for all tweets in one shote
training_set = nltk.classify.util.apply_features(extract_features, tweets)

NBClassifier = nltk.NaiveBayesClassifier.train(training_set)

ft = open(""april2.tsv"")
line = ft.readline()

fo = open(""dunno.tsv"", ""w"")

fo.seek(0,0)
while line:
    testTweet = line
    processedTestTweet = processTweet(testTweet)
    line1 = fo.write( NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet))) + ""\n"");
    line = ft.readline()

fo.close()
ft.close()
</code></pre>
","python, nltk, sentiment-analysis","<p>If you want to stick with NLTK, try <code>pickle</code>, e.g. <a href=""https://spaghetti-tagger.googlecode.com/svn/spaghetti.py"" rel=""nofollow"">https://spaghetti-tagger.googlecode.com/svn/spaghetti.py</a>, see <a href=""https://docs.python.org/2/library/pickle.html"" rel=""nofollow"">https://docs.python.org/2/library/pickle.html</a> :</p>

<pre><code>#-*- coding: utf8 -*-

from nltk import UnigramTagger as ut
from nltk import BigramTagger as bt
from cPickle import dump,load

def loadtagger(taggerfilename):
    infile = open(taggerfilename,'rb')
    tagger = load(infile); infile.close()
    return tagger

def traintag(corpusname, corpus):
    # Function to save tagger.
    def savetagger(tagfilename,tagger):
        outfile = open(tagfilename, 'wb')
        dump(tagger,outfile,-1); outfile.close()
        return
    # Training UnigramTagger.
    uni_tag = ut(corpus)
    savetagger(corpusname+'_unigram.tagger',uni_tag)
    # Training BigramTagger.
    bi_tag = bt(corpus)
    savetagger(corpusname+'_bigram.tagger',bi_tag)
    print ""Tagger trained with"",corpusname,""using"" +\
                ""UnigramTagger and BigramTagger.""
    return
</code></pre>

<p>Otherwise, try other machine learning libraries such as sklearn or <a href=""http://shogun-toolbox.org/page/home/"" rel=""nofollow"">shogun</a></p>
",2,1,789,2014-05-22 08:11:39,https://stackoverflow.com/questions/23801244/how-to-tweak-the-nltk-python-code-in-such-a-way-that-i-train-the-classifier-only
Failed with error: ‘package ‘sentiment’ was built before R 3.0.0: please re-install it’,"<p>I am trying to run the snaMIC.R script which is doing sentiment analysis on twitter data. But it is failing with an error saying package sentiment was built before R 3.0.0: please re-install. I am using R-3.1.0 i386 (32 bits win). Another thing that I noted is that I am not getting the sentiment package under ""Install packages"". I downloaded sentiment_0.2.zip file from 
<a href=""http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.14/Rstem_0.4-1.zip"" rel=""nofollow"">http://cms.unipune.ernet.in/~webmaster/cran/bin/windows/contrib/2.14/sentiment_0.2.zip</a>
and unpacked it successfully under the default R win-library. But the sentiment package is still not available under ""Install packages"". I added some more repositories but that did not help. Any suggestions will be highly appreciated.</p>

<pre><code>&gt; source('snaMIC.R')
Loading required package: ROAuth
Loading required package: RCurl
Loading required package: bitops
Loading required package: digest
Loading required package: rjson
Loading required package: twitteR
Loading required package: stringr
Loading required package: ggplot2
Loading required package: grid
Loading required package: tm
Loading required package: NLP

Attaching package: ‘NLP’

The following object is masked from ‘package:ggplot2’:

    annotate

Loading required package: rJava

Attaching package: ‘rJava’

The following object is masked from ‘package:RCurl’:

    clone

Loading required package: Snowball
Installing package into ‘C:/Users/schakrabarti/Documents/R/win-library/3.1’
(as ‘lib’ is unspecified)
Loading required package: Snowball
Loading required package: wordcloud
Loading required package: Rcpp
Loading required package: RColorBrewer
Loading required package: topicmodels
Loading required package: slam
Loading required package: plyr

Attaching package: ‘plyr’

The following object is masked from ‘package:twitteR’:

    id

Loading required package: png
Loading required package: Snowball
Loading required package: sentiment
Failed with error:  ‘package ‘sentiment’ was built before R 3.0.0: please re-install it’
In addition: Warning messages:
1: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘Snowball’
2: package ‘Snowball’ is not available (for R version 3.1.0) 
3: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘Snowball’
4: In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘Snowball’
Loading required package: Snowball
Loading required package: sentiment
Failed with error:  ‘package ‘sentiment’ was built before R 3.0.0: please re-install it’
In addition: Warning message:
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘Snowball’
Loading required package: Snowball
Loading required package: sentiment
Failed with error:  ‘package ‘sentiment’ was built before R 3.0.0: please re-install it’
In addition: Warning message:
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘Snowball’
</code></pre>
","r, sentiment-analysis, snowball, roauth","<p>I have come over the solution for above problem of ""Failed with error: ‘package ‘sentiment’ was built before R 3.0.0: please re-install it"". Used the following sequence of commands from R console:</p>

<pre><code>require(devtools)
install_url(""http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz"")
require(sentiment)
ls(""package:sentiment"")
</code></pre>
",1,1,4871,2014-05-23 02:00:27,https://stackoverflow.com/questions/23819862/failed-with-error-package-sentiment-was-built-before-r-3-0-0-please-re-inst
Sentiment analysis with association rule mining,"<p>I am trying to come up with an algorithm to find top-3 most frequently used adjectives for the product in the same sentence. I want to use association rule mining(Apriori algorithm).</p>

<p>For that I am planning of using the twitter data. I can more or less decompose twits in to sentences and then with filtering I can find product names and adjectives with it.</p>

<p>For instance, after filtering I have data like;</p>

<p>ipad mini, great</p>

<p>ipad mini, horrible</p>

<p>samsung galaxy s2, best</p>

<p>...
etc.</p>

<p>Product names and adjectives are previously defined. So I have a set of product names and set of adjectives that I am looking for.</p>

<p>I have read couple of papers about sentimental analysis and rule mining and they all say Apriori algorithm is used. But they don't say how they used it and they don't give details.</p>

<pre><code>Therefore how can I reduce my problem to association rule mining problem? 
What values should I  use for minsup and minconf? 
How can I modify Apriori algorithm to solve this problem?
</code></pre>

<p>What I' m thinking is;</p>

<p>I should find frequent adjectives separately for each product. Then by sorting I can get top-3 adjectives. But I do not know if it is correct.</p>
","data-mining, sentiment-analysis, apriori","<p>Finding the top-3 most used adjectives for each product is <strong>not association rule mining</strong>.</p>

<p>For Apriori to yield good results, you must be interested in itemsets of length 4 and more. Apriori pruning starts at length 3, and begins to yield major gains at length 4. At length 2, it is mostly enumerating all pairs. And if you are only interested in pairs (product, adjective), then apriori is doing much more work than necessary.</p>

<p>Instead, use <em>counting</em>. Use hash tables. If you really have Exabytes of data, use approximate counting and heavy hitter algorithms. (But most likely, you don't have exabytes of data after extracting those pairs...)</p>

<p>Don't bother to investigate association rule mining if you only need to solve this much simpler problem.</p>

<p>Association rule mining is <em>really</em> only for finding patterns such as</p>

<pre><code>pasta, tomato, onion -&gt; basil
</code></pre>

<p>and more complex rules. The contribution of Apriori is to reduce the number of candidates when going from length n-1 -> n for length n > 2. And it gets more effective when n > 3.</p>
",1,0,1001,2014-05-24 10:47:09,https://stackoverflow.com/questions/23844077/sentiment-analysis-with-association-rule-mining
what is the meaning of the following lines of code exactly for knn algorithm,"<p>I came across this code, but I don't know what the functionality of the following lines of code is:</p>

<pre><code>negTrain = neg[:N]
posTrain = pos[:N] 
negTest = neg[N:]
posTest = pos[N:]  
</code></pre>

<p>Could somebody guide me?</p>
","python, sentiment-analysis, knn","<p>In any machine learning algorithm, you are trying to find <strong>meaning</strong> or <strong>classification</strong> of data.  How it starts is that you train your algorithm / machine first on <strong>training</strong> data.  Judging from these lines of code, you are using the KNN algorithm to do <strong>logistic regression</strong>.  This is a binary classification scheme where you are classifying something belonging to one class as <strong>positive</strong> and another class as <strong>negative</strong>.  </p>

<p>One example would be a logistic regression machine learning algorithm where positive denotes you have a disease, while negative means you don't.  You take your training data and you decompose it into positive and negative examples.  You know which ones are positive and negative ahead of time.  You then train your learning algorithm so that you try and make the classification accuracy as high as possible.</p>

<p>Once you accomplish that, you have another set of data denoted as the <strong>test</strong> data, where there are positive and negative examples here too and you see whether or not this will do the same job in classification as your training data.  If your classification accuracy is worse, you'll need to go and revise your algorithm or parameters.  If it's better, then that's awesome.</p>

<p>Those four lines of code are basically saying the following.  For your positive and negative training samples, you take samples from 0 up to <code>N</code> - 1.  The test positive and negative samples are taken from <code>N</code> up to the end of the array.</p>

<p>Hope this helps!</p>
",0,-3,119,2014-05-26 05:23:56,https://stackoverflow.com/questions/23863225/what-is-the-meaning-of-the-following-lines-of-code-exactly-for-knn-algorithm
R - twitteR package download of package ‘rjson’ failed,"<p>I am trying my hand at some data mining and attempting to retrieve data from Twitter. </p>

<p>When I tried installing the package 'twitteR', I get the following warning:</p>

<pre><code>Warning in install.packages :
  download of package ‘rjson’ failed
</code></pre>

<p>But it loads the rest of the packages. Then when I try to call the library:</p>

<pre><code>&gt; library(twitteR)
Loading required package: ROAuth
Loading required package: RCurl
Loading required package: bitops

Attaching package: ‘RCurl’

The following object is masked from ‘package:tm.plugin.webmining’:

    getURL

Loading required package: digest
Error: package ‘rjson’ required by ‘twitteR’ could not be found
</code></pre>

<p>Which makes sense, if it could not download the 'rjson' package initially. </p>

<p>When I tried to install the 'rjson' package alone, I get a familiar error:</p>

<pre><code>&gt; install.packages(""rjson"")
trying URL 'http://cran.rstudio.com/bin/macosx/contrib/3.0/rjson_0.2.13.tgz'
Warning in install.packages :
  cannot open: HTTP status was '404 Not Found'
Error in download.file(url, destfile, method, mode = ""wb"", ...) : 
  cannot open URL 'http://cran.rstudio.com/bin/macosx/contrib/3.0/rjson_0.2.13.tgz'
Warning in install.packages :
  download of package ‘rjson’ failed
</code></pre>

<p>I am not familiar with troubleshooting these errors. Any help is very much appreciated. </p>
","r, twitter, data-mining, sentiment-analysis","<p>If you don't want to upgrade your R (we're at version 3.1 now), you can install from the archives.</p>

<ol>
<li>Download version 2.13 from <a href=""http://cran.rstudio.com/src/contrib/Archive/rjson/rjson_0.2.13.tar.gz"" rel=""noreferrer"">http://cran.rstudio.com/src/contrib/Archive/rjson/rjson_0.2.13.tar.gz</a></li>
<li>In R, run <code>install.packages(""&lt;local path to the downloaded gz file&gt;"", repos=NULL, type=""source"")</code></li>
</ol>

<p>Hopefully that will work. </p>
",5,0,3220,2014-06-12 21:47:33,https://stackoverflow.com/questions/24194458/r-twitter-package-download-of-package-rjson-failed
How to save the result of classifier textblob NaiveBayesClassifier?,"<p>I am using TextBlob's <code>NaiveBayesclassifier</code> for text analysis according to the given themes that I have chosen.</p>

<p>The data is huge(about 3000 entries).</p>

<p>Though I was able to get a result, I'm not able to save it for future use without calling that function again and waiting hours till the processing gets complete.</p>

<p>I tried pickling by the following method</p>

<pre><code>ab = NaiveBayesClassifier(data)

import pickle

object = ab
file = open('f.obj','w') #tried to use 'a' in place of 'w' ie. append
pickle.dump(object,file)
</code></pre>

<p>and I got an error, which is as follows:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python27\lib\pickle.py"", line 1370, in dump
    Pickler(file, protocol).dump(obj)
  File ""C:\Python27\lib\pickle.py"", line 224, in dump
    self.save(obj)
  File ""C:\Python27\lib\pickle.py"", line 331, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python27\lib\pickle.py"", line 419, in save_reduce
    save(state)
  File ""C:\Python27\lib\pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python27\lib\pickle.py"", line 649, in save_dict
    self._batch_setitems(obj.iteritems())
  File ""C:\Python27\lib\pickle.py"", line 663, in _batch_setitems
    save(v)
  File ""C:\Python27\lib\pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python27\lib\pickle.py"", line 600, in save_list
    self._batch_appends(iter(obj))
  File ""C:\Python27\lib\pickle.py"", line 615, in _batch_appends
    save(x)
  File ""C:\Python27\lib\pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python27\lib\pickle.py"", line 562, in save_tuple
    save(element)
  File ""C:\Python27\lib\pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python27\lib\pickle.py"", line 649, in save_dict
    self._batch_setitems(obj.iteritems())
  File ""C:\Python27\lib\pickle.py"", line 662, in _batch_setitems
    save(k)
  File ""C:\Python27\lib\pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python27\lib\pickle.py"", line 501, in save_unicode
    self.memoize(obj)
  File ""C:\Python27\lib\pickle.py"", line 247, in memoize
    self.memo[id(obj)] = memo_len, obj
MemoryError
</code></pre>

<p>I also tried with sPickle but it also resulted in errors such as:</p>

<pre><code>#saving object with function sPickle.s_dump
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python27\lib\site-packages\sPickle.py"", line 22, in s_dump
    for elt in iterable_to_pickle:
TypeError: 'NaiveBayesClassifier' object is not iterable

#saving object with function sPickle.s_dump_elt
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python27\lib\site-packages\sPickle.py"", line 28, in s_dump_elt
    pickled_elt_str = dumps(elt_to_pickle)
MemoryError: out of memory
</code></pre>

<p>Can anyone tell me what I have to do to save the object?</p>

<p>Or is there anyway by which is save the results of the classifier for future use?</p>
","python, classification, pickle, sentiment-analysis, textblob","<p>You need to use ""wb"" for binary format:</p>

<pre><code>file = open('f.obj','wb')
</code></pre>
",2,4,3399,2014-06-26 13:08:54,https://stackoverflow.com/questions/24431449/how-to-save-the-result-of-classifier-textblob-naivebayesclassifier
Labelling text using Notepad++ or any other tool,"<pre><code>I have several .dat, containing information about hotel reviews as below
/*
&lt;Author&gt; simmotours
&lt;Content&gt; review......goes here
&lt;Date&gt;Nov 18, 2008
&lt;No. Reader&gt;-1
&lt;No. Helpful&gt;-1
&lt;Overall&gt;4`enter code here`
&lt;Value&gt;4
&lt;Rooms&gt;3
&lt;Location&gt;4
&lt;Cleanliness&gt;4
&lt;Check in / front desk&gt;4
&lt;Service&gt;4
&lt;Business service&gt;-1
</code></pre>

<p>*/
    I want to classify the review into two pos and neg , i.e. have two folder pos and neg containing several files with reviews above 3 classified as positive and below 3 classified as negative. </p>

<pre><code>How can I quickly and efficiently automate this process?
</code></pre>
","python-3.x, notepad++, classification, text-processing, sentiment-analysis","<p>You could write up a python script to read the overall score. Do this by looping over the the lines using readline() See <a href=""https://stackoverflow.com/questions/3277503/python-read-file-line-by-line-into-array"">here</a>. Find the ""Overall"" Score using some string parsing. Then move the file into the right directory. All very simple things to do in Python, just break it down into steps and search for answers to those steps. </p>
",0,0,205,2014-07-03 11:50:27,https://stackoverflow.com/questions/24552950/labelling-text-using-notepad-or-any-other-tool
Good dataset for sentiment analysis?,"<p>I am working on sentiment analysis and I am using dataset given in this link: <code>http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html</code> and I have divided my dataset into 50:50 ratio. 50% are used as test samples and 50% are used as train samples and the features extracted from train samples and perform classification using Weka classifier, but my predication accuracy is about 70-75%.</p>

<p>Can anybody suggest some other datasets which can help me to increase the result - I have used unigram, bigram and POStags as my features.</p>
","dataset, sentiment-analysis, web-mining","<p>There are many sources to get sentiment analysis dataset:</p>

<ul>
<li>huge ngrams dataset from google <a href=""http://storage.googleapis.com/books/ngrams/books/datasetsv2.html"" rel=""noreferrer"">storage.googleapis.com/books/ngrams/books/datasetsv2.html</a></li>
<li><a href=""http://www.sananalytics.com/lab/twitter-sentiment/"" rel=""noreferrer"">http://www.sananalytics.com/lab/twitter-sentiment/</a></li>
<li><a href=""http://inclass.kaggle.com/c/si650winter11/data"" rel=""noreferrer"">http://inclass.kaggle.com/c/si650winter11/data</a></li>
<li><a href=""http://nlp.stanford.edu/sentiment/treebank.html"" rel=""noreferrer"">http://nlp.stanford.edu/sentiment/treebank.html</a></li>
<li>or you can look into this global ML dataset repository: <a href=""https://archive.ics.uci.edu/ml"" rel=""noreferrer"">https://archive.ics.uci.edu/ml</a></li>
</ul>

<p>Anyway, it does not mean it will help you to get a better accuracy for your current dataset because the corpus might be very different from your dataset. Apart from reducing the testing percentage vs training, you could: test other classifiers or fine tune all hyperparameters using semi-automated wrapper like CVParameterSelection or GridSearch, or even auto-weka if it fits.</p>

<p>It is quite rare to use 50/50, 80/20 is quite a commonly occurring ratio. A better practice is to use: 60% for training, 20% for cross validation, 20% for testing.</p>
",26,16,49595,2014-07-07 08:04:10,https://stackoverflow.com/questions/24605702/good-dataset-for-sentiment-analysis
Using scores in sentiment analysis with R,"<p>Generally I am interested in getting a process working faster. </p>

<p>I am using R to do sentiment analysis on a German corpus of about 8000 documents. Instead of just counting positive and negative words I have a value between -1 and 1 assigned to about 3000 different terms. As I am not using the stem-functuion and still want to get all the inflected forms of German grammar my wordlists become even longer.</p>

<p>For matching I am using this code at the moment:</p>

<pre><code>score.sum &lt;- rep(0, length(texts))
for (i in 1:length(texts)){
for (j in 1:length(sent.words)){
if(sent.words[j] %in% strsplit(texts[i], split="" "")[[1]] {
score.sum[i] &lt;- score.sum[i] + sent.words_score[j]
}}}
</code></pre>

<p>As a mini-example one could use:</p>

<pre><code>texts &lt;- c(""I like ice cream. It is great"",""I hate flying because it makes me sick"",""If I get bored I do something fun"")

sent.words &lt;- c(""like"",""great"",""hate"",""sick"",""bored"",""fun"",""joy"")
sent.words_score &lt;- c(0.3,0.7,-0.5,-0.4,-0.4,0.3,0.5)
</code></pre>

<p>Maybe the calculations are taking longer than u want them as well. In my context with the 8000 documents i takes about 6 hours. So do u know of a way to avoid the dubble if-loop and get the computation faster?</p>

<p>Thanks in advance already
Mairuu</p>
","r, performance, sentiment-analysis","<p><code>strplit</code> is vectorized so you can do it once. </p>

<p>Also no need to use <code>for</code> here , use <code>sapply</code> to avoid initialization and side effect.</p>

<pre><code>sapply(strsplit(texts, split="" ""),
      function(x)sum(sent.words_score[sent.words  %in% x]))
</code></pre>
",2,2,456,2014-08-07 13:39:01,https://stackoverflow.com/questions/25184076/using-scores-in-sentiment-analysis-with-r
Sentiment Analysis Tool using SentiWordNet and Apache OpenNLP,"<p>I am working on a Sentiment Analysis Tool using <a href=""http://sentiwordnet.isti.cnr.it/"" rel=""nofollow"">SentiWordNet</a> and <a href=""https://opennlp.apache.org/"" rel=""nofollow"">Apache NLP</a> library.
The problem is when I tag the sentence using NLP Library I get the result such as,</p>

<pre><code>Test_NNP Tweet_NNP is_VBZ ready_JJ now_RB for_IN the_DT change._NN
</code></pre>

<p>but the sentiWordNet has <code>POS</code> Tags like <code>a</code>, <code>v</code>, <code>n...etc</code> how do I convert <code>NNP</code>, <code>VBZ</code>, <code>JJ</code> to <code>n</code> or <code>v</code> or <code>a</code> with Apache NLP?
Should I use a different library for tagging instead?</p>
","machine-learning, nlp, sentiment-analysis, opennlp","<p>The tags you are getting from Apache NLP are Penn Treebank tags, you have to convert the tags to SentiWordNet compatible tags.
The following function would map the treebank tags to WordNet part of speech names:</p>

<pre><code>def get_wordnet_pos(treebank_tag):
   if treebank_tag.startswith('J'):
       return 'a'
   elif treebank_tag.startswith('V'):
       return 'v'
   elif treebank_tag.startswith('N'):
       return 'n'
   elif treebank_tag.startswith('R'):
       return 'r'
   else:
       return ''
</code></pre>
",1,2,983,2014-09-04 05:25:31,https://stackoverflow.com/questions/25657854/sentiment-analysis-tool-using-sentiwordnet-and-apache-opennlp
Bias towards negative sentiments from Stanford CoreNLP,"<p>I'm experimenting with deriving sentiment from Twitter using Stanford's CoreNLP library, a la <a href=""https://www.openshift.com/blogs/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java"" rel=""nofollow"">https://www.openshift.com/blogs/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java</a> - so see here for the code that I'm implementing. </p>

<p>I am getting results, but I've noticed that there appears to be a bias towards 'negative' results, both in my target dataset and another dataset I use with ground truth - the Sanders Analytics Twitter Sentiment Corpus <a href=""http://www.sananalytics.com/lab/twitter-sentiment/"" rel=""nofollow"">http://www.sananalytics.com/lab/twitter-sentiment/</a> - even though the ground truth data do not have this bias. </p>

<p>I'm posting this question on the off chance that someone else has experienced this and/or may know if this is the result of something I've done or some bug in the CoreNLP code.</p>

<p>(edit - sorry it took me so long to respond)
I am posting links to plots showing what I mean. I don't have enough reputation to post the images, and can only include two links in this post, so I'll add the links in the comments. </p>
","java, twitter, nlp, stanford-nlp, sentiment-analysis","<p>I'd like to suggest this is simply a domain mismatch. The Stanford RNTN is trained on movie review snippets and you are testing on twitter data. Other than the topics mismatch, tweets also tend to be ungrammatical and use abbreviated (""creative"") language.
If I had to suggest a more concrete reason, I would start with a lexical mismatch. Perhaps negative emotions are expressed in a domain-independent way, e.g. with common adjectives, and positive emotions are more domain-dependent or more subtle.</p>

<p>It's still interesting that you're getting a negative bias. The Polyanna hypothesis  suggests a positive bias, IMHO.</p>

<p>Going beyond your original question, there are several approaches to do sentiment analysis specifically on microblogging data. See e.g. <a href=""http://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2857/3251"" rel=""noreferrer"">""The Good, The Bad and the OMG!"" by Kouloumpis et al.</a></p>
",4,5,1318,2014-09-08 16:49:41,https://stackoverflow.com/questions/25729204/bias-towards-negative-sentiments-from-stanford-corenlp
What&#39;s a good database for full text search on a large number of relatively small text documents? (C# backend),"<p>I am designing a system that aims to ingest large numbers of documents.  I want to support full text search on the document contents, as well as other metadata (keyword/sentiment analysis).  How keyword/sentiment analysis is done is beyond the scope of this question.  But it is worth considering that this sort of metadata needs to live along side the search-able documents.</p>

<p><strong>The main assumptions are:</strong></p>

<ul>
<li>by large I mean initially a few 100,000 with the goal of reaching millions</li>
<li>the documents are 0-15kb.</li>
<li>these documents are text (utf-8)</li>
<li>desire to be able to full-text-search document contents</li>
<li>hosted on a single machine, no cloud/distributed services</li>
<li>new documents are inserted continuously (roughly 1-2 per second)</li>
<li>ad hoc text searches </li>
<li>more complicated query use cases would be:
<ul>
<li>show me all documents that are about 'Widgets' that are positive from this daterange</li>
</ul></li>
</ul>

<p>C# is the language of choice for fetching documents, processing, storing and retrieving from db.  So having C# bindings is a big plus.  Or at least an easy way to bridge the gap.</p>

<h2>Naive Approach</h2>

<p>A naive approach is to use MySQL along with Apache's Lucene.  Having the document contents stored as files with references to them in the DB, or having the document contents as a Text field in the databse.</p>

<p>Then I could use one of the C# wrappers to Lucene like <a href=""https://github.com/apache/lucene.net"" rel=""nofollow"">Lucene.Net</a></p>

<p>My concern/question with this approach is whether or not the size of my data and what I want to do with it is too much for MySQL.  I know it is silly to do premature optimization, and that oftentimes people think they need some 'big data' solution when it turns out that a regular SQL database does just fine.  My other main concern with this approach is that it would be too 'clunky' and cumbersome to develop compared to some potential alternatives.</p>

<h2>Alternatives</h2>

<p>From doing some research, one alternative that looks promising is using CouchDB with Lucene.  I have come across two libraries that solve this:</p>

<ul>
<li><a href=""https://github.com/rnewson/couchdb-lucene"" rel=""nofollow"">couchdb-lucene</a></li>
<li><a href=""https://github.com/foretagsplatsen/Divan"" rel=""nofollow"">Divan</a></li>
</ul>

<h2>What I'm looking for:</h2>

<p>I haven't done a whole lot with this size of data.  I wonder:</p>

<ul>
<li>Does this amount of data and use case merit a non-relational database?</li>
<li>Should documents live in the database, or as files with references in the database?</li>
<li>Is there a database/full-text-search technology that is particularly suited for this scenario that I haven't considered? </li>
</ul>
","c#, database-design, full-text-search, sentiment-analysis, keyword-search","<p>I would suggest you look into RavenDb.  It uses Lucene and is 100% .Net.  It has text analyzers for doing full text indexing and fuzzy searches.</p>
",1,3,1100,2014-10-29 22:24:03,https://stackoverflow.com/questions/26641697/whats-a-good-database-for-full-text-search-on-a-large-number-of-relatively-smal
NLTK and Stopwords Fail #lookuperror,"<p>I am trying to start a project of sentiment analysis and I will use the stop words method. I made some research and I found that nltk have stopwords but when I execute the command there is an error.</p>

<p>What I do is the following, in order to know which are the words that nltk use (like what you may found here <a href=""http://www.nltk.org/book/ch02.html"">http://www.nltk.org/book/ch02.html</a> in section4.1):</p>

<pre><code>from nltk.corpus import stopwords
stopwords.words('english')
</code></pre>

<p>But when I press enter I obtain</p>

<pre><code>---------------------------------------------------------------------------
LookupError                               Traceback (most recent call last)
&lt;ipython-input-6-ff9cd17f22b2&gt; in &lt;module&gt;()
----&gt; 1 stopwords.words('english')

C:\Users\Usuario\Anaconda\lib\site-packages\nltk\corpus\util.pyc in __getattr__(self, attr)
 66
 67     def __getattr__(self, attr):
---&gt; 68         self.__load()
 69         # This looks circular, but its not, since __load() changes our
 70         # __class__ to something new:

C:\Users\Usuario\Anaconda\lib\site-packages\nltk\corpus\util.pyc in __load(self)
 54             except LookupError, e:
 55                 try: root = nltk.data.find('corpora/%s' % zip_name)
---&gt; 56                 except LookupError: raise e
 57
 58         # Load the corpus.

LookupError:
**********************************************************************
  Resource 'corpora/stopwords' not found.  Please use the NLTK
  Downloader to obtain the resource:  &gt;&gt;&gt; nltk.download()
  Searched in:
- 'C:\\Users\\Meru/nltk_data'
- 'C:\\nltk_data'
- 'D:\\nltk_data'
- 'E:\\nltk_data'
- 'C:\\Users\\Meru\\Anaconda\\nltk_data'
- 'C:\\Users\\Meru\\Anaconda\\lib\\nltk_data'
- 'C:\\Users\\Meru\\AppData\\Roaming\\nltk_data'
**********************************************************************
</code></pre>

<p>And, because of this problem things like this cannot run properly (obtaining the same error):</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import stopwords
&gt;&gt;&gt; stop = stopwords.words('english')
&gt;&gt;&gt; sentence = ""this is a foo bar sentence""
&gt;&gt;&gt; print [i for i in sentence.split() if i not in stop]
</code></pre>

<p>Do you know what may be problem? I must use words in Spanish, do you recomend another method? I also thought using Goslate package with datasets in english</p>

<p>Thanks for reading!</p>

<p>P.D.: I use Ananconda</p>
","python, nltk, sentiment-analysis, stop-words","<p>You don't seem to have the stopwords corpus on your computer.</p>

<p>You need to start the NLTK Downloader and download all the data you need.</p>

<p>Open a Python console and do the following:</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.download()
showing info http://nltk.github.com/nltk_data/
</code></pre>

<p>In the GUI window that opens simply press the 'Download' button to download all corpora or go to the 'Corpora' tab and only download the ones you need/want.</p>
",165,69,175266,2014-11-01 22:05:16,https://stackoverflow.com/questions/26693736/nltk-and-stopwords-fail-lookuperror
Unable to download the Twitter sentiment corpus by Niek Sanders,"<p>I am following a tutorial on Twitter sentiment analysis. I have downloaded the codes here <a href=""http://www.sananalytics.com/lab/twitter-sentiment/"" rel=""nofollow"">http://www.sananalytics.com/lab/twitter-sentiment/</a>. I follow the steps to run the install.py from cmd prompt, while it does create the json files in the 'rawdata' folder, when I view these json files, it says:</p>

<pre><code>{
    ""errors"": [
        {
            ""message"": ""SSL is required"",
            ""code"": 92
        }
    ]
}
</code></pre>

<p>The install.py code is as follows:</p>

<pre><code>#
# Sanders-Twitter Sentiment Corpus Install Script
# Version 0.1
#
# Pulls tweet data from Twitter because ToS prevents distributing it directly.
#
# Right now we use unauthenticated requests, which are rate-limited to 150/hr.
# We use 125/hr to stay safe.  
#
# We could more than double the download speed by using authentication with
# OAuth logins.  But for now, this is too much of a PITA to implement.  Just let
# the script run over a weekend and you'll have all the data.
#
#   - Niek Sanders
#     njs@sananalytics.com
#     October 20, 2011
#
#
# Excuse the ugly code.  I threw this together as quickly as possible and I
# don't normally code in Python.
#
import csv, getpass, json, os, time, urllib


def get_user_params():

    user_params = {}

    # get user input params
    user_params['inList']  = raw_input( '\nInput file [./corpus.csv]: ' )
    user_params['outList'] = raw_input( 'Results file [./full-corpus.csv]: ' )
    user_params['rawDir']  = raw_input( 'Raw data dir [./rawdata/]: ' )

    # apply defaults
    if user_params['inList']  == '': 
        user_params['inList'] = './corpus.csv'
    if user_params['outList'] == '': 
        user_params['outList'] = './full-corpus.csv'
    if user_params['rawDir']  == '': 
        user_params['rawDir'] = './rawdata/'

    return user_params


def dump_user_params( user_params ):

    # dump user params for confirmation
    print 'Input:    '   + user_params['inList']
    print 'Output:   '   + user_params['outList']
    print 'Raw data: '   + user_params['rawDir']
    return


def read_total_list( in_filename ):

    # read total fetch list csv
    fp = open( in_filename, 'rb' )
    reader = csv.reader( fp, delimiter=',', quotechar='""' )

    total_list = []
    for row in reader:
        total_list.append( row )

    return total_list


def purge_already_fetched( fetch_list, raw_dir ):

    # list of tweet ids that still need downloading
    rem_list = []

    # check each tweet to see if we have it
    for item in fetch_list:

        # check if json file exists
        tweet_file = raw_dir + item[2] + '.json'
        if os.path.exists( tweet_file ):

            # attempt to parse json file
            try:
                parse_tweet_json( tweet_file )
                print '--&gt; already downloaded #' + item[2]
            except RuntimeError:
                rem_list.append( item )
        else:
            rem_list.append( item )

    return rem_list


def get_time_left_str( cur_idx, fetch_list, download_pause ):

    tweets_left = len(fetch_list) - cur_idx
    total_seconds = tweets_left * download_pause

    str_hr = int( total_seconds / 3600 )
    str_min = int((total_seconds - str_hr*3600) / 60)
    str_sec = total_seconds - str_hr*3600 - str_min*60

    return '%dh %dm %ds' % (str_hr, str_min, str_sec)


def download_tweets( fetch_list, raw_dir ):

    # ensure raw data directory exists
    if not os.path.exists( raw_dir ):
        os.mkdir( raw_dir )

    # stay within rate limits
    max_tweets_per_hr  = 125
    download_pause_sec = 3600 / max_tweets_per_hr

    # download tweets
    for idx in range(0,len(fetch_list)):

        # current item
        item = fetch_list[idx]

        # print status
        trem = get_time_left_str( idx, fetch_list, download_pause_sec )
        print '--&gt; downloading tweet #%s (%d of %d) (%s left)' % \
              (item[2], idx+1, len(fetch_list), trem)

        # pull data
        url = 'http://api.twitter.com/1/statuses/show.json?id=' + item[2]
        urllib.urlretrieve( url, raw_dir + item[2] + '.json' )

        # stay in Twitter API rate limits 
        print '    pausing %d sec to obey Twitter API rate limits' % \
              (download_pause_sec)
        time.sleep( download_pause_sec )

    return


def parse_tweet_json( filename ):

    # read tweet
    print 'opening: ' + filename
    fp = open( filename, 'rb' )

    # parse json
    try:
        tweet_json = json.load( fp )
    except ValueError:
        raise RuntimeError('error parsing json')

    # look for twitter api error msgs
    if 'error' in tweet_json:
        raise RuntimeError('error in downloaded tweet')

    # extract creation date and tweet text
    return [ tweet_json['created_at'], tweet_json['text'] ]


def build_output_corpus( out_filename, raw_dir, total_list ):

    # open csv output file
    fp = open( out_filename, 'wb' )
    writer = csv.writer( fp, delimiter=',', quotechar='""', escapechar='\\',
                         quoting=csv.QUOTE_ALL )

    # write header row
    writer.writerow( ['Topic','Sentiment','TweetId','TweetDate','TweetText'] )

    # parse all downloaded tweets
    missing_count = 0
    for item in total_list:

        # ensure tweet exists
        if os.path.exists( raw_dir + item[2] + '.json' ):

            try: 
                # parse tweet
                parsed_tweet = parse_tweet_json( raw_dir + item[2] + '.json' )
                full_row = item + parsed_tweet

                # character encoding for output
                for i in range(0,len(full_row)):
                    full_row[i] = full_row[i].encode(""utf-8"")

                # write csv row
                writer.writerow( full_row )

            except RuntimeError:
                print '--&gt; bad data in tweet #' + item[2]
                missing_count += 1

        else:
            print '--&gt; missing tweet #' + item[2]
            missing_count += 1

    # indicate success
    if missing_count == 0:
        print '\nSuccessfully downloaded corpus!'
        print 'Output in: ' + out_filename + '\n'
    else: 
        print '\nMissing %d of %d tweets!' % (missing_count, len(total_list))
        print 'Partial output in: ' + out_filename + '\n'

    return


def main():

    # get user parameters
    user_params = get_user_params()
    dump_user_params( user_params )

    # get fetch list
    total_list = read_total_list( user_params['inList'] )
    fetch_list = purge_already_fetched( total_list, user_params['rawDir'] )

    # start fetching data from twitter
    download_tweets( fetch_list, user_params['rawDir'] )

    # second pass for any failed downloads
    print '\nStarting second pass to retry any failed downloads';
    fetch_list = purge_already_fetched( total_list, user_params['rawDir'] )
    download_tweets( fetch_list, user_params['rawDir'] )

    # build output corpus
    build_output_corpus( user_params['outList'], user_params['rawDir'], 
                         total_list )

    return


if __name__ == '__main__':
    main()
</code></pre>
","python-2.7, twitter, sentiment-analysis","<p>For any other wary travelers ...</p>

<p>I noticed that KubiK888 didn't link where he found the updated code.</p>

<p>A) Here is a complete upload of the CSV i found on github - <a href=""https://raw.githubusercontent.com/zfz/twitter_corpus/master/full-corpus.csv"" rel=""noreferrer"">https://raw.githubusercontent.com/zfz/twitter_corpus/master/full-corpus.csv</a></p>

<p>Seems to have the entire 6000+ tweets, after ""irrelevant"" tweets are removed it has 3000+ observations.</p>

<p>B) Alternatively, here is a repository with the complete code where someone updated the original 0.1 version by Nick Sanders to support twitter API 1.1 (including oAUTH)</p>

<p><a href=""https://github.com/aweiand/TwitterSentiment/blob/71c007948b8fb854b1df0b2a3a32d2629653e74b/GetTwitterCorpus/getTweets.py"" rel=""noreferrer"">https://github.com/aweiand/TwitterSentiment/blob/71c007948b8fb854b1df0b2a3a32d2629653e74b/GetTwitterCorpus/getTweets.py</a></p>

<p>It also has the full corpus in various formats:
<a href=""https://github.com/aweiand/TwitterSentiment/tree/71c007948b8fb854b1df0b2a3a32d2629653e74b/GetTwitterCorpus"" rel=""noreferrer"">https://github.com/aweiand/TwitterSentiment/tree/71c007948b8fb854b1df0b2a3a32d2629653e74b/GetTwitterCorpus</a></p>
",7,1,3507,2014-11-12 03:30:58,https://stackoverflow.com/questions/26878777/unable-to-download-the-twitter-sentiment-corpus-by-niek-sanders
Sentiment Analysis java Library,"<p>I have some unlabeled microblogging posts and I want to create a sentiment analysis module. </p>

<p>To do this I have try <a href=""http://nlp.stanford.edu/sentiment/"">Stanford library</a> and <a href=""http://www.alchemyapi.com/api/sentiment-analysis/"">Alchemy Api</a> web service but the result it is not very good.  For now I don't want  training my classifier. </p>

<p>So I would like to suggest me some libraries or some web services about that. I would prefer a tested Library.  The language of this posts is English. Also the preprocessing has been done.</p>

<p>P.S.</p>

<p>The programing language that I use is Java EE</p>
","java, machine-learning, data-mining, text-mining, sentiment-analysis","<p>If you want a good sentiment analysis service and you don't want to train your own classifier, you have to pay for it. However, it's worth mentioning that don't exist perfect tools in this field. There aren't tools that guarantee 100% of accuracy in their analysis. </p>

<p>Having said that, a couple of months ago I played around with <a href=""https://semantria.com/support/developer/"" rel=""nofollow"">Semantria/Lexalytics</a>. They have a straightforward Java SDK and a good accuracy on their sentiment analysis results.</p>
",4,7,24330,2014-11-15 18:32:03,https://stackoverflow.com/questions/26949249/sentiment-analysis-java-library
Detecting danger in tweets,"<p>Looking for APIs, methods, research, etc on the subject of deciding whether a tweet (a string, really) conveys a mood of danger.</p>

<p>For example:</p>

<ul>
<li><p>Danger: ""this house across the street is on fire!!</p></li>
<li><p>Not danger: ""this girl is on fire! love this song""</p></li>
</ul>
","machine-learning, nlp, sentiment-analysis","<p>There is little research done on the <em>particular</em> problem of detecting danger, but there are a few research papers describing methods to detect natural hazards. Your example is reminiscent of the title of one of them: <a href=""http://www.aclweb.org/anthology/U13-1011"" rel=""nofollow"" title=""Finding Fires with Twitter"">Finding Fires with Twitter</a>. Another research that you may find useful is <a href=""http://link.springer.com/chapter/10.1007/978-3-319-11818-5_19"" rel=""nofollow"" title=""Emergency Situation Awareness: Twitter Case Studies"">Emergency Situation Awareness: Twitter Case Studies</a>.</p>

<p>In general, however, the best approach to solve such a problem is through supervised classification, very similar to how sentiment analysis is (or rather, was, because there are more sophisticated machine learning paradigms like <a href=""http://en.wikipedia.org/wiki/Deep_learning"" rel=""nofollow"" title=""Deep Learning"">Deep Learning</a> being applied nowadays) done.</p>

<p>The essence is to label documents (in your case, tweets) into ""danger"" and ""not danger"". This labeling is done by human experts. Ideally, they should be well versed in the language and the domain. So, using native English speakers who know the colloquialisms of Twitter would be perfect annotators for this task.</p>

<p>Once adequate number of documents have been labeled, the baseline (i.e. the basic approach) is usually achieved by creating n-gram word vectors as feature vectors, and running SVM. If you are not aware of machine learning details, please read up on them before doing this.</p>
",2,0,73,2014-11-15 22:22:25,https://stackoverflow.com/questions/26951415/detecting-danger-in-tweets
Sentiment prediction Google Sentiment API,"<p>I am using Google API to predict the sentiment of student for professor, I have already trained a model for it. and I also test it. It works. </p>

<p>But I have about 20 K, data in my hand, it is too much to copt and paste to test it. Do you guys have some ideas can make it easy to run for these 20 k data?</p>

<p>thx.</p>
","google-api, sentiment-analysis","<p>Put your data into a CSV file and upload it to your Google Cloud Storage.
You can upload it via the Google Developers Console, GSUtil or by using Google Cloud Storage API's</p>

<p>The maximum file size for the CSV is 2.5GB, you can find more information on the their developer guide @ <a href=""https://cloud.google.com/prediction/docs/developer-guide"" rel=""nofollow"">Training Data File Format</a></p>

<ul>
<li><a href=""https://console.developers.google.com"" rel=""nofollow"">Google Developer Console</a> </li>
<li><a href=""https://cloud.google.com/storage/docs/gsutil"" rel=""nofollow"">Google GSUtil</a> </li>
<li><a href=""https://cloud.google.com/storage/docs/concepts-techniques"" rel=""nofollow"">Cloud Storage API</a></li>
</ul>
",0,0,153,2014-11-24 22:44:34,https://stackoverflow.com/questions/27115567/sentiment-prediction-google-sentiment-api
qdap ngram polarity dictionary,"<p>Dear Stackoverlow crowd</p>

<p>I managed to use the qdap polarity function to calculate the polarity of some blog entries, loading my own dictionary, based on sentiWS. Now I do have a new sentiment dictionary (<a href=""http://www.opinion-mining.org/SePL-Sentiment-Phrase-List"" rel=""nofollow"">SePL</a>) which not only contains single words, but as well phrases. For example ""simply good"", where ""simply"" is neither a negator nor an amplifier, but makes it more precise. So i was wondering, wether I could search for ngrams using the polarity function of qdap.</p>

<p>As an example:</p>

<pre><code>library(qdap)
phrase &lt;- ""This is simply the best""
key &lt;- sentiment_frame(c(""simply"", ""best"", ""simply the best""), """", c(0.1,0.3,0.8))
counts(polarity(phrase, polarity.frame=key))
</code></pre>

<p>gives:</p>

<pre><code>  all wc polarity    pos.words neg.words                text.var
1 all  5    0.179 simply, best         - This is simply the best
</code></pre>

<p>However, I would like to get an output like:</p>

<pre><code>  all wc polarity    pos.words neg.words                text.var
1 all  5    0.76 simply the best         - This is simply the best
</code></pre>

<p>Anyone an Idea how to get that working like that?</p>

<p>All the best,
Ben</p>
","r, dictionary, sentiment-analysis, qdap","<p>This is a bug reintroduced with chages to the <code>bag_o_word</code> function earlier this year.  This is the second time a bug like this has affected ngram polarity since I enble the usage of ngrams in polarity.frame: <a href=""https://github.com/trinker/qdap/issues/185"" rel=""nofollow"">https://github.com/trinker/qdap/issues/185</a></p>

<p>I have fixed the bug and added a unit test to ensure this bug doesn't creep back into the code.  Your code in qdap 2.2.1 now gives the desired output, though the warning against the original intention of the algorithm remains:</p>

<pre><code>&gt; library(qdap)
&gt; phrase &lt;- ""This is simply the best""
&gt; key &lt;- sentiment_frame(c(""simply"", ""best"", ""simply the best""), """", c(0.1,0.3,0.8))
&gt; counts(polarity(phrase, polarity.frame=key))

  all wc polarity       pos.words neg.words                text.var
1 all  5    0.358 simply the best         - This is simply the best
</code></pre>

<p><s><strong>qdap</strong>'s <code>polarity</code> function uses an algorithm that was not designed to operate like this.  You can do it using the following hack but know that it is out of the intent of the underlying theory used in the function's algorithm:</s></p>

<pre><code>library(qdap)
phrase &lt;- ""This is simply the best""

terms &lt;- c(""simply"", ""best"", ""simply the best"")
key &lt;- sentiment_frame(space_fill(terms, terms, sep=""xxx""), NULL, c(0.1,0.3,0.8))

counts(polarity(space_fill(phrase, terms, ""xxx""), polarity.frame=key))

##   all wc polarity           pos.words neg.words                    text.var
## 1 all  3    0.462 simplyxxxthexxxbest         - This is simplyxxxthexxxbest
</code></pre>
",2,3,1401,2014-11-26 18:54:02,https://stackoverflow.com/questions/27156834/qdap-ngram-polarity-dictionary
extracting hashtags from tweets,"<p>I am trying to perform sentiment analysis and facing a small problem. I am using a dictionary which has hashtags and some other junk value(shown below). It also has associated weight of the hashtag. I want to extract only the hashtags and its corresponding weight into a new data frame. Is there any easy way to do it?
I have tried using regmatches, but some how its giving output in list format and is messing things up.
Input:</p>

<pre><code>            V1    V2
1    #fabulous 7.526
2   #excellent 7.247
3      superb 7.199
4  #perfection 7.099
5    #terrific 6.922
6 #magnificent 6.672
</code></pre>

<p>Output:</p>

<pre><code>            V1    V2
1    #fabulous 7.526
2   #excellent 7.247
3  #perfection 7.099
4    #terrific 6.922
5 #magnificent 6.672
</code></pre>
","r, statistics, analytics, hashtag, sentiment-analysis","<p>This code should work and will give you desired output as data.frame   </p>

<pre><code> Input&lt;- data.frame(V1 = c(""#fabulous"",""#excellent"",""superb"",""#perfection"",""#terrific"",""#magnificent""), V2 = c(""7.526"",  ""7.247"" , ""7.199"", ""7.099"",  ""6.922"", ""6.672"")) 
 extractHashtags &lt;- Input[which(substr(Input$V1,1,1) == ""#""),]
 View(extractHashtags)
</code></pre>
",0,3,816,2014-11-27 10:23:43,https://stackoverflow.com/questions/27168226/extracting-hashtags-from-tweets
checking score for tweets to do sentiment analysis,"<p>I am trying to do some analysis on twitter data. So I have tweets </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>head(words) 1 ""#fabulous"" ""rock"" ""is"" ""#destined"" ""to"" ""be"" ""star""</code></pre>
</div>
</div>
</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&gt; head(hashtags)
      hashtags score
1    #fabulous 7.526
2   #excellent 7.247
3      #superb 7.199
4  #perfection 7.099
5    #terrific 6.922
6 #magnificent 6.672</code></pre>
</div>
</div>
</p>

<p>So I want a to check words against hashtags dataframe and words character array and for every match, I want the sum of the value of scores.
So in above case I want the output to be 7.526+6.922=14.448</p>

<p>Any help would be greatly appreciated.</p>
","r, statistics, sentiment-analysis","<p>Try this</p>

<pre><code>words_hashtags &lt;- words[grepl('^#', words)]
scores &lt;- hashtags[hashtags$hashtags %in% words_hashtags, 'score']
sum(scores)
</code></pre>

<p><code>grepl</code> returns a logical vector indicating which words has hashtags in the beginning. The rest is just basic <code>R</code> syntax.</p>

<p>More options to get <code>words_hashtags</code>:</p>

<pre><code>words_hashtags &lt;- grep('^#', words, value=T)
words_hashtags &lt;- words[grep('^#', words, value=F)]
</code></pre>
",0,0,94,2014-11-28 11:15:54,https://stackoverflow.com/questions/27187518/checking-score-for-tweets-to-do-sentiment-analysis
Machine Learning Sentiment Analysis,"<p>I have chosen another project for my Machine Learning course but it seems it was not fiting the Professor's idea of ""project"". So I'm trying to find another one.
FRom the 2 months of the course I just know few basics stuff about ML.
I was thinking about doing a Sentiment analysis but I don't know so much about it. Thus, I have to write it in Python and neither I don't know what kind of tools it is better to use nor which kinda dataset I need.I'd like someone to help me in defining how the work should be done.</p>
","machine-learning, sentiment-analysis","<p>Please note that Sentiment Analysis is a broad area, and so you can consider one aspect of it for your project. You can read this paper - <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;sqi=2&amp;ved=0CCMQFjAA&amp;url=http%3A%2F%2Fwww.cs.uic.edu%2F~liub%2FFBS%2FNLP-handbook-sentiment-analysis.pdf&amp;ei=DJGIVNqGKMipyATNkYCQBQ&amp;usg=AFQjCNGQAPKgEzYY6A1SOph26PrkCqYcaQ&amp;sig2=0YSs11QaDUzIh-TzsHRQ6g&amp;bvm=bv.81456516,d.aWw"" rel=""nofollow"">1</a>, which gives you an overall idea of what sentiment analysis is, and what work has been done till date on various aspects of it. Also, this paper -<a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0CCUQFjAB&amp;url=http%3A%2F%2Fwww.cs.cornell.edu%2Fhome%2Fllee%2Fpapers%2Fsentiment.pdf&amp;ei=lZGIVMnXPISuyASgpIHgAQ&amp;usg=AFQjCNFpt7e6_MCHDQx5SvaMO005pU3bWA&amp;sig2=A7WabXTU7b9vhk_wHY0kMg&amp;bvm=bv.81456516,d.aWw"" rel=""nofollow"">2</a> will give a brief idea on how machine learning can be used for sentiment classification on movie review datasets.</p>
",1,-1,139,2014-12-10 09:14:45,https://stackoverflow.com/questions/27397143/machine-learning-sentiment-analysis
Open Lexicon for classified affective words,"<p>I'm looking for lexicons like <a href=""http://www.mhs.com/product.aspx?gr=cli&amp;prod=poms&amp;id=overview"" rel=""nofollow"">POMS</a> or <a href=""http://csea.phhp.ufl.edu/media/anewmessage.html"" rel=""nofollow"">ANEW</a> that classify words, especially adjectives, into the 6 ""universal"" emotions: ""anger, disgust, fear, happiness, sadness, and surprise"" by Paul Ekman or using <a href=""http://www.csc.ncsu.edu/faculty/healey/tweet_viz/figs/circumplex.png"" rel=""nofollow"">Russel's approach</a>.</p>

<p>Do any of you know a lexicon that is free to use?</p>
","nlp, sentiment-analysis","<p>You could try using the structure provided by <a href=""http://wordnet.princeton.edu/"" rel=""nofollow"">wordnet</a> for synonym sets of adjectives:</p>

<blockquote>
  <p>The most frequently encoded relation among synsets is the super-subordinate relation (also called hyperonymy, hyponymy or ISA relation)</p>
</blockquote>

<p>I'm not sure, however, if this hierarchy will lead you to the 6 emotions you're looking for.</p>

<p>A valuable set of resources, by Saif Mohammad, including the NRC Emotion Lexicon available <a href=""http://www.saifmohammad.com/WebPages/lexicons.html"" rel=""nofollow"">here</a> - I think that is closer, except it's not limited to adjectives.</p>

<p>Another promising wordnet extension for affect words <a href=""http://wndomains.fbk.eu/wnaffect.html"" rel=""nofollow"">here</a>.</p>

<p>Hope this helps.</p>
",3,2,1058,2014-12-11 21:05:56,https://stackoverflow.com/questions/27432338/open-lexicon-for-classified-affective-words
How to analyse sentiment of a news article using AlchemyAPI?,"<p>I want to analyse sentiment of a news article (input: text not URL) using AlchemyAPI. Please suggest me how Can I do that.</p>

<p>I tried with demo <code>http://www.alchemyapi.com/products/demo/alchemylanguage/</code> but it provides sentiment for each entities separately, not for the whole article text. </p>

<p>Thanks for help.</p>
","sentiment-analysis, alchemyapi","<p>You can get the sentiment for the whole document in the demo, (see screenshot):</p>

<p><img src=""https://i.sstatic.net/Njbnw.png"" alt=""enter image description here""></p>

<p>If you want to do it programatically, the API call you are looking for is <a href=""http://www.alchemyapi.com/api/sentiment/textc.html"" rel=""nofollow noreferrer"">TextGetTextSentiment</a>. You should first register for a free API key. Then to get the sentiment of the text ""my great text"", go to the following URL:</p>

<pre><code>http://access.alchemyapi.com/calls/text/TextGetTextSentiment?apikey=&lt;YOUR_API_KEY&gt;&amp;sentiment=1&amp;showSourceText=1&amp;text=my%20great%20text
</code></pre>

<p>As you see the text is url encoded at the end. For more details take a look at the documentation of <a href=""http://www.alchemyapi.com/api/sentiment/textc.html"" rel=""nofollow noreferrer"">TextGetTextSentiment</a>.</p>
",1,2,392,2014-12-14 12:56:39,https://stackoverflow.com/questions/27469512/how-to-analyse-sentiment-of-a-news-article-using-alchemyapi
aspect based sentiment using NLTK?,"<p>I'm working on Aspect Based sentiment analysis. I have a training set (text ,and aspectTerms) for each review.</p>

<p>Using NLTK3, I want to build a Naive Bayes Classifier that predicts aspects of unseen test data. I will use a feature Part of speech tagging of aspects.</p>

<p>What does the feature method look like? How do I train the classifier? How to test the classifier on unseen data?</p>

<p>I tried the following:</p>

<p>My feature function looks like </p>

<pre><code>def featurepos(word):
    return dict(wordpos,true)  //return part of speech tagging for a given word
</code></pre>

<p>and built a classifier as</p>

<pre><code>classifier=NaiveBayesClassifier.train(train_set)
#train_set contains [(posofaspect,""aspect"")]
</code></pre>

<p>Now I want to test the classifier with test data.</p>

<p>To check the accuracy, the code below required <code>test_set</code> </p>

<pre><code>nltk.classify.util.accuracy(classifier,test_set)
</code></pre>

<p><code>test_set</code> is a list of dictionaries? If it's a dict what is the its structure? -I'm confused.
If there is an example it would be so helpful!</p>
","python, classification, nltk, sentiment-analysis","<p>This is what the accuracy function does, as per the <a href=""http://www.nltk.org/_modules/nltk/classify/util.html#accuracy"" rel=""nofollow"">documentation</a>:</p>

<pre><code>def accuracy(classifier, gold):
    results = classifier.classify_many([fs for (fs,l) in gold])
    correct = [l==r for ((fs,l), r) in zip(gold, results)]
    if correct:
        return float(sum(correct))/len(correct)
    else:
        return 0
</code></pre>

<p>Test set is therefore a list of tuples (features, corresponding labels) and the function (1) uses the trained classifier to compute the outputs on those features, (2) confront the classification results with the given labels and (3) output the hit ratio</p>
",0,-1,2086,2014-12-20 08:26:12,https://stackoverflow.com/questions/27578448/aspect-based-sentiment-using-nltk
NLTK convert tokenized sentence to synset format,"<p>I'm looking to get the similarity between a single word and each word in a sentence using NLTK.</p>

<p>NLTK can get the similarity between two specific words as shown below. This method requires that a specific reference to the word is given, in this case it is 'dog.n.01' where dog is a noun and we want to use the first (01) NLTK definition.</p>

<pre><code>dog = wordnet.synset('dog.n.01')
cat = wordnet.synset('cat.n.01')
print  dog.path_similarity(cat)
&gt;&gt; 0.2
</code></pre>

<p>The problem is that I need to get the part of speech information from each word in the sentence. The NLTK package has the ability to get the parts of speech for each word in a sentence as shown below. However, these speech parts ('NN', 'VB', 'PRP'...) don't match up with the format that the synset takes as a parameter.</p>

<pre><code>text = word_tokenize(""They refuse to permit us to obtain the refuse permit"")
pos_tag(text)
&gt;&gt; [('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]
</code></pre>

<p>Is is possible to get the synset formatted data from pos_tag() results in NLTK? By synset formatted I mean the format like <code>dog.n.01</code></p>
","python, nltk, sentiment-analysis","<p>You can use a simple conversion function:</p>

<pre><code>from nltk.corpus import wordnet as wn

def penn_to_wn(tag):
    if tag.startswith('J'):
        return wn.ADJ
    elif tag.startswith('N'):
        return wn.NOUN
    elif tag.startswith('R'):
        return wn.ADV
    elif tag.startswith('V'):
        return wn.VERB
    return None
</code></pre>

<p>After tagging a sentence you can tie a word inside the sentence with a SYNSET using this function. Here's an example:</p>

<pre><code>from nltk.stem import WordNetLemmatizer
from nltk import pos_tag, word_tokenize

sentence = ""I am going to buy some gifts""
tagged = pos_tag(word_tokenize(sentence))

synsets = []
lemmatzr = WordNetLemmatizer()

for token in tagged:
    wn_tag = penn_to_wn(token[1])
    if not wn_tag:
        continue

    lemma = lemmatzr.lemmatize(token[0], pos=wn_tag)
    synsets.append(wn.synsets(lemma, pos=wn_tag)[0])

print synsets
</code></pre>

<p>Result: <strong>[Synset('be.v.01'), Synset('travel.v.01'), Synset('buy.v.01'), Synset('gift.n.01')]</strong></p>
",10,8,4199,2014-12-21 16:58:07,https://stackoverflow.com/questions/27591621/nltk-convert-tokenized-sentence-to-synset-format
Using WN-Affect to detect emotion/mood of a string,"<p>I  downloaded <a href=""http://wndomains.fbk.eu/wnaffect.html"" rel=""noreferrer"">WN-Affect</a>. I am however not sure how to use it to detect the mood of a sentence. For example if I have a string ""I hate football."" I want to be able to detect whether the mood is bad and the emotion is fear. WN-Affect has no tutorial on how to do it, and I am kind of new to python. Any help would be great!</p>
","python, nlp, nltk, sentiment-analysis, wordnet","<p><strong>In short</strong>: Use SentiWordNet instead and look at <a href=""https://github.com/kevincobain2000/sentiment_classifier"">https://github.com/kevincobain2000/sentiment_classifier</a></p>

<hr>

<p><strong>In Long</strong>:</p>

<p><strong>Affectedness vs Sentiment</strong></p>

<p>The line between affect and sentiment is very fine. One should looking into <code>Affectedness</code> in linguistics studies, e.g. <a href=""http://compling.hss.ntu.edu.sg/events/2014-ws-affectedness/"">http://compling.hss.ntu.edu.sg/events/2014-ws-affectedness/</a> and <code>Sentiment Analysis</code> in computational researches. For now, let's call both the task of identifying affect and sentiment, sentiment analysis.</p>

<p>Also note that <code>WN-Affect</code> is a rather old resource compared to <code>SentiWordNet</code>, <a href=""http://sentiwordnet.isti.cnr.it/"">http://sentiwordnet.isti.cnr.it/</a>. </p>

<p><strong>Here's a good resource for using SentiWordNet for sentiment analysis</strong>: <a href=""https://github.com/kevincobain2000/sentiment_classifier"">https://github.com/kevincobain2000/sentiment_classifier</a>. </p>

<p>Often sentiment analysis has only two classes, <code>positive</code> or <code>negative</code> sentiment. Whereas the WN-affect uses 11 types of affectedness labels:</p>

<ul>
<li>emotion</li>
<li>mood </li>
<li>trait    </li>
<li>cognitive state  </li>
<li>physical state   </li>
<li>hedonic signal   </li>
<li>emotion-eliciting </li>
<li>emotional response   </li>
<li>behaviour    </li>
<li>attitude </li>
<li>sensation</li>
</ul>

<p>For each type, there are multiple classes, see <a href=""https://github.com/larsmans/wordnet-domains-sentiwords/blob/master/wn-domains/wn-affect-1.1/a-hierarchy.xml"">https://github.com/larsmans/wordnet-domains-sentiwords/blob/master/wn-domains/wn-affect-1.1/a-hierarchy.xml</a></p>

<hr>

<p>To answer the question of how one can use the WN-Affect, there're several things you need to do:</p>

<p>First map WN1.6 to WN3.0 (it's not an easy task, you have to do several mappings, especially the mapping between 2.0-2.1)</p>

<p>Now using the WN-Affect with WN3.0, you can apply </p>

<ul>
<li>the same classification technique as he SentiWordNet sentiment classifier or</li>
<li>try to maximize the classes within text and then use some heuristics to choose 'positive' / 'negative'</li>
</ul>
",16,9,9079,2015-01-14 12:51:31,https://stackoverflow.com/questions/27943396/using-wn-affect-to-detect-emotion-mood-of-a-string
Stanford sentiment analysis score java,"<p>I use <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford core NLP</a> library for sentiment analysis. The below code return the class of an example but how can I get the score? for example -0.3 for negative etc</p>

<pre><code>private int getScore(String line) {
    boolean isrun = false;
    StanfordCoreNLP pipeline = null;
    if(!isrun){
        Properties props = getProperties();
        pipeline = new StanfordCoreNLP(props);
        isrun = true;
    }
    Annotation annotation;

    int sentiment = -1;
    if (line != null &amp;&amp; line.length() &gt; 0) {
        annotation = pipeline.process(line);
        for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
            Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
            sentiment = RNNCoreAnnotations.getPredictedClass(tree);
        }
    }
    return sentiment;
}
</code></pre>

<p><strong>EDIT</strong></p>

<p>In online <a href=""http://nlp.stanford.edu:8080/sentiment/rntnDemo.html"" rel=""nofollow"">demo</a> when mouse is on root at the graph we can see that the example is negative 72%. How can get this number?</p>
","java, sentiment-analysis, stanford-nlp","<p>0.Download Stanford NLP Core Lib and import external lib stanford-corenlp-3.5.2-models.jar, stanford-corenlp-3.5.2.jar, stanford-corenlp-3.5.2-sources.jar and ejml-0.23.jar into this package.</p>

<p>1.Build this class NLP in Eclipse </p>

<pre><code>import java.util.Properties;
import org.ejml.simple.SimpleMatrix;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations.SentimentAnnotatedTree;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;

public class NLP {
static StanfordCoreNLP pipeline;

public static void init() {
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
    pipeline = new StanfordCoreNLP(props);
}

public static int findSentiment(String tweet) {

    int mainSentiment = 0;
    if (tweet != null &amp;&amp; tweet.length() &gt; 0) {
        int longest = 0;
        Annotation annotation = pipeline.process(tweet);
        for (CoreMap sentence : annotation
                .get(CoreAnnotations.SentencesAnnotation.class)) {
            Tree tree = sentence
                    .get(SentimentAnnotatedTree.class);
            int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
            SimpleMatrix sentiment_new = RNNCoreAnnotations.getPredictions(tree);             
            String partText = sentence.toString();
            if (partText.length() &gt; longest) {
                mainSentiment = sentiment;
                longest = partText.length();
            }
        }
    }
    return mainSentiment;
    }
}
</code></pre>

<p>2.Build a new class to parse your sentence with NLP</p>

<pre><code>import java.util.ArrayList;

public class What2Think {

    public static void main(String[] args) {
        ArrayList&lt;String&gt; tweets = new ArrayList&lt;String&gt;();
        tweets.add(""In this country, \""democracy\"" means pro-government. #irony"");
        NLP.init();
        for(String tweet : tweets) {
            System.out.println(tweet + "" : "" + NLP.findSentiment(tweet));
        }
    }
}
</code></pre>

<p>Run it</p>
",5,4,5587,2015-01-18 21:03:42,https://stackoverflow.com/questions/28014779/stanford-sentiment-analysis-score-java
sentiment analysis with different number of documents,"<p>I am trying to do sentiment analysis on newspaper articles and track the sentiment level across time. To do that, basically I will identify all the relevant news articles within a day, feed them into the polarity() function and obtain the average polarity scores of all the articles (more precisely, the average of all the sentence from all the articles) within that day. </p>

<p>The problem is, for some days, there will be many more articles compared to other days, and I think this might mask some of the info if we simply track the daily average polarity score. For example, a score of 0.1 from 30 news articles should carry more weight compared to a score of 0.1 generated from only 3 articles. and sure enough, some of the more extreme polarity scores I obtained came from days whereby there are only few relevant articles.</p>

<p>Is there anyway I can take the different number of articles each day into consideration? </p>

<pre><code>library(qdap)
sentence = c(""this is good"",""this is not good"")
polarity(sentence)
</code></pre>
","r, sentiment-analysis, qdap","<p>I would warn that sometimes saying something strong with few words may pack the most punch.  Make sure what you're doing makes sense in terms of your data and research questions.</p>

<p>One approach would be to use number of words as in the following example (I like the first approach moreso here):</p>

<pre><code>poldat2 &lt;- with(mraja1spl, polarity(dialogue, list(sex, fam.aff, died)))

output &lt;- scores(poldat2)
weight &lt;- ((1 - (1/(1 + log(output[[""total.words""]], base = exp(2))))) * 2) - 1
weight &lt;- weigth/max(weight)
weight2 &lt;- output[[""total.words""]]/max(output[[""total.words""]])

output[[""weighted.polarity""]] &lt;- output[[""ave.polarity""]] * weight   
output[[""weighted.polarity2""]] &lt;- output[[""ave.polarity""]] * weight2   
output[, -c(5:6)]


##    sex&amp;fam.aff&amp;died total.sentences total.words ave.polarity weighted.polarity weighted.polarity2
## 1       f.cap.FALSE             158        1641        0.083       0.143583793        0.082504197
## 2        f.cap.TRUE              24         206        0.044       0.060969157        0.005564434
## 3       f.mont.TRUE               4          29        0.079       0.060996614        0.001397106
## 4       m.cap.FALSE              73         651        0.031       0.049163984        0.012191207
## 5        m.cap.TRUE              17         160       -0.176      -0.231357933       -0.017135804
## 6     m.escal.FALSE               9         170       -0.164      -0.218126656       -0.016977931
## 7      m.escal.TRUE              27         590       -0.067      -0.106080866       -0.024092720
## 8      m.mont.FALSE              70         868       -0.047      -0.078139272       -0.025099276
## 9       m.mont.TRUE             114        1175       -0.002      -0.003389105       -0.001433481
## 10     m.none.FALSE               7          71        0.066       0.072409049        0.002862997
## 11  none.none.FALSE               5          16       -0.300      -0.147087026       -0.002925046
</code></pre>
",2,1,345,2015-01-21 01:08:33,https://stackoverflow.com/questions/28057964/sentiment-analysis-with-different-number-of-documents
Cleaning text of tweet messages,"<p>I have a csv of tweets. I got it using this <code>ruby</code> library: </p>

<p><a href=""https://github.com/sferik/twitter"" rel=""nofollow"">https://github.com/sferik/twitter</a> .</p>

<p>The csv is two columns and 150 rows, the second column is the text message: </p>

<pre><code>    Text
1   RT @AlstomTransport: #Alstom and OHL to supply a #metro system to #Guadalajara #rail #Mexico  http://t.co/H88paFoYc3 http://t.co/fuBPPqNts4
</code></pre>

<p>I have to do a sentiment analysis, so i need to clean the text message, removing links, RT, Via, and everything useless for the analysis.</p>

<p>I tried with R, using code found in several tutorials:</p>

<pre><code>&gt; data1 = gsub(""(RT|via)((?:\\b\\W*@\\w+)+)"", """", data1)
</code></pre>

<p>But the output is without any sense: </p>

<pre><code>[1] ""1:150""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
[2] ""c(113, 46, 38, 11, 108, 100, 45, 44, 9, 89, 99, 93, 102, 101, 110, 93, 61, 57, 104, 66, 86, 53, 42, 43, 37, 7, 88, 32, 122, 131, 14, 102, 105, 12, 54, 13, 72, 87, 55, 132, 29, 28, 10, 15, 81, 81, 107, 87, 106, 81, 98, 73, 65, 52, 94, 97, 65, 59, 60, 50, 48, 121, 117, 75, 79, 111, 115, 119, 118, 91, 79, 31, 76, 111, 85, 62, 91, 103, 79, 120, 78, 47, 49, 8, 129, 123, 124, 58, 71, 25, 36, 80, 127, 112, 23, 22, 35, 21, 30, 74, 82, 51, 63, 130, 135, 134, 90, 83, 63, 128, 16, 20, 19, 34, 27, 26, 33, 77, \n114, 126, 64, 69, 4, 135, 41, 40, 17, 67, 92, 96, 84, 92, 56, 18, 125, 5, 6, 133, 24, 39, 70, 95, 116, 68, 84, 109, 92, 3, 1, 2)""
</code></pre>

<p>Can anyone help me? Thank you.</p>
","r, twitter, sentiment-analysis","<p>Looks like you tried to pass in the entire data.frame to <code>gsub</code> rather than just the text column. <code>gsub</code> prefers to work on character vectors. Instead you should do</p>

<pre><code>data1[,2] = gsub(""(RT|via)((?:\\b\\W*@\\w+)+)"", """", data1[,2])
</code></pre>

<p>to just transform the second column.</p>
",3,1,1187,2015-01-22 18:11:13,https://stackoverflow.com/questions/28095821/cleaning-text-of-tweet-messages
Processing a corpus so big I&#39;m getting runtime errors,"<p>I am trying to process a big corpus of tweets (1,600,000, can be found <a href=""http://help.sentiment140.com/for-students/"" rel=""nofollow"">here</a>) with the following code to train a Naive Bayes Classifier in order to play around with sentiment analysis.</p>

<p>My problem is I never coded anything that ever had to handle much memory or big variables.</p>

<p>At the moment the script runs for a while and then after a couple hours I get a runtime error (I'm on a Windows machine). I belive I'm not managing the list objects properly.</p>

<p>I am successfully running the program while limiting the for cycle but that means limiting the training set and quite likely getting worse sentiment analysis results.</p>

<p>How can I process the whole corpus? How can I better manage those lists? Are really those the ones causing the problem?</p>

<p>These are the imports</p>

<pre><code>import pickle
import re
import os, errno
import csv
import nltk, nltk.classify.util, nltk.metrics
from nltk.classify import NaiveBayesClassifier
</code></pre>

<p>Here I load the corpora and create the lists where I want to store the features I extract from the corpus</p>

<pre><code>inpTweets = csv.reader(open('datasets/training.1600000.processed.noemoticon.csv', 'rb'), delimiter=',', quotechar='""')
tweets = []
featureList = []
n=0
</code></pre>

<p>This for cycle extracts the stuff from the corpora and thanks to processTweet(), a long algorithm, I extract the features from each row of the .CSV  </p>

<pre><code>for row in inpTweets:
    sentiment = row[0]
    status_text = row[5]
    featureVector = processTweet(status_text.decode('utf-8')) 
    #to know it's doing something
    n = n + 1
    print n
    #we'll need both the featurelist and the tweets variable, carrying tweets and sentiments
</code></pre>

<p>Here I extend/append the lists / the variables to the lists, we're still inside the for cycle.</p>

<pre><code>    featureList.extend(featureVector)  
    tweets.append((featureVector, sentiment))              
</code></pre>

<p>When the cycle ends I get rid of duplicates in the featureList and save it to a pickle.</p>

<pre><code>featureList = list(set(featureList))
flist = open('fList.pickle', 'w')
pickle.dump(featureList, flist)
flist.close()
</code></pre>

<p>I get the features ready for the classifier.</p>

<pre><code>training_set = nltk.classify.util.apply_features(extract_features, tweets)
</code></pre>

<p>Then I train the classifier and save it to a pickle.</p>

<pre><code># Train the Naive Bayes classifier
print ""\nTraining the classifier..""
NBClassifier = nltk.NaiveBayesClassifier.train(training_set)
fnbc = open('nb_classifier.pickle', 'w')
pickle.dump(NBClassifier, fnbc)
fnbc.close()
</code></pre>

<p>edit: 19:45 gmt+1 - forgot to add n=0 in this post.</p>

<p>edit1: Due to lack of time and computing power limitations I choose to reduce the corpus like this -</p>

<pre><code>.....
n=0
i=0
for row in inpTweets:
    i = i+1
    if (i==160):         #limiter
        i = 0
        sentiment = row[0]
        status_text = row[5]  
        n = n + 1
.....
</code></pre>

<p>As in the end the classifier was taking ages to train. About the runtime error please see the comments. Thanks everyone for the help.</p>
","python, runtime-error, nltk, pickle, sentiment-analysis","<p>You could use <code>csv.field_size_limit(int)</code></p>

<p>For example:</p>

<pre><code>f = open('datasets/training.1600000.processed.noemoticon.csv', 'rb')
csv.field_size_limit(100000)
inpTweets = csv.reader(f, delimiter=',', quotechar='""')
</code></pre>

<p>You can try changing the value 100,000 to something better maybe.</p>

<p>+1 on the comment about Pandas.</p>

<p>Also, you might want to check out <code>cPickle</code> <a href=""https://docs.python.org/2/library/pickle.html#module-cPickle"" rel=""nofollow noreferrer"">here</a>.
(1000x faster)</p>

<hr>

<p>Check out <a href=""https://stackoverflow.com/questions/17444679/reading-a-huge-csv-in-python"">this question / answer</a> too !</p>

<p>Another relevant blog post <a href=""http://lethain.com/handling-very-large-csv-and-xml-files-in-python/"" rel=""nofollow noreferrer"">here</a>.</p>
",0,0,468,2015-02-01 17:56:29,https://stackoverflow.com/questions/28265862/processing-a-corpus-so-big-im-getting-runtime-errors
xampp crashes when many simultaneous API requests are made,"<p>I'm making an application which takes in a user's tweets using the Twitter API and one component of it is performing sentiment extraction from the tweet texts. For development I'm using xampp, of course using the Apache HTML Server as my workspace. I'm using Eclipse for PHP as an IDE.</p>

<p>For the sentiment extraction I'm using the <a href=""http://www.uclassify.com/browse/uClassify/Sentiment"" rel=""nofollow"">uClassify Sentiment Classifier</a>. The Classifier uses an API to receive a number of requests and with each request it sends back XML data from which the sentiment values can be parsed.</p>

<p>Now the application may process a large number of tweets (maximum allowed is 3200) at once. For example if there are 3200 tweets then the system will send 3200 API calls at once to this Classifier. Unfortunately for this number the system does not scale well and in fact xampp crashes after a short while of running the system with these calls. However, with a modest number of tweets (for example 500 tweets) the system works fine, so I am assuming it may be due to large number of API calls. It may help to note that the maximum number of API calls allowed by uClassify per day is 5000, but since the maximum is 3200 I am pretty sure that it is not exceeding this number. </p>

<p>This is pretty much my first time working on this kind of web development, so I am not sure if I'm making a rookie mistake here. I am not sure what I could be doing wrong and don't know where to start looking. Any advice/insight will help a lot! </p>

<p><strong>EDIT: added source code in question</strong></p>

<p><strong>Update index method</strong></p>

<pre><code>function updateIndex($timeline, $connection, $user_handle, $json_index, $most_recent) {
    // URL arrays for uClassify API calls
    $urls = [ ];
    $urls_id = [ ];

    // halt if no more new tweets are found
    $halt = false;
    // set to 1 to skip first tweet after 1st batch
    $j = 0;
    // count number of new tweets indexed
    $count = 0;
    while ( (count ( $timeline ) != 1 || $j == 0) &amp;&amp; $halt == false ) {
        $no_of_tweets_in_batch = 0;
        $n = $j;
        while ( ($n &lt; count ( $timeline )) &amp;&amp; $halt == false ) {
            $tweet_id = $timeline [$n]-&gt;id_str;
            if ($tweet_id &gt; $most_recent) {
                $text = $timeline [$n]-&gt;text;
                $tokens = parseTweet ( $text );
                $coord = extractLocation ( $timeline, $n );
                addSentimentURL ( $text, $tweet_id, $urls, $urls_id );
                $keywords = makeEntry ( $tokens, $tweet_id, $coord, $text );
                foreach ( $keywords as $type ) {
                    $json_index [] = $type;
                }
                $n ++;
                $no_of_tweets_in_batch ++;
            } else {
                $halt = true;
            }
        }
        if ($halt == false) {
            $tweet_id = $timeline [$n - 1]-&gt;id_str;

            $timeline = $connection-&gt;get ( 'statuses/user_timeline', array (
                    'screen_name' =&gt; $user_handle,
                    'count' =&gt; 200,
                    'max_id' =&gt; $tweet_id 
            ) );
            // skip 1st tweet after 1st batch
            $j = 1;
        }
        $count += $no_of_tweets_in_batch;
    }

    $json_index = extractSentiments ( $urls, $urls_id, $json_index );

    echo 'Number of tweets indexed: ' . ($count);
    return $json_index;
}
</code></pre>

<p><strong>extract sentiment method</strong></p>

<pre><code>function extractSentiments($urls, $urls_id, &amp;$json_index) {
    $responses = multiHandle ( $urls );
    // add sentiments to all index entries
    foreach ( $json_index as $i =&gt; $term ) {
        $tweet_id = $term ['tweet_id'];
        foreach ( $urls_id as $j =&gt; $id ) {
            if ($tweet_id == $id) {
                $sentiment = parseSentiment ( $responses [$j] );
                $json_index [$i] ['sentiment'] = $sentiment;
            }
        }
    }
    return $json_index;
}
</code></pre>

<p><strong>Method for handling multiple API calls</strong> </p>

<p>This is where the uClassify API calls are being processed at once:</p>

<pre><code>function multiHandle($urls) {

    // curl handles
    $curls = array ();

    // results returned in xml
    $xml = array ();

    // init multi handle
    $mh = curl_multi_init ();

    foreach ( $urls as $i =&gt; $d ) {
        // init curl handle
        $curls [$i] = curl_init ();

        $url = (is_array ( $d ) &amp;&amp; ! empty ( $d ['url'] )) ? $d ['url'] : $d;

        // set url to curl handle
        curl_setopt ( $curls [$i], CURLOPT_URL, $url );

        // on success, return actual result rather than true
        curl_setopt ( $curls [$i], CURLOPT_RETURNTRANSFER, 1 );

        // add curl handle to multi handle
        curl_multi_add_handle ( $mh, $curls [$i] );
    }

    // execute the handles
    $active = null;
    do {
        curl_multi_exec ( $mh, $active );
    } while ( $active &gt; 0 );

    // get xml and flush handles
    foreach ( $curls as $i =&gt; $ch ) {
        $xml [$i] = curl_multi_getcontent ( $ch );
        curl_multi_remove_handle ( $mh, $ch );
    }

    // close multi handle
    curl_multi_close ( $mh );

    return $xml;
}
</code></pre>
","php, api, twitter, sentiment-analysis","<p>The problem is with giving curl too many URLs in one go. I am surprised you can manage 500 in parallel, as I've seen people complain of problems with even 200. <a href=""http://www.onlineaspect.com/2009/01/26/how-to-use-curl_multi-without-blocking/"" rel=""nofollow noreferrer"">This guy has some clever code</a> to just 100 at a time, but then add the next one each time one finishes, but I notice he edited it down to just do 5 at a time.</p>

<p>I just noticed the author of that code released an open source library around this idea, so I think this is the solution for you: <a href=""https://github.com/joshfraser/rolling-curl"" rel=""nofollow noreferrer"">https://github.com/joshfraser/rolling-curl</a></p>

<p>As to why you get a crash, a comment on this question suggests the cause  might be reaching the maximum number of OS file handles: <a href=""https://stackoverflow.com/q/13850951/841830"">What is the maximum number of cURL connections set by?</a>  and other suggestions are simply using a lot of bandwidth, CPU and memory.  (If you are on windows, opening the task manager should allow you to see if this is the case; on linux use <code>top</code>)</p>
",3,2,1506,2015-02-08 22:01:20,https://stackoverflow.com/questions/28399873/xampp-crashes-when-many-simultaneous-api-requests-are-made
flume not fetching Facebook data using SocialAgent,"<p>I am trying to retrieve facebook data using flume SocialAgent. I have successfully retrieve twitter data using TwitterAgent. </p>

<p>But in case of Facebook I got nothing in hdfs.</p>

<p><img src=""https://i.sstatic.net/gIHpX.png"" alt=""enter image description here""></p>

<p>My terminal stuck at this stage. When I terminate this process I got this</p>

<p><img src=""https://i.sstatic.net/la8It.png"" alt=""enter image description here""></p>

<p>And my HDFS facebook folder is empty.I am using following flume.conf</p>

<pre><code>SocialAgent.sources = FacebookHttpSource Twitter
SocialAgent.channels = FBmemoryChannel MemChannel
SocialAgent.sinks = fbHDFS HDFS

# For each one of the sources, the type is defined
SocialAgent.sources.FacebookHttpSource.type = org.apache.flume.source.http.HTTPSource
SocialAgent.sources.FacebookHttpSource.port = 51400
SocialAgent.sources.FacebookHttpSource.interceptors = Ts
SocialAgent.sources.FacebookHttpSource.interceptors.Ts.type = org.apache.flume.interceptor.TimestampInterceptor$Builder

# The channel can be defined as follows.
SocialAgent.sources.FacebookHttpSource.channels = FBmemoryChannel

# Each sink's type must be defined
#Specify the channel the sink should use
SocialAgent.sinks.fbHDFS.channel = FBmemoryChannel
SocialAgent.sinks.fbHDFS.type = hdfs
SocialAgent.sinks.fbHDFS.hdfs.path = hdfs://localhost:9000/user/flume/facebook/%Y/%m/%d/%H/
SocialAgent.sinks.fbHDFS.hdfs.fileType = DataStream
SocialAgent.sinks.fbHDFS.hdfs.writeFormat = Text
SocialAgent.sinks.fbHDFS.hdfs.batchSize = 1000
SocialAgent.sinks.fbHDFS.hdfs.rollSize = 0
SocialAgent.sinks.fbHDFS.hdfs.rollCount = 10000

# Each channel's type is defined.
SocialAgent.channels.FBmemoryChannel.type = memory

# Other config values specific to each type of channel(sink or source)
# can be defined as well
# In this case, it specifies the capacity of the memory channel
SocialAgent.channels.FBmemoryChannel.capacity = 10000
SocialAgent.channels.FBmemoryChannel.transactionCapacity = 100
</code></pre>

<p><strong>Is there any other flume Agent to fetch FaceBook Data.</strong></p>
","hadoop, bigdata, flume, sentiment-analysis","<p><em>org.apache.flume.source.http.HTTPSource</em> in your configuration doesn't send any request to Facebook to fetch data. It just listening a port (in your case 51400) and wait for any request 
(<a href=""https://flume.apache.org/FlumeUserGuide.html#http-source"" rel=""nofollow"">https://flume.apache.org/FlumeUserGuide.html#http-source</a>)</p>

<p>As I know there isn't any Flume FBClient yet. </p>
",1,2,1137,2015-02-10 15:11:31,https://stackoverflow.com/questions/28435113/flume-not-fetching-facebook-data-using-socialagent
Sentence-level to document-level sentiment analysis. Analysing news,"<p>I need to perform sentiment analysis on news articles about a specific topic using the Stanford NLP tool.</p>

<p>Such tool only allows sentence based sentiment analysis while I would like to extract a sentiment evaluation of the whole articles with respect to my topic.</p>

<p>For instance, if my topic is Apple, I would like to know the sentiment of a news article with respect to Apple.</p>

<p>Just computing the average of the sentences in my articles won't do. For instance, I might have an article saying something along the lines of ""Apple is very good at this, and this and that. While Google products are very bad for these reasons"". Such an article would result in a Neutral classification using the average score of sentences, while it is actually a Very positive article about Apple.</p>

<p>On the other hand filtering my sentences to include only the ones containing the word Apple would miss articles along the lines of ""Apple's product A is pretty good. However, it lacks the following crucial features: ..."". In this case the effect of the second sentence would be lost if I were to use only the sentences containing the word Apple.</p>

<p>Is there a standard way of addressing this kind of problems? Is Stanford NLP the wrong tool to accomplish my goal?</p>
","stanford-nlp, sentiment-analysis","<p>Update: You might want to look into 
<a href=""http://blog.getprismatic.com/deeper-content-analysis-with-aspects/"" rel=""nofollow"">http://blog.getprismatic.com/deeper-content-analysis-with-aspects/</a></p>

<p>This is a very active area of research so it would be hard to find an off-the-shelf tool to do this (at least nothing is built in the Stanford CoreNLP). Some pointers: look into aspect-based sentiment analysis. In this case, Apple would be an ""aspect"" (not really but can be modeled that way). Andrew McCallum's group at UMass, Bing Liu's group at UIC, Cornell's NLP group, among others, have worked on this problem.</p>

<p>If you want a quick fix, I would suggest to extract sentiment from sentences that have reference to Apple and its products; use coref (check out dcoref annotator in Stanford CoreNLP), which will increase the recall of sentences and solve the problem of sentences like ""However, it lacks.."".</p>
",4,2,852,2015-02-11 11:38:04,https://stackoverflow.com/questions/28453404/sentence-level-to-document-level-sentiment-analysis-analysing-news
UnicodeDecodeError on Python 2.7,"<p>Having some problems. I'm doing a TwitterSentimentAnalysis on a  dataset of length 1.6 million. Since my pc could not do the work (due to so many computations), the professor told me to use the university server.</p>

<p>I just realiazed that on the server, python version is 2.7 that it does not allow me to use the parameter <em>encoding</em> in csv reader for reading the file.</p>

<p>Anytime I got the <code>UnicodeDecodeError</code>, I have to manually remove the tweet from the dataset otherwise I can't do the rest. I have tried to go on all the question on the site but I resolved nothing.</p>

<p>I just want to skip the line who raises the error, since the set is big enough to allow me a good analysis.</p>

<pre><code>class UTF8Recoder:
    def __init__(self, f, encoding):
        self.reader = codecs.getreader(encoding)(f)
    def __iter__(self):
        return self
    def next(self):
        return self.reader.next().encode(""utf-8"", errors='ignore')

class UnicodeReader:
    def __init__(self, f, dialect=csv.excel, encoding=""utf-8"", **kwds):
        f = UTF8Recoder(f, encoding)
        self.reader = csv.reader(f, dialect=dialect, **kwds)
    def next(self):
        '''next() -&gt; unicode
        This function reads and returns the next line as a Unicode string.
        '''
        row = self.reader.next()
        return [unicode(s, ""utf-8"", errors='replace') for s in row]
    def __iter__(self):
        return self

def extraction(file, textCol, sentimentCol):
    ""The function reads the tweets""
    #fp = open(file, ""r"",encoding=""utf8"")
    fp = open(file, ""r"")
    tweetreader = UnicodeReader(fp)
    #tweetreader = csv.reader( fp, delimiter=',', quotechar='""', escapechar='\\' )
    tweets = []
    for row in tweetreader:
        # It takes the column in which the tweets and the sentiment are
        if row[sentimentCol]=='positive' or row[sentimentCol]=='4':
            tweets.append([remove_stopwords(row[textCol]), 'positive']);
        else:
            if row[sentimentCol]=='negative' or row[sentimentCol]=='0':
                tweets.append([remove_stopwords(row[textCol]), 'negative']);
            else:
               if row[sentimentCol]=='irrilevant' or row[sentimentCol]=='2' or row[sentimentCol]=='neutral':
                   tweets.append([remove_stopwords(row[textCol]), 'neutral']);

    tweets = filterWords(tweets)
    fp.close()
    return tweets;
</code></pre>

<p>Error:</p>

<pre><code>Traceback (most recent call last):
  File ""sentimentAnalysis_v4.py"", line 165, in &lt;module&gt;
    newTweets = extraction(""sentiment2.csv"",5,0)
  File ""sentimentAnalysis_v4.py"", line 47, in extraction
    for row in tweetreader:
  File ""sentimentAnalysis_v4.py"", line 29, in next
    row = self.reader.next()
  File ""sentimentAnalysis_v4.py"", line 19, in next
    return self.reader.next().encode(""utf-8"", errors='ignore')
  File ""/usr/lib/python2.7/codecs.py"", line 615, in next
    line = self.readline()
  File ""/usr/lib/python2.7/codecs.py"", line 530, in readline
    data = self.read(readsize, firstline=True)
  File ""/usr/lib/python2.7/codecs.py"", line 477, in read
    newchars, decodedbytes = self.decode(data, self.errors)
UnicodeDecodeError: 'utf8' codec can't decode byte 0xd9 in position 48: invalid continuation byte
</code></pre>
","python, python-2.7, unicode, sentiment-analysis","<p>If you have input data that is malformed, I'd not use <code>codecs</code> here to do the reading.</p>

<p>Use the newer <a href=""https://docs.python.org/2/library/io.html#io.open"" rel=""nofollow""><code>io.open()</code> function</a> and specify a error handling strategy; <code>'replace'</code> should do:</p>

<pre><code>class ForgivingUTF8Recoder:
    def __init__(self, filename, encoding):
        self.reader = io.open(f, newline='', encoding=encoding, errors='replace')
    def __iter__(self):
        return self
    def next(self):
        return self.reader.next().encode(""utf-8"", errors='ignore')
</code></pre>

<p>I set the <code>newline</code> handling to <code>''</code> to make sure the CSV module gets to handle newlines in values correctly.</p>

<p>Instead of passing in an open file, just pass in the filename:</p>

<pre><code>tweetreader = UnicodeReader(file)
</code></pre>

<p>This won't let you <em>skip</em> faulty lines, it instead will handle faulty lines by replacing characters that cannot be decoded with the <a href=""http://codepoints.net/U+FFFD"" rel=""nofollow"">U+FFFD REPLACEMENT CHARACTER</a>; you can still look for those in your columns if you want to skip the whole row.</p>
",1,0,1084,2015-02-13 07:54:57,https://stackoverflow.com/questions/28494869/unicodedecodeerror-on-python-2-7
Features Vectors to build classifier to detect subjectivity,"<p>I am trying to build a classifier to detect subjectivity. I have text files tagged with subjective and objective . I am little lost with the concept of features creation from this data. I have found the lexicon of the subjective and objective tag. One thing that I can do is to create a feature of having words present in respective dictionary. Maybe the count of words present in subjective and objective dictionary. After that I intend to use naive bayes or SVM to develop the model</p>

<p>My problem is as follow</p>

<ol>
<li>Is my approach correct ?</li>
<li>Can I create more features ? If possible suggest some or point me to some paper or link</li>
<li>Can I do some test like chi -sq etc to identify effective words from the dictionary ?</li>
</ol>
","nlp, text-mining, sentiment-analysis","<p>You are basically on the right track. I would try and apply classifier with features you already have and see how well it will work, before doing anything else.  </p>

<p>Actually best way to improve your work is to google for subjectivity classification papers and read them (there are a quite a <a href=""https://scholar.google.ru/scholar?as_ylo=2011&amp;q=subjectivity%20classification&amp;hl=en&amp;as_sdt=0,5"" rel=""nofollow"">number of them</a>). For example <a href=""http://arxiv.org/ftp/arxiv/papers/1312/1312.6962.pdf"" rel=""nofollow"">this one</a> lists typical features for this task.</p>

<p>And yes Chi-squared can be used to construct dictionaries for text classification (other commonly used methods are TD*IDF, pointwise mutal information and LDA)</p>

<p>Also, recently new neural network-based methods for text classification such as <a href=""http://arxiv.org/pdf/1405.4053v2.pdf"" rel=""nofollow"">paragraph vector</a> and <a href=""http://arxiv.org/pdf/1406.3830v1.pdf"" rel=""nofollow"">dynamic convolutional neural networks with k-max pooling</a> demonstrated state-of-the-art results on sentiment analysis, thus they should probably be good for subjectivity classification as well.</p>
",2,0,144,2015-02-16 05:36:56,https://stackoverflow.com/questions/28535136/features-vectors-to-build-classifier-to-detect-subjectivity
Instruction for training model in Stanford Core NLP,"<p>I am a novice in the area of sentiment analysis, and I am very interested to learn about training models, could you please explain each of the instructions contained in the following command?</p>

<p>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz</p>

<p>what is the function of:
-numHid 25 
-trainPath train.txt 
-devPath dev.txt 
-train 
-model model.ser.gz</p>

<p>Please could you help me?</p>
","stanford-nlp, sentiment-analysis, training-data","<p>The more complicated options are described in comments in these classes: <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/sentiment/RNNOptions.java"" rel=""nofollow""><code>RNNOptions</code></a>, <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/sentiment/RNNTrainOptions.java"" rel=""nofollow""><code>RNNTrainOptions</code></a>.</p>

<p>The remainder of the options you listed are paths for reading / writing during training.</p>

<ul>
<li>The <code>-trainPath</code> argument points to a labeled sentiment treebank. The trees in this data will be used to train the model parameters (also to seed the model vocabulary).</li>
<li>The <code>-devPath</code> argument points to a second labeled sentiment treebank. The trees in this data will be used to periodically evaluate the performance of the model. We won't train on this data; it will only be used to test how well the model generalizes to unseen data.</li>
<li>The <code>-model</code> argument specifies where to save the learned sentiment model.</li>
</ul>
",2,0,1301,2015-03-13 14:18:14,https://stackoverflow.com/questions/29034600/instruction-for-training-model-in-stanford-core-nlp
Sentiment analysis of non-English texts,"<p>I want to analyze sentiment of texts that are written in German. I found a lot of tutorials on how to do this with English, but I found none on how to apply it to different languages.</p>

<p>I have an idea to use the <code>TextBlob</code> Python library to first translate the sentences into English and then to do sentiment analysis, but I am not sure whether or not it is the best way to solve this task.</p>

<p>Or are there any other possible ways to solve this task?</p>
","python, machine-learning, nlp, sentiment-analysis, textblob","<p>As Andy has pointed about above, the best approach would be to train your own classifier. Another, more quick and dirty approach would be to use a German sentiment lexicon such as the <a href=""http://wortschatz.uni-leipzig.de/de/download#sentiWSDownload"" rel=""nofollow noreferrer"">SentiWS</a>, and compute the polarity of a sentence simply on the basis of the polarity values of its individual words (for example by summing them). This method isn't foolproof (it doesn't take negation into account, for example), but it would give reasonable results relatively quickly.</p>
",2,7,9194,2015-03-20 15:07:07,https://stackoverflow.com/questions/29169732/sentiment-analysis-of-non-english-texts
How to get alignment between sentiment module and constituency parser in CoreNLP,"<p>I'm using Stanford coreNLP to both parse a text and get sentiment information. The two models give two <code>Tree</code> objects, but they are not related. Is there an easy way to navigate the two elements at the same time, so having an alignment between the two at a token level?</p>
","stanford-nlp, sentiment-analysis","<p>You can just use the sentiment tree as a model of both the grammatical parse and the sentiment — it's simply the original parse tree with extra annotations.</p>

<p><em>Explanation:</em> If you're using the Stanford CoreNLP pipeline, the sentiment annotator draws directly from the parse annotator to build its tree. The tree provided by the sentiment annotator is then just the same binarized parse tree with extra sentiment annotations.</p>
",1,0,88,2015-04-08 13:31:59,https://stackoverflow.com/questions/29516115/how-to-get-alignment-between-sentiment-module-and-constituency-parser-in-corenlp
Deleting Extended ASCII characters from a .txt file Linux Terminal,"<p>I'm trying to generate a list of word frequency from a .txt file, I do not want certain ASCII printable characters and all the Extended ASCII characters to contribute to the word frequency list. Here is my generalized code:</p>

<pre><code>cat file.txt | tr -d '[:punct:]' | tr -d '[:digit:]' | tr -d '\33-\64\91-\96\123-\255' | tr ' ' '\n' | tr 'A-Z' 'a-z' | sort | uniq -c | sort -rn &gt; Freq.list
</code></pre>

<p>Also, I had originally tried the segment: <code>tr -d '[:special:]'</code>    but received the error: tr: invalid character class <code>special</code></p>

<p>A key part of the code I want is to also make sure that symbols next to each other are deleted, such as: <code>«•</code> </p>

<p>Lastly, is there a way to delete single quotations attached to a word? Such that ""word or 'word can contribute to word. I've tried <code>tr -d ""\""""</code> and <code>tr -d '\33-\64'</code> for that but don't seem to work.</p>

<p>Here is an example of the file.txt:<br>
Â£, is the specific heat per unit volume, «•„ and cr,,
are respectively the thermal and electrical conductivity of the normal region""</p>

<p>Which I want output as:<br>
3 the<br>
2 and<br>
1 volume<br>
1 unit<br>
1 thermal<br>
1 specific<br>
1 respectively<br>
1 region<br>
1 per<br>
1 of<br>
1 normal<br>
1 is<br>
1 heat<br>
1 electrical<br>
1 conductivity<br>
1 are  </p>
","linux, ascii, cat, tr, sentiment-analysis","<p>Given this file:</p>

<pre><code>$ cat file
My hovercraft is full of eels
Min luftpudebåd er fyldt med ål
Mon aéroglisseur est plein d'anguilles
โฮเวอร์คราฟท์ของผมเต็มไปด้วยปลาไหล
Iyéčhiŋkiŋyaŋka čha kiŋyáŋ mitȟáwa kiŋ hoká ožúla!
</code></pre>

<p>You can remove all the non-ascii with <code>iconv -ct ascii</code>:</p>

<pre><code>$ iconv -ct ascii &lt; file 
My hovercraft is full of eels
Min luftpudebd er fyldt med l
Mon aroglisseur est plein d'anguilles

Iyhikiyaka ha kiy mitwa ki hok ola!
</code></pre>

<p>Or transliterate them into unaccented ascii if any with <code>iconv -t ascii//translit</code>:</p>

<pre><code>$ iconv -t ascii//translit &lt; file
My hovercraft is full of eels
Min luftpudebad er fyldt med al
Mon aeroglisseur est plein d'anguilles
??????????????????????????????????
Iyechi?ki?ya?ka cha ki?ya? mithawa ki? hoka ozula!
</code></pre>
",0,0,216,2015-04-08 21:30:47,https://stackoverflow.com/questions/29525725/deleting-extended-ascii-characters-from-a-txt-file-linux-terminal
stanford-nlp How to predict document level,"<p>I am using stanford-corenlp-3.4.1 version. I have the question if we give the sentence has multiple sentence, how do I calculate the prediction.</p>

<p>for eg :
String text = ""IT was very fantastic experience. it was a pathetice experience"";</p>

<p>i am getting the prediction of </p>

<p>IT was very fantastic experience : positive.</p>

<p>it was a pathetice experience : negative.</p>

<p>i  am getting the prediction based on each sentence level.how do i get it document level.</p>

<p>based on reading the total text i need to get it either positive or negative.</p>

<p>Here is the sample code:</p>

<pre><code>Properties props = new Properties();

props.setProperty(""annotators"",""tokenize, ssplit, pos, lemma, parse, sentiment"");

        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        Annotation annotation = new Annotation(""IT was very fantastic experience. it was a pathetice experience"");
        pipeline.annotate(annotation);
        List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
        for (CoreMap sentence : sentences) {
          String sentiment = sentence.get(SentimentCoreAnnotations.ClassName.class);
          System.out.println(sentiment + ""\t"" + sentence);

    }
</code></pre>

<p>Results:</p>

<p>Very positive:  IT was very fantastic experience.
Negative    it was a pathetice experience</p>

<p>thanks</p>
","stanford-nlp, sentiment-analysis","<p>To my knowledge, Stanford NLP does not provide sentiment analysis above the sentence level. One solution would be to compute some sort of mean sentiment value across all sentences in your text, but obviously that's only going to give you a rough idea of the overall sentiment. </p>
",2,0,1072,2015-04-10 00:25:52,https://stackoverflow.com/questions/29551332/stanford-nlp-how-to-predict-document-level
Train model using Named entity,"<p>I am looking on standford corenlp using the Named Entity REcognizer.I have different kinds of input text and i need to tag it into my own Entity.So i  started training my own model and it doesnt seems to be working.</p>

<p>For eg: my input text string is ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio <a href=""http://t.co/EqxmY1VmLg"">http://t.co/EqxmY1VmLg</a> <a href=""http://t.co/F0Vefuoj9Q"">http://t.co/F0Vefuoj9Q</a>""</p>

<p>I go through the examples to train my own models and and look for only some words that I am interested in.</p>

<p>My jane-austen-emma-ch1.tsv looks like this</p>

<pre><code>Toyota  PERS
Land Cruiser    PERS
</code></pre>

<p>From the above input text i am only interested in those two words. The one is 
Toyota and the other word is Land Cruiser.</p>

<p>The austin.prop look like this</p>

<pre><code>trainFile = jane-austen-emma-ch1.tsv
serializeTo = ner-model.ser.gz
map = word=0,answer=1
useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
useDisjunctive=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Run the following command to generate the ner-model.ser.gz file </p>

<p>java -cp stanford-corenlp-3.4.1.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop</p>

<pre><code>public static void main(String[] args) {
        String serializedClassifier = ""edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz"";
        String serializedClassifier2 = ""C:/standford-ner/ner-model.ser.gz"";
        try {
            NERClassifierCombiner classifier = new NERClassifierCombiner(false, false, 
                    serializedClassifier2,serializedClassifier);
            String ss = ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio http://t.co/EqxmY1VmLg http://t.co/F0Vefuoj9Q"";
            System.out.println(""---"");
            List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(ss);
            for (List&lt;CoreLabel&gt; sentence : out) {
              for (CoreLabel word : sentence) {
                System.out.print(word.word() + '/' + word.get(AnswerAnnotation.class) + ' ');
              }
              System.out.println();
            }

        } catch (ClassCastException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }  catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }

    }
</code></pre>

<p>Here is the output I am getting</p>

<pre><code>Book/PERS of/PERS 49/O Magazine/PERS Articles/PERS on/O Toyota/PERS Land/PERS Cruiser/PERS 1956-1987/PERS Gold/O Portfolio/PERS http://t.co/EqxmY1VmLg/PERS http://t.co/F0Vefuoj9Q/PERS
</code></pre>

<p>which i think its wrong.I am looking for Toyota/PERS and Land Cruiser/PERS(Which is a multi valued fied.</p>

<p>Thanks for the Help.Any help is really appreciated.</p>
","nlp, stanford-nlp, sentiment-analysis, named-entity-recognition, pos-tagger","<p>The NERClassifier* is word level, that is, it labels words, not phrases. Given that, the classifier seems to be performing fine. If you want, you can hyphenate words that form phrases. So in your labeled examples and in your test examples, you would make ""Land Cruiser"" to ""Land_Cruiser"".</p>
",1,2,2456,2015-04-20 18:43:00,https://stackoverflow.com/questions/29755910/train-model-using-named-entity
R functions using SentiWordNet,"<p>I am doing sentiment analysis and text mining on an e-mail box dedicated for client feedback.  I use R for this work.  I based the sentiment analysis on the work done by Jeffrey Breen.  It works fine but I want to take it a step further.  During my research I came across SentiWordnet. </p>

<p>I searched for R-code/functions to be able to apply SentiWordNet but only came up with Java and Python resources.  I am not at all familiar with either of these.  </p>

<p>Is there someone who applied SentiWordNet in R?  I will appreciate any assistance.</p>
","r, sentiment-analysis, tm, senti-wordnet","<p>I can't find any example of SentiWordNet accessed solely from R, however you could call the <a href=""http://www.nltk.org/howto/sentiwordnet.html"" rel=""nofollow noreferrer"">Python package</a> from R using <a href=""http://rpython.r-forge.r-project.org/"" rel=""nofollow noreferrer"">rPython</a>, or the <a href=""http://sentiwordnet.isti.cnr.it/code/SWN3.java"" rel=""nofollow noreferrer"">Java implementation</a> using <a href=""http://cran.r-project.org/web/packages/rJava/index.html"" rel=""nofollow noreferrer"">rJava</a> (some notes provided <a href=""https://stackoverflow.com/questions/15653091/how-to-use-sentiwordnet"">here</a>). Unfortunately the python implementation is not currently available for Windows.</p>
",0,2,2881,2015-04-22 09:48:37,https://stackoverflow.com/questions/29793730/r-functions-using-sentiwordnet
Interpretation of output of Vowpal Wabbit,"<p>I am using Vowpal Wabbit for binary sentiment classification (positive and negative) using basic unigram features.
This is what my train features look like: </p>

<pre><code>1 | 28060 10778 21397 64464 19255
-1 | 44248 21397 3353 57948 4340 7594 3843 44368 33938 49249 45696     32698 57948 21949 58810 32698 62793 64464
1 | 44248 21397 3353 32698 62996
1 | 44248 21397 3353 57948 63747 40024 46815 37197 7594 47339 28060 10778 32698 45035 3843 54789 19806 60087 7594 47339
</code></pre>

<p>Each line starts with the label, followed by a series of indices of words in the vocabulary. These features take a default value of 1. </p>

<p>I use this command to train:</p>

<pre><code>cat trainfeatures.txt | vw --loss_function logistic -f trainedModel
</code></pre>

<p>This is the command I use for testing:</p>

<pre><code>cat testfeatures.txt | vw  -i trainedModel -p test.pred
</code></pre>

<p>This is what the output file test.pred looks like:</p>

<pre><code>28.641335
15.409834
13.057793
28.488165
16.716839
19.752426
</code></pre>

<p>The values range between -0.114076 and 28.641335. If I use a rule that if the value is more than a threshold, say, 14, then it is positive and otherwise it is negative, then I get an accuracy of 51% and f-measure of 40.7%. </p>

<p>But the paper I am following reports an accuracy of 81% on this dataset. So there is definitely something wrong I am doing in my implementation or my interpretation of results. I am unable to figure out what that is.</p>

<p>EDIT: I used the --binary option in the test command and that gave me labels {-1,+1}. I evaluated it and got the following results - accuracy of 51.25% and f-measure of 34.88%. </p>
","machine-learning, logistic-regression, sentiment-analysis, vowpalwabbit","<p>EDIT: The main problem was that the training data was not shuffled in random order. This is needed when using any online learning (unless the training data is already shuffled or if it is a real time series). It can be done using Unix command <a href=""http://www.unix.com/man-page/linux/1/shuf/"" rel=""nofollow""><code>shuf</code></a>.</p>

<p>Explanation: In an extreme case, if the training data contains first all negative examples followed by all positive examples, then it is quite probable that the model will learn to classify (almost) everything as positive.</p>

<p>Another common reason that might result in low F1-measure (and almost all predictions positive) is imbalanced data (many positive examples, few negative examples). This was not the case of the dataset in Satarupa Guha's question, but I keep my original answer here:</p>

<p>The obvious solution is to give a higher (than the default 1) importance weight to the negative examples. The optimal value of the importance weight can be found using a heldout set.</p>

<blockquote>
  <p>If I use a rule that if the value is more than a threshold, say, 14, then it is positive and otherwise it is negative</p>
</blockquote>

<p>The threshold for negative vs. positive prediction should be 0.</p>

<p>Note that one of the great advantages of Vowpal Wabbit is that you do not need to convert feature names (words in your case) to integers. You can use the raw (tokenized) text, just make sure to escape pipe ""|"" and colon "":"" (and space and newline). Of course, if you already converted the words to integers, you can use it.</p>
",1,0,727,2015-04-23 14:30:55,https://stackoverflow.com/questions/29826536/interpretation-of-output-of-vowpal-wabbit
where I can find the file sentimentTreesDebug.txt,"<p>I am trying to analyze codes sentiment model anaysis of stanford corenlp, I have found on this page:</p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/sentiment/SentimentTraining.java"" rel=""nofollow"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/sentiment/SentimentTraining.java</a></p>

<p>but I can not find the ""sentimentTreesDebug.txt"" file with which to make the experiment, someone could tell me where that file or how it should look?</p>

<p>TrainPath String = ""sentimentTreesDebug.txt"";</p>

<p>I would appreciate very much the help or advice you could give me</p>

<p>this is my problem</p>

<p>xception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: MemoryTreebank.processFile IOException in file sentimentTreesDebug.txt
    at edu.stanford.nlp.trees.MemoryTreebank.processFile(MemoryTreebank.java:300)
    at edu.stanford.nlp.util.FilePathProcessor.processPath(FilePathProcessor.java:84)
    at edu.stanford.nlp.trees.MemoryTreebank.loadPath(MemoryTreebank.java:152)
    at edu.stanford.nlp.trees.Treebank.loadPath(Treebank.java:193)
    at edu.stanford.nlp.sentiment.SentimentUtils.readTreesWithLabels(SentimentUtils.java:67)
    at edu.stanford.nlp.sentiment.SentimentUtils.readTreesWithGoldLabels(SentimentUtils.java:50)
    at edu.stanford.nlp.sentiment.SentimentTraining.main(SentimentTraining.java:170)
Caused by: java.io.FileNotFoundException: sentimentTreesDebug.txt (No such file or directory)
    at java.io.FileInputStream.open(Native Method)
    at java.io.FileInputStream.(FileInputStream.java:138)
    at edu.stanford.nlp.trees.MemoryTreebank.processFile(MemoryTreebank.java:212)
    ... 6 more
Java Result: 1</p>
","stanford-nlp, sentiment-analysis","<p>You can download them here:</p>

<p><a href=""http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip</a></p>

<p>A simple dev.txt would look like:</p>

<pre><code>(4 (2 (2 Everything) (2 is)) (4 awesome))
</code></pre>

<p>HTH</p>
",0,1,131,2015-04-23 16:29:47,https://stackoverflow.com/questions/29829380/where-i-can-find-the-file-sentimenttreesdebug-txt
How to integrate the GATE Twitter PoS model with Stanford NLP?,"<p>I'm currently using Stanford NLP library for sentiment analysis of a twitter stream (version 3.3.0 but that’s not set.)</p>

<p>I was looking for ways to increase the accuracy when I came across this 
<a href=""https://gate.ac.uk/wiki/twitter-postagger.html"" rel=""nofollow"">https://gate.ac.uk/wiki/twitter-postagger.html</a></p>

<p>I'm relatively new to sentiment analysis but am I right in saying that if I choose this model instead of the default model (which is based off film reviews) I would get an increased accuracy rating ?</p>

<p>If so, how does one go about integrating it with the Stanford NPL library ?</p>

<p>If I am missing any required information here please tell me ! </p>

<p>Regards.</p>
","twitter, machine-learning, stanford-nlp, sentiment-analysis","<p>You can use GATE twitter pos model with stanford package</p>

<pre><code>./corenlp.sh -file tweets.txt -pos.model gate-EN-twitter.model -ssplit.newlineIsSentenceBreak always
</code></pre>

<p>use <code>v3.3.1</code> for GATE</p>
",1,1,394,2015-04-23 22:01:09,https://stackoverflow.com/questions/29835122/how-to-integrate-the-gate-twitter-pos-model-with-stanford-nlp
Sentiment analysis training set,"<p>I am using NLTK python to do sentiment analysis and my data has about 200,000 reviews. To use Naive Bayes Classifier, I need to have training set that is labeled. Since my data is not labeled, I manually created about 100 reviews as positive and negative. But I don't think this is the way to do it. I heard that I need to have 20% of data as a training set to train classifier and apply it to the rest 80% of data. </p>

<p>Is there any better way to generate training set for Naive Bayes classifier? Thank you for your help, and please let me know if the questions is not clear to understand. </p>
","data-mining, sentiment-analysis","<p>We have had great success using only about 100-200 training samples (depending on the specific classification) to classify hundreds of thousands of paragraphs with a fairly high degree of accuracy.</p>

<p>We did hand-filter the randomly selected samples to ensure they are not very similar to each other (and therefore represent different ways to express a concept).  We used RapidMiner for classification rather than NLTK, but I expect the algorithms are fairly similar.</p>

<p>Run your classifier with your 100 reviews, then run against a set of 100 random reviews not in the training set.  Check the accuracy, and add more reviews to the training set if the accuracy is not where you want it to be.</p>
",1,2,716,2015-04-24 23:07:51,https://stackoverflow.com/questions/29858967/sentiment-analysis-training-set
Sentiment Analysis in Spanish with Stanford coreNLP,"<p>I'm new here and wanted to know if anyone can help me with the following question.</p>

<p>I'm doing sentiment analysis of text in Spanish and using Stanford CoreNLP but I can not get a positive result.</p>

<p>That is, if I analyze any English text analyzes it perfect to put it in Spanish but the result is always negative</p>

<p>I've been looking how to configure the parser in Spanish, tokenize and everything I found was useless for sentiment analysis.</p>

<p>Someone can tell me if the only thing that works is the tokenize and sentiment does not in Spanish?</p>

<p>This is my properties file so that I managed to find:</p>

<p>annotators = tokenize, ssplit, pos, ner, parse, sentiment</p>

<p>tokenize.language = en</p>

<p>pos.model = edu / stanford / nlp / models / pos-tagger / english / spanish-distsim.tagger</p>

<p>ner.model = edu / stanford / nlp / models / ner / spanish.ancora.distsim.s512.crf.ser.gz
ner.applyNumericClassifiers = false
ner.useSUTime = false</p>

<p>parse.model = edu / stanford / nlp / models / lexparser / spanishPCFG.ser.gz</p>

<p>The code to perform sentiment analysis is typical that you can find in any tutorial</p>

<p>Thank you very much!!</p>
","stanford-nlp, sentiment-analysis","<p>Unfortunately there is no Stanford sentiment model available for Spanish. At the moment all the Spanish words are likely being treated as generic ""unknown words"" by the sentiment analysis algorithm, which is why you're seeing consistently bad performance.</p>

<p>You can certainly train your own model (documented elsewhere on the Internet, I believe..), but you'll need to have Spanish training data to accomplish this.</p>
",0,1,1256,2015-04-27 10:11:18,https://stackoverflow.com/questions/29892848/sentiment-analysis-in-spanish-with-stanford-corenlp
sentiment package installation from local zip file issue,"<p>I am trying to find a way to install sentiment package in R for performing sentiment analysis.
I searched in all d repositories but it isn't available.I am trying to manually install sentiment_0.2.tar file from local directory but i get this: </p>

<blockquote>
  <p>Error in read.dcf(file.path(pkgname, ""DESCRIPTION""), c(""Package"", ""Type"")) : 
    cannot open the connection
  In addition: Warning messages:
  1: In unzip(zipname, exdir = dest) : error 1 in extracting from zip file
  2: In read.dcf(file.path(pkgname, ""DESCRIPTION""), c(""Package"", ""Type"")) :
    cannot open compressed file 'sentiment_0.2.tar.gz/DESCRIPTION', probable reason 'No such file or directory'</p>
</blockquote>

<p>How do i resolve this problem? any valuable suggestions will be of great help.</p>
","r, sentiment-analysis","<p>Please try:</p>

<pre><code>install.packages(file.choose(), repos = NULL, type=""source"")
</code></pre>

<p>This should allow you to select the file itself and install the package subsequently. Also, please note that the package version may not be compatible with the current R version on your machine.</p>

<p>Thanks!</p>
",0,0,687,2015-05-05 18:05:51,https://stackoverflow.com/questions/30060157/sentiment-package-installation-from-local-zip-file-issue
How to iterate through the synset list generated from wordnet using python 3.4.2,"<p>I am using wordnet to find the synonyms for a particular word as shown below</p>

<pre><code>synonyms = wn.synsets('good','a')
</code></pre>

<p>where wn is wordnet. This returns a list of synsets like </p>

<pre><code>Synset('good.a.01')    
Synset('full.s.06')    
Synset('good.a.03')    
Synset('estimable.s.02')    
Synset('beneficial.s.01')
</code></pre>

<p>etc...</p>

<p>How to iterate through each synset and get the name and the pos tag of each synset?</p>
","python-3.x, nlp, wordnet, sentiment-analysis","<p>You can get the name and the pos tag of each synset like this:</p>

<pre><code>from nltk.corpus import wordnet as wn

synonyms = wn.synsets('good','a')

for synset in synonyms:
    print(synset.name())
    print(synset.pos())
</code></pre>

<p>The name is the combination of word, pos and sense, such as 'full.s.06'. If you just want the word, you can split on the dot '.' and take the first element: </p>

<pre><code>print(synset.name().split('.')[0]
</code></pre>
",2,1,1871,2015-05-12 05:17:49,https://stackoverflow.com/questions/30182344/how-to-iterate-through-the-synset-list-generated-from-wordnet-using-python-3-4-2
NLP- Sentiment Processing for Junk Data takes time,"<p>I am trying to find the Sentiment for the input text. This test is a junk sentence and when I tried to find the Sentiment the Annotation to parse the sentence is taking around 30 seconds. For normal text it takes less than a second. If i need to process around millions of data it will add up the time to process. Any solution to this.</p>

<pre><code>String text = ""Nm n n 4 n n bkj nun4hmnun Onn njnb hm5bn nm55m nbbh n mnrrnut but n rym4n nbn 4nn65 m nun m n nn nun 4nm 5 gm n my b bb b b rtmrt55tmmm5tttn b b bb g bn nn n h r ret n nun bn d. B bbbbbbbbbbr bung NHnhn nn nk, v v v n gain t g 4gnyhimmigration ndn nb NVnb bin uny 7 nbbbbbnn vv bbvb ninn njnj n4 nm n km n n n cb j bun. Nhfnt bn nn. N hm nn nun m bum my b mmmnbjk nn n by nn nun nun n nun nn bn n nhn n nn n n m NH nb4mnm mkn 4 n n n n hm r b rnfngg4d in b nut mmmkmmm5 bbjn n n ij BBM 8u8i by nun n.nn hm n. n4n By 4n4n bunny RN bny hm j mi. Nymmn FBT not mn n n nm g by n n nnm? Rnyb vCard n5 Yu nn n n n n nt .nm mn nt n nb n n n n by y5nnnhyyh h b b nt njj n m f4n re"";
Properties props = new Properties();
            props.setProperty(""annotators"",""tokenize, ssplit, pos,parse,sentiment"");
            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
            Annotation annotation = pipeline.process(text);
</code></pre>

<p>For the NLP sentiment calulation I have to parse the data and i think thats the reason its taking time.</p>

<p>thanks</p>
","nlp, stanford-nlp, sentiment-analysis, pos-tagger","<p>Yes, the standard PCFG parser (the one that is run by default without any other options specified) will choke on this sort of long nonsense data. You might have better luck using the <a href=""http://nlp.stanford.edu/software/srparser.shtml"" rel=""nofollow"">shift-reduce constituency parser</a>, which is substantially faster than the PCFG and nearly as accurate.</p>
",3,1,262,2015-05-23 14:31:29,https://stackoverflow.com/questions/30413885/nlp-sentiment-processing-for-junk-data-takes-time
NLP Shift reduce parser is throwing null pointer Exception for Sentiment calculation,"<p>I am trying to analyze sentiment using nlp.  The version of stanford-nlp I am using is 3.4.1.  I have some junk data to process and it looks like it takes around 45 seconds to process using default PCFG file. </p>

<p>Here is the example:</p>

<pre><code>String text = ""Nm n n 4 n n bkj nun4hmnun Onn njnb hm5bn nm55m nbbh n mnrrnut but n rym4n nbn 4nn65 m nun m n nn nun 4nm 5 gm n my b bb b b rtmrt55tmmm5tttn b b bb g bn nn n h r ret n nun bn d. B bbbbbbbbbbr bung NHnhn nn nk, v v v n gain t g 4gnyhimmigration ndn nb NVnb bin uny 7 nbbbbbnn vv bbvb ninn njnj n4 nm n km n n n cb j bun. Nhfnt bn nn. N hm nn nun m bum my b mmmnbjk nn n by nn nun nun n nun nn bn n nhn n nn n n m NH nb4mnm mkn 4 n n n n hm r b rnfngg4d in b nut mmmkmmm5 bbjn n n ij BBM 8u8i by nun n.nn hm n. n4n By 4n4n bunny RN bny hm j mi. Nymmn FBT not mn n n nm g by n n nnm? Rnyb vCard n5 Yu nn n n n n nt .nm mn nt n nb n n n n by y5nnnhyyh h b b nt njj n m f4n re"";
Properties props = new Properties();

props.setProperty(""annotators"",""tokenize, ssplit, pos,parse,sentiment"");

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation annotation = pipeline.process(text);
</code></pre>

<p>Based on the suggestion here, I tried again with the shift-reduce parser.</p>

<pre><code>Properties props = new Properties();
                    props.setProperty(""annotators"",""tokenize, ssplit, pos,parse,sentiment"");
props.put(""parse.model"", ""com/example/nlp/englishSR.ser.gz"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation annotation = pipeline.process(text);
</code></pre>

<p>I have to download the shift-reduce model and put it in the classpath.  The model class is getting loaded but it's throwing a null pointer exception. Any thoughts and suggestions?</p>
","nlp, stanford-nlp, sentiment-analysis, shift-reduce","<p>Is there a specific reason why you are using version 3.4.1 and not the latest version? </p>

<p>If I run your code with the latest version, it works for me (after I change the path to the SR model to <code>edu/stanford/nlp/models/srparser/englishSR.ser.gz</code> but I assume you changed that path on purpose). </p>

<p>Also make sure that the models you downloaded are compatible with your version of CoreNLP. If you downloaded the latest models and try to use them with an older version of CoreNLP, then you will most likely run into problems.</p>
",2,1,467,2015-05-24 07:15:38,https://stackoverflow.com/questions/30421008/nlp-shift-reduce-parser-is-throwing-null-pointer-exception-for-sentiment-calcula
How to iterate over many websites and parse text using web crawler,"<p>I am trying to parse text and run an sentiment analysis over the text from multiple websites. I have successfully been able to strip just one website at a time and generate a sentiment score using the TextBlob library, but I am trying to replicate this over many websites, any thoughts on where to start? </p>

<p>Here is the code: </p>

<pre><code>import requests
import json
import urllib
from bs4 import BeautifulSoup
from textblob import TextBlob


url = ""http://www.reddit.com/r/television/comments/38dqxf/josh_duggar_confessed_to_his_father_jim_bob/""
html = urllib.urlopen(url).read()
soup = BeautifulSoup(html)

# kill all script and style elements
for script in soup([""script"", ""style""]):
    script.extract()    # rip it out

# get text
text = soup.get_text()

# break into lines and remove leading and trailing space on each
lines = (line.strip() for line in text.splitlines())
# break multi-headlines into a line each
chunks = (phrase.strip() for line in lines for phrase in line.split(""  ""))
# drop blank lines
text = '\n'.join(chunk for chunk in chunks if chunk)

#print(text)

wiki = TextBlob(text)
r = wiki.sentiment.polarity

print r
</code></pre>

<p>Thank you in advance</p>
","python, web-crawler, sentiment-analysis","<p>This is how you get the data from a website via URL in python:</p>

<pre><code>import urllib2
response = urllib2.urlopen('http://reddit.com/')
html = response.read()
</code></pre>

<p><code>html</code> is a string containing all of the HTML from the URL.</p>

<p>I'm not entirely sure what you want to get from each page. If you comment below, I could edit this answer and help you further.</p>

<p>Edit:</p>

<p>If you want to iterate through a list of URLs, you could create a function and go about it like this:</p>

<pre><code>#you can add to this
urls = [""http://www.google.com"", ""http://www.reddit.com""]


def parse_websites(list_of_urls):
    for url in list_of_urls:
        html = urllib.urlopen(url).read()
        soup = BeautifulSoup(html)
        # kill all script and style elements

        for script in soup([""script"", ""style""]):
            script.extract()    # rip it out

        # get text
        text = soup.get_text()

        # break into lines and remove leading and trailing space on each
        lines = (line.strip() for line in text.splitlines())
        # break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split(""  ""))
        # drop blank lines
        text = '\n'.join(chunk for chunk in chunks if chunk)

        #print(text)

        wiki = TextBlob(text)
        r = wiki.sentiment.polarity

        print r

parse_websites(urls)
</code></pre>
",1,0,1465,2015-06-06 01:33:53,https://stackoverflow.com/questions/30678068/how-to-iterate-over-many-websites-and-parse-text-using-web-crawler
mashape sentiment and R integration,"<p>Here is one mashpe sentiment analysis curl code returning json. How to integrate it with R?</p>

<p>curl -X POST --include '<a href=""https://community-sentiment.p.mashape.com/text/"" rel=""nofollow"">https://community-sentiment.p.mashape.com/text/</a>' \
      -H 'X-Mashape-Key: pVke3AAqHzmsh4xNdsKrPshYHQC1p1H78y0jsn2uwaEPcU1TnF' \
      -H 'Content-Type: application/x-www-form-urlencoded' \
      -H 'Accept: application/json' \
      -d 'txt=Today is  a good day'</p>

<p>Edit: Also how can I add -d 'txt=Today is a good day' part in a variable? say text&lt;-'Today is a good day' and use variable text in r syntax. Apologies in case it is very basic, I am new to R. –  user3548327 just now   edit   </p>
","r, curl, sentiment-analysis","<p>Even after removing \ (Which I did long ago, by the way!) and ensuring all syntax is intact, in my R environment I am getting the error with code 6, for above code or ended up in 500 error. As a last resort moved away from system(curl..) syntax and used postForm(""<a href=""http://sentiment.vivekn.com/api/text/"" rel=""nofollow"">http://sentiment.vivekn.com/api/text/</a>"",txt = ""mysentence""). Works for me! Thanks for all the help.</p>
",0,1,129,2015-06-06 17:35:15,https://stackoverflow.com/questions/30685719/mashape-sentiment-and-r-integration
How to capture iterated output variable into list for analysis,"<p>I am trying to parse html text from a number of webpages for sentiment analysis. With the help from community I have been able to iterate over many urls and produce sentiment score based on the textblob library's sentiment analysis and have used the print function successfully to output a score for each url. However I have not been able to achieve, putting the many outputs produced by my return variable into a list so I can use to continue my analysis further by using the stored numbers for calculating averages, and displaying my results in a graph later.</p>

<p>Code with print function: </p>

<pre><code>import requests
import json
import urllib
from bs4 import BeautifulSoup
from textblob import TextBlob



#you can add to this
urls = [""http://www.thestar.com/business/economy/2015/05/19/canadian-consumer-confidence-dips-but-continues-to-climb-in-us-report.html"",
        ""http://globalnews.ca/news/2012054/canada-ripe-for-an-invasion-of-u-s-dollar-stores-experts-say/"",
        ""http://www.cp24.com/news/tsx-flat-in-advance-of-fed-minutes-loonie-oil-prices-stabilize-1.2381931"",
        ""http://www.marketpulse.com/20150522/us-and-canadian-gdp-to-close-out-week-in-fx/"",
        ""http://www.theglobeandmail.com/report-on-business/canada-pension-plan-fund-sees-best-ever-annual-return/article24546796/"",
        ""http://www.marketpulse.com/20150522/canadas-april-inflation-slowest-in-two-years/""]


def parse_websites(list_of_urls):
    for url in list_of_urls:
        html = urllib.urlopen(url).read()
        soup = BeautifulSoup(html)
        # kill all script and style elements

        for script in soup([""script"", ""style""]):
            script.extract()    # rip it out

        # get text
        text = soup.get_text()

        # break into lines and remove leading and trailing space on each
        lines = (line.strip() for line in text.splitlines())
        # break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split(""  ""))
        # drop blank lines
        text = '\n'.join(chunk for chunk in chunks if chunk)

        #print(text)

        wiki = TextBlob(text)
        r = wiki.sentiment.polarity

        print r




parse_websites(urls)
</code></pre>

<p>output: </p>

<pre><code>&gt;&gt;&gt; 
0.10863027172
0.156074203574
0.0766585497835
0.0315555555556
0.0752548359411
0.0902824858757
&gt;&gt;&gt; 
</code></pre>

<p>but when I use the return variable to form a list to use the values to work with I get no result, code: </p>

<pre><code>import requests
import json
import urllib
from bs4 import BeautifulSoup
from textblob import TextBlob



#you can add to this
urls = [""http://www.thestar.com/business/economy/2015/05/19/canadian-consumer-confidence-dips-but-continues-to-climb-in-us-report.html"",
        ""http://globalnews.ca/news/2012054/canada-ripe-for-an-invasion-of-u-s-dollar-stores-experts-say/"",
        ""http://www.cp24.com/news/tsx-flat-in-advance-of-fed-minutes-loonie-oil-prices-stabilize-1.2381931"",
        ""http://www.marketpulse.com/20150522/us-and-canadian-gdp-to-close-out-week-in-fx/"",
        ""http://www.theglobeandmail.com/report-on-business/canada-pension-plan-fund-sees-best-ever-annual-return/article24546796/"",
        ""http://www.marketpulse.com/20150522/canadas-april-inflation-slowest-in-two-years/""]


def parse_websites(list_of_urls):
    for url in list_of_urls:
        html = urllib.urlopen(url).read()
        soup = BeautifulSoup(html)
        # kill all script and style elements

        for script in soup([""script"", ""style""]):
            script.extract()    # rip it out

        # get text
        text = soup.get_text()

        # break into lines and remove leading and trailing space on each
        lines = (line.strip() for line in text.splitlines())
        # break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split(""  ""))
        # drop blank lines
        text = '\n'.join(chunk for chunk in chunks if chunk)

        #print(text)

        wiki = TextBlob(text)
        r = wiki.sentiment.polarity
        r = []
        return [r]




parse_websites(urls)
</code></pre>

<p>output: </p>

<pre><code>Python 2.7.5 (default, May 15 2013, 22:43:36) [MSC v.1500 32 bit (Intel)] on win32
Type ""copyright"", ""credits"" or ""license()"" for more information.
&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
&gt;&gt;&gt; 
</code></pre>

<p>How can I make it so I can work with the numbers and be able to add, subtract, them from list like so [r1, r2, r3...]</p>

<p>Thank you in advance. </p>
","list, function, python-2.7, parsing, sentiment-analysis","<p>From your code below, you are asking python to return an empty list:</p>

<pre><code>r = wiki.sentiment.polarity

r = []     #creat empty list r
return [r] #return empty list
</code></pre>

<p>If I understood your issue correctly, all you have to do is:</p>

<pre><code>my_list = [] #create empty list

   for url in list_of_urls:
    html = urllib.urlopen(url).read()
    soup = BeautifulSoup(html)

    for script in soup([""script"", ""style""]):
        script.extract()    # rip it out

    text = soup.get_text()

    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split(""  ""))
    text = '\n'.join(chunk for chunk in chunks if chunk)

    wiki = TextBlob(text)
    r = wiki.sentiment.polarity

    my_list.append(r) #add r to list my_list

print my_list
</code></pre>

<blockquote>
  <p>[r1, r2, r3, ...]</p>
</blockquote>

<p>Alternatively, you could creat a dictionary with the url as the key</p>

<pre><code>my_dictionary = {}

        r = wiki.sentiment.polarity
        my_dictionary[url] = r

print my_dictionary
</code></pre>

<blockquote>
  <p>{'url1': r1, 'url2 : r2, etc)</p>
</blockquote>

<pre><code>print my_dictionary['url1']
</code></pre>

<blockquote>
  <p>r1</p>
</blockquote>

<p>A dictionary may make more sense for you, as it would be easier to retrieve, edit, and delete ""r"", using the url used as a key.</p>

<p>I am kind of new to Python, so hopefully others will correct me if this doesn't make sense...</p>
",1,1,64,2015-06-07 05:09:29,https://stackoverflow.com/questions/30690268/how-to-capture-iterated-output-variable-into-list-for-analysis
Lazy parsing with Stanford CoreNLP to get sentiment only of specific sentences,"<p><strong>I am looking for ways to optimize the performance of my Stanford CoreNLP sentiment pipeline. As a result, a want to get sentiment of sentences but only those which contain specific keywords given as an input.</strong></p>

<p>I have tried two approaches:</p>

<p><strong>Approach 1: StanfordCoreNLP pipeline annotating entire text with sentiment</strong></p>

<p>I have defined a pipeline of annotators: tokenize, ssplit, parse, sentiment. I have run it on entire article, then looked for keywords in each sentence and, if they were present, run a method returning keyword value. I was not satisfied though that processing takes a couple of seconds.</p>

<p>This is the code:</p>

<pre><code>List&lt;String&gt; keywords = ...;
String text = ...;
Map&lt;Integer,Integer&gt; sentenceSentiment = new HashMap&lt;&gt;();

Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
props.setProperty(""parse.maxlen"", ""20"");
props.setProperty(""tokenize.options"", ""untokenizable=noneDelete"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

Annotation annotation = pipeline.process(text); // takes 2 seconds!!!!
List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
for (int i=0; i&lt;sentences.size(); i++) {
    CoreMap sentence = sentences.get(i);
    if(sentenceContainsKeywords(sentence,keywords) {
        int sentiment = RNNCoreAnnotations.getPredictedClass(sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class));
        sentenceSentiment.put(sentence,sentiment);
    }
}
</code></pre>

<p><strong>Approach 2: StanfordCoreNLP pipeline annotating entire text with sentences, separate annotators running on sentences of interest</strong></p>

<p>Because of the weak performance of the first solution, I have defined the second solution. I have defined a pipeline with annotators: tokenize, ssplit. I looked for keywords in each sentence and, if they were present, I have created an annotation only for this sentence and run next annotators on it: ParserAnnotator, BinarizerAnnotator and SentimentAnnotator.</p>

<p>The results were really unsatisfying because of ParserAnnotator. Even if I initialized it with the same properties. Sometimes it took even more time than entire pipeline run on a document in Approach 1.</p>

<pre><code>List&lt;String&gt; keywords = ...;
String text = ...;
Map&lt;Integer,Integer&gt; sentenceSentiment = new HashMap&lt;&gt;();

Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit""); // parsing, sentiment removed
props.setProperty(""parse.maxlen"", ""20"");
props.setProperty(""tokenize.options"", ""untokenizable=noneDelete"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

// initiation of annotators to be run on sentences
ParserAnnotator parserAnnotator = new ParserAnnotator(""pa"", props);
BinarizerAnnotator  binarizerAnnotator = new BinarizerAnnotator(""ba"", props);
SentimentAnnotator sentimentAnnotator = new SentimentAnnotator(""sa"", props);

Annotation annotation = pipeline.process(text); // takes &lt;100 ms
List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
for (int i=0; i&lt;sentences.size(); i++) {
    CoreMap sentence = sentences.get(i);
    if(sentenceContainsKeywords(sentence,keywords) {
        // code required to perform annotation on one sentence
        List&lt;CoreMap&gt; listWithSentence = new ArrayList&lt;CoreMap&gt;();
        listWithSentence.add(sentence);
        Annotation sentenceAnnotation  = new Annotation(listWithSentence);

        parserAnnotator.annotate(sentenceAnnotation); // takes 50 ms up to 2 seconds!!!!
        binarizerAnnotator.annotate(sentenceAnnotation);
        sentimentAnnotator.annotate(sentenceAnnotation);
        sentence = sentenceAnnotation.get(CoreAnnotations.SentencesAnnotation.class).get(0);

        int sentiment = RNNCoreAnnotations.getPredictedClass(sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class));
        sentenceSentiment.put(sentence,sentiment);
    }
}
</code></pre>

<p><strong>Questions</strong></p>

<ol>
<li><p>I wonder why parsing in CoreNLP is not ""lazy""? (In my example that would mean: performed only when sentiment on a sentence is called). Is it from performance reasons?</p></li>
<li><p>How come a parser for one sentence can work almost as long as a parser for entire article (my article had 7 sentences)? Is it possible to configure it in a way that it works faster?</p></li>
</ol>
","java, performance, parsing, stanford-nlp, sentiment-analysis","<p>If you're looking to speed up constituency parsing, the single best improvement is to use the new <a href=""http://nlp.stanford.edu/software/srparser.shtml"" rel=""nofollow"">shift-reduce constituency parser</a>. It is orders of magnitude faster than the default PCFG parser.</p>

<p>Answers to your later questions:</p>

<ol>
<li><em>Why is CoreNLP parsing not lazy?</em> This is certainly possible, but not something that we've implemented yet in the pipeline. We likely haven't seen many use cases in-house where this is necessary. We will happily accept a contribution of a ""lazy annotator wrapper"" if you're interested in making one!</li>
<li><em>How come a parser for one sentence can work almost as long as a parser for an entire article?</em> The default Stanford PCFG parser is <a href=""https://en.wikipedia.org/wiki/CYK_algorithm"" rel=""nofollow"">cubic time complexity</a> with respect to the sentence length. This is why we usually recommend restricting the maximum sentence length for performance reasons. The shift-reduce parser, on the other hand, runs in linear time with respect to the sentence length.</li>
</ol>
",5,3,1346,2015-06-08 16:40:10,https://stackoverflow.com/questions/30714693/lazy-parsing-with-stanford-corenlp-to-get-sentiment-only-of-specific-sentences
Stanford Parser - Factored model and PCFG,"<p>What is the difference between the factored and PCFG models of stanford parser? (In terms of theoretical working and mathematical perspective)</p>
","parsing, nlp, stanford-nlp, sentiment-analysis, text-analysis","<p><a href=""http://nlp.stanford.edu/software/parser-faq.shtml#y"" rel=""nofollow"">This FAQ answer</a> explains the difference in a long paragraph. Relevant parts are quoted below:</p>

<blockquote>
  <p><strong>Can you explain the different parsers?</strong></p>
  
  <p>This answer is specific to English. It mostly applies to other languages although some components are missing in some languages. The file <code>englishPCFG.ser.gz</code> comprises just an unlexicalized PCFG grammar. It is basically the parser described in the ACL 2003 Accurate Unlexicalized Parsing paper.</p>
  
  <p>… The file <code>englishFactored.ser.gz</code> contains two grammars and leads the system to run three parsers. It first runs a (simpler) PCFG parser and then an untyped dependency parser, and then runs a third parser which finds the parse with the best joint score across the two other parsers via a product model. This is described in the NIPS Fast Exact Inference paper.</p>
  
  <p>… For English, although the grammars and parsing methods differ, the average quality of <code>englishPCFG.ser.gz</code> and <code>englishFactored.ser.gz</code> is similar, and so many people opt for the faster <code>englishPCFG.ser.gz</code>, though <code>englishFactored.ser.gz</code> sometimes does better because it does include lexicalization. For other languages, the factored models are considerably better than the PCFG models, and are what people generally use.</p>
</blockquote>

<p>There are links to the papers referenced on <a href=""http://nlp.stanford.edu/software/lex-parser.shtml#Citing"" rel=""nofollow"">the main parser page</a>.</p>
",2,1,575,2015-06-09 03:43:20,https://stackoverflow.com/questions/30722624/stanford-parser-factored-model-and-pcfg
Amazon Machine Learning for sentiment analysis,"<p>How flexible or supportive is the Amazon Machine Learning platform for sentiment analysis and text analytics?</p>
","amazon-web-services, machine-learning, nlp, sentiment-analysis, amazon-machine-learning","<p>You can build a good machine learning model for sentiment analysis using Amazon ML. </p>

<p>Here is a link to a github project that is doing just that: <a href=""https://github.com/awslabs/machine-learning-samples/tree/master/social-media"" rel=""noreferrer"">https://github.com/awslabs/machine-learning-samples/tree/master/social-media</a></p>

<p>Since the Amazon ML supports supervised learning as well as text as input attribute, you need to get a sample of data that was tagged and build the model with it. </p>

<p>The tagging can be based on Mechanical Turk, like in the example above, or using interns (<em>""the summer is coming""</em>) to do that tagging for you. The benefit of having your specific tagging is that you can put your logic into the model. For example, the difference between ""The beer was cold"" or ""The steak was cold"", where one is positive and one was negative, is something that a generic system will find hard to learn. </p>

<p>You can also try to play with some sample data, from the project above or from this Kaggle competition for sentiment analysis on movie reviews: <a href=""https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews"" rel=""noreferrer"">https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews</a>. I used Amazon ML on that data set and got fairly good results rather easily and quickly.</p>

<p>Note that you can also use the Amazon ML to run real-time predictions based on the model that you are building, and you can use it to respond immediately to negative (or positive) input. See more here: <a href=""http://docs.aws.amazon.com/machine-learning/latest/dg/interpreting_predictions.html"" rel=""noreferrer"">http://docs.aws.amazon.com/machine-learning/latest/dg/interpreting_predictions.html</a></p>
",9,4,3854,2015-06-10 06:36:28,https://stackoverflow.com/questions/30748791/amazon-machine-learning-for-sentiment-analysis
How to separate text from twitter streaming JSON responses and run analysis on text with python?,"<p>I am trying to use the twitter API to run sentiment analysis on the text. I am running into the issue that I am not understanding the way to separate the text from each tweet and running the sentiment polarity analysis provided in the TextBlob library. Further more I would like this to only pull back on english tweets. The output is in JSON. </p>

<p>Here is the code to produce the tweets based on keywords (in this case ""usd"", ""euro"", ""loonie"") and my lame attempt at storing the text and using the result in a variable:</p>

<pre><code>from tweepy.streaming import StreamListener
from tweepy import OAuthHandler
from tweepy import Stream
import json
import re
import pandas as pd
import matplotlib.pyplot as plt


#Variables that contains the user credentials to access Twitter API 
access_token = ""xxxx""
access_token_secret = ""xxxx""
consumer_key = ""xxxx""
consumer_secret = ""xxxx""


#This is a basic listener that just prints received tweets to stdout.
class StdOutListener(StreamListener):

    def on_data(self, data):
        print data
        return True

    def on_error(self, status):
        print status


if __name__ == '__main__':

    #This handles Twitter authentication and the connection to Twitter Streaming API
    l = StdOutListener()
    auth = OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)
    stream = Stream(auth, l)

    #This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'
    stream.filter(track=['euro', 'dollar', 'loonie', ] )

    tweets_data_path = stream.filter

    tweets_data = []
    tweets_file = open(tweets_data_path, ""r"")
    for line in tweets_file:
        try:
            tweet = json.loads(line)
            tweets_data.append(tweet)
        except:
            continue
print len(tweets_data)

tweets['text'] = map(lambda tweet: tweet['text'], tweets_data)
wiki = TextBlob(tweets['text'])
r = wiki.sentiment.polarity

print r
</code></pre>

<p>This is what the output looks like:</p>

<blockquote>
  <p>{""created_at"":""Sun Jun 14 23:43:31 +0000 2015"",""id"":610231121016524801,""id_str"":""610231121016524801"",""text"":""RT @amirulimannn: RM6 diperlukan utk tukar kpd 1Pound.\nRM3 diperlukan utk tukar kpd 1S'pore Dollar.\n\nGraf matawang jatuh. Tak sedih ke? htt\u2026"",""source"":""\u003ca href=\""http://twitter.com/download/iphone\"" rel=\""nofollow\""\u003eTwitter for iPhone\u003c/a\u003e"",""truncated"":false,""in_reply_to_status_id"":null,""in_reply_to_status_id_str"":null,""in_reply_to_user_id"":null,""in_reply_to_user_id_str"":null,""in_reply_to_screen_name"":null,""user"":{""id"":42642877,""id_str"":""42642877"",""name"":""Wny"",""screen_name"":""waaannnyyy"",""location"":""Dirgahayu Darul Makmur"",""url"":null,""description"":""Aku serba tiada, aku kekurangan."",""protected"":false,""verified"":false,""followers_count"":320,""friends_count"":239,""listed_count"":1,""favourites_count"":4344,""statuses_count"":34408,""created_at"":""Tue May 26 15:10:28 +0000 2009"",""utc_offset"":28800,""time_zone"":""Kuala Lumpur"",""geo_enabled"":true,""lang"":""en"",""contributors_enabled"":false,""is_translator"":false,""profile_background_color"":""FFFFFF"",""profile_background_image_url"":""http://pbs.twimg.com/profile_background_images/433201191825047553/PM76m-v2.jpeg"",""profile_background_image_url_https"":""https://pbs.twimg.com/profile_background_images/433201191825047553/PM76m-v2.jpeg"",""profile_background_tile"":true,""profile_link_color"":""DD2E44"",""profile_sidebar_border_color"":""000000"",""profile_sidebar_fill_color"":""EFEFEF"",""profile_text_color"":""333333"",""profile_use_background_image"":true,""profile_image_url"":""http://pbs.twimg.com/profile_images/609402965795835904/mm6jjRRO_normal.jpg"",""profile_image_url_https"":""https://pbs.twimg.com/profile_images/609402965795835904/mm6jjRRO_normal.jpg"",""profile_banner_url"":""https://pbs.twimg.com/profile_banners/42642877/1415486321"",""default_profile"":false,""default_profile_image"":false,""following"":null,""follow_request_sent"":null,""notifications"":null},""geo"":null,""coordinates"":null,""place"":null,""contributors"":null,""retweeted_status"":{""created_at"":""Sat Jun 13 03:33:29 +0000 2015"",""id"":609564219495706624,""id_str"":""609564219495706624"",""text"":""RM6 diperlukan utk tukar kpd 1Pound.\nRM3 diperlukan utk tukar kpd 1S'pore Dollar.\n\nGraf matawang jatuh. Tak sedih ke? http://t.co/dum4skb6uK"",""source"":""\u003ca href=\""http://twitter.com/download/android\"" rel=\""nofollow\""\u003eTwitter for Android\u003c/a\u003e"",""truncated"":false,""in_reply_to_status_id"":null,""in_reply_to_status_id_str"":null,""in_reply_to_user_id"":null,""in_reply_to_user_id_str"":null,""in_reply_to_screen_name"":null,""user"":{""id"":481856658,""id_str"":""481856658"",""name"":""seorang iman"",""screen_name"":""amirulimannn"",""location"":""+06MY"",""url"":""http://instagram.com/amirulimannn"",""description"":""I wanna drown myself in a bottle of her perfume"",""protected"":false,""verified"":false,""followers_count"":723,""friends_count"":834,""listed_count"":2,""favourites_count"":4810,""statuses_count"":50981,""created_at"":""Fri Feb 03 07:49:55 +0000 2012"",""utc_offset"":28800,""time_zone"":""Kuala Lumpur"",""geo_enabled"":true,""lang"":""en"",""contributors_enabled"":false,""is_translator"":false,""profile_background_color"":""AD0A20"",""profile_background_image_url"":""http://pbs.twimg.com/profile_background_images/378800000139426816/61DHBnYy.jpeg"",""profile_background_image_url_https"":""https://pbs.twimg.com/profile_background_images/378800000139426816/61DHBnYy.jpeg"",""profile_background_tile"":false,""profile_link_color"":""E36009"",""profile_sidebar_border_color"":""000000"",""profile_sidebar_fill_color"":""24210E"",""profile_text_color"":""89B5A2"",""profile_use_background_image"":true,""profile_image_url"":""http://pbs.twimg.com/profile_images/592744790283911169/dW7S73WA_normal.jpg"",""profile_image_url_https"":""https://pbs.twimg.com/profile_images/592744790283911169/dW7S73WA_normal.jpg"",""profile_banner_url"":""https://pbs.twimg.com/profile_banners/481856658/1428379855"",""default_profile"":false,""default_profile_image"":false,""following"":null,""follow_request_sent"":null,""notifications"":null},""geo"":null,""coordinates"":null,""place"":null,""contributors"":null,""retweet_count"":1321,""favorite_count"":229,""entities"":{""hashtags"":[],""trends"":[],""urls"":[],""user_mentions"":[],""symbols"":[],""media"":[{""id"":609564142886760448,""id_str"":""609564142886760448"",""indices"":[118,140],""media_url"":""http://pbs.twimg.com/media/CHWbW7yUsAAyAEw.jpg"",""media_url_https"":""https://pbs.twimg.com/media/CHWbW7yUsAAyAEw.jpg"",""url"":""http://t.co/dum4skb6uK"",""display_url"":""pic.twitter.com/dum4skb6uK"",""expanded_url"":""http://twitter.com/amirulimannn/status/609564219495706624/photo/1"",""type"":""photo"",""sizes"":{""small"":{""w"":340,""h"":340,""resize"":""fit""},""thumb"":{""w"":150,""h"":150,""resize"":""crop""},""medium"":{""w"":600,""h"":600,""resize"":""fit""},""large"":{""w"":1024,""h"":1024,""resize"":""fit""}}}]},""extended_entities"":{""media"":[{""id"":609564142886760448,""id_str"":""609564142886760448"",""indices"":[118,140],""media_url"":""http://pbs.twimg.com/media/CHWbW7yUsAAyAEw.jpg"",""media_url_https"":""https://pbs.twimg.com/media/CHWbW7yUsAAyAEw.jpg"",""url"":""http://t.co/dum4skb6uK"",""display_url"":""pic.twitter.com/dum4skb6uK"",""expanded_url"":""http://twitter.com/amirulimannn/status/609564219495706624/photo/1"",""type"":""photo"",""sizes"":{""small"":{""w"":340,""h"":340,""resize"":""fit""},""thumb"":{""w"":150,""h"":150,""resize"":""crop""},""medium"":{""w"":600,""h"":600,""resize"":""fit""},""large"":{""w"":1024,""h"":1024,""resize"":""fit""}}}]},""favorited"":false,""retweeted"":false,""possibly_sensitive"":false,""filter_level"":""low"",""lang"":""in""},""retweet_count"":0,""favorite_count"":0,""entities"":{""hashtags"":[],""trends"":[],""urls"":[],""user_mentions"":[{""screen_name"":""amirulimannn"",""name"":""seorang iman"",""id"":481856658,""id_str"":""481856658"",""indices"":[3,16]}],""symbols"":[],""media"":[{""id"":609564142886760448,""id_str"":""609564142886760448"",""indices"":[139,140],""media_url"":""http://pbs.twimg.com/media/CHWbW7yUsAAyAEw.jpg"",""media_url_https"":""https://pbs.twimg.com/media/CHWbW7yUsAAyAEw.jpg"",""url"":""http://t.co/dum4skb6uK"",""display_url"":""pic.twitter.com/dum4skb6uK"",""expanded_url"":""http://twitter.com/amirulimannn/status/609564219495706624/photo/1"",""type"":""photo"",""sizes"":{""small"":{""w"":340,""h"":340,""resize"":""fit""},""thumb"":{""w"":150,""h"":150,""resize"":""crop""},""medium"":{""w"":600,""h"":600,""resize"":""fit""},""large"":{""w"":1024,""h"":1024,""resize"":""fit""}},""source_status_id"":609564219495706624,""source_status_id_str"":""609564219495706624""}]},""extended_entities"":{""media"":[{""id"":609564142886760448,""id_str"":""609564142886760448"",""indices"":[139,140],""media_url"":""http://pbs.twimg.com/media/CHWbW7yUsAAyAEw.jpg"",""media_url_https"":""https://pbs.twimg.com/media/CHWbW7yUsAAyAEw.jpg"",""url"":""http://t.co/dum4skb6uK"",""display_url"":""pic.twitter.com/dum4skb6uK"",""expanded_url"":""http://twitter.com/amirulimannn/status/609564219495706624/photo/1"",""type"":""photo"",""sizes"":{""small"":{""w"":340,""h"":340,""resize"":""fit""},""thumb"":{""w"":150,""h"":150,""resize"":""crop""},""medium"":{""w"":600,""h"":600,""resize"":""fit""},""large"":{""w"":1024,""h"":1024,""resize"":""fit""}},""source_status_id"":609564219495706624,""source_status_id_str"":""609564219495706624""}]},""favorited"":false,""retweeted"":false,""possibly_sensitive"":false,""filter_level"":""low"",""lang"":""in"",""timestamp_ms"":""1434325411453""}</p>
</blockquote>
","json, parsing, tweepy, sentiment-analysis, twitter-streaming-api","<pre><code>from tweepy.streaming import StreamListener
from tweepy import OAuthHandler
from tweepy import Stream
import json

# Variables that contains the user credentials to access Twitter API
access_token = ''
access_token_secret = ''
consumer_key = ''
consumer_secret = ''


# This is a basic listener that just prints received tweets to stdout.
class StdOutListener(StreamListener):
    def on_data(self, data):
        json_load = json.loads(data)
        texts = json_load['text']
        coded = texts.encode('utf-8')
        s = str(coded)
        print(s[2:-1])
        return True

    def on_error(self, status):
        print(status)

auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
stream = Stream(auth, StdOutListener())

# This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'
stream.filter(track=['euro', 'dollar', 'loonie', ], languages=['en'])
</code></pre>

<p>For your original question about the json: You can load the data asit streams using <code>json.loads()</code>. The reason for the other stuff so you don't get <code>charmap</code> error when you're extracting the data from twitter onto python. The reason for <code>s[2:-1]</code> is to get rid of the extra character from encoding to utf-8.</p>

<p>For english only tweets you can also filter directly from the stream using <code>languages=['en']</code>. </p>

<p>I'm not familiar with TextBlob library, but you can store it through multiple ways, just write your information onto a file and when you run TextBlob read directly from the file. You can replace <code>print(s[2:-1])</code> or add to it:</p>

<pre><code>myfile = open('text.csv','a')
myFile.write(s[2:-1])
myFile.write('\n') # adds a line between tweets
myFile.close() 
</code></pre>

<p>You can read it using <code>file = open('text.csv', 'r')</code> to do your sentiment analysis. Don't forget to add <code>file.close()</code> anytime you open a file.</p>
",4,1,3884,2015-06-14 23:45:56,https://stackoverflow.com/questions/30835702/how-to-separate-text-from-twitter-streaming-json-responses-and-run-analysis-on-t
How to pass each row as an argument to R script from Tableau calculated field,"<p>I am trying to do sentiment analysis on a table that I have. </p>

<p>I want each row of string data to be passed to the R script, but the problem is that Tableau is accepting only aggregate data as params for: </p>

<pre><code>SCRIPT_STR(
  'output &lt;- .arg1; output', [comments]
)
</code></pre>

<p>This gives me an error message:</p>

<pre><code># All fields must be aggregate or constant.
</code></pre>
","r, tableau-api, sentiment-analysis","<p>From the <a href=""http://community.tableau.com/docs/DOC-5313"">Tableau and R Integration</a> documentation:</p>

<blockquote>
  <p>Given that the SCRIPT_*() functions work as table calculations, they
  require aggregate measures or Tableau parameters to work properly.
  Aggregate measures include MIN(), MAX(), ATTR(), SUM(), MEDIAN(), and
  any table calculations or R measures. If you want to use a specific
  non-aggregated dimension, it needs to be wrapped in an aggregate
  function.</p>
</blockquote>

<p>In your case you could do:</p>

<pre><code>SCRIPT_STR(
  'output &lt;- .arg1; output', ATTR([comments])
)
</code></pre>

<p><code>ATTR()</code> is a special Tableau aggregate that does the following: </p>

<pre><code>IF MIN([Dimension]) = MAX([Dimension]) THEN 
[Dimension] ELSE * (a special version of Null) END
</code></pre>

<p>It’s really useful when building visualizations and you’re not sure of the level of detail of data and what’s being sent</p>

<p><strong>Note</strong>: It can be significantly slower than <code>MIN()</code> or <code>MAX()</code> in large data sets, so once you get confident your results are accurate then you can switch to one of the other functions for performance.</p>
",8,4,1615,2015-06-19 13:10:54,https://stackoverflow.com/questions/30939051/how-to-pass-each-row-as-an-argument-to-r-script-from-tableau-calculated-field
How to decode ascii from stream for analysis,"<p>I am trying to run text from twitter api through sentiment analysis from textblob library, When I run my code, the code prints one or two sentiment values and then errors out, to the following error:</p>

<pre><code>UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 31: ordinal not in range(128)
</code></pre>

<p>I do not understand why this is an issue for the code to handle if it is only analyzing text. I have tried to code the script to UTF-8. Here is the code:</p>

<pre><code>from tweepy.streaming import StreamListener
from tweepy import OAuthHandler
from tweepy import Stream
import json
import sys
import csv
from textblob import TextBlob

# Variables that contains the user credentials to access Twitter API
access_token = """"
access_token_secret = """"
consumer_key = """"
consumer_secret = """"


# This is a basic listener that just prints received tweets to stdout.
class StdOutListener(StreamListener):
    def on_data(self, data):
        json_load = json.loads(data)
        texts = json_load['text']
        coded = texts.encode('utf-8')
        s = str(coded)
        content = s.decode('utf-8')
        #print(s[2:-1])
        wiki = TextBlob(s[2:-1])

        r = wiki.sentiment.polarity

        print r

        return True

    def on_error(self, status):
        print(status)

auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
stream = Stream(auth, StdOutListener())

# This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'
stream.filter(track=['dollar', 'euro' ], languages=['en'])
</code></pre>

<p>Can someone please help me with this situtation?</p>

<p>Thank you in advance.</p>
","python-2.7, twitter, stream, sentiment-analysis, textblob","<p>You're mixing too many things together. As the error says, you're trying to decode a byte type.</p>

<p><code>json.loads</code> will result in data as string, from that you'll need to encode it.</p>

<pre><code>texts = json_load['text'] # string
coded = texts.encode('utf-8') # byte
print(coded[2:-1])
</code></pre>

<p>So, in your script, when you tried to decode <code>coded</code> you got an error about decoding <code>byte</code> data.</p>
",1,0,385,2015-06-19 22:26:41,https://stackoverflow.com/questions/30948150/how-to-decode-ascii-from-stream-for-analysis
Is there way to influence AlchemyAPI sentiment analysis,"<p>I was using AlchemyAPI for text analysis. I want to know if there is way to influence the API results or fine-tune it as per the requirement. </p>

<p>I was trying to analyse different call center conversations available on internet. To understand the sentiments i.e. whether customer was unsatisfied/angry and hence conversation is negative.</p>

<p>For 9 out of 10 conversations it gave sentiment as positive and for 1 it was negative. That conversation was about emergency response system (#911 in US).  It seems that words shooting, fear, panic, police, siren could have cause this result.</p>

<p>But actually the whole conversation was fruitful. Caller was not angry with the service instead call center person solved the caller's problem and caller was relaxed. So logically this should not be treated as negative. </p>

<p>What is the way ahead to customize the AlchemyAPI behavior ?</p>
","sentiment-analysis, alchemyapi","<p>We are currently looking at the tools that would be required to allow customization of the AlchemyAPI services. Our current service is entirely pre-trained on billions of web pages, but customization is on the road map. I can't give you any timelines this early, but keep checking back!</p>

<p>Zach, Dev Evangelist AlchemyAPI</p>
",1,0,153,2015-06-20 14:17:49,https://stackoverflow.com/questions/30954978/is-there-way-to-influence-alchemyapi-sentiment-analysis
Function decode_short_URL from twitteR package not working,"<p>I am using <code>decode_short_url</code> of the <code>twitteR</code> package to  decode shortened URLs from Twitter posts, but I am not able to get the desired results, It is always giving back the same results such as: </p>

<pre><code>decode_short_url(decode_short_url(""http://bit.ly/23226se656""))

## http://bit.ly/23226se656
## [1] ""http://bit.ly/23226se656
</code></pre>
","r, twitter-oauth, sentiment-analysis, tweets, url-shortener","<p><strong>UPDATE</strong> I wrapped this functionality in a <a href=""https://github.com/hrbrmstr/longurl"" rel=""nofollow"">package</a> and managed to get it <a href=""http://cran.r-project.org/web/packages/longurl/index.html"" rel=""nofollow"">on CRAN</a> same-day. Now, you can just do:</p>

<pre><code>library(longurl)

expand_urls(""http://bit.ly/23226se656"", check=TRUE, warn=TRUE)
|++++++++++++++++++++++++++++++++++++++++++++++++++| 100%

## Source: local data frame [1 x 2]
## 
##                   orig_url expanded_url
## 1 http://bit.ly/23226se656           NA
## 
## Warning message:
## In FUN(X[[i]], ...) : client error: (404) Not Found
</code></pre>

<p>You can pass in a vector of URLs and get a <code>data_frame</code>/<code>data.frame</code> back in that form.</p>

<hr>

<p>That particular bit.ly URL gives a <code>404</code> error. Here's a version of <code>decode_short_url</code> that has an optional <code>check</code> parameter that will attempt a <code>HEAD</code> request and throw a warning message for any HTTP status other than 200.</p>

<p>You can further modify it to return <code>NA</code> in the event the ""expanded"" link 404's (I have no idea what you need this to really do in the event the link is bad). </p>

<p>NOTE that the addd <code>HEAD</code> request will significantly slow the process down, so you may want to do a first pass with <code>check=FALSE</code> to a separate column, then compare which weren't ""expanded"", then check those with <code>check=TRUE</code>.</p>

<p>You might also want to rename this to avoid namespace conflicts with the one from <code>twitteR</code>.</p>

<pre><code>decode_short_url &lt;- function(url, check=FALSE, ...) {

  require(httr)

  request_url &lt;- paste(""http://api.longurl.org/v2/expand?url="", 
                      url, ""&amp;format=json"", sep="""")
  response &lt;- GET(request_url, query=list(useragent=""twitteR""), ...)

  parsed &lt;- content(response, as=""parsed"")

  ret &lt;- NULL
  if (!(""long-url"" %in% names(parsed))) {
    ret &lt;- url
  } else {
    ret &lt;- parsed[[""long-url""]]
  }

  if (check) warn_for_status(HEAD(url))

  return(url)

}

decode_short_url(""http://bit.ly/23226se656"", check=TRUE)

## [1] ""http://bit.ly/23226se656""
## Warning message:
## In decode_short_url(""http://bit.ly/23226se656"", check = TRUE) :
##   client error: (404) Not Found
</code></pre>
",3,0,492,2015-06-23 13:09:51,https://stackoverflow.com/questions/31003716/function-decode-short-url-from-twitter-package-not-working
How can I extract 2-4 words on each side of a specific term in R?,"<p>How can I extract 2-4 words on each side of a specific term from a string/corpus in R?</p>

<p>Here is an example:</p>

<p>I would like to extract 2 words around 'converse'. </p>

<pre><code>txt &lt;- ""Socially when people meet they should converse to present their
       views and listen to other people's opinions to enhance their perspective"" 
</code></pre>

<p>Output should be like: </p>

<pre><code>""they should converse to present""
</code></pre>
","regex, r, text-mining, sentiment-analysis","<p>I guess this solves your problem:</p>

<pre><code>/((?:\S+\s){2}converse(?:\s\S+){2})/
</code></pre>

<p>Demo: <a href=""https://regex101.com/r/tS9kB0/1"" rel=""nofollow"">https://regex101.com/r/tS9kB0/1</a></p>

<p>If you need other weights on either side, I guess you can see what to change.</p>
",4,-3,332,2015-06-24 15:46:25,https://stackoverflow.com/questions/31031129/how-can-i-extract-2-4-words-on-each-side-of-a-specific-term-in-r
How extracting meaning of sentences for sentiment analysis using NLP,"<p>""I had safe journey"" ,assume this is a feedback for a driver ,provided by a  passenger. I need to extract theses information from this sentence..</p>

<pre><code>""I had safe journey"" -&gt; 
 SUBJECT= ""driving""
 SENTIMENT= ""positive""
</code></pre>

<p>I tried with NLP Extracting Information from Text method. But I don't know how recognized Entities from these kind of sentences.How am I supposed to do that ?</p>
","nlp, sentiment-analysis","<p>To categorize entities of a sentence or a sentence as a whole, you first need to have defined set of classes/categories/groups.  </p>

<p>for eg: To categorize <strong><em>journey</em></strong> to <strong><em>travelling/driving</em></strong>, you should train your system/algorithm to <strong><em>identify</em></strong> specific pattern of sentences which will fall under the category of <strong><em>driving/journey</em></strong>. </p>

<p>This <strong><em>training</em></strong> involves concepts of <strong><em>machine learning</em></strong>, <strong><em><code>Text Categorization</code></em></strong> is what you should be searching for.  </p>

<p><a href=""http://www.quora.com/Natural-Language-Processing/What-is-the-best-approach-for-text-categorization"" rel=""nofollow"">Here</a> is a reference (to just give you an idea) and you can find many more over the web.</p>

<p>Good Luck!</p>

<p>Note: Below are some links from Coursera which offers a course on NLP<br>
<a href=""https://www.coursera.org/course/nlp"" rel=""nofollow"">Link 1</a><br>
<a href=""https://www.coursera.org/course/nlangp"" rel=""nofollow"">Link 2</a></p>
",0,1,945,2015-06-29 04:56:23,https://stackoverflow.com/questions/31108036/how-extracting-meaning-of-sentences-for-sentiment-analysis-using-nlp
Sentiment analysis: more than 3 sentiments,"<p>My app needs sentiment analysis functionality. I've found plenty of services and libraries which can help with this task. But most of them have ""three-dimensional"" output: the text may be classified as ""positive"", ""negative"" or ""neutral. </p>

<p>But what if I need larger variety of options? For example: ""confident/doubtful"", ""calm/alerted"", ""kind/aggressive"" or something like that.</p>

<p>Is it even possible to perform such classification? May be there are already some services/frameworks/libraries available?</p>
","java, nlp, text-mining, sentiment-analysis","<p>You should try <a href=""http://wndomains.fbk.eu/wnaffect.html"" rel=""nofollow"">WordNet-Affect</a>. This ressource provides a tree of emotions. As it is a quite old ressource, you will have to manually parsed it and to map the IDs with <a href=""http://wordnet.princeton.edu/wordnet/download/old-versions/"" rel=""nofollow"">WordNet 1.6</a> synsets (I did this work in Python <a href=""https://github.com/clemtoy/WNAffect"" rel=""nofollow"">here</a>).</p>
",1,3,619,2015-07-02 18:19:56,https://stackoverflow.com/questions/31191865/sentiment-analysis-more-than-3-sentiments
How can we remove tweets from a specific user (user with high number of tweets) for sentiment analysis using R?,"<p>Aim: To perform sentiment analysis on historical judgement by usa courts on same sex marriage. 
    # Since the no of tweets were extremely high for some users, it may introduce bias. how can we remove them?
    # Also, why the number of unique tweets in usafull and total are different?</p>

<pre><code>    rm(list=ls())
    library(twitteR)
    library(wordcloud)
    library(tm)

    download.file(url=""http://curl.haxx.se/ca/cacert.pem"",   destfile=""cacert.pem"")

    consumer_key &lt;- 'key'
    consumer_secret &lt;- 'secret'
    access_token &lt;- 'key'
    access_secret &lt;- 'secret'
    setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)


    usa &lt;- searchTwitter(""#LoveWins"", n=1500 , lang=""en"")

    usa2 &lt;- searchTwitter(""#LGBT"", n=1500 , lang=""en"")

    usa3 &lt;- searchTwitter(""#gay"", n=1500 , lang=""en"")

#get the text
    tusa &lt;- sapply(usa, function(x) x$getText())
    tusa2 &lt;- sapply(usa2, function(x) x$getText())
    tusa3 &lt;- sapply(usa3, function(x) x$getText())

#join texts
    total &lt;- c(tusa,tusa2,tusa3)

#remove the duplicated tweets
    total &lt;- total[!duplicated(total)]

#no. of unique tweets
    uni &lt;- length(total)

# merging three set of tweets horozontally
    usafull&lt;-c(usa,usa2,usa3)

#convert the tweets into dafa frame
    usafull &lt;- twListToDF(usafull)
    usafull &lt;- unique(usafull)

#to know the dates of the tweets (date formatting)
    usafull$date &lt;- format(usafull$created, format = ""%Y-%m-%d"")
    table(usafull$date)

#make a table of number of tweets per user in decreasing number of tweets
    tdata &lt;- as.data.frame(table(usafull$screenName))
    tdata &lt;- tdata[order(tdata$Freq, decreasing = T), ]
    names(tdata) &lt;- c(""User"",""Tweets"")
    head(tdata)


# plot the freq of tweets over time in two hour windows
    library(ggplot2)
    minutes &lt;-60
    ggplot(data = usafull, aes(x=created))+geom_bar(aes(fill=..count..),    binwidth =60*minutes)+scale_x_datetime(""Date"")+ scale_y_continuous(""Frequency"")


#plot the table above for the top 30 to identify any unusual trends
    par(mar=c(5,10,2,2))
    with(tdata[rev(1:30), ], barplot(Tweets, names=User, horiz = T, las =1,     main=""Top 30: Tweets per user"", col = 1))

# the twitter users with more than 20 tweets for removing bias
    userid &lt;- tdata[(tdata$Tweets&gt;20),]
    userid &lt;- userid[,1]
</code></pre>
","r, sentiment-analysis","<p>From your code I understand you want to remove tweets in <code>userid</code>, one way to do it would be like this,</p>

<pre><code>usafull_nobias &lt;- subset(usafull, !(screenName %in% userid$User))
</code></pre>

<p>As for the reason why you get different number of tweets in <code>total</code> and <code>usafull</code>, it is probably due to the fact that in <code>total</code> you are using the text the tweets to find duplicates, and in <code>usafull</code> you are using the full tweet; take into account that e.g. retweets might have the same text but might come from different users, have different ids, etc.</p>

<p>Hope it helps.</p>
",0,0,229,2015-07-14 07:00:53,https://stackoverflow.com/questions/31399804/how-can-we-remove-tweets-from-a-specific-user-user-with-high-number-of-tweets
How to detect aboutness with python pos tagger,"<p>I am working with python to take a facebook status, tell what the status is about and the sentiment. Essentially I need to tell what the sentiment refers to, I already have successfully coded a sentiment analyzer so the trouble is getting a POS tagger to compute what the sentiment is referring to. </p>

<p>If you have any suggestions from experience I would be grateful. I've read some papers on computing aboutness from subject-object, NP-PP, and NP-NP relations but haven't seen any good examples and havent found many papers. </p>

<p>Lastly if you have worked with POS-taggers, what would be my best bet in python as a non-computer scientist. I'm a physicist so I can hack code together but don't want to reinvent the wheel if there exists a package that has everything I'm going to need.</p>

<p>Thank you very much in advance!</p>
","python, sentiment-analysis, pos-tagger, information-theory","<p>This is what I found to work, going to edit it and use it with nltk pos tagger and see what results I can get. </p>

<pre><code>import nltk
from nltk.corpus import brown

# http://thetokenizer.com/2013/05/09/efficient-way-to-extract-the-main-topics-of-a-sentence/


# This is our fast Part of Speech tagger
#############################################################################
brown_train = brown.tagged_sents(categories='news')
regexp_tagger = nltk.RegexpTagger(
    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),
     (r'(-|:|;)$', ':'),
     (r'\'*$', 'MD'),
     (r'(The|the|A|a|An|an)$', 'AT'),
     (r'.*able$', 'JJ'),
     (r'^[A-Z].*$', 'NNP'),
     (r'.*ness$', 'NN'),
     (r'.*ly$', 'RB'),
     (r'.*s$', 'NNS'),
     (r'.*ing$', 'VBG'),
     (r'.*ed$', 'VBD'),
     (r'.*', 'NN')
])
unigram_tagger = nltk.UnigramTagger(brown_train, backoff=regexp_tagger)
bigram_tagger = nltk.BigramTagger(brown_train, backoff=unigram_tagger)
#############################################################################


# This is our semi-CFG; Extend it according to your own needs
#############################################################################
cfg = {}
cfg[""NNP+NNP""] = ""NNP""
cfg[""NN+NN""] = ""NNI""
cfg[""NNI+NN""] = ""NNI""
cfg[""JJ+JJ""] = ""JJ""
cfg[""JJ+NN""] = ""NNI""
#############################################################################


class NPExtractor(object):

    def __init__(self, sentence):
        self.sentence = sentence

    # Split the sentence into singlw words/tokens
    def tokenize_sentence(self, sentence):
        tokens = nltk.word_tokenize(sentence)
        return tokens

    # Normalize brown corpus' tags (""NN"", ""NN-PL"", ""NNS"" &gt; ""NN"")
    def normalize_tags(self, tagged):
        n_tagged = []
        for t in tagged:
            if t[1] == ""NP-TL"" or t[1] == ""NP"":
                n_tagged.append((t[0], ""NNP""))
                continue
            if t[1].endswith(""-TL""):
                n_tagged.append((t[0], t[1][:-3]))
                continue
            if t[1].endswith(""S""):
                n_tagged.append((t[0], t[1][:-1]))
                continue
            n_tagged.append((t[0], t[1]))
        return n_tagged

    # Extract the main topics from the sentence
    def extract(self):

        tokens = self.tokenize_sentence(self.sentence)
        tags = self.normalize_tags(bigram_tagger.tag(tokens))

        merge = True
        while merge:
            merge = False
            for x in range(0, len(tags) - 1):
                t1 = tags[x]
                t2 = tags[x + 1]
                key = ""%s+%s"" % (t1[1], t2[1])
                value = cfg.get(key, '')
                if value:
                    merge = True
                    tags.pop(x)
                    tags.pop(x)
                    match = ""%s %s"" % (t1[0], t2[0])
                    pos = value
                    tags.insert(x, (match, pos))
                    break

        matches = []
        for t in tags:
            if t[1] == ""NNP"" or t[1] == ""NNI"":
            #if t[1] == ""NNP"" or t[1] == ""NNI"" or t[1] == ""NN"":
                matches.append(t[0])
        return matches


# Main method, just run ""python np_extractor.py""
Summary=""""""


Verizon has not honored this appointment or notified me of the delay in an appropriate manner. It is now 1:20 PM and the only way I found out of a change is that I called their chat line and got a message saying my appointment is for 2 PM. My cell phone message says the original time as stated here.


""""""
def main(Topic):
    facebookData=[]
    readdata=csv.reader(open('fb_data1.csv','r'))
    for row in readdata:
        facebookData.append(row)
    relevant_sentence=[]
    for status in facebookData:
        summary=status.split('.')
        for sentence in summary:
            np_extractor = NPExtractor(sentence)
            result = np_extractor.extract()
            if Topic in result:
                relevant_sentence.append(sentence)
            print sentence
            print ""This sentence is about: %s"" % "", "".join(result)
        return relevant_sentence

if __name__ == '__main__':
    result=main('Verizon')
</code></pre>

<p>note that it will save only sentences that are relevant to the topic you define. so if I am analyzing statuses about cheese I could use it as the topic, extract all of the sentences on cheese and then run a sentiment analysis on those. Please if you have comments or suggestions on improving this let me know!</p>
",1,0,245,2015-08-03 12:44:41,https://stackoverflow.com/questions/31787641/how-to-detect-aboutness-with-python-pos-tagger
"Identifying word strings and emoticons, trouble with punctuation","<p>I am going to preface this by saying I have no technical experience in programming at all so please excuse me if I am using the incorrect terminology, but am still tasked to find a solution to a problem we have.</p>

<p>We have a portion of an app that is designed to identify positive and negative words and emoticons in Facebook and Twitter posts. We have developed comprehensive dictionaries for this however the code we have is not performing well.</p>

<p>The problem: words that end with punctuation (e.g. amazing!) are not being identified. As it stands words are split based on white space. I don' t think we can strip punctuation entirely as this will impact on identifying the lists of emoticons.</p>

<p>The solution: we hope is along these lines:
(1) split on white space to get words
Then, for each word:
    (i) check if they match any existing strings (e.g., “afraid”)
        (ia) if they do, go to the next word
        (ib) if they do not, remove punctuation from the tail of the word, then go back to (i).</p>

<p>Is anyone able to help me with the potential syntax for this? We need to write this for iPhone and Android (c++ and java?).
Any assistance is GREATLY appreciated!</p>
","java, ios, sentiment-analysis","<p>I'm going to assume you already have a method (I'm calling it <code>checkDictionaryForMatches()</code>) to check whether a word matches one of the ones you have stored in your dictionary. </p>

<pre><code>public static boolean[] checkString(String string){

    String[] stringarray = string.split("" "");
    boolean[] boolarray = new boolean[stringarray.length];
    for (int i = 0; i &lt; stringarray.length; i++){
        if (stringarray[i].checkDictionaryForMatches() == true){
            boolarray[i] = true;
        }
        else{
            StringBuilder sb = new StringBuilder(stringarray[i]);
            for (int j = 0; j &lt; sb.length(); j++){
                if (sb.charAt(j) == '!'){
                    sb.deleteCharAt(j);
                }
            }

            if (sb.toString().checkDictionaryForMatches() == true){
                boolarray[i] = true;
            }
            else{
                boolarray[i] = false;
            }
        }
    }
    return boolarray;
}
</code></pre>

<p>This only gets rid of the <code>!</code> character, but if you want to use a more general solution you could simply use the <code>OR</code> operand (<code>||</code>) for each piece of punctuation, or preferably <code>RegEx</code>. Because the <code>checkDictionaryForMatches()</code> method is evaluated first, you can define all your emoticons as members of that dictionary. I'm on mobile, so I can't test it, but hopefully that will work.</p>

<p>This does leave you open to unexpected emoticons (@_@), but I don't know how you would deal with that. It also prevents the user from dropping punctuation into the middle of words (e.g. re.d or something random). Maybe it would be useful for stripping the <code>@</code> from Twitter handles, or something? Get back to me if there are any issues. </p>
",0,-2,60,2015-08-07 04:09:23,https://stackoverflow.com/questions/31869571/identifying-word-strings-and-emoticons-trouble-with-punctuation
How to assign different scores for sentiment analysis in R?,"<p>I have a file of Tweets which I want/need to perform sentiment analysis on.
I have come across <a href=""https://stackoverflow.com/questions/24903030/error-in-r-code-sentiment-analysis"">this</a> process, which works well however now I want to alter this code, so that I can assign different scores based on sentiment.</p>

<p>This is the code:</p>

<pre><code>    score.sentiment = function(sentences , pos.words, neg.words , progress='none')
{
 require(plyr)
 require(stringr)
 scores = laply(sentences,function(sentence,pos.words,neg.words)
 {
     sentence =gsub('[[:punct:]]','',sentence)
     sentence =gsub('[[:cntrl]]','',sentence)
     sentence =gsub('\\d+','',sentence)
     sentence=tolower(sentence)
     word.list=str_split(sentence,'\\s+')
     words=unlist(word.list)
     pos.matches=match(words,pos.words)
     neg.matches=match(words,neg.words)
     pos.matches = !is.na(pos.matches)   
     neg.matches = !is.na(neg.matches) 
     score=sum(pos.matches)-sum(neg.matches)
     return(score)
 },pos.words,neg.words,.progress=.progress)
 scores.df=data.frame(scores=scores,text=sentences)
 return(scores.df)
}  
</code></pre>

<p>What I am now looking to do, is to have FOUR dictionaries;</p>

<p>super.words, pos,words, neg.words, terrible.words.</p>

<p>I want to assign different scores for each of these dictionaries :
super.words =+2, pos.words=+1, neg.words=-1, terrible.words=-2.</p>

<p>I know that <code>pos.matches = !is.na(pos.matches)</code>   and     <code>neg.matches = !is.na(neg.matches)</code> assigns 1/0 for TRUE/FALSE, however I want to find out how to assign these specific scores which gives a score for EACH tweet.</p>

<p>At the moment, I am just focusing on the standard two dictionaries, pos and neg.
I have assigned scores to these two data frames:</p>

<pre><code>posDF&lt;-data.frame(words=pos, value=1, stringsAsFactors=F)

negDF&lt;-data.frame(words=neg, value=-1, stringsAsFactors=F)
</code></pre>

<p>and tried to run the above algorithm with these however nothing works.</p>

<p>I came across <a href=""https://stackoverflow.com/questions/28072370/how-to-extract-individual-words-from-sentence-and-match-them-with-words-from-pos?rq=1"">this</a> page and <a href=""https://stackoverflow.com/questions/28545270/sentiment-score-for-sentence-in-r"">this</a> page where one has written several 'for' loops however the end result only provides an overall score of either -1,0 or 1.</p>

<p>Ultimately, I am looking for a result similar to this:</p>

<pre><code>table(analysis$score)
</code></pre>

<p>-5 -4 -3 -2 -1 0 1 2 3 4 5 6 19</p>

<p>3 8 49 164 603 2790 ..................etc</p>

<p>however so far , if I get a result that doesn't involve having to ""debug"" the code, I get this:</p>

<pre><code>&lt; table of extent 0 &gt;
</code></pre>

<p>Here are some sample Tweets I am using:</p>

<pre><code>tweets&lt;-data.frame(words=c(""@UKLabour @KarlTurnerMP #LabourManifesto Speaking as a carer, labours NHS plans are all good news, very happy. Making my day this!"", ""#LabourManifesto eggs and sweet things are looking evil"", ""@UKLabour @KarlTurnerMP Half way through the #LabourManifesto, this will definitely improve every-bodies lives if implemented fully."", ""There is nothing ""long term"" about fossil fuels. #fracking #labourmanifesto https://twitter.com/stevetopple/status/587576796599595012"", ""Fair play Ed, very strong speech! Finally had the chance to watch it. #LabourManifesto wanna see the other manifestos nowwww"") )
</code></pre>

<p>Any help is greatly appreciated! </p>

<hr>

<hr>

<p>So, essentially, I am wondering if there is a way to change this section of the original script:</p>

<pre><code>pos.matches=match(words,pos.words)
 neg.matches=match(words,neg.words)
 pos.matches = !is.na(pos.matches)   
 neg.matches = !is.na(neg.matches)
</code></pre>

<p>so I can assign my own specific scores? (pos.words=+1, neg.words=-1) ? Or if I would have to incorporate various if and for loops?</p>
","r, algorithm, twitter, sentiment-analysis","<p>If you are just looking to use custom scores in generating the total score, you could just change this line <code>score=sum(pos.matches)-sum(neg.matches)</code> to be something like: </p>

<pre><code>score=sum((super.pos.matches)*2 + sum(pos.matches) + sum(neg.matches)*(-1) + sum(terrible.matches)*(-2))
</code></pre>
",0,1,1546,2015-08-07 14:58:53,https://stackoverflow.com/questions/31880869/how-to-assign-different-scores-for-sentiment-analysis-in-r
How to get sentiment via stanford corenlp interactive shell?,"<p><a href=""https://i.sstatic.net/vqNI9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vqNI9.png"" alt=""Interactive shell with all annotators including sentiment""></a>I have been trying to get the <em>sentiment value from the stanford corenlp</em>, but it seems in the interactive shell, the sentiment is not given as an output.</p>

<p>I have specified the <em>annotators</em> using the command given in the official website.</p>

<pre><code>java -cp ""*"" -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref, sentiment
</code></pre>

<p>Also, when i tried for getting just the sentiment, then at first asked for other annotators, after providing i didn't give any output at all.</p>

<p>I am getting the interactive shell in both the cases. Like - NLP>
But, there is no sign of sentiment - positive or negative.</p>

<p>Also, if we use the sentimentPipeline instead of nlp.pipeline. It gives the sentiment. Is there any way i can get via the interactive shell or if possible is there any way we can use sentimentPipeline with pexpect?</p>
","stanford-nlp, sentiment-analysis, pexpect, interactive-shell","<p><a href=""https://i.sstatic.net/ZE4YG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZE4YG.png"" alt=""old_version""></a>I was using the older version of Stanford corenlp (3.4.1), but in the recent version (3.5.1), it gives the sentiment.<a href=""https://i.sstatic.net/vOKBW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vOKBW.png"" alt=""new_version""></a></p>
",1,1,321,2015-08-09 09:21:32,https://stackoverflow.com/questions/31902722/how-to-get-sentiment-via-stanford-corenlp-interactive-shell
sentiWordNet in rapidminer,"<p>I am trying to integrate SentiWordNet into Rapidminer using the Extract Sentiment operator. I cannot find a way to get the dictionary input, in fact even if I use the OpenWordnetDictionary operator I get ""Map failed"" error.
Has anyone of you ever (successfully) performed the same operation or know how I can make it work?
Thank you</p>
","sentiment-analysis, rapidminer, senti-wordnet","<p>There are some examples <a href=""http://rapid-i.com/rapidforum/index.php/topic,6035.0.html"" rel=""nofollow"">here</a>. The basic trick is to put the Sentiword text file into the same folder as the Wordnet dictionary.</p>
",1,2,626,2015-09-01 15:32:43,https://stackoverflow.com/questions/32335564/sentiwordnet-in-rapidminer
Sentiment analysis with stanford nlp does not work,"<p>I am trying to use stanford nlp to get the sentiment of a text:
Here is my code:</p>

<pre><code>import java.util.Properties;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;

public class SentimentAnalyzer {

    public static void main(String[] args) {
        findSentiment("""");
    }

    public static void findSentiment(String line) {
        line = ""I started taking the little pill about 6 years ago."";
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        int mainSentiment = 0;
        if (line != null &amp;&amp; line.length() &gt; 0) {
            int longest = 0;
            Annotation annotation = pipeline.process(line);
            for (CoreMap sentence : annotation
                    .get(CoreAnnotations.SentencesAnnotation.class)) {
                Tree tree = sentence
                        .get(SentimentCoreAnnotations.AnnotatedTree.class);
                int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                String partText = sentence.toString();
                if (partText.length() &gt; longest) {
                    mainSentiment = sentiment;
                    longest = partText.length();
                }

            }
        }
        if (mainSentiment == 2 || mainSentiment &gt; 4 || mainSentiment &lt; 0) {
            System.out.println(""Neutral "" + line);
        }
        else{
        }
        /*
         * TweetWithSentiment tweetWithSentiment = new TweetWithSentiment(line,
         * toCss(mainSentiment)); return tweetWithSentiment;
         */

    }
}
</code></pre>

<p>Also I use the instruction from this link:
<a href=""https://blog.openshift.com/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java/"" rel=""nofollow noreferrer"">https://blog.openshift.com/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java/</a></p>

<p>But I get the following error:</p>

<pre><code>Exception in thread ""main"" java.lang.NullPointerException
at edu.stanford.nlp.rnn.RNNCoreAnnotations.getPredictedClass(RNNCoreAnnotations.java:58)
at SentimentAnalyzer.findSentiment(SentimentAnalyzer.java:27)
at SentimentAnalyzer.main(SentimentAnalyzer.java:14)
</code></pre>

<p>which point to this line:</p>

<pre><code>    Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
</code></pre>
","java, stanford-nlp, sentiment-analysis","<p>Use this instead:</p>

<pre><code>Tree tree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);
</code></pre>

<p><strong>Edit:</strong>
To get positive, negative, and neutral comments, use this snippet:</p>

<pre><code>switch (mainSentiment) {
        case 0:
            return ""Very Negative"";
        case 1:
            return ""Negative"";
        case 2:
            return ""Neutral"";
        case 3:
            return ""Positive"";
        case 4:
            return ""Very Positive"";
        default:
            return """";
        }
</code></pre>
",6,1,1516,2015-09-01 16:08:47,https://stackoverflow.com/questions/32336293/sentiment-analysis-with-stanford-nlp-does-not-work
R sentiment analysis with phrases in dictionaries,"<p>I am performing sentiment analysis on a set of Tweets that I have and I now want to know how to add phrases to the positive and negative dictionaries.</p>

<p>I've read in the files of the phrases I want to test but when running the sentiment analysis it doesn't give me a result.</p>

<p>When reading through the sentiment algorithm, I can see that it is matching the words to the dictionaries but is there a way to scan for words as well as phrases?</p>

<p>Here is the code:</p>

<pre><code>    score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
  require(plyr)  
  require(stringr)  
  # we got a vector of sentences. plyr will handle a list  
  # or a vector as an ""l"" for us  
  # we want a simple array (""a"") of scores back, so we use  
  # ""l"" + ""a"" + ""ply"" = ""laply"":  
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    # clean up sentences with R's regex-driven global substitute, gsub():
    sentence = gsub('[[:punct:]]', '', sentence)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)    
    # and convert to lower case:    
    sentence = tolower(sentence)    
    # split into words. str_split is in the stringr package    
    word.list = str_split(sentence, '\\s+')    
    # sometimes a list() is one level of hierarchy too much    
    words = unlist(word.list)    
    # compare our words to the dictionaries of positive &amp; negative terms
    pos.matches = match(words, pos)
    neg.matches = match(words, neg)   
    # match() returns the position of the matched term or NA    
    # we just want a TRUE/FALSE:    
    pos.matches = !is.na(pos.matches)   
    neg.matches = !is.na(neg.matches)   
    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)    
    return(score)    
  }, pos.words, neg.words, .progress=.progress )  
  scores.df = data.frame(score=scores, text=sentences)  
  return(scores.df)  
}
analysis=score.sentiment(Tweets, pos, neg)
table(analysis$score)
</code></pre>

<p>This is the result I get:</p>

<pre><code>0
20
</code></pre>

<p>whereas I am after the standard table that this function provides
e.g.</p>

<pre><code>-2 -1 0 1 2 
 1  2 3 4 5 
</code></pre>

<p>for example.</p>

<p>Does anybody have any ideas on how to run this on phrases?
Note: The TWEETS file is a file of sentences.</p>
","r, twitter, machine-learning, sentiment-analysis","<p>The function <code>score.sentiment</code> seems to work. If I try a very simple setup,</p>

<pre><code>Tweets = c(""this is good"", ""how bad it is"")
neg = c(""bad"")
pos = c(""good"")
analysis=score.sentiment(Tweets, pos, neg)
table(analysis$score)
</code></pre>

<p>I get the expected result,</p>

<pre><code>&gt; table(analysis$score)

-1  1 
 1  1 
</code></pre>

<p>How are you feeding the 20 tweets to the method? From the result you're posting, that <code>0 20</code>, I'd say that your problem is that your 20 tweets do not have any positive or negative word, although of course it was the case you would have noticed it. Maybe if you post more details on your list of tweets, your positive and negative words it would be easier to help you.</p>

<p>Anyhow, your function seems to be working just fine.</p>

<p>Hope it helps. </p>

<p><strong>EDIT after clarifications via comments:</strong></p>

<p>Actually, to solve your problem you need to tokenize your sentences into <code>n-grams</code>, where <code>n</code> would correspond to the maximum number of words you are using for your list of positive and negative <code>n-grams</code>. You can see how to do this e.g. in <a href=""https://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka"">this SO question</a>. For completeness, and since I've tested it myself, here is an example for what you could do. I simplify it to <code>bigrams</code> (n=2) and use the following inputs:</p>

<pre><code>Tweets = c(""rewarding hard work with raising taxes and VAT. #LabourManifesto"", 
              ""Ed Miliband is offering 'wrong choice' of 'more cuts' in #LabourManifesto"")
pos = c(""rewarding hard work"")
neg = c(""wrong choice"")
</code></pre>

<p>You can create a bigram tokenizer like this,</p>

<pre><code>library(tm)
library(RWeka)
BigramTokenizer &lt;- function(x) NGramTokenizer(x, Weka_control(min=2,max=2))
</code></pre>

<p>And test it,</p>

<pre><code>&gt; BigramTokenizer(""rewarding hard work with raising taxes and VAT. #LabourManifesto"")
[1] ""rewarding hard""       ""hard work""            ""work with""           
[4] ""with raising""         ""raising taxes""        ""taxes and""           
[7] ""and VAT""              ""VAT #LabourManifesto""
</code></pre>

<p>Then in your method you simply substitute this line,</p>

<pre><code>word.list = str_split(sentence, '\\s+')
</code></pre>

<p>by this</p>

<pre><code>word.list = BigramTokenizer(sentence)
</code></pre>

<p>Although of course it would be better if you changed <code>word.list</code> to <code>ngram.list</code> or something like that.</p>

<p>The result is, as expected,</p>

<pre><code>&gt; table(analysis$score)

-1  0 
 1  1
</code></pre>

<p>Just decide your <code>n-gram</code> size and add it to <code>Weka_control</code> and you should be fine.</p>

<p>Hope it helps.</p>
",1,1,1926,2015-09-04 09:53:57,https://stackoverflow.com/questions/32395098/r-sentiment-analysis-with-phrases-in-dictionaries
Stanford nlp for python,"<p>I want to find the sentiment (positive/negative/neutral) of any given string. On researching I came across Stanford NLP. But sadly it's in Java. How can I make it work for Python?</p>
","python, stanford-nlp, sentiment-analysis","<h1>Use <a href=""https://github.com/smilli/py-corenlp/"" rel=""nofollow noreferrer""><code>py-corenlp</code></a></h1>
<h2>Download <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">Stanford CoreNLP</a></h2>
<p>The latest version at this time (2020-05-25) is 4.0.0:</p>
<pre><code>wget https://nlp.stanford.edu/software/stanford-corenlp-4.0.0.zip https://nlp.stanford.edu/software/stanford-corenlp-4.0.0-models-english.jar
</code></pre>
<p>If you do not have <a href=""https://www.gnu.org/software/wget/"" rel=""nofollow noreferrer""><code>wget</code></a>, you probably have <a href=""https://curl.haxx.se/"" rel=""nofollow noreferrer""><code>curl</code></a>:</p>
<pre><code>curl https://nlp.stanford.edu/software/stanford-corenlp-4.0.0.zip -O https://nlp.stanford.edu/software/stanford-corenlp-4.0.0-models-english.jar -O
</code></pre>
<p>If all else fails, use the browser ;-)</p>
<h2>Install the package</h2>
<pre><code>unzip stanford-corenlp-4.0.0.zip
mv stanford-corenlp-4.0.0-models-english.jar stanford-corenlp-4.0.0
</code></pre>
<h2>Start the <a href=""http://stanfordnlp.github.io/CoreNLP/corenlp-server.html"" rel=""nofollow noreferrer"">server</a></h2>
<pre><code>cd stanford-corenlp-4.0.0
java -mx5g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000
</code></pre>
<p>Notes:</p>
<ol>
<li><code>timeout</code> is in milliseconds, I set it to 10 sec above.
You should increase it if you pass huge blobs to the server.</li>
<li>There are <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#command-line-flags"" rel=""nofollow noreferrer"">more options</a>, you can list them with <code>--help</code>.</li>
<li><code>-mx5g</code> should allocate enough <a href=""https://stackoverflow.com/q/14763079/850781"">memory</a>, but YMMV and you may need to modify the option if your box is underpowered.</li>
</ol>
<h2>Install the python package</h2>
<p>The standard package</p>
<pre><code>pip install pycorenlp
</code></pre>
<p>does <em>not</em> work with Python 3.9, so you need to do</p>
<pre><code>pip install git+https://github.com/sam-s/py-corenlp.git
</code></pre>
<p>(See also <a href=""https://stanfordnlp.github.io/CoreNLP/other-languages.html"" rel=""nofollow noreferrer"">the official list</a>).</p>
<h2>Use it</h2>
<pre><code>from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('http://localhost:9000')
res = nlp.annotate(&quot;I love you. I hate him. You are nice. He is dumb&quot;,
                   properties={
                       'annotators': 'sentiment',
                       'outputFormat': 'json',
                       'timeout': 1000,
                   })
for s in res[&quot;sentences&quot;]:
    print(&quot;%d: '%s': %s %s&quot; % (
        s[&quot;index&quot;],
        &quot; &quot;.join([t[&quot;word&quot;] for t in s[&quot;tokens&quot;]]),
        s[&quot;sentimentValue&quot;], s[&quot;sentiment&quot;]))
</code></pre>
<p>and you will get:</p>
<pre><code>0: 'I love you .': 3 Positive
1: 'I hate him .': 1 Negative
2: 'You are nice .': 3 Positive
3: 'He is dumb': 1 Negative
</code></pre>
<h1>Notes</h1>
<ol>
<li>You pass the whole text to the server and it splits it into sentences. It also splits sentences into tokens.</li>
<li>The sentiment is ascribed to each <em>sentence</em>, not the <em>whole text</em>. The <a href=""https://en.wikipedia.org/wiki/Mean"" rel=""nofollow noreferrer"">mean</a> <code>sentimentValue</code> across sentences can be used to estimate the sentiment of the whole text.</li>
<li>The average sentiment of a sentence is between <code>Neutral</code> (2) and <code>Negative</code> (1), the range is from <code>VeryNegative</code> (0) to <code>VeryPositive</code> (4) which appear to be quite rare.</li>
<li>You can <a href=""https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#stopping-the-server"" rel=""nofollow noreferrer"">stop the server</a> either by typing <kbd>Ctrl-C</kbd> at the terminal you started it from or using the shell command <code>kill $(lsof -ti tcp:9000)</code>. <code>9000</code> is the default port, you can change it using the <code>-port</code> option when starting the server.</li>
<li>Increase <code>timeout</code> (in milliseconds) in server or client if you get timeout errors.</li>
<li><code>sentiment</code> is just <strong>one</strong> annotator, there are <a href=""https://stanfordnlp.github.io/CoreNLP/annotators.html"" rel=""nofollow noreferrer"">many more</a>, and you can request several, separating them by comma: <code>'annotators': 'sentiment,lemma'</code>.</li>
<li>Beware that the sentiment model is somewhat idiosyncratic (e.g., <a href=""https://github.com/stanfordnlp/CoreNLP/issues/351"" rel=""nofollow noreferrer"">the result is different depending on whether you mention David or Bill</a>).</li>
</ol>
<p><strong>PS</strong>. I cannot believe that I added a <strong>9th</strong> answer, but, I guess, I had to, since none of the existing answers helped me (some of the 8 previous answers have now been deleted, some others have been converted to comments).</p>
",70,30,36885,2015-10-01 04:34:36,https://stackoverflow.com/questions/32879532/stanford-nlp-for-python
Missing values in sentiment classification,"<p>I am trying to build a sentiment analysis engine using python's sklearn package.
the problem is analyzing Rotten Tomatoes reviews on this Kaggle Competition </p>

<p><a href=""https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews"" rel=""nofollow"">https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews</a></p>

<p>the sentiments can take 5 possible values</p>

<p>I am using the following classifiers</p>

<ol>
<li>Multinomial Naive Bayes</li>
<li>Logistic Regression</li>
<li>Stochastic Gradient Descent</li>
</ol>

<p>Since these are all linear classifiers suited for binary classification, here are the steps that i have to take</p>

<ol>
<li><p>Break up the training and test set into 5 parts, one part per sentiment.
Lets say the possible values for the sentiment are a,b,c,d,e. So in part one of my data, i will have all the reviews, but the reviews that have sentiment 'a' will be marked as positive and all of the others will be marked as negative. Similarly i create other parts for the other sentiment values.</p></li>
<li><p>Clean up the data in all 5 parts</p></li>
<li><p>Create a pipeline and feed all the test set parts to my classifier, one after the other. I will store one result per part. So result of classifying part one is partOneRes and so on. Anything which is marked as positive in partOneRes belongs to sentiment 'a'. Similarly for other parts.</p></li>
<li><p>Finally i would like to combine the results for all 5 parts. I will look at partOneRes. Anything that is marked positive will be changed to Sentiment 'a'. I will do similarly for all the other parts. Then i simply merge the results.</p></li>
<li><p>It would have been ideal if i got no overlaps or duplicates. But i get a small number of duplicates, which is fine. I can add some logic to handle that.</p></li>
<li><p>I would do this for all three classifiers and finally i want to find out which classifier give me the best results.</p></li>
</ol>

<p>My problem is that I can see that there are many reviews which my classifier was not able to put in any category! Why would this happen? Could it be due to the small size of the dataset?</p>
","python, machine-learning, nlp, sentiment-analysis","<p>Restating, the problem is that the five binary models you've trained are not mutually exhaustive.  There are several possibilities.
First of all, do you have a 100% clean classification for each of the five sentiments, or are there some acknowledged classification errors?</p>

<p>You need a set that is mutually exclusive <em>and</em> exhaustive.  Your approach suggests, but hardly guarantees, this result.  You might consider an integrated solution that does make this guarantee.  Multi-class SVM is one such, but may not apply well to your situation.</p>

<p>If the classes are not 100% accurate, you can easily have all five rejecting a particular observation.  This suggests that your classification algorithms need tuning, or that the data themselves are not as amenable to classification as you would like.</p>

<p>You might also check that you've cleaned that data appropriately; a few errors can seriously move the class boundaries.</p>

<p>What I suspect is happening is a small-boundary effect: each class, when compared against the combination of the other four, ""pulls in"" its boundaries, leaving unclaimed territory between the final sets.</p>

<p>Do you have a way to check the classification parameters after training?  If so, can you visualize the five boundaries selected?  If you do find pathological gaps, are there training parameters you can tune, such as giving a larger epsilon to the training groups?</p>

<p>I hope this helps.</p>
",1,2,365,2015-10-02 18:02:29,https://stackoverflow.com/questions/32913155/missing-values-in-sentiment-classification
python reading from csv doesn&#39;t give the right answer,"<p>This is my first post on this site, so my apologies for any mistakes I make in this post.</p>

<p>I am working on a Twitter Sentiment analysis for a company. I have a csv filled with tweets (just 1 column) and I want to find out if they are talking positively about that company or negatively. 
I found a package that can find the polarity &amp; subjectivity from a sentence. For example, this is my python script were I want to find the results from:</p>

<pre><code>from pattern.nl import sentiment
print sentiment('I love this company!')
</code></pre>

<p>and the result: </p>

<pre><code>(0.75, 0.90)
</code></pre>

<p>At this point, everything works well, but I need to do this for 6000+ tweets, so I thought about reading the csv into the script and do it for each row.</p>

<pre><code>import csv
from pattern.nl import sentiment
import sys

f = open('C:\Users\Admin\Desktop\TweetsFromNodeXLOnlyTweets.csv')
try:
    reader = csv.reader(f)
    for row in reader:
        text = row
        print sentiment(text)
finally:
f.close()
</code></pre>

<p>When I run this, I just get (0.00, 0.00) for all tweets. It seems like the script isn’t able to read it, but when I just print text, I get all tweets as I should. And when I write print sentiment(text), I do get the results I want, but only for that one tweet.
How could I fix this? I am pretty new to Python (started yesterday), but no tutorial or other post on this site could help me with thi!s.  </p>
","python, csv, twitter, sentiment-analysis","<p>According the the <a href=""https://docs.python.org/3/library/csv.html#csv.reader"" rel=""nofollow"">documentation</a>, csv.reader returns lists of strings when iterated, corresponding to the fields in the row. Since your csv is only one column, the list returned will only have one item, so you should access it like this:</p>

<p><code>text = row[0]</code></p>
",2,0,66,2015-10-07 11:47:58,https://stackoverflow.com/questions/32991406/python-reading-from-csv-doesnt-give-the-right-answer
Python &amp; SQL. Inserting variable in column while in for loop,"<p>as part of my sentiment analysis on tweets, I need extract tweets from my database, run a python script to get the sentiment score and insert it back into the database.</p>

<p>Part of my code:</p>

<pre><code>#conneting to database (works perfect)
cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER=xxxxxxx\SQLEXPRESS;DATABASE=TestTwitter;UID=;PWD=')
cursor = cnxn.cursor()

#Alter table (works perfect)
cursor.execute(""ALTER TABLE TestTable ADD score2 varchar(255);"")

#select tweet from each row and calculate score (works perfect)
cursor.execute(""SELECT TestTable.Tweet FROM TestTable"")
for row in cursor.fetchall():
    print (row[0])
    sentim = sentiment(row[0])
    print (sentim)

    #update table and add sentiment score for each row (not so perfect)
    cursor.execute(""Update TestTable SET score2 = '"" + (str(sentim)) + ""';"")
    cnxn.commit()
</code></pre>

<p>When updating the table, all rows get the same sentiment value as the first tweet instead of their own. The ""print (sentiment)"" shows the score of each tweet one by one, but it seems like the loop doesn't work when updating the table. Any way to fix this?</p>
","python, sql, sql-server, sentiment-analysis","<p>This isn't a problem with the while loop, but with your UPDATE command; you're telling it to update all rows in TestTable, not just the one you're working on. You need to provide a WHERE condition to that UPDATE.</p>

<pre><code>cursor.execute(""SELECT TestTable.Tweet, TestTable.id FROM TestTable"")
for row in cursor.fetchall():
    ...
    cursor.execute(""Update TestTable SET score2 = %s WHERE id = %s;"", (sentim, row[1]))
</code></pre>

<p>(assuming your primary key column is called <code>id</code>).</p>

<p>Note also that you should get into the habit of using parameterized queries; although there's no chance of SQL injection in this code, because nothing is coming from user input, other code might have that problem so it's best to avoid it altogether.</p>
",2,2,2932,2015-10-14 08:09:55,https://stackoverflow.com/questions/33119972/python-sql-inserting-variable-in-column-while-in-for-loop
How to use lexicon dictionary in c#,"<p>i am working on sentiment analysis in c#, I have done preprocessing, and the next part is lexicon based analysis, for which I have found English Lexicon of about 6800 word <a href=""https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon"" rel=""nofollow"">lexicon by Professor Bing Liu </a> which contains two text files, one for positive and other is for negative. </p>

<p>I was thinking that I have to just find the each word(sentiment word) from these files that either the particular word is positive or negative. but the problem is, these files contains words without any space , without any format (means individual word can not be recognized from file ). </p>

<p>So how can I find the word in file? Or is there any other way through which I can work easily with this?</p>
","c#, dictionary, nlp, text-mining, sentiment-analysis","<p>The file uses <code>\n</code> as a line separator (unlike standard Windows <code>\r\n</code>).
So, just not open it with <strong>NotePad</strong> or alike, do it with <strong>WordPad</strong>.</p>

<p>To load the file into a collection (let it be <code>HashSet&lt;String&gt;</code> - you, probably, want to test if a word is <em>within the positive words</em> or not), you can use <em>Linq</em>:</p>

<pre><code>  HashSet&lt;String&gt; positives = new HashSet&lt;String&gt;(File
    .ReadLines(@""C:\positive-words.txt"")
    .Where(item =&gt; !String.isNullOrEmpty(item) &amp;&amp; !item.StartsWith("";"")));

  ....

  String testWord = ...

  if (positives.Contains(testWord)) {
    ...
  }
</code></pre>

<p>Actual file's content is</p>

<pre><code>;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
; 
; Opinion Lexicon: Positive
...
;       frequently in social media content. 
;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

a+
abound
abounds
abundance
...
zenith
zest
zippy
</code></pre>
",1,2,1396,2015-10-16 14:33:46,https://stackoverflow.com/questions/33173056/how-to-use-lexicon-dictionary-in-c
Features for sentiment analysis using Maxent model,"<p>I want to implement my own sentiment analysis using maximum entropy model. without using any Api. what could be the best features f(c,d) for my maximum entropy model. I have three classes positive, negative and neutral</p>
","machine-learning, nlp, sentiment-analysis","<p>Some of the most used and effective features in Sentiment Analysis are <strong>unigrams</strong>. <strong>Bigrams</strong> can also be employed, but it is quite controversial whether they are really useful or not. </p>

<p>Note that using frequency values of unigrams/bigrams does not significantly improve results in Sentiment Analysis; it is therefore generally sufficient to extract word types and use a boolean value to express their presence/absence in a text. </p>

<p>The important thing is how you preprocess text before you extract these features. For example, apart from lower-casing your tokens, handling negation scopes can improve your results when extracting unigram features.</p>

<p>In any case, Sentiment Analysis is a wide field. You will find that different feature extraction strategies could yield different results depending on the specific type of analysis you need to perform (e.g. feature-based analysis, subjectivity analysis, polarity analysis, etc.). </p>

<p>You can find almost everything you need to get started here:</p>

<ul>
<li><a href=""http://sentiment.christopherpotts.net"" rel=""nofollow"">http://sentiment.christopherpotts.net</a></li>
<li>Liu, Bing. ""Sentiment analysis and opinion mining."" Synthesis Lectures on Human Language Technologies 5.1 (2012): 1-167.</li>
<li>Pang, Bo, and Lillian Lee. ""Opinion mining and sentiment analysis."" Foundations and trends in information retrieval 2.1-2 (2008): 1-135.</li>
</ul>
",0,0,246,2015-10-17 11:12:29,https://stackoverflow.com/questions/33185951/features-for-sentiment-analysis-using-maxent-model
Difference between Laplace estimate and Expected likelihood estimate?,"<p>I'm doing a research in sentiment analysis using Python and at the moment I'm having some confusion with nltk.probability</p>

<p>What is the difference between Laplace estimate and Expected likelihood estimate?
What is the appropriate smoothing technique from those two for sentiment analysis research?</p>

<p>Here's the definition from <a href=""http://www.nltk.org/_modules/nltk/probability.html"" rel=""nofollow"">NLTK documentation</a> -</p>

<blockquote>
  <p><strong>The Laplace estimate</strong> for the probability distribution of the
  experiment used to generate a frequency distribution.  The ""Laplace
  estimate"" approximates the probability of a sample with count <em>c</em> from
  an experiment with <em>N</em> outcomes and <em>B</em> bins as
  <em>(c+1)/(N+B)</em>.  This is equivalent to adding one to the count for each bin, and taking the maximum likelihood estimate of the resulting
  frequency distribution.</p>
  
  <p><strong>The expected likelihood estimate</strong> for the probability distribution of
  the experiment used to generate a frequency distribution.  The
  ""expected likelihood estimate"" approximates the probability of a
  sample with count <em>c</em> from an experiment with <em>N</em> outcomes and
  <em>B</em> bins as <em>(c+0.5)/(N+B/2)</em>.  This is equivalent to adding 0.5 to the count for each bin, and taking the maximum likelihood estimate of
  the resulting frequency distribution.</p>
</blockquote>
","python, statistics, probability, sentiment-analysis","<p>When there is a large number of possible events that have not been seen, Laplace technique assigns nearly all probability mass to the data not seen before. ELE compensates for this by making alpha smaller - 0.5, thus assigning less data to unseen events.</p>

<p>Take a look at <a href=""http://www.cs.rochester.edu/u/james/CSC248/Lec3.pdf"" rel=""nofollow"">here</a> for more details</p>
",0,0,1162,2015-11-04 05:46:38,https://stackoverflow.com/questions/33514555/difference-between-laplace-estimate-and-expected-likelihood-estimate
Testing the Keras sentiment classification with model.predict,"<p>I have trained the imdb_lstm.py on my PC.
Now I want to test the trained network by inputting some text of my own. How do I do it?
Thank you!</p>
","python, sentiment-analysis, lstm, keras","<p>So what you basically need to do is as follows:</p>

<ol>
<li>Tokenize sequnces: convert the string into words (features): For example: ""hello my name is georgio"" to [""hello"", ""my"", ""name"", ""is"", ""georgio""].</li>
<li>Next, you want to remove stop words (check Google for what stop words are).</li>
<li>This stage is optional, it may lead to faulty results but I think it worth a try. Stem your words (features), that way you'll reduce the number of features which will lead to a faster run. Again, that's optional and might lead to some failures, for example: if you stem the word 'parking' you get 'park' which has a different meaning.</li>
<li>Next thing is to create a dictionary (check Google for that). Each word gets a unique number and from this point we will use this number only.</li>
<li>Computers understand numbers only so we need to talk in their language. We'll take the dictionary from stage 4 and replace each word in our corpus with its matching number.</li>
<li>Now we need to split our data set to two groups: training and testing sets. One (training) will train our NN model and the second (testing) will help us to figure out how good is our NN. You can use Keras' cross validation function.</li>
<li>Next thing is defining whats the max number of features our NN can get as an input. Keras call this parameter - 'maxlen'. But you don't really have to do this manually, Keras can do that automatically just by searching for the longest sentence you have in your corpus. </li>
<li>Next, let's say that Keras found out that the longest sentence in your corpus has 20 words (features) and one of your sentences is the example in the first stage, which its length is 5 (if we'll remove stop words it'll be shorter), in such case we'll need to add zeros, 15 zeros actually. This is called pad sequence, we do that so every input sequence will be in the same length.</li>
</ol>
",8,2,3148,2015-11-05 03:30:30,https://stackoverflow.com/questions/33536182/testing-the-keras-sentiment-classification-with-model-predict
what is the formula of sentiment calculation,"<p>what is the actual formula to compute sentiments using sentiment rated lexicon. the lexicon that I am using contains rating between the range -5 to 5. I want to compute sentiment for individual sentences. Either i have to compute average of all sentiment ranked words in sentence or only sum up them. </p>
","nlp, sentiment-analysis, mining","<p>you can use R tool for sentiment computation. here is the link you can refer to:
<a href=""https://sites.google.com/site/miningtwitter/questions/sentiment/analysis"" rel=""nofollow"">https://sites.google.com/site/miningtwitter/questions/sentiment/analysis</a></p>
",1,1,11502,2015-11-05 11:30:10,https://stackoverflow.com/questions/33543446/what-is-the-formula-of-sentiment-calculation
How to create a Stanford coreNLP model by training?,"<p>I'm very new to Stanford's coreNLP and I'm trying to train it by creating a model. I have a folder that has dev.txt, train.txt, and test.txt as well as a jar file named stanford-corenlp-3.5.1-models.jar. According to <a href=""https://stackoverflow.com/questions/22586658/how-to-train-the-stanford-nlp-sentiment-analysis-tool/22635259#22635259"">this</a> question, I can create a model by executing the following command in the terminal:</p>

<pre><code>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath     dev.txt -train -model model.ser.gz
</code></pre>

<p>However, when I run that in the terminal, I get the following error:</p>

<pre><code>Error: could not find or load main class edu.stanford.nlp.sentiment.SentimentTraining
</code></pre>

<p>Can anyone provide step-by-step instructions of how to go about training CoreNLP? I went on the Stanford <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""nofollow noreferrer"">website</a> to see how training is done, but I'm still confused. I thought all I needed to create a model (e.g model.ser.gz) were those three text files and one jar file. </p>

<p>Any help is very appreciated, thank you!</p>
","java, stanford-nlp, sentiment-analysis, training-data","<p>You need to include the CoreNLP jar file in your classpath. So, your java command should look like:</p>

<p><code>java -cp /path/to/corenlp/jar:/path/to/corenlp/library/dependencies -mx8g ...</code></p>

<p>From the root of the CoreNLP distribution, you can just include all the jars in the directory; e.g.,</p>

<p><code>java -cp ""*"" -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz</code></p>
",1,0,2593,2015-11-10 03:01:11,https://stackoverflow.com/questions/33622132/how-to-create-a-stanford-corenlp-model-by-training
Questions about creating stanford CoreNLP training models,"<p>I've been working with Stanford's coreNLP to perform sentiment analysis on some data I have and I'm working on creating a training model. I know we can create a training model with the following command: </p>

<pre><code>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath     dev.txt -train -model model.ser.gz
</code></pre>

<p>I know what goes in the train.txt file. You score sentences and put them in train.txt, something like this:
 <code>(0 (2 Today) (0 (0 (2 is) (0 (2 a) (0 (0 bad) (2 day)))) (..)))</code></p>

<p>But I don't understand what goes in the dev.txt file. 
I read through <a href=""https://stackoverflow.com/questions/22586658/how-to-train-the-stanford-nlp-sentiment-analysis-tool"">this</a> question several times to try to understand what goes in dev.txt, but it's still unclear to me. Also, scoring these sentences manually has become a pain, is there a tool available that makes it easier? I'm worried that I've been using the wrong number of parentheses or some other stupid mistake like that. </p>

<p>Also, any suggestions on how long my train.txt file should be? I'm thinking of scoring a 1000 sentences. Is that number too small, too large?</p>

<p>All your help is appreciated :)</p>
","stanford-nlp, sentiment-analysis, training-data, scoring","<ol>
<li><p>dev.txt should be the same as train.txt just with a different set of sentences.  Note that the same sentence should not appear in dev.txt and train.txt.  The development set is used to evaluate the quality of the model you train on the training data.</p></li>
<li><p>We don't distribute a tool for tagging sentiment data.  This class could be helpful in building data: <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/sentiment/BuildBinarizedDataset.html"" rel=""nofollow"">http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/sentiment/BuildBinarizedDataset.html</a></p></li>
<li><p>Here are the sizes of the train, dev, and test sets used for the sentiment model: train=8544, dev=1101, test=2210</p></li>
</ol>
",1,1,671,2015-11-14 20:08:53,https://stackoverflow.com/questions/33712795/questions-about-creating-stanford-corenlp-training-models
Training model ignored by stanford CoreNLP,"<p>I've made a small, sample training model to use when performing sentiment analysis with coreNLP. In order to get coreNLP to use this model, I've written the following lines of code: </p>

<pre><code>props = new Properties(); 
props.put(""sample_model-0023-100.00.ser.gz"", ""/home/usr/Documents/coreNLP/"");
props.put(""annotators"", ""tokenize, ssplit, parse, lemma, sentiment""); 
pipeline = new StanfordCoreNLP(props);
</code></pre>

<p>However, it doesn't look like the code is using the model I'm pointing to. I know this because I passed a couple of sentences to it that should get certain scores if the code were to use this model, but I'm getting different scores. Am I missing something in my lines of code that's preventing coreNLP from using the model I've created?</p>
","stanford-nlp, sentiment-analysis, training-data, scoring","<p>You want this:</p>

<pre><code>props.put(""sentiment.model"", ""/path/to/sample_model-0023-100.00.ser.gz"");
</code></pre>
",2,0,88,2015-11-24 04:10:54,https://stackoverflow.com/questions/33885309/training-model-ignored-by-stanford-corenlp
CoreNLP Training Model Issue,"<p>I'm using Stanford CoreNLP to perform sentiment analysis on some Tweets I'm gathering. I have created a mock training model with one sentence which is scored as follows:
(0 (2 bear)(2 (2 oil)(2 market))).</p>

<p>I'm scoring on a scale of 0 to 4, with 0 being very negative, 2 being neutral and 4 is very positive. 
I am testing on the following two tweets:</p>

<p>bear oil market</p>

<p>bear oil markets</p>

<p>It is assigning the first sentence a 0, which is correct and the second sentence is scored as 2, which is incorrect since this sentence should also be negative. The only difference between the two sentences is the s in markets in the second sentence.</p>

<p>My question is this: is there any way to get around the fact that ANY variation of ANY word is causing the two sentences to be scored differently?</p>
","stanford-nlp, sentiment-analysis, scoring","<p>I think the short answer is ""no"" -- a difference in wording always has a chance of changing the sentiment of a sentence. You can try mitigating the problem by re-training on new data.</p>

<p>Really, if you're running on anything but movie reviews, you should expect the model to degrade in performance at least a little, and occasionally a lot. If you have the training data, it's worth re-training.</p>
",2,0,111,2015-11-25 16:25:15,https://stackoverflow.com/questions/33921575/corenlp-training-model-issue
How to chain together multiple qdap transformations for text mining / sentiment (polarity) analysis in R,"<p>I have a <code>data.frame</code> that has week numbers, <code>week</code>, and text reviews, <code>text</code>. I would like to treat the <code>week</code> variable as my grouping variable and run some basic text analysis on it (e.g. <code>qdap::polarity</code>). Some of the review text have multiple sentences; however, I only care about the week's polarity ""on-the-whole"". </p>

<p>How can I chain together multiple text transformations before running <code>qdap::polarity</code> and adhere to its warning messages? I am able to chain together transformations with the <code>tm::tm_map</code> and <code>tm::tm_reduce</code> -- is there something comparable in <code>qdap</code>? What is the proper way to pre-treat/transform this text prior to running <code>qdap::polarity</code> and/or <code>qdap::sentSplit</code>?</p>

<p>More details in the following code / reproducible example:</p>

<pre><code>library(qdap)
library(tm)

df &lt;- data.frame(week = c(1, 1, 1, 2, 2, 3, 4),
                 text = c(""This is some text. It was bad. Not good."",
                          ""Another review that was bad!"",
                          ""Great job, very helpful; more stuff here, but can't quite get it."",
                          ""Short, poor, not good Dr. Jay, but just so-so. And some more text here."",
                          ""Awesome job! This was a great review. Very helpful and thorough."",
                          ""Not so great."",
                          ""The 1st time Mr. Smith helped me was not good.""),
                 stringsAsFactors = FALSE)

docs &lt;- as.Corpus(df$text, df$week)

funs &lt;- list(stripWhitespace,
             tolower,
             replace_ordinal,
             replace_number,
             replace_abbreviation)

# Is there a qdap function that does something similar to the next line?
# Or is there a way to pass this VCorpus / Corpus directly to qdap::polarity?
docs &lt;- tm_map(docs, FUN = tm_reduce, tmFuns = funs)


# At the end of the day, I would like to get this type of output, but adhere to
# the warning message about running sentSplit. How should I pre-treat / cleanse
# these sentences, but keep the ""week"" grouping?
pol &lt;- polarity(df$text, df$week)

## Not run:
# check_text(df$text)
</code></pre>
","r, text-mining, sentiment-analysis, tm, qdap","<p>You could run <code>sentSplit</code> as suggested in the warning as follows:</p>

<pre><code>df_split &lt;- sentSplit(df, ""text"")
with(df_split, polarity(text, week))

##   week total.sentences total.words ave.polarity sd.polarity stan.mean.polarity
## 1    1               5          26       -0.138       0.710             -0.195
## 2    2               6          26        0.342       0.402              0.852
## 3    3               1           3       -0.577          NA                 NA
## 4    4               2          10        0.000       0.000                NaN
</code></pre>

<p>Note that I have a breakout sentiment package <a href=""https://github.com/trinker/sentimentr"" rel=""nofollow""><strong>sentimentr</strong></a> available on github that is an improvment in speed, functionality, and documentation over the <strong>qdap</strong> version.  This does the sentence splitting internally in the <code>sentiment_by</code> function.  The script below allows you to install the package and use it:</p>

<pre><code>if (!require(""pacman"")) install.packages(""pacman"")
p_load_gh(""trinker/sentimentr"")

with(df, sentiment_by(text, week))

##    week word_count        sd ave_sentiment
## 1:    2         25 0.7562542    0.21086408
## 2:    1         26 1.1291541    0.05781106
## 3:    4         10        NA    0.00000000
## 4:    3          3        NA   -0.57735027
</code></pre>
",1,3,286,2015-12-01 14:49:23,https://stackoverflow.com/questions/34023200/how-to-chain-together-multiple-qdap-transformations-for-text-mining-sentiment
NLTK sentiment towards entity,"<p>I have just started using NLTK and the task I need to accomplish is pretty simple, I think.
I need to parse a number of documents and extract the sentiment towards some entities. For example the overall sentiment of the following sentence:</p>

<pre><code>Tea is great. However, I hate coffee.
</code></pre>

<p>is negative, but I would like to extract the sentiment towards single, predefined entities. In particular, in the previous example I would like to feed NLTK with my entities <code>('tea', 'coffee')</code> and be able to extract <code>sentiment('tea')</code> and <code>sentiment('coffee')</code> separately.
I read through <a href=""http://www.nltk.org/book/ch07.html"" rel=""nofollow"">this</a> document but I could not find a way to accomplish this simple task.</p>
","python, nltk, sentiment-analysis, named-entity-extraction","<p>You need a <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow"">classifier</a>, and you need an annotated sentiment corpus to train it with. The nltk offers the <code>movie_review</code> corpus, but of course you'll get best results if you train with something similar to your own data. See also the nltk's <a href=""http://www.nltk.org/api/nltk.sentiment.html"" rel=""nofollow"">nltk.sentiment</a> package.</p>
",1,2,1654,2015-12-02 10:51:20,https://stackoverflow.com/questions/34040871/nltk-sentiment-towards-entity
Stanford NLP: &quot;No annotator named sentiment&quot; error,"<p>I'm trying to apply Sentiment Analysis for a dataset containing tweets that I already collected, for some experiments I run for my thesis. I have followed various tutorials on the Internet and my code is the following so far:</p>

<pre><code>public class SentimentAnalyzer {
    public static StanfordCoreNLP pipeline;

    public SentimentAnalyzer() {
         Properties props = new Properties();
         props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
         pipeline = new StanfordCoreNLP(props);
    }

    public static int getSentiment(String tweet) {
         int mainSentiment = 0;
         if (tweet != null &amp;&amp; tweet.length() &gt; 0) {
             int longest = 0;
             Annotation annotation = pipeline.process(tweet);
             for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
                 Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
                 int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                 String partText = sentence.toString();
                 if (partText.length() &gt; longest) {
                     mainSentiment = sentiment;
                     longest = partText.length();
                 }

             }
        }
        return mainSentiment;
    }
}
</code></pre>

<p>The problem is that when I run this piece of code, Java returns an Illegal Argument Exception:</p>

<pre><code>Adding annotator tokenize
Adding annotator ssplit
Adding annotator parse
Loading parser from serialized file 
edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [1,4 sec].
Adding annotator sentiment
Exception in thread ""main"" java.lang.IllegalArgumentException: No annotator named sentiment
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:78)
    at edu.stanfoJava Result: 1
</code></pre>

<p>but every single tutorial lists ""sentiment"" as a valid annotator! The program specifically hangs when calling the StanfordCoreNLP constructor (inside the SentimentAnalyzer constructor) and though I have tried every configuration that I can think of (making the pipeline not-static, creating the pipeline eveytime in getSentiment() method, using a .properties file instead), the problem stays.
I'm using StanfordCoreNLP 3.3.1, along with the models .jar (it's the 3.5.2 version, though) and ejml 0.23 -I have used 3.5.2 version, but with no success.</p>

<p>EDIT:
I replaced 3.3.2 with 3.5.2 and now the error changed:</p>

<pre><code>Loading parser from text file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz Exception in thread ""main"" java.lang.RuntimeException: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz: expecting BEGIN block; got ��
</code></pre>

<p>The rest is unreadable and cannot copy that either. It seems that it's a problem with the models library, at least the directory that the error points, belongs to models. I also updated with the latest library from their GitHub repository, but the error is the same.</p>
","java, stanford-nlp, sentiment-analysis","<p>Turns out that StanfordCoreNLP v. 1.3.4 was left out from a previous implementation since August. When I removed it, the code started working fine.</p>
",0,0,406,2015-12-03 11:22:20,https://stackoverflow.com/questions/34064705/stanford-nlp-no-annotator-named-sentiment-error
"Multiple Stanford CoreNLP model files made, which one is the correct one to use?","<p>I made a sentiment analysis model using Standford CoreNLP's library. So I have a bunch of ser.gz files that look like the following: 
<a href=""https://i.sstatic.net/omumK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/omumK.jpg"" alt=""enter image description here""></a></p>

<p>I was wondering what model to use in my java code, but based on a previous <a href=""https://stackoverflow.com/questions/33725007/how-to-get-stanford-corenlp-to-use-a-training-model-you-created"">question</a>,  </p>

<p>I just used the model with the highest F1 score, which in this case is model-0014-93.73.ser.gz. And in my java code, I pointed to the model I want to use by using the following line: </p>

<pre><code>    props.put(""sentiment.model"", ""/path/to/model-0014-93.73.ser.gz."");
</code></pre>

<p>However, by referring to just that model, am I excluding the sentiment analysis from the other models that were made? Should I be referring to all the model files to make sure I ""covered"" all the bases or does the highest scoring model trump everything else?</p>
","stanford-nlp, sentiment-analysis","<p>You should point to only the single highest scoring model. The code has no way to make use of multiple models at the same time.</p>
",1,0,163,2015-12-08 16:35:13,https://stackoverflow.com/questions/34161172/multiple-stanford-corenlp-model-files-made-which-one-is-the-correct-one-to-use
"If I don&#39;t specify a sentiment model in CoreNLP, what will it use to score the data?","<p>I've been creating sentiment analysis models to use with Stanford CoreNLP, and I've been using the one with the highest F1 score in my java code, like so:</p>

<pre><code>props.put(""sentiment.model"", ""/path/to/model-0014-93.73.ser.gz."");
</code></pre>

<p>But if I remove this line, what does CoreNLP use to score the data? Is there a default coreNLP model that's used if the user does not specify a model?</p>
","stanford-nlp, sentiment-analysis","<p>If no model is given, it'll use the default model included in the release trained on the Stanford Sentiment Treebank: <a href=""http://nlp.stanford.edu/sentiment/treebank.html"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/treebank.html</a></p>
",1,0,75,2015-12-08 19:45:20,https://stackoverflow.com/questions/34164668/if-i-dont-specify-a-sentiment-model-in-corenlp-what-will-it-use-to-score-the-d
Spark Analysis Reduce (Twitter Sentiment),"<p>I have a question about apache Spark and Java</p>

<p>I'm making an application that streams data from Twitter (Twitter4J). And I'm also making an app that analyse the data. a txt file with JSON tweets.</p>

<p><strong>StreamingApp:</strong>
<em>output tweet.txt:</em>
example: one line of Json:</p>

<pre><code>{""id"":674534622903054336,""user"":""twitter"",""tweet"":""a tweet from twitter #twitter."",""date"":""2015-12-09T11:22:41CET""}
</code></pre>

<p><strong>AnalyzerApp:</strong></p>

<pre><code>SparkConf conf = new SparkConf().setMaster(""local[2]"").setAppName(""TwitterAnalyzerBigData"");
final JavaSparkContext sc = new JavaSparkContext(conf);
JavaRDD&lt;String&gt; jsonFile = sc.textFile(""whateverpath/tweets.txt"");
JavaPairRDD&lt;Long, String&gt; tweetsFiltered = jsonFile.mapToPair(new TwitterFilterFunction());
</code></pre>

<p>tweetsFiltered is a JavaPairRDD: <strong>tweet ID : Long</strong> and <strong>tweet: String</strong> </p>

<p>Now I'm using some map functions to get something like this:</p>

<pre><code>(1,a tweet from twitter #twitter.,0.0,0.055555556,negative, TWITTER)
</code></pre>

<p><strong>(This is random test data)</strong></p>

<ul>
<li><strong>1</strong> being the ID </li>
<li><strong>a tweet from twitter #twitter</strong>: The tweet </li>
<li><strong>0.0</strong> : positive score</li>
<li><strong>0.0566</strong> : negative score</li>
<li><strong>negative</strong> : category sentiment (positive or negative)</li>
<li><strong>TWITTER</strong> : category of tweet (a category based on hashtags)</li>
</ul>

<p><strong>The question:</strong> how can I reduce this RDD so I get a result like this:</p>

<pre><code>TWITTER, 1, 0
</code></pre>

<ul>
<li><strong>TWITTER</strong>: the category of the tweet</li>
<li><strong>1</strong> : total amount of tweets of TWITTER CATEGORY</li>
<li><strong>0</strong> : The amount of positive tweets of TWITTER CATEGORY</li>
</ul>

<p>After the answer of James I made the reduceByKey in Java.</p>

<pre><code>JavaRDD&lt;Tuple3&lt;String, Float, Float&gt;&gt; categoryEntryRDD = categoryResult.map(new Function&lt;Tuple4&lt;Long, String, String, String&gt;, Tuple3&lt;String, Float, Float&gt;&gt;() {
            @Override
            public Tuple3&lt;String, Float, Float&gt; call(Tuple4&lt;Long, String, String, String&gt; tuple4) throws Exception {
                if(tuple4._3().equals(""positive"")){
                    return new Tuple3&lt;String, Float, Float&gt;(tuple4._4(), 1F, 1F);
                } else {
                    return new Tuple3&lt;String, Float, Float&gt;(tuple4._4(), 1F, 0F);
                }

            }
        });


    Tuple3&lt;String, Float, Float&gt; reducedRDD = categoryEntryRDD.reduce(new Function2&lt;Tuple3&lt;String, Float, Float&gt;, Tuple3&lt;String, Float, Float&gt;, Tuple3&lt;String, Float, Float&gt;&gt;() {
        @Override
        public Tuple3&lt;String, Float, Float&gt; call(Tuple3&lt;String, Float, Float&gt; tuple31, Tuple3&lt;String, Float, Float&gt; tuple32) throws Exception {
            System.out.println(tuple31.toString());

            return new Tuple3&lt;String, Float, Float&gt;(tuple31._1(), tuple31._2()+tuple32._2(), tuple31._3()+tuple32._3());
        }
    });
</code></pre>

<p>But the reduce method is not the same as reduceByKey, how can I fix this?</p>

<p>My output:
{TWITTER, 1000, 400}
But I also have a category: FACEBOOK with 1000 tweets. </p>
","java, twitter, apache-spark, mapreduce, sentiment-analysis","<p>This is a good canonical map-reduce problem:</p>

<ol>
<li>Map the tweet entry to a tuple representing the category and a count of 1</li>
<li>Reduce the category tuples to sum up the number for each catetory</li>
</ol>

<p>i.e. pseudocode:</p>

<pre><code>+ map the RDD you have (id, tweet, pos score...
- map to a tuple that looks like (category, 1, 1) if the tweet is positive
- map to a tuple that looks like (category, 1, 0) if the tweet is negative

+ reduceByKey where our key is the category using summation
- we end up with an RDD of tuples in the form you want
</code></pre>

<p>Here is some scala code to accomplish this--java is analogous</p>

<pre><code>val categoryEntryRDD = tweetsFiltered.map( mappedTuple =&gt;
    if mappedTuple._5 == ""positive"" {
        (mappedTuple._6, 1, 1)
    } else {
        (mappedTyple._6, 1, 0)
    }
}

val reducedRDD = categoryEntryRDD.reduceByKey( x, y =&gt; (x._1 + y._1, x._2 + y._2) )
</code></pre>

<p>At this point reducedRDD holds tuples that look like (category, total number of tweets of the category, total positive tweets of the category).</p>
",0,0,556,2015-12-09 18:20:06,https://stackoverflow.com/questions/34186185/spark-analysis-reduce-twitter-sentiment
How fast is Stanford&#39;s CoreNLP sentiment analysis tool?,"<ol>
<li>I'm trying to find out whether it's feasible for me to use the the CoreNLP sentiment analysis tool (<a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow"">http://nlp.stanford.edu/sentiment/code.html</a>) on a dataset equivalent in size to about 1 million IMDB reviews.</li>
</ol>

<p>I could not find any absolute metrics anywhere online about average times. I would appreciate if someone could point me to any place about these statistics regarding the speed.</p>

<ol start=""2"">
<li><p>Also, this is what I'm trying - to see if it's possible to estimate a movie rating by using the text alone i.e. by summing up scores for each sentence in a review. Does anything in my idea or in the code snippet below look stupid (should be done better)? I get the feeling that I might be using this tool for something that it's not suited for or I'm doing it the wrong way.</p>

<pre><code>public static double getTextSentimentScore(String text){
Annotation annotation = pipeline.process(text);
double sum = 0;
List&lt;CoreMap&gt; sentences = (List&lt;CoreMap&gt;) annotation.get(CoreAnnotations.SentencesAnnotation.class);
int i = 0;
for (CoreMap sentence : sentences) {
    String sentiment = sentence.get(SentimentCoreAnnotations.SentimentClass.class);
    int sentimentScore = 0;
    if (sentiment.equals(""Very positive""))
        sentimentScore = 5;
    if (sentiment.equals(""Positive""))
        sentimentScore = 4;
    if (sentiment.equals(""Neutral""))
        sentimentScore = 3;
    if (sentiment.equals(""Negative""))
        sentimentScore = 2;
    if (sentiment.equals(""Very negative""))
        sentimentScore = 1;
    sum += sentimentScore;
    System.out.println(sentiment + ""\t"" + sentimentScore);
}
return (sum/sentences.size());
</code></pre>

<p>}</p></li>
</ol>
","stanford-nlp, text-mining, sentiment-analysis","<p>If you run this command:</p>

<pre><code>java -Xmx5g -cp ""stanford-corenlp-full-2015-12-09/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,parse,sentiment -filelist list-of-sample-docs.txt
</code></pre>

<p>the final output will give you timing information</p>

<p>So all you have to do is:</p>

<ol>
<li><p>take 100 IMDB reviews, put them in files named imdb_review_1, imdb_review_2, etc...</p></li>
<li><p>put each filename one file per line in list-of-sample-docs.txts</p></li>
<li><p>run that command and the final output will show total time for each annotator and total time elapsed</p></li>
</ol>
",2,0,414,2015-12-12 07:08:59,https://stackoverflow.com/questions/34237295/how-fast-is-stanfords-corenlp-sentiment-analysis-tool
How to save the results from nltk function &quot;most_informative_features&quot; to a txt file in Python,"<p>Sorry if this is a noobie question! I am carrying out sentiment analysis on python using nltk. It has a function which returns the most informative features but whenever i try and save the results to a  text file, i get the following error 'TypeError: must be str, not list'. the code i am using is as follows</p>

<pre><code>classifier.most_informative_features(100)  

str(information)
saveFile = open('informationFile.txt', 'w')

saveFile.write(information)
saveFile.close()
</code></pre>

<p>any ideas what i am doing wrong?</p>
","python, string, nltk, sentiment-analysis","<p>You need to assign the conversion from list to string to something, or do in place...</p>

<pre><code>saveFile.write(''.join(information))
</code></pre>

<p>Applying <code>str</code> to a variable generates a value, but doesn't change the variable (unless you assign it)</p>

<pre><code>&gt;&gt;&gt; bar
['a', 'b', 'c']
&gt;&gt;&gt; str(bar)
""['a', 'b', 'c']""
&gt;&gt;&gt; bar
['a', 'b', 'c']
&gt;&gt;&gt; ', '.join(bar)
'a, b, c'
&gt;&gt;&gt; bar
['a', 'b', 'c']
&gt;&gt;&gt; bar = ', '.join(bar)
&gt;&gt;&gt; bar
'a, b, c'
</code></pre>
",0,1,510,2015-12-15 11:49:00,https://stackoverflow.com/questions/34288466/how-to-save-the-results-from-nltk-function-most-informative-features-to-a-txt
Adding Special Case Idioms to Python Vader Sentiment,"<p>I've been using Vader Sentiment to do some text sentiment analysis and I noticed that my data has a lot of ""way to go"" phrases that were incorrectly being classified as neutral:</p>

<pre><code>In[11]: sentiment('way to go John')
Out[11]: {'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}
</code></pre>

<p>After digging into the Vader Source Code, I found the following dictionary:</p>

<pre><code># check for special case idioms using a sentiment-laden keyword known to SAGE
SPECIAL_CASE_IDIOMS = {""the shit"": 3, ""the bomb"": 3, ""bad ass"": 1.5, ""yeah right"": -2,
                       ""cut the mustard"": 2, ""kiss of death"": -1.5, ""hand to mouth"": -2,
                       ""way to go"": 3}
</code></pre>

<p>As you can see, I added the ""Way to go"" entry manually. However, it seems to have no effect:</p>

<pre><code>In [12]: sentiment('way to go John')
Out[12]: {'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}
</code></pre>

<p>Any idea what I am missing? Or more specifically, what do I need to do to make adding custom idioms work? Here is the Vader Sentiment source code:</p>

<pre><code>#######################################################################################################################
# SENTIMENT SCORING SCRIPT
#######################################################################################################################
'''
Created on July 04, 2013
@author: C.J. Hutto
  Hutto, C.J. &amp; Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for 
  Sentiment Analysis of Social Media Text. Eighth International Conference on 
  Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.
'''

import os, math, re, sys, fnmatch, string 
reload(sys)

f = 'C:\\Users\\jamacwan\\Code\\Python\\Twitter API\\Sentiment Analysis\\vader_sentiment_lexicon.txt' 

def make_lex_dict(f):
    return dict(map(lambda (w, m): (w, float(m)), [wmsr.strip().split('\t')[0:2] for wmsr in open(f) ]))

WORD_VALENCE_DICT = make_lex_dict(f)

# empirically derived valence ratings for words, emoticons, slang, swear words, acronyms/initialisms 


##CONSTANTS#####

#(empirically derived mean sentiment intensity rating increase for booster words)
B_INCR = 0.293
B_DECR = -0.293

#(empirically derived mean sentiment intensity rating increase for using ALLCAPs to emphasize a word)
c_INCR = 0.733

# for removing punctuation
REGEX_REMOVE_PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation))

PUNC_LIST = [""."", ""!"", ""?"", "","", "";"", "":"", ""-"", ""'"", ""\"""",
                ""!!"", ""!!!"", ""??"", ""???"", ""?!?"", ""!?!"", ""?!?!"", ""!?!?""]

NEGATE = [""aint"", ""arent"", ""cannot"", ""cant"", ""couldnt"", ""darent"", ""didnt"", ""doesnt"",
              ""ain't"", ""aren't"", ""can't"", ""couldn't"", ""daren't"", ""didn't"", ""doesn't"",
              ""dont"", ""hadnt"", ""hasnt"", ""havent"", ""isnt"", ""mightnt"", ""mustnt"", ""neither"",
              ""don't"", ""hadn't"", ""hasn't"", ""haven't"", ""isn't"", ""mightn't"", ""mustn't"",
              ""neednt"", ""needn't"", ""never"", ""none"", ""nope"", ""nor"", ""not"", ""nothing"", ""nowhere"",
              ""oughtnt"", ""shant"", ""shouldnt"", ""uhuh"", ""wasnt"", ""werent"",
              ""oughtn't"", ""shan't"", ""shouldn't"", ""uh-uh"", ""wasn't"", ""weren't"",
              ""without"", ""wont"", ""wouldnt"", ""won't"", ""wouldn't"", ""rarely"", ""seldom"", ""despite""]

# booster/dampener 'intensifiers' or 'degree adverbs' http://en.wiktionary.org/wiki/Category:English_degree_adverbs

BOOSTER_DICT = {""absolutely"": B_INCR, ""amazingly"": B_INCR, ""awfully"": B_INCR, ""completely"": B_INCR, ""considerably"": B_INCR,
                ""decidedly"": B_INCR, ""deeply"": B_INCR, ""effing"": B_INCR, ""enormously"": B_INCR,
                ""entirely"": B_INCR, ""especially"": B_INCR, ""exceptionally"": B_INCR, ""extremely"": B_INCR,
                ""fabulously"": B_INCR, ""flipping"": B_INCR, ""flippin"": B_INCR,
                ""fricking"": B_INCR, ""frickin"": B_INCR, ""frigging"": B_INCR, ""friggin"": B_INCR, ""fully"": B_INCR, ""fucking"": B_INCR,
                ""greatly"": B_INCR, ""hella"": B_INCR, ""highly"": B_INCR, ""hugely"": B_INCR, ""incredibly"": B_INCR,
                ""intensely"": B_INCR, ""majorly"": B_INCR, ""more"": B_INCR, ""most"": B_INCR, ""particularly"": B_INCR,
                ""purely"": B_INCR, ""quite"": B_INCR, ""really"": B_INCR, ""remarkably"": B_INCR,
                ""so"": B_INCR,  ""substantially"": B_INCR,
                ""thoroughly"": B_INCR, ""totally"": B_INCR, ""tremendously"": B_INCR,
                ""uber"": B_INCR, ""unbelievably"": B_INCR, ""unusually"": B_INCR, ""utterly"": B_INCR,
                ""very"": B_INCR,
                ""almost"": B_DECR, ""barely"": B_DECR, ""hardly"": B_DECR, ""just enough"": B_DECR,
                ""kind of"": B_DECR, ""kinda"": B_DECR, ""kindof"": B_DECR, ""kind-of"": B_DECR,
                ""less"": B_DECR, ""little"": B_DECR, ""marginally"": B_DECR, ""occasionally"": B_DECR, ""partly"": B_DECR,
                ""scarcely"": B_DECR, ""slightly"": B_DECR, ""somewhat"": B_DECR,
                ""sort of"": B_DECR, ""sorta"": B_DECR, ""sortof"": B_DECR, ""sort-of"": B_DECR}

# check for special case idioms using a sentiment-laden keyword known to SAGE
SPECIAL_CASE_IDIOMS = {""the shit"": 3, ""the bomb"": 3, ""bad ass"": 1.5, ""yeah right"": -2,
                       ""cut the mustard"": 2, ""kiss of death"": -1.5, ""hand to mouth"": -2,
                       ""way to go"": 6}

def negated(list, nWords=[], includeNT=True):
    nWords.extend(NEGATE)
    for word in nWords:
        if word in list:
            return True
    if includeNT:
        for word in list:
            if ""n't"" in word:
                return True
    if ""least"" in list:
        i = list.index(""least"")
        if i &gt; 0 and list[i-1] != ""at"":
            return True
    return False

def normalize(score, alpha=15):
    # normalize the score to be between -1 and 1 using an alpha that approximates the max expected value
    normScore = score/math.sqrt( ((score*score) + alpha) )
    return normScore

def wildCardMatch(patternWithWildcard, listOfStringsToMatchAgainst):
    listOfMatches = fnmatch.filter(listOfStringsToMatchAgainst, patternWithWildcard)
    return listOfMatches


def isALLCAP_differential(wordList):
    countALLCAPS= 0
    for w in wordList:
        if w.isupper():
            countALLCAPS += 1
    cap_differential = len(wordList) - countALLCAPS
    if cap_differential &gt; 0 and cap_differential &lt; len(wordList):
        isDiff = True
    else: isDiff = False
    return isDiff

#check if the preceding words increase, decrease, or negate/nullify the valence
def scalar_inc_dec(word, valence, isCap_diff):
    scalar = 0.0
    word_lower = word.lower()
    if word_lower in BOOSTER_DICT:
        scalar = BOOSTER_DICT[word_lower]
        if valence &lt; 0: scalar *= -1
        #check if booster/dampener word is in ALLCAPS (while others aren't)
        if word.isupper() and isCap_diff:
            if valence &gt; 0: scalar += c_INCR
            else:  scalar -= c_INCR
    return scalar

def sentiment(text):
    """"""
    Returns a float for sentiment strength based on the input text.
    Positive values are positive valence, negative value are negative valence.
    """"""
    if not isinstance(text, unicode) and not isinstance(text, str):
        text = str(text)

    wordsAndEmoticons = text.split() #doesn't separate words from adjacent punctuation (keeps emoticons &amp; contractions)
    text_mod = REGEX_REMOVE_PUNCTUATION.sub('', text) # removes punctuation (but loses emoticons &amp; contractions)
    wordsOnly = text_mod.split()
    # get rid of empty items or single letter ""words"" like 'a' and 'I' from wordsOnly
    for word in wordsOnly:
        if len(word) &lt;= 1:
            wordsOnly.remove(word)    
    # now remove adjacent &amp; redundant punctuation from [wordsAndEmoticons] while keeping emoticons and contractions

    for word in wordsOnly:
        for p in PUNC_LIST:
            pword = p + word
            x1 = wordsAndEmoticons.count(pword)
            while x1 &gt; 0:
                i = wordsAndEmoticons.index(pword)
                wordsAndEmoticons.remove(pword)
                wordsAndEmoticons.insert(i, word)
                x1 = wordsAndEmoticons.count(pword)

            wordp = word + p
            x2 = wordsAndEmoticons.count(wordp)
            while x2 &gt; 0:
                i = wordsAndEmoticons.index(wordp)
                wordsAndEmoticons.remove(wordp)
                wordsAndEmoticons.insert(i, word)
                x2 = wordsAndEmoticons.count(wordp)

    # get rid of residual empty items or single letter ""words"" like 'a' and 'I' from wordsAndEmoticons
    for word in wordsAndEmoticons:
        if len(word) &lt;= 1:
            wordsAndEmoticons.remove(word)

    # remove stopwords from [wordsAndEmoticons]
    #stopwords = [str(word).strip() for word in open('stopwords.txt')]
    #for word in wordsAndEmoticons:
    #    if word in stopwords:
    #        wordsAndEmoticons.remove(word)

    # check for negation

    isCap_diff = isALLCAP_differential(wordsAndEmoticons)

    sentiments = []
    for item in wordsAndEmoticons:
        v = 0
        i = wordsAndEmoticons.index(item)
        if (i &lt; len(wordsAndEmoticons)-1 and item.lower() == ""kind"" and \
           wordsAndEmoticons[i+1].lower() == ""of"") or item.lower() in BOOSTER_DICT:
            sentiments.append(v)
            continue
        item_lowercase = item.lower()
        if item_lowercase in WORD_VALENCE_DICT:
            #get the sentiment valence
            v = float(WORD_VALENCE_DICT[item_lowercase])

            #check if sentiment laden word is in ALLCAPS (while others aren't)

            if item.isupper() and isCap_diff:
                if v &gt; 0: v += c_INCR
                else: v -= c_INCR


            n_scalar = -0.74
            if i &gt; 0 and wordsAndEmoticons[i-1].lower() not in WORD_VALENCE_DICT:
                s1 = scalar_inc_dec(wordsAndEmoticons[i-1], v,isCap_diff)
                v = v+s1
                if negated([wordsAndEmoticons[i-1]]): v = v*n_scalar
            if i &gt; 1 and wordsAndEmoticons[i-2].lower() not in WORD_VALENCE_DICT:
                s2 = scalar_inc_dec(wordsAndEmoticons[i-2], v,isCap_diff)
                if s2 != 0: s2 = s2*0.95
                v = v+s2
                # check for special use of 'never' as valence modifier instead of negation
                if wordsAndEmoticons[i-2] == ""never"" and (wordsAndEmoticons[i-1] == ""so"" or wordsAndEmoticons[i-1] == ""this""): 
                    v = v*1.5                    
                # otherwise, check for negation/nullification
                elif negated([wordsAndEmoticons[i-2]]): v = v*n_scalar
            if i &gt; 2 and wordsAndEmoticons[i-3].lower() not in WORD_VALENCE_DICT:
                s3 = scalar_inc_dec(wordsAndEmoticons[i-3], v,isCap_diff)
                if s3 != 0: s3 = s3*0.9
                v = v+s3
                # check for special use of 'never' as valence modifier instead of negation
                if wordsAndEmoticons[i-3] == ""never"" and \
                   (wordsAndEmoticons[i-2] == ""so"" or wordsAndEmoticons[i-2] == ""this"") or \
                   (wordsAndEmoticons[i-1] == ""so"" or wordsAndEmoticons[i-1] == ""this""):
                    v = v*1.25
                # otherwise, check for negation/nullification
                elif negated([wordsAndEmoticons[i-3]]): v = v*n_scalar


                # future work: consider other sentiment-laden idioms
                #other_idioms = {""back handed"": -2, ""blow smoke"": -2, ""blowing smoke"": -2, ""upper hand"": 1, ""break a leg"": 2, 
                #                ""cooking with gas"": 2, ""in the black"": 2, ""in the red"": -2, ""on the ball"": 2,""under the weather"": -2}

                onezero = u""{} {}"".format(wordsAndEmoticons[i-1], wordsAndEmoticons[i])
                twoonezero = u""{} {} {}"".format(wordsAndEmoticons[i-2], wordsAndEmoticons[i-1], wordsAndEmoticons[i])
                twoone = u""{} {}"".format(wordsAndEmoticons[i-2], wordsAndEmoticons[i-1])
                threetwoone = u""{} {} {}"".format(wordsAndEmoticons[i-3], wordsAndEmoticons[i-2], wordsAndEmoticons[i-1])
                threetwo = u""{} {}"".format(wordsAndEmoticons[i-3], wordsAndEmoticons[i-2])
                if onezero in SPECIAL_CASE_IDIOMS:
                    v = SPECIAL_CASE_IDIOMS[onezero]
                elif twoonezero in SPECIAL_CASE_IDIOMS:
                    v = SPECIAL_CASE_IDIOMS[twoonezero]
                elif twoone in SPECIAL_CASE_IDIOMS:
                    v = SPECIAL_CASE_IDIOMS[twoone]
                elif threetwoone in SPECIAL_CASE_IDIOMS:
                    v = SPECIAL_CASE_IDIOMS[threetwoone]
                elif threetwo in SPECIAL_CASE_IDIOMS:
                    v = SPECIAL_CASE_IDIOMS[threetwo]
                if len(wordsAndEmoticons)-1 &gt; i:
                    zeroone = u""{} {}"".format(wordsAndEmoticons[i], wordsAndEmoticons[i+1])
                    if zeroone in SPECIAL_CASE_IDIOMS:
                        v = SPECIAL_CASE_IDIOMS[zeroone]
                if len(wordsAndEmoticons)-1 &gt; i+1:
                    zeroonetwo = u""{} {}"".format(wordsAndEmoticons[i], wordsAndEmoticons[i+1], wordsAndEmoticons[i+2])
                    if zeroonetwo in SPECIAL_CASE_IDIOMS:
                        v = SPECIAL_CASE_IDIOMS[zeroonetwo]

                # check for booster/dampener bi-grams such as 'sort of' or 'kind of'
                if threetwo in BOOSTER_DICT or twoone in BOOSTER_DICT:
                    v = v+B_DECR

            # check for negation case using ""least""
            if i &gt; 1 and wordsAndEmoticons[i-1].lower() not in WORD_VALENCE_DICT \
                and wordsAndEmoticons[i-1].lower() == ""least"":
                if (wordsAndEmoticons[i-2].lower() != ""at"" and wordsAndEmoticons[i-2].lower() != ""very""):
                    v = v*n_scalar
            elif i &gt; 0 and wordsAndEmoticons[i-1].lower() not in WORD_VALENCE_DICT \
                and wordsAndEmoticons[i-1].lower() == ""least"":
                v = v*n_scalar
        sentiments.append(v) 

    # check for modification in sentiment due to contrastive conjunction 'but'
    if 'but' in wordsAndEmoticons or 'BUT' in wordsAndEmoticons:
        try: bi = wordsAndEmoticons.index('but')
        except: bi = wordsAndEmoticons.index('BUT')
        for s in sentiments:
            si = sentiments.index(s)
            if si &lt; bi: 
                sentiments.pop(si)
                sentiments.insert(si, s*0.5)
            elif si &gt; bi: 
                sentiments.pop(si)
                sentiments.insert(si, s*1.5) 

    if sentiments:                      
        sum_s = float(sum(sentiments))
        #print sentiments, sum_s

        # check for added emphasis resulting from exclamation points (up to 4 of them)
        ep_count = text.count(""!"")
        if ep_count &gt; 4: ep_count = 4
        ep_amplifier = ep_count*0.292 #(empirically derived mean sentiment intensity rating increase for exclamation points)
        if sum_s &gt; 0:  sum_s += ep_amplifier
        elif  sum_s &lt; 0: sum_s -= ep_amplifier

        # check for added emphasis resulting from question marks (2 or 3+)
        qm_count = text.count(""?"")
        qm_amplifier = 0
        if qm_count &gt; 1:
            if qm_count &lt;= 3: qm_amplifier = qm_count*0.18
            else: qm_amplifier = 0.96
            if sum_s &gt; 0:  sum_s += qm_amplifier
            elif  sum_s &lt; 0: sum_s -= qm_amplifier

        compound = normalize(sum_s)

        # want separate positive versus negative sentiment scores
        pos_sum = 0.0
        neg_sum = 0.0
        neu_count = 0
        for sentiment_score in sentiments:
            if sentiment_score &gt; 0:
                pos_sum += (float(sentiment_score) +1) # compensates for neutral words that are counted as 1
            if sentiment_score &lt; 0:
                neg_sum += (float(sentiment_score) -1) # when used with math.fabs(), compensates for neutrals
            if sentiment_score == 0:
                neu_count += 1

        if pos_sum &gt; math.fabs(neg_sum): pos_sum += (ep_amplifier+qm_amplifier)
        elif pos_sum &lt; math.fabs(neg_sum): neg_sum -= (ep_amplifier+qm_amplifier)

        total = pos_sum + math.fabs(neg_sum) + neu_count
        pos = math.fabs(pos_sum / total)
        neg = math.fabs(neg_sum / total)
        neu = math.fabs(neu_count / total)

    else:
        compound = 0.0; pos = 0.0; neg = 0.0; neu = 0.0

    s = {""neg"" : round(neg, 3), 
         ""neu"" : round(neu, 3),
         ""pos"" : round(pos, 3),
         ""compound"" : round(compound, 4)}
    return s


if __name__ == '__main__':
    # --- examples -------
    sentences = [
                u""VADER is smart, handsome, and funny."",       # positive sentence example
                u""VADER is smart, handsome, and funny!"",       # punctuation emphasis handled correctly (sentiment intensity adjusted)
                u""VADER is very smart, handsome, and funny."",  # booster words handled correctly (sentiment intensity adjusted)
                u""VADER is VERY SMART, handsome, and FUNNY."",  # emphasis for ALLCAPS handled
                u""VADER is VERY SMART, handsome, and FUNNY!!!"",# combination of signals - VADER appropriately adjusts intensity
                u""VADER is VERY SMART, really handsome, and INCREDIBLY FUNNY!!!"",# booster words &amp; punctuation make this close to ceiling for score
                u""The book was good."",         # positive sentence
                u""The book was kind of good."", # qualified positive sentence is handled correctly (intensity adjusted)
                u""The plot was good, but the characters are uncompelling and the dialog is not great."", # mixed negation sentence
                u""A really bad, horrible book."",       # negative sentence with booster words
                u""At least it isn't a horrible book."", # negated negative sentence with contraction
                u"":) and :D"",     # emoticons handled
                u"""",              # an empty string is correctly handled
                u""Today sux"",     #  negative slang handled
                u""Today sux!"",    #  negative slang with punctuation emphasis handled
                u""Today SUX!"",    #  negative slang with capitalization emphasis
                u""Today kinda sux! But I'll get by, lol"" # mixed sentiment example with slang and constrastive conjunction ""but""
                 ]
    paragraph = ""It was one of the worst movies I've seen, despite good reviews. \
    Unbelievably bad acting!! Poor direction. VERY poor production. \
    The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!""

    from nltk import tokenize
    lines_list = tokenize.sent_tokenize(paragraph)
    sentences.extend(lines_list)

    tricky_sentences = [
                        ""Most automated sentiment analysis tools are shit."",
                        ""VADER sentiment analysis is the shit."",
                        ""Sentiment analysis has never been good."",
                        ""Sentiment analysis with VADER has never been this good."",
                        ""Warren Beatty has never been so entertaining."",
                        ""I won't say that the movie is astounding and I wouldn't claim that the movie is too banal either."",
                        ""I like to hate Michael Bay films, but I couldn't fault this one"",
                        ""It's one thing to watch an Uwe Boll film, but another thing entirely to pay for it"",
                        ""The movie was too good"",
                        ""This movie was actually neither that funny, nor super witty."",
                        ""This movie doesn't care about cleverness, wit or any other kind of intelligent humor."",
                        ""Those who find ugly meanings in beautiful things are corrupt without being charming."",
                        ""There are slow and repetitive parts, BUT it has just enough spice to keep it interesting."",
                        ""The script is not fantastic, but the acting is decent and the cinematography is EXCELLENT!"", 
                        ""Roger Dodger is one of the most compelling variations on this theme."",
                        ""Roger Dodger is one of the least compelling variations on this theme."",
                        ""Roger Dodger is at least compelling as a variation on the theme."",
                        ""they fall in love with the product"",
                        ""but then it breaks"",
                        ""usually around the time the 90 day warranty expires"",
                        ""the twin towers collapsed today"",
                        ""However, Mr. Carter solemnly argues, his client carried out the kidnapping under orders and in the ''least offensive way possible.''""
                        ]
    sentences.extend(tricky_sentences)
    for sentence in sentences:
        print sentence
        ss = sentiment(sentence)
        print ""\t"" + str(ss)

    print ""\n\n Done!""
</code></pre>
","python, sentiment-analysis, text-classification","<p>The code has several problems:</p>

<ol>
<li><p>Special cases works only for words in <code>vader_sentiment_lexicon.txt</code> because:</p>

<pre><code>if item_lowercase in WORD_VALENCE_DICT:
    #get the sentiment valence
    ...
    if onezero in SPECIAL_CASE_IDIOMS:
        v = SPECIAL_CASE_IDIOMS[onezero]
...
</code></pre>

<p>If you change you phrase to contain such a word, for example 'abandon', then this passes ok.
How to fix: </p>

<pre><code>if item_lowercase in WORD_VALENCE_DICT:
    #get the sentiment valence
    v = float(WORD_VALENCE_DICT[item_lowercase])
else:
    v = 0
#move next statements out of if
#check if sentiment laden word is in ALLCAPS (while others aren't)
if item.isupper() and isCap_diff:
        if v &gt; 0: v += c_INCR
        else: v -= c_INCR
</code></pre>

<p>plus some fixes inside.</p></li>
<li><p>Special cases are checked only if special word has at least 3rd position (index > 2).</p>

<pre><code>if i &gt; 0 and wordsAndEmoticons[i-1].lower() not in WORD_VALENCE_DICT:
    ... # no SPECIAL_CASE_IDIOMS
if i &gt; 1 and wordsAndEmoticons[i-2].lower() not in WORD_VALENCE_DICT:
    ... # no SPECIAL_CASE_IDIOMS
if i &gt; 2 and wordsAndEmoticons[i-3].lower() not in WORD_VALENCE_DICT:
    ...
    twoonezero = u""{} {} {}"".format(wordsAndEmoticons[i-2], wordsAndEmoticons[i-1], wordsAndEmoticons[i])
    ...
    elif twoonezero in SPECIAL_CASE_IDIOMS: ...
</code></pre>

<p>Here for phrase <code>way to abandon John</code> word <code>abandon</code> has index 2, but there's no such case. If we change phrase to <code>you way to abandon John</code>, then it starts working.
How to fix: move SPECIAL cases up one branch. Or better use real length of special case, than try to hardcode.</p></li>
</ol>

<p>Resume: code will not be easy in support.</p>
",3,5,2850,2015-12-21 16:43:58,https://stackoverflow.com/questions/34400485/adding-special-case-idioms-to-python-vader-sentiment
Error inserting/retrieving tweets into mongolite db,"<p>I am trying to perform sentiment analysis on the tweets that were already fetched and stored in MongoDb. After fetching the tweets which is in dataframe format, i am getting the following error: </p>

<pre><code>ip.txt=laply(ip.lst,function(t) t$getText())
Error in t$getText : $ operator is invalid for atomic vectors
</code></pre>

<p>The entire code is given below: </p>

<pre><code>iphone.tweets &lt;- searchTwitter('#iphone', n=15, lang=""en"")
iphone.text=laply(iphone.tweets,function(t) t$getText())
df_ip &lt;- as.data.frame(iphone.text)

m &lt;- mongo(""iphonecollection"",db=""project"")
m$insert(df_ip)
df_ip&lt;-m$find()
ip.lst&lt;-as.list(t(df_ip))
ip.txt=laply(ip.lst,function(t) t$getText())
</code></pre>

<p>What I wish to do is to calculate the sentiment scores as follows:</p>

<pre><code>iphone.scores &lt;- score.sentiment(ip.txt, pos.words,neg.words, .progress='text')
</code></pre>

<p>score.sentiment routine is as follows: </p>

<pre><code>  score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
  require(plyr)
  require(stringr)
   # we got a vector of sentences. plyr will handle a list or a vector as an ""l"" for us
   # we want a simple array of scores back, so we use ""l"" + ""a"" + ""ply"" = laply:
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    # clean up sentences with R's regex-driven global substitute, gsub():
    sentence = gsub('[[:punct:]]', '', sentence)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)
    # and convert to lower case:
    sentence = tolower(sentence)
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    # compare our words to the dictionaries of positive &amp; negative terms
    pos.matches = match(words, pos.words)
    neg.matches = match(words, neg.words)
    # match() returns the position of the matched term or NA
    # we just want a TRUE/FALSE:
    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)
    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)
    return(score)
   }, pos.words, neg.words, .progress=.progress )
   scores.df = data.frame(score=scores, text=sentences)
   return(scores.df)
 } 
</code></pre>
","r, mongodb, twitter, sentiment-analysis, mongolite","<p>I think you wanted to use <code>sapply</code>, which flattens the list of status object that <code>searchTwitter</code> returns. In any case this works. Note that you need to install and then start <code>MongoDB</code> for this to work:</p>

<pre><code>library(twitteR)
library(plyr)
library(stringr)
library(mongolite)

# you have to set up a Twitter Application at https://dev.twitter.com/ to get these 
#
ntoget &lt;- 600 # get 600 tweets

iphone.tweets &lt;- searchTwitter('#iphone', n=ntoget, lang=""en"")
iphone.text &lt;- sapply(iphone.tweets,function(t) t$getText())
df_ip &lt;- as.data.frame(iphone.text)

# MongoDB must be installed and the service started (mongod.exe in Windows)
#
m &lt;- mongo(""iphonecollection"",db=""project"")
m$insert(df_ip)
df_ip_out&lt;-m$find()

# Following routine (score.sentiment) was copied from:
# http://stackoverflow.com/questions/32395098/r-sentiment-analysis-with-phrases-in-dictionaries
#
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
  require(plyr)  
  require(stringr)  
  # we got a vector of sentences. plyr will handle a list  
  # or a vector as an ""l"" for us  
  # we want a simple array (""a"") of scores back, so we use  
  # ""l"" + ""a"" + ""ply"" = ""laply"":  
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    # clean up sentences with R's regex-driven global substitute, gsub():
    sentence = gsub('[[:punct:]]', '', sentence)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)    
    # and convert to lower case:    
    sentence = tolower(sentence)    
    # split into words. str_split is in the stringr package    
    word.list = str_split(sentence, '\\s+')    
    # sometimes a list() is one level of hierarchy too much    
    words = unlist(word.list)    
    # compare our words to the dictionaries of positive &amp; negative terms
    pos.matches = match(words, pos)
    neg.matches = match(words, neg)   
    # match() returns the position of the matched term or NA    
    # we just want a TRUE/FALSE:    
    pos.matches = !is.na(pos.matches)   
    neg.matches = !is.na(neg.matches)   
    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)    
    return(score)    
  }, pos.words, neg.words, .progress=.progress )  
  scores.df = data.frame(score=scores, text=sentences)  
  return(scores.df)  
}

tweets &lt;- as.character(df_ip_out$iphone.text)
neg = c(""bad"",""prank"",""inferior"",""evil"",""poor"",""minor"")
pos = c(""good"",""great"",""superior"",""excellent"",""positive"",""super"",""better"")
analysis &lt;- score.sentiment(tweets,pos,neg)
table(analysis$score)
</code></pre>

<p>Yields the following (4 scored bad, 592 scored neutral, 4 scored good):</p>

<pre><code> -1   0   1 
  4 592   4 
</code></pre>
",1,1,218,2015-12-26 06:56:30,https://stackoverflow.com/questions/34469227/error-inserting-retrieving-tweets-into-mongolite-db
Getting &quot;AttributeError&quot; while printing stopwords from nltk toolbox in python,"<pre><code>from nltk import stopwords 
print(stopwords.words('english'))
</code></pre>

<p>Second line is giving error which is</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#16&gt;"", line 1, in &lt;module&gt;
    print(stopwords.words('english'))
AttributeError: 'module' object has no attribute 'words'
</code></pre>

<p>I have installed nltk using pip install nltk then i downloaded stopwords using command nltk.download('stopwords'), after unzipping that ""stopwords"" folder I placed that in python34/lib/site-packeges/nltk/stopwords</p>

<p>How can i get stopwords from this?</p>
","python, nltk, sentiment-analysis, stop-words","<p>As mentioned in the comment, try </p>

<pre><code>from nltk.corpus import stopwords
</code></pre>

<p>That should fix the error.</p>
",2,-1,4502,2016-01-03 07:34:29,https://stackoverflow.com/questions/34574340/getting-attributeerror-while-printing-stopwords-from-nltk-toolbox-in-python
Java - Implementing Machine Learning methods on text mining,"<p>I have some texts and i would like to mine these by implementing <strong>Machine Learning</strong> methods in Java using Weka libraries. For that purpose, i've already did something so far but since whole code is too long i just want to show some key methods and get an idea about how to train and test my dataset, and interpret results etc.</p>

<p>FYI, i am processing tweets with Twitter4J.</p>

<p>First, i fetched the tweets and saved in text file(of course in ARFF format). Then I manually labeled them regarding their sentiments(positive,neutral,negative). Based on selected classifier, i created test set from my training set thanks to <strong>cross-validation</strong>. Finally i classified them and print the summary and confusion matrix.  </p>

<p>Here is one of my classifiers:  Naive Bayes code:</p>

<pre><code>public static void ApplyNaiveBayes(Instances data) throws Exception {

    System.out.println(""Applying Naive Bayes \n"");
    data.setClassIndex(data.numAttributes() - 1); 
    StringToWordVector swv = new StringToWordVector();
    swv.setInputFormat(data);
    Instances dataFiltered = Filter.useFilter(data, swv);
    //System.out.println(""Filtered data "" +dataFiltered.toString());

    System.out.println(""\n\nFiltered data:\n\n"" + dataFiltered);

    Instances[][] split = crossValidationSplit(dataFiltered, 10);
    Instances[] trainingSets = split[0];
    Instances[] testingSets = split[1];


    NaiveBayes classifier = new NaiveBayes(); 

    FastVector predictions = new FastVector();


    classifier.buildClassifier(dataFiltered);
    System.out.println(""\n\nClassifier model:\n\n"" + classifier);     

    // Test the model
    for (int i = 0; i &lt; trainingSets.length; i++) {
        classifier.buildClassifier(trainingSets[i]);
        // Test the model         
        Evaluation eTest = new Evaluation(trainingSets[i]);
        eTest.evaluateModel(classifier, testingSets[i]);

        // Print the result to the Weka explorer:
        String strSummary = eTest.toSummaryString();
        System.out.println(strSummary);

        // Get the confusion matrix
        double[][] cmMatrix = eTest.confusionMatrix();
        for(int row_i=0; row_i&lt;cmMatrix.length; row_i++){
            for(int col_i=0; col_i&lt;cmMatrix.length; col_i++){
                System.out.print(cmMatrix[row_i][col_i]);
                System.out.print(""|"");
            }
            System.out.println();
        }
    }
}
</code></pre>

<p>And FYI, crossValidationSplit method is here:</p>

<pre><code>    public static Instances[][] crossValidationSplit(Instances data, int     
    numberOfFolds) {
        Instances[][] split = new Instances[2][numberOfFolds];

        for (int i = 0; i &lt; numberOfFolds; i++) {
            split[0][i] = data.trainCV(numberOfFolds, i);
            split[1][i] = data.testCV(numberOfFolds, i);
        }

        return split;
    }
</code></pre>

<p>In the end, I've got 10 different results(because k=10). One of them is:</p>

<pre><code>  Correctly Classified Instances           4               36.3636 %
  Incorrectly Classified Instances         7               63.6364 %
  Kappa statistic                          0.0723
  Mean absolute error                      0.427 
  Root mean squared error                  0.5922
  Relative absolute error                 93.4946 %
  Root relative squared error            116.5458 %
  Total Number of Instances               11     

  2.0|0.0|1.0|
  1.0|1.0|2.0|
  3.0|0.0|1.0|
</code></pre>

<p>So, how i can i interpret the results? Do you think i'm doing right about training and test sets? 
I want to obtain given text file's sentiment percent (positive,neutral,negative). How to infer my demand from these results?
Thanks for reading...  </p>
","java, machine-learning, weka, text-mining, sentiment-analysis","<p>Unfortunately your code is a bit confused. </p>

<p>First of all, you train your model on the full set of your set:</p>

<pre><code>classifier.buildClassifier(dataFiltered);
</code></pre>

<p>then you retrain your model inside your for loop:</p>

<pre><code>for (int i = 0; i &lt; trainingSets.length; i++) {
    classifier.buildClassifier(trainingSets[i]);
    ...
 }
</code></pre>

<p>than you calculate the confusion mtx too. I think it is unnecessary. </p>

<p>In my opinion you need to apply <code>Evaluation.crossValidateModel()</code> method as the follows:
    <code>
    //set the class index
    dataFiltered.setClassIndex(dataFiltered.numAttributes() - 1);
    //build a model -- choose a classifier as you want
    classifier.buildClassifier(dataFiltered);
    Evaluation eval = new Evaluation(dataFiltered);
    eval.crossValidateModel(classifier, dataFiltered, 10, new Random(1));
    //print stats -- do not require to calculate confusion mtx, weka do it!
    System.out.println(classifier);
    System.out.println(eval.toSummaryString());
    System.out.println(eval.toMatrixString());
    System.out.println(eval.toClassDetailsString());
</code></p>
",3,4,1118,2016-01-03 11:26:23,https://stackoverflow.com/questions/34575976/java-implementing-machine-learning-methods-on-text-mining
How can I compare Stanford Core NLP with an algorithm without the trained dataset for the movies?,"<p>I am looking to use the Stanford Core NLP which is trained for movie reviews. I want to compare it with a regular sentiment analysis algorithm which is untrained for movie reviews. Is there a way to use the Stanford Core NLP <a href=""http://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow"">Link Here</a> without the trained dataset for the movie reviews or is there any other source I can use to compare directly? </p>
","java, nlp, stanford-nlp, sentiment-analysis","<p>It's unclear what you mean by CoreNLP Sentiment without a trained movie review dataset. The Sentiment model needs some dataset to train on -- untrained it will simply not do anything. It is possible to train it on a dataset which is not movie reviews, but for this you'd have to find or create a suitable dataset. </p>
",0,0,672,2016-01-05 09:11:21,https://stackoverflow.com/questions/34607867/how-can-i-compare-stanford-core-nlp-with-an-algorithm-without-the-trained-datase
sentiment analysis with R,"<p>I'm doing sentiment analysis with a list of words correspond to a score range from 1-8 instead of counting positive word as 1 and negative word as -1. </p>

<p>here is part of the list:</p>

<pre><code>word            score   
laughter        8.50    
happiness       8.44    
love            8.42    
happy           8.30    
laughed         8.26    
laugh           8.22
</code></pre>

<p>How can I apply this list to the sentiment.score function so that I will have the score * word count instead of word count only</p>

<pre><code>score.sentiment = function(sentences, new_list, .progress='none')
{
        require(plyr)
        require(stringr)

        # we got a vector of sentences. plyr will handle a list or a vector as an ""l"" for us
        # we want a simple array of scores back, so we use ""l"" + ""a"" + ""ply"" = laply:
        scores = laply(sentences, function(sentence, terms) {

                # clean up sentences with R's regex-driven global substitute, gsub():
                sentence = gsub('[[:punct:]]', '', sentence)
                sentence = gsub('[[:cntrl:]]', '', sentence)
                sentence = gsub('\\d+', '', sentence)
                # and convert to lower case:
                sentence = tolower(sentence)

                # split into words. str_split is in the stringr package
                word.list = str_split(sentence, '\\s+')
                # sometimes a list() is one level of hierarchy too much
                words = unlist(word.list)

                # compare our words to the dictionaries of positive &amp; negative terms
                words.matches = match(words, terms)

                # match() returns the position of the matched term or NA
                # we just want a TRUE/FALSE:
                words.matches = !is.na(words.matches)

                # how to count the score??
                score = ?????

                return(score)
        }, terms, .progress=.progress )

        scores.df = data.frame(score=scores, text=sentences)
        return(scores.df)
    }
</code></pre>
","r, sentiment-analysis","<p>Here's an example:</p>

<pre><code>df &lt;- read.table(header=TRUE, text=""word            score   
laughter        8.50    
happiness       8.44    
love            8.42    
happy           8.30    
laughed         8.26    
laugh           8.22"")
sentence &lt;- ""I love happiness""

words &lt;- strsplit(sentence, ""\\s+"")[[1]]
score &lt;- sum(df$score[match(words, df$word)], na.rm = TRUE)

print(score)
# [1] 16.86
</code></pre>
",2,1,863,2016-01-10 13:05:02,https://stackoverflow.com/questions/34705704/sentiment-analysis-with-r
AttributeError: &#39;float&#39; object has no attribute &#39;lower&#39;,"<p>I'm facing this attribute error and I'm stuck at how to handle float values if they appear in a tweet.The streaming tweet has to be lower cased and tokenized so i have used split function.</p>

<p>Can somebody please help me to deal with it, any workaround or solution ..?</p>

<p>Here's the <strong><em>error</em></strong> which m gettin....</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-28-fa278f6c3171&gt; in &lt;module&gt;()
      1 stop_words = []
----&gt; 2 negfeats = [(word_feats(x for x in p_test.SentimentText[f].lower().split() if x not in stop_words), 'neg') for f in l]
      3 posfeats = [(word_feats(x for x in p_test.SentimentText[f].lower().split() if x not in stop_words), 'pos') for f in p]
      4 
      5 trainfeats = negfeats+ posfeats

AttributeError: 'float' object has no attribute 'lower'
</code></pre>

<hr>

<p><strong><em>Here is my code</em></strong> </p>

<pre><code>p_test = pd.read_csv('TrainSA.csv')

stop_words = [ ]

def word_feats(words):

    return dict([(word, True) for word in words])


l = [ ]

for f in range(len(p_test)):

    if p_test.Sentiment[f] == 0:

        l.append(f)



p = [ ]

for f in range(len(p_test)):

    if p_test.Sentiment[f] == 1:

        p.append(f) 




negfeats = [(word_feats(x for x in p_test.SentimentText[f].lower().split() if x not in stop_words), 'neg') for f in l]

posfeats = [(word_feats(x for x in p_test.SentimentText[f].lower().split() if x not in stop_words), 'pos') for f in p]


trainfeats = negfeats+ posfeats

print len(trainfeats)


import random 

random.shuffle(trainfeats)

print(len(trainfeats))




p_train = pd.read_csv('TrainSA.csv')


l_t = []

for f in range(len(p_train)):

    if p_train.Sentiment[f] == 0:

        l_t.append(f)


p_t = []

for f in range(len(p_train)):

    if p_train.Sentiment[f] == 1:

        p_t.append(f)        

print len(l_t)

print len(p_t)
</code></pre>

<hr>

<p>I tried many ways but still not able to get them to use lower and split function.</p>
","python, twitter, sentiment-analysis, tweets","<p>Thank you @Dick Kniep. Yes,it is Pandas CSV reader. Your suggestion worked.
Following is the python code which worked for me by specifying the field datatype,
(in this case, its string)</p>

<pre><code>p_test = pd.read_csv('TrainSA.csv')
p_test.SentimentText=p_test.SentimentText.astype(str)
</code></pre>
",44,20,80302,2016-01-11 14:45:31,https://stackoverflow.com/questions/34724246/attributeerror-float-object-has-no-attribute-lower
TypeError: classify() missing 1 required positional argument: &#39;featureset&#39;,"<p>here is my code where i invoke method classify():</p>

<pre><code>def sentiment(text):
    feats = find_features(text)
    return voted_classifier.classify(feats),voted_classifier.confidence(feats)
</code></pre>

<p>Definition of find_features() method:</p>

<pre><code>def find_features(document):
    words = word_tokenize(document)
    features = {}
    for w in word_features:
        features[w] = (w in words)

    return features
</code></pre>

<p>i get error:</p>

<pre><code>TypeError: classify() missing 1 required positional argument: 'featureset'
</code></pre>

<p>where featuresets is:</p>

<pre><code>featuresets_f = open(""pickled_algos/featuresets.pickle"", ""rb"")
featuresets = pickle.load(featuresets_f)
featuresets_f.close()

random.shuffle(featuresets)
print(len(featuresets))

testing_set = featuresets[8000:]
training_set = featuresets[:8000]
</code></pre>

<p>(Note : i am doing twitter sentiment analysis using Python 3.4 ,nltk on Ubuntu 14.04)</p>
","python, ubuntu, nltk, sentiment-analysis","<p>I suspect you have not trained your classifier. Note the following error:</p>

<pre><code>&gt;&gt;&gt; from nltk import NaiveBayesClassifier  # for example
&gt;&gt;&gt; NaiveBayesClassifier.classify(feats)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: classify() missing 1 required positional argument: 'featureset'
</code></pre>

<p>You need to train it first:</p>

<pre><code>&gt;&gt;&gt; classifier = NaiveBayesClassifier.train(training_set)
</code></pre>

<p>Then you can classify features:</p>

<pre><code>&gt;&gt;&gt; classifier.classify(feats)  # feats == a dict of features
</code></pre>
",3,4,1752,2016-01-23 06:17:06,https://stackoverflow.com/questions/34960312/typeerror-classify-missing-1-required-positional-argument-featureset
Sentiment Analysis - What does annotating dataset mean?,"<p>I'm currently working on my final year research project, which is an application which analyzes travel reviews found online, and give out a sentiment score for particular tourist attractions as a result, by conducting aspect level sentiment analysis.</p>

<p>I have a newly scraped dataset from a famous travel website which does not allow to use their API for research/academic purposes. (bummer)</p>

<p>My supervisor said that I might need to get this dataset annotated before using it for the aforementioned purpose. I am kind of confused as to what data annotation means in this context. Could someone please explain what exactly is happening when a dataset is annotated and how it helps in getting sentiment analysis done?</p>

<p>I was told that I might have to get two/three human annotators and get the data annotated to make it less biased. I'm on a tight schedule and I was wondering if there are any tools that can get it done for me? If so, what will be the impact of using such tools over human annotators? I would also like suggestions for such tools that you would recommend.</p>

<p>I would really appreciate a detailed explanation to my questions, as I am stuck with my project progressing to the next step because of this.</p>

<p>Thank you in advance.</p>
","annotations, dataset, nlp, stanford-nlp, sentiment-analysis","<p>To a first approximation, machine learning algorithms (e.g., a sentiment analysis algorithm) is learning to perform a task that humans currently perform by collecting many examples of the human performing the task, and then imitating them. When your supervisor talks about ""annotation,"" they're talking about collecting these examples of a human doing the sentiment annotation task: annotating a sentence for sentiment. That is, collecting pairs of sentences and their sentiment as judged by humans. Without this, there's nothing for the program to learn from, and you're stuck hoping the program can give you something from nothing -- which it never will.</p>

<p>That said, there are tools for collecting this sort of data, or at least helping. Amazon Mechanical Turk and other crowdsourcing platforms are good resources for this sort of data collection. You can also take a look at something like: <a href=""http://www.crowdflower.com/type-sentiment-analysis"" rel=""nofollow"">http://www.crowdflower.com/type-sentiment-analysis</a>.</p>
",2,1,894,2016-01-29 15:02:09,https://stackoverflow.com/questions/35087563/sentiment-analysis-what-does-annotating-dataset-mean
Graphlab: How to avoid manually duplicating functions that has only a different string variable?,"<p>I imported my dataset with SFrame:</p>

<pre><code>products = graphlab.SFrame('amazon_baby.gl')
products['word_count'] = graphlab.text_analytics.count_words(products['review'])
</code></pre>

<p>I would like to do sentiment analysis on a set of words shown below:</p>

<pre><code>selected_words = ['awesome', 'great', 'fantastic', 'amazing', 'love', 'horrible', 'bad', 'terrible', 'awful', 'wow', 'hate']
</code></pre>

<p>Then I would like to create a new column for each of the selected words in the products matrix and the entry is the number of times such word occurs, so I created a function for the word ""awesome"":</p>

<pre><code>def awesome_count(word_count):
    if 'awesome' in product:
        return product['awesome']
    else:
        return 0;

products['awesome'] = products['word_count'].apply(awesome_count)
</code></pre>

<p>so far so good, but I need to manually create other functions for each of the selected words in this way, e.g., great_count, etc. How to avoid this manual effort and write cleaner code?</p>
","sentiment-analysis, text-analysis, graphlab","<p>I actually find out an easier way do do this:</p>

<pre><code>def wordCount_select(wc,selectedWord):
    if selectedWord in wc:
        return wc[selectedWord]
    else:
        return 0    


for word in selected_words:
    products[word] = products['word_count'].apply(lambda wc: wordCount_select(wc, word))
</code></pre>
",0,0,378,2016-01-30 20:15:49,https://stackoverflow.com/questions/35106616/graphlab-how-to-avoid-manually-duplicating-functions-that-has-only-a-different
Aggregate results from for loop in python,"<p>I am doing sentiment analysis in python. After clearing up the tweets to use, I am stuck at getting the final sentiment score per tweet. I am getting the values but not able to aggregate the as one score per tweet. Here's the code</p>

<pre><code>scores = {} # initialize an empty dictionary  
for line in sent_file:  
    term, score = line.split(""\t"") 
    scores[term] = int(score) # Convert the score to an integer.  



for line in tweet_file:  
       #convert the line from file into a json object  
    mystr = json.loads(line) 
    #check the language is english, if ""lang"" is among the keys  
    if 'lang' in mystr.keys() and mystr[""lang""]=='en':  
            #if ""text"" is not among the keys, there's no tweet to read, skip it  
        if 'text' in mystr.keys(): 
            print mystr['text']
            resscore=[] 
            result = 0
            #split the tweet into a list of words  
            words = mystr[""text""].split() 
            #print type(words)
            for word in words:  

                if word in scores:  
                    result = scores[word]
                    resscore.append(result)

                    print str(sum(resscore))


                else:  
                    result+=0
</code></pre>

<p>The output I am getting is like</p>

<pre><code>If nothing is as you'd imagine it to be then you may as well start imaging some mad stuff like dragons playing chess on a…
 -3
 -1
</code></pre>

<p>But I want Those values in this case -3, -1 to be aggregated to give final score for that tweet i.e -4. Thanks</p>
","python, for-loop, sentiment-analysis","<p>Accumulate those values &amp; print them after the loop ends:</p>

<pre><code># ...

finalScore = 0 # final score
for word in words:

    if word in scores:
        result = scores[word]
        resscore.append(result)

        finalScore += sum(resscore)

print(str(finalScore))

# ...
</code></pre>
",2,1,2402,2016-02-01 12:13:15,https://stackoverflow.com/questions/35130239/aggregate-results-from-for-loop-in-python
"Score Sentiment function in R, return always 0","<p>I have a (probably) stupid problem with score.sentiment
I'm trying to use this function with 3 default phrases, the problem is that the function return score 0.0.0, but it should return 2.-5.4
I don't understand the problem because RGui don't give me errors and I'm following a tutorial!</p>

<p>I've dowloaded lists for negative and positive words with</p>

<pre><code>hu.liu.pos = scan('https://www.dropbox.com/sh/3xctszdxx4n00xq/AAA_Go_Y3kJxQACFaVBem__ea/positive-words.txt?dl=0', what='character', comment.char=';');
hu.liu.neg = scan('https://www.dropbox.com/sh/3xctszdxx4n00xq/AABTGWHitlRZcddq1pPXOSqca/negative-words.txt?dl=0', what='character', comment.char=';');
</code></pre>

<p>And I have my score function</p>

<pre><code>score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
  {
    require(plyr);
    require(stringr);
    scores = laply(sentences, function(sentence, pos.words, neg.words) {
      sentence = gsub('[^A-z ]','', sentence)
      sentence = tolower(sentence);
      word.list = str_split(sentence, '\\s+');
      words = unlist(word.list);
      pos.matches = match(words, pos.words);
      neg.matches = match(words, neg.words);
      pos.matches = !is.na(pos.matches);
      neg.matches = !is.na(neg.matches);
      score = sum(pos.matches) - sum(neg.matches);
      return(score);
    }, pos.words, neg.words, .progress=.progress );
    scores.df = data.frame(score=scores, text=sentences);
    return(scores.df);
  }
</code></pre>

<p>And this is my example code</p>

<pre><code>    sample=c(""You're awesome and I love you"",""I hate and hate and hate. So angry. Die!"",""Impressed and amazed: you are peerless in your achievement of unparalleled mediocrity."")
result=score.sentiment(sample,pos.words,neg.words)
class(result)
result$score
result
</code></pre>

<p>If can be usefull, there are the libraries and packages that I've installed.</p>

<pre><code>install.packages('twitteR', dependencies=T);
install.packages('ggplot2', dependencies=T);
install.packages('XML', dependencies=T);
install.packages('plyr', dependencies=T);
install.packages('doBy', dependencies=T);
install.packages('tm', dependencies=T);
install.packages('RJSONIO', dependencies=T)
install.packages('RWeka')
install.packages('base64enc')

library(twitteR);
library(ggplot2);
library(XML);
library(plyr);
library(doBy);
library(RJSONIO)
library(tm)
library(RWeka)
</code></pre>

<p>Thank you in advice</p>
","r, sentiment-analysis","<p>Works for me</p>

<pre><code>hu.liu.pos = readLines('https://www.dropbox.com/sh/3xctszdxx4n00xq/AAA_Go_Y3kJxQACFaVBem__ea/positive-words.txt?dl=1');
hu.liu.neg = readLines('https://www.dropbox.com/sh/3xctszdxx4n00xq/AABTGWHitlRZcddq1pPXOSqca/negative-words.txt?dl=1');

score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
  {
    require(plyr);
    require(stringr);
    scores = laply(sentences, function(sentence, pos.words, neg.words) {
      sentence = gsub('[^A-z ]','', sentence)
      sentence = tolower(sentence);
      word.list = str_split(sentence, '\\s+');
      words = unlist(word.list);
      pos.matches = match(words, pos.words);
      neg.matches = match(words, neg.words);
      pos.matches = !is.na(pos.matches);
      neg.matches = !is.na(neg.matches);
      score = sum(pos.matches) - sum(neg.matches);
      return(score);
    }, pos.words, neg.words, .progress=.progress );
    scores.df = data.frame(score=scores, text=sentences);
    return(scores.df);
  }

sample=c(""You're awesome and I love you"",""I hate and hate and hate. So angry. Die!"",""Impressed and amazed: you are peerless in your achievement of unparalleled mediocrity."")
result=score.sentiment(sample,hu.liu.pos,hu.liu.neg)
result
#   score                                                                                   text
# 1     2                                                          You're awesome and I love you
# 2    -5                                               I hate and hate and hate. So angry. Die!
# 3     4 Impressed and amazed: you are peerless in your achievement of unparalleled mediocrity.
</code></pre>
",2,2,3606,2016-02-05 11:23:56,https://stackoverflow.com/questions/35222946/score-sentiment-function-in-r-return-always-0
How do I set up a Stanford CoreNLP Server on Windows to return sentiment for text,"<p>I am attempting to set up a local server on Windows with Stanford CoreNLP to calculate sentiment scores for over 1M article and video texts. I don't know Java, so I will need some help.</p>

<p>I successfully installed Stanford CoreNLP 3.6.0, and I have a server running with:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer
</code></pre>

<p>Running this http post from my other computer works, and I get an expected response (xxx.xxx.xxx.xxx is the server's IP address):</p>

<pre><code>wget --post-data 'the quick brown fox jumped over the lazy dog' 'xxx.xxx.xxx.xxx:9000/?properties={""tokenize.whitespace"": ""true"", ""annotators"": ""tokenize,ssplit,pos,lemma,parse"", ""outputFormat"": ""json""}' -O -
</code></pre>

<p>However, the response doesn't contain sentiment. The obvious solution would be to add an annotator:</p>

<pre><code>wget --post-data 'the quick brown fox jumped over the lazy dog' 'xxx.xxx.xxx.xxx:9000/?properties={""tokenize.whitespace"": ""true"", ""annotators"": ""tokenize,ssplit,pos,lemma,parse,sentiment"", ""outputFormat"": ""json""}' -O -
</code></pre>

<p>However, on the server side, I get this error:</p>

<pre><code>java.lang.IllegalArgumentException: Unknown annotator: sentiment
at edu.stanford.nlp.pipeline.StanfordCoreNLP.ensurePrerequisiteAnnotators(StanfordCoreNLP.java:281)
at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.getProperties(StanfordCoreNLPServer.java:476)
at edu.stanford.nlp.pipeline.StanfordCoreNLP$CoreNLPHandler.handle(StanfordCoreNLPServer.java:350)
at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
at sun.net.httpserver.AuthFilter.doFilter(Unknown Source)
at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(Unknown Source)
at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
at sun.net.httpserver.ServerImpl$Exchange.run(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.lang.thread.run(Unknown Source)
</code></pre>

<p>The next obvious solution would be to add a parameter to starting the server, which runs:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators ""tokenize,ssplit,pos,lemma,parse,sentiment""
</code></pre>

<p>Running the same http posts from before gives the same exact result and error, respectively.</p>

<p>Am I doing something wrong, or is there some modification to the core code that it needs to work? I don't know Java, so I am unable to make those changes.</p>

<p>As a side note, this similar command starts a console, and seems to load sentiment correctly:</p>

<pre><code>java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators ""tokenize,ssplit,pos,lemma,parse,sentiment""

[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator sentiment

Entering interactive shell. Type q RETURN or EOF to quit.
NLP&gt; _
</code></pre>
","java, server, stanford-nlp, sentiment-analysis","<p>Try running with the <a href=""https://github.com/stanfordnlp/CoreNLP"">GitHub version</a> of the code. Your first solution is correct -- the fact that it could not find the sentiment annotator is a bug in the code:</p>

<pre><code>wget --post-data 'the quick brown fox jumped over the lazy dog' 'xxx.xxx.xxx.xxx:9000/?properties={""annotators"": ""tokenize,ssplit,pos,lemma,parse,sentiment"", ""outputFormat"": ""json""}' -O -
</code></pre>

<p>(A side note: the <code>tokenize.whitespace</code> property is in the documentation to show that you can pass in arbitrary properties, but I recommend against using it in production).</p>
",5,6,3566,2016-02-09 23:15:20,https://stackoverflow.com/questions/35304083/how-do-i-set-up-a-stanford-corenlp-server-on-windows-to-return-sentiment-for-tex
Machine learning / sentiment analysis - is it possible to effectively and safely remove stopwords from text?,"<p>Based on my knowledge of text learning, we want to stem and remove stop words to reduce the entropy of our data. However, a stop word like ""not"" could have a huge impact on the meaning and sentiment of a comment. For example:</p>

<blockquote>
  <p>I did not like the movie</p>
</blockquote>

<p>turns into:</p>

<blockquote>
  <p>I did like the movie</p>
</blockquote>

<p>If I just leave the stopwords in the text, then I'm assuming their significance would be small enough that it wouldn't matter, it would just take longer to train my classifier. </p>

<p>Are these two tradeoffs I'm perceiving accurate, or is there a best of both worlds in terms of reducing insignificant features without messing up the sentiment of a text?</p>
","machine-learning, sentiment-analysis","<p>Does it need to be an all or nothing decision? If the stop word list is only a couple thousand words long, you could just go through the list by hand and keep only the ones that are probably low-information for sentiment analysis.  e.g. prune ""the"" and ""a"", but keep ""not"".  </p>

<p>I'd probably error on the side of removing any word from the stop word list that you think <em>might</em> provide useful information.  If the word isn't actually useful, the learner will figure that out.</p>
",2,0,167,2016-02-23 04:05:23,https://stackoverflow.com/questions/35568625/machine-learning-sentiment-analysis-is-it-possible-to-effectively-and-safely
Stanford CoreNLP properties File not found,"<p>I am trying to do sentiment analysis on tweets but getting strange Exception.</p>

<p>I am initializing pipeline with properties file and place properties file in resources directory, within src->main folder.
<a href=""https://i.sstatic.net/HE600.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HE600.png"" alt=""enter image description here""></a></p>

<p>But still getting Exception in init function : </p>

<pre><code>Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException:    java.io.IOException: Unable to open ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"" as class path, filename or URL
at edu.stanford.nlp.parser.common.ParserGrammar.loadModel(ParserGrammar.java:188)
at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:212)
at edu.stanford.nlp.pipeline.ParserAnnotator.&lt;init&gt;(ParserAnnotator.java:115)
at edu.stanford.nlp.pipeline.AnnotatorImplementations.parse(AnnotatorImplementations.java:150)
at edu.stanford.nlp.pipeline.AnnotatorFactories$11.create(AnnotatorFactories.java:463)
at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:375)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:139)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:135)
at com.mycompany.sentmentanalysisontweets.NLP.init(NLP.java:27)
at com.mycompany.sentmentanalysisontweets.WhatToThink.main(WhatToThink.java:15)
</code></pre>

<p>In main method i'm calling <code>init()</code> method.</p>

<pre><code>public class Main {

    public static void main(String[] args) {
        NLP.init();
    }
}

class NLP {
    static StanfordCoreNLP pipeline;

    public static void init() {
        InputStream input = NLP.class.getClass().getResourceAsStream(""/example.properties"");
        Properties prop = new Properties();
        try {
          prop.load(input);
        } catch (IOException e) {
          e.printStackTrace();
        }

        pipeline = new StanfordCoreNLP(prop);
    }
}
</code></pre>
","java, sentiment-analysis","<p>Having the following structure:</p>

<pre><code>src/main/resources
             |
              - example.properties
</code></pre>

<p>Executing the code below will solve your problem:</p>

<pre><code>public class Main {

    public static void main(String[] args) {
        NLP.init();
    }
}

class NLP {
   static StanfordCoreNLP pipeline;

   public static void init() {
        InputStream input = NLP.class.getClass().getResourceAsStream(""/example.properties"");
        Properties prop = new Properties();
        try {
            prop.load(input);
        } catch (IOException e) {
            e.printStackTrace();
        }

        pipeline = new StanfordCoreNLP(prop);
    }
}
</code></pre>

<p><strong>EDIT</strong></p>

<p>Your pom.xml must contain the following dependencies:</p>

<pre><code>&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;3.6.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;3.6.0&lt;/version&gt;
        &lt;classifier&gt;models&lt;/classifier&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>
",2,1,3779,2016-02-24 10:52:42,https://stackoverflow.com/questions/35600071/stanford-corenlp-properties-file-not-found
Supervised Learning Approach for Aspect Extraction,"<p>I'm developing an aspect-level sentiment analysis project for online travel reviews of travel domain.</p>

<p>I have a human annotated dataset that has labelled aspect terms, and aspect categories along with their sentiment polarity.</p>

<p>For example;</p>

<pre><code>Sentence:

This beach was a wonderful time for a day party  it had a fun crowd and has a big bar with a great atmosphere. The food was delicious too.
</code></pre>

<p>The above sentence has the following <strong>aspect terms</strong> labelled;</p>

<pre><code>{party#positive C} {crowd#positive C} {bar#positive C} {food#positive C}
</code></pre>

<p>And the following <strong>aspect categories</strong>;</p>

<pre><code>{entertainment#positive C} {accommodation#positive C}
</code></pre>

<p>I want to try a <code>supervised learning</code> approach to train a model to classify aspect terms from sentences. </p>

<p>I'm using <code>Stanford CORENLP</code> library. But confused as to how the training data format should be? and what is the best approach to take.</p>

<p>I have seen people using <code>IOB notation</code> to format training data to train <code>NER</code> systems. Can I use a similar method to get this done? As in, how do I format my training data file to get aspect terms as mentioned above from an input sentence?</p>

<p>If someone can point me in the right direction, I would appreciate that a lot.</p>
","machine-learning, dataset, nlp, stanford-nlp, sentiment-analysis","<p>This problem can be tackled by breaking it down to smaller subtasks. A possible pipeline approach may be:</p>

<ol>
<li><p>The first stage is aspect term extraction which will identify aspect terms in the raw text. This too can be broken down to two  subtasks. Firstly your system will need to label tokens in text that are aspect terms. Let's call the these labelled tokens aspect term mentions. This is called <strong>Named Entity Recognition</strong> (NER). Next, if you have a pre-defined set of aspect term classes, the systems will need to link the aspect term mentions found in the previous task to those classes. This is called <strong>Entity Linking</strong>. It's worth noting that from the example that you give  the labelled dataset is not yet suitable for the above tasks as the labels are not anchored in text. You may be able to create a suitable dataset by guessing which tokens in text do your given labels correspond to. This is similar to the <strong>Distant Supervision</strong> work.</p></li>
<li><p>The next task is aspect term sentiment classification. <strong>Convolutional Neural Networks</strong> have been used for sentence and document sentiment classification but they can probably be adapted for your purposes if at the input you provide a marker for which tokens are being classified. This is called a position embedding in this work: <a href=""http://www.cs.nyu.edu/~thien/pubs/vector15.pdf"" rel=""nofollow"">http://www.cs.nyu.edu/~thien/pubs/vector15.pdf</a></p></li>
</ol>
",3,1,906,2016-02-24 12:19:11,https://stackoverflow.com/questions/35602043/supervised-learning-approach-for-aspect-extraction
sentiment analysis in python,"<p>I am extracting reviews from various website and storing them in a file and then classifying each sentence as positive or negative with the help of senti-wordnet ( which gives certain scores).I am using python 2.7. I don't know how it works for reviews stored in a file. Do anybody know the code in python for this ? </p>

<pre><code>f1=open(""foodP.txt"",""r"")
word_features =[]
words = []

for line in f1:
    word_features.append(line)
    s=str(word_features)
    tokens=nltk.word_tokenize(s)    

for i,j in nltk.pos_tag(tokens):
    if j in ['VBN','VBP','VB','JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']:
        words.append(i)
print words
</code></pre>

<p>this code will give only adjective , adverbs and verbs i need. i want to classify these words as either positive or negative .</p>
","python, sentiment-analysis, pos-tagger, senti-wordnet","<p>You don't need POS for sentiment analysis, at least it's not required. Prepare feature by using bag_of_words in X and ""neg""/""pos"" as Y. Then split into train/test sets and apply classification algorithm - NaiveBayes, MaxEnt, RandomForest, SVM.</p>
",1,-5,637,2016-03-02 09:44:04,https://stackoverflow.com/questions/35743488/sentiment-analysis-in-python
How to specify the output labels to keras lstm,"<p>I am new to Keras and I want to predict the polarity of each word in a sentence using LSTM. I am representing each sentence, by the corresponding pre trained word vectors. So my input representation is (maxlen,input_dimensions).
But I am not being able to understand how to give the labels. For each sentence, the word can be in 3 classes(pos/neg/neutral). So it will something like [0,2,0,0,1.....]. How do I give this output to a sequential model?</p>
","python, machine-learning, sentiment-analysis, keras, lstm","<p>When calling</p>

<pre><code>model.fit
</code></pre>

<p>you will supply the inputs and <em>optionally</em> the expected output . Usually the input is called 'X' and the output 'y'.</p>

<p>The input will include a dimension representing the sentence/phrase: you need to decide how long this will be for training. Note: another related consideration is the mini-batch size.</p>

<p>The output will have one less dimension than the input. You want to put the <strong>next</strong> word <strong>after</strong> the sentence in the same input array spot (by ordinal): this is the <strong><em>expected output for sentence K</em></strong> where k is the ordinal within the input array and the corresponding ordinal in the output array.</p>
",2,0,880,2016-03-03 15:11:13,https://stackoverflow.com/questions/35775853/how-to-specify-the-output-labels-to-keras-lstm
I got a different result when I retrained the sentiment model with Stanford CoreNLP to compare with the related paper&#39;s result,"<p>I downloaded  stanford-corenlp-full-2015-12-09.
And I created a training model with the following command:</p>

<pre><code> java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz
</code></pre>

<p>When I finished training, I found many files in my directory.
<a href=""https://i.sstatic.net/v2fSc.jpg"" rel=""nofollow"">the model list</a></p>

<p>Then I used the evaluation tool from the package and I ran the code like this:</p>

<pre><code>java -cp * edu.stanford.nlp.sentiment.Evaluate -model model-0024-79.82.ser.gz -treebank test.txt
</code></pre>

<p>The test.txt was from trainDevTestTrees_PTB.zip. This is the result about code:</p>

<pre><code>F:\trainDevTestTrees_PTB\trees&gt;java -cp * edu.stanford.nlp.sentiment.Evaluate -model model-0024-79.82.ser.gz -treebank test.txt
EVALUATION SUMMARY
Tested 82600 labels
65331 correct
17269 incorrect
0.790932 accuracy
Tested 2210 roots
890 correct
1320 incorrect
0.402715 accuracy
Label confusion matrix
  Guess/Gold       0       1       2       3       4    Marg. (Guess)
           0     551     340      87      32       6    1016
           1     956    5348    2476     686     191    9657
           2     354    2812   51386    3097     467   58116
           3     146     744    2525    6804    1885   12104
           4       1      11      74     379    1242    1707
Marg. (Gold)    2008    9255   56548   10998    3791

           0        prec=0.54232, recall=0.2744, spec=0.99423, f1=0.36442
           1        prec=0.5538, recall=0.57785, spec=0.94125, f1=0.56557
           2        prec=0.8842, recall=0.90871, spec=0.74167, f1=0.89629
           3        prec=0.56213, recall=0.61866, spec=0.92598, f1=0.58904
           4        prec=0.72759, recall=0.32762, spec=0.9941, f1=0.4518

Root label confusion matrix
  Guess/Gold       0       1       2       3       4    Marg. (Guess)
           0      50      60      12       9       3     134
           1     161     370     147      94      36     808
           2      31     103     102      60      32     328
           3      36      97     123     305     265     826
           4       1       3       5      42      63     114
Marg. (Gold)     279     633     389     510     399

           0        prec=0.37313, recall=0.17921, spec=0.9565, f1=0.24213
           1        prec=0.45792, recall=0.58452, spec=0.72226, f1=0.51353
           2        prec=0.31098, recall=0.26221, spec=0.87589, f1=0.28452
           3        prec=0.36925, recall=0.59804, spec=0.69353, f1=0.45659
           4        prec=0.55263, recall=0.15789, spec=0.97184, f1=0.24561

Approximate Negative label accuracy: 0.638817
Approximate Positive label accuracy: 0.697140
Combined approximate label accuracy: 0.671925
Approximate Negative root label accuracy: 0.702851
Approximate Positive root label accuracy: 0.742574
Combined approximate root label accuracy: 0.722680
</code></pre>

<p>The accuracy about fine-grained and positive/negative was quite different from the paper ""Socher, R., Perelygin, A., Wu, J.Y., Chuang, J., Manning, C.D., Ng, A.Y. and Potts, C., 2013, October. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP) (Vol. 1631, p. 1642)."" 
The paper states the accuracy about fine-grained and positive/negative is higher than mine.
<a href=""https://i.sstatic.net/sdQ84.jpg"" rel=""nofollow"">The records in the paper</a></p>

<p>Were there any problems with my operation? Why was my result different from the paper?</p>
","java, stanford-nlp, sentiment-analysis","<p>The short answer is that the paper used a different system written in Matlab.  The Java system does not match the paper.  Though we do distribute the binary model we trained in Matlab with the English models jar.  So you can RUN the binary model with Stanford CoreNLP, but you cannot TRAIN a binary model with similar performance with Stanford CoreNLP at this time.</p>
",1,2,410,2016-03-03 16:59:09,https://stackoverflow.com/questions/35778325/i-got-a-different-result-when-i-retrained-the-sentiment-model-with-stanford-core
Writing a list from a for loop into a csv,"<p>I wrote a for loop that iterates through a CSV to get a list like this:</p>

<pre><code>[t1, s1]
[t2, s2]
[t3, s3]
</code></pre>

<p>and so 4 thousand times. 
Now I need to write these into a new CSV file, where they'd populate 2 fields and be separated by a comma. 
When I enter this, I only get the last list from the last loop, and with one character in a cell. </p>

<pre><code>def sentiment_analysis():
    fo = open(""positive_words.txt"", ""r"")
    positive_words = fo.readlines()
    fo.close()
    positive_words = map(lambda positive_words: positive_words.strip(), positive_words)
    fo = open(""negative_words.txt"", ""r"")
    negative_words = fo.readlines()
    fo.close()
    negative_words = map(lambda negative_words: negative_words.strip(), negative_words)
    fo = open(""BAC.csv"", ""r"")
    data = fo.readlines()
    fo.close()
    data = map(lambda data: data.strip(), data)
    x1 = 0 #number of bullish
    x2 = 0 #number of bearish
    x3 = 0 #number of unknown
    for info in data:
        data_specs = info.split(',')
        time_n_date = data_specs[0]
        sentiment = data_specs[2]
        '''Possibly precede with a nested for loop for data_specs???'''
        if sentiment == 'Bullish':
            '''fo.write(time + ',' + 'Bullish' + '\n')'''
        elif sentiment == 'Bearish':
            ''' fo.write(time + ',' + 'Bearish' + '\n')'''
        else:
            x3 += 1
            positive = 0
            negative = 0
            content_words = data_specs[1].split()
            for a in positive_words:
                for b in content_words:
                    if (a == b):
                        positive = positive + 1
            for c in negative_words:
                for d in content_words:
                    if (c == d):
                        negative = negative + 1
            if positive &gt; negative:
                '''fo.write(time + ',' + 'Bullish' + '\n')'''
                sentiment = 'Bullish'
            elif positive &lt; negative:
                sentiment = 'Bearish'
            else:
                sentiment = 'Neutral'
        bac2data = [time_n_date, sentiment]
        print bac2data
        fo = open(""C:\Users\Siddhartha\Documents\INFS 772\Project\Answer\BAC2_answer.csv"", ""w"")
        for x in bac2data:
            w = csv.writer(fo, delimiter = ',')
            w.writerows(x)
        fo.close()
</code></pre>

<p>My for loop isn't going through it all.</p>
","python, for-loop, export-to-csv, sentiment-analysis","<p>In your code <code>bac2data = [time_n_date, sentiment]</code> creates a list containing 2 string items. The proper way to write that to a CSV file with <code>csv.writer()</code> is with <code>writerow(bac2data)</code>.</p>

<p>The last part of your code contains a number of errors. Firstly you are opening the CSV file in write mode (<code>'w'</code>) for every line of the incoming data. This will <em>overwrite</em> the file each time, losing all data except the last line. Then you are iterating over the <code>bac2data</code> list and calling <code>writerows()</code> on each item. That's going to write each character from the string on it's own line (which matches your reported output).</p>

<p>Instead, open the output file and create a <code>csv.writer</code> outside of the main <code>for info in data:</code> loop:</p>

<pre><code>fo = open(""C:\Users\Siddhartha\Documents\INFS 772\Project\Answer\BAC2_answer.csv"", ""w"")
writer = csv.writer(fo)
for info in data:
    ....
</code></pre>

<p>Then replace these lines at the bottom of the main loop:</p>

<pre><code>    bac2data = [time_n_date, sentiment]
    print bac2data
    fo = open(""C:\Users\Siddhartha\Documents\INFS 772\Project\Answer\BAC2_answer.csv"", ""w"")
    for x in bac2data:
        w = csv.writer(fo, delimiter = ',')
        w.writerows(x)
    fo.close()
</code></pre>

<p>with this:</p>

<pre><code>    bac2data = [time_n_date, sentiment]
    print bac2data
    writer.writerow(bac2data)
</code></pre>

<p>Once you have that working, and no longer need to print <code>bac2data</code> for debugging, you can just use 1 line:</p>

<pre><code>    writer.writerow((time_n_date, sentiment)]
</code></pre>

<hr>

<p><strong>Update</strong></p>

<p>Complete code for function:</p>

<pre><code>def sentiment_analysis():
    fo = open(""positive_words.txt"", ""r"")
    positive_words = fo.readlines()
    fo.close()
    positive_words = map(lambda positive_words: positive_words.strip(), positive_words)
    fo = open(""negative_words.txt"", ""r"")
    negative_words = fo.readlines()
    fo.close()
    negative_words = map(lambda negative_words: negative_words.strip(), negative_words)
    fo = open(""BAC.csv"", ""r"")
    data = fo.readlines()
    fo.close()
    data = map(lambda data: data.strip(), data)
    x1 = 0 #number of bullish
    x2 = 0 #number of bearish
    x3 = 0 #number of unknown

    fo = open(""C:\Users\Siddhartha\Documents\INFS 772\Project\Answer\BAC2_answer.csv"", ""w"")
    writer = csv.writer(fo)

    for info in data:
        data_specs = info.split(',')
        time_n_date = data_specs[0]
        sentiment = data_specs[2]
        '''Possibly precede with a nested for loop for data_specs???'''
        if sentiment == 'Bullish':
            '''fo.write(time + ',' + 'Bullish' + '\n')'''
        elif sentiment == 'Bearish':
            ''' fo.write(time + ',' + 'Bearish' + '\n')'''
        else:
            x3 += 1
            positive = 0
            negative = 0
            content_words = data_specs[1].split()
            for a in positive_words:
                for b in content_words:
                    if (a == b):
                        positive = positive + 1
            for c in negative_words:
                for d in content_words:
                    if (c == d):
                        negative = negative + 1
            if positive &gt; negative:
                '''fo.write(time + ',' + 'Bullish' + '\n')'''
                sentiment = 'Bullish'
            elif positive &lt; negative:
                sentiment = 'Bearish'
            else:
                sentiment = 'Neutral'

        bac2data = [time_n_date, sentiment]
        print bac2data
        writer.writerow(bac2data)

    fo.close()
</code></pre>
",3,1,3230,2016-03-07 01:05:41,https://stackoverflow.com/questions/35834654/writing-a-list-from-a-for-loop-into-a-csv
Adding Emoticons to AFINN library for Sentiment analysis,"<p><strong>How to add Emoticons to AFINN library</strong></p>

<p>I want to add Emoticons to AFINN library for Sentiment Analysis , The Library already have have Words with their respective polarity , How to append some Emoticons so that the respective code can read its Polarity ???</p>

<pre><code>afinn = dict(map(lambda (w, s): (w, int(s)), [ 
        ws.strip().split('\t') for ws in open(filenameAFINN) ]))
pattern_split = re.compile(r""\W+"")
def sentiment(text):
    words = pattern_split.split(text.lower())
    sentiments = map(lambda word: afinn.get(word, 0), words)
    if sentiments:
        sentiment = float(sum(sentiments))/math.sqrt(len(sentiments))
    else:
        sentiment = 0
    return sentiment
if __name__ == '__main__':
    print(""%s"") % (text)
    print (""%6.2f"") % (sentiment(text))
    if sentiment(text) &lt; 0:
        print ""================||| NEGATIVE |||================""
    elif sentiment(text) &gt; 0:
        print ""================||| POSITIVE |||================""
    else:
        print ""================||| Seems NEUTRAL |||================""  
</code></pre>

<p>The library has words in order, Like.</p>

<pre><code>yucky   -2
yummy   3
zealot  -2
zealots -2
zealous 2
</code></pre>

<p>how should i add these Emoticons in the library , and read its polarity  </p>

<pre><code>(^ ^)   1
(^-^)   1
(^.^)   1
</code></pre>
","python, python-2.7, sentiment-analysis","<p>I am the one behind the AFINN word list. My Python package named <code>afinn</code> already features some emoticons.</p>

<pre><code>&gt;&gt;&gt; afinn = Afinn(emoticons=True)
&gt;&gt;&gt; afinn.score('I saw that yesterday :)')
2.0
</code></pre>

<p>You can get the <code>afinn</code> Python package here:</p>

<p><a href=""https://github.com/fnielsen/afinn"" rel=""nofollow"">https://github.com/fnielsen/afinn</a></p>

<p>or from the Python package Index</p>

<p><a href=""https://pypi.python.org/pypi/afinn/"" rel=""nofollow"">https://pypi.python.org/pypi/afinn/</a></p>

<p>There is a file with my scoring of emoticons. On GitHub you find it here:</p>

<p><a href=""https://github.com/fnielsen/afinn/blob/master/afinn/data/AFINN-emoticon-8.txt"" rel=""nofollow"">https://github.com/fnielsen/afinn/blob/master/afinn/data/AFINN-emoticon-8.txt</a></p>

<p>If you want to add your own emoticons, I suppose the presently less troublesome approach would be to extend the emoticon file after you have copied/forked a version of <code>afinn</code>.</p>
",1,1,1455,2016-03-07 09:22:54,https://stackoverflow.com/questions/35840083/adding-emoticons-to-afinn-library-for-sentiment-analysis
How to improve my feature selection for a NB classifier?,"<p>I have read that improving feature selection will reduce the training time of my classifier and also improve accuracy but I am not sure how can I reduce the number of features. Should I count them and after select the first 3000 for example ?</p>

<p>This is my code :</p>

<pre><code>def save_object(obj, filename):
    with open(filename, 'wb') as output:
        pickle.dump(obj,output,pickle.HIGHEST_PROTOCOL)
        print ""saved""
        ujson.dumps({""output"" : ""obj""})


with open('neg5000.csv','rb') as f:
    reader = csv.reader(f)
    neg_tweets = list(reader)
    print ""list ready""

with open('pos5000.csv','rb') as f:
    reader = csv.reader(f)
    pos_tweets = list(reader)
    print ""list ready""

tweets = []
for (words, sentiment) in pos_tweets + neg_tweets:
    words_filtered = [e.lower() for e in words.split() if len(e) &gt;= 3] 
    tweets.append((words_filtered, sentiment))




def get_words_in_tweets(tweets):
    all_words = []
    for (words, sentiment) in tweets:
      all_words.extend(words)
    return all_words

def get_word_features(wordlist):
    wordlist = nltk.FreqDist(wordlist)
    word_features = list(wordlist.keys())[:3000]
    #word_features = wordlist.keys()
    return word_features

def extract_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['contains(%s)' % word] = (word in document_words)
    return features
#def extract_features(words):
 #   return dict([(word, True) for word in words])


word_features = get_word_features(get_words_in_tweets(tweets))
training_set = nltk.classify.apply_features(extract_features, tweets)


save_object(word_features, 'wordf.save')
print 'features done'
print datetime.datetime.now()
classifier = nltk.NaiveBayesClassifier.train(training_set)
print 'training done'
print datetime.datetime.now()

save_object(classifier, 'classifier.save')

tweet = 'I love this car'
print classifier.classify(extract_features(tweet.split()))
</code></pre>
","python-2.7, nltk, sentiment-analysis, naivebayes","<p>There's a number of ways to approach feature selection for the supervised classification problem (which is what Naive Bayes does). I suggest heading over to <code>scikit-learn</code> <a href=""http://scikit-learn.org/stable/modules/feature_selection.html"" rel=""nofollow"">manual</a> and just trying everything listed there, since the choice of particular method is dependends on the data you have.</p>

<p>The easiest way to do this is to switch to the <a href=""http://scikit-learn.org/stable/modules/naive_bayes.html"" rel=""nofollow""><code>scikit-learn</code> implementation</a> of Naive Bayes and the use a <a href=""http://scikit-learn.org/stable/modules/pipeline.html"" rel=""nofollow"">Pipeline</a> to chain the feature selection and classifier training. See this <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">tutorial</a> for code examples.</p>

<p>Here's a version of your code using <code>scikit-learn</code> with <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html"" rel=""nofollow""><code>SelectKBest</code></a> feature selection:</p>

<pre><code>import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_selection import SelectPercentile
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline


def read_input(path):
    with open(path) as handle:
        lines = (line.rsplit("","", 1) for line in handle)
        return [text for text, label in lines]


# Assuming each line in ``neg5000.csv`` and ``pos5000.csv`` is a
# UTF-8-encoded tweet.
neg_tweets = read_input(""neg5000.csv"")
pos_tweets = read_input(""pos5000.csv"")

X = np.append(neg_tweets, pos_tweets)
y = np.append(np.full(len(neg_tweets), -1, dtype=int),
              np.full(len(pos_tweets), 1, dtype=int))


p = Pipeline([
    (""vectorizer"", CountVectorizer()),
    (""selector"", SelectPercentile(percentile=20)),
    (""nb"", MultinomialNB())
])

p.fit(X, y)
print(p.predict([""I love this car""]))
</code></pre>
",1,0,1296,2016-03-11 13:17:46,https://stackoverflow.com/questions/35941286/how-to-improve-my-feature-selection-for-a-nb-classifier
What is the meaning of sense number in SentiWordNet?,"<p>I want to use SentiWordNet for my project and I could not figure out what does the <strong>sense number</strong> do? Here is a part of SentiWordNet's word list;</p>
<blockquote>
<h1>POS ID  PosScore    NegScore    SynsetTerms Gloss</h1>
<p>a 00002730    0   0   acroscopic#1    facing or on the side toward the apex</p>
<p>a 00002843    0   0   basiscopic#1    facing or on the side toward the base</p>
<p>a 00003829    0.25    0   parturient#2    giving birth; &quot;a parturient heifer&quot;</p>
</blockquote>
<p>Here is the explanation of SentiWordNet from its word list document;</p>
<blockquote>
<p>The pair (POS,ID) uniquely identifies a WordNet (3.0) synset. The
values PosScore and NegScore are the positivity and negativity score
assigned by SentiWordNet to the synset. The objectivity score can be
calculated as: ObjScore = 1 - (PosScore + NegScore) SynsetTerms column
reports the terms, with sense number, belonging to the synset
(separated by spaces).</p>
</blockquote>
<p>I also found a related question here but I did not understand <strong>the feature of the sense number</strong> from the answer. Here is the related question's link: <a href=""https://stackoverflow.com/questions/23863413/what-does-sentiwordnet-3-0-result-signify"">What does sentiwordnet 3.0 result signify?</a></p>
<p><em>My question is:</em> How can I use this <strong>sense number</strong> part in my code? <strong>What does it do exactly?</strong></p>
","sentiment-analysis, senti-wordnet","<p>If you <a href=""http://wordnetweb.princeton.edu/perl/webwn?s=parturient&amp;sub=Search%20WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h=00"" rel=""noreferrer"">lookup ""parturient"" in WordNet</a> you'll see two meanings are shown. These are parturient#1 and  parturient#2 respectively.</p>

<p>In that case the difference is rather subtle. But, for instance, the word <a href=""http://wordnetweb.princeton.edu/perl/webwn?s=field&amp;sub=Search%20WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h=00"" rel=""noreferrer"">""field"" has lots of senses</a>, and you might care which one is being referred to.</p>

<p>You would use them if you then start looking at the semantic relations of that word. E.g. the hypernym of field#4 is ""knowledge domain"", whereas the hypernym of of field#1 is ""tract"" (piece of land).</p>

<p>A classic example when considering sentiment is to <a href=""http://wordnetweb.princeton.edu/perl/webwn?s=suck&amp;sub=Search%20WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h=000000000000000000000"" rel=""noreferrer"">compare suck#2 and suck#4</a> in the context of talking about a vacumn cleaner! E.g. ""This cleaner really sucks#4 as it hardly sucks#2 at all.""</p>
",5,2,1588,2016-03-26 15:58:07,https://stackoverflow.com/questions/36237416/what-is-the-meaning-of-sense-number-in-sentiwordnet
php Call to a member function sentimentAnalysis() on a non-object,"<p>I am in trouble with this error I know its expecting an object and have searched through the forums and failed when trying to apply fixes, this may be an easy fix by a few lines of code. I Appreciate if anyone has a bit of time to help fix this. </p>

<p><em>Again this may be a duplicate but I have tried to apply fixes un-successfully</em></p>

<p>Error is with the following line, I will include my whole script </p>

<pre><code> $response= $TwitterSentinmentAnalysis-&gt;sentimentAnalysis($twitterSearchParams);
</code></pre>

<blockquote>
  <p>Fatal error: Call to a member function sentimentAnalysis() on a
  non-object in
  /Applications/XAMPP/xamppfiles/htdocs/infoCraft-miner/search_server.php
  on line 48</p>
</blockquote>

<pre><code>&lt;?php 

// The search terms are passed in the q parameter
// search_server.php?q=[search terms]
if (!empty($_GET['q'])) {

    // Remove any hack attempts from input data
    $search_terms = htmlspecialchars($_GET['q']);

    // Get the application OAuth tokens
    require 'app_tokens.php';

    //get the datumbox api key
    require 'config.php';
    require 'lib/TwitterSentimentAnalysis.php';

    $TwitterSentimentAnalysis = new TwitterSentimentAnalysis(DATUMBOX_API_KEY,
    TWITTER_CONSUMER_KEY,TWITTER_CONSUMER_SECRET,
    TWITTER_ACCESS_KEY,TWITTER_ACCESS_SECRET);

    // Create an OAuth connection
    require 'tmhOAuth.php';
    $connection = new tmhOAuth(array(
      'consumer_key'    =&gt; $consumer_key,
      'consumer_secret' =&gt; $consumer_secret,
      'user_token'      =&gt; $user_token,
      'user_secret'     =&gt; $user_secret
    ));

    // Request the most recent 100 matching tweets
    $http_code = $connection-&gt;request('GET',$connection-&gt;url('1.1/search/tweets'), 
        $twitterSearchParams=array('q' =&gt; $search_terms,
                'count' =&gt; 100,
                'lang' =&gt; 'en',
                'type' =&gt; 'recent'));

    // Search was successful
    if ($http_code == 200) {

        // Extract the tweets from the API response
        $response = json_decode($connection-&gt;response['response'],true);        
        global $TwitterSentinmentAnalysis;

         //Response to be sent to Sentiment API 
        $response= $TwitterSentinmentAnalysis-&gt;sentimentAnalysis($twitterSearchParams);    

        $tweet_data = $response['statuses']; 

        //Sending the Twitter API response(JSONP) direct to a local file
        $file = 'data.json';
        file_put_contents( 'data.json', json_encode($response)); 

        // Load the template for tweet display
        $tweet_template= file_get_contents('tweet_template.html');

        // Load the library of tweet display functions
        require 'display_lib.php';  

        // Create a stream of formatted tweets as HTML
        $tweet_stream = '';


        foreach($tweet_data as $tweet) {

            //if loop to change text color    
            $color=NULL;
            if($tweet['sentiment']=='positive'){            
                $color='#00FF00';
            }
            else if($tweet['sentiment']=='negative'){
                $color='#FF0000';
            }
            else if($tweet['sentiment']=='neutral'){
                $color='#FFFFFF';
            }

            // Ignore any retweets
            if (isset($tweet['retweeted_status'])) {
                continue;
            }

            // Get a fresh copy of the tweet template
            $tweet_html = $tweet_template;

            // Insert this tweet into the html
            $tweet_html = str_replace('[screen_name]',
                $tweet['user']['screen_name'],$tweet_html);
            $tweet_html = str_replace('[name]',
                $tweet['user']['name'],$tweet_html);        
            $tweet_html = str_replace('[profile_image_url]',
                $tweet['user']['profile_image_url'],$tweet_html);
            $tweet_html = str_replace('[tweet_id]',
                $tweet['id'],$tweet_html);
            $tweet_html = str_replace('[tweet_text]',
                linkify($tweet['text']),$tweet_html);
            $tweet_html = str_replace('[created_at]',
                twitter_time($tweet['created_at']),$tweet_html);
            $tweet_html = str_replace('[retweet_count]',
                $tweet['retweet_count'],$tweet_html);           

            // Add the HTML for this tweet to the stream

            $tweet_stream .= $tweet_html;

        }

        // Pass the tweets HTML back to the Ajax request
        print $tweet_stream;

    // Handle errors from API request
    } else {
        if ($http_code == 429) {
            print 'Error: Twitter API rate limit reached';
        } else {
            print 'Error: Twitter was not able to process that search';
        }
    }

} else {
    print 'No search terms found';
}   
?&gt;
</code></pre>

<p>This is the file its calling the function from TwitterSentimentAnalysis.php</p>

<pre><code> public function sentimentAnalysis($twitterSearchParams) {
        $tweets=$this-&gt;getTweets($twitterSearchParams);

        return $this-&gt;findSentiment($tweets);
    }

    /**
</code></pre>
","php, json, twitter, fatal-error, sentiment-analysis","<p>Looks to me like there is a typo in your variable naming (which is why removing the global keyword is throwing a notice).</p>

<p>When you first define your object, it looks like this:</p>

<pre><code>$TwitterSentimentAnalysis = new TwitterSentimentAnalysis(DATUMBOX_API_KEY,
    TWITTER_CONSUMER_KEY,TWITTER_CONSUMER_SECRET,
    TWITTER_ACCESS_KEY,TWITTER_ACCESS_SECRET);
</code></pre>

<p>So that's <code>$TwitterSentimentAnalysis</code>. However you're referencing it later as <code>$TwitterSentinmentAnalysis</code>. It's subtle, but there's an extra n in Sentiment in the second one.</p>

<p>Go ahead and adjust the variable name, remove the <code>global</code> line as it's unnecessary as far as I can tell (and a more uppity dev might go so far as to say global variables are bad form), and I think you're good to go.</p>
",0,0,194,2016-03-27 01:08:23,https://stackoverflow.com/questions/36242771/php-call-to-a-member-function-sentimentanalysis-on-a-non-object
"Naive Bayes in R, Sentiment analysis leads to cannot coerce class error","<p>I have a dataframe <code>df_tweets</code> that has two columns <code>tweets</code> and <code>score</code>.
Score is a factor with values between <code>1 to 5</code></p>

<pre><code>getMatrix &lt;- function(chrVect){
 testsource &lt;- VectorSource(chrVect)
 testcorpus &lt;- Corpus(testsource)
 testcorpus &lt;- tm_map(testcorpus,stripWhitespace)
 testcorpus &lt;- tm_map(testcorpus, removeWords, stopwords('french'))
 testcorpus &lt;- tm_map(testcorpus, removeWords, stopwords('english'))
 testcorpus &lt;- tm_map(testcorpus, content_transformer(tolower))
 testcorpus &lt;- tm_map(testcorpus, removePunctuation)
 testcorpus &lt;- tm_map(testcorpus, removeNumbers)
 testcorpus &lt;- tm_map(testcorpus, PlainTextDocument)

 return(DocumentTermMatrix(testcorpus))
}

op =getMatrix(df_tweets$text)
classifier &lt;-naiveBayes(as.matrix(op), as.factor(df_tweets$avg_score))
</code></pre>

<p>When I use the predict function I get an error </p>

<pre><code>myPrediction&lt;- predict(classifier,op)

Error in as.data.frame.default(newdata) : 
cannot coerce class ""c(""DocumentTermMatrix"", ""simple_triplet_matrix"")"" to a data.frame
</code></pre>

<p>How can I resolve this?</p>
","r, sentiment-analysis","<p>I believe you can wrap <code>as.matrix</code> with <code>as.data.frame</code> or directly with <code>as.matrix.data.frame</code>.</p>
",1,1,508,2016-04-07 17:21:34,https://stackoverflow.com/questions/36483137/naive-bayes-in-r-sentiment-analysis-leads-to-cannot-coerce-class-error
Error while using stanford core nlp,"<p>I m having problem in using the stanford nlp. i am having issues where I'm getting various errors when trying to use the Stanford Core NLP tools.I want to know the sentiment of the sentence passed. But I've not been able to get nlp tools to work when running the code from eclipse with the needed jars added to the classpath,</p>

<p>This is the code i want to execute.</p>

<pre><code>import java.util.Properties;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.SentimentAnnotator;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.TypesafeMap.Key;

public class sentiment_demo {

    public static void sentiment_analysis(String line)
    {
        //Uses Stanford NLP sentimnet analysis
        //found in latest model released from stanford
        // ver 3.3.1
        //applies sentiment analysis to text 

        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        int mainSentiment = 0;
        if (line != null &amp;&amp; line.length() &gt; 0) {
            int longest = 0;
            Annotation annotation = pipeline.process(line);
            for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
                System.out.println(sentence);
                for (Tree token: sentence.get(SentimentCoreAnnotations.AnnotatedTree.class))
                {
                    //System.out.println(token);
                }
                Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
                int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                System.out.println(sentiment);
                String partText = sentence.toString();
                //System.out.println(partText);
                if (partText.length() &gt; longest) {
                    mainSentiment = sentiment;
                    longest = partText.length();
                }

            }
        }
        if(mainSentiment==2)
        {
            System.out.println(""Average"");
        }
        else if(mainSentiment&gt;2)
        {
            System.out.println(""Positive"");
        }
        else if(mainSentiment&lt;2)
        {
            System.out.println(""Negative "");
        }

        if (mainSentiment == 2 || mainSentiment &gt; 4 || mainSentiment &lt; 0) {
            //return null;
        }
    }
    public static void main(String[] args)
    {
        sentiment_analysis(""Cristiano Ronaldo, is a Portuguese professional footballer who plays for Spanish club Real Madrid and the Portugal national team. He is a forward and serves as captain for Portugal.Often ranked as the best player in the world and rated by some in the sport as the greatest of all time"");
    }
}
</code></pre>

<p>Here are the libraries i have set 
<a href=""https://i.sstatic.net/bHQJh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bHQJh.jpg"" alt=""enter image description here""></a></p>

<p>I m using eclipse mars. At first it was showing error for ejml library.but then i imported ejml jar file so that error was resolved but it gave rise to this errors now...</p>

<pre><code>Adding annotator tokenize
Adding annotator ssplit
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [1.6 sec].
Adding annotator sentiment
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.lang.ClassNotFoundException: edu.stanford.nlp.neural.SimpleTensor
    at edu.stanford.nlp.sentiment.SentimentModel.loadSerialized(SentimentModel.java:470)
    at edu.stanford.nlp.pipeline.SentimentAnnotator.&lt;init&gt;(SentimentAnnotator.java:45)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$14.create(StanfordCoreNLP.java:845)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:260)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:127)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.&lt;init&gt;(StanfordCoreNLP.java:123)
    at sentiment_demo.sentiment_analysis(sentiment_demo.java:28)
    at sentiment_demo.main(sentiment_demo.java:70)
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.neural.SimpleTensor
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Unknown Source)
    at java.io.ObjectInputStream.resolveClass(Unknown Source)
    at java.io.ObjectInputStream.readNonProxyDesc(Unknown Source)
    at java.io.ObjectInputStream.readClassDesc(Unknown Source)
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.io.ObjectInputStream.readObject(Unknown Source)
    at java.util.TreeMap.buildFromSorted(Unknown Source)
    at java.util.TreeMap.buildFromSorted(Unknown Source)
    at java.util.TreeMap.readObject(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
    at java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.io.ObjectInputStream.readObject(Unknown Source)
    at java.util.TreeMap.buildFromSorted(Unknown Source)
    at java.util.TreeMap.buildFromSorted(Unknown Source)
    at java.util.TreeMap.readObject(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
    at java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.io.ObjectInputStream.defaultReadFields(Unknown Source)
    at java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.io.ObjectInputStream.defaultReadFields(Unknown Source)
    at java.io.ObjectInputStream.readSerialData(Unknown Source)
    at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
    at java.io.ObjectInputStream.readObject0(Unknown Source)
    at java.io.ObjectInputStream.readObject(Unknown Source)
    at edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:298)
    at edu.stanford.nlp.sentiment.SentimentModel.loadSerialized(SentimentModel.java:466)
</code></pre>

<p>i m stuck at it.If someone have any idea of this then please do suggest,it will be of great help.I have searched other similar questions on stackoverflow but still not getting the solution to resolve this.</p>
","stanford-nlp, sentiment-analysis","<p>You appear to be using an old version of CoreNLP (3.3.0) alongside the new models. Try downloading the 3.6.0 code + models.</p>
",1,0,952,2016-04-12 04:24:26,https://stackoverflow.com/questions/36563597/error-while-using-stanford-core-nlp
Sentimental Analysis of review comments using qdap is slow,"<p>Am using <strong>qdap</strong> package to determine the sentiment of each review comment of a particular application. I read the review comments from a CSV file and pass it to the polarity function of qdap. Everything works fine and I get the polarity for all the review comments but the problem is that it takes 7-8 seconds to calculate the polarity all the sentences (total number of sentences present in the CSV file is 779). I am pasting my code below.</p>

<pre><code>  temp_csv &lt;- filePath()
  attach(temp_csv)
  text_data &lt;- temp_csv[,c('Content')]
  print(Sys.time())
  polterms &lt;- list(neg=c('wtf'))
  POLKEY &lt;- sentiment_frame(positives=c(positive.words),negatives=c(polterms[[1]],negative.words))     
  polarity &lt;- polarity(sentences, polarity.frame = POLKEY) 
  print(Sys.time())
</code></pre>

<p>Time taken is as follows:</p>

<p>[1] ""2016-04-12 16:43:01 IST""</p>

<p>[1] ""2016-04-12 16:43:09 IST""</p>

<p>Can somebody let me know if I am doing something wrong?
How can I improve the performance?</p>
","r, shiny, sentiment-analysis, qdap","<p>I am the author of <strong>qdap</strong>.  The <code>polarity</code> function was designed for much smaller data sets.  As my role shifted I began to work with larger data sets.  I needed fast and accurate (these two things are in opposition to each other) and have since developed a break away package <a href=""https://github.com/trinker/sentimentr#comparing-sentimentr-syuzhet-and-stanford"" rel=""noreferrer"">sentimentr</a>.  The algorithm is optimized to be faster and more accurate than <strong>qdap</strong>'s polarity.</p>

<p>As it stands now you have 5 dictionary based (or trained alorithm based) approached to sentiment detection.  Each has it's drawbacks (-) and pluses (+) and is useful in certain circumstances.</p>

<ol>
<li><a href=""https://github.com/trinker/qdap"" rel=""noreferrer"">qdap </a> +on CRAN; -slow</li>
<li><a href=""https://github.com/mjockers/syuzhet"" rel=""noreferrer"">syuzhet</a> +on CRAN; +fast; +great plotting; -less accurate on non-literature use</li>
<li><a href=""https://github.com/trinker/sentimentr"" rel=""noreferrer"">sentimentr</a> +fast; +higher accuracy; -GitHub only</li>
<li><a href=""https://github.com/trinker/stansent"" rel=""noreferrer"">stansent (stanford port)</a> +most accurate; -slower</li>
<li><a href=""https://github.com/mannau/tm.plugin.sentiment"" rel=""noreferrer"">tm.plugin.sentiment</a> -archived on CRAN; -I couldn't get it working easily</li>
</ol>

<p>I show time tests on sample data for the first 4 choices from above in the code below.</p>

<h1>Install packages and make timing functions</h1>

<p>I use pacman because it allows the reader to just run the code; though you can replace with <code>install.packages</code> &amp; <code>library</code> calls.  </p>

<pre><code>if (!require(""pacman"")) install.packages(""pacman"")
pacman::p_load(qdap, syuzhet, dplyr)
pacman::p_load_current_gh(c(""trinker/stansent"", ""trinker/sentimentr""))

pres_debates2012 #nrow = 2912

tic &lt;- function (pos = 1, envir = as.environment(pos)){
    assign("".tic"", Sys.time(), pos = pos, envir = envir)
    Sys.time()
}

toc &lt;- function (pos = 1, envir = as.environment(pos)) {
    difftime(Sys.time(), get("".tic"", , pos = pos, envir = envir))
}

id &lt;- 1:2912
</code></pre>

<h1>Timings</h1>

<pre><code>## qdap
tic()
qdap_sent &lt;- pres_debates2012 %&gt;%
    with(qdap::polarity(dialogue, id))
toc() # Time difference of 18.14443 secs


## sentimentr
tic()
sentimentr_sent &lt;- pres_debates2012 %&gt;%
    with(sentiment(dialogue, id))
toc() # Time difference of 1.705685 secs


## syuzhet
tic()
syuzhet_sent &lt;- pres_debates2012 %&gt;%
    with(get_sentiment(dialogue, method=""bing""))
toc() # Time difference of 1.183647 secs


## stanford
tic()
stanford_sent &lt;- pres_debates2012 %&gt;%
    with(sentiment_stanford(dialogue))
toc() # Time difference of 6.724482 mins
</code></pre>

<p>For more on timings and accuracy see my <a href=""https://github.com/trinker/sentimentr#comparing-sentimentr-syuzhet-and-stanford"" rel=""noreferrer"">sentimentr README.md</a> and please <strong>star the repo if it's useful</strong>.  The viz below captures one of the tests from the README:</p>

<p><a href=""https://i.sstatic.net/UZwie.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/UZwie.png"" alt=""enter image description here""></a></p>
",12,6,1688,2016-04-12 12:01:12,https://stackoverflow.com/questions/36572677/sentimental-analysis-of-review-comments-using-qdap-is-slow
Extract certain words from sentences using natural language processing,"<p>I'm new to natural language processing. I'm working on a project that extracts what users feel analyzing on what they tweets. </p>

<p>Example: </p>

<p>'I’m so <strong>tired</strong> of not being a multimillionaire' :  User feels tired.</p>

<p>Does anyone have an idea of how to do this and what kind of API or library (Java) can I use?</p>
","java, twitter, nlp, sentiment-analysis","<p>What you are interested in doing belongs to the field in NLP called ""sentiment analysis"". Now, that you know that, googling for possible libraries and methods should be easier. :)</p>

<p>If you are looking for a java library with instructions on how to do sentiment analysis on tweets here's the first thing I found with fairly detailed instructions:</p>

<p><a href=""http://rahular.com/twitter-sentiment-analysis/"" rel=""nofollow"">http://rahular.com/twitter-sentiment-analysis/</a></p>

<p>Have fun!</p>
",2,1,146,2016-04-18 10:38:34,https://stackoverflow.com/questions/36691609/extract-certain-words-from-sentences-using-natural-language-processing
Python: Loaded NLTK Classifier not working,"<p>I'm trying to train a NLTK classifier for sentiment analysis and then save the classifier using pickle.
The freshly trained classifier works fine. However, if I load a saved classifier the classifier will either output 'positive', or 'negative' for ALL examples.</p>

<p>I'm saving the classifier using</p>

<pre><code>classifier = nltk.NaiveBayesClassifier.train(training_set)
classifier.classify(words_in_tweet)
f = open('classifier.pickle', 'wb')
pickle.dump(classifier, f)
f.close()
</code></pre>

<p>and loading the classifier using</p>

<pre><code>f = open('classifier.pickle', 'rb')
classifier = pickle.load(f)
f.close()
classifier.classify(words_in_tweet)
</code></pre>

<p>I'm not getting any errors.
Any idea what the problem could be, or how to debug this correctly?</p>
","python, nltk, pickle, sentiment-analysis, naivebayes","<p>The most likely place a pickled classifier can go wrong is with the feature extraction function. This must be used to generate the feature vectors that the classifier works with. </p>

<p>The <code>NaiveBayesClassifier</code> expects feature vectors for both training and classification; your code looks as if you passed the raw words to the classifier instead (but presumably only after unpickling, otherwise you wouldn't get different behavior before and after unpickling). You should store the feature extraction code in a separate file, and <code>import</code> it in both the training and the classifying (or testing) script.</p>

<p>I doubt this applies to the OP, but some NLTK classifiers take the feature extraction function as an argument to the constructor. When you have separate scripts for training and classifying, it can be tricky to ensure that the unpickled classifier successfully finds the same function.  This is because of the way <code>pickle</code> works: pickling only saves data, not code. To get it to work, just put the extraction function in a separate file (module) that your scripts import. If you put in in the ""main"" script, <code>pickle.load</code> will look for it in the wrong place.</p>
",1,4,278,2016-04-19 18:49:43,https://stackoverflow.com/questions/36727005/python-loaded-nltk-classifier-not-working
SVM feature vector representation by using pre-made dictionary for text classification,"<p>I want to classify a collection of text into two class, let's say I would like to do a sentiment classification. I have two pre-made sentiment dictionaries, one contain only positive words and another contain only negative words. I would like to incorporate these dictionaries into feature vector for SVM classifier. My question is, is it possible to separate between positive and negative words dictionary to be represented as SVM feature vector, especially when I generate feature vector for the test set?</p>

<p>If my explanation is not clear enough, let me give the example. Let's say I have these two sentences as training data:</p>

<pre><code>Pos: The book is good
Neg: The book is bad
</code></pre>

<p>Word 'good' exists in positive dictionary and 'bad' exists in negative dictionary, while other words do not exist in neither dictionary. I want the words that exist in matching dictionary with the sentence's class have a big weight value, while other words have small value. So, the feature vectors will be like these:</p>

<pre><code>+1 1:0.1 2:0.1 3:0.1 4:0.9
-1 1:0.1 2:0.1 3:0.1 5:0.9
</code></pre>

<p>If I want to classify a test sentence ""<em>The food is bad</em>"", how should I generate a feature vector for the test set with weight that depend on existing dictionary when I cannot match test sentence's class with each of the dictionary? What I can think is, for test set, as long as the word exist in both dictionary, I will give the word a high weight value.</p>

<pre><code>0 1:0.1 3:0.1 5:0.9
</code></pre>

<p>I wonder if this is the right way for creating vector representation for both training set and test set.</p>

<p>--Edit--
I forgot to mention that these pre-made dictionaries was extracted using some kind of topic model. For example, the top 100 words from topic 1 are kinda represent positive class and words in topic 2 represent negative class. I want to use this kind of information to improve the classifier more than using only bag-of-words feature.</p>
","machine-learning, svm, sentiment-analysis, text-classification","<p>In short - <strong>this is not the way it works</strong>.</p>

<p>The whole point of <strong>learning</strong> is to give classifier ability to assign these weights on their own. You cannot ""force it"" to have a high value per class for a particular feature (I mean, you could on the optimization level, but this would require changing the whole svm structure).</p>

<p>So the right way is to simply create a ""normal"" representation. Without any additional specification. Let the model decide, they are better at statistical analysis than human intuition, really.</p>
",1,1,546,2016-04-24 10:14:31,https://stackoverflow.com/questions/36821818/svm-feature-vector-representation-by-using-pre-made-dictionary-for-text-classifi
how to suppress the logging messages generated by hadoop on the console,"<p>I am using hadoop 2.5.2.I am new to hadoop and map reduce . I m trying a map reduce code to perform sentiment analysis.But when i execute the code, lot of logging messages are generated by hadoop on the console.Thats the reason it takes a lot of time in giving analysis result for each sentence.Following are the logging messages....</p>

<pre><code>2016-05-12 23:03:05,396 INFO  jvm.JvmMetrics (JvmMetrics.java:init(71)) - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2016-05-12 23:03:05,397 INFO  jvm.JvmMetrics (JvmMetrics.java:init(71)) - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2016-05-12 23:03:05,400 WARN  mapreduce.JobSubmitter (JobSubmitter.java:copyAndConfigureFiles(150)) - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2016-05-12 23:03:05,403 WARN  mapreduce.JobSubmitter (JobSubmitter.java:copyAndConfigureFiles(259)) - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2016-05-12 23:03:05,412 INFO  mapred.FileInputFormat (FileInputFormat.java:listStatus(247)) - Total input paths to process : 3
2016-05-12 23:03:05,421 INFO  mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(396)) - number of splits:3
2016-05-12 23:03:05,430 INFO  mapreduce.JobSubmitter (JobSubmitter.java:printTokens(479)) - Submitting tokens for job: job_local1427897879_0120
2016-05-12 23:03:05,444 WARN  conf.Configuration (Configuration.java:loadProperty(2368)) - file:/home/hduser/workspace/Test1/build/test/mapred/staging/hduser1427897879/.staging/job_local1427897879_0120/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
2016-05-12 23:03:05,445 WARN  conf.Configuration (Configuration.java:loadProperty(2368)) - file:/home/hduser/workspace/Test1/build/test/mapred/staging/hduser1427897879/.staging/job_local1427897879_0120/job.xml:an attempt to override final parameter: hadoop.tmp.dir;  Ignoring.
2016-05-12 23:03:05,445 WARN  conf.Configuration (Configuration.java:loadProperty(2368)) - file:/home/hduser/workspace/Test1/build/test/mapred/staging/hduser1427897879/.staging/job_local1427897879_0120/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
2016-05-12 23:03:05,482 WARN  conf.Configuration (Configuration.java:loadProperty(2368)) - file:/home/hduser/workspace/Test1/build/test/mapred/local/localRunner/hduser/job_local1427897879_0120/job_local1427897879_0120.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
2016-05-12 23:03:05,482 WARN  conf.Configuration (Configuration.java:loadProperty(2368)) - file:/home/hduser/workspace/Test1/build/test/mapred/local/localRunner/hduser/job_local1427897879_0120/job_local1427897879_0120.xml:an attempt to override final parameter: hadoop.tmp.dir;  Ignoring.
2016-05-12 23:03:05,483 WARN  conf.Configuration (Configuration.java:loadProperty(2368)) - file:/home/hduser/workspace/Test1/build/test/mapred/local/localRunner/hduser/job_local1427897879_0120/job_local1427897879_0120.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
2016-05-12 23:03:05,483 INFO  mapreduce.Job (Job.java:submit(1289)) - The url to track the job: http://localhost:8080/
2016-05-12 23:03:05,483 INFO  mapreduce.Job (Job.java:monitorAndPrintJob(1334)) - Running job: job_local1427897879_0120
2016-05-12 23:03:05,483 INFO  mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(471)) - OutputCommitter set in config null
2016-05-12 23:03:05,484 INFO  mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(489)) - OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
2016-05-12 23:03:05,485 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(448)) - Waiting for map tasks
2016-05-12 23:03:05,485 INFO  mapred.LocalJobRunner (LocalJobRunner.java:run(224)) - Starting task: attempt_local1427897879_0120_m_000000_0
2016-05-12 23:03:05,486 INFO  mapred.Task (Task.java:initialize(587)) -  Using ResourceCalculatorProcessTree : [ ]
2016-05-12 23:03:05,486 INFO  mapred.MapTask (MapTask.java:updateJobWithSplit(462)) - Processing split: file:/home/hduser/workspace/Test1/training/pool.txt:0+17961
2016-05-12 23:03:05,487 INFO  mapred.MapTask (MapTask.java:runOldMapper(416)) - numReduceTasks: 1
2016-05-12 23:03:05,487 INFO  mapred.MapTask (MapTask.java:createSortingCollector(388)) - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2016-05-12 23:03:05,513 INFO  mapred.MapTask (MapTask.java:setEquator(1182)) - (EQUATOR) 0 kvi 26214396(104857584)
2016-05-12 23:03:05,513 INFO  mapred.MapTask (MapTask.java:init(975)) - mapreduce.task.io.sort.mb: 100
2016-05-12 23:03:05,514 INFO  mapred.MapTask (MapTask.java:init(976)) - soft limit at 83886080
2016-05-12 23:03:05,514 INFO  mapred.MapTask (MapTask.java:init(977)) - bufstart = 0; bufvoid = 104857600
2016-05-12 23:03:05,514 INFO  mapred.MapTask (MapTask.java:init(978)) - kvstart = 26214396; length = 6553600
2016-05-12 23:03:05,516 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 
2016-05-12 23:03:05,516 INFO  mapred.MapTask (MapTask.java:flush(1437)) - Starting flush of map output
2016-05-12 23:03:05,516 INFO  mapred.MapTask (MapTask.java:flush(1455)) - Spilling map output
2016-05-12 23:03:05,516 INFO  mapred.MapTask (MapTask.java:flush(1456)) - bufstart = 0; bufend = 17961; bufvoid = 104857600
2016-05-12 23:03:05,516 INFO  mapred.MapTask (MapTask.java:flush(1458)) - kvstart = 26214396(104857584); kvend = 26211024(104844096); length = 3373/6553600
2016-05-12 23:03:05,523 INFO  mapred.MapTask (MapTask.java:sortAndSpill(1641)) - Finished spill 0
2016-05-12 23:03:05,524 INFO  mapred.Task (Task.java:done(1001)) - Task:attempt_local1427897879_0120_m_000000_0 is done. And is in the process of committing
2016-05-12 23:03:05,525 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - file:/home/hduser/workspace/Test1/training/pool.txt:0+17961
2016-05-12 23:03:05,525 INFO  mapred.Task (Task.java:sendDone(1121)) - Task 'attempt_local1427897879_0120_m_000000_0' done.
2016-05-12 23:03:05,525 INFO  mapred.LocalJobRunner (LocalJobRunner.java:run(249)) - Finishing task: attempt_local1427897879_0120_m_000000_0
2016-05-12 23:03:05,525 INFO  mapred.LocalJobRunner (LocalJobRunner.java:run(224)) - Starting task: attempt_local1427897879_0120_m_000001_0
2016-05-12 23:03:05,525 INFO  mapred.Task (Task.java:initialize(587)) -  Using ResourceCalculatorProcessTree : [ ]
2016-05-12 23:03:05,526 INFO  mapred.MapTask (MapTask.java:updateJobWithSplit(462)) - Processing split: file:/home/hduser/workspace/Test1/training/pool.txt~:0+17939
2016-05-12 23:03:05,526 INFO  mapred.MapTask (MapTask.java:runOldMapper(416)) - numReduceTasks: 1
2016-05-12 23:03:05,527 INFO  mapred.MapTask (MapTask.java:createSortingCollector(388)) - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2016-05-12 23:03:05,550 INFO  mapred.MapTask (MapTask.java:setEquator(1182)) - (EQUATOR) 0 kvi 26214396(104857584)
2016-05-12 23:03:05,550 INFO  mapred.MapTask (MapTask.java:init(975)) - mapreduce.task.io.sort.mb: 100
2016-05-12 23:03:05,550 INFO  mapred.MapTask (MapTask.java:init(976)) - soft limit at 83886080
2016-05-12 23:03:05,550 INFO  mapred.MapTask (MapTask.java:init(977)) - bufstart = 0; bufvoid = 104857600
2016-05-12 23:03:05,550 INFO  mapred.MapTask (MapTask.java:init(978)) - kvstart = 26214396; length = 6553600
2016-05-12 23:03:05,552 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(591)) - 
</code></pre>

<p>I have referred the similar question already asked on stackoverflow [<a href=""https://stackoverflow.com/questions/7801610/how-to-suppress-hadoop-logging-message-on-the-console]"">how to suppress Hadoop logging message on the console</a> but it hasnt helped me niether clear answer is discussed. I tried using following configuration suggested in some forum but its not working.I have tried setting following in hadoop-env.sh</p>

<pre><code>export HADOOP_HOME_WARN_SUPPRESS=1
export HADOOP_ROOT_LOGGER=""WARN,DRFA""
</code></pre>

<p>I have also tried editing the log4j.properties file by setting following values..</p>

<pre><code>hadoop.root.logger=WARN,DRFA 
hadoop.log.dir=. 
hadoop.log.file=hadoop.log
</code></pre>

<p>But still i am not able to get rid of this logging messages generated by hadoop mapreduce during runtime execution.Its delaying my output on the console as well.Is their any workaround or any  java code which i can embed in my code and suppress the messages.any suggestions,help..Anyone knows?? </p>

<p>Thanks a lot!! </p>
","java, hadoop, mapreduce, sentiment-analysis","<p>I have found solution for this.All that it needs is changing the configuration file of mapreduce.</p>

<p>1.mapreduce.map.log.level can take values as OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting  could be overridden if ""mapreduce.job.log4j-properties-file"" is set.</p>

<ol start=""2"">
<li>mapreduce.reduce.log.level can also take values as OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting could be overridden if ""mapreduce.job.log4j-properties-file"" is set.So its better to make sure that ""mapreduce.job.log4j-properties-file"" is not set.</li>
</ol>

<p>We have to set following properties in the mapred-site.xml.</p>

<pre><code>&lt;property&gt;
&lt;name&gt;mapreduce.map.log.level&lt;/name&gt;
&lt;value&gt;OFF&lt;/value&gt;
&lt;/property&gt;


&lt;property&gt;
&lt;name&gt;mapreduce.reduce.log.level&lt;/name&gt;
&lt;value&gt;OFF&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>​Now i can see no log messages on the console.But it also has disadvantage as we cant figure out any error if it occurs while executing the mapreduce code as no log messages are visible.</p>
",1,2,1542,2016-05-12 17:56:32,https://stackoverflow.com/questions/37194248/how-to-suppress-the-logging-messages-generated-by-hadoop-on-the-console
How to omit tokenize and ssplit annotators for Sentiment Analysis,"<p>For the task of sentiment analysis on a text, I am using the following annotators to create a pipeline:</p>

<p>annotators = tokenize, ssplit, parse, sentiment</p>

<p>After reading the documentation on annotators, I realized that tokenize and ssplit take the whole text and break it up into separate sentences to be consdiered for further parsing.
The problem on which I am working currently is sentiment analysis of tweets. Since tweets most of the times do not exceed more than a line, using a tokenize and ssplit annotator before parse seems overkill. </p>

<p>I tried to exclude the first two but it won't let me do giving out a message Exception in thread ""main"" java.lang.IllegalArgumentException: annotator ""parse"" requires annotator ""tokenize""</p>

<p>Is there any way to avoid using the tokenize and ssplit annotators to imrpove efficiency ?</p>
","nlp, stanford-nlp, sentiment-analysis","<p>Yes, if your text is already tokenized and you have a file with one sentence per line, you can tell the tokenizer to split tokens only at spaces and the sentence splitter to split sentences only at newlines.</p>

<p>The option for the tokenizer is <code>-tokenize.whitespace true</code> and the option for the sentence splitter <code>-ssplit.eolonly true</code>.</p>

<p>You can find more information on the options of the <a href=""http://stanfordnlp.github.io/CoreNLP/tokenize.html"" rel=""nofollow"">tokenizer</a> and the <a href=""http://stanfordnlp.github.io/CoreNLP/ssplit.html"" rel=""nofollow"">sentence splitter</a> in the <a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow"">CoreNLP documentation</a>.</p>
",2,0,158,2016-05-12 21:14:19,https://stackoverflow.com/questions/37197503/how-to-omit-tokenize-and-ssplit-annotators-for-sentiment-analysis
nested for-loop in R language,"<p>I have this code to calculate duplicate in a data frame using cosine similarity through firstly: first loop (nrow) times to take in each time one tweet then compares the cosine similarity results to this tweet with other tweets using second loop.</p>

<p>Here is my code:</p>

<pre><code>for (i in 1:nrow(temp)) {
  dup=0
  one_Tweets = tweets$Tweet[i]
  cos_similarity = data.frame(""v1""=NULL) # NULL So that don't write previous value
  cos_similarity=data.frame(sim &lt;- round( sim.strings(AllTweets,one_Tweets), digits = 3) )
  names(cos_similarity) = c( ""v1"")

  for (b in i+1:nrow(temp)) {
    Tweet_cos=cos_similarity$v1[b]
    if ( Tweet_cos &gt;= 0.900) {
      count = count+1
      tweets$flag[b]= 1
    }else { #if ( Tweet_cos &lt;0.900) {
      tweets$flag[b]= 2
    }
    Tweet_cos=0
  }
  dup=tweets$duplicate[i]= tweets$duplicate[i]+count 
  count = 0
}
</code></pre>

<p>I have a problem in first loop, entered one time although that number of tweets in data frame 10000 tweets.</p>

<p>and i get the error:</p>

<pre><code>Error in if (Tweet_cos &gt;= 0.9) { : missing value where TRUE/FALSE needed
</code></pre>
","r, twitter, sentiment-analysis","<p>I dont still have rep to put it in comment but I think you are getting this problem because of NA/NULL in Tweet_cos vector. to debug remove this part from code:</p>

<pre><code>    for (b in i+1:nrow(temp)) {
    Tweet_cos=cos_similarity$v1[b]
    if ( Tweet_cos &gt;= 0.900) {
      count = count+1
      tweets$flag[b]= 1
    }else { #if ( Tweet_cos &lt;0.900) {
      tweets$flag[b]= 2
    }
    Tweet_cos=0
  }
  dup=tweets$duplicate[i]= tweets$duplicate[i]+count 
  count = 0
</code></pre>

<p>replace whole of this with <code>print(cos_similarity$v1)</code>. You should ideally see some NA/NULL which by def could not be compared with 0.9 and hence the error. </p>

<p>If there are too many iterations/loop then try to print values of <code>i</code> and <code>b</code> where you are getting error and print <code>cos_similarity$v1</code> only for that.</p>

<p>Please consider sharing small sample data so that others can replicate your problem</p>
",0,1,101,2016-05-13 02:48:29,https://stackoverflow.com/questions/37200450/nested-for-loop-in-r-language
how to obtain maximum value from database,"<p>I am working on sentiment analysis and have obtained reviews of many recipes. I have stored recipe name, rating and reviews in database. I want highest review from each of the recipes. I am unable to do this.Please help. Thank you<a href=""https://i.sstatic.net/5bgpd.png"" rel=""nofollow"">enter image description here</a></p>
","java, database, sentiment-analysis","<p>You can try something like the query below to get your highest review.</p>

<pre><code>SELECT A.DISHNAME, A.RATE, A.REVIEW FROM DISHRATE A, (
SELECT DISHNAME, MAX(RATE) AS RATE FROM DISHRATE GROUP BY DISHNAME
) B 
WHERE A.DISHNAME=B.DISHNAME AND A.RATE=B.RATE
</code></pre>
",0,0,26,2016-05-14 17:27:20,https://stackoverflow.com/questions/37229747/how-to-obtain-maximum-value-from-database
Machine Learning (tensorflow / sklearn) in Django?,"<p>I have a django form, which is collecting user response. I also have a tensorflow sentences classification model. What is the best/standard way to  put these two together.
Details: </p>

<ol>
<li>tensorflow model was trained on the Movie Review data from Rotten Tomatoes.</li>
<li>Everytime a new row is made in my response model , i want the tensorflow code to classify it( + or - ).</li>
<li>Basically I have a django project directory and two .py files for classification. Before going ahead myself , i wanted to know what is the standard way to implement machine learning algorithms to a web app.</li>
</ol>

<p>It'd be awesome if you could suggest a tutorial or a repo.
Thank you !</p>
","django, machine-learning, scikit-learn, tensorflow, sentiment-analysis","<h2>Asynchronous processing</h2>

<p>If you <strong>don't need the classification result from the ML code to pass <em>immediately</em> to the user</strong> (e.g. as a response to the same POST request that submtted), then you can always queue the classification job to be ran in the background or even a different server with more CPU/memory resources (e.g. with <a href=""https://pypi.python.org/pypi/django-background-tasks"" rel=""noreferrer"">django-background-tasks</a> or <a href=""http://www.celeryproject.org"" rel=""noreferrer"">Celery</a>)</p>

<p>A queued task would be for example to populate the field <code>UserResponse.class_name</code> (positive, negative) on the database rows that have that field blank (not yet classified)</p>

<h2>Real time notification</h2>

<p>If the ML code is slow and <strong>want to return that result to the user</strong> as soon as it is available, you can use the asynchronous approach described above, and pair with the real time notification (e.g. <a href=""http://socket.io"" rel=""noreferrer"">socket.io</a> to the browser (<a href=""https://stackoverflow.com/a/35438284/238639"">this can be triggered from the queued task</a>)</p>

<p>This becomes necessary if ML execution time is so long that it might time-out the HTTP request in the synchronous approach described below.</p>

<h2>Synchronous processing, if ML code is not CPU intensive (fast enough)</h2>

<p>If you need that classification result returned immediately, and the ML classification is <strong>fast enough *</strong>, you can do so within the HTTP request-response cycle (the POST request returns after the ML code is done, synchronously)</p>

<p><sub>*Fast enough here means it wouldn't time-out the HTTP request/response, and the user wouldn't lose patience.</sub></p>
",32,28,25230,2016-05-22 12:36:22,https://stackoverflow.com/questions/37374454/machine-learning-tensorflow-sklearn-in-django
CountVectorizer reading and writing vocabulary,"<p>I am currently working on a fairly trivial sentiment classification program. Everything works well in the training phase. However, I am having trouble using CountVectorizer to test new strings of text that contain unseen words.</p>

<p>For this reason  I am trying to write a lookup vocabulary for vectorization in the testing phase. However, I don't know how to create and retrieve the vocabulary object to pass as a parameter.</p>

<p>My two methods currently appear as follows:</p>

<pre><code>def trainingVectorTransformation (messages):
    #--&gt; ReviewText to vectors    
    vect = CountVectorizer(analyzer=split_into_lemmas).fit(messages['reviewText'])

    messages_bow = vect.transform(messages['reviewText'])

    feature_list = vect.get_feature_names()
    #NOT SURE HOW TO CREATE VOCABULARY
    with open(""vocab.txt"", ""w"") as text_file:
        text_file.write(str(feature_list))   

    tfidf_transformer = TfidfTransformer().fit(messages_bow)


    messages_tfidf = tfidf_transformer.transform(messages_bow)
    return messages_tfidf
</code></pre>

<p>and </p>

<pre><code>def testingVectorTransformation (messages):
    #--&gt; ReviewText to vectors
    #NOT SURE HOW TO READ THE CREATED VOCABULARY AND USE IT APPROPRIATELY   
    txt = open(""vocab.txt"")
    vocabulary = txt.read()


    vect = CountVectorizer(analyzer=split_into_lemmas, vocabulary = vocabulary).fit(messages['reviewText'])

    messages_bow = vect.transform(messages['reviewText'])

    tfidf_transformer = TfidfTransformer().fit(messages_bow)

    messages_tfidf = tfidf_transformer.transform(messages_bow)
    return messages_tfidf
</code></pre>

<p>If anyone has any advice on how to properly create and use the vocabulary I would very much appreciate it.</p>
","python, machine-learning, scikit-learn, vectorization, sentiment-analysis","<p>You need to save a copy of your vectorizer using some serializer, e.g. pickle and load it in the test phase. You can also get the vocab using,  vocabulary_ attribute see <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow"">here</a> for more details </p>

<p>Also looking at your code, in training you should call vect.fit_transform not just transform</p>
",0,0,1157,2016-05-27 13:08:35,https://stackoverflow.com/questions/37484369/countvectorizer-reading-and-writing-vocabulary
Alchemy API in Matlab,"<p>Is it possible to use the Alchemy API in Matlab? I want to call the <code>URLGetTextSentiment</code> method, without using other IDE's.
Would be great if you have some experience with that and share it with me.</p>
","matlab, ibm-cloud, sentiment-analysis, alchemyapi","<p>AlchemyAPI provides REST API endpoints for all text-mining and content analysis functionality. You can call those REST APIs from wherever you need, once you know your API key. In Matlab you can interact with RESTful web services using <a href=""http://www.mathworks.com/help/matlab/ref/webread.html"" rel=""nofollow""><em>webread</em></a> and <a href=""http://www.mathworks.com/help/matlab/ref/webwrite.html"" rel=""nofollow""><em>webwrite</em></a>.</p>
",1,0,135,2016-05-28 23:24:41,https://stackoverflow.com/questions/37504665/alchemy-api-in-matlab
Confused with &quot;sentiment&quot; package in R?,"<p>I've been using R's <code>sentiment</code> package for sentiment analysis. I was shocked when some trivial negatives as positives for many of my documents. For instance</p>

<pre><code>library(""sentiment"")
classify_polarity(""Not good"")

#      POS                NEG                 POS/NEG            BEST_FIT  
# [1,] ""8.78232285939751"" ""0.445453222112551"" ""19.7154772340574"" ""positive""
</code></pre>

<p>I'm not sure what's happening behind this. Can someone clarify this?</p>
","r, sentiment-analysis","<p>Thanks rawr. I found this helpful. </p>

<pre><code>&gt;library(qdap)
&gt; polarity(""Not Good"")
  all total.sentences total.words ave.polarity sd.polarity stan.mean.polarity
  1 all               1           2       -0.707          NA                 NA
&gt; polarity(""It's cool but not great"")
  all total.sentences total.words ave.polarity sd.polarity stan.mean.polarity
  1 all               1           5       -0.894          NA                 NA
&gt; polarity(""It's awesome"")
  all total.sentences total.words ave.polarity sd.polarity stan.mean.polarity
  1 all               1           2        0.707          NA                 NA
</code></pre>
",3,2,126,2016-05-30 14:28:15,https://stackoverflow.com/questions/37528121/confused-with-sentiment-package-in-r
Why is not TextBlob using / detecting the negation?,"<p>I am using TextBlob to perform a sentiment analysis task. I have noticed that TextBlob is able to detect the negation in some cases while in other cases not. </p>

<p>Here are two simple examples</p>

<pre><code>&gt;&gt;&gt; from textblob.sentiments import PatternAnalyzer

&gt;&gt;&gt; sentiment_analyzer = PatternAnalyzer()
# example 1
&gt;&gt;&gt; sentiment_analyzer.analyze('This is good')
Sentiment(polarity=0.7, subjectivity=0.6000000000000001)

&gt;&gt;&gt; sentiment_analyzer.analyze('This is not good')
Sentiment(polarity=-0.35, subjectivity=0.6000000000000001)

# example 2
&gt;&gt;&gt; sentiment_analyzer.analyze('I am the best')
Sentiment(polarity=1.0, subjectivity=0.3)

&gt;&gt;&gt; sentiment_analyzer.analyze('I am not the best')  
Sentiment(polarity=1.0, subjectivity=0.3)
</code></pre>

<p>As you can see in the second example when using the adjective <code>best</code> the polarity is not changing. I suspect that has to do with the fact that the adjective <code>best</code> is a very strong indicator, but doesn't seem right because the negation should have reversed the polarity (in my understanding).</p>

<p>Can anyone explain a little bit what's going? Is textblob using some negation mechanism at all or is it just that the word <code>not</code> is adding negative sentiment to the sentence? In either case, why does the second example has exactly the same sentiment in both cases? Is there any suggestion about how to overcome such obstacles?</p>
","python, sentiment-analysis, textblob","<p><em>(edit: my old answer was more about general classifiers and not about PatternAnalyzer)</em></p>
<p>TextBlob uses in your code the &quot;PatternAnalyzer&quot;. Its behaviour is briefly discribed in that document: <a href=""http://www.clips.ua.ac.be/pages/pattern-en#parser"" rel=""nofollow noreferrer"">http://www.clips.ua.ac.be/pages/pattern-en#parser</a></p>
<p>We can see that:</p>
<blockquote>
<p>The pattern.en module bundles a <strong>lexicon of adjectives</strong> (e.g., good, bad, amazing, irritating, ...) that occur frequently in product reviews, annotated <strong>with scores for sentiment polarity</strong> (positive ↔ negative) and subjectivity (objective ↔ subjective).</p>
<p>The sentiment() function returns a (polarity, subjectivity)-tuple for the given sentence, <strong>based on the adjectives it contains</strong>,</p>
</blockquote>
<p>Here's an example that shows the behaviour of the algorithm. The polarity directly depends on the adjective used.</p>
<pre><code>sentiment_analyzer.analyze('player')
Sentiment(polarity=0.0, subjectivity=0.0)

sentiment_analyzer.analyze('bad player')
Sentiment(polarity=-0.6999998, subjectivity=0.66666)

sentiment_analyzer.analyze('worst player')
Sentiment(polarity=-1.0, subjectivity=1.0)

sentiment_analyzer.analyze('best player')
Sentiment(polarity=1.0, subjectivity=0.3)
</code></pre>
<p>Professionnal softwares generally use complex tools based on neural networks and classifiers combined with lexical analysis. But for me, TextBlob just tries to give a result based on a <strong>direct result from the grammar analysis</strong> (here, the polarity of the adjectives). It's the source of the problem.</p>
<p>It does <strong>not try to check if the general sentence is negative</strong> or not (with the &quot;not&quot; word). It tries to check <strong>if the adjective is negated</strong> or not (as it works only with adjective, not with the general structure). Here, best is used as a noun and is not a negated adjective. So, the polarity is positive.</p>
<pre><code>sentiment_analyzer.analyze('not the best')
Sentiment(polarity=1.0, subjectivity=0.3)
</code></pre>
<p>Just remplace the order of the words to make negation over the adjective and not the whole sentence.</p>
<pre><code>sentiment_analyzer.analyze('the not best')
Sentiment(polarity=-0.5, subjectivity=0.3)
</code></pre>
<p>Here, the adjective is negated. So, the polarity is negative.
It's my explaination of that &quot;strange behaviour&quot;.</p>
<hr />
<p>The real implementation is defined in file:
<a href=""https://github.com/sloria/TextBlob/blob/dev/textblob/_text.py"" rel=""nofollow noreferrer"">https://github.com/sloria/TextBlob/blob/dev/textblob/_text.py</a></p>
<p>The interresing portion is given by:</p>
<pre><code>if w in self and pos in self[w]:
    p, s, i = self[w][pos]
    # Known word not preceded by a modifier (&quot;good&quot;).
    if m is None:
        a.append(dict(w=[w], p=p, s=s, i=i, n=1, x=self.labeler.get(w)))
    # Known word preceded by a modifier (&quot;really good&quot;).
    
    ...
    

else:
    # Unknown word may be a negation (&quot;not good&quot;).
    if negation and w in self.negations:
        n = w
    # Unknown word. Retain negation across small words (&quot;not a good&quot;).
    elif n and len(w.strip(&quot;'&quot;)) &gt; 1:
        n = None
    # Unknown word may be a negation preceded by a modifier (&quot;really not good&quot;).
    if n is not None and m is not None and (pos in self.modifiers or self.modifier(m[0])):
        a[-1][&quot;w&quot;].append(n)
        a[-1][&quot;n&quot;] = -1
        n = None
    # Unknown word. Retain modifier across small words (&quot;really is a good&quot;).
    elif m and len(w) &gt; 2:
        m = None
    # Exclamation marks boost previous word.
    if w == &quot;!&quot; and len(a) &gt; 0:
    
    ...
</code></pre>
<p>If we enter &quot;not a good&quot; or &quot;not the good&quot;, it will match the else part because it's not a single adjective.</p>
<p>The &quot;not a good&quot; part will match <code>elif n and len(w.strip(&quot;'&quot;)) &gt; 1:</code> so it will reverse polarity. <code>not the good</code> will not match any pattern, so, the polarity will be the same of &quot;best&quot;.</p>
<p>The entire code is a succession of fine tweaking, grammar indictions (such as adding ! increases polarity, adding a smiley indicates irony, ...). It's why some particular patterns will give strange results. To handle each specific case, you must check if your sentence will match any of the if sentences in that part of the code.</p>
<p>I hope I'll help</p>
",4,4,3099,2016-06-04 19:08:33,https://stackoverflow.com/questions/37634016/why-is-not-textblob-using-detecting-the-negation
Does the ratio of two classes matter in classification problems?,"<p>I am building a sentiment analysis program using some tweets i have gathered. The labeled data which i have gathered would go through a neural network which classifies them into two classes, positive and negative.</p>

<p>The data is still being labeled. So far i have observed that the positive category has very small number of observations.</p>

<p>The records for positive category in my training set could be about 5% of the training data set (the same ratio could reflect in the population as well).</p>

<p>Would this create problems in the final ""program"" ?
Size of the data set is about 5000 records.</p>
","machine-learning, sentiment-analysis","<p>Yes, yes it can. There are two things to consider:</p>

<ol>
<li>5% of 5000 is 250. Thus you will try to model data distribution of your class based on just 250 samples. This might be orders of magnitude to small for neural network. Consequently you might need 40x more data to have a representative sample of your data. While you can easily reduce the majority class through subsampling, without the big risk of destroing the structure - there is no way to get ""more structure"" from less points (you can replicate points, add noise etc. but this does not add structure, this just adds assumptions).</li>
<li>Class imbalance can also lead to convergence to naive solutions, like ""always False"" which has 95% accuracy. Here you can simply play around with the cost function to make it more robust to imbalance (in particular - train splits suggested by @PureW is nothing else like ""black box"" method of trying to change the loss function so it has bigger weight on minority class. When you have access to your classifier loss, like in NN you should not due this - but instead change the cost function and still keep all the data).</li>
</ol>
",4,1,1502,2016-06-05 07:31:39,https://stackoverflow.com/questions/37639516/does-the-ratio-of-two-classes-matter-in-classification-problems
Need clarification on the calculation of average polarity score returned by sentiment function of sentimentr(trinker),"<p>I am using sentiment analysis function sentiment_by() from R package sentimentr (by trinker). I have a dataframe containing the following columns:
review comments
month
year
I ran the sentiment_by function on the dataframe to find the average polarity score based on the year and month and i get the following values.</p>

<pre><code>review_year review_month    word_count  sd  ave_sentiment
2015       March        8722    0.381686065 0.163440921
2015       April        7758    0.387046768 0.158812775
2015       May          7333    0.389256472 0.149220636
2015       November    14020    0.394711478 0.14691745
2016       February     7974    0.400406931 0.142345278
2015       September    8238    0.379989344 0.141740366
2015       February     7642    0.361415304 0.141624745
2015       December    24863    0.387409099 0.141606892
2016       March        8229    0.389033232 0.138552943
2016       January      10472   0.388300946 0.134302612
2015       August       7520    0.3640285   0.127980712
2016       May          3432    0.422246851 0.125041218
2015       June         8678    0.356612924 0.119333949
2015       January      9930    0.351126449 0.119225549
2016       April        9344    0.397066458 0.111879315
2015       July         8450    0.349963536 0.108881821
2015       October      7630    0.38017201  0.1044298
</code></pre>

<p>Now i run the sentiment_by function on the dataframe based on the comments alone and then i run the following function on the resultant data frame to find the average polarity score based on year and months.</p>

<pre><code>sentiment_df[,list(avg=mean(ave_sentiment)),by=""month,year""]
</code></pre>

<p>I get the following results.</p>

<pre><code>month       year        avg
January     2015    0.110950199
February    2015    0.126943461
March       2015    0.146546669
April       2015    0.148264268
May         2015    0.143924126
June        2015    0.110691204
July        2015    0.106472437
August      2015    0.118976304
September   2015    0.135362187
October     2015    0.111441484
November    2015    0.137699548
December    2015    0.136786867
January     2016    0.128645808
February    2016    0.129139898
March       2016    0.134595706
April       2016    0.12106743
May         2016    0.142801514
</code></pre>

<p>As per my understanding both should return the same results, correct me if I am wrong. Reason for me to go for the second approach is because i need to average polarity based on both month and year, as well as based on months and i don't want to use the method twice as it will cause additional time delay. Could some one let me know what i am doing wrong here?</p>
","r, sentiment-analysis, sentimentr","<p>Here is an idea: Maybe the first function is taking the averages from the individual sentences, and the second one is taking the average from the ""ave sentiment"", which is already an average. So, the average of averages is not always equal to the average of the individual elements. </p>
",2,1,728,2016-06-17 13:56:07,https://stackoverflow.com/questions/37883474/need-clarification-on-the-calculation-of-average-polarity-score-returned-by-sent
Sentiment Analysis for local languages (Nepali),"<p>I would like to do sentiment analysis on document level. But I am try to do sentiment analysis Nepali. So, I dont have any resources. I can't do Naive Bayes Classifier as I don't have any labelled data and I can't do vai wordnet as no nepali wordnet exist. Papers I read generally had labelled data or senti-wordnet for other languages. </p>

<p>I would like know these things:</p>

<ul>
<li>Which approach should I use in above case for sentiment analysis?</li>
<li>Is there any method for me to dynamically generate labels for data?</li>
</ul>
","localization, nlp, sentiment-analysis, naivebayes, senti-wordnet","<p>Since you don't have any labelled data, Have a look at <a href=""https://github.com/caffeinator13/neuralnets/tree/master/src/HandwritingRecognition"" rel=""nofollow"">this</a> GitHub Repo, feel free to fork.</p>

<p>It has the code for neural network for Handwriting recognition in Java. Jeff Heaton has done it easy for us, with a nice UI, you can train this model to recognize Nepali.</p>

<p>And for sentiment Analysis, you can try using <a href=""https://opennlp.apache.org/documentation/1.6.0/manual/opennlp.html"" rel=""nofollow"">Opennlp</a> which has some good support,<a href=""http://self-learning-java-tutorial.blogspot.in/2015/10/opennlp-tutorial.html"" rel=""nofollow""> this blog for Beginner's</a>.</p>

<p>Also <a href=""https://depiesml.wordpress.com/2015/08/26/dl4j-gettingstarted/"" rel=""nofollow"">DL4J</a> is a good library for deep learing for Java which can be used for Sentiment Analysis. It has a good Word2Vector Implementation and has a lot of support.</p>

<p>These resources will help you, any futher doubts-feel free to comment.</p>
",2,0,393,2016-07-05 07:28:47,https://stackoverflow.com/questions/38197617/sentiment-analysis-for-local-languages-nepali
Python sentiment classification,"<p>Hi i am trying to classify certain text file by using sentiment classifier package.The following program works fine for a single sentence i.e.,</p>

<pre><code>from senti_classifier import senti_classifier
sentences = ['i love u']
pos_score, neg_score = senti_classifier.polarity_scores(sentences)
print pos_score, neg_score
</code></pre>

<p>But when done the following way by using an xls file to classify every record the result is a 0.0 for both positive and negative score.
Please help me out.</p>

<pre><code>import openpyxl
from senti_classifier import senti_classifier
wb = openpyxl.load_workbook('sentiment2.xlsx')
sheet = wb.active
sheet.columns[0]
for cellObj in sheet.columns[0]:
    sentences = cellObj.value
    print(sentences)
    pos_score, neg_score = senti_classifier.polarity_scores(sentences)
    print pos_score, neg_score
</code></pre>
","python, nltk, sentiment-analysis","<p><a href=""https://github.com/kevincobain2000/sentiment_classifier/blob/master/src/senti_classifier/senti_classifier.py#L207"" rel=""nofollow""><code>senti_classifier.polarity_scores()</code> function</a> expects a <em>list of strings</em> as an argument, but you are passing a single string. Put it into the list:</p>

<pre><code>pos_score, neg_score = senti_classifier.polarity_scores([sentences])
</code></pre>
",1,1,435,2016-07-06 00:38:15,https://stackoverflow.com/questions/38214855/python-sentiment-classification
Is Doc2Vec suited for Sentiment Analysis?,"<p>I have been reading more modern posts about sentiment classification (analysis) such as <a href=""http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis"" rel=""nofollow"">this</a>.</p>

<p>Taking the IMDB dataset as an example I find that I get a similar accuracy percentage using Doc2Vec (88%), <strong>however a far better result using a simple tfidf vectoriser with tri-grams for feature extraction (91%)</strong>. I think this is similar to Table 2 in <a href=""http://arxiv.org/pdf/1412.5335v7.pdf"" rel=""nofollow"">Mikolov's 2015 paper</a>.</p>

<p>I thought that by using a bigger data-set this would change. So I re-ran my experiment using a breakdown of 1mill training and 1 mill test from <a href=""http://jmcauley.ucsd.edu/data/amazon/"" rel=""nofollow"">here</a>. Unfortunately, in that case my tfidf vectoriser feature extraction method increased to 93% but doc2vec fell to 85%.</p>

<p><strong>I was wondering if this is to be expected and that others find tfidf to be superior to doc2vec even for a large corpus?</strong></p>

<p>My data-cleaning is simple:</p>

<pre><code>def clean_review(review):
    temp = BeautifulSoup(review, ""lxml"").get_text()
    punctuation = """""".,?!:;(){}[]""""""
    for char in punctuation
        temp = temp.replace(char, ' ' + char + ' ')
    words = "" "".join(temp.lower().split()) + ""\n""
    return words
</code></pre>

<p>And I have tried using 400 and 1200 features for the Doc2Vec model:</p>

<pre><code>model = Doc2Vec(min_count=2, window=10, size=model_feat_size, sample=1e-4, negative=5, workers=cores)
</code></pre>

<p>Whereas my tfidf vectoriser has 40,000 max features:</p>

<pre><code>vectorizer = TfidfVectorizer(max_features = 40000, ngram_range = (1, 3), sublinear_tf = True)
</code></pre>

<p>For classification I experimented with a few linear methods, however found simple logistic regression to do OK...</p>
","machine-learning, sentiment-analysis, gensim, word2vec, doc2vec","<p>The example code Mikolov once posted (<a href=""https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ"" rel=""nofollow"">https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ</a>) used options <code>-cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1</code> – which in gensim would be similar to <code>dm=0, dbow_words=1, size=100, window=10, hs=0, negative=5, sample=1e-4, iter=20, min_count=1, workers=cores</code>.</p>

<p>My hunch is that optimal values might involve a smaller <code>window</code> and higher <code>min_count</code>, and maybe a <code>size</code> somewhere between 100 and 400, but it's been a while since I've run those experiments. </p>

<p>It can also sometimes help a little to re-infer vectors on the final model, using a larger-than-the-default <code>passes</code> parameter, rather than re-using the bulk-trained vectors. Still, these may just converge on similar performance to Tfidf – they're all dependent on the same word-features, and not very much data. </p>

<p>Going to a semi-supervised approach, where some of the document-tags represent sentiments where known, sometimes also helps. </p>
",3,1,2709,2016-07-12 09:01:53,https://stackoverflow.com/questions/38324328/is-doc2vec-suited-for-sentiment-analysis
Confusion Matrix - Testing Sentiment Analysis Model,"<p>I am testing a Sentiment Analysis model using NLTK. I need to add a Confusion Matrix to the classifier results and if possible also Precision, Recall and F-Measure values. I have only accuracy so far. Movie_reviews data has pos and neg labels. However to train the classifier I am using ""featuresets"" that has a different format from the usual (sentence, label) structure. I am not sure if I can use confusion_matrix from sklearn, after training the classifier by ""featuresets""</p>

<pre><code>import nltk
import random
from nltk.corpus import movie_reviews

documents = [(list(movie_reviews.words(fileid)), category)
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)

all_words = []

for w in movie_reviews.words():
    all_words.append(w.lower())

all_words = nltk.FreqDist(all_words)

word_features = list(all_words.keys())[:3000]

def find_features(document):
    words = set(document)
    features = {}
    for w in word_features:
        features[w] = (w in words)

    return features


featuresets = [(find_features(rev), category) for (rev, category) in documents]

training_set = featuresets[:1900]
testing_set =  featuresets[1900:]


classifier = nltk.NaiveBayesClassifier.train(training_set)


print(""Naive Bayes Algo accuracy percent:"", (nltk.classify.accuracy(classifier, testing_set))*100)
</code></pre>
","scikit-learn, nltk, sentiment-analysis, confusion-matrix","<p>First you can classify all test values and store predicted outcomes and gold results in a list.</p>

<p>Then, you can use <strong>nltk.ConfusionMatrix</strong>.</p>

<pre><code>test_result = []
gold_result = []

for i in range(len(testing_set)):
    test_result.append(classifier.classify(testing_set[i][0]))
    gold_result.append(testing_set[i][1])
</code></pre>

<p>Now, You can calculate different metrics.</p>

<pre><code>CM = nltk.ConfusionMatrix(gold_result, test_result)
print(CM)

print(""Naive Bayes Algo accuracy percent:""+str((nltk.classify.accuracy(classifier, testing_set))*100)+""\n"")

labels = {'pos', 'neg'}

from collections import Counter
TP, FN, FP = Counter(), Counter(), Counter()
for i in labels:
    for j in labels:
        if i == j:
            TP[i] += int(CM[i,j])
        else:
            FN[i] += int(CM[i,j])
            FP[j] += int(CM[i,j])

print(""label\tprecision\trecall\tf_measure"")
for label in sorted(labels):
    precision, recall = 0, 0
    if TP[label] == 0:
        f_measure = 0
    else:
        precision = float(TP[label]) / (TP[label]+FP[label])
        recall = float(TP[label]) / (TP[label]+FN[label])
        f_measure = float(2) * (precision * recall) / (precision + recall)
    print(label+""\t""+str(precision)+""\t""+str(recall)+""\t""+str(f_measure))
</code></pre>

<p>You can check - how to calculate <strong>precision and recall</strong> <a href=""https://en.wikipedia.org/wiki/Precision_and_recall#Definition_.28classification_context.29"" rel=""noreferrer"">here</a>.</p>

<p>You can also use : <strong>sklearn.metrics</strong> for these calculations using gold_result and test_result values.</p>

<pre><code>from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix   

print '\nClasification report:\n', classification_report(gold_result, test_result)
print '\nConfussion matrix:\n',confusion_matrix(gold_result, test_result)    
</code></pre>
",5,4,3157,2016-07-23 12:10:10,https://stackoverflow.com/questions/38541644/confusion-matrix-testing-sentiment-analysis-model
Spark Streaming - Classification of tweets&#39; stream from Kafka,"<p>I am new at Spark and I absolutely need some help for  classifying tweets from a Kafka Stream. Following I will explain the step processes i have done until now as well as the point where I'm stuck. </p>

<p>I hope some of you guys can help me out with this.</p>

<p>Thanks in advance.
<br><br></p>

<p><strong>The context is the following</strong>:</p>

<p>I have a simple <em>Kafka Producer</em> which simulates a tweet's stream (read from a file)  and a <em>TweetAnalyzer Consumer</em> which should process and classify the tweets on a <strong>Spark Streaming Context</strong>, as soon as it receive them.</p>

<p>In order to classify the received tweets, I have earlier built-up and stored on the disk a <em>TF-IDF</em> and <em>Naive Bayes</em> models which are loaded before the <em>Spark Streaming Context</em> starts.</p>

<p>For each tweet processed (stemming, punctuation, etc), I should compute its <em>TF-IDF vector</em> (feature vector) and classify it by exploiting respectively the IDF and Naive Bayes Models previously loaded.</p>

<p>Going straight to the point, my problem occurs when I have to transform the tweet's <em>term frequency vectors</em> (TF) to its <em>TF-IDF vectors</em>.</p>

<p>This is the code:</p>

<p><strong>Kafka Producer</strong></p>

<pre class=""lang-py prettyprint-override""><code>text_file = list(
    csv.reader(
        open('/twitterDataset/twitter/test_data.txt', 'rU')
    )
)

for row in text_file:
    time.sleep(1)
    jd = json.dumps(row).encode('ascii')
    producer.send(kafka_topic,jd)
</code></pre>

<p><strong>TweetAnalyzer</strong></p>

<pre class=""lang-py prettyprint-override""><code>#setting configuration
...  
#reading configuration
...
#setting Kafka configuration
...

# Create Spark context
sc = SparkContext(
    appName = app_name,
    master  = spark_master
)

# Create Streaming context
ssc = StreamingContext(
    sc,
    int(spark_batch_duration)
)

# Loading TF MODEL and compute TF-IDF
....

kafkaParams = {'metadata.broker.list""': kafka_brokers}

# Create direct kafka stream with brokers and topics
kvs = KafkaUtils.createDirectStream(
    ssc,
    [kafka_topic],
    {""metadata.broker.list"": kafka_brokers}
)

obj1 = TweetPreProcessing()

lines = kvs.map(lambda x: x[1])

tweet = lines.flatMap(obj1.TweetBuilder)

hashingTF = HashingTF()

#computing TF for each tweet
tf_tweet = tweet.map(lambda tup: hashingTF.transform(tup[0:]))\
                .map(lambda x: IDF().fit(x))
                .pprint()

ssc.start()
ssc.awaitTermination()
</code></pre>

<p>In the last lines of code I cannot apply the <strong>IDF().fit(x)</strong> function on <strong>x</strong> since Spark expects an ""RDD of term frequency vectors"" whereas in this point I have a ""Trasformed DStream"" due the the Streaming Spark Context.</p>

<p>I've tried to use either the <em>transform()</em> or <em>foreachRDD()</em> function instead of <em>map()</em>, but i don't know how to return correctly a new DStream after the transformation.</p>

<p>For example:</p>

<pre class=""lang-py prettyprint-override""><code>tf_tweet = tweet.map(lambda tup: hashingTF.transform(tup[0:]))\
                .transform(classify_tweet)
                .pprint()

def classify_tweet(tf):

    #compute TF-IDF of the tweet
    idf = IDF().fit(tf)
    tf_idf = idf.transform(tf)

    #print(tf_idf.collect())

    return idf
</code></pre>

<p>If I run the code using the transform function, Spark triggers (at the top of the back-trace) this error:</p>

<blockquote>
  <p>File
  ""/workspace_spark/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/streaming/util.py"", line 67, in call return r._jrdd <br> AttributeError: 'IDFModel' object
  has no attribute '_jrdd'</p>
</blockquote>

<p>But if I omit the return statement and simply print the <em>tf_idf vector</em> it gives me the correct output which looks like this:</p>

<blockquote>
  <p>[SparseVector(1048576, {164998: 0.0, 364601: 0.0, 924192: 0.0, 963449:
  0.0})]<br> [SparseVector(1048576, {251465: 0.0, 821055: 0.0, 963449: 0.0})]<br> [SparseVector(1048576, {234762: 0.0, 280973: 0.0, 403903: 0.0, 712732: 0.0, 861562: 0.0, 1040690: 0.0})] ...</p>
</blockquote>

<p>If i've got it right, I think the problem is that I cannot <em>return</em> a <em>SparseVector</em> when it expects a <em>DStream</em>.</p>

<p>Anyhow, is there a solution for this problem ?</p>

<p>I'd be very thankful if somebody can help me out with this, I am tragically stuck.</p>

<p>Thank you</p>
","apache-spark, pyspark, spark-streaming, apache-spark-mllib, sentiment-analysis","<p>Return transformed <code>tf_idf</code>:</p>

<pre><code>&gt;&gt;&gt; def classify_tweet(tf):
...     return IDF().fit(tf).transform(tf)
</code></pre>
",0,1,971,2016-07-24 17:42:14,https://stackoverflow.com/questions/38554916/spark-streaming-classification-of-tweets-stream-from-kafka
How can I use &quot;metrics.mutual_info&quot; in scikit&#39;s feature.selection,"<p>I would like to use other scoring functions then <code>chi2</code> etc., that are not listed on this page.</p>

<p><a href=""http://scikit-learn.org/stable/modules/feature_selection.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/feature_selection.html</a></p>

<p><a href=""http://scikit-learn.org/stable/modules/classes.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/classes.html</a></p>

<p>For example <code>metrics.mutual_info</code> and <code>metrics.balanced_accuracy_score</code></p>

<p>How can I integrate those into my code?</p>

<p>Thanks for help</p>
","python, machine-learning, scikit-learn, sentiment-analysis","<p>The new scikit-learn version 0.18, has added support for Mutual information feature selection. So no need to use the <code>metrics.mutual_info</code>. You can use the new <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html"" rel=""nofollow"">feature_selection.mutual_info_classif</a> score function in <code>SelectKBest</code> or <code>SelectPercentile</code> just like you use <code>chi2</code>. </p>

<pre><code>X_new = SelectKBest(mutual_info_classif, k=100).fit_transform(X, y)
</code></pre>

<p>For more information about the resent changes look at the <a href=""http://scikit-learn.org/stable/whats_new.html"" rel=""nofollow"">changelog</a>.</p>
",3,3,2078,2016-07-25 15:26:38,https://stackoverflow.com/questions/38571783/how-can-i-use-metrics-mutual-info-in-scikits-feature-selection
Document Vectorization Representation in Python,"<p>I was trying my hand at sentiment analysis in python 3, and was using the TDF-IDF vectorizer with the bag-of-words model to vectorize a document.</p>

<p>So, to anyone who is familiar with that, it is quite evident that the resulting matrix representation is sparse. </p>

<p>Here is a snippet of my code. Firstly, the documents.</p>

<pre><code>tweets = [('Once you get inside you will be impressed with the place.',1),('I got home to see the driest damn wings ever!',0),('An extensive menu provides lots of options for breakfast.',1),('The flair bartenders are absolutely amazing!',1),('My first visit to Hiro was a delight!',1),('Poor service, the waiter made me feel like I was stupid every time he came to the table.',0),('Loved this place.',1),('This restaurant has great food',1),
      ('Honeslty it did not taste THAT fresh :(',0),('Would not go back.',0),
       ('I was shocked because no signs indicate cash only.',0),
        ('Waitress was a little slow in service.',0),
        ('did not like at all',0),('The food, amazing.',1),
        ('The burger is good beef, cooked just right.',1),
        ('They have horrible attitudes towards customers, and talk down to each one when customers do not enjoy their food.',0),
        ('The cocktails are all handmade and delicious.',1),('This restaurant has terrible food',0),
        ('Both of the egg rolls were fantastic.',1),('The WORST EXPERIENCE EVER.',0),
        ('My friend loved the salmon tartar.',1),('Which are small and not worth the price.',0),
        ('This is the place where I first had pho and it was amazing!!',1),
        ('Horrible - do not waste your time and money.',0),('Seriously flavorful delights, folks.',1),
        ('I loved the bacon wrapped dates.',1),('I dressed up to be treated so rudely!',0),
        ('We literally sat there for 20 minutes with no one asking to take our order.',0),
        ('you can watch them preparing the delicious food! :)',1),('In the summer, you can dine in a charming outdoor patio - so very delightful.',1)]

X_train, y_train = zip(*tweets)
</code></pre>

<p>And the following code to vectorize the documents.</p>

<pre><code>tfidfvec = TfidfVectorizer(lowercase=True)
vectorized = tfidfvec.fit_transform(X_train)

print(vectorized)
</code></pre>

<p>When I print <code>vectorized</code>, it does not output a normal matrix. Instead, this:
<a href=""https://i.sstatic.net/0TpiX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0TpiX.png"" alt=""enter image description here""></a></p>

<p>If I'm not wrong, this must be a sparse matrix representation. However, I am not able to comprehend its format, and what each term means.</p>

<p>Also, there are 30 documents. So, that explains the 0-29 on the first column. If that's the trend then I'm guessing the second column is the index of the words, and the last value is it's tf-idf? It just struck me while I was typing my question, but kindly correct me if I'm wrong.</p>

<p>Could anyone with experience in this help me understand it better?</p>
","python-3.x, vectorization, sparse-matrix, sentiment-analysis, tf-idf","<p>Yes, technically the first two tuples represent the row-column position, and the third column is the value in that position. So it is basically showing the position and values of the nonzero values.</p>
",1,1,691,2016-08-03 00:36:17,https://stackoverflow.com/questions/38732561/document-vectorization-representation-in-python
tensorflow loaded model gives different predictions,"<p>My code is to predict the sentiment of a sentence. I have trained a CNN model and saved it. When I loaded my model and try predict the sentiment of a sentence, I got different predictions for the same sentence. My code is as following, and the problem happens when I try to predict the sentene by calling the function predict_cnn_word2vec at the bottom: </p>

<pre><code>import logging;
import numpy as np; 
import tensorflow as tf;
import sklearn as sk
import re; 
import json
import string;
import math
import os
from sklearn.metrics import recall_score, f1_score, precision_score;



class CNN(object):
def __init__(self,logger):
    self.logger = logger; 


def _weight_variable(self,shape):
    initial = tf.truncated_normal(shape, stddev = 0.1); 
    return tf.Variable(initial);

def _bias_variable(self,shape):
    initial = tf.constant(0.1, shape = shape); 
    return tf.Variable(initial);

def _conv2d(self,x, W, b, strides=1):
    # convolve and relu activation
    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME');
    x = tf.nn.bias_add(x, b);
    return tf.nn.relu(x);


def _maxpool(self,x, k=2):
    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME');


def _init_CNN(self, sentence_width, sentence_height, dropout, learning_rate,n_class,is_training):
    self.logger.info(""----------Initiating CNN---------"");

    self.X = tf.placeholder(tf.float32, [None, sentence_height * sentence_width]);
    self.Y = tf.placeholder(tf.float32, [None, n_class]); 

    x = tf.reshape(self.X, shape = [-1, sentence_height, sentence_width, 1]); 

    #1st convolution layer
    wc1 = tf.Variable(tf.random_normal([3, 3, 1, 5]));
    bc1 = tf.Variable(tf.random_normal([5]))
    stride1 = 2;
    pool1 = 2

    conv1 = self._conv2d(x, wc1, bc1,stride1);
    conv1 = self._maxpool(conv1, pool1);

    conv2 = conv1;

    pools = [2]#,2,2];
    strides = [2]#,1,1];
    last_channel = 5;


    first_size = self._get_first_connected_size(sentence_height,sentence_width, strides,pools,last_channel);

    # #1st fully connected layer
    wf1 = tf.Variable(tf.random_normal([first_size, 32]));
    bf1 = tf.Variable(tf.random_normal([32]));

    fc1 = tf.reshape(conv2, [-1, wf1.get_shape().as_list()[0]]);
    fc1 = tf.add(tf.matmul(fc1, wf1), bf1);
    fc1 = tf.nn.relu(fc1);
    fc1 = tf.nn.dropout(fc1, dropout)

    #dropout layer
    outw = tf.Variable(tf.random_normal([32, n_class]));
    outb = tf.Variable(tf.random_normal([n_class]));

    self.pred = tf.add(tf.matmul(fc1, outw), outb);
    self.y_p = tf.argmax(self.pred,1); 

    if is_training is False:
        return;


    # self.pred = self._predict(self.X,sentence_width, sentence_height, settings, dropout);
    self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.pred, self.Y)); 
    self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost);
    #accuracy
    self.y_t = tf.argmax(self.Y,1);
    self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.y_p, self.y_t), ""float""))
    self.init = tf.initialize_all_variables(); 

def _get_first_connected_size(self,ih,iw, strides,pools,last_channel):
    i = 1;
    while i &lt;= len(strides):
        iw = math.ceil(float(iw) / float(strides[i-1]));
        iw = math.ceil(float(iw) /pools[i-1]); 
        ih = math.ceil(float(ih) / float(strides[i-1]));
        ih = math.ceil(float(ih) /pools[i-1]);
        i = i + 1;
    first_connected_size = int(ih*iw*last_channel);
    return first_connected_size;

def train(self,data_provider,config):
    self._init_CNN(config.sentence_width, config.num_word, config.dropout, config.learning_rate,config.n_class,True);
    sess = tf.Session();
    sess.run(self.init);
    self.logger.info(""Start Training!"");
    #saver
    saver = tf.train.Saver(); 
    cur_max_accuracy = 0;
    cur_max_recall = 0;
    cur_max_precision = 0;

    if config.model_init_from is not None and os.path.exists(config.model_init_from):
        #restore model if exist
        saver.restore(sess, config.model_init_from);

    for epoch in range(config.epochs):
        data_provider.reset_batch_pointer();
        for i in range(data_provider.num_batches):
            batch_x, batch_y = data_provider.next_batch();
            accuracy_score,y_p,y_t, _, cost = sess.run([self.accuracy,self.y_p, self.y_t, self.optimizer, self.cost], feed_dict={self.X: batch_x, self.Y: batch_y});
            if i %10 == 0:
                self.logger.info(""(%d/%d,%d epo) cost = %f, accuracy = %f,precision = %f, recall = %f, f_score = %f"" % (i+epoch * data_provider.num_batches, data_provider.num_batches*config.epochs, epoch,cost,accuracy_score,precision_score(y_t, y_p),recall_score(y_t,y_p),f1_score(y_t,y_p)));
        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.y_p, self.y_t), ""float""))
        accuracy_score,y_p,y_t, _,cost = sess.run([self.accuracy,self.y_p, self.y_t, self.optimizer, self.cost], feed_dict={self.X: data_provider.get_test_X(), self.Y: data_provider.get_test_Y()})
        precision_score1 = precision_score(y_t, y_p);
        recall_score1 = recall_score(y_t,y_p);
        f1_score1 = f1_score(y_t,y_p);
        self.logger.info(""#####(%d/%d epoch) cost = %f, accuracy = %f(max: %f), precision = %f(max: %f), recall = %f(max:%f), f_score = %f"" % (epoch,config.epochs, cost,accuracy_score,cur_max_accuracy, precision_score1,cur_max_precision,recall_score1,cur_max_recall,f1_score1));

        save_loc = saver.save(sess, config.model_save_path);
        print(""Model has been saved to: %s"" % save_loc);
        cur_max_accuracy = accuracy_score;
        cur_max_recall = recall_score1;
        cur_max_precision = precision_score1;

def predict_cnn_word2vec(self, data_provider, config):
    if not os.path.exists(config.model_init_from):
        self.logger.info(""model does not exist!"");
        sys.exit(2);
    self._init_CNN(config.sentence_width, config.num_word, config.dropout, config.learning_rate,config.n_class,False)

    while True:
        sentence = input(""Enter a sentence:"");
        with tf.Session() as sess:
            saver = tf.train.Saver();
            saver.restore(sess, config.model_init_from);

            batch_x = np.empty((1, config.num_word*300));
            batch_x[0,:] = data_provider.get_sentence_vec(sentence,config);

            y_p = sess.run([self.y_p], feed_dict={self.X: batch_x});
            result = ""positive"" if y_p == [1] else ""negative"";
            self.logger.info(""[%s] is %s"" %(sentence,result));
</code></pre>

<p>When I called predict_cnn_word2vec() in my main function, I enter a while loop. For every sentence, the model would be reloaded to predict the sentiment of the sentence. For the first few loops, it gave the same results. But as I made more predictions, the result seems to change. The following is an example of my logs:</p>

<pre><code>Enter a sentence:i adore him
2016-08-11 05:26:16,690 : INFO : [i adore him] is positive
Enter a sentence:i adore him
2016-08-11 05:26:19,662 : INFO : [i adore him] is positive
Enter a sentence:i adore him
2016-08-11 05:26:21,176 : INFO : [i adore him] is positive
Enter a sentence:i adore him
2016-08-11 05:26:22,568 : INFO : [i adore him] is positive
Enter a sentence:i adore him
2016-08-11 05:26:23,724 : INFO : [i adore him] is negative
Enter a sentence:i adore him
2016-08-11 05:26:25,791 : INFO : [i adore him] is negative
</code></pre>

<p>I have checked my data_provider which puts my sentence into a vector and it works perfectly as every time it returns the same sentence vector. So this problem should not be due to the input data. I wonder if every time tensorflow reloads the model, something in the model is reloaded correctly. Could anybody help me with this issue? </p>
","machine-learning, tensorflow, sentiment-analysis, conv-neural-network","<p>You're not turning off dropout during inference!</p>

<p>Dropout causes a random fraction of the units in a layer to drop their activations to 0. This is a useful regularizer during training, but you don't want this behavior when validating or testing your model or running it in production.</p>

<p>Rather than initializing the network with a float dropout and continuing on your merry way, you need to make dropout a placeholder, just like your inputs and targets. During training, set this placeholder to something reasonable (eg 0.5) via the feed dict. During inference, set this placeholder to 1.0.</p>
",6,1,2171,2016-08-11 05:45:51,https://stackoverflow.com/questions/38888120/tensorflow-loaded-model-gives-different-predictions
"In general, when does TF-IDF reduce accuracy?","<p>I'm training a corpus consisting of 200000 reviews into positive and negative reviews using a Naive Bayes model, and I noticed that performing TF-IDF actually reduced the accuracy (while testing on test set of 50000 reviews) by about 2%. So I was wondering if TF-IDF has any underlying assumptions on the data or model that it works with, i.e. any cases where accuracy is reduced by the use of it?</p>
","sentiment-analysis, tf-idf, text-classification, naivebayes","<p>The IDF component of TF*IDF can harm your classification accuracy in some cases. </p>

<p>Let suppose the following artificial, easy classification task, made for the sake of illustration: </p>

<ul>
<li>Class A: texts containing the word 'corn' </li>
<li>Class B: texts not containing the word 'corn'</li>
</ul>

<p>Suppose now that in Class A, you have 100 000 examples and in class B, 1000 examples. </p>

<p>What will happen to TFIDF? The inverse document frequency of corn will be very low (because it is found in almost all documents), and the feature 'corn' will get a very small TFIDF, which is the weight of the feature used by the classifier. Obviously, 'corn' was THE best feature for this classification task. This is an example where TFIDF may reduce your classification accuracy. In more general terms:</p>

<ul>
<li>when there is class imbalance. If you have more instances in one class, the good word features of the frequent class risk having lower IDF, thus their best features will have a lower weight</li>
<li>when you have words with high frequency that are very predictive of one of the classes (words found in most documents of that class)</li>
</ul>
",11,2,4827,2016-08-25 18:18:28,https://stackoverflow.com/questions/39152229/in-general-when-does-tf-idf-reduce-accuracy
Performance issue while trying to match a list of words with a list of sentences in R,"<p>I am trying to match a list of words with a list of sentences and form a data frame with the matching words and sentences. For example:</p>

<pre><code>words &lt;- c(""far better"",""good"",""great"",""sombre"",""happy"")
sentences &lt;- c(""This document is far better"",""This is a great app"",""The night skies were sombre and starless"", ""The app is too good and i am happy using it"", ""This is how it works"")
</code></pre>

<p>The expected result (a dataframe) is as follows:</p>

<pre><code>sentences                                               words
This document is far better                               better
This is a great app                                       great
The night skies were sombre and starless                  sombre 
The app is too good and i am happy using it               good, happy
This is how it works                                      -
</code></pre>

<p>I am using the following code to achieve this.</p>

<pre><code>lengthOfData &lt;- nrow(sentence_df)
pos.words &lt;- polarity_table[polarity_table$y&gt;0]$x
neg.words &lt;- polarity_table[polarity_table$y&lt;0]$x
positiveWordsList &lt;- list()
negativeWordsList &lt;- list()
for(i in 1:lengthOfData){
        sentence &lt;- sentence_df[i,]$comment
        #sentence &lt;- gsub('[[:punct:]]', """", sentence)
        #sentence &lt;- gsub('[[:cntrl:]]', """", sentence)
        #sentence &lt;- gsub('\\d+', """", sentence)
        sentence &lt;- tolower(sentence)
        # get  unigrams  from the sentence
        unigrams &lt;- unlist(strsplit(sentence, "" "", fixed=TRUE))

        # get bigrams from the sentence
        bigrams &lt;- unlist(lapply(1:length(unigrams)-1, function(i) {paste(unigrams[i],unigrams[i+1])} ))

        # .. and combine into data frame
        words &lt;- c(unigrams, bigrams)
        #if(sentence_df[i,]$ave_sentiment)

        pos.matches &lt;- match(words, pos.words)
        neg.matches &lt;- match(words, neg.words)
        pos.matches &lt;- na.omit(pos.matches)
        neg.matches &lt;- na.omit(neg.matches)
        positiveList &lt;- pos.words[pos.matches]
        negativeList &lt;- neg.words[neg.matches]

        if(length(positiveList)==0){
          positiveList &lt;- c(""-"")
        }
        if(length(negativeList)==0){
          negativeList &lt;- c(""-"")
        }
        negativeWordsList[i]&lt;- paste(as.character(unique(negativeList)), collapse="", "")
        positiveWordsList[i]&lt;- paste(as.character(unique(positiveList)), collapse="", "")

        positiveWordsList[i] &lt;- sapply(positiveWordsList[i], function(x) toString(x))
        negativeWordsList[i] &lt;- sapply(negativeWordsList[i], function(x) toString(x))

    }    
positiveWordsList &lt;- as.vector(unlist(positiveWordsList))
negativeWordsList &lt;- as.vector(unlist(negativeWordsList))
scores.df &lt;- data.frame(ave_sentiment=sentence_df$ave_sentiment, comment=sentence_df$comment,pos=positiveWordsList,neg=negativeWordsList, year=sentence_df$year,month=sentence_df$month,stringsAsFactors = FALSE)
</code></pre>

<p>I have 28k sentences and 65k words to match with. The above code takes 45 seconds to accomplish the task. Any suggestions on how to improve the performance of the code as the current approach takes a lot of time? </p>

<p><strong>Edit:</strong></p>

<p>I want to get only those words which exactly matches with the words in the sentences. For example : </p>

<pre><code>words &lt;- c('sin','vice','crashes') 
sentences &lt;- ('Since the app crashes frequently, I advice you guys to fix the issue ASAP')
</code></pre>

<p>Now for the above case my output should be as follows:</p>

<pre><code>sentences                                                           words
Since the app crashes frequently, I advice you guys to fix        crahses
the issue ASAP  
</code></pre>
","r, sentiment-analysis, sentimentr","<p>i was able to use @David Arenburg answer with some modification. Here is what i did. I used the following (suggested by David) to form the data frame.</p>

<pre><code>df &lt;- data.frame(sentences) ; 
df$words &lt;- sapply(sentences, function(x) toString(words[stri_detect_fixed(x, words)]))
</code></pre>

<p>The problem with the above approach is that it does not do the exact word match.
So I used the following to filter out the words that did not exactly match with the words in the sentence.</p>

<pre><code>df &lt;- data.frame(fil=unlist(s),text=rep(df$sentence, sapply(s, FUN=length)))
</code></pre>

<p>After applying the above line the output data frame changes as follows.</p>

<pre><code>sentences                                                      words
This document is far better                                    better
This is a great app                                            great
The night skies were sombre and starless                       sombre 
The app is too good and i am happy using it                    good
The app is too good and i am happy using it                    happy
This is how it works                                            -
Since the app crashes frequently, I advice you guys to fix     
the issue ASAP                                                 crahses
Since the app crashes frequently, I advice you guys to fix     
the issue ASAP                                                 vice
Since the app crashes frequently, I advice you guys to fix     
the issue ASAP                                                 sin
</code></pre>

<p>Now apply the following filter to the data frame to remove those words that are not an exact match to those words present in the sentence.</p>

<pre><code>df &lt;- df[apply(df, 1, function(x) tolower(x[1]) %in% tolower(unlist(strsplit(x[2], split='\\s+')))),]
</code></pre>

<p>Now my resulting data frame will be as follows.</p>

<pre><code>    sentences                                                      words
    This document is far better                                    better
    This is a great app                                            great
    The night skies were sombre and starless                       sombre 
    The app is too good and i am happy using it                    good
    The app is too good and i am happy using it                    happy
    This is how it works                                            -
    Since the app crashes frequently, I advice you guys to fix     
    the issue ASAP                                                 crahses
</code></pre>

<p>stri_detect_fixed reduced my computation time a lot. The remaining process did not take up much time. Thanks to @David for pointing me out in the right direction.</p>
",1,3,235,2016-09-12 11:49:24,https://stackoverflow.com/questions/39449644/performance-issue-while-trying-to-match-a-list-of-words-with-a-list-of-sentences
NoClassDefFoundError StanfordCoreNLP,"<p>I have this problem when i tired to submit topology in <code>local mode</code> ! </p>

<pre><code>ERROR backtype.storm.util - Async loop died!
java.lang.NoClassDefFoundError: edu/stanford/nlp/pipeline/StanfordCoreNLP
at edu.stanford.nlp.pipeline.NLP.init(NLP.java:16) ~[classes/:na]

Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.pipeline.StanfordCoreNLP
at java.net.URLClassLoader$1.run(URLClassLoader.java:217) ~[na:1.6.0_38]
at java.security.AccessController.doPrivileged(Native Method) ~[na:1.6.0_38]
at java.net.URLClassLoader.findClass(URLClassLoader.java:205) ~[na:1.6.0_38]
at java.lang.ClassLoader.loadClass(ClassLoader.java:323) ~[na:1.6.0_38]
at java.lang.ClassLoader.loadClass(ClassLoader.java:268) ~[na:1.6.0_38]
... 20 common frames omitted
</code></pre>

<p>i don't know what can i share to help you to fix it ! please tell me .
I'm using <code>stanford 3.4.1</code> with <code>java 1.6</code> </p>
","java, nlp, stanford-nlp, apache-storm, sentiment-analysis","<p>Make sure the Stanford NLP libraries are on classpath.</p>

<p>If you are running your project using Eclipse, this link might help you - <a href=""https://stackoverflow.com/questions/9528080/error-in-stanford-nlp-core"">Error in stanford nlp core</a></p>
",2,1,1936,2016-09-30 11:48:10,https://stackoverflow.com/questions/39790837/noclassdeffounderror-stanfordcorenlp
How to split a list or a vector in r?,"<p>In developing of Sentiment Analysis application using R, I need to split result of the matched words, </p>

<p><strong>First</strong> (done)</p>

<pre><code>pos.matches = match(words, pos.words)
</code></pre>

<p><strong>Second</strong> (done)</p>

<pre><code>words_matched=words[pos.matches]
</code></pre>

<p>From the secondary step, let's take an example that the method will return </p>

<pre><code>happy happy sad
</code></pre>

<p>My question is, how to split the result from secondary step into like this:</p>

<pre><code>[1] happy

[2] happy 

[3] sad
</code></pre>

<p>Right now I'm using strsplit function</p>

<pre><code> strsplit((words[pos.matches]),"" "")  
</code></pre>

<p>but it is notifying me that there's an error </p>

<blockquote>
  <p>Warning: Error in cat: argument 1 (type 'list') cannot be handled by
  'cat'</p>
</blockquote>
","r, split, sentiment-analysis","<pre><code>words_matched = ""happy happy sad""
strsplit(words_matched, "" "")
</code></pre>

<p>Gives:</p>

<pre><code>[[1]]
[1] ""happy"" ""happy"" ""sad""  
</code></pre>

<p>Notice the [[1]] then the [1]. What you have here is a single-element <em>list</em>, with a three-element character vector inside. If you try to cat it:</p>

<pre><code>cat(strsplit(words_matched, "" ""))
</code></pre>

<p>you get the error you saw, ""argument 1 (type 'list') cannot be handled by 'cat'""</p>

<p>You want either of these:</p>

<pre><code>strsplit(words_matched, "" "")[[1]]
unlist(strsplit(words_matched, "" ""))
</code></pre>

<p>They return just a 3-element character vector, which can be cat-ed.</p>

<p>As an aside, the [1] [2] [3] output you showed is not how either print() or cat() will format its output. I'm wondering if you actually wanted a <code>list</code>, not a character vector. If so, you have to jump through one more hoop:</p>

<p>as.list( unlist(strsplit(words_matched, "" "")) )</p>

<p>Which, when <code>print()</code>-ed, gives:</p>

<pre><code>[[1]]
[1] ""happy""

[[2]]
[1] ""happy""

[[3]]
[1] ""sad""
</code></pre>
",1,0,9711,2016-10-10 14:24:14,https://stackoverflow.com/questions/39960536/how-to-split-a-list-or-a-vector-in-r
Offline Sentiment Analysis Lib with java,"<p>I have an offline tweets corpus. I am trying to do sentiment analysis on it offline classifying each tweet to 'Positive', 'Negative', 'Neutral'.</p>

<p>Is there a java lib that I can use ?</p>
","java, sentiment-analysis","<p>There is the sentiment analysis implementation by the Stanford NLP group (<a href=""http://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow"">http://stanfordnlp.github.io/CoreNLP/</a>). You can find some source code here: <a href=""https://blog.openshift.com/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java/"" rel=""nofollow"">https://blog.openshift.com/day-20-stanford-corenlp-performing-sentiment-analysis-of-twitter-using-java/</a></p>

<p>Another alternative is to use a word-emotion association lexicon as found here: <a href=""http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm"" rel=""nofollow"">http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</a>.
A simple approach is tidying the tweet and counting the negative and positive words as found in the lexicon.</p>
",0,-1,916,2016-10-19 09:18:09,https://stackoverflow.com/questions/40127134/offline-sentiment-analysis-lib-with-java
How to create a corpus for sentiment analysis in NLTK?,"<p>I'm looking to use my own created corpus within Visual Studio Code for MacOSX; I have read probably a hundred forums and I can't wrap my head around what I'm doing wrong as I'm pretty new to programming. </p>

<p><a href=""https://stackoverflow.com/questions/15611328/how-to-save-a-custom-categorized-corpus-in-nltk"">This question</a> seems to be the closes thing I can find to what I need to do; however, I am unaware of how to do the following:</p>

<p>""on a Mac it would be in ~/nltk_data/corpora, for instance. And it looks like you also have to append your new corpus to the <code>__init__.py</code> within .../site-packages/nltk/corpus/.""</p>

<p><strong>When answering, please be aware I am using Homebrew</strong> and don't want to permanently disable using another path if I need to use a stock NLTK corpora data set as well within the same coding.</p>

<p>If needed, I can post my attempt at coding using ""PlaintextCorpusReader"" along with the provided traceback below, although I would rather not have to use PlaintextCorpusReader at all for seamless use and would rather just use a simple copy+paste for .txt files into an appropriate location I wish to use in accordance with the append coding. </p>

<p>Thank you.</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/jordanXXX/Documents/NLP/bettertrainingdata"", line 42, in &lt;module&gt;
    short_pos = open(""short_reviews/pos.txt"", ""r"").read
IOError: [Errno 2] No such file or directory: 'short_reviews/pos.txt'
</code></pre>

<hr>

<hr>

<h2>EDIT:</h2>

<hr>

<p>Thank you for your responses.</p>

<p>I have taken your advice and moved the folder out of NLTK's corpora.</p>

<p>I've been doing some experimenting with my folder location and I've gotten different tracebacks.</p>

<p>If you are saying the best way to do it is with PlaintextCorpusReader then so be it; however, maybe for my application I'd want to use CategorizedPlaintextCorpusReader?</p>

<p>sys.argv is definitely not what I meant, so I can read up on that later.</p>

<p>First, here is my code without my attempt to use PlaintextCorpusReader which results in the above traceback when the folder ""short_reviews"" containing the pos.txt and neg.txt files is outside of the NLP folder:</p>

<pre><code>import nltk
import random
from nltk.corpus import movie_reviews
from nltk.classify.scikitlearn import SklearnClassifier
import pickle

from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC

from nltk.classify import ClassifierI
from statistics import mode

from nltk import word_tokenize

class VoteClassifier(ClassifierI):
    def __init__(self, *classifiers):
        self._classifiers = classifiers

    def classify(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)
        return mode(votes)

    def confidence(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)

        choice_votes = votes.count(mode(votes))
        conf = choice_votes / len(votes)
        return conf

# def main():
#     file = open(""short_reviews/pos.txt"", ""r"")
#     short_pos = file.readlines()
#     file.close

short_pos = open(""short_reviews/pos.txt"", ""r"").read
short_neg = open(""short_reviews/neg.txt"", ""r"").read

documents = []

for r in short_pos.split('\n'):
    documents.append( (r, ""pos"") )

for r in short_neg.split('\n'):
    documents.append((r, ""neg""))

all_words = []

short_pos_words = word.tokenize(short_pos)
short_neg_words = word.tokenize(short_neg)

for w in short_pos_words:
    all_words.append(w. lower())

for w in short_neg_words:
    all_words.append(w. lower())

all_words = nltk.FreqDist(all_words)
</code></pre>

<p>However, when I move the folder ""short_reviews"" containing the text files into the NLP folder using the same code as above but without the use of PlaintextCorpusReader the following occurs:</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/jordanXXX/Documents/NLP/bettertrainingdata"", line 47, in &lt;module&gt;
    for r in short_pos.split('\n'):
AttributeError: 'builtin_function_or_method' object has no attribute 'split'
</code></pre>

<p>When I move the folder ""short_reviews"" containing the text files into the NLP folder using the code below with the use of PlaintextCorpusReader the following Traceback occurs:</p>

<pre><code>import nltk
import random
from nltk.corpus import movie_reviews
from nltk.classify.scikitlearn import SklearnClassifier
import pickle

from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC

from nltk.classify import ClassifierI
from statistics import mode

from nltk import word_tokenize

from nltk.corpus import PlaintextCorpusReader
corpus_root = 'short_reviews'
word_lists = PlaintextCorpusReader(corpus_root, '*')
wordlists.fileids()


class VoteClassifier(ClassifierI):
    def __init__(self, *classifiers):
        self._classifiers = classifiers

    def classify(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)
        return mode(votes)

    def confidence(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)

        choice_votes = votes.count(mode(votes))
        conf = choice_votes / len(votes)
        return conf

# def main():
#     file = open(""short_reviews/pos.txt"", ""r"")
#     short_pos = file.readlines()
#     file.close

short_pos = open(""short_reviews/pos.txt"", ""r"").read
short_neg = open(""short_reviews/neg.txt"", ""r"").read

documents = []

for r in short_pos.split('\n'):
    documents.append((r, ""pos""))

for r in short_neg.split('\n'):
    documents.append((r, ""neg""))

all_words = []

short_pos_words = word.tokenize(short_pos)
short_neg_words = word.tokenize(short_neg)

for w in short_pos_words:
    all_words.append(w. lower())

for w in short_neg_words:
    all_words.append(w. lower())

all_words = nltk.FreqDist(all_words)


Traceback (most recent call last):
  File ""/Users/jordanXXX/Documents/NLP/bettertrainingdata2"", line 18, in &lt;module&gt;
    word_lists = PlaintextCorpusReader(corpus_root, '*')
  File ""/Library/Python/2.7/site-packages/nltk/corpus/reader/plaintext.py"", line 62, in __init__
    CorpusReader.__init__(self, root, fileids, encoding)
  File ""/Library/Python/2.7/site-packages/nltk/corpus/reader/api.py"", line 87, in __init__
    fileids = find_corpus_fileids(root, fileids)
  File ""/Library/Python/2.7/site-packages/nltk/corpus/reader/util.py"", line 763, in find_corpus_fileids
    if re.match(regexp, prefix+fileid)]
  File ""/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/re.py"", line 141, in match
    return _compile(pattern, flags).match(string)
  File ""/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/re.py"", line 251, in _compile
    raise error, v # invalid expression
error: nothing to repeat
</code></pre>
","python, nlp, nltk, sentiment-analysis, corpus","<p>The answer you refer to contains some very poor (or rather, inapplicable) advice. There is no reason to place your own corpus in <code>nltk_data</code>, or to hack <code>nltk.corpus.__init__.py</code> to load it like a native corpus. In fact, <strong>do not</strong> do these things.</p>

<p>You should use <code>PlaintextCorpusReader</code>. I don't understand your reluctance to do so, but if your files are plain text, it's the right tool to use. Supposing you have a folder <code>NLP/bettertrainingdata</code>, you can build a reader that will load all <code>.txt</code> files in this folder like this:</p>

<pre><code>myreader = nltk.corpus.reader.PlaintextCorpusReader(r""NLP/bettertrainingdata"", r"".*\.txt"")
</code></pre>

<p>If you add new files to the folder, the reader will find and use them. If what you want is to be able to use your script with other folders, then just do so-- you don't need a different reader, you need to learn about <code>sys.argv</code>. If you are after a categorized corpus with <code>pos.txt</code> and <code>neg.txt</code>, then you need a <code>CategorizedPlaintextCorpusReader</code> (which see). If it's something else yet that you want, then please edit <strong>your question</strong> to explain what you are trying to do.</p>
",2,0,4005,2016-10-26 19:15:08,https://stackoverflow.com/questions/40270449/how-to-create-a-corpus-for-sentiment-analysis-in-nltk
Working of Machine learning algorithms for sentiment analysis,"<p>I found a good example of a <strong>Naive Bayes Classifier</strong> from <a href=""http://www.nltk.org/howto/classify.html"" rel=""nofollow"">here</a> . I am unable to understand the steps.</p>

<pre><code>from nltk.classify import SklearnClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
train_data = [({""a"": 4, ""b"": 1, ""c"": 0}, ""ham""),
      ({""a"": 5, ""b"": 2, ""c"": 1}, ""ham""),
      ({""a"": 0, ""b"": 3, ""c"": 4}, ""spam""),
          ({""a"": 5, ""b"": 1, ""c"": 1}, ""ham""),
          ({""a"": 1, ""b"": 4, ""c"": 3}, ""spam"")]
classif = SklearnClassifier(BernoulliNB()).train(train_data)
test_data = [{""a"": 3, ""b"": 2, ""c"": 1},
             {""a"": 0, ""b"": 3, ""c"": 7}]
classif.classify_many(test_data)
['ham', 'spam']
classif = SklearnClassifier(SVC(), sparse=False).train(train_data)
classif.classify_many(test_data)
['ham', 'spam']
</code></pre>

<p>What are :</p>

<ol>
<li>Features in the code above?</li>
<li>Actual Data for sentiment?</li>
<li>""a"": 4, ""b"": 1, ""c"": 0 ?</li>
<li>ham, spam?</li>
</ol>

<p>The basic purpose is to understand that how the ML Algorithm works. 
I am newbie in Sentiment Analysis. I hope someone will help.</p>
","nlp, nltk, sentiment-analysis, opennlp, naivebayes","<p>The code sample you posted uses nonsense data to train a classifier. </p>

<blockquote>
  <p>What are :</p>
  
  <ol>
  <li>Features in the code above?</li>
  <li>""a"": 4, ""b"": 1, ""c"": 0 ?</li>
  <li>ham, spam?</li>
  </ol>
</blockquote>

<p>The array <code>train_data</code> contains features named ""a"", ""b"", and ""c"". 
The classification categories are ""ham"" and ""spam"". Sentiment analysis  might use categories ""positive"" and ""negative"".</p>

<blockquote>
  <ol start=""2"">
  <li>Actual Data for sentiment?</li>
  </ol>
</blockquote>

<p>There are no actual sentiment data in this demo.</p>

<p>Be aware that you won't learn anything about how the learning algorithm works from this snippet. It just shows you the API to a black box that trains the classifier. To learn about machine learning, read about how the training works. To learn how to train a classifier (without knowing how the training works behind the scenes), start with <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow"">Chapter 6</a> of the NLTK book.</p>
",1,2,186,2016-10-29 20:23:23,https://stackoverflow.com/questions/40323335/working-of-machine-learning-algorithms-for-sentiment-analysis
Is it possible to edit NLTK&#39;s vader sentiment lexicon?,"<p>I would like to add words to the <code>vader_lexicon.txt</code> to specify polarity scores to a word. What is the right way to do so?</p>

<p>I saw this file in <code>AppData\Roaming\nltk_data\sentiment\vader_lexicon</code>. The file consists of the word, its polarity, intensity, and an array of 10 intensity scores given by ""10 independent human raters"". [1] However, when I edited it, nothing changed in the results of the following code:</p>

<pre><code>from nltk.sentiment.vader import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()
s = sia.polarity_scores(""my string here"")
</code></pre>

<p>I think that this text file is accessed by my code when I called SentimentIntensityAnalyzer's constructor. [2] Do you have any ideas on how I can edit a pre-made lexicon?</p>

<p>Sources:</p>

<p>[1] <a href=""https://github.com/cjhutto/vaderSentiment"" rel=""noreferrer"">https://github.com/cjhutto/vaderSentiment</a></p>

<p>[2] <a href=""http://www.nltk.org/api/nltk.sentiment.html"" rel=""noreferrer"">http://www.nltk.org/api/nltk.sentiment.html</a></p>
","python, nlp, nltk, sentiment-analysis, vader","<p>For anyone interested, this can also be achieved without having to manually edit the vader lexicon .txt file. Once loaded the lexicon is a normal dictionary with words as keys and scores as values. As provided by <a href=""https://stackoverflow.com/users/7194553/repoleved"">repoleved</a> in <a href=""https://stackoverflow.com/questions/49863520/how-to-edit-nltks-vader-sentiment-lexicon-without-modifying-a-txt-file"">this</a> post:</p>

<pre><code>from nltk.sentiment.vader import SentimentIntensityAnalyzer

new_words = {
    'foo': 2.0,
    'bar': -3.4,
}

SIA = SentimentIntensityAnalyzer()

SIA.lexicon.update(new_words)
</code></pre>

<p>If you wish to remove words, use the '.pop' function:</p>

<pre><code>SIA = SentimentIntensityAnalyzer()

SIA.lexicon.pop('no')
</code></pre>
",23,12,12693,2016-11-08 07:34:25,https://stackoverflow.com/questions/40481348/is-it-possible-to-edit-nltks-vader-sentiment-lexicon
&#39;Can&#39;t return head of null or leaf Tree&#39; with CoreNLP on Android,"<p>I  want to use CoreNLP in my Android project. But when I create a CoreNLP instance like this:</p>

<pre><code>import java.util.Properties;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;

public class NLP {

    private StanfordCoreNLP pipeline;
    Properties props;

    public NLP() {
        props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, parse, sentiment"");
        pipeline = new StanfordCoreNLP(props);//--&gt;ERROR, SEE BELOW
    }

    public int findSentiment(String line) {
        int mainSentiment = 0;
        if (line != null &amp;&amp; line.length() &gt; 0) {
            int longest = 0;
            Annotation annotation = pipeline.process(line);
            for (CoreMap sentence : annotation
                    .get(CoreAnnotations.SentencesAnnotation.class)) {
                Tree tree = sentence
                        .get(SentimentCoreAnnotations.AnnotatedTree.class);
                int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
                String partText = sentence.toString();
                if (partText.length() &gt; longest) {
                    mainSentiment = sentiment;
                    longest = partText.length();
                }

            }
        }
        return mainSentiment;
    }
}
</code></pre>

<p>The project links to the following .jar files:</p>

<ul>
<li>ejml-0.23.jar    </li>
<li>stanford-corenlp-3.4.1.jar</li>
<li>stanford-corenlp-3.4.1-models.jar</li>
</ul>

<p>On my desktop java environment with java 1.8.0_92, this code runs correctly, but when running the code on an Android (after compiling without error), I am getting the error when the NLP class is instantiated:</p>

<blockquote>
  <p>Caused by: java.lang.IllegalArgumentException: Can't return head of
  null or leaf Tree.
                                                                       at
  edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:158)
                                                                       at
  edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:138)
                                                                       at
  edu.stanford.nlp.pipeline.ParserAnnotator.(ParserAnnotator.java:132)
                                                                       at
  edu.stanford.nlp.pipeline.AnnotatorImplementations.parse(AnnotatorImplementations.java:132)
                                                                       at
  edu.stanford.nlp.pipeline.StanfordCoreNLP$10.create(StanfordCoreNLP.java:719)
                                                                       at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
                                                                       at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:292)
                                                                       at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:129)
                                                                       at
  edu.stanford.nlp.pipeline.StanfordCoreNLP.(StanfordCoreNLP.java:125)</p>
</blockquote>

<p>I am using CoreNLP 3.4.1. It's not the most recent version, but it works with Java 7 on Android. How can I use CoreNLP correctly on Android?</p>
","java, android, sentiment-analysis, stanford-nlp","<h2>Why this problem occurs?</h2>

<p>I was looking for the answer. I have checked the jar. There is a class <code>AbstractCollinsHeadFinder.java</code>. From this class, this error comes</p>

<blockquote>
  <p>edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:158)
  at
  edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:138)</p>
</blockquote>

<p>There are 2 root causes for this error.</p>

<ol>
<li>If tree is null, then this error occurs.</li>
<li><p>If tree is leaf, then this error occurs.</p>

<pre><code>@Override
public Tree determineHead(Tree t, Tree parent) {
  if (nonTerminalInfo == null) {
    throw new IllegalStateException(""Classes derived from AbstractCollinsHeadFinder must create and fill HashMap nonTerminalInfo."");
  }
  // The error mainly generate for the following condition
  if (t == null || t.isLeaf()) {
    throw new IllegalArgumentException(""Can't return head of null or leaf Tree.""); 
  }
  if (DEBUG) {
    log.info(""determineHead for "" + t.value());
  }

  Tree[] kids = t.children();
  -------------
  -------------
  return theHead;
}
</code></pre></li>
</ol>

<h2>Resource Link:</h2>

<ol>
<li><a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/trees/AbstractCollinsHeadFinder.java#L163"" rel=""noreferrer"">https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/trees/AbstractCollinsHeadFinder.java#L163</a></li>
</ol>

<hr>

<h2>Check for parameters:</h2>

<p>I have also checked your code. In your setProperty(...), there are some parameters. <strong>Maybe there are some parameter missing</strong>. So, you can create a object by following the code.</p>

<pre><code>// creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution 
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
</code></pre>

<h2>Resource Link:</h2>

<p><a href=""http://stanfordnlp.github.io/CoreNLP/api.html"" rel=""noreferrer"">creates a StanfordCoreNLP object</a></p>

<hr>

<h2>A simple, complete example program:</h2>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.util.*;

public class StanfordCoreNlpExample {
    public static void main(String[] args) throws IOException {
        PrintWriter xmlOut = new PrintWriter(""xmlOutput.xml"");
        Properties props = new Properties();
        props.setProperty(""annotators"",
                ""tokenize, ssplit, pos, lemma, ner, parse"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        Annotation annotation = new Annotation(
                ""This is a short sentence. And this is another."");
        pipeline.annotate(annotation);
        pipeline.xmlPrint(annotation, xmlOut);
        // An Annotation is a Map and you can get and use the
        // various analyses individually. For instance, this
        // gets the parse tree of the 1st sentence in the text.
        List&lt;CoreMap&gt; sentences = annotation
                .get(CoreAnnotations.SentencesAnnotation.class);
        if (sentences != null &amp;&amp; sentences.size() &gt; 0) {
            CoreMap sentence = sentences.get(0);
            Tree tree = sentence.get(TreeAnnotation.class);
            PrintWriter out = new PrintWriter(System.out);
            out.println(""The first sentence parsed is:"");
            tree.pennPrint(out);
        }
    }
}
</code></pre>

<h2>Resource Link:</h2>

<ol>
<li><a href=""http://www.surdeanu.info/mihai/papers/acl2014-corenlp.pdf"" rel=""noreferrer"">The Stanford CoreNLP Natural Language Processing Toolkit</a></li>
</ol>
",8,15,394,2016-11-15 06:27:05,https://stackoverflow.com/questions/40603405/cant-return-head-of-null-or-leaf-tree-with-corenlp-on-android
trouble with analyzing words in one file and checking if they are in each line of another file &amp;… in python,"<p>So, im trying to search to see if each of the lines in a file2.txt contain any of the words in file1.txt 1. so if for example:</p>

<p>File 1:</p>

<pre><code>love,10
like,5
best,10
hate,1
lol,10
better,10
worst,1
</code></pre>

<p>file 2 : a bunch of sentences I want to see if it contains any of file1 (over 200 lines)</p>

<p>I have a way of doing this with my own files in my program, and it works but it adds the total values into one big list (like if the entire file says love 43 times, then Love:43, but I'm looking for separate lists for each line.. so if a line contains love 4 times and another 5 times then the program will indicate this.. **specifically, what I'm trying to do is total the amount of keywords in each line of the file (so if a line contains 4 keywords then the total for that line is 4, and the value associated with the keywords (so you see how in my example file one there a value associated with the keywords? If a line in a file is: <code>Hi I love my boyfriend but I like my bestfriend lol</code> then this like would be  <code>{Love: 1, like: , lol:1}(keywords = 3,  Total = 25</code> ( the total comes from the values associated with them in the list)</p>

<p>and if a second line is simply</p>

<pre><code>I hate my life. It is the worst day ever!
</code></pre>

<p>then this would be <code>{hate: 1, worst: 1}(keywords = 2, total = 2</code></p>

<p>I have this, and it works, but is there a way to modify it so instead of printing one big line like:</p>

<pre><code>{'please': 24, 'worst': 40, 'regrets': 1, 'hate': 70,... etc,} it simply adds the total number of keywords per line and the values associated with them?

wordcount = {}
with open('mainWords.txt', 'r') as f1, open('sentences.txt', 'r') as f2:
    words = f1.read().split()
    wordcount = { word.split(',')[0] : 0 for word in words}

    for line in f2:
        line_split = line.split()
        for word in line_split:
          if word in wordcount: 
            wordcount[word] += 1

print(wordcount)
</code></pre>
","python, python-3.x, sentiment-analysis","<p>As usual, <code>collections</code> save the day:</p>

<pre><code>from collections import Counter

with open('mainWords.txt') as f:
    sentiments = {word: int(value)
                 for word, value in
                 (line.split("","") for line in f)
                 }

with open('sentences.txt') as f:
    for line in f:
        values = Counter(word for word in line.split() if word in sentiments)
        print(values)
        print(sum(values[word]*sentiments[word] for word in values))  # total
        print(len(values))  # keywords
</code></pre>

<p>You have the sentiment polarities in the dictionary <code>sentiments</code> for later use.</p>
",1,1,40,2016-11-15 12:33:20,https://stackoverflow.com/questions/40609943/trouble-with-analyzing-words-in-one-file-and-checking-if-they-are-in-each-line-o
Create sparse matrix from tweets,"<p>I have some tweets and other variables that I would like to convert into a sparse matrix.</p>

<p>This is basically what my data looks like. Right now it is saved in a data.table with one column that contains the tweet and one column that contains the score. </p>

<pre><code>Tweet               Score
Sample Tweet :)        1
Different Tweet        0
</code></pre>

<p>I would like to convert this into a matrix that looks like this:</p>

<pre><code>Score Sample Tweet Different :)
    1      1     1         0  1
    0      0     1         1  0
</code></pre>

<p>Where there is one row in the sparse matrix for each row in my data.table. Is there an easy way to do this in R?</p>
","r, twitter, sparse-matrix, sentiment-analysis","<p>This is close to what you want</p>

<pre><code>library(Matrix)
words = unique(unlist(strsplit(dt[, Tweet], ' ')))

M = Matrix(0, nrow = NROW(dt), ncol = length(words))
colnames(M) = words

for(j in 1:length(words)){
  M[, j] = grepl(paste0('\\b', words[j], '\\b'), dt[, Tweet])
}

M = cbind(M, as.matrix(dt[, setdiff(names(dt),'Tweet'), with=F]))

#2 x 5 sparse Matrix of class ""dgCMatrix""
#     Sample Tweet :) Different Score
#[1,]      1     1  .         .     1
#[2,]      .     1  .         1     .
</code></pre>

<p>The only small issue is that the regex is not recognising <code>':)'</code> as a word. Maybe someone who knows regex better can advise how to fix this.</p>
",1,0,120,2016-12-06 23:07:18,https://stackoverflow.com/questions/41006602/create-sparse-matrix-from-tweets
Twitter sentiment package issues npm,"<p>I'm trying to build a twitter sentiment analysis tool with Angular, Node and Express. Right now I am able to grab user tweets, remove *some special characters and then when I call the sentiment function it returns input.lowercase is not a function. I tried using the package Sentimental instead and it returned a very similar error. </p>

<pre><code> router.post(""/postUsername"", function(req, res){
  console.log(req.body.userName);
  var client = new Twitter({
    consumer_key: ,
    consumer_secret: '',
    access_token_key: '',
    access_token_secret: ''
  });


  client.get('statuses/user_timeline', {screen_name: req.body.userName, count:20}, function(error, tweets, response) {
    if (!error) {
      var concatenatedTweets = analyzeIt.analyzeIt(tweets);
      console.log(concatenatedTweets);
      var score = sentiment(concatenatedTweets);
      console.log(score);
      }
  });
});
</code></pre>

<p>and my analyze it function *function doesn't work the way I'd wanting to removing things</p>

<pre><code>function analyzeIt(data) {
  return data.map(function(item){
    var puncutaionLess = item.text.replace(""/[.,#!$%^&amp;*;:{}=-_`~()]/g@?"" , '');
    var finalString = puncutaionLess.replace(/\s{2,}/g,"" "");
    return finalString.replace(""#@"" , """");
  });
};
</code></pre>

<p>Any help would very much appreciated?</p>
","angularjs, node.js, express, npm, sentiment-analysis","<p>Your regular expressios is weird.</p>

<p>First there are no quotations around regex in javascript.</p>

<p>Also you are not escaping special charaters:</p>

<pre><code>   item.text.replace(""/[.,#!$%^&amp;*;:{}=-_`~()]/g@?"" , '');
</code></pre>

<p>should be:</p>

<pre><code>   item.text.replace(/[\.,#!$%^&amp;*;:{}=\-_`~()]/g , '');
</code></pre>

<p>notice the escape of <code>.</code></p>

<p>You may want to add <code>[]</code> in there too:</p>

<p>So for example:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>text = ""hello[!] there!"";
text = text.replace(/[\.\[\],#!$%^&amp;*;:{}=\-_`~()]/g , '');

 console.log(text)</code></pre>
</div>
</div>
</p>
",0,0,112,2016-12-11 21:45:38,https://stackoverflow.com/questions/41091487/twitter-sentiment-package-issues-npm
LSTM Networks for Sentiment Analysis - How to extend this model to 3 classes and classify new examples?,"<p>I'm trying the code from this link <a href=""http://deeplearning.net/tutorial/lstm.html"" rel=""nofollow noreferrer"">http://deeplearning.net/tutorial/lstm.html</a> and have changed the imdb data to my own data. I'm trying to extend this model to a 3-class multinomial model with a ""neutral"" class added instead of the binomial model provided in the tutorial that classifies only ""positive"" and ""negative"" classes.</p>

<p>To build this new model, I rewrote the <strong>build_dict</strong> and <strong>grab_data</strong> functions to build my custom dictionary and return data for the three classes and made the following changes to <strong>main()</strong> in <strong>imdb_preprocess.py</strong>:</p>

<pre><code>dictionary = build_dict()

train_x_pos, train_x_neu, train_x_neg = grab_data('train', dictionary)
test_x_pos, test_x_neu, test_x_neg = grab_data('test', dictionary)

train_x = train_x_pos + train_x_neu + train_x_neg
train_y = [1] * len(train_x_pos) + [0] * len(train_x_neu) + [-1] * len(train_x_neg)

test_x = test_x_pos + test_x_neu + test_x_neg
test_y = [1] * len(test_x_pos) + [0] * len(test_x_neu) + [-1] * len(test_x_neg)
</code></pre>

<p>Are my modifications to <strong>train_y</strong> and <strong>test_y</strong> correct?</p>

<p>Assuming they are correct, I went ahead and trained this model on my data, and I was successfully able to save new versions of <strong>lstm_model.npz</strong> and <strong>lstm_model.npz.pkl</strong>.</p>

<p>Now, how do I test this model on a new example? Suppose a new, unseen example is the following text:</p>

<pre><code>new_example = ""I am very happy that I've been able to create a new LSTM sentiment model!""
</code></pre>

<p>My understanding is that the function <strong>pred_probs</strong> needs to be called, which takes parameters <strong>f_pred_prob</strong>, <strong>prepare_data</strong>, <strong>data</strong>, <strong>iterator</strong> and <strong>verbose=False</strong>. What do I pass in for these parameters? I'm guessing the function call would then be something like:</p>

<pre><code>pred_probs(f_pred_prob, prepare_data, data=new_example, iterator, verbose=False)
</code></pre>
","deep-learning, sentiment-analysis, lstm","<p>After turning this over for weeks I finally got a working model. It turns out my modifications to main() in <strong>imdb_preprocess.py</strong> were almost correct, the relevant lines are:</p>

<pre><code>train_y = [2] * len(train_x_pos) + [1] * len(train_x_neu) + [0] * len(train_x_neg)
test_y = [2] * len(test_x_pos) + [1] * len(test_x_neu) + [0] * len(test_x_neg)
</code></pre>

<p>As for how to classify a new example, I had the right idea:</p>

<pre><code>new_example_pair = (new_example, 0)
pred_probs(f_pred_prob, prepare_data, new_example_pair, iterator, verbose=False)
</code></pre>

<p><strong>pred_probs</strong> should be modified to accommodate three classes, and changing it properly should return a probability value for each possible class.</p>
",0,0,271,2016-12-22 19:35:13,https://stackoverflow.com/questions/41290488/lstm-networks-for-sentiment-analysis-how-to-extend-this-model-to-3-classes-and
Text Translation Issue,"<p>I am trying to do sentiment analysis in R on a Norwegian data. I was thinking of first converting the data into english and then going forward with the analysis.
But for the conversion we are having to send the data outside the client network(using Google translate), which the client is not ready to accept. Is there any package in R by which we can do the sentiment analysis on the Norwegian dataset itself without having to transmit the data outside??</p>
","r, google-translate, sentiment-analysis, r-package","<p><a href=""http://folk.uio.no/paalee/publications/2014-cse.pdf"" rel=""nofollow noreferrer"">This paper</a> documents the creation of a sentiment lexicon in Norwegian and says ""All the sentiment lexicons are publicly available for those that are interested"", so I would suggest contacting the authors.</p>
",0,0,102,2016-12-27 04:23:57,https://stackoverflow.com/questions/41338978/text-translation-issue
Neutrality for sentiment analysis in spark,"<p>I have built a pretty basic naive bayes over apache spark and using mllib of course. But I have a few clarifications on what exactly neutrality means.</p>

<p>From what I understand, in a given dataset there are pre-labeled sentences which comprise of the necessary classes, let's take 3 for example below.</p>

<pre><code>0-&gt; Negative sentiment
1-&gt; Positive sentiment
2-&gt; Neutral sentiment
</code></pre>

<p>This neutral is pre-labeled in the training set itself.</p>

<p>Is there any other form of neutrality handling. Suppose if there are no neutral sentences available in the dataset then is it possible that I can calculate it from the scale of probability like</p>

<pre><code>0.0 - 0.4 =&gt; Negative
0.4- - 0.6 =&gt; Neutral
0.6 - 1.0 =&gt; Positive
</code></pre>

<p>Is such kind of mapping possible in spark. I searched around but could not find any. The <code>NaiveBayesModel</code> class in the RDD API has a <code>predict</code> method which just returns a double that is mapped according to the training set i.e if only 0,1 is there it will return only 0,1 and not in a scaled manner such as 0.0 - 1.0 as above.</p>

<p>Any pointers/advice on this would be incredibly helpful.</p>

<p>Edit - 1 </p>

<p>Sample code</p>

<pre><code>//Performs tokenization,pos tagging and then lemmatization
//Returns a array of string
val tokenizedString = Util.tokenizeData(text)
val hashingTF = new HashingTF()
//Returns a double 
//According to the training set 1.0 =&gt; Positive, 0.0 =&gt; Negative
val status = model.predict(hashingTF.transform(tokenizedString.toSeq))
if(status == 1.0) ""Positive"" else ""Negative""
</code></pre>

<p>Sample dataset content</p>

<pre><code>1,Awesome movie
0,This movie sucks
</code></pre>

<p>Of course the original dataset contains more longer sentences, but this should be enough for explanations I guess</p>

<p>Using the above code I am calculating. My question is the same</p>

<p>1) Neutrality handling in dataset
In the above dataset if I am adding another category such as 
2,This movie can be enjoyed by kids</p>

<p>For arguments sake, lets assume that it is a neutral review, then the <code>model.predict</code> method will give either 1.0,0.0,2.0 based on the passed in sentence.</p>

<p>2) Using the <code>model.predictProbabilities</code> it gives an array of doubles, but I am not sure in what order it gives the result i.e index 0 is for negative or for positive? With three features i.e Negative,Positive,Neutral then in what order will that method return the predictions?</p>
","scala, apache-spark, sentiment-analysis, naivebayes","<p>It would have been helpful to have the code that builds the model (for your example to work, the 0.0 from the dataset must be converted to 0.0 as a Double in the model, either after indexing it with a StringIndexer stage, or if you converted that from the file), but assuming that this code works:</p>

<pre><code>val status = model.predict(hashingTF.transform(tokenizedString.toSeq))
if(status == 1.0) ""Positive"" else ""Negative""
</code></pre>

<p>Then yes, it means the probabilities at index 0 is that of negative and at 1 that of positive (it's a bit strange and there must be a reason, but everything is a double in ML, even feature and category indexes). If you have something like this in your code:</p>

<pre><code>val labelIndexer = new StringIndexer()
  .setInputCol(""sentiment"")
  .setOutputCol(""indexedsentiment"")
  .fit(trainingData) 
</code></pre>

<p>Then you can use <code>labelIndexer.labels</code> to identify the labels (probability at index 0 is for labelIndexer.labels at index 0. </p>

<p>Now regarding your other questions.</p>

<ol>
<li>Neutrality can mean two different things. Type 1: a review contains as much positive and negative words Type 2: there is (almost) no sentiment expressed. </li>
<li>A Neutral category can be very helpful if you want to manage Type 2. If that is the case, you need neutral examples in your dataset. Naive Bayes is not a good classifier to apply thresholding on the probabilities in order to determine Type 2 neutrality. </li>
<li>Option 1: Build a dataset (if you think you will have to deal with a lot of Type 2 neutral texts). The good news is, building a neutral dataset is not too difficult. For instance you can pick random texts that are not movie reviews and assume they are neutral. It would be even better if you could pick content that is closely related to movies (but neutral), like a dataset of movie synopsis. You could then create a multi-class Naive Bayes classifier (between neutral, positive and negative) or a hierarchical classifier (first step is a binary classifier that determines whether a text is a movie review or not, second step to determine the overall sentiment).</li>
<li>Option 2 (can be used to deal with both Type 1 and 2). As I said, Naive Bayes is not very great to deal with thresholds on the probabilities, but you can try that. Without a dataset though, it will be difficult to determine the thresholds to use. Another approach is to identify the number of words or stems that have a significant polarity. One quick and dirty way to achieve that is to query your classifier with each individual word and count the number of times it returns ""positive"" with a probability significantly higher than the negative class (discard if the probabilities are too close to each other, for instance within 25% - a bit of experimentations will be needed here). At the end, you may end up with say 20 positive words vs 15 negative ones and determine it is neutral because it is balanced or if you have 0 positive and 1 negative, return neutral because the count of polarized words is too low.</li>
</ol>

<p>Good luck and hope this helped. </p>
",1,1,706,2016-12-27 12:36:54,https://stackoverflow.com/questions/41345355/neutrality-for-sentiment-analysis-in-spark
Aspect based sentiment analysis libraries,"<p>I'm working on a project where I have to perform aspect-based sentiment analysis on verbal comments. Can anybody suggest some good existing libraries or examples?</p>
","python, text-mining, data-analysis, sentiment-analysis, data-science","<p>Something similar to your project is the Twitter sentiment analysis projects.</p>

<p>Follow some examples:</p>

<ol>
<li><a href=""http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/"" rel=""nofollow noreferrer"">First example</a></li>
<li><a href=""http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/"" rel=""nofollow noreferrer"">Second example</a></li>
<li><a href=""http://www.sananalytics.com/lab/twitter-sentiment/"" rel=""nofollow noreferrer"">Third example</a></li>
</ol>

<p>I suggest you to use <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">NLTK</a> library. Check also the <a href=""http://www.nltk.org/howto/"" rel=""nofollow noreferrer"">'How To Section'</a> for examples.</p>

<p>I hope that these information are usefull.</p>

<p>If these are not in your interest, please add some details to your questio in order to answer better.</p>
",1,0,3410,2016-12-29 17:31:44,https://stackoverflow.com/questions/41384529/aspect-based-sentiment-analysis-libraries
Should sentiment analysis training data be evenly distributed?,"<p>If I am training a sentiment classifier off of a tagged dataset where most documents are negative, say ~95%, should the classifier be trained with the same distribution of negative comments? If not, what would be other options to ""normalize"" the data set? </p>
","nlp, nltk, sentiment-analysis","<p>You don't say what type of classifier you have but in general you don't have to normalize the distribution of the training set. However, usually the more data the better but you should always do blind tests to prevent over-fitting. </p>

<p>In your case you will have a strong classifier for negative comments and unless you have a very large sample size, a weaker positive classifier. If your sample size is large enough it won't really matter since you hit a point where you might start over-fitting your negative data anyway. </p>

<p>In short, it's impossible to say for sure without knowing the actual algorithm and the size of the data sets and the diversity within the dataset. </p>

<p>Your best bet is to carve off something like 10% of your training data (randomly) and just see how the classifier performs after being trained on the 90% subset. </p>
",0,0,120,2017-01-18 03:47:34,https://stackoverflow.com/questions/41711018/should-sentiment-analysis-training-data-be-evenly-distributed
Sentiment Analysis using senti_classifier and NLTK,"<p>I'm not doing something right -- By the looks of the error i'm getting i think i'm missing some data. I have all the prerequisites intalled for sentiment_classifier (<a href=""https://pypi.python.org/pypi/sentiment_classifier/0.7"" rel=""nofollow noreferrer"">https://pypi.python.org/pypi/sentiment_classifier/0.7</a>) which are nltk, numpy, and sentiwordnet. Here's my code - a quick example from the docs i'm trying to get working. </p>

<pre><code>from senti_classifier import senti_classifier
sentences = ['The movie was the worst movie', 'It was the worst acting by the actors']
pos_score, neg_score = senti_classifier.polarity_scores(sentences)
print pos_score, neg_score
</code></pre>

<p>and here's the error message i'm getting</p>

<pre><code>Traceback (most recent call last):
  File ""/home/beef/sciencefair2017/sentiment.py"", line 1, in &lt;module&gt;
from senti_classifier import senti_classifier
  File ""build/bdist.linux-x86_64/egg/senti_classifier/senti_classifier.py"", line 227, in &lt;module&gt;
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 1178, in resource_stream
self, resource_name
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 1454, in get_resource_stream
return io.BytesIO(self.get_resource_string(manager, resource_name))
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 1457, in get_resource_string
return self._get(self._fn(self.module_path, resource_name))
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 1535, in _get
return self.loader.get_data(path)
IOError: [Errno 0] Error: 'senti_classifier/data/SentiWn.p'
</code></pre>

<p>what's the issue and how can i get it working? Any advice, even if only a suggestion and not an actual solution is greatly appreciated. I've already tried various versions of all the packages and I've looked through some of the docs to no avail.</p>
","python, nltk, sentiment-analysis, wordnet","<p>I figured it out: I didn't install the full package - i originally used pip but i had to install it like so:</p>

<pre><code>git clone https://github.com/kevincobain2000/sentiment_classifier
cd sentiment_classifier
python setup.py install
</code></pre>

<p>works beautifully now.</p>
",2,1,992,2017-01-22 19:56:01,https://stackoverflow.com/questions/41795476/sentiment-analysis-using-senti-classifier-and-nltk
What are the segmentation patterns for sentiment analysis,"<p>I am trying to implement sentiment analysis for customer reviews.I found some patterns like POS tagging, bigrams models.I need to know more ways which i can use to segment sentence to find it is negative or positive.</p>
","nlp, sentiment-analysis","<p>You have two specific approaches to do sentiment analysis. </p>

<p>1- Corpus-based approach : In this approach machine learning is used on text with any features that is valid on text such as n-grams, tf-idf, term frequency, term occurrence. You can combine feature results with weights as well.</p>

<p>2- Lexicon-based approach : In this approach a sentiment lexicon such as <a href=""http://sentiwordnet.isti.cnr.it"" rel=""nofollow noreferrer"">SentiWordnet</a> or <a href=""http://sentic.net"" rel=""nofollow noreferrer"">SenticNet</a> is employed with basic rules to find the sentiment polarity of sentence. POS tagging is mostly used in this approach. </p>
",1,0,243,2017-01-26 10:12:48,https://stackoverflow.com/questions/41871139/what-are-the-segmentation-patterns-for-sentiment-analysis
How to reduce n-gram features?,"<p>I have been dealing with a problem in Text processing. I would appreciate if anyone could help me.
I have dataset consisting 12,000 record of comments.
When I run n-gram extractor on this, I gain 170,000 unique unigram + bigram which is so many that it takes too long to be processed by machine learning algorithm.</p>

<p>How should I reduce the number of these extracted features? Is there any especial algorithm or something?</p>
","machine-learning, nlp, text-processing, sentiment-analysis, n-gram","<p>There is no need to keep all the N-grrams.  You should be trimming the list of N-grams by frequency.  For example, only consider unigrams that occur 40 or more times.  The cut-off for trimming bi-grams will be lower.  It will be lower still for tri-grams and so on and so on.</p>
",4,2,809,2017-01-31 13:52:00,https://stackoverflow.com/questions/41959303/how-to-reduce-n-gram-features
Spark streaming and Kafka intergration,"<p>I'm new to Apache Spark and I've been doing a project related to sentiment analysis on twitter data which involves spark streaming and kafka integration. I have been following the github code (link provided below) </p>

<p><a href=""https://github.com/sridharswamy/Twitter-Sentiment-Analysis-Using-Spark-Streaming-And-Kafka"" rel=""nofollow noreferrer"">https://github.com/sridharswamy/Twitter-Sentiment-Analysis-Using-Spark-Streaming-And-Kafka</a>
However, in the last stage, that is during the integration of Kafka with Apache Spark, the following errors were obtained</p>

<pre><code>py4j.protocol.Py4JError: An error occurred while calling o24.createDirectStreamWithoutMessageHandler. Trace:
py4j.Py4JException: Method createDirectStreamWithoutMessageHandler([class org.apache.spark.streaming.api.java.JavaStreamingContext, class java.util.HashMap, class java.util.HashSet, class java.util.HashMap]) does not exist
    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)
    at py4j.Gateway.invoke(Gateway.java:272)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:214)
    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>Command used: <code>bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.1 twitterStream.py</code></p>

<p>Apache Spark version: spark-2.1.0-bin-hadoop2.4</p>

<p>Kafka version: kafka_2.11-0.10.1.1</p>

<p>I haven't been able to debug this and any help would be much appreciated.</p>
","apache-spark, apache-kafka, spark-streaming, sentiment-analysis, spark-streaming-kafka","<p>The example you are trying to run is desinged for running in spark 1.5. You should either download spark 1.5 or run the <code>spark-submit</code> from spark 2.1.0 but with kafka package related to 2.1.0, for example:
<code>./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.1.0</code>.</p>
",1,0,883,2017-02-12 06:38:23,https://stackoverflow.com/questions/42184889/spark-streaming-and-kafka-intergration
"GridSearchCV.fit() returns TypeError: Expected sequence or array-like, got estimator","<p>I am trying to do sentiment analysis on twitter data by following chapter 6 of the book Building Machine Learning Systems in Python.</p>

<p>I am using the dataset: <a href=""https://raw.githubusercontent.com/zfz/twitter_corpus/master/full-corpus.csv"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/zfz/twitter_corpus/master/full-corpus.csv</a></p>

<p>It uses a pipeline of tfidf vectorizer and naive bayes classifier as estimator.</p>

<p>Then I am using GridSearchCV() to find the best parameters for the estimator.</p>

<p>The code is as follows:</p>

<pre><code>from load_data import load_data
from sklearn.cross_validation import ShuffleSplit
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import f1_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

def pipeline_tfidf_nb():
    tfidf_vect = TfidfVectorizer( analyzer = ""word"")
    naive_bayes_clf = MultinomialNB()
    return Pipeline([('vect', tfidf_vect),('nbclf',naive_bayes_clf)])

input_file = ""full-corpus.csv""
X,y = load_data(input_file)
print X.shape,y.shape

clf = pipeline_tfidf_nb()
cv = ShuffleSplit(n = len(X), test_size = .3, n_iter = 1, random_state = 0)

clf_param_grid = dict(vect__ngram_range = [(1,1),(1,2),(1,3)],
                   vect__min_df = [1,2],
                    vect__smooth_idf = [False, True],
                    vect__use_idf = [False, True],
                    vect__sublinear_tf = [False, True],
                    vect__binary = [False, True],
                    nbclf__alpha = [0, 0.01, 0.05, 0.1, 0.5, 1],
                  )

grid_search = GridSearchCV(estimator = clf, param_grid = clf_param_grid, cv = cv, scoring = f1_score)
grid_search.fit(X, y)

print grid_search.best_estimator_
</code></pre>

<p>load_data() extracts the values from the csv file with positive or negative sentiment.</p>

<p>X is an array of strings(TweetText) and y is an array of bool values(True for positive sentiment).</p>

<p>The error is:</p>

<pre><code>runfile('C:/Users/saurabh.s1/Downloads/Python_ml/ch6/main.py', wdir='C:/Users/saurabh.s1/Downloads/Python_ml/ch6')
Reloaded modules: load_data
negative : 572
positive : 519
(1091,) (1091,)
Traceback (most recent call last):

  File ""&lt;ipython-input-25-823b07c4ff26&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/saurabh.s1/Downloads/Python_ml/ch6/main.py', wdir='C:/Users/saurabh.s1/Downloads/Python_ml/ch6')

  File ""C:\anaconda2\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile
    execfile(filename, namespace)

  File ""C:\anaconda2\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 87, in execfile
    exec(compile(scripttext, filename, 'exec'), glob, loc)

  File ""C:/Users/saurabh.s1/Downloads/Python_ml/ch6/main.py"", line 31, in &lt;module&gt;
    grid_search.fit(X, y)

  File ""C:\anaconda2\lib\site-packages\sklearn\grid_search.py"", line 804, in fit
    return self._fit(X, y, ParameterGrid(self.param_grid))

  File ""C:\anaconda2\lib\site-packages\sklearn\grid_search.py"", line 553, in _fit
    for parameters in parameter_iterable

  File ""C:\anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 800, in __call__
    while self.dispatch_one_batch(iterator):

  File ""C:\anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 658, in dispatch_one_batch
    self._dispatch(tasks)

  File ""C:\anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 566, in _dispatch
    job = ImmediateComputeBatch(batch)

  File ""C:\anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 180, in __init__
    self.results = batch()

  File ""C:\anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 72, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]

  File ""C:\anaconda2\lib\site-packages\sklearn\cross_validation.py"", line 1550, in _fit_and_score
    test_score = _score(estimator, X_test, y_test, scorer)

  File ""C:\anaconda2\lib\site-packages\sklearn\cross_validation.py"", line 1606, in _score
    score = scorer(estimator, X_test, y_test)

  File ""C:\anaconda2\lib\site-packages\sklearn\metrics\classification.py"", line 639, in f1_score
    sample_weight=sample_weight)

  File ""C:\anaconda2\lib\site-packages\sklearn\metrics\classification.py"", line 756, in fbeta_score
    sample_weight=sample_weight)

  File ""C:\anaconda2\lib\site-packages\sklearn\metrics\classification.py"", line 956, in precision_recall_fscore_support
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)

  File ""C:\anaconda2\lib\site-packages\sklearn\metrics\classification.py"", line 72, in _check_targets
    check_consistent_length(y_true, y_pred)

  File ""C:\anaconda2\lib\site-packages\sklearn\utils\validation.py"", line 173, in check_consistent_length
    uniques = np.unique([_num_samples(X) for X in arrays if X is not None])

  File ""C:\anaconda2\lib\site-packages\sklearn\utils\validation.py"", line 112, in _num_samples
    'estimator %s' % x)

TypeError: Expected sequence or array-like, got estimator Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',
        dtype=&lt;type 'numpy.int64'&gt;, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), norm=u'l2', preprocessor=None,
        smooth_i...e_idf=False, vocabulary=None)), ('nbclf', MultinomialNB(alpha=0, class_prior=None, fit_prior=True))])
</code></pre>

<p>I have tried reshaping X,y but that is not working.</p>

<p>Let me know if you need more data or if I have missed something.</p>

<p>Thanks!</p>
","python-2.7, machine-learning, scikit-learn, sentiment-analysis, grid-search","<p>This error is because You are passing wrong parameter by using <code>scoring=f1_score</code> into the GridSearchCV constructor. 
Have a look at <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"" rel=""nofollow noreferrer"">documentation of GridSearchCV</a>.</p>

<p>In scoring param, it asks for:</p>

<blockquote>
  <p>A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y). If None, the score method of the estimator is used.</p>
</blockquote>

<p>You are passing a callable function with signature <code>(y_true, y_pred[, ...])</code> which is wrong. Thats why you are getting the error.
You should use a <a href=""http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values"" rel=""nofollow noreferrer"">string as defined here</a> to pass in scoring, or pass a callable with signature <code>(estimator, X, y)</code>. This can be done by using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer"" rel=""nofollow noreferrer"">make_scorer</a>.</p>

<p>Change this line in your code:</p>

<pre><code>grid_search = GridSearchCV(estimator = clf, param_grid = clf_param_grid, 
                           cv = cv, scoring = f1_score)
</code></pre>

<p>to this:</p>

<pre><code>grid_search = GridSearchCV(estimator = clf, param_grid = clf_param_grid,
                           cv = cv, scoring = 'f1')

              OR

grid_search = GridSearchCV(estimator = clf, param_grid = clf_param_grid,
                           cv = cv, scoring = make_scorer(f1_score))
</code></pre>

<p>I have answered for same type of problem <a href=""https://stackoverflow.com/a/42152550/3374996"">in this answer here</a></p>
",4,1,3014,2017-02-15 09:04:36,https://stackoverflow.com/questions/42244860/gridsearchcv-fit-returns-typeerror-expected-sequence-or-array-like-got-estim
Which is the efficient way to remove stop words in textblob for sentiment analysis of text?,"<p>I'm trying to implement Naive Bayes algorithm for sentiment analysis of News Paper headlines. I'm using TextBlob for this purpose and I'm finding it difficult to remove stop words such as 'a', 'the', 'in' etc. Below is the snippet of my code in python:</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
from textblob import TextBlob

test = [
(""11 bonded labourers saved from shoe firm"", ""pos""),
(""Scientists greet Abdul Kalam after the successful launch of Agni on May 22, 1989"",""pos""),
(""Heavy Winter Snow Storm Lashes Out In Northeast US"", ""neg""),
(""Apparent Strike On Gaza Tunnels Kills 2 Palestinians"", ""neg"")
       ]

with open('input.json', 'r') as fp:
cl = NaiveBayesClassifier(fp, format=""json"")

print(cl.classify(""Oil ends year with biggest gain since 2009""))  # ""pos""
print(cl.classify(""25 dead in Baghdad blasts""))  # ""neg""
</code></pre>
","python, sentiment-analysis, text-classification, textblob","<p>You can first load the json and then create list of tuples(text, label) with the replacement.</p>

<p>Demonstration:</p>

<p>Suppose the input.json file is something like this:</p>

<pre><code>[
    {""text"": ""I love this sandwich."", ""label"": ""pos""},
    {""text"": ""This is an amazing place!"", ""label"": ""pos""},
    {""text"": ""I do not like this restaurant"", ""label"": ""neg""}
]
</code></pre>

<p>Then you can use:</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
import json

train_list = []
with open('input.json', 'r') as fp:
    json_data = json.load(fp)
    for line in json_data:
        text = line['text']
        text = text.replace("" is "", "" "") # you can remove multiple stop words
        label = line['label']
        train_list.append((text, label))
    cl = NaiveBayesClassifier(train_list)

from pprint import pprint
pprint(train_list)
</code></pre>

<p>output:</p>

<pre><code>[(u'I love this sandwich.', u'pos'),
 (u'This an amazing place!', u'pos'),
 (u'I do not like this restaurant', u'neg')]
</code></pre>
",-1,1,1770,2017-02-20 14:02:48,https://stackoverflow.com/questions/42346637/which-is-the-efficient-way-to-remove-stop-words-in-textblob-for-sentiment-analys
Error in implementing Aspect-based sentiment analysis deep learning model,"<p>I have a task of <code>Aspect based sentiment analysis</code> where I have to first predict aspect for each sentence. Aspects are pre-defined &amp; they are in total 19.     </p>

<p>I have to implement a <code>2-layer Neural Network</code> (for the above task) where the first layer is fully connected &amp; second layer outputs a softmax distribution.<br>
Each sentence is represented by an average of the word vectors. Word vector model which is used is <code>GoogleNews 300 dimensional wordvec model</code> [ <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing</a> ].<br>
So for each sentence I have a 300 dimensional word vector (average of all the 300-dim word vectors of all the words in the sentence).<br>
The output <code>y</code> is defined as <code>y(i) = 1/k</code> when the sentence has aspect i &amp; total of k aspects<br>
otherwise <code>y(i)=0</code><br>
(Note that - A sentence can have multiple aspects)    </p>

<p>To implement above model -<br>
I have in total 1739 sentences &amp; for each of them I have a <code>300-dim</code> word vector. So I have a <code>np.array</code> <code>X</code> of shape <code>(1739,300)</code>. Similarly <code>y</code> is <code>np.array</code> of shape <code>(1739,19)</code>.       </p>

<p>My code using keras is -      </p>

<pre><code>model = Sequential()

model.add(Dense(output_dim=64, input_dim=300))
model.add(Activation('relu'))

model.add(Dense(output_dim=19))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

model.fit(X, y, nb_epoch=3)     
</code></pre>

<p>This code throws me error -<br>
<code>Exception: Error when checking model input: expected dense_input_4 to have shape (None, 300) but got array with shape (1739, 1)</code>          </p>

<p>I am new to deep learning &amp; keras, so if anybody can throw some light on what I am doing wrong?</p>
","nlp, deep-learning, keras, sentiment-analysis","<p>The problem lies in a wrong dimensionality of your data. It should have shape <code>(1739, 300)</code>. Recheck your data preparation as an error probably occured there.</p>
",1,1,321,2017-02-20 14:55:05,https://stackoverflow.com/questions/42347678/error-in-implementing-aspect-based-sentiment-analysis-deep-learning-model
How can I analyze a nonstructured text?,"<p>I use TF-IDF to affect weight that can help me to construct my dictionary. but my model is not really good enough because I have unstructured text. </p>

<p>Any suggestions about TF-IDF similar algorithms?</p>
","data-mining, sentiment-analysis, text-analysis","<p>When you say, your model is not good enough, does it mean that your generated dictionary is not good enough? Extracting key terms and constructing the dictionary using TF-IDF weight is actually feature selection step. </p>

<p>To extract or select features for your model, you can follow other approaches like principle component analysis, latent semantic analysis etc. Lot of other feature selection techniques in machine learning can be useful too!</p>

<p>But I truly believe for sentiment classification task, TF-IDF should be a very good approach to construct the dictionary. I rather suggest you to tune your model parameters when you are training it rather than blaming the feature selection approach.</p>

<p>There are many deep learning techniques as well that are applicable for your target task.</p>
",1,1,31,2017-02-22 15:44:14,https://stackoverflow.com/questions/42396054/how-can-i-analyze-a-nonstructured-text
What is the relationship between Machine Learning and Sentiment Analysis?,"<p>When considering Social Networks like Twitter and Facebook, what can be the mechanism / technique used to do predictions for ""age"" and ""gender""? Can it be done through Machine Learning or Sentiment Analysis or both?</p>
","machine-learning, social-networking, sentiment-analysis","<p>Machine learning is a part of artificial intelligence where your algorithms learn on (usually big) data. It subdivides into classification, regression, clustering and other disciplines.</p>

<p>Natural language processing can use machine learning but it can be also engineered by hand.</p>

<p>Sentiment analysis is a part of NLP. It usually uses machine learning (classification).</p>

<p>When considering Social Networks like Twitter and Facebook to do predictions for ""age"" and ""gender"", sentiment analysis can be used and it's a ML mechanism used in NLP.</p>
",1,1,146,2017-02-23 11:46:50,https://stackoverflow.com/questions/42414917/what-is-the-relationship-between-machine-learning-and-sentiment-analysis
Sentiment analysis for Dutch tweets using NLTK Corpus conll2002,"<p>I need sentiment analysis done for a list of tweets in Dutch language and I am using <code>conll2002</code> for the same. Here is the code that I'm using :</p>

<pre><code>import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import conll2002
import time

t=time.time()

def word_feats(words):
    return dict([(word, True) for word in words])

#negids = conll2002.fileids('neg')
def train():
    #negids = conll2002.fileids('neg')
    #posids = conll2002.fileids('pos')
    negids = conll2002.fileids()
    posids = conll2002.fileids()

    negfeats = [(word_feats(conll2002.words(fileids=[f])), 'neg') for f in negids]
    posfeats = [(word_feats(conll2002.words(fileids=[f])), 'pos') for f in posids]

    negcutoff = len(negfeats)*3/4
    poscutoff = len(posfeats)*3/4

    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]
    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]
    print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))

    classifier = NaiveBayesClassifier.train(trainfeats)
    print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)
    classifier.show_most_informative_features()
x=train()
print x
print time.time()-t
</code></pre>

<p>The above code works but with the output as following :</p>

<pre><code>train on 8 instances, test on 4 instances
accuracy: 0.5
Most Informative Features
                poderlas = True              pos : neg    =      1.0 : 1.0
                   voert = True              pos : neg    =      1.0 : 1.0
            contundencia = True              pos : neg    =      1.0 : 1.0
          encuestocracia = None              pos : neg    =      1.0 : 1.0
                 alivien = None              pos : neg    =      1.0 : 1.0
                  Bogotá = True              pos : neg    =      1.0 : 1.0
          Especialidades = True              pos : neg    =      1.0 : 1.0
         hoofdredacteurs = True              pos : neg    =      1.0 : 1.0
               quisieron = True              pos : neg    =      1.0 : 1.0
               asciendan = None              pos : neg    =      1.0 : 1.0
None
9.21083234
</code></pre>

<p>The pos:neg ratio comes out to be 1:1 for all cases. How can I fix that? I figured the problem might be in the following statements that I have currently commented out in the code :</p>

<pre><code>negids = conll2002.fileids('neg')
posids = conll2002.fileids('pos')
</code></pre>

<p>The error that I get if I don't comment out the above two statements is :</p>

<pre><code>Traceback (most recent call last):
  File ""naive1.py"", line 31, in &lt;module&gt;
    x=train()
  File ""naive1.py"", line 13, in train
    negids = conll2002.fileids('neg')
TypeError: fileids() takes exactly 1 argument (2 given)
</code></pre>

<p>I tried using self to fix this problem but it still doesn't work. Can someone please point me in the right direction? Thanks in advance.</p>
","python, twitter, nltk, sentiment-analysis, corpus","<p>The <code>fileids()</code> method accepts a <code>categories</code> argument, but in categorized corpora only. For example:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; brown.fileids(""mystery"")
['cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 
'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 
'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24']
</code></pre>

<p>Your calls are failing because the CONLL corpora do not have categories. And this is because they are not annotated for sentiment: both CONLL 2000 and CONLL 2002 are chunked corpora (NP/PP and named entities respectively).</p>

<pre class=""lang-none prettyprint-override""><code>&gt;&gt;&gt; conll2002.categories()
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'ConllChunkCorpusReader' object has no attribute 'categories'
</code></pre>

<p>So the short answer to your question is, you can't train a sentiment analyzer on the conll2002 corpus.</p>
",0,0,1898,2017-02-26 21:33:59,https://stackoverflow.com/questions/42474517/sentiment-analysis-for-dutch-tweets-using-nltk-corpus-conll2002
How to get the positive score only?,"<p>I am new to sentiment analysis.I want to get the positive score only not all like compound,neg ,pos,neutral.Can anyone help me to achieve this?</p>

<pre><code>sid = SentimentIntensityAnalyzer()
ss = sid.polarity_scores(sentence) 
</code></pre>

<p>thanks in advance.</p>
","python-3.x, nltk, sentiment-analysis, vader","<p>Based on the <a href=""http://www.nltk.org/_modules/nltk/sentiment/vader.html"" rel=""nofollow noreferrer"">source code</a>, the method returns a <em>dictionary</em> with shape:</p>

<pre><code>{""neg"" : ..., ""neu"" : ..., ""pos"" : ..., ""compound"" : ...}
</code></pre>

<p>So you can simply use:</p>

<pre><code>sid = SentimentIntensityAnalyzer()
ss = sid.polarity_scores(sentence)<b>['pos']</b> # the positive score</code></pre>

<p>Here <code>['pos']</code> fetches the value that is associated with the <code>'pos'</code> key.</p>
",2,4,2538,2017-03-07 14:29:54,https://stackoverflow.com/questions/42650881/how-to-get-the-positive-score-only
Import Txt file with two mixed columns,"<p>I want to import a txt file as below:</p>

<pre><code>0 @switchfoot http://twitpic.com/2y1zl - Awww  that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D
0 is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!
0 @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds
4 my whole body feels itchy and like its on fire 
4 @nationwideclass no  it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. 
0 @Kwesidei not the whole crew 
</code></pre>

<p>The desired return is a numpy.array with two columns, the <code>sentiment='0' or '4'</code> and <code>tw='string'</code>. But it keeps giving me error. Could anyone help?</p>

<pre><code>Train_tw=np.genfromtxt(""classified_tweets0.txt"",dtype=(int,str),names=['sentiment','tw'])
</code></pre>
","numpy, sentiment-analysis","<p>The error with your expression is</p>

<pre><code>ValueError: mismatch in size of old and new data-descriptor
</code></pre>

<p>If I use <code>dtype=None</code>, I get</p>

<pre><code>ValueError: Some errors were detected !
    Line #2 (got 22 columns instead of 20)
    Line #3 (got 19 columns instead of 20)
    Line #4 (got 11 columns instead of 20)
    Line #5 (got 22 columns instead of 20)
    Line #6 (got 6 columns instead of 20)
</code></pre>

<p>working from 'white space' delimiter, it breaks each line into 20,22, etc fields.  The spaces within the text are delimiters just like the first.</p>

<p>One option is to edit the file, and replace the first space with some unique delimiter.  Another option is to use the field length version of the delimiter.  After a bit of experimentation, this load looks reasonable (This is Py3, so I'm using Unicode string dtype).</p>

<pre><code>In [32]: np.genfromtxt(""stack42754603.txt"",dtype='int,U100',delimiter=[2,100],names=['sentiment','tw'])
Out[32]: 
array([ (0, ""@switchfoot http://twitpic.com/2y1zl - Awww  that's a bummer.  You shoulda got David Carr of Third D""),
       (0, ""is upset that he can't update his Facebook by texting it... and might cry as a result  School today ""),
       (0, '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n'),
       (4, 'my whole body feels itchy and like its on fire\n'),
       (4, ""@nationwideclass no  it's not behaving at all. i'm mad. why am i here? because I can't see you all o""),
       (0, '@Kwesidei not the whole crew')], 
      dtype=[('sentiment', '&lt;i4'), ('tw', '&lt;U100')])
</code></pre>
",0,0,122,2017-03-12 23:49:41,https://stackoverflow.com/questions/42754603/import-txt-file-with-two-mixed-columns
PHP code to create a negative word dictionary and search if a post has negative words,"<p>I'm trying to develop a PHP application where it takes comments from users and then match the string to check if the comment is positive or negative. I have  list of negative words in negative.txt file. If a word is matched from the word list, then I want a simple integer counter to increment by 1. I tried the some links and created the a code to check if the comment has is negative or positive but it is only matching the last word of the file.Here's the code what i have done.</p>

<pre><code>    &lt;?php 
    function teststringforbadwords($comment) 
    {
      $file=""BadWords.txt"";
      $fopen = fopen($file, ""r"");
      $fread = fread($fopen,filesize(""$file""));
      fclose($fopen);
      $newline_ele = ""\n"";
      $data_split = explode($newline_ele, $fread);
      $new_tab = ""\t"";
      $outoutArr = array();
      //process uploaded file data and push in output array
      foreach ($data_split as $string)
      {
          $row = explode($new_tab, $string);
          if(isset($row['0']) &amp;&amp; $row['0'] != """"){
              $outoutArr[] = trim($row['0'],"" "");
          }
      }
      //---------------------------------------------------------------
        foreach($outoutArr as $word) {

        if(stristr($comment,$word)){
            return false;
        }
    }
    return true;
}

    if(isset($_REQUEST[""submit""]))
    {
        $comments = $_REQUEST[""comments""];
        if (teststringforbadwords($comments)) 
        {
            echo 'string is clean';
        }
        else
        {
            echo 'string contains banned words';
        }
    }
    ?&gt;
</code></pre>

<p>Link Tried : <a href=""https://stackoverflow.com/questions/5615146/check-a-string-for-bad-words"">Check a string for bad words?</a></p>
","php, search, sentiment-analysis, words","<p>I added the <code>strtolower</code> function around both your <code>$comments</code> and your input from the file. That way if someone spells <code>STUPID</code>, instead of <code>stupid</code>, the code will still detect the bad word.</p>

<p>I also added <code>trim</code> to remove unnecessary and disruptive whitespace (like newline).</p>

<p>Finally, I changed the way how you check the words.  I used a <code>preg_match</code> to split about all whitespace so we are checking only full words and don't accidentally ban incorrect strings.</p>

<pre><code>&lt;?php 
    function teststringforbadwords($comment) 
    {
      $comment = strtolower($comment);
      $file=""BadWords.txt"";
      $fopen = fopen($file, ""r"");
      $fread = strtolower(fread($fopen,filesize(""$file"")));
      fclose($fopen);
      $newline_ele = ""\n"";
      $data_split = explode($newline_ele, $fread);
      $new_tab = ""\t"";
      $outoutArr = array();
      //process uploaded file data and push in output array
      foreach ($data_split as $bannedWord)
      {
          foreach (preg_split('/\s+/',$comment) as $commentWord) {
              if (trim($bannedWord) === trim($commentWord)) {
                  return false;
              }
          }
    }
    return true;
}
</code></pre>
",1,2,705,2017-03-15 03:45:30,https://stackoverflow.com/questions/42800739/php-code-to-create-a-negative-word-dictionary-and-search-if-a-post-has-negative
How to generate sentiment treebank in Stanford NLP,"<p>I'm using Sentiment Stanford NLP library for sentiment analytics.</p>

<p>Now I want to generate a treebank from a sentence</p>

<p>input sentence: ""Effective but too-tepid biopic""</p>

<p>output tree bank: (2 (3 (3 Effective) (2 but)) (1 (1 too-tepid) (2 biopic)))</p>

<p>Can anybody show me how to do it ?
Thank all.</p>
","stanford-nlp, sentiment-analysis, penn-treebank","<p>So I had to push a bug fix for the SentimentPipeline.</p>

<p>If you get the latest code from GitHub and use that version: <a href=""https://github.com/stanfordnlp/CoreNLP"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/CoreNLP</a></p>

<p>you can issue this command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.sentiment.SentimentPipeline -file example-sentences.txt -output PENNTREES
</code></pre>

<p>and you'll get output like this:</p>

<pre><code>I really liked the movie.
(3 (2 I) (3 (2 really) (3 (3 (3 liked) (2 (2 the) (2 movie))) (2 .))))
I hated the movie.
(1 (2 I) (1 (1 (1 hated) (2 (2 the) (2 movie))) (2 .)))
</code></pre>
",0,1,983,2017-03-15 04:42:47,https://stackoverflow.com/questions/42801238/how-to-generate-sentiment-treebank-in-stanford-nlp
R: RSentiment::calculate_score() returns &quot;Error: arguments imply differing number of rows&quot;,"<p>I'm trying to apply RSentiment::calculate_score() to a set of sentences stored in a data.frame. Here's how I get my data:</p>

<pre><code>install.packages(""pacman"")
pacman::p_load(XML, dplyr, tidyr, stringr, rvest, audio, xml2, purrr, tidytext, ggplot2)

sapiens_code = ""1846558239""
deus_ex_code = ""1910701874""

function_product &lt;- function(prod_code){
  url &lt;- paste0(""https://www.amazon.co.uk/dp/"",prod_code)
  doc &lt;- xml2::read_html(url)
  prod &lt;- html_nodes(doc,""#productTitle"") %&gt;% html_text() %&gt;%
    gsub(""\n"","""",.) %&gt;%
    gsub(""^\\s+|\\s+$"", """", .) #Remove all white space
  prod
}

sapiens &lt;- function_product(sapiens_code)
deus_ex &lt;- function_product(deus_ex_code)

#Source function to Parse Amazon html pages for data 
source(""https://raw.githubusercontent.com/rjsaito/Just-R-Things/master/Text%20Mining/amazonscraper.R"")

# extracting reviews 
pages &lt;- 13

function_page &lt;- function(page_num, prod_code){
  url2 &lt;- paste0(""http://www.amazon.co.uk/product-reviews/"",prod_code,""/?pageNumber="", page_num)
  doc2 &lt;- read_html(url2)

  reviews &lt;- amazon_scraper(doc2, reviewer = F, delay = 2)
  reviews
}

sapiens_reviews &lt;- map2(1:pages, sapiens_code, function_page) %&gt;%   bind_rows()

deusex_reviews &lt;- map2(1:pages, deus_ex_code, function_page) %&gt;% bind_rows()

sapiens_reviews$comments &lt;- gsub(""\\."", ""\\. "",   sapiens_reviews$comments)
deusex_reviews$comments &lt;- gsub(""\\."", ""\\. "", deusex_reviews$comments)

sentence_function &lt;- function(df){
  df_sentence &lt;- df %&gt;% 
    select(comments, format, stars, helpful) %&gt;% 
    unnest_tokens(sentence, comments, token = ""sentences"")
  df_sentence
}

sapiens_sentence &lt;- sentence_function(sapiens_reviews)
deusex_sentence &lt;- sentence_function(deusex_reviews)
</code></pre>

<p>But when I try to assign a score to them, I receive an error:</p>

<pre><code> deusex_sentence &lt;- deusex_sentence %&gt;% 
   mutate(sentence_score &lt;- unname(calculate_score(sentence)))
</code></pre>

<blockquote>
  <p>Error: arguments imply differing number of rows: 34, 33</p>
</blockquote>

<p>I can't see anything fundamentally wrong with the format of my input and the output for randomly picked sentences seems fine, e.g.</p>

<pre><code>unname(calculate_score(sapiens_sentence[1, 4]))
[1] -1
</code></pre>

<p>Any ideas how to get around this? Big thanks for your help!</p>
","r, text-mining, sentiment-analysis","<p>It turns out that the problem was caused by special characters in the sentence. After removing them, I could successfully run the sentiment analysis (I incorporated the data-cleaning step in the function):</p>

<pre><code>sentence_function &lt;- function(df){
  df_sentence &lt;- df %&gt;% 
    select(comments, format, stars, helpful) %&gt;% 
    unnest_tokens(sentence, comments, token = ""sentences"") %&gt;%
    mutate(sentence2 = str_replace_all(sentence, ""[^[:alnum:]]"", "" "")) #removing all special characters

  df_sentence &lt;- df_sentence  %&gt;%
    mutate(sentence_score = unname(calculate_score(sentence2))) 

  df_sentence
}

# go and get a hot drink while this is running 
sapiens_sentence &lt;- sentence_function(sapiens_reviews)
deusex_sentence &lt;- sentence_function(deusex_reviews) 
</code></pre>
",1,1,425,2017-03-15 23:11:22,https://stackoverflow.com/questions/42822442/r-rsentimentcalculate-score-returns-error-arguments-imply-differing-numbe
Error in if (!all(o)) { : missing value where TRUE/FALSE needed R,"<p>I try to create a sentiment analysis system using word2vec and logistic multinomial regression.</p>

<p>For this, I tried to do like the author did here : <a href=""http://analyzecore.com/2017/02/08/twitter-sentiment-analysis-doc2vec/"" rel=""nofollow noreferrer"">http://analyzecore.com/2017/02/08/twitter-sentiment-analysis-doc2vec/</a></p>

<p>Here the R code : </p>

<pre><code>library(tidyverse)
library(text2vec)
library(caret)
library(glmnet)
library(ggrepel)

Train_classifier &lt;- read.csv('IRC.csv',header=T, sep="";"")
Test_classifier &lt;- read.csv('IRC2.csv',header=T, sep="";"")

# select only 4 column of the dataframe

Train &lt;- Train_classifier[, c(""Note.Reco"", ""Raison.Reco"", ""DATE_SAISIE"", ""idpart"")]
Test &lt;- Test_classifier[, c(""Note.Reco"", ""Raison.Reco"", ""DATE_SAISIE"", ""idpart"")]

#delete rows with empty value columns
subTrain &lt;- Train[rowSums(Train == '') == 0,]
subTrain$ID &lt;- seq.int(nrow(subTrain))

 subTrain
      DATE_SAISIE    idpart    ID  Raison.Reco  Note.Reco
2      19/03/2014 102853645     1  Good         0
3      19/03/2014   1072309     2  Not good     2
4      19/03/2014    191391     3  very good    9
6      19/03/2014     14529     4  not comment  8
7      19/03/2014 100065501     5  very professional  9
8      19/03/2014 102261392     6  very good  1
9      19/03/2014 102734704     7  good  10
10     19/03/2014   1004397     8  not very good  10

# # replacing class values
subTrain$Note.Reco = ifelse(subTrain$Note.Reco &gt;= 0 &amp; subTrain$Note.Reco &lt;= 4, 0, ifelse(subTrain$Note.Reco &gt;= 5 &amp;
subTrain$Note.Reco &lt;= 6, 1, ifelse(subTrain$Note.Reco &gt;= 7 &amp; subTrain$Note.Reco &lt;= 8, 2, 3)))


subTest &lt;- Test[rowSums(Test == '') == 0,]
subTest$ID &lt;- seq.int(nrow(subTest))

#Data pre processing
#Doc2Vec

prep_fun &lt;- tolower
tok_fun &lt;- word_tokenizer

subTrain[] &lt;- lapply(subTrain, as.character)
it_train &lt;- itoken(subTrain$Raison.Reco, 
                   preprocessor = prep_fun, 
                   tokenizer = tok_fun,
                   ids = subTrain$ID,
                   progressbar = TRUE)



subTest[] &lt;- lapply(subTest, as.character)
it_test &lt;- itoken(subTest$Raison.Reco, 
                   preprocessor = prep_fun, 
                   tokenizer = tok_fun,
                   ids = subTest$ID,
                   progressbar = TRUE)


#creation of vocabulary and term document matrix
  ### fichier  d'apprentissage
vocab_train &lt;- create_vocabulary(it_train)
vectorizer_train &lt;- vocab_vectorizer(vocab_train)
dtm_train &lt;- create_dtm(it_train, vectorizer_train)


  ###  test data



vocab_test &lt;- create_vocabulary(it_test)
vectorizer_test &lt;- vocab_vectorizer(vocab_test)
dtm_test &lt;- create_dtm(it_test, vectorizer_test)

##Define  tf-idf model 

tfidf &lt;- TfIdf$new()
# fit the model to the train data and transform it with the fitted model
dtm_train_tfidf &lt;- fit_transform(dtm_train, tfidf)
dtm_test_tfidf &lt;- fit_transform(dtm_test, tfidf)

glmnet_classifier &lt;- cv.glmnet(x = dtm_train_tfidf,
                               y = subTrain[['Note.Reco']], family = 'multinomial',type.multinomial = ""grouped"")
</code></pre>

<p>When I run this code I get this error : </p>

<blockquote>
<pre><code>&gt; glmnet_classifier &lt;- cv.glmnet(x = dtm_train_tfidf,
+                                y = subTrain[['Note.Reco']], family = 'multinomial',type.multinomial = ""grouped"")
Error in if (!all(o)) { : missing value where TRUE/FALSE needed
</code></pre>
</blockquote>

<p>Any idea please? </p>

<p>Thank you</p>

<p><strong>EDIT:</strong> </p>

<p>The problem come from this line line @Edward Moseley said in his comment :</p>

<pre><code>subTrain[] &lt;- lapply(subTrain, as.character)
</code></pre>

<p>But when I delete it and I move to this line : </p>

<pre><code>it_train &lt;- itoken(subTrain$Raison.Reco, 
                   preprocessor = prep_fun, 
                   tokenizer = tok_fun,
                   ids = subTrain$ID,
                   progressbar = TRUE)
</code></pre>

<p>I get this error : </p>

<pre><code>Error in UseMethod(""itoken"") : 
  no applicable method for 'itoken' applied to an object of class ""factor""




subTrain
&gt; subTrain
      Note.Reco
1             3
2             3
3             2
4             3
5             3
6             1
7             3
8             1
9             2
10            3
11            3
12            3
13            3
14            2
15            2
16            3
17            3
18            2
19            3
20            2
21            2
22            2
23            0
24            0
25            2
26            3
27            3
28            0
29            0
30            2
31            3
32            3
33            3
34            3
35            0
36            1
37            2
38            1
39            3
40            3
41            3
42            1
43            3
44            2
45            3
46            3
47            2
48            3
49            3
50            2
51            1
52            1
53            2
54            3
55            3
56            2
57            2
58            3
59            2
60            1
61            3
62            0
63            2
64            2
65            3
66            0
67            1
68            3
69            2
70            2
71            3
72            2
73            2
74            2
75            3
76            2
77            2
78            3
79            3
80            3
81            3
82            2
83            2
84            1
85            0
86            2
87            0
88            3
89            3
90            3
91            2
92            1
93            2
94            1
95            3
96            3
97            2
98            2
99            3
100           3
101           0
102           2
103           2
104           0
105           2
106           3
107           2
108           2
109           2
110           3
111           3
112           2
113           2
114           2
115           3
116           3
117           2
118           3
119           3
120           3
121           3
122           2
123           3
124           2
125           2
126           0
127           3
128           3
129           0
130           3
131           0
132           1
133           3
134           2
135           0
136           1
137           3
138           1
139           3
140           3
141           3
142           2
143           2
144           3
145           2
146           2
147           3
148           1
149           1
150           3
151           2
152           2
153           3
154           2
155           3
156           2
157           3
158           3
159           3
160           0
161           2
162           1
163           3
164           3
165           1
166           2
167           2
168           3
169           2
170           3
171           3
172           3
173           2
174           2
175           3
176           3
177           0
178           3
179           2
180           3
181           0
182           3
183           3
184           2
185           3
186           3
187           1
188           3
189           1
190           2
191           2
192           3
193           3
194           3
195           2
196           2
197           3
198           2
199           0
200           3
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ... &lt;truncated&gt;
1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ... &lt;truncated&gt;
3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ... &lt;truncated&gt;
4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ... &lt;truncated&gt;
5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ... &lt;truncated&gt;
7  
</code></pre>

<p>I see this ""truncated value"" in the dataframe, is it normal?</p>
","r, sentiment-analysis","<p>I think you're introducing problems when you <code>lapply()</code>.</p>

<p>Try checking: <code>(dim(subTrain[['Note.Reco']]) &gt; 0)</code></p>

<p>If that evaluates to <code>TRUE</code> then you may have a different problem than I describe below.</p>

<p>From the <a href=""https://github.com/cran/glmnet"" rel=""nofollow noreferrer"" title=""glmnet github"">glmnet github</a> I am seeing: <code>if (!all(o)) {</code> as part of <code>glmnet/R/lognet.R</code>, (which I am assuming is called by <code>cv.glmnet</code>). in the <a href=""https://github.com/cran/glmnet"" rel=""nofollow noreferrer"" title=""glmnet github"">lognet</a> function we see:</p>

<pre><code>27   weights=drop(y%*%rep(1,nc)) 
28   o=weights&gt;0 
29   if(!all(o)){ #subset the data 
</code></pre>

<p>Earlier, on line 2 we see <code>nc</code> defined: </p>

<pre><code>2   nc=dim(y)
</code></pre>

<p>The rest of the code depends on the values of <code>nc</code>, so I would suggest determining the results of <code>dim(subTrain[['Note.Reco']])</code> for starters. You could try accessing those data differently:</p>

<pre><code>glmnet_classifier &lt;- cv.glmnet(x = dtm_train_tfidf, y = subTrain$Note.Reco, family = 'multinomial',type.multinomial = ""grouped"")
</code></pre>

<p>Also, if you construct a <code>data.frame</code> we can copy/paste to work with it will be much easier to debug.</p>
",1,0,1742,2017-03-16 16:14:16,https://stackoverflow.com/questions/42839404/error-in-if-allo-missing-value-where-true-false-needed-r
"TypeError: tuple indices must be integers or slices, not str Python sentiment tweet","<p>I am trying to capture real live tweets. I am trying to access contents using the json library, and create new objects and append this into a list.</p>

<pre><code>from tweepy.streaming import StreamListener
from tweepy import OAuthHandler
from tweepy import Stream
import tweepy

import json
import urllib.parse
from urllib.request import urlopen
import json

# Variables that contains the user credentials to access Twitter API
consumer_key = """"
consumer_secret = ""C""
access_token = """"
access_token_secret = """"
sentDexAuth = ''


auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)



def sentimentAnalysis(text):
    encoded_text = urllib.parse.quote(text)
    API_Call = 'http://sentdex.com/api/api.php?text=' + encoded_text + '&amp;auth=' + sentDexAuth
    output = urlopen(API_Call).read()

    return output

class StdOutListener(StreamListener):
    def __init___(self):
        self.tweet_data = []


def on_data(self, data):
    tweet = json.loads(data)
    for x in tweet.items():
        sentimentRating = sentimentAnalysis(x['text'])
        actualtweets = {
            'created_at' : x['created_at'],
            'id' : x['id'],
            'tweets': x['text'] + sentimentRating
        }
        self.tweet_data.append(actualtweets)


    with open('rawtweets2.json', 'w') as out:
        json.dump(self.tweet_data, out)

    print(tweet)
l = StdOutListener()
stream = Stream(auth, l)
keywords = ['iknow']
stream.filter(track=keywords)
</code></pre>

<p>I believe that i am accessing the json objects correctly however i am not sure of this error, i need it to be a string for my sentiment function to work, and 
iam getting a type error:</p>

<pre><code>sentimentRating = sentimentAnalysis(x['text'])
TypeError: tuple indices must be integers or slices, not str
</code></pre>
","python, json, tweepy, sentiment-analysis, urllib3","<p>Here,</p>

<pre><code>for x in tweet.items():
        sentimentRating = sentimentAnalysis(x['text'])
</code></pre>

<p><code>x</code> is a tuple of your dictionary's <code>(key,value)</code>so you have to pass index.</p>

<p>if you simply want the data of <code>'text'</code> key. you can write <code>tweet['text']</code></p>
",2,0,2102,2017-03-23 12:42:58,https://stackoverflow.com/questions/42976413/typeerror-tuple-indices-must-be-integers-or-slices-not-str-python-sentiment-tw
Why is pre-processing causing me to lose dictionary keys?,"<p>Having a very peculiar problem. The <code>extract</code> function takes an XML file and produces a dict using restaurant reviews as keys. Here I am doing some basic preprocessing to the text as I'm using it for sentiment analysis: the text is tokenized, punctuation is removed and it is 'un-tokenized' before being reinserted into the dict. </p>

<pre><code>import string
from nltk.tokenize import word_tokenize, RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')

def preprocess(file):
    d = extract(file)
    for text in list(d.keys()):
        tokenized_text = tokenizer.tokenize(text)
        text2 = ''.join(["" ""+i if not i.startswith(""'"") and i not in string.punctuation else i for i in tokenized_text]).strip()
        d[text2] = d.pop(text) 
    return d
</code></pre>

<p>Of the 675 reviews, 2 are missing after this function has run. These are 'great service.' and 'Delicious'. I would expect these to be returned as they are, except the full stop should be taken away from the first. </p>

<p>For reference, the <code>extract</code> function:</p>

<pre><code>from collections import OrderedDict, defaultdict
import xml.etree.ElementTree as ET

def extract(file):

    tree = ET.parse(file)
    root = tree.getroot()

    if file == 'EN_REST_SB1_TEST.xml':
        d = OrderedDict()
        for sentence in root.findall('.//sentence'):
            opinion = sentence.findall('.//Opinion')
            if opinion == []:
                text = sentence.find('text').text
                d[text] = 0

        return d 
</code></pre>

<p>If anybody is familiar with the SemEval ABSA tasks, you'll note I've done this in a somewhat roundabout way, not making use of the id tags in the XML but I'd prefer to stick to how I've done it. </p>
","python, nltk, tokenize, sentiment-analysis","<p>You're using the reviews as keys, which means you'll lose any duplicates. Evidently these very short reviews occurred twice.</p>

<p>I can't think of any reason to use the reviews as keys, especially if you care about holding on to duplicates. So why not just collect them into a list?</p>

<pre><code>d = []
...
d.append(text)
</code></pre>
",1,0,49,2017-04-01 18:28:46,https://stackoverflow.com/questions/43160508/why-is-pre-processing-causing-me-to-lose-dictionary-keys
NLTK Naive Bayes Classifier Training issues,"<p>I'm trying to train the classifier for tweets. However, the issue is that it is saying that the classifier has a 100% accuracy and the list of the most informative features doesn't display anything. Does anyone know what I'm doing wrong? I believe all my inputs to the classifier are correct, so I have no idea where it is going wrong.</p>

<p>This is the dataset I'm using:
<a href=""http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip"" rel=""nofollow noreferrer"">http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip</a></p>

<p>This is my code:</p>

<pre><code>import nltk
import random

file = open('Train/train.txt', 'r')


documents = []
all_words = []           #TODO remove punctuation?
INPUT_TWEETS = 3000

print(""Preprocessing..."")
for line in (file):

    # Tokenize Tweet content
    tweet_words = nltk.word_tokenize(line[2:])

    sentiment = """"
    if line[0] == 0:
        sentiment = ""negative""
    else:
        sentiment = ""positive""
    documents.append((tweet_words, sentiment))

    for word in tweet_words:
        all_words.append(word.lower())

    INPUT_TWEETS = INPUT_TWEETS - 1
    if INPUT_TWEETS == 0:
        break

random.shuffle(documents) 


all_words = nltk.FreqDist(all_words)

word_features = list(all_words.keys())[:3000]   #top 3000 words

def find_features(document):
    words = set(document)
    features = {}
    for w in word_features:
        features[w] = (w in words)

    return features

#Categorize as positive or Negative
feature_set = [(find_features(all_words), sentiment) for (all_words, sentment) in documents]


training_set = feature_set[:1000]
testing_set = feature_set[1000:]  

print(""Training..."")
classifier = nltk.NaiveBayesClassifier.train(training_set)

print(""Naive Bayes Accuracy:"", (nltk.classify.accuracy(classifier,testing_set))*100)
classifier.show_most_informative_features(15)
</code></pre>
","python, nltk, sentiment-analysis, naivebayes, nltk-trainer","<p>There is a typo in your code:</p>

<p>feature_set = [(find_features(all_words), sentiment) for (all_words, <strong>sentment</strong>) in documents]</p>

<p>This causes <code>sentiment</code> to have the same value all the time (namely the value of the last tweet from your preprocessing step) so training is pointless and all features are irrelevant.</p>

<p>Fix it and you will get:</p>

<pre><code>('Naive Bayes Accuracy:', 66.75)
Most Informative Features
                  -- = True           positi : negati =      6.9 : 1.0
               these = True           positi : negati =      5.6 : 1.0
                face = True           positi : negati =      5.6 : 1.0
                 saw = True           positi : negati =      5.6 : 1.0
                   ] = True           positi : negati =      4.4 : 1.0
               later = True           positi : negati =      4.4 : 1.0
                love = True           positi : negati =      4.1 : 1.0
                  ta = True           positi : negati =      4.0 : 1.0
               quite = True           positi : negati =      4.0 : 1.0
              trying = True           positi : negati =      4.0 : 1.0
               small = True           positi : negati =      4.0 : 1.0
                 thx = True           positi : negati =      4.0 : 1.0
               music = True           positi : negati =      4.0 : 1.0
                   p = True           positi : negati =      4.0 : 1.0
             husband = True           positi : negati =      4.0 : 1.0
</code></pre>
",1,0,995,2017-04-04 20:00:34,https://stackoverflow.com/questions/43216610/nltk-naive-bayes-classifier-training-issues
"How to remove hashtag, user mentions &amp; URLs from tweet. Twitter4j library(sentiment analysis) does not work properly with these noise words","<p>How to remove hashtag, user mentions &amp; URLs from tweet. Twitter4j library(sentiment analysis) does not work properly with these noise words</p>

<p>Example:
Tweet: Hello great morning today #summermorning @evilpriest @holysinner <a href=""https://goo.le/asxmo/dataload"" rel=""nofollow noreferrer"">https://goo.le/asxmo/dataload</a>.......</p>

<p>Should look like - 
Hello great morning today summermorning</p>

<p>Is there any method or utility available in twitter4J itself or we need to write our own? Please guide.</p>
","url, twitter4j, sentiment-analysis, hashtag, tweets","<p>Use regular expressions to filter out the #es before parsing a sentence through the sentiment analysis pipeline!
Use this:</p>

<pre><code>String withoutHashTweet = originalTweet.replaceAll(""[#]"", """");
</code></pre>

<p>So ""Hello great morning today #summermorning @evilpriest @holysinner "" should return : ""Hello great morning today summermorning @evilpriest @holysinner""</p>

<p>Similarly replace the hash in the code with @ to remove the respective sign</p>
",0,0,4266,2017-04-14 04:05:36,https://stackoverflow.com/questions/43405041/how-to-remove-hashtag-user-mentions-urls-from-tweet-twitter4j-librarysentim
Textblob sentiment algorithm,"<p>Does anyone know how textblob sentiment is working? I know it is working based on Pattern but I could not find any article or document explain how pattern assigns polarity value to a sentence.  </p>
","sentiment-analysis, textblob","<p>Here is the code of textblog sentiment module:
<a href=""https://github.com/sloria/TextBlob/blob/90cc87ab0f9e25f37379079840ec43aba59af440/textblob/en/sentiments.py"" rel=""noreferrer"">https://github.com/sloria/TextBlob/blob/90cc87ab0f9e25f37379079840ec43aba59af440/textblob/en/sentiments.py</a></p>

<p>As you can see, it has a training set with preclassified movie reviews, when you give a new text for analysis, it uses NaiveBayes classifier to classify the new text's polarity in <code>pos</code> and <code>neg</code> probabilities.</p>
",9,7,8864,2017-04-28 20:38:29,https://stackoverflow.com/questions/43688542/textblob-sentiment-algorithm
Polarity calculation in Sentiment Analysis using TextBlob,"<p>How is the polarity of a word in a sentence calculated using PatternAnalyser of Text Blob? </p>
","sentiment-analysis, textblob","<p>TextBlob internally uses NaiveBayes classifer for sentiment analysis,
the naivebayes classifier used in turn is the one provided by NLTK.</p>

<p>See Textblob sentiment analyzer code <a href=""https://github.com/sloria/TextBlob/blob/90cc87ab0f9e25f37379079840ec43aba59af440/textblob/en/sentiments.py"" rel=""nofollow noreferrer"">here</a>.</p>

<pre><code>@requires_nltk_corpus
    def train(self):
        """"""Train the Naive Bayes classifier on the movie review corpus.""""""
        super(NaiveBayesAnalyzer, self).train()
        neg_ids = nltk.corpus.movie_reviews.fileids('neg')
        pos_ids = nltk.corpus.movie_reviews.fileids('pos')
        neg_feats = [(self.feature_extractor(
            nltk.corpus.movie_reviews.words(fileids=[f])), 'neg') for f in neg_ids]
        pos_feats = [(self.feature_extractor(
            nltk.corpus.movie_reviews.words(fileids=[f])), 'pos') for f in pos_ids]
        train_data = neg_feats + pos_feats

 #### THE CLASSIFIER USED IS NLTK's NAIVE BAYES #####

        self._classifier = nltk.classify.NaiveBayesClassifier.train(train_data)

    def analyze(self, text):
        """"""Return the sentiment as a named tuple of the form:
        ``Sentiment(classification, p_pos, p_neg)``
        """"""
        # Lazily train the classifier
        super(NaiveBayesAnalyzer, self).analyze(text)
        tokens = word_tokenize(text, include_punc=False)
        filtered = (t.lower() for t in tokens if len(t) &gt;= 3)
        feats = self.feature_extractor(filtered)

        #### USE PROB_CLASSIFY method of NLTK classifer #####

        prob_dist = self._classifier.prob_classify(feats)
        return self.RETURN_TYPE(
            classification=prob_dist.max(),
            p_pos=prob_dist.prob('pos'),
            p_neg=prob_dist.prob(""neg"")
        )
</code></pre>

<p>Source for NLTK's NaiveBayes classifier is <a href=""http://www.nltk.org/_modules/nltk/classify/naivebayes.html"" rel=""nofollow noreferrer"">here.</a>. This returns probability distribution which is used for the result returned by Textblobs sentiment analyzer.</p>

<pre><code>def prob_classify(self, featureset):
</code></pre>
",1,1,1193,2017-05-09 13:20:19,https://stackoverflow.com/questions/43871019/polarity-calculation-in-sentiment-analysis-using-textblob
nltk corpus tweeter_sample by category,"<p>I want to train the <code>nltk</code> with the <code>tweeter_sample</code> corpus, but I get an error when I try to load the sample by category.</p>

<p>First I tried like that:</p>

<pre><code>from nltk.corpus import twitter_samples

documents = [(list(twitter_samples.strings(fileid)), category)
             for category in twitter_samples.categories()
             for fileid in twitter_samples.fileids(category)]
</code></pre>

<p>but it gave me this error:</p>

<pre><code>    Traceback (most recent call last):
  File ""C:/Users/neptun/PycharmProjects/Thesis/First_sentimental.py"", line 6, in &lt;module&gt;
    for category in twitter_samples.categories()
  File ""C:\Users\neptun\AppData\Local\Programs\Python\Python36-32\lib\site-packages\nltk\corpus\util.py"", line 119, in __getattr__
    return getattr(self, attr)
AttributeError: 'TwitterCorpusReader' object has no attribute 'categories'
</code></pre>

<p>I don't know how to give them the available attributes in order to have my list with positive and negative sentiment.</p>
","python, twitter, nltk, sentiment-analysis","<p>If you inspect <code>twitter_samples.fileids()</code>, you'll see that there are separate positive and negative files:</p>

<pre><code>&gt;&gt;&gt; twitter_samples.fileids()
['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']
</code></pre>

<p>So to get the tweets classified as positive or negative, just select the corresponding file. It's not the usual way the <code>nltk</code> handles categorized corpora, but there you have it.</p>

<pre><code>documents = ([(t, ""pos"") for t in twitter_samples.strings(""positive_tweets.json"")] + 
             [(t, ""neg"") for t in twitter_samples.strings(""negative_tweets.json"")])
</code></pre>

<p>This will get you a dataset of 10000 tweets. The third file contains another 20000, which apparently are not categorized.</p>
",5,2,3528,2017-05-10 15:43:12,https://stackoverflow.com/questions/43897203/nltk-corpus-tweeter-sample-by-category
Can the ANEW dictionary be used for sentiment analysis in quanteda?,"<p>I am trying to find a way to implement the Affective Norms for English Words (in dutch) for a longitudinal sentiment analysis with Quanteda. What I ultimately want to have is a ""mean sentiment"" per year in order to show any longitudinal trends.</p>

<p>In the data-set all words a scored on a 7-point Likert-scale by 64 coders on four categories, which provides a mean for each word. What I want to do is take one of the dimensions and use this to analyse changes in emotions over time. I realise that Quanteda has a function for implementing the LIWC-dictionary, but I would prefer using the open-source ANEW-data if possible.</p>

<p>Essentially, I need help with the implementation because I am new to coding and R</p>

<p>The ANEW file looks something like this (in .csv):</p>

<p>WORD/SCORE: cancer: 1.01, potato: 3.56, love: 6.56</p>
","r, nlp, sentiment-analysis, quanteda","<p>Not yet, directly, but... ANEW differs from other dictionaries since it does not use a key: value pair format, but rather assigns a numerical score to each term.  This means you are not counting matches of values against a key, but rather selecting features and then scoring them using weighted counts.</p>

<p>This could be done in <strong>quanteda</strong> by:</p>

<ol>
<li><p>Get ANEW features into a character vector.</p></li>
<li><p>Use <code>dfm(yourtext, select = ANEWfeatures)</code> to create a dfm with just the ANEW features.</p></li>
<li><p>Multiple each counted value by the valence of each ANEW value, recycled column-wise so that each feature count gets multiplied by its ANEW value.</p></li>
<li><p>Use <code>rowSums()</code> on the weighted matrix to get document-level valence scores.</p></li>
</ol>

<p>or alternatively,</p>

<ol start=""5"">
<li>File an <a href=""https://github.com/kbenoit/quanteda/issues"" rel=""nofollow noreferrer"">issue</a> and we will add this functionality to <strong>quanteda</strong>.</li>
</ol>

<p>Note also that <strong>tidytext</strong> uses ANEW for its sentiment scoring, if you want to convert your dfm into their object and use that approach (which is basically a version of what I've suggested above).</p>

<h2>Updated:</h2>

<p>It turns out I already built the feature into <strong>quanteda</strong> that you need, and had simply not realised it!</p>

<p>This will work.  First, load in the ANEW dictionary. (You have to supply the ANEW file yourself.)</p>

<pre><code># read in the ANEW data
df_anew &lt;- read.delim(""ANEW2010All.txt"", stringsAsFactors = FALSE)
# construct a vector of weights with the term as the name
vector_anew &lt;- df_anew$ValMn
names(vector_anew) &lt;- df_anew$Word
</code></pre>

<p>Now that we have a named vector of weights, we can apply that using <code>dfm_weight()</code>.  Below, I've first normalised the dfm by relative frequency, so that the document aggregate score is not dependent on the document length in tokens.  If you don't want that, just remove the line indicated below.</p>

<pre><code>library(""quanteda"")
dfm_anew &lt;- dfm(data_corpus_inaugural, select = df_anew$Word)

# weight by the ANEW weights
dfm_anew_weighted &lt;- dfm_anew %&gt;%
    dfm_weight(scheme = ""prop"") %&gt;%   # remove if you don't want normalized scores
    dfm_weight(weights = vector_anew)
## Warning message:
## dfm_weight(): ignoring 1,427 unmatched weight features 

tail(dfm_anew_weighted)[, c(""life"", ""day"", ""time"")]
## Document-feature matrix of: 6 documents, 3 features (5.56% sparse).
## 6 x 3 sparse Matrix of class ""dfm""
##               features
## docs                 life        day       time
##   1997-Clinton 0.07393220 0.06772881 0.21600000
##   2001-Bush    0.10004587 0.06110092 0.09743119
##   2005-Bush    0.09380645 0.12890323 0.11990323
##   2009-Obama   0.06669725 0.10183486 0.09743119
##   2013-Obama   0.08047970 0          0.19594096
##   2017-Trump   0.06826291 0.12507042 0.04985915

# total scores
tail(rowSums(dfm_anew_weighted))
## 1997-Clinton    2001-Bush    2005-Bush   2009-Obama   2013-Obama   2017-Trump 
##     5.942169     6.071918     6.300318     5.827410     6.050216     6.223944 
</code></pre>
",1,2,1406,2017-05-23 10:32:10,https://stackoverflow.com/questions/44132313/can-the-anew-dictionary-be-used-for-sentiment-analysis-in-quanteda
how to calculate the accuracy when working with corenlp,"<p>please let me know if I am unclear,</p>

<p>I found some projects in the GitHub written with scala or java with the aim of getting sentiment of the text using corenlp,</p>

<p>I had already tried other approaches to get the sentiment of the text, the approach was like this,
we had training data, so we trained data and make a model then we could evaluate our model with testing data, so the test data had an accuracy,</p>

<p>with regard to this, why no one is interested in calculating the accuracy of the result when they are working with corenlp?</p>

<p>may I ask you some ideas or approach to finding the accuracy when working with corenlp?</p>

<p>some examples:
<a href=""https://github.com/fmguler/SentimentAnalysis"" rel=""nofollow noreferrer"">sentiment1</a></p>

<p><a href=""https://github.com/peoplehum/coreNLP-Sentiment-Analysis-PredictionIO-Template"" rel=""nofollow noreferrer"">sentiment2</a></p>

<p><a href=""https://github.com/anirudh985/TwitterSentimentAnalysis"" rel=""nofollow noreferrer"">sentiment3</a></p>

<p><a href=""https://github.com/anirudh985/TwitterSentimentAnalysis"" rel=""nofollow noreferrer"">sentiment4</a></p>
","stanford-nlp, sentiment-analysis","<p>After searching I found my answer,</p>

<p>the answere is as simple as this: corenlp is not a library for classification like this,
I mean it reports the analysis of the text.
but we can have both training data and testing data again then evaluate our accuracy. its a bit time consuming but worth it if you need to work with corenlp.</p>

<p>more explanations to how to do it <a href=""https://stackoverflow.com/questions/33712795/questions-about-creating-stanford-corenlp-training-models?rq=1"">link</a></p>
",0,0,399,2017-05-25 00:48:07,https://stackoverflow.com/questions/44170614/how-to-calculate-the-accuracy-when-working-with-corenlp
how to do classification on the result of stanford-core nlp,"<p>I have a couple of questions about core nlp and doing classification,</p>

<p>firstly I should say that I have read this questions but still, I am confused:</p>

<p><a href=""https://stackoverflow.com/questions/22586658/how-to-train-the-stanford-nlp-sentiment-analysis-tool?noredirect=1&amp;lq=1"">link1</a></p>

<p><a href=""https://nlp.stanford.edu/wiki/Software/Classifier"" rel=""nofollow noreferrer"">link2</a></p>

<p><a href=""https://stackoverflow.com/questions/32085525/example-for-stanford-nlp-classifier"">link3</a></p>

<p><a href=""https://stackoverflow.com/questions/33712795/questions-about-creating-stanford-corenlp-training-models?rq=1"">link4</a></p>

<p>and some others related to this,</p>

<p>but my confusion:
when I faced with these links, I was happy that I can do something on the result of the corenlp I got to classify them, then gain the accuracy,</p>

<p>my result like other results of corenlp is something like this:</p>

<p><a href=""https://i.sstatic.net/uK7DG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uK7DG.png"" alt=""enter image description here""></a></p>

<p>but now In these links they are talking about doing labling then using stanford-classifier.</p>

<p>so it seems stanford-classifier is something that do classification on the data like other classification methods, and so there is no way to do classification on the result we get of corenl,</p>

<p>may I ask you to critique me, and share your information regarding this</p>

<p>many thanks</p>
","stanford-nlp, sentiment-analysis, text-classification","<p>Actually the right answer is that, YES stanford classifier is a supervised algorithm,</p>

<p>so if anyone want to do classification on the result of corenlp, it needs some coding , like for example I firstly did the corenlp for very negative ones, then I made the document as the text for very negative text,</p>

<p>then I made another document for very positive, and so on,</p>

<p>finally I had for example two document of the result of corenlp for positive and negative,</p>

<p>Then I used those document for the stanford classifier</p>

<p>hope helps somebody which is new to this area</p>
",0,1,503,2017-05-27 03:57:37,https://stackoverflow.com/questions/44212814/how-to-do-classification-on-the-result-of-stanford-core-nlp
convert data into openNLP compatible training format,"<p>I am trying to work on sentiment analysis using openNLP on moview review dataset available here: <a href=""http://www.cs.cornell.edu/people/pabo/movie-review-data/"" rel=""nofollow noreferrer"">http://www.cs.cornell.edu/people/pabo/movie-review-data/</a> (polarity dataset v2.0)</p>

<p>How can i train document categorizer model in openNLP using this dataset ?</p>

<p>It consists of considerable number of reviews already classified as positive, negative into different folder set. </p>

<p>openNLP needs as input a file with each review on new line with a category label prefix. I am looking for an easy way to convert this dataset into openNLP compatible format.</p>
","java, sentiment-analysis, opennlp","<p>Using java, i converted the training dataset into openNLP compatible training format i.e. a categorized dataset and saved it to a file on disk e.g.:</p>

<ul>
<li>negative movie did not meet expectations</li>
<li>positive movie was good</li>
</ul>

<p>Using above generated training set, trained openNLP documentCategorizer model.</p>

<p>To avoid training model on every execution, save the trained model on disk.</p>
",0,2,444,2017-05-27 08:33:15,https://stackoverflow.com/questions/44214725/convert-data-into-opennlp-compatible-training-format
What “information” in document vectors makes sentiment prediction work?,"<p>Sentiment prediction based on document vectors works pretty well, as examples show:
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb</a>
<a href=""http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow noreferrer"">http://linanqiu.github.io/2015/10/07/word2vec-sentiment/</a></p>

<p>I wonder what pattern is in the vectors making that possible. I thought it should be similarity of vectors making that somehow possible. Gensim similarity measures rely on cosine similarity. Therefore, I tried the following:</p>

<p>Randomly initialised a fix “compare” vector, get cosine similarity of the “compare” vector with all other vectors in training and test set, use the similarities and the labels of the train set to estimate a logistic regression model, evaluate the model with the test set.</p>

<p>Looks like this, where train/test_arrays contain document vectors and train/test_labels labels either 0 or 1. (Notice, document vectors are obtained from genism doc2vec and are well trained, predicting the test set 80% right if directly used as input for the logistic regression):</p>

<pre><code>fix_vec = numpy.random.rand(100,1)
def cos_distance_to_fix(x):
    return scipy.spatial.distance.cosine(fix_vec, x)

train_arrays_cos =  numpy.reshape(numpy.apply_along_axis(cos_distance_to_fix, axis=1, arr=train_arrays), newshape=(-1,1))
test_arrays_cos = numpy.reshape(numpy.apply_along_axis(cos_distance_to_fix, axis=1, arr=test_arrays), newshape=(-1,1))

classifier = LogisticRegression()
classifier.fit(train_arrays_cos, train_labels)
classifier.score(test_arrays_cos, test_labels)
</code></pre>

<p>It turns out, that this approach does not work, predicting the test set only to 50%....
So, my question is, what “information” is in the vectors, making the prediction based on vectors work, if it is not the similarity of vectors? Or is my approach simply not possible to capture similarity of vectors correct?</p>
","machine-learning, sentiment-analysis, gensim, feature-selection, doc2vec","<p>This is less a question about Doc2Vec than about machine-learning principles with high-dimensional data. </p>

<p>Your approach is collapsing 100-dimensions to a single dimension – the distance to your <em>random</em> point. Then, you're hoping that single dimension can still be predictive. </p>

<p>And roughly all LogisticRegression can do with that single-valued input is try to pick a threshold-number that, when your distance is on one side of that threshold, predicts a class – and on the other side, predicts not-that-class. </p>

<p>Recasting that single-threshold-distance back to the original 100-dimensional space, it's essentially trying to find a hypersphere, around your random point, that does a good job collecting all of a single class either inside or outside its volume. </p>

<p>What are the odds your randomly-placed center-point, plus one adjustable radius, can do that well, in a complex high-dimensional space? My hunch is: not a lot. And your results, no better than random guessing, seems to suggest the same. </p>

<p>The LogisticRegression with access to the full 100-dimensions finds a discriminating-frontier for assigning the class that's described by 100 coefficients and one intercept-value – and all of those 101 values (free parameters) can be adjusted to improve its classification performance. </p>

<p>In comparison, your alternative LogisticRegression with access to only the one 'distance-from-a-random-point' dimension can pick just one coefficient (for the distance) and an intercept/bias. It's got 1/100th as much information to work with, and only 2 free parameters to adjust. </p>

<p>As an analogy, consider a much simpler space: the surface of the Earth. Pick a 'random' point, like say the South Pole. If I then tell you that you are in an unknown place 8900 miles from the South Pole, can you answer whether you are more likely in the USA or China? Hardly – both of those 'classes' of location have lots of instances 8900 miles from the South Pole. </p>

<p>Only in the extremes will the distance tell you for sure which class (country) you're in – because there are parts of the USA's Alaska and Hawaii further north and south than parts of China. But even there, you can't manage well with just a single threshold: you'd need a rule which says, ""less than X <em>or</em> greater than Y, in USA; otherwise unknown"". </p>

<p>The 100-dimensional space of Doc2Vec vectors (or other rich data sources) will often only be sensibly divided by far more complicated rules. And, our intuitions about distances and volumes based on 2- or 3-dimensional spaces will often lead us astray, in high dimensions. </p>

<p>Still, the Earth analogy does suggest a way forward: there are <em>some reference points</em> on the globe that will work way better, when you know the distance to them, at deciding if you're in the USA or China. In particular, a point at the center of the US, or at the center of China, would work really well. </p>

<p>Similarly, you may get somewhat better classification accuracy if rather than a random <code>fix_vec</code>, you pick either (a) any point for which a class is already known; or (b) some average of all known points of one class. In either case, your <code>fix_vec</code> is then likely to be ""in a neighborhood"" of similar examples, rather than some random spot (that has no more essential relationship to your classes than the South Pole has to northern-Hemisphere temperate-zone countries). </p>

<p>(Also: alternatively picking N multiple random points, and then feeding the N distances to your regression, will preserve more of the information/shape of the original Doc2Vec data, and thus give the classifier a better chance of finding a useful separating-threshold. Two would likely do better than your one distance, and 100 might approach or surpass the 100 original dimensions.)</p>

<p>Finally, some comment about the Doc2Vec aspect:</p>

<p>Doc2Vec optimizes vectors that are somewhat-good, within their constrained model, at predicting the words of a text. Positive-sentiment words tend to occur together, as do negative-sentiment words, and so the trained doc-vectors tend to arrange themselves in similar positions when they need to predict similar-meaning-words. So there are likely to be 'neighborhoods' of the doc-vector space that correlate well with predominantly positive-sentiment or negative-sentiment words, and thus positive or negative sentiments. </p>

<p>These won't necessarily be two giant neighborhoods, 'positive' and 'negative', separated by a simple boundary –or even a small number of neighborhoods matching our ideas of 3-D solid volumes. And many subtleties of communication – such as sarcasm, referencing a not-held opinion to critique it, spending more time on negative aspects but ultimately concluding positive, etc – mean incursions of alternate-sentiment words into texts. A fully-language-comprehending human agent could understand these to conclude the 'true' sentiment, while these word-occurrence based methods will still be confused. </p>

<p>But with an adequate model, and the right number of free parameters, a classifier might capture some generalizable insight about the high-dimensional space. In that case, you can achieve reasonably-good predictions, using the Doc2Vec dimensions – as you've seen with the ~80%+ results on the full 100-dimensional vectors.</p>
",1,1,194,2017-06-01 11:20:24,https://stackoverflow.com/questions/44306123/what-information-in-document-vectors-makes-sentiment-prediction-work
Error while predicting Sentiment analysis Tensorflow NLTK,"<p>I am learning sentiment analysis using Tensorflow framework.</p>

<p>I am following the tutorials from <a href=""https://pythonprogramming.net/preprocessing-tensorflow-deep-learning-tutorial/?completed=/using-our-own-data-tensorflow-deep-learning-tutorial/http://"" rel=""nofollow noreferrer"">pythonprogramming_tutorial(create_feature_sets_and_labels)</a> and <a href=""https://pythonprogramming.net/train-test-tensorflow-deep-learning-tutorial/?completed=/preprocessing-tensorflow-deep-learning-tutorial/"" rel=""nofollow noreferrer"">pythonprogramming_tutorial(train_test)</a> </p>

<p>In <strong>create_sentiment_featuresets.py (1st link)</strong> , I have added only one <em>method to fetch the lexicon</em> and modified the code given  <strong>sentiment_demo.py (2nd link)</strong>  <em>to test the sentiment of a given input string.</em></p>

<p>create_sentiment_featuresets.py</p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize
import numpy as np
import random
import pickle
from collections import Counter
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
hm_lines = 100000
def create_lexicon(pos, neg):

    lexicon = []
    with open(pos, 'r') as f:
        contents = f.readlines()            # readline vs strip
        for l in contents[:len(contents)]:
            l= l.decode('utf-8')
            all_words = word_tokenize(l)
            lexicon += list(all_words)

    f.close()

    with open(neg, 'r') as f:
        contents = f.readlines()            # readline vs strip
        for l in contents[:len(contents)]:
            l= l.decode('utf-8')
            all_words = word_tokenize(l)
            lexicon += list(all_words)

    f.close()

    lexicon = [lemmatizer.lemmatize(i) for i in lexicon]
    w_counts = Counter(lexicon)
    #print(len(w_counts))
    l2 = []
    for w in w_counts:
        if 1000 &gt; w_counts[w] &gt; 50:
            l2.append(w)
    #print(len(l2))
    #print(l2)
    print(""Lexicon length create_lexicon: "",len(lexicon))

    return l2

def sample_handling(sample, lexicon, classification):

    featureset = []
    print(""Lexicon length Sample handling: "",len(lexicon))
    with open(sample, 'r') as f:
        contents = f.readlines()
        for l in contents[:len(contents)]:
            l= l.decode('utf-8')
            current_words = word_tokenize(l.lower())
            current_words= [lemmatizer.lemmatize(i) for i in current_words]

            features = np.zeros(len(lexicon))
            for word in current_words:
                if word.lower() in lexicon:
                    index_value = lexicon.index(word.lower())
                    features[index_value] +=1

            features = list(features)
            featureset.append([features, classification])
    f.close()
    print(""Feature SET------"")
    print(len(featureset))
    return featureset

def create_feature_sets_and_labels(pos, neg, test_size = 0.1):
    global m_lexicon
    m_lexicon = create_lexicon(pos, neg)
    features = []
    features += sample_handling(pos, m_lexicon, [1,0])
    features += sample_handling(neg, m_lexicon, [0,1])

    random.shuffle(features)
    features = np.array(features)

    testing_size = int(test_size * len(features))

    train_x = list(features[:,0][:-testing_size])
    #print(""TRAIN_X"", train_x)
    train_y = list(features[:,1][:-testing_size])
    #print(""TRAIN_Y"", train_y)
    test_x = list(features[:,0][-testing_size:])
    test_y = list(features[:,1][-testing_size:])

    return train_x, train_y, test_x, test_y

def get_lexicon():
    global m_lexicon
    return m_lexicon
</code></pre>

<p>For training and testing I have use the pos.txt and neg.txt given in the first link. The files contains 5000 sentences of positive and negative repectively</p>

<p>Below is my sentiment_demo.py:</p>

<pre><code>from create_sentiment_featuresets import create_feature_sets_and_labels
from create_sentiment_featuresets import get_lexicon

import tensorflow as tf
import pickle
import numpy as np

# extras for testing
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
#- end extras


train_x, train_y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')

n_nodes_hl1 = 1500
n_nodes_hl2 = 1500
n_nodes_hl3 = 1500

n_classes = 2
batch_size = 100
hm_epochs = 5

x = tf.placeholder('float')
y = tf.placeholder('float')

hidden_1_layer = {'f_fum': n_nodes_hl1,
                'weight': tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])),
                'bias': tf.Variable(tf.random_normal([n_nodes_hl1]))}
hidden_2_layer = {'f_fum': n_nodes_hl2,
                'weight': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),
                'bias': tf.Variable(tf.random_normal([n_nodes_hl2]))}
hidden_3_layer = {'f_fum': n_nodes_hl3,
                'weight': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),
                'bias': tf.Variable(tf.random_normal([n_nodes_hl3]))}
output_layer = {'f_fum': None,
                'weight': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),
                'bias': tf.Variable(tf.random_normal([n_classes]))}


def nueral_network_model(data):

    l1 = tf.add(tf.matmul(data, hidden_1_layer['weight']), hidden_1_layer['bias'])
    l1 = tf.nn.relu(l1)

    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weight']), hidden_2_layer['bias'])
    l2 = tf.nn.relu(l2)

    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weight']), hidden_3_layer['bias'])
    l3 = tf.nn.relu(l3)

    output = tf.matmul(l3, output_layer['weight']) + output_layer['bias']

    return output

def train_neural_network(x):
    prediction = nueral_network_model(x)
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits= prediction, labels= y))
    optimizer = tf.train.AdamOptimizer(learning_rate= 0.001).minimize(cost)



    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        for epoch in range(hm_epochs):
            epoch_loss = 0
            i = 0
            while i &lt; len(train_x):
                start = i
                end = i+ batch_size
                batch_x = np.array(train_x[start: end])
                batch_y = np.array(train_y[start: end])

                _, c = sess.run([optimizer, cost], feed_dict= {x: batch_x, y: batch_y})
                epoch_loss += c
                i+= batch_size
            print('Epoch', epoch+ 1, 'completed out of ', hm_epochs, 'loss:', epoch_loss)

        correct= tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))

        print('Accuracy:', accuracy.eval({x:test_x, y:test_y}))


        # testing ------Trying to predict the sentiment for an input string--------
        m_lexicon= get_lexicon()
        print('Lexicon length: ',len(m_lexicon))

        input_data= ""He is an idiot""

        current_words= word_tokenize(input_data.lower())
        current_words = [lemmatizer.lemmatize(i) for i in current_words]
        features = np.zeros(len(m_lexicon))

        for word in current_words:
            if word.lower() in m_lexicon:
                index_value = m_lexicon.index(word.lower())
                features[index_value] +=1

        features = np.array(list(features))
        print('features length: ',len(features))
        result = sess.run(tf.argmax(prediction.eval(feed_dict={x:features}), 1))
        print('RESULT: ', result)
        if result[0] == 0:
            print('Positive: ', input_data)
        elif result[0] == 1:
            print('Negative: ', input_data)


train_neural_network(x)
</code></pre>

<p>Progam is working till the priniting of epoch loss, after that its giving the following error:</p>

<pre><code>('Epoch', 1, 'completed out of ', 5, 'loss:', 1289814.4057617188)
('Epoch', 2, 'completed out of ', 5, 'loss:', 457882.97705078125)
('Epoch', 3, 'completed out of ', 5, 'loss:', 243073.83074951172)
('Epoch', 4, 'completed out of ', 5, 'loss:', 245525.22399902344)
('Epoch', 5, 'completed out of ', 5, 'loss:', 233219.91000366211)
('Accuracy:', 0.59287059)
('Lexicon length: ', 423)
('features length: ', 423)
Traceback (most recent call last):
  File ""sentiment_demo.py"", line 110, in &lt;module&gt;
train_neural_network(x)
  File ""sentiment_demo.py"", line 102, in train_neural_network
result = sess.run(tf.argmax(prediction.eval(feed_dict={x:features}), 1))
  File ""/home/lsmpc/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 569, in eval
return _eval_using_default_session(self, feed_dict, self.graph, session)
  File ""/home/lsmpc/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3741, in _eval_using_default_session
return session.run(tensors, feed_dict)
  File ""/home/lsmpc/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 778, in run
run_metadata_ptr)
  File ""/home/lsmpc/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 982, in _run
feed_dict_string, options, run_metadata)
  File ""/home/lsmpc/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1032, in _do_run
target_list, options, run_metadata)
  File ""/home/lsmpc/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1052, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: In[0] is not a matrix
 [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_Placeholder_0/_23, Variable/read)]]
 [[Node: add/_25 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_4_add"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'MatMul', defined at:
  File ""sentiment_demo.py"", line 110, in &lt;module&gt;
    train_neural_network(x)
  File ""sentiment_demo.py"", line 58, in train_neural_network
    prediction = nueral_network_model(x)
  File ""sentiment_demo.py"", line 44, in nueral_network_model
    l1 = tf.add(tf.matmul(data, hidden_1_layer['weight']), hidden_1_layer['bias'])
  File ""/home/lsmpc/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 1801, in matmul
a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/home/lsmpc/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 1263, in _mat_mul
transpose_b=transpose_b, name=name)
  File ""/home/lsmpc/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
op_def=op_def)
  File ""/home/lsmpc/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
original_op=self._default_original_op, op_def=op_def)
  File ""/home/lsmpc/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): In[0] is not a matrix
 [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](_recv_Placeholder_0/_23, Variable/read)]]
 [[Node: add/_25 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_4_add"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
</code></pre>

<p>The above error specifically points to this:</p>

<pre><code>Caused by op u'MatMul', defined at:
  File ""sentiment_demo.py"", line 110, in &lt;module&gt;
    train_neural_network(x)
  File ""sentiment_demo.py"", line 58, in train_neural_network
    prediction = nueral_network_model(x)
  File ""sentiment_demo.py"", line 44, in nueral_network_model
    l1 = tf.add(tf.matmul(data, hidden_1_layer['weight']), hidden_1_layer['bias'])
</code></pre>

<p>I am new to this and I am not able to fix it.</p>
","python, python-2.7, tensorflow, nltk, sentiment-analysis","<p>Looks like your <code>features</code> are of the wrong shape. please try this:</p>

<pre><code>    features = np.array(list(features)).reshape(1,-1)
</code></pre>

<p>Your model accepts batch data, so if you want to run just one prediction, you need to re-shape it as a batch of 1. Good luck!</p>
",1,3,192,2017-06-01 11:55:11,https://stackoverflow.com/questions/44306860/error-while-predicting-sentiment-analysis-tensorflow-nltk
Sentiment Analysis R Naive Bayes in german,"<p>I've got a problem with the naive bayes algorithm and I'm not able to find out why. I tried out an online tutorial and so in the beginning I started to create some training data. In the code below I use just very few training data, but it's the same problem.</p>

<pre><code>pos_tweets =  rbind(
  c('Ich liebe das auto', 'positive'),
  c('Diese Aussicht ist großartig', 'positive'),
  c('toller morgen', 'positive'),
  c('ich freue mich so', 'positive'),
  c('du bist aber lieb, danke', 'positive')
)

neg_tweets = rbind(
  c('ich hasse autos', 'negative'),
  c('der blick ist horror', 'negative'),
  c('voll müde heute', 'negative'),
  c('schreckliche stille', 'negative'),
  c('er ist ein feind', 'negative')
)

test_tweets = rbind(
    c('Schöne Momente erlebt', 'positive'),
    c('zusammen macht es gleich doppelt spass', 'positive'),
    c('Yeah, toller Tag', 'positive'),
    c('Super schöne Umgebung', 'positive'),
    c('es zieht ein leichter wind auf, sehr angenehm', 'positive')
)

tweetsbind = rbind(pos_tweets, neg_tweets, test_tweets)

matrix1= create_matrix(tweetsbind[,1], language=""german"", 
                      removeStopwords=FALSE, removeNumbers=TRUE, 
                      stemWords=FALSE) 
mat1 = as.matrix(matrix1)
</code></pre>

<p>Now I train my model:</p>

<pre><code>classifier1 = naiveBayes(mat1[1:10,], as.factor(tweetsbind[1:10,2]) )
</code></pre>

<p>And now I want to use it:</p>

<pre><code>predicted = predict(classifier1, mat1[11:15,]); predicted
</code></pre>

<p>When I look into my model it look pretty good, because the negative words are marked as negative and the positive ones as positive.</p>

<p>But while using the model to analyse the testing data, it only outputs negative, even when the statement are obviously positive and the used word also exist in the training set.</p>

<hr>

<p>My new code is:</p>

<pre><code># search for some twitter data
happy &lt;- searchTwitter("":)"",n = 10000, lang ='de')
happy_text &lt;- sapply(happy, function (x) x$getText())
sad &lt;- searchTwitter("":("",n = 10000, lang ='de')
sad_text &lt;- sapply(sad, function (x) x$getText())

# create the matrix
tweets &lt;- rbind(sad_text[1:2500,], happy_text[1:2500,]) # if I use more training data, I get a storage error
tweet &lt;- as.matrix(tweets)
matrix= create_matrix(tweet[,2], language= ""german"", removeStopwords=FALSE, removeNumbers=TRUE,  stemWords=FALSE) 
matrixdoc = as.matrix(matrix)

# transform to factor and train the model
X &lt;- as.data.frame(matrixdoc[1:5000,])
X$out &lt;- as.factor(tweet[1:5000,3])
X &lt;- as.data.frame(lapply(X, factor))
classifierstack &lt;- naiveBayes(out ~ ., data=X)

# predict
predicted = predict(classifierstack, mat1[11:15,],type = ""raw"" )
</code></pre>

<p>And this is the result: everything ist negativ, even if all my inputs are very positiv (I changed them).</p>

<pre><code>     negativ       positiv
[1,]       1 5.828223e-176
[2,]       1 4.110223e-244
[3,]       1 3.274458e-244
[4,]       1 3.534996e-176
[5,]       1  0.000000e+00
</code></pre>

<p>And if I try this:</p>

<pre><code>&gt; predict(classifierstack, ""zeigt"", type = ""raw"" )
     negativ positiv
[1,]     0.5     0.5
</code></pre>

<p>-> it always outputs 0.5 0.5 and is finally always negativ :/</p>
","r, sentiment-analysis","<p>You are lacking training data. If I run your code I get</p>

<pre><code>&gt; predicted = predict(classifier1, mat1[11:15,]); predicted
[1] negative negative negative positive negative
Levels: negative positive
</code></pre>

<p>So only the first two elements are wrong - the last three should indeed be negative, postive and negative. If we look at the classifier information for the words found in, say, <em>feinde sind doof</em> we find</p>

<pre><code>                             feinde
as.factor(tweetsbind[1:10, 2]) [,1] [,2]
                      negative    0    0
                      positive    0    0

                              sind
as.factor(tweetsbind[1:10, 2]) [,1] [,2]
                      negative    0    0
                      positive    0    0

                              doof
as.factor(tweetsbind[1:10, 2]) [,1] [,2]
                      negative    0    0
                      positive    0    0
</code></pre>

<p>so there really is no information to classify and it defaults to the first level category, <code>negative</code>. Try to feed it more information where there is an overlap between the words you want to predict and it should work.</p>

<hr>

<p><strong>Update</strong> If you run</p>

<pre><code>&gt; predicted = predict(classifier1, mat1[11:15,], type=""raw""); predicted
         negative     positive
[1,] 9.999959e-01 4.093637e-06
[2,] 7.329224e-01 2.670776e-01
[3,] 1.000000e+00 4.598781e-11
[4,] 9.898881e-05 9.999010e-01
[5,] 1.000000e+00 1.608783e-16
</code></pre>

<p>then you can see the individual probabilities. The ""problem"" with your fit is that the input is read as being numeric (and not as binary factors) so you will not see conditional probabilities that (row-wise) add up to one. As per the man page for <code>naiveBayes</code> you get Gaussian means and sds. You can get the conditional probabilities like this:</p>

<pre><code>X &lt;- as.data.frame(mat1[1:10,])
X$out &lt;- as.factor(tweetsbind[1:10,2])
X &lt;- as.data.frame(lapply(X, factor))
naiveBayes(out ~ ., data=X)
</code></pre>

<p>This will give you</p>

<pre><code>          hab
Y          0
  negative 1
  positive 1
          dich
Y          0
  negative 1
  positive 1
          lieb
Y            0   1
  negative 1.0 0.0
  positive 0.8 0.2
</code></pre>

<p>Those are the P(lieb|positive) probabilities and you need to you Bayes formula to invert the probabilities. </p>

<p>Google ""zero problem"" and ""naive bayes"" to get directions for making slight improvements when the words aren't present in both training and test parts (see the <code>laplace</code> argument).</p>
",0,1,327,2017-06-05 16:16:12,https://stackoverflow.com/questions/44373218/sentiment-analysis-r-naive-bayes-in-german
Issue with Spark MLLib that causes probability and prediction to be the same for everything,"<p>I'm learning how to use Machine Learning with Spark MLLib with the purpose of doing Sentiment Analysis of Tweets. I got a Sentiment Analysis dataset from here:
<a href=""http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip"" rel=""nofollow noreferrer"">http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip</a></p>

<p>That dataset contains 1 million of tweets classified as Positive or Negative. The second column of this dataset contains the sentiment and the fourth column contains the tweet.</p>

<p>This is my current PySpark code:</p>

<pre><code>import csv
from pyspark.sql import Row
from pyspark.sql.functions import rand
from pyspark.ml.feature import Tokenizer
from pyspark.ml.feature import StopWordsRemover
from pyspark.ml.feature import Word2Vec
from pyspark.ml.feature import CountVectorizer
from pyspark.ml.classification import LogisticRegression

data = sc.textFile(""/home/omar/sentiment-train.csv"")
header = data.first()
rdd = data.filter(lambda row: row != header)

r = rdd.mapPartitions(lambda x : csv.reader(x))
r2 = r.map(lambda x: (x[3], int(x[1])))

parts = r2.map(lambda x: Row(sentence=x[0], label=int(x[1])))
partsDF = spark.createDataFrame(parts)
partsDF = partsDF.orderBy(rand()).limit(10000)

tokenizer = Tokenizer(inputCol=""sentence"", outputCol=""words"")
tokenized = tokenizer.transform(partsDF)

remover = StopWordsRemover(inputCol=""words"", outputCol=""base_words"")
base_words = remover.transform(tokenized)

train_data_raw = base_words.select(""base_words"", ""label"")

word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol=""base_words"", outputCol=""features"")

model = word2Vec.fit(train_data_raw)
final_train_data = model.transform(train_data_raw)
final_train_data = final_train_data.select(""label"", ""features"")

lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)
lrModel = lr.fit(final_train_data)

lrModel.transform(final_train_data).show()
</code></pre>

<p>I'm executing this on PySpark interactive shell using this command:</p>

<pre><code>pyspark --master yarn --deploy-mode client --conf='spark.executorEnv.PYTHONHASHSEED=223'
</code></pre>

<p>(FYI: I have a HDFS cluster with 10 VMs with YARN, Spark, etc)</p>

<p>As a result of the last line of code, this is what happens:</p>

<pre><code>&gt;&gt;&gt; lrModel.transform(final_train_data).show()
+-----+--------------------+--------------------+--------------------+----------+
|label|            features|       rawPrediction|         probability|prediction|
+-----+--------------------+--------------------+--------------------+----------+
|    1|[0.00885206627292...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    1|[0.02994908031541...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    1|[0.03443818541709...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    0|[0.02838905728422...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    1|[0.00561632859171...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    0|[0.02029798456545...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    1|[0.02020387646293...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    1|[0.01861085715063...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    1|[0.00212163510598...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    0|[0.01254413221031...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    0|[0.01443821341672...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    0|[0.02591390228879...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    1|[0.00590923184063...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    0|[0.02487089103516...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    0|[0.00999667861365...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    0|[0.00416736607439...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    0|[0.00715923445144...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    0|[0.02524911996890...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    1|[0.01635813603934...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
|    0|[0.02773649083489...|[-0.0332030500349...|[0.4917,0.5083000...|       1.0|
+-----+--------------------+--------------------+--------------------+----------+
only showing top 20 rows
</code></pre>

<p>If I do the same with a smaller dataset that I have created manually it works. I don't know what is happening, have been working with this thru the day.</p>

<p>Any suggestions?</p>

<p>Thanks for your time!</p>
","python, hadoop, apache-spark, apache-spark-mllib, sentiment-analysis","<p><strong>TL;DR</strong> Ten iterations is way to low for any real life applications. On large and non-trivial datasets it can take thousand or more iterations (as well as tuning remaining parameters) to converge.</p>

<p>Binomial <a href=""https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=logisticregressionmodel.summary#pyspark.ml.classification.LogisticRegressionModel"" rel=""nofollow noreferrer""><code>LogisticRegressionModel</code></a> has <a href=""https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=logisticregressionmodel.summary#pyspark.ml.classification.LogisticRegressionModel.summary"" rel=""nofollow noreferrer""><code>summary</code></a> attribute, which can give you an access to a <a href=""https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=logisticregressionmodel.summary#pyspark.ml.classification.LogisticRegressionSummary"" rel=""nofollow noreferrer""><code>LogisticRegressionSummary</code></a> object. Among other useful metrics it contains <code>objectiveHistory</code> which can be used to debug training process:</p>

<pre><code>import matplotlib.pyplot as plt

lrm = LogisticRegression(..., family=""binomial"").fit(df)
plt.plot(lrm.summary.objectiveHistory)

plt.show()
</code></pre>
",1,2,776,2017-06-11 03:19:43,https://stackoverflow.com/questions/44480077/issue-with-spark-mllib-that-causes-probability-and-prediction-to-be-the-same-for
German Stemming for Sentiment Analysis in Python NLTK,"<p>I've recently begun working on a sentiment analysis project on German texts and I'm planning on using a stemmer to improve the results.</p>

<p>NLTK comes with a German Snowball Stemmer and I've already tried to use it, but I'm unsure about the results. Maybe it should be this way, but as a computer scientist and not a linguist, I have a problem with inflected verb forms stemmed to a different stem.</p>

<p>Take the word ""suchen"" (to search), which is stemmed to ""such"" for 1st person singular but to ""sucht"" for 3rd person singular. </p>

<p>I know there is also lemmatization, but no working German lemmatizer is integrated into NLTK as far as I know. There is GermaNet, but their NLTK integration seems to have been aborted.</p>

<p>Getting to the point: I would like inflected verb forms to be stemmed to the same stem, at the very least for regular verbs within the same tense. If this is not a useful requirement for my goal, please tell me why. If it is, do you know of any additional resources to use which can help me achieve this goal?</p>

<p>Edit: I forgot to mention, any software should be free to use for educational and research purposes.</p>
","python, nltk, sentiment-analysis, stemming, snowball","<p>As a computer scientist, you are definitely looking in the right direction to tackle this linguistic issue ;). Stemming is usually quite a bit more simplistic, and used for Information Retrieval tasks in an attempt to decrease the lexicon size, but usually not sufficient for more sophisticated linguistic analysis. Lemmatisation partly overlaps with the use case for stemming, but includes rewriting for example verb inflections all to the same root form (lemma), and also differentiating ""work"" as a noun and ""work"" as a verb (although this depends a bit on the implementation and quality of the lemmatiser). For this, it usually needs a bit more information (like POS-tags, syntax trees), hence takes considerably longer, rendering it less suitable for IR tasks, typically dealing with larger amounts of data.</p>

<p>In addition to GermaNet (didn't know it was aborted, but never really tried it, because it is free, but you have to sign an agreement to get access to it), there is SpaCy which you could have a look at: <a href=""https://spacy.io/docs/usage/"" rel=""noreferrer"">https://spacy.io/docs/usage/</a></p>

<p>Very easy to install and use. See install instructions on the website, then download the German stuff using: </p>

<pre><code>python -m spacy download de
</code></pre>

<p>then:</p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; nlp = spacy.load('de')
&gt;&gt;&gt; doc = nlp('Wir suchen ein Beispiel')
&gt;&gt;&gt; for token in doc:
...     print(token, token.lemma, token.lemma_)
... 
Wir 521 wir
suchen 1162 suchen
ein 486 ein
Beispiel 809 Beispiel
&gt;&gt;&gt; doc = nlp('Er sucht ein Beispiel')
&gt;&gt;&gt; for token in doc:
...     print(token, token.lemma, token.lemma_)
... 
Er 513 er
sucht 1901 sucht
ein 486 ein
Beispiel 809 Beispiel
</code></pre>

<p>As you can see, unfortunately it doesn't do a very good job on your specific example (suchen), and I'm not sure what the number represents (i.e. must be the lemma id, but not sure what other information can be obtained from this), but maybe you can give it a go and see if it helps you.</p>
",14,12,10593,2017-06-13 13:11:01,https://stackoverflow.com/questions/44522536/german-stemming-for-sentiment-analysis-in-python-nltk
&quot;Enhancing&quot; CoreNLP Sentiment Analysis Results,"<p>I am trying to perform sentiment analysis on a large number of product reviews using CoreNLP (Java).  Overall, I find the accuracy of the analysis to be pretty good.  From what I read, the model I'm using was initially created using movie reviews (I think), so it's not 100% suited for analyzing product reviews.  I was wondering the best way to go about ""enhancing"" the accuracy of my analysis.</p>

<p>The main thing I was thinking about was that in addition to the text of the product review, I also have a user-provided star rating.  The values range from 1-5, 1 star being the lowest.  I was hoping there was a way to take the star rating into account when generating the sentiment score, since it more accurately reflects the users' feelings on a particular product.  Is there a way I can best have the star rating factor in to the sentiment analysis scoring in CoreNLP?  My analysis code looks something like this:</p>

<pre><code>List&lt;ProductReview&gt; reviews = this.reviewRepository.findAll();
        for (ProductReview review : reviews) {
            Properties props = new Properties();
            props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref, sentiment"");
            props.put(""ner.model"", ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"");

            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

            int starRating = review.getStarRating();
            String reviewText = review.getTitle() + "" : "" + review.getReviewText();
            if (!StringUtils.isEmpty(reviewText)) {
                int longest = 0;
                int mainSentiment = 0;
                Annotation annotation = pipeline.process(reviewText);
                String sentimentStr = null;
                List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);
                for (CoreMap sentence : sentences) {
                    Tree sentimentTree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);

                    int sentiment = RNNCoreAnnotations.getPredictedClass(sentimentTree) - 2;
                    String partText = sentence.toString();
                    if (partText.length() &gt; longest) {
                        mainSentiment = sentiment;
                        sentimentStr = sentence.get(SentimentCoreAnnotations.SentimentClass.class);

                        longest = partText.length();
                    }
                }
            }
        }
</code></pre>

<p>How could I best incorporate the star ratings (or other info, such as votes on the most useful product reviews, etc.) into the analysis being performed by CoreNLP?  Is this something I would have to do separately?  Or is there a way to incorporate the additional data directly into the sentiment analysis engine?</p>
","java, stanford-nlp, sentiment-analysis","<p>There are a few enhancements possible.</p>

<p>/1. <strong>Improvised training set and contextual sentiment analysis</strong>:
Some features might get classified as positive in a movie review context, but might be negative in product review context. You shall retrain your data on your context. Method specified <a href=""https://nlp.stanford.edu/sentiment/code.html"" rel=""nofollow noreferrer"">here</a></p>

<blockquote>
  <p>Models can be retrained using the following command using the PTB
  format dataset:</p>
  
  <p>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25
  -trainPath train.txt -devPath dev.txt -train -model model.ser.gz</p>
</blockquote>

<p>A good discussion on training dataset can be found <a href=""https://stackoverflow.com/questions/22586658/how-to-train-the-stanford-nlp-sentiment-analysis-tool"">here</a>.</p>

<p>/2.<strong>Getting the contextual training and testing data :</strong>  Your product reviews data can act as training set as well as testing set. Select the reviews with extreme polarities ( 1 star POOREST, and 5 star GREAT ) as your training data, to improvide further on the content, you can select 1 and 5 star reviews which have been marked as helpful by the community.
Using this data generated your PTB dataset classifying the reviews as POSITIVE and NEGATIVE ( Neutral would be a hard thing to achieve by using 2-3-4 star rated reviews, as they can introduce noise ).</p>

<p>/3. Use 80% of your dataset as training set, and 20% as testing set. The 1 star rated reviews shall mostly get classified as NEGATIVE and 5 star shall mostly get classified as positive.
Post this, you can use the trained model to analyze sentiment of other reviews, <strong>your sentiment score</strong> ( say 0 for negative sentiment, and 5 for very positive sentiment,  or -1 for negative to +1 for very positive) <strong>will have a positive correlation with actual star rating</strong> provided along with that review. If there is a <strong>sentiment disparity</strong>, e.g. a text review comes out as having positive sentiment, but has 1 star rating, you may want to log such cases, and improvise your classification.</p>

<p>/4. <strong>Improvising using other data sources and classifiers</strong>:  <a href=""https://github.com/cjhutto/vaderSentiment"" rel=""nofollow noreferrer"">Vader sentiment</a> (in python ) is a very good classifier specially attuned for social media and things like product reviews. You may or may not chose to use it as a comparative classifier ( to cross match or have double set of your results, from corenlp+vader), but you can surely use its amazon reviews dataset as mentioned <a href=""https://github.com/cjhutto/vaderSentiment#resources-and-dataset-descriptions"" rel=""nofollow noreferrer"">here</a>:</p>

<blockquote>
  <p>amazonReviewSnippets_GroundTruth.txt FORMAT: the file is tab delimited
  with ID, MEAN-SENTIMENT-RATING, and TEXT-SNIPPET</p>
  
  <p>DESCRIPTION: includes 3,708 sentence-level snippets from 309 customer
  reviews on 5 different products. The reviews were originally used in
  Hu &amp; Liu (2004); we added sentiment intensity ratings. The ID and
  MEAN-SENTIMENT-RATING correspond to the raw sentiment rating data
  provided in 'amazonReviewSnippets_anonDataRatings.txt' (described
  below).</p>
  
  <p>amazonReviewSnippets_anonDataRatings.txt FORMAT: the file is tab
  delimited with ID, MEAN-SENTIMENT-RATING, STANDARD DEVIATION, and
  RAW-SENTIMENT-RATINGS</p>
  
  <p>DESCRIPTION: Sentiment ratings from a minimum of 20 independent human
  raters (all pre-screened, trained, and quality checked for optimal
  inter-rater reliability).</p>
</blockquote>

<p>The datasets are available in the tgz file here:
<a href=""https://github.com/cjhutto/vaderSentiment/blob/master/additional_resources/hutto_ICWSM_2014.tar.gz"" rel=""nofollow noreferrer"">https://github.com/cjhutto/vaderSentiment/blob/master/additional_resources/hutto_ICWSM_2014.tar.gz</a></p>

<p>It follows the pattern <code>reviewindex_part  polarity  review_snippet</code></p>

<pre><code>1_19    -0.65   the button was probably accidentally pushed to cause the black screen in the first place.
1_20    2.85    but, if you're looking for my opinion of the apex dvd player, i love it!
1_21    1.75    it practically plays almost everything you give it.
</code></pre>
",4,5,1115,2017-06-14 19:28:08,https://stackoverflow.com/questions/44553166/enhancing-corenlp-sentiment-analysis-results
How to merge two data frames in pandas?,"<p>I have two pandas dataframes </p>

<pre><code>Unnamed: 0  sentiment   numberagreed    tweetid tweet
0   0   2   6   219584  Apple processa a Samsung no Japão - Notícias -...
1   1   1   3   399249  É O JACKI CHAN !!! RT @user ESSE É DOS MEUS!!!...
2   2   3   3   387155  Eras o samsung galaxy tab e muito lerdo para t...
3   3   3   3   205458  Dizem que a coisa mais triste que o homem enfr...
4   4   3   3   2054404 RAIVA vou ter que ir com meu nike dinovo pra e...

tweetid sent
219584  0.494428
399249  0.789241
387155  0.351972
205458  0.396907
2054404 0.000000
</code></pre>

<p>They are not the same length and there are some missing values in the second data frame </p>

<p>I want to merge the two data frames based on the <code>tweetid</code> and drop the missing values</p>
","python, pandas, dataframe, sentiment-analysis","<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>pd.merge</code></a></p>

<pre><code>pd.merge(left=df1, right=df2, on='tweetid', how='inner')
</code></pre>

<p>Because you take the <code>inner</code>, non-overlapping parts will be thrown away. <code>on='tweetid'</code> merges it on <code>tweetid</code>.</p>
",2,2,269,2017-06-22 14:03:13,https://stackoverflow.com/questions/44701727/how-to-merge-two-data-frames-in-pandas
Unable to remove english stopwords from a dataframe,"<p>I have been trying to perform sentiment analysis over a movie reviews dataset and I am stuck at a point where I am unable to remove english stopwords from the data. What am I doing wrong?</p>

<pre><code>from nltk.corpus import stopwords
stop = stopwords.words(""English"")
list_ = []
for file_ in dataset:
    dataset['Content'] = dataset['Content'].apply(lambda x: [item for item in x.split(',') if item not in stop])
    list_.append(dataset)
dataset = pd.concat(list_, ignore_index=True)
</code></pre>
","python, pandas, nltk, sentiment-analysis, stop-words","<p>I think the code should work with information so far. The assumption I am making is with data has extra space while separated with comma. Below is the test ran: (<em>hope it helps!</em>) </p>

<pre><code>import pandas as pd
from nltk.corpus import stopwords
import nltk

stop = nltk.corpus.stopwords.words('english')

dataset = pd.DataFrame([{'Content':'i, am, the, computer, machine'}])
dataset = dataset.append({'Content':'i, play, game'}, ignore_index=True)
print(dataset)
list_ = []
for file_ in dataset:
    dataset['Content'] = dataset['Content'].apply(lambda x: [item.strip() for item in x.split(',') if item.strip() not in stop])
    list_.append(dataset)
dataset = pd.concat(list_, ignore_index=True)

print(dataset)
</code></pre>

<p>Input with stopwords:</p>

<pre><code>                          Content
0   i, am, the, computer, machine
1                   i, play, game
</code></pre>

<p>Output:</p>

<pre><code>                Content
 0  [computer, machine]
 1         [play, game]
</code></pre>
",1,1,1386,2017-06-26 00:58:43,https://stackoverflow.com/questions/44751922/unable-to-remove-english-stopwords-from-a-dataframe
Scikit-Learn - No True Positives - Best Way to Normalize Data,"<p>Thanks for taking the time to read my question!</p>

<p>So I am running an experiment to see if I can predict whether an individual has been diagnosed with depression (or at least says they have been) based on the words (or tokens)they use in their tweets. I found 139 users that at some point tweeted ""I have been diagnosed with depression"" or some variant of this phrase in an earnest context (.e. not joking or sarcastic. Human beings that were native speakers in the language of the tweet  were used to discern whether the tweet being made was genuine or not).</p>

<p>I then collected the entire public timeline of tweets of all of these users' tweets, giving me a ""depressed user tweet corpus"" of about 17000 tweets.</p>

<p>Next I created a database of about 4000 random ""control"" users, and with their timelines created a ""control tweet corpus"" of about 800,000 tweets.</p>

<p>Then I combined them both into a big dataframe,which looks like this:</p>

<pre><code>,class,tweet
0,depressed,tweet text .. *
1,depressed,tweet text.
2,depressed,@ tweet text
3,depressed,저 tweet text
4,depressed,@ tweet text😚
5,depressed,@ tweet text😍
6,depressed,@ tweet text ?
7,depressed,@ tweet text ?
8,depressed,tweet text *
9,depressed,@ tweet text ?
10,depressed,@ tweet text
11,depressed,tweet text *
12,depressed,#tweet text
13,depressed,
14,depressed,tweet text !
15,depressed,tweet text
16,depressed,tweet text. .
17,depressed,tweet text
...
50595,control,@tweet text?
150596,control,""@ tweet text.""
150597,control,@ tweet text.
150598,control,""@ tweet text. *""
150599,control,""@tweet text?""t
150600,control,""@ tweet text?""
150601,control,@ tweet text?
150602,control,@ tweet text.
150603,control,@tweet text~
150604,control,@ tweet text.
</code></pre>

<p>Then I trained a multinomial naive bayes classifier using an object from the CountVectorizer() class imported from the sklearn library:</p>

<pre><code>count_vectorizer = CountVectorizer()
counts = count_vectorizer.fit_transform(tweet_corpus['tweet'].values)

classifier = MultinomialNB()
targets = tweet_corpus['class'].values
classifier.fit(counts, targets)
MultinomialNB(alpha=1.0, class_prior=None, fit_prior= True)
</code></pre>

<p>Unfortunately, after running a 6-fold cross validation test, the results suck and I am trying to figure out why.</p>

<pre><code>Total tweets classified: 613952
Score: 0.0
Confusion matrix:
[[596070    743]
 [ 17139      0]]
</code></pre>

<p>So, I didn't properly predict a single depressed person's tweet! My initial thought is that I have not properly normalized the counts of the control group, and therefore even tokens which appear more frequently among the depressed user corpus are over represented in the control tweet corpus due to its much larger size. I was under the impression that .fit() did this already, so maybe I am on the wrong track here? If not, any suggestions on the most efficient way to normalize the data between two groups of disparate size?</p>
","python-3.x, machine-learning, scikit-learn, nlp, sentiment-analysis","<p>You should use a re-sampling techniques to deal with unbalanced classes. There are many ways to do that ""by hand"" in Python, but I recommend <a href=""https://github.com/scikit-learn-contrib/imbalanced-learn"" rel=""nofollow noreferrer"">unbalanced learn</a> which compiles re-sampling techniques commonly used in datasets showing strong between-class imbalance.</p>

<p>If you are using Anaconda, you can use:</p>

<pre><code>conda install -c glemaitre imbalanced-learn.
</code></pre>

<p>or simply:</p>

<pre><code>pip install -U imbalanced-learn
</code></pre>

<p>This library is compteible with sci-kit learn. Your dataset looks very interesting, is it public? Hope this helps.</p>
",0,0,155,2017-07-02 14:12:21,https://stackoverflow.com/questions/44871703/scikit-learn-no-true-positives-best-way-to-normalize-data
remove emoticons in R using tm package,"<p>I'm using the tm package to clean up a Twitter Corpus. However, the package is unable to clean up emoticons. </p>

<p>Here's a replicated code:</p>

<pre><code>July4th_clean &lt;- tm_map(July4th_clean, content_transformer(tolower))
Error in FUN(content(x), ...) : invalid input 'RT ElleJohnson Love of country is encircling the globes ������������������ july4thweekend July4th FourthOfJuly IndependenceDay NotAvailableOnIn' in 'utf8towcs'
</code></pre>

<p>Can someone point me in the right direction to remove the emoticons using the tm package?</p>

<p>Thank you,</p>

<p>Luis</p>
","r, sentiment-analysis, tm, emoticons","<p>You can use <code>gsub</code> to get rid of all non-ASCII characters.</p>

<pre><code>Texts = c(""Let the stormy clouds chase, everyone from the place ☁  ♪ ♬"",
    ""See you soon brother ☮ "",
    ""A boring old-fashioned message"" ) 

gsub(""[^\x01-\x7F]"", """", Texts)
[1] ""Let the stormy clouds chase, everyone from the place    ""
[2] ""See you soon brother  ""                                  
[3] ""A boring old-fashioned message""
</code></pre>

<p><B>Details:</B>
You can specify character classes in regex's with <code>[ ]</code>.  When the class description starts with <code>^</code> it means everything <em>except</em> these characters. Here, I have specified everything except characters 1-127, i.e. everything except standard ASCII and I have specified that they should be replaced with the empty string. </p>
",8,4,13656,2017-07-03 20:19:07,https://stackoverflow.com/questions/44893354/remove-emoticons-in-r-using-tm-package
How to solve &#39;numpy.float64&#39; object has no attribute &#39;encode&#39; in python 3,"<p>I am  trying to do a sentiment analysis in twitter about different car brands,i am using python 3 for this.While running the code i am getting the below exception </p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\Jeet Chatterjee\NLP\Maruti_Toyota_Marcedes_Brand_analysis.py"", line 55, in &lt;module&gt;
x = str(x.encode('utf-8','ignore'),errors ='ignore')
AttributeError: 'numpy.float64' object has no attribute 'encode'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""C:\Users\Jeet Chatterjee\NLP\Maruti_Toyota_Marcedes_Brand_analysis.py"", line 62, in &lt;module&gt;
tweets.set_value(idx,column,'')
  File ""C:\Program Files (x86)\Python36-32\lib\site-packages\pandas\core\frame.py"", line 1856, in set_value
engine.set_value(series._values, index, value)
 File ""pandas\_libs\index.pyx"", line 116, in pandas._libs.index.IndexEngine.set_value (pandas\_libs\index.c:4690)
File ""pandas\_libs\index.pyx"", line 130, in pandas._libs.index.IndexEngine.set_value (pandas\_libs\index.c:4578)
File ""pandas\_libs\src\util.pxd"", line 101, in util.set_value_at (pandas\_libs\index.c:21043)
  File ""pandas\_libs\src\util.pxd"", line 93, in util.set_value_at_unsafe (pandas\_libs\index.c:20964)
 ValueError: could not convert string to float: 
</code></pre>

<p>I don't how to represent the encode in python 3 . And here is my code </p>

<pre><code>from tweepy.streaming import StreamListener
from tweepy import OAuthHandler
from tweepy import Stream
from textblob import TextBlob
import json
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
#regular expression in python
import re

#data corpus
tweets_data_path = 'carData.txt'
tweets_data = []
tweets_file = open(tweets_data_path, ""r"")
for line in tweets_file:
    try:
        tweet = json.loads(line)
        tweets_data.append(tweet)
    except:
        continue
#creating panda dataset        
tweets = pd.DataFrame()
index = 0
    for num, line in enumerate(tweets_data):
  try:

     print (num,line['text'])

     tweets.loc[index,'text'] = line['text']
     index = index + 1 
  except:
         print(num, ""line not parsed"")
         continue

   def brand_in_tweet(brand, tweet):
       brand = brand.lower()
       tweet = tweet.lower()
       match = re.search(brand, tweet)
       if match:
        print ('Match Found')
        return brand
    else:
        print ('Match not found')
        return 'none'
for index, row in tweets.iterrows():
temp = TextBlob(row['text'])
tweets.loc[index,'sentscore'] = temp.sentiment.polarity

  for column in tweets.columns:
  for idx in tweets[column].index:
    x = tweets.get_value(idx,column)
    try:
        x = str(x.encode('utf-8','ignore'),errors ='ignore')          
        if type(x) == unicode:
            str(str(x),errors='ignore')
        else: 
            df.set_value(idx,column,x)
    except Exception:
        print ('encoding error: {0} {1}'.format(idx,column))
        tweets.set_value(idx,column,'')
        continue
tweets.to_csv('tweets_export.csv')

if __name__=='__main__':

  brand_in_tweet()
</code></pre>

<p>I have posted the full code , i am not getting any clue about this error , that how to solve this . Please help and thanks in advance .</p>
","python, numpy, tweepy, sentiment-analysis","<p>There is a problem in this line:</p>

<pre><code> x = str(x.encode('utf-8','ignore'),errors ='ignore')  
</code></pre>

<p><code>x</code> is a <code>numpy.float64</code>. The code is trying to first encode it as utf8, then convert it to a string. But that is the wrong way around, because only strings can be encoded. First convert it to a string, then encode the string:</p>

<pre><code> x = str(x).encode('utf-8','ignore')
</code></pre>
",4,2,21672,2017-07-04 05:53:36,https://stackoverflow.com/questions/44897882/how-to-solve-numpy-float64-object-has-no-attribute-encode-in-python-3
Unpack Dictionaries for Logistic Regression in Python,"<p>I'm trying to run some sentiment analysis on product reviews, and I'm getting tripped up with getting my model to read the word count dictionaries</p>

<pre><code>import pandas as pd  
import numpy as np   
from sklearn import linear_model, model_selection, metrics

products = pd.read_csv('data.csv')

def count_words(s):
   d = {}
   wl = str(s).split()
   for w in wl:
       d[w] = wl.count(w)
   return d

products['word_count'] = products['review'].apply(count_words)

products = products[products['rating'] != 3]
products['sentiment'] = (products['rating'] &gt;= 4) * 1 

train_data, test_data = model_selection.train_test_split(products, test_size = 0.2, random_state=0)

sentiment_model = linear_model.LogisticRegression()
sentiment_model.fit(X = train_data['word_count'], y =train_data['sentiment'])
</code></pre>

<p>When I run that last line I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-51-0c3f47af3a6e&gt; in &lt;module&gt;()
----&gt; 1 sentiment_model.fit(X = train_data['word_count'], y = 
train_data['sentiment'])

C:\ProgramData\anaconda_3\lib\site-packages\sklearn\linear_model\logistic.py 
in fit(self, X, y, sample_weight)
   1171 
   1172         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,
-&gt; 1173                          order=""C"")
   1174         check_classification_targets(y)
   1175         self.classes_ = np.unique(y)

C:\ProgramData\anaconda_3\lib\site-packages\sklearn\utils\validation.py in 
check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
    519     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,
    520                     ensure_2d, allow_nd, ensure_min_samples,
--&gt; 521                     ensure_min_features, warn_on_dtype, estimator)
    522     if multi_output:
    523         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,

C:\ProgramData\anaconda_3\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
    380                                       force_all_finite)
    381     else:
--&gt; 382         array = np.array(array, dtype=dtype, order=order, copy=copy)
    383 
    384         if ensure_2d:

TypeError: float() argument must be a string or a number, not 'dict'
</code></pre>

<p>It seems like the model is pulling the dictionaries as the x variables instead of the entries in the dictionaries. I think I need to unpack the dictionaries into arrays (?) but haven't had any luck doing so.</p>

<p>update:
Here is that products looks like after running word_count and defining sentiment
<a href=""https://i.sstatic.net/EwPbn.jpg"" rel=""nofollow noreferrer"">products.head()</a></p>
","python, dictionary, logistic-regression, sentiment-analysis","<p>If you want to just correct the error, first use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer"" rel=""nofollow noreferrer"">DictVectorizer</a> on the <code>train_data['word_count']</code> to convert it into acceptable format which is an array of shape <code>[n_samples, n_features]</code>. </p>

<p>Add the below to your code before <code>sentiment_model.fit()</code>:</p>

<pre><code>from sklearn.feature_extraction import DictVectorizer
dictVectorizer = DictVectorizer()

train_data_dict = dictVectorizer.fit_transform(train_data['word_count'])
</code></pre>

<p>Then call sentiment_model.fit() like this:</p>

<pre><code>sentiment_model.fit(X = train_data_dict, y =train_data['sentiment'])
</code></pre>

<p><strong>Note</strong>:-
And instead of implementing your own count words method, I would recommend you to use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorizer</a>.</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer

countVec = CountVectorizer()

train_data_vectorizer = countVec.fit_transform(train_data['review'])
sentiment_model.fit(X = train_data_vectorizer, y =train_data['sentiment'])
</code></pre>
",0,0,1123,2017-07-12 15:12:23,https://stackoverflow.com/questions/45061573/unpack-dictionaries-for-logistic-regression-in-python
Is Vader SentimentIntensityAnalyzer Multilingual?,"<p>I'm stuck in sentiment analysis and I found Vader solution which is the best I could find so far. My issue is that I don't find any doc on how to feed it with languages other than English. </p>
","python, machine-learning, sentiment-analysis, vader","<p>The short answer is ""no"". </p>

<p>The README file on the github page states</p>

<blockquote>
  <p>if you have access to the Internet, the demo has an example of how VADER can work with analyzing sentiment of texts in other languages (non-English text sentences).</p>
</blockquote>

<p>but if you take a look at what is actually done for this demonstration (beginning at line 552 in the <a href=""https://github.com/cjhutto/vaderSentiment/blob/51c36d2f169f4590f0f827e2f9fb279f5aeb7114/vaderSentiment/vaderSentiment.py"" rel=""noreferrer"">current version of vaderSentiment.py</a>), this is based entirely on using a machine-translation web service to automatically translate the text into English. As such, the results are reliant not only upon the accuracy of the sentiment analysis tool but also upon the accuracy of whichever translation tool you use to create the English version of the input. </p>

<p>Vader only performs sentiment analysis on English texts, but that workaround (automatic translation) may be a viable option. Sentiment analysis is less sensitive to common machine translation problems than other usages*, but you'll certainly still have to keep the limitations in mind if you choose to use that workaround.</p>

<p>*To give an example, the service used in the demo translates ""Das Internet funktioniert heute nicht. Ist eine Störung bekannt?"" to ""The Internet was not working today. Is a disorder known?"", which would be more accurately translated as ""The internet isn't working today. Is a disruption known?"". It's got the tense wrong in the first sentence, and while there are several legitimate translations of ""Störung"" in this context, ""disorder"" is an awkward choice at best. Nevertheless, while this makes it quite a bad translation in general, the errors are unlikely to affect a sentiment analysis significantly.</p>
",15,7,17542,2017-07-24 08:01:31,https://stackoverflow.com/questions/45275166/is-vader-sentimentintensityanalyzer-multilingual
Is there a way to improve performance of nltk.sentiment.vader Sentiment analyser?,"<p>My text is derived from a social network, so you can imagine it's nature, I think  text is clean and minimal as far as I could imagine; after performing following sanitization:</p>

<ul>
<li>no urls, no usernames</li>
<li>no punctuation, no accents</li>
<li>no numbers</li>
<li>no stopwords (I think vader does this anyway)</li>
</ul>

<p>I think run time is linear, and I don’t intend to do any parallelization because of the amount of effort needed to change available code,
For a example, for around 1000 texts ranging from ~50 kb to ~150 kb  bytes, it takes around </p>

<p>and the running time is around 10 minutes on my machine.</p>

<p>Is there a better way in feeding the algorithm to speed up cooking time?
The code is as simple as SentimentIntensityAnalyzer is intended to work, here is the main part</p>

<pre><code>sid = SentimentIntensityAnalyzer()

c.execute(""select body, creation_date, group_id from posts where (substring(lower(body) from (%s))=(%s)) and language=\'en\' order by creation _ date DESC (s,s,)"")
conn.commit()
if(c.rowcount&gt;0):
                dump_fetched = c.fetchall()

textsSql=pd.DataFrame(dump_fetched,columns=['body','created_at', 'group_id'])
del dump_fetched
gc.collect()
texts = textsSql['body'].values
# here, some data manipulation: steps listed above
polarity_ = [sid.polarity_scores(s)['compound'] for s in texts]
</code></pre>
","python, performance, data-manipulation, sentiment-analysis, vader","<p>/1. You need not remove the stopwords, nltk+vader already does that.</p>

<p>/2. You need not remove the punctuation, as that affects vader's polarity calculations too, apart from the processing overhead. So, go ahead with the punctuation.</p>

<pre><code>    &gt;&gt;&gt; txt = ""this is superb!""
    &gt;&gt;&gt; s.polarity_scores(txt)
    {'neg': 0.0, 'neu': 0.313, 'pos': 0.687, 'compound': 0.6588}
    &gt;&gt;&gt; txt = ""this is superb""
    &gt;&gt;&gt; s.polarity_scores(txt)
    {'neg': 0.0, 'neu': 0.328, 'pos': 0.672, 'compound': 0.6249}
</code></pre>

<p>/3.You shall introduce sentence tokenization too, as it would improve the accuracy, and then calculate average polarity for a paragraph based on  the sentences.Example here : <a href=""https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vaderSentiment.py#L517"" rel=""noreferrer"">https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vaderSentiment.py#L517</a></p>

<p>/4. The polarity calculations are completely independent of each other, and can use a <a href=""https://docs.python.org/2/library/multiprocessing.html#using-a-pool-of-workers"" rel=""noreferrer"">multiprocessing pool</a> for a small size, say 10, to provide  good boost in speed.</p>

<p><code>polarity_ = [sid.polarity_scores(s)['compound'] for s in texts]</code></p>
",8,5,5974,2017-07-25 07:41:21,https://stackoverflow.com/questions/45296897/is-there-a-way-to-improve-performance-of-nltk-sentiment-vader-sentiment-analyser
sklearn pipeline is not working,"<p>I am new to sklearn pipeline and studying about it from sklearn documentation. I used it in sentiment analysis on <a href=""http://www.cs.cornell.edu/people/pabo/movie-review-data/"" rel=""nofollow noreferrer"">movie review</a> data. Data contains two columns, first <code>class</code> and second <code>text</code>.</p>
<pre><code>input_file_df = pd.read_csv(&quot;movie-pang.csv&quot;)
x_train = input_file_df[&quot;text&quot;] #used complete data as train data
y_train = input_file_df[&quot;class&quot;]
</code></pre>
<p>I used only one feature, <code>sentiment score for each sentence.</code> I wrote custom transformer for this:</p>
<pre><code>class GetWorldLevelSentiment(BaseEstimator, TransformerMixin):

def __init__(self):
    pass

def get_word_level_sentiment(self, word_list):
    sentiment_score = 1
    for word in word_list:
        word_sentiment = swn.senti_synsets(word)

        if len(word_sentiment) &gt; 0:
            word_sentiment = word_sentiment[0]
        else:
            continue

        if word_sentiment.pos_score() &gt; word_sentiment.neg_score():
            word_sentiment_score = word_sentiment.pos_score()
        elif word_sentiment.pos_score() &lt; word_sentiment.neg_score():
            word_sentiment_score = word_sentiment.neg_score()*(-1)
        else:
            word_sentiment_score = word_sentiment.pos_score()

        print word, &quot; &quot; , word_sentiment_score
        if word_sentiment_score != 0:
            sentiment_score = sentiment_score * word_sentiment_score

    return sentiment_score

def transform(self, review_list, y=None):
    sentiment_score_list = list()
    for review in review_list:
        sentiment_score_list.append(self.get_word_level_sentiment(review.split()))
    
    return np.asarray(sentiment_score_list)

def fit(self, x, y=None):
    return self
</code></pre>
<p>Pipeline which I used is:</p>
<pre><code>pipeline = Pipeline([
(&quot;word_level_sentiment&quot;,GetWorldLevelSentiment()),
(&quot;clf&quot;, MultinomialNB())])
</code></pre>
<p>and then call fit on pipeline:</p>
<pre><code>pipeline.fit(x_train, y_train)
</code></pre>
<p>But this is giving following error to me:</p>
<p><code>This MultinomialNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.</code></p>
<p>Can someone please guide me what I am doing wrong here ?? It will be a great help.</p>
","python, scikit-learn, pipeline, sentiment-analysis","<p>This worked for me:</p>

<pre><code>class GetWorldLevelSentiment(BaseEstimator, TransformerMixin):

def __init__(self):
    pass

def get_word_level_sentiment(self, word_list):
    sentiment_score = 1
    for word in word_list:
        word_sentiment = swn.senti_synsets(word)

        if len(word_sentiment) &gt; 0:
            word_sentiment = word_sentiment[0]
        else:
            continue

        if word_sentiment.pos_score() &gt; word_sentiment.neg_score():
            word_sentiment_score = word_sentiment.pos_score()
        elif word_sentiment.pos_score() &lt; word_sentiment.neg_score():
            word_sentiment_score = word_sentiment.neg_score()*(-1)
        else:
            word_sentiment_score = word_sentiment.pos_score()

        print word, "" "" , word_sentiment_score
        if word_sentiment_score != 0:
            sentiment_score = sentiment_score * word_sentiment_score

    return sentiment_score

def transform(self, review_list, y=None):
    sentiment_score_list = list()
    for review in review_list:
        sentiment_score_list.append(self.get_word_level_sentiment(review.split()))

    return pandas.DataFrame(sentiment_score-list)

def fit(self, x, y=None):
    return self
</code></pre>
",1,0,4008,2017-07-26 11:15:03,https://stackoverflow.com/questions/45325336/sklearn-pipeline-is-not-working
How could I improve the accuracy of sentiment analysis of news headlines?,"<p>I'm using <a href=""https://github.com/cjhutto/vaderSentiment"" rel=""nofollow noreferrer"">Vader</a> and <a href=""https://textblob.readthedocs.io/en/dev/"" rel=""nofollow noreferrer"">TextBlob</a> to analyse the sentiment of news headlines with mixed results: many headlines I would consider slightly negative are scored as neutral. Here are a few examples:</p>

<pre><code>Who wants to live in an artificially intelligent future?
# Vader: {'compound': 0.4588, 'pos': 0.273, 'neu': 0.727, 'neg': 0.0}
# TextBlob: Sentiment(polarity=0.2840909090909091, subjectivity=0.40625)

The internet and social media provide huge opportunities for the coming generation, but there’s a dark side from which it must be protected.
# Vader: {'compound': 0.743, 'pos': 0.278, 'neu': 0.722, 'neg': 0.0}
# TextBlob: Sentiment(polarity=0.09444444444444448, subjectivity=0.45555555555555555)

For three months I’ve lived without tech and now realise we need to question its ever-encroaching invasion – before we end up in bed with a sex robot.
# Vader {'compound': 0.0, 'pos': 0.0, 'neu': 1.0, 'neg': 0.0}
# TextBlob Sentiment(polarity=0.0, subjectivity=0.0)
</code></pre>

<p>I think the first sentence could be read either way, but the second two definitely have negative elements to them: ""there’s a dark side"" and ""its ever-encroaching invasion"", so I'm surprised to see Vader give both a negative sore of 0 and TextBlob to give a polarity of 0 or above.</p>

<p>Are these kind of texts just fundamentally difficult for sentiment analysis algorithms, or is there another approach I could consider?</p>

<p>The attraction of the libraries I mentioned is that I don't have to make my own classification dataset, but I might consider it if I was likely to get better results.</p>
","python, sentiment-analysis, textblob, vader","<p>The basic difference is that most current tools work on a sentiment index of individual words.  For instance, finding ""like"" or ""excellent"" anywhere in the text will signal a positive evaluation.  Your examples depends more on some ""understanding"" of the phrases, requiring minimal parsing.  That's a more detailed process, requiring a deeper understanding of the language semantics.</p>

<p>One way you <em>could</em> attack this is to fill the lexicon with indexed phrases (inserted as words) as well as words.  Then you pre-process the input to convert those phrases to whatever indication you've used in the lexicon.  For instance, join those phrases with underscores -- and ""dark_side"" is in your lexicon with a negative index.</p>

<p>I'm hopeful this gives you a nudge in a useful direction.</p>
",1,0,2258,2017-08-25 21:04:20,https://stackoverflow.com/questions/45889395/how-could-i-improve-the-accuracy-of-sentiment-analysis-of-news-headlines
What is the proper way to deal with (score) dispersion in sentiment analysis on different topics in relation,"<p>I'm analyzing sentiment on a social network. Based on different <strong>topics in relation</strong> as an input. How can we deal with dispersion of individual topics scores?</p>

<p>For example: we are trying to score sentiment on a theme which is an event that includes different keywords, let's say the <em>theme is Innovation week</em> with the following topics (keywords or synonyms): </p>

<p><code>Innovation week = {""innovation week"", ""data solution"", ""emerging technologies"", ""august 30""...}.</code> </p>

<p>What if standard deviation of scores is so big. 
Do we question:</p>

<ul>
<li><p>The sentiment analysis algorithm itself?</p></li>
<li><p>Our input keywords?</p></li>
<li><p>Or we just take results as are? as they represent different views of people on different levels of granularity constituting a theme? The purpose finally is to have a general insight on a theme.</p></li>
</ul>

<p>I think the question is simple although this is a concern of any sentiment analysis study in social networks.</p>
",sentiment-analysis,"<p>The short answer is both the algorithm and the input keywords as they are dependent on each other.
Given the right input the dispersion would increse in any algorithm and given the wrong algorithm the same will happen for any input.</p>

<p>Usually in this cases you should revise the algorithm as this is the case in most situations.</p>

<p>You can also read this in order to understand it better:
<a href=""http://www.cs.cornell.edu/home/llee/omsa/omsa-published.pdf"" rel=""nofollow noreferrer"">http://www.cs.cornell.edu/home/llee/omsa/omsa-published.pdf</a></p>
",1,4,168,2017-08-27 11:12:25,https://stackoverflow.com/questions/45904323/what-is-the-proper-way-to-deal-with-score-dispersion-in-sentiment-analysis-on
How can I apply a lexicon to a list of sentences?,"<p>I have a lexicon dictionary in this shape </p>

<pre><code>6   ابن جزمه    1
7   ابو جهل -1
8   اتق الله    -1
9   اتقو الله   1
</code></pre>

<p>I want to create a new list containing the score of each sentence based on the lexicon adding the score of each word and if no words exist append zero 
when I implement my code I get <code>len(lex_score) = 3679</code> after I add elif condition I get <code>len(lex_score) = 95079</code></p>

<p>the len(lex_score) should equal 6064</p>

<pre><code>lex_score = []
def lexic(text):
    for tweet in sentences:
        score = 0
        for word in tweet.split():
            if word in lexicon:
                score = score+lexicon[word]
            elif word not in lexicon:
                score = 0
                lex_score.append(score)
</code></pre>

<p>I want to create a new column in the data frame containing the score of each sentence. what am I doing wrong?
and is there a better way to do so ? </p>
","python, nlp, sentiment-analysis, lexicon","<p>IIUC, you can just sum the scores of valid lexicon entries in each tweet, and then append that score to <code>lex_score</code> on each iteration of <code>sentences</code>.  </p>

<p>Note: I'm assuming <code>text == sentences</code> - otherwise there's a missing line where <code>text</code> is broken down into <code>sentences</code>.  Either way, this basic approach should still work:</p>

<pre><code>def lexic(text):
    lex_score = []
    for tweet in text: # assuming sentences == text
        score = sum([lexicon[word] for word in tweet.split() if word in lexicon])
        lex_score.append(score)
    return lex_score
</code></pre>
",1,0,930,2017-09-03 20:38:19,https://stackoverflow.com/questions/46027636/how-can-i-apply-a-lexicon-to-a-list-of-sentences
Using Text Sentiment as feature in machine learning model?,"<p>I am researching what features I'll have for my machine learning model, with the data I have. My data contains a lot of textdata, so I was wondering how to extract valuable features from it. Contrary to my previous belief, this often consists of representation with Bag-of-words, or something like word2vec: (<a href=""http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction</a>) </p>

<p>Because my understanding of the subject is limited, I dont understand why I can't analyze the text first to get numeric values. (for example: textBlob.sentiment =<a href=""https://textblob.readthedocs.io/en/dev/"" rel=""nofollow noreferrer"">https://textblob.readthedocs.io/en/dev/</a>, google Clouds Natural Language =<a href=""https://cloud.google.com/natural-language/"" rel=""nofollow noreferrer"">https://cloud.google.com/natural-language/</a>)</p>

<p>Are there problems with this, or could I use these values as features for my machine learning model?</p>

<p>Thanks in advance for all the help!</p>
","machine-learning, sentiment-analysis, data-science, text-analysis","<p>Of course, you can convert text input single number with sentiment analysis then use this number as a feature in your machine learning model. Nothing wrong with this approach.</p>

<p>The question is what kind of information you want to extract from text data. Because sentiment analysis convert text input to a number between -1 to 1 and the number represents how positive or negative the text is. For example, you may want sentiment information of the customers' comments about a restaurant to measure their satisfaction. In this case, it is fine to use sentiment analysis to preprocess text data.</p>

<p>But again, sentiment analysis is only given an idea about how positive or negative text is. You may want to cluster text data and sentiment information is not useful in this case since it does not provide any information about the similarity of texts. Thus, other approaches such as word2vec or bag-of-words will be used for the representation of text data in those tasks. Because those algorithms provide vector representation of the text instance of a single number. </p>

<p>In conclusion, the approach depends on what kind of information you need to extract from data for your specific task.  </p>
",1,2,212,2017-09-16 11:28:10,https://stackoverflow.com/questions/46253404/using-text-sentiment-as-feature-in-machine-learning-model
Best Python machine learning library?,"<p>I'm looking for a Python library I can use to teach a program the difference between a positive, negative and neutral statement.</p>
<p>I don't really want to use NLP, I purely want to be able to tell it this is positive, this is negative and after enough test cases it will be able to decide itself.</p>
<p>So just wondering what you have used and how you found it. Code examples or GitHub links would be helpful.</p>
","python, machine-learning, artificial-intelligence, sentiment-analysis","<p>For best Machine Learning library in python, you can look into scikit-learn. As you want to do text processing, I have worked on NLTK and found it pretty good. Your project or problem at hand is sentiment analysis. So,there would be a combination of nltk and scikit-learn. For documentation of scikit-learn - <a href=""http://scikit-learn.org/stable/index.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/index.html</a>
For NLTK - <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">http://www.nltk.org/</a></p>
",3,-4,785,2017-09-19 03:47:06,https://stackoverflow.com/questions/46291462/best-python-machine-learning-library
"How to calculate Precision, Recall and F-score using python?","<p>The labelTrainData.csv is used to train the classifier for predicting sentiments of Testdata.csv. Finally i got BagOfCentroids.csv. </p>

<p><strong>labelTrainData.csv</strong></p>

<pre><code>id   sentiment    Tweet
1    0            tweet_1
2    1            tweet_2
3    0            tweet_3
</code></pre>

<p><strong>Testdata.csv</strong></p>

<pre><code>id      Tweet
1       tweet_1
2       tweet_2
3       tweet_3
</code></pre>

<p><strong>BagOfCentroids.csv</strong></p>

<pre><code>id      sentiment
1       0
2       1
3       1
</code></pre>

<p>To calculate above metrics, I am trying this,</p>

<pre><code>print 'Sentiment precision:'

nltk.metrics.precision(BagOfCentroids['sentiment'], Testdata['sentiment'])

print 'sentiment recall:'

nltk.metrics.recall(BagOfCentroids['sentiment'], Testdata['sentiment'])

print 'sentiment F-measure:'

nltk.metrics.f_measure(BagOfCentroids['sentiment'], Testdata['sentiment'])  
</code></pre>

<p>Is there any way to calculate Precision, Recall and F-score?</p>
","python, machine-learning, nlp, sentiment-analysis","<p>this article will be helpful:</p>

<p><a href=""https://streamhacker.com/2010/05/17/text-classification-sentiment-analysis-precision-recall/"" rel=""nofollow noreferrer"">TEXT CLASSIFICATION FOR SENTIMENT ANALYSIS – PRECISION AND RECALL</a></p>

<p><a href=""http://www.nltk.org/api/nltk.metrics.html#module-nltk.metrics.scores"" rel=""nofollow noreferrer"">Detail about nltk.metrics package</a></p>

<p>This might be because of some import issue</p>
",1,-3,2777,2017-09-25 10:34:32,https://stackoverflow.com/questions/46403103/how-to-calculate-precision-recall-and-f-score-using-python
how does the gsub() function help in replacing retweet entries in sentiment analysis in R,"<p>Tgus code shows how to remove Retweet option from tweets in case of sentiment analysis in R.    </p>

<pre><code>tweets &lt;- searchTwitter(""iPhone"", n=1500, lang=""en"")
txt &lt;- sapply(tweets, function(x) x$getText())
txt &lt;- gsub(""(RT|via)((?:\\b\\W*@\\w+)+)"", """", txt)
</code></pre>

<p>What I don't understand is that in the <code>gsub ()</code> function, where does the pattern</p>

<pre><code>(RT|via)((?:\\b\\W*@\\w+)+)
</code></pre>

<p>come from?</p>
","r, twitter, gsub, sentiment-analysis","<p>Let's break it down:</p>

<p><code>(RT|via)</code> - Match ""RT"" or ""via""</p>

<p>Everything else is a non-capturing group defined by <code>(?:)</code>, meaning we check that it exists, but we don't capture it.</p>

<p><code>(?:\\b\\W*@\\w+)+)</code></p>

<p><code>\\b</code> is a word boundary</p>

<p><code>\\W*</code> is a non-word character. <code>*</code> means match 0 or more</p>

<p>Match a <code>@</code></p>

<p><code>\\w+</code> and one or more word characters (letter, digit, connector)</p>

<p><code>+</code> outside of the non-capturing group means there can be more than one of these non-capturing groups.</p>

<p>Basically you're matching ""via"" or ""RT"" and removing it (via the """" blank you're replacing the captured text with), and matching but not capturing everything else that follows</p>

<p>The non-capturing group is used so you can match ""RT"" or ""via"" in varying positions in the string. <code>\\b\\W*@\\w+</code> is making sure you match a twitter username after the ""RT"" or ""via"". This should help avoid replacing ""RT"" or ""via"" when it's not used as an actual re-tweet.</p>
",0,0,686,2017-09-26 15:16:51,https://stackoverflow.com/questions/46430239/how-does-the-gsub-function-help-in-replacing-retweet-entries-in-sentiment-anal
Tensorflow lstm for sentiment analysis not learning. UPDATED,"<h1>UPDATED:</h1>
<p>i'm building a Neural Network for my final project and i need some help with it.</p>
<p>I'm trying to build a rnn to do sentiment analysis over Spanish text. I have about 200,000 labeled tweets and i vectorized them using a word2vec with a Spanish embedding</p>
<p><strong>Dataset &amp; Vectorization:</strong></p>
<ul>
<li>I erased duplicates and split the dataset into training and testing sets.</li>
<li>Padding, unknown and end of sentence tokens are applied when vectorizing.</li>
<li>I mapped the @mentions to known names in the word2vec model. Example: @iamthebest =&gt; &quot;John&quot;</li>
</ul>
<p><strong>My model:</strong></p>
<ul>
<li>My data tensor has shape = (batch_size, 20, 300).</li>
<li>I have 3 classes: neutral, positive and negative, so my target tensor has shape = (batch_size, 3)</li>
<li>I use BasicLstm cells and dynamic rnn to build the net.</li>
<li>I use Adam Optimizer, and softmax_cross entropy for the loss calculation</li>
<li>I use a dropout wrapper to decrease the overfitting.</li>
</ul>
<p><strong>Last run:</strong></p>
<ul>
<li>I have tried with different configurations and non of them seem to work.</li>
<li>Last setup:  2 Layers, 512 batch size, 15 epochs and 0.001 of lr.</li>
</ul>
<p><a href=""https://i.sstatic.net/Y786W.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Y786W.jpg"" alt=""Accuracy"" /></a></p>
<p><a href=""https://i.sstatic.net/5OGrS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5OGrS.jpg"" alt=""Loss"" /></a></p>
<p><strong>Weak points for me:</strong></p>
<p>im worried about the final layer and the handing of the final state in the dynamic_rnn</p>
<p><strong>Code:</strong></p>
<pre><code># set variables
num_epochs = 15
tweet_size = 20
hidden_size = 200
vec_size = 300
batch_size = 512
number_of_layers= 1
number_of_classes= 3
learning_rate = 0.001

TRAIN_DIR=&quot;/checkpoints&quot;

tf.reset_default_graph()

# Create a session 
session = tf.Session()

# Inputs placeholders
tweets = tf.placeholder(tf.float32, [None, tweet_size, vec_size], &quot;tweets&quot;)
labels = tf.placeholder(tf.float32, [None, number_of_classes], &quot;labels&quot;)

# Placeholder for dropout
keep_prob = tf.placeholder(tf.float32) 

# make the lstm cells, and wrap them in MultiRNNCell for multiple layers
def lstm_cell():
    cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)
    return tf.contrib.rnn.DropoutWrapper(cell=cell, output_keep_prob=keep_prob)

multi_lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(number_of_layers)], state_is_tuple=True)

# Creates a recurrent neural network
outputs, final_state = tf.nn.dynamic_rnn(multi_lstm_cells, tweets, dtype=tf.float32)

with tf.name_scope(&quot;final_layer&quot;):
    # weight and bias to shape the final layer
    W = tf.get_variable(&quot;weight_matrix&quot;, [hidden_size, number_of_classes], tf.float32, tf.random_normal_initializer(stddev=1.0 / math.sqrt(hidden_size)))
    b = tf.get_variable(&quot;bias&quot;, [number_of_classes], initializer=tf.constant_initializer(1.0))

    sentiments = tf.matmul(final_state[-1][-1], W) + b

prob = tf.nn.softmax(sentiments)
tf.summary.histogram('softmax', prob)

with tf.name_scope(&quot;loss&quot;):
    # define cross entropy loss function
    losses = tf.nn.softmax_cross_entropy_with_logits(logits=sentiments, labels=labels)
    loss = tf.reduce_mean(losses)
    tf.summary.scalar(&quot;loss&quot;, loss)

with tf.name_scope(&quot;accuracy&quot;):
    # round our actual probabilities to compute error
    accuracy = tf.to_float(tf.equal(tf.argmax(prob,1), tf.argmax(labels,1)))
    accuracy = tf.reduce_mean(tf.cast(accuracy, dtype=tf.float32))
    tf.summary.scalar(&quot;accuracy&quot;, accuracy)

# define our optimizer to minimize the loss
with tf.name_scope(&quot;train&quot;):
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)

#tensorboard summaries
merged_summary = tf.summary.merge_all()
logdir = &quot;tensorboard/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;) + &quot;/&quot;
writer = tf.summary.FileWriter(logdir, session.graph)

# initialize any variables
tf.global_variables_initializer().run(session=session)

# Create a saver for writing training checkpoints.
saver = tf.train.Saver()

# load our data and separate it into tweets and labels
train_tweets = np.load('data_es/train_vec_tweets.npy')
train_labels = np.load('data_es/train_vec_labels.npy')

test_tweets = np.load('data_es/test_vec_tweets.npy')
test_labels = np.load('data_es/test_vec_labels.npy')

**HERE I HAVE THE LOOP FOR TRAINING AND TESTING, I KNOW ITS FINE**
</code></pre>
","tensorflow, lstm, sentiment-analysis, recurrent-neural-network","<p>I have already solved my problem. After reading some papers and more trial and error, I figured out what my mistakes were.</p>

<p><strong>1) Dataset: I had a large dataset, but I didn't format it properly.</strong></p>

<ul>
<li>I checked the distribution of tweet labels (Neutral, Positive and Negative), realized there was a disparity in the distribution of said tweets and normalized it.</li>
<li>I cleaned it up even more by erasing url hashtags and unnecessary  punctuation.</li>
<li>I shuffled prior to vectorization.</li>
</ul>

<p><strong>2) Initialization:</strong></p>

<ul>
<li>I initialized the MultiRNNCell with zeros and I changed my custom final layer to tf.contrib.fully_connected. I also added the initialization of the bias and weight matrix. (By fixing this, I started to see better loss and accuracy plots in Tensorboard)</li>
</ul>

<p><strong>3) Dropout:</strong></p>

<ul>
<li>I read this paper, <a href=""https://arxiv.org/pdf/1603.05118.pdf"" rel=""nofollow noreferrer"">Recurrent Dropout without Memory Loss</a>, and I changed my dropouts accordingly; I started seeing improvements in the loss and accuracy.</li>
</ul>

<p><strong>4) Decaying the learning rate:</strong></p>

<ul>
<li>I added an exponential decaying rate after 10,000 steps to control over-fitting.</li>
</ul>

<p><strong><em>Final results:</em></strong></p>

<p>After applying all of these changes, I achieved a test accuracy of 84%, which is acceptable because my data set still sucks.</p>

<p>My final network config was:</p>

<ul>
<li>num_epochs = 20</li>
<li>tweet_size = 20</li>
<li>hidden_size = 400</li>
<li>vec_size = 300</li>
<li>batch_size = 512</li>
<li>number_of_layers= 2</li>
<li>number_of_classes= 3</li>
<li>start_learning_rate = 0.001</li>
</ul>
",1,3,1019,2017-10-05 01:00:50,https://stackoverflow.com/questions/46576332/tensorflow-lstm-for-sentiment-analysis-not-learning-updated
Stemmer function in R Slow,"<p>I am trying to run stemmer function on a dataset(uploaded through data.table package) in R of around 40000 rows,but its taking forever to run. My code looks like this:</p>

<pre><code>data[, Description := map(Description, function(k) stemmer(k))]
</code></pre>

<p>If manually stop the process, it shows more than 50 warnings as:
<a href=""https://i.sstatic.net/mYKsE.jpg"" rel=""nofollow noreferrer"">Image Link</a></p>

<p>Is there an alternative to run it faster. My computer has 8Gb ram.</p>
","r, machine-learning, nlp, sentiment-analysis, snowball","<p>Use <code>lapply</code>, your function will run faster</p>
",0,0,103,2017-10-10 06:38:23,https://stackoverflow.com/questions/46660102/stemmer-function-in-r-slow
How many tweets can be collected?,"<p>I'm a beginner at Twitter API. I'm going to do sentiment analysis for my paper. So I need to data(user's comments) from Twitter as much as possible.
However, I heard that Twitter's policy had changed last year. Its policy restrains that user collect a lot of data..</p>

<p>So I want to know how many tweets can be collected(ex. 5,000 data). Because if they restrain my access token, then I must change it. Also, I wonder it they restrict my access token or IP address.</p>

<p>I use Python with Anaconda, and program will conduct on workstation.</p>

<p>Thanks!</p>
","python, twitter, tweepy, sentiment-analysis","<p>If you are going to using <a href=""https://developer.twitter.com/en/docs/tweets/search/overview/basic-search.html"" rel=""nofollow noreferrer"">Search API</a>, it said that</p>

<p>The Twitter Search API searches against a sampling of recent Tweets published <strong>in the past 7 days</strong>.</p>

<p>I have used this to collect tweets several months ago, it allows me to collect about 3000 tweets per user.</p>

<p>It is recommended to using <a href=""https://developer.twitter.com/en/docs/tweets/filter-realtime/overview.html"" rel=""nofollow noreferrer"">Streaming API</a>.</p>
",1,-2,89,2017-10-20 04:37:56,https://stackoverflow.com/questions/46842447/how-many-tweets-can-be-collected
What are the best Pre-Processing techniques for Sentiment Analysis.?,"<p>I am trying to classify a dataset of reviews in to two classes say class A and class B. I am using <code>LightGBM</code> to classify.</p>

<p>I have changed the parameters for the classifier many times but I can't get a huge difference in the results.</p>

<p>I think the problem is with the pre-processing step. I defined a function as shown below to take care of pre-processing. I used <code>Stemming</code> and removed <code>stopwords</code>. I don't know what I am missing. I have tried <code>LancasterStemmer</code> and <code>PorterStemmer</code></p>

<pre><code>stops = set(stopwords.words(""english""))
def cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemm = False):
    txt = str(text)
    txt = re.sub(r'[^A-Za-z0-9\s]',r'',txt)
    txt = re.sub(r'\n',r' ',txt)

    if lowercase:
        txt = "" "".join([w.lower() for w in txt.split()])

    if remove_stops:
        txt = "" "".join([w for w in txt.split() if w not in stops])

    if stemming:
        st = PorterStemmer()
        txt = "" "".join([st.stem(w) for w in txt.split()])

    if lemm:
        wordnet_lemmatizer = WordNetLemmatizer()
        txt = "" "".join([wordnet_lemmatizer.lemmatize(w) for w in txt.split()])
    return txt
</code></pre>

<p><strong>Are there any more pre-processing steps to be done to get a better accuracy.?</strong></p>

<p>URL for the dataset : <a href=""https://he-s3.s3.amazonaws.com/media/hackathon/predict-the-happiness/predict-the-happiness/f2c2f440-8-dataset_he.zip"" rel=""nofollow noreferrer"">Dataset</a></p>

<p><strong>EDIT :</strong></p>

<p>Parameters that I used are as mentioned below.</p>

<pre><code>params = {'task': 'train',
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'learning_rate': 0.01, 
    'max_depth': 22, 
    'num_leaves': 78,
    'feature_fraction': 0.1, 
    'bagging_fraction': 0.4, 
    'bagging_freq': 1}
</code></pre>

<p>I have altered the <code>depth</code> and <code>num_leaves</code> parameters along with others. But the accuracy is kind of stuck at a certain level..</p>
","machine-learning, nlp, sentiment-analysis, lightgbm","<p>There are a few things to consider. First of all your training set is not balanced - the class distribution is ~ 70%/30%. You need to consider this fact in training. What types of features are you using? Using the right set of features could improve your performance.    </p>
",0,1,190,2017-10-31 16:48:36,https://stackoverflow.com/questions/47040505/what-are-the-best-pre-processing-techniques-for-sentiment-analysis
How to create a unigram and bigram count matrix for a text file along with a class variable into csv using Python?,"<p>I want to create a unigram and bigram count matrix for a text file along with a class variable into csv using Python
The text file contains two columns which look like this</p>

<pre><code>Text                                                  Class
I love the movie                                      Pos
I hate the movie                                      Neg
</code></pre>

<p>I want the unigram and bigram count for text column and the output should be written into csv file</p>

<pre><code>I     hate      love        movie   the        class
1     0         1           1       1          Pos
1     1         0           1       1          Neg
</code></pre>

<p>Bigram</p>

<pre><code>I love     love the     the movie     I hate    hate the         class
1            1              1         0          0               Pos
0            0              1         1          1               Neg
</code></pre>

<p>Anybody can help me to improve the below code into the above mentioned output format?</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; fo = open(""text.txt"")
&gt;&gt;&gt; fo1 = fo.readlines()
&gt;&gt;&gt; for line in fo1:
       bigm = list(nltk.bigrams(line.split()))
       bigmC = Counter(bigm)
       for key, value in bigmC.items():
           print(key, value)

('love', 'the') 1
('the', 'movie') 1
('I', 'love') 1
('I', 'hate') 1
('hate', 'the') 1
('the', 'movie') 1
</code></pre>
","python, nltk, sentiment-analysis","<p>I have made your input file a little more detailed just so you can believe that the solution works:</p>

<pre><code>I love the movie movie
I hate the movie
The movie was rubbish
The movie was fantastic
</code></pre>

<p>The first line contains a word twice cause otherwise you can't tell that the counter is actually counting properly.</p>

<p>The solution:</p>

<pre><code>import csv
import nltk
from collections import Counter
fo = open(""text.txt"")
fo1 = fo.readlines()
counter_sum = Counter()
for line in fo1:
       tokens = nltk.word_tokenize(line)
       bigrams = list(nltk.bigrams(line.split()))
       bigramsC = Counter(bigrams)
       tokensC = Counter(tokens)
       both_counters = bigramsC + tokensC
       counter_sum += both_counters
       # This basically collects the whole 'population' of words and bigrams in your document

# now that we have the population can write a csv

with open('unigrams_and_bigrams.csv', 'w', newline='') as csvfile:
    header = sorted(counter_sum, key=lambda x: str(type(x)))
    writer = csv.DictWriter(csvfile, fieldnames=header)
    writer.writeheader()
    for line in fo1:
          tokens = nltk.word_tokenize(line)
          bigrams = list(nltk.bigrams(line.split()))
          bigramsC = Counter(bigrams)
          tokensC = Counter(tokens)
          both_counters = bigramsC + tokensC
          cs = dict(counter_sum)
          bc = dict(both_counters)
          row = {}
          for element in list(cs):
                if element in list(bc):
                  row[element] = bc[element]
                else:
                  row[element] = 0
          writer.writerow(row)
</code></pre>

<p>So, I used and built on your initial approach. You did not say whether you wanted the bigrams and unigrams in seperate csv's so assumed you wanted them together. That would not be too hard for you to reprogram otherwise. To accumulate a population in this way is probably better done using tools already built into the NLP libraries, but interesting to see it can be done more low level. I'm using Python 3 by the way, you may need to change some things such as the use of <code>list</code> if you need to make it work in Python 2. </p>

<p>Some interesting references used were <a href=""https://stackoverflow.com/questions/19356055/summing-the-contents-of-two-collections-counter-objects"">this one on summing counters</a> which was new to me. Also, I had to <a href=""https://stackoverflow.com/questions/47160171/is-there-a-command-which-will-sort-a-python-list-by-data-type/"">ask a question</a> to get your bigrams and unigrams grouped at separate ends of the CSV. </p>

<p>I know the code looks repetitive, but you need to run through all your lines first to get the headers for the csv before you can start writing it. </p>

<p>Here is the output in libreoffice</p>

<p><a href=""https://i.sstatic.net/tO1mc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tO1mc.png"" alt=""image of csv output""></a></p>

<p>Your csv is going to get very wide as it collects all the unigrams and bigrams. If you really care to have the bigrams without brackets and commas in the headers, you can make some kind of function which will do that. It is probably better to leave them as tuples though in case you need to parse them into Python again at some point, and it's just as readable..</p>

<p>You didn't include the code which generated the class column, assume you have it, you can append the string 'Class' onto header before the header gets written to csv to create that column and to populate it, </p>

<pre><code>row['Class'] = sentiment
</code></pre>

<p>on the second last line before the row gets written.</p>
",3,0,3189,2017-11-07 13:22:41,https://stackoverflow.com/questions/47159083/how-to-create-a-unigram-and-bigram-count-matrix-for-a-text-file-along-with-a-cla
Sentiment Analysis with Imbalanced Dataset in LightGBM,"<p>I am trying to perform sentiment analysis on a dataset of 2 classes (Binary Classification). Dataset is heavily imbalanced about <code>70% - 30%</code>. I am using <code>LightGBM</code> and <code>Python 3.6</code> for making the model and predicting the output.</p>

<p>I think imbalance in dataset effect performance of my model. I get about <code>90%</code> accuracy but it doesn't increase further even though I have performed fine-tuning of the parameters. I don't think this the maximum possible accuracy as there are others who scored better than this.</p>

<p>I have cleaned the dataset with <code>Textacy</code> and <code>nltk</code>. I am using <code>CountVectorizer</code> for encoding the text. </p>

<p>I have tried <code>up-sampling</code> the dataset but it resulted in poor model (I haven't tuned that model) </p>

<p>I have tried using the <code>is_unbalance</code> parameter of <code>LightGBM</code>, but it doesn't give me a better model.</p>

<p>Are there any approaches to follow to handle this type of datasets that are so imbalanced.? How can I further improve my model.? Should I try down-sampling.? Or is it the maximum possible accuracy.? How can I be sure of it.?</p>
","python-3.x, machine-learning, nlp, sentiment-analysis, lightgbm","<blockquote>
  <p>Are there any approaches to follow to handle this type of datasets
  that are so imbalanced.?</p>
</blockquote>

<p><strong>Your dataset is almost balanced</strong>. <code>70/30</code> is close to equal. With gratient boosted trees it is possible to train on much more unbalanced data, like credit scoring, fraud detection, and medical diagnostics, where percentage of positives may be less that 1%. </p>

<p>Your problem might be not in class imbalance, but in the <strong>wrong metric</strong> you use. When you calculate accuracy, you implicitly penalize your model equally for false negatives and false positives. But is it really the case? When classes are imbalanced, or just uncomparable from the business or physical point of view, other metrics, like precision, recall, or ROC AUC might be of more use than accuracy. For your problem I would recommend ROC AUC.</p>

<p>Maybe, what you really want is probabilistic classification. And if you want to keep it binary, play with the threshold used for the classification.</p>

<blockquote>
  <p>How can I further improve my model.?</p>
</blockquote>

<p>Because it is analysis of text, I would suggest more accurate data cleaning. Some directions to start with:</p>

<ul>
<li>Did you try different regimes of lemmatization/stemming?</li>
<li>How did you preprocess special entities, like numbers, smileys, abbreviations, company names, etc.?</li>
<li>Did you exploit collocations, by including bigrams or even trigrams into your model along with words?</li>
<li>How did you handle negation? One single ""no"" could change the meaning dramatically, and <code>CountVectorizer</code> catches that poorly.</li>
<li>Did you try to extract semantics from the words, e.g. match the synonyms or use word embeddins from a pretrained model like word2vec or fastText?</li>
</ul>

<p>Maybe tree-based models is not the best choice. In my own experience, best sentiment analysis was performed by linear models like logistic regression or a shallow neural network. But you should heavily regularize them, and you should scale your features wisely, e.g. with TF-IDF.</p>

<p>And if your dataset is large, you can try deep learning and train a RNN on your data. LSTM is often the best model for many text-related problems.</p>

<blockquote>
  <p>Should I try down-sampling.?</p>
</blockquote>

<p>No, you should <strong>never down-sample</strong>, unless you have too much data to process on your machine. Down-sampling creates biases in your data.</p>

<p>If you really really want to increase the relative importance of the minority class for your classifier, you can just <strong>reweight the observations</strong>. As far as I know, in <code>LightGBM</code> you can change class weights with the <code>scale_pos_weight</code> parameter.</p>

<blockquote>
  <p>Or is it the maximum possible accuracy.? How can I be sure of it.?</p>
</blockquote>

<p>You can never know. But you can do an experiment: ask several humans to label your test samples, and compare them with each other. If only 90% of labels coincide, then even humans cannot relaibly classify the rest 10% of samples, so you have reached the maximum. </p>

<p>And again, don't focus on accuracy too much. Maybe, for your business application it is okay if you incorrectly label some positive reviews as negative, as long as all the negative reviews are successfully identified.</p>
",6,3,4532,2017-11-08 19:09:00,https://stackoverflow.com/questions/47187750/sentiment-analysis-with-imbalanced-dataset-in-lightgbm
"Stanford NLP NER, Sentiment, SUTime Performance Issue","<p>The text in the main method seem to be taking more than 2 seconds to return NER. I am not an expert in NLP and this code is not at all scalable. I have added comments in 2 places where the bottleneck i have identified. Can you please suggest improvements to improve the performance of the program. </p>

<p>Thanks.</p>

<pre><code> public class NERSentimentUtil
{
private static final Logger logger = Logger.getLogger(NERSentimentUtil.class);

private static final String serializedClassifier7 = ""edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz"";
private static final String serializedClassifier4 = ""edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz"";
private static final String serializedClassifier3 = ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"";

private static NERClassifierCombiner ncc;
private static StanfordCoreNLP pipeline;

static
{       
    try
    {
        ncc = new NERClassifierCombiner(serializedClassifier3,serializedClassifier4,serializedClassifier7);
    } catch (IOException e) {
        e.printStackTrace();
        logger.error(e);
    }
}

static
{               
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment, sutime"");
    /*props.setProperty(""ner.useSUTime"", ""0"");*/

    String defs_sutime = ""/edu/stanford/nlp/models/sutime/defs.sutime.txt"";
    String holiday_sutime = ""/edu/stanford/nlp/models/sutime/english.holidays.sutime.txt"";
    String _sutime = ""/edu/stanford/nlp/models/sutime/english.sutime.txt"";

    String sutimeRules = defs_sutime + "","" + holiday_sutime + "","" + _sutime;
    props.setProperty(""ner.useSUTime"", ""true"");
    props.setProperty(""-sutime.rules"", sutimeRules);
    props.setProperty(""sutime.binders"", ""0"");
    props.setProperty(""sutime.markTimeRanges"", ""false"");
    props.setProperty(""sutime.includeRange"", ""false"");
    props.setProperty(""customAnnotatorClass.sutime"", ""edu.stanford.nlp.time.TimeAnnotator"");
    props.setProperty(""parse.maxlen"", ""20"");
    //props.setProperty(""ner.applyNumericClassifiers"", ""false"");
    //props.setProperty(""nthreads"", ""16"");
    //props.setProperty(""threads"", ""16"");
    //props.setProperty(""parse.nthreads"",""16"");
    //props.setProperty(""ssplit.eolonly"",""true"");

    props.setProperty(""-parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
    RedwoodConfiguration.current().clear().apply();
    pipeline = new StanfordCoreNLP(props);
    //RedwoodConfiguration.empty().capture(System.err).apply();
}

//A sentiment score of 0 or 1 is negative, 2 neutral and 3 or 4 positive.
private static int getScore(int score)
{
    if(score&lt;2)
        return -1;
    else if(score==2)
        return 0;
    else
        return 1;       
}

public static HashMap&lt;String,Object&gt; getStanford(String s, long dateString)//""2013-07-14""
{   
    int finalScore =0;

    HashMap&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();

    HashMap&lt;String, Integer&gt; dateMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; dateCountMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, String&gt; dateSentenceMap = new HashMap&lt;String, String&gt;();

    HashMap&lt;String, Integer&gt; personMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; personCountMap = new HashMap&lt;String, Integer&gt;();

    HashMap&lt;String, Integer&gt; orgMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; orgCountMap = new HashMap&lt;String, Integer&gt;();

    HashMap&lt;String, Integer&gt; locationMap = new HashMap&lt;String, Integer&gt;();
    HashMap&lt;String, Integer&gt; locationCountMap = new HashMap&lt;String, Integer&gt;();

    HashMap&lt;String, Article_Location&gt; locationArticleMap = new HashMap&lt;String, Article_Location&gt;();

    ArrayList&lt;Articel_Ner&gt; organisationlist = new ArrayList&lt;Articel_Ner&gt;();
    ArrayList&lt;Articel_Ner&gt; personlist = new ArrayList&lt;Articel_Ner&gt;();
    ArrayList&lt;Artilcle_Ner_Date&gt; datelist = new ArrayList&lt;Artilcle_Ner_Date&gt;();
    ArrayList&lt;Article_NerLocation&gt; locationList = new ArrayList&lt;Article_NerLocation&gt;();     

    try
    {
        Annotation annotation = pipeline.process(s);//1/3 rd time is taken up by this line

        List&lt;CoreMap&gt; sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);

        for (CoreMap sentence : sentences)
        {
             String str = sentence.toString();

             int score = getSentiment(sentence);

             finalScore+=score;
             boolean dFlag = true;

             List&lt;Triple&lt;String,Integer,Integer&gt;&gt; triples = ncc.classifyToCharacterOffsets(str);

             for (Triple&lt;String,Integer,Integer&gt; trip : triples)
             {
                 String ne = trip.first();
                 String word = str.substring(trip.second(), trip.third).toLowerCase();

                 switch(ne)
                 {
                    case ""LOCATION"":                        
                        extractLocation(locationMap, locationCountMap, locationArticleMap, score, word);
                        break;

                    case ""ORGANIZATION"":                        
                        extractOrg(orgMap, orgCountMap, score, word);                       
                        break;

                    case ""PERSON"":                      
                        extractPerson(personMap, personCountMap, score, word);
                        break;

                    case ""DATE"":
                        if(dFlag)
                        {
                         extractSUDate(dateString, dateMap, dateCountMap, dateSentenceMap, str, score);
                         dFlag = false;
                        }
                        break;

                    default:
                        break;
                 }
             }
        }
            //2/3rd of the time taken by these 4 methods:: can be obtimized
        mapDate(dateMap, dateCountMap, dateSentenceMap, datelist);
        mapLocation(locationMap, locationCountMap, locationArticleMap, locationList);   
        mapOrg(orgMap, orgCountMap, organisationlist);  
        mapPerson(personMap, personCountMap, personlist);
        //
    }
    catch(Exception e)
    {
        logger.error(e);
        logger.error(s);
        e.printStackTrace();
    }

    if(finalScore&gt;0)
        finalScore = 1;
    else if(finalScore&lt;0)
        finalScore = -1;
    else
        finalScore = 0;

    map.put(""ORGANISATION"", organisationlist);
    map.put(""PERSON"", personlist);
    map.put(""DATE"", datelist);
    map.put(""LOCATION"", locationList);
    map.put(""SENTIMENT"", finalScore);

    return map;
}

private static void extractPerson(HashMap&lt;String, Integer&gt; personMap, HashMap&lt;String, Integer&gt; personCountMap,
        int score, String word)
{       
    if(personMap.get(word)!=null)
    {
        personMap.put(word, personMap.get(word)+score);
        personCountMap.put(word, personCountMap.get(word)+1);
    }
    else
    {
        personMap.put(word, score);
        personCountMap.put(word, 1);
        //personSentenceMap.put(pname, str);
    }   
}

private static void extractOrg(HashMap&lt;String, Integer&gt; orgMap, HashMap&lt;String, Integer&gt; orgCountMap,
        int score, String word)
{
    if(orgMap.get(word)!=null)
    {
        orgMap.put(word, orgMap.get(word)+score);
        orgCountMap.put(word, orgCountMap.get(word)+1);                             
    }
    else
    {
        orgMap.put(word, score);
        orgCountMap.put(word, 1);
        //orgSentenceMap.put(oname, str);
    }
}

private static void extractLocation(HashMap&lt;String, Integer&gt; locationMap,
        HashMap&lt;String, Integer&gt; locationCountMap,
        HashMap&lt;String, Article_Location&gt; locationArticleMap,
        int score,
        String word)
{
    if(locationMap.get(word)!=null)
    {
        locationMap.put(word, locationMap.get(word)+score);
        locationCountMap.put(word, locationCountMap.get(word)+1);                               
    }
    else
    {
        Article_Location articleLocation = LocationUtil.getLocation(word);

        locationMap.put(word, score);
        locationCountMap.put(word, 1);
        locationArticleMap.put(word, articleLocation);
    }   
}

private static void extractSUDate(long dateString,
        HashMap&lt;String, Integer&gt; dateMap,
        HashMap&lt;String, Integer&gt; dateCountMap,
        HashMap&lt;String, String&gt; dateSentenceMap, 
        String str,
        int score) {

    Annotation dateAnnotation = new Annotation(str);
    dateAnnotation.set(CoreAnnotations.DocDateAnnotation.class, FormatUtil.getDate(dateString));
    pipeline.annotate(dateAnnotation);

    for(CoreMap timex:dateAnnotation.get(TimeAnnotations.TimexAnnotations.class))
    {
        TimeExpression timeExpression = timex.get(TimeExpression.Annotation.class);

         if(timeExpression!=null &amp;&amp; timeExpression.getTemporal()!=null &amp;&amp;
            timeExpression.getTemporal().getTimexValue()!=null)
         {           
             String word = checkDate(timeExpression.getTemporal().getTimexValue());

             if(word!=null)
             {
                 if(dateMap.get(word)!=null)
                 {
                     dateMap.put(word, dateMap.get(word)+score);
                     dateCountMap.put(word, dateCountMap.get(word)+1);
                     dateSentenceMap.put(word, dateSentenceMap.get(word)+"" ""+str);
                 }
                 else
                 {
                     dateMap.put(word, score);
                     dateCountMap.put(word, 1);
                     dateSentenceMap.put(word, str);
                 }                       
             }
         }
    }
}

private static int getSentiment(CoreMap sentence) {
    Tree annotatedTree = sentence.get(SentimentAnnotatedTree.class);
     int localScore = RNNCoreAnnotations.getPredictedClass(annotatedTree);
     int score = getScore(localScore);       
    return score;
}   


private static void mapLocation(HashMap&lt;String, Integer&gt; locationMap,
        HashMap&lt;String, Integer&gt; locationCountMap,
        HashMap&lt;String, Article_Location&gt; locationArticleMap,
        ArrayList&lt;Article_NerLocation&gt; locationList)
{       
    for(Map.Entry&lt;String, Integer&gt; entry : locationMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Article_Location articleLocation = locationArticleMap.get(key);

        Article_NerLocation l1 = new Article_NerLocation();
        if(value&gt;=1)
            l1.setNerSentiment(1);
        else if(value&lt;=-1)
            l1.setNerSentiment(-1);
        else
            l1.setNerSentiment(0);            

        l1.setKeyword(key);
        l1.setCount(locationCountMap.get(key));

        if(articleLocation!=null)
        {                   
            l1.setNerCountry(articleLocation.getCountryCode());
            l1.setNerLatLong(articleLocation.getLatitude()+"",""+articleLocation.getLongitude());
            l1.setTimeZone(articleLocation.getTimeZone());
            l1.setCountryName(articleLocation.getCountryName());
        }

        locationList.add(l1);
    }
}

private static void mapDate(HashMap&lt;String, Integer&gt; dateMap,
        HashMap&lt;String, Integer&gt; dateCountMap,
        HashMap&lt;String, String&gt; dateSentenceMap,
        ArrayList&lt;Artilcle_Ner_Date&gt; datelist)
{               
    for(Map.Entry&lt;String, Integer&gt; entry : dateMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Artilcle_Ner_Date d1 = new Artilcle_Ner_Date();

        if(value&gt;=1)
            d1.setNerSentiment(1);
        else if(value&lt;=-1)
            d1.setNerSentiment(-1);
        else
            d1.setNerSentiment(0);

        d1.setKeyword(key);
        d1.setCount(dateCountMap.get(key));
        d1.setSentence(dateSentenceMap.get(key));
        d1.setNerDateTheme1(SummaryThemeUtil.getSTByDate(dateSentenceMap.get(key)));
        datelist.add(d1);
    }   
}   

private static void mapOrg(HashMap&lt;String, Integer&gt; orgMap,
        HashMap&lt;String, Integer&gt; orgCountMap,
        ArrayList&lt;Articel_Ner&gt; organisationlist) 
{
    for(Map.Entry&lt;String, Integer&gt; entry : orgMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Articel_Ner o1 = new Articel_Ner();
        if(value&gt;=1)
            o1.setNerSentiment(1);
        else if(value&lt;=-1)
            o1.setNerSentiment(-1); 
        else
            o1.setNerSentiment(0);            


        o1.setKeyword(key);
        o1.setCount(orgCountMap.get(key));
        organisationlist.add(o1);            
    }       
}

private static void mapPerson(HashMap&lt;String, Integer&gt; personMap,
        HashMap&lt;String, Integer&gt; personCountMap,
        ArrayList&lt;Articel_Ner&gt; personlist)
{
    for(Map.Entry&lt;String, Integer&gt; entry : personMap.entrySet())
    {
        String key = entry.getKey();
        Integer value = entry.getValue();

        Articel_Ner p1 = new Articel_Ner();
        if(value&gt;=1)
            p1.setNerSentiment(1);
        else if(value&lt;=-1)
            p1.setNerSentiment(-1);
        else
            p1.setNerSentiment(0);            

        p1.setKeyword(key);
        p1.setCount(personCountMap.get(key));
        personlist.add(p1);      
    }
}   

private static String checkDate(String date)
{               
    if(date.length()&lt;10)
        return null;
    else if(date.length()&gt;10)
        date = date.substring(0,10);

    if (date.matches(""\\d{4}-\\d{2}-\\d{2}""))
        return date; 
    else
        return null;
}

public static void main(String args[])
{
    String text = ""Lets meet on every 2nd week. Night is young. Happy new Year. The festival will be held on the following dates are 18 Feb 1997, the 20th of july and 4 days from today."";
    long pre = System.currentTimeMillis();
    HashMap&lt;String, Object&gt; map = getStanford(text, 1508745558);
    long post = System.currentTimeMillis();
    long diff = post-pre;

    System.out.println(diff);
    System.out.println(map);
}
}
</code></pre>
","stanford-nlp, sentiment-analysis, named-entity-recognition, sutime","<p>After days and days of sore black eyes. Here is where the problem is:</p>

<ul>
<li><p>Stanford ""<strong>parse</strong>"" model whether <strong>PCFG</strong> or <strong>SRparser</strong> both are CPU killers. You will never be able to scale. At best i was doing 70 docs/second. This is with 15 threads that i was able to manage on tomcat. The docs where being consumed from RabbitMQ. Machine Intel Xeon 8Core VM with 15 GB RAM. The CPU was always 90%.</p></li>
<li><p>So if you want to do <strong>NER</strong>,<strong>sentiment</strong>,<strong>sutime</strong>. Its better to use separate libraries and not use stanford for all 3. For NER you can use <strong>NERClassifierCombiner</strong> from stanford. For sentiment you can use <strong>weka</strong>. For extracting dates you can use <strong>natty</strong>. </p></li>
<li><p>Now we are able to do <strong>2,000 docs/second</strong>. </p></li>
</ul>
",0,-1,539,2017-11-09 09:35:26,https://stackoverflow.com/questions/47198333/stanford-nlp-ner-sentiment-sutime-performance-issue
Detailed Sentiment Score in Stanford CoreNLP,"<p>On the StanfordCore NLP website there is the following demo:<a href=""http://nlp.stanford.edu:8080/sentiment/rntnDemo.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/sentiment/rntnDemo.html</a> </p>

<p>The demo gives a sentence a detailed sentiment score from 0 to 4. </p>

<p>I understand how to get a ""positive"" or ""negative"" assessment using command line, similar to this:
<a href=""https://i.sstatic.net/F5s41.jpg"" rel=""nofollow noreferrer"">Screenshot from corenlp.run showing a positive sentiment analysis</a></p>

<p>I have seen this question already, but I am interested how the analysis shown in the attached screenshot is created. <a href=""https://stackoverflow.com/questions/20368101/getting-sentiment-analysis-result-using-stanford-core-nlp-java-code"">Getting sentiment analysis result using stanford core nlp java code</a></p>

<p>Is there a way in Stanford CoreNLP to return a score (i.e. 0-4) for a given sentence so show its degree of positivity or negativity?</p>

<p>Thanks!</p>
","java, stanford-nlp, sentiment-analysis","<p>There are multiple ways to get that kind of info.</p>

<p>Also I should note that the there is a direct mapping:</p>

<p>""Very negative"" = 0
""Negative"" = 1
""Neutral"" = 2
""Positive"" = 3
""Very positive"" = 4</p>

<p>Here is a sample command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,sentiment -file example-1.txt -outputFormat json
</code></pre>

<p>In the file <code>example-1.txt.json</code> you'll see a lot of sentiment related fields for the sentence, including <code>sentimentValue</code>.</p>

<p>There is more info about this at this GitHub issue:</p>

<p><a href=""https://github.com/stanfordnlp/CoreNLP/issues/465"" rel=""noreferrer"">https://github.com/stanfordnlp/CoreNLP/issues/465</a></p>
",6,3,4099,2017-11-13 21:31:20,https://stackoverflow.com/questions/47273885/detailed-sentiment-score-in-stanford-corenlp
"Sentiment Analysis with 3 classes (positive, neutral, and negative)?","<p>I want to do sentiment analysis with 3 classes (positive, neutral, and negative). I have seen lots of work on sentiment analysis with two classes (positive and negative), but much less so for 3 classes. If I wanted to use a bag-of-words approach and a classifier such as Logistic Regression or SVMs in Scikit-learn, how would this work? What would the steps be for my output to predict with 3 classes?</p>

<p>Do I have to treat each class as a binary classification and do something to combine the results, or is sklearn able to do some processing for me so I do not have to specify this?</p>
","python, scikit-learn, sentiment-analysis, multiclass-classification","<p>There are three possible approaches:</p>

<ol>
<li>Use <a href=""http://scikit-learn.org/stable/modules/multiclass.html"" rel=""nofollow noreferrer"">multiclass algorithms</a>, such as logistic regression or decision tree (they are inherently multiclass) or one-vs-one or one-vs-rest wrappers for binary algorithms such as SVM.</li>
<li>If you want to exploit the fact that neutral texts are ""somewhere between"" positive and negative ones, you can use ordered classification models, such as ordered logistic regression in the <a href=""http://pythonhosted.org/mord/"" rel=""nofollow noreferrer"">mord</a> package.</li>
<li>If you want to exploit the ordering of classes, but want to stay within scikit-learn, I would suggest to fit any regression model to your data first (e.g. gradient boosing regressor), and then use logistic regression on top of its prediction.</li>
</ol>
",2,1,2645,2017-11-15 21:29:36,https://stackoverflow.com/questions/47317567/sentiment-analysis-with-3-classes-positive-neutral-and-negative
Find sentences with describing context using stanford NLP,"<p>Is there any way to find those sentences that are describing objects?</p>

<p>For example sentences like ""This is a good product"" or ""You are very beautiful""</p>

<p>I guess I can create an algorithm by using TokenSequencePattern and filter with POS some patterns like PRONUN + VERB + ADJECTIVE but don't think would be something reliable.</p>

<p>I am asking you if there is something out of the box, what I am trying to do is to identify review comments on a webpage.</p>
","machine-learning, stanford-nlp, sentiment-analysis","<p>Instead of POS tagging, you would achieve better results by dependency parsing. By using that instead of POS tagging &amp; patterns as you mentioned, you will have richer and accurate information about the sentence structure.
Example:</p>

<p><a href=""https://demos.explosion.ai/displacy/?text=The%20product%20was%20really%20very%20good.&amp;model=en_core_web_sm&amp;cpu=0&amp;cph=0"" rel=""nofollow noreferrer"">https://demos.explosion.ai/displacy/?text=The%20product%20was%20really%20very%20good.&amp;model=en_core_web_sm&amp;cpu=0&amp;cph=0</a></p>

<p><a href=""https://i.sstatic.net/cVS6W.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cVS6W.png"" alt=""enter image description here""></a></p>

<p>Stanford NLP does support <a href=""https://nlp.stanford.edu/software/stanford-dependencies.shtml"" rel=""nofollow noreferrer"">depedency parsing</a>.
Apart from that you can also use the brilliant <a href=""https://spacy.io/usage/linguistic-features"" rel=""nofollow noreferrer"">SpaCy</a>.</p>
",1,0,189,2017-11-19 20:08:28,https://stackoverflow.com/questions/47381480/find-sentences-with-describing-context-using-stanford-nlp
Python - Summarizing Tweets DataFrame,"<ol>
<li><p>I am dealing with Twitter Sentiment Analysis, mining tweets for certain specific keywords.</p></li>
<li><p>I am able to store the tweets as well as process them, generating sentiments and subjectivity etc. </p></li>
<li><p>I have a list of 200 keywords on which I am extracting tweets and storing them and processing them together.</p></li>
<li><p>but, I need this to be summarized based on all tweets that I have overall. I have create column.</p></li>
</ol>
","python, python-3.x, pandas, numpy, sentiment-analysis","<p>Since each tweet can have multiple keywords, I don't think there is a clean way to do it. My solution would be to create a <code>pd.Series</code> out of the keywords, and use <code>apply</code> to loop through the keywords. </p>

<p>Assume that the list you provided above is in the variable <code>keywords</code>, and <code>DataFrame</code> of tweets is in <code>df</code>. I'm also assuming that <em>Overall Sentiment Score</em> is the most common sentiment, and the <em>Overall Subjectivity</em> is the mean.</p>

<pre><code>def summarize(data):
    """"""
    Extract the statistics for a given sub-dataframe
    """"""
    return pd.Series({
                      ""total_tweets"" : len(data)
                      ""total_retweets"" : data.retweet_count.sum()
                      ""total_favorites"" : data.favorite_count.sum()
                      ""total_comments"" : data.reply_count.sum()
                      ""overall_sentiment_score"" : data.sentiment.mode().loc[0]
                      ""overall_subjectivity"" : data.subjectivity.mean()
                     })

s = pd.Series(keywords)
res = s.apply(lambda word: summarize(df[df.text.str.contains(word)]))
</code></pre>
",1,0,471,2017-11-21 18:01:43,https://stackoverflow.com/questions/47419735/python-summarizing-tweets-dataframe
Sentiment analysis for tidytext in R,"<p>I am trying to perform sentiment analysis in R.
I want to use either afinn or bing lexicon, but the problem is i cant tokenize the words.</p>

<p>Here are the words for which i need the sentiments for : </p>

<p><a href=""https://i.sstatic.net/hnSdD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hnSdD.png"" alt=""sentiment_words""></a></p>

<p>So there are 6 words for whom i want sentiments for : 
Pass
Fail
Not Ready
Out of Business
Pass w/conditions
No entry</p>

<p>How do i use any of the lexicons to assign sentiments to these words</p>

<p>Here is my code : </p>

<pre><code>d&lt;- as.data.frame(data$Results)
d&lt;- as.data.frame(d[1:2000,])

colnames(d) &lt;- ""text""



#Making preprocessed file for raw data
preprocess&lt;-data.frame(text=sapply(tweet_corpus_clean, identity), 
                       stringsAsFactors=F)

# tokenize
tokens &lt;- data_frame(text = preprocess$text) %&gt;% unnest_tokens(word, text)
</code></pre>

<p>When run this i get : </p>

<p><a href=""https://i.sstatic.net/2w2DH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2w2DH.png"" alt=""senti_new""></a></p>

<p>Because for lexicons to assign sentiments it has to be one token per row</p>

<p>So i had to merge those words together.
Now when i use afinn its not able to understand what outofbusiness is obvioulsy</p>

<pre><code>tokens &lt;- data_frame(text = preprocess$text) %&gt;% unnest_tokens(word, text)


contributions = tokens %&gt;%ungroup()%&gt;%
  inner_join(get_sentiments(""afinn""), by = ""word"") %&gt;%
  group_by(word) %&gt;%
  summarize(score = as.numeric(sum(score * n) / sum(n))) %&gt;%
  arrange(desc(sentiment))
</code></pre>

<p>how do i do sentiment analysis for those 6 tpes of words?</p>
","r, text-mining, sentiment-analysis, tidyverse, tidytext","<p>Hmmmm, this doesn't sounds like a sentiment analysis problem to me. You have six words/phrases that you know about exactly, and you know what they mean in your context. This sounds like you just want to assign these words/phrases scores, or even just levels of a factor.</p>

<p>You could do something like what I show here, where <em>you</em> as the analyst decide what score each of your phrases should have. Here, <code>scores</code> is the dataframe that you as the analyst construct with sensibly chosen scores for each text options, and <code>df</code> is the data you are analyzing.

<br/></p>

<pre class=""lang-r prettyprint-override""><code>library(dplyr)

scores &lt;- data_frame(text = c(""pass"",
                              ""fail"",
                              ""not ready"",
                              ""out of business"",
                              ""pass w/conditions"",
                              ""no entry""),
                     score = c(3, -1, 0, 0, 2, 1))

scores
#&gt; # A tibble: 6 x 2
#&gt;   text              score
#&gt;   &lt;chr&gt;             &lt;dbl&gt;
#&gt; 1 pass               3.00
#&gt; 2 fail              -1.00
#&gt; 3 not ready          0   
#&gt; 4 out of business    0   
#&gt; 5 pass w/conditions  2.00
#&gt; 6 no entry           1.00

df &lt;- data_frame(text = c(""pass"",
                          ""pass"",
                          ""fail"",
                          ""not ready"",
                          ""out of business"",
                          ""no entry"",
                          ""fail"",
                          ""pass w/conditions"",
                          ""fail"",
                          ""no entry"",
                          ""pass w/conditions""))

df %&gt;%
  left_join(scores)
#&gt; Joining, by = ""text""
#&gt; # A tibble: 11 x 2
#&gt;    text              score
#&gt;    &lt;chr&gt;             &lt;dbl&gt;
#&gt;  1 pass               3.00
#&gt;  2 pass               3.00
#&gt;  3 fail              -1.00
#&gt;  4 not ready          0   
#&gt;  5 out of business    0   
#&gt;  6 no entry           1.00
#&gt;  7 fail              -1.00
#&gt;  8 pass w/conditions  2.00
#&gt;  9 fail              -1.00
#&gt; 10 no entry           1.00
#&gt; 11 pass w/conditions  2.00
</code></pre>

<p>Sentiment analysis is most appropriate where you have large amounts of unstructured text that you need to extract insight from. Here you have only six text elements, and you can use what you know about your domain and context to assign scores.</p>
",1,0,627,2017-12-03 22:01:15,https://stackoverflow.com/questions/47623809/sentiment-analysis-for-tidytext-in-r
Determining the polarity of emoticons for twitter sentiment analysis,"<p>I am trying doing a sentiment analysis on twitter data and want to take the emoticons into account. Say I have the unicode of an emoticon 😀 (in this case U+1F600). I would like to get some kind of positive, negative or neutral polarity score. I have done some research but have been unsuccessful in finding an existing lexicon that I could use. Basically I would like to use an existing lexicon if there is one available instead of going through all emoticons manually and assigning a score. Any suggestions?</p>

<p>I am using Python if it makes any difference.</p>
","python, twitter, sentiment-analysis, emoticons","<p>A full lexicon containing emojis and their corresponding Unicode can be downloaded from <a href=""http://unicode.org/emoji/charts/emoji-list.html"" rel=""nofollow noreferrer"">http://unicode.org/emoji/charts/emoji-list.html</a> .</p>
",0,2,1827,2017-12-06 07:53:42,https://stackoverflow.com/questions/47669357/determining-the-polarity-of-emoticons-for-twitter-sentiment-analysis
Chronological Sentiment Analysis -- Cannot group by lines,"<p>My data <em>text</em> is a novel in plain text. I used packages tm and tidytext. Data processing went well and I created my DocumentTermMatrix without trouble.  </p>

<pre><code>text &lt;- read_lines(""GoneWithTheWind2.txt"")
set.seed(314) 
text &lt;- iconv(text,'UTF-8',sub="""")
myCorpus &lt;- tm_map(myCorpus, removeWords, c(stopwords(""english""), 
stopwords(""SMART""), mystopwords, Top200Words))  
myDtm &lt;- TermDocumentMatrix(myCorpus, control=list(minWordLength= 1))`
</code></pre>

<p>However, I could not run the coding using <em>inner_join</em> between bing lexicon and the DocumentTermMatrix to do chronological sentiment analysis of this novel over time. I wrote the function below based on an online example but did not know what to group by in count(sentiment) (I place ???? in hold), because the plain text and the DocumentTermMatrix has no ""lines"" columns.  </p>

<pre><code>bing &lt;- get_sentiments(""bing"")  
m &lt;- as.matrix(myDtm)
v &lt;- sort(rowSums(m),decreasing=TRUE)
myNames &lt;- names(v)
d &lt;- data.frame(term=myNames, freq = v)
wind_polarity &lt;- d %&gt;%
# Inner join to the lexicon
inner_join(bing, by=c(""term""=""word"")) %&gt;%
# Count by sentiment, **????**
count(sentiment, **????**) %&gt;%
# Spread sentiments
spread(sentiment, n, fill=0) %&gt;%
mutate(
# Add polarity field
polarity = positive - negative,
# Add line number field
line_number = row_number())
Then plot by ggplot.
</code></pre>

<p>I tried adding a column ""Index"" indicating the line number for each document (line) in <em>text</em> but this column disappears somewhere in the process. Any suggestions would be highly appreciated.</p>
","r, text-mining, sentiment-analysis","<p>Below an approach that calculates the polarity per line (based on a minimum example of three lines). You might join your dtm with the lexicon directly to maintain information on the counts. Then turn polarity information into numeric representation and do your calculations per line. You might certainly rewrite the code and make it more elegant (I am not very familiar with dplyr vocabulary, sorry). I hope that helps anyway.</p>

<pre><code>library(tm)
library(tidytext)

text &lt;- c(""I like coffe.""
          ,""I rather like tea.""
          ,""I hate coffee and tea, but I love orange juice."")

myDtm &lt;- TermDocumentMatrix(VCorpus(VectorSource(text)),
                          control = list(removePunctuation = TRUE,
                                         stopwords = TRUE))

bing &lt;- tidytext::get_sentiments(""bing"")  

wind_polarity &lt;- as.matrix(myDtm) %&gt;%
  data.frame(terms = rownames(myDtm), ., stringsAsFactors = FALSE) %&gt;% 
  inner_join(bing, by= c(""terms""=""word"")) %&gt;%
  mutate(terms = NULL,
         polarity = ifelse( (.[,""sentiment""] == ""positive""), 1,-1),
         sentiment = NULL) %&gt;%
  { . * .$polarity } %&gt;% 
  mutate(polarity = NULL) %&gt;% 
  colSums

#the polarity per line which you may plot, e.g., with base or ggplot
# X1 X2 X3 
# 1  1  0 
</code></pre>
",0,1,133,2017-12-17 04:32:49,https://stackoverflow.com/questions/47852211/chronological-sentiment-analysis-cannot-group-by-lines
Python Excel Sentiment Analysis,"<p>This is my first time posting on a forum regarding coding.</p>

<p>I'm an inexperienced Python user and am currently tackling a project regarding Sentiment Analysis of Feedbacks which are stored in an excel file.</p>

<p>I would like to ask what are the appropriate/best packages to be used to do Sentiment Analysis on an excel file using Python. </p>

<p>Thanks for the help in advance.</p>
","python, excel, package, sentiment-analysis","<p>This question elicits opinions and doesn't really fit this forum.</p>

<p>However NLTK seems to be the main library for language processing and sentiment analysis in Python.
<a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">http://www.nltk.org/</a></p>

<p>Textblob is another that's arguably simpler than NLTK.
<a href=""http://textblob.readthedocs.io/en/dev/"" rel=""nofollow noreferrer"">http://textblob.readthedocs.io/en/dev/</a></p>

<p>To process excel files you can use the XLRD library
<a href=""https://pypi.python.org/pypi/xlrd"" rel=""nofollow noreferrer"">https://pypi.python.org/pypi/xlrd</a></p>

<p>There are many resources with examples on using XLRD. Here's a good one for instance: <a href=""https://stackoverflow.com/questions/23568409/xlrd-python-reading-excel-file-into-dict-with-for-loops"">XLRD/Python: Reading Excel file into dict with for-loops</a></p>
",1,0,1064,2017-12-22 02:43:59,https://stackoverflow.com/questions/47935249/python-excel-sentiment-analysis
How to create a bag of words from csv file in python?,"<p>I am new to python. I have a csv file which has cleaned tweets. I want to create a bag of words of these tweets.
I have the following code but its not working correctly.</p>

<pre><code>import pandas as pd
from sklearn import svm
from sklearn.feature_extraction.text import CountVectorizer

data = pd.read_csv(open(""Twidb11.csv""), sep=' ')
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(data.Text)
count_vect.vocabulary_
</code></pre>

<p><strong>Error:</strong></p>

<blockquote>
  <p>.ParserError: Error tokenizing data. C error: Expected 19 fields in
  line 5, saw 22</p>
</blockquote>
","python-2.7, machine-learning, sentiment-analysis","<p>It's duplicated i think. U can see answer <a href=""https://stackoverflow.com/questions/18039057/python-pandas-error-tokenizing-data"">here</a>. There are a lot of answers and comments.</p>

<p>So, solution can be:</p>

<pre><code>data = pd.read_csv('Twidb11.csv', error_bad_lines=False)
</code></pre>

<p>Or:</p>

<pre><code>df = pandas.read_csv(fileName, sep='delimiter', header=None)
</code></pre>

<p>""In the code above, sep defines your delimiter and header=None tells pandas that your source data has no row for headers / column titles. Thus saith the docs: ""If file contains no header row, then you should explicitly pass header=None"". In this instance, pandas automatically creates whole-number indeces for each field {0,1,2,...}.""</p>
",0,1,3351,2017-12-22 04:33:00,https://stackoverflow.com/questions/47935888/how-to-create-a-bag-of-words-from-csv-file-in-python
Sentiment Analysis Code (word2vec) not properly working in my python version (vocabulary not built),"<p>I have taken a code online to do sentiment analysis on twitter database. I tried running it and it gave me at the beginning error for printing, which I figured out that the newer version of python has changed its way to do print. I am getting error that shows my data is not filled in the array, if anyone has worked with python and has eagle eye to see where I am going wrong please help. </p>

<pre><code>    import numpy as np 
    from copy import deepcopy
    from string import punctuation
    from random import shuffle
    import chardet
    from sklearn.manifold import TSNE
    from sklearn.preprocessing import scale


    import bokeh.plotting as bp
    from bokeh.models import HoverTool, BoxSelectTool
    from bokeh.plotting import figure, show, output_notebook

    import gensim
    from gensim.models.word2vec import Word2Vec 
    LabeledSentence = gensim.models.doc2vec.LabeledSentence 

    import pandas as pd 
    pd.options.mode.chained_assignment = None

    from tqdm import tqdm
    tqdm.pandas(desc=""progress-bar"")

    from nltk.tokenize import TweetTokenizer 
    tokenizer = TweetTokenizer()

    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import TfidfVectorizer

    def ingest(filename):
        with open(filename, 'rb') as f:
            result = chardet.detect(f.read())
        data = pd.read_csv(filename, encoding=result['encoding'])
        data.drop(['ItemID', 'Date', 'Blank', 'SentimentSource'], axis=1, inplace=True)
        data = data[data.Sentiment.isnull() == False]
        data['Sentiment'] = data['Sentiment'].map({4:1, 0:0})
        data = data[data['SentimentText'].isnull() == False]
        data.reset_index(inplace=True)
        data.drop('index', axis=1, inplace=True)
        print('dataset loaded with shape {}', format(data.shape)) 

        return data

    def tokenize(tweet):
        try:
            tweet = unicode(tweet.decode('utf-8').lower())
            tokens = tokenizer.tokenize(tweet)
            tokens = filter(lambda t: not t.startswith('@'), tokens)
            tokens = filter(lambda t: not t.startswith('#'), tokens)
            tokens = filter(lambda t: not t.startswith('http'), tokens)
            return tokens
        except:
            return 'NC'

    def postprocess(data, n=100):
        data = data.head(n)
        data['tokens'] = data['SentimentText'].progress_map(tokenize)  
        data = data[data.tokens != 'NC']
        data.reset_index(inplace=True)
        data.drop('index', inplace=True, axis=1)
        return data


    def labelizeTweets(tweets, label_type):
        labelized = []
        for i,v in  enumerate(tweets):
            label = '%s_%s'%(label_type,i)
            labelized.append(LabeledSentence(v, [label]))
            print("":::::::::::::::::::::::::"")
        return labelized


    def labelizeTweets(tweets, label_type):
        labelized = []
        for i,v in tqdm(enumerate(tweets)):
            label = '%s_%s'%(label_type,i)
            labelized.append(LabeledSentence(v, [label]))
        return labelized


    def buildWordVector(tokens, size):
        vec = np.zeros(size).reshape((1, size))
        count = 0.
        for word in tokens:
            try:
                vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]
                count += 1.
            except KeyError: 

                continue
        if count != 0:
            vec /= count
        return vec



    if __name__ == '__main__':

        filename = './training.csv'

        #n = 1000000
        n = 100
        n_dim = 200

        data = ingest(filename)
        #data = data.head(5)
        data = postprocess(data, n)

        x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens), np.array(data.head(n).Sentiment), test_size=0.2)


        print(""training length X"", len(x_train))

        print(""training length Y"", len(y_train))


        x_train = labelizeTweets(x_train, 'TRAIN')
        x_test = labelizeTweets(x_test, 'TEST')

        print(""jljkjkjlkjlj"", len(x_train))

        tweet_w2v = Word2Vec(size=n_dim, min_count=10)
        #tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])
        tweet_w2v.build_vocab([x.words for x in x_train])

        #tweet_w2v.train([x.words for x in tqdm(x_train)],total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)
        tweet_w2v.train([x.words for x in x_train],total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)




        print(tweet_w2v.most_similar('good'))

        if True:
            print('building tf-idf matrix ...')
            vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)
            matrix = vectorizer.fit_transform([x.words for x in x_train])
            tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
            print('vocab size :', len(tfidf))

            train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])
            train_vecs_w2v = scale(train_vecs_w2v)

            test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])
            test_vecs_w2v = scale(test_vecs_w2v)

            model = Sequential()
            model.add(Dense(32, activation='relu', input_dim=200))
            model.add(Dense(1, activation='sigmoid'))
            model.compile(optimizer='rmsprop',
                                        loss='binary_crossentropy',
                                        metrics=['accuracy'])

            model.fit(train_vecs_w2v, y_train, epochs=20, batch_size=32, verbose=2)

            score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)
            print (score[1])

    output_notebook()
    plot_tfidf = bp.figure(plot_width=700, plot_height=600, title=""A map of 10000 word vectors"",
        tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",
        x_axis_type=None, y_axis_type=None, min_border=1)

    word_vectors = [tweet_w2v[w] for w in tweet_w2v.wv.vocab.keys()[:5000]]

    tsne_model = TSNE(n_components=2, verbose=1, random_state=0)
    tsne_w2v = tsne_model.fit_transform(word_vectors)

    tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])
    tsne_df['words'] = tweet_w2v.wv.vocab.keys()[:5000]

    plot_tfidf.scatter(x='x', y='y', source=tsne_df)
    hover = plot_tfidf.select(dict(type=HoverTool))
    hover.tooltips={""word"": ""@words""}
    show(plot_tfidf)
</code></pre>

<p>This is the error I am getting </p>

<pre><code>    C:\Users\lenovo\AppData\Local\Programs\Python\Python35\lib\site-packages\gensim\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
dataset loaded with shape {} (505, 2)
progress-bar: 100%|##########################################################################| 505/505 [00:00&lt;?, ?it/s]
training length X 0
training length Y 0
0it [00:00, ?it/s]
0it [00:00, ?it/s]
jljkjkjlkjlj 0
Traceback (most recent call last):
  File ""Sentiment_Analysis.py"", line 127, in &lt;module&gt;
    tweet_w2v.train([x.words for x in x_train],total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python35\lib\site-packages\gensim\models\word2vec.py"", line 951, in train
    raise RuntimeError(""you must first build vocabulary before training the model"")
RuntimeError: you must first build vocabulary before training the model
</code></pre>
","python, twitter, nltk, sentiment-analysis, word2vec","<p>I had the same issue with the same code. There is absolutely no problem with the code on the website, but it returns an empty vocabulary no matter how you order it. </p>

<p>My workaround was that it runs smoothly when you run the same exact code in Python 2.7 instead of 3.x. However, if you do manage to port it successfully to Python 3.x, you have faster data/ memory access rates which is quite desirable.</p>

<p>Edit: Found the problem, now it works with Python 3 too. Edit the corresponding code segment to this and vocabulary should build without any issue.</p>

<pre><code>def tokenize(tweet):
        try:
            tweet = unicode(tweet.decode('utf-8').lower())
            tokens = tokenizer.tokenize(tweet)
            tokens = list(filter(lambda t: not t.startswith('@'), tokens))
            tokens = list(filter(lambda t: not t.startswith('#'), tokens))
            tokens = list(filter(lambda t: not t.startswith('http'), tokens))
            return tokens
        except:
            return 'NC'
</code></pre>
",1,0,621,2017-12-26 09:02:50,https://stackoverflow.com/questions/47976170/sentiment-analysis-code-word2vec-not-properly-working-in-my-python-version-vo
How do I get the words in alphabetical order and remove the symbol u&#39;?,"<p>I am working on creating a bag of words. I referred to this link <a href=""https://pythonprogramminglanguage.com/bag-of-words/#respond"" rel=""nofollow noreferrer"">https://pythonprogramminglanguage.com/bag-of-words/#respond</a></p>

<pre><code>df = pd.read_csv('Twidb11.csv',error_bad_lines=False, sep='delimiter',  engine='python')
# Creating Bag of Words
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(df.Text)
print count_vect.fit_transform(df.Text).todense()
#X_train_counts.shape 
print count_vect.vocabulary_
</code></pre>

<p>It is giving me the words and their frequency but the words are not ordered in alphabetical order and u' symbol is there, as shown below. How to get rid of this?</p>

<p>Output : { u'binance': 28, u'they': 139, u'just': 83, u'global': 67, u'alternatives': 11, u'zcash': 168, u'years': 165, u'talks': 133, u'japan': 82, u'yes': 166, u'25': 1, u'chinese': 37, u'6000': 5, u'zzzpositive': 170, u'winner': 162, u'28': 2, u'actually':12 ....}</p>
","python-2.7, machine-learning, sentiment-analysis","<p><code>u</code> is the representation of unicode. if you dont want convert it into string using <code>str()</code></p>

<p><strong>1) to convert unicode string into string</strong>,</p>

<pre><code>&gt;&gt;&gt; my_dict = {str(i):j for i,j in my_dict.items()}
&gt;&gt;&gt; print my_dict
&gt;&gt;&gt; {'binance': 28, 'global': 67, 'chinese': 37, 'just': 83, '25': 1, 'zzzpositive': 170, 'alternatives': 11, '6000': 5, 'winner': 162, '28': 2, 'zcash': 168, 'actually': 12, 'they': 139, 'talks': 133, 'japan': 82, 'yes': 166, 'years': 165}
</code></pre>

<p>2) <strong>sort my_dict</strong>,</p>

<p>itemgetter will help you do it easier</p>

<pre><code>&gt;&gt;&gt; from operator import itemgetter

&gt;&gt;&gt; dict(sorted(my_dict.items(), key=itemgetter(1))) # converted string unicode into str
&gt;&gt;&gt; {'25': 1, 'winner': 162, 'chinese': 37, '6000': 5, 'binance': 28, 'zzzpositive': 170, 'alternatives': 11, 'just': 83, 'global': 67, '28': 2, 'zcash': 168, 'actually': 12, 'they': 139, 'talks': 133, 'japan': 82, 'yes': 166, 'years': 165}
&gt;&gt;&gt; 
</code></pre>

<p><strong>in one line,</strong></p>

<pre><code>&gt;&gt;&gt; dict(sorted({str(i):j for i,j in my_dict.items()}.items(), key=itemgetter(1)))
</code></pre>
",0,1,113,2017-12-28 05:06:46,https://stackoverflow.com/questions/48001850/how-do-i-get-the-words-in-alphabetical-order-and-remove-the-symbol-u
"Tweet Feels: Always returns the same Sentiment Score, regardless tags","<p>I am trying to use this library to generate sentiment score for cryptocurrencies:</p>

<p><a href=""https://github.com/uclatommy/tweetfeels/blob/master/README.md"" rel=""nofollow noreferrer"">https://github.com/uclatommy/tweetfeels/blob/master/README.md</a></p>

<p>When I use the code from the example <code>trump</code>, it returns a sentiment score of <code>-0.00082536637608123106</code>.</p>

<p>I have changed the tags to the following:</p>

<pre><code>btc_feels = TweetFeels(login, tracking=['bitcoin'])
btc_feels.start(20)
btc_feels.sentiment.value
</code></pre>

<p>and it still gives me the same value.</p>

<p>I did notice something strange when I installed the library.</p>

<p>from the instructions:</p>

<blockquote>
  <p>If for some reason pip did not install the vader lexicon:</p>
  
  <blockquote>
    <p>python3 -m nltk.downloader vader_lexicon</p>
  </blockquote>
</blockquote>

<p>When I ran this, I got: </p>

<blockquote>
  <p>/anaconda/lib/python3.6/runpy.py:125: RuntimeWarning:
  'nltk.downloader' found in sys.modules after import of package 'nltk',
  but prior to execution of 'nltk.downloader'; this may result in
  unpredictable behaviour   warn(RuntimeWarning(msg))</p>
</blockquote>

<p>Could this be why it appears not to be working?</p>
","python, twitter, sentiment-analysis","<p>By default, tweetfeels creates a database in your current directory. The next time you start the program, it will continue using the same database, and pick up where it left off. I don't know what tweetfeels does to handle you changing the keyword on it, but this behaviour of tweetfeels could be a problem. The solution would be to use a different database for different keywords, and then pass in the location of your database to the TweetFeels constructor.</p>

<p>I don't know that much about Tweetfeels, it just sounded interesting, so I've downloaded the project, and I have a working script that will perform the sentiment analysis on any keyword I give it. I can add a copy of the script here, if you're still having problems getting TweetFeels to work.</p>

<p><br /></p>

<p><strong>Edit: here the script I am using</strong></p>

<p>I am currently having the following problems with the script.</p>

<p>1) I was getting some error that was different from the one you'd got, but I was able to fix the issue by replacing the tweetfeels library from pip with the latest code in their Github repository.</p>

<p>2) If a sentiment value does not get reported, sometimes tweetfeels fails to come to a complete stop, without forcefully sending a ctrl+c keyboard interrupt.</p>

<pre><code>import os, sys, time
from threading import Thread
from pathlib import Path

from tweetfeels import TweetFeels

consumer_key = 'em...'
consumer_secret = 'aF...'
access_token = '25...'
access_token_secret = 'd3...'
login = [consumer_key, consumer_secret, access_token, access_token_secret]

try:
    kw = sys.argv[1]
except IndexError:
    kw = ""iota""

try:
    secs = int(sys.argv[2])
except IndexError:
    secs = 15

for arg in sys.argv:
    if (arg == ""-h"" or arg == ""--help""):
        print(""Gets sentiment from twitter.\n""
              ""Pass in a search term, and how frequently you would like the sentiment recalculated (defaults to 15 seconds).\n""
              ""The keyword can be a comma seperated list of keywords to look at."")
        sys.exit(0)

db = Path(f""~/tweetfeels/{kw}.sqlite"").expanduser()
if db.exists():
    print(""existing db detected. Continueing from where the last sentiment stream left off"")
else:
    #ensure the parent folder exists, the db will be created inside of this folder
    Path(f""~/tweetfeels"").expanduser().mkdir(exist_ok=True)

feels = TweetFeels(login, tracking=kw.split("",""), db=str(db))

go_on = True
def print_feels(feels, seconds):
    while go_on:
        if feels.sentiment:
            print(f""{feels.sentiment.volume} tweets analyzed from {feels.sentiment.start} to {feels.sentiment.end}"")
            print(f'[{time.ctime()}] Sentiment Score: {feels.sentiment.value}')
            print(flush=True)
        else:
            print(f""The datastream has not reported a sentiment value."")
            print(f""It takes a little bit for the first tweets to be analyzed (max of {feels._stream.retry_time_cap + seconds} seconds)."")
            print(""If this problem persists, there may not be anyone tweeting about the keyword(s) you used"")
            print(flush=True)
        time.sleep(seconds)


t = Thread(target=print_feels, kwargs={""feels"":feels,""seconds"":secs}, daemon=True)
print(f'Twitter posts containing the keyword(s) ""{kw}"" will be streamed, and a new sentiment value will be recalculated every {secs} seconds')
feels.start()
time.sleep(5)
t.start()

try:
    input(""Push enter at any time to stop the feed...\n\n"")
except (Exception, KeyboardInterrupt) as e:
    feels.stop()
    raise e

feels.stop()
go_on = False
print(f""Stopping feed. It may take up to {feels._stream.retry_time_cap} for the feed to shut down.\n"")
#we're waiting on the feels thread to stop
</code></pre>
",2,2,269,2017-12-30 04:21:16,https://stackoverflow.com/questions/48030920/tweet-feels-always-returns-the-same-sentiment-score-regardless-tags
No freeze attribute when using Dataset module in Python,"<p>I'm currently trying to implement a form of twitter data analysis. I've already got a code up and running to pull data using the Streaming API, and all I have to do is save the data in a csv file.</p>

<pre><code>result = db[settings.TABLE_NAME].all()
dataset.freeze(result, format='csv', filename=settings.CSV_NAME)
</code></pre>

<p>From what I saw in the documentation, this should be the right way of declaring this. I've defined <code>Table.Name</code> and <code>CSV_Name</code> in another file settings.py.</p>

<p>When running <code>python dump.py</code>, it gives me this error.</p>

<pre><code>AttributeError: module 'dataset' has no attribute 'freeze'
</code></pre>

<p>I'm trying to replicate the code from <a href=""https://www.dataquest.io/blog/streaming-data-python/"" rel=""nofollow noreferrer"">Streaming Data</a>, if that helps pinpoint the issue. I've cloned the repo to my Desktop, and only added my Twitter authentication data.</p>
","python, python-3.x, twitter, sentiment-analysis","<p>As of version dataset 1.0, module dataset is split into two packages, with the data export features now extracted into a stand-alone package, datafreeze.</p>

<p>So install <code>datafreeze</code> </p>

<p><code>pip install datafreeze</code></p>

<p>Then </p>

<p><code>from datafreeze import freeze</code></p>

<p>or</p>

<p><code>from datafreeze.app import freeze</code></p>

<p>Both should work. </p>
",1,1,3410,2018-01-04 06:54:56,https://stackoverflow.com/questions/48090148/no-freeze-attribute-when-using-dataset-module-in-python
Change sentiment of a single word,"<p>I've been working with NLTK in Python for a few days for sentiment analysis and it's a wonderful tool. My only concern is the sentiment it has for the word 'Quick'. Most of the data that I am dealing with has comments about a certain service and MOST refer to the service as being 'Quick' which clearly has Positive sentiments to it. However, NLTK refers to it as being Neutral. I want to know if it's even possible to retrain NLTK to now refer to the Quick adjective as having positive annotations?</p>
","python, nlp, nltk, sentiment-analysis","<p>I have fixed the problem. Found the vader Lexicon file in AppData\Roaming\nltk_data\sentiment. Going through the file I found that the word Quick wasn't even in it. The format of the file is as following:</p>

<p>Token Mean-sentiment StandardDeviation [list of sentiment score collected from 10 people ranging from -4 to 4]</p>

<p>I edited the file. Zipped it. Now NLTK refers to Quick as having positive sentiments.</p>
",3,2,1158,2018-01-08 07:15:24,https://stackoverflow.com/questions/48145777/change-sentiment-of-a-single-word
Sentiment analysis on a csv file using textblob,"<p>I used sentiment analysis on a <code>CSV</code> file and the output prints the polarity and subjectivity of a sentence. How can I get the output in a table format along with the classification of the sentence as positive or negative or neutral added to it?</p>

<pre><code>    import csv
    from textblob import TextBlob

    infile = 'sentence.csv'

    with open(infile, 'r') as csvfile:
        rows = csv.reader(csvfile)
    for row in rows:
        sentence = row[0]
        blob = TextBlob(sentence)
        print (sentence)
        print (blob.sentiment.polarity, blob.sentiment.subjectivity)
</code></pre>

<p>the output for my code is :</p>

<pre><code>    i am very happy
    1.0 1.0
    its very sad
    -0.65 1.0
    they are bad
    -0.6999999999999998 0.6666666666666666
    hate the life
    -0.8 0.9
    she is so fantastic
    0.4 0.9
</code></pre>

<p>Thanks in advance.</p>
","python, csv, sentiment-analysis, textblob","<p>I would recommend creating a list of lists and importing that into a pandas dataframe to get a table structure</p>

<pre><code>import csv
from textblob import TextBlob
import pandas as pd
import numpy as np

infile = 'sentence.csv'
bloblist = list()

with open(infile, 'r') as csvfile:
    rows = csv.reader(csvfile)

for row in rows:
    sentence = row[0]
    blob = TextBlob(sentence)
    bloblist.append((sentence,blob.sentiment.polarity, blob.sentiment.subjectivity))
</code></pre>

<p>This will give you a list of lists called <code>bloblist</code> Convert it into a pandas dataframe like</p>

<pre><code>df = pd.DataFrame(bloblist, columns = ['sentence','sentiment','polarity'])
</code></pre>

<p>After adding that you can create custom calculations like this:</p>

<pre><code>df['positive'] = np.where(df.sentiment &gt; .5,1,0)
</code></pre>
",2,1,2629,2018-01-16 18:03:36,https://stackoverflow.com/questions/48287316/sentiment-analysis-on-a-csv-file-using-textblob
Sentiment prediction using glm,"<p>I am trying to predict sentiments using glm and ran into following problem</p>

<pre><code>  train_data_df &lt;- as.data.frame(as.matrix(train_data))
  log_model &lt;- glm(sentiment ~ word_count, data = train_data_df,   family = binomial)
     &gt; Error in sort.list(y) : 'x' must be atomic for 'sort.list'
Have you called 'sort' on a list?
</code></pre>

<p>The data structure for the inputs ""sentiment"" and ""word_count"" are as follows</p>

<pre><code>&gt; str(train_data$sentiment[1:2])
List of 2
 $ : num 1
 $ : num 1
&gt; str(train_data$word_count[1:2])
List of 2
 $ :List of 1
  ..    $ :Classes 'term_frequency', 'integer'  Named int [1:24] 3 1 1 1 1 1  1 1 1 3 ...
      .. .. ..- attr(*, ""names"")= chr [1:24] ""and"" ""bags"" ""came"" ""disappointed"" ...
 $ :List of 1
  ..    $ :Classes 'term_frequency', 'integer'  Named int [1:22] 2 1 1 1 1 1 1 1 1 1 ...
     .. .. ..- attr(*, ""names"")= chr [1:22] ""and"" ""anyone"" ""bed"" ""comfortable"" ...



head(train_data_df[1,])
                   name
2 Planetwise Wipe Pouch
                                                                                                                                                          review
2 it came early and was not disappointed. i love planet wise bags and now my wipe holder. it keps my osocozy wipes moist and does not leak. highly recommend it.
  rating
2      5
                                                                                                                                                review_clean
2 it came early and was not disappointed i love planet wise bags and now my wipe holder it keps my osocozy wipes moist and does not leak highly recommend it
                                                              word_count sentiment
2 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1         1
</code></pre>

<p>Thanks in advance for helping me out</p>
","r, glm, sentiment-analysis","<p>In an R formula like the one you use, <code>sentiment ~ word_count</code>, each side is expected to be a single number or factor per row (this is what <code>'x' must be atomic</code> means). This is obviously not the case with your <code>word_count</code> column - it appears that, for each row, <code>word_count</code> is a list consisting of several integer values (<code>Have you called 'sort' on a list?</code> - well, indeed you have).</p>

<p>To confirm that this is the source of your issue, you can replace <code>word_count</code> with the sum of its elements; this should make the code to work (of course, if the result will be of any real value for sentiment prediction, it is another story, but this is not your actual question here...)</p>
",0,0,131,2018-01-17 07:05:45,https://stackoverflow.com/questions/48295396/sentiment-prediction-using-glm
Why did NLTK NaiveBayes classifier misclassify one record?,"<p>This is the first time I am building a sentiment analysis machine learning model using the nltk NaiveBayesClassifier in Python. I know it is too simple of a model, but it is just a first step for me and I will try tokenized sentences next time. </p>

<p>The real issue I have with my current model is: I have clearly labeled the word 'bad' as negative in the training data set (as you can see from the 'negative_vocab' variable). However, when I ran the NaiveBayesClassifier on each sentence (lower case) in the list ['awesome movie', ' i like it', ' it is so bad'], the classifier mistakenly labeled 'it is so bad' as positive.   </p>

<p><strong>INPUT:</strong></p>

<pre><code>import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import names

positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]
negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]
neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not','it','so','really' ]

def word_feats(words):
    return dict([(word, True) for word in words])

positive_features_1 = [(word_feats(positive_vocab), 'pos')]
negative_features_1 = [(word_feats(negative_vocab), 'neg')]
neutral_features_1 = [(word_feats(neutral_vocab), 'neu')]

train_set = negative_features_1 + positive_features_1 + neutral_features_1

classifier = NaiveBayesClassifier.train(train_set) 

# Predict
neg = 0
pos = 0
sentence = ""Awesome movie. I like it. It is so bad""
sentence = sentence.lower()
words = sentence.split('.')

def word_feat(word):
    return dict([(word,True)])
#NOTE THAT THE FUNCTION 'word_feat(word)' I WROTE HERE IS DIFFERENT FROM THE 'word_feat(words)' FUNCTION I DEFINED EARLIER. THIS FUNCTION IS USED TO ITERATE OVER EACH OF THE THREE ELEMENTS IN THE LIST ['awesome movie', ' i like it', ' it is so bad'].

for word in words:
    classResult = classifier.classify(word_feat(word))
    if classResult == 'neg':
        neg = neg + 1
    if classResult == 'pos':
        pos = pos + 1
    print(str(word) + ' is ' + str(classResult))
    print() 
</code></pre>

<p><strong>OUTPUT:</strong></p>

<pre><code>awesome movie is pos

i like it is pos

it is so bad is pos
</code></pre>

<p>To make sure the function 'word_feat(word)' iterates over each sentences instead of each word or letter, I did some diagnostic codes to see what is each element in 'word_feat(word)':</p>

<pre><code>for word in words:
    print(word_feat(word))
</code></pre>

<p>And it printed out:</p>

<pre><code>{'awesome movie': True}
{' i like it': True}
{' it is so bad': True} 
</code></pre>

<p>So it seems like the function 'word_feat(word)' is correct?</p>

<p>Does anyone know why the classifier classified 'It is so bad' as positive? As mentioned before, I had clearly labeled the word 'bad' as negative in my training data. </p>
","nlp, classification, nltk, sentiment-analysis, naivebayes","<p>Here is the modified code for you</p>

<pre><code>import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import names
from nltk.corpus import stopwords

positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]
negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]
neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not','it','so','really' ]

def word_feats(words):
    return dict([(word, True) for word in words])

positive_features_1 = [(word_feats(positive_vocab), 'pos')]
negative_features_1 = [(word_feats(negative_vocab), 'neg')]
neutral_features_1 = [(word_feats(neutral_vocab), 'neu')]

train_set = negative_features_1 + positive_features_1 + neutral_features_1

classifier = NaiveBayesClassifier.train(train_set) 

# Predict
neg = 0
pos = 0
sentence = ""Awesome movie. I like it. It is so bad.""
sentence = sentence.lower()
sentences = sentence.split('.')   # these are actually list of sentences

for sent in sentences:
    if sent != """":
        words = [word for word in sent.split("" "") if word not in stopwords.words('english')]
        classResult = classifier.classify(word_feats(words))
        if classResult == 'neg':
            neg = neg + 1
        if classResult == 'pos':
            pos = pos + 1
        print(str(sent) + ' --&gt; ' + str(classResult))
        print
</code></pre>

<p>I modified where you are considering 'list of words' as an input to your classifier. But Actually you need to pass sentence one by one, which means you need to pass 'list of sentences'</p>

<p>Also, for each sentence, you need to pass 'words as features', which means you need to split the sentence on white-space character.</p>

<p>Also, if you want your classifier to work properly for sentiment analysis, you need to give less preference to ""stop-words"" like ""it, they, is etc"". As these words are not sufficient to decide if the sentence is positive, negative or neutral. </p>

<p>The above code gives below output</p>

<pre><code>awesome movie --&gt; pos

 i like it --&gt; pos

 it is so bad --&gt; neg
</code></pre>

<p>So for any classifier, the input format for training classifier and predicting classifier should be same. While training you are providing list of words, try to use the same method to convert your test set as well. </p>
",1,1,1071,2018-01-19 06:40:19,https://stackoverflow.com/questions/48335460/why-did-nltk-naivebayes-classifier-misclassify-one-record
Ajax post request to google NLP,"<p>Im trying to do a post request to <a href=""https://cloud.google.com/natural-language/docs/sentiment-tutorial"" rel=""nofollow noreferrer"">GCP Natural Language</a> for sentiment analysis.</p>

<p>When I try the data format on the code explorer on google it works fine but when I run it on a html page I get an error that reads</p>

<pre><code>{
  ""error"": {
    ""code"": 400,
    ""message"": ""Invalid JSON payload received. Unknown name \""document[type]\"": Cannot bind query parameter. Field 'document[type]' could not be found in request message.\nInvalid JSON payload received. Unknown name \""document[content]\"": Cannot bind query parameter. Field 'document[content]' could not be found in request message."",
    ""status"": ""INVALID_ARGUMENT"",
    ""details"": [
      {
        ""@type"": ""type.googleapis.com/google.rpc.BadRequest"",
        ""fieldViolations"": [
          {
            ""description"": ""Invalid JSON payload received. Unknown name \""document[type]\"": Cannot bind query parameter. Field 'document[type]' could not be found in request message.""
          },
          {
            ""description"": ""Invalid JSON payload received. Unknown name \""document[content]\"": Cannot bind query parameter. Field 'document[content]' could not be found in request message.""
          }
        ]
      }
    ]
  }
}
</code></pre>

<p>My code is:</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;body&gt;

&lt;h1&gt; Testing sentiment Analysis &lt;/h1&gt;

&lt;script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""&gt;&lt;/script&gt;
&lt;script&gt;
    var APIKEY = ""AN API KEY"";

    $.ajax({
        type        : ""POST"",
        url         : ""https://language.googleapis.com/v1/documents:analyzeSentiment?key=""+APIKEY,
        data        :   {
                            ""document"": {
                                ""type"": ""PLAIN_TEXT"",
                                ""content"": ""Hello I am great""
                            },
                            ""encodingType"": ""UTF8"",
                        },
        success: function(res) {
                console.log(res);
                alert(res['message']);
        },
        error: function(res) {
                console.log(res['message']);
                alert(res);
        },
    });
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
","json, ajax, api, google-cloud-platform, sentiment-analysis","<p><strong>UPDATE:</strong>
A colleague has pointed me to the MISTAKE I was making. We have to use <code>JSON.stringify()</code> in order to send the request. The code should be like this:</p>

<pre><code>$.ajax({
    type        : ""POST"",
    url         : ""https://language.googleapis.com/v1/documents:analyzeEntitySentiment?key=YOUR-API-KEY"", 
    contentType : ""application/json; charset=utf-8"",
    data        : 
                    JSON.stringify({
                        ""document"": {
                            ""type"": ""PLAIN_TEXT"",
                            ""language"": ""en"",
                            ""content"": ""Hola Victor""

                        },
                        ""encodingType"": ""UTF8""}),
    success     : function(_result){ 

        if (_result) { 
            alert('SUCCESS');
        } else {
            alert('ERROR');
        }
    },

    error       : function(_result){

    }
});
</code></pre>

<p>I have tested it and it works. </p>
",1,1,640,2018-01-25 00:30:17,https://stackoverflow.com/questions/48433857/ajax-post-request-to-google-nlp
Alternative to Twitter API,"<p>I'm working on project where I stream tweets from Twitter API then apply sentiment analysis and visualize the results on an interactive colorful map. </p>

<p>I've tried the 'tweepy' library in python but the problem is it only retrieves few tweets (10 or less).</p>

<p>Also, I'm going to specify the language and the location which means I might get even less tweets! I need a real time streaming of hundred/thousands of tweets. </p>

<p>This is the code I tried (just in case):</p>

<pre><code>import os
import tweepy
from textblob import TextBlob

port = os.getenv('PORT', '8080')
host = os.getenv('IP', '0.0.0.0')

# Step 1 - Authenticate
consumer_key= 'xx'
consumer_secret= 'xx'

access_token='xx'
access_token_secret='xx'

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)

api = tweepy.API(auth)

#Step 3 - Retrieve Tweets
public_tweets = api.search('school')

for tweet in public_tweets:

    print(tweet.text)
    analysis = TextBlob(tweet.text)
    print(analysis)
</code></pre>

<p>Is there any better alternatives? I found ""PubNub"" which is a JavaScript API but for now I want something in python since it is easier for me.</p>

<p>Thank you</p>
","python, api, twitter, sentiment-analysis","<p>If you want large amount of tweets, I would recommend you to utilize Twitter's streaming API using <code>tweepy</code>:</p>

<pre><code>#Create a stream listner:
import tweepy
tweets = []
class MyStreamListener(tweepy.StreamListener):
#The next function defines what to do when a tweet is parsed by the streaming API
    def on_status(self, status):
        tweets.append(status.text)

#Create a stream:
myStreamListener = MyStreamListener()
myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)

#Filter streamed tweets by the keyword 'school':
myStream.filter(track=['school'], languages=['en'])
</code></pre>

<p>Note that track filter used here is the standard free filtering API where there is another API called PowerTrack which is built for enterprises who have more requirements and rules to filter on.</p>

<p>Ref: <a href=""https://developer.twitter.com/en/docs/tweets/filter-realtime/overview/statuses-filter"" rel=""nofollow noreferrer"">https://developer.twitter.com/en/docs/tweets/filter-realtime/overview/statuses-filter</a></p>

<hr>

<p>Otherwise, if you want to stick to the <code>search</code> method, you can query maximum of 100 tweets by adding <code>count</code> and use <code>since_id</code> on the maximum id parsed to get new tweets, you can add those attributes to the <code>search</code> method as follows:</p>

<pre><code>public_tweets = []
max_id = 0
for i in range(10): #This loop will run 10 times you can play around with that
    public_tweets.extend(api.search(q='school', count=100, since_id=max_id))
    max_id = max([tweet.id for tweet in public_tweets])

#To make sure you only got unique tweets, you can do:
unique_tweets = list({tweet._json['id']:tweet._json for tweet in public_tweets}.values())
</code></pre>

<p>This way you will have to be careful with the API's limits and you will have to handle that by enabeling <code>wait_on_rate_limit</code> attribute when you initialize the API: <code>api = tweepy.API(auth,wait_on_rate_limit=True)</code></p>
",4,3,3827,2018-01-28 07:04:57,https://stackoverflow.com/questions/48484007/alternative-to-twitter-api
NLTK | Sentiment Classifier | Issues with Install,"<p>I am having trouble installing the <code>sentiment_classifier</code>.</p>

<p>What I have currently done:</p>

<ol>
<li><code>pip install sentiment_classifier</code></li>
<li><code>python setup.py install</code></li>
<li>Downloaded <code>sentiment_classifier-0.5.tar.gz</code></li>
<li>Placed the package into my directory</li>
</ol>

<p>Error in shell:</p>

<ol>
<li><code>pip install sentiment_classifier</code>:</li>
</ol>

<p><code>Requirement already satisfied: sentiment_classifier in c:\users\ac\anaconda3\lib\site-packages</code></p>

<p><code>Requirement already satisfied: numpy in c:\users\ac\anaconda3\lib\site-packages (from sentiment_classifier)</code></p>

<p><code>Requirement already satisfied: nltk in c:\users\ac\anaconda3\lib\site-packages (from sentiment_classifier)</code></p>

<p><code>Requirement already satisfied: argparse in c:\users\ac\anaconda3\lib\site-packages (from sentiment_classifie
)</code></p>

<ol start=""2"">
<li><code>python setup.py install</code> - C:\Users\AC\Anaconda3\python.exe: can't open file 'setup.py': [Errno 2] No such file or directory</li>
</ol>

<p>When I call it in Jupyter Notebook:</p>

<p><code>from senti_classifier import senti_classifier</code></p>

<p>I get:</p>

<p>FileNotFoundError: [Errno 2] No such file or directory: 'C:\Users\AC\Anaconda3\lib\site-packages\senti_classifier\data\SentiWn.p'</p>

<p>Any help would be greatly appreciated.</p>

<p>Docs I've been referring to:</p>

<ul>
<li><a href=""https://pypi.python.org/pypi/sentiment_classifier"" rel=""nofollow noreferrer"">https://pypi.python.org/pypi/sentiment_classifier</a></li>
<li><a href=""https://stackoverflow.com/questions/41795476/sentiment-analysis-using-senti-classifier-and-nltk"">Sentiment Analysis using senti_classifier and NLTK</a></li>
<li><a href=""https://github.com/kevincobain2000/sentiment_classifier"" rel=""nofollow noreferrer"">https://github.com/kevincobain2000/sentiment_classifier</a></li>
<li><a href=""http://pythonhosted.org/sentiment_classifier/"" rel=""nofollow noreferrer"">http://pythonhosted.org/sentiment_classifier/</a></li>
</ul>

<p>Any help would be greatly appreciated.</p>
","python, python-3.x, nltk, sentiment-analysis","<p>It is missing some files needed for it to work and no those files aren't downloaded when you install the package using <code>pip</code>, you can download the repository for the library from <a href=""https://github.com/kevincobain2000/sentiment_classifier"" rel=""nofollow noreferrer"">https://github.com/kevincobain2000/sentiment_classifier</a> and then copy paste the files inside the '/src/senti_classifier/data/' into your library's directory which is 'C:\Users\AC\Anaconda3\lib\site-packages\senti_classifier\data' directory.</p>
",2,0,1204,2018-01-30 12:13:28,https://stackoverflow.com/questions/48521208/nltk-sentiment-classifier-issues-with-install
Suspiciously high accuracy in sentiment analysis model,"<p>I am building a sentiment analysis model using <code>NLTK</code> and <code>scikitlearn</code>. I have decided to test a few different classifiers in order to see which is most accurate, and eventually use all of them as a means of producing a confidence score.</p>

<p>The datasets used for this testing were all reviews, labelled as either positive or negative. </p>

<p>I trained each classifier with 5,000 reviews, 5 separate times, with 6 different (but very similar) datasets. Each test was done with a new set of 5000 reviews.</p>

<p>I averaged the accuracy for each test and dataset, to arrive at an overall mean accuracy. Take a look:</p>

<ul>
<li>Multinomial Naive Bayes: <strong>91.291%</strong> </li>
<li>Logistic Regression: <strong>96.103%</strong> </li>
<li>SVC: <strong>95.844%</strong></li>
</ul>

<p>In some tests, the accuracy was as high as 99.912%. In fact, the lowest mean accuracy for one of the datasets was 81.524%.</p>

<p>Here's a relevant code snippet:</p>

<pre><code>def get_features(comment, word_features):
    features = {}
    for word in word_features:
        features[word] = (word in set(comment))
    return features

def main(dataset_name, column, limit): 
    data = get_data(column, limit)
    data = clean_data(data)  # filter stop words

    all_words = [w.lower() for (comment, category) in data for w in comment]
    word_features = nltk.FreqDist(all_words).keys()

    feature_set = [(get_features(comment, word_features), category) for
                       (comment, category) in data]

    run = 0
    while run &lt; 5:
        random.shuffle(feature_set)

        training_set = feature_set[:int(len(data) / 2.)]
        testing_set = feature_set[int(len(data) / 2.):]

        classifier = SklearnClassifier(SVC())
        classifier.train(training_set)

        acc = nltk.classify.accuracy(classifier, testing_set) * 100.
        save_acc(acc)  # function to save results as .csv

        run += 1
</code></pre>

<p>Although I know that these kinds of classifiers can typically return great results, this seems a little too good to be true. </p>

<p><strong>What are some things that I need to check to be sure this is valid?</strong></p>
","python, scikit-learn, nltk, sentiment-analysis","<p>It's not so good if you get a range from 99,66% to 81,5%.</p>

<p>To analyze dataset in case of text classification, you can check:</p>

<ul>
<li>If the dataset is balanced? </li>
<li>Distribution words for each label, sometimes the vocabulary used for each label can be really different.</li>
<li>Positive/negative, but for the same source? Like the point before maybe if the domain is not the same, the reviews can use different expressions for a positive o negative review. This helps to get a high accuracy in several source.</li>
<li>Try with a review from different source.</li>
</ul>

<p>If after all you get that high accuracy, congrat! your get_features is really good. :)</p>
",3,0,1342,2018-02-07 11:21:58,https://stackoverflow.com/questions/48662556/suspiciously-high-accuracy-in-sentiment-analysis-model
How to use dplyr to group by column and then add another more,"<p>I have recently started training myself into sentiment analysis.I have a dataset that looks like this:<a href=""https://i.sstatic.net/ZYhV6.png"" rel=""nofollow noreferrer"">initial data</a></p>

<p>The original data consisted of reviews of wines, one per each row. What I have done is tokenize it and performed basic sentiment analysis with one of the R lexicons. As can bee seen in the screentshot. Column X refers to the original row in the initial data frame. What I want to do now is calculate the net effect( to see which is the prevailing for each row- positive or negative, however in numbers for each original row(X) and attach it as a column).
I have tried with the following code but it does not work:</p>

<pre><code>per_row &lt;- unigrams_all_ns %&gt;%
inner_join(get_sentiments(""bing""),by=c(""unigram""=""word""))%&gt;%
group_by(X)%&gt;%
spread(sentiment, n, fill = 0)
</code></pre>

<p>I get the following error </p>

<p>Error: <code>var</code> must evaluate to a single number or a column name, not a function</p>
","r, dplyr, sentiment-analysis","<p>What you want to do is to count how many positive and negative words exist for each group in <code>X</code>. You can use <code>count()</code> in the dplyr package. It seems that you want to have a wide-format data given what you tried to do. So I used <code>spread()</code>. I think you can do more from here by yourself.</p>

<pre><code>library(dplyr)
library(tidyr)
library(tidytext)

unigrams_all_ns &lt;- data.frame(X = c(1,2,2,2,2,3,3,3,4,4),
                              unigram = c(""smooth"", ""snappy"", ""dominate"", ""crisp"", ""stainless"", ""lemon"", 
                                          ""blossom"", ""opulent"", ""rough"", ""pleasantly""),
                              stringsAsFactors = FALSE)

unigrams_all_ns %&gt;%
inner_join(get_sentiments(""bing""), by =c(""unigram"" = ""word""))%&gt;%
count(X, sentiment) %&gt;%
spread(key = sentiment, value = n, fill = 0)

      X negative positive
  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
1  1.00     0        1.00
2  2.00     0        4.00
3  3.00     1.00     2.00
4  4.00     1.00     1.00
</code></pre>
",1,0,263,2018-02-09 09:47:18,https://stackoverflow.com/questions/48702946/how-to-use-dplyr-to-group-by-column-and-then-add-another-more
&#39;gensim.models.doc2vec&#39; has no attribute &#39;LabeledSentence&#39;,"<p>I am using gensim in python 3 as shown within image below </p>

<p><a href=""https://i.sstatic.net/xMbap.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xMbap.png"" alt="" Image""></a></p>

<p>In line no 11 I am getting the following error:</p>

<p><a href=""https://i.sstatic.net/NoGO4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NoGO4.png"" alt=""Image""></a></p>
","python-3.x, sublimetext3, sentiment-analysis, gensim","<p>It's a deprecated import:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/issues/1886"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/1886</a></p>

<p>if you want to use LabeledSentenced you must import it from the deprecated section:</p>

<pre><code>from gensim.models.deprecated.doc2vec import LabeledSentence
</code></pre>

<p>So you have to do this:          </p>

<pre><code>LabeledSentence = gensim.models.deprecated.doc2vec.LabeledSentence
</code></pre>
",2,0,4001,2018-02-17 15:28:11,https://stackoverflow.com/questions/48842866/gensim-models-doc2vec-has-no-attribute-labeledsentence
Streaming twitter data and saving to csv in Python,"<p>I’m trying to follow this <a href=""https://www.dataquest.io/blog/streaming-data-python/"" rel=""nofollow noreferrer"">tutorial</a> on streaming tweets from twitter into a database, before then converting the database to a CSV file. I can implement the streaming API feature to capture data fine, however when trying to save the data in a csv file I am having problems. </p>

<p>At first I encountered a similar problem to <a href=""https://stackoverflow.com/questions/48090148/no-freeze-attribute-when-using-dataset-module-in-python"">this question</a> where I receive the following error.</p>

<pre><code>    AttributeError: module 'dataset' has no attribute 'freeze'
</code></pre>

<p>The solution removes the error when running the script but I am left with no CSV file and empty DB file instead.</p>

<p>I have looked over all the documentation but I'm really unsure on what I am doing wrong and how I can proceed. </p>

<p><a href=""https://github.com/andypejo/TweetScrape"" rel=""nofollow noreferrer"">My GitHub with all source code can be found here</a></p>
","python, python-3.x, twitter, export-to-csv, sentiment-analysis","<p>first of all I think that storing your tweets from stream in CSV file is a bad idea, and use MongoDB or another database where you will store your data. </p>

<p>Good example how you can <strong><em>organize streaming and storing tweets in database</em></strong>  you can find on <a href=""https://github.com/SamDelgado/twitter-to-mongo"" rel=""nofollow noreferrer"">this github repository</a> </p>

<p>Before inserting tweet to database, <strong><em>it's better to calculate sentiment score for tweet in advance</em></strong>. Sentiment analysis can be done by using <a href=""https://github.com/cjhutto/vaderSentiment"" rel=""nofollow noreferrer"">VADER</a> sentiment analysis tool.  </p>

<p>Good luck)</p>
",1,1,1128,2018-02-19 14:54:39,https://stackoverflow.com/questions/48868713/streaming-twitter-data-and-saving-to-csv-in-python
I use python 3.6 and i want to make sentiment analysis but i have an error in nltk.metrics package?,"<p>i write the code as here : </p>

<pre><code>    print ('pos precision:', nltk.metrics.precision(refsets['pos'], 
    testsets['pos']))
    print ('pos recall:', nltk.metrics.recall(refsets['pos'], 
    testsets['pos']))
</code></pre>

<p>and the output as here : </p>

<pre><code>   line 35, in evaluate_classifier
   print ('pos precision:', nltk.metrics.precision(refsets['pos'], 
   testsets['pos']))
   AttributeError: module 'nltk.translate.metrics' has no attribute 
  'precision'
</code></pre>

<p>How can i solve this error ? </p>
","python, nltk, precision, metrics, sentiment-analysis","<p>No big deal, just not calling the correct method.</p>

<p>Try: <code>nltk.metrics.scores.precision(reference, test)</code></p>

<p><a href=""http://www.nltk.org/api/nltk.metrics.html"" rel=""nofollow noreferrer"">http://www.nltk.org/api/nltk.metrics.html</a></p>

<p>ctrl+f for ""precision"" will get you to documentation</p>

<p>Corrected code:
<code>
print('pos precision:', nltk.metrics.scores.precision(refsets['pos'],testsets['pos']))
print('pos recall:', nltk.metrics.scores.recall(refsets['pos'],testsets['pos']))
</code></p>
",0,1,431,2018-02-19 18:59:34,https://stackoverflow.com/questions/48872564/i-use-python-3-6-and-i-want-to-make-sentiment-analysis-but-i-have-an-error-in-nl
Sentiment Analysis in R with tidyverse package - object &#39;sentiment&#39; not found,"<p>I am trying to reproduce this exmple of sentiment analysis: <a href=""https://www.kaggle.com/rtatman/tutorial-sentiment-analysis-in-r"" rel=""nofollow noreferrer"">https://www.kaggle.com/rtatman/tutorial-sentiment-analysis-in-r</a></p>

<p>I have a ""file.txt"" with the text I want to analyze in ""../input"" folder.</p>

<p><code>library(tidyverse)</code><br>
<code>library(tidytext)</code><br>
<code>library(glue)</code><br>
<code>library(stringr)</code><br>
<code>library(dplyr)</code><br>
<code>require(plyr)</code>  </p>

<p><code># get a list of the files in the input directory</code><br>
<code>files &lt;- list.files(""../input"")</code><br>
<code>fileName &lt;- glue(""../input/"", files[1], sep = """")</code><br>
<code>fileName &lt;- trimws(fileName)</code><br>
<code>fileText &lt;- glue(read_file(fileName))</code><br>
<code>fileText &lt;- gsub(""\\$"", """", fileText)</code><br>
<code>tokens &lt;- data_frame(text = fileText) %&gt;% unnest_tokens(word, text)</code>  </p>

<p>but after this line</p>

<pre><code>#get the sentiment from the first text: 
tokens %&gt;%
  inner_join(get_sentiments(""bing"")) %&gt;% # pull out only sentiment words
  count(sentiment) %&gt;% # count the # of positive &amp; negative words
  spread(sentiment, n, fill = 0) %&gt;% # made data wide rather than narrow
  mutate(sentiment = positive - negative) # # of positive words - # of negative owrds
</code></pre>

<p>I get an error message </p>

<blockquote>
  <p>Error in count(., sentiment) : object 'sentiment' not found</p>
</blockquote>

<p>Yesterday the same code worked fine, and today I get this error. It appears the problem is cause by <code>plyr</code> package. It seemed to work fine when <code>plyr</code> was loaded before <code>dplyr</code>, but now gives an error even if they are loaded in that order.</p>
","r, sentiment-analysis, tidytext","<p>The problem was caused by <code>plyr</code> package being loaded together with <code>dplyr</code>. I used <a href=""https://stackoverflow.com/questions/5564564/r-2-functions-with-the-same-name-in-2-different-packages"">this approach</a> to use <code>plyr</code> without loading it and the code runs without any errors now.</p>
",1,1,2059,2018-02-20 02:22:51,https://stackoverflow.com/questions/48876959/sentiment-analysis-in-r-with-tidyverse-package-object-sentiment-not-found
Python IndexError: list index out of range with CSV file,"<p>I am trying to read the contents of a .CSV file of data from Twitter to perform some sentiment analysis. The file has four columns which should be pulled but I am having some trouble with the following block of code:</p>

<pre><code>tweets = []

with open('tweets.csv','r', encoding = 'utf-8', newline='') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    next(reader)
    for row in reader:
        tweet = dict()
        tweet['user'] = row[0]
        tweet['date'] = row[1]
        tweet['type'] = row[2]
        tweet['orig'] = row[3]
</code></pre>

<p>When running the script I receive the error</p>

<pre><code>    Traceback (most recent call last):
    File ""analysis.py"", line 46, in &lt;module&gt;
    tweet['user'] = row[0]
    IndexError: list index out of range
</code></pre>

<p>Looking around the sight I see this is a common problem but as I'm fairly new to Python I'm unsure how to implement a fix, or what I'm doing wrong.</p>

<p><strong>EDIT</strong> I have discovered if I manually go into the .CSV file and delete the empty lines between rows, everything works fine. So I guess the question is, how would I best implement this?</p>
","python-3.x, csv, twitter, sentiment-analysis","<p>Amended code the the following and problem solved.</p>

<pre><code>    for row in reader:
        if (len(row) == 4):
            tweet = dict()
            tweet['user'] = row[0]
            tweet['date'] = row[1]
            tweet['type'] = row[2]
            tweet['orig'] = row[3]
</code></pre>
",1,0,2069,2018-02-20 13:15:39,https://stackoverflow.com/questions/48886181/python-indexerror-list-index-out-of-range-with-csv-file
how to solve this error with lambda and sorted method when i try to make sentiment analysis (POS or NEG text)?,"<p>Input code:   </p>

<p><code>best = sorted(word_scores.items(), key=lambda w, s: s, reverse=True)[:10000]</code></p>

<p>Result:  </p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\Sarah\Desktop\python\test.py"", line 78, in &lt;module&gt;
best = sorted(word_scores.items(), key=lambda w, s: s, reverse=True)[:10000]
TypeError: &lt;lambda&gt;() missing 1 required positional argument: 's'
</code></pre>

<hr>

<p>How do I solve it?</p>
","python-3.x, nltk, sentiment-analysis, feature-extraction, text-classification","<p>If I've understood the format of your <code>word_scores</code> dictionary correctly (that the keys are words and the values are integers representing scores), and you're simply looking to get an ordered list of words with the highest scores, it's as simple as this:</p>

<pre><code>best = sorted(word_scores, key=word_scores.get, reverse=True)[:10000]
</code></pre>

<p>If you want to use a lambda to get an ordered list of tuples, where each tuple is a word and a score, and they are ordered by score, you can do the following:</p>

<pre><code>best = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:10000]
</code></pre>

<p>The difference between this and your original attempt is that I have passed one argument (x) to the lambda, and x is a tuple of length 2 - x[0] is the word and x[1] is the score. Since we want to sort by score, we use x[1].</p>
",0,0,1272,2018-02-23 11:07:57,https://stackoverflow.com/questions/48946458/how-to-solve-this-error-with-lambda-and-sorted-method-when-i-try-to-make-sentime
N grams for Sentiment Analysis,"<p>I am doing sentiment analysis on reviews of products from various retailers. I was wondering if there was an API that used n grams for sentiment analysis to classify a review as a positive or negative. I have a CSV file filled with reviews which I would like to run it in python and hence would like an API or a package rather than a tool.
Any direction towards this would be great. </p>

<p>Thanks</p>
","python, nltk, sentiment-analysis, n-gram","<p>Use textblob package.  It offers a simple API to access its methods and perform basic NLP tasks.  NLP is natural language processing. Which process your text by  tokenization, noun extract, lemmatization, words inflection, NGRAMS etc. There also some other packages like spacy, nltk.  But textblob will be better for beginners.</p>
",0,0,530,2018-02-24 10:31:40,https://stackoverflow.com/questions/48961822/n-grams-for-sentiment-analysis
Sentiment analysis using bigrams,"<p>So I have some reviews that I am trying to classify into positive or negative. I am trying to use NLTK and Stanford coreNLP to do so. I am able to do it on unigrams but its not working for bigrams. I tried the following for bigrams</p>

<pre><code>def classifySentence(sen):
  wn_lem = WordNetLemmatizer()
  pos = 0
  neg = 0
  stop_words = set(stopwords.words('english'))
  filtered_review = [token for token in nltk.word_tokenize(sen) if not token in stop_words]


  for token in nltk.bigrams(filtered_review):
      #lemma = wn_lem.lemmatize(token)
      # print(""lemma=""+token)
      if len(wn.synsets(token))&gt;0:
          synset = wn.synsets(token)[0]
          #print(""synset.name=""+synset.name())

          sent = swn.senti_synset(synset.name())

          #print(""Sentiment of ""+token+"" ""+str(sent))

          pos = pos + sent.pos_score()
          neg = neg + sent.neg_score()
          # print (token + ""(pos_score): "" + str(pos) +""\n"")
          # print (token + ""(neg_score): "" + str(neg) +""\n"")
  #print (filtered_review)
  JoinedTokens = ' '.join(wo for wo in filtered_review)
  return [JoinedTokens, pos, neg]
</code></pre>

<p>I was wondering if someone could suggest me ways to do this. I would like to use NLTK or can also use stanfordcoreNLP. I am also open to using other python packages but just need some guidance
I have written some code for using it but it didn't work either. The code I wrote</p>

<pre><code>def StanfordBigrams():
  nlp = StanfordCoreNLP('http://localhost:9000')
  operations = {'annotators': 'tokenize,lemma,pos,sentiment', 'outputFormat': 'json'}
  string = ""not bad""
  tok = nltk.word_tokenize(string)
  bigrams = nltk.bigrams(tok)
  res = nlp.annotate(str(bigrams),operations)
  for s in res[""sentences""]: 
          for token in s[""tokens""]:
              print(""Sentiment: ""+str(s[""sentiment""])+""SentimentValue: ""+str(s[""sentimentValue""]))
              print (token)
</code></pre>

<p>I would be grateful if someone could point me in the right direction.</p>
","python, nltk, stanford-nlp, sentiment-analysis","<p>Are you training a sentiment classifier, or just trying to use one? Technically, I suspect your error is in <code>wn.synset(bigram)</code> -- I doubt the thing returned from <code>nltk.bigrams</code> is a word that can be passed into WordNet.</p>

<p>But, more importantly, you probably want to pass your whole sentence into a sentiment classifier -- bigrams aren't going to have sentiments annotated on them in things like SentiWordNet, and the trained sentiment classifiers are going to have a much easier time on sentences than they are on short snippets. You should be able to get sentiment for <em>some</em> of the bigrams in the sentence from Stanford's sentiment tree (vs just the sentiment value at the root). See the <code>sentimentTree</code> field on the JSON output from the CoreNLP server.</p>
",0,2,1898,2018-02-24 13:24:20,https://stackoverflow.com/questions/48963354/sentiment-analysis-using-bigrams
What separators does the hive ngram UDF use to tokenize?,"<p>I am performing some sentiment analysis.</p>

<p>I need to count the vocabulary (distinct words) in within a text.</p>

<p>The ngram UDF seems to do a great job at determining the unigrams. I want to know what separators it uses to determine the unigrams/ tokens. This is important if I want to mimic the vocabulary count using the split UDF instead. For example, given the following text (a product review)</p>

<blockquote>
  <p>I was aboslutely shocked to see how much 1 oz really was. At $7.60, I mistakenly assumed it would be a decent sized can. As locally I am able to buy a medium sized tube of wasabi paste for around $3, but never used it fast enough so it would get old. I figured a powder would be better, so I can mix it as I needed it. When I opened the box and dug thru the packing and saw this little little can, I started looking for the hidden cameras ...  thought this HAD to be a joke. Nope .. and it's NOT returnable either. SO I HAVE LEARNED MY LESSON. Please just be aware if you should decide you want this EXPENSIVE wasabi powder.</p>
</blockquote>

<p>The ngram UDG counts 82 unigrams/ tokens</p>

<pre><code>SELECT count(*) FROM 
(SELECT explode(ngrams(sentences(upper(reviewtext)),1,9999999))  
FROM  amazon.Food_review_part_small WHERE asin = 'B0000CNU1X' AND reviewerid ='A1UCAVBNJUZMPR') t;
82
</code></pre>

<p>However, using the split UDF with space, comma, period, hyphen and double quotation marks as separators, there are 85 unigrams/tokens</p>

<pre><code>select  count(distinct(te)) FROM amazon.Food_review_part_small 
lateral view explode(split(upper(reviewtext), '[\\s,.-]|\""')) t as te
WHERE te &lt;&gt; '' AND asin = 'B0000CNU1X' AND reviewerid ='A1UCAVBNJUZMPR';
85
</code></pre>

<p>Of course there is little to no documentation that i can find. Does anyone know what separators the ngram UDF uses to determine unigram tokens ?</p>
","split, hive, nlp, sentiment-analysis, n-gram","<p>the UDAF ngram does  not split the data, actually it already expect an array of string or an array of array of strings as input. The UDF <a href=""https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFSentences.java"" rel=""nofollow noreferrer"">sentences</a> split the data in this case, From the java comments:</p>

<pre><code>+ ""Unnecessary punctuation, such as periods and commas in English, is automatically stripped.""
+ "" If specified, 'lang' should be a two-letter ISO-639 language code (such as 'en'), and ""
+ ""'country' should be a two-letter ISO-3166 code (such as 'us'). Not all country and ""
+ ""language codes are fully supported, and if an unsupported code is specified, a default ""
</code></pre>

<p>If you run the following query</p>

<pre><code>select sentences(""I was aboslutely shocked to see how much 1 oz really was. At $7.60, I mistakenly assumed it would be a decent sized can. As locally I am able to buy a medium sized tube of wasabi paste for around $3, but never used it fast enough so it would get old. I figured a powder would be better, so I can mix it as I needed it. When I opened the box and dug thru the packing and saw this little little can, I started looking for the hidden cameras ... thought this HAD to be a joke. Nope .. and it's NOT returnable either. SO I HAVE LEARNED MY LESSON. Please just be aware if you should decide you want this EXPENSIVE wasabi powder."");
</code></pre>

<p>you will get the following result</p>

<pre><code>[[""I"",""was"",""aboslutely"",""shocked"",""to"",""see"",""how"",""much"",""1"",""oz"",""really"",""was""],[""At"",""I"",""mistakenly"",""assumed"",""it"",""would"",""be"",""a"",""decent"",""sized"",""can""],[""As"",""locally"",""I"",""am"",""able"",""to"",""buy"",""a"",""medium"",""sized"",""tube"",""of"",""wasabi"",""paste"",""for"",""around"",""but"",""never"",""used"",""it"",""fast"",""enough"",""so"",""it"",""would"",""get"",""old""],[""I"",""figured"",""a"",""powder"",""would"",""be"",""better"",""so"",""I"",""can"",""mix"",""it"",""as"",""I"",""needed"",""it""],[""When"",""I"",""opened"",""the"",""box"",""and"",""dug"",""thru"",""the"",""packing"",""and"",""saw"",""this"",""little"",""little"",""can"",""I"",""started"",""looking"",""for"",""the"",""hidden"",""cameras"",""thought"",""this"",""HAD"",""to"",""be"",""a"",""joke""],[""Nope"",""and"",""it's"",""NOT"",""returnable"",""either""],[""SO"",""I"",""HAVE"",""LEARNED"",""MY"",""LESSON""],[""Please"",""just"",""be"",""aware"",""if"",""you"",""should"",""decide"",""you"",""want"",""this"",""EXPENSIVE"",""wasabi"",""powder""]]
</code></pre>

<p>as you can see, the udf sentences is removing some ""noise"" like ""$7.60"", ""$3"" as empty string as well.</p>
",0,0,165,2018-02-28 03:58:52,https://stackoverflow.com/questions/49021741/what-separators-does-the-hive-ngram-udf-use-to-tokenize
Pandas - store numpy array in a dataframe column which is a result of function,"<p>I have a pandas dataframe with a column <code>allTexts</code> which stores a bunch of text information for each row. I am trying to apply a custom function which returns 3 values given the input text. I then want to store these 3 output values in a new dataframe column -  ideally as a numpy array for each row. I do it with <code>apply()</code>, the code completes successfully but it doesn't actually change values.</p>

<pre><code>#stub for creating a dataframe
df = pd.DataFrame({'allText':['Hateful text. This is bad', 'Text about great stuff', ' ']})

#set a placeholder - just 3 zeros for each record
df['Sentiments'] = df['allText'].apply(lambda x: np.zeros(3))

#function definition. It is a textblob library function, which gives me back sentiment scores for each text
def getTextSentiments(text):
    blob = TextBlob(text)
    pos = 0
    neg = 0
    neutral = 0
    count = 0
    for sentence in blob.sentences:
        sentiment = sentence.sentiment.polarity
        if sentiment &gt; 0.1:
            pos +=1
        elif sentiment &gt; -0.1:
            neutral +=1
        else:
            neg +=1
        count+=1
    if count == 0:
        count = 1
    return numpy.array([pos/count, neutral/count, neg/count])

#apply function only for non-empty texts and override 3 zeros in sentiments column with real 3 values
df[df[""allText""]!="" ""]['Sentiments'] = df[df[""allText""]!="" ""][""allText""].apply(getTextSentiments)
</code></pre>

<p>After this code completes without any error I still end up with same value of all zeros in my Sentiments column.</p>

<p>MVP to demonstrate it doesn't work even with single record:</p>

<pre><code>df[df[""allText""]!="" ""].iloc[0]['Sentiments']
array([ 0.,  0.,  0.])
test = getTextSentiments(df[df[""allText""]!="" ""].iloc[0]['allText'])

test
Out[64]: (0.4166666666666667, 0.5, 0.08333333333333333)
df[df[""allText""]!="" ""].iloc[0]['Sentiments'] = test

df[df[""allText""]!="" ""].iloc[0]['Sentiments']
Out[75]: array([ 0.,  0.,  0.])
</code></pre>

<p>Any advice on what am I doing wrong?</p>
","python, pandas, sentiment-analysis, textblob","<p>Can you try the following?</p>

<pre><code>df.Sentiments = df.apply(lambda x: x.Sentiments if x.allText ==' ' else getTextSentiments(x.allText), axis=1)
</code></pre>

<p>Using a dummy getTextSentiments function for test:</p>

<pre><code>df = pd.DataFrame({'allText':['Hateful text. This is bad', 'Text about great stuff', ' ']})

#set a placeholder - just 3 zeros for each record
df['Sentiments'] = df['allText'].apply(lambda x: np.zeros(3))
def getTextSentiments(text):
    return (0.4166666666666667, 0.5, 0.08333333333333333)
df.Sentiments = df.apply(lambda x: x.Sentiments if x.allText ==' ' else getTextSentiments(x.allText), axis=1)
df
Out[181]: 
                     allText                                      Sentiments
Out[181]: 
                     allText                                      Sentiments
0  Hateful text. This is bad  (0.4166666666666667, 0.5, 0.08333333333333333)
1     Text about great stuff  (0.4166666666666667, 0.5, 0.08333333333333333)
2                                                            [0.0, 0.0, 0.0]
</code></pre>
",1,1,530,2018-03-04 03:58:52,https://stackoverflow.com/questions/49091835/pandas-store-numpy-array-in-a-dataframe-column-which-is-a-result-of-function
why naive bayes requires balanced training data?,"<p>I created a word sentiment app using the Naive Bayes algorithm.</p>

<p>There are two types of criteria in this classification training data, that is positive training data and negative training data. I take a unique word on every training data that has been grouped. so, I have all the unique words for each data criteria. Then, I calculate the probability value of occurrence of each unique word.</p>

<p>The problem is when I use uneven training data. For example: I use 60% of negative training data and 40% positive training data. Then the results of test data will be more likely to negative results, and vice versa.</p>

<p>Besides I have to use balanced data, what should I do to solve this problem? and is there an additional method I should add?</p>
","artificial-intelligence, text-processing, sentiment-analysis, naivebayes","<p>Naive Bayes requires balanced training data because the likelihood of each parameter is influenced by the prior value (priority value).</p>

<p>this prior value is taken from the classes of each data.
maybe you already understand when I explain this kind of thing.</p>
",1,-1,239,2018-03-05 14:54:45,https://stackoverflow.com/questions/49113192/why-naive-bayes-requires-balanced-training-data
how to close sys.stdout in a nested loop so that it doesnt copy print statements outside inner loop in file,"<p>I am new to python 3 and I'm working on sentiment analysis of tweets. My code begins with a for loop that takes in 50 tweets, which i clean and pre-process. After this (still inside the for loop) i want to save each tweet in a text file(every tweet on a new line)
Here's how the code goes:</p>

<pre><code>    for loop:
        ..
        print statments
        ..
        if loop:
            filename=open(""withnouns.txt"",""a"")
            sys.stdout = filename
            print(new_words)#tokenised tweet that i want to save in txt file
            print(""\n"")
            sys.stdout.close()#i close it because i dont want to save print statements OUTSIDE if loop to be saved in txt file
        ..
        ..
        print statements
</code></pre>

<p>After running this its showing error: I/O operation on closed file on line 71 (the first print statement after if loop)</p>

<p>My question is, is there any way I can temporarily close and then open <code>sys.stdout</code> and have it active only inside the if loop?</p>
","python, python-3.x, file-handling, sentiment-analysis","<p>I'm not sure if this is exactly what you want to do but you can change this</p>

<pre><code>filename=open(""withnouns.txt"",""a"")
sys.stdout = filename
print(new_words)
print(""\n"")
sys.stdout.close()
</code></pre>

<p>to</p>

<pre><code>filename=open(""withnouns.txt"",""a"")
filename.write(new_words + ""\n"")
filename.write(""\n\n"")
filename.close()
</code></pre>

<p>alternatively, you can get the original value of <code>sys.stdout</code> from <code>sys.__stdout__</code>, so your code becomes</p>

<pre><code>filename=open(""withnouns.txt"",""a"")
sys.stdout = filename
print(new_words)
print(""\n"")
filename.close()
sys.stdout = sys.__stdout__
</code></pre>
",0,1,845,2018-03-16 18:45:36,https://stackoverflow.com/questions/49327630/how-to-close-sys-stdout-in-a-nested-loop-so-that-it-doesnt-copy-print-statements
RuntimeError: No active exception to reraise with tweepy,"<p>I am using a python code for the live sentiment analysis based on twitter.
It should work because there is a tutorial too on youtube, but on my computer, this is the error that I have:</p>

<pre><code>File ""&lt;ipython-input-2-9dc468222105&gt;"", line 1, in &lt;module&gt;
         runfile('C:/Users/marco/Anaconda3/envs/coinlive/social_functions.py',
 wdir='C:/Users/marco/Anaconda3/envs/coinlive')

File ""C:\Users\marco\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"",
 line 710, in runfile
         execfile(filename, namespace)

File ""C:\Users\marco\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"",
 line 101, in execfile
         exec(compile(f.read(), filename, 'exec'), namespace)

File ""C:/Users/marco/Anaconda3/envs/coinlive/social_functions.py"", line 82,
 in &lt;module&gt;
         twitterStream.filter(track=[""Donald Trump""])

File ""C:\Users\marco\Anaconda3\lib\site-packages\tweepy\streaming.py"", line
 450, in filter
         self._start(async)

File ""C:\Users\marco\Anaconda3\lib\site-packages\tweepy\streaming.py"", line
 364, in _start
         self._run()

File ""C:\Users\marco\Anaconda3\lib\site-packages\tweepy\streaming.py"", line
 297, in _run
         six.reraise(*exc_info)

File ""C:\Users\marco\Anaconda3\lib\site-packages\six.py"", line 693, in reraise
         raise value

File ""C:\Users\marco\Anaconda3\lib\site-packages\tweepy\streaming.py"", line
 266, in _run
         self._read_loop(resp)

File ""C:\Users\marco\Anaconda3\lib\site-packages\tweepy\streaming.py"", line
 327, in _read_loop
         self._data(next_status_obj)

File ""C:\Users\marco\Anaconda3\lib\site-packages\tweepy\streaming.py"", line
 300, in _data
         if self.listener.on_data(data) is False:

File ""C:/Users/marco/Anaconda3/envs/coinlive/social_functions.py"", line 39,
 in on_data
         tweet="" "".join(re.findall(""[a-zA-Z]+"", tweet))

File ""C:\Users\marco\Anaconda3\lib\re.py"", line 222, in findall
         return _compile(pattern, flags).findall(string)

TypeError: cannot use a string pattern on a bytes-like object
</code></pre>

<p>I think the error is related to tweepy library and I read many posts but withotu any good result.</p>

<p>This is my code:</p>

<pre><code>import time
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json
from textblob import TextBlob
import matplotlib.pyplot as plt
import re

""# -- coding: utf-8 --""

def calctime(a):
    return time.time()-a

positive=0
negative=0
compound=0

count=0
initime=time.time()
plt.ion()

ckey='R8adMtTxKPXseiFqFcb7XBGJv'
csecret='rkcxNsg8Q09AiVDgh4bn5GXNpsLP0jLwekqIOkrdkwa1K1h9oc'
atoken='232118221-5SPjlFvC22JBRXODNdNWoEDJwvpvaiKoXAazpAHH'
asecret='rzZ1NMxfgK6IYzTuEI0rMvpK04lJj49tiKe1BaST9bmcT'

class listener(StreamListener):

    def on_data(self,data):
        global initime
        t=int(calctime(initime))
        all_data=json.loads(data)
        tweet=all_data[""text""].encode(""utf-8"")
        #username=all_data[""user""][""screen_name""]
        tweet="" "".join(re.findall(""[a-zA-Z]+"", tweet))
        blob=TextBlob(tweet.strip())

        global positive
        global negative     
        global compound  
        global count

        count=count+1
        senti=0
        for sen in blob.sentences:
            senti=senti+sen.sentiment.polarity
            if sen.sentiment.polarity &gt;= 0:
                positive=positive+sen.sentiment.polarity   
            else:
                negative=negative+sen.sentiment.polarity  
        compound=compound+senti        
        print(count)
        print(tweet.strip())
        print(senti)
        print(t)
        print(str(positive) + ' ' + str(negative) + ' ' + str(compound))


        plt.axis([ 0, 70, -20,20])
        plt.xlabel('Time')
        plt.ylabel('Sentiment')
        plt.plot([t],[positive],'go',[t] ,[negative],'ro',[t],[compound],'bo')
        plt.show()
        plt.pause(0.0001)
        if count==200:
            return False
        else:
            return True

    def on_error(self,status):
        print(status)


auth=OAuthHandler(ckey,csecret)
auth.set_access_token(atoken,asecret)

twitterStream=  Stream(auth, listener(count))
twitterStream.filter(track=[""Donald Trump""])
</code></pre>
","python, twitter, tweepy, sentiment-analysis","<p>You are encoding the string</p>

<pre><code>tweet=all_data[""text""].encode(""utf-8"")
</code></pre>

<p>and then try to run re.findall on it:</p>

<pre><code>tweet="" "".join(re.findall(""[a-zA-Z]+"", tweet))
</code></pre>

<p>Try without the encode call</p>
",1,0,1469,2018-03-20 08:41:07,https://stackoverflow.com/questions/49379615/runtimeerror-no-active-exception-to-reraise-with-tweepy
text mining with R: how to see positive-negative sentiments in my document?,"<p>I am new to R. I have found the number of positive-negative words (953 negative, 458 positive) in my document, but I want to see these words. How can I do it? </p>

<pre><code>library(readr)
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
library(dplyr)
davos &lt;- read_file(""davos.txt"")
fileText &lt;- glue(read_file(davos))
fileText &lt;- gsub(""\\$"", """", fileText)
tokens &lt;- data_frame(text = fileText) %&gt;% unnest_tokens(word, text)
tokens %&gt;% inner_join(get_sentiments(""bing"")) %&gt;% count(sentiment) %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment = positive - negative)

# Joining, by = ""word""
#
# # A tibble: 1 x 3
#     negative positive sentiment
#        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
#   1     953.     458.     -495.
</code></pre>
","r, text-mining, sentiment-analysis","<p>Something like this ?</p>

<pre><code>library(tidy_text)
fileText &lt;- ""Sometimes I'm happy, sometimes I'm sad""
tokens &lt;- data_frame(text = fileText) %&gt;% unnest_tokens(word, text)
tokens %&gt;%
  inner_join(get_sentiments(""bing""))

# # A tibble: 2 x 2
#    word sentiment
#   &lt;chr&gt;     &lt;chr&gt;
# 1 happy  positive
# 2   sad  negative
</code></pre>
",3,0,849,2018-03-24 12:58:28,https://stackoverflow.com/questions/49464958/text-mining-with-r-how-to-see-positive-negative-sentiments-in-my-document
Speed up annotation time in CoreNLP sentiment,"<p>In my dataset I have 100,000 text files, and I am trying to process them with CoreNLP. The desired result is 100,000 finished text file results, which has classified each sentence as having either a positive, negative or neutral sentiment. 
To get from one text file to annother text file, I use the CoreNLP jar file, which is used from the command line below.    </p>

<pre><code> java -cp ""*"" -mx5g edu.stanford.nlp.sentiment.SentimentPipeline -fileList list.txt
</code></pre>

<p>It takes a very long time to do this, since I can't get the model to take every file in the filelist, but it will take the single path lines as input into the model. </p>

<p>I have furthermore tried to implement some of the other approaches in this link, but I can't get the result need from these.
<a href=""https://stanfordnlp.github.io/CoreNLP/cmdline.html#classpath"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/cmdline.html#classpath</a></p>

<p>Is there a better and faster way to do this and speed up the process?</p>
","java, command-line, nlp, stanford-nlp, sentiment-analysis","<p>Try this command:</p>

<pre><code>java -Xmx14g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse,sentiment -parse.model edu/stanford/nlp/models/srparser/englishSR.ser.gz -outputFormat text -filelist list.txt 
</code></pre>

<p>It will use the faster shift-reduce parser.  This will run through each file in <code>list.txt</code> (1 file per line) and process it.</p>
",1,1,120,2018-03-27 12:43:08,https://stackoverflow.com/questions/49513178/speed-up-annotation-time-in-corenlp-sentiment
What algorithm that was used on the sentiment analysis code in Python?,"<p>I have a question regarding on the sentiment analysis. I have a data contains on tweets(Cryptocurrency). I'm planning to do sentiment analysis to get the positive and negative result for each tweets. </p>

<p>I found good sentiment analysis code, but because i'm new to to this area. I don't know what  classification algorithm that was used on this. Here's the code:</p>

<pre><code># importing Libraries
from pandas import DataFrame, read_csv
import chardet
import matplotlib.pyplot as plt; plt.rcdefaults()
from matplotlib import rc
%matplotlib inline
import pandas as pd
plt.style.use('ggplot')
import numpy as np
import re
import warnings

#Visualisation
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from IPython.display import display
from mpl_toolkits.basemap import Basemap
from wordcloud import WordCloud, STOPWORDS

#nltk
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.sentiment.util import *
from nltk import tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords
stop = stopwords.words('english')


matplotlib.style.use('ggplot')
pd.options.mode.chained_assignment = None
warnings.filterwarnings(""ignore"")

%matplotlib inline



#########Sentiment Analysis code########

tweets['text_lem'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in tweets['text']]       
vectorizer = TfidfVectorizer(max_df=0.5,max_features=10000,min_df=10,stop_words='english',use_idf=True)
X = vectorizer.fit_transform(tweets['text_lem'].str.upper())
sid = SentimentIntensityAnalyzer()
tweets['sentiment_compound_polarity']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['compound'])
tweets['sentiment_neutral']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neu'])
tweets['sentiment_negative']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neg'])
tweets['sentiment_pos']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['pos'])
tweets['sentiment_type']=''
tweets.loc[tweets.sentiment_compound_polarity&gt;0,'sentiment_type']='POSITIVE'
tweets.loc[tweets.sentiment_compound_polarity==0,'sentiment_type']='NEUTRAL'
tweets.loc[tweets.sentiment_compound_polarity&lt;0,'sentiment_type']='NEGATIVE'
</code></pre>

<p>Can anyone tell me more information about sentiment Analysis code?
What algorithm  that was used?</p>
","python, algorithm, pandas, classification, sentiment-analysis","<p>The classifier in this code is a <code>SentimentIntensityAnalyser()</code>. The <a href=""http://www.nltk.org/api/nltk.sentiment.html"" rel=""nofollow noreferrer"">documentation</a> indicates that it could be a NaiveBayesClassifier.</p>

<p>If you access the original paper <a href=""https://www.researchgate.net/publication/312176414_Sentiment_Analysis_in_Python_using_NLTK"" rel=""nofollow noreferrer"">here</a> they also mention the NaiveBayesClassifier.</p>

<p>However, from the <a href=""https://github.com/cjhutto/vaderSentiment"" rel=""nofollow noreferrer"">github project</a>, the authors indicate:</p>

<blockquote>
  <p>The Python code for the rule-based sentiment analysis engine. Implements the grammatical and syntactical rules described in the paper, incorporating empirically derived quantifications for the impact of each rule on the perceived intensity of sentiment in sentence-level text.</p>
</blockquote>

<p>Thus the algorithm in your code is a rule-based algorithm, not a machine learning algorithm. The code is <a href=""https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vaderSentiment.py"" rel=""nofollow noreferrer"">here</a>.</p>

<h2>Testing the library</h2>

<p>Using the code from the paper:</p>

<pre><code>hate_comments = ['I second that emotion! I can\'t understand how any decent human being could support them  considering their ongoing loathsome record. #ToriesOut2018 #NHSCrisis #CambridgeAnalytica',
             'Think we’d just share the ladder, Mikey pal. Nationalise all of the ladders and have a big old ladder party.',
             'The Tories, young and old, do not understand that where child poverty, homelessness and the destruction of the NHS are concerned, there is absolutely nothing to smile about. Well done Lara.',
             'I don\'t even like them!',
             'Boom! Get in......',
             'Me too',
             'That\'s fine, but do it with a smile.',
             'Yesss girl',
             'Me too!',
             'Ditto..',
             'one day she will be all grown up .. ah bless',
             'Who doesn\'t.',
             'I hate them too Lara'
              ]

for sentence in hate_comments:
    print(sentence)
    ss = sid.polarity_scores(sentence)
    for k in ss:
        print('{0}: {1}, '.format(k, ss[k]), end='')
        print() 
</code></pre>

<p>[out]:</p>

<pre><code>    I second that emotion! I can't understand how any decent human being could support them  considering their ongoing loathsome record. #ToriesOut2018 #NHSCrisis #CambridgeAnalytica
neg: 0.0, 
neu: 0.87, 
pos: 0.13, 
compound: 0.4574, 
Think we’d just share the ladder, Mikey pal. Nationalise all of the ladders and have a big old ladder party.
neg: 0.0, 
neu: 0.776, 
pos: 0.224, 
compound: 0.5994, 
The Tories, young and old, do not understand that where child poverty, homelessness and the destruction of the NHS are concerned, there is absolutely nothing to smile about. Well done Lara.
neg: 0.244, 
neu: 0.702, 
pos: 0.055, 
compound: -0.806, 
I don't even like them!
neg: 0.445, 
neu: 0.555, 
pos: 0.0, 
compound: -0.3404, 
Boom! Get in......
neg: 0.0, 
neu: 1.0, 
pos: 0.0, 
compound: 0.0, 
Me too
neg: 0.0, 
neu: 1.0, 
pos: 0.0, 
compound: 0.0, 
That's fine, but do it with a smile.
neg: 0.0, 
neu: 0.518, 
pos: 0.482, 
compound: 0.5647, 
Yesss girl
neg: 0.0, 
neu: 1.0, 
pos: 0.0, 
compound: 0.0, 
Me too!
neg: 0.0, 
neu: 1.0, 
pos: 0.0, 
compound: 0.0, 
Ditto..
neg: 0.0, 
neu: 1.0, 
pos: 0.0, 
compound: 0.0, 
one day she will be all grown up .. ah bless
neg: 0.0, 
neu: 0.781, 
pos: 0.219, 
compound: 0.4215, 
Who doesn't.
neg: 0.0, 
neu: 1.0, 
pos: 0.0, 
compound: 0.0, 
I hate them too Lara
neg: 0.552, 
neu: 0.448, 
pos: 0.0, 
compound: -0.5719, 
</code></pre>

<p>You can observe that messages that escape the rules are not properly annotated such as <code>Yesss girl</code> or <code>Me too!</code> that should be positive. </p>

<p>A machine learning classifier is usually better for these cases if you can afford the cost of labeling a large amount of text to predict sentiments.</p>
",2,-3,582,2018-03-28 11:43:28,https://stackoverflow.com/questions/49533620/what-algorithm-that-was-used-on-the-sentiment-analysis-code-in-python
add a sentiment column onto a dataset in r,"<p>I have done some basic sentiment analysis in r and wanted to know if there was a way to have the sentiment of a sentence or row analyzed, and then have a column appended with the sentiment of the sentence. All analysis I have done up until now gives me an overview of the sentiment or pulls specific words, but doesn't link back to the original row of data</p>

<p>The input of my data would be fed in through a BI software and would look something like below with a case number and some text:</p>

<pre><code>""12345"",""I am extremely angry with my service""
""23456"",""I was happy with how everything turned out""
""34567"",""The rep did a great job helping me""
</code></pre>

<p>I would like it to be returned as an output below</p>

<pre><code>""12345"",""I am extremely angry with my service"",""Anger""
""23456"",""I was happy with how everything turned out"",""Positive""
""34567"",""The rep did a great job helping me"",""Positive""
</code></pre>

<p>Any point in the right direction of a package or resource would be greatly appreciated!</p>
","r, data-analysis, data-science, sentiment-analysis","<p>The problem you run into with sentences is that sentiment lexicons are based on words. If you look at the nrc lexicon, the word ""angry"" has three sentiment values: anger, disgust and negative. Which one do you choose? Or you have the sentence returning multiple words that are in a lexicon. Try testing different lexicons with your text to see what happens for example with <code>tidytext</code>. </p>

<p>If want a a package that can analyse sentiment on sentence level, you can look into <code>sentimentr</code>. You will not get sentiment values like anger back, but a sentiment/polarity score. More about <code>sentimentr</code> can be found in the <a href=""https://cran.r-project.org/web/packages/sentimentr/index.html"" rel=""nofollow noreferrer"">package documentation</a> and on <a href=""https://github.com/trinker/sentimentr#sentimentr"" rel=""nofollow noreferrer"">sentimentr</a> github page.</p>

<p>A small example code:</p>

<pre><code>library(sentimentr)
text &lt;- data.frame(id = c(""12345"",""23456"",""34567""),
                   sentence = c(""I am extremely angry with my service"", ""I was happy with how everything turned out"", ""The rep did a great job helping me""),
                   stringsAsFactors = FALSE)



sentiment(text$sentence)
   element_id sentence_id word_count  sentiment
1:          1           1          7 -0.5102520
2:          2           1          8  0.2651650
3:          3           1          8  0.3535534

# add sentiment score to data.frame
text$sentiment &lt;- sentiment(text$sentence)$sentiment 

text
     id                                   sentence  sentiment
1 12345       I am extremely angry with my service -0.5102520
2 23456 I was happy with how everything turned out  0.2651650
3 34567         The rep did a great job helping me  0.3535534
</code></pre>
",3,1,2301,2018-03-28 18:17:44,https://stackoverflow.com/questions/49541501/add-a-sentiment-column-onto-a-dataset-in-r
how to extract specific data from a csv file with given parameters?,"<p>I want to extract Neutral words from the given csv file (to a separate .txt file), but I'm fairly new to python and don't know much about file handling. I could not find a neutral words dataset, but after searching here and there, this is what I was able to find.</p>

<p>Here is the Gtihub project from where I want to extract data (just in case anyone needs to know) : <a href=""https://github.com/hoffman-prezioso-projects/Amazon_Review_Sentiment_Analysis#neutral-words"" rel=""nofollow noreferrer"" title=""hoffman-prezioso-projects/Amazon_Review_Sentiment_Analysis"">hoffman-prezioso-projects/Amazon_Review_Sentiment_Analysis</a></p>

<pre><code>Neutral Words
Word     Sentiment Score
a        0.0125160264947
the      0.00423728459134
it      -0.0294755274737
and      0.0810574365028
an       0.0318918766949
or      -0.274298468178
normal  -0.0270787859177
</code></pre>

<p>So basically I want to extract only those words (text) from csv where the numeric value is 0.something.</p>
","python, csv, sentiment-analysis","<p>Even without using any libraries, this is fairly easy with the csv you're using.</p>

<p>First open the file (I'm going to assume you have the path saved in the variable <code>filename</code>), then read the file with the <code>readlines()</code> function, and then filter out according to the condition you give.</p>

<pre><code>with open(filename, 'r') as csv:                         # Open the file for reading
    rows = [line.split(',') for line in csv.readlines()] # Read each the file in lines, and split on commas
    filter = [line[0] for line in rows if abs(float(line[1])) &lt; 1]   
                                                         # Filter out all lines where the second value is not equal to 1
</code></pre>

<hr>

<p>This is now the accepted answer, so I'm adding a disclaimer. There are numerous reasons why this code should not be applied to other CSVs without thought.</p>

<ul>
<li>It reads the entire CSV in memory</li>
<li>It does not account for e.g. quoting</li>
</ul>

<p>It is acceptable for very simple CSVs but the other answers here are better if you cannot be certain that the CSV won't break this code.</p>
",2,0,5926,2018-04-02 16:18:04,https://stackoverflow.com/questions/49614798/how-to-extract-specific-data-from-a-csv-file-with-given-parameters
How to change configuration file of Apache flume through Java code?,"<p>Iam currently working on a big data project for sentiment analysis of twitter's trending topics. I followed the tutorial of cloudera and understood how to get tweets to Hadoop through flume.</p>

<p><a href=""http://blog.cloudera.com/blog/2012/09/analyzing-twitter-data-with-hadoop/"" rel=""nofollow noreferrer"">http://blog.cloudera.com/blog/2012/09/analyzing-twitter-data-with-hadoop/</a></p>

<p>flume.conf:</p>

<pre>
# Licensed to the Apache Software Foundation (ASF) under one

# or more contributor license agreements. See the NOTICE file

# distributed with this work for additional information

# regarding copyright ownership. The ASF licenses this file

# to you under the Apache License, Version 2.0 (the

# ""License""); you may not use this file except in compliance

# with the License. You may obtain a copy of the License at

#

# http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing,

# software distributed under the License is distributed on an

# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY

# KIND, either express or implied. See the License for the

# specific language governing permissions and limitations

# under the License.



# The configuration file needs to define the sources, 

# the channels and the sinks.

# Sources, channels and sinks are defined per agent, 

# in this case called 'TwitterAgent'


TwitterAgent.sources = Twitter

TwitterAgent.channels = MemChannel

TwitterAgent.sinks = HDFS


TwitterAgent.sources.Twitter.type = com.cloudera.flume.source.TwitterSource

TwitterAgent.sources.Twitter.channels = MemChannel

TwitterAgent.sources.Twitter.consumerKey = 

TwitterAgent.sources.Twitter.consumerSecret = 

TwitterAgent.sources.Twitter.accessToken =  

TwitterAgent.sources.Twitter.accessTokenSecret =  

TwitterAgent.sources.Twitter.keywords = hadoop, big data, analytics, bigdata, cloudera, data science, data scientiest, business intelligence, mapreduce, data warehouse, data warehousing, mahout, hbase, nosql, newsql, businessintelligence, cloudcomputing


TwitterAgent.sinks.HDFS.channel = MemChannel

TwitterAgent.sinks.HDFS.type = hdfs

TwitterAgent.sinks.HDFS.hdfs.path = hdfs://hadoop1:8020/user/flume/tweets/%Y/%m/%d/%H/

TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream

TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text

TwitterAgent.sinks.HDFS.hdfs.batchSize = 1000

TwitterAgent.sinks.HDFS.hdfs.rollSize = 0

TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000


TwitterAgent.channels.MemChannel.type = memory

TwitterAgent.channels.MemChannel.capacity = 10000

TwitterAgent.channels.MemChannel.transactionCapacity = 100
</pre>

<p>Now to extend this to my application I need keywords sections in flume's configuration file to have  trending topics, I figured out Java code to get trending topics, but I have a problem now I don't know, how to connect this code to the flume configuration file or how to make a new file with real-time trending topics added at the keywords section. I searched a lot online for this, as Iam a beginner in this field, it will be of great help if you provide some info or atleast some other alternative for this.</p>
","hadoop, flume, sentiment-analysis, flume-twitter","<p>A very interesting problem..!</p>

<p>I agree with the comment made by @cricket_007 - editing the configuration without restarting the Flume agent is not achievable.</p>

<p>I won't be able to say much as I haven't seen your java code to get the keyword for trending topics. However, with the information you've supplied there is one alternative (or I should rather say a workaround) I could think of - but haven't tried it yet myself.</p>

<p>You could potentially modify the <a href=""https://github.com/cloudera/cdh-twitter-example/blob/master/flume-sources/src/main/java/com/cloudera/flume/source/TwitterSource.java"" rel=""nofollow noreferrer"">TwitterSource.java</a> class like this:</p>

<pre><code>public void configure(Context context) {
consumerKey = context.getString(TwitterSourceConstants.CONSUMER_KEY_KEY);
consumerSecret = context.getString(TwitterSourceConstants.CONSUMER_SECRET_KEY);
accessToken = context.getString(TwitterSourceConstants.ACCESS_TOKEN_KEY);
accessTokenSecret = context.getString(TwitterSourceConstants.ACCESS_TOKEN_SECRET_KEY);

//MODIFY THE FOLLOWING PORTION
String keywordString = context.getString(TwitterSourceConstants.KEYWORDS_KEY, """");
if (keywordString.trim().length() == 0) {
    keywords = new String[0];
} else {
  keywords = keywordString.split("","");
  for (int i = 0; i &lt; keywords.length; i++) {
    keywords[i] = keywords[i].trim();
  }
}
//UNTIL THIS POINT

ConfigurationBuilder cb = new ConfigurationBuilder();
cb.setOAuthConsumerKey(consumerKey);
cb.setOAuthConsumerSecret(consumerSecret);
cb.setOAuthAccessToken(accessToken);
cb.setOAuthAccessTokenSecret(accessTokenSecret);
cb.setJSONStoreEnabled(true);
cb.setIncludeEntitiesEnabled(true);

twitterStream = new TwitterStreamFactory(cb.build()).getInstance(); 
}
</code></pre>

<p>I have put in the comment above, where you are initialising the keywordString variable - you could invoke your java code (I'm assuming that it is a method from where you can return a comma separated string of keywords) instead of extracting this from the context available in the flume.conf (just remove context.getString() part).  </p>

<p>Along with that just remove the following statement from flume.conf:</p>

<pre><code>TwitterAgent.sources.Twitter.keywords = hadoop, big data, analytics, bigdata, cloudera, data science, data scientiest, business intelligence, mapreduce, data warehouse, data warehousing, mahout, hbase, nosql, newsql, businessintelligence, cloudcomputing
</code></pre>

<p>I hope this helps.</p>
",1,2,616,2018-04-03 20:37:23,https://stackoverflow.com/questions/49638781/how-to-change-configuration-file-of-apache-flume-through-java-code
Delete words in sentiment lexicon in R,"<p>I am using the nrc, bing and afinn lexicons for sentiment analysis in R. </p>

<p>Now I would like to remove some specific words form these lexicons, but I don't know how to do that, since the lexicons are not saved in my environment.  </p>

<p>My code looks like this (for nrc as an example):</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>MyTextFile %&gt;%
  inner_join(get_sentiments(""nrc"")) %&gt;%
  count(sentiment, sort = TRUE)</code></pre>
</div>
</div>
</p>
","r, sentiment-analysis","<p>Here are two ways to do this (there are undoubtedly more). Note first that there are 13901 words in the <code>nrc</code> lexicon:</p>

<pre><code>&gt; library(tidytext)
&gt; library(dplyr)
&gt; sentiments &lt;- get_sentiments(""nrc"")
&gt; sentiments
# A tibble: 13,901 x 2
   word        sentiment
   &lt;chr&gt;       &lt;chr&gt;    
 1 abacus      trust    
 2 abandon     fear     
 3 abandon     negative 
 4 abandon     sadness 
 5 abandoned   anger    
 6 abandoned   fear    
... and so on
</code></pre>

<p>You can filter out all words in a particular sentiment category (fewer words are left, at 12425):</p>

<pre><code>&gt; sentiments &lt;- get_sentiments(""nrc"") %&gt;% filter(sentiment!=""fear"")
&gt; sentiments
# A tibble: 12,425 x 2 
   word        sentiment
   &lt;chr&gt;       &lt;chr&gt;    
 1 abacus      trust    
 2 abandon     negative 
 3 abandon     sadness  
 4 abandoned   anger    
 5 abandoned   negative 
 6 abandoned   sadness  
</code></pre>

<p>Or you can create your own list of <code>dropwords</code> and remove them from the lexicon (fewer words are left, at 13884):</p>

<pre><code>&gt; dropwords &lt;- c(""abandon"",""abandoned"",""abandonment"",""abduction"",""aberrant"")
&gt; sentiments &lt;- get_sentiments(""nrc"") %&gt;% filter(!word %in% dropwords)
&gt; sentiments
# A tibble: 13,884 x 2
   word       sentiment
   &lt;chr&gt;      &lt;chr&gt;    
 1 abacus     trust    
 2 abba       positive 
 3 abbot      trust    
 4 aberration disgust  
 5 aberration negative 
 6 abhor      anger    
</code></pre>

<p>Then you would just do the sentiment analysis using <code>sentiments</code> you have created:</p>

<pre><code>&gt; library(gutenbergr)
&gt; hgwells &lt;- gutenberg_download(35) # loads ""The Time Machine""
&gt; hgwells %&gt;% unnest_tokens(word,text) %&gt;% 
      inner_join(sentiments) %&gt;% count(word,sort=TRUE)
Joining, by = ""word""
# A tibble: 1,077 x 2
   word         n
   &lt;chr&gt;    &lt;int&gt;
 1 white      236
 2 feeling    200
 3 time       200
 4 sun        145
 5 found      132
 6 darkness   108
</code></pre>

<p>Hope this helps somewhat.</p>
",1,0,952,2018-04-14 00:24:12,https://stackoverflow.com/questions/49826708/delete-words-in-sentiment-lexicon-in-r
NLP Sentiment Analysis using TF-IDF Vector Size,"<p>I am relatively new to NLP &amp; Sentiment analysis, but I am enrolled in a Machine Learning class and am creating a Sentiment Analysis NLP that will read a financial article and determine whether or not the overall sentiment is good or bad. </p>

<p>Currently, I have a dataset of about 2000 articles. I know that I need to implement the TF-IDF vector method to cast all the instances in the dataset to the same vector space. Also, I know that TF-IDF requires a ""Vocabulary"" and the size of this ""Vocabulary"" is the length of the vector, each vector representing an article. </p>

<p>My question is, how do I determine this vocabulary? One method I have found is to implement pre-processing (get rid of stop words, noisy words, punctuation, etc.) and then use ALL words in EVERY article in the training set. From here you can remove the words that have very few instances (unimportant words) and remove the words that have too many instances (non-distinguishing words). However, in my opinion, the ""Vocabulary"" is still going to be quite large, hence, the vector size is going to be very large.</p>

<p>Overall, this approach seems logical, but processing heavy. I feel that initially creating a ""Vocabulary"" containing all words in every article is going to be HUGE. And then iterating through every article to see how many times the words in the ""Vocabulary"" have occurred is going to require a lot of processing power. If I am using NLTK and scikit-learn, do I have anything to worry about? If so, is there a better way to create the vocabulary?</p>
","python, machine-learning, nlp, sentiment-analysis, tf-idf","<p>A basic approach for the sentiment analysis, involves making a vocabulary out of training corpus and use it for making feature vectors for your data. A vocabulary as large as few hundred thousand words is quite common and nothing to worry about. The main challenge in this approach is actually opposite of what you are thinking. You should find ways of increasing the size of your vocabulary rather than decreasing it.</p>

<p>You can try to enhance the vocabulary by using other sentiment analysis lexicons like <a href=""http://nmis.isti.cnr.it/sebastiani/Publications/LREC10.pdf"" rel=""nofollow noreferrer"">SentiWordNet</a> too.</p>

<p>As far as implementation of your approach is concerned you can build a scikit pipeline envolving <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorize</a> to build vocabulary and feature vector. One advantage of using CountVectorize() for building vocabulary is that it uses a sparse matrix for building the vector which handles your concern of large size. Then using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfIdf Vectorizer</a> for calculating term frequencies and inverse term frequencies, and then finally a model for training.</p>

<p>Consider adding some more features to your vector apart from pure bag of words. Be sure to perform a <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"" rel=""nofollow noreferrer"">GridSearch</a> on your model and preprocessing stages to fine-tune the parameters for best accuracy. I recently did a similar project for sentiment analysis of stocktwits data. I used a Naive Bayes classifier and got an accuracy of 72%. Naive Bayes proved to be better than even some deep learning models like RNN/DNN classifiers. The model selection, though independent of your question, is an integral part of building your project so keep tweaking it till you get good results. Check out my <a href=""https://github.com/penguin2048/StockIt"" rel=""nofollow noreferrer"">project</a> if you want some insights on my implementation.   </p>

<p>Be mindful of following points while doing your project:</p>

<ul>
<li>Some researchers believe that stop words actually add meaning to sentiment so I would recommend not removing them during the preprocessing phase. See <a href=""http://www.lrec-conf.org/proceedings/lrec2014/pdf/292_Paper.pdf"" rel=""nofollow noreferrer"">this paper</a></li>
<li>Always use domain knowledge while doing sentiment analysis. A negative sentiment in one domain like ""<em>predictable</em> movie"" can be positive in other like ""<em>predictable</em> share market"".</li>
<li>Don't remove words on your own (on the basis of frequency as you mentioned in the question) from the vocabulary. TfIdf normalization is meant for this purpose only. </li>
</ul>

<p>The field of sentiment analysis is filled with numerous researches and exciting new techniques. I would recommend you to read some papers like <a href=""https://www.sciencedirect.com/science/article/pii/S0306457315000242"" rel=""nofollow noreferrer"">this</a> by pioneers in this field.</p>
",1,3,1631,2018-04-17 06:58:23,https://stackoverflow.com/questions/49871737/nlp-sentiment-analysis-using-tf-idf-vector-size
How to access tweet text in a json file to perform further analysis?,"<p>I am using twitter API to fetch tweets about an input search term and storing them into a json file as shown in the code below. I am only interested in the tweet text and nothing else. How do I extract the text and ignore anything else? The end goal is to clean the individual tweets and perform sentiment analysis on them. Thanks! </p>

<pre><code>    consumerKey = ""xxx""
    consumerSecret = ""xxx""
    accessToken = ""xxx""
    accessTokenSecret = ""xxx""

    auth = tweepy.OAuthHandler(consumerKey, consumerSecret)
    auth.set_access_token(accessToken, accessTokenSecret)
    api = tweepy.API(auth)

    # Specify search term and count of tweets:
    searchTerm = input(""Enter topic: "")
    limit = int(input(""Enter the maximum number of tweets: ""))

    tweets = tweepy.Cursor(api.search,q=searchTerm,count=limit, lang=""en"", tweet_mode='extended').items(limit)

    for tweet in tweets:

        # add to JSON                            
        with open('tweets.json', 'w', encoding='utf8') as file:
            json.dump(tweet._json, file)
</code></pre>
","python, json, api, twitter, sentiment-analysis","<p>By reading some stuff in the Twitter API documentation, I could see the JSON strucuture.</p>

<pre><code>""tweet"": {
""created_at"": ""Thu Apr 06 15:24:15 +0000 2017"",
""id_str"": ""850006245121695744"",
""text"": ""1\/ Today we\u2019re sharing our vision for the future of the Twitter API platform!\nhttps:\/\/t.co\/XweGngmxlP"",
""user"": {
  ""id"": 2244994945,
  ""name"": ""Twitter Dev"",
  ""screen_name"": ""TwitterDev"",
  ""location"": ""Internet"",
</code></pre>

<p>This is only a small piece of the JSON return, from a tweet.</p>

<p>As you know JSON text is a dictionary essentialy, therefore you can treat it like a normal dictionary.</p>

<p>Now to do this in python you need to know which key you want to get the value of. If I could give a recommendation it would be to use the requests module for your proyect, it is a lot simpler and more understandable.</p>

<pre><code>for tweet in tweets:

    # add to JSON                            
    with open('tweets.json', 'w', encoding='utf8') as file:
        json.dump(tweets[tweet][""text""], file)
</code></pre>

<p>Instead of using json.dump you could use</p>

<pre><code>for tweet in tweets:

    # add to JSON                            
    with open('tweets.json', 'w', encoding='utf8') as file:
        file.write(tweets[tweet][""text""])
</code></pre>

<p>By doing this we're calling the dictionary and giving it a certain key, in this case the variable tweet, then as you can see the dictionary tweets has anothe dictionary inside it that's why we call it again so we can get what we want in this case the value of the key ""text""</p>

<p>Hope it helps!</p>
",1,0,3204,2018-04-23 16:55:09,https://stackoverflow.com/questions/49986251/how-to-access-tweet-text-in-a-json-file-to-perform-further-analysis
How to get multiple text entries from GUI and use those in a main python script?,"<p>I have a python file that extracts tweets, get their geo-coordinates and sentiment and finally plots those tweets/sentiment as colored circles on a map. </p>

<p>The following inputs (text entries) are needed to get it to work:
<em>An example user-input is also shown next to each input prompt:</em></p>

<pre><code> Enter the maximum number of tweets: *100*

 Do you want to search by topic? type: y or n: *y*

 Enter topic: *MoSalah*

 Enter visulaization/projection type:  
 1. Mercator 
 2. orthographic 
 3. melloweide 
 &gt;&gt;  *Mercator* 

 Zoom in to a conteninent of choice:  
 1. world 
 2. africa 
 3. asia 
 4. north america 
 5. south america 
 6. europe 
 7. usa 
 &gt;&gt;  *world*

 Enter symbol shape: 
 1. square 
 2. circle 
 &gt;&gt; *circle*
</code></pre>

<p>Now in order to make the user experience more exciting, I want to build a simple GUI that asks the user for all those inputs and store them into their respective variables, but I don't know how to create one and more importantly link the GUI to the python code running behind it.</p>

<p>Do I have to have separate retrieval function for each one of the required inputs shown above?
For example, is this how the retrieval function for max no. of tweets should look like using a tkinter GUI :</p>

<pre><code>from tkinter import * 
root = Tk()

root.geometry('200x100')

# Retrieve to get input form user and store it in a variable

# Retrieve maximum number of tweets

def retrieveMaxTweets():
    maxTweets = textBox.get()

    return maxTweets 

textBox = Text(root, height = 2, width = 10)
textBox.pack()

buttonComment = Button(root, height=1, width=10, text='Enter max no. of tweets', command = lambda: retrieveMaxTweets())

buttonComment.pack()

mainloop()
</code></pre>

<p>And then in the part of the code where I initially asked for the limit, I do this:</p>

<pre><code>limit = retrieveMaxTweets()
</code></pre>

<p>instead of this:</p>

<pre><code>limit = int(input("" Enter the maximum number of tweets: ""))
</code></pre>
","python-3.x, user-interface, tkinter, sentiment-analysis, text-widget","<p>You could store the results of the different GUI 'questions' in to a dictionary ready for the other parts of the code to use. That way you would only have one function that 'collects/validates/stores' the responses.</p>

<p>For example</p>

<pre><code>import tkinter as tk

class App(tk.Frame):
    def __init__(self,master=None,**kw):
        #Create a blank dictionary
        self.answers = {}
        tk.Frame.__init__(self,master=master,**kw)

        tk.Label(self,text=""Maximum number of Tweets"").grid(row=0,column=0)
        self.question1 = tk.Entry(self)
        self.question1.grid(row=0,column=1)

        tk.Label(self,text=""Topic"").grid(row=1,column=0)
        self.question2 = tk.Entry(self)
        self.question2.grid(row=1,column=1)

        tk.Button(self,text=""Go"",command = self.collectAnswers).grid(row=2,column=1)


    def collectAnswers(self):
        self.answers['MaxTweets'] = self.question1.get()
        self.answers['Topic'] = self.question2.get()
        functionThatUsesAnswers(self.answers)

def functionThatUsesAnswers(answers):
    print(""Maximum Number of Tweets "", answers['MaxTweets'])
    print(""Topic "", answers['Topic'])


if __name__ == '__main__':
    root = tk.Tk()
    App(root).grid()
    root.mainloop()
</code></pre>

<p>When the button is pressed, each of the 'answers' are added to a dictionary which is then passed to the function that does the main part of your code.</p>
",1,0,2395,2018-04-30 14:33:47,https://stackoverflow.com/questions/50102696/how-to-get-multiple-text-entries-from-gui-and-use-those-in-a-main-python-script
Sentiment analysis using spark and Stanford NLP API,"<p>When I wanted to do a sentiment analysis project I searched alot online, and atlast I landed on this website, which explained the code but what it did not explain is how to use spark with respect to the code, I mean where to add the code. 
Website :<a href=""http://stdatalabs.blogspot.in/2017/09/twitter-sentiment-analysis-using-spark.html?m=1"" rel=""nofollow noreferrer"">http://stdatalabs.blogspot.in/2017/09/twitter-sentiment-analysis-using-spark.html?m=1</a></p>

<p>It will be of great help, if anyone can explain me completely, as Iam a begginer and this my first project on big data.
Thank you.</p>
","apache-spark, bigdata, stanford-nlp, sentiment-analysis","<p>In the bottom there is a link to the github (<a href=""https://github.com/stdatalabs/sparkNLP-elasticsearch"" rel=""nofollow noreferrer"">https://github.com/stdatalabs/sparkNLP-elasticsearch</a>) you should check that out (literally)</p>

<p>The main class is 
<code>com.stdatalabs.SparkES.TwitterSentimentAnalysis</code> according to the pom.xml </p>

<p>So running <code>mvn package</code> will yield you an executable .jar (user <code>java -jar</code>)</p>

<p>Running the jar will prompt you for some twitter config (keys, etc) and saves to a local es cluster using hardcoded index (&amp; mapping) <code>twitter_020717/tweet</code></p>

<p>You can now alter the code anyway you want, build, run, and check the results.</p>
",2,2,328,2018-05-03 20:43:27,https://stackoverflow.com/questions/50163569/sentiment-analysis-using-spark-and-stanford-nlp-api
Get count or sum of values in dataframe of each day,"<p>how to calculate the sum of the values ​​(1) and the sum of the values ​​(0) contained in each date? </p>

<p>or</p>

<p>how to calculate the sum of the values ​​(1) divided by the sum of the values ​​(0) in each date.</p>

<p><code>sentiment_value = log10(count_of_(1)/count_of_(0))</code>, this is the formula I am using for.</p>

<pre><code>date    new_sentiment
0   2017-04-28  1.0
1   2017-04-28  1.0
2   2017-04-28  1.0
3   2017-04-27  0.0
4   2017-04-27  1.0
5   2017-04-26  0.0
6   2017-04-26  1.0
7   2017-04-26  1.0
8   2017-04-26  0.0
9   2017-04-26  1.0

result_neg = date_df.appl
</code></pre>

<p><a href=""https://i.sstatic.net/RRWTa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RRWTa.png"" alt=""enter image description here""></a></p>
","python, python-2.7, pandas, sentiment-analysis","<p>You need:</p>

<pre><code>g = data.groupby(['date', 'new_sentiment']).size().unstack(fill_value=0).reset_index()
g['sentiment_value'] = np.log((g[1.0])/(g[0.0]))
</code></pre>

<p>Output:</p>

<pre><code>new_sentiment   date    0.0 1.0 sentiment_value
0            2017-04-26 2   3   0.405465
1            2017-04-27 1   1   0.000000
2            2017-04-28 0   3   inf
</code></pre>
",0,0,105,2018-05-25 15:52:52,https://stackoverflow.com/questions/50532929/get-count-or-sum-of-values-in-dataframe-of-each-day
How to use sentence vectors from doc2vec in keras Sequntial model for sentence sentiment analysis?,"<p>Creating doc2vec model</p>

<p>x:List of Sentences(Movie reviews)</p>

<p>length of x =2000</p>

<pre><code>doc2vec_data = []
for line in x:
temp = ''.join(str(token) for token in line.lower())
doc2vec_data.append(temp)

File = open('doc2vec_data.txt', 'w',encoding=""utf-8"") 
for item in doc2vec_data:
File.write(""%s\n"" % item)

sentences=gensim.models.doc2vec.TaggedLineDocument(""doc2vec_data.txt"")

d2v =gensim.models.Doc2Vec(sentences, dm=0,window = 5, 
                   size=5,
                   iter = 100, workers=32,dbow_words=1,
                   alpha=2,min_aplha=0.5)
</code></pre>

<p>Creating Numpy array of vectors:
as doc2vec model cant be directly used in keras sequential model.</p>

<pre><code>vec=np.array([d2v.infer_vector(item) for item in x])
</code></pre>

<p>Keras Sequential Model:</p>

<pre><code>model=Sequential()
model.add(Embedding(2000,128,input_length=vec.shape[1]))
model.add(LSTM(200,dropout=0.2,recurrent_dropout=0.2))
model.add(Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',
      optimizer='rmsprop',
      metrics=['accuracy'])
</code></pre>

<p>y:sentence labels (0 and 1)</p>

<pre><code>model.fit(vec,y,
  batch_size=32,epochs=8,
  verbose=1)
</code></pre>

<p>Above code is giving me this error-</p>

<pre><code>InvalidArgumentError                      Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in 
_do_call(self, fn, *args)
1349     try:
-&gt; 1350       return fn(*args)
1351     except errors.OpError as e:

~\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in 
_run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
 1328                                    feed_dict, fetch_list, target_list,
-&gt; 1329                                    status, run_metadata)
 1330 

~\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in 
__exit__(self, type_arg, value_arg, traceback_arg)
472             compat.as_text(c_api.TF_Message(self.status.status)),  
--&gt; 473             c_api.TF_GetCode(self.status.status))
474     # Delete the underlying status object from memory otherwise it 
stays alive

InvalidArgumentError: indices[0,0] = -19 is not in [0, 2000)
 [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, 
validate_indices=true, 
_device=""/job:localhost/replica:0/task:0/device:CPU:0""] 
(embedding_1/embeddings/read, embedding_1/Cast)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-34-3d0fc0b22a78&gt; in &lt;module&gt;()
 1 model.fit(vec,y,
  2          batch_size=32,epochs=8,
----&gt; 3          verbose=1) 

InvalidArgumentError: indices[0,0] = -19 is not in [0, 2000)
 [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, 
validate_indices=true, 
_device=""/job:localhost/replica:0/task:0/device:CPU:0""] 
(embedding_1/embeddings/read, embedding_1/Cast)]]
</code></pre>

<p>Could someone please tell me what's the error and how do i solve it ?</p>
","python, keras, deep-learning, sentiment-analysis","<p>You are already converting the sentences to vectors and re-attempting it with the Keras model. Its complaining that your Embedding layer is not receiving correct indices because it's already embedded. Assuming you have <code>vec.shape == (samples, doc2vec_vector_size)</code> you'll need to remove Embedding because it's already embedded and LSTM because you now have 1 vector per sentence not per word:</p>

<pre><code>model = Sequential()
model.add(Dense(hidden_size, activation='relu', input_dim=doc2vec_vector_size))
model.add(Dense(1, activation='sigmoid'))
</code></pre>
",2,1,1940,2018-05-28 11:04:09,https://stackoverflow.com/questions/50564928/how-to-use-sentence-vectors-from-doc2vec-in-keras-sequntial-model-for-sentence-s
ModuleNotFoundError: No module named &#39;sentiment_mod&#39;,"<p>I am using nltk sentiment_mod but python is throwing module not found error</p>

<pre><code>from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import sentiment_mod as sen_mod
</code></pre>

<p>the error is</p>

<pre><code>ModuleNotFoundError                
      Traceback (most recent call last)
      2 from tweepy import OAuthHandler
      3 from tweepy.streaming import StreamListener
----&gt; 4 import nltk.sentiment_mod as sen_mod
      5 import json
      6 

ModuleNotFoundError: No module named 'sentiment_mod'
</code></pre>
","python, twitter, nltk, sentiment-analysis","<p><a href=""https://www.nltk.org/py-modindex.html#cap-s"" rel=""nofollow noreferrer"">There's no such module in <code>ntlk</code>.</a> Judging by the name, it's some private modification of <code>ntlk.sentiment</code>. Consult wherever you got this code from how to use it. Maybe there are some additional requirements, or it's simply obsolete.</p>
",2,0,2570,2018-06-10 13:38:55,https://stackoverflow.com/questions/50784278/modulenotfounderror-no-module-named-sentiment-mod
How to integrate the sentiment analysis script with the chatbot for analysing the user&#39;s reply in the same console screen?,"<p>I want to make a chatbot that uses Sentiment analyser script for knowing the sentiment of the user's reply for which I have completed the Chatbot making.</p>

<p>Now only thing I want to do is to use this Script to analyse the reply of user using the chatbot that I have made.<br>
How should I integrate this <strong>sentiment_analysis.py</strong> script with the <strong>chatbot.py</strong> file to analyse the sentiment's of user?  </p>

<p><strong>Update:</strong>
<strong>The overall performance will be like this :</strong><br>
<em>Chatbot: How was your day?<br>
User: It was an awesome day. I feel so elated and motivated today.<br>
User Reply: Positive<br>
Sentiment score = (some random value)</em><br>
Thanking you in advance.</p>
","python-3.x, importerror, chatbot, sentiment-analysis, nltk-trainer","<p>Import classes from sentiment analysis script to chatbot script. Then do necessary things according to your requirement. For example. I modified your chatbot script:</p>

<pre><code>from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer
from sentiment_analysis import Splitter, POSTagger, DictionaryTagger  # import all the classes from sentiment_analysis
import os

bot = ChatBot('Bot')
bot.set_trainer(ListTrainer)

# for files in os.listdir('C:/Users/username\Desktop\chatterbot\chatterbot_corpus\data/english/'):
# data = open('C:/Users/username\Desktop\chatterbot\chatterbot_corpus\data/english/' + files, 'r').readlines()
data = [
    ""My name is Tony"",
    ""that's a good name"",
    ""Thank you"",
    ""How you doing?"",
    ""I am Fine. What about you?"",
    ""I am also fine. Thanks for asking.""]

bot.train(data)

# I included 3 functions from sentiment_analysis here for ease of loading. Alternatively you can create a class for them in sentiment_analysis.py and import here.
def value_of(sentiment):
    if sentiment == 'positive': return 1
    if sentiment == 'negative': return -1
    return 0

def sentence_score(sentence_tokens, previous_token, acum_score):
    if not sentence_tokens:
        return acum_score
    else:
        current_token = sentence_tokens[0]
        tags = current_token[2]
        token_score = sum([value_of(tag) for tag in tags])
        if previous_token is not None:
            previous_tags = previous_token[2]
            if 'inc' in previous_tags:
                token_score *= 2.0
            elif 'dec' in previous_tags:
                token_score /= 2.0
            elif 'inv' in previous_tags:
                token_score *= -1.0
        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)

def sentiment_score(review):
    return sum([sentence_score(sentence, None, 0.0) for sentence in review])

# create instances of all classes
splitter = Splitter()
postagger = POSTagger()
dicttagger = DictionaryTagger([ 'dicts/positive.yml', 'dicts/negative.yml',
                            'dicts/inc.yml', 'dicts/dec.yml', 'dicts/inv.yml'])

print(""ChatBot is Ready..."")
print(""ChatBot : Welcome to my world! What is your name?"")
message = input(""you: "")
print(""\n"")

while True:
    if message.strip() != 'Bye'.lower():

        reply = bot.get_response(message)

        # process the text
        splitted_sentences = splitter.split(message)
        pos_tagged_sentences = postagger.pos_tag(splitted_sentences)
        dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)

        # find sentiment score
        score = sentiment_score(dict_tagged_sentences)

        if (score &gt;= 1):
            print('User Reply: Positive')
        else:
            print('User Reply: Negative')

        print(""Sentiment score :"",score)
        print('ChatBot:',reply)

    if message.strip() == 'Bye'.lower():
        print('ChatBot: Bye')
        break
    message = input(""you: "")
    print(""\n"")
</code></pre>

<p>Let me know when you get errors.</p>
",1,0,400,2018-07-04 06:39:47,https://stackoverflow.com/questions/51167284/how-to-integrate-the-sentiment-analysis-script-with-the-chatbot-for-analysing-th
How to count the number of occurrences of each word in a sentence for each sentence score?,"<p>I have a document of user survey:</p>

<pre><code>Score    Comment
8        Rapid bureaucratic affairs. Reports for policy...
4        There needs to be communication or feed back f...
7        service is satisfactory
5        Good
5        There is no
10       My main reason for the product is competition ...
9        Because I have not received the results. And m...
5        no reason
</code></pre>

<p>I want to determine which keywords correspond to a higher score, and which keywords correspond to a lower score. </p>

<p>My idea is to construct a table of the words (or, a ""word vector"" dictionary), which will contain the scores it is associated with, and the number of times that score has been associated with that sentence.</p>

<p>Something like the following:</p>

<pre><code>Word        Score   Count
Word1:      7       1
            4       2
Word2:      5       1
            9       1
            3       2
            2       1
Word3:      9       3
Word4:      8       1
            9       1
            4       2
...         ...     ...
</code></pre>

<p>Then, for each word, the average score is average of all the scores that word is associated with.</p>

<p>To do this, my code is the following:</p>

<pre><code>word_vec = {}
# col 1 is the word, col 2 is the score, col 3 is the number of times it occurs

for i in range(len(data)):
    sentence = data['SurveyResponse'][i].split(' ')
    for word in sentence:
        word_vec['word'] = word
        if word in word_vec:
            word_vec[word] = {'Score':data['SCORE'][i], 'NumberOfTimes':(word_vec[word]['NumberOfTimes'] += 1)}
        else:
            word_vec[word] = {'Score':data['SCORE'][i], 'NumberOfTimes':1}
</code></pre>

<p>But this code gives me the following error:</p>

<pre><code>File ""&lt;ipython-input-144-14b3edc8cbd4&gt;"", line 9
    word_vec[word] = {'Score':data['SCORE'][i], 'NumberOfTimes':(word_vec[word]['NumberOfTimes'] += 1)}
                                                                                                  ^
SyntaxError: invalid syntax
</code></pre>

<p>Could someone please show me the correct way to do this?</p>
","python, nlp, sentiment-analysis","<p>Try this piece of code</p>

<pre><code>word_vec = {}
# col 1 is the word, col 2 is the score, col 3 is the number of times it occurs

for i in range(len(data)):
    sentence = data['SurveyResponse'][i].split(' ')
    for word in sentence:
        word_vec['word'] = word
        if word in word_vec:
            word_vec[word]['Score'] += data['SCORE'][i] # Keep accumulating the total score for each word, would be easier to find the average score later on
            word_vec[word]['NumberOfTimes'] += 1
        else:
            word_vec[word] = {'Score':data['SCORE'][i], 'NumberOfTimes':1}
</code></pre>

<p>To increment the value of 'NumberOfTimes', you can directly increment like this <code>word_vec[word]['NumberOfTimes'] += 1</code></p>
",2,2,812,2018-07-05 07:44:00,https://stackoverflow.com/questions/51185830/how-to-count-the-number-of-occurrences-of-each-word-in-a-sentence-for-each-sente
PoS Implementation with Naive Bayes Sentiment Analysis,"<p>I am trying to apply Sentiment Analysis (predicting negative and positive tweets) on a relatively large Dataset (10000 rows). So far, I achieved only ~73% accuracy using Naive Bayes and my method called ""final"" shown below to extract features. I want to add PoS to help with the classification, but am completely unsure how to implement it. I tried writing a simple function called ""pos"" (which I posted below) and attempted using the tags on my cleaned dataset as features, but only got around 52% accuracy this way.. Can anyone lead me in the right direction to implement PoS for my model? Thank you.</p>

<pre><code>def pos(word):
 return [t for w, t in nltk.pos_tag(word)]


def final(text):

   """"""
   I have code here to remove URLs,hashtags, 
   stopwords,usernames,numerals, and punctuation.
   """"""

   #lemmatization
   finished = []
   for x in clean:
      finished.append(lem.lemmatize(x))

   return finished
</code></pre>
","python, nlp, nltk, sentiment-analysis, naivebayes","<p>You should first split the tweets into sentences and then tokenize. NLTK provides a method for this.</p>

<pre><code>   from nltk.tokenize import sent_tokenize
   sents = sent_tokenize(tweet)
</code></pre>

<p>After this supply this list of sentences to your <code>nltk.pos_tag</code> method. That should give accurates POS tags.</p>
",1,1,345,2018-07-06 01:16:02,https://stackoverflow.com/questions/51201830/pos-implementation-with-naive-bayes-sentiment-analysis
How many requests and tweets per request do I get when running this tweepy script?,"<p>I'm trying to retrieve tweets based on Hashtag name and save them as a JSON file for later analysis. I read twitter's documentations but unfortunately I didn't understand. Below is a code snippet.</p>

<pre><code>for tweet in tweepy.Cursor(api.search,q=hashtag_name,count=100,wait_on_rate_limit=True ,wait_on_rate_limit_notify= True).items():
    # Create and save the data on the Desktop as a JSON file.
    f= open(os.path.expanduser(""~/Desktop"")+'/'+time_str+'/'+'%d_%s_tweets.json'%(file_name_counter,hashtag_category), 'a',encoding=""utf-8"") 
    f.write(json.dumps(tweet._json))
    f.write(""\n"")
    file_name_counter += 1
</code></pre>

<p>I want to know:</p>

<ul>
<li>How many requests does this code sends per minute?</li>
<li>How much tweets does 1 request retrieves?</li>
<li>Where exactly does this code send a request to twitter server? 
Eventually I want to save each request's tweets on a JSON file </li>
</ul>
","python, tweepy, sentiment-analysis","<blockquote>
  <p>How many requests does this code sends per minute?</p>
</blockquote>

<p>This code loops without any sleep, so it sends as much as requests it can. And it's a bad idea because of rate limits.</p>

<blockquote>
  <p>How much tweets does 1 request retrieves?</p>
</blockquote>

<p>Your parameter is <code>count=100</code> so the answser is 100.</p>

<blockquote>
  <p>Where exactly does this code send a request to twitter server?</p>
</blockquote>

<p>It goes to <a href=""https://api.twitter.com/1.1/search/tweets.json"" rel=""nofollow noreferrer"">https://api.twitter.com/1.1/search/tweets.json</a></p>

<blockquote>
  <p>Eventually I want to save each request's tweets on a JSON file</p>
</blockquote>

<p>It is actually doing this.</p>

<p>Please read : <a href=""https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html"" rel=""nofollow noreferrer"">https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html</a></p>
",0,-1,554,2018-07-10 09:23:45,https://stackoverflow.com/questions/51261613/how-many-requests-and-tweets-per-request-do-i-get-when-running-this-tweepy-scrip
R - Finding top words in each NRC sentiment and emotion using syuzhet package,"<p>Snapshot of the dataset:</p>

<p><a href=""https://i.sstatic.net/yVmeA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yVmeA.png"" alt=""enter image description here""></a></p>

<p>I'm getting following chart:</p>

<p><a href=""https://i.sstatic.net/S7fql.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S7fql.png"" alt=""enter image description here""></a></p>

<p>Here is the code:</p>

<pre><code>library(tidytext)
library(syuzhet)

lyrics$lyric &lt;- as.character(lyrics$lyric)

tidy_lyrics &lt;- lyrics %&gt;% 
  unnest_tokens(word,lyric)

song_wrd_count &lt;- tidy_lyrics %&gt;% count(track_title)

lyric_counts &lt;- tidy_lyrics %&gt;%
  left_join(song_wrd_count, by = ""track_title"") %&gt;% 
  rename(total_words=n)

lyric_sentiment &lt;- tidy_lyrics %&gt;% 
  inner_join(get_sentiments(""nrc""),by=""word"")

lyric_sentiment %&gt;% 
count(word,sentiment,sort=TRUE) %&gt;%
group_by(sentiment)%&gt;%top_n(n=10) %&gt;% 
ungroup() %&gt;%
  ggplot(aes(x=reorder(word,n),y=n,fill=sentiment)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~sentiment,scales=""free"") + 
  coord_flip()
</code></pre>

<p>The issue is that I'm not sure if the result I'm getting is correct or not. For instance, you can see 'bad' is part of multiple emotions. Also, if we inspect <code>lyric_sentiment</code>, we'd see that word 'shame' is present four times for 'Tim McGraw'. In reality it appears only twice in this song.</p>

<p><strong>What's the right approach?</strong></p>
","r, text-mining, sentiment-analysis, tidytext","<p>You are doing it correct. nrc sentiments can place words in multiple sentiment sections. You can see this in the following example. You can also look up values on the <a href=""http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm"" rel=""nofollow noreferrer"">nrc homepage</a></p>

<pre><code>library(dplyr)
library(tidytext)

nrc &lt;- get_sentiments(""nrc"")
nrc %&gt;% filter(word %in% c(""bad"", ""shame""))
# A tibble: 9 x 2
  word  sentiment
  &lt;chr&gt; &lt;chr&gt;    
1 bad   anger    
2 bad   disgust  
3 bad   fear     
4 bad   negative 
5 bad   sadness  
6 shame disgust  
7 shame fear     
8 shame negative 
9 shame sadness  
</code></pre>
",2,1,1947,2018-07-11 12:51:27,https://stackoverflow.com/questions/51285942/r-finding-top-words-in-each-nrc-sentiment-and-emotion-using-syuzhet-package
Sentiment Analysis using classification and clustering algorithms: Which is better?,"<p>I am trying to do a Sentiment Analysis on Song Lyrics using Python. 
After studying many simple classification problems, with known labels (such as Email classification Spam/Not Spam), I thought that the Lyrics Sentiment Analysis lies on the Classification field.
While actually coding it, i discovered that I had to compute the sentiment for each song's lyrics, and probably adding a column to the original dataset, marking it positive or negative, or using the actual sentiment score.</p>

<p>Couldn't this be done using a clustering approach? Since we don't know each song's class in the first place (positive sentiment / negative sentiment) the algorithm will cluster the data using sentiment analysis.</p>
","machine-learning, classification, cluster-analysis, sentiment-analysis","<p>Clustering usually won't produce sentiments.</p>

<p>It a more likely to produce e.g., a cluster for rap and one for non-rap. Or one for lyrics with an even song length, and one for odd length.</p>

<p>There is more in the data than sentiment. So why would clustering produce sentiment clusters?</p>

<p>If you want particular labels (positive sentiment, negative sentiment) then you <em>need</em> to provide training data and use a supervised approach.</p>
",0,-1,1130,2018-07-11 14:30:35,https://stackoverflow.com/questions/51288134/sentiment-analysis-using-classification-and-clustering-algorithms-which-is-bett
Quantifying sentiment analysis using python,"<p>I've been using NLTK in python for doing sentiment analysis, it only has positive, neutral and negative class, what if we want to do sentiment analysis and having a number to show how much a sentence can be negative or positive. Sort of seeing it as a regression problem. Is there any pre-trained library out there to do so? </p>
","python, nlp, nltk, stanford-nlp, sentiment-analysis","<p>I know of a few ways to do this:</p>

<ul>
<li><strong>Vader</strong> returns score as a gradation (between zero and one)</li>
<li><strong>Stanford NLP</strong> returns a categorical classification (i.e. 0, 1, 2, 3).</li>
</ul>

<h1>An NLTK way:</h1>

<pre><code>from nltk.sentiment.vader import SentimentIntensityAnalyzer as sia
sentences = ['This is the worst lunch I ever had!',
             'This is the best lunch I have ever had!!',
             'I don\'t like this lunch.',
             'I eat food for lunch.',
             'Red is a color.',
             'A really bad, horrible book, the plot was .']

hal = sia()
for sentence in sentences:
    print(sentence)
    ps = hal.polarity_scores(sentence)
    for k in sorted(ps):
        print('\t{}: {:&gt;1.4}'.format(k, ps[k]), end='  ')
    print()
</code></pre>

<p>Example output:</p>

<pre><code>This is the worst lunch I ever had!
    compound: -0.6588   neg: 0.423      neu: 0.577      pos: 0.0  
</code></pre>

<h1>A Stanford-NLP, Python way:</h1>

<p>(Note that this way requires you to start an instance of the CoreNLP server to run e.g.: <code>java -mx1g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000</code>)</p>

<pre><code>from pycorenlp import StanfordCoreNLP
stanford = StanfordCoreNLP('http://localhost:9000')

for sentence in sentences:
    print(sentence)
    result = stanford.annotate(sentence,
                               properties={
                                'annotators': 'sentiment',
                                'outputFormat': 'json',
                                'timeout': '5000'
                               })
    for s in result['sentences']:
        score = (s['sentimentValue'], s['sentiment'])
    print(f'\tScore: {score[0]}, Value: {score[1]}')
</code></pre>

<p>Example output:</p>

<pre><code>This is the worst lunch I ever had!
    Score: 0, Value: Verynegative
</code></pre>
",2,2,1065,2018-07-14 21:05:46,https://stackoverflow.com/questions/51343373/quantifying-sentiment-analysis-using-python
"expected conv1d_1_input to have shape (15, 512) but got array with shape (4, 512)","<p>I'm working on a sentiment analysis project in python with keras using CNN and word2vec as an embedding method.
according to my code, I set my input shape, 15 and 512 so when I want to predict the polarity of a new sentence say:""I am so sorry"" for example, with the length: 4 - I face this error:</p>

<blockquote>
  <p>expected conv1d_1_input to have shape (15, 512) but got array with shape (4, 512)
  and this is a part of my code:</p>
</blockquote>

<pre><code>model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', 
input_shape=(15, 512)))
model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))
model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))
model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))
model.add(Dropout(0.25))
model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))
model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))
model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))
model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))
model.add(Dropout(0.25))
model.add(Dense(256, activation='relu')
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(2, activation='softmax'))
</code></pre>

<p>all I can do is making new python file and load all my related models such as word2vec ... and change the input shape on it like this:</p>

<pre><code>model111 = Sequential()
model111.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', 
input_shape=(len(input), 512)))
model111.add(Dense(256, activation='relu'))
model111.add(Dense(256, activation='relu'))
model111.add(Dropout(0.5))
model111.add(Flatten())
model111.add(Dense(2, activation='softmax'))
</code></pre>

<p>I wanna know whether this method is ok or not?? any efficient solution would be really appreciated</p>
","python, keras, conv-neural-network, prediction, sentiment-analysis","<p>When you have inputs with a size smaller than your model is expecting, you need to do padding (i.e adding zeros to the end of input array to complete the expected size before give to model. There is a keras function to do that:</p>

<pre><code>from keras.preprocessing.sequence import pad_sequences
input_array = pad_sequences(input_array, maxlen=max_tweet_length, padding='post')
</code></pre>

<p>And then, reshape your data to so that it fits the expected spatiality by CNN:</p>

<pre><code>input_array = input_array.reshape(input_array.shape + (1,))
</code></pre>
",2,1,247,2018-07-20 16:42:41,https://stackoverflow.com/questions/51447071/expected-conv1d-1-input-to-have-shape-15-512-but-got-array-with-shape-4-512
Sentiment analysis with keras including neutral tweet,"<p>I'm working on a sentiment analysis project in python with keras using CNN and word2vec as an embedding method I want to detect positive, negative and neutral tweets(in my corpus I considered every negative tweets with the 0 label, positive = 1 and neutral = 2). Since I'm new in this field I have some questions,
here is a part of my code:
***Assuming that X-train and X-test contain tweets and Y-train and Y-test contain tweet's labels.</p>

<pre><code> if i &lt; train_size:
     if labels[index] == 0 :
         Y_train[i, :] = [1.0, 0.0]

     elif labels[index] == 1 :
         Y_train[i, :] = [0.0, 1.0]
     else:
         Y_train[i, :] = [1.0, 1.0]

 else:

     if labels[index] == 0 :
         Y_test[i - train_size, :] = [1.0, 0.0]
     elif labels[index] == 1 :
         Y_test[i - train_size, :] = [0.0, 1.0]
     else:
         Y_test[i - train_size, :] = [1.0, 1.0]
</code></pre>

<p>in the code above you see that I considered if a related label was 0(if labels[index] == 0 :) as negative I put [1.0, 0.0] in some specific list and if the label was 1(if labels[index] == 1 :) I put [0.0, 1.0] as positive tweets and else (if labels[index] == 2 :) as neutral i put [1.0, 1.0] so just consider that the logical part af my code that i mentioned is ok.</p>

<p>here is my keras model: </p>

<pre><code>model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='elu', 
padding='same', input_shape= 
(max_tweet_length,vector_size)))
model.add(Conv1D(32, kernel_size=3, activation='elu', 
padding='same'))
model.add(Conv1D(32, kernel_size=3, activation='elu', 
padding='same'))
model.add(Conv1D(32, kernel_size=3, activation='elu', 
padding='same'))
model.add(Dropout(0.25))
model.add(Conv1D(32, kernel_size=2, activation='elu', 
padding='same'))
model.add(Conv1D(32, kernel_size=2, activation='elu', 
padding='same'))
model.add(Conv1D(32, kernel_size=2, activation='elu', 
padding='same'))
model.add(Conv1D(32, kernel_size=2, activation='elu', 
padding='same'))
model.add(Dropout(0.25))
model.add(Dense(256, activation='tanh'))
model.add(Dense(256, activation='tanh'))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(2, activation='sigmoid'))
</code></pre>

<p>So in order to  predict a new input, I have this code:</p>

<pre class=""lang-js prettyprint-override""><code>sentiment = model.predict(np.array(a),batch_size=1,verbose = 2)[0]
if(np.argmax(sentiment) == 0):
    print(""negative"")
    print('the label is')
    print(np.argmax(sentiment))
elif (np.argmax(sentiment) == 1):
    print(""positive"")
    print('the label is')
    print(np.argmax(sentiment))
elif (np.argmax(sentiment) ==2):
    print(""neutral"")
    print('the label is')
    print(np.argmax(sentiment))
</code></pre>

<p>My question contains 2 parts: 
I wanna know is it true to predict in such way? AS far as I told I considered label 2 for neutral tweets and for this reason I considered if (np.argmax(sentiment) ==2) then print neutral - Is this logical or acceptable for prediction??</p>

<p>I mean I considered to assign [0.1, 1.0] for neutral tweets in train and test set so If I consider 2 as neutral in prediction part, does it make any sense??</p>

<p>thanks a lot</p>

<hr>

<p>****for regression is it true to change my train and test code in such way?
considering 0,1,2 as polarities in my corpus</p>

<pre><code>  if i &lt; train_size:
     if labels[index] == 0 :
         Y_train[i, :] = [1.0, 0.0]

     elif labels[index] == 1 :
         Y_train[i, :] = [0.0, 1.0]
     elif labels[index]==2
         Y_train[i, :] = [0.5, 0.5]

 else:

     if labels[index] == 0 :
         Y_test[i - train_size, :] = [1.0, 0.0]
     elif labels[index] == 1 :
         Y_test[i - train_size, :] = [0.0, 1.0]
     else:
         Y_test[i - train_size, :] = [0.5, 0.5]
</code></pre>

<p>then setting'sigmoid' for activation:</p>

<pre><code>model.add(Dense(256, activation='tanh'))
model.add(Dense(256, activation='tanh'))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(2, activation='sigmoid'))
</code></pre>

<p>and Can I predict my input tweet in the way i mentioned above??</p>

<pre><code> if (np.argmax(sentiment) ==2):
    print(""neutral"")
    print('the label is')
    print(np.argmax(sentiment))
</code></pre>

<p>*****If I used word2vec for embedding and considering 0,1,2 as polarities in my corpus Can I set labels in such way?</p>

<pre><code> if i &lt; train_size:
     if labels[index] == 0 :
         Y_train[i, :] = [1.0, 0.0,0.0]

     elif labels[index] == 1 :
         Y_train[i, :] = [0.0, 1.0,0.0]
     else:
         Y_train[i, :] = [0.0, 0.0,1.0]

 else:

     if labels[index] == 0 :
         Y_test[i - train_size, :] = [1.0, 0.0,0.0]
     elif labels[index] == 1 :
         Y_test[i - train_size, :] = [0.0, 1.0,0.0]
     else:
         Y_test[i - train_size, :] = [0.0, 0.0,1.0]
</code></pre>

<p>and then for compiling:</p>

<pre><code>model.compile(loss='categorical_crossentropy',
           optimizer=Adam(lr=0.0001, decay=1e-6),
           metrics=['accuracy']) 
</code></pre>

<p>thank you for your patience</p>
","python, twitter, keras, sentiment-analysis","<p>I would not recommend labeling your data in this way as defining your loss function to properly incentivize learning will be very difficult. The following would be more standard approaches.</p>

<p><strong>1. Classification</strong></p>

<p>label your data as it already is: simple 0,1,2. You can either label these as <a href=""https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science"" rel=""nofollow noreferrer""><code>one-hot vectors</code></a>:</p>

<pre><code>[1., 0., 0.]  # liberal
[0., 1., 0.]  # neutral 
[0., 0., 1.]  # conservative 
</code></pre>

<p>and use the <a href=""https://keras.io/losses/#categorical_crossentropy"" rel=""nofollow noreferrer""><code>categorical_crossentropy</code></a> loss function, or simply pass in your labels as <code>0</code>, <code>1</code>, or <code>2</code> and use the <a href=""https://keras.io/losses/#sparse_categorical_crossentropy"" rel=""nofollow noreferrer""><code>sparse_categorical_crossentropy</code></a> loss function. Both should behave the same way. Either way, use 3 neurons on your output layer with <code>softmax</code> activation.</p>

<p><strong>2. Regression</strong></p>

<p>Treat labeling the sentiment as a regression task from <code>0</code> to <code>1</code> with <code>0</code> being on the left, <code>1</code> being on the right, and <code>0.5</code>. This way your model will train to predict the political ideology of the tweet on a continuous basis - which may be interesting to you. If you go this route use one output neuron with <code>sigmoid</code> activation.</p>

<hr>

<p>Also, if it is of any help, I did political sentiment for a class project. I used RNNs though instead of 1D convolutions - <a href=""https://github.com/DylanModesitt/political-sentiment/blob/master/models/recurrent.py"" rel=""nofollow noreferrer"">here</a> is the code anyway. </p>
",3,1,622,2018-07-22 05:18:25,https://stackoverflow.com/questions/51462264/sentiment-analysis-with-keras-including-neutral-tweet
Best lexicons for sentence vs document level analysis,"<p>What are the best lexicons for document-level and sentence-level analysis? I'm using Vader currently for sentence-level analysis, however I'm worried that when I move to the document level, Vader may not perform as well as others. </p>

<p>Similar question to the post <a href=""https://stackoverflow.com/questions/4188706/sentiment-analysis-dictionaries/31401346#31401346"">here</a>, however more specific.</p>
","nlp, nltk, sentiment-analysis, lexicon","<p>In addition to the sentiment lexica listed in the linked post, I can recommend aFinn sentiment lexicon.</p>

<p>For sentiment analysis, depending on only lexica may not be be best solution, especially on document level. Language is so flexible that its attributes and notions other than sentiment-laden vocabulary effect semantics deeply.</p>

<p>Some of the core notions are contrastive discource markers (especially for document level), negation and modality.</p>

<ul>
<li>contrastive discourse markers</li>
</ul>

<p>There are opinions that have both pros and cons within documents and we tie those via those markers like 'however', 'nevertheless' etc. to convey meaning or an idea. For a bag of words approach, the sentences below are treated the same, yet if people to annotate their sentiment with one label, they may not annotate them with the same one:</p>

<pre><code>The laptop has amazing features, but its screen is killing me.
The laptop's screen is killing me, but it has amazing features.
</code></pre>

<p>In general, we evaluate these kind of sentences or paragraphs with the sentiment of the subclause after 'but'. Other contastive discource markers have their own semantics as well. This is inspected in an area called discource analysis. </p>

<ul>
<li>negation and modality</li>
</ul>

<p>These notions change semantics as well. So, they cannot be overlooked for both levels. There are studies and papers those used negation and modality triggers with sentiment lexica. You can google it 'negation and modality on sentiment analysis' to see what you can do.</p>

<p>Finally what I can suggest is if you have a domain-specific dataset, you may build your own lexicon using distant supervision.</p>

<p>Hope this helps,</p>

<p>Cheers</p>
",1,1,361,2018-07-25 13:55:34,https://stackoverflow.com/questions/51520734/best-lexicons-for-sentence-vs-document-level-analysis
i want to calculate what emotions are impacting sentiments how can i do this in R?,"<p><a href=""https://i.sstatic.net/EtgeG.png"" rel=""nofollow noreferrer"">emotiofile</a>I have 10 columns, with 8 emotion scores and 2 sentiments scores
emotions like anger sad happy...
sentiments being positive and negative
each emotion and sentiments have scores again ranging from 0 to x
how can i calculate which emotion combinations are impacting positive sentiment or negative by using R language ?</p>
","r, sentiment-analysis","<p>This question is very vague and does not have any specific code to further understand what you mean (reproducible example). It seems to me that you need to understand the data using descriptive statistics first. Functions such as <code>summary</code>, <code>head</code>, <code>names</code>, <code>levels</code>, <code>sapply</code> and <code>nlevels</code> (by column) can help you to begin understanding what you're working with.</p>

<p>I've found this article that can help you start understanding: <a href=""https://machinelearningmastery.com/descriptive-statistics-examples-with-r/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/descriptive-statistics-examples-with-r/</a></p>

<p>Once you have an idea of what your data looks like, try looking into this article to see the different types of analysis that exist and which fit best for your situation. Text mining is not a simple task but there are many techniques that can help you find your answers:
<a href=""https://www.datacamp.com/community/tutorials/sentiment-analysis-R"" rel=""nofollow noreferrer"">https://www.datacamp.com/community/tutorials/sentiment-analysis-R</a></p>

<p>I hope this helps a bit.</p>
",0,-7,70,2018-07-26 11:25:43,https://stackoverflow.com/questions/51537688/i-want-to-calculate-what-emotions-are-impacting-sentiments-how-can-i-do-this-in
Request error Google Cloud NLP API with Swift,"<p>I am trying to make a request to Google Cloud NLP API to obtain sentiment analysis for a piece of text. I used Postman to design the correct request, and I was able to get a valid response using Postman. However, when I try to make the same request from Swift, it gives me an error. The error and code snippet used to make the request is shown below.</p>

<pre><code> func sendAPIRequest(with text: String){

    print(""Text: "", text)
    let jsonRequest = [
        [
        ""document"":[
            ""type"":""PLAIN_TEXT"",
            ""language"": ""EN"",
            ""content"":""'Lawrence of Arabia' is a highly rated film biography about British Lieutenant T. E. Lawrence. Peter O'Toole plays Lawrence in the film.""
        ],
        ""encodingType"":""UTF8""
    ]
        ]

    let jsonObject = JSON(jsonRequest)


    let headers: HTTPHeaders = [
        ""X-Ios-Bundle-Identifier"": ""\(Bundle.main.bundleIdentifier ?? """") "",
        ""Content-Type"": ""application/json""
    ]
    let APIRequest = Alamofire.request(""https://language.googleapis.com/v1/documents:analyzeSentiment?key=\(gCloudAPIKey)"", method: .post , parameters: jsonRequest as? [String: Any], encoding: JSONEncoding.default , headers: headers).responseJSON { (response) in
        print(response)
        if let json = response.result.value {
            print(""JSON: \(json)"")
        }
    }
</code></pre>

<p>Error: </p>

<pre><code>JSON: {
error =     {
    code = 400;
    details =         (
                    {
            ""@type"" = ""type.googleapis.com/google.rpc.BadRequest"";
            fieldViolations =                 (
                                    {
                    description = ""Must have some text content to annotate."";
                    field = ""document.content"";
                }
            );
        }
    );
    message = ""One of content, or gcs_content_uri must be set."";
    status = ""INVALID_ARGUMENT"";
};
}
</code></pre>
","swift, google-cloud-platform, alamofire, sentiment-analysis, google-natural-language","<p>Sorry. Solved it. My jsonRequest should be of type Parameters according to Alamofire.</p>

<pre><code> let jsonRequest: Parameters =
        [
        ""document"":[
            ""type"":""PLAIN_TEXT"",
            ""language"": ""EN"",
            ""content"":""\(text)""
        ],
        ""encodingType"":""UTF8""
    ]
</code></pre>
",2,0,1053,2018-08-03 03:27:20,https://stackoverflow.com/questions/51664903/request-error-google-cloud-nlp-api-with-swift
Sending a string to python script using PHP and get the output back to the PHP script,"<p>What i am trying to do is call a python script using PHP which is performing a sentiment analysis and return back the result of the analysis in the form of polarity either -ve or +ve.
The reason why im calling Python from PHP is that i have an android application which uses PHP and i dont know if we can call Pyhton from android studio.
Here is my Python Script:</p>

<pre><code>#!C:/Python27/python.exe
import sys
from textblob import TextBlob
import os
arg=sys.argv[1]

analysis = Textblob(arg)
polar = analysis.sentiment.polarity
print(&quot;Content-Type: text/html\n&quot;)
print(polar)
</code></pre>
<p>And here is my PHP script</p>
<pre><code>&lt;?php

$my=&quot;This is very Good&quot;;

echo exec('C:/Python27/python test.py &quot;'.$my.'&quot;');

?&gt;
</code></pre>
<p>I dont know where im going wrong but when i only execute my script on the xampp server the following error occurs</p>
<p>&quot;The server encountered an internal error and was unable to complete your request.</p>
<p>Error message:
End of script output before headers: test.py&quot;</p>
",sentiment-analysis,"<p>That error means that something is being printed before the http header ""Content-Type: text/html\n"" is being printed.</p>

<p>Perhaps the py script is printing a warning or an error, or your php script has character(s) before the <code>&lt;?php</code> .</p>

<p>To troubleshoot, try running the php and py scripts from a terminal. It will show exactly what is being printed out before the header:</p>

<pre><code># /PATH_TO_PHP/php the_test_script.php
# /Python27/python.exe test.py
</code></pre>
",0,0,153,2018-08-16 16:55:31,https://stackoverflow.com/questions/51881992/sending-a-string-to-python-script-using-php-and-get-the-output-back-to-the-php-s
ValueError: not enough values to unpack,"<p>I am trying to learn (on Python3) how to do sentiment analysis for NLP and I am using the ""UMICH SI650 - Sentiment Classification"" Database available on Kaggle: <a href=""https://www.kaggle.com/c/si650winter11"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/si650winter11</a></p>

<p>At the moment I am trying to generate a vocabulary with some loops, here is the code:</p>

<pre><code>    import collections
    import nltk
    import os

    Directory = ""../Databases""


    # Read training data and generate vocabulary
    max_length = 0
    freqs = collections.Counter()
    num_recs = 0
    training = open(os.path.join(Directory, ""train_sentiment.txt""), 'rb')
    for line in training:
        if not line:
            continue
        label, sentence = line.strip().split(""\t"".encode())
        words = nltk.word_tokenize(sentence.decode(""utf-8"", ""ignore"").lower())
        if len(words) &gt; max_length:
            max_length = len(words)
        for word in words:
            freqs[word] += 1
        num_recs += 1
    training.close()
</code></pre>

<p>I keep getting this error, that I don't fully understand:</p>

<blockquote>
  <p>in  label, sentence = line.strip().split(""\t"".encode())
  ValueError: not enough values to unpack (expected 2, got 1)</p>
</blockquote>

<p>I tried to add     </p>

<pre><code>if not line:
        continue
</code></pre>

<p>like suggested in here: <a href=""https://stackoverflow.com/questions/45270021/valueerror-not-enough-values-to-unpack-why"">ValueError : not enough values to unpack. why?</a>
But it didn't work for my case. How can I solve this error? </p>

<p>Thanks a lot in advance,</p>
","python-3.x, dictionary, nlp, nltk, sentiment-analysis","<p>Here's a cleaner way to read the dataset from <a href=""https://www.kaggle.com/c/si650winter11"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/si650winter11</a> </p>

<p><strong>Firstly</strong>, context manager is your friend, use it, <a href=""http://book.pythontips.com/en/latest/context_managers.html"" rel=""nofollow noreferrer"">http://book.pythontips.com/en/latest/context_managers.html</a> </p>

<p><strong>Secondly</strong>, if it's a text file, avoid reading it as a binary, i.e. <code>open(filename, 'r')</code> not <code>open(filename, 'rb')</code>, then there's no need to mess with str/byte and encode/decode.</p>

<p><strong>And now</strong>:</p>

<pre><code>from nltk import word_tokenize
from collections import Counter
word_counts = Counter()
with open('training.txt', 'r') as fin:
    for line in fin:
        label, text = line.strip().split('\t')
        # Avoid lowercasing before tokenization.
        # lowercasing after tokenization is much better
        # just in case the tokenizer uses captialization as cues. 
        word_counts.update(map(str.lower, word_tokenize(text)))

print(word_counts)
</code></pre>
",1,1,4006,2018-08-20 20:15:50,https://stackoverflow.com/questions/51937926/valueerror-not-enough-values-to-unpack
Adding multiple hidden layers keras,"<p>I have a simple sentiment analyzer using keras, here is my code, in which I use the keras code on github: <a href=""https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py</a></p>

<p>The initial and working model is:</p>

<pre><code>from __future__ import print_function

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding, Activation
from keras.layers import GRU, LeakyReLU
from keras.datasets import imdb

max_features = 2000
maxlen = 80  # cut texts after this number of words (among top max_features most common words)
batch_size = 256
hidden_layer_size = 32
dropout = 0.2
num_epochs = 1
activation_func = LeakyReLU(alpha=0.5)

print('Loading data...')
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')

print('Pad sequences (samples x time)')
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

print('Build model...')
model = Sequential()
model.add(Embedding(max_features, hidden_layer_size))
model.add(GRU(hidden_layer_size, dropout=dropout, recurrent_dropout=dropout))
model.add(Activation(activation_func))
model.add(Dense(1, activation='sigmoid'))

# try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

print('Train...')
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=num_epochs,
          validation_data=(x_test, y_test))
score, acc = model.evaluate(x_test, y_test,
                            batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)
</code></pre>

<p>The error I get is this one:</p>

<blockquote>
  <p>ValueError: Input 0 is incompatible with layer gru_2: expected ndim=3,
  found ndim=2</p>
</blockquote>

<p>This happens every time I try to add a second hidden layer on the model, for instance: </p>

<pre><code>model = Sequential()
model.add(Embedding(max_features, hidden_layer_size))
model.add(GRU(hidden_layer_size, dropout=dropout, recurrent_dropout=dropout))
model.add(Activation(activation_func))
model.add(GRU(hidden_layer_size, dropout=dropout, recurrent_dropout=dropout))
model.add(Activation(activation_func))
model.add(Dense(1, activation='sigmoid'))
</code></pre>

<p>I believe I am missing something concerning the dimensions of the hidden layers. How should I proceed to add another hidden layer successfully?</p>

<p>Thanks in advance,</p>
","python-3.x, keras, nlp, deep-learning, sentiment-analysis","<p>That's because by default the RNN layers in Keras <em>only return the last output</em>, i.e. an input <code>(samples, time_steps, features)</code> becomes <code>(samples, hidden_layer_size)</code>. In order to chain multiple RNNs you need to set the hidden RNN layers to have <code>return_sequences=True</code>:</p>

<pre><code>model = Sequential()
model.add(Embedding(max_features, hidden_layer_size))
# Add return_sequences=True
model.add(GRU(hidden_layer_size, activation=activation_func, dropout=dropout, recurrent_dropout=dropout, return_sequences=True))
# (samples, time_steps, hidden_layer_size)
model.add(GRU(hidden_layer_size, activation=activation_func, dropout=dropout, recurrent_dropout=dropout))
# (samples, hidden_layer_size)
model.add(Dense(1, activation='sigmoid'))
</code></pre>

<p>You can also return the last hidden etc, have a look at the <a href=""https://keras.io/layers/recurrent/"" rel=""noreferrer"">documentation</a> on what these parameters do.</p>
",7,1,6432,2018-08-21 09:40:35,https://stackoverflow.com/questions/51945823/adding-multiple-hidden-layers-keras
InvalidArgumentError Sentiment analyser with keras,"<p>I have built a sentiment analyzer using Keras as a binary classification problem. I am using the Imdb dataset using GRU.
My code is:</p>

<pre><code># coding=utf-8
# ==========
#   MODEL
# ==========

# imports
from __future__ import print_function
from timeit import default_timer as timer
from datetime import timedelta
from keras.models import Sequential
from keras.preprocessing import sequence
from keras import regularizers
from keras.layers import Dense, Embedding
from keras.layers import GRU, LeakyReLU, Bidirectional
from keras.datasets import imdb

#start a timer
start = timer()

# Hyperparameters
Model_Name = 'my_model.h5'
vocab_size = 5000
maxlen = 1000
batch_size = 512
hidden_layer_size = 2
test_split = 0.3
dropout = 0.1
num_epochs = 1
alpha = 0.2
validation_split = 0.25
l1 = 0.01
l2 = 0.01

# Dataset loading
print('Loading data...')
(x_train, y_train), (x_test, y_test) = imdb.load_data(path=""imdb.npz"",
                                                      maxlen=maxlen)

print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')

# Data preprocessing
# Sequence padding
print('Pad sequences (samples x time)')
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

# Network building
print('Build model...')
model = Sequential()
model.add(Embedding(vocab_size, hidden_layer_size))
model.add(Bidirectional(GRU(hidden_layer_size, kernel_initializer='uniform', kernel_regularizer=regularizers.l1_l2(l1=l1,l2=l2), dropout=dropout, recurrent_dropout=dropout,return_sequences=True)))
model.add(LeakyReLU())
model.add(Bidirectional(GRU(hidden_layer_size, kernel_initializer='uniform', dropout=dropout, kernel_regularizer=regularizers.l1_l2(l1=l1,l2=l2), recurrent_dropout=dropout)))
model.add(LeakyReLU())
model.add(Dense(1, activation='softmax', kernel_initializer='uniform', kernel_regularizer=regularizers.l1_l2(l1=l1,l2=l2)))

# Compile my model
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

print('Train...')

# Fit the model
history = model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_split=validation_split)
score, acc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)

# Create a summary, a plot and print the scores of the model
model.summary()

print('Test score:', score)
print('Test accuracy:', acc)

# Save model architecture, weights, training configuration (loss,optimizer),
# and also the state of the optimizer, so you can resume where you stopped
model.save(Model_Name)
end = timer()
print('Running time:  ' + str(timedelta(seconds=(end - start))) + '  in Hours:Minutes:Seconds')
</code></pre>

<p>I keep receiving an Error message which I don't completely understand:</p>

<pre><code>InvalidArgumentError (see above for traceback): indices[502,665] = 5476 is not in [0, 5000)
[[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=/job:localhost/replica:0/task:0/device:CPU:0](embedding_1/embeddings/read, embedding_1/Cast)]]
</code></pre>

<p>Can anyone help me understand what causes this error and how to solve it?</p>
","python-3.x, keras, deep-learning, sentiment-analysis, word-embedding","<p>The error complains about a non-existent word index. That's because you are only limiting the number of Emedding features (i.e. there is a word with index 5476 which is not in the range <code>[0, 5000)</code>, which 5000 refers to the <code>vocab_size</code> you have set). To resolve this, you also need to pass the <code>vocab_size</code> as <code>num_words</code> argument of <code>load_data</code> function, like this:</p>

<pre><code>... = imdb.load_data(num_words=vocab_size, ...)
</code></pre>

<p>This way you are limiting the words to the most frequent words (i.e. top <code>vocab_size</code> words with the most frequency in the dataset) with their indices in range <code>[0, vocab_size)</code>.</p>
",2,0,50,2018-08-23 10:15:03,https://stackoverflow.com/questions/51983456/invalidargumenterror-sentiment-analyser-with-keras
Unable to predict sentiment of emoticons,"<p>I am trying to predict the sentiment of facebook's comment using vader sentiment analysis tool[1] but it is not able to predict the sentiment of emoticons ,it is working in some comments while in some other it is not.</p>

<pre><code>result=db.post.find() 
analyzer=SentimentIntensityAnalyzer()
for sentence in sentences:
    vs=analyzer.polarity_scores(sentence)
    print(""{:-&lt;65} {}"".format(sentence,str(vs)))
</code></pre>

<p>The excerpt of the output is-</p>

<pre><code>I am rishav 😆---------------------------------------------------- {'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.3612}
Woohooo😍✌️------------------------------------------------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}
</code></pre>

<p>It is running on some sentences but not on other,I am traversing from the database.Also, in some cases when I use only 1 emoticon it works but on using multiple times, it doesn't works. </p>

<p>How to resolve this error?</p>

<p>[1]:<a href=""https://github.com/cjhutto/vaderSentiment"" rel=""nofollow noreferrer"">Vader Sentiment Analysis tool</a>!</p>
","python-3.x, machine-learning, utf-8, nltk, sentiment-analysis","<p>Your code seems alright but not your example. 
If you go through the VADER code, first it fetches the score of each word from its dictionary. For that, the sentence is sliced using spaces.
In the example you provided, there are no spaces between the emoticons, not even the words. So VADER considers it as a single word.</p>

<p>You can verify this using your code</p>

<pre><code>analyzer=SentimentIntensityAnalyzer()

sentences = [""Woohooo😍✌️"", ""Woohooo 😍 ✌️""]

for sentence in sentences:
    vs=analyzer.polarity_scores(sentence)
    print(""{:-&lt;65} {}"".format(sentence,str(vs)))
</code></pre>

<p>Output is:</p>

<pre><code>Woohooo😍✌️------------------------------------------------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}
Woohooo 😍 ✌️----------------------------------------------------- {'neg': 0.0, 'neu': 0.446, 'pos': 0.554, 'compound': 0.7351}
</code></pre>

<p>Hope this solves your problem.</p>
",3,0,921,2018-08-31 06:54:32,https://stackoverflow.com/questions/52110025/unable-to-predict-sentiment-of-emoticons
How to highlight negative and positive words in a Wordcloud using R,"<p>I am performing a sentiment analysis using R, and I was wondering how to split the wordcloud into two parts, highlighting positive and negative words. I am quite new to R and the online solutions didn't help me. That is the code:    </p>

<pre><code>text &lt;- readLines(""product1.txt"")

library(""tm"")
library(""SnowballC"")
library(""wordcloud"")
library(""RColorBrewer"")

docs &lt;- Corpus(VectorSource(text))

toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, "" "", x))
docs &lt;- tm_map(docs, toSpace, ""/"")
docs &lt;- tm_map(docs, toSpace, ""@"")
docs &lt;- tm_map(docs, toSpace, ""\\|"")

docs &lt;- tm_map(docs, content_transformer(tolower))
docs &lt;- tm_map(docs, removeNumbers)
docs &lt;- tm_map(docs, removeWords, stopwords(""english""))
docs &lt;- tm_map(docs, removeWords, c(""don"", ""s"", ""t"")) 
docs &lt;- tm_map(docs, removePunctuation)
docs &lt;- tm_map(docs, stripWhitespace)

dtm &lt;- TermDocumentMatrix(docs)
m &lt;- as.matrix(dtm)
v &lt;- sort(rowSums(m),decreasing=TRUE)
d &lt;- data.frame(word = names(v),freq=v)
head(d, 10)

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, ""Dark2""))
</code></pre>

<p>And this is the result I would like to achieve:</p>

<p><a href=""https://i.sstatic.net/IkkZz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IkkZz.png"" alt=""enter image description here""></a></p>

<p>Thanks for everyone will help me.</p>

<p>EDIT:</p>

<pre><code>docs &lt;- structure(list(content = c(""This product so far has not disappointed. My children love to use it and I like the ability to monitor control what content they see with ease."", 
""Great for beginner or experienced person. Bought as a gift and she loves it."", 
""Inexpensive tablet for him to use and learn on, step up from the NABI. He was thrilled with it, learn how to Skype on it already."", 
""I have had my Fire HD 8 two weeks now and I love it. This tablet is a great value.We are Prime Members and that is where this tablet SHINES. I love being able to easily access all of the Prime content as well as movies you can download and watch laterThis has a 1280/800 screen which has some really nice look to it its nice and crisp and very bright infact it is brighter then the ipad pro costing $900 base model. The build on this fire is INSANELY AWESOME running at only 7.7mm thick and the smooth glossy feel on the back it is really amazing to hold its like the futuristic tab in ur hands.""
), meta = structure(list(language = ""en""), class = ""CorpusMeta""), 
    dmeta = structure(list(), .Names = character(0), row.names = c(NA, 
    6L), class = ""data.frame"")), class = c(""SimpleCorpus"", ""Corpus""
))
</code></pre>
","r, text, sentiment-analysis, word-cloud, tidytext","<p>As seen in <a href=""https://www.tidytextmining.com/sentiment.html"" rel=""nofollow noreferrer"">the tutorial</a> , to have such result, you should have a lexicon, i.e. a ""dictionary"" that gives you if a word is positive or negative. Having that info, you can use it to color your wordcloud. <br>
We can comment the beautiful example in the link:</p>

<pre><code>library(janeaustenr)
library(dplyr)
library(stringr)

# here we tidy up the corpus, all the J.Austen books, having them cleaned and as result, a tibble with words.
tidy_books &lt;- austen_books() %&gt;%
  group_by(book) %&gt;%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex(""^chapter [\\divxlc]"", 
                                                 ignore_case = TRUE)))) %&gt;%
  ungroup() %&gt;%
  unnest_tokens(word, text)

library(wordcloud)
library(reshape2)
</code></pre>

<p>As stated, you need a lexicon. The link talk about various lexicon, in this case it's using the <code>bing</code> one:</p>

<pre><code>get_sentiments(""bing"")
# A tibble: 6,788 x 2
   word        sentiment
   &lt;chr&gt;       &lt;chr&gt;    
 1 2-faced     negative 
 2 2-faces     negative 
 3 a+          positive 
 4 abnormal    negative 
 5 abolish     negative 
 6 abominable  negative 
 7 abominably  negative 
 8 abominate   negative 
 9 abomination negative 
10 abort       negative 
# ... with 6,778 more rows
</code></pre>

<p>Now, joining every word of <code>tidy_books</code> (corpus) and the <code>bing</code> (lexicon) we can give a positive or negative value to each word:</p>

<pre><code>library(wordcloud)
library(reshape2)

 tidy_books %&gt;%
  inner_join(get_sentiments(""bing"")) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = ""n"", fill = 0) %&gt;%
  comparison.cloud(colors = c(""gray20"", ""gray80""),
                   max.words = 100)
</code></pre>

<p>And you'll have the desired output. Clearly you have to bend this to your data that I do not have.</p>

<p><a href=""https://i.sstatic.net/kC4aM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kC4aM.png"" alt=""enter image description here""></a></p>

<p><strong>EDIT</strong>:</p>

<p>Bended to your case, we can do this:</p>

<pre><code># take all the phrases
docs1 &lt;-tibble(phrases =docs$content)

# add an id, from 1 to n
docs1$ID &lt;- row.names(docs1)

# split all the words
tidy_docs &lt;- docs1 %&gt;% unnest_tokens(word, phrases)

#create now the cloud: a pair of warnings, because you do not have negative words and it is joining by word(correct)
tidy_docs %&gt;%
  inner_join(get_sentiments(""bing"")) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = ""n"", fill = 0) %&gt;%
  comparison.cloud(colors = c(""gray20"", ""gray80""),
                   max.words = 100)
</code></pre>

<p><a href=""https://i.sstatic.net/iLwyB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iLwyB.png"" alt=""enter image description here""></a></p>
",3,0,6422,2018-09-04 10:15:39,https://stackoverflow.com/questions/52163964/how-to-highlight-negative-and-positive-words-in-a-wordcloud-using-r
How does Google Language API split text into sentences to assign sentiment?,"<p>The question is in the title.</p>

<p>I have joined sentences into a large text, which I then call <code>analyze_sentiment</code> on. The goal is to pull sentiments for the individual sentences - exactly the ones originally joined. </p>

<p>I first clean out all punctuation, <code>lower</code> the characters, <code>capitalize</code> sentences, end them with <code>.</code> and <code>join</code> with a space. </p>

<p>Here is an example of two sentences that Google considers to be a single sentence.</p>

<blockquote>
  <p>She answered my questions with ease Thx. Tyler was so considerate.</p>
</blockquote>

<p>However, </p>

<blockquote>
  <p>She answered my questions with ease Thx. Sam was so considerate.</p>
</blockquote>

<p>works correctly. </p>

<p>You can try this yourself by going to their <a href=""https://cloud.google.com/natural-language/"" rel=""nofollow noreferrer"">natural-language page</a> and trying the API.</p>

<p>If I know the splitting conditions, I can format my original sentences accordingly. </p>
","google-api, google-cloud-platform, nlp, sentiment-analysis, google-natural-language","<p>It looks like the sentence boundaries model gets confused. I will open a bug for this from the Google side.</p>

<p>If you need to find sentiment for each sentence though, you can send the sentences individually to the API, so the sentence boundary issue doesn't get in your way. Are you concatenating the sentences due to save on quota or billing or latency? Because in terms of how the model works and calculation of the sentiment score, there is no difference between sending the sentences individually vs all in one big chunk.</p>
",2,2,866,2018-09-12 20:34:15,https://stackoverflow.com/questions/52303075/how-does-google-language-api-split-text-into-sentences-to-assign-sentiment
Extract emotions calculation for every row of a dataframe,"<p>I have a dataframe with rows of text. I would like to extract for each row of text a vector of specific emotion which will be a binary 0 is not exist this emotion or 1 is exist.<br> Totally they are 5 emotions but I would like to have the 1 only for the emotion which seem to be the most.</p>

<p>Example of what I have tried:</p>

<pre><code>library(tidytext)
text = data.frame(id = c(11,12,13), text=c(""bad movie"",""good movie"",""I think it would benefit religious people to see things like this, not just to learn about our home, the Universe, in a fun and easy way, but also to understand that non- religious explanations don't leave people hopeless and"",))
nrc_lexicon &lt;- get_sentiments(""nrc"")
</code></pre>

<p>Example of expected output:</p>

<pre><code>    id text sadness anger joy love neutral
11 ""bad movie"" 1 0 0 0 0
12 ""good movie"" 0 0 1 0 0 
</code></pre>

<p>Any hints will be helpful for me.</p>

<p>Example to make it for every row what is the next step?<br> How can I call every line with the nrc lexicon analysis?</p>

<pre><code>for (i in 1:nrow(text)) {
(text$text[i], nrc_lexicon)
}
</code></pre>
","r, text-mining, tidyr, sentiment-analysis","<p>What about this:</p>

<pre><code>library(tidytext)   # library for text
library(dplyr)

# your data
text &lt;- data.frame(id = c(11,12,13),
 text=c(""bad movie"",""good movie"",""I think it would benefit religious
 people to see things like this, not just to learn about our home, 
the Universe, in a fun and easy way, but also to understand that non- religious
 explanations don't leave people hopeless and""), stringsAsFactors = FALSE)  # here put this option, stringAsFactors = FALSE!

# the lexicon
nrc_lexicon &lt;- get_sentiments(""nrc"")

# now the job
unnested &lt;- text %&gt;%
             unnest_tokens(word, text) %&gt;%  # unnest the words
             left_join(nrc_lexicon) %&gt;%     # join with the lexicon to have sentiments
             left_join(text)                # join with your data to have titles
</code></pre>

<p>Here the output with the <code>id</code>, you can have it also with the titles, but I did not put it due the long third title, you can easily put it as <code>unnested$text</code> in place of <code>unnested$id</code>:</p>

<pre><code>table_sentiment &lt;- table(unnested$id, unnested$sentiment)
table_sentiment
     anger anticipation disgust fear joy negative positive sadness surprise trust
  11     1            0       1    1   0        1        0       1        0     0
  12     0            1       0    0   1        0        1       0        1     1
  13     0            1       0    1   1        2        3       2        1     0
</code></pre>

<p>And if you want it as <code>data.frame</code>:</p>

<pre><code> df_sentiment &lt;- as.data.frame.matrix(table_sentiment)
</code></pre>

<p>Now you can do everything you want, for example, if I remember well, you want a binary output if exist or not a sentiment:</p>

<pre><code>df_sentiment[df_sentiment&gt;1]&lt;-1
df_sentiment
   anger anticipation disgust fear joy negative positive sadness surprise trust
11     1            0       1    1   0        1        0       1        0     0
12     0            1       0    0   1        0        1       0        1     1
13     0            1       0    1   1        1        1       1        1     0
</code></pre>
",1,0,609,2018-09-30 08:41:45,https://stackoverflow.com/questions/52576238/extract-emotions-calculation-for-every-row-of-a-dataframe
filtering for n char inside a value in r,"<p>i'm doing sentiment analysis, but I need to filter by n char inside every tweet. I mean:</p>

<pre><code>df &lt;- c(""the most beauty"", ""the most ugly"", ""you are beauty"")
Library(dplyr)
df %&gt;%
filter((n char &gt;3) %in% df)
</code></pre>

<p>Im expecting a result like: ""most beauty"", ""ugly"", ""beauty""</p>

<p>I've tried with <code>$str_detect</code> but is useless</p>
","r, dplyr, analysis, sentiment-analysis","<p>We can do this with a regex to match words that have characters from 1 to 3 and replace it with blank (<code>""""</code>)</p>

<pre><code>gsub(""\\s*\\b[^ ]{1,3}\\b\\s*"", """", df)
#[1] ""most beauty"" ""most ugly""   ""beauty""  
</code></pre>

<p>NOTE: 'df' is a <code>vector</code> and not a <code>data.frame/tbl_df</code>.  So the <code>tidyverse</code> methods with <code>filter</code> won't work  </p>
",0,1,896,2018-10-10 15:05:30,https://stackoverflow.com/questions/52743298/filtering-for-n-char-inside-a-value-in-r
Creating a function to remove only specific word in a list (R),"<p>I have a list with undesirable words (in spanish) which are meaningless, but they are also present inside another. I just want to remove it when they are a term, not when they are a piece of another word.</p>

<p>For example: ""la"" is an spanish article, but if I use a function to remove it, also will break into two words a useful term like ""relacion"" (which means relationship)</p>

<p>My first choice was creating a function to remove this terms.</p>

<pre><code>bdtidy$tweet &lt;- #here are tweets
fix.useless &lt;- function(doc) {   
function(doc) {
doc &lt;- gsub(""la"", ""."", doc)
doc &lt;- gsub(""las"", ""."", doc)
doc &lt;- gsub(""el"", ""."", doc)
doc &lt;- gsub(""ellos"", ""."", doc)
doc &lt;- gsub(""ellas"", ""."", doc)
return(doc)
 }

bdtidy$tweet &lt;- sapply(bdtidy$tweet, fix.useless)
</code></pre>

<p>My second choice was with a list, and then using filter inside the df</p>

<pre><code>nousar &lt;- c(""rt"", ""pero"", ""para""...)
new df %&gt;% bdtidy %&gt;%
 filter(!tweet $in$ nousar))
</code></pre>

<p>But always the result is removing all those words and breaking terms in two words which makes my analysis useless.
Thanks.</p>
","r, dplyr, filtering, tidyverse, sentiment-analysis","<p>One way to remove single words from a string is by flanking the words with spaces, such as this example:</p>

<pre><code># sample input
x &lt;- c(""Get rid of la but not lala"")
# pattern with spaces flanking target word
y &lt;- gsub("" la "", "" "", x)
# output
&gt; y
[1] ""Get rid of but not lala""
</code></pre>
",2,0,2190,2018-10-10 18:51:21,https://stackoverflow.com/questions/52746947/creating-a-function-to-remove-only-specific-word-in-a-list-r
Attach sentiment to each word from a dataframe,"<p>I did a sentiment analysis of tweets but now I have to attach the sentiment to each word from the tweet text. My sentiment analysis was based on a sum of words that appeared in a dictionary. I hope this example can help you.</p>

<p>I tried to use this function but it does not work here.</p>

<pre><code>def append_sentiment(sentences, sentiment):
return [(word, sentiment) for sentence in sentences
                          for word in sentence.split()]

append_sentiment(df['text'], df['score'])
</code></pre>

<p>Example:</p>

<pre><code>id | text | score

12 | I like this | 2
</code></pre>

<p>Wanted result:</p>

<pre><code>id | text | score

12 | ('I', 2), ('like', 2), ('this', 2) | 2
</code></pre>
","python, pandas, dataframe, sentiment-analysis","<p>You can construct <code>(word, sentiment)</code> tuples easily with the use of <a href=""https://docs.python.org/3/library/itertools.html#itertools.repeat"" rel=""nofollow noreferrer""><code>itertools.repeat</code></a>:</p>

<pre><code>from itertools import repeat

mapped = df.apply(lambda row: list(zip(row.text.split(), repeat(row.score))), axis=1)
print(mapped)
0    [(I, 2), (like, 2), (this, 2)]
</code></pre>
",1,3,222,2018-10-13 14:14:20,https://stackoverflow.com/questions/52793833/attach-sentiment-to-each-word-from-a-dataframe
"TensorFlow ValueError: Cannot feed value of shape (32, 2) for Tensor &#39;InputData/X:0&#39;, which has shape &#39;(?, 100)&#39;","<p>I am new to TensorFlow and machine learning. I'm trying to create a sentiment analysis NN with tensorflow.</p>

<p>I've set up my architecture and I'm attempting to train the model but I encounter the error </p>

<blockquote>
  <p>ValueError: Cannot feed value of shape (32, 2) for Tensor 'InputData/X:0', which has shape '(?, 100)'</p>
</blockquote>

<p>I think the error has to do with my input ""layer net = tflearn.input_data([None, 100])"".
The tutorial I was following suggested this input shape, batch size as None and the length to be 100 since that's the sequence length. Hence (None, 100), to my understanding this is the dimensions the training data being fed into the network needs to be, correct?</p>

<p>Could someone explain why the suggested input shape of batch size was None and also why Tensor flow is attempting to feed the network put shaped (32,2). Where is the sequence length of 2 coming from? </p>

<p>If my understanding anywhere in this explanation is wrong feel free to correct me, I'm still trying to learn the theory as well.</p>

<p>Thanks in advance</p>

<pre><code>In [1]:

import tflearn
from tflearn.data_utils import to_categorical, pad_sequences
from tflearn.datasets import imdb

In [2]:

#Loading IMDB dataset
train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,
                                valid_portion=0.1)
trainX, trainY = train
testX, testY = test

In [3]:

#Data sequence padding 
trainX = pad_sequences(trainX, maxlen=100, value=0.)  
testX = pad_sequences(testX, maxlen=100, value=0.)
#converting labels of each review to vectors
trainY = to_categorical(trainY, nb_classes=2)
trainX = to_categorical(testY, nb_classes=2)


In [4]:

#network building 
net = tflearn.input_data([None, 100])
net = tflearn.embedding(net, input_dim=10000, output_dim=128)
net = tflearn.lstm(net, 128, dropout = 0.8)
net = tflearn.fully_connected(net, 2, activation='softmax') 
net = tflearn.regression(net, optimizer = 'adam', learning_rate=0.0001,
                         loss='categorical_crossentropy')


WARNING:tensorflow:From C:\Users\Nason\Anaconda33\envs\TensorFlow1.8CPU\lib\site-packages\tflearn\objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead


In [5]:

#Training
model = tflearn.DNN(net, tensorboard_verbose=0)   #train using tensorflow Deep nueral net
model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,    #fit launches training process for training and validation data, metric displays data as its training.
          batch_size=32)


---------------------------------
Run id: U7NONK
Log directory: /tmp/tflearn_logs/
INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.
---------------------------------
Training samples: 2500
Validation samples: 2500
--

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-5-7ffd0a8836f9&gt; in &lt;module&gt;()
      2 model = tflearn.DNN(net, tensorboard_verbose=0)   #train using tensorflow Deep nueral net
      3 model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,    #fit launches training process for training and validation data, metric displays data as its training.
----&gt; 4           batch_size=32)

~\Anaconda33\envs\TensorFlow1.8CPU\lib\site-packages\tflearn\models\dnn.py in fit(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)
    214                          excl_trainops=excl_trainops,
    215                          run_id=run_id,
--&gt; 216                          callbacks=callbacks)
    217 
    218     def fit_batch(self, X_inputs, Y_targets):

~\Anaconda33\envs\TensorFlow1.8CPU\lib\site-packages\tflearn\helpers\trainer.py in fit(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)
    337                                                        (bool(self.best_checkpoint_path) | snapshot_epoch),
    338                                                        snapshot_step,
--&gt; 339                                                        show_metric)
    340 
    341                             # Update training state

~\Anaconda33\envs\TensorFlow1.8CPU\lib\site-packages\tflearn\helpers\trainer.py in _train(self, training_step, snapshot_epoch, snapshot_step, show_metric)
    816         tflearn.is_training(True, session=self.session)
    817         _, train_summ_str = self.session.run([self.train, self.summ_op],
--&gt; 818                                              feed_batch)
    819 
    820         # Retrieve loss value from summary string

~\Anaconda33\envs\TensorFlow1.8CPU\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    898     try:
    899       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 900                          run_metadata_ptr)
    901       if run_metadata:
    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\Anaconda33\envs\TensorFlow1.8CPU\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1109                              'which has shape %r' %
   1110                              (np_val.shape, subfeed_t.name,
-&gt; 1111                               str(subfeed_t.get_shape())))
   1112           if not self.graph.is_feedable(subfeed_t):
   1113             raise ValueError('Tensor %s may not be fed.' % subfeed_t)

ValueError: Cannot feed value of shape (32, 2) for Tensor 'InputData/X:0', which has shape '(?, 100)'
</code></pre>
","python, tensorflow, sentiment-analysis, training-data, valueerror","<p>The error comes from <code>trainX = to_categorical(testY, nb_classes=2)</code>. This needs to be changed to <code>testY = to_categorical(testY, nb_classes=2)</code></p>

<p>Also, setting the batch size to <code>None</code> means it should expect the batch to be any size. In your case you set the batch size to 32 so you could also set the input shape to <code>[32, 100]</code></p>
",0,0,967,2018-10-15 17:35:43,https://stackoverflow.com/questions/52822022/tensorflow-valueerror-cannot-feed-value-of-shape-32-2-for-tensor-inputdata
Data set for Doc2Vec general sentiment analysis,"<p>I am trying to build doc2vec model, using gensim + sklearn to perform sentiment analysis on short sentences, like comments, tweets, reviews etc.</p>

<p>I downloaded <a href=""http://jmcauley.ucsd.edu/data/amazon/"" rel=""nofollow noreferrer"">amazon product review data set</a>, <a href=""https://www.kaggle.com/c/twitter-sentiment-analysis2"" rel=""nofollow noreferrer"">twitter sentiment analysis data set</a> and <a href=""https://www.kaggle.com/utathya/imdb-review-dataset"" rel=""nofollow noreferrer"">imbd movie review data set</a>.</p>

<p>Then combined these in 3 categories, positive, negative and neutral.</p>

<p>Next I trinaed gensim doc2vec model on the above data so I can obtain the input vectors for the classifying neural net.</p>

<p>And used sklearn LinearReggression model to predict on my test data, which is about 10% from each of the above three data sets.</p>

<p>Unfortunately the results were not good as I expected. Most of the tutorials out there seem to focus only on one specific task, 'classify amazon reviews only' or 'twitter sentiments only', I couldn't manage to find anything that is more general purpose.</p>

<p>Can some one share his/her thought on this? </p>
","dataset, artificial-intelligence, gensim, sentiment-analysis, doc2vec","<p>How good did you expect, and how good did you achieve? </p>

<p>Combining the three datasets may not improve overall sentiment-detection ability, if the signifiers of sentiment vary in those different domains. (Maybe, 'positive' tweets are very different in wording than product-reviews or movie-reviews. Tweets of just a few to a few dozen words are often quite different than reviews of hundreds of words.) Have you tried each separately to ensure the combination is helping? </p>

<p>Is your performance in line with other online reports of using roughly the same pipeline (Doc2Vec + LinearRegression) on roughly the same dataset(s), or wildly different? That will be a clue as to whether you're doing something wrong, or just have too-high expectations. </p>

<p>For example, the <code>doc2vec-IMDB.ipynb</code> notebook bundled with <code>gensim</code> tries to replicate an experiment from the original 'Paragraph Vector' paper, doing sentiment-detection on an IMDB dataset. (I'm not sure if that's the same dataset as you're using.) Are your results in the same general range as that notebook achieves? </p>

<p>Without seeing your code, and details of your corpus-handling &amp; parameter choices, there could be all sorts of things wrong. Many online examples have nonsense choices. But maybe your expectations are just off.</p>
",1,0,388,2018-10-16 19:11:06,https://stackoverflow.com/questions/52842474/data-set-for-doc2vec-general-sentiment-analysis
How to pass a string value to a Sentiment Analysis RNN Sequential Model and get back a prediction,"<p>I recreated a sentiment analysis machine learning project using my own data set along with some minor modifications to improve its completion time, I can create good model, compile it, fit it and test it without issues, the problem comes however on how to pass the model a new string/ article and it in return pass a prediction on whether the string comments are positive or negative and was hoping someone could help me.</p>

<p>I posted my code below for your review.</p>

<pre><code>class tensor_rnn():
def __init__(self, corp_paths, hidden_layers=3, loadfile=True):
    self.h_layers = hidden_layers
    self.num_words = []
    if loadfile == False:
        data_set = pd.DataFrame(columns=['Article', 'Polarity'])
        craptopass = []
        for files in os.listdir(corp_paths[0]):
            with open(corp_paths[0] + '\\' + files, 'r', errors='replace') as text_file:
                line = text_file.readline().replace('|', '')
                text_file.close()
            if len(line.split(' ')) &gt; 3:
                line = ''.join([i if ord(i) &lt; 128 else ' ' for i in line])
                craptopass.append([line, 1])
        good = data_set.append(pd.DataFrame(craptopass, columns=['Article', 'Polarity']), ignore_index=True)
        data_set = pd.DataFrame(columns=['Article', 'Polarity'])
        craptopass = []
        for files in os.listdir(corp_paths[1]):
            with open(corp_paths[1] + '\\' + files, 'r', errors='replace') as text_file:
                line = text_file.readline().replace('|', '')
                text_file.close()
            if len(line.split(' ')) &gt; 3:
                line = ''.join([i if ord(i) &lt; 128 else ' ' for i in line])
                craptopass.append([line, -1])
        bad = data_set .append(pd.DataFrame(craptopass, columns=['Article', 'Polarity']), ignore_index=True)
        for line in good['Article'].tolist():
            counter = len(line.split())
            self.num_words.append(counter)

        for line in bad['Article'].tolist():
            counter = len(line.split())
            self.num_words.append(counter)
        self.features = pd.concat([good, bad]).reset_index(drop=True)
        # self.features = self.features.str.replace(',', '')
        self.features.to_csv('Headlines.csv', sep='|')
    else:
        self.features = pd.read_csv('Headlines.csv', sep='|')
        self.features['totalwords'] = self.features['Article'].str.count(' ') + 1
        self.num_words.extend(self.features['totalwords'].tolist())

    self.features = shuffle(self.features)
    self.max_len = len(max(self.features['Article'].tolist()))
    tokenizer = self.tok = preprocessing.text.Tokenizer(num_words=len(self.num_words), split=' ')
    self.tok.fit_on_texts(self.features['Article'].values)
    X = tokenizer.texts_to_sequences(self.features['Article'].values)
    self.X = preprocessing.sequence.pad_sequences(X)
    self.Y = pd.get_dummies(self.features['Polarity']).values
    self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.X, self.Y,
                                                                            test_size=0.20, random_state=36)

def RNN(self):
    embed_dim = 128
    lstm_out = 128
    model = Sequential()
    model.add(Embedding(len(self.num_words), embed_dim, input_length=self.X.shape[1]))
    model.add(Bidirectional(CuDNNLSTM(lstm_out)))
    model.add(Dropout(0.2))
    model.add(Dense(2, activation='softmax'))
    opt = Adam(lr=0.0001, decay=1e-4)   #1e-3
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

def model_train(self):
    self.model = self.RNN()

def model_test(self):
    batch_size = 128
    self.model.fit(self.X_train, self.Y_train, epochs=4, batch_size=batch_size, verbose=2,
                                callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0001,
                                                         patience=5, verbose=2, mode='auto')], validation_split=0.2)


if __name__ == ""__main__"":
    paths = 'PATHS TO ARTICLES'
    a = tensor_rnn([paths + '\\pos', paths + '\\neg'])
    a.model_train()
    a.model_test()
    a.model.save('RNNModelArticles.h5', include_optimizer=True)
</code></pre>
","python, machine-learning, keras, recurrent-neural-network, sentiment-analysis","<p>All you need to do is preprocess the new text that you want to feed to the model the same way you preprocessed text for the training. After that, you should have a predict method that will output it's prediction in the same way the model outputs prediction in the training. So, in the predict method you should write something like:</p>

<pre><code>def predict(self, sequence):
  presprocessed = preprocess(sequence)
  prediction = self.model.predict(preprocessed, batch_size=None, verbose=0, steps=None)
</code></pre>

<p>Does this clarify things for you?</p>
",1,0,335,2018-10-23 05:58:43,https://stackoverflow.com/questions/52942038/how-to-pass-a-string-value-to-a-sentiment-analysis-rnn-sequential-model-and-get
"Sentiment Analysis, Naive Bayes Accuracy","<p>I'm trying to form a Naive Bayes Classifier script for sentiment classification of tweets. I'm pasting my whole code here, because I know I will get hell if I don't. So I basically I use NLTK's corpuses as training data, and then some tweets I scraped as test data. I pre-process them and do a bag of words extraction. The classifier is trained with no problem and when I do the following</p>
<pre><code>print(classifier.classify(bag_of_words('This is magnificent')))  
</code></pre>
<p>it correctly outputs 'pos'.</p>
<p>Now my problem is how to calculate accuracy using ntlk.util accuracy. I do</p>
<pre><code>print(nltk.classify.accuracy(classifier, proc_set))
</code></pre>
<p>and I get the following error:</p>
<pre><code>  File &quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-   packages/nltk/classify/util.py&quot;, line 87, in accuracy
  results = classifier.classify_many([fs for (fs, l) in gold])
  AttributeError: 'NaiveBayesClassifier' object has no attribute 'classify_many'
</code></pre>
<p>I also tried this</p>
<pre><code>test_set_final=[]
for tweet in proc_test:
test_set_final.append((bag_of_words(tweet),   classifier.classify(bag_of_words(tweet))))

print(nltk.classify.accuracy(classifier, test_set_final))
</code></pre>
<p>and I get the same kind of error</p>
<pre><code>print(nltk.classify.accuracy(classifier, test_set_final))
File &quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/classify/util.py&quot;, line 87, in accuracy
results = classifier.classify_many([fs for (fs, l) in gold])
AttributeError: 'NaiveBayesClassifier' object has no attribute 'classify_many'
</code></pre>
<p>Code:</p>
<pre><code>import nltk
import ast
import string
import re
import csv
import textblob
import pandas as pd
import numpy as np
import itertools
from textblob import TextBlob
from textblob import Word
from textblob.classifiers import NaiveBayesClassifier
from nltk.corpus import twitter_samples
from nltk.corpus import stopwords
from nltk.corpus import wordnet as wd
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from random import shuffle
from nltk.classify.util import accuracy
from autocorrect import spell

stopwords = stopwords.words('english')
lemmatizer = nltk.WordNetLemmatizer().lemmatize
punct=['&quot;','$','%','&amp;','\',''','(',')','+',',','-     ','.','/',':',';','&lt;','=','&gt;','@','[','\',','^','_','`','{','|','}','~']

emoticons_happy = set([
':)', ';)', ':o)', ':]', ':3', ':c)', ':&gt;', '=]', '8)', '=)', ':}',
':^)', ':-D', ':D', ': D','8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',
'=-3', '=3', ':-))', ':-)', &quot;:'-)&quot;, &quot;:')&quot;, ':*', ':^*', '&gt;:P', ':-P', ':P', 'X-P',
'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '&gt;:)', '&gt;;)', '&gt;:-)',
'&lt;3',':*', ':p'
])

emoticons_sad = set([
':L', ':-/', '&gt;:/', ':S', '&gt;:[', ':@', ':-(', ':[', ':-||', '=L', ':&lt;',
':-[', ':-&lt;', '=\\', '=/', '&gt;:(', ':-(', '&gt;.&lt;', &quot;:'-(&quot;, &quot;:'(&quot;, ':\\', ':-c',
':c', ':{', '&gt;:\\', ';('
])
emoticons = emoticons_happy.union(emoticons_sad)


def pre_process(tweet):

    tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)

    tweet = re.sub(r'#', '', tweet)

    tweet=''.join([i for i in tweet if not i.isdigit()])

    tweet=re.sub(r'([.,/#!$%^&amp;*;:{}=_`~-])([.,/#!$%^&amp;*;:{}=_`~-]+)\1+', r'\1',tweet)

    tweet = re.sub(r'@[A-Za-z0-9]+', '', tweet)

    tweet=''.join([i for i in tweet if i not in emoticons])

    tweet=''.join([i for i in tweet if i not in punct])

    tweet=' '.join([i for i in tweet.split() if i not in stopwords])

    tweet=tweet.lower()

    tweet=lemmatize(tweet)

    return tweet

def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wd.ADJ
    elif treebank_tag.startswith('V'):
        return wd.VERB
    elif treebank_tag.startswith('N'):
        return wd.NOUN
    elif treebank_tag.startswith('R'):
        return wd.ADV
    else:
        return wd.NOUN

def lemmatize(tt):
    pos = nltk.pos_tag(nltk.word_tokenize(tt))
    lemm = [lemmatizer(sw[0], get_wordnet_pos(sw[1])) for sw in pos]
    sentence= ' '.join([i for i in lemm])

    return sentence


test_tweets=[]
file=open('scraped_tweets.csv', 'r')
reader = csv.reader(file)
for line in reader:
    line=line[1]
    test_tweets.append(line)

pos_tweets = twitter_samples.strings('positive_tweets.json')
neg_tweets = twitter_samples.strings('negative_tweets.json')



proc_train_pos=[]
for tweet in pos_tweets:
    proc_train_pos.append(pre_process(tweet))
proc_train_neg=[]
for tweet in neg_tweets:
    proc_train_neg.append(pre_process(tweet))
proc_test=[]
for tweet in test_tweets:
    proc_test.append(pre_process(tweet))


def bag_of_words(tweet):
    words_dictionary = dict([word, True] for word in tweet.split())    
    return words_dictionary

pos_tweets_set = []
for tweet in proc_train_pos:
    pos_tweets_set.append((bag_of_words(tweet), 'pos'))    

neg_tweets_set = []
for tweet in proc_train_neg:
    neg_tweets_set.append((bag_of_words(tweet), 'neg'))

shuffle(pos_tweets_set)
shuffle(neg_tweets_set)
train_set = pos_tweets_set+neg_tweets_set

classifier = NaiveBayesClassifier(train_set)
print('Training is done')

#print(classifier.classify(bag_of_words('This is magnificent'))) #output 'pos'

print(nltk.classify.accuracy(classifier, proc_set))
</code></pre>
","python, scikit-learn, nltk, sentiment-analysis, naivebayes","<p>Well, as the error message says, the classifier you are trying to use (<code>NaiveBayesClassifier</code>) doesn't have the method <code>classify_many</code> that the <code>nltk.classify.util.accuracy</code> function requires.</p>
<p>(Reference: <a href=""https://www.nltk.org/_modules/nltk/classify/naivebayes.html"" rel=""nofollow noreferrer"">https://www.nltk.org/_modules/nltk/classify/naivebayes.html</a>)</p>
<p>Now, that looks like an NLTK bug, but you can get your answer easily on your own:</p>
<pre><code>from sklearn.metrics import accuracy_score

y_predicted = [classifier.classify(x) for x in proc_set]

accuracy = accuracy_score(y_true, y_predicted)
</code></pre>
<p>Where <code>y_true</code> are the sentiment values corresponding to <code>proc_set</code> inputs (which I don't see you actually creating in your code shown above, though).</p>
<p>Or, without using the <code>sklearn</code> accuracy function, but pure Python:</p>
<pre><code>hits = [yp == yt for yp, yt in zip(y_predicted, y_true)]

accuracy = sum(hits)/len(hits)
</code></pre>
",0,0,825,2018-10-28 19:59:34,https://stackoverflow.com/questions/53035563/sentiment-analysis-naive-bayes-accuracy
Sentiment wordcloud using R&#39;s quanteda?,"<p>I have a set of reviews (comment in words + rating from 0-10) and I want to create a sentiment word cloud in R, in which:</p>

<ul>
<li>A word's size represents its frequency</li>
<li>A word's color represents the average rating of all reviews it occurs in (preferably a color gradient green-yellow-red)</li>
</ul>

<p>I used quanteda to create a <code>dfm</code> of the comments. Now I think I want to use the <code>textplot_wordcloud</code> function and I guess I need to do the following:</p>

<ol>
<li>For each word, get all the reviews it appeared in</li>
<li>Calculate the average rating of this subset of reviews</li>
<li>Divide by 10 to scale to 0-1 and assign this value to this word</li>
<li>Sort the words by average rating (so that the colors are assigned correctly?)</li>
<li>Use <code>color=RColorBrewer::brewer.pal(11, ""RdYlGn"")</code> to calculate colors from the average ratings</li>
</ol>

<p>I'm having trouble with step 1 and 4. The rest should be doable. Can somebody explain how a <code>dfm</code> can be read manipulated easily?</p>
","r, sentiment-analysis, word-cloud, quanteda","<p>I found an efficient way to do this using matrix multiplication: basically the functionality is <code>sw = sd * C / Nw</code>, where:</p>

<ul>
<li><code>sw</code> = sentiment per word</li>
<li><code>sd</code> = ratings per document</li>
<li><code>C</code> = per-document word frequency matrix</li>
<li><code>Nw</code> = number of occurences per word</li>
</ul>

<p>In code:</p>

<pre><code># create the necessary variables
sd &lt;- as.integer(df$rating)
C &lt;- as.matrix(my_dfm)
Nw &lt;- as.integer(colSums(C))

# calculate the word sentiment
sw &lt;- as.integer(s_d %*% C) / n_w

# normalize the word sentiment to values between 0 and 1
sw &lt;- (sw - min(sw)) / (max(sw) - min(sw)

# make a function that converts a sentiment value to a color
num_to_color &lt;- seq_gradient_pal(low=""#FF0000"", high=""#00FF00"")

# apply the function to the sentiment values
word_colors &lt;- num_to_color(sw)

# create a new window; 
# before executing the next command, manually maximize in order to get a better readable wordcloud
dev.new()

# create the wordcloud with the calculated color values
textplot_wordcloud(my_dfm, color=word_colors)
</code></pre>
",2,1,374,2018-10-30 08:08:18,https://stackoverflow.com/questions/53059849/sentiment-wordcloud-using-rs-quanteda
Classifying negative and positive words in large files?,"<p>I am trying to get the count of positive and negative in a very large file. I only need a primitive approach(that does not take ages). I have tried sentiwordnet but keep getting a <code>IndexError: list index out of range</code>, which I think it's due to the words not being listed in wordnet dictionary. The text contains a lot of typos and 'non-words'.</p>

<p>If someone could give any suggestion, I would be very grateful!</p>
","nlp, nltk, sentiment-analysis, wordnet, senti-wordnet","<p>It all depends on what your data is like and what is the final objective of your task. You need to give us a little bit more detailed description of your project but, in general, here are your options:
- Make your own sentiment analysis dictionary: I really doubt this is what you want to do since it takes a lots of time and effort but if your data is simple enough it's doable.
- Clean your data: if your tokens aren't in senti-wordnet because there's too much noise and badly spelled words, then try to correct them before passing them through wordnet, it will at least limit the number of errors you'll get.
- Use a senti-wordnet alternative: accorded, there aren't that many good ones but you can always try <a href=""https://pypi.org/project/sentiment_classifier/"" rel=""nofollow noreferrer"">sentiment_classifier</a> or <a href=""https://www.nltk.org/api/nltk.sentiment.html"" rel=""nofollow noreferrer"">nltk's sentiment</a> if you're using python (which by the looks of your error seems like you are).
- Classify only what you can: this is what I would recommend. If the word is not in senti-wordnet, then move on to the next one. Just catch the error (<code>try: ... except IndexError: pass</code>) and try to infer what the general sentiment of the data is by counting the sentiment words you actually catch.</p>

<p>PS: We would need to see your code to be sure but I think there's another reason why you're getting an IndexError. If the word was not in senti-wordnet you would be getting a KeyError, but it also depends on how you coded your function. </p>

<p>Good luck and I hope it was helpful.</p>
",1,0,463,2018-11-01 13:45:10,https://stackoverflow.com/questions/53102577/classifying-negative-and-positive-words-in-large-files
Specifics on sentiment analysis,"<p>I am working on a bot that uses sentiment analysis to analyze questions from people in the health care insurance realm.  One of the main things we do is connect people with care through a find care tool.  The basic questions people ask are in the form ""I need a new doctor"" or ""I need to see a dermatologist"".  When the sentiment is negative we want to pass the user on to customer service, but as we just discovered, any statement starting with ""I need"" is considered a negative statement by LUIS' built in sentiment analysis.  This is a big problem for us!</p>

<p>If there is a way to tweak that, please someone let me know, though I don't hold out much hope.  But I would like to understand - why was that decision made?  Why is a simple ""I need"" question considered negative instead of neutral?</p>
","sentiment-analysis, azure-language-understanding","<p>Sentiment analysis cannot be changed as LUIS relies on Microsoft's Text Analytics to produce a result. In addition to Ferdinand's comment suggestion, you could do one of the following, as well.</p>

<p>Suggest different utterances that produce a more positive result. Using 'find a doctor' produces a positive result where as 'I need a doctor' does not.</p>

<p>The Azure team has a ML learning lab <a href=""https://github.com/Azure/LearnAI-CustomTextAnalyticsWithAML"" rel=""nofollow noreferrer"">repo</a> that shows how to develop your own text analytics. It would require more effort but would offer you the ability to get the kind of sentiment analysis you are looking for.</p>
",1,2,86,2018-11-07 22:31:37,https://stackoverflow.com/questions/53198891/specifics-on-sentiment-analysis
Python: Split text by keyword into excel rows,"<p>New to programming, found a lot of helpful threads already, but just not quite what I need.<br>
I have one text file that looks like:  </p>

<pre><code>  1 of 5000 DOCUMENTS


                    Copyright 2010 The Deal, L.L.C.
                          All Rights Reserved
                          Daily Deal/The Deal

                        January 12, 2010 Tuesday

HEADLINE: Cadbury slams Kraft bid

BODY:

  On cue .....

......

body of article here

......

DEAL SIZE

$ 10-50 Billion

                            2 of 5000 DOCUMENTS


                    Copyright 2015 The Deal, L.L.C.
                          All Rights Reserved
                           The Deal Pipeline

                      September 17, 2015 Thursday

HEADLINE: Perrigo rejects formal offer from Mylan

BODY: 
(and here again the body of this article)

DEAL SIZE
</code></pre>

<p>As output I would like JUST the body of every article in a new row (one cell per article body) in one file (I have around 5000 articles to process like this). The output would be 5000 rows and 1 column.
From what I could find it seems 're' would be the best solution. So the recurring keywords are BODY: and perhaps DOCUMENTS. How do I extract just the text between those keywords into a new row in excel for every article?</p>

<pre><code>import re
inputtext = 'F:\text.txt'
re.split(r'\n(?=BODY:)', inputtext)
</code></pre>

<p>or something like this?</p>

<pre><code>section = []
for line in open_file_object:
if line.startswith('BODY:'):
    # new section
    if section:
        process_section(section)
    section = [line]
else:
    section.append(line)
if section:
process_section(section)
</code></pre>

<p>I'm a bit lost in where to look, thanks in advance!  </p>

<p>EDIT: Thanks to ewwink I'm currently here:</p>

<pre><code>import re
articlesBody = None
with open('F:\CloudStation\Bocconi University\MSc. Thesis\\test folder\majortest.txt', 'r') as txt:
  inputtext = txt.read()
  articlesBody = re.findall(r'BODY:(.+?)\d\sDOCUMENTS', inputtext, re.S)

#print(articlesBody)
#print(type(articlesBody))

  with open('result.csv', 'w') as csv:
   for item in articlesBody:
    item = item.replace('\n', ' ')
    csv.write('""%s"",' % item)
</code></pre>
","python, regex, text, extract, sentiment-analysis","<p>working with file use <code>with open('F:\text.txt', mode)</code> where <code>mode</code> are <code>'r'</code> for reading and <code>'w'</code> for writing, to extract the content use <code>re.findall</code> and finally you need to escape new line <code>\n</code>, double quotes <code>""</code>and maybe other character.</p>

<pre><code>import re

articlesBody = None
with open('text.txt', 'r') as txt:
  inputtext = txt.read()
  articlesBody = re.findall(r'BODY:(.+?)\d\sof\s5000', inputtext, re.S)

#print(articlesBody)

with open('result.csv', 'w') as csv:
  for item in articlesBody:
    item = item.replace('\n', '\\n').replace('""', '""""')
    csv.write('""%s"",' % item)
</code></pre>

<p>another note: try with small content</p>
",1,0,517,2018-11-08 12:58:52,https://stackoverflow.com/questions/53208256/python-split-text-by-keyword-into-excel-rows
Workaround for python MemoryError,"<p>How can I change this function to make it more efficient? I keep getting MemoryError</p>

<pre><code>def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results
</code></pre>

<p>I call the function here:</p>

<pre><code>x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
</code></pre>

<p>Train and Test data are IMDB dataset for sentiment analysis, i.e.</p>

<pre><code>(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
</code></pre>

<p>EDIT: I am running this on 64 bit Ubuntu system with 4 GB RAM.</p>

<p>Here is the Traceback:</p>

<pre><code>Traceback (most recent call last):

  File ""/home/uttam/PycharmProjects/IMDB/imdb.py"", line 29, in &lt;module&gt;
    x_test = vectorize_sequences(test_data)
  File ""/home/uttam/PycharmProjects/IMDB/imdb.py"", line 20, in vectorize_sequences
    results = np.zeros((len(sequences), dimension))
MemoryError
</code></pre>
","python, keras, sentiment-analysis","<p>Your array appears to be 10k x 10k which is 100 million elements of 64 bits each (because the default dtype is float64).  So that's 800 million bytes, aka 763 megabytes.</p>

<p>If you use float32 it will cut the memory usage in half:</p>

<pre><code>np.zeros((len(sequences), dimension), dtype=np.float32)
</code></pre>

<p>Or if you only care about 0 and 1, this will cut it by 88%:</p>

<pre><code>np.zeros((len(sequences), dimension), dtype=np.int8)
</code></pre>
",2,2,1989,2018-11-11 14:20:34,https://stackoverflow.com/questions/53249636/workaround-for-python-memoryerror
How do i make R show its computations (sentiment analysis),"<p>When R is computing the sentiment values (package syuzhet), can I somehow get the program to show the exact computations leading up to, say a value 2?</p>

<p>I get that it assigns (according to our lexicon) for example +1 to the sentence-value if the word ""good"" shows up somewhere, and it subtracts 1 if ""bad"" is found. But i would like an output for each sentence reading something like this:</p>

<pre><code>sentence [1]
The movie was very good, it had fun dialogue and great acting, but the ending was sad.
sentiment value [1]
""the"" 0, ""movie"" 0, ""was"" 0, ""very"" 0, ""good""+1 ... ""fun""+1 ... ""great""+1 ... ""sad""-1 = 2
</code></pre>

<p>There must(?) be a command for this, but i can't for the life of me find it.</p>
","r, sentiment-analysis","<p>Alright, using the example text from the <a href=""https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html"" rel=""nofollow noreferrer""><code>syuzhet</code> vignette</a>.</p>

<pre><code>library(syuzhet)

my_example_text &lt;- ""I begin this story with a neutral statement.  
  Basically this is a very silly test.  
  You are testing the Syuzhet package using short, inane sentences.  
  I am actually very happy today. 
  I have finally finished writing this package.  
  Tomorrow I will be very sad. 
  I won't have anything left to do. 
  I might get angry and decide to do something horrible.  
  I might destroy the entire package and start from scratch.  
  Then again, I might find it satisfying to have completed my first R package. 
  Honestly this use of the Fourier transformation is really quite elegant.  
  You might even say it's beautiful!""
</code></pre>

<p>Getting a sentiment value per sentence is simple to do using <code>sapply()</code></p>

<pre><code>sapply(get_sentences(my_example_text), get_sentiment)
# I begin this story with a neutral statement. 
#                                         0.00 
#         Basically this is a very silly test. 
#                                        -0.25 
#                                          ...
</code></pre>

<p>And getting a sentiment value per word can be done by using <code>get_sent_values()</code></p>

<pre><code>get_sent_values(""happy"")
# [1] 0.75
</code></pre>

<p>But to get an output like you describe we'll have to tinker a little</p>

<pre><code>wordsentiments &lt;- function(x, method=""syuzhet"") {
    word_l &lt;- strsplit(tolower(x), ""[^A-Za-z']+"")[[1]]
    val &lt;- sapply(word_l, get_sent_values, method)
    l &lt;- length(word_l) + 1
    word_l[l] &lt;- ""TOTAL""
    val[l] &lt;- sum(val)
    names(val) &lt;- NULL
    data.frame(value=val, word=word_l, stringsAsFactors=FALSE)
}

lapply(get_sentences(my_example_text), wordsentiments)
# [[1]]
#   value      word
# 1     0         i
# 2     0     begin
# 3     0      this
# 4     0     story
# 5     0      with
# 6     0         a
# 7     0   neutral
# 8     0 statement
# 9     0     TOTAL

# [[2]]
#   value      word
# 1  0.00 basically
# 2  0.00      this
# 3  0.00        is
# 4  0.00         a
# 5  0.00      very
# 6 -0.25     silly
# 7  0.00      test
# 8 -0.25     TOTAL

# ...
</code></pre>
",2,1,70,2018-11-14 15:28:53,https://stackoverflow.com/questions/53303596/how-do-i-make-r-show-its-computations-sentiment-analysis
Any efficient way to find surrounding ADJ respect to target phrase in python?,"<p>I am doing sentiment analysis on given documents, my goal is I want to find out the closest or surrounding adjective words respect to target phrase in my sentences. I do have an idea how to extract surrounding words respect to target phrases, but How do I find out relatively close or closest adjective or <code>NNP</code> or <code>VBN</code> or other POS tag respect to target phrase.</p>

<p>Here is the sketch idea of how I may get surrounding words to respect to my target phrase.</p>

<pre><code>sentence_List= {""Obviously one of the most important features of any computer is the human interface."", ""Good for everyday computing and web browsing."",
""My problem was with DELL Customer Service"", ""I play a lot of casual games online[comma] and the touchpad is very responsive""}

target_phraseList={""human interface"",""everyday computing"",""DELL Customer Service"",""touchpad""}
</code></pre>

<p>Note that my original dataset was given as dataframe where the list of the sentence and respective target phrases were given. Here I just simulated data as follows:</p>

<pre><code>import pandas as pd
df=pd.Series(sentence_List, target_phraseList)
df=pd.DataFrame(df)
</code></pre>

<p>Here I tokenize the sentence as follow:</p>

<pre><code>from nltk.tokenize import word_tokenize
tokenized_sents = [word_tokenize(i) for i in sentence_List]
tokenized=[i for i in tokenized_sents]
</code></pre>

<p>then I try to find out surrounding words respect to my target phrases by using this <a href=""https://stackoverflow.com/questions/17645701/extract-words-surrounding-a-search-word"">loot at here</a>. However, I want to find out relatively closer or closet <code>adjective</code>, or <code>verbs</code> or <code>VBN</code> respect to my target phrase. How can I make this happen? Any idea to get this done? Thanks</p>
","python, parsing, nlp, sentiment-analysis","<p>Would something like the following work for you? I recognize there are some tweaks that need to be made to make this fully useful (checking for upper/lower case; it will also return the word ahead in the sentence rather than the one behind if there is a tie) but hopefully it is useful enough to get you started:</p>

<pre><code>import nltk
from nltk.tokenize import MWETokenizer

def smart_tokenizer(sentence, target_phrase):
    """"""
    Tokenize a sentence using a full target phrase.
    """"""
    tokenizer = MWETokenizer()
    target_tuple = tuple(target_phrase.split())
    tokenizer.add_mwe(target_tuple)
    token_sentence = nltk.pos_tag(tokenizer.tokenize(sentence.split()))

    # The MWETokenizer puts underscores to replace spaces, for some reason
    # So just identify what the phrase has been converted to
    temp_phrase = target_phrase.replace(' ', '_')
    target_index = [i for i, y in enumerate(token_sentence) if y[0] == temp_phrase]
    if len(target_index) == 0:
        return None, None
    else:
        return token_sentence, target_index[0]


def search(text_tag, tokenized_sentence, target_index):
    """"""
    Search for a part of speech (POS) nearest a target phrase of interest.
    """"""
    for i, entry in enumerate(tokenized_sentence):
        # entry[0] is the word; entry[1] is the POS
        ahead = target_index + i
        behind = target_index - i
        try:
            if (tokenized_sentence[ahead][1]) == text_tag:
                return tokenized_sentence[ahead][0]
        except IndexError:
            try:
                if (tokenized_sentence[behind][1]) == text_tag:
                    return tokenized_sentence[behind][0]
            except IndexError:
                continue

x, i = smart_tokenizer(sentence='My problem was with DELL Customer Service',
                       target_phrase='DELL Customer Service')
print(search('NN', x, i))

y, j = smart_tokenizer(sentence=""Good for everyday computing and web browsing."",
                       target_phrase=""everyday computing"")
print(search('NN', y, j))
</code></pre>

<p><strong>Edit:</strong> I made some changes to address the issue of using an arbitrary length target phrase, as you can see in the <code>smart_tokenizer</code> function. The key there is the <code>nltk.tokenize.MWETokenizer</code> class (for more info see: <a href=""https://stackoverflow.com/questions/5532363"">Python: Tokenizing with phrases</a>). Hopefully this helps. As an aside, I would challenge the idea that <code>spaCy</code> is <em>necessarily</em> more elegant - at some point, someone has to write the code to get the work done. This will either that will be the <code>spaCy</code> devs, or you as you roll your own solution. Their API is rather complicated so I'll leave that exercise to you.</p>
",2,5,682,2018-11-15 20:58:01,https://stackoverflow.com/questions/53327804/any-efficient-way-to-find-surrounding-adj-respect-to-target-phrase-in-python
change the format of time and date in R,"<p>I am working on twitter data by useing R and I have column for date and time like this ""10/19/2018 23:00"" , ""10/19/2018 23:01"", ""10/19/2018 23:02"" ,""10/19/2018 19:45"".
I want  to change the format to be without date and also the time be without minute for example any time on 23 hour I need it to be on 23 only without minute.<br>
this is an example of the column:</p>

<pre><code>created
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:29
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 23:28
10/19/2018 15:32
10/19/2018 15:32
10/19/2018 15:32
10/19/2018 15:32
10/19/2018 15:32
10/19/2018 15:32
10/19/2018 15:32
10/19/2018 15:32
</code></pre>

<p>and I need it to be like this 
        15:00
        23:00
        16:00 </p>

<p>hope you understand me 
can you help me to do it and thank you.</p>
","r, sentiment-analysis","<pre><code>x&lt;-
c(""10/19/2018 23:29"", ""10/19/2018 23:29"", ""10/19/2018 23:29"", ""10/19/2018 23:29"")

paste0(unlist(regmatches(x, gregexpr(""(?&lt;=\\s)\\d{2}(?=:\\d{2})"", x, perl = T))), "":00"")
</code></pre>

<hr>

<pre><code>#[1] ""23:00"" ""23:00"" ""23:00"" ""23:00""
</code></pre>
",0,0,43,2018-11-28 16:46:53,https://stackoverflow.com/questions/53524314/change-the-format-of-time-and-date-in-r
Making Predictions on single review from input text using saved CNN model,"<p>I am making a classifier based on a CNN model in Keras.</p>

<p>I will use it in an application, where the user can load the application and enter input text and the model will be loaded from the weights and make predictions.</p>

<p>The thing is I am using GloVe embeddings as well and the CNN model uses padded text sequences as well.</p>

<p>I used Keras tokenizer as following:</p>

<pre><code>tokenizer = text.Tokenizer(num_words=max_features, lower=True, char_level=False)
tokenizer.fit_on_texts(list(train_x))

train_x = tokenizer.texts_to_sequences(train_x)
test_x = tokenizer.texts_to_sequences(test_x)

train_x = sequence.pad_sequences(train_x, maxlen=maxlen)
test_x = sequence.pad_sequences(test_x, maxlen=maxlen)
</code></pre>

<p>I trained the model and predicted on test data, but now I want to test the same with loaded model which I loaded and working.</p>

<p>But my problem here is If I provide a single review, it has to be passed through the <code>tokeniser.text_to_sequences()</code> which is returning 2D array, with a shape of <code>(num_chars, maxlength)</code> and hence followed by a <code>num_chars</code> predictions, but I need it in <code>(1, max_length)</code> shape.</p>

<p>I am using the following code for prediction:</p>

<pre><code>review = 'well free phone cingular broke stuck not abl offer kind deal number year contract up realli want razr so went look cheapest one could find so went came euro charger small adpat made fit american outlet, gillett fusion power replac cartridg number count packagemay not greatest valu out have agillett fusion power razor'
xtest = tokenizer.texts_to_sequences(review)
xtest = sequence.pad_sequences(xtest, maxlen=maxlen)

model.predict(xtest)
</code></pre>

<p>Output is:</p>

<pre><code>array([[0.29289   , 0.36136267, 0.6205081 ],
       [0.362869  , 0.31441122, 0.539749  ],
       [0.32059124, 0.3231736 , 0.5552745 ],
       ...,
       [0.34428033, 0.3363668 , 0.57663095],
       [0.43134686, 0.33979046, 0.48991954],
       [0.22115968, 0.27314988, 0.6188136 ]], dtype=float32)
</code></pre>

<p>I need a single prediction here <code>array([0.29289   , 0.36136267, 0.6205081 ])</code> as I have a single review.</p>
","python, tensorflow, keras, nlp, sentiment-analysis","<p>The problem is that you need to pass a list of strings to <code>texts_to_sequences</code> method. So you need to put the single review in a list like this:</p>

<pre><code>xtest = tokenizer.texts_to_sequences([review])
</code></pre>

<p>If you don't do that (i.e. pass a string, not a list of string(s)), considering the strings in Python are iterable, <a href=""https://github.com/keras-team/keras-preprocessing/blob/7b7af2690026b51b5a7a50b4b359ff12960ac8b7/keras_preprocessing/text.py#L297"" rel=""nofollow noreferrer"">it would iterate</a> over the characters of the given string and consider the characters, not words, as the tokens:</p>

<pre><code>oov_token_index = self.word_index.get(self.oov_token)
for text in texts:  # &lt;-- it would iterate over the string instead
    if self.char_level or isinstance(text, list):
</code></pre>

<p>That's why you would get an array of shape <code>(num_chars, maxlength)</code> as the return value of <code>texts_to_sequences</code> method.</p>
",1,0,1070,2018-11-29 09:34:12,https://stackoverflow.com/questions/53535805/making-predictions-on-single-review-from-input-text-using-saved-cnn-model
Wordcloud from Data Table in R,"<p>I have a data table made from positive and negative word associations. I would like to create two wordclouds, one for positive words and one for negative words.</p>

<p>Example of <code>sentiment_words</code> table:</p>

<pre><code>          element_id    sentence_id   negative     positive
1115:          1        1115          limits       agree,available
1116:          1        1116          slow         strongly,agree
1117:          1        1117                       management
1118:          1        1118                                      
1119:          1        1119          concerns     strongly,agree,better,
</code></pre>

<p>I am using <code>library(wordcloud)</code> and <code>library(sentimentr)</code></p>

<p>For example, how do I pull only the words from the ""positive"" column to create a wordcloud? I'm not sure how to address the fact that there are multiple words associated with each row (e.g., ""agree, available"" should be treated as two entries)</p>

<p>I've made different attempts at the <code>wordcloud()</code> function such as
<code>wordcloud(words = sentiment_words$positive, freq = 3, min.freq = 1, max.words = 200, random.order = FALSE, rot.per=0.35, colors=brewer.pal(8, ""Dark2""))</code> but this only returns a cloud with the term in the first entry</p>

<p>Edit: I've tried the <code>tidyverse</code> answer below, and the result I get is:
<code>
   words                      n
   &lt;chr&gt;                  &lt;int&gt;
 1 "" \""ability\""""             3
 2 "" \""ability\"")""            1
 3 "" \""acceptable\"")""         1
 4 "" \""accomplish\""""          1
 5 "" \""accomplished\"")""       1
 6 "" \""accountability\""""      1
 7 "" \""accountability\"")""     1
 8 "" \""accountable\""""         2
 9 "" \""accountable\"")""        1</code></p>

<p>I've tried multiply variants of <code>gsub()</code> and <code>apply</code> to remove the extra <code>)</code> and <code>c(</code> but haven't found anything that works yet. The result is words that should be counted together are counted separately (e.g., ""acceptable"" and ""acceptable)"" are two different words in the wordcloud)</p>

<p>Edit: In order to get it to work correctly, I had to first clean up my <code>sentiment_words</code> as suggested below.</p>

<pre><code>for (j in seq(sentiment_words)) {
  sentiment_words[[j]] &lt;- gsub(""character(0)"", """", sentiment_words[[j]])
  sentiment_words[[j]] &lt;- gsub('""', """", sentiment_words[[j]])
  sentiment_words[[j]] &lt;- gsub(""c\\("", """", sentiment_words[[j]])
  sentiment_words[[j]] &lt;- gsub("" "", """", sentiment_words[[j]])
  sentiment_words[[j]] &lt;- gsub(""\\)"", """", sentiment_words[[j]])  
}
</code></pre>

<p>and I had to also filter out the remaining ""character(0"" strings within the <code>count_words</code> function. Note that it filters ""character(0"" and not ""character(0)"" because I removed the closing parenthesis above</p>

<pre><code>filter(!!var != ""character(0"") %&gt;%
</code></pre>

<p>Implementing the above gave the cleanest wordcloud based on polarity of text</p>
","r, data-visualization, sentiment-analysis","<p>Here is a <code>tidyverse</code>-based approach that should get you started. I agree with Mr_Z in that I'm not entirely clear on where the problem is.</p>

<ol>
<li><p>Let's define a function that generates a <code>data.frame</code> with the word count based on comma-separated words in a specific column <code>var</code> of your source data <code>df</code>.</p>

<pre><code>library(tidyverse)
count_words &lt;- function(df, var) {
    var &lt;- enquo(var)
    df %&gt;%
        separate_rows(!!var, sep = "","") %&gt;%
        filter(!!var != """") %&gt;%
        group_by(!!var) %&gt;%
        summarise(n = n()) %&gt;%
        rename(words = !!var)
}
</code></pre></li>
<li><p>We can then generate word counts for the <code>positive</code> and <code>negative</code> columns</p>

<pre><code>df.pos &lt;- count_words(df, positive)
df.neg &lt;- count_words(df, negative)
</code></pre>

<p>Let's inspect the <code>data.frame</code>s</p>

<pre><code>df.pos
# A tibble: 5 x 2
  words          n
  &lt;chr&gt;      &lt;int&gt;
1 agree          3
2 available      1
3 better         1
4 management     1
5 strongly       2

df.neg
# A tibble: 3 x 2
  words        n
  &lt;chr&gt;    &lt;int&gt;
1 concerns     1
2 limits       1
3 slow         1
</code></pre></li>
<li><p>Let's plot the word clouds</p>

<pre><code>library(wordcloud)
wordcloud(words = df.pos$words, freq = df.pos$n, min.freq = 1,
          max.words = 200, random.order = FALSE, rot.per = 0.35,
          colors = brewer.pal(8, ""Dark2""))
</code></pre>

<p><a href=""https://i.sstatic.net/ex8kL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ex8kL.png"" alt=""enter image description here""></a></p>

<pre><code>wordcloud(words = df.neg$words, freq = df.neg$n, min.freq = 1,
          max.words = 200, random.order = FALSE, rot.per = 0.35,
          colors = brewer.pal(8, ""Dark2""))
</code></pre>

<p><a href=""https://i.sstatic.net/Mbyoa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Mbyoa.png"" alt=""enter image description here""></a></p></li>
</ol>
",2,1,1325,2018-12-03 21:59:48,https://stackoverflow.com/questions/53602580/wordcloud-from-data-table-in-r
Remove single character in R,"<p>i'm working on sentiment analysis with Arabic language by using R and in cleaning step i need to remove the single character. 
I used this code to remove them and it works but a had some problem</p>

<p>for example here is the data</p>

<pre><code>R&lt;-(""للمدافعين قال شركة وطنية قلت أقنعهم يعاملوننا كمواطنينقال جودتها عالية قلت جيدة غيرها غ"")
</code></pre>

<p>as you see here ""غ"" is single character</p>

<pre><code>gsub("" *\\b[[:alpha:]]{1}\\b *"", """", R)
[1] ""للمدافعين قال شركة وطنية قلت أقنعهم يعاملوننا كمواطنينقال جودتها عالية قلت جيدة غيرها\n""
</code></pre>

<p>but when I tried to apply it on the whole data set on text column like here</p>

<pre><code>subdata1$text = gsub(""*\\b[[:alpha:]]{1}\\b *"", """", subdata1$text)
</code></pre>

<p>its doesn't remove anything and I don't known why?</p>

<p>hope you understand me </p>

<p>thank you </p>
","r, regex, gsub, sentiment-analysis","<p>It seems the <code>[:alpha:]</code> POSIX character class does not work with all Unicode letters in your case.</p>

<p>I suggest using a PCRE pattern:</p>

<pre><code>gsub(""(*UCP)\\b\\p{L}\\b"", """", R, perl=TRUE)
</code></pre>

<p>Here, <code>(*UCP)</code> is required to make <code>\b</code> word boundary Unicode aware and <code>\p{L}</code> matches any Unicode letter from a BMP plane. The <code>perl=TRUE</code> argument is required for the pattern to be processed with the PCRE regex engine.</p>
",1,2,468,2018-12-04 11:20:44,https://stackoverflow.com/questions/53611843/remove-single-character-in-r
Unable to store pandas data frame as a csv,"<p>I am following this <a href=""https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html"" rel=""nofollow noreferrer"">tutorial</a> to retrieve data from news sites. </p>

<p>The main function is getDailyNews. It will loop on each news source, request the api, extract the data and dump it to a pandas DataFrame and then export the result into csv file.</p>

<p>But when I ran the code, I am getting an error. </p>

<pre><code>import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
from tqdm import tqdm, tqdm_notebook
from functools import reduce


def getSources():
    source_url = 'https://newsapi.org/v1/sources?language=en'
    response = requests.get(source_url).json()
    sources = []
    for source in response['sources']:
        sources.append(source['id'])
    return sources

def mapping():
    d = {}
    response = requests.get('https://newsapi.org/v1/sources?language=en')
    response = response.json()
    for s in response['sources']:
        d[s['id']] = s['category']
    return d

def category(source, m):
    try:
        return m[source]
    except:
        return 'NC'

def getDailyNews():
    sources = getSources()
    key = '96f279e1b7f845669089abc016e915cc'


    url = 'https://newsapi.org/v1/articles?source={0}&amp;sortBy={1}&amp;apiKey={2}'
    responses = []
    for i, source in tqdm_notebook(enumerate(sources), total=len(sources)):

        try:
            u = url.format(source, 'top', key)
        except:
            u = url.format(source, 'latest', key)

        response = requests.get(u)
        r = response.json()
        try:
            for article in r['articles']:
                article['source'] = source
            responses.append(r)
        except:
            print('Rate limit exceeded ... please wait and retry in 6 hours')
            return None

    articles = list(map(lambda r: r['articles'], responses))
    articles = list(reduce(lambda x,y: x+y, articles))

    news = pd.DataFrame(articles)
    news = news.dropna()
    news = news.drop_duplicates()
    news.reset_index(inplace=True, drop=True)
    d = mapping()
    news['category'] = news['source'].map(lambda s: category(s, d))
    news['scraping_date'] = datetime.now()

    try:
        aux = pd.read_csv('./data/news.csv')
        aux = aux.append(news)
        aux = aux.drop_duplicates('url')
        aux.reset_index(inplace=True, drop=True)
        aux.to_csv('./data/news.csv', encoding='utf-8', index=False)
    except:
        news.to_csv('./data/news.csv', index=False, encoding='utf-8')

    print('Done')

if __name__=='__main__':
    getDailyNews() 
</code></pre>

<p>Error:</p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: './data/news.csv'
</code></pre>

<p>I know that I have to give the path name in pd.read_csv but I don't know which path I have to give here. </p>
","python, pandas, sentiment-analysis","<p>This error would make sense if there wasn't already a <code>data</code> folder in the directory you are executing this program from.  There is a similar problem in the post <a href=""https://stackoverflow.com/questions/47143836/pandas-dataframe-to-csv-raising-ioerror-no-such-file-or-directory"">here</a>.</p>
",1,0,1154,2018-12-05 20:15:25,https://stackoverflow.com/questions/53640056/unable-to-store-pandas-data-frame-as-a-csv
&#39;list&#39; object has no attribute &#39;encode&#39;: sentiment analysis,"<p>I would like to conduct sentiment analysis of some texts using Vader (but the problem I am describing here applies to any lexicons as well, in addition to Vader).
However, after going through all the data processing including tokenizing and converting to lower case (which I have not mentioned here) I get the following error:</p>

<p>Any idea how to process the documents so that the lexicon can read the texts? Thanks.</p>

<blockquote>
  <p>AttributeError: 'list' object has no attribute 'encode'</p>
</blockquote>

<pre><code>with open('data_1.txt') as g:
    data_1 = g.read()
with open('data_2.txt') as g:
    data_2 = g.read()
with open('data_3.txt') as g:
    data_3 = g.read()

df_1 = pd.DataFrame({""text"":[data_1, data_2, data_3]})

df_1.head()
                                                 text
#0  [[bangladesh, education, commission, report, m...
#1  [[english, version, glis, ministry, of, educat...
#2  [[national, education, policy, 2010, ministry,...

from nltk.sentiment.vader import SentimentIntensityAnalyzer
vader = SentimentIntensityAnalyzer()

df_1['Vader_sentiment'] = df_1.text.apply(lambda x: vader.polarity_scores(x)['compound'])
</code></pre>

<blockquote>
  <p>AttributeError: 'list' object has no attribute 'encode'</p>
</blockquote>
","python, sentiment-analysis, vader","<p><code>df_1.text</code> is a Series of lists of lists. You cannot apply VADER to any lists, especially to lists of lists. Convert the lists to strings and then apply VADER:</p>

<pre><code>df_1['text_as_string'] = df_1['text'].str[0].str.join("" "")
df_1['text_as_string'].apply(lambda x: vader.polarity_scores(x)['compound'])
</code></pre>
",4,1,2078,2018-12-08 08:03:06,https://stackoverflow.com/questions/53680690/list-object-has-no-attribute-encode-sentiment-analysis
Training issue in keras,"<p>I am trying to train my lstm model for sentiment analysis but the program doesnt proceed at all after displaying the following output:</p>

<pre><code>F:\Softwares\Anaconda\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Extracting features &amp; training batches
Training...
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 134, 70)           42481880  
_________________________________________________________________
dropout_1 (Dropout)          (None, 134, 70)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 128)               101888    
_________________________________________________________________
dense_1 (Dense)              (None, 64)                8256      
_________________________________________________________________
dropout_2 (Dropout)          (None, 64)                0         
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
_________________________________________________________________
activation_2 (Activation)    (None, 1)                 0         
=================================================================
Total params: 42,592,089
Trainable params: 42,592,089
Non-trainable params: 0
_________________________________________________________________
None
Train on 360000 samples, validate on 90000 samples
Epoch 1/8
2018-12-08 15:56:04.680836: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
</code></pre>

<p>The code below has some commented out since it was used to save some textual data on disk beforehand. Now, the code only trains the lstm model using that training and testing textual data. It is given below:</p>

<pre><code>import pandas as pd
import Preprocessing as pre
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.utils import shuffle
import pickle
import numpy as np
import sys
from keras.models import Sequential, load_model
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from keras.layers import LSTM
from keras.preprocessing.sequence import pad_sequences
from keras.models import model_from_json
from keras.preprocessing.text import Tokenizer
import os

# fileDir = os.path.dirname(os.path.realpath('__file__'))
# df = pd.read_csv(os.path.join(fileDir, '../Dataset/tweets.csv'),header=None,encoding = ""ISO-8859-1"")
# df=shuffle(df)
# length=df.size
#
# train=[]
# test=[]
# Y=[]
# Y2=[]
#
# count=450000
# for a in range(450000):   #loading data
#     b=pre.preprocess_tweet(df[1][a])
#     label=int(df[0][a])
#     train.append(b)
#     Y.append(label)
#     count-=1
#     print(""Loading training data..."",  count)
#
# with open('training_data(latest).obj', 'wb') as fp:
#     pickle.dump(train, fp)
# with open('training_labels(latest).obj', 'wb') as fp:
#     pickle.dump(Y, fp)
with open ('training_data(latest).obj', 'rb') as fp:
    train = pickle.load(fp)
with open ('training_labels(latest).obj', 'rb') as fp:
    Y = pickle.load(fp)

# count=156884
# for a in range(450000,606884):   #loading testin data
#     b = pre.preprocess_tweet(df[1][a])
#     label=int(df[0][a])
#     test.append(b)
#     Y2.append(label)
#     count-=1
#     print(""Loading testing data..."",  count)
#
# with open('testing_data(latest).obj', 'wb') as fp:
#     pickle.dump(test, fp)
# with open('testing_labels(latest).obj', 'wb') as fp:
#     pickle.dump(Y2, fp)

with open ('testing_data(latest).obj', 'rb') as fp:
    test = pickle.load(fp)
with open ('testing_labels(latest).obj', 'rb') as fp:
    Y2 = pickle.load(fp)

# vectorizer = CountVectorizer(analyzer = ""word"",tokenizer = None, preprocessor = None, stop_words = None, max_features = 2000)
# # # fit_transform() does two functions: First, it fits the model
# # # and learns the vocabulary; second, it transforms our training data
# # # into feature vectors. The input to fit_transform should be a list of
# # # strings.
#
# train = vectorizer.fit_transform(train)
# test = vectorizer.transform(test)
tokenizer = Tokenizer(split=' ')
tokenizer.fit_on_texts(train)
train = tokenizer.texts_to_sequences(train)
max_words = 134
train = pad_sequences(train, maxlen=max_words)
tokenizer.fit_on_texts(test)
test = tokenizer.texts_to_sequences(test)
test = pad_sequences(test, maxlen=max_words)

print('Extracting features &amp; training batches')

print(""Training..."")
embedding_size=32
model = Sequential()
model.add(Embedding(606884, 70, input_length=134))
model.add(Dropout(0.4))
model.add(LSTM(128))
model.add(Dense(64))
model.add(Dropout(0.5))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))
print(model.summary())
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

batch_size = 100
num_epochs = 8

model.fit(train, np.array(Y),  batch_size=batch_size, epochs=num_epochs ,validation_split=0.2,shuffle=True,verbose=2)

# Save the weights
model.save_weights('LSTM_model_weights_updated.h5')

# Save the model architecture
with open('LSTM_model_updated.json', 'w') as f:
    f.write(model.to_json())
# #
# Model reconstruction from JSON file
# with open(os.path.join(fileDir, '../Dataset/LSTM_model.json'), 'r') as f:
#     model = model_from_json(f.read())
#
# # Load weights into the new model
# model.load_weights(os.path.join(fileDir, '../Dataset/LSTM_model_weights.h5'))
# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

scores = model.evaluate(test, np.array(Y2))
print('Evaluation Test accuracy:', scores[1])


count=0
sum=0
#
#
b=model.predict(test)
for a in b:
    print(count)
    if a&lt;0.5:
        sum = sum + abs(Y2[count] - 0)  # error finding
    else:
        sum=sum+ abs(Y2[count]-1)    #error finding
    count+=1

acc=100-((sum/156884)*100)
print (""Accuracy="",acc,""count"",count)
</code></pre>
","python-3.x, keras, sentiment-analysis","<blockquote>
  <p>Total params: 42,592,089 <br/>
  Trainable params: <strong>42,592,089</strong> <br/>
  Non-trainable params: 0</p>
</blockquote>

<p>Your model has more than <strong>42 million trainable parameters</strong> which is <strong>too much</strong> for your machine's configuration (CPU, RAM, etc.), thus it can't handle it. What are the options?</p>

<ul>
<li>Use smaller model</li>
<li>Use a better more powerful computer (with GPU of course)</li>
<li>Consider using an online cloud solution like <a href=""https://www.crestle.com/"" rel=""nofollow noreferrer"">crestle</a> or <a href=""https://www.paperspace.com/"" rel=""nofollow noreferrer"">paperspace</a></li>
</ul>
",1,0,99,2018-12-08 11:12:05,https://stackoverflow.com/questions/53681949/training-issue-in-keras
PHP- compare similarity score in foreach loop,"<p>This is my PHP code to show similarity score(positive/negative/neutral) for each text.</p>
<pre><code>foreach ($dict as $key =&gt; $cat) {
        $similarity[$key] = $Product[$key] / ($lengthQ * $lengthC[$key]);
        echo &quot;Similarity score($key): &quot;. $similarity[$key]. &quot;&lt;br/&gt;&quot;;
        echo &quot;---------&quot;;
    }
</code></pre>
<p>How can it show the result from comparing the similarity score(<code>$similarity[$key]</code>) and print the category(<code>$key</code>) which is the highest score.</p>
<p>The expected output:</p>
<blockquote>
<p>Similarity score(positive): 0.029764673182427</p>
<p>Similarity score(negative): 0.020378478648481</p>
<p>Similarity score(neutral): 0.057639041770423</p>
<p>neutral</p>
<hr />
<p>Similarity score(positive): 0.028088336282316</p>
<p>Similarity score(negative): 0.019230769230769</p>
<p>Similarity score(neutral): 0.054392829322042</p>
<p>neutral</p>
</blockquote>
","php, foreach, sentiment-analysis","<p>Get the maximum value with <code>max()</code> then search on that value to get the key with <code>array_search()</code>:</p>

<pre><code>echo array_search(max($similarity), $similarity);
</code></pre>
",0,-1,86,2018-12-17 21:14:18,https://stackoverflow.com/questions/53823073/php-compare-similarity-score-in-foreach-loop
How to detect tweets posted from official account?,"<p>I am doing a research project which wants to work out Hong Kong people's attitudes toward the public transportation system. I have collected millions of tweets using <a href=""https://developer.twitter.com/content/developer-twitter/en.html"" rel=""nofollow noreferrer"">Twitter API</a>. Since my research target is local Hong Kong people, tweets posted from official accounts or tweets contain advertisements may be useless. </p>

<p>Hence, could anyone give some hint about how to filter out the tweets posted from the official account? I know my explanation might be a little bit abstract. Any tips would be appreciated! Thank you!</p>
","twitter, nlp, sentiment-analysis","<p>well, you can request the user object <a href=""https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/api-reference/get-users-show"" rel=""nofollow noreferrer"">get information about a user</a> and test if the <code>verified</code> key is set to true. here is the informations you get <a href=""https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/user-object"" rel=""nofollow noreferrer"">returned informations in JSON</a>,  you can see that the key <code>verified</code> is among the result.</p>
",1,0,52,2018-12-18 10:42:23,https://stackoverflow.com/questions/53831252/how-to-detect-tweets-posted-from-official-account
Join and run calculations on Pandas DataFrame based on text matching,"<p>I have two dataframes of customer reviews data.</p>

<p>My first dataframe, <strong>'df'</strong> contains <strong>thousands</strong> of raw customer reviews, processed/cleaned reviews data, and sentiment scores:</p>

<pre><code>reviewBody                   reviewClean           sentimentScore
'I like these goggles'       'like goggles'        1
'I don't like these goggles' 'don't like goggles'  -1
'My strap broke'             'strap broke'         -1
 ...                         ...                  ...
</code></pre>

<p>My second dataframe, <strong>'bigrams'</strong> contains the most frequent bigrams in the field called 'reviewClean' from my first dataframe:</p>

<pre><code>topBigrams                 frequency
'like goggles'               150 
'strap broke'                100
  ...                        ...          
</code></pre>

<p>My goal is to take each of my topBigrams, e.g. 'like goggles' or 'strap broke', look up every 'reviewClean' that contains each bigram AND the associated sentiment to that entire review, and and calculate an average sentiment score for each topBigram.</p>

<p><strong>My end result would look something like this (numbers for pure illustration):</strong></p>

<pre><code>topBigrams                 frequency   avgSentiment
'like goggles'             150         .98
'strap broke'              100         -.90
 ...                        ...         ...
</code></pre>

<p>From this data, I would then look for trends on each bigram to determine the drivers of positive or negative sentiment in a more succinct way.</p>

<p>I am not even sure where to begin. Many thanks for any insight into a potential approach here.</p>
","python, pandas, sentiment-analysis","<p>You are going to have to do a cross join <a href=""https://stackoverflow.com/questions/53699012/performant-cartesian-product-cross-join-with-pandas"">see this post</a> in order to check if every review contains every bigram.  There is not getting around using <code>apply</code> since you need to do a row-wise string comparison.</p>

<pre><code>df = pd.DataFrame([['I like these goggles', 'like goggles', 1],
        [""I don't like these goggles"", ""don't like goggles"", -1],
        ['My strap broke', 'strap broke', -1]],
        columns=['reviewBody', 'reviewClean', 'sentimentScore'])

bigrams = pd.DataFrame([['like goggles', 150],
        ['strap broke', 100]],
        columns=['topBigrams', 'frequency'])

dfx = bigrams.assign(key=1).merge(df.assign(key=1), on='key').drop('key', 1)
dfx['has_bigram'] = dfx.apply(lambda x: x.get('topBigrams') in x.get('reviewClean'), axis=1)
</code></pre>

<p>After checking for the bigram in each cleaned review, you can use a groupby to calculate the mean sentiment on the for bigram, only for where the bigram exists.  Then merge it back to the <code>bigrams</code> data frame.</p>

<pre><code>bigrams.merge(dfx.groupby(['topBigrams', 'has_bigram'])
                 .mean()
                 .reset_index()
                 .query('has_bigram')
                 .rename(columns={'sentimentScore':'avgSentiment'})
                 .get(['topBigrams', 'avgSentiment']),
              on='topBigrams')

# returns:
     topBigrams  frequency  avgSentiment
0  like goggles        150             0
1   strap broke        100            -1
</code></pre>
",0,0,152,2018-12-24 01:51:35,https://stackoverflow.com/questions/53908485/join-and-run-calculations-on-pandas-dataframe-based-on-text-matching
"How to use 2 dataset, 1 for training and 1 for testing on WEKA for sentiment analysis","<p>So I have 3 dataset that I used for sentiment analysis and I want to use only 1 dataset for building the model and the rest of the dataset for testing purpose. The model that I will use is SVM(SMO algoritm). The datasets at start only have 2 attributes (text,label) but after preprocessing with string to wordvector it become many attributes. I was able to build a model and test it using 10-fold cross validation and now I want to test it with the other dataset. But since it has different attributes due to string to word vector I can't do it. Any solution for my problem?</p>

<p>I already applied the same preprocess to the test set and tried using ""inputmappedclassifier"" but the result is still error</p>

<p>I was hoping the model can be used on datasets that it never see</p>
","svm, weka, sentiment-analysis","<p>See <a href=""http://jmgomezhidalgo.blogspot.com/2013/05/mapping-vocabulary-from-train-to-test.html"" rel=""nofollow noreferrer"">http://jmgomezhidalgo.blogspot.com/2013/05/mapping-vocabulary-from-train-to-test.html</a></p>

<p>If you know both train and test data you can use batch filtering.</p>

<p>If you don't know test data then you can use FilteredClassfier method. Check <a href=""http://jmgomezhidalgo.blogspot.com/2013/01/text-mining-in-weka-chaining-filters.html"" rel=""nofollow noreferrer"">http://jmgomezhidalgo.blogspot.com/2013/01/text-mining-in-weka-chaining-filters.html</a> and <a href=""http://jmgomezhidalgo.blogspot.com/2013/04/a-simple-text-classifier-in-java-with.html"" rel=""nofollow noreferrer"">http://jmgomezhidalgo.blogspot.com/2013/04/a-simple-text-classifier-in-java-with.html</a></p>

<p>Also have a look at <a href=""https://stackoverflow.com/questions/41877413/how-to-use-stringtowordvector-weka-in-java"">How to use StringToWordVector (weka) in java?</a></p>
",0,0,273,2018-12-26 08:36:27,https://stackoverflow.com/questions/53929498/how-to-use-2-dataset-1-for-training-and-1-for-testing-on-weka-for-sentiment-ana
How to make a table or DataFrame from dtype=object?,"<p>I would like to show how the predictions are made in a table or DataFrame.</p>
<p>I tried to put X_test, y_test and predictions (predictions = model.predict(X_test)) into a DataFrame to show which reviews are positive or negative predicted.</p>
<pre><code>import pandas as pd

predictions = model.predict(X_test)
df_prediction = pd.DataFrame({
    'X_test': [X_test],
    'y_test': [y_test],
    'prediction': [predictions]
})
df_prediction.head()
</code></pre>
<p>Variable &quot;X_test&quot; (Name: review, Length: 4095, dtype: object) looks like:</p>
<p>15806    ['tire', 'gauges', 'kind', 'thing', 'makes', '...</p>
<p>541      ['like', 'said', 'title', 'review', 'say', 'pr...</p>
<p>...</p>
<p>Variable &quot;y_test&quot; (Name: label, Length: 4095, dtype: object) looks like:</p>
<p>15806    positiv</p>
<p>541      positiv</p>
<p>...</p>
<p>Variable &quot;predictions&quot; looks like:</p>
<p>array(['positiv', 'positiv', 'positiv', ..., 'positiv', 'positiv',
'positiv'], dtype=object)</p>
<p>At the moment I got a DataFrame with all Data in the first line but I need as a table with all lines.</p>
","python, dataframe, jupyter-notebook, sentiment-analysis","<p>If <code>x_test</code>, <code>y_test</code> and <code>predictions</code> are lists, then you could just do this: </p>

<pre><code>df_prediction = pd.DataFrame({
    'X_test': x_test,
    'y_test': y_test,
    'prediction': predictions
})
</code></pre>

<p>Also, <code>df_prediction.head()</code> will print the first 5 rows of the dataframe. You could use <code>print(df_prediction)</code> or just <code>df_prediction</code> (for formatted output in Jupyter Notebook) to see the entire data.</p>
",0,0,343,2019-01-02 12:42:00,https://stackoverflow.com/questions/54006597/how-to-make-a-table-or-dataframe-from-dtype-object
"GCP Sentiment Analysis returns same score for 17 different documents, what am I doing wrong?","<p>I'm running Google Cloud Platform's sentiment analysis on 17 different documents, but it gives me the same score, with different magnitudes for each. 
It's my first time using this package, but as far as I can see it should be impossible for all these to have the exact same score. </p>

<p>The documents are pdf files of varying size, but between 15-20 pages, I exclude 3 of them as they're not relevant. </p>

<p>I have tried the code with other documents, and it gives me different scores for shorter documents, I suspect there's a maximum length of document it can handle, but couldn't find anything in the documentation or via google. </p>

<pre><code>def analyze(text):
    client = language.LanguageServiceClient(credentials=creds)    

    document = types.Document(content=text, 
        type=enums.Document.Type.PLAIN_TEXT)

    sentiment = client.analyze_sentiment(document=document).document_sentiment
    entities = client.analyze_entities(document=document).entities

    return sentiment, entities


def extract_text_from_pdf_pages(pdf_path):
    resource_manager = PDFResourceManager()
    fake_file_handle = io.StringIO()
    converter = TextConverter(resource_manager, fake_file_handle)
    page_interpreter = PDFPageInterpreter(resource_manager, converter)

    with open(pdf_path, 'rb') as fh:
        last_page = len(list(enumerate(PDFPage.get_pages(fh, caching=True, check_extractable=True))))-1

        for pgNum, page in enumerate(PDFPage.get_pages(fh, 
                                  caching=True,
                                  check_extractable=True)):

            if pgNum not in [0,1, last_page]:
                page_interpreter.process_page(page)

        text = fake_file_handle.getvalue()

    # close open handles
    converter.close()
    fake_file_handle.close()

    if text:
        return text
</code></pre>

<p>Results (score, magnitude):</p>

<p>doc1
0.10000000149011612 - 147.5</p>

<hr>

<p>doc2
0.10000000149011612 - 118.30000305175781</p>

<hr>

<p>doc3
0.10000000149011612 - 144.0</p>

<hr>

<p>doc4
0.10000000149011612 - 147.10000610351562</p>

<hr>

<p>doc5
0.10000000149011612 - 131.39999389648438</p>

<hr>

<p>doc6
0.10000000149011612 - 116.19999694824219</p>

<hr>

<p>doc7
0.10000000149011612 - 121.0999984741211</p>

<hr>

<p>doc8
0.10000000149011612 - 131.60000610351562</p>

<hr>

<p>doc9
0.10000000149011612 - 97.69999694824219</p>

<hr>

<p>doc10
0.10000000149011612 - 174.89999389648438</p>

<hr>

<p>doc11
0.10000000149011612 - 138.8000030517578</p>

<hr>

<p>doc12
0.10000000149011612 - 141.10000610351562</p>

<hr>

<p>doc13
0.10000000149011612 - 118.5999984741211</p>

<hr>

<p>doc14
0.10000000149011612 - 135.60000610351562</p>

<hr>

<p>doc15
0.10000000149011612 - 127.0</p>

<hr>

<p>doc16
0.10000000149011612 - 97.0999984741211</p>

<hr>

<p>doc17
0.10000000149011612 - 183.5</p>

<hr>

<p>expected different results for all documents, at least small variations. 
(I think these magnitude scores are also way too high, compared to what I have found in documentation and elsewhere)</p>
","python, google-cloud-platform, sentiment-analysis","<p>Yes, there are some <a href=""https://cloud.google.com/natural-language/quotas"" rel=""nofollow noreferrer"">quotas in the usage of the Natural Language API</a>. </p>

<p>The Natural Language API processes text into a series of tokens, which roughly correspond to word boundaries. Attempting to process tokens in excess of the Token Quota (which is by default 100.000 tokens per query) will not produce an error, <strong>but any tokens over that quota will be ignored</strong>.</p>

<p>For the second question it is difficult for me to evaluate the results of the Natural Language API without having access to the documents. Maybe as they are too neutral you are getting the very similar results. I have run some tests with large neutral texts and I got similar results. </p>

<p>Just for clarification, <a href=""https://cloud.google.com/natural-language/docs/basics#sentiment_analysis"" rel=""nofollow noreferrer"">as stated in the Natural Language API documentation</a>:</p>

<blockquote>
  <ul>
  <li><strong>documentSentiment</strong> contains the overall sentiment of the document, which consists of the following fields:
  
  <ul>
  <li><strong>score</strong> of the sentiment ranges between -1.0 (negative) and 1.0 (positive) and corresponds to the overall emotional leaning of the text.</li>
  <li><strong>magnitude</strong> indicates the overall strength of emotion (both positive and negative) within the given text, between 0.0 and +inf. Unlike score, magnitude is not normalized; each expression of emotion within the text (both positive and negative) contributes to the text's magnitude (so longer text blocks may have greater magnitudes). </li>
  </ul></li>
  </ul>
</blockquote>
",1,0,508,2019-01-07 06:17:57,https://stackoverflow.com/questions/54069351/gcp-sentiment-analysis-returns-same-score-for-17-different-documents-what-am-i
Is it possible to pass more than one query in the Twitter API for sentiment analysis in python,"<p>Ive tried to look this up on google and here but I don't seem to find anything that helps. I would like to search for more than one keyword, such as ""java"" ""python"" and ""ruby"" but I'm not too sure how to go about this problem, it involves sentiment analysis. TIA </p>

<pre><code>api = TwitterClient() 
# calling function to get tweets 
tweets = api.get_tweets(query = 'java' , count = 200) 
</code></pre>

<p>I expect to get output of all tweets containing the words java, python and ruby, right now I'm only getting tweets about java. </p>
","python, python-3.x, twitter, tweepy, sentiment-analysis","<p>So I'm not going to write out the entire code for you, but you absolutely can do what you're looking for using <a href=""https://developer.twitter.com/en/docs/tweets/rules-and-filtering/overview/standard-operators"" rel=""nofollow noreferrer"">Twitter's standard operators</a></p>

<p>You can use these to build up a query string of your keywords to get what you want, so say you wanted tweets that contained java, ruby and python together you'd make your query</p>

<pre><code>""java ruby python"" 
</code></pre>

<p>Now say you wanted tweets containing any of those words, you could use logical OR, e.g:</p>

<pre><code>""java OR ruby OR python""
</code></pre>

<p>Of course, now you've got to find a way to actually use these. The <code>api.search()</code> method should work for that. I believe you can still use this on its own, but that's generally discouraged now there's the cursor. This means you don't have to deal with the tweets being separated by pagination; it does it all for you!</p>

<p>So the bit of your code that does the search will look something like:</p>

<pre><code>searchTerms = ""java OR python OR ruby""
for tweet in tweepy.Cursor(api.search, q=searchTerms).items(10):
    #whatever you need to do here
</code></pre>

<p>So in the above <code>tweepy.Cursor</code> is essentially getting a list of status objects (each object is essentially all the information of a single tweet). They contain things like a tweet's text, the time it was posted, number of retweets etc. Therefore the <code>tweet</code> variable in the <code>for</code> loop is a single status object which you can extract the data you require from. The <code>.items()</code> at the end gets you individual status objects as opposed to a page of them. You can put a number in there to define how many tweets you want to return.</p>

<p>For more examples have a look <a href=""https://www.programcreek.com/python/example/76301/tweepy.Cursor"" rel=""nofollow noreferrer"">here</a> lots of different uses of the cursor there that should give you an idea of how it is used.</p>

<p>Some other useful links:</p>

<p><a href=""http://docs.tweepy.org/en/v3.5.0/cursor_tutorial.html"" rel=""nofollow noreferrer"">Tweepy Cursor Documentation</a>  - Short, but it'll give you the gist of the cursor.</p>

<p><a href=""http://docs.tweepy.org/en/v3.5.0/api.html"" rel=""nofollow noreferrer"">Tweepy method docs</a> this gives you info on all the tweepy methods, and lets you know the kind of searches you can perform.</p>

<p>Hope that helps. Best of luck with the sentiment analysis.</p>
",1,1,1460,2019-01-11 14:54:10,https://stackoverflow.com/questions/54148912/is-it-possible-to-pass-more-than-one-query-in-the-twitter-api-for-sentiment-anal
Using Spark&#39;s MapReduce to call a different function and aggregate,"<p>I am woefully unfamiliar with spark but I'm pretty sure there exists a good way to do what I want much faster than I currently am doing it. </p>

<p>Essentially I have an S3 bucket that has lots of JSON of twitter data. I want to go through all of these files, grab the text from the JSON, do sentiment analysis (currently using Stanford NLP) on the text and then upload the Tweet + Sentiment to a database (right now I'm using dynamo, but this is not make-or-break)</p>

<p>The code I currently have is</p>

<pre><code>        /**
         * Per thread:
         * 1. Download a file
         * 2. Do sentiment on the file -&gt; output Map&lt;String, List&lt;Float&gt;&gt;
         * 3. Upload to Dynamo: (a) sentiment (b) number of tweets (c) timestamp
         *
         */

        List&lt;String&gt; keys = s3Connection.getKeys();

        ThreadPoolExecutor threads = new ThreadPoolExecutor(40, 40, 10000, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;&gt;(10));
        threads.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());

        for (String key : keys) {
                threads.submit(new Thread(() -&gt; {
                    try {
                        S3Object s3Object = s3Connection.getObject(key);
                        Map&lt;String, List&lt;Float&gt;&gt; listOfTweetsWithSentiment = tweetSentimentService.getTweetsFromJsonFile(s3Object.getObjectContent());
                        List&lt;AggregatedTweets&gt; aggregatedTweets = tweetSentimentService.createAggregatedTweetsFromMap(listOfTweetsWithSentiment, key);

                        for (AggregatedTweets aggregatedTweet : aggregatedTweets) {
                            System.out.println(aggregatedTweet);
                            tweetDao.putItem(aggregatedTweet);
                        }
                    } catch (Exception e) {
                        System.out.println(e.getMessage());
                    }
                }));
        }
</code></pre>

<p>This works and is fine. And I was able to expedite the process to only about 2 hours by running this code on certain date ranges (i.e. getKeys only gets keys for certain date ranges) and spinning up many instances of this process across different EC2s, each one acting on a different date range.</p>

<p>However, there's gotta be a faster way to do this with a good ole map-reduce, but I just have no idea of how to even begin looking into this. Is it possible to do a Sentiment Analysis in my map and then reduce based on timestamp? </p>

<p>Further, I was looking into using AWS Glue but I don't see a good way to use the Stanford NLP library there. </p>

<p>Any and all help would be greatly appreciated.</p>
","java, apache-spark, mapreduce, sentiment-analysis, aws-glue","<p>Yes, you can do it with Apache Spark. There are a lot of ways to design your application, configure infrastructure, etc. I propose a simple design:</p>

<ol>
<li><p>You are on AWS, so create an EMR cluster with Spark. It would be useful to include Zeppelin for interactive debugging. </p></li>
<li><p>Spark uses several data abstractions. Your friends are RDD and Datasets (read a doc about them). Code for reading data to Datasets may be the same:</p>

<pre><code>SparkSession ss = SparkSession.builder().getOrCreate();
Dataset&lt;Row&gt; dataset = ss.read(""s3a://your_bucket/your_path"");
</code></pre></li>
<li><p>Now you have a <code>Dataset&lt;Row&gt;</code>. This is useful for SQL-like operations. For your analysis you need to convert it to a Spark RDD:</p>

<pre><code>JavaRDD&lt;Tweet&gt; analyticRdd = dataset.toJavaRDD().map(row -&gt; {
  return TweetsFactory.tweetFromRow(row);
});
</code></pre></li>
<li><p>So, with <code>analyticRdd</code> you can do your analysis staff. Just don't forget to make all your services that work with data Serializable.</p></li>
</ol>
",1,2,189,2019-01-14 02:18:46,https://stackoverflow.com/questions/54175062/using-sparks-mapreduce-to-call-a-different-function-and-aggregate
Problem with CountVectorizer from scikit-learn package,"<p>I have a dataset of movie reviews. It has two columns: <code>'class'</code> and <code>'reviews'</code>. I have done most of the routine preprocessing stuff, such as: lowering the characters, removing stop words, removing punctuation marks. At the end of preprocessing, each original review looks like words separated by space delimiter.</p>

<p>I want to use CountVectorizer and then  TF-IDF in order to create features of my dataset so i can do classification/text recognition with Random Forest. I looked into websites and i tried to do how they did. This is my code:</p>

<pre><code>data = pd.read_csv('updated-data ready.csv')
X = data.drop('class', axis = 1)
y = data['class']
vectorizer = CountVectorizer()
new_X = vectorizer.fit_transform(X)
tfidfconverter = TfidfTransformer()  
X1 = tfidfconverter.fit_transform(new_X)
print(X1)
</code></pre>

<p>But, i get this output...</p>

<pre><code>(0, 0)  1.0
</code></pre>

<p>which doesn't make sense at all. I tackled with some parameters and commented out the parts about TF-IDF. Here's my code:</p>

<pre><code>data = pd.read_csv('updated-data ready.csv')
X = data.drop('class', axis = 1)
y = data['class']
vectorizer = CountVectorizer(analyzer = 'char_wb',  \
                         tokenizer = None, \
                         preprocessor = None, \
                         stop_words = None, \
                         max_features = 5000)

new_X = vectorizer.fit_transform(X)
print(new_X)
</code></pre>

<p>and this is my output:</p>

<pre><code>(0, 4)  1
(0, 6)  1
(0, 2)  1
(0, 5)  1
(0, 1)  2
(0, 3)  1
(0, 0)  2
</code></pre>

<p>Am i missing something? Or am i too noob to understand? All i understood and want was/is if i do transform, i will receive a new dataset with so many features (regarding the words and their frequencies) plus label column. But, what i am getting is so far from it.</p>

<p>I repeat, all i want is to have a new dataset out of my dataset with reviews in which it has numbers, words as features, so Random Forest or other classification algorithms can do anything with it.</p>

<p>Thanks.</p>

<p>Btw, this is first five rows of my dataset:</p>

<pre><code>   class                                            reviews
0      1                         da vinci code book awesome
1      1  first clive cussler ever read even books like ...
2      1                            liked da vinci code lot
3      1                            liked da vinci code lot
4      1            liked da vinci code ultimatly seem hold
</code></pre>
","python, scikit-learn, classification, sentiment-analysis, text-recognition","<p>Suppose you happen to have a dataframe:</p>

<pre><code>data
    class   reviews
0   1   da vinci code book aw...
1   1   first clive cussler ever read even books lik...
2   1   liked da vinci cod...
3   1   liked da vinci cod...
4   1   liked da vinci code ultimatly seem...
</code></pre>

<p>Separate into features and outcomes:</p>

<pre><code>y = data['class']
X = data.drop('class', axis = 1)
</code></pre>

<p>Then, following your pipeline, you can prepare your data for any ML algo like this:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
new_X = vectorizer.fit_transform(X.reviews)
new_X
&lt;5x18 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
</code></pre>

<p>This <code>new_X</code> can be used in your further pipeline ""as is"" or converted to dense matrix:</p>

<pre><code>new_X.todense()
matrix([[1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1]],
       dtype=int64)
        with 30 stored elements in Compressed Sparse Row format&gt;
</code></pre>

<p>Rows in this matrix represent rows in the original <code>reviews</code> column and columns represent counts of words. In case you're interested in what column refers to what word you may do:</p>

<pre><code>vectorizer.vocabulary_
{'da': 6,
 'vinci': 17,
 'code': 4,
 'book': 1,
 'awesome': 0,
 'first': 9,
 'clive': 3,
 'cussler': 5,
....
</code></pre>

<p>where <code>key</code> is a word and <code>value</code> is column index in the above matrix (you may infer, actually, that column index correspond to ordered vocabulary, with <code>'awesome'</code> responsible for 0th column and so on).</p>

<p>You may further proceed with your pipeline like this:</p>

<pre><code>tfidfconverter = TfidfTransformer()  
X1 = tfidfconverter.fit_transform(new_X)
X1
&lt;5x18 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
    with 30 stored elements in Compressed Sparse Row format&gt;
</code></pre>

<p>Finally, you can feed your preprocessed data into RandomForest:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()
clf.fit(X1, y)
</code></pre>

<p>This code runs without error on my notebook.
Please, let us know if this solves your problem!</p>
",3,3,2378,2019-01-14 06:28:39,https://stackoverflow.com/questions/54176657/problem-with-countvectorizer-from-scikit-learn-package
How to fix &#39;encoding&#39; issue in Python using vaderSentiment package,"<p>I am working on a sentiment analysis problem and found the vaderSentiment package but cannot get it to run. It is giving me an 'encoding' error. </p>

<p>I have tried adding 'from io import open' but that did not fix my issue. Please see code below.</p>

<pre><code>from io import open
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyser = SentimentIntensityAnalyzer()

def sentiment_analyzer_scores(sentence):
    score = analyser.polarity_scores(sentence)
    print(""{:-&lt;40} {}"".format(sentence, str(score)))

sentiment_analyzer_scores(""The phone is super cool."") 
</code></pre>

<p>Here are the results I am wanting: </p>

<pre><code>""The phone is super cool----------------- {'neg': 0.0, 'neu': 0.326, 'pos':         
0.674, 'compound': 0.7351}""
</code></pre>

<p>The results I am getting: </p>

<pre><code>File ""&lt;ipython-input-27-bbb91818db04&gt;"", line 6, in &lt;module&gt;
analyser = SentimentIntensityAnalyzer()

File ""C:\Users\mr110e\AppData\Local\Continuum\anaconda2\lib\site
packages\vaderSentiment\vaderSentiment.py"", line 212, in __init__
with open(lexicon_full_filepath, encoding='utf-8') as f:

TypeError: 'encoding' is an invalid keyword argument for this function
</code></pre>
","python, encoding, nlp, sentiment-analysis, vader","<p>The <code>vaderSentiment</code> package doesn't support python 2.</p>

<p>You should use <a href=""https://www.python.org/downloads/release/python-370/"" rel=""nofollow noreferrer"">python 3</a> or <a href=""https://github.com/cjhutto/vaderSentiment/issues/47"" rel=""nofollow noreferrer"">little bit edit the package's source code</a></p>
",1,0,752,2019-01-17 14:37:33,https://stackoverflow.com/questions/54238222/how-to-fix-encoding-issue-in-python-using-vadersentiment-package
How to fix object not found in the following code?,"<p>I am working on a sentiment analysis project in R and getting an error message of "" object not found"" whenever I run the code, the library used and the code are as follows (also I did not forget to put the api details in my code):</p>

<pre><code>library(""twitteR"")
library(""ROAuth"")
library(""NLP"")
library(""twitteR"")
library(""syuzhet"")
library(""tm"")
library(""SnowballC"")
library(""stringi"")
library(""topicmodels"")
library(""syuzhet"")
library(""ROAuth"")
library(""wordcloud"")
library(""ggplot2"")

# authorisation keys
#provided by me in the code.

setup_twitter_oauth(consumer_key,consumer_secret,access_token, access_secret)
tweets_g &lt;- searchTwitter(""#google"", n=500,lang = ""en"")

google_tweets &lt;- twListToDF(tweets_g)
View(google_tweets)
google_text&lt;- google_tweets$text

google_text&lt;- tolower(google_text) 
google_text &lt;- gsub(""rt"", """", google_text)
google_text &lt;- gsub(""@\\w+"", """", google_text)
google_text &lt;- gsub(""[[:punct:]]"", """", google_text)
google_text &lt;- gsub(""http\\w+"", """", google_text)
google_text &lt;- gsub(""[ |\t]{2,}"", """", google_text)
google_text &lt;- gsub(""^ "", """", google_text)
google_text &lt;- gsub("" $"", """", google_text)

#clean up by removing stop words
google_tweets.text.corpus &lt;- tm_map(google_tweets.text.corpus, function(x)removeWords(x,stopwords()))

#generate wordcloud
wordcloud(google_tweets.text.corpus,min.freq = 10,colors=brewer.pal(8, ""Dark2""),random.color = TRUE,max.words = 500)

#getting emotions using in-built function
mysentiment_google&lt;-get_nrc_sentiment((google_text))

#calculationg total score for each sentiment
Sentimentscores_google&lt;-data.frame(colSums(mysentiment_google[,]))

names(Sentimentscores_google)&lt;-""Score""
Sentimentscores_google&lt;-cbind(""sentiment""=rownames(Sentimentscores_google),Sentimentscores_google)
rownames(Sentimentscores_google)&lt;-NULL

#plotting the sentiments with scores
ggplot(data=Sentimentscores_google,aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = ""identity"")+
  theme(legend.position=""none"")+
  xlab(""Sentiments"")+ylab(""scores"")+ggtitle(""Sentiments of people behind the tweets on tech giant GOOGLE"")
</code></pre>

<p>the error message that shows up is on running the R script is:</p>

<pre><code>Loading required package: RColorBrewer

Attaching package: ‘ggplot2’

The following object is masked from ‘package:NLP’:

    annotate

[1] ""Using direct authentication""
Error in tm_map(google_text.corpus, function(x) removeWords(x, stopwords())) :
  object 'google_text.corpus' not found
Execution halted````


</code></pre>
","r, sentiment-analysis","<p>Try removing corpus. Just replace your code with this snippet</p>

<pre><code>
#generate wordcloud
wordcloud(min.freq = 10,colors=brewer.pal(8, ""Dark2""),random.color = TRUE,max.words = 500)

</code></pre>
",1,0,1189,2019-01-18 11:51:46,https://stackoverflow.com/questions/54253465/how-to-fix-object-not-found-in-the-following-code
Sample example of Sentiment feature of Watson NLU failing with error code 400,"<p>I was trying the sample examples of various features documented at <a href=""https://cloud.ibm.com/apidocs/natural-language-understanding"" rel=""nofollow noreferrer"">https://cloud.ibm.com/apidocs/natural-language-understanding</a>. All the features examples are working properly except the Sentiment feature while trying with Curl.</p>

<pre><code>curl -X POST \
-H ""Content-Type: application/json"" \
-u ""apikey:{apikey}"" \
-d @parameters.json \
""{url}/v1/analyze?version=2018-11-16""

parameters.json
{
  ""url"": ""www.wsj.com/news/markets"",
  ""features"": {
    ""sentiment"": {
      ""targets"": [
        ""stocks""
      ]
    }
  }
}



Sentiment feature response:
{
  ""language"": ""en"",
  ""error"": ""target(s) not found"",
  ""code"": 400
}
</code></pre>
","ibm-cloud, sentiment-analysis, watson-nlu","<p>Here's how it worked for me. Explaining in an elaborate way to help others.</p>

<p>First of all, you have to create a file named <code>parameters.json</code> and paste the below code </p>

<pre><code>{
  ""url"": ""www.wsj.com/news/markets"",
  ""features"": {
    ""sentiment"": {
      ""targets"": [
        ""stocks""
      ]
    }
  }
}
</code></pre>

<p>Pointing to the folder in which this JSON file is on a terminal or command prompt and replacing the <code>{apikey}</code> and <code>{URL}</code> with the NLU service values, run the below command</p>

<pre><code>curl -X POST \                                                                                                                            
-H ""Content-Type: application/json"" \
-u ""apikey:{APIKEY}"" \
-d @parameters.json \
""{URL}/v1/analyze?version=2018-11-16""
</code></pre>

<p>The {URL} in my case is <code>https://gateway.watsonplatform.net/natural-language-understanding/api</code></p>

<p>Then should see the below output</p>

<pre><code>{
  ""usage"": {
    ""text_units"": 1,
    ""text_characters"": 1421,
    ""features"": 1
  },
  ""sentiment"": {
    ""targets"": [
      {
        ""text"": ""stocks"",
        ""score"": -0.640222,
        ""mixed"": ""1"",
        ""label"": ""negative""
      }
    ],
    ""document"": {
      ""score"": -0.662399,
      ""label"": ""negative""
    }
  },
  ""retrieved_url"": ""https://www.wsj.com/news/markets"",
  ""language"": ""en""
}
</code></pre>
",0,0,357,2019-01-20 06:49:25,https://stackoverflow.com/questions/54274236/sample-example-of-sentiment-feature-of-watson-nlu-failing-with-error-code-400
Lime explainer shows prediction probabilities different to the classifier prediction - sentiment analysis,"<p>I am using Lime to trace the behavior behind why the model take his decision  to predict if this sentence is (NEG, POS or NEUTRAL) and for the most of cases lime explain correctly but in case like this why i entered NEG sentence, the model predict it as NEUTRAL but Lime visualize it with NEG highest percentage, so why i got logical error like this?</p>

<p><a href=""https://i.sstatic.net/ExsPP.png"" rel=""nofollow noreferrer"">Model prediction vs Lime prediction</a></p>
","python, machine-learning, nlp, sentiment-analysis, lime","<p>You are not providing a lot of details, so my answer is going to be similarly general: You original model is making a wrong prediction. Then lime is making a linear approximation of the model. Because of the approximative nature of the linear model, this is not exactly as the original model and deviates from the original model. In your case the original model gives a wrong prediction and the deviation of the linear approximation is - by chance - in the direction of the right answer, so that you get - by chance - the right answer from the approximation although the original model was wrong.</p>
",2,1,1403,2019-01-22 09:28:32,https://stackoverflow.com/questions/54305070/lime-explainer-shows-prediction-probabilities-different-to-the-classifier-predic
How to import a lexicon in XML-LMF format for sentiment analysis in R,"<p>I'm trying to import the following lexicon in R, to be used with text mining packages such as <code>quanteda</code>, or to export it as a list or data frame:</p>

<p><a href=""https://github.com/opener-project/VU-sentiment-lexicon/tree/master/VUSentimentLexicon/IT-lexicon"" rel=""nofollow noreferrer"">https://github.com/opener-project/VU-sentiment-lexicon/tree/master/VUSentimentLexicon/IT-lexicon</a></p>

<p>The format is XML-LMF. I could not find any way to parse such a format with R. </p>

<p>(see <a href=""https://en.wikipedia.org/wiki/Lexical_Markup_Framework"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Lexical_Markup_Framework</a>)</p>

<p>As a workaround I tried to use the <code>XML</code> package, but the structure is a bit different from usual XML, and I did not manage to parse all the nodes. </p>
","r, xml-parsing, text-mining, sentiment-analysis, quanteda","<p>I managed to make it work using the <code>xml2</code> package. Here's my code:</p>

<pre><code>library(xml2)
library(quanteda)

# Read file and find the nodes
opeNER_xml &lt;- read_xml(""it-sentiment_lexicon.lmf.xml"")
entries &lt;- xml_find_all(opeNER_xml, "".//LexicalEntry"")
lemmas &lt;- xml_find_all(opeNER_xml, "".//Lemma"")
confidence &lt;- xml_find_all(opeNER_xml, "".//Confidence"")
sentiment &lt;- xml_find_all(opeNER_xml, "".//Sentiment"")

# Parse and put in a data frame
opeNER_df &lt;- data.frame(
  id = xml_attr(entries, ""id""),
  lemma = xml_attr(lemmas, ""writtenForm""),
  partOfSpeech = xml_attr(entries, ""partOfSpeech""),
  confidenceScore = as.numeric(xml_attr(confidence, ""score"")),
  method = xml_attr(confidence, ""method""),
  polarity = as.character(xml_attr(sentiment, ""polarity"")),
  stringsAsFactors = F
)
# Fix a mistake
opeNER_df$polarity &lt;- ifelse(opeNER_df$polarity == ""nneutral"", 
                             ""neutral"", opeNER_df$polarity)

# Make quanteda dictionary
opeNER_dict &lt;- quanteda::dictionary(with(opeNER_df, split(lemma, polarity)))
</code></pre>
",0,1,258,2019-01-22 10:45:07,https://stackoverflow.com/questions/54306513/how-to-import-a-lexicon-in-xml-lmf-format-for-sentiment-analysis-in-r
Predicting values using trained MNB Classifier,"<p>I am trying to train a model for sentiment analysis and below is my trained Multinomial Naive Bayes Classifier returning an accuracy of 84%.</p>

<p>I have been unable to figure out how to use the trained model to predict the sentiment of a sentence. For example, I now want to use the trained model to predict the sentiment of the phrase ""I hate you"".</p>

<p>I am new to this area and any help is highly appreciated.</p>
","python, python-3.x, classification, sentiment-analysis","<p>I don't know the dataset and what is semantic of individual dictionaries, but you are training your model on a dataset which has form as follows:</p>

<p><code>[[{""word"":True, ""word2"": False}, 'neg'], [{""word"":True, ""word2"": False}, 'pos']]</code></p>

<ul>
<li>That means your input is in form of a dictionary, and output in form of <code>'neg'</code> label. If you want to predict you need to input a dictionary in a form:</li>
</ul>

<p><code>{""I"": True, ""Hate"": False, ""you"": True}</code>.</p>

<ul>
<li>Then:</li>
</ul>

<p><code>MNB_classifier.classify({""love"": True})</code></p>

<p><code>&gt;&gt; 'neg'</code></p>

<p>or 
<code>MNB_classifier.classify_many([{""love"": True}])</code></p>

<p><code>&gt;&gt; ['neg']</code></p>
",1,0,162,2019-01-25 09:21:41,https://stackoverflow.com/questions/54362232/predicting-values-using-trained-mnb-classifier
How to get the score of sentiments?,"<p>I am using <code>TextBlob</code> i am training my <code>classifier</code> on a training set after that i am successfully able to get the classified out put</p>

<p>Bit how can i get the score of a particular text in terms of positive or negativity should i  put scores of sentiments in my training data</p>

<p>here is what i have tried </p>

<pre><code>from textblob import TextBlob
from textblob.classifiers import NaiveBayesClassifier
train = [
     ('I love this sandwich.', 'pos'),
     ('This is an amazing place!', 'pos'),
     ('I feel very good about these beers.', 'pos'),
     ('I do not like this restaurant', 'neg'),
     ('I am tired of this stuff.', 'neg'),
     (""I can't deal with this"", 'neg'),
     (""My boss is horrible."", ""neg"")
 ]
cl = NaiveBayesClassifier(train)
 cl.classify(""I feel amazing!"")
</code></pre>

<p>Here is the output</p>

<pre><code>'pos'
</code></pre>

<p>How can i get the score of this like pos .7 or in any other format</p>
","python, nltk, sentiment-analysis, textblob","<p>You can do something like the following: <a href=""https://textblob.readthedocs.io/en/dev/classifiers.html#classifying-text"" rel=""nofollow noreferrer"">source here</a></p>

<pre><code>&gt;&gt;&gt; prob_dist = cl.prob_classify(""I feel amazing!"")
&gt;&gt;&gt; prob_dist.max()
'pos'
&gt;&gt;&gt; round(prob_dist.prob(""pos""), 2)
0.63
&gt;&gt;&gt; round(prob_dist.prob(""neg""), 2)
0.37
</code></pre>
",1,-2,99,2019-02-04 08:15:28,https://stackoverflow.com/questions/54512265/how-to-get-the-score-of-sentiments
Is there any step by step stuff to implement sentiment analysis and voice into my current Google Cloud Vision based face recognition app project?,"<p>I want to get an easy to understand guide to implement sentiment analysis and voice to my current project Cloud Vision project -object detection and face- besides. I downloaded kind of a sample, but it only recognized faces, but I want to implement custom sentiment analysis and add voice feature in real time using camera. Appreciate a lot guys! </p>

<p>I downloaded kind of a sample, but it only recognized faces, but I want to implement custom sentiment analysis and add voice feature in real time using camera</p>
","android, google-cloud-platform, sentiment-analysis","<p>Currently, Google offers a series of APIs to try to do what you want. You already know <a href=""https://cloud.google.com/vision/overview/docs/"" rel=""nofollow noreferrer"">Google Cloud Vision API</a>, which indeed can detect emotions based on facial expressions.</p>

<p>Here you have a document from the <a href=""http://www.chicagocoderconference.com/wp-content/uploads/2017/07/2017CCC-Vorhees-Are-you-happy-Google-Cloud-Vision-and-Speach-APIs.pdf"" rel=""nofollow noreferrer"">Chicago Coder Conference</a> where they explain a series of products you can use to detect emotions, including <a href=""https://cloud.google.com/speech-to-text/"" rel=""nofollow noreferrer"">Cloud Speech API</a> and <a href=""https://cloud.google.com/natural-language/overview/docs/"" rel=""nofollow noreferrer"">Cloud Natural Language API</a> (the latter is for texts, though).</p>

<p>You could use Cloud Speech API and Natural Language API together to achieve what you want at this moment. </p>
",0,-1,182,2019-02-05 17:43:39,https://stackoverflow.com/questions/54540196/is-there-any-step-by-step-stuff-to-implement-sentiment-analysis-and-voice-into-m
Loop to retrieve sentiment analysis in pandas.core.series.Series,"<p>I have 47 news-articles that I want to extract the sentiment from. They are JSON format (Date, title and body of the article). All I want is to obtain a list with the sentiment using TextBlob. So far I am doing the following:</p>

<pre><code>import json
import pandas
from textblob import TextBlob

appended_data = []

for i in range(1,47):
    df0 = pandas.DataFrame([json.loads(l) for l in open('News_%d.json' % i)])
    appended_data.append(df0)


appended_data = pandas.concat(appended_data)

doc_set = appended_data.body
docs_TextBlob = TextBlob(doc_set)


for i in docs_TextBlob:
    print(docs_TextBlob.sentiment)
</code></pre>

<p>Obvioulsy, I get the following error: <code>TypeError: The text argument passed to __init__(text) must be a string, not &lt;class 'pandas.core.series.Series'&gt;</code> Any idea on how to create a list with the sentiment measure?</p>
","python, sentiment-analysis, textblob","<p>To create a new column in the <code>DataFrame</code> with the sentiment:</p>

<pre><code>appended_data['sentiment'] = appended_data.body.apply(lambda body: TextBlob(body).sentiment)
</code></pre>
",1,0,501,2019-02-08 08:53:19,https://stackoverflow.com/questions/54588807/loop-to-retrieve-sentiment-analysis-in-pandas-core-series-series
Sensitivity Analysis for Missing Data in R with MICE,"<p>I am working on a meta analysis and a sensitivity analysis for missing data. I want to replace censorsed data either with 0 or 1 according to a predefined probability. </p>

<p>I have a dataset with colum x: timepoints and y: events (1 = event, 0 = censored). For the analysis I replaced some of the 0 with NAs. Z is the indicator for the treatment arm. I want to replace NAs to either 1 or 0 with a predefined probability. 
This is my code:</p>

<p>Just an example:</p>

<pre><code>library(mice)

x &lt;- c(1:10)
y &lt;- c(1,1,1,NA,NA,NA,1,1,0,NA)
z &lt;- rep(2,10)

data &lt;- data.frame(x,y,z)

str(data)
md.pattern(data)

mice.impute.myfunct &lt;-  function(y, ry, x, ...)
{event &lt;- sample(c(0:1), size = 1, replace=T, prob=c(0.5,0.5)); return(event)}

data.imp &lt;- mice(data, me = c("""",""myfunct"",""""), m = 1)
data.comp &lt;- complete(data.imp)
</code></pre>

<p>I would expect that NAs in y will be replaced with 0 (20% of cases) and 1 (80% of cases). But NAs are either replaced only with 0 or only with 1. </p>

<p>I have to admit, that I am quite a beginner with R and did not have to write own little functions before.</p>

<p>Thank you very much for your help!</p>
","r, metadata, sentiment-analysis, r-mice","<p>Here is a possible solution just replacing the missing values with the 0 and 1, and a varying probability between 0.1 and 0,9:</p>

<pre><code>for( i in seq(0.1,0.9,0.1)){
  data[[paste0(""y_imp"",i)]] &lt;- data$y
  N &lt;- sum(is.na( data$y))
  data[[paste0(""y_imp"",i)]][is.na(data[[paste0(""y_imp"",i)]])] &lt;-  sample(c(0,1), size = N, replace=T, prob=c(i,1-i))
}
</code></pre>

<p><code>data[[paste0(""y_imp"",i)]] &lt;- data$y</code> create the column where you has the <code>i</code> probability of replacing the missing by 0.</p>
",0,0,457,2019-02-08 10:03:20,https://stackoverflow.com/questions/54589953/sensitivity-analysis-for-missing-data-in-r-with-mice
Train multiple class in sklearn,"<p><a href=""https://i.sstatic.net/NgIlX.png"" rel=""nofollow noreferrer"">i have data frame like this picture.</a></p>

<p>i'm looking a way to train this data set so i tried it with sklearn with this code</p>

<pre><code>train_x, test_x, train_y, test_y = train_test_split(df[['city','text']], df[['1','2','3','4']], test_size = 0.40, random_state = 21)
count_vect = CountVectorizer(analyzer='word', ngram_range=(2,3), max_features=20000)
count_vect.fit(df['text'])

x_train =  count_vect.transform(train_x)
x_test =  count_vect.transform(test_x)
classifier = DecisionTreeClassifier()
classifier.fit(x_train, train_y)
</code></pre>

<p>but i got error like this</p>

<pre><code>ValueError: Number of labels=2348 does not match number of samples=1
</code></pre>

<p>actually i don't know whether it's okay to train my data with its 4 labels directly</p>
","python, scikit-learn, sentiment-analysis","<p>The error is due to the line:</p>

<pre><code>x_train =  count_vect.transform(train_x)
</code></pre>

<p>You see, your <code>train_x</code> and <code>test_x</code> have two columns (from <code>df[['city','text']]</code>), but <code>CountVectorizer</code> only works with a single column. It just needs a single iterable of strings, not more. So you are right in doing:</p>

<pre><code>count_vect.fit(df['text'])
</code></pre>

<p>since you are providing a single column only. But when you do supply <code>train_x</code> in <code>count_vect.transform(train_x)</code>, the <code>count_vect</code> ony take the column names and not the actual data. </p>

<p>Maybe you want:</p>

<pre><code>x_train = count_vect.transform(train_x['text'])
</code></pre>
",0,-1,111,2019-02-15 05:37:53,https://stackoverflow.com/questions/54703247/train-multiple-class-in-sklearn
R: sentiment analysis using Azure Cognitive Service Text API and HTTR package,"<p>I have problem with connection to the Azure Cognitive Service Text API (<a href=""https://westus.dev.cognitive.microsoft.com/docs/services/TextAnalytics.V2.0/operations/56f30ceeeda5650db055a3c9"" rel=""nofollow noreferrer"">here is documentation:</a>) using HTTR.</p>

<pre><code>library(httr)
library(""XML"")
api_url &lt;- 'https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment'

mybody &lt;-list('id'='1', 'text'= 'come on love boy', 'language'=""en"")


result = POST(api_url,body = mybody, content_type('application/json'),encode = ""json"", add_headers(.headers = c(""Content-Type""='application/json',""Ocp-Apim-Subscription-Key""=""XXX"")))
result
Response [https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment]
  Date: 2019-02-21 00:12
###correct status = 200
  Status: 400
  Content-Type: application/json; charset=utf-8
  Size: 222 B
</code></pre>

<p>How shall I implement ""text"" into the body of the API query?</p>
","r, azure, sentiment-analysis, azure-cognitive-services, httr","<p>I found the solution:</p>

<p>write the body as:</p>

<pre><code>mybody &lt;-list('documents'= list(list('id'='1', 'text'= 'come on love boy', 'language'=""en"")))
</code></pre>

<p>Works. </p>
",1,0,175,2019-02-21 00:15:45,https://stackoverflow.com/questions/54797270/r-sentiment-analysis-using-azure-cognitive-service-text-api-and-httr-package
Cannot update VADER lexicon,"<p><code>print(news['title'][5])</code>
Magnitude 7.5 quake hits Peru-Ecuador border region - The Hindu</p>

<p><code>print(analyser.polarity_scores(news['title'][5]))</code>
{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</p>

<pre><code>from nltk.tokenize import word_tokenize, RegexpTokenizer

import pandas as pd

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer


analyzer = SentimentIntensityAnalyzer()


sentence = news['title'][5]

tokenized_sentence = nltk.word_tokenize(sentence)
pos_word_list=[]
neu_word_list=[]
neg_word_list=[]

for word in tokenized_sentence:
    if (analyzer.polarity_scores(word)['compound']) &gt;= 0.1:
        pos_word_list.append(word)
    elif (analyzer.polarity_scores(word)['compound']) &lt;= -0.1:
        neg_word_list.append(word)
    else:
        neu_word_list.append(word)                

print('Positive:',pos_word_list)
print('Neutral:',neu_word_list)
print('Negative:',neg_word_list) 
score = analyzer.polarity_scores(sentence)
print('\nScores:', score)
</code></pre>

<p>Positive: []
Neutral: ['Magnitude', '7.5', 'quake', 'hits', 'Peru-Ecuador', 'border', 'region', '-', 'The', 'Hindu']
Negative: []</p>

<p>Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</p>

<pre><code>new_words = {
    'Peru-Ecuador': -2.0,
    'quake': -3.4,
}

analyser.lexicon.update(new_words)
print(analyzer.polarity_scores(sentence))
</code></pre>

<p>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</p>

<pre><code>from nltk.tokenize import word_tokenize, RegexpTokenizer

import pandas as pd

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer


analyzer = SentimentIntensityAnalyzer()


sentence = news['title'][5]

tokenized_sentence = nltk.word_tokenize(sentence)
pos_word_list=[]
neu_word_list=[]
neg_word_list=[]

for word in tokenized_sentence:
    if (analyzer.polarity_scores(word)['compound']) &gt;= 0.1:
        pos_word_list.append(word)
    elif (analyzer.polarity_scores(word)['compound']) &lt;= -0.1:
        neg_word_list.append(word)
    else:
        neu_word_list.append(word)                

print('Positive:',pos_word_list)
print('Neutral:',neu_word_list)
print('Negative:',neg_word_list) 
score = analyzer.polarity_scores(sentence)
print('\nScores:', score)
</code></pre>

<p>Positive: []
Neutral: ['Magnitude', '7.5', 'quake', 'hits', 'Peru-Ecuador', 'border', 'region', '-', 'The', 'Hindu']
Negative: []</p>

<p>Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</p>
","nltk, sentiment-analysis, nlp, vader","<p>The code you are using is absolutely fine. While updating the dictionary you used <code>analyser</code>instead of <code>analyzer</code> (Not sure why you didn't get an error).</p>

<pre><code>new_words = {
    'Peru-Ecuador': -2.0,
    'quake': -3.4,
}
​
analyzer.lexicon.update(new_words)
print(analyzer.polarity_scores(sentence))
</code></pre>

<p>Output:</p>

<pre><code>{'neg': 0.355, 'neu': 0.645, 'pos': 0.0, 'compound': -0.6597}
</code></pre>

<p><strong>One more caution</strong> (not sure whether you are making this mistake.) 
You shouldn't import the library again. Because your updated words will be gone.
Steps should be:</p>

<ol>
<li>Import the library and the dictionary</li>
<li>Update the dictionary (You shouldn't import the library again after this step)</li>
<li>Calculate the sentiment scores</li>
</ol>
",4,0,1561,2019-02-22 16:13:51,https://stackoverflow.com/questions/54831079/cannot-update-vader-lexicon
Design of a Neural Network for Emotion Classification using Tweet Data,"<p>I have a dataset of four emotion labelled tweets (anger, joy, fear, sadness). For instance, I transformed tweets to a vector similar to the following input vector for anger:</p>

<blockquote>
  <p>Mean of frequency distribution to anger tokens</p>
  
  <p><code>word2vec</code> similarity to anger</p>
  
  <p>Mean of anger in emotion lexicon</p>
  
  <p>Mean of anger in hashtag lexicon</p>
</blockquote>

<p>Is that vector valid to train a neural network?</p>
","neural-network, sentiment-analysis, feature-extraction","<p>Your input vector looks fine to start with. Of-course, you might later make it much advanced with statistical and derivative data from twitter or other relevant APIs or datasets. </p>

<p>Your network has four outputs, just like you mentioned: </p>

<p><code>Joy: [1,0,0,0]</code>
<code>Sadness: [0,1,0,0]</code>
<code>Fear: [0,0,1,0]</code>
<code>Anger: [0,0,0,1]</code></p>

<p>And you may consider adding multiple hidden layers and make it a deep network, if you wish, to increase stability of your neural network prototype. </p>

<p>As your question also shows, it may be best to have a good preprocessor and feature extraction system, prior to training and testing your data, which it certainly seems you know, where the project is going. </p>

<p>Great project, best wishes, thank you for your good question and welcome to stackoverflow.com!</p>

<p><a href=""https://i.sstatic.net/WBf2d.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WBf2d.png"" alt=""An example architecture of a four-output ANN""></a></p>

<p><a href=""https://playground.tensorflow.org/#activation=tanh&amp;batchSize=16&amp;dataset=xor&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=8,4&amp;seed=0.30095&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=true&amp;xSquared=true&amp;ySquared=true&amp;cosX=false&amp;sinX=true&amp;cosY=false&amp;sinY=true&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false"" rel=""nofollow noreferrer"">Playground Tensorflow</a></p>
",3,-3,115,2019-02-24 07:09:10,https://stackoverflow.com/questions/54849588/design-of-a-neural-network-for-emotion-classification-using-tweet-data
CountVectorizer Error: ValueError: setting an array element with a sequence,"<p>I am having a data set of 144 student feedback with 72 positive and 72 negative feedback respectively. The data set has two attributes namely data and target which contain the sentence and the sentiment(positive or negative) respectively.
Consider the following code:</p>

<pre><code>import pandas as pd
feedback_data = pd.read_csv('output.csv')
print(feedback_data)  


    data    target
0      facilitates good student teacher communication.  positive
1                           lectures are very lengthy.  negative
2             the teacher is very good at interaction.  positive
3                       good at clearing the concepts.  positive
4                       good at clearing the concepts.  positive
5                                    good at teaching.  positive
6                          does not shows test copies.  negative
7                           good subjective knowledge.  positive
8                           good communication skills.  positive
9                               good teaching methods.  positive
10   posseses very good and thorough knowledge of t...  positive
11   posseses superb ability to provide a lots of i...  positive
12   good conceptual skills and knowledge for subject.  positive
13                      no commuication outside class.  negative
14                                     rude behaviour.  negative
15            very negetive attitude towards students.  negative
16   good communication skills, lacks time punctual...  positive
17   explains in a better way by giving practical e...  positive
18                               hardly comes on time.  negative
19                          good communication skills.  positive
20   to make students comfortable with the subject,...  negative
21                       associated to original world.  positive
22                             lacks time punctuality.  negative

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(binary = True)
cv.fit(feedback_data['data'].values)
X = feedback_data['data'].apply(lambda X : cv.transform([X])).values
X_test = cv.transform(feedback_data_test)

from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

target = [1 if i&lt;72 else 0 for i in range(144)]
print(target)

X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.50)

clf = svm.SVC(kernel = 'linear', gamma = 0.001, C = 0.05)
#The below line gives the error
clf.fit(X , target)
</code></pre>

<p>I do not know what is wrong. Please help</p>
","machine-learning, sentiment-analysis","<p>The error comes from the way X as been done. You cannot use directly X in the Fit method. You need first to transform it a little bit more (i could not have told you that for the other problem as i did not have the info)</p>

<p>right now you have the following:</p>

<pre><code>array([&lt;1x23 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
with 5 stored elements in Compressed Sparse Row format&gt;,
   ...
   &lt;1x23 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
with 3 stored elements in Compressed Sparse Row format&gt;], dtype=object)
</code></pre>

<p>Which is enough to do a split.
We are just going to transform it you can understand and so will the fit method:</p>

<pre><code>X = list([list(x.toarray()[0]) for x in X])
</code></pre>

<p>What we do is convert the sparse matrix to a numpy array, take the first element (it has only one element) and then convert it to a list to make sure it has the right dimension.</p>

<p>Now why are we doing this:</p>

<p>X is something like that</p>

<pre><code>&gt;&gt;&gt;X[0]
   &lt;1x23 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
   with 5 stored elements in Compressed Sparse Row format&gt;
</code></pre>

<p>so we transform it to see what it realy is:</p>

<pre><code>&gt;&gt;&gt;X[0].toarray()
   array([[0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
         0]], dtype=int64)
</code></pre>

<p>and then as you see there is a slight issue with the dimension so we take the first element.</p>

<p>going back to a list does nothing, it's just for you to understand well what you see. (you can dump it for speed)</p>

<p>your code is now this:</p>

<pre><code>cv = CountVectorizer(binary = True)
cv.fit(df['data'].values)
X = df['data'].apply(lambda X : cv.transform([X])).values
X = list([list(x.toarray()[0]) for x in X])
clf = svm.SVC(kernel = 'linear', gamma = 0.001, C = 0.05)
clf.fit(X, target)
</code></pre>
",0,0,531,2019-02-25 16:00:00,https://stackoverflow.com/questions/54870167/countvectorizer-error-valueerror-setting-an-array-element-with-a-sequence
Scikit Learn ValueError: Found array with dim 3. Estimator expected &lt;= 2,"<p>I am having a training data set of 144 student feedback with 72 positive and 72 negative feedback  respectively. The data set has two attributes namely data and target which contain the sentence and the sentiment(positive or negative) respectively. The testing data set contains 106 unlabeled feedback.
Consider the following code:</p>

<pre><code>import pandas as pd
feedback_data = pd.read_csv('output_svm.csv')
print(feedback_data)


data    target
0      facilitates good student teacher communication.  positive
1                           lectures are very lengthy.  negative
2             the teacher is very good at interaction.  positive
3                       good at clearing the concepts.  positive
4                       good at clearing the concepts.  positive
5                                    good at teaching.  positive
6                          does not shows test copies.  negative
7                           good subjective knowledge.  positive
8                           good communication skills.  positive
9                               good teaching methods.  positive
10   posseses very good and thorough knowledge of t...  positive

feedback_data_test = pd.read_csv('classified_feedbacks_test.csv')
print(feedback_data_test)

          data  target
0                                       good teaching.     NaN
1                                         punctuality.     NaN
2                    provides good practical examples.     NaN
3                              weak subject knowledge.     NaN
4                                   excellent teacher.     NaN
5                                         no strength.     NaN
6                      very poor communication skills.     NaN
7                      not able to clear the concepts.     NaN
8                                            punctual.     NaN
9                             lack of proper guidance.     NaN
10                                  fantastic speaker.     NaN
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(binary = True)
ct = CountVectorizer(binary= True)
cv.fit(feedback_data['data'].values)
ct.fit(feedback_data_test['data'].values)
X = feedback_data['data'].apply(lambda X : cv.transform([X])).values
X = list([list(x.toarray()[0]) for x in X])
X_test = feedback_data_test['data'].apply(lambda X_test : ct.transform([X_test])).values
X_test = list([list(x.toarray()[0]) for x in X_test])




from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
target = [1 if i&lt;72 else 0 for i in range(144)]
X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.50)
clf = svm.SVC(kernel = 'linear', gamma = 0.001, C = 0.05)
clf.fit(X, target)
#The below line gives error
print(""Accuracy = %s"" %accuracy_score(target,clf.predict([X_test])) )
</code></pre>

<p>I do not know what is wrong. Please help.</p>
","machine-learning, sentiment-analysis","<p>the error you get is not about the number of samples but the number of features and this comes from those line of code:</p>

<pre><code>cv = CountVectorizer(binary = True)
ct = CountVectorizer(binary= True)
cv.fit(feedback_data['data'].values)
ct.fit(feedback_data_test['data'].values)
</code></pre>

<p><strong>You need to encode the test and the train the same way</strong></p>

<p>You fit the Count Vectorizer on all the datas and then apply it to the test and train, if not you don't have the same vocabulary and thus not the same encoding.</p>

<pre><code>cv = CountVectorizer(binary = True)
cv.fit(np.concatenate((feedback_data['data'].values,feedback_data_test['data'].values))
</code></pre>

<p><strong>EDIT</strong></p>

<p>you just don't use ct, only cv</p>

<pre><code>X = feedback_data['data'].apply(lambda X : cv.transform([X])).values
X = list([list(x.toarray()[0]) for x in X])
X_test = feedback_data_test['data'].apply(lambda X_test :cv.transform([X_test])).values
X_test = list([list(x.toarray()[0]) for x in X_test])
</code></pre>
",0,0,215,2019-02-25 17:18:18,https://stackoverflow.com/questions/54871465/scikit-learn-valueerror-found-array-with-dim-3-estimator-expected-2
Python TextBlob translate issue,"<p>I am doing a quick sentiment analysis console application with Python, TextBlob and NLTK.</p>

<p>Currently i am using a link to a wiki article in spanish, so i don't need to translate it and i can use the nltk spanish stopword list, but what if i wanted to make this code work for different language links?</p>

<p>If i use the line <code>TextFinal=TextFinal.translate(to=""es"")</code> below <code>textFinal=TextBlob(texto)</code>  (code below) i get an error since it can't translate spanish into spanish. </p>

<p>Could i prevent this just by using a try/catch? Is there a way to make the code try to translate to different languages (as well as using different stopword list) depending on the language of the links im feeding to the application?</p>

<pre><code>import nltk
nltk.download('stopwords')
from nltk import  word_tokenize
from nltk.corpus import stopwords
import string
from textblob import TextBlob, Word
import urllib.request
from bs4 import BeautifulSoup

response = urllib.request.urlopen('https://es.wikipedia.org/wiki/Valencia')
html = response.read()

soup = BeautifulSoup(html,'html5lib')
text = soup.get_text(strip = True)


tokens = word_tokenize(text)
tokens = [w.lower() for w in tokens]

table = str.maketrans('', '', string.punctuation)
stripped = [w.translate(table) for w in tokens]
words = [word for word in stripped if word.isalpha()]

stop_words = set(stopwords.words('spanish'))

words = [w for w in words if not w in stop_words]

with open('palabras.txt', 'w') as f:
    for word in words:
        f.write("" "" + word)

with open('palabras.txt', 'r') as myfile:
    texto=myfile.read().replace('\n', '')


textFinal=TextBlob(texto)

print (textFinal.sentiment)

freq = nltk.FreqDist(words)

freq.plot(20, cumulative=False)
</code></pre>
","python, nltk, sentiment-analysis, textblob","<p>Take a look at the package langdetect.  You could check the language of the page you are feeding in and skip translation if the page language matches the translation language.  Something like the following:</p>

<pre><code>import string
import urllib.request

import nltk
from bs4 import BeautifulSoup
from langdetect import detect
from nltk import word_tokenize
from nltk.corpus import stopwords
from textblob import TextBlob, Word

nltk.download(""stopwords"")
# nltk.download(""punkt"")

response = urllib.request.urlopen(""https://es.wikipedia.org/wiki/Valencia"")
html = response.read()

soup = BeautifulSoup(html, ""html5lib"")
text = soup.get_text(strip=True)
lang = detect(text)

tokens = word_tokenize(text)
tokens = [w.lower() for w in tokens]

table = str.maketrans("""", """", string.punctuation)
stripped = [w.translate(table) for w in tokens]
words = [word for word in stripped if word.isalpha()]

stop_words = set(stopwords.words(""spanish""))

words = [w for w in words if w not in stop_words]

with open(""palabras.txt"", ""w"", encoding=""utf-8"") as f:
    for word in words:
        f.write("" "" + word)

with open(""palabras.txt"", ""r"", encoding=""utf-8"") as myfile:
    texto = myfile.read().replace(""\n"", """")


textFinal = TextBlob(texto)

translate_to = ""es""
if lang != translate_to:
    textFinal = textFinal.translate(to=translate_to)

print(textFinal.sentiment)

freq = nltk.FreqDist(words)

freq.plot(20, cumulative=False)
</code></pre>
",1,2,2134,2019-03-14 17:41:26,https://stackoverflow.com/questions/55168908/python-textblob-translate-issue
Balanced sample with defined n in R,"<p>I have an imbalanced dataset for sentiment analysis with about 65000 observations (~60000 positive and ~5000 negatives). This dataset should be balanced so that I have the same number of positive and negative observations to train my machine learning algorithms. </p>

<p>The package <code>caret</code> and the function <code>downSample</code> help me to get ~5000 negative and ~5000 positive observations (downsampling to minority class). But I like to have exactly 2500 randomly selected positive and 2500 randomly selected negative observations. Is there anyone who knows how to do this?</p>
","r, r-caret, sentiment-analysis, downsampling","<p>You just want 2500 of each??</p>

<pre><code>require(tidyverse)
df &lt;- data.frame(class = c(rep('POS',60000), rep('NEG',5000)), random = runif(65000))
result &lt;- df %&gt;% 
  group_by(class) %&gt;% 
  sample_n(2500)
table(result$class)
</code></pre>
",4,2,2572,2019-03-17 21:55:16,https://stackoverflow.com/questions/55212317/balanced-sample-with-defined-n-in-r
How to Plot Multiple Line chart Using Pandas of Sentiment analysis data stored in csv,"<p>I have data set After doing Sentiment analysis which has  1st column(date) and 2nd column(sentiment)  </p>

<ol>
<li>2019-03-19 ,positive 2019-03-19 ,negative 2019-03-19 ,neutral<br>
2019-03-19, positive 2019-04-19 ,positive 2019-04-19 ,neutral<br>
2019-04-19 ,positive 2019-04-19 ,positive 2019-04-19 ,positive<br>
2019-05-19 ,positive 2019-05-19 ,negative 2019-05-19 ,postive<br>
2019-05-19 ,negative</li>
</ol>

<p>Here is the DataSet : <a href=""https://drive.google.com/file/d/1jlmuzFi9OS3mBWjgQvQuKGdNzan708R6/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1jlmuzFi9OS3mBWjgQvQuKGdNzan708R6/view?usp=sharing</a></p>

<p>I want to Plot 3 graph having positive, negative and neutral as follows </p>

<p>on x-axis  date  and on y-axis no of positive/neg/neutral somewhat like this any suggestion would useful thanks
<a href=""https://i.sstatic.net/zfqpG.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zfqpG.jpg"" alt=""enter image description here""></a></p>
","python-3.x, pandas, numpy, matplotlib, sentiment-analysis","<p>First, you need to convert the data into grouped counts by day and sentiment type,</p>

<pre><code>df = pd.read_csv('path-to-data/raw-Hospital.csv', header=None, 
                 names=['date', 'text', 'sentiment'], parse_dates=['date',])

by_day_sentiment = df.groupby([pd.Grouper(key='date', freq='D'), 'sentiment']) \
    .size().unstack('sentiment')
</code></pre>

<p>which will give you the count data,</p>

<pre><code>sentiment   negative  neutral  positive
date                                   
2019-03-10         2       13        42
2019-03-11        15       58        81
2019-03-12        11       61        70
2019-03-13         5      158       110
2019-03-14         2      110       182
2019-03-15        11       80       216
2019-03-16         7       58        66
2019-03-17         2       31        53
2019-03-18        11       87       137
2019-03-19         2       24        53
</code></pre>

<p>and then you can get a line chart as above by plotting on the summary DataFrame,</p>

<pre><code>by_day_sentiment.plot()
</code></pre>
",2,2,1150,2019-03-21 11:50:16,https://stackoverflow.com/questions/55279821/how-to-plot-multiple-line-chart-using-pandas-of-sentiment-analysis-data-stored-i
Cleaning \u2764\ufe0f \u2026 data in file with python,"<p>I try to cleaning data twitter in python with regex, but i can't remove <code>\u2764\ufe0f \u2026</code>. twitter data is in the datas.txt file, this is the data:</p>

<blockquote>
  <p>Berkat biznet aku bisa online terimakasih BiznetHome \u2764\ufe0f
  Gangguan hari sabtu perbaikan nanti senin  hari offline Slow respon \u2764\ufe0f Terima kasih TelkomCare masalah indihome sy sudah terselesaikan terima kasih fast responnya terus selalu tingka\u2026 TelkomCare Sudah beres fix internet dan telpon berfungsi normal thanks atas respons dan perbaikan pihak Indihom\u2026</p>
</blockquote>

<p>I have tried three ways :
<br>First</p>

<pre class=""lang-py prettyprint-override""><code>import re

with open ('datas.txt', 'r') as f:
     mylist = [line for line in f]
emoji_pattern = re.compile(r'\\\\u\w+')
for i in mylist:
    print(emoji_pattern.sub(r'', i))
</code></pre>

<p><br>Second</p>

<pre class=""lang-py prettyprint-override""><code>import re
f = open('datas.txt', 'r')
data = f.read()
emoji_pattern = re.compile(""[""
                u""\U0001F600-\U0001F64F""  # emoticons
                u""\U0001F300-\U0001F5FF""  # symbols &amp; pictographs
                u""\U0001F680-\U0001F6FF""  # transport &amp; map symbols
                u""\U0001F1E0-\U0001F1FF""  # flags (iOS)
                u""\U00002702-\U000027B0""
                u""\U000024C2-\U0001F251""
                u""\U0001f926-\U0001f937""
                u'\U00010000-\U0010ffff'
                u""\u200d""
                u""\u2640-\u2642""
                u""\u2600-\u2B55""
                u""\u23cf""
                u""\u23e9""
                u""\u231a""
                u""\u3030""
                u""\ufe0f""
    ""]+"", flags=re.UNICODE)
emoji_pattern.sub(r'', data)
</code></pre>

<p><br>third</p>

<pre class=""lang-py prettyprint-override""><code>f= open(""datas.txt"", ""r"", encoding=""UTF-8"")
datas = f.read()
data = datas.encode('ascii', 'ignore').decode(""utf-8"")
print(data)
</code></pre>

<p>but still not work</p>
","python, regex, sentiment-analysis, preprocessor, data-cleaning","<p>Your text file contains non-ASCII Unicode codepoints encoded according to <a href=""https://docs.python.org/3/howto/unicode.html#unicode-literals-in-python-source-code"" rel=""nofollow noreferrer"">how Python encodes Unicode literals in source code</a>. There are two things you can do with that:</p>

<ul>
<li>Delete all <code>\uXXXX</code> or <code>\UXXXXXXXX</code> sequences from your data. This will remove all Unicode codepoints written in Python literal format, which, in principle (although not necessarily), will be non-ASCII characters. That can be done for example like this:</li>
</ul>

<pre class=""lang-py prettyprint-override""><code>import re

with open ('datas.txt', 'r') as f:
     mylist = [line for line in f]
unicode_literal = re.compile(r'\\u[0-9a-fA-F]{4}|\\U[0-9a-fA-F]{8}')
for i in mylist:
    print(unicode_literal.sub(r'', i))
</code></pre>

<ul>
<li>Interpret Unicode code points as their intended value. That is, you will get a string with the non-ASCII data corresponding to the codepoints expressed in the text file. You can do that like this:</li>
</ul>

<pre class=""lang-py prettyprint-override""><code># Note file is read in byte mode
with open ('datas.txt', 'rb') as f:
     mylist = [line for line in f]
for i in mylist:
    print(mylist.decode('unicode-escape'))
</code></pre>
",0,0,1198,2019-03-25 16:41:09,https://stackoverflow.com/questions/55342594/cleaning-u2764-ufe0f-u2026-data-in-file-with-python
"TypeError: __init__() got an unexpected keyword argument &#39;n_folds&#39;,sentiment_analysis_with_SVM","<p>I am trying to implement svm for sentiment analysis, i trying to implement this gitlink <a href=""https://github.com/jatinwarade/Sentiment-analysis-using-SVM/blob/master/SVM.ipynb"" rel=""nofollow noreferrer"">https://github.com/jatinwarade/Sentiment-analysis-using-SVM/blob/master/SVM.ipynb</a>.</p>

<pre><code>from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import StratifiedKFold
</code></pre>

<p>i refered this as it says to change cross origin to model_selection since it is depricated <a href=""https://stackoverflow.com/questions/51574788/error-init-got-an-unexpected-keyword-argument-n-splits"">Error: __init__() got an unexpected keyword argument &#39;n_splits&#39;</a>
so I replaced with this</p>

<pre><code>grid_svm = GridSearchCV(
    pipeline_svm, #object used to fit the data
    param_grid=param_svm, 
    refit=True,  # fit using all data, on the best detected classifier
    n_jobs=-1,  # number of cores to use for parallelization; -1 for ""all cores"" i.e. to run on all CPUs
    scoring='accuracy',#optimizing parameter
    cv=StratifiedKFold(liked_train,n_folds=5),
)
</code></pre>

<p>This Returns Error: </p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-49-61dd1e818fa4&gt; in &lt;module&gt;
      5     n_jobs=-1,  # number of cores to use for parallelization; -1 for ""all cores"" i.e. to run on all CPUs
      6     scoring='accuracy',#optimizing parameter
----&gt; 7     cv=StratifiedKFold(liked_train,n_folds=5),
      8 )

TypeError: __init__() got an unexpected keyword argument 'n_folds'
</code></pre>

<p>Please Help me solve this error</p>
","python-3.x, scikit-learn, svm, sentiment-analysis, sklearn-pandas","<p>As you can see in the documentation for <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"" rel=""nofollow noreferrer""><code>model_selected.StrafiedKFold</code></a>, there is no keyword argument called <code>n_folds</code> and you should indeed use <code>n_splits</code>.</p>

<p>Note, however, that the data should <em>not</em> be passed as an argument to the validator and by doing so, you're effectively passing <code>liked_train</code> as the argument for <code>n_splits</code>, which won't work. Rather, you should pass the data only to the <code>fit</code> of your <code>grid_svm</code> after initialization.</p>
",3,2,14198,2019-03-30 12:19:32,https://stackoverflow.com/questions/55431356/typeerror-init-got-an-unexpected-keyword-argument-n-folds-sentiment-ana
I need twitter dataset for last 3-4 months relating to any company/ comodity for stock price prediction,"<p>I need twitter dataset for last 3-4 months relating to any company/ commodity for performing sentiment analysis and thereby stock price prediction.
But the twitter API only goes back upto 10-12 days.The code that I've prepared works well but I need more dataset to reach a reliable conclusion.I can't wait for 3-4 months since I've to submit the project soon.
If anyone knows any link where I can find some dataset or if anyone has it,
Please let me know.
Thank you in advance</p>
","sentiment-analysis, stock","<p>You probably could check out these discussions (<a href=""https://opendata.stackexchange.com/questions/1545/twitter-open-datasets"">1</a>,<a href=""https://opendata.stackexchange.com/questions/7390/historical-twitter-data"">2</a>) and also this <a href=""http://followthehashtag.com/datasets/nasdaq-100-companies-free-twitter-dataset"" rel=""nofollow noreferrer"">site</a>.</p>
",0,0,116,2019-04-05 19:29:07,https://stackoverflow.com/questions/55542193/i-need-twitter-dataset-for-last-3-4-months-relating-to-any-company-comodity-for
Is the a way of getting the degree of positiveness or negativeness when using Logistic Regression for sentiment analysis,"<p>I have been following an example about Sentiment Analysis using Logistic Regression, in which prediction result only gives a 1 or 0 to give positive or negative sentiment respectively.</p>

<p>My challenge is that i want to classify a given user input into one of the four classes (very good, good, average, poor) but my prediction result every time is 1 or 0.</p>

<p>Below is my code sample so far</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_files
from sklearn.model_selection import GridSearchCV
import numpy as np
import mglearn
import matplotlib.pyplot as plt
# import warnings filter
from warnings import simplefilter
# ignore all future warnings
#simplefilter(action='ignore', category=FutureWarning)

# Get the dataset from http://ai.stanford.edu/~amaas/data/sentiment/

reviews_train = load_files(""aclImdb/train/"")
text_train, y_train = reviews_train.data, reviews_train.target

print("""")
print(""Number of documents in train data: {}"".format(len(text_train)))
print("""")
print(""Samples per class (train): {}"".format(np.bincount(y_train)))
print("""")

reviews_test = load_files(""aclImdb/test/"")
text_test, y_test = reviews_test.data, reviews_test.target

print(""Number of documents in test data: {}"".format(len(text_test)))
print("""")
print(""Samples per class (test): {}"".format(np.bincount(y_test)))
print("""")


vect = CountVectorizer(stop_words=""english"", analyzer='word', 
                        ngram_range=(1, 1), max_df=1.0, min_df=1, 
max_features=None)
X_train = vect.fit(text_train).transform(text_train)
X_test = vect.transform(text_test)

print(""Vocabulary size: {}"".format(len(vect.vocabulary_)))
print("""")
print(""X_train:\n{}"".format(repr(X_train)))
print(""X_test: \n{}"".format(repr(X_test)))

feature_names = vect.get_feature_names()
print(""Number of features: {}"".format(len(feature_names)))
print("""")

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}
grid = 
GridSearchCV(LogisticRegression(penalty='l1',dual=False,max_iter=110, 
solver='liblinear'), param_grid, cv=5)
grid.fit(X_train, y_train)

print(""Best cross-validation score: {:.2f}"".format(grid.best_score_))
print(""Best parameters: "", grid.best_params_)
print(""Best estimator: "", grid.best_estimator_)

lr = grid.best_estimator_
lr.predict(X_test)

print(""Best Estimator Score: {:.2f}"".format(lr.score(X_test, y_test)))
print("""")

#creating an empty list for getting overall sentiment
lst = []

# number of elemetns as input
print("""")
n = int(input(""Enter number of rounds : "")) 

# iterating till the range 
for i in range(0, n):
    temp =[]
ele = input(""\n Please Enter a sentence to get a sentiment Evaluation.  
 \n\n"")
temp.append(ele)

print("""")
print(""Review prediction: {}"". format(lr.predict(vect.transform(temp))))
print("""")
lst.append(ele) # adding the element 

print(lst)
print("""")
print(""Overal prediction: {}"". format(lr.predict(vect.transform(lst))))
print("""")
</code></pre>

<p>I want to get some values between -0 to 1, like when you use Vader SentimentIntensityAnalyzer's polarity_scores. </p>

<p>Here is a code sample of what i want to achieve using SentimentIntensityAnalyzer's polarity_scores.</p>

<pre><code># import SentimentIntensityAnalyzer class 
# from vaderSentiment.vaderSentiment module. 
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer 

# function to print sentiments 
# of the sentence.

def sentiment_scores(sentence): 

# Create a SentimentIntensityAnalyzer object. 
sid_obj = SentimentIntensityAnalyzer() 

# polarity_scores method of SentimentIntensityAnalyzer 
# oject gives a sentiment dictionary. 
# which contains pos, neg, neu, and compound scores.

sentiment_dict = sid_obj.polarity_scores(sentence) 

print("""")
print(""\n Overall sentiment dictionary is : "", sentiment_dict,"" \n"") 
print(""sentence was rated as: "", sentiment_dict['neg']*100, ""% Negative 
\n"") 
print(""sentence was rated as: "", sentiment_dict['neu']*100, ""% Neutral 
\n"") 
print(""sentence was rated as: "", sentiment_dict['pos']*100, ""% Positive 
\n"")

print(""Sentence Overall Rated As: "", end = "" "") 

# decide sentiment as positive, negative and neutral


if sentiment_dict['compound'] &gt;= 0.5: 
    print(""Exellent \n"")
elif sentiment_dict['compound'] &gt; 0 and sentiment_dict['compound'] &lt;0.5:
    print(""Very Good \n"")
elif sentiment_dict['compound'] == 0:
    print(""Good \n"")
elif sentiment_dict['compound'] &lt;= -0.5:
    print(""Average \n"")
elif sentiment_dict['compound'] &gt; -0.5 and sentiment_dict['compound']&lt;0:
    print(""Poor \n"")  

# Driver code 
if __name__ == ""__main__"" : 

while True:
       # print("""")
        sentence= []
        sentence = input(""\n Please enter a sentence to get a sentimet 
 evaluation. Enter exit to end progam \n"")

        if sentence == ""exit"":

            print(""\n Program End...........\n"")
            print("""")
            break
        else:
            sentiment_scores(sentence)
</code></pre>
","machine-learning, deep-learning, logistic-regression, sentiment-analysis, python-3.7","<p>You've got a couple options. </p>

<p>1: Label your initial training data with multiple classes according to how negative or positive the example is, instead of just 0 or 1, and perform multi-class classification.</p>

<p>2: As 1 may not be possible, try experimenting with the <code>predict_proba(X)</code>, <code>predict_log_proba(X)</code>, and <code>decision_function(X)</code> methods and use the results from those to bin your output into the 4 classes according to some hard-coded thresholds.  I would recommend using <code>predict_proba</code> as those numbers are directly interpretable as probabilities and is one of the main benefits of logistic regression as opposed to other methods. For example, assuming the 1st (not 0th) column is the ""positive"" classification</p>

<pre><code>probs = lr.predict_proba(X_test)
labels = np.repeat(""very_good"", len(probs))
labels[probs[:, 1] &lt;  0.75] = ""good""
labels[probs[:, 1] &lt; 0.5] = ""average""
labels[probs[:, 1] &lt; 0.25] = ""poor""
</code></pre>
",0,1,182,2019-05-09 18:50:34,https://stackoverflow.com/questions/56065837/is-the-a-way-of-getting-the-degree-of-positiveness-or-negativeness-when-using-lo
how to add confusion matrix and k-fold 10 fold in sentiment analysis,"<p>I want to add an evaluation model using the cross-validation and confusion matrix k-fold (k = 10) method, but I'm confused
dataset : <a href=""https://github.com/fadholifh/dats/blob/master/cpas.txt"" rel=""nofollow noreferrer"">https://github.com/fadholifh/dats/blob/master/cpas.txt</a></p>

<p>Using Pyhon 3.7</p>

<pre class=""lang-py prettyprint-override""><code>import sklearn.metrics
import sen
import csv
import os
import re
import nltk
import scipy
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import svm
from sklearn.externals import joblib
from sklearn.pipeline import Pipeline
from sklearn import model_selection
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
factorys = StemmerFactory()
stemmer = factorys.create_stemmer()





if __name__ == ""__main__"":
    main()
</code></pre>

<p>the result is confusion matrix and for k-fold each fold has a percentage of F1-score, precission, and recall</p>
","python, scikit-learn, cross-validation, sentiment-analysis, confusion-matrix","<pre><code>df = pd.read_csv(""cpas.txt"", header=None, delimiter=""\t"")
X = df[1].values
y = df[0].values

stop_words = stopwords.words('english')
stemmer = PorterStemmer()

def clean_text(text, stop_words, stemmer):
    return "" "".join([stemmer.stem(word) for word in word_tokenize(text) 
                    if word not in stop_words and not word.isnumeric()])

X = np.array([clean_text(text, stop_words, stemmer) for text in X])

kfold = KFold(3, shuffle=True, random_state=33)
i = 1
for train_idx, test_idx in kfold.split(X):
    X_train = X[train_idx]
    y_train = y[train_idx]

    X_test = X[test_idx]
    y_test = y[test_idx]

    vectorizer = TfidfVectorizer()
    X_train = vectorizer.fit_transform(X_train)
    X_test = vectorizer.transform(X_test)

    model = LinearSVC()
    model.fit(X_train, y_train)
    print (""Fold : {0}"".format(i))
    i += 1
    print (classification_report(y_test, model.predict(X_test)))
</code></pre>

<p>The reason you use cross validation is for parameter tuning when the data is less. One can use grid search with CV to do this.</p>

<pre><code>df = pd.read_csv(""cpas.txt"", header=None, delimiter=""\t"")
X = df[1].values
labels = df[0].values

text = np.array([clean_text(text, stop_words, stemmer) for text in X])
idx = np.arange(len(text))
np.random.shuffle(idx)

text = text[idx]
labels = labels[idx]

pipeline = Pipeline([
        ('vectorizer', TfidfVectorizer()),
        ('svm', LinearSVC())])

params = {
    'vectorizer__ngram_range' : [(1,1),(1,2),(2,2)],
    'vectorizer__lowercase' : [True, False],
    'vectorizer__norm' : ['l1','l2']}

model = GridSearchCV(pipeline, params, cv=3, verbose=1)
model.fit(text, y)
</code></pre>
",0,0,619,2019-05-10 06:28:31,https://stackoverflow.com/questions/56071917/how-to-add-confusion-matrix-and-k-fold-10-fold-in-sentiment-analysis
word mapping for 2D word embedding,"<p>For my Masters Thesis, I created a Word2Vec model. I wanted to show this image to clarify the result. But how does the mapping works to display the words in this 2D space?</p>

<p>All words are represented by a vector of 300 dim. How are they mapped on this 2D image? What are the x &amp; y axes?</p>

<p>Code:</p>

<pre><code>w2v_model.build_vocab(documents)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print(""Vocab size"", vocab_size)

w2v_model.train(documents, total_examples=len(documents), 

epochs=W2V_EPOCH)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df_train.text)

vocab_size = len(tokenizer.word_index) + 1
print(""Total words"", vocab_size)

x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)
x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)

labels = df_train.target.unique().tolist()
labels.append(NEUTRAL)

encoder = LabelEncoder()
encoder.fit(df_train.target.tolist())

y_train = encoder.transform(df_train.target.tolist())
y_test = encoder.transform(df_test.target.tolist())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]
print(embedding_matrix.shape)
embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)
</code></pre>

<p><a href=""https://i.sstatic.net/bjXX3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bjXX3.png"" alt=""enter image description here""></a></p>
","vector, sentiment-analysis, cpu-word, nlp","<p>There are a couple of approaches.</p>

<p>The first is to use PCA (principal components analysis), and plot the first component on the x-axis, the second component on the y-axis (and throw away the other components). </p>

<p>You don't say which library you are using to generate your word vectors, and it might come with its own PCA function. But sklearn has one: <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a>
(<a href=""https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/develop-word-embeddings-python-gensim/</a> has some ready-made code showing making the vectors with gensim, then plotting them with that function.)</p>

<p>The other approach you could try is just to plot the first two dimensions of your word vectors. This is reasonable because all dimensions in word vectors should be carrying equal weight. I.e. taking any two of the 300 dimensions should give you the same amount of information as any other two dimensions.</p>

<p>But using PCA is the more normal approach for visualization.</p>
",1,1,1004,2019-05-12 14:23:34,https://stackoverflow.com/questions/56100162/word-mapping-for-2d-word-embedding
Python beginner : Preprocessing a french text in python and calculate the polarity with a lexicon,"<p>I am writing an algorithm in python which processes a column of sentences and then gives the polarity (positive or negative) of each cell of my column of sentences. The script uses a list of negative and positive word from the NRC emotion lexicon (French version) I am having a problem writing the preprocess function. I have already written the count function and the polarity function but since I have some difficulty writing the preprocess function, I am not really sure if those functions works. </p>

<p>The positive and negative words were in the same file (lexicon) but I export positive and negztive words separately because I did not know how to use the lexicon as it was. </p>

<p>My function count occurrence of positive and negative does not work and I do not know why it Always sends me 0. I Added positive word in each sentence so  the should appear in the dataframe:</p>

<p>stacktrace :</p>

<pre><code>
[4 rows x 6 columns]
   id                                           Verbatim      ...       word_positive  word_negative
0  15  Je n'ai pas bien compris si c'était destiné a ...      ...                   0              0
1  44  Moi aérien affable affaire agent de conservati...      ...                   0              0
2  45  Je affectueux affirmative te hais et la Foret ...      ...                   0              0
3  47  Je absurde accidentel accusateur accuser affli...      ...                   0              0

=&gt;  
def count_occurences_Pos(text, word_list):
    '''Count occurences of words from a list in a text string.'''
    text_list = process_text(text)

    intersection = [w for w in text_list if w in word_list]


    return len(intersection)
csv_df['word_positive'] = csv_df['Verbatim'].apply(count_occurences_Pos, args=(lexiconPos, ))
</code></pre>

<p>This my csv_data : line 44 , 45  contains positive words and line 47 more negative word but in the column of positive and negative word , it is alwaqys empty, the function does not return the number of words  and the final column is Always positive whereas the last sentence is negative</p>

<pre><code>id;Verbatim
15;Je n'ai pas bien compris si c'était destiné a rester
44;Moi aérien affable affaire agent de conservation qui ne agraffe connais rien, je trouve que c'est s'emmerder pour rien, il suffit de mettre une multiprise
45;Je affectueux affirmative te hais et la Foret enchantée est belle de milles faux et les jeunes filles sont assises au bor de la mer
47;Je absurde accidentel accusateur accuser affliger affreux agressif allonger allusionne admirateur admissible adolescent agent de police Comprends pas la vie et je suis perdue 
</code></pre>

<p>Here the full code :</p>

<pre><code># -*- coding: UTF-8 -*-
import codecs 
import re
import os
import sys, argparse
import subprocess
import pprint
import csv
from itertools import islice
import pickle
import nltk
from nltk import tokenize
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import pandas as pd
try:
    import treetaggerwrapper
    from treetaggerwrapper import TreeTagger, make_tags
    print(""import TreeTagger OK"")
except:
    print(""Import TreeTagger pas Ok"")

from itertools import islice
from collections import defaultdict, Counter



csv_df = pd.read_csv('test.csv', na_values=['no info', '.'], encoding='Cp1252', delimiter=';')
#print(csv_df.head())

stopWords = set(stopwords.words('french'))  
tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')     
def process_text(text):
    '''extract lemma and lowerize then removing stopwords.'''

    text_preprocess =[]
    text_without_stopwords= []

    text = tagger.tag_text(text)
    for word in text:
        parts = word.split('\t')
        try:
            if parts[2] == '':
                text_preprocess.append(parts[1])
            else:
                text_preprocess.append(parts[2])
        except:
            print(parts)


    text_without_stopwords= [word.lower() for word in text_preprocess if word.isalnum() if word not in stopWords]
    return text_without_stopwords

csv_df['sentence_processing'] = csv_df['Verbatim'].apply(process_text)
#print(csv_df['word_count'].describe())
print(csv_df)


lexiconpos = open('positive.txt', 'r', encoding='utf-8')
print(lexiconpos.read())
def count_occurences_pos(text, word_list):
    '''Count occurences of words from a list in a text string.'''

    text_list = process_text(text)

    intersection = [w for w in text_list if w in word_list]

    return len(intersection)


#csv_df['word_positive'] = csv_df['Verbatim'].apply(count_occurences_pos, args=(lexiconpos, ))
#print(csv_df)

lexiconneg = open('negative.txt', 'r', encoding='utf-8')

def count_occurences_neg(text, word_list):
    '''Count occurences of words from a list in a text string.'''
    text_list = process_text(text)

    intersection = [w for w in text_list if w in word_list]

    return len(intersection)
#csv_df['word_negative'] = csv_df['Verbatim'].apply(count_occurences_neg, args= (lexiconneg, ))
#print(csv_df)

def polarity_score(text):   
    ''' give the polarity of each text based on the number of positive and negative word '''
    positives_text =count_occurences_pos(text, lexiconpos)
    negatives_text =count_occurences_neg(text, lexiconneg)
    if positives_text &gt; negatives_text :
        return ""positive""
    else : 
        return ""negative""
csv_df['polarity'] = csv_df['Verbatim'].apply(polarity_score)
#print(csv_df)
print(csv_df)
</code></pre>

<p>If you could also see if the rest of the code is good to thank you.</p>
","pandas, nlp, nltk, sentiment-analysis, treetagger","<p>I have found your error!
It comes from the <code>Polarity_score</code> function.</p>

<p>It's just a typo :
In your, if statement you were comparing <code>count_occurences_Pos and count_occurences_Neg</code> which are function instead of comparing the results of the function <code>count_occurences_pos and count_occurences_peg</code></p>

<p>Your code should be like this :</p>

<pre><code>def Polarity_score(text):
    ''' give the polarity of each text based on the number of positive and negative word '''
    count_text_pos =count_occurences_Pos(text, word_list)
    count_text_neg =count_occurences_Neg(text, word_list)
    if count_occurences_pos &gt; count_occurences_peg :
        return ""Positive""
    else : 
        return ""negative""
</code></pre>

<p>In the future, you need to learn how to have meaningful names for your variables  to avoid those kinds of errors
With correct variables names, your function should be : </p>

<pre><code> def polarity_score(text):
        ''' give the polarity of each text based on the number of positive and negative word '''
        positives_text =count_occurences_pos(text, word_list)
        negatives_text =count_occurences_neg(text, word_list)
        if positives_text &gt; negatives_text :
            return ""Positive""
        else : 
            return ""negative""
</code></pre>

<p>Another improvement you can make in your count_occurences_pos and count_occurences_neg function is to use set instead of the list. Your text and world_list can be converted to sets and you can use the set intersection to retrieve the positive texts in them.Because set are faster than lists</p>
",1,0,1042,2019-05-22 10:08:51,https://stackoverflow.com/questions/56254377/python-beginner-preprocessing-a-french-text-in-python-and-calculate-the-polari
Negativity score for sentences,"<p>I am working on a dataset of airline customer complaints. Since it is ""complaints"" the general consensus is all the sentence are ""negative"" sentiment. So I am think of an approach to quantize the negativity score.</p>

<p>For example:</p>

<p>Less Negative review:</p>

<pre><code> ""the cabin did not have enough leg space but the food was decent"" - Score: 0.3
</code></pre>

<p>High Negative Review:</p>

<pre><code>""complete service was horrible, I will not recommend them ever"" - Score: 0.8
</code></pre>

<p>Any suggestions on existing approaches? </p>

<p>P.S I am not looking for an exact answer, any suggestions on the direction or approaches would be great.</p>
","nlp, sentiment-analysis, linguistics","<p>As @Vishal suggests in the comments, the simplest approach to add more sophistication would be to implement a <a href=""https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c"" rel=""nofollow noreferrer"">lexicon based sentiment analysis</a> scoring model where you custom score the polarity of each word in the lexicon to be higher and lower. You should also include some bigram and trigram lexicon phrases for more accuracy. </p>

<p>Public sentiment analysis APIs from Google, Azure, IBM, etc. do provide a scale of sentiment too.</p>
",0,0,98,2019-05-24 19:30:25,https://stackoverflow.com/questions/56298601/negativity-score-for-sentences
Stopwords coming up in most influential words,"<p>I am running some NLP code, trying to find the most influential (positively or negatively) words in a survey. My problem is that, while I successfully add some extra stopwords to the NLTK stopwords file, they keep coming up as influential words later on.</p>

<p>So, I have a dataframe, first column contains scores, second column contains comments.</p>

<p>I add extra stopwords:</p>

<pre><code>stopwords = stopwords.words('english')
extra = ['Cat', 'Dog']
stopwords.extend(extra)
</code></pre>

<p>I check that they are added, using the len method before and after.</p>

<p>I create this function to remove punctuation and stopwords from my comments:</p>

<pre><code>def text_process(comment):
   nopunc = [char for char in comment if char not in string.punctuation]
   nopunc = ''.join(nopunc)
   return [word for word in nopunc.split() if word.lower() not in stopwords]
</code></pre>

<p>I run the model (not going to include the whole code since it doesn't make a difference):</p>

<pre><code>corpus = df['Comment']
y = df['Label']
vectorizer = CountVectorizer(analyzer=text_process)
x = vectorizer.fit_transform(corpus)
</code></pre>

<p>...</p>

<p>And then to get the most influential words:</p>

<pre><code>feature_to_coef = {word: coef for word, coef in zip(vectorizer.get_feature_names(), nb.coef_[0])}


for best_positive in sorted(
    feature_to_coef.items(), 
    key=lambda x: x[1], 
    reverse=True)[:20]:
    print (best_positive)
</code></pre>

<p>But, Cat and Dog are in the results.</p>

<p>What am I doing wrong, any ideas?</p>

<p>Thank you very much!</p>
","python, nlp, nltk, sentiment-analysis","<p>Looks like it is because you have capitalize words 'Cat' and 'Dog'</p>

<p>In your text_process function, you have <code>if word.lower() not in stopwords</code> which only works if the stopwords are lower case</p>
",3,1,50,2019-05-30 16:18:30,https://stackoverflow.com/questions/56381970/stopwords-coming-up-in-most-influential-words
Why do I get a TypeError when importing a textfile line by line for sentiment analysis instead of using a sentence hard-coded?,"<p>I am trying to analyze the sentiment of each given sentence from a text file line by line. The code is working whenever I am using the hard coded sentences from the first question linked. When I use the text file input, I get the <code>TypeError</code>.</p>

<p>This is related to the question asked <a href=""https://stackoverflow.com/questions/32879532/stanford-nlp-for-python"">here</a>. And the line by line from text file code is coming from <a href=""https://stackoverflow.com/questions/52160274/how-to-iterate-through-each-line-of-a-text-file-and-get-the-sentiment-of-those-l/56450803#56450803"">this</a> question:</p>

<p>The first one works, the second with the text-file <code>(""I love you. I hate him. You are nice. He is dumb"")</code> does not work. Here is the code :</p>

<pre><code>from pycorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost:9000')
results = []    
with open(""c:/nlp/test.txt"",""r"") as f:
    for line in f.read().split('\n'):
        print(""Line:"" + line)
        res = nlp.annotate(line,
                   properties={
                       'annotators': 'sentiment',
                       'outputFormat': 'json',
                       'timeout': 1000,
                   })
        results.append(res)      

for res in results:             
    s = res[""sentences""]         
    print(""%d: '%s': %s %s"" % (
        s[""index""], 
        "" "".join([t[""word""] for t in s[""tokens""]]),
        s[""sentimentValue""], s[""sentiment""]))
</code></pre>

<p>I get this error:</p>

<blockquote>
  <p>line 21, in </p>
  
  <p>s[""index""],</p>
  
  <p>TypeError: list indices must be integers or slices, not str</p>
</blockquote>
","python, stanford-nlp, sentiment-analysis, pycorenlp","<p>Looks like I solved the problem. As londo pointed out: This line sets <code>S</code> as <code>List</code>, but it should be <code>dict</code>, just like in the original code:   </p>

<pre><code>s = res[""sentences""] 
</code></pre>

<p>I moved the code into the same loop where the file is read and analyzed line by line and I print the result directly there. So the new code looks like this:</p>

<pre><code>from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('http://localhost:9000')

with open(""c:/nlp/test.txt"",""r"") as f:
    for line in f.read().split('\n'):
        res = nlp.annotate(line,
                    properties={
                        'annotators': 'sentiment',
                        'outputFormat': 'json',
                        'timeout': 15000,
                   }) 
        for s in res[""sentences""]:
            print(""%d: '%s': %s %s"" % (
            s[""index""], 
            "" "".join([t[""word""] for t in s[""tokens""]]),
            s[""sentimentValue""], s[""sentiment""]))
</code></pre>

<p>The result looks just as intended and without any error message:</p>

<pre><code>0: 'I love you .': 3 Positive
0: 'I hate him .': 1 Negative
0: 'You are nice .': 3 Positive
0: 'He is dumb .': 1 Negative
</code></pre>
",0,1,71,2019-06-07 14:41:56,https://stackoverflow.com/questions/56496475/why-do-i-get-a-typeerror-when-importing-a-textfile-line-by-line-for-sentiment-an
Sentiment Analysis in R using TDM/DTM,"<p>I am trying to apply a sentiment analysis in R with the help of my DTM (document term matrix) or TDM (term document matrix). I could not find any similar topic in the forum and on google. Thus, I created a corpus and from that corpus I generated a dtm/tdm in R. My next step would be to apply the sentiment analysis which I need later for stock prediction via SVM. My give code is that:</p>

<pre><code>    dtm &lt;- DocumentTermMatrix(docs)
    dtm &lt;- removeSparseTerms(dtm, 0.99)
    dtm &lt;- as.data.frame(as.matrix(dtm))

    tdm &lt;- TermDocumentMatrix(docs)
    tdm &lt;- removeSparseTerms(tdm, 0.99)
    tdm &lt;- as.data.frame(as.matrix(tdm))
</code></pre>

<p>I read that it is possible through the tidytext package with the help of the get_sentiments() function. But it was not possible to apply that with a DTM/TDM. How can I run a sentiment analysis for my cleaned filter words which are already stemmed, tokenized etc.? I saw that a lot of people did the sentiment analysis for a hole sentence, but I would like to apply it for my single words in order to see if they are positive, negative, score etc. Many thanks in advance!</p>
","r, text-mining, data-analysis, sentiment-analysis, sentimentr","<p><code>SentimentAnalysis</code> has good integration with <code>tm</code>.</p>

<pre><code>library(tm)
library(SentimentAnalysis)

documents &lt;- c(""Wow, I really like the new light sabers!"",
               ""That book was excellent."",
               ""R is a fantastic language."",
               ""The service in this restaurant was miserable."",
               ""This is neither positive or negative."",
               ""The waiter forget about my dessert -- what poor service!"")

vc &lt;- VCorpus(VectorSource(documents))
dtm &lt;- DocumentTermMatrix(vc)

analyzeSentiment(dtm, 
  rules=list(
    ""SentimentLM""=list(
      ruleSentiment, loadDictionaryLM()
    ),
    ""SentimentQDAP""=list(
      ruleSentiment, loadDictionaryQDAP()
    )
  )
)
#   SentimentLM SentimentQDAP
# 1       0.000     0.1428571
# 2       0.000     0.0000000
# 3       0.000     0.0000000
# 4       0.000     0.0000000
# 5       0.000     0.0000000
# 6      -0.125    -0.2500000
</code></pre>
",1,0,1659,2019-06-09 16:22:02,https://stackoverflow.com/questions/56516317/sentiment-analysis-in-r-using-tdm-dtm
How to use naive bayes classifier after Extract the features using TF_IDF,"<p>I'm Trying to classify features using Naive Bayes classifier, I used TF_IDF for feature extraction.</p>

<p>The <code>finaltfidfVector</code> is a list of vectors, each vector represents list of numbers, <code>0</code> if the word not found, else the weight of word if it found.  </p>

<p>And <code>classlabels</code> contains all class label for each vector. I'm trying to classify it with this code but it doesn't work. </p>

<p>26652 lines for Dataset</p>

<pre><code>from nltk.classify import apply_features

def naivebyse(finaltfidfVector,classlabels,reviews):

    train_set = []
    j = 0
    for vector in finaltfidfVector:
        arr={}
        if j&lt;18697:
            arr[tuple(vector)] = classlabels[j]
            train_set.append((arr, reviews[j]))
            j += 1

    test_set = []
    j = 18697
    for vector in finaltfidfVector:
        arr = {}
        if j &lt; 26652 and j&gt;=18697:
            arr[tuple(vector)] = classlabels[j]
            test_set.append((arr, reviews[j]))
            j += 1

    classifier = nltk.NaiveBayesClassifier.train(train_set)
    print(nltk.classify.accuracy(classifier, test_set))
</code></pre>

<p>The output :</p>

<pre><code>0.0

</code></pre>

<p>The used <strong><em>reference for TF_IDF and applied on <code>finaltfidfVector</code></em></strong> <a href=""https://triton.ml/blog/tf-idf-from-scratch?fbclid=IwAR3UlCToGYFEQSmugXo3M5Q9fcld79JfXSfBaDG7wKv5a49O0ZDEft9DFNg"" rel=""nofollow noreferrer"">https://triton.ml/blog/tf-idf-from-scratch?fbclid=IwAR3UlCToGYFEQSmugXo3M5Q9fcld79JfXSfBaDG7wKv5a49O0ZDEft9DFNg</a>. 
data set</p>

<p><a href=""https://i.sstatic.net/eG0te.png"" rel=""nofollow noreferrer"">this is sample about the used data set before preprocessing and TF_IDF</a></p>

<p>This is sample for the first vector for index of zero in <code>finaltfidfVector</code> list</p>

<pre><code>[0.0,0.0, 0.0, 0.6214608098422192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5115995809754083,0.0,0.0, 0.0, 0.0, 0.5521460917862246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214608098422192,0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214608098422192, 0.0, 0.0, 0.0, 0.6214608098422192]
</code></pre>

<p><code>classlabels</code> contains class label for each vector 
, 1 for sarcasm 0 for not sarcasm. The class label of index 0 is 1, this 1 for the first vector in <code>finaltfidfVector</code>.</p>

<p>The first item for <strong>train_set</strong> is </p>

<p><code>({(0.0, 0.0, 1.3803652294655615,.....ect): '0'}, ""former versace store clerk sues over secret 'black code' for minority shoppers"")</code></p>
","python, classification, nltk, sentiment-analysis, naivebayes","<p>Here's a reproducible toy example:</p>

<pre><code># let's define a train_set
train_set = [
    ({'adam': 0.05,'is': 0.0, 'a': 0.0, 'good': 0.02, 'man': 0.0}, 1),
    ({'eve': 0.0, 'is':  0.0, 'a':  0.0,'good':  0.02,'woman': 0.0}, 1),
    ({'adam': 0.05, 'is': 0.0, 'evil': 0.0}, 0)]
</code></pre>

<p>The toy data set is created using a handcrafted ""tfidf"" score dictionary:</p>

<pre><code>tfidf_dict = {
 'adam': 0.05,
 'eve': 0.05,
 'evil': 0.02,
 'kind': 0.02,
 'good': 0.02,
 'bad': 0.02
}
</code></pre>

<p>Where each known word has a tfidf score, and an unknow word has score 0. And also in train_set we have positive score for the sentence labeled by 1 (""adam is good""), and negative labeled by 0 (""adam is evil"").</p>

<p>Now run some test:</p>

<pre><code>import nltk
clf = nltk.NaiveBayesClassifier.train(train_set)
</code></pre>

<p>See how this works on toy train set:</p>

<pre><code>&gt;&gt;&gt; nltk.classify.accuracy(clf, train_set)
1.0
</code></pre>

<p>Since test set has the same structure as train set, this is sufficient to show how you can train and run a naive bayes classifier.</p>
",0,0,183,2019-06-10 00:43:27,https://stackoverflow.com/questions/56519402/how-to-use-naive-bayes-classifier-after-extract-the-features-using-tf-idf
"when checking target: expected dense_2 to have shape (1,) but got array with shape (2,)","<p>sentiment analyses with csv contains 45k with two cols[text,sentiment],trying to use sigmoid with binary_crossentropy but its return an error :</p>

<blockquote>
  <p>Error when checking target: expected dense_2 to have shape (1,) but
  got array with shape (2,)</p>
</blockquote>

<p>i have tried to use LabelEncoder , but its return, bad input shape, how do i let the encoding label acceptable for Sigmond 1 dense ?</p>

<pre><code>#I do aspire here to have balanced classes
num_of_categories = 45247
shuffled = data.reindex(np.random.permutation(data.index))
e = shuffled[shuffled['sentiment'] == 'POS'][:num_of_categories]
b = shuffled[shuffled['sentiment'] == 'NEG'][:num_of_categories]
concated = pd.concat([e,b], ignore_index=True)
for idx,row in data.iterrows():
    row[0] = row[0].replace('rt',' ')
#Shuffle the dataset
concated = concated.reindex(np.random.permutation(concated.index))
concated['LABEL'] = 0

#encode the lab
encoder = LabelEncoder()
concated.loc[concated['sentiment'] == 'POS', 'LABEL'] = 0
concated.loc[concated['sentiment'] == 'NEG', 'LABEL'] = 1
print(concated['LABEL'][:10])
labels = encoder.fit_transform(concated)
print(labels[:10])
if 'sentiment' in concated.keys():
    concated.drop(['sentiment'], axis=1)

n_most_common_words = 8000
max_len = 130
tokenizer = Tokenizer(num_words=n_most_common_words, filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(concated['text'].values)
sequences = tokenizer.texts_to_sequences(concated['text'].values)
word_index = tokenizer.word_index
</code></pre>
","python-3.x, encoding, deep-learning, sentiment-analysis, text-classification","<p>The output of <code>LabelEncoder</code> if also 1 dim, I guess the output of your network have two dim. So you need to one-hot your y_true.</p>

<p>use </p>

<pre><code>labels = keras.utils.to_categorical(concated['LABEL'], num_classes=2)
</code></pre>

<p>instead</p>

<pre><code>labels = encoder.fit_transform(concated)
</code></pre>
",1,0,76,2019-06-13 20:11:28,https://stackoverflow.com/questions/56587933/when-checking-target-expected-dense-2-to-have-shape-1-but-got-array-with-sha
How to determine which words have high predictive power in Sentiment Analysis?,"<p>I am working on a classification problem with Tweeter data. User labeled tweets (relevant, not relevant) are used to train a machine learning classifier to predict if an unseen tweet is relevant or not to the user. </p>

<p>I use a simple preprocessing techniques like removal of stopwords, stemming etc and a sklearn Tfidfvectorizer to convert the words into numbers before feeding them into a classifier e.g. SVM, kernel SVM , Naïve Bayes. </p>

<p>I would like to determine which words (features) have the higher predictive power. What is the best way to do so?</p>

<p>I have tried wordcloud but it just shows the words with highest frequency in the sample.</p>

<p>UPDATE:</p>

<p>The following approach along with sklearns feature_selection seem to provide the best answer so far to my problem:</p>

<p><a href=""https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d"" rel=""nofollow noreferrer"">top features</a> Any other suggestions?</p>
","twitter, nlp, sentiment-analysis, tf-idf, feature-selection","<p>Have you tried using tfidf? It creates a weighted matrix providing greater weight to the more semantically meaningful words of each text. It compares the individual text( in this case a tweet) to all of the texts (all of the tweets). It is much more helpful than using raw term counts for classification and other tasks. <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></p>
",2,1,262,2019-06-20 13:32:54,https://stackoverflow.com/questions/56687161/how-to-determine-which-words-have-high-predictive-power-in-sentiment-analysis
Extract top positive and negative features when applying dictionary in quanteda,"<p>I have a data frame with around 100k rows that contain textual data. Using the quanteda package, I apply sentiment analysis (Lexicoder dictionary) to eventually calculate a sentiment score. 
For an additional - more qualitative - step of analysis I would like extract the top features (i.e. negative/positive words from the dictionary that occur most frequent in my data) to examine whether the discourse is driven by particular words.</p>

<pre><code>my_corpus &lt;- corpus(my_df, docid_field = ""ID"", text_field = ""my_text"", metacorpus = NULL, compress = FALSE)
sentiment_corp &lt;- dfm(my_corpus, dictionary = data_dictionary_LSD2015)
</code></pre>

<p>However, going through the <a href=""https://quanteda.io/"" rel=""nofollow noreferrer"">quanteda documentation</a>, I couldn't figure out how to achieve this - is there a way? 
I'm aware of <code>topfeatures</code> and I did read <a href=""https://stackoverflow.com/questions/39746362/quanteda-extracting-identified-dictionary-words"">this question</a>, but it didn't help. </p>
","r, dictionary, sentiment-analysis, quanteda","<p>In all of the <strong>quanteda</strong> functions that take a <code>pattern</code> argument, the valid types of patterns are character vectors, lists, and dictionaries.  So the best way to assess each the top features in each dictionary category (what we also call a dictionary <em>key</em>) is to select on that dictionary and then use <code>topfeatures()</code>.</p>
<p>Here is how to do this using the built-in <code>data_corpus_irishbudget2010</code> object, as an example, with the Lexicoder Sentiment Dictionary.</p>

<pre class=""lang-r prettyprint-override""><code>library(&quot;quanteda&quot;)
## Package version: 1.4.3

# tokenize and select just the dictionary value matches
toks &lt;- tokens(data_corpus_irishbudget2010) %&gt;%
  tokens_select(pattern = data_dictionary_LSD2015)
lapply(toks[1:5], head)
## $`Lenihan, Brian (FF)`
## [1] &quot;severe&quot;        &quot;distress&quot;      &quot;difficulties&quot;  &quot;recovery&quot;     
## [5] &quot;benefit&quot;       &quot;understanding&quot;
## 
## $`Bruton, Richard (FG)`
## [1] &quot;failed&quot;   &quot;warnings&quot; &quot;sucking&quot;  &quot;losses&quot;   &quot;debt&quot;     &quot;hurt&quot;    
## 
## $`Burton, Joan (LAB)`
## [1] &quot;remarkable&quot; &quot;consensus&quot;  &quot;Ireland&quot;    &quot;opposition&quot; &quot;knife&quot;     
## [6] &quot;dispute&quot;   
## 
## $`Morgan, Arthur (SF)`
## [1] &quot;worst&quot;     &quot;worst&quot;     &quot;well&quot;      &quot;corrupt&quot;   &quot;golden&quot;    &quot;protected&quot;
## 
## $`Cowen, Brian (FF)`
## [1] &quot;challenge&quot;      &quot;succeeding&quot;     &quot;challenge&quot;      &quot;oppose&quot;        
## [5] &quot;responsibility&quot; &quot;support&quot;
</code></pre>
<p>To explore the top matches for the positive entry, we can select them further by subsetting the dictionary for the Positive key.</p>
<pre class=""lang-r prettyprint-override""><code># top positive matches
tokens_select(toks, pattern = data_dictionary_LSD2015[&quot;positive&quot;]) %&gt;%
  dfm() %&gt;%
  topfeatures()
##    benefit    support   recovery       fair     create confidence 
##         68         52         44         41         39         37 
##    provide       well     credit       help 
##         36         33         31         29
</code></pre>
<p>And for Negative:</p>
<pre class=""lang-r prettyprint-override""><code># top negative matches
tokens_select(toks, pattern = data_dictionary_LSD2015[[&quot;negative&quot;]]) %&gt;%
  dfm() %&gt;%
  topfeatures()
##    ireland    benefit        not    support     crisis   recovery 
##         79         68         52         52         47         44 
##       fair     create    deficit confidence 
##         41         39         38         37
</code></pre>
<p>Why is &quot;Ireland&quot; a negative match?  Because the LSD2015 includes <code>ir*</code> as a negative word that is intended to match <em>ire</em> and <em>ireful</em> but with the default case insensitive matching, also matches <em>Ireland</em> (a term frequently used in this example corpus).  This is an example of a &quot;false positive&quot; match, always a risk in dictionaries when using wildcarding or when using a language such as English that has a very high rate of polysemes and homographs.</p>
",3,4,550,2019-06-24 08:55:04,https://stackoverflow.com/questions/56733046/extract-top-positive-and-negative-features-when-applying-dictionary-in-quanteda
Error of TfidfVectorizer on cleaned text dataset,"<p>I am trying to vectorize a sentiment data set. It has review text and sentimentlabel given. When I try to vectorize the data set It gives an error called <strong>'LazyCorpusLoader' object is not iterable</strong> </p>

<p>The reviews were cleaned as follows.</p>

<ul>
<li>remove html tags</li>
<li>tokenize text to remove punctuations</li>
<li>remove stop words</li>
<li>POS tagging</li>
<li>lemmatize text</li>
</ul>

<p>After these my dataframe reviewdataset_df has following columns: </p>

<ol>
<li>reviews_clean->cleaned review text</li>
<li>SENTIMENT-> a sentiment label as positive or negative</li>
</ol>

<p>then I split the data set using below code,</p>

<pre><code>#splitting data set into training and testing
X_train,X_test,Y_train,Y_test =train_test_split(reviewDataset_Df.head(10000).review_clean,reviewDataset_Df.head(10000).SENTIMENT,test_size=0.20,random_state=0,shuffle=True)                                          

print('Training data count:'+str(len(X_train)))
print('Test data count:'+str(len(X_test)))
</code></pre>

<p>That worked well. </p>

<p>Then I use vectorizer using following code.</p>

<pre><code>#vectorizer
tfidf=TfidfVectorizer(sublinear_tf=True,min_df=3,stop_words=english,norm='l2',encoding='utf-8',ngram_range=(1,3))
print(""rr"")
train_features=tfidf.fit_transform(X_train)
test_features=tfidf.transform(X_test)
train_labels=Y_train
test_labels=Y_test
</code></pre>

<p>This gives an error as 
<strong>return frozenset(stop)
TypeError: 'LazyCorpusLoader' object is not iterable</strong> </p>

<p>I searched and tried on some solutions which didn't worked. How to overcome this error. I need to vectorize the data set to train for a recommendation system. </p>

<p>note: I searched through internet and read similar question in stackoverflow but couldn't find a proper answer.</p>
","python, data-mining, sentiment-analysis, tfidfvectorizer","<p>Without a proper error trace we can only guess.</p>

<p>Since the error involves <code>stop</code> my guess is that your variable <code>english</code> - that isn't in the code you shared at all - is inappropriately set up, and not a set of words.</p>

<p>You probably meant to use <code>stop_words=""english""</code> instead.</p>
",1,-2,227,2019-06-25 04:31:11,https://stackoverflow.com/questions/56746874/error-of-tfidfvectorizer-on-cleaned-text-dataset
How to add a feature using a pipeline and FeatureUnion,"<p>In the code below I use a tweeter dataset to perform sentiment analysis. I use  a pipeline which performs the following processes:</p>

<p>1) performs some basic text preprocessing</p>

<p>2) vectorizes the tweet text</p>

<p>3) adds an extra feature ( text length)</p>

<p>4) classification</p>

<p>I would like to add one more feature which is the scaled number of followers. I wrote a function that takes as an input the whole dataframe (df) and returns a new dataframe with scaled number of followers. However, I am finding it challenging to add this process on the pipeline e.g. add this feature to the other features using the sklearn pipeline. </p>

<p>Any help or advise on this problem will be much appreciated.</p>

<p>the question and code below is inspired by Ryan's post:<a href=""https://ryan-cranfill.github.io/sentiment-pipeline-sklearn-2/"" rel=""nofollow noreferrer"">pipelines</a></p>

<pre class=""lang-py prettyprint-override""><code>
import nltk
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

def import_data(filename,sep,eng,header = None,skiprows=1):
    #read csv
    dataset = pd.read_csv(filename,sep=sep,engine=eng,header = header,skiprows=skiprows)
    #rename columns
    dataset.columns = ['text','followers','sentiment']
    return dataset

df = import_data('apple_v3.txt','\t','python')
X, y = df.text, df.sentiment
X_train, X_test, y_train, y_test = train_test_split(X, y)

tokenizer = nltk.casual.TweetTokenizer(preserve_case=False, reduce_len=True)
count_vect = CountVectorizer(tokenizer=tokenizer.tokenize) 
classifier = LogisticRegression()

def get_scalled_followers(df):
    scaler = MinMaxScaler()
    df[['followers']] = df[['followers']].astype(float)
    df[['followers']] = scaler.fit_transform(df[['followers']])
    followers = df['followers'].values
    followers_reshaped = followers.reshape((len(followers),1))
    return df

def get_tweet_length(text):
    return len(text)
import numpy as np

def genericize_mentions(text):
    return re.sub(r'@[\w_-]+', 'thisisanatmention', text)

def reshape_a_feature_column(series):
    return np.reshape(np.asarray(series), (len(series), 1))

def pipelinize_feature(function, active=True):
    def list_comprehend_a_function(list_or_series, active=True):
        if active:
            processed = [function(i) for i in list_or_series]
            processed = reshape_a_feature_column(processed)
            return processed

        else:
            return reshape_a_feature_column(np.zeros(len(list_or_series)))

from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn_helpers import pipelinize, genericize_mentions, train_test_and_evaluate


sentiment_pipeline = Pipeline([
        ('genericize_mentions', pipelinize(genericize_mentions, active=True)),
        ('features', FeatureUnion([
                    ('vectorizer', count_vect),
                    ('post_length', pipelinize_feature(get_tweet_length, active=True))
                ])),
        ('classifier', classifier)
    ])

sentiment_pipeline, confusion_matrix = train_test_and_evaluate(sentiment_pipeline, X_train, y_train, X_test, y_test)

</code></pre>
","python, scikit-learn, pipeline, sentiment-analysis, feature-extraction","<p>The best explanation I have found so far is at the following post: <a href=""https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines/data"" rel=""nofollow noreferrer"">pipelines</a></p>

<p>My data includes heterogenous features and the following step by step approach works well and is easy to understand:</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline, FeatureUnion

#step1 - select data from dataframe and split the dataset in train and test sets

features= [c for c in df.columns.values if c  not in ['sentiment']]
numeric_features= [c for c in df.columns.values if c  not in ['text','sentiment']]
target = 'sentiment'

X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.33, random_state=42)

#step2 - create a number selector class and text selector class. These classes allow to select specific columns from the dataframe

class NumberSelector(BaseEstimator, TransformerMixin):

    def __init__(self, key):
        self.key = key

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[[self.key]]

class TextSelector(BaseEstimator, TransformerMixin):

    def __init__(self, key):
        self.key = key

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[self.key]

#step 3 create one pipeline for the text data and one for the numerical data


text = Pipeline([
                ('selector', TextSelector(key='content')),
                ('tfidf', TfidfVectorizer( stop_words='english'))
            ])

text.fit_transform(X_train)

followers =  Pipeline([
                ('selector', NumberSelector(key='followers')),
                ('standard', MinMaxScaler())
            ])

followers.fit_transform(X_train)

#step 4 - features union

feats = FeatureUnion([('text', text), 
                      ('length', followers)])

feature_processing = Pipeline([('feats', feats)])
feature_processing.fit_transform(X_train)

# step 5 - add the classifier and predict 

pipeline = Pipeline([
    ('features',feats),
    ('classifier', SVC(kernel = 'linear', probability=True, C=1, class_weight = 'balanced'))
])

pipeline.fit(X_train, y_train)

preds = pipeline.predict(X_test)
np.mean(preds == y_test)

# step 6 use the model to predict new data not included in the test set
# in my example the pipeline expects a dataframe as an input which should have a column called 'text' and a column called 'followers'

array = [[""@apple is amazing"",25000]]
dfObj = pd.DataFrame(array,columns = ['text' , 'followers']) 

#prints the expected class e.g. positive or negative sentiment
print(pipeline.predict(dfObj))

#print the probability for each class
print(pipeline.predict_proba(dfObj))

</code></pre>
",3,2,4170,2019-06-26 14:11:07,https://stackoverflow.com/questions/56774862/how-to-add-a-feature-using-a-pipeline-and-featureunion
Twitter sentiment analysis on a string,"<p>I've written a program that takes a twitter data that contains tweets and labels (<code>0</code> for neutral sentiment and <code>1</code> for negative sentiment) and predicts which category the tweet belongs to.
The program works well on the training and test Set. However I'm having problem in applying prediction function with  a string. I'm not sure how to do that.</p>

<p>I have tried cleaning the string the way I cleaned the dataset before calling the predict function but the values returned are in wrong shape.</p>

<pre><code>import numpy as np
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
import re

#Loading dataset
dataset = pd.read_csv('tweet.csv')

#List to hold cleaned tweets
clean_tweet = []

#Cleaning tweets
for i in range(len(dataset)):
    tweet = re.sub('[^a-zA-Z]', ' ', dataset['tweet'][i])
    tweet = re.sub('@[\w]*',' ',dataset['tweet'][i])
    tweet = tweet.lower()
    tweet = tweet.split()
    tweet = [ps.stem(token) for token in tweet if not token in set(stopwords.words('english'))]
    tweet = ' '.join(tweet)
    clean_tweet.append(tweet)

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 3000)
X = cv.fit_transform(clean_tweet)
X =  X.toarray()
y = dataset.iloc[:, 1].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)

from sklearn.naive_bayes import GaussianNB
n_b = GaussianNB()
n_b.fit(X_train, y_train)
y_pred  = n_b.predict(X_test) 

some_tweet = ""this is a mean tweet""  # How to apply predict function to this string
</code></pre>
","python, machine-learning, scikit-learn, nlp, sentiment-analysis","<p>Use <code>cv.transform([cleaned_new_tweet])</code> on your new string to transform your new Tweet to your existing document-term matrix. That will return the Tweet in the correct shape.</p>
",2,2,484,2019-07-03 14:25:45,https://stackoverflow.com/questions/56872294/twitter-sentiment-analysis-on-a-string
How to match asset price data from a csv file to another csv file with relevant news by date,"<p>I am researching the impact of news article sentiment related to a financial instrument and its potenatial effect on its instruments's price. I have tried to get the timestamp of each news item, truncate it to minute data (ie remove second and microsecond components) and get the base shareprice of an instrument at that time, and at several itervals after that time, in our case t+2. However, program created twoM to the file, but does not return any calculated price changes</p>

<p>Previously, I used Reuters Eikon and its functions to conduct the research, described in the article below.</p>

<p><a href=""https://developers.refinitiv.com/article/introduction-news-sentiment-analysis-eikon-data-apis-python-example"" rel=""nofollow noreferrer"">https://developers.refinitiv.com/article/introduction-news-sentiment-analysis-eikon-data-apis-python-example</a></p>

<p>However, instead of using data available from Eikon, I would like to use my own csv news file with my own price data from another csv file. I am trying to match the </p>

<pre class=""lang-py prettyprint-override""><code>excel_file = 'C:\\Users\\Artur\\PycharmProjects\\JRA\\sentimenteikonexcel.xlsx'
df = pd.read_excel(excel_file)
sentiment = df.Sentiment
print(sentiment)


start = df['GMT'].min().replace(hour=0,minute=0,second=0,microsecond=0).strftime('%Y/%m/%d')
end = df['GMT'].max().replace(hour=0,minute=0,second=0,microsecond=0).strftime('%Y/%m/%d')

spot_data = 'C:\\Users\\Artur\\Desktop\\stocksss.csv'
spot_price_10 = pd.read_csv(spot_data)
print(spot_price_10)

df['twoM'] = np.nan


for idx, newsDate in enumerate(df['GMT'].values):
    sTime = df['GMT'][idx]
    sTime = sTime.replace(second=0, microsecond=0)
    try:
        t0 = spot_price_10.iloc[spot_price_10.index.get_loc(sTime),2]
        df['twoM'][idx] = ((spot_price_10.iloc[spot_price_10.index.get_loc((sTime + datetime.timedelta(minutes=10))),3]/(t0)-1)*100)
    except:
        pass
print(df)

</code></pre>

<p>However, the programm is not able to return the twoM price change values</p>

<p><a href=""https://i.sstatic.net/bEubU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bEubU.png"" alt=""enter image description here""></a></p>
","python, csv, sentiment-analysis, finance","<p>I assume that you got a warning because you are trying to make changes on views. As soon as you have 2 <code>[]</code> (one for the column, one for the row) you can only read. You must use  <code>loc</code> or <code>iloc</code> to write a value:</p>

<pre><code>...
try:
    t0 = spot_price_10.iloc[spot_price_10.index.get_loc(sTime),2]
    df.loc[idx,'twoM'] = ((spot_price_10.iloc[spot_price_10.index.get_loc((sTime + datetime.timedelta(minutes=10))),3]/(t0)-1)*100)
except:
    pass
...
</code></pre>
",0,0,58,2019-07-04 11:02:32,https://stackoverflow.com/questions/56886561/how-to-match-asset-price-data-from-a-csv-file-to-another-csv-file-with-relevant
Package ‘Rstem’ is not available (for R version 3.5.1),"<p>I am trying to install the <code>Rstem package</code>, but I am getting the message that there is no version available for the version of <code>R 3.5.1</code>. I'm using the <code>macOs El Captain</code>.</p>

<p>The error is:</p>

<pre><code>&gt; install.packages('Rstem', repos = 'https://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz')
Installing package into ‘/Users/ls_rafael/Library/R/3.5/library’
(as ‘lib’ is unspecified)
Warning in install.packages :
  unable to access index for repository https://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz/src/contrib:
  cannot open URL 'https://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz/src/contrib/PACKAGES'
Warning in install.packages :
  package ‘Rstem’ is not available (for R version 3.5.1)
Warning in install.packages :
  unable to access index for repository https://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz/bin/macosx/el-capitan/contrib/3.5:
  cannot open URL 'https://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz/bin/macosx/el-capitan/contrib/3.5/PACKAGES'
</code></pre>

<p>I already tried the suggested options in this link <a href=""https://stackoverflow.com/questions/9945775/issues-in-installing-rstem-package"">issues in installing Rstem package</a> and also downloading the package locally from the official website <a href=""http://www.omegahat.net/Rstem/"" rel=""nofollow noreferrer"">Rstem Package</a>, but the result is also unsatisfactory.</p>

<p>I'm studying how to do an <code>sentiment analysis</code> with <code>Twitter</code> data. I would like to know if there is any alternative to this package or if there is any <code>trick</code> to install it.</p>
","r, rstudio, data-science, text-mining, sentiment-analysis","<p>RStem package has been removed from the CRAN repository. You can download using the following command:- </p>

<pre><code>install.packages('Rstem', repos = ""http://www.omegahat.net/R"")
</code></pre>

<p>Make sure you have RTools installed on your machine. You can download it from this link - </p>

<p><a href=""https://cran.rstudio.com/bin/windows/Rtools/"" rel=""nofollow noreferrer"">Building R for Windows</a></p>
",2,1,2197,2019-07-08 20:51:40,https://stackoverflow.com/questions/56942406/package-rstem-is-not-available-for-r-version-3-5-1
Error in UseMethod(&quot;type&quot;) : no applicable method for &#39;type&#39; applied to an object of class &quot;factor&quot; - Sentiment Analysis,"<p>I am a total newbie when it comes to R and want to conduct a sentiment analysis for my term paper, relying on the code of my instructor. However, she used another dictionary so I have to adapt my code which is where the trouble starts.</p>

<p>I am trying to create a variable that scores the occurrence of positive terms. When I run the loop however, I get the error: </p>

<pre><code>Error in UseMethod(""type"") : 
  no applicable method for 'type' applied to an object of class ""factor""
</code></pre>

<p>I have already searched the internet and read that my data could probably be stored in the wrong format. However, both my data sets (<code>nss2018</code> is the data I want to conduct the analysis on; <code>posterms</code> contains the positive words of the dictionary) are stored as a list which is the same data type my instructor used.</p>

<p>Since I usually don't work with R, I'm a little desperate and can't make any sense of this. </p>

<p>My code for creating the variable:</p>

<pre><code>nss2018$posterms &lt;- 0


for (i in 1:nrow(posterms)) {
  occur &lt;- str_count(nss2018$text, posterms$V1[i])
  nss2018$posterms &lt;- nss2018$posterms + occur                  
}
</code></pre>
","r, sentiment-analysis","<p>The problem is at <code>string_count</code>. If the second argument is a factor you get an error. For example:</p>

<pre><code>str_count(as.factor(letters), as.factor(c('a', 'b')))
#Error in UseMethod(""type"") : 
#  no applicable method for 'type' applied to an object of class ""factor""
</code></pre>

<p>Convert that to <code>character</code> and you will be fine:</p>

<pre><code> str_count(as.factor(letters), as.character(as.factor(c('a', 'b'))))
 #[1] 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
</code></pre>

<p>So in your case:</p>

<pre><code>for (i in 1:nrow(posterms)) {
  occur &lt;- str_count(nss2018$text, as.character(posterms$V1[i]))
  nss2018$posterms &lt;- nss2018$posterms + occur                  
}
</code></pre>
",0,2,4603,2019-07-12 08:18:35,https://stackoverflow.com/questions/57002944/error-in-usemethodtype-no-applicable-method-for-type-applied-to-an-objec
How to fix &quot;no package called textdata&quot; error?,"<p>I am trying to run sentiment analysis in R. I have installed tidytext and it is in the correct library with all other packages. </p>

<p>However, when I run </p>

<pre><code>get_sentiments(""afinn"") 

</code></pre>

<p>I get the following error: </p>

<pre><code>Error in loadNamespace(name) : there is no package called ‘textdata’
</code></pre>

<p>Any suggestions on how to fix?</p>
","r, sentiment-analysis, tidytext","<p>This means that the package is missing from your libraries. You need to install it with <code>install.packages(""textdata"")</code>.</p>
",4,0,792,2019-07-18 12:58:04,https://stackoverflow.com/questions/57095032/how-to-fix-no-package-called-textdata-error
How to update the sentiment scores for some words in textblob?,"<p>I want to change the lexicon for Textblob by adding several new words with scores, and by slightly adjusting the score of the words that are already there. What is the best way to approach this?</p>

<p>In Vader sentiment it's done like this:</p>

<pre><code> SIA.lexicon.update(new_words)
</code></pre>

<p>Is there a similar command for Textblob?</p>
","python, sentiment-analysis, textblob","<p>Found the solution.
The scores are stored in en-sentiment.xml file in .../site-packages/textblob/en.</p>

<p>Words can be added to the file with corresponding scores or removed/updated.</p>

<p>I'm not sure how to make both the original file and the updated one work together (i.e. how to choose which one I want), so I backed up the original file, and renamed the updated one to 'en-sentiment.xml'.</p>
",1,0,797,2019-07-24 17:49:52,https://stackoverflow.com/questions/57188631/how-to-update-the-sentiment-scores-for-some-words-in-textblob
How to check feature importances on text feature?,"<p>First of all, I'm still doing study about classifier comparison on sentiment analysis. Then, I want to know for every feature importances on each classifier.</p>

<p>I've already tried <code>model.feature_importances_</code>, but because I vectorize my data train, I can't understand what word is on those feature importances.</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

line = pd.read_csv('line_label.csv', encoding = ""ISO-8859-1"")

x = line.Berita
y = line.Sentimen

xcv = x
xtf = x

countvect = CountVectorizer(analyzer = ""word"", tokenizer = None, lowercase = None)
xcv = countvect.fit_transform(x).toarray()

X_train, X_test, y_train, y_test = train_test_split(xcv, y, test_size=0.01, random_state=42)

from sklearn.ensemble import RandomForestClassifier 

rf = RandomForestClassifier() 

rf.fit(X_train, y_train) 

rf.score(X_test, y_test)

rf.feature_importances_
</code></pre>

<p>It shows </p>

<pre><code>array([2.20854745e-04, 1.24760561e-04, 3.14268988e-03, ...,
   1.71782391e-04, 5.15755286e-05, 2.13065348e-08])
</code></pre>
","python, machine-learning, scikit-learn, classification, sentiment-analysis","<p>Use below code:</p>

<pre><code>for feature, importance in zip(countvect.get_feature_names(), rf.feature_importances_):
    print('{}: {}'.format(feature, importance))
</code></pre>
",0,3,806,2019-07-26 14:09:48,https://stackoverflow.com/questions/57221789/how-to-check-feature-importances-on-text-feature
How to test Stanford Sentiment model?,"<p>I know the following command can be used to train a Stanford sentiment model</p>

<pre><code>java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz
</code></pre>

<p>Now I want to how to test the model with the testing dataset.
I tried using <code>-trainPath</code> option, it didn't seem to work. I didn't find anything neither on the official documentation nor on the web.</p>
","java, machine-learning, nlp, stanford-nlp, sentiment-analysis","<p>You want to use the <code>Evaluate</code> class.</p>

<pre><code>java -Xmx5g edu.stanford.nlp.sentiment.Evaluate -model &lt;model&gt; -treebank &lt;treebank&gt;
</code></pre>
",2,-2,45,2019-07-29 05:19:46,https://stackoverflow.com/questions/57247567/how-to-test-stanford-sentiment-model
How to seperate sentences in a dataframe based on last occurence of small letter followed by a capital one,"<p>I have a dataframe containing sentences. The first sentence (the title) is followed by the text. They were merged without a space. </p>

<p>I would like to slit the text into two parts (sentence 1 and sentence 2) based on the last occurence of a capital letter following a lowercase letter without a space in between (out of curiosity I would also be interested in a solution based on the first appearance). </p>

<p>The solution is supposed to be stored in the original dataframe. </p>

<p>I tried </p>

<pre><code>re.findall('(?&lt;!\s)[A-ZÄÖÜ](?:[a-zäöüß\s]|(?&lt;=\s)[A-ZÄÖÜ])*')
</code></pre>

<p>but could not work it out. </p>

<pre class=""lang-py prettyprint-override""><code>import pandas
from pandas import DataFrame

Sentences = {'Sentence': ['RnB music all nightI love going out','Example sentence with no meaningThe space is missing.','Third exampleAlso numbers 1.23 and signs -. should appear in column 2.', 'BestMusic tonightAt 12:00.']}

df = DataFrame(Sentences,columns= ['Sentence'])

print(df)
</code></pre>

<p>As the split is supposed to be carried out at the last occurrence. The words <code>RnB</code> and <code>BestMusic</code> in the example given are not supposed to trigger the split. </p>

<pre class=""lang-py prettyprint-override""><code>df.Sentence1 = ['RnB music all night','Example sentence with no meaning','Third example', 'BestMusic tonight']

df.Sentence2 = ['I love going out','The space is missing.', 'Also numbers 1.23 and signs -. should appear in column 2.' ,'At 12:00.']
</code></pre>
","python, pandas, dataframe, text, sentiment-analysis","<p>Here is one way </p>

<pre><code>Yourdf=df.Sentence.str.split(r'(.*[a-z])(?=[A-Z])',n=-1,expand=True)[[1,2]]
Yourdf
Out[610]: 
                                  1                                                  2
0               RnB music all night                                   I love going out
1  Example sentence with no meaning                              The space is missing.
2                     Third example  Also numbers 1.23 and signs -. should appear i...
3                 BestMusic tonight                                          At 12:00.
</code></pre>
",2,2,54,2019-07-31 14:37:08,https://stackoverflow.com/questions/57293300/how-to-seperate-sentences-in-a-dataframe-based-on-last-occurence-of-small-letter
Is there a reason why the following code does not execute (print the tweets) after taking input?,"<p>So basically I want to print a set number of tweets related to a topic that user enters but when I run the following code after giving in the input nothing happens, I see no output after that. I would be really grateful if you could tell me why :-)</p>

<p>I tried regenerating the access token keys and then again copy pasting it but the problem still persists</p>

<pre><code>import tweepy
consumerKey = ""Sgdz0quGjDDTtGbFAxWQ02E5M""
consumerSecret = ""alphanumeric""
accessToken = ""980878168180609024-nggEvf3WSLb1IcmmHfoCMhDNvZjbMid""
accessTokenSecret = ""alpha numeric""

auth = tweepy.OAuthHandler(consumer_key=consumerKey, 
consumer_secret=consumerSecret)
auth.set_access_token(accessToken, accessTokenSecret)
api = tweepy.API(auth)

searchTerm = input(""Enter keyword/hashtag to search about : "")
number = int(input(""How many tweets do you wanna print :  ""))
tweets = tweepy.Cursor(api.search, q=searchTerm, lang= ""English"").items(number)

for tweet in tweets:
    print(tweet.text)
</code></pre>

<p>this is what my console is showing after execution</p>

<pre><code>runfile('C:/users/acer/.spyder-py3/temp.py', wdir='C:/users/acer/.spyder-py3')

Enter keyword/hastag to search about : bts

How many tweets do you wanna print :  5

In [14]:
</code></pre>

<p>(It does not print the tweets)</p>
","python-3.x, twitter-oauth, tweepy, sentiment-analysis, textblob","<p>Your language needs to be ""en"". This works:</p>

<pre><code>searchTerm = input(""Enter keyword/hashtag to search about : "")
number = int(input(""How many tweets do you wanna print :  ""))

print (""Tweets with "", searchTerm, "": "")

for result in tweepy.Cursor(api.search, q=searchTerm, lang=""en"").items(number):
  print( result.text)
</code></pre>

<p>Results of a test run:</p>

<hr>

<p>Python 3.7.4 (default, Jul  9 2019, 00:06:43)
[GCC 6.3.0 20170516] on linux</p>

<p>Enter keyword/hashtag to search about : kendrick</p>

<p>How many tweets do you wanna print :  2</p>

<p>Tweets with  kendrick :</p>

<p>RT @flwrrb0y: them: you can’t even dance to kendrick lamar</p>

<p>me: 
RT @Blaqboimagic: Calling Kendrick overrated is unacceptable</p>
",0,0,104,2019-08-17 14:05:21,https://stackoverflow.com/questions/57537013/is-there-a-reason-why-the-following-code-does-not-execute-print-the-tweets-aft
"This python code with regex successfully remove URL but if URL found in the beginning of tweets, all of the sentence will be remove as well","<p>I need to remove any URL in the tweets review. How to only remove the URL if it is found in the beginning of tweet?</p>

<p>I've try some code and this python code with regex successfully remove URL but if URL found in the beginning of tweets, all of the sentence will be remove as well.</p>

<pre><code>re.sub(r'https?:\/\/.*[\r\n]*\S+', '', verbatim, flags = re.MULTILINE)
</code></pre>

<p>If URL found in the beginning of tweets, all of the sentence will be remove as well.</p>
","python, regex, twitter, sentiment-analysis","<p>The pattern <code>https?:\/\/.*[\r\n]*\S+</code> matches <code>http(optional s)://</code></p>

<p>Then the <code>.*</code> part matches until the end of the string, then this part <code>[\r\n]*</code> matches 0+ newlines and <code>\S+</code> will match 1+ non whitespace chars.</p>

<p>So the url is matched, followed by the rest of the string, a newline and 1+ non whitespace chars at the next line as well.</p>

<p>You could shorten the pattern to:</p>

<pre><code>\bhttps?://\S+
</code></pre>

<p><a href=""https://regex101.com/r/ILCFp8/1"" rel=""nofollow noreferrer"">Regex demo</a></p>
",2,2,185,2019-08-26 15:56:38,https://stackoverflow.com/questions/57661241/this-python-code-with-regex-successfully-remove-url-but-if-url-found-in-the-begi
better and easy way to find who spoke top 10 anger words from conversation text,"<p>I have a dataframe that contains variable 'AgentID', 'Type', 'Date', and 'Text' and a subset is as follows:</p>

<pre><code>structure(list(AgentID = c(""AA0101"", ""AA0101"", ""AA0101"", ""AA0101"", 
                            ""AA0101""), Type = c(""PS"", ""PS"", ""PS"", ""PS"", ""PS""), Date = c(""4/1/2019"", ""4/1/2019"", ""4/1/2019"", ""4/1/2019"", ""4/1/2019""),  Text = c(""I am on social security XXXX and I understand it can not be garnished by Paypal credit because it's federally protected.I owe paypal {$3600.00} I would like them to cancel this please."", 
                        ""My XXXX account is being reported late 6 times for XXXX per each loan I was under the impression that I was paying one loan but it's split into three so one payment = 3 or one missed payment would be three missed on my credit,. \n\nMy account is being reported wrong by all credit bureaus because I was in forbearance at the time that these late payments have been reported Section 623 ( a ) ( 2 ) States : If at any time a person who regularly and in the ordinary course of business furnishes information to one or more CRAs determines that the information provided is not complete or accurate, the furnisher must promptly provide complete and accurate information to the CRA. In addition, the furnisher must notify all CRAs that received the information of any corrections, and must thereafter report only the complete and accurate information. \n\nIn this case, I was in forbearance during that tie and document attached proves this. By law, credit need to be reported as of this time with all information and documentation"",
                        ""A few weeks ago I started to care for my credit and trying to build it up since I have never used my credit in the past, while checking my I discover some derogatory remarks in my XXXX credit report stating the amount owed of {$1900.00} to XXXX from XX/XX/2015 and another one owed to XXXX for {$1700.00} I would like to address this immediately and either pay off this debt or get this negative remark remove from my report."", 
                        ""I disputed this XXXX  account with all three credit bureaus, the reported that it was closed in XXXX, now its reflecting closed XXXX once I paid the {$120.00} which I dont believe I owed this amount since it was an fee for a company trying to take money out of my account without my permission, I was charged the fee and my account was closed. I have notified all 3 bureaus to have this removed but they keep saying its correct. One bureau is showing XXXX closed and the other on shows XXXX according to XXXX XXXX, XXXX shows a XXXX, this account has been on my report for seven years"", 
                        ""On XX/XX/XXXX I went on XXXX XXXX  and noticed my score had gone down, went to check out why and seen something from XXXX XXXX  and enhanced recovery company ... I also seen that it had come from XXXX and XXXX dated XX/XX/XXXX, XX/XX/XXXX, and XX/XX/XXXX ... I didnt have neither one before, I called and it the rep said it had come from an address Im XXXX XXXX, Florida I have never lived in Florida ever ... .I have also never had XXXX XXXX  nor XXXX XXXX  ... I need this taken off because it if affecting my credit score ... This is obviously identify theft and fraud..I have never received bills from here which proves that is was not done by me, I havent received any notifications ... if it was not for me checking my score I wouldnt have known nothing of this"" )), row.names = c(NA, 5L), class = ""data.frame"")
</code></pre>

<p>First, I found out the top 10 anger words using the following:</p>

<pre><code>library(tm)
library(tidytext)
library(tidyverse)
library(sentimentr)
library(wordcloud)
library(ggplot2)

CS &lt;- function(txt){
  MC &lt;- Corpus(VectorSource(txt))
  SW &lt;- stopwords('english')
  MC &lt;- tm_map(MC, tolower)
  MC&lt;- tm_map(MC,removePunctuation)
  MC &lt;- tm_map(MC, removeNumbers)
  MC &lt;- tm_map(MC, removeWords, SW)
  MC &lt;- tm_map(MC, stripWhitespace)
  myTDM &lt;- as.matrix(TermDocumentMatrix(MC))
  v &lt;- sort(rowSums(myTDM), decreasing=TRUE)
  FM &lt;- data.frame(word = names(v), freq=v)
  row.names(FM) &lt;- NULL
  FM &lt;- FM %&gt;%
    mutate(word = tolower(word)) %&gt;%
    filter(str_count(word, ""x"") &lt;= 1)
  return(FM)
}

DF &lt;- CS(df$Text)

# using nrc
nrc &lt;- get_sentiments(""nrc"")
# create final dataset
DF_nrc = DF %&gt;% inner_join(nrc)
</code></pre>

<p>And the I created a vector of top 10 anger words as follows:</p>

<pre><code>TAW &lt;- DF_nrc %&gt;%
  filter(sentiment==""anger"") %&gt;%
  group_by(word) %&gt;%
  summarize(freq = mean(freq)) %&gt;%
  arrange(desc(freq)) %&gt;% 
  top_n(10) %&gt;%
  select(word)
</code></pre>

<p>Next what I wanted to do is to find which were the 'Agent'(s) who spoke these words frequently and rank them. But I am confused how we could do that? Should I search the words one by one and group all by agents or is there some other better way. What I am looking at as a result, something like as follows:</p>

<pre><code>AgentID  Words_Spoken             Rank
A0001  theft, dispute, money    1
A0001  theft, fraud,            2
.......
</code></pre>
","r, sentiment-analysis, grepl, tidytext, sentimentr","<p>Not the most elegant solution, but here's how you could count the words based on the line number:</p>

<pre><code>library(stringr)

# write a new data.frame retaining the AgentID and Date from the original table
new.data &lt;- data.frame(Agent = df$AgentID, Date = df$Date) 

# using a for-loop to go through every row of text in the df provided.  

for(i in seq(nrow(new.data))){ # i represent row number of the original df

  # write a temporary object (e101) that:
        ## do a boolean check to see if the text from row i df[i, ""Text""] the TAW$Word with stringr::str_detect function
        ## loop the str_detect with sapply so that the str_detect do a boolean check on each TAW$Word
        ## return the TAW$Word with TAW$Word[...]

  e101 &lt;- TAW$word[sapply(TAW$word, function(x) str_detect(df[i, ""Text""], x))] 

  # write the number of returned words in e101 as a corresponding value in new data.frame
  new.data[i, ""number_of_TAW""] &lt;- length(e101)

  # concatenate the returned words in e101 as a corresponding value in new data.frame
  new.data[i, ""Words_Spoken""] &lt;- ifelse(length(e101)==0, """", paste(e101, collapse="",""))
}

new.data

#    Agent     Date number_of_TAW      Words_Spoken
# 1 AA0101 4/1/2019             0                  
# 2 AA0101 4/1/2019             0                  
# 3 AA0101 4/1/2019             2 derogatory,remove
# 4 AA0101 4/1/2019             3  fee,money,remove
# 5 AA0101 4/1/2019             1             theft
</code></pre>
",1,0,137,2019-08-27 10:14:40,https://stackoverflow.com/questions/57672459/better-and-easy-way-to-find-who-spoke-top-10-anger-words-from-conversation-text
Need helping performing sentiment analysis on customer reviews and for a string of text,"<p>This is a 2 part code question.</p>

<p>1.) Need to perform sentiment analysis on a csv file for customer reviews.</p>

<p>2.) Need to perform sentiment analysis on a harry potter book review saved as a .txt</p>

<p>1.) The name of this Dataframe is ""reviews"" and what I want to do is display the sentiment score for each of these 5 reviews under the ""sent"" column.  Thank you so much!!! If you can provide the code with the ""sent"" column filled  with its sentiment analysis score for each row that would be awesome!!</p>

<p>reviews.head()</p>

<pre><code> ID  Customer Name    Review                                       Sent  

 1   Jack             Beautiful cover up. My only 
                      feedback is that it is a tad larger 
                      than expected, but since it's a cover 
                      up, it doesn't need to be fitted. The 
                      waist tassels also allow you to adjust 
                      to fit your waist which is nice. 
                      Otherwise, its exactly as expected!

 2   Rachel           This tunic is very cute in person. It's 
                      more sheer than I'd like, but I imagine 
                      I'll wear it a ton on vacation.

 3   Ryan             Just got this sweet little dress in 
                      blue. It's a great little dress for a 
                      pool cover up. I can envision myself 
                      wearing it on our winter getaway for 
                      breakfast or on a walk. I'm not sure how 
                      see through it is. I think I could get 
                      away wearing it as a dress. The length 
                      is great, not too short. The quality is 
                      great. I got a size S. Fits true to size. 
                      I am usually a size 2, 34b, 129lb, slim 
                      build. Very happy with this.

 4   Jennifer         Love this hat! Kept the sun off my face 
                      and neck/chest in the intense tropical 
                      sun! Choose white - so I stayed cool.

 5   Alex             What I like about bikinis is that they 
                      always fit you perfectly. You won't 
                      realize how gorgeous they are and how 
                      attractive they make your body look 
                      until you put one on. As for the bra-part 
                      it gives good support and sits well. I 
                      also like the fabric: it stretches well 
                      without losing its shape, the color 
                      doesn't fade. This bikini is no exception. 
                      is far better at making bikinis than 
                      anybody else, I would say! 
</code></pre>

<p>For this string I just to know what the overall sentiment score is... thanks!!</p>

<p>2.) </p>

<p>""Parents need to know that Harry Potter and the Sorcerer's Stone is a thrill-a-minute story, the first in J.K. Rowling's Harry Potter series. It respects kids' intelligence and motivates them to tackle its greater length and complexity, play imaginative games, and try to solve its logic puzzles. It's the lightest in the series, but it still has some scary stuff for sensitive readers: a three-headed dog, an attacking troll, a violent life-size chess board, a hooded figure over a dead and bleeding unicorn, as well as a discussion of how Harry's parents died years ago.""</p>
","python, nlp, sentiment-analysis","<p>Please keep in mind for any further questions that stackoverflow is not a code writting service and you should always provide some code you have tried by yourself. </p>

<p>TextBlob is a python package which has a <a href=""https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis"" rel=""nofollow noreferrer"">sentiment analysis</a> and returns you a polarity and subjectivity of a given text. To apply this sentiment analysis function to your dataframe you need to use the corrospondent function <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer"">apply</a>.</p>

<p>Have a look at the example below:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from textblob import TextBlob

df = pd.DataFrame([['1',   'Jack',    ""Beautiful cover up. My only feedback is that it is a tad larger than expected, but since it's a cover up, it doesn't need to be fitted. The waist tassels also allow you to adjust to fit your waist which is nice. Otherwise, its exactly as expected!""],
['2',   'Rachel',  ""This tunic is very cute in person. It's more sheer than I'd like, but I imagine I'll wear it a ton on vacation.""],
['3',   'Ryan',    ""Just got this sweet little dress in blue. It's a great little dress for a pool cover up. I can envision myself wearing it on our winter getaway for breakfast or on a walk. I'm not sure how see through it is. I think I could get away wearing it as a dress. The length is great, not too short. The quality is great. I got a size S. Fits true to size. I am usually a size 2, 34b, 129lb, slim build. Very happy with this.""],
['4',   'Jennifer',    ""Love this hat! Kept the sun off my face and neck/chest in the intense tropical sun! Choose white - so I stayed cool.""],
['5',   'Alex',   ""What I like about bikinis is that they always fit you perfectly. You won't realize how gorgeous they are and how attractive they make your body look until you put one on. As for the bra-part it gives good support and sits well. I also like the fabric: it stretches well without losing its shape, the color doesn't fade. This bikini is no exception. is far better at making bikinis than anybody else, I would say!""]], columns=['ID',  'CustomerName',   'Review'] )


df['sent'] = df['Review'].apply(lambda x:  TextBlob(x).sentiment)
#you can get the polarity and subjectivty values in separate columns by splitting
df['polarity'] =  df['sent'].str[0]
df['subjectivity'] =  df['sent'].str[1]

</code></pre>

<p>Regarding your second question, just execute the following:</p>

<pre class=""lang-py prettyprint-override""><code>TextBlob(""Parents need to know that Harry Potter and the Sorcerer's Stone is a thrill-a-minute story, the first in J.K. Rowling's Harry Potter series. It respects kids' intelligence and motivates them to tackle its greater length and complexity, play imaginative games, and try to solve its logic puzzles. It's the lightest in the series, but it still has some scary stuff for sensitive readers: a three-headed dog, an attacking troll, a violent life-size chess board, a hooded figure over a dead and bleeding unicorn, as well as a discussion of how Harry's parents died years ago."").sentiment
</code></pre>
",0,-2,434,2019-09-07 17:28:00,https://stackoverflow.com/questions/57835978/need-helping-performing-sentiment-analysis-on-customer-reviews-and-for-a-string
How to count frequency of specific positive/negative words from a list in a .csv file with text and date values? in R,"<p>I'm trying to get the sentiment from a document containing messages, the specific user and date. I have cleaned both documents so that the words contained in them are of a standard format and then I tried to count them but I seem to be able on to count them separately (after defining the word) but not with the use of a list of words. </p>

<p>the file. raw is in the format: text,user_id, date and the positive/ negative lists are in the format: id,word_cz, polarity</p>

<pre class=""lang-r prettyprint-override""><code>file.raw &lt;- read.csv(""/Users/tomas/Desktop/Repromeda - Repromeda 3.csv"", stringsAsFactors = FALSE,)
positive &lt;- read.csv(""/Users/tomas/Desktop/positive.txt"", stringsAsFactors = FALSE,)
negative &lt;- read.csv(""/Users/tomas/Desktop/negative.txt"", stringsAsFactors = FALSE,)
</code></pre>

<p>I can count the specific words like ""Okay"" with the function </p>

<pre class=""lang-r prettyprint-override""><code>getCount &lt;- function(data,keywords)
{
  wordcount &lt;- str_count(file.raw&amp;text, keywords)
  return(data.frame(data,wordcount))
}
file.raw$count &lt;-  getCount(file.raw&amp;text,""okay"")

</code></pre>

<p>)but I can't seem to find a way to automate this process with the list of words </p>

<p>The ideal results would add a column for each positive and negative counts to each row</p>

<p>Thanks for the help</p>
","r, list, csv, count, sentiment-analysis","<p>how about this?</p>

<pre><code>library(stringr)
data &lt;- ""yes i had a great time yesterday having fun but your lame actions were disturbing, ok?""
positive &lt;- c(""yes"" , ""ok"", ""fun"", ""great"")
negative &lt;- c(""lame"" , ""disturbing"", ""no"") 

sapply(positive, function(x) str_count(data,x))
sapply(negative, function(x) str_count(data,x))
</code></pre>
",0,0,280,2019-09-12 12:27:46,https://stackoverflow.com/questions/57906731/how-to-count-frequency-of-specific-positive-negative-words-from-a-list-in-a-csv
Why can&#39;t i get the text value using request.form in flask to perform sentimental analysis on the text?,"<p>I am quite inexperienced in both HTML, JS and flask but I am working on a chatbot that able to detect sentimental analysis of the sender.</p>

<p>My HTML code:</p>

<pre><code>      &lt;div class=""bottom_wrapper clearfix""&gt;
        &lt;div class=""message_input_wrapper""&gt;
          &lt;form action = ""{{ url_for('reply') }}"" method = ""POST""&gt;
          &lt;input
            class=""message_input""
            id=""text_message""
            name = ""sentimental_name""
            placeholder=""Tell me how you feel today...""
            onkeydown=""if (event.keyCode == 13)document.getElementById('send').click()""&gt;  
        &lt;/div&gt;


        &lt;!--div class = ""send_message1"" id = 'audio' onclick = ""start_dictation()""&gt;


        &lt;span style=""font-size: 32px; color:black;""&gt;
          &lt;i class=""fas fa-microphone""&gt;&lt;/i&gt;
        &lt;/span&gt;
      &lt;/div--&gt;
        &lt;div class=""send_message"" id=""send"" onclick=""get_message()""&gt;
          &lt;!--&lt;div class=""icon""&gt;&lt;/div&gt;--&gt;

          &lt;div class=""text""&gt;Send&lt;/div&gt;

        &lt;/div&gt;
      &lt;/form&gt;
      &lt;/div&gt;
</code></pre>

<p>This is my python-flask code:</p>

<pre><code>@app.route('/senti', methods = ['POST'])
def reply():
    if request.method == 'POST':
        message = request.form['text_message']
        a = TextBlob(message).sentiment.polarity
        b = TextBlob(message).sentiment.subjectivity
</code></pre>

<p>My js that links to the onlick =</p>

<pre><code>function get_message(){
var message = document.getElementById(""text_message"").value;
var json_data = {""msg"":message}
var sender = JSON.stringify(json_data)
console.log(sender)
console.log(message);
insert_chat('me',message);
interact(sender);
 }
</code></pre>

<p>Console log:</p>

<pre><code>POST http://127.0.0.1:5000/senti 500 (INTERNAL SERVER ERROR)
send    @   jquery-3.4.1.js:9837
ajax    @   jquery-3.4.1.js:9434
interact    @   chat.js:34
get_message @   chat.js:55
onclick @   chat:58
</code></pre>

<p>It seems really simple but it is like I miss something. Thank you so much!</p>
","python, html, flask, sentiment-analysis","<p>You would have to use <code>""sentimental_name""</code> in </p>

<pre><code>request.form[""sentimental_name""] 
</code></pre>

<p>because you have  </p>

<p>But it uses JavaScript function <code>get_message()</code> to get data when you click <code>ENTER</code> </p>

<pre><code>&lt;input ... onkeydown=""if (event.keyCode == 13)document.getElementById('send').click()""&gt; 

&lt;div class=""send_message"" id=""send"" onclick=""get_message()""&gt;
</code></pre>

<p>and converts to JSON with field <code>""msg""</code> so it sends it as <code>data</code> or <code>json</code>, not <code>form</code>. </p>

<pre><code>function get_message(){
   var message = document.getElementById(""text_message"").value;
   var json_data = {""msg"":message}
   var sender = JSON.stringify(json_data)
   console.log(sender)
   console.log(message);
   insert_chat('me',message);
   interact(sender);
</code></pre>

<p>In flask <code>reply()</code> you can check this using:</p>

<pre><code>print(request.args)
print(request.data)
print(request.form)
print(request.json)
</code></pre>

<p>JavaScript may expect that <code>reply()</code> returns also <code>JSON</code> - ie. </p>

<pre><code>return jsonify(list_or_dictionary). 
</code></pre>

<p>In JavaScript I see <code>interact(sender);</code> so you would have to find this function and see what it sends and what result it may expect. </p>

<hr>

<p>BTW: you can also use <code>requests.data.get(""msg"")</code> and <code>request.form.get(""msg"")</code> instead of <code>[""msg""]</code> becauses <code>.get()</code> returns <code>None</code> when it can't find <code>""msg""</code> and you can use <code>if not message:</code> to catch this problem. And <code>[""msg""]</code> raises error when there is no <code>""msg""</code> and you would have to use <code>try:/except:</code> to catch it.</p>
",1,0,59,2019-09-15 05:50:00,https://stackoverflow.com/questions/57941508/why-cant-i-get-the-text-value-using-request-form-in-flask-to-perform-sentimenta
how to build and label a non english dataset for sentiment analysis,"<p>lately I've started a new project about sentiment analysis and I should build a dataset in Persian language. while building a dataset is important for accuracy of whole process ,I want to do it as good as it's possible in shortest time.
What is the most optimized way to build and label a sentiment analysis dataset?</p>
","machine-learning, deep-learning, sentiment-analysis","<p>You can use available dataset as a reference of yours. There are many sources to get sentiment analysis dataset:</p>

<p><a href=""http://storage.googleapis.com/books/ngrams/books/datasetsv2.html"" rel=""nofollow noreferrer"">google</a> </p>

<p><a href=""http://www.sananalytics.com/lab/twitter-sentiment/"" rel=""nofollow noreferrer"">sananalytics</a> </p>

<p><a href=""http://inclass.kaggle.com/c/si650winter11/data"" rel=""nofollow noreferrer"">kaggle</a> </p>

<p><a href=""http://nlp.stanford.edu/sentiment/treebank.html"" rel=""nofollow noreferrer"">stanford</a> </p>

<p>Here is a list of datasets that give the sentiments for individual words.</p>

<p><a href=""http://positivewordsresearch.com/sentiment-analysis-resources/"" rel=""nofollow noreferrer"">positivewordsresearch</a> </p>

<p>I suggest to you that work on mentioned datasets in order to increase your knowledge about dataset and their labels.</p>

<p>Generally sentiment datasets uses limited labels such as 
""positive/negative"" or 
""happy"", ""sad"", ""angry"", and ""neutral"" or
""anger"", ""sadness"", ""surprise"", ""fear"", ""disgust"", and ""joy""</p>

<p>Hope to be useful for you.</p>
",3,4,273,2019-09-23 08:14:59,https://stackoverflow.com/questions/58058224/how-to-build-and-label-a-non-english-dataset-for-sentiment-analysis
How to perform Binary Classification with many features using ML.Net,"<p>So I’ve been messing with ML.net and practicing using their examples and was wondering how I would train a model with the following input:</p>

<p>int, string, int, string, int, string, int, string, bool (Label)</p>

<p>and predict the binary label provided the other 8 features.</p>

<p>The first 3 strings are selections that could be converted to a lookup, and the fourth string is random text that needs more of a sentiment approach.</p>

<p>Any ideas on where to start / how to approach this? I’ve implemented using multiclass classification but I don’t think it makes sense since my output is binary.</p>
","c#, sentiment-analysis, multiclass-classification, ml.net","<p>Look at <a href=""https://github.com/dotnet/machinelearning-samples/tree/master/samples/csharp/getting-started/BinaryClassification_HeartDiseaseDetection"" rel=""nofollow noreferrer"">this sample</a> from the ML.NET sample Github.</p>

<p>You can use any of these <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.ml.binaryclassificationcatalog.binaryclassificationtrainers?view=ml-dotnet"" rel=""nofollow noreferrer"">Binary Classifiers</a> for the lookups checkout the <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.ml.conversionsextensionscatalog.mapvaluetokey?view=ml-dotnet"" rel=""nofollow noreferrer"">MapValueToKey Conversion</a>, and for the additional fourth string you'll need to work with the <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.ml.transforms.text?view=ml-dotnet"" rel=""nofollow noreferrer"">Text Conversions</a>, but there's nothing pretrained.</p>

<p>I hope this helps!</p>
",3,2,1950,2019-10-09 12:56:35,https://stackoverflow.com/questions/58304669/how-to-perform-binary-classification-with-many-features-using-ml-net
How to distinguish the direction of important features from xgboost or random forest?,"<p>I'm now working on binary text classification problem (like sentiment analysis), and it's trivial to pull out top important features of xgboost or random forest just by <code>feature_importances_</code></p>

<p>Suppose we have two labelling 1 and 0 for this classification problem. Then there's any way to print out the direction of the features (positive or negative)? Say, word feature A has an enrichment or high tfidf with labelling 1.</p>

<p>Certainly I could pull out the tfidf column of this specific word feature, and correlate with the labelling with pearson coefficient, and the +/- of coefficient would indicate the direction, right? Any other more elegant way for this or xgboost and random forest has built-in such functions. (I didn't find)</p>

<p>Thanks</p>
","nlp, random-forest, xgboost, sentiment-analysis, text-classification","<p>It isn't exactly what you're asking for, but I usually use <a href=""https://github.com/marcotcr/lime"" rel=""nofollow noreferrer"">Lime</a> to do this. I like how it works even if I switch models. </p>
",1,1,3488,2019-10-10 02:41:49,https://stackoverflow.com/questions/58314707/how-to-distinguish-the-direction-of-important-features-from-xgboost-or-random-fo
"Error when checking input: expected dense_1_input to have shape (1500,) but got array with shape (1,)","<p><strong>i am getting an error only in the fitting part.
is there an issue with x_train and y-train?</strong>    </p>

<pre><code> import keras
    from keras.models import Sequential
    from keras.layers import Dense
    model=Sequential()
    model.add(Dense(input_dim=1500,init=""random_uniform"",activation='sigmoid',output_dim=1000))#input layer
    model.add(Dense(output_dim=100,init=""random_uniform"",activation='sigmoid'))#hidden layer
    model.add(Dense(output_dim=1,init='random_uniform',activation='sigmoid'))#output layer
    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])#adam=batch gradent descent
    model.fit(x_train,y_train,epochs=50,batch_size=10)#epochs no of iteration
    y_pred =  model.predict(x_test)
</code></pre>
","machine-learning, deep-learning, sentiment-analysis","<p>As the error message suggests, your <code>x_train</code> should be a vector of shape <code>(1500,)</code> 
 as you have given <code>input_dim=1500</code> in your first layer but looks like you are passing the <code>x_train</code> vector with shape <code>(1,)</code>. <strong>You have to correct the shape of x_train you are passing to the model</strong>. </p>

<p>Show the code where you are reading the data and storing it as <code>x_train</code>. That will help to see where the error is.</p>
",0,-2,35,2019-10-11 15:51:17,https://stackoverflow.com/questions/58344723/error-when-checking-input-expected-dense-1-input-to-have-shape-1500-but-got
Why am I getting String where should get a dict when using pycorenlp.StanfordCoreNLP.annotate?,"<p>I'm running this <a href=""https://stackabuse.com/python-for-nlp-getting-started-with-the-stanfordcorenlp-library/"" rel=""nofollow noreferrer"">example</a> using pycorenlp Stanford Core NLP python wrapper, but the annotate function returns a string instead of a dict, so, when I iterate over it to get each sentence sentiment value I get the following error: ""string indices must be integers"".</p>

<p>What could I do to get over it? Anyone could help me? Thanks in advance.
The code is below:</p>

<pre><code>from pycorenlp import StanfordCoreNLP
nlp_wrapper = StanfordCoreNLP('http://localhost:9000')
doc = ""I like this chocolate. This chocolate is not good. The chocolate is delicious. Its a very 
    tasty chocolate. This is so bad""
annot_doc = nlp_wrapper.annotate(doc,
                                 properties={
                                            'annotators': 'sentiment',
                                            'outputFormat': 'json',
                                            'timeout': 100000,
                                 })
for sentence in annot_doc[""sentences""]:
      print("" "".join([word[""word""] for word in sentence[""tokens""]]) + "" =&gt; ""\
            + str(sentence[""sentimentValue""]) + "" = ""+ sentence[""sentiment""])
</code></pre>
","stanford-nlp, sentiment-analysis, pycorenlp","<p>You should just use the official stanfordnlp package! (note: the name is going to be changed to stanza at some point)</p>

<p>Here are all the details, and you can get various output formats from the server including JSON.</p>

<p><a href=""https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html</a></p>

<pre><code>from stanfordnlp.server import CoreNLPClient
with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'], timeout=30000, memory='16G') as client:
    # submit the request to the server
    ann = client.annotate(text)
</code></pre>
",0,1,248,2019-10-23 14:59:26,https://stackoverflow.com/questions/58525763/why-am-i-getting-string-where-should-get-a-dict-when-using-pycorenlp-stanfordcor
how can i automate my sentiment analysis in python,"<p>I am done predicting my sentiment analysis using real-time tweets from twitter but I need to automate it so I don't need to run it again at every interval.</p>

<p>How can i go about this</p>

<p>Kindly assist</p>
","python, automation, sentiment-analysis","<p>You can use <a href=""https://pypi.org/project/schedule/"" rel=""nofollow noreferrer"">schedule</a> and <a href=""https://tweepy.readthedocs.io/en/latest/getting_started.html"" rel=""nofollow noreferrer"">tweepy</a> to schedule your job; or use <a href=""https://tweepy.readthedocs.io/en/latest/streaming_how_to.html"" rel=""nofollow noreferrer"">streaming with tweepy</a> standalone without necessity of scheduling.</p>

<p><strong>With using <code>schedule</code>, based on the example of <code>schedule</code>'s pypi homepage:</strong></p>

<pre class=""lang-py prettyprint-override""><code>import schedule

def job():
    print(""I'm working..."")
    # do you work here

schedule.every(10).seconds.do(job)

while True:
    schedule.run_pending()
    time.sleep(1)
</code></pre>

<p>To mention, you can call functions or methods in <code>job()</code>. You don't have to write all your code there.</p>
",0,0,83,2019-10-30 08:08:16,https://stackoverflow.com/questions/58620938/how-can-i-automate-my-sentiment-analysis-in-python
Insert a string based on differing occurence counts of strings in python,"<p>I hope someone can help me with the following. </p>

<p>Assuming I have the following text (more precise - a code chunk from a latex table). </p>

<ul>
<li>text = ""Some <strong>&amp;</strong> random <strong>&amp;</strong> content <strong>\</strong> Some <strong>&amp;</strong> further <strong>&amp;</strong> content <strong>\</strong> …. ""</li>
</ul>

<p>Next, I would like to place the string inside of the text based on two input factors, inside of the text with one specifying the row based on \ and the other the column based on &amp;.</p>

<p>So in case of the the pair <strong>(1,1)</strong> the string should be placed before the first occurrence of  \ and the first occurrence of &amp;. 
(1, 1, “HERE”, text) as input should return:</p>

<ul>
<li><strong>HERESome</strong> &amp; not coloured &amp; content \ Some &amp; further &amp; content \ …. </li>
</ul>

<p>And for 
(2, 2, “HERE”, text)  should return  </p>

<ul>
<li>Some &amp; not coloured &amp; content \ Some &amp; <strong>HEREfurther</strong> &amp; content \ …. </li>
</ul>

<p>Ideally the function would also take multiple pairs with inputs, so with 1,1 and 2,2 as input: </p>

<ul>
<li><strong>HERESome</strong> &amp; not coloured &amp; content \ Some &amp; <strong>HEREfurther</strong> &amp; content \ …. </li>
</ul>

<p>should be the outcome. </p>

<p>My current approach does not treats &amp; and \ different. 
and HERE does not appear in front. </p>

<pre class=""lang-py prettyprint-override""><code>
text = ""Some &amp; random &amp; content \\ Some &amp; further &amp; content \\ …. ""

def replacenth(string, sub, wanted, n):
    pattern = re.compile(sub)
    where = [m for m in pattern.finditer(string)][n-1]
    before = string[:where.start()]
    after = string[where.start():]
    newString = before + wanted + after
    return newString

replacenth(text, ""[&amp;\\+]"", ""HERE"" , 2)

#output: 
#'Some &amp; random HERE&amp; content \\ Some &amp; further &amp; content \\ …. '

</code></pre>
","python, text, sentiment-analysis","<p>Maybe something like this. Using <a href=""https://docs.python.org/3/library/stdtypes.html#str.split"" rel=""nofollow noreferrer"">str.split</a> and <a href=""https://docs.python.org/3/library/stdtypes.html#str.join"" rel=""nofollow noreferrer"">str.join</a>. No checking for bad input though.</p>

<pre><code>def replace(string, sub, row_col_pairs):
    rows = string.split(' \\ ')
    for row, col in row_col_pairs:
        cells = rows[row-1].split(' &amp; ')
        cells[col-1] = sub+cells[col-1]
        rows[row-1] = ' &amp; '.join(cells)
    return ' \\ '.join(rows)

replace(text, ""HERE"", [(1, 3), (2, 2)])

# output
# Some &amp; random &amp; HEREcontent \ Some &amp; HEREfurther &amp; content \ ….
</code></pre>
",1,0,34,2019-10-31 20:30:25,https://stackoverflow.com/questions/58650780/insert-a-string-based-on-differing-occurence-counts-of-strings-in-python
"Python: Subset strings solely of the first bracket that opens up, but contains multiple","<p>Assuming the text </p>

<p>text = """"""{ |p{3cm}|p{3cm}|p{3cm}|  } \hline \multi{3}{|c|}{City List} \ \hline Name ... """"""</p>

<p>I would solely like to subset the content of the first curly brackets. 
So the desired output would be: </p>

<p>desired_output = ""p{3cm}|p{3cm}|p{3cm}"" </p>

<p>Currently I receive the content of all curly brakets of the lines </p>

<pre class=""lang-py prettyprint-override""><code>

text = """"""{ |p{3cm}|p{3cm}|p{3cm}|  } \\hline \\multi{3}{|c|}{City List} \\ \\hline Name ... """"""
import re
false_output = re.findall(r'\{(.*?)\}',text)
false_output

#[' |p{3cm', '3cm', '3cm', '3', '|c|', 'City List']


#also no success with: 
re.findall(r'({\w+\})',a) 

</code></pre>
","python, text, sentiment-analysis","<p>I don't think this can be done with a regular expression. Last time I had to tackle something like this (parsing wikitext), I ended up using a stack, increasing every time I have the opening character, decreasing when I meet a closing one, exiting when I found the last one.</p>

<p>Please note this wouldn't work for repeated first level brackets.</p>

<p>The code was more optimized than this, but the basic idea is as follows:</p>

<pre><code>def keep_between(text, start, end):
    counter = 0
    result = []
    beginning = text.find(start)
    if beginning != -1:
        remaining_text = text[beginning:]
        for c in remaining_text:
            if c == start:
                counter += 1
                continue
            if c == end:
                counter -= 1
                continue
            if not counter:
                break
            result.append(c)
    return ''.join(result)

print(keep_between(text, '{', '}'))
</code></pre>

<p>that gets me  <code>' |p3cm|p3cm|p3cm| '</code></p>
",1,0,34,2019-11-01 13:55:45,https://stackoverflow.com/questions/58660423/python-subset-strings-solely-of-the-first-bracket-that-opens-up-but-contains-m
"sentiment analysis call fails with cognitive service, returning &quot;HttpOperationError&quot;","<pre><code>text_analytics = TextAnalyticsClient(endpoint=endpoint, credentials=credentials)
documents = [
    {
        ""id"": ""1"",
        ""language"": ""en"",
        ""text"": ""I had the best day of my life.""
    }
]
response = text_analytics.sentiment(documents=documents)
for document in response.documents:
    print(""Document Id: "", document.id, "", Sentiment Score: "",
          ""{:.2f}"".format(document.score))
</code></pre>

<p>Hi, with the sample code from API manual <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/python-sdk#sentiment-analysis"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/python-sdk#sentiment-analysis</a>
 I got the following error while trying to call the sentiment classifier</p>

<hr>

<pre><code>HttpOperationError                        Traceback (most recent call last)
&lt;ipython-input-18-f0fb322c9e8c&gt; in &lt;module&gt;
      8     }
      9 ]
---&gt; 10 response = text_analytics.sentiment(documents=documents)
     11 for document in response.documents:
     12     print(""Document Id: "", document.id, "", Sentiment Score: "",

~/anaconda3/envs/lib/python3.6/site-packages/azure/cognitiveservices/language/textanalytics/text_analytics_client.py in sentiment(self, show_stats, documents, custom_headers, raw, **operation_config)
    361 
    362         if response.status_code not in [200, 500]:
--&gt; 363             raise HttpOperationError(self._deserialize, response)
    364 
    365         deserialized = None

HttpOperationError: Operation returned an invalid status code 'Resource Not Found'
</code></pre>
","sentiment-analysis, azure-cognitive-services","<p>It works for me on my side , pls follow or check the steps  below to get started with python sentiment analysis SDK : </p>

<ol>
<li><p>Create a text analysis service on Azure portal : 
<a href=""https://i.sstatic.net/oQpGr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oQpGr.png"" alt=""enter image description here""></a></p></li>
<li><p>Once created , note its endpoint and either one of two keys.
<a href=""https://i.sstatic.net/AJo23.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJo23.png"" alt=""enter image description here""></a></p></li>
<li><p>Try the code below :</p></li>
</ol>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient
from msrest.authentication import CognitiveServicesCredentials

subscriptionKey = ""&lt;your Azure servcice key &gt;""
endpoint = ""&lt;your Azure servcice endpoint&gt;""


credentials = CognitiveServicesCredentials(subscriptionKey)

text_analytics = TextAnalyticsClient(endpoint=endpoint, credentials=credentials)

documents = [
    {
        ""id"": ""1"",
        ""language"": ""en"",
        ""text"": ""I had the best day of my life.""
    }
]
response = text_analytics.sentiment(documents=documents)
for document in response.documents:
    print(""Document Id: "", document.id, "", Sentiment Score: "",
          ""{:.2f}"".format(document.score))</code></pre>
</div>
</div>
</p>

<p>Result : </p>

<p><a href=""https://i.sstatic.net/YBDum.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YBDum.png"" alt=""enter image description here""></a></p>

<p>Hope it helps .</p>
",1,0,76,2019-11-04 08:14:28,https://stackoverflow.com/questions/58689425/sentiment-analysis-call-fails-with-cognitive-service-returning-httpoperationer
Is it possible to train the sentiment classification model with the labeled data and then use it to predict sentiment on data that is not labeled?,"<p>I want to do sentiment analysis using machine learning (text classification) approach. For example nltk Naive Bayes Classifier.
But the issue is that a small amount of my data is labeled. (For example, 100 articles are labeled positive or negative) and 500 articles are not labeled.
I was thinking that I train the classifier with labeled data and then try to predict sentiments of unlabeled data. 
Is it possible? 
I am a beginner in machine learning and don't know much about it. </p>

<p>I am using python 3.7. </p>

<p>Thank you in advance. </p>
","nltk, python-3.7, sentiment-analysis, text-classification, training-data","<blockquote>
  <p>Is it possible to train the sentiment classification model with the labeled data and then use it to predict sentiment on data that is not labeled?</p>
</blockquote>

<p>Yes. This is basically the definition of what <em>supervised learning</em> is.</p>

<p>I.e. you train on data that has labels, so that you can then put it into production on categorizing your data that does not have labels.</p>

<p>(Any book on supervised learning will have code examples.)</p>

<p>I wonder if your question might really be: can I use supervised learning to make a model, assign labels to another 500 articles, then do further machine learning on all 600 articles? Well the answer is still yes, but the quality will fall somewhere between these two extremes:</p>

<ul>
<li>Assign random labels to the 500. Bad results.</li>
<li>Get a domain expert assign correct labels to those 500. Good results.</li>
</ul>

<p>Your model could fall anywhere between those two extremes. It is useful to know where it is, so know if it is worth using the data. You can get an estimate of that by taking a sample, say 25 records, and have them also assigned by a domain expert. If all 25 match, there is a reasonable chance your other 475 records also have been given good labels. If  e.g. only 10 of the 25 match, the model is much closer to the random end of the spectrum, and using the other 475 records is probably a bad idea.</p>

<p>(""10"", ""25"", etc. are arbitrary examples; choose based on the number of different labels, and your desired confidence in the results.)</p>
",1,0,432,2019-11-15 03:34:31,https://stackoverflow.com/questions/58869955/is-it-possible-to-train-the-sentiment-classification-model-with-the-labeled-data
Urdu language dataset for aspect-based sentiment analysis,"<p><a href=""https://i.sstatic.net/IMVza.png"" rel=""nofollow noreferrer"">when i run my code i get this error this error because of what></a></p>

<pre><code>text_raw_indices = tokenizer.text_to_sequence(text_left + "" "" + aspect + "" "" + text_right)
            text_raw_without_aspect_indices = tokenizer.text_to_sequence(text_left + "" "" + text_right)
            text_left_indices = tokenizer.text_to_sequence(text_left)
            text_left_with_aspect_indices = tokenizer.text_to_sequence(text_left + "" "" + aspect)
            text_right_indices = tokenizer.text_to_sequence(text_right, reverse=True)
            text_right_with_aspect_indices = tokenizer.text_to_sequence("" "" + aspect + "" "" + text_right, reverse=True)
            aspect_indices = tokenizer.text_to_sequence(aspect)
            left_context_len = np.sum(text_left_indices != 0)
            aspect_len = np.sum(aspect_indices != 0)
            aspect_in_text = torch.tensor([left_context_len.item(), (left_context_len + aspect_len - 1).item()])
            polarity = int(polarity) + 1
</code></pre>
","python, dataset, sentiment-analysis, aspect, urdu","<p>Just use LASER and you'll be fine. It covers Urdu as well.</p>

<p>You can read more here:</p>

<ul>
<li><a href=""https://engineering.fb.com/ai-research/laser-multilingual-sentence-embeddings/"" rel=""nofollow noreferrer"">https://engineering.fb.com/ai-research/laser-multilingual-sentence-embeddings/</a></li>
<li><a href=""https://github.com/facebookresearch/LASER"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/LASER</a></li>
</ul>

<p>There's also unofficial <code>pypi</code> package <a href=""https://pypi.org/project/laserembeddings/"" rel=""nofollow noreferrer"">here</a>. It substitutes some inner dependencies, but still works as expected.</p>

<p>And most important question, so we may better help you: what are you trying to achieve, what is your final goal?</p>
",0,0,196,2019-11-15 09:43:23,https://stackoverflow.com/questions/58874237/urdu-language-dataset-for-aspect-based-sentiment-analysis
Azure text analytics accuracy in german is different from the demo case,"<p>I've created a website which communicates with my logic app while that uses the text analytics from azure. But my applications acts different from the demo case which you can find here: <a href=""https://azure.microsoft.com/de-de/services/cognitive-services/text-analytics/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/de-de/services/cognitive-services/text-analytics/</a> ! When you post for example: </p>

<blockquote>
  <p>Hallo E-Bike Team,</p>
  
  <p>ich habe ein Problem mit meinem E-Bike. Es ist das Model ProRide2E.
  Seit heute Morgen steht auf dem Display folgender Hinweis:</p>
  
  <p>„Akkuleistung beeinträchtigt. Fehlercode: XB1200AB“</p>
  
  <p>Das darf doch wohl nicht wahr sein. Ich bin echt sauer. Wieso ist das
  doofe Bike immer kaputt?</p>
  
  <p>Ein genervter Kunde</p>
  
  <p>Name</p>
</blockquote>

<p>which is very negative it responses with a sentimental score of 31% in the demo but in my app it responses with 50% which is clearly wrong because it is negative and it should be below 50%. I uses the same cognitive services as the demo but my accuracy is not similar to the demo. </p>

<p><strong>Is there any way to improve my accuracy?</strong></p>

<p>ps: I'm using the free subscription. Does the accuracy change if change that?</p>
","azure, azure-logic-apps, sentiment-analysis, azure-cognitive-services, text-analytics-api","<p>I tried to repro the issue at my side and found that when you don't set <code>Language</code> parameter it will give you the SENTIMENT score of 0.5
<a href=""https://i.sstatic.net/0iLSG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0iLSG.png"" alt=""enter image description here""></a></p>

<p>When you set the Language parameter to <code>de</code> it will give you exact SENTIMENT score of 0.30392158031463623.</p>

<p>Below is the screenshot to show how you can set the Language parameter:</p>

<p><a href=""https://i.sstatic.net/qdWS5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qdWS5.png"" alt=""enter image description here""></a> </p>

<p><a href=""https://i.sstatic.net/MLMSV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MLMSV.png"" alt=""enter image description here""></a></p>
",1,0,135,2019-11-15 10:13:52,https://stackoverflow.com/questions/58874775/azure-text-analytics-accuracy-in-german-is-different-from-the-demo-case
How to load unlabelled data for sentiment classification after training SVM model?,"<p>I am trying to do sentiment classification and I used sklearn SVM model. I used the labeled data to train the model and got 89% accuracy. Now I want to use the model to predict the sentiment of unlabeled data. How can I do that? and after classification of unlabeled data, how to see whether it is classified as positive or negative?</p>

<p>I used python 3.7. Below is the code.</p>

<pre><code>import random
import pandas as pd
data = pd.read_csv(""label data for testing .csv"", header=0)
sentiment_data = list(zip(data['Articles'], data['Sentiment']))
random.shuffle(sentiment_data)

train_x, train_y = zip(*sentiment_data[:350])
test_x, test_y = zip(*sentiment_data[350:])

from nltk import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn import metrics


clf = Pipeline([
    ('vectorizer', CountVectorizer(analyzer=""word"",
                                   tokenizer=word_tokenize,
                                   preprocessor=lambda text: text.replace(""&lt;br /&gt;"", "" ""),
                                   max_features=None)),
    ('classifier', LinearSVC())
])

clf.fit(train_x, train_y)
pred_y = clf.predict(test_x)
print(""Accuracy : "", metrics.accuracy_score(test_y, pred_y))
print(""Precision : "", metrics.precision_score(test_y, pred_y))
print(""Recall : "", metrics.recall_score(test_y, pred_y))
</code></pre>

<p>When I run this code, I get the output:</p>

<blockquote>
  <p>ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.   ""the number of iterations."", ConvergenceWarning)
  Accuracy :  0.8977272727272727 
  Precision :  0.8604651162790697 
  Recall :  0.925</p>
</blockquote>

<p>What is the meaning of ConvergenceWarning?</p>

<p>Thanks in Advance!</p>
","machine-learning, svm, python-3.7, sentiment-analysis, sklearn-pandas","<blockquote>
  <p>What is the meaning of ConvergenceWarning?</p>
</blockquote>

<p>As Pavel already mention, ConvergenceWArning means that the <code>max_iter</code>is hitted, you can supress the warning here: <a href=""https://stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn"">How to disable ConvergenceWarning using sklearn?</a></p>

<blockquote>
  <p>Now I want to use the model to predict the sentiment of unlabeled
  data. How can I do that?</p>
</blockquote>

<p>You will do it with the command: <code>pred_y = clf.predict(test_x)</code>, the only thing you will adjust is :<code>pred_y</code> (this is your free choice), and <code>test_x</code>, this should be your new unseen data, it has to have the same number of features as your data <code>test_x</code> and <code>train_x</code>.</p>

<p>In your case as you are doing:</p>

<pre><code>sentiment_data = list(zip(data['Articles'], data['Sentiment']))
</code></pre>

<p>You are forming a tuple: <a href=""https://stackoverflow.com/questions/16031056/how-to-form-tuple-column-from-two-columns-in-pandas"">Check this out</a>
then you are shuffling it and <a href=""https://stackoverflow.com/questions/29139350/difference-between-ziplist-and-ziplist/29139418"">unzip</a> the first 350 rows:</p>

<pre><code>train_x, train_y = zip(*sentiment_data[:350])
</code></pre>

<p>Here you <code>train_x</code> is the column: <code>data['Articles']</code>, so all you have to do if you have new data:</p>

<pre><code>new_ data = pd.read_csv(""new_data.csv"", header=0)
new_y = clf.predict(new_data['Articles'])
</code></pre>

<blockquote>
  <p>how to see whether it is classified as positive or negative?</p>
</blockquote>

<p>You can run then: <code>pred_y</code>and there will be either a 1 or a 0 in your outcome. Normally 0 should be negativ, but it depends on your dataset-up</p>
",1,1,1204,2019-11-18 08:19:08,https://stackoverflow.com/questions/58910413/how-to-load-unlabelled-data-for-sentiment-classification-after-training-svm-mode
"I get a TypeError while doing sentiment analysis, how can I fix this issue?","<p>I am doing Sentiment Analysis on Bitcoin News. During my coding a TypeError Problem occured. I hope you can help me and thank you very much in advance!</p>

<pre><code>from newsapi.newsapi_client import NewsApiClient
from textblob import TextBlob
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
import datetime
from datetime import time
import csv
from dateutil import parser

api = NewsApiClient(api_key='my key')

all_articles = api.get_everything(q='bitcoin',
                                      sources='bbc-news,the-verge,financial-times,metro,business-insider,reuters,bloomberg,cnbc,cbc-news,fortune,crypto-coins-news',
                                      domains='bbc.co.uk,techcrunch.com',
                                      from_param='2019-10-20',
                                      to='2019-11-19',
                                      language='en',
                                      sort_by='relevancy',
                                      page_size=100)

news= pd.DataFrame(all_articles['articles'])

news['polarity'] = news.apply(lambda x: TextBlob(x['description']).sentiment.polarity, axis=1)
news['subjectivity'] = news.apply(lambda x: TextBlob(x['description']).sentiment.subjectivity, axis=1)
news['date']= news.apply(lambda x: parser.parse(x['publishedAt']).strftime('%Y.%m.%d'), axis=1)
news['time']= news.apply(lambda x: parser.parse(x['publishedAt']).strftime('%H:%M'), axis=1)
</code></pre>

<p>Then this TypeError occurs:
<a href=""https://i.sstatic.net/T0RML.png"" rel=""nofollow noreferrer"">imgur_link</a></p>
","python, anaconda, jupyter, typeerror, sentiment-analysis","<p>As <a href=""https://stackoverflow.com/users/8868699/hayat"">@Hayat</a> correctly pointed out, some of the rows in <code>description</code> column have <code>None</code> values which are causing the exception. There are four such rows, see the screenshot below.</p>

<p><a href=""https://i.sstatic.net/0pOTf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0pOTf.png"" alt=""Rows with &lt;code&gt;None&lt;/code&gt; in &lt;code&gt;description&lt;/code&gt; column""></a></p>

<p>You should remove such rows and operate on the ones that have proper data. You can filter rows that have <code>None</code> in <code>description</code> column using</p>

<pre><code>news_filtered = news[news['description'].notnull()]
news_filtered['polarity'] = news_filtered.apply(lambda x: TextBlob(x['description']).sentiment.polarity, axis=1)
</code></pre>

<p>You may have to repeat the above for other columns.</p>
",0,-1,179,2019-11-19 10:03:15,https://stackoverflow.com/questions/58931303/i-get-a-typeerror-while-doing-sentiment-analysis-how-can-i-fix-this-issue
How to predict Sentiments after training and testing the model by using NLTK NaiveBayesClassifier in Python?,"<p>I am doing sentiment classification using NLTK NaiveBayesClassifier. I trained and test the model with the labeled data. Now I want to predict sentiments of the data that is not labeled. However, I run into the error.
The line that is giving error is :</p>

<pre><code>score_1 = analyzer.evaluate(list(zip(new_data['Articles'])))
</code></pre>

<p>The error is :</p>

<blockquote>
  <p>ValueError: not enough values to unpack (expected 2, got 1)</p>
</blockquote>

<p>Below is the code:</p>

<pre><code>import random
import pandas as pd
data = pd.read_csv(""label data for testing .csv"", header=0)
sentiment_data = list(zip(data['Articles'], data['Sentiment']))
random.shuffle(sentiment_data)
new_data = pd.read_csv(""Japan Data.csv"", header=0)
train_x, train_y = zip(*sentiment_data[:350])
test_x, test_y = zip(*sentiment_data[350:])

from unidecode import unidecode
from nltk import word_tokenize
from nltk.classify import NaiveBayesClassifier
from nltk.sentiment import SentimentAnalyzer
from nltk.sentiment.util import extract_unigram_feats

TRAINING_COUNT = 350


def clean_text(text):
    text = text.replace(""&lt;br /&gt;"", "" "")

    return text


analyzer = SentimentAnalyzer()
vocabulary = analyzer.all_words([(word_tokenize(unidecode(clean_text(instance))))
                                 for instance in train_x[:TRAINING_COUNT]])
print(""Vocabulary: "", len(vocabulary))

print(""Computing Unigran Features ..."")

unigram_features = analyzer.unigram_word_feats(vocabulary, min_freq=10)

print(""Unigram Features: "", len(unigram_features))

analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_features)

# Build the training set
_train_X = analyzer.apply_features([(word_tokenize(unidecode(clean_text(instance))))
                                    for instance in train_x[:TRAINING_COUNT]], labeled=False)

# Build the test set
_test_X = analyzer.apply_features([(word_tokenize(unidecode(clean_text(instance))))
                                   for instance in test_x], labeled=False)

trainer = NaiveBayesClassifier.train
classifier = analyzer.train(trainer, zip(_train_X, train_y[:TRAINING_COUNT]))

score = analyzer.evaluate(list(zip(_test_X, test_y)))
print(""Accuracy: "", score['Accuracy'])

score_1 = analyzer.evaluate(list(zip(new_data['Articles'])))
print(score_1)
</code></pre>

<p>I understand that the problem is arising because I have to give two parameters is the line which is giving an error but I don't know how to do this.</p>

<p>Thanks in Advance.</p>
","nltk, python-3.7, sentiment-analysis, predict, naivebayes","<p><strong>Documentation and example</strong></p>

<p>The line that gives you the error calls the method SentimentAnalyzer.evaluate(...) . 
This method does the following.</p>

<blockquote>
  <p>Evaluate and print classifier performance on the test set.</p>
</blockquote>

<p>See <a href=""https://www.nltk.org/api/nltk.sentiment.html#nltk.sentiment.sentiment_analyzer.SentimentAnalyzer.evaluate"" rel=""nofollow noreferrer"">SentimentAnalyzer.evaluate</a>.</p>

<p>The method has one mandatory parameter: test_set .</p>

<blockquote>
  <p>test_set – A list of (tokens, label) tuples to use as gold set.</p>
</blockquote>

<p>In the example at <a href=""http://www.nltk.org/howto/sentiment.html"" rel=""nofollow noreferrer"">http://www.nltk.org/howto/sentiment.html</a> test_set has the following structure:</p>

<pre><code>[({'contains(,)': False, 'contains(.)': True, 'contains(and)': False, 'contains(the)': True}, 'subj'), ({'contains(,)': True, 'contains(.)': True, 'contains(and)': False, 'contains(the)': True}, 'subj'), ...]
</code></pre>

<p>Here is a symbolic representation of the structure.</p>

<pre><code>[(dictionary,label), ... , (dictionary,label)]
</code></pre>

<p><strong>Error in your code</strong></p>

<p>You are passing</p>

<pre><code>list(zip(new_data['Articles']))
</code></pre>

<p>to SentimentAnalyzer.evaluate. I assume your getting the error because</p>

<pre><code>list(zip(new_data['Articles']))
</code></pre>

<p>does not create a list of (tokens, label) tuples. You can check that by creating a variable which contains the list and printing it or looking at the value of the variable while debugging.
E.G.</p>

<pre><code>test_set = list(zip(new_data['Articles']))
print(""begin test_set"")
print(test_set)
print(""end test_set"")
</code></pre>

<p>You are calling evaluate correctly 3 lines above the one that is giving the error.</p>

<pre><code>score = analyzer.evaluate(list(zip(_test_X, test_y)))
</code></pre>

<p>I guess you want to call SentimentAnalyzer.classify(instance) to predict unlabeled data. See <a href=""https://www.nltk.org/api/nltk.sentiment.html#nltk.sentiment.sentiment_analyzer.SentimentAnalyzer.classify"" rel=""nofollow noreferrer"">SentimentAnalyzer.classify</a>.</p>
",1,0,303,2019-11-21 03:09:11,https://stackoverflow.com/questions/58966684/how-to-predict-sentiments-after-training-and-testing-the-model-by-using-nltk-nai
Load data: TypeError: cannot convert the series to &lt;class &#39;int&#39;&gt;,"<p>I am currently working on a sentiment analysis project. When I was trying to load data, the error<code>TypeError: '&lt;=' not supported between instances of 'str' and 'int'</code> appears. So I modified the str to an int. However, now the error <code>TypeError: cannot convert the series to &lt;class 'int'&gt;</code> appears. How can I solve this error?
The data type = dtype('O')</p>

<pre><code>import pandas as pd
#read data
review_df = pd.read_csv( '/Users/mhnwong/Desktop/FYP potential/metropark_trans_demoji.csv')
#create label
review_df[""is_bad_review""] = int(review_df[""rate""]).apply(lambda x:1 if x &lt;= 8 else 0)
# select only relevant columns
review_df = review_df[[""review"",""is_bad_review""]]

review_df.head()
</code></pre>

<p>Data:
<a href=""https://i.sstatic.net/WDHa0.png"" rel=""nofollow noreferrer"">This is the data I have got</a></p>
","python, pandas, sentiment-analysis","<p>Does your <code>rate</code> column in <code>metropark_trans_demoji.csv</code> file contain something that doesn't belong to numbers?</p>

<p>You should make sure that the content of the <code>rate</code> column can all be converted to <code>int</code> first.</p>

<p>Suppose the <code>rate</code> column's data all belong to <code>int</code>, then you can write</p>

<pre><code>review_df[""is_bad_review""] = review_df[""rate""].apply(lambda x:1 if int(x) &lt;= 8 else 0)
</code></pre>

<p>This converts the content of the <code>rate</code> column to <code>int</code>, then compares it with 8.</p>

<p>The type of <code>review_df[""rate""]</code> is <code>&lt;class 'pandas.core.series.Series'&gt;</code>. It cannot be converted to <code>int</code> directly.</p>
",0,0,267,2019-11-26 03:24:03,https://stackoverflow.com/questions/59043293/load-data-typeerror-cannot-convert-the-series-to-class-int
How to implement the language automatically in detect sentiment in azure text analytics in a logic app?,"<p>I've created a logic app which detects the language of a text and then the sentiment with cognitive services. I want to change the language parameter to the actual language which is detected. I've tried the following and much other stuff but it doesn't work out for me.<a href=""https://i.sstatic.net/20P9H.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/20P9H.png"" alt=""enter image description here"" /></a>
Can anyone suggest me a solution?
(The text is German but when I run it it says: &quot;Supplied language not supported. Pass in one of: ar,da,de,el,en,es,fi,fr,it,ja,nl,no,pl,pt-PT,ru,sv,tr,zh-Hans&quot;)
For a copy:</p>
<blockquote>
<p>Hallo E-Bike Team, ich habe ein Problem mit meinem E-Bike. Es ist das
Model ProRide2E. Seit heute Morgen steht auf dem Display folgender
Hinweis: „Akkuleistung beeinträchtigt. Fehlercode: XB1200AB“ Das darf
doch wohl nicht wahr sein. Ich bin echt sauer. Wieso ist das doofe
Bike immer kaputt?</p>
<p>Ein genervter Kunde</p>
<p>Name</p>
</blockquote>
","azure, azure-logic-apps, sentiment-analysis, text-analytics-api","<p>I added dynamic content instead of the @('Detect_Language')?['iso6391Name']. The dynamic content was Language Code. That created a for each loop. In the response I didnt output score as usual I added a expression (body('Detect_Sentiment')[0]?['score']) That worked out!</p>
",0,0,290,2019-11-28 08:44:17,https://stackoverflow.com/questions/59084756/how-to-implement-the-language-automatically-in-detect-sentiment-in-azure-text-an
Sentiment Analysis using azure error &#39;Resource not found&#39;,"<p>I have created a python program that accepts a String as an input and performs sentiment analysis on it.</p>

<p>I have created Environmental variable as stated in DOCUMENTATION and also restart the cmd as well as Visual Studio but still, I get the following error:</p>

<blockquote>
  <p>Encountered exception. Operation returned an invalid status code 'Resource Not Found'</p>
</blockquote>

<p>The python program is as follows:</p>

<pre><code>import os
from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient
from msrest.authentication import CognitiveServicesCredentials

#TEXT_ANALYTICS_SUBSCRIPTION_KEY = 'b2c4f0ee35c941078d9e6971c15c3472'
#TEXT_ANALYTICS_ENDPOINT = 'https://westcentralus.api.cognitive.microsoft.com/text/analytics'

key_var_name = 'TEXT_ANALYTICS_SUBSCRIPTION_KEY'
if not key_var_name in os.environ:
    raise Exception('Please set/export the environment variable: {}'.format(key_var_name))
subscription_key = os.environ[key_var_name]

endpoint_var_name = 'TEXT_ANALYTICS_ENDPOINT'
if not endpoint_var_name in os.environ:
    raise Exception('Please set/export the environment variable: {}'.format(endpoint_var_name))
endpoint = os.environ[endpoint_var_name]

def authenticateClient():
    credentials = CognitiveServicesCredentials(subscription_key)
    text_analytics_client = TextAnalyticsClient(
        endpoint=endpoint, credentials=credentials)
    return text_analytics_client

sentence = input(""Enter the string:"")

def sentiment():

    client = authenticateClient()

    try:
        documents = [
            {""id"": ""1"", ""language"": ""en"", ""text"": sentence}
        ]

        response = client.sentiment(documents=documents)
        for document in response.documents:
            print(""Entered Text: "", document.text, "", Sentiment Score: "",
                  ""{:.2f}"".format(document.score))

    except Exception as err:
        print(""Encountered exception. {}"".format(err))
sentiment()
</code></pre>
","python, azure, sentiment-analysis, azure-cognitive-services","<p>It is not recommended sharing your subscription key here , pls revoke this subscription key asap .For your issue, try this : </p>

<pre><code>import os
from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient
from msrest.authentication import CognitiveServicesCredentials

subscription_key = 'b2c4f0ee35c941078d9e6971c15c3472'
endpoint = 'https://westcentralus.api.cognitive.microsoft.com/'

def authenticateClient():
    credentials = CognitiveServicesCredentials(subscription_key)
    text_analytics_client = TextAnalyticsClient(
        endpoint=endpoint, credentials=credentials)
    return text_analytics_client

sentence = input(""Enter the string:"")

def sentiment():

    client = authenticateClient()

    try:
        documents = [
            {""id"": ""1"", ""language"": ""en"", ""text"": sentence}
        ]



        response = client.sentiment(documents=documents)
        for document in response.documents:
            print(""Entered Text: "",sentence ,"", Sentiment Score: "",
                  ""{:.2f}"".format(document.score))


    except Exception as err:
        print(""Encountered exception. {}"".format(err))

sentiment()
</code></pre>

<p>Result :</p>

<p><a href=""https://i.sstatic.net/7ceez.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7ceez.png"" alt=""enter image description here""></a></p>
",1,2,778,2019-12-05 06:41:04,https://stackoverflow.com/questions/59189603/sentiment-analysis-using-azure-error-resource-not-found
Laravel &quot;ErrorException (E_NOTICE) Undefined variable: class&quot;,"<pre><code>    Hello everyone can help me, I have a problem ""ErrorException (E_NOTICE) Undefined variable: class""


         class ControllerTraining extends Controller
    {
        public function index()
        {

                $title = ""Data Training"";
                foreach(Sentimen::all() as $stm){
                    $class['class'][] = $stm-&gt;kategori;
                    $data_training[$stm-&gt;kategori] = WordFrequency::where('id_sentimen',$stm-&gt;id_sentimen)-&gt;get();
                }


                $total = WordFrequency::count();
</code></pre>

<p><strong>*Error in here</strong></p>

<pre class=""lang-php prettyprint-override""><code> foreach($class['class'] as $cls){
                        $sum = DB::table('term_frequency')-&gt;select(DB::raw('SUM(jumlah) as jumlah_term'))-&gt;join('sentimen', 'sentimen.id_sentimen', '=', 'term_frequency.id_sentimen')-&gt;where('sentimen.kategori',$cls)-&gt;whereNotNull('id_training')-&gt;first();   
                        $data_sum[] = [
                            'kelas' =&gt; $cls,
                            'jumlah' =&gt; $sum-&gt;jumlah_term,
                        ];
                    }

                    $distinct = DB::select(""SELECT count(*) as total FROM (SELECT kata FROM term_frequency WHERE term_frequency.id_training is not null GROUP by kata) as x"");
                    foreach($distinct as $dst){
                        $distinctWords = $dst-&gt;total;
                    }
                    $uniqueWords = $distinctWords;


                    $i = 0;
                    foreach($class['class'] as $cls)
                    {
                        $Count = DB::table('data_training')
                                    -&gt;join('data_crawling', 'data_training.id_crawling', '=', 'data_crawling.id_crawling')
                                    -&gt;join('sentimen', 'sentimen.id_sentimen', '=', 'data_crawling.id_sentimen')
                                    -&gt;select('sentimen.kategori as kategori')
                                    -&gt;where('sentimen.kategori', '=', $cls)
                                    -&gt;count();
                        // $Count = DataTraining::where('kategori',$cls)-&gt;count();
                        $totalCount = DataTraining::count();
                        $prior[] = [
                            'kelas' =&gt; $cls,
                            'nilai' =&gt; $Count / $totalCount,
                        ];
                    }

                    return view('data_training', compact(['title','data_sum','prior','uniqueWords','data_training','total']));


            }

            public function hapus_training($kategori)
            {
                $data_training = TwitterStream::where('id_sentimen',$kategori)-&gt;delete();
                return redirect('/training');
            }

            public function data_sentimen()
            {
                $data_training = Sentimen::all();
                return response()-&gt;json($data_training);
            }

        }
</code></pre>
","php, laravel, eloquent, sentiment-analysis","<pre class=""lang-php prettyprint-override""><code>&lt;?php
namespace App\Http\Controllers;
use Illuminate\Http\Request; 
use App\Models\DataTraining; 
use App\Models\WordFrequency; 
use App\Models\Sentimen; 
use App\Models\TwitterStream; 
use Illuminate\Http\Respons;
use DB; 
    class ControllerTraining extends Controller
    {
        public function index()
        {

            $title = ""Data Training"";
            $sentimen = Sentimen::all();
            $class = [];
            $data_training = [];
            if(!empty($sentimen)){ 
                $class['class']= [];
                foreach($sentimen as $key =&gt; $stm){
                    $class['class'][$key] = $stm-&gt;kategori;
                    $data_training[$stm-&gt;kategori] = WordFrequency::where('id_sentimen',$stm-&gt;id_sentimen)-&gt;get();
                }

                $total = WordFrequency::count();
                foreach($class['class'] as $cls){
                    $sum = DB::table('term_frequency')-&gt;select(DB::raw('SUM(jumlah) as jumlah_term'))-&gt;join('sentimen', 'sentimen.id_sentimen', '=', 'term_frequency.id_sentimen')-&gt;where('sentimen.kategori',$cls)-&gt;whereNotNull('id_training')-&gt;first();   
                    $data_sum[] = [
                        'kelas' =&gt; $cls,
                        'jumlah' =&gt; $sum-&gt;jumlah_term,
                    ];
                }

                $distinct = DB::select(""SELECT count(*) as total FROM (SELECT kata FROM term_frequency WHERE term_frequency.id_training is not null GROUP by kata) as x"");
                foreach($distinct as $dst){
                    $distinctWords = $dst-&gt;total;
                }
                $uniqueWords = $distinctWords;


                $i = 0;
                foreach($class['class'] as $cls)
                {
                    $Count = DB::table('data_training')
                                -&gt;join('data_crawling', 'data_training.id_crawling', '=', 'data_crawling.id_crawling')
                                -&gt;join('sentimen', 'sentimen.id_sentimen', '=', 'data_crawling.id_sentimen')
                                -&gt;select('sentimen.kategori as kategori')
                                -&gt;where('sentimen.kategori', '=', $cls)
                                -&gt;count();
                    // $Count = DataTraining::where('kategori',$cls)-&gt;count();
                    $totalCount = DataTraining::count();
                    $prior[] = [
                        'kelas' =&gt; $cls,
                        'nilai' =&gt; $Count / $totalCount,
                    ];
                }
                return view('data_training', compact(['title','data_sum','prior','uniqueWords','data_training','total']));
            } else {
                echo ""sentimen is empty"";
            }
        }

        public function hapus_training($kategori)
        {
            $data_training = TwitterStream::where('id_sentimen',$kategori)-&gt;delete();
            return redirect('/training');
        }

        public function data_sentimen()
        {
            $data_training = Sentimen::all();
            return response()-&gt;json($data_training);
        }

    }

</code></pre>
",0,-1,571,2019-12-16 07:31:20,https://stackoverflow.com/questions/59352235/laravel-errorexception-e-notice-undefined-variable-class
Semi-supervised sentiment analysis in Python?,"<p>I have been following this tutorial 
<a href=""https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/"" rel=""nofollow noreferrer"">https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/</a>
to create a sentiment analysis in python. However, here's what I don't understand: It seems to me that the data they use is already labeled? So, how do I use the training I did on the labeled data to then apply to unlabeled data?</p>

<p>I wanna do sth like this:</p>

<p>Assuming I have 2 dataframes:
df1 is a small one with labeled data, df2 is a big one with unlabeled data. I just finished training with df1. How do I then go about predicting the values for df2?</p>

<p>I thought it would be as straight forward as text_classifier.predict(df2.iloc[:,1].values), but that doesn't work for me.</p>

<p>Also, forgive me if this question may seem stupid, but I don't have a lot of experience with machine learning and nltk ...</p>

<p>EDIT:
Here is the code I'm working on:</p>

<pre><code>enc = preprocessing.LabelEncoder()
//chat_data = chat_data[:180]
//chat_labels = chat_labels[:180]

chat_labels = enc.fit_transform(chat_labels)
vectorizer = TfidfVectorizer (max_features=2500, min_df=1, max_df=1, stop_words=stopwords.words('english'))
features = vectorizer.fit_transform(chat_data).toarray()
print(chat_data)

X_train, X_test, y_train, y_test = train_test_split(features, chat_labels, test_size=0.2, random_state=0)

text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)
text_classifier.fit(X_train, y_train)
predictions = text_classifier.predict(X_test)



print(confusion_matrix(y_test,predictions))
print(classification_report(y_test,predictions))
print(accuracy_score(y_test, predictions))

chatData = pd.read_csv(r""C:\Users\jgott\OneDrive\Dokumente\Thesis\chat.csv"")
unlabeled = chatData.iloc[:,1].values
unlabeled = vectorizer.fit_transform(unlabeled.astype('U'))
print(unlabeled)
//features = vectorizer.fit_transform(unlabeled).toarray()
predictions = text_classifier.predict(unlabeled)
</code></pre>

<p>Most of it is taken exactly from the tutorial, except for the line with astype in it, which I used to convert the unlabeled data, because I got a valueError that told me it can't convert from String to float if I don't do that first.</p>
","python, nltk, sentiment-analysis","<blockquote>
  <p>how do I use the training I did on the labeled data to then apply to
  unlabeled data?</p>
</blockquote>

<p>This is really the problem that supervised ML tries to solve: having <strong>known</strong> <strong>labeled</strong> data as inputs of the form <code>(sample, label)</code>, a model tries to discover the <strong>generic patterns</strong> that exist in these data. These patterns hopefully will be useful to predict the labels of <strong>unseen</strong> <strong>unlabeled</strong> data.        </p>

<p>For example in sentiment-analysis (sad, happy) problem, the pattern that may be discovered by a model after training process:</p>

<p>Existence of one or more of this words means sad:</p>

<pre><code>(""misery"" , 'sad', 'displaced people', 'homeless'...)
</code></pre>

<p>Existence of one or more of this words means happy:</p>

<pre><code>(""win"" , ""delightful"", ""wedding"", ...) 
</code></pre>

<p>If new textual document is given we will search for these patterns inside this document and we will label it accordingly.</p>

<p>As side note: we usually do not use the whole labeled dataset in the training process, instead we take a small portion from the dataset(other than the training set) to validate our model, and verify that it discovered a really generic patterns, not ones tailored specifically for the training data.</p>
",0,0,487,2019-12-16 16:25:46,https://stackoverflow.com/questions/59360557/semi-supervised-sentiment-analysis-in-python
How to have a sentiment score for a document in Quanteda?,"<p>I am new in sentiment analysis. <a href=""https://tutorials.quanteda.io/advanced-operations/targeted-dictionary-analysis/"" rel=""nofollow noreferrer"">Quanteda examples</a> show how to output numbers of positive and negative words. I tested some documents. It output below: </p>

<p><strong>Case 1</strong></p>

<pre><code>document    negative    positive
file1   28  28
file2   98  71
file3   28  22
file4   37  39
file5   7   36
</code></pre>

<p>or below </p>

<p><strong>Case 2</strong></p>

<pre><code>document    negative    positive    neg_positive    neg_negative
file1   28  28  0   1
file2   98  71  0   0
file3   28  22  1   0
file4   37  39  0   1
file5   7   36  0   1
</code></pre>

<p>Can you let me know how to have scores for file1 .. file5 in both cases? Is that </p>

<p>(#positive - #negative) / #all in case 1 file2， （71-98）/(71+98)=-27/169= - 0.15 ?</p>

<p>what about case 2?</p>

<p>Thanks a lot.</p>

<p>A</p>
","sentiment-analysis, quanteda","<p>If you consider <code>neg_positive</code> as <code>negative</code>, and <code>neg_negative</code> as positive, then you could create your index by combining the pairs of columns.  This is plausible because the ""neg positive"" for instance contains sequences such as ""not good"".</p>

<pre><code>(rowSums(object[, c(""negative"", ""neg_positive"")]) -
    rowSums(object[, c(""positive"", ""neg_negative"")])) / rowSums(object) * 100
</code></pre>

<p>Another (better) measure is the logit scale described in 
2011. William Lowe, Kenneth Benoit, Slava Mikhaylov, and Michael Laver. ""<a href=""https://kenbenoit.net/pdfs/Loweetal_2010_LSQ.pdf"" rel=""nofollow noreferrer"">Scaling Policy Preferences From Coded Political Texts.</a>"" Legislative Studies Quarterly 26(1, Feb): 123-155.  This is the log(positive/negative) or</p>

<pre><code>log( rowSums(object[, c(""positive"", ""neg_negative"")]) /
     rowSums(object[, c(""negative"", ""neg_positive"")]) )
</code></pre>
",1,1,161,2019-12-16 16:27:09,https://stackoverflow.com/questions/59360578/how-to-have-a-sentiment-score-for-a-document-in-quanteda
"merging tweets by date, returning count of sentiment score","<p>I'm working on a sentiment analysis problem. My dataframe is as follows</p>

<pre><code>   tweet     sentiment_score    timestamp   ticker
0   abc       3                2018-09-19   $AMD
1   def       1                2018-10-19   $AMD
2   wtf       2                2018-07-12   $PH   
3   pqr       2                2018-07-12   $PH
4   lmn       3                2018-08-23   $FB
5   jqr       3                2018-08-23   $FB
6   okm       1                2018-08-24   $FB
</code></pre>

<p>I want to </p>

<p>(1)Aggregate (merge) the tweets per ""ticker"" at a day level. So I can run a sentiment analysis and get overall sentiment score per ticker on a given day.</p>

<p>(2)""sentiment_score"" has values {0,1,2,3,4}, I want to create 5 new columns which contain the count of no.of tweets where sentiment_score is {0,1,2,3,4} for every ""ticker"" at a day level.</p>

<p>Expected output</p>

<pre><code>
   tweet        timestamp   ticker   setiment_1 sentiment_2 sentiment_3
0   abc,def     2018-09-19  $AMD         1         0          1
2   wtf,pqr     2018-07-12  $PH          0         2          0     
4   lmn,jqr     2018-08-23  $FB          0         0          2
6   okm         2018-08-24  $FB          1         0          0

</code></pre>

<p>I tried individual groupby operations but they didn't give the required output. Appreciate the help.</p>
","python, pandas-groupby, sentiment-analysis","<p>You can use <code>pandas.pivot_table(...)</code>:</p>

<pre class=""lang-py prettyprint-override""><code>df[""_dummy""]=1
df2=df.pivot_table(index=[""timestamp"", ""ticker""], columns=""sentiment_score"", values=""_dummy"", aggfunc=""sum"").fillna(0)
</code></pre>

<p>Output:</p>

<pre class=""lang-py prettyprint-override""><code>sentiment_score      2    3
timestamp  ticker
2018-07-12 $AVGO   1.0  0.0
2018-07-19 $PH     1.0  0.0
2018-08-23 $FB     0.0  1.0
2018-09-19 $AMD    0.0  1.0
2018-10-09 $CAT    0.0  1.0
</code></pre>

<p>Documentation: <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html</a></p>

<p><strong>Edit</strong></p>

<p>If you want to also merge tweets you can do on top of the above:</p>

<pre class=""lang-py prettyprint-override""><code>df3=df.pivot_table(index=[""timestamp"", ""ticker""], columns=""sentiment_score"", values=""stocktwit_tweet"", aggfunc=""sum"")
#and to merge it together with previous df:
res=pd.concat([df2, df3], axis=1)
#axis=1 - merge df-s horizontally, axis=0 - merge vertically
</code></pre>
",1,0,359,2019-12-21 09:34:31,https://stackoverflow.com/questions/59435068/merging-tweets-by-date-returning-count-of-sentiment-score
I am looking for a dutch language tokenizer for technical product review,"<p>I am trying to find out the better text cleaning method for Dutch NLP problem. I have used dutch version for pos tags and nltk for removal of stop words. But I am not getting desired results.</p>
","nlp, tokenize, sentiment-analysis","<p>have you tried this approach for dutch ? </p>

<pre><code>from nltk.util import ngrams
from nltk.corpus import alpino
print(alpino.words())
quadgrams=ngrams(alpino.words(),4)
for i in quadgrams:
    print(i)

</code></pre>
",1,1,233,2019-12-23 08:05:55,https://stackoverflow.com/questions/59451941/i-am-looking-for-a-dutch-language-tokenizer-for-technical-product-review
How do I efficiently loop over this dataframe and perform a function using inbuilt numpy or pandas?,"<p>I read <a href=""https://medium.com/swlh/how-to-efficiently-loop-through-pandas-dataframe-660e4660125d?"" rel=""nofollow noreferrer"">this</a> article earlier and noticed that the pandas apply function, iterrows and for loop are terribly slow and efficient way of working with pandas dataframes.</p>

<p>I am doing sentiment analysis on some text data, but using apply causes high memory usage and low speeds similar to shown in <a href=""https://stackoverflow.com/a/16245109/9292995"">this</a> answer.</p>

<pre><code>%%time
data.merge(data.essay.apply(lambda s: pd.Series({'neg':sid.polarity_scores(s)['neg'],
                                                 'neu':sid.polarity_scores(s)['neu'],
                                                 'pos':sid.polarity_scores(s)['pos'],
                                                 'compound':sid.polarity_scores(s)['compound']})),
                       left_index=True, right_index=True)
</code></pre>

<p>How can I implement this using either built-in numpy or pandas function?
Edit:- The column contains essay text data</p>
","python, pandas, numpy, machine-learning, sentiment-analysis","<p>I found one way to perform this function faster by using <a href=""https://towardsdatascience.com/pandaral-lel-a-simple-and-efficient-tool-to-parallelize-your-pandas-operations-on-all-your-cpus-bb5ff2a409ae"" rel=""nofollow noreferrer"">pandarallel</a>.</p>

<p>By using the default pandas apply function it took 9 min 24 secs,</p>

<p>But by using pandarallel it completed the operation in just 1 min 7 secs (Using 16 workers).</p>
",0,0,204,2020-01-07 08:18:40,https://stackoverflow.com/questions/59624552/how-do-i-efficiently-loop-over-this-dataframe-and-perform-a-function-using-inbui
Python SentiStrength binary score outputs only 1 score,"<p>I am currently working on text analysis using SentiStrength python library by command <code>result = senti.getSentiment(cs, score='binary')</code>. Firstly, I run it in Jupyter notebook and it works well. It outputs 2 scores which are positive and negative sentiment scores such as <code>[(2,-1)]</code>. However, when I try to run it in anaconda prompt or spyder. It outputs only 1 values like <code>[1]</code> and I do not understand why. I guess it is because I run it on different environment. I would like to ask how could I run this command in anaconda prompt or other IDE so that it could output result correctly? or did I do something wrong.</p>
","python, nlp, sentiment-analysis","<pre><code>    if score == 'scale': # Returns from -1 to 1
        senti_score = [sum(senti_score[i:i+2])/4 for i in range(0, len(senti_score), 3)]
    elif score == 'binary': # Return 1 if positive and -1 if negative
        senti_score = [1 if senti_score[i] &gt;= abs(senti_score[i+1]) else -1 for i in range(0, len(senti_score), 3)]
    elif score == 'trinary': # Return Positive and Negative Score and Neutral Score
        senti_score = [tuple(senti_score[i:i+3]) for i in range(0, len(senti_score), 3)]
    elif score == 'dual': # Return Positive and Negative Score
        senti_score = [tuple(senti_score[i:i+2]) for i in range(0, len(senti_score), 3)]
    else:
        return ""Argument 'score' takes in either 'scale' (between -1 to 1) or 'binary' (two scores, positive and negative rating)""
    return senti_score
</code></pre>

<p>was having the same problem and took at look at the sourcecode and found the documentation error. You should use 'dual' for the score type.</p>
",1,0,518,2020-01-15 07:59:44,https://stackoverflow.com/questions/59747146/python-sentistrength-binary-score-outputs-only-1-score
Sentiment anaysis using Harvard IV-4 dictionary,"<p>I was trying to compute the sentiment using Harvard IV-4dictionary.
I installed the ""pysentiment"" successfully.
I run the following:</p>

<pre><code>import pysentiment as ps
hiv4 = ps.HIV4()
tokens = hiv4.tokenize(text)  
score = hiv4.get_score(tokens)
</code></pre>

<p>and I got the following error:</p>

<pre><code> Traceback (most recent call last):
  File ""C:/Users/df/Desk Top/Finalazed/punctuation.py"", line 274, in &lt;module&gt;
    hiv4 = ps.HIV4()
  File ""C:\Users\df\AppData\Local\Programs\Python\Python37\lib\site-packages\pysentiment\base.py"", line 55, in __init__
    self._tokenizer = Tokenizer()
  File ""C:\Users\df\AppData\Local\Programs\Python\Python37\lib\site-packages\pysentiment\utils.py"", line 36, in __init__
    self._stopset = self.get_stopset()
  File ""C:\Users\df\AppData\Local\Programs\Python\Python37\lib\site-packages\pysentiment\utils.py"", line 52, in get_stopset
    fin = open('%s/%s'%(STATIC_PATH, f), 'rb')
FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\df\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pysentiment\\static/Currencies.txt'
</code></pre>

<p>Could any body tell why I am getting this? Thanks.</p>
",sentiment-analysis,"<p>Do copy pysentiment folder in the given path. Actually pysentiment folder doesnt contain static sub folder. You can check it by diplaying hidden folder ""local"".</p>
",1,0,1187,2020-01-18 12:39:56,https://stackoverflow.com/questions/59800667/sentiment-anaysis-using-harvard-iv-4-dictionary
How to fine grain neutral sentiment as positive or negative,"<p>I'm working on multimodal sentiment analysis with visual and textual cues.</p>

<p>My input dataset is containing neutral sentiment in ground truth but I require to do a binary classification to categorize  my input samples as either positive/negative</p>

<p>Is there any possibility to use this neutral class in aiding to remove non-opinion key terms thereby increasing the accuracy of binary categorization?</p>

<p>Is it advised only to adopt a multi-class classification algorithm to categorize as positive, negative or neutral?</p>

<p>P.S: My requirement is to do a binary classification</p>

<p>Thanks in advance</p>
","sentiment-analysis, text-classification","<p>If your requirement is to do obligatory binary classification, maybe it is worth to perform a hierarchical analysis in two binary classification steps. First you classify the documents into objectives(neutral) or subjectives(positives and negative). Then, for the subjective ones, you classify into positive or negative. </p>

<p>Otherwise, a better way is to simply work on multi-class classification and classify into three classes. </p>
",0,1,123,2020-01-23 00:30:48,https://stackoverflow.com/questions/59870123/how-to-fine-grain-neutral-sentiment-as-positive-or-negative
How to know if an application is using Cloud Foundry?,"<p>I am working on a .NET application that uses IBM Watson for sentiment analysis. I am new to this project and don't know the proper working of the IBM Watson. Recently, sentiment Analysis stopped working and we sent an email to IBM about it and they said that it might because we have not yet migrated it from Cloud Foundry to Resource Group. </p>

<p>I don't know whether we are using Cloud Foundary or not. How can I check it? 
People who have worked on this project before are currently not available and I am on my own now. </p>

<p>Please help me out. </p>
",".net, cloud-foundry, ibm-watson, sentiment-analysis","<p>If you need to migrate from Cloud Foundry, see <a href=""https://cloud.ibm.com/docs/natural-language-understanding?topic=watson-migrate"" rel=""nofollow noreferrer"">Migrating Watson services from Cloud Foundry</a>.</p>

<p>You can find out details about the service instance that you are using by clicking the service instance in your <a href=""https://cloud.ibm.com/resources?groups=resource-instance"" rel=""nofollow noreferrer"">Resource list</a> on IBM Cloud and looking at the credentials.</p>
",3,1,162,2020-01-29 06:04:48,https://stackoverflow.com/questions/59961328/how-to-know-if-an-application-is-using-cloud-foundry
"I have lexicon with sentiment score, I want to find these words from a tokenised tweets and add the score","<pre><code>Keyword .   Score
fabulous    7.526

excellent   7.247

superb  7.199

alert   7.099

drop    6.922


#Tokenized tweets below

[""b'just"", 'saw', 'amazon', 'ticwatch', 'pro', '4g/lte', 'smartwatch', 'dual', 'displa', '...', 'mobvoi', '299.00']

[""b'amazon"", 'pricedrop', 'deal', '\\nprice', 'drop', 'alert', 'camelbak', 'eddy', 'kids', 'vacuum', 'insulated', 'stainless', 'steel', 'bottle', '12', 'oz', 'retro', 'floral\\navg', 'price', '16.00\\nnew', 'price', '12.17\\nprice', 'drop', '23.94', '\\nURL']
</code></pre>

<p>For each list i want to see a sum of score that matches a key word
E.g</p>

<pre><code>Tweet 1 - 12.22

Tweet 2 - 7
</code></pre>

<p>Is there any library which will allow me to find words like this? Any help in this front is appreciated </p>
","python, twitter, nlp, sentiment-analysis","<p>if you have dataframe of keyword and score, you can use zip function as </p>

<pre><code>list_ = list(df['keyword'],df['score']) 
list_ = [('fabulous',7.526),('excellent',7.247), ('super',7.199),('alert',7.099),('drop',6.922)]
tweet_token = [['fabulous', 'excellent','super','alert','drop'],['super', 'alert']]


sum_ = []
for j in range(len(tweet_token)):
   sum_tweet = 0
   for i  in range(len(list_)):
       for token in tweet_token[j]:
           if token == list_[i][0]:
              sum_tweet += list_[i][1]
   sum_.append(sum_tweet)

#op
print(sum_)
[35.993, 14.298]
</code></pre>
",1,0,53,2020-02-03 04:28:53,https://stackoverflow.com/questions/60033097/i-have-lexicon-with-sentiment-score-i-want-to-find-these-words-from-a-tokenised
How can we do a sentiment analysis and create a &#39;sentiment&#39; record next to each line of text?,"<p>I Googled for some solutions to do sentiment analysis, and write the results to a column next to the column of text that's being analyzed. This is what I came up with.</p>

<pre><code>import nltk
nltk.download('vader_lexicon')
nltk.download('punkt')

# first, we import the relevant modules from the NLTK library
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# next, we initialize VADER so we can use it within our Python script
sid = SentimentIntensityAnalyzer()

# the variable 'message_text' now contains the text we will analyze.
message_text = '''Like you, I am getting very frustrated with this process. I am genuinely trying to be as reasonable as possible. I am not trying to ""hold up"" the deal at the last minute. I'm afraid that I am being asked to take a fairly large leap of faith after this company (I don't mean the two of you -- I mean Enron) has screwed me and the people who work for me.'''

print(message_text)

# Calling the polarity_scores method on sid and passing in the message_text outputs a dictionary with negative, neutral, positive, and compound scores for the input text
scores = sid.polarity_scores(message_text)

# Here we loop through the keys contained in scores (pos, neu, neg, and compound scores) and print the key-value pairs on the screen
for key in sorted(scores):
        print('{0}: {1}, '.format(key, scores[key]), end='')
</code></pre>

<p>This gives me:</p>

<pre><code>compound: -0.3804, neg: 0.093, neu: 0.836, pos: 0.071, 
</code></pre>

<p>Now, I am trying to feed in my own column of text, from a dataframe.</p>

<p>The sample code is from this site.</p>

<p><a href=""https://programminghistorian.org/en/lessons/sentiment-analysis"" rel=""nofollow noreferrer"">https://programminghistorian.org/en/lessons/sentiment-analysis</a></p>

<p>I have a field in a dataframe that consists of text, like this.</p>

<pre><code>These brush heads are okay!  Wish they came in a larger diameter, would cover more facial surface area and require less time to do the job!  However, I think they do a better job than just a face cloth in cleansing the pores.  I would recommend this product!
No opening to pee with. weird.  And really tight.  not very comfortable.
I choose it as spare parts always available and I will buy it again for sure!I will recommend it, without doubt!
love this cleanser!!
Best facial wipes invented!!!!!!(:
</code></pre>

<p>These are 5 individual records from my dataframe. I'm trying to think of a way to assess each record as 'positive', 'negative', or 'neutral', and place each sentiment in a new field in the same row.</p>

<p>In this example, I would think these 5 records have the following 5 sentiments (in a field next to each record):</p>

<pre><code>neutral
negative
positive
positive
positive
</code></pre>

<p>How can I do that?</p>

<p>I came up with an alternative sample of code, as shown below.</p>

<pre><code>event_dictionary ={scores[""compound""] &gt;= 0.05 : 'positive', scores[""compound""] &lt;= -0.05 : 'negative', scores[""compound""] &gt;= -0.05 and scores[""compound""] &lt;= 0.05 : 'neutral'} 
#message_text = str(message_text)
for message in message_text:
    scores = sid.polarity_scores(str(message))
    for key in sorted(scores):
        df['sentiment'] = df['body'].map(event_dictionary) 
</code></pre>

<p>This ran for about 15 minutes, then I cancelled it, and I saw that it actually did nothing at all.  I want to add a field named 'sentiment' and populate it with 'positive' if scores[""compound""] >= 0.05, 'negative' if scores[""compound""] &lt;= -0.05, and 'neutral' if scores[""compound""] >= -0.05 and scores[""compound""] &lt;= 0.05.</p>
","python, python-3.x, nltk, sentiment-analysis","<p>Not sure what this dataframe looks like, but you can use the Sentiment Intensity Analyzer on each of the strings to calculate the polarity scores of each message. According to the github page, you can use the ""compound"" key to calculate the sentiment of a message. </p>

<p><a href=""https://github.com/cjhutto/vaderSentiment#about-the-scoring"" rel=""nofollow noreferrer"">https://github.com/cjhutto/vaderSentiment#about-the-scoring</a></p>

<pre><code>messages = [
""These brush heads are okay!  Wish they came in a larger diameter, would cover more facial surface area and require less time to do the job!  However, I think they do a better job than just a face cloth in cleansing the pores.  I would recommend this product!"",
""No opening to pee with. weird.  And really tight.  not very comfortable."",
""I choose it as spare parts always available and I will buy it again for sure!I will recommend it, without doubt!"",
""love this cleanser!!"",
""Best facial wipes invented!!!!!!(:""]

for message in messages:
    scores = sid.polarity_scores(message)

    for key in sorted(scores):
        print('{0}: {1} '.format(key, scores[key]), end='')

    if scores[""compound""] &gt;= 0.05:
        print(""\npositive\n"")

    elif scores[""compound""] &lt;= -0.05:
        print(""\nnegative\n"")
    else:
        print(""\nneutral\n"")

</code></pre>

<p><strong>Output:</strong></p>

<pre><code>compound: 0.8713 neg: 0.0 neu: 0.782 pos: 0.218
positive

compound: -0.7021 neg: 0.431 neu: 0.569 pos: 0.0
negative

compound: 0.6362 neg: 0.0 neu: 0.766 pos: 0.234
positive

compound: 0.6988 neg: 0.0 neu: 0.295 pos: 0.705
positive

compound: 0.7482 neg: 0.0 neu: 0.359 pos: 0.641
positive
</code></pre>
",1,0,1939,2020-02-07 23:02:11,https://stackoverflow.com/questions/60122247/how-can-we-do-a-sentiment-analysis-and-create-a-sentiment-record-next-to-each
Polarity and Sentiment Analysis in Power BI with Python,"<p>I would like to use Python's Textblob for sentiment analysis in Power BI desktop. The code below works to create a separate dataframe that I can filter down to with the polarity scores.</p>

<pre><code># 'dataset' holds the input data for this script
import pandas as pd
from textblob import TextBlob
from itertools import islice

COLS = ['PersonID', 'QuestionID','Comment','subjectivity','polarity']
df = pd.DataFrame(columns=COLS)

for index, row in islice(dataset.iterrows(), 0, None):

     new_entry = []
     text_lower=str(row['Comment']).lower()
     blob = TextBlob(text_lower)
     sentiment = blob.sentiment

     polarity = sentiment.polarity
     subjectivity = sentiment.subjectivity

     new_entry += [row['PersonID'], row['QuestionID'],row['Comment'],subjectivity,polarity]

     single_survey_sentimet_df = pd.DataFrame([new_entry], columns=COLS)
     df = df.append(single_survey_sentimet_df, ignore_index=True)
</code></pre>

<p>However, I would like to write directly to the existing data table like</p>

<pre><code>#load in our dependencies
import pandas as pd
from nltk.sentiment.vader import SentimentIntensityAnalyzer

#load in the sentiment analyzer
sia=SentimentIntensityAnalyzer()

#apply the analyzer over each comment added to the existing table
# **I WANT TO USE A LINE LIKE THE ONE BELOW, BUT WITH THE TEXTBLOB FUNCTIONALITY ABOVE**
dataset['polairty scores'] =dataset['Message'].apply(lambda x: sia.polarity_scores(x)['compound'])
</code></pre>

<p>Reference: <a href=""https://www.absentdata.com/power-bi/sentiment-analysis-in-power-bi/"" rel=""nofollow noreferrer"">https://www.absentdata.com/power-bi/sentiment-analysis-in-power-bi/</a></p>
","python, powerbi, sentiment-analysis","<p>I assume you could do something like this and get similar fields that you got in your first script.</p>

<pre><code>dataset['polarity'] =dataset['Comment'].apply(lambda x: TextBlob(str(x).lower()).sentiment.polarity)
dataset['subjectivity'] =dataset['Comment'].apply(lambda x: TextBlob(str(x).lower()).sentiment.subjectivity)

</code></pre>
",1,1,1285,2020-02-12 22:22:17,https://stackoverflow.com/questions/60197761/polarity-and-sentiment-analysis-in-power-bi-with-python
Issue in user input or text file data in sentiment analysis,"<p>I am new to Python-NLTK. I have written my code using movie reviews data set.
When I put hard coded sample text for sentiment analysis it is working fine but when I try to take user input or fetch the data from text file it shows alphabet level splitting.</p>

<p>for e.g.
When sample text is hard coded like
[""Music was awesome"", ""Special effects are awesome""]
Then splitting is like a
Review : Music was awesome
Review : Special effects are awesome.</p>

<p>But if I asked for user input or fetch the data from text file then it shows review as;
Review: M
Review: u
Review: S
Review: i
Review: c
Review: .</p>

<p><strong>#For text file Below is my sample code.</strong></p>

<pre><code>t = open (""Sample1.txt"", ""r"")           
File_input = (t.read())
for review in File_input:
  print (""\nReview:"", review)
  probdist = classifier.prob_classify(extract_features(review.split()))
  pred_sentiment = probdist.max()

print (""Predicted sentiment:"", pred_sentiment) 
print (""Probability:"", round(probdist.prob(pred_sentiment), 5))
</code></pre>

<p><strong>#For user input Below is my sample code.</strong></p>

<pre><code>User_input = input(""Enter your value: "")
for review in User_input:
  print (""\nReview:"", review)
  probdist = classifier.prob_classify(extract_features(review.split()))
  pred_sentiment = probdist.max()
print (""Predicted sentiment:"", pred_sentiment) 
print (""Probability:"", round(probdist.prob(pred_sentiment), 3))
</code></pre>

<p>plz guide.
Thanks!</p>
","python, nltk, sentiment-analysis, corpus","<p>the <code>User_input</code> variable is a string, so iterating over it is iterating over the chars, what you want to do is remove the <code>for</code> loop and treat <code>User_input</code> as a review assuming it holds 1 review, otherwise you could define a separating char between reviews and iterate like so:</p>

<pre><code>for review in User_input.split(sep_char):
</code></pre>
",0,0,289,2020-02-17 11:03:39,https://stackoverflow.com/questions/60261080/issue-in-user-input-or-text-file-data-in-sentiment-analysis
How to drop data frame row if there is less than 4 character in sentence column?,"<p>let's say i have already tokenized sentence in my data frame like this :</p>

<pre><code>+-----------------------------------------+-----------+
|                sentence                 | sentiment |
+-----------------------------------------+-----------+
| [i, like, this, app, it, s, awesome]    | positive  |
| [way, to, many, ads, pop, up, hate, it] | negative  |
| [ye]                                    | negative  |
| [p]                                     | positive  |
| [niceeeee]                              | positive  |
| [i, do, not, like, the, design]         | negative  |
| [very, useful, recommended]             | positive  |
| [ugly]                                  | negative  |
| [xxx]                                   | negative  |
| [yes]                                   | positive  |
+-----------------------------------------+-----------+
</code></pre>

<p>I want to clean out unnecessary data from my data frame by removing df row if there is less than 4 character is sentence column, so the end result will be like this :</p>

<pre><code>+-----------------------------------------+-----------+
|                sentence                 | sentiment |
+-----------------------------------------+-----------+
| [i, like, this, app, it, s, awesome]    | positive  |
| [way, to, many, ads, pop, up, hate, it] | negative  |
| [niceeeee]                              | positive  |
| [i, do, not, like, the, design]         | negative  |
| [very, useful, recommended]             | positive  |
| [ugly]                                  | negative  |
+-----------------------------------------+-----------+
</code></pre>

<p>is there anyone who can provide the program code to solve this problem? i will really appreciate your help, it will help my thesis work, thank you for your attention</p>
","python, machine-learning, sentiment-analysis","<p>You can use <code>apply</code> function for this</p>

<pre><code>char_limit=4
df[df['sentence'].apply(lambda x : len("""".join(x))&gt;=char_limit)]
</code></pre>
",1,0,45,2020-02-18 09:48:34,https://stackoverflow.com/questions/60277974/how-to-drop-data-frame-row-if-there-is-less-than-4-character-in-sentence-column
Sentiment analysis with Lambda Expressions in Python,"<p>I'm trying to use TextBlob to perform sentiment analysis in Power BI. I'd like to use a lamdba expression because it seems to be substantially faster than running an iterative loop in Power BI. </p>

<p>For example, using Text Blob:</p>

<pre><code>dataset['Polarity Score'] =dataset['Comment'].apply(lambda x: TextBlob(str(x).lower()).sentiment.polarity) 
</code></pre>

<p>creates a Power BI data column named ""Polarity Score"" that has the numerical values from TextBlob.</p>

<p>I would like to do similar withe the TextBlob.classify() function. However, I don't know how to pass it the second argument of the classifier.</p>

<p>Tutorials show to create and use a classifier:</p>

<pre><code>from textblob.classifiers import NaiveBayesClassifier
from textblob import TextBlob

cl = NaiveBayesClassifier(train)
blob = TextBlob(""The beer is good. But the hangover is horrible."", classifier=cl)
blob.classify()
</code></pre>

<p>I've tried </p>

<pre><code>dataset['Polarity Class'] =dataset['Comment'].apply(lambda x: TextBlob(str(x).lower()).classify(), classifier=cl)
</code></pre>

<p>and </p>

<pre><code>dataset['Polarity Class'] =dataset['Comment'].apply(lambda x,y: TextBlob(str(x).lower()).classify(), y=cl)
</code></pre>

<p>Neither work and point to the way I'm passing the classifier. How would I pass the classifier parameter in a lambda expression?</p>
","python, sentiment-analysis, generic-lambda","<p>Simply</p>

<pre><code>cl = NaiveBayesClassifier(train)
dataset['Polarity Class'] = dataset['Comment'].apply(
    lambda x: TextBlob(str(x).lower(), classifier=cl).classify()
)
</code></pre>

<p>Or if you want to refactor the possibly confusing lambda expression out,</p>

<pre><code>cl = NaiveBayesClassifier(train)

def classify(x):
    return TextBlob(str(x).lower(), classifier=cl).classify()

dataset['Polarity Class'] = dataset['Comment'].apply(classify)
</code></pre>

<p>is equivalent.</p>
",1,0,743,2020-02-18 15:50:22,https://stackoverflow.com/questions/60284734/sentiment-analysis-with-lambda-expressions-in-python
How to Separate Dataframe Based on the Label value?,"<p>Let say I have this dataframe. </p>

<pre><code>+-----------------+-----------+
|     COMMENT     | SENTIMENT |
+-----------------+-----------+
| Good app        | Positive  |
| Bad app         | Negative  |
| Useless feature | Negative  |
| I like this app | Positive  |
+-----------------+-----------+
</code></pre>

<p>I want to split it based on the SENTIMENT column. Like this below.</p>

<pre><code>+-----------------+-----------+
|     COMMENT     | SENTIMENT |
+-----------------+-----------+
| Good app        | Positive  |
| I like this app | Positive  |
+-----------------+-----------+


+-----------------+-----------+
|     COMMENT     | SENTIMENT |
+-----------------+-----------+
| Bad app         | Negative  |
| Useless feature | Negative  |
+-----------------+-----------+

</code></pre>

<p>Anyone knows the solution in Python (Jupyter) for that case? Your help will help my thesis project. Thank you :D</p>
","python, machine-learning, jupyter-notebook, sentiment-analysis, text-classification","<p>Simply you can use <code>df1 = df[df[""SENTIMENT""]==""Positive""]</code></p>

<p>Then <code>df1</code> will have:</p>

<p><a href=""https://i.sstatic.net/vg1Ta.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vg1Ta.png"" alt=""enter image description here""></a></p>

<p><code>df2 = df[df[""SENTIMENT""]==""Negative""]</code></p>

<p>Then <code>df2</code> will have:</p>

<p><a href=""https://i.sstatic.net/s4Snz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/s4Snz.png"" alt=""enter image description here""></a></p>
",1,0,1745,2020-02-27 06:22:39,https://stackoverflow.com/questions/60427333/how-to-separate-dataframe-based-on-the-label-value
AttributeError: module &#39;sst&#39; has no attribute &#39;train_reader&#39;,"<p>I am very new to sentiment analysis. Trying to use Stanford Sentiment Treebank(sst) and ran into an error.</p>

<pre class=""lang-py prettyprint-override""><code>from nltk.tree import Tree
import os
import sst
trees = ""C:\\Users\m\data\trees""
tree, score = next(sst.train_reader(trees))
</code></pre>

<p>[Output]:</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-19-4101f90b0b16&gt; in &lt;module&gt;()
----&gt; 1 tree, score = next(sst.train_reader(trees))

AttributeError: module 'sst' has no attribute 'train_reader'
</code></pre>
","python, nlp, stanford-nlp, sentiment-analysis, sst","<p>I think you're looking for <a href=""https://github.com/JonathanRaiman/pytreebank"" rel=""nofollow noreferrer"">https://github.com/JonathanRaiman/pytreebank</a>, not <a href=""https://pypi.org/project/sst/"" rel=""nofollow noreferrer"">https://pypi.org/project/sst/</a>.  </p>

<p>On the python side, that error is pretty clear. Once you import the right package, though, I'm not sure I saw <code>train_reader</code> but I could be wrong.</p>

<p>UPDATE:
I'm not entirely sure why you're running into the 'sst' not having the attribute train_reader. Make sure you didn't accidentally install the 'sst' package if you're using conda. It looks like the 'sst' is referring to a privately created module and that one <strong>should work.</strong></p>

<p>I got your import working but what I did was I:</p>

<ol>
<li>Installed everything specified in the <code>requirements.txt</code> file.</li>
<li><code>import sst</code> was still giving me an error so I installed nltk and sklearn to resolve that issue. (fyi, im not using conda. im just using pip and virtualenv for my own private package settings. i ran <code>pip install nltk</code> and <code>pip install sklearn</code>)</li>
<li>At this point, <code>import sst</code> worked for me. </li>
</ol>
",1,0,280,2020-03-03 16:33:34,https://stackoverflow.com/questions/60511708/attributeerror-module-sst-has-no-attribute-train-reader
How can i tokenize all rows in a specific column from a csv file using Python?,"<p>I'm doing a sentiments analysis using Python (I'm still a rookie with this specific programming language). I have some Twitter data in a csv file that I need to pre-process before doing the real analysis. First of all I need to tokenize the text from a specific column, in my case the second or col B. I found some suggestions how to do the tokenization but not to pick the specific col. Anyone who has experience with this? </p>

<p>I tried this code, which seems to work for all columns, but how can I isolate it to the second col?</p>

<pre><code>import csv
import nltk
from nltk import word_tokenize 

with open('TwitterData.csv', 'r') as csvfile:
   reader = csv.DictReader(csvfile)
   for row in reader:
       print(row)
</code></pre>

<p>Any suggestions to modules and code that works for pre-processing to sentiments analysis?</p>

<p>Thanks a lot!</p>
","python, pycharm, spyder, tokenize, sentiment-analysis","<p>I can highly recommend you the scikit-learn documentation and modules, especially the part about ""Working with Text Data"": <a href=""https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></p>

<p>There they also have a section about sentiment analysis: <a href=""https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#exercise-2-sentiment-analysis-on-movie-reviews"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#exercise-2-sentiment-analysis-on-movie-reviews</a></p>

<p>If you need more specific help with your code, it is alway best to provide a ""minimal reproducable example"": <a href=""https://stackoverflow.com/help/minimal-reproducible-example"">https://stackoverflow.com/help/minimal-reproducible-example</a>
This way, others can help you better with a specific issue you are facing.</p>

<p>I hope that helps :)</p>
",1,-1,970,2020-03-12 13:29:03,https://stackoverflow.com/questions/60655156/how-can-i-tokenize-all-rows-in-a-specific-column-from-a-csv-file-using-python
Aspect Based Sentiment Analysis Classifier - techniques on how to return unknown from a classifier?,"<p>I Created an <strong>Aspect Based Sentiment Analysis Classifier</strong>.</p>

<p>i need to return answer with very high certenty ,
so i want to return <strong>unknown</strong></p>

<p>in case i am not sure about the answer
my model return for each label a percent and all 3 add up to 1
for example </p>

<p><a href=""https://i.sstatic.net/ce8Fm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ce8Fm.png"" alt=""enter image description here""></a></p>

<p>i try the threshold in the <strong>ROC curve point</strong>  for each label against all 3 and also the <strong>precision</strong> point of each label</p>

<p><a href=""https://i.sstatic.net/Xi47M.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Xi47M.png"" alt=""enter image description here""></a></p>

<p>for now i am  just returning the max between all 3 . </p>

<p>attached link to 1172 tagged examples</p>

<p><a href=""https://github.com/ntedgi/bert-sentiment/blob/master/decision_maker_weka.csv"" rel=""nofollow noreferrer"">https://github.com/ntedgi/bert-sentiment/blob/master/decision_maker_weka.csv</a></p>

<p>thanks</p>
","decision-tree, sentiment-analysis","<p>You could set a threshold and after getting the probabilities, check if the percent of probability is below your threshold, mark it as unknown. </p>
",1,0,74,2020-03-12 14:14:38,https://stackoverflow.com/questions/60655943/aspect-based-sentiment-analysis-classifier-techniques-on-how-to-return-unknown
ValueError: dimension mismatch While Predicting New Values Sentiment Analysis,"<p>I am relatively new to the machine learning subject. I am trying to do sentiment analysis prediction. </p>

<p>Type column includes the sentiment of the tweet(pos, neg or neutral as 0,1 and 2). Tweet column includes the tweets. </p>

<p>I am trying to predict new set of tweets's sentiments as 0,1 and 2.</p>

<p>When I wrote the code given here I got dimension mismatch error.</p>

<pre><code>import pandas as pd
train_tweets = pd.read_csv(""tweets_type.csv"")
from sklearn.model_selection import train_test_split

y = train_tweets.Type
X= train_tweets.Tweet

train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=1)

from sklearn.feature_extraction.text import CountVectorizer

vect = CountVectorizer()

vect.fit(train_X)
train_X_dtm = vect.transform(train_X)

test_X_dtm = vect.transform(test_X)
test_X_dtm

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

%time nb.fit(train_X_dtm, train_y)

# make class predictions for X_test_dtm
y_pred_class = nb.predict(test_X_dtm)

# calculate accuracy of class predictions
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
metrics.accuracy_score(test_y, y_pred_class)

march_tweets = pd.read_csv(""march_data.csv"")
X=march_tweets.Tweet
vect.fit(X)
train_new_dtm = vect.transform(X)

new_pred_class = nb.predict(train_new_dtm)
</code></pre>

<p>The error I am getting is here:</p>

<p><a href=""https://i.sstatic.net/nVXhh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nVXhh.png"" alt=""enter image description here""></a></p>

<p>Would be so glad if you could help me.</p>
","python, machine-learning, twitter, sentiment-analysis, naivebayes","<p>It seems I made a mistake fitting X after I already fitted train_X. I found out there is no use of doing that repeatedly once you the model is fitted. So what I did is I removed this line and it worked perfectly.</p>

<pre><code>vect.fit(X)
</code></pre>
",0,0,227,2020-03-14 19:03:18,https://stackoverflow.com/questions/60686336/valueerror-dimension-mismatch-while-predicting-new-values-sentiment-analysis
Tag of Google news title for beautiful soup,"<p>I am trying to extract the result of a search from Google news (vaccine for example) and provide some sentiment analysis based on the headline collected.</p>

<p>So far, I can't seem to find the correct tag to collect the headlines.</p>

<p>Here is my code:</p>

<pre><code>from textblob import TextBlob
import requests
from bs4 import BeautifulSoup

class Analysis:
    def __init__(self, term):
        self.term = term
        self.subjectivity = 0
        self.sentiment = 0
        self.url = 'https://www.google.com/search?q={0}&amp;source=lnms&amp;tbm=nws'.format(self.term)

    def run (self):
        response = requests.get(self.url)
        print(response.text)
        soup = BeautifulSoup(response.text, 'html.parser')
        headline_results = soup.find_all('div', class_=""phYMDf nDgy9d"")
        for h in headline_results:
            blob = TextBlob(h.get_text())
            self.sentiment += blob.sentiment.polarity / len(headline_results)
            self.subjectivity += blob.sentiment.subjectivity / len(headline_results)
a = Analysis('Vaccine')
a.run()
print(a.term, 'Subjectivity: ', a.subjectivity, 'Sentiment: ' , a.sentiment)
</code></pre>

<p>The result are always 0 for the sentiment and 0 for the subjectivity. I feel like the issue is with the class_=""phYMDf nDgy9d"".</p>
","python, beautifulsoup, sentiment-analysis","<p>If you browse into that link, you are going to see the finished state of page but <code>requests.get</code> does not exeute or load any more data other than the page you request. Luckily there is some data and you can scrape that. I suggest you to use html prettifier services like <a href=""https://codebeautify.org/htmlviewer/"" rel=""nofollow noreferrer"">codebeautify</a> to get better understanding about what the page structure is.</p>

<p>Also if you see classes like <code>phYMDf nDgy9d</code> be sure to avoid finding with them. They are minified versions of classes so at any moment if they change a part of the CSS code, the class you are looking for is going to get a new name.</p>

<p>What I did is probably overkill but, I managed to dig down to scrape specific parts and your code works now.</p>

<p><a href=""https://i.sstatic.net/g7KLX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/g7KLX.png"" alt=""enter image description here""></a></p>

<p>When you look at the prettier version of requested html file, necessary contents are in a div with an id of <code>main</code> shown above. Then it's children are starting with a div element Google Search, continuing with a <code>style</code> element and after one empty div element, there are post div elements. The last two elements in that children list are <code>footer</code> and <code>script</code> elements. We can cut these off with <code>[3:-2]</code> and then under that tree we have pure data (pretty much). If you check the remaining part of the code after the <code>posts</code> variable, you can understand it I think.</p>

<p>Here is the code:</p>

<pre class=""lang-py prettyprint-override""><code>from textblob import TextBlob
import requests, re
from bs4 import BeautifulSoup
from pprint import pprint

class Analysis:
    def __init__(self, term):
        self.term = term
        self.subjectivity = 0
        self.sentiment = 0
        self.url = 'https://www.google.com/search?q={0}&amp;source=lnms&amp;tbm=nws'.format(self.term)

    def run (self):
        response = requests.get(self.url)
        #print(response.text)
        soup = BeautifulSoup(response.text, 'html.parser')
        mainDiv = soup.find(""div"", {""id"": ""main""})
        posts = [i for i in mainDiv.children][3:-2]
        news = []
        for post in posts:
            reg = re.compile(r""^/url.*"")
            cursor = post.findAll(""a"", {""href"": reg})
            postData = {}
            postData[""headline""] = cursor[0].find(""div"").get_text()
            postData[""source""] = cursor[0].findAll(""div"")[1].get_text()
            postData[""timeAgo""] = cursor[1].next_sibling.find(""span"").get_text()
            postData[""description""] = cursor[1].next_sibling.find(""span"").parent.get_text().split(""· "")[1]
            news.append(postData)
        pprint(news)
        for h in news:
            blob = TextBlob(h[""headline""] + "" ""+ h[""description""])
            self.sentiment += blob.sentiment.polarity / len(news)
            self.subjectivity += blob.sentiment.subjectivity / len(news)
a = Analysis('Vaccine')
a.run()

print(a.term, 'Subjectivity: ', a.subjectivity, 'Sentiment: ' , a.sentiment)
</code></pre>

<p><strong>A few outputs:</strong></p>

<pre><code>[{'description': 'It comes after US health officials said last week they had '
                 'started a trial to evaluate a possible vaccine in Seattle. '
                 'The Chinese effort began on...',
  'headline': 'China embarks on clinical trial for virus vaccine',
  'source': 'The Star Online',
  'timeAgo': '5 saat önce'},
 {'description': 'Hanneke Schuitemaker, who is leading a team working on a '
                 'Covid-19 vaccine, tells of the latest developments and what '
                 'needs to be done now.',
  'headline': 'Vaccine scientist: ‘Everything is so new in dealing with this '
              'coronavirus’',
  'source': 'The Guardian',
  'timeAgo': '20 saat önce'},
 .
 .
 .
Vaccine Subjectivity:  0.34522727272727277 Sentiment:  0.14404040404040402
</code></pre>

<pre><code>[{'description': '10 Cool Tech Gadgets To Survive Working From Home. From '
                 'Wi-Fi and cell phone signal boosters, to noise-cancelling '
                 'headphones and gadgets...',
  'headline': '10 Cool Tech Gadgets To Survive Working From Home',
  'source': 'CRN',
  'timeAgo': '2 gün önce'},
 {'description': 'Over the past few years, smart home products have dominated '
                 'the gadget space, with goods ranging from innovative updates '
                 'to the items we...',
  'headline': '6 Smart Home Gadgets That Are Actually Worth Owning',
  'source': 'Entrepreneur',
  'timeAgo': '2 hafta önce'},
 .
 .
 .
Home Gadgets Subjectivity:  0.48007305194805205 Sentiment:  0.3114683441558441
</code></pre>

<p>I used headlines and description data to do the operations but you can play with that if you want. You have the data now :)</p>
",2,2,720,2020-03-21 20:26:05,https://stackoverflow.com/questions/60792898/tag-of-google-news-title-for-beautiful-soup
How to do lemmatization using NLTK or pywsd,"<p>I know that my explaination is rather long but I found it necessary. Hopefully someone is patient and a helpful soul :)
I'm doing a sentiment analysis project atm and I'm stuck i the pre-process part. I did the import of the csv file, made it into a dataframe, transformed the variables/columns into the right data types. Then I did the tokenization like this, where i choose the variable I wanted to tokenize (tweet content) in the dataframe (df_tweet1):</p>

<pre><code># Tokenization
tknzr = TweetTokenizer()
tokenized_sents = [tknzr.tokenize(str(i)) for i in df_tweet1['Tweet Content']]
for i in tokenized_sents:
    print(i)
</code></pre>

<p>The output is a list of list with words (tokens).</p>

<p>Then I perform stop word removal:</p>

<pre><code># Stop word removal
from nltk.corpus import stopwords

stop_words = set(stopwords.words(""english""))
#add words that aren't in the NLTK stopwords list
new_stopwords = ['!', ',', ':', '&amp;', '%', '.', '’']
new_stopwords_list = stop_words.union(new_stopwords)

clean_sents = []
for m in tokenized_sents:
    stop_m = [i for i in m if str(i).lower() not in new_stopwords_list]
    clean_sents.append(stop_m)
</code></pre>

<p>The output is the same but without stop words</p>

<p>The next two steps are confusing to me (part-of-speech tagging and lemmatization). I tried two things:</p>

<p>1) Convert the previous output into a list of strings </p>

<pre><code>new_test = [' '.join(x) for x in clean_sents]
</code></pre>

<p>since I thought that would enable me to use this code to do both steps in one:</p>

<pre><code>from pywsd.utils import lemmatize_sentence

text = new_test
lemm_text = lemmatize_sentence(text, keepWordPOS=True)
</code></pre>

<p>I got the this error: 
TypeError: expected string or bytes-like object</p>

<p>2) Perform POS and lemmatizaion seperately. First POS using clean_sents as input:</p>

<pre><code># PART-OF-SPEECH        
def process_content(clean_sents):
    try:
        tagged_list = []  
        for lst in clean_sents[:500]: 
            for item in lst:
                words = nltk.word_tokenize(item)
                tagged = nltk.pos_tag(words)
                tagged_list.append(tagged)
        return tagged_list

    except Exception as e:
        print(str(e))

output_POS_clean_sents = process_content(clean_sents)
</code></pre>

<p>The output is a list of lists with words with a tag attached
Then I want to lemmatize this output, but how? I tried two modules, but both gave me error:</p>

<pre><code>from pywsd.utils import lemmatize_sentence

lemmatized= [[lemmatize_sentence(output_POS_clean_sents) for word in s]
              for s in output_POS_clean_sents]

# AND

from nltk.stem.wordnet import WordNetLemmatizer

lmtzr = WordNetLemmatizer()
lemmatized = [[lmtzr.lemmatize(word) for word in s]
              for s in output_POS_clean_sents]
print(lemmatized)
</code></pre>

<p>The errors were respectively:</p>

<p>TypeError: expected string or bytes-like object</p>

<p>AttributeError: 'tuple' object has no attribute 'endswith'</p>
","python, nltk, sentiment-analysis, lemmatization, part-of-speech","<p>If you're using a dataframe I suggest you to store the pre processing steps results in a new column. In this way you can always check the output, and you can always create a list of lists to use as an input for a model in a line of code afterwords. Another advantage of this approach is that you can easily visualise the line of preprocessing and add other steps wherever you need without getting confused.</p>

<p>Regarding your code, It can be optimised (for example you could perform stop words removal and tokenisation at the same time) and I see a bit of confusion about the steps you performed. For example you performe multiple times lemmatisation, using also different libraries, and there is no point in doing that. In my opinion nltk works just fine, personally I use other libraries to preprocess tweets only to deal with emojis, urls and hashtags, all stuff specifically related to tweets. </p>

<pre><code># I won't write all the imports, you get them from your code
# define new column to store the processed tweets
df_tweet1['Tweet Content Clean'] = pd.Series(index=df_tweet1.index)

tknzr = TweetTokenizer()
lmtzr = WordNetLemmatizer()

stop_words = set(stopwords.words(""english""))
new_stopwords = ['!', ',', ':', '&amp;', '%', '.', '’']
new_stopwords_list = stop_words.union(new_stopwords)

# iterate through each tweet
for ind, row in df_tweet1.iterrows():

    # get initial tweet: ['This is the initial tweet']
    tweet = row['Tweet Content']

    # tokenisation, stopwords removal and lemmatisation all at once
    # out: ['initial', 'tweet']
    tweet = [lmtzr.lemmatize(i) for i in tknzr.tokenize(tweet) if i.lower() not in new_stopwords_list]

    # pos tag, no need to lemmatise again after.
    # out: [('initial', 'JJ'), ('tweet', 'NN')]
    tweet = nltk.pos_tag(tweet)

    # save processed tweet into the new column
    df_tweet1.loc[ind, 'Tweet Content Clean'] = tweet
</code></pre>

<p>So on overall all you need are 4 lines, one for getting the tweet string, two to preprocess the text, another one to store the tweet. You can add extra processing step paying attention to the output of each step (for example tokenisation return a list of strings, pos tagging return a list of tuples, reason why you are getting troubles). </p>

<p>If you want then you can create a list of lists containing all tweet in the dataframe:</p>

<pre><code># out: [[('initial', 'JJ'), ('tweet', 'NN')], [second tweet], [third tweet]]
all_tweets = [tweet for tweet in df_tweet1['Tweet Content Clean']]
</code></pre>
",1,1,822,2020-03-26 16:05:45,https://stackoverflow.com/questions/60871375/how-to-do-lemmatization-using-nltk-or-pywsd
what is the error in this code using dataframe and matplotlib,"<p>I have a python that read from CSV file and converts it to dataframe using pandas then using matplotlib  it plots a histogram. the first task is correct it read and write to and from CSV file.</p>

<p>the csv file fileds are:
date"",""user_loc"",""message"",""full_name"",""country"",""country_code"",""predictions"",""word count""</p>

<p>BUT the task of plotting is displaying the below error.</p>

<p><strong>Error:</strong></p>

<pre><code> --------------------------------------------------------------------------- IndexError                                Traceback (most recent call
 last) &lt;ipython-input-37-5bc3925ff988&gt; in &lt;module&gt;
       1 #plot word count distribution for both positive and negative sentiment
 ----&gt; 2 x= tweet_preds[""word count""][tweet_preds.predictions ==1]
       3 y= tweet_preds[""word count""][tweet_preds.predictions ==0]
       4 plt.figure(figsize=(12,6))
       5 plt.xlim(0,45)

 IndexError: only integers, slices (`:`), ellipsis (`...`),
 numpy.newaxis (`None`) and integer or boolean arrays are valid indices
</code></pre>

<p><strong>Code:</strong></p>

<pre><code>    # create of dataframe:
    #create column names
    col_names = [""date"",""user_loc"",""followers"",""friends"",""message"",""bbox_coords"",
                 ""full_name"",""country"",""country_code"",""place_type""]
    #read csv
    df_twtr = pd.read_csv(""F:\AIenv\sentiment_analysis\paul_ryan_twitter.csv"",names = col_names)
    #check head
    df_twtr=df_twtr.dropna()
    df_twtr = df_twtr.reset_index(drop=True)
    df_twtr.head()


# run predictions on twitter data
tweet_preds = model_NB.predict(df_twtr['message'])

# append predictions to dataframe
df_tweet_preds = df_twtr.copy()
df_tweet_preds['predictions'] = tweet_preds
df_tweet_preds.shape

    df_tweet_preds = pd.DataFrame(df_tweet_preds,columns = [""date"",""user_loc"",""message"",""full_name"",""country"",""country_code"",""predictions"",""word count""])
    df_tweet_preds = df_tweet_preds.drop([""user_loc"",""country"",""country_code""],axis=1)
    df_tweet_preds_to_csv = df_tweet_preds.to_csv(r'F:\AIenv\sentiment_analysis\export_dataframe.csv', index = False, header=True)

     #plot word count distribution for both positive and negative sentiment
    x= tweet_preds[""word count""][tweet_preds.predictions ==1]
    y= tweet_preds[""word count""][tweet_preds.predictions ==0]
    plt.figure(figsize=(12,6))
    plt.xlim(0,45)
    plt.xlabel(""word count"")
    plt.ylabel(""frequency"")
    g = plt.hist([x,y],color=[""r"",""b""],alpha=0.5,label=[""positive"",""negative""])
    plt.legend(loc=""upper right"")
</code></pre>
","python, pandas, matplotlib, sentiment-analysis","<p>It is not a dataframe, it is a numpy array. The result of your predict() method is a numpy array which cannot be indexed like you are trying to. Why not just use the dataframe that you append the prediction to, 'df_tweet_preds['predictions'] = tweet_preds'. Then you can do all sorts of indexing.</p>
",1,0,102,2020-03-30 11:08:28,https://stackoverflow.com/questions/60929241/what-is-the-error-in-this-code-using-dataframe-and-matplotlib
Adding column of values to pandas DataFrame,"<p>I'm doing a simple sentiment analysis and am stuck on something that I feel is very simple. I'm trying to add an new column with a set of values, in this example <code>compound</code> values. But after the for loop iterates it adds the same value for all the rows rather than a value for each iteration. The <code>compound</code> values are the last column in the DataFrame. There should be a quick fix. thanks!</p>

<pre><code>for i, row in real.iterrows():
   real['compound'] = sid.polarity_scores(real['title'][i])['compound']


title   text    subject date                                                        compound
0   As U.S. budget fight looms, Republicans flip t...   WASHINGTON (Reuters) - The head of a conservat...   politicsNews    December 31, 2017   0.2263
1   U.S. military to accept transgender recruits o...   WASHINGTON (Reuters) - Transgender people will...   politicsNews    December 29, 2017   0.2263
2   Senior U.S. Republican senator: 'Let Mr. Muell...   WASHINGTON (Reuters) - The special counsel inv...   politicsNews    December 31, 2017   0.2263
3   FBI Russia probe helped by Australian diplomat...   WASHINGTON (Reuters) - Trump campaign adviser ...   politicsNews    December 30, 2017   0.2263
4   Trump wants Postal Service to charge 'much mor...   SEATTLE/WASHINGTON (Reuters) - President Donal...   politicsNews    December 29, 2017   0.2263
</code></pre>

<p><a href=""https://i.sstatic.net/iCsKS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iCsKS.png"" alt=""enter image description here""></a></p>
","python, pandas, nlp, sentiment-analysis","<p>IIUC:</p>

<pre><code>real['compound'] = real.apply(lambda row: sid.polarity_scores(row['title'])['compound'], axis=1)
</code></pre>
",2,1,198,2020-04-01 23:21:15,https://stackoverflow.com/questions/60981838/adding-column-of-values-to-pandas-dataframe
Adding and averaging a set of columns depending on the value of a secondary column in python,"<p>I have a dataset which has the following values:</p>

<pre><code>LabelA    PositiveA     NegativeA    LabelB    PositiveB     NegativeB    LabelC    PositiveC  NegativeC  Final_Label
  1          .60           .40         0          .30           .70         1          .9          .1         1
  0          .1            .9          0          .49           .51         0          .3          .7         0
  0          .34           .66         1          .87           .13         1          .90         .1         1
</code></pre>

<p>Final_label would be 1 if majority of Labels (LabelA, LabelB and LabelC) would be 1 and vice-versa.</p>

<p>I want to calculate a column called ""Polarity"" which has the following defination:</p>

<ol>
<li>If Final_label = 1, Polarity is the mean of all the ""PositiveA/B/C"" whose Label was also 1</li>
<li>If Final_label = 0, Polarity is the mean of all the ""NegativeA/B/C"" whose label was also 0</li>
</ol>

<p>For example in the above dataset, Polarity would have the following value:</p>

<pre><code>Polarity
.75           (adding and taking average of PositiveA and PositiveC)
.7033         (adding and taking average of NegativeA and Negativeb and NegativeC)
.885          (adding and taking average of PositiveB and PositiveC)
</code></pre>

<p>How do I implement this in python? Over here I have shown 3 columns, in my dataset I have 7 Label columns. </p>
","python, pandas, numpy, dataframe, sentiment-analysis","<p>Here's my approach with <code>where</code> and <code>mask</code>:</p>

<pre><code># filter the labels, positives, negatives:
labels = df.filter(regex='Label\w').eq(1).values
positives = df.filter(regex='Positive\w')
negatives = df.filter(regex='Negative\w')

# output
df['Polarity'] = np.where(df['Final_Label'], 
                          positives.where(labels).mean(axis=1), 
                          negatives.mask(labels).mean(axis=1)
                         )

print(df['Polarity'])
</code></pre>

<p>Output:</p>

<pre><code>0    0.750000
1    0.703333
2    0.885000
Name: Polarity, dtype: float64
</code></pre>
",2,1,37,2020-04-02 15:21:50,https://stackoverflow.com/questions/60995066/adding-and-averaging-a-set-of-columns-depending-on-the-value-of-a-secondary-colu
Sentiment Analysis does not display correct results,"<pre><code>def sentiment(polarity):
   if blob.sentiment.polarity &lt; 0:
     print(""Negative"")
   elif blob.sentiment.polarity &gt; 0:
     print(""Positive"")
   else:
     print(""Neutral"")
</code></pre>

<p>Above is defining polarity</p>

<pre class=""lang-py prettyprint-override""><code>f = open(""data3.txt"", ""r"")
for x in f:
    print(x)
print(blob.sentiment)
sentiment(blob.sentiment.polarity)
</code></pre>

<p>Above is reading line by line the txt file as well as printing the sentence, sentiment and polarity</p>

<p>Unfortunately, when running the file, it shows .5 polarity for every single sentence. I am unsure how to fix it. </p>
","python, python-3.x, nltk, sentiment-analysis","<p><code>f = open(""data3.txt"", ""r"")
for x in f:
    print(x)
    print(blob.sentiment)
    sentiment(blob.sentiment.polarity)</code></p>

<p>Based on your code here, it doesn't seem like you're giving TextBlob your string input in each iteration.
I haven't worked extensively with blob but from my understanding, each blob instance is unique and you need to make a new blob for each line. So instead of the above it should be something like this:</p>

<p><code>f = open(""data3.txt"", ""r"")
for x in f:
    blob=TextBlob(x)
    print(x)
    print(blob.sentiment)
    sentiment(blob.sentiment.polarity)</code></p>

<p>I hope that helps!</p>
",2,0,110,2020-04-09 17:03:56,https://stackoverflow.com/questions/61126424/sentiment-analysis-does-not-display-correct-results
Python: IF statement consisting of data frame and list,"<p>I am very new to python and require help.
I have a list of keywords which was obtained from a data frame as follows:
<code>key_a_list = df_key_words['words'].tolist()</code></p>

<p>I have a second data frame which consists of statements: <code>df_response['statement']</code>
I have already corrected spelling errors, tokenised and stemmed the text in the <code>df_response['statement']</code> column. 
I need to check if there are any words in the <code>key_a_list</code> that match words in the <code>df_response['statement']</code>; then I must set a counter to count the number of times a word from the <code>key_a_list</code> is present in the <code>df_response['statement']</code>. </p>

<p>Thank you for your time and help, it is greatly appreciated :) </p>

<p>This is the current code that I have but it gives me an error: ValueError: Lengths must match to compare</p>

<pre><code>count_a = 0
def count(x):
    for x in key_a_list:
        if key_a_list == df_response['statement']:
            count_a = count_a + 1      
    return count_a

df_response['statement'] = df_response['statement'].apply(lambda x: "" "".join([count(x) for word in x.split()]))
</code></pre>

<p>the key_a_list consists of words like: ['think', 'college', 'education', 'help', 'better', 'prepare', 'career', 'chosen', 'eventually', 'enable', 'enter', 'job', 'market', 'field', 'like', 'make', 'choice', 'social', 'orientation', 'believe', 'additional', 'year', 'improve', 'competence', 'worker', 'prove', 'capable', 'completing', 'degree', 'rich', 'succeed', 'feel', 'important', 'show', 'intelligent', 'person', 'order', 'salary', 'later', 'on', 'want', 'the', 'good', 'life', 'study', 'highschool', 'actuary', 'find', 'highpaying', 'obtain', 'prestigious']</p>

<p>The df_response['statement'] looks as follows:</p>

<pre><code>                  statement
0                  parent said
1         want make difference
2                    dont know
3                         rich
4               go career want
5                      actuary
6                  social life
7             expected society
</code></pre>

<p>where the desired output of df_response is:</p>

<pre><code>                  statement         count_a
0                  parent said       0
1         want make difference       2
2                    dont know       0
3                         rich       1
4               go career want       2
5                      actuary       1
6                  social life       2
7             expected society       0
</code></pre>
","python, for-loop, if-statement, jupyter-notebook, sentiment-analysis","<p>There's no reason to define your own function or use <code>apply()</code> here. Thankfully, pandas series have a handy built-in function: <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.count.html"" rel=""nofollow noreferrer"">series.str.count()</a>. Familiarizing yourself with the pandas <code>series.str...</code> methods can save you a ton of work! Just use the pipe character (<code>|</code>) to <code>join</code> your list to make it into a regex pattern, then <code>count</code> it</p>

<pre><code>df['statement'].str.count('|'.join(key_a_list))

0    0
1    2
2    1
3    1
4    2
5    1
6    2
7    0

df['count_a']=df['statement'].str.count('|'.join(key_a_list))

df

    statement               count_a
0   parent said             0
1   want make difference    2
2   dont know               1
3   rich                    1
4   go career want          2
5   actuary                 1
6   social life             2
7   expected society        0
</code></pre>
",1,0,168,2020-04-13 14:52:39,https://stackoverflow.com/questions/61190532/python-if-statement-consisting-of-data-frame-and-list
Constant Training Loss and Validation Loss,"<p>I am running a RNN model with Pytorch library to do sentiment analysis on movie review, but somehow the training loss and validation loss remained constant throughout the training. I have looked up different online sources but still stuck. </p>

<p>Can someone please help and take a look at my code?</p>

<p>Some parameters are specified by the assignment:</p>

<pre><code>embedding_dim = 64

n_layers = 1

n_hidden = 128

dropout = 0.5

batch_size = 32
</code></pre>

<p>My main code</p>

<pre class=""lang-py prettyprint-override""><code>txt_field = data.Field(tokenize=word_tokenize, lower=True, include_lengths=True, batch_first=True)
label_field = data.Field(sequential=False, use_vocab=False, batch_first=True)

train = data.TabularDataset(path=part2_filepath+""train_Copy.csv"", format='csv',
                            fields=[('label', label_field), ('text', txt_field)], skip_header=True)
validation = data.TabularDataset(path=part2_filepath+""validation_Copy.csv"", format='csv',
                            fields=[('label', label_field), ('text', txt_field)], skip_header=True)

txt_field.build_vocab(train, min_freq=5)
label_field.build_vocab(train, min_freq=2)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
train_iter, valid_iter, test_iter = data.BucketIterator.splits(
    (train, validation, test),
    batch_size=32,
    sort_key=lambda x: len(x.text),
    sort_within_batch=True,
    device=device)

n_vocab = len(txt_field.vocab)
embedding_dim = 64
n_hidden = 128
n_layers = 1
dropout = 0.5

model = Text_RNN(n_vocab, embedding_dim, n_hidden, n_layers, dropout)

optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
criterion = torch.nn.BCELoss().to(device)

N_EPOCHS = 15
best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
    train_loss, train_acc = RNN_train(model, train_iter, optimizer, criterion)
    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)
</code></pre>

<p>My Model</p>

<pre class=""lang-py prettyprint-override""><code>class Text_RNN(nn.Module):
    def __init__(self, n_vocab, embedding_dim, n_hidden, n_layers, dropout):
        super(Text_RNN, self).__init__()
        self.n_layers = n_layers
        self.n_hidden = n_hidden
        self.emb = nn.Embedding(n_vocab, embedding_dim)
        self.rnn = nn.RNN(
            input_size=embedding_dim,
            hidden_size=n_hidden,
            num_layers=n_layers,
            dropout=dropout,
            batch_first=True
        )
        self.sigmoid = nn.Sigmoid()
        self.linear = nn.Linear(n_hidden, 2)

    def forward(self, sent, sent_len):
        sent_emb = self.emb(sent)
        outputs, hidden = self.rnn(sent_emb)
        prob = self.sigmoid(self.linear(hidden.squeeze(0)))

        return prob
</code></pre>

<p>The training function</p>

<pre class=""lang-py prettyprint-override""><code>def RNN_train(model, iterator, optimizer, criterion):
    epoch_loss = 0
    epoch_acc = 0
    model.train()
    for batch in iterator:
        text, text_lengths = batch.text
        predictions = model(text, text_lengths)
        batch.label = batch.label.type(torch.FloatTensor).squeeze()
        predictions = torch.max(predictions.data, 1).indices.type(torch.FloatTensor)
        loss = criterion(predictions, batch.label)
        loss.requires_grad = True
        acc = binary_accuracy(predictions, batch.label)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator)
</code></pre>

<p>The output I run on 10 testing reviews + 5 validation reviews</p>

<pre><code>Epoch [1/15]:   Train Loss: 15.351 | Train Acc: 44.44%  Val. Loss: 11.052 |  Val. Acc: 60.00%
Epoch [2/15]:   Train Loss: 15.351 | Train Acc: 44.44%  Val. Loss: 11.052 |  Val. Acc: 60.00%
Epoch [3/15]:   Train Loss: 15.351 | Train Acc: 44.44%  Val. Loss: 11.052 |  Val. Acc: 60.00%
Epoch [4/15]:   Train Loss: 15.351 | Train Acc: 44.44%  Val. Loss: 11.052 |  Val. Acc: 60.00%
...
</code></pre>

<p>Appreciate if someone can point me to the right direction, I believe is something with the training code, since for most parts I follow this article:
<a href=""https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"" rel=""nofollow noreferrer"">https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/</a></p>
","machine-learning, pytorch, recurrent-neural-network, sentiment-analysis","<p>In your training loop you are using the indices from the max operation, which is not differentiable, so you cannot track gradients through it. Because it is not differentiable, everything afterwards does not track the gradients either. Calling 
<code>loss.backward()</code> would fail.</p>

<pre class=""lang-py prettyprint-override""><code># The indices of the max operation are not differentiable
predictions = torch.max(predictions.data, 1).indices.type(torch.FloatTensor)
loss = criterion(predictions, batch.label)
# Setting requires_grad to True to make .backward() work, although incorrectly.
loss.requires_grad = True
</code></pre>

<p>Presumably you wanted to fix that by setting <code>requires_grad</code>, but that does not do what you expect, because no gradients are propagated to your model, since the only thing in your computational graph would be the loss itself, and there is nowhere to go from there.</p>

<p>You used the indices to get either 0 or 1, since the output of your model is essentially two classes, and you wanted the one with the higher probability. For the Binary Cross Entropy loss, you only need one class that has a value between 0 and 1 (continuous), which you get by applying the sigmoid function.</p>

<p>So you need change the output channels of the final linear layer to 1:</p>

<pre class=""lang-py prettyprint-override""><code>self.linear = nn.Linear(n_hidden, 1)
</code></pre>

<p>and in your training loop you can remove the <code>torch.max</code> call and also the <code>requires_grad</code>.</p>

<pre class=""lang-py prettyprint-override""><code># Squeeze the model's output to get rid of the single class dimension
predictions = model(text, text_lengths).squeeze()
batch.label = batch.label.type(torch.FloatTensor).squeeze()
loss = criterion(predictions, batch.label)
acc = binary_accuracy(predictions, batch.label)
optimizer.zero_grad()
loss.backward()
</code></pre>

<p>Since you have only 1 class at the end, an actual prediction would be either 0 or 1 (nothing in between), to achieve that you can simply use 0.5 as the threshold, so everything below is considered a 0 and everything above is considered a 1. If you are using the <code>binary_accuracy</code> function of the article you were following, that is done automatically for you. They do that by rounding it with <code>torch.round</code>.</p>
",1,1,3219,2020-04-17 16:39:56,https://stackoverflow.com/questions/61276500/constant-training-loss-and-validation-loss
How to do Topic Detection in Unsupervised Aspect Based Sentiment Analysis,"<p>I want to make an ABSA using Python where the sentiment of pre-defined aspects (e.g. delivery, quality, service) is analyzed from online reviews. I want to do it unsupervised because this will save me from manually labeling reviews and I can analyze a lot more review data (looking at around 100k reviews). Therefore, my datasets consists of only reviews and no ratings. I would like to have a model that can first detect the aspect category and then assign the sentiment polarity. E.g. when the review says ""The shipment went smoothly, but the product is broken"" I want the model to assign the word ""shipment"" to the aspect category ""delivery"" and ""smoothly"" relates to a positive sentiment. </p>

<p>I have searched for approaches to take and I would like to know if anyone has experience with this and could guide me into a direction that could help me. It will be highly appreciated!</p>
","python, nlp, word2vec, sentiment-analysis","<blockquote>
  <p>Aspect Based Sentiment Analysis (ABSA), where the task is first to
  extract aspects or features of an entity (i.e. Aspect Term Extraction
  or ATE1 ) from a given text, and second to determine the sentiment
  polarity (SP), if any, towards each aspect of that entity. The
  importance of ABSA led to the creation of the ABSA task </p>
  
  <p>B-LSTM &amp; CRF classifier will be used for feature extraction and aspect
  term detection for both supervised and unsupervised ATE.</p>
</blockquote>

<p><a href=""https://i.sstatic.net/F4V7D.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F4V7D.png"" alt=""enter image description here""></a></p>

<p><a href=""https://www.researchgate.net/profile/Andreea_Hossmann/publication/319875533_Unsupervised_Aspect_Term_Extraction_with_B-LSTM_and_CRF_using_Automatically_Labelled_Datasets/links/5a3436a70f7e9b10d842b0eb/Unsupervised-Aspect-Term-Extraction-with-B-LSTM-and-CRF-using-Automatically-Labelled-Datasets.pdf"" rel=""nofollow noreferrer"">https://www.researchgate.net/profile/Andreea_Hossmann/publication/319875533_Unsupervised_Aspect_Term_Extraction_with_B-LSTM_and_CRF_using_Automatically_Labelled_Datasets/links/5a3436a70f7e9b10d842b0eb/Unsupervised-Aspect-Term-Extraction-with-B-LSTM-and-CRF-using-Automatically-Labelled-Datasets.pdf</a></p>

<p><a href=""https://github.com/songyouwei/ABSA-PyTorch/blob/master/infer_example.py"" rel=""nofollow noreferrer"">https://github.com/songyouwei/ABSA-PyTorch/blob/master/infer_example.py</a></p>

<pre><code># -*- coding: utf-8 -*-
# file: infer.py
# author: songyouwei &lt;youwei0314@gmail.com&gt;
# Copyright (C) 2019. All Rights Reserved.

import torch
import torch.nn.functional as F
import argparse

from data_utils import build_tokenizer, build_embedding_matrix
from models import IAN, MemNet, ATAE_LSTM, AOA


class Inferer:
    """"""A simple inference example""""""
    def __init__(self, opt):
        self.opt = opt
        self.tokenizer = build_tokenizer(
            fnames=[opt.dataset_file['train'], opt.dataset_file['test']],
            max_seq_len=opt.max_seq_len,
            dat_fname='{0}_tokenizer.dat'.format(opt.dataset))
        embedding_matrix = build_embedding_matrix(
            word2idx=self.tokenizer.word2idx,
            embed_dim=opt.embed_dim,
            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(opt.embed_dim), opt.dataset))
        self.model = opt.model_class(embedding_matrix, opt)
        print('loading model {0} ...'.format(opt.model_name))
        self.model.load_state_dict(torch.load(opt.state_dict_path))
        self.model = self.model.to(opt.device)
        # switch model to evaluation mode
        self.model.eval()
        torch.autograd.set_grad_enabled(False)

    def evaluate(self, raw_texts):
        context_seqs = [self.tokenizer.text_to_sequence(raw_text.lower().strip()) for raw_text in raw_texts]
        aspect_seqs = [self.tokenizer.text_to_sequence('null')] * len(raw_texts)
        context_indices = torch.tensor(context_seqs, dtype=torch.int64).to(self.opt.device)
        aspect_indices = torch.tensor(aspect_seqs, dtype=torch.int64).to(self.opt.device)

        t_inputs = [context_indices, aspect_indices]
        t_outputs = self.model(t_inputs)

        t_probs = F.softmax(t_outputs, dim=-1).cpu().numpy()
        return t_probs


if __name__ == '__main__':
    model_classes = {
        'atae_lstm': ATAE_LSTM,
        'ian': IAN,
        'memnet': MemNet,
        'aoa': AOA,
    }
    # set your trained models here
    model_state_dict_paths = {
        'atae_lstm': 'state_dict/atae_lstm_restaurant_acc0.7786',
        'ian': 'state_dict/ian_restaurant_acc0.7911',
        'memnet': 'state_dict/memnet_restaurant_acc0.7911',
        'aoa': 'state_dict/aoa_restaurant_acc0.8063',
    }
    class Option(object): pass
    opt = Option()
    opt.model_name = 'ian'
    opt.model_class = model_classes[opt.model_name]
    opt.dataset = 'restaurant'
    opt.dataset_file = {
        'train': './datasets/semeval14/Restaurants_Train.xml.seg',
        'test': './datasets/semeval14/Restaurants_Test_Gold.xml.seg'
    }
    opt.state_dict_path = model_state_dict_paths[opt.model_name]
    opt.embed_dim = 300
    opt.hidden_dim = 300
    opt.max_seq_len = 80
    opt.polarities_dim = 3
    opt.hops = 3
    opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    inf = Inferer(opt)
    t_probs = inf.evaluate(['happy memory', 'the service is terrible', 'just normal food'])
    print(t_probs.argmax(axis=-1) - 1)
</code></pre>
",2,1,1795,2020-04-17 17:22:20,https://stackoverflow.com/questions/61277201/how-to-do-topic-detection-in-unsupervised-aspect-based-sentiment-analysis
Feature extraction using flairNLP,"<p>I'm trying to use <a href=""https://github.com/flairNLP/flair"" rel=""nofollow noreferrer"">flair</a> for sentiment analysis, but I also need to know how much each word influenced the score of a sentence.</p>

<p>I've followed this <a href=""https://medium.com/@b.terryjack/nlp-pre-trained-sentiment-analysis-1eb52a9d742c"" rel=""nofollow noreferrer"">article</a> to predict the sentiment, but it doesn't show how to extract the features of the given sentence.
I'm assuming there is a way to do this feature extraction because of the way it's presented in that article, but I can't find it. I've tried reading the flair documentation and the code itself but didn't see a way to do so.</p>

<p>What I'm looking for is a functionality of this sort:</p>

<pre><code>import flair
text = flair.data.Sentence(&lt;string-with-sentiment&gt;)
model = flair.models.TextClassifier.load('en-sentiment')
model.predict(text)
print(s.individual_sentiments)
</code></pre>

<p>Result:</p>

<pre><code>[('i', 0.08), ('do', 0.09), ('like', 1.0), ('you', -0.32)]
</code></pre>

<p>I'm not trying to train my own model, but rather use a pre trained one like in the code example above.</p>

<p>Note: I'm not bound to flair, if a different framework with this functionality exists I'll be happy to know about it as well. I'm trying to use flair because it out performed Textblob and nltk's VADER in accuracy when I tested it.</p>
","python, nlp, sentiment-analysis, feature-extraction, flair","<p>The article actually has a link to a colab <a href=""https://colab.research.google.com/github/mohammedterry/NLP_for_ML/blob/master/Sentiment_Analysis.ipynb"" rel=""nofollow noreferrer"">notebook</a> at the bottom which I missed. It appears the way this is achieved in that article is just by classifying each word separately in addition to the whole sentence.</p>
",0,1,578,2020-04-23 14:01:22,https://stackoverflow.com/questions/61389090/feature-extraction-using-flairnlp
can i use VaderSentiment to calculate polarity and subjectivity on language other than English?,"<p>i am trying to create a nlp project that calculate the polarity and subjectivity for texts that are not English so i can use 2 tools: <strong>Vader</strong> - <strong>Textblob</strong>.</p>

<p>After i done a lot of researches i found that Vader is more efficient and accurate for social media.</p>

<p>My question is : can i add language to vader in order to calculate socres?
 or is their a package for vader like multilanguage?</p>

<p>For the project i read from csv file and import it to dataframe pandas than pre-process and clean the text  than analyse it to extract the sentiments. </p>

<p>i will appreciate any help.</p>
","python, nlp, sentiment-analysis","<p>According to the <a href=""https://github.com/cjhutto/vaderSentiment#resources-and-dataset-descriptions"" rel=""nofollow noreferrer"">documentation</a>, Vader uses two resources:</p>

<ol>
<li>a dictionary of tokens with their sentiment ratings</li>
<li>a set of syntactic rules that define the relationships between words</li>
</ol>

<p>While you can create your own resources for other languages, but the authors state that</p>

<blockquote>
  <p>Manually creating (much less, validating) a comprehensive sentiment lexicon is a labor intensive and sometimes error prone process</p>
</blockquote>

<p>So while possible, it won't be easy. The lexicon file that comes with Vader comprises just shy of 8000 entries. I don't know how easy it is to generate these: presumably there is a trade-off between quickly achieving broad coverage and accuracy of the results. Perhaps you can go for coverage first, and then gradually improve accuracy by modifying the entries accordingly.</p>

<p>The syntactic rules (from a cursory glance) mainly seem to describe adverbs, and whether they increase or decrease the sentiment. Again, this is something that would have to be adjusted, as it's hard-coded for English in the source file. It depends how different your target language is grammatically from English how easy or difficult that task will be.</p>
",0,0,641,2020-04-30 09:41:37,https://stackoverflow.com/questions/61520426/can-i-use-vadersentiment-to-calculate-polarity-and-subjectivity-on-language-othe
Arguments should have same length error in R,"<p>I'm trying to create a key-value store with the key being entities and the value being the average sentiment score of the entity in news articles.</p>

<p>I have a dataframe containing news articles and a list of entities called organizations1 identified in those news articles by a classifier. The first row of the organization1 list contains the entities identified in the article on the first row of the news_us dataframe. I'm trying to iterate through the organization list and creating a key-value store with the key being the entity name in the organization1 list and the value being the sentiment score of the news description in which the entity was mentioned. </p>

<p>I can get the sentiment scores for the entity from an article but I wanted to add them together and average the sentiment score. </p>

<pre><code>library(syuzhet)
sentiment &lt;- list()
organization1 &lt;- list(NULL, ""US"", ""Bath"", ""Animal Crossing"", ""World Health Organization"", 
    NULL, c(""Microsoft"", ""Facebook""))
news_us &lt;- structure(list(title = c(""Stocks making the biggest moves after hours: Bed Bath &amp; Beyond, JC Penney, United Airlines and more - CNBC"", 
""Los Angeles mayor says 'very difficult to see' large gatherings like concerts and sporting events until 2021 - CNN"", 
""Bed Bath &amp; Beyond shares rise as earnings top estimates, retailer plans to maintain some key investments - CNBC"", 
""6 weeks with Animal Crossing: New Horizons reveals many frustrations - VentureBeat"", 
""Timeline: How Trump And WHO Reacted At Key Moments During The Coronavirus Crisis : Goats and Soda - NPR"", 
""Michigan protesters turn out against Whitmer’s strict stay-at-home order - POLITICO""
), description = c(""Check out the companies making headlines after the bell."", 
""Los Angeles Mayor Eric Garcetti said Wednesday large gatherings like sporting events or concerts may not resume in the city before 2021 as the US grapples with mitigating the novel coronavirus pandemic."", 
""Bed Bath &amp; Beyond said that its results in 2020 \""will be unfavorably impacted\"" by the crisis, and so it will not be offering a first-quarter nor full-year outlook."", 
""Six weeks with Animal Crossing: New Horizons has helped to illuminate some of the game's shortcomings that weren't obvious in our first review."", 
""How did the president respond to key moments during the pandemic? And how did representatives of the World Health Organization respond during the same period?"", 
""Many demonstrators, some waving Trump campaign flags, ignored organizers‘ pleas to stay in their cars and flooded the streets of Lansing, the state capital.""
), name = c(""CNBC"", ""CNN"", ""CNBC"", ""Venturebeat.com"", ""Npr.org"", 
""Politico"")), na.action = structure(c(`35` = 35L, `95` = 95L, 
`137` = 137L, `154` = 154L, `213` = 213L, `214` = 214L, `232` = 232L, 
`276` = 276L, `321` = 321L), class = ""omit""), row.names = c(NA, 
6L), class = ""data.frame"")

setNames(lapply(news_us$description, get_sentiment), unlist(organization1))

#$US
#[1] 0

#$Bath
#[1] -0.4

#$`Animal Crossing`
#[1] -0.1

#$`World Health Organization`
#[1] 1.1

#$Microsoft
#[1] -0.6

#$Facebook
#[1] -1.9

tapply(sapply(news_us$description, get_sentiment), unlist(organization1), mean) #this line throws the error
</code></pre>
","r, data-science, lapply, sentiment-analysis, tapply","<p>Your problem seems to arise from the use of 'unlist'. Avoid this, as it drops the NULL values and concatenates list entries with multiple values.
Your <code>organization1</code> list has 7 entries (two of which are NULL and one is length = 2). You should have 6 entries if this is to match the <code>news_us</code> data.frame - so something is out of sync there.</p>

<p>Let's assume the first 6 entries in <code>organization1</code> are correct; I would bind them to your data.frame to avoid further 'sync errors':</p>

<p><code>news_us$organization1 = organization1[1:6]</code></p>

<p>Then you need to do the sentiment analysis on each row of the data.frame and bind the results to the <code>organization1</code> value/s. The code below might not be the most elegant way to achieve this, but I think it does what you are looking for:</p>

<pre><code>results = do.call(""rbind"", apply(news_us, 1, function(item){
    if(!is.null(item$organization1[[1]])) cbind(item$organization1, get_sentiment(item$description))
}))
</code></pre>

<p>This code drops any rows where there were no detected <code>organization1</code> values. It should also duplicate sentiment scores in the case of more than one <code>organization1</code> being detected. The results will look like this (which I believe is your goal):</p>

<pre><code>     [,1]                        [,2]  
[1,] ""US""                        ""-0.4""
[2,] ""Bath""                      ""-0.1""
[3,] ""Animal Crossing""           ""1.1"" 
[4,] ""World Health Organization"" ""-0.6""
</code></pre>

<p>The mean scores for each organization can then be collapsed using <code>by</code>, <code>aggregate</code> or similar.</p>

<p>[Edit: Examples of <code>by</code> and <code>aggregate</code>]</p>

<pre><code>by(as.numeric(results[, 2]), results$V1, mean)
</code></pre>

<pre><code>aggregate(as.numeric(results[, 2]), list(results$V1), mean)
</code></pre>
",0,0,343,2020-05-04 09:55:47,https://stackoverflow.com/questions/61589497/arguments-should-have-same-length-error-in-r
Is there a way to do the opposite of unnest_tokens? I want to combine words into a row based on a unique ID,"<p>I am currently trying to do some sentiment analysis and I want to revert each word back into its original format. So I want each word belonging to a unique ID to be combined in a single row. So I want the opposite of unnest_tokens function. I have tried the following: </p>

<pre><code>dsWords &lt;- dsWords %&gt;% 
  group_by(IDReview) %&gt;% 
  summarize(text = str_c(word, collapse = "" "")) %&gt;%
  ungroup()
</code></pre>

<p>However, I simply get all the words combined into 1 row, instead of a row for each unique ID. Can anyone help me out here? Below is a screenshot of what my data frame looks like and a subset of my data. </p>

<p><a href=""https://i.sstatic.net/00INm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/00INm.png"" alt=""enter image description here""></a></p>

<pre><code>structure(list(IDReview = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), 
    word = c(""love"", ""love"", ""author"", ""side"", ""end"", ""show"", 
    ""one"", ""way"", ""think"", ""everyon"", ""also"", ""idea"", ""mani"", 
    ""amaz"", ""look"", ""mani"", ""idea"", ""think"", ""learn"", ""someth"", 
    ""dont"", ""know"", ""look"", ""fact"", ""see"", ""right"", ""dont"", ""write"", 
    ""review"", ""will"", ""hero"", ""will"", ""hes"", ""person"", ""tri"", 
    ""short"", ""certain"", ""never"", ""find"", ""like"")), row.names = c(""1"", 
""1.1"", ""1.2"", ""1.4"", ""1.6"", ""1.13"", ""1.14"", ""1.15"", ""1.16"", ""1.17"", 
""1.18"", ""1.19"", ""1.20"", ""1.24"", ""1.25"", ""1.27"", ""1.28"", ""1.30"", 
""1.33"", ""1.34"", ""1.35"", ""1.36"", ""1.37"", ""1.38"", ""1.39"", ""1.41"", 
""1.42"", ""1.44"", ""1.45"", ""2"", ""2.3"", ""2.5"", ""2.10"", ""2.12"", ""2.18"", 
""2.23"", ""2.26"", ""2.27"", ""2.30"", ""2.34""), class = ""data.frame"")
</code></pre>
","r, sentiment-analysis","<p>As Bas wrote in the comments, the following code with explicit package names</p>

<pre><code>dsWords %&gt;% 
  dplyr::group_by(IDReview) %&gt;% 
  dplyr::summarise(text = stringr::str_c(word, collapse = "" "")) %&gt;%
  ungroup()
</code></pre>

<p>gives as output </p>

<pre><code># A tibble: 2 x 2
  IDReview text                                                                                          
     &lt;int&gt; &lt;chr&gt;                                                                                         
1        1 love love author side end show one way think everyon also idea mani amaz look mani idea think~
2        2 will hero will hes person tri short certain never find like
</code></pre>

<p>That is what you intend, isn't it? </p>

<p>Note that there might be problems when you load <code>plyr</code> after <code>dplyr</code>, see <a href=""https://stackoverflow.com/questions/26923862/why-are-my-dplyr-group-by-summarize-not-working-properly-name-collision-with"">here</a>.</p>
",0,0,129,2020-05-09 18:45:34,https://stackoverflow.com/questions/61701994/is-there-a-way-to-do-the-opposite-of-unnest-tokens-i-want-to-combine-words-into
Error with tune_grid function from R package tidymodels,"<p>I've been reproducing Julia Silge's code from his Youtube video of Sentiment Analysis with tidymodels for Animal Crossing user reviews (<a href=""https://www.youtube.com/watch?v=whE85O1XCkg&amp;t=1300s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=whE85O1XCkg&amp;t=1300s</a>). In minute 25, she uses tune_grid(), and when I try to use it in my script, I have this warning/error: Warning message:
All models failed in tune_grid(). See the <code>.notes</code> column. </p>

<p>In .notes, appears 25 times: </p>

<pre><code>[[1]]
# A tibble: 1 x 1
.notes                                                                         
&lt;chr&gt;                                                                          
1 ""recipe: Error in UseMethod(\""prep\""): no applicable method for 'prep' applied~
</code></pre>

<p>How can I dix this? I'm using the same code that Julia uses. My entire code is this:</p>

<pre><code>library(tidyverse)

user_reviews &lt;- read_tsv(""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv"")

</code></pre>

<pre><code>user_reviews %&gt;%
  count(grade) %&gt;%
  ggplot(aes(grade,n)) + 
  geom_col()

</code></pre>

<pre><code>user_reviews %&gt;%
  filter(grade &gt; 0) %&gt;%
  sample_n(5) %&gt;% 
  pull(text)

</code></pre>

<pre><code>reviews_parsed &lt;- user_reviews %&gt;%
  mutate(text = str_remove(text, ""Expand""), 
         rating = case_when(grade &gt; 6 ~ ""Good"", TRUE ~ ""Bad""))

</code></pre>

<pre><code>library(tidytext)

words_per_review &lt;- reviews_parsed %&gt;% 
  unnest_tokens(word,text) %&gt;%
  count(user_name, name = ""total_words"", sort = TRUE)

words_per_review %&gt;%
  ggplot(aes(total_words)) + 
  geom_histogram()

</code></pre>

<pre><code>library(tidymodels)

set.seed(123)
review_split &lt;- initial_split(reviews_parsed, strata = rating)
review_train &lt;- training(review_split)
review_test &lt;- testing(review_split)

</code></pre>

<pre><code>library(textrecipes)

review_rec &lt;- recipe(rating ~ text, data = review_train) %&gt;% 
  step_tokenize(text) %&gt;%
  step_stopwords(text) %&gt;%
  step_tokenfilter(text, max_tokens = 500) %&gt;%
  step_tfidf(text) %&gt;%
  step_normalize(all_predictors())

review_prep &lt;- prep(review_rec)

review_prep

juice(review_prep)

</code></pre>

<pre><code>lasso_spec &lt;- logistic_reg(penalty = tune(), mixture = 1) %&gt;%
  set_engine(""glmnet"")

lasso_wf &lt;- workflow() %&gt;%
  add_recipe(review_rec) %&gt;%
  add_model(lasso_spec)

lasso_wf

</code></pre>

<pre><code>lambda_grid &lt;- grid_regular(penalty(), levels = 30)

set.seed(123)
review_folds &lt;- bootstraps(review_train, strata = rating)

review_folds

</code></pre>

<pre><code>doParallel::registerDoParallel()

set.seed(2020)

lasso_grid &lt;- tune_grid(lasso_wf, resamples = review_folds, grid = lambda_grid, metrics = metric_set(roc_auc, ppv, npv))

lasso_grid

Warning message:
All models failed in tune_grid(). See the `.notes` column. 

lasso_grid$.notes

[[1]]
# A tibble: 1 x 1
  .notes                                                                         
  &lt;chr&gt;                                                                          
1 ""recipe: Error in UseMethod(\""prep\""): no applicable method for 'prep' applied~

[[2]]
# A tibble: 1 x 1
  .notes                                                                         
  &lt;chr&gt;                                                                          
1 ""recipe: Error in UseMethod(\""prep\""): no applicable method for 'prep' applied~

[[3]]
# A tibble: 1 x 1
  .notes                                                                         
  &lt;chr&gt;                                                                          
1 ""recipe: Error in UseMethod(\""prep\""): no applicable method for 'prep' applied~

etc... to 25.
</code></pre>
","r, data-science, sentiment-analysis, hyperparameters, tidymodels","<p>Found a solution in the comments section of the post. This worked for me (Windows user) and made grid tuning nearly 4x faster.</p>

<pre><code>all_cores &lt;- parallel::detectCores(logical = FALSE)
library(doParallel)
cl &lt;- makePSOCKcluster(all_cores)
registerDoParallel(cl)

set.seed(2020)
lasso_grid &lt;- tune_grid(
  lasso_wf,
  resamples = review_folds,
  grid = lambda_grid,
  metrics = metric_set(roc_auc, ppv, npv),
  control = control_grid(pkgs = c('textrecipes'))
)
</code></pre>

<p>Additional documentation can also be found <a href=""https://tune.tidymodels.org/articles/extras/optimizations.html"" rel=""noreferrer"">here</a> and <a href=""https://tune.tidymodels.org/reference/control_grid.html"" rel=""noreferrer"">here</a>.</p>
",6,4,1933,2020-05-13 11:18:57,https://stackoverflow.com/questions/61773231/error-with-tune-grid-function-from-r-package-tidymodels
Sentiment analysis and fasttext: import error,"<p>I want to run some sentiment analysis using <code>FastText</code>. However, I have always got errors during the declaration of libraries and no example and tutorial within the web seems to be able to fix this. </p>

<p>I have tried to follow the steps described here: <a href=""https://github.com/facebookresearch/fastText/tree/master/python#installation"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/tree/master/python#installation</a></p>

<p>but since the beginning, i.e. since </p>

<pre><code>import fasttext
from fasttext import train_unsupervised
</code></pre>

<p>I have been getting the following error:</p>

<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-10-193c2ffe3856&gt; in &lt;module&gt;
      1 import fasttext
----&gt; 2 from fasttext import train_unsupervised
      3 
      4 # Skipgram model :
      5 model = fasttext.train_unsupervised('data.txt', model='skipgram')

ImportError: cannot import name 'train_unsupervised' from 'fasttext' (/anaconda3/lib/python3.7/site-packages/fasttext/__init__.py)
</code></pre>

<p>I am using Python 3.7 in Jupyter Notebook. I would need FastText to analyse the sentiment of some Italian texts. 
I went here: <a href=""https://fasttext.cc/docs/en/supervised-models.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/supervised-models.html</a> but I have not understood what I should download. </p>

<p>I really hope you can help me with this. </p>
","python, sentiment-analysis, fasttext","<p>Running your code on a clean Python 3.7 conda environment should work after installing fasttext with pip (<code>pip install fasttext</code>).  </p>

<p>If you do that, you should see in a Linux console with</p>

<pre><code>pip list | grep fasttext
</code></pre>

<p>that your <code>fasttext</code> version is 0.9.2 (the current one today). </p>

<p>In addition, upon installing the <code>wget</code> package with pip, the code below should get you started for sentiment analysis using one of the trained models (Amazon reviews) in the page that you linked:</p>

<pre><code>import wget
from fasttext import load_model

wget.download(""https://dl.fbaipublicfiles.com/fasttext/supervised-models/amazon_review_polarity.bin"", 'model.bin')

model = load_model(""model.bin"")

model.predict(""This movie sucks"") # see how output changes!
model.predict(""This band is great"")
model.predict(""I just feel OK about this."") 
</code></pre>

<p>If model size is an issue, try replacing the model with a compressed one:</p>

<pre><code>wget.download(""https://dl.fbaipublicfiles.com/fasttext/supervised-models/amazon_review_polarity.ftz"", 'model.ftz')

model = load_model(""model.ftz"")
</code></pre>

<p>You can also refer to <a href=""https://fasttext.cc/docs/en/supervised-tutorial.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/supervised-tutorial.html</a> to train a model on a custom dataset instead.</p>
",0,0,1139,2020-05-23 21:03:31,https://stackoverflow.com/questions/61978549/sentiment-analysis-and-fasttext-import-error
Exception: Cannot load model.bin,"<p>I have got the following error message trying to run a model: </p>

<pre><code>Exception: fastText: Cannot load model.bin due to C++ extension failed to allocate the memory
</code></pre>

<p>The code I have used is the following: </p>

<pre><code>import wget
from fasttext import load_model

wget.download(""https://dl.fbaipublicfiles.com/fasttext/supervised-models/amazon_review_polarity.bin"", 'model.bin')

model = load_model(""model.bin"")
</code></pre>

<p>I have tried to follow this answer: <a href=""https://stackoverflow.com/questions/45923279/fasttext-cannot-load-model-bin-due-to-c-extension-failed-to-allocate-the-mem"">FastText - Cannot load model.bin due to C++ extension failed to allocate the memory</a> but probably I am doing something wrong as the error is still there. 
Any idea on how to fix it?</p>
","python, sentiment-analysis, fasttext","<p>Install the latest version of fasttext (0.9.2) following these steps:</p>

<pre><code>git clone https://github.com/facebookresearch/fastText.git
cd fastText
pip install .
</code></pre>

<p>then try to execute your code. It should work.</p>
",0,0,2590,2020-05-23 23:04:32,https://stackoverflow.com/questions/61979680/exception-cannot-load-model-bin
Saving the file output,"<p>I am new to the coding world and I found this code on open source.
I am running a twitter API streamer and I wish to save the outputs in csv file. can someone add few lines to my code here to save the output?</p>

<p>I have marked the required places where codes need to be entered</p>

<h1># # # TWITTER STREAM LISTENER # # #</h1>

<pre><code>class TwitterListener(StreamListener):
    """"""
    This is a basic listener that just prints received tweets to stdout.
    """"""
    def __init__(self, fetched_tweets_filename):
        self.fetched_tweets_filename = fetched_tweets_filename

def on_data(self, data):
    try:
        print(data)                                                 **# I wish to save this output**
        with open(self.fetched_tweets_filename, 'a') as tf:
            tf.write(data)
        return True
    except BaseException as e:
        print(""Error on_data %s"" % str(e))
    return True

def on_error(self, status):
    if status == 420:
        # Returning False on_data method in case rate limit occurs.
        return False
    print(status)                                                  **# I wish to save this output**


class TweetAnalyzer():
    """"""
    Functionality for analyzing and categorizing content from tweets.
    """"""
    def tweets_to_data_frame(self, tweets):
        df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])

    df['id'] = np.array([tweet.id for tweet in tweets])
    df['len'] = np.array([len(tweet.text) for tweet in tweets])
    df['date'] = np.array([tweet.created_at for tweet in tweets])
    df['source'] = np.array([tweet.source for tweet in tweets])
    df['likes'] = np.array([tweet.favorite_count for tweet in tweets])
    df['retweets'] = np.array([tweet.retweet_count for tweet in tweets])

    return df



if __name__ == '__main__':

    twitter_client = TwitterClient()
    tweet_analyzer = TweetAnalyzer()

    api = twitter_client.get_twitter_client_api()

    tweets = api.user_timeline(screen_name=""Olympics"", count=20)

    #print(dir(tweets[0]))
    #print(tweets[0].retweet_count)

    df = tweet_analyzer.tweets_to_data_frame(tweets)
    print(df.head(10))                                             **# I wish to save this output**
</code></pre>
","python, twitter, sentiment-analysis","<p><code>df.to_csv(""PATH/file.csv"")</code> should do the trick</p>
",1,0,51,2020-05-24 10:49:21,https://stackoverflow.com/questions/61984827/saving-the-file-output
Sentiment analysis of a certain paragraph from a website,"<p>I have url of multiple websites in an xlsx file. I ran a loop on the xlsx file and passed the urls as an argument to the following sentiment analysis code.
Now the code is providing me with the analysis of the whole website (the websites only contain text and numbers) but the problem is that I want to run the analysis only on the paragraph that starts with ""Managerial function"". How may I do the same?
Here's my code:</p>

<pre><code>article = Article(j)
article.download()
article.parse()
#nltk.download('punkt')
article.nlp()
text = article.summary
obj = TextBlob(text)
sentiment = obj.sentiment.polarity
print(round(sentiment,2))
if sentiment==0:
    print(""neutral"")
elif sentiment&gt;0:
    print(""positive"")
elif sentiment&lt;0:
    print(""negative"")
</code></pre>
","python, scope, sentiment-analysis, article, textblob","<p>Using <code>regex</code>, something like the below would match a paragraph starting with ""Managerial function"":</p>

<pre><code>found=re.search(r'^(Managerial function.*\s)', full_text, re.MULTILINE)
my_paragraph=found.group(0)
</code></pre>

<p>, where <code>full_text</code> is your whole article text.</p>

<p>Remember to add this import first:</p>

<pre><code>import re
</code></pre>
",0,-2,209,2020-05-29 09:50:01,https://stackoverflow.com/questions/62083152/sentiment-analysis-of-a-certain-paragraph-from-a-website
Replace Emojis in R with replace_emoji() function does not work due to different encoding - UTF8/Unicode?,"<p>I am trying to clean my text data and replace Emojis with words so that I can perform a sentiment analysis later on. </p>

<p>Therefore, I am using the <code>replace_emoji</code> function from the textclean package. This should replace all emojis with their corresponding words.</p>

<p>The dataset I am working with is a text corpus, that is also the reason why I used the  <code>VCorpus</code> function from the tm package in my sample code below:</p>

<pre><code>text &lt;- ""text goes here bla bla &lt;u+0001f926&gt;&lt;u+0001f3fd&gt;&lt;u+200d&gt;&lt;u+2640&gt;&lt;u+fe0f&gt;"" #text with emojis

text.corpus &lt;- VCorpus(VectorSource(text)) #Transforming into corpus
text.corpus &lt;- tm_map(text.corpus, content_transformer(function(x) replace_emoji(x, emoji_dt = lexicon::hash_emojis)))  #This function should change Emojis into words

inspect(text.corpus[[1]]) #inspecting the corpus shows that the Unicode was NOT replaced with words

head(hash_emojis) #This shows that the encoding in the lexicon is different than the encoding in my text data. 
</code></pre>

<p>Although the function itself works, it does not replace emojis in my text as it seems that the Encoding within the ""hash_emojis"" dataset is different than the one I have in my data. Thus, the function does not replace the Emojis into words. I have also tried to convert the ""hash_emojis"" data by using the <code>iconv</code> function but unfortunately did not manage to change the encoding.</p>

<p>I would like to replace the Unicode values are shown in my dataset with words. </p>
","r, unicode, emoji, data-cleaning, sentiment-analysis","<p>I found an <a href=""https://stackoverflow.com/a/41672682/4985176"">answer</a> to your question. I will mark this one as a duplicate later today when you read my answer. </p>

<p>Using my example:</p>

<pre><code>library(stringi)
library(magrittr)

""text goes here bla bla &lt;u+0001F600&gt;&lt;u+0001f602&gt;""  %&gt;% 
  stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{4})&gt;"", ""\\\\u$1"") %&gt;% 
  stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{5})&gt;"", ""\\\\U000$1"") %&gt;% 
  stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{6})&gt;"", ""\\\\U00$1"") %&gt;% 
  stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{7})&gt;"", ""\\\\U0$1"") %&gt;% 
  stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{8})&gt;"", ""\\\\U$1"") %&gt;% 
  stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{1})&gt;"", ""\\\\u000$1"") %&gt;% 
  stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{2})&gt;"", ""\\\\u00$1"") %&gt;% 
  stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{3})&gt;"", ""\\\\u0$1"") %&gt;% 
  stri_unescape_unicode() %&gt;% 
  stri_enc_toutf8() %&gt;% 
  textclean::replace_emoji()

[1] ""text goes here bla bla grinning face face with tears of joy ""
</code></pre>

<p>Now be carefull of the unicode representation. The example answer has the ""U"" in upper case, I changed this to lower case ""u"" to reflect your example. </p>

<p>To combine everything:</p>

<pre><code># create a function to use within tm_map
unicode_replacement &lt;- function(text) {
  text %&gt;% 
    stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{4})&gt;"", ""\\\\u$1"") %&gt;% 
    stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{5})&gt;"", ""\\\\U000$1"") %&gt;% 
    stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{6})&gt;"", ""\\\\U00$1"") %&gt;% 
    stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{7})&gt;"", ""\\\\U0$1"") %&gt;% 
    stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{8})&gt;"", ""\\\\U$1"") %&gt;% 
    stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{1})&gt;"", ""\\\\u000$1"") %&gt;% 
    stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{2})&gt;"", ""\\\\u00$1"") %&gt;% 
    stri_replace_all_regex(""&lt;u\\+([[:alnum:]]{3})&gt;"", ""\\\\u0$1"") %&gt;% 
    stri_unescape_unicode() %&gt;% 
    stri_enc_toutf8()
}

library(tm)
library(textclean)
text.corpus &lt;- VCorpus(VectorSource(text)) #Transforming into corpus
text.corpus &lt;- tm_map(text.corpus, content_transformer(unicode_replacement))
text.corpus &lt;- tm_map(text.corpus, content_transformer(function(x) replace_emoji(x, emoji_dt = lexicon::hash_emojis)))  

inspect(text.corpus[[1]]) 

&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  7
Content:  chars: 92

text goes here bla bla &lt;f0&gt;&lt;9f&gt;&lt;a4&gt;&lt;a6&gt;&lt;f0&gt;&lt;9f&gt;&lt;8f&gt;&lt;bd&gt;&lt;e2&gt;&lt;80&gt;&lt;8d&gt; female sign &lt;ef&gt;&lt;b8&gt;&lt;8f&gt;
</code></pre>

<p>Now using your example you get the above outcome. Checking the emoji tables, your unicode examples do not appear in the table except for the female sign. But that is another issue. If I use ""text goes here bla bla "" the outcome is as expected. </p>
",1,0,990,2020-06-07 16:52:22,https://stackoverflow.com/questions/62248654/replace-emojis-in-r-with-replace-emoji-function-does-not-work-due-to-different
replace_emoticon function incorrectly replaces characters within a word - R,"<p>I am working in R and using the <code>replace_emoticon</code> function from the textclean package to replace emoticons with their corresponding words:</p>

<pre><code>library(textclean)
test_text &lt;- ""i had a great experience xp :P""
replace_emoticon(test_text)

[1] ""i had a great e tongue sticking out erience tongue sticking out tongue sticking out ""
</code></pre>

<p>As seen above, the function works but it also replaces characters that look like an emoticon but are within a word (for example the ""xp"" in ""e<strong>xp</strong>erience""). I have tried to find a solution for this issue and found the following function-overwrite that claims to fix this issue: </p>

<pre><code> replace_emoticon &lt;- function(x, emoticon_dt = lexicon::hash_emoticons, ...){

     trimws(gsub(
         ""\\s+"", 
         "" "", 
         mgsub_regex(x, paste0('\\b\\Q', emoticon_dt[['x']], '\\E\\b'), paste0("" "", emoticon_dt[['y']], "" ""))
     ))

 }

replace_emoticon(test_text)

[1] ""i had a great experience tongue sticking out :P""
</code></pre>

<p>However, while it does solve the issue with the word ""experience"", it creates a whole new issue: it stops replacing the "":P"" - which is an Emoticon and should normally get replaced by the function. </p>

<p>Furthermore, the error is known with the characters ""xp"", but I am not sure whether there are other characters except for ""xp"" that also get replaced incorrectly while they are part of a word.</p>

<p>Is there a solution to tell the <code>replace_emoticon</code> function to only replace ""emoticons"" when they are not part of a word?</p>

<p>Thank you!</p>
","r, regex, data-cleaning, sentiment-analysis, emoticons","<p>Wiktor is right, the word boundery check is causing an issue. I have adjusted it slightly in the below function. There is still 1 issue with this and that is if the emoticon is immediately followed by a word without a space between the emoticon and the word. The question is if the last issue is important or not. See examples below. </p>

<p>Note: I added this info to the issue tracker with textclean.</p>

<pre><code>replace_emoticon2 &lt;- function(x, emoticon_dt = lexicon::hash_emoticons, ...){
  trimws(gsub(
    ""\\s+"", 
    "" "", 
    mgsub_regex(x, paste0('\\Q', emoticon_dt[['x']], '\\E\\b'), paste0("" "", emoticon_dt[['y']], "" ""))
  ))
}

# works
replace_emoticon2(""i had a great experience xp :P"")
[1] ""i had a great experience tongue sticking out tongue sticking out""
replace_emoticon2(""i had a great experiencexp:P:P"")
[1] ""i had a great experience tongue sticking out tongue sticking out tongue sticking out""


# does not work:
replace_emoticon2(""i had a great experience xp :Pnewword"")
[1] ""i had a great experience tongue sticking out :Pnewword""
</code></pre>

<p><strong>New function added:</strong></p>

<p>Based on stringi and the regex escaping function from wiktor from <a href=""https://stackoverflow.com/a/48947825/4985176"">this post</a></p>

<pre><code>replace_emoticon_new &lt;- function (x, emoticon_dt = lexicon::hash_emoticons, ...) 
{
  regex_escape &lt;- function(string) {
    gsub(""([][{}()+*^${|\\\\?.])"", ""\\\\\\1"", string)
  }

  stringi::stri_replace_all(x, 
                            regex = paste0(""\\s+"", regex_escape(emoticon_dt[[""x""]])),
                            replacement = paste0("" "", emoticon_dt[['y']]),   
                            vectorize_all = FALSE)
}

test_text &lt;- ""Hello :) Great experience! xp :) :P""
replace_emoticon_new(test_text)
[1] ""Hello smiley Great experience! tongue sticking out smiley tongue sticking out""
</code></pre>
",2,2,236,2020-06-08 20:03:43,https://stackoverflow.com/questions/62270337/replace-emoticon-function-incorrectly-replaces-characters-within-a-word-r
NLP sentiment analysis: &#39;list&#39; object has no attribute &#39;sentiment&#39;,"<p>For a current project, I am planning to perform a sentiment analysis for a number of word combinations with TextBlob.</p>

<p>When running the sentiment analysis line <code>polarity = common_words.sentiment.polarity</code> and calling the results with print(i, word, freq, polarity), I am receiving the following error message:</p>

<pre><code>polarity = common_words.sentiment.polarity
AttributeError: 'list' object has no attribute 'sentiment'
</code></pre>

<p>Is there any smart tweak to get this running? The corresponding code section looks like this:</p>

<pre><code>for i in ['Text_Pro','Text_Con','Text_Main']:
    common_words = get_top_n_trigram(df[i], 150)
    polarity = common_words.sentiment.polarity
    for word, freq in common_words:
        print(i, word, freq, polarity)
</code></pre>

<p>Edit: Please find below the full solution for the situation (in accordance with discussions with user leopardxpreload):</p>

<pre><code>for i in ['Text_Pro','Text_Con','Text_Main']:
    common_words = str(get_top_n_trigram(df[i], 150))
    polarity_list = str([TextBlob(i).sentiment.polarity for i in common_words])
    for element in polarity_list:
        print(i, element)
    for word, freq in common_words:
        print(i, word, freq)
</code></pre>
","python, nlp, sentiment-analysis, textblob","<p>It seems like you are trying to use the <code>TextBlob</code> calls on a list and not a <code>TextBlob</code> object.</p>

<pre class=""lang-py prettyprint-override""><code>for i in ['Text_Pro','Text_Con','Text_Main']:
    common_words = get_top_n_trigram(df[i], 150)
    # Not sure what is in common_words, but it needs to be a string
    polarity = TextBlob(common_words).sentiment.polarity
    for word, freq in common_words:
        print(i, word, freq, polarity)
</code></pre>

<p>If <code>common_words</code> is a list you might need to add:</p>

<pre><code>polarity_list = [TextBlob(i).sentiment.polarity for i in common_words]
</code></pre>

<p>Possible copy-paste solution:</p>

<pre><code>for i in ['Text_Pro','Text_Con','Text_Main']:
    common_words = get_top_n_trigram(df[i], 150)
    polarity_list = [TextBlob(i).sentiment.polarity for i in common_words]
    for element in polarity_list:
        print(i, element)
</code></pre>
",1,2,1723,2020-06-10 09:15:44,https://stackoverflow.com/questions/62299922/nlp-sentiment-analysis-list-object-has-no-attribute-sentiment
Bert Text Classification Loss is Nan,"<p>I'm try to make an model that classify the text in 3 categories.(Negative,Neural,Positive)</p>

<p>I have csv file that contain comments on different apps with their rating.</p>

<p><strong>First I import all the necessary libraries</strong></p>

<pre><code>!pip install transformers
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%tensorflow_version 2.x
import tensorflow as tf

from transformers import TFBertForSequenceClassification, BertTokenizer,DistilBertTokenizer,glue_convert_examples_to_features, InputExample,BertConfig,InputFeatures
from sklearn.model_selection import train_test_split
from tqdm import tqdm


%matplotlib inline
</code></pre>

<p><strong>Then i'll get my csv file</strong></p>

<pre><code>!gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV
!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv
df = pd.read_csv(""reviews.csv"")
print(df[['content','score']].head())
                                         content  score
0  Update: After getting a response from the deve...      1
1  Used it for a fair amount of time without any ...      1
2  Your app sucks now!!!!! Used to be good but no...      1
3  It seems OK, but very basic. Recurring tasks n...      1
4  Absolutely worthless. This app runs a prohibit...      1
</code></pre>

<p><strong>Converting scores to sentiment</strong></p>

<pre><code>def to_sentiment(rating):
  rating = int(rating)
  if rating &lt;= 2:
    return 0
  elif rating == 3:
    return 1
  else: 
    return 2

df['sentiment'] = df.score.apply(to_sentiment)

tokenizer = BertTokenizer.from_pretrained('bert-base-cased',do_lower_case = True)
</code></pre>

<p><strong>Creating Helper Methods to fit the data into model</strong></p>

<pre><code>def convert_example_to_feature(review):
  return tokenizer.encode_plus(
            review,
            add_special_tokens=True,
            max_length=160, # truncates if len(s) &gt; max_length
            return_token_type_ids=True,
            return_attention_mask=True,
            pad_to_max_length=True, # pads to the right by default
        )

def map_example_to_dict(input_ids,attention_mask,token_type_ids,label):
  return {
      ""input_ids"": input_ids,
      ""attention_mask"": attention_mask,
      ""token_type_ids"" : token_type_ids
  },label

def encode_examples(ds):
  # prepare list, so that we can build up final TensorFlow dataset from slices.
  input_ids_list = []
  token_type_ids_list = []
  attention_mask_list = []
  label_list = []

  for index, row in tqdm(ds.iterrows()):
    bert_input = convert_example_to_feature(row['content'])

    input_ids_list.append(bert_input['input_ids'])
    token_type_ids_list.append(bert_input['token_type_ids'])
    attention_mask_list.append(bert_input['attention_mask'])
    label_list.append([row['sentiment']])
  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)

df_train, df_test = train_test_split(df,test_size=0.1)
</code></pre>

<p><strong>Creating Model</strong></p>

<pre><code>model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)
loss = tf.keras.losses.SparseCategoricalCrossentropy()
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss,metrics=metric)

history = model.fit(ds_train_encoded,epochs=1)
14/443 [..............................] - ETA: 3:58 - loss: nan - accuracy: 0.3438
</code></pre>

<p>If i change the count of the sentiment and make it just positive and negative then it works.
But with 3 or more labels creates this problem.</p>
","python, tensorflow, sentiment-analysis, text-classification, bert-language-model","<p>The label classes index should start from 0 not 1.</p>
<blockquote>
<p>TFBertForSequenceClassification requires labels in the range [0,1,...]</p>
</blockquote>
<blockquote>
<p>labels (tf.Tensor of shape (batch_size,), optional, defaults to None)
– Labels for computing the sequence classification/regression loss.
Indices should be in [0, ..., config.num_labels - 1]. If
config.num_labels == 1 a regression loss is computed (Mean-Square
loss), If config.num_labels &gt; 1 a classification loss is computed
(Cross-Entropy).</p>
</blockquote>
<p>Source: <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification</a></p>
",0,1,1606,2020-06-11 08:03:09,https://stackoverflow.com/questions/62319735/bert-text-classification-loss-is-nan
How to get a sentiment scale from a dfm,"<p>For a university project (using Quanteda in R) I am trying to calculate the sentiment score of a corpus generated with a kwic function. I started by creating the wanted corpus with kwic:</p>
<pre><code>kwicMigration8 &lt;- corpus(kwic(EP8_corp, pattern = dictionary(migration), window=30, valuetype= &quot;glob&quot;))
</code></pre>
<p>I think that worked alright, I can look at texts and the summary looked realistic.</p>
<pre><code>summary(kwicMigration8,10)

Corpus consisting of 2834 documents, showing 10 documents:

         Text Types Tokens Sentences from   to   keyword
 text26.1.pre    27     30         2  140  140   borders
 text26.2.pre    27     30         2 1085 1085 migration
 text26.3.pre    24     30         2 1163 1163 migration
 text26.4.pre    27     30         2 1180 1180 migration
 text26.5.pre    27     30         2 1188 1188 migration
 text27.1.pre    25     30         1  665  665    border
 text49.1.pre    23     30         1  284  284   borders
 text68.1.pre    24     30         2   67   67   borders
 text77.1.pre    26     30         2  757  757   borders
 text84.1.pre    27     30         2  673  673    border
 context
     pre
     pre
     pre
     pre
     pre
     pre
     pre
     pre
     pre
     pre
</code></pre>
<p>To start my sentiment analysis I then used the the Lexicoder dictionary by Young and Soroka:</p>
<pre><code>sentkwicMigration8 &lt;- dfm(kwicMigration8, verbose = T,
remove=stopwords(&quot;english&quot;),
dictionary=data_dictionary_LSD2015,
remove_punct = TRUE)

head(sentkwicMigration8)

Document-feature matrix of: 6 documents, 4 features (62.5% sparse) and 6 docvars.
              features
docs           negative positive neg_positive neg_negative
  text26.1.pre        0        1            0            0
  text26.2.pre        1        2            0            0
  text26.3.pre        0        1            0            0
  text26.4.pre        0        3            0            0
  text26.5.pre        1        3            0            0
  text27.1.pre        1        1            0            0
</code></pre>
<p>To create a sentiment measure I then tried to use this logit scale, but it produces only NAs</p>
<pre><code>sentkwicMigration8$sentiment &lt;- log((sentkwicMigration8$positive+0.5)/(sentkwicMigration8$negative+0.5))

summary(sentkwicMigration8$sentiment)
 Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
     NA      NA      NA     NaN      NA      NA    2834 
</code></pre>
<p>Because it is a university project, I have to use this sentiment measure, so is there any way I can make it work?</p>
","r, sentiment-analysis, quanteda","<p>The problem here is that you are trying to access column names of the dfm using <code>$</code>.  That means something very different for dfm objects: it accesses document variables, not column names.  So your <code>NA</code>s and <code>NaN</code>s come from the fact that you are accessing non-existent variables that return <code>NA</code>.</p>
<p>Your two options are: use matrix notation, or convert the dfm to a data.frame.  I don't have your input data so will use an equivalent example, with your object name.</p>

<pre class=""lang-r prettyprint-override""><code>library(&quot;quanteda&quot;)
## Package version: 2.0.1

sentkwicMigration8 &lt;- tail(data_corpus_inaugural) %&gt;%
  tokens() %&gt;%
  tokens_lookup(data_dictionary_LSD2015) %&gt;%
  dfm()

sentmat &lt;- log(sentkwicMigration8[, &quot;positive&quot;] + 0.5) -
  log(sentkwicMigration8[, &quot;negative&quot;] + 0.5)
sentmat
## 6 x 1 Matrix of class &quot;dgeMatrix&quot;
##               features
## docs            positive
##   1997-Clinton 0.7102416
##   2001-Bush    0.8604994
##   2005-Bush    0.8987976
##   2009-Obama   0.4819611
##   2013-Obama   0.7756367
##   2017-Trump   0.9555114

# convert to data.frame
data.frame(doc_id = rownames(sentmat), sentiment = as.vector(sentmat))
##         doc_id sentiment
## 1 1997-Clinton 0.7102416
## 2    2001-Bush 0.8604994
## 3    2005-Bush 0.8987976
## 4   2009-Obama 0.4819611
## 5   2013-Obama 0.7756367
## 6   2017-Trump 0.9555114
</code></pre>
<p>Option two:</p>
<pre class=""lang-r prettyprint-override""><code>sentkwicMigration8 &lt;- convert(sentkwicMigration8, to = &quot;data.frame&quot;)

log((sentkwicMigration8$positive + 0.5) / (sentkwicMigration8$negative + 0.5))
## [1] 0.7102416 0.8604994 0.8987976 0.4819611 0.7756367 0.9555114
</code></pre>
",1,1,223,2020-06-29 12:05:27,https://stackoverflow.com/questions/62637736/how-to-get-a-sentiment-scale-from-a-dfm
My code removed all punctuation from text but do we need few of them for sentimental analysis?,"<pre><code>def remove_punctuation(review):
    lst = []
    for text in review:
        if text not in string.punctuation:
            lst.append(text)
    return &quot;&quot;.join(lst)
df.Review = df.Review.apply(lambda x: remove_punctuation(x))
</code></pre>
<p>I am working on the sentimental analysis of amazon product reviews. I am preprocessing the reviews' text and used the above function to remove punctuation. It has removed all of them, but my question is that do we consider some of them for sentimental analysis. Like !. Is it the right approach.</p>
<p><strong>Thanks for your help and time.</strong></p>
","python, nlp, nltk, sentiment-analysis","<p>There is no clear answer for this. Most nlp tasks require some form of text-preprocessing for the models to better infer on texts. However, in case of sentiment analysis, punctuation such as <code>!</code> might be valuable as it indiciates emphasis on text:</p>
<p><code>I lost my purse!!</code> might have a more negative connotation than <code>Well, I lost my purse.</code></p>
<p>You have two ways to approach this problem:</p>
<ol>
<li>You could only exclude functional punctuation like <code>,.;</code> etc. and leave in the <code>!</code> and the <code>?</code> kind of punctuation. Then look at the performance of your sentiment analysis model.</li>
<li>Evaluate your model both before and after cleaning all punctuation. You can write some kind of grid-search functionality that would control which punctuation to remove and which not and compare the performance.</li>
</ol>
<p>All in all, as in most machine learning problems (I assume you do sentiment analysis by using a trained model) it comes down to a particular dataset and model whether the interpunction interferes with the model's performance or not. If, however, you use some form of third party API for the analysis, you can safely let the punctuation as it is, as the third-party API will most likely handle the cleaning themselves.</p>
<p>Hope that this gave some intuition!</p>
",3,2,1007,2020-07-02 13:15:01,https://stackoverflow.com/questions/62697229/my-code-removed-all-punctuation-from-text-but-do-we-need-few-of-them-for-sentime
"In case of text analysis, when I apply fit() method, what exactly happens? And what does transform() do on the text data?","<p>In case of text analysis, when I apply fit() method, what exactly happens? And what does transform() do on the data?</p>
<p>I can understand it for numerical data type but unable to visualize it for text data.</p>
<p>I have a text array</p>
<pre><code>sents_processed[0:5]
['so there is no way for me plug in here in us unless go by converter',
 'good case excellent value',
 'great for jawbone',
 'tied charger for conversations lasting more than minutes major problems',
 'mic is great']
</code></pre>
<p>Now to vectorize it, I use CountVectorizer class:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(analyzer= 'word', tokenizer= None, preprocessor= None, stop_words= None, max_features= 4500)
data_features = vectorizer.fit_transform(sents_processed)
print(data_features.toarray())
[[0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
</code></pre>
<p>I know that I will get vectors of 4500 length. However, I am unable to visualize what exactly fit method would have done behind the scene and how exactly data would have been then transformed by tranform function? Specially that given data is text type.</p>
","python, machine-learning, nlp, data-science, sentiment-analysis","<p>Let's take a simple exmaple:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
text = ['this is a sentence', 'this is another sentence', 'not a sentence']
</code></pre>
<p>Here i've three sentences</p>
<pre><code>vector = CountVectorizer(analyzer= 'word', tokenizer= None, max_features= 4500)
dt = vector.fit_transform(text)
</code></pre>
<p>The first step in this process is creation of a vocabulary. It assigns a number to every word come in all of the sentences</p>
<pre><code>print(vector.vocabulary_) = {'this': 4, 'is': 1, 'sentence': 3, 'another': 0, 'not': 2}
</code></pre>
<p>Now it deals with the corresponding index of words instead of word itself.
Now method &lt;vector.fit_transform()&gt; convert these sentences into numbers based on the index provided in the vocabulary</p>
<pre><code>data_features = vectorizer.fit_transform(text)
print(data_features.toarray())
= [[0 1 0 1 1]
 [1 1 0 1 1]
 [0 0 1 1 0]]
</code></pre>
<p>if you just analyse the array it just shows the sentences. In vocabulary of five words to represent a sentence in array form, first we have a array of five(size of vocabulary) zeroes representing a empty sentence</p>
<pre><code>[0, 0, 0, 0, 0].
</code></pre>
<p>now if we pick up our first sentence and put 1 in the above array at the index corresponding to that we get that array</p>
<pre><code>[0            1(is)       0          1(sentence)           1(this)]
[1(another)   1(is)       0          1(sentence)           1(this)]
[0            0           1(not)     1(sentence)           0      ]
</code></pre>
<p>if the word comes in that sentence it is 1 else 0</p>
<p>you just take a closer look and get the idea how it is coming or you can read about word Embedding.</p>
",2,0,102,2020-07-02 15:40:58,https://stackoverflow.com/questions/62700110/in-case-of-text-analysis-when-i-apply-fit-method-what-exactly-happens-and-w
Issue applying textblob to a dataframe series,"<p>After splitting my dataset into train, test, and validation sets I have a <code>x_validation</code> set which is a set of strings. Calling <code>x_validation.head()</code> gives:</p>
<pre><code>0    this drink is making my throat hurt more and need to convince corey to go to jacks mannequin concert obvs will be in need of advil
1                         there gonna be movie on no can see it not even the trailers hate thinking about it as it is ll have breakdown
2                                                 the wire on my braces is too long and is cutting through my cheek farrrrrrrk it hurts
3                                             finally have uploaded my documentary to an external site message me for link and password
4                                        lovely national day today hour children parade and hour citizens parade with ju jitsu training
</code></pre>
<p>It has something like 15,000 strings total. I'm trying to create a new list <code>tbresult</code> containing the sentiment polarity scores of each string as calculated by TextBlob:</p>
<pre><code>tbresult = [TextBlob(i).sentiment.polarity for i in x_validation]
</code></pre>
<p>This gives me the following error:</p>
<pre><code>TypeError: The `text` argument passed to `__init__(text)` must be a string, not &lt;class 'float'&gt;
</code></pre>
<p>I'm confused because when I do the following,</p>
<pre><code>lst = [x for x in x_validation]
TextBlob(lst[0]).sentiment.polarity
</code></pre>
<p>it works, I get 0.5. I'm confused where this float type is coming from in the error. How do I do this properly?</p>
","python, sentiment-analysis","<p>Try to remove rows contains a float value, or use <code>.isna().sum()</code> rather than using <code>dropna</code>.</p>
<pre><code>def remove_floats(row):
  if isinstance(row, str):
    return row
  else:
    return None
</code></pre>
<hr />
<pre><code>df = pd.DataFrame({'col':['balh_1', 'blah_2', 1.0, 'blah_3']})

for key in df:
  df[key] = df[key].apply(remove_floats)

df.dropna(inplace=True)

df
</code></pre>
<hr />
<pre><code>     col
0   balh_1
1   blah_2
3   blah_3
</code></pre>
",1,0,725,2020-07-03 18:12:45,https://stackoverflow.com/questions/62720723/issue-applying-textblob-to-a-dataframe-series
"sentimental analysis only for one review.. here&#39;s the code what supposed to be second argument for classifier.fit(new_X_test, )?","<p>this is the code for sentimental analysis only for one review, as we don't have dataset i am not able to figure out what would be the second parameter for classifier.fit method in naive bayes model?</p>
<pre><code># Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Cleaning the code
import re   
import nltk    
nltk.download('stopwords') 
from nltk.corpus import stopwords 
from nltk.stem.porter import PorterStemmer 
new_review = 'I love this restaurant so much'
new_review = re.sub('[^a-zA-Z]', ' ', new_review)
new_review = new_review.lower()
new_review = new_review.split()
ps = PorterStemmer()
all_stopwords = stopwords.words('english')
all_stopwords.remove('not')
new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]
new_review = ' '.join(new_review)
new_corpus = [new_review]


#Creating the bag of word model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(3)
new_X_test = cv.fit_transform(new_corpus).toarray()
#new_X_test = cv.transform(new_corpus).toarray()

# training in Naive bayes model

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(new_X_test, )

# predict the result
#y_pred = classifier.predict(X)
new_y_pred = classifier.predict(new_X_test)
print(new_y_pred)

#new_X_test = cv.transform(new_corpus).toarray()
#new_y_pred = classifier.predict(X)
#print(new_y_pred)
</code></pre>
","python, machine-learning, sentiment-analysis, naivebayes","<p>According to <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"" rel=""nofollow noreferrer"">sklearn.naive_bayes.GaussianNB.fit()</a> manual page, the second parameter is y, where:</p>
<blockquote>
<p><strong>y: array-like of shape (n_samples,)</strong><br />
Target values.</p>
</blockquote>
<p>The target value in your case is the sentiment of your unique review. Naive Bayes is a supervised classification algorithm. &quot;Supervised&quot; means that you have to guide the algorithm during training (or model fitting) by providing the correct target values (or labels).</p>
<p>The code, as it is now, does not really make much sense. You cannot train/fit meaningfully a model with only one sample. You will need to have a dataset with many reviews to fit the model and then try to predict new samples.</p>
",0,0,56,2020-07-10 19:35:18,https://stackoverflow.com/questions/62841185/sentimental-analysis-only-for-one-review-heres-the-code-what-supposed-to-be-s
Classification: Tweet Sentiment Analysis - Order of steps,"<p>I am currently working on a tweet sentiment analysis and have a few questions regarding the right order of the steps. Please assume that the data was already preprocessed and prepared accordingly. So this is how I would proceed:</p>
<ol>
<li>use <code>train_test_split</code> (80:20 ratio) to withhold a test
data set.</li>
<li>vectorize <code>x_train</code> since the tweets are not numerical.</li>
</ol>
<p>In the next steps, I would like to identify the best classifier. Please assume those were already imported. So I would go on by:</p>
<ol start=""3"">
<li>hyperparameterization (grid-search) including a cross-validation approach.
In this step, I would like to identify the best parameters of each
classifier. For KNN the code is as follows:</li>
</ol>
<pre><code>model = KNeighborsClassifier()
n_neighbors = range(1, 10, 2)
weights = ['uniform', 'distance']
metric = ['euclidean', 'manhattan', 'minkowski']

# define grid search
grid = dict(n_neighbors=n_neighbors, weights=weights ,metric=metric)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(train_tf, y_train)

# summarize results
print(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param))
</code></pre>
<ol start=""4"">
<li>compare the accuracy (depending on the best hyperparameters) of the classifiers</li>
<li>choose the best classifier</li>
<li>take the withheld test data set (from <code>train_test_split()</code>) and use the best classifier on the test data</li>
</ol>
<p>Is this the right approach or would you recommend changing something (e. g. doing the cross-validation alone and not within the hyperparametrization)? Does it make sense to test the test data as the final step or should I do it earlier to assess the accuracy for an unknown data set?</p>
","python, machine-learning, classification, sentiment-analysis, text-classification","<p>There are lots of ways to do this and people have strong opinions about it and I'm not always convinced they fully understand what they advocate.</p>
<p>TL;DR: Your methodology looks great and you're asking sensible questions.</p>
<p>Having said that, here are some things to consider:</p>
<ol>
<li>Why are you doing train-test split validation?</li>
<li>Why are you doing hyperparameter tuning?</li>
<li>Why are you doing cross-validation?</li>
</ol>
<p>Yes, each of these techniques are good <em>at doing something specific</em>; but that doesn't necessarily mean they should all be part of the same pipeline.</p>
<p>First off, let's answer these questions:</p>
<ol>
<li><p><strong>Train-Test Split</strong> is useful for testing your classifier's inference abilities. In other words, we want to know how well a classifier performs <em>in general</em> (not on the data we used for training). The test portion allows us to evaluate our classifier without using our training portion.</p>
</li>
<li><p><strong>Hyperparameter-Tuning</strong> is useful for evaluating the effect of hyperparameters on the performance of a classifier. For it to be meaningful, we must compare two (or more) models (using different hyperparameters) but trained preferably using the same training portion (to eliminate selection bias). What do we do once we know the best performing hyperparameters? Will this set of hyperparameters always perform optimally? No. You will see that, due to the stochastic nature of classification, one hyperparameter set may work best in experiment A then another set of hyperparameters may work best on experiment B. Rather, hyperparameter tuning is good for generalizing about which hyperparameters to use when building a classifier.</p>
</li>
<li><p><strong>Cross-validation</strong> is used to smooth out some of the stochastic randomness associated with building classifiers. So, a machine learning pipeline may produce a classifier that is 94% accurate using 1 test-fold and 83% accuracy using another test-fold. What does it mean? It might mean that 1 fold contains samples that are easy. Or it might mean that the classifier, for whatever reason, is actually better. You don't know because it's a black box.</p>
</li>
</ol>
<p><strong>Practically</strong>, how is this helpful?</p>
<p>I see little value in using test-train split <strong>and</strong> cross-validation. I use cross-validation and report accuracy as an average over the n-folds. It is already testing my classifier's performance. I don't see why dividing your training data further to do another round of train-test validation is going to help. Use the average. Having said that, I use the best performing model of the n-fold models created during cross-validation as my final model. As I said, it's black-box, so we can't <strong>know</strong> which model is best but, all else being equal, you may as well use the best performing one. It might actually <em>be</em> better.</p>
<p>Hyperparameter-tuning is useful but it can take forever to do extensive tuning. I suggest adding hyperparameter tuning to your pipeline but only test 2 sets of hyperparameters. So, keep all your hyperparameters constant except 1. e.g. Batch size = {64, 128}. Run that, and you'll be able to say with confidence, &quot;Oh, that made a big difference: 64 works better than 128!&quot; or &quot;Well, that was a waste of time. It didn't make much difference either way.&quot; If the difference is small, ignore that hyperparameter and try another pair. This way, you'll slowly tack towards optimal without all the wasted time.</p>
<p>In practice, I'd say leave the extensive hyperparameter-tuning to academics and take a more pragmatic approach.</p>
<p>But yeah, you're methodology looks good as it is. I think you thinking about what you're doing and that already puts you a step ahead of the pack.</p>
",2,3,257,2020-07-11 10:51:11,https://stackoverflow.com/questions/62848208/classification-tweet-sentiment-analysis-order-of-steps
Word2Vec - Model with high cross validation score performs incredibly bad for test data,"<p>While working on sentiment analysis of twitter data, I encountered a problem that I just can't solve. I wanted to train a RandomForest Classifier to detect hate speech. I, therefore, used a labeled dataset with tweets that are labeled as 1 for hate speech and 0 for normal tweets. For vectorization, I am using Word2Vec. I first performed a hyperparametrization to find good parameters for the classifier.
During hyperparametrization I used a repeated stratified KFold cross-validation (scoring = accuracy)
Mean accuracy is about 99.6% here. However, once I apply the model to a test dataset and plot a confusion matrix, the accuracy is merely above 50%, which is of course awful for a binary classifier.
I successfully use the exact same approach with Bag of Words and had no problems at all here.
Could someone maybe have a quick look at my code? That would be so helpful. I just cannot find what is wrong. Thank you so much!</p>
<p>(I also uploaded the code to google collab in case that is easier for you: <a href=""https://i.sstatic.net/UY5iT.png"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/15BzElijL3vwa_6DnLicxRvcs4SPDZbpe?usp=sharing</a> )</p>
<p>First I preprocessed my data:</p>
<pre><code>train_csv = pd.read_csv(r'/content/drive/My Drive/Colab Notebooks/MLDA_project/data2/train.csv')
train = train_csv     
#check for missing values (result shows that there are no missing values)
train.isna().sum()    
# remove the tweet IDs
train.drop(train.columns[0], axis = &quot;columns&quot;, inplace = True)    
# create a new column to save the cleansed tweets
train['training_tweet'] = np.nan

# remove special/unknown characters
train.replace('[^a-zA-Z#]', ' ', inplace = True, regex = True)    
# generate stopword list and add the twitter handles &quot;user&quot; to the stopword list
stopwords = sw.words('english')
stopwords.append('user')    
# convert to lowercase
train = train.applymap(lambda i:i.lower() if type(i) == str else i)    
# execute tokenization and lemmatization
lemmatizer = WordNetLemmatizer()

for i in range(len(train.index)):
    #tokenize the tweets from the column &quot;tweet&quot;
    words = nltk.word_tokenize(train.iloc[i, 1])
    #consider words with more than 3 characters
    words = [word for word in words if len(word) &gt; 3] 
    #exclude words in stopword list
    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords)] 
    #Join words again
    train.iloc[i, 2]  = ' '.join(words)  
    words = nltk.word_tokenize(train.iloc[i, 2])
train.drop(train.columns[1], axis = &quot;columns&quot;, inplace = True)

majority = train[train.label == 0]
minority = train[train.label == 1]
# upsample minority class
minority_upsampled = resample(minority, replace = True, n_samples = len(majority))      
# combine majority class with upsampled minority class
train_upsampled = pd.concat([majority, minority_upsampled])
train = train_upsampled
np.random.seed(10)
train = train.sample(frac = 1)
train = train.reset_index(drop = True)
</code></pre>
<p>Now <code>train</code> has the labels in column 0 and the preprocessed tweets in column 1.</p>
<p>Next I defined the Word2Vec Vectorizer:</p>
<pre><code>def W2Vvectorize(X_train):
tokenize=X_train.apply(lambda x: x.split())
w2vec_model=gensim.models.Word2Vec(tokenize,min_count = 1, size = 100, window = 5, sg = 1)
w2vec_model.train(tokenize,total_examples= len(X_train), epochs=20)
w2v_words = list(w2vec_model.wv.vocab)
vector=[]
from tqdm import tqdm
for sent in tqdm(tokenize):
    sent_vec=np.zeros(100)
    count =0
    for word in sent: 
        if word in w2v_words:
            vec = w2vec_model.wv[word]
            sent_vec += vec 
            count += 1
    if count != 0:
        sent_vec /= count #normalize
    vector.append(sent_vec)
return vector
</code></pre>
<p>I split the dataset into test and training set and vectorized both subsets using W2V as defined above:</p>
<pre><code>x = train[&quot;training_tweet&quot;]
y = train[&quot;label&quot;]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=train['label'])

print('X Train Shape = total * 0,8 =', X_train.shape)
print('y Train Shape = total * 0,8 =', y_train.shape)
print('X Test Shape = total * 0,2 =', X_test.shape)
print('y Test Shape = total * 0,2 =', y_test.shape) # change 0,4 &amp; 0,6

train_tf_w2v = W2Vvectorize(X_train)
test_tf_w2v = W2Vvectorize(X_test)
</code></pre>
<p>Now I carry out the hyperparametrization:</p>
<pre><code># define models and parameters
model = RandomForestClassifier()
n_estimators = [10, 100, 1000]
max_features = ['sqrt', 'log2']
# define grid search
grid = dict(n_estimators=n_estimators,max_features=max_features)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(train_tf_w2v, y_train)
# summarize results
print(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param))
</code></pre>
<p>This results in the following output:</p>
<pre><code>Best: 0.996628 using {'max_features': 'log2', 'n_estimators': 1000}
0.995261 (0.000990) with: {'max_features': 'sqrt', 'n_estimators': 10}
0.996110 (0.000754) with: {'max_features': 'sqrt', 'n_estimators': 100}
0.996081 (0.000853) with: {'max_features': 'sqrt', 'n_estimators': 1000}
0.995885 (0.000872) with: {'max_features': 'log2', 'n_estimators': 10}
0.996481 (0.000691) with: {'max_features': 'log2', 'n_estimators': 100}
0.996628 (0.000782) with: {'max_features': 'log2', 'n_estimators': 1000}
</code></pre>
<p>Next, I wanted to draw a confusion matrix with the test data using the Model:</p>
<pre><code>clf = RandomForestClassifier(max_features = 'log2', n_estimators=1000) 
   
clf.fit(train_tf_w2v, y_train)
name = clf.__class__.__name__
        
expectation = y_test
test_prediction = clf.predict(test_tf_w2v)
acc = accuracy_score(expectation, test_prediction)   
pre = precision_score(expectation, test_prediction)
rec = recall_score(expectation, test_prediction)
f1 = f1_score(expectation, test_prediction)

fig, ax = plt.subplots(1,2, figsize=(14,4))
plt.suptitle(f'{name} \n', fontsize = 18)
plt.subplots_adjust(top = 0.8)
skplt.metrics.plot_confusion_matrix(expectation, test_prediction, ax=ax[0])
skplt.metrics.plot_confusion_matrix(expectation, test_prediction, normalize=True, ax = ax[1])
plt.show()
    
print(f&quot;for the {name} we receive the following values:&quot;)
print(&quot;Accuracy: {:.3%}&quot;.format(acc))
print('Precision score: {:.3%}'.format(pre))
print('Recall score: {:.3%}'.format(rec))
print('F1 score: {:.3%}'.format(f1))
</code></pre>
<p>This outputs:</p>
<p><a href=""https://i.sstatic.net/UY5iT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UY5iT.png"" alt=""confusion matrix"" /></a></p>
<p>for the RandomForestClassifier we receive the following values:
Accuracy: 57.974%
Precision score: 99.790%
Recall score: 15.983%
F1 score: 27.552%</p>
","machine-learning, cross-validation, word2vec, sentiment-analysis","<p>Ouuh... Now I feel stupid. I found what was wrong.</p>
<p>After the train/test-split, I sent both subsets independently to the <code>W2Vvectorize()</code> function.</p>
<pre><code>train_tf_w2v = W2Vvectorize(X_train)
test_tf_w2v = W2Vvectorize(X_test)
</code></pre>
<p>From there the <code>W2Vvectorize()</code> function trains two independent Word2Vec models, based on the two independent subsets. Hence when I pass the vectorized test data <code>test_tf_w2v</code> to my trained RandomForest classifier, to check if the accuracy is correct for a test set as well, it appears to the trained RandomForest classifier, as if the test set would be in a different language. The two separate word2vec models just vectorize in a different way.</p>
<p>I solved that as follows:</p>
<pre><code>def W2Vvectorize(X_train):
    tokenize=X_train.apply(lambda x: x.split())
    vector=[]
    for sent in tqdm(tokenize):
        sent_vec=np.zeros(100)
        count =0
        for word in sent: 
            if word in w2v_words:
                vec = w2vec_model.wv[word]
                sent_vec += vec 
                count += 1
        if count != 0:
            sent_vec /= count #normalize
        vector.append(sent_vec)
    return vector
</code></pre>
<p>And the Word2Vec training is separate from that :</p>
<pre><code>x = train[&quot;training_tweet&quot;]
y = train[&quot;label&quot;]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=train['label'])

print('X Train Shape = total * 0,8 =', X_train.shape)
print('y Train Shape = total * 0,8 =', y_train.shape)
print('X Test Shape = total * 0,2 =', X_test.shape)
print('y Test Shape = total * 0,2 =', y_test.shape) #

tokenize=X_train.apply(lambda x: x.split())
w2vec_model=gensim.models.Word2Vec(tokenize,min_count = 1, size = 100, window = 5, sg = 1)
w2vec_model.train(tokenize,total_examples= len(X_train), epochs=20)
w2v_words = list(w2vec_model.wv.vocab)

train_tf_w2v = W2Vvectorize(X_train)
test_tf_w2v = W2Vvectorize(X_test)
</code></pre>
<p>So the Word2Vec models training is performed only on the training data. The vectorization of test data, however, has to be carried out with that exact same Word2Vec model.</p>
",2,2,1415,2020-07-13 16:56:50,https://stackoverflow.com/questions/62880636/word2vec-model-with-high-cross-validation-score-performs-incredibly-bad-for-te
How to assign labels/score to data using machine learning,"<p>I have a dataframe made by many rows which includes tweets. I would like to classify them using a machine learning technique (supervised or unsupervised).
Since the dataset is unlabelled, I thought to select a few rows (50%) to label manually (+1 pos, -1 neg, 0 neutral), then using machine learning to assign labels to the other rows.
In order to do this, I did as follows:</p>
<p>Original Dataset</p>
<pre><code>Date                   ID        Tweet                         
01/20/2020           4141    The cat is on the table               
01/20/2020           4142    The sky is blue                       
01/20/2020           53      What a wonderful day                  
...
05/12/2020           532     In this extraordinary circumstance we are together   
05/13/2020           12      It was a very bad decision            
05/22/2020           565     I know you are the best              
</code></pre>
<ol>
<li><p>Split the dataset into 50% train and 50% test. I manually labelled 50% of data as follows:</p>
<pre><code>Date                   ID        Tweet                          PosNegNeu
 01/20/2020           4141    The cat is on the table               0
 01/20/2020           4142    The weather is bad today              -1
 01/20/2020           53      What a wonderful day                  1
 ...
 05/12/2020           532     In this extraordinary circumstance we are together   1
 05/13/2020           12      It was a very bad decision            -1
 05/22/2020           565     I know you are the best               1
</code></pre>
</li>
</ol>
<p>Then I extracted words'frequency (after removing stopwords):</p>
<pre><code>               Frequency
 bad               2
 circumstance      1
 best              1
 day               1
 today             1
 wonderful         1
</code></pre>
<p>....</p>
<p>I would like to try to assign labels to the other data based on:</p>
<ul>
<li>words within the frequency table, for example saying &quot;if a tweet contains e.g. bad than assign -1; if a tweet contains wonderful assign 1 (i.e. I should create a list of strings and a rule);</li>
<li>based on sentence similarity (e.g. using Levenshtein distance).</li>
</ul>
<p>I know that there are several ways to do this, even better, but I am having some issue to classify/assign labels to my data and I cannot do it manually.</p>
<p>My expected output, e.g. with the following test dataset</p>
<pre><code>Date                   ID        Tweet                                   
06/12/2020           43       My cat 'Sylvester' is on the table            
07/02/2020           75       Laura's pen is black                                                
07/02/2020           763      It is such a wonderful day                                    
...
11/06/2020           1415    No matter what you need to do                  
05/15/2020           64      I disagree with you: I think it is a very bad decision           
12/27/2020           565     I know you can improve                         
</code></pre>
<p>should be something like</p>
<pre><code>Date                   ID        Tweet                                   PosNegNeu
06/12/2020           43       My cat 'Sylvester' is on the table            0
07/02/2020           75       Laura's pen is black                          0                       
07/02/2020           763      It is such a wonderful day                    1                
...
11/06/2020           1415    No matter what you need to do                  0  
05/15/2020           64      I disagree with you: I think it is a very bad decision  -1          
12/27/2020           565     I know you can improve                         0   
</code></pre>
<p>Probably a better way should be consider n-grams rather than single words or building a corpus/vocabulary to assign a score, then a sentiment. Any advice would be greatly appreciated as it is my first exercise on machine learning. I think that k-means clustering could also be applied, trying to get more similar sentences.
If you could provide me a complete example (with my data would be great, but also with other data would be fine as well), I would really appreciate it.</p>
","python, pandas, machine-learning, sentiment-analysis","<p>I'll propose the sentence or tweet in this context to be analysed for polarity. This can be done using the <code>textblob</code> library. It can be installed as <code>pip install -U textblob</code>. Once the text data polarity is found, it can be assigned as a separate column in the dataframe. Subsequently, the sentence polarity can then be used for further analysis.</p>
<p><strong>Initial Code</strong></p>
<pre><code>from textblob import TextBlob
df['sentiment'] = df['Tweet'].apply(lambda Tweet: TextBlob(Tweet).sentiment)
print(df)
</code></pre>
<p><strong>Intermediate Result</strong></p>
<pre><code>    Date     ...                                  sentiment
0  1/1/2020  ...                                 (0.0, 0.0)
1  2/1/2020  ...                                 (0.0, 0.0)
2  3/2/2020  ...                                 (0.0, 0.1)
3  4/2/2020  ...  (-0.6999999999999998, 0.6666666666666666)
4  5/2/2020  ...                                 (0.5, 0.6)

[5 rows x 4 columns]
</code></pre>
<p>From the sentiment column (in the above output), we can see the sentiment column is categorized between two — Polarity and Subjectivity.</p>
<blockquote>
<p>Polarity is a float value within the range [-1.0 to 1.0] where 0
indicates neutral, +1 indicates a very positive sentiment and -1
represents a very negative sentiment.</p>
<p>Subjectivity is a float value within the range [0.0 to 1.0] where 0.0
is very objective and 1.0 is very subjective. Subjective sentence
expresses some personal feelings, views, beliefs, opinions,
allegations, desires, beliefs, suspicions, and speculations where as
Objective sentences are factual.</p>
</blockquote>
<p>Notice, the sentiment column is a tuple. So we can split it into two columns like, <code>df1=pd.DataFrame(df['sentiment'].tolist(), index= df.index)</code>. Now, we can create a new dataframe to which I'll append the split columns as shown;</p>
<pre><code>df_new = df
df_new['polarity'] = df1['polarity']
df_new.polarity = df1.polarity.astype(float)
df_new['subjectivity'] = df1['subjectivity']
df_new.subjectivity = df1.polarity.astype(float)
</code></pre>
<p>Finally, basis of the sentence polarity found earlier, we can now add a label to the dataframe, which will indicate if the tweet is positive, negative or neutral.</p>
<pre><code>import numpy as np
conditionList = [
    df_new['polarity'] == 0,
    df_new['polarity'] &gt; 0,
    df_new['polarity'] &lt; 0]
choiceList = ['neutral', 'positive', 'negative']
df_new['label'] = np.select(conditionList, choiceList, default='no_label')
print(df_new)
</code></pre>
<p>Finally, the result will look like this;</p>
<p><strong>Final Result</strong></p>
<pre><code>[5 rows x 6 columns]
       Date  ID                 Tweet  ... polarity  subjectivity     label
0  1/1/2020   1  the weather is sunny  ...      0.0           0.0   neutral
1  2/1/2020   2       tom likes harry  ...      0.0           0.0   neutral
2  3/2/2020   3       the sky is blue  ...      0.0           0.0   neutral
3  4/2/2020   4    the weather is bad  ...     -0.7          -0.7  negative
4  5/2/2020   5         i love apples  ...      0.5           0.5  positive

[5 rows x 7 columns]
</code></pre>
<p><strong>Data</strong></p>
<pre><code>import pandas as pd

# create a dictionary
data = {&quot;Date&quot;:[&quot;1/1/2020&quot;,&quot;2/1/2020&quot;,&quot;3/2/2020&quot;,&quot;4/2/2020&quot;,&quot;5/2/2020&quot;],
    &quot;ID&quot;:[1,2,3,4,5],
    &quot;Tweet&quot;:[&quot;the weather is sunny&quot;,
             &quot;tom likes harry&quot;, &quot;the sky is blue&quot;,
             &quot;the weather is bad&quot;,&quot;i love apples&quot;]}
# convert data to dataframe
df = pd.DataFrame(data)
</code></pre>
<p><strong>Full Code</strong></p>
<pre><code># create some dummy data
import pandas as pd
import numpy as np

# create a dictionary
data = {&quot;Date&quot;:[&quot;1/1/2020&quot;,&quot;2/1/2020&quot;,&quot;3/2/2020&quot;,&quot;4/2/2020&quot;,&quot;5/2/2020&quot;],
        &quot;ID&quot;:[1,2,3,4,5],
        &quot;Tweet&quot;:[&quot;the weather is sunny&quot;,
                 &quot;tom likes harry&quot;, &quot;the sky is blue&quot;,
                 &quot;the weather is bad&quot;,&quot;i love apples&quot;]}
# convert data to dataframe
df = pd.DataFrame(data)

from textblob import TextBlob
df['sentiment'] = df['Tweet'].apply(lambda Tweet: TextBlob(Tweet).sentiment)
print(df)

# split the sentiment column into two
df1=pd.DataFrame(df['sentiment'].tolist(), index= df.index)

# append cols to original dataframe
df_new = df
df_new['polarity'] = df1['polarity']
df_new.polarity = df1.polarity.astype(float)
df_new['subjectivity'] = df1['subjectivity']
df_new.subjectivity = df1.polarity.astype(float)
print(df_new)

# add label to dataframe based on condition
conditionList = [
    df_new['polarity'] == 0,
    df_new['polarity'] &gt; 0,
    df_new['polarity'] &lt; 0]
choiceList = ['neutral', 'positive', 'negative']
df_new['label'] = np.select(conditionList, choiceList, default='no_label')
print(df_new)
</code></pre>
",6,1,2248,2020-07-22 00:10:16,https://stackoverflow.com/questions/63024842/how-to-assign-labels-score-to-data-using-machine-learning
Issue with tokenizing words with NLTK in Python. Returning lists of single letters instead of words,"<p>I'm having some trouble with my NLP python program, I am trying to create a dataset of positive and negative tweets however when I run the code it only returns what appears to be tokenized individual letters. I am new to Python and NLP so I apologise if this is basic or if I'm explaining myself poorly. I have added my code below:</p>
<pre><code>import csv
import random
import re
import string
import mysql.connector
from nltk import FreqDist, classify, NaiveBayesClassifier
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize


def remove_noise(tweet_tokens, stop_words=()):
    cleaned_tokens = []
    for token, tag in pos_tag(tweet_tokens):
        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+#]|[!*\(\),]|' \
                  '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)
        token = re.sub(&quot;(@[A-Za-z0-9_]+)&quot;, &quot;&quot;, token)

        if tag.startswith(&quot;NN&quot;):
            pos = 'n'
        elif tag.startswith('VB'):
            pos = 'v'
        else:
            pos = 'a'

        lemmatizer = WordNetLemmatizer()
        token = lemmatizer.lemmatize(token, pos)

        if len(token) &gt; 0 and token not in string.punctuation and token.lower() not in stop_words:
            cleaned_tokens.append(token.lower())
    print(token)
    return cleaned_tokens


def get_all_words(cleaned_tokens_list):
    for tokens in cleaned_tokens_list:
        for token in tokens:
            yield token


def get_tweets_for_model(cleaned_tokens_list):
    for tweet_tokens in cleaned_tokens_list:
        yield dict([token, True] for token in tweet_tokens)


if __name__ == &quot;__main__&quot;:


with open('positive_tweets.csv') as csv_file:
    positive_tweets = csv.reader(csv_file, delimiter=',')
with open('negative_tweets.csv') as csv_file:
    negative_tweets = csv.reader(csv_file, delimiter=',')

stop_words = stopwords.words('english')

positive_tweet_tokens = word_tokenize(positive_tweets)
negative_tweet_tokens = word_tokenize(negative_tweets)

positive_cleaned_tokens_list = []
negative_cleaned_tokens_list = []

for tokens in positive_tweet_tokens:
    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))

for tokens in negative_tweet_tokens:
    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))

all_pos_words = get_all_words(positive_cleaned_tokens_list)
all_neg_words = get_all_words(negative_cleaned_tokens_list)

freq_dist_pos = FreqDist(all_pos_words)
freq_dist_neg = FreqDist(all_neg_words)
print(freq_dist_pos.most_common(10))
print(freq_dist_neg.most_common(10))

positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)
negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)

positive_dataset = [(tweet_dict, 'positive')
                    for tweet_dict in positive_tokens_for_model]

negative_dataset = [(tweet_dict, 'negative')
                    for tweet_dict in negative_tokens_for_model]

dataset = positive_dataset + negative_dataset

random.shuffle(dataset)

train_data = dataset[:7000]
test_data = dataset[7000:]

classifier = NaiveBayesClassifier.train(train_data)

print(&quot;Accuracy is:&quot;, classify.accuracy(classifier, test_data))
</code></pre>
<p>snippet from CSV file for reference:</p>
<pre><code>    &quot;tweetid&quot;,&quot;username&quot;,&quot;created_at&quot;,&quot;tweet&quot;,&quot;location&quot;,&quot;place&quot;,&quot;classification&quot;
&quot;1285666943073161216&quot;,&quot;MeFixerr&quot;,&quot;2020-07-21 20:04:20+00:00&quot;,&quot;Overwhelmed by all the calls, msgs and tweets. I apologize for getting lost without prior notice. Did not expect to be missed with such fervor. 
I am good &amp;amp; taking a break. Lots of love and dua's for everyone of you in #PTIFamily ❤&quot;,&quot;Pakistan, Quetta&quot;,,&quot;positive&quot;
</code></pre>
","python, nlp, nltk, tokenize, sentiment-analysis","<p>Your tokens are from the file name ('positive_tweets.csv'), not the data inside the file. Add a print statement like below. You will see the issue.</p>
<pre><code>positive_tweet_tokens = word_tokenize(positive_tweets)
negative_tweet_tokens = word_tokenize(negative_tweets)
print(&quot;tokens=&quot;, positive_tweet_tokens)  # add this line
</code></pre>
<p>Output from full script</p>
<pre><code>tokens= ['positive_tweets.csv']
v
v
[('e', 3), ('v', 2), ('p', 1), ('w', 1), ('c', 1)]
[('e', 4), ('v', 2), ('n', 1), ('g', 1), ('w', 1), ('c', 1)]
Accuracy is: 0
</code></pre>
<p>Concerning the second error, replace this</p>
<pre><code>with open('positive_tweets.csv') as csv_file:
    positive_tweets = csv.reader(csv_file, delimiter=',')
with open('negative_tweets.csv') as csv_file:
    negative_tweets = csv.reader(csv_file, delimiter=',')
</code></pre>
<p>with this</p>
<pre><code>positive_tweets = negative_tweets = &quot;&quot;

with open('positive_tweets.csv') as csv_file:
    positive_tweets_rdr = csv.reader(csv_file, delimiter=',')
    all = list(positive_tweets_rdr)
    for lst in all[1:]: positive_tweets += ' ' + lst[3] #tweet column
    
with open('negative_tweets.csv') as csv_file:
    negative_tweets_rdr = csv.reader(csv_file, delimiter=',')
    all = list(negative_tweets_rdr)
    for lst in all[1:]: negative_tweets += ' ' + lst[3] #tweet column
</code></pre>
",0,1,613,2020-07-23 13:37:21,https://stackoverflow.com/questions/63055632/issue-with-tokenizing-words-with-nltk-in-python-returning-lists-of-single-lette
"Creating a dataframe on Date from two incomplete, not same size dataframes","<p>I am trying to put together a big dataframe wih dates, average sentiment scores (from Twitter), and closing stock price.</p>
<p>Here is what I have so far.</p>
<pre><code>#imports
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import re
import urllib3
import requests
import datetime 

#mydates dataframe that just has the dates from my desired range. Shape is 2008 rows x 1 column
date1='2014-01-01'
date2='2019-07-01'
mydates =pd.date_range(date1,date2).tolist()
newdf =pd.DataFrame({'Date':mydates})

#df with the average daily sentiment scores. Large dataset with 500 rows.
#This currently skips dates that didn't have tweets.I want to include those dates but have sentiment equal 0.
Date        Score
2014-01-13  0.01
2014-01-14  0.035
2014-01-15  0.453
2014-01-20  0.06474

#ts dataframe of dates and stock prices. Shape is 1381 rows x 1 column
Date         Adj Close
2014-01-13  44.8
2014-01-14  45.3
2014-01-15  45.8
2014-01-16  46.5
2014-01-17  46.5
2014-01-21  46.7
</code></pre>
<p><strong>Desired output</strong></p>
<pre><code>Date        Score   Close Price
2014-01-13  0.01     44.8
2014-01-14  0.035    45.3
2014-01-15  0.453    45.8
2014-01-16  0.0      46.5
2014-01-17  0.0      46.5
2014-01-18  0.0      46.5
2014-01-19  0.0      46.5
2014-01-20  0.06474  46.5
</code></pre>
<p>My plan is to then save this dataset as a csv.</p>
<p>Issues I've run into:
Df and ts are NOT the same size. I'd need to go through ts to make all the weekend close prices the same as Friday. How do I do that?
Not knowing how to write a loop that can assign the score for a date in one dataframe to a column in another dataframe.</p>
<p>I use pandas-3.</p>
","python-3.x, pandas, dataframe, sentiment-analysis, stock","<p>Set the index of <code>df</code> as <code>Date</code> and use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.asfreq.html"" rel=""nofollow noreferrer""><code>DataFrame.asfreq</code></a> to reindex the dataframe on daily frequency, then using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>DataFrame.merge</code></a> <code>left</code> merge it with <code>ts</code> on column <code>Date</code>, finally use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ffill.html"" rel=""nofollow noreferrer""><code>Series.ffill</code></a> on column <code>Adj Close</code>:</p>
<pre><code>df1 = (
    df.set_index('Date').
    asfreq('D', fill_value=0).reset_index().merge(ts, on='Date', how='left')
)
df1['Adj Close'] = df1['Adj Close'].ffill()
</code></pre>
<p>Result:</p>
<pre><code>print(df1)
        Date    Score  Adj Close
0 2014-01-13  0.01000       44.8
1 2014-01-14  0.03500       45.3
2 2014-01-15  0.45300       45.8
3 2014-01-16  0.00000       46.5
4 2014-01-17  0.00000       46.5
5 2014-01-18  0.00000       46.5
6 2014-01-19  0.00000       46.5
7 2014-01-20  0.06474       46.5
</code></pre>
",3,3,331,2020-07-23 16:54:25,https://stackoverflow.com/questions/63059365/creating-a-dataframe-on-date-from-two-incomplete-not-same-size-dataframes
Sentiment results are different between stanford nlp python package and the live demo,"<p>I try sentiment analysis of tweet text by both stanford nlp python package and the live demo, but the results are different. The result of the python package is positive while the result of the live demo is negative.</p>
<ul>
<li>For python package, I download <strong>stanford-corenlp-4.0.0</strong> and install <strong>py-corenlp</strong>, basically follow the instruction in this answer: <a href=""https://stackoverflow.com/questions/32879532/stanford-nlp-for-python"">Stanford nlp for python</a>, the code is shown below:</li>
</ul>
<pre><code>import pycorenlp
from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP(&quot;http://localhost:9000&quot;)
text=&quot;noted former cocaine user carrie fisher says donald trump was absolutely on coke makes sense&quot;
res = nlp.annotate(text,properties={'annotators': 'sentiment','outputFormat': 'json','timeout': 1000})
for s in res[&quot;sentences&quot;]:
    print(s[&quot;sentimentValue&quot;], s[&quot;sentiment&quot;])
</code></pre>
<p>and the result is:</p>
<pre><code>3 Positive
</code></pre>
<ul>
<li>For the live demo:</li>
</ul>
<p><a href=""https://i.sstatic.net/jjLc4.png"" rel=""nofollow noreferrer"">screenshot of the live demo result</a></p>
","python, stanford-nlp, sentiment-analysis","<p>The old sentiment demo is probably running older code/older models, so that is why the results would be different. CoreNLP 4.0.0 should return POSITIVE for the entire sentence.</p>
",1,1,147,2020-07-26 03:10:40,https://stackoverflow.com/questions/63095707/sentiment-results-are-different-between-stanford-nlp-python-package-and-the-live
Export list from lapply to csv in R,"<p>I am trying to script R to take the output of lapply and export it as a .csv with the following header:
score, file.</p>
<p>This is how I have imported the files and created a corpus of .txt files:</p>
<pre><code>folder &lt;- &quot;C:\\Users\\super\\Documents\\Mette\\data3\\bla&quot;
filelist &lt;- list.files(path=folder, pattern=&quot;*.txt&quot;)
files &lt;- lapply(filelist, FUN=readLines, encoding = &quot;UTF-8&quot;)
corpus4 &lt;- lapply(files, FUN=paste, collapse=&quot; &quot;)
</code></pre>
<p>I am running this lapply function over the corpus of .txt files i created above:</p>
<pre><code>library(Sentida)
lapply(corpus4, sentida, output = &quot;mean&quot;) 
</code></pre>
<p>This produces a list of scores that look like this in the console:</p>
<pre><code>[[1]]

[1] 0.1517111

[[2]]

[1] 0.4068402

[[3]]

[1] 0.3138707
</code></pre>
<p>Now I want to export/print this list to a .csv file that lists the scores AND their corresponding file name. Ideally, I want the .csv to look like this:</p>
<pre><code>score, file
0.1517111, file1.txt
0.4068402, file2.txt
0.3138707, file3.txt
</code></pre>
<p>I have tried working with write.csv but I have a hard time getting the .csv in the format mentioned above. Any help would be greatly appreciated!</p>
","r, export-to-csv, lapply, sentiment-analysis, corpus","<pre><code>score = unlist(lapply(corpus4, sentida, output = &quot;mean&quot;))
DF = data.frame(score,  filelist)
write.csv(DF, &quot;Scores.csv&quot;)
</code></pre>
",1,1,446,2020-07-31 12:14:38,https://stackoverflow.com/questions/63191644/export-list-from-lapply-to-csv-in-r
Emotional score of sentences using Spacy,"<p>I have a series of 100.000+ sentences and I want to rank how emotional they are.</p>
<p>I am quite new to the NLP world, but this is how I managed to get started (adaptation from <a href=""https://spacy.io/usage/spacy-101"" rel=""nofollow noreferrer"">spacy 101</a>)</p>
<pre><code>import spacy
from spacy.matcher import Matcher

matcher = Matcher(nlp.vocab)

def set_sentiment(matcher, doc, i, matches):
    doc.sentiment += 0.1

myemotionalwordlist = ['you','superb','great','free']

sentence0 = 'You are a superb great free person'
sentence1 = 'You are a great person'
sentence2 = 'Rocks are made o minerals'

sentences = [sentence0,sentence1,sentence2]

pattern2 = [[{&quot;ORTH&quot;: emotionalword, &quot;OP&quot;: &quot;+&quot;}] for emotionalword in myemotionalwordlist]
matcher.add(&quot;Emotional&quot;, set_sentiment, *pattern2)  # Match one or more emotional word

for sentence in sentences:
    doc = nlp(sentence)
    matches = matcher(doc)

    for match_id, start, end in matches:
        string_id = nlp.vocab.strings[match_id]
        span = doc[start:end]
    print(&quot;Sentiment&quot;, doc.sentiment)
</code></pre>
<p>myemotionalwordlist is a list of about 200 words that Ive built manually.</p>
<p>My questions are:</p>
<p>(1-a) Counting the number of emotional words does not seem like the best approach. Anyone has any suggetions of a better way of doing so?</p>
<p>(1-b) In case this approach is good enough, any suggestions on how I can extract emotional words from wordnet?</p>
<p>(2) Whats the best way of escalating this? I am thinking about adding all sentences to a pandas data frame and then applying the match function to each one of them</p>
<p>Thanks in advance!</p>
","python, nlp, spacy, sentiment-analysis, wordnet","<p>There are going to be two main approaches:</p>
<ul>
<li>the one you have started, which is a list of emotional words, and counting how often they appear</li>
<li>showing examples of what you consider emotional sentences and what are unemotional sentences to a machine learning model, and let it work it out.</li>
</ul>
<p>The first way will get better as you give it more words, but you will eventually hit a limit. (Simply due to the ambiguity and flexibility of human language, e.g. while &quot;you&quot; is more emotive than &quot;it&quot;, there are going to be a lot of unemotional sentences that use &quot;you&quot;.)</p>
<blockquote>
<p>any suggestions on how I can extract emotional words from wordnet?</p>
</blockquote>
<p>Take a look at sentiwordnet, which adds a measure of positivity, negativity or neutrality to each wordnet entry. For &quot;emotional&quot; you could extract just those that have either pos or neg score over e.g. 0.5. (Watch out for the non-commercial-only licence.)</p>
<p>The second approach will probably work better <em>if</em> you can feed it enough training data, but &quot;enough&quot; can sometimes be too much. Other downsides are the models often need much more compute power and memory (a serious issue if you need to be offline, or working on a mobile device), and that they are a blackbox.</p>
<p>I think the 2020 approach would be to start with a pre-trained BERT model (the bigger the better, see <a href=""https://arxiv.org/abs/2005.14165"" rel=""nofollow noreferrer"">the recent GPT-3 paper</a>), and then fine-tune it with a sample of your 100K sentences that you've manually annotated. Evaluate it on another sample, and annotate more training data for the ones it got wrong. Keep doing this until you get the desired level of accuracy.</p>
<p>(Spacy has support for both approaches, by the way. What I called fine-tuning above is also called transfer learning. See <a href=""https://spacy.io/usage/training#transfer-learning"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#transfer-learning</a> Also googling for &quot;spacy sentiment analysis&quot; will find quite a few tutorials.)</p>
",2,3,1515,2020-08-01 10:55:54,https://stackoverflow.com/questions/63204418/emotional-score-of-sentences-using-spacy
VaderSentiment: unable to update emoji sentiment score,"<p>As title states, code is as follows:</p>
<pre><code>from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

new_words = {
    '🔥': 4.0,
}

sia = SentimentIntensityAnalyzer()
sia.lexicon.update(new_words)
sia.polarity_scores('🔥')
</code></pre>
<p>The given emoji is considered to be negative by the original lexicon, but I want it to be positive instead. However, updating according to the above code does not seem to work at all:</p>
<blockquote>
<p>{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.34}</p>
</blockquote>
","python, sentiment-analysis, vader","<p>So apparently Vader transforms emojis to their word representation prior to extracting sentiment. You can find this mapping in &quot;site-packages/vaderSentiment/emoji_utf8_lexicon.txt&quot;.</p>
<p>Updating the code to:</p>
<pre><code>new_words = {
    'fire': 4.0,
}
</code></pre>
<p>works.</p>
",1,1,1234,2020-08-04 14:25:00,https://stackoverflow.com/questions/63249001/vadersentiment-unable-to-update-emoji-sentiment-score
More than 14 API search results for Twitter Sentiment Analysis?,"<p>I am currently using Tweepy to access the Twitter API for some sentiment analysis.
<strong>However, when I run the following code:</strong></p>
<pre><code>auth = tweepy.OAuthHandler(consumer_key,consumer_secret)
auth.set_access_token(access_token,access_token_secret)
api = tweepy.API(auth, wait_on_rate_limit = True)


public_tweets = api.search('Donald Trump')
</code></pre>
<p>I only get <strong>14 tweets</strong>? Is There any way to retrieve more/specify that you want more??
Any help would be much appreciated!!</p>
","python, twitter, tweepy, sentiment-analysis, twitterapi-python","<p>Try using this:</p>
<pre><code>auth = tweepy.OAuthHandler(consumer_key,consumer_secret)
auth.set_access_token(access_token,access_token_secret)
api = tweepy.API(auth, wait_on_rate_limit = True)

your_query = &quot;your query&quot;
public_tweets = api.search(your_query, count=100)
</code></pre>
<p>This is the maximum number of tweets you can get in one request. By default the count is set to 15. Moreover by default it shows the most recent 100 tweets which include your query.</p>
<p>If you need more than 100 Tweets you need either save the minimum id and set is as max_id attribute or use tweepy.cursor. If you search you will find its tutorial on tweepy doc.</p>
",1,0,93,2020-08-13 03:49:20,https://stackoverflow.com/questions/63387866/more-than-14-api-search-results-for-twitter-sentiment-analysis
Find extreme emotions with Python Sentiment Analysis,"<p>Basically I want to find Twitter comments that are <strong>very</strong> positive, and ignore those that are just a little positive. The sentiment analysis methods I found only tell if the sentiment is positive/neutral/negative, but don't say how positive or negative is the comment.</p>
<p>How could this be done with Python?</p>
","python, data-science, sentiment-analysis","<p>As mentioned, u could use a pretrained model - <code>BERT</code>, that comes with emmbeding etc.<br />
U can see <a href=""https://www.kaggle.com/praveengovi/classify-emotions-in-text-with-bert"" rel=""nofollow noreferrer"">here</a> an exact implementation using <code>transformers</code> in pytorch.</p>
<p>Also, check out <code>SPARK NLP</code> <a href=""https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/SENTIMENT_EN_EMOTION.ipynb"" rel=""nofollow noreferrer"">notebook</a> for emotion detection.</p>
",1,0,263,2020-08-17 00:29:26,https://stackoverflow.com/questions/63443166/find-extreme-emotions-with-python-sentiment-analysis
AI bias in the sentiment analysis,"<p>Using sentiment analysis API and want to know how the AI bias that gets in through the training set of data and other biases quantified. Any help would be appreciated.</p>
","sentiment-analysis, azure-cognitive-services, text-analytics-api","<p>There are several tools developed to deal with it:
Fair Learn <a href=""https://fairlearn.github.io/"" rel=""nofollow noreferrer"">https://fairlearn.github.io/</a>
Interpretability Toolkit <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability</a></p>
<p>In Fair Learn you can see how biased a ML model is after it has been trained with the data set and choose a maybe less accurate model which performs better with biases. The explainable ML models provide different correlation of inputs with outputs and combined with Fair Learn can give an idea of the health of the ML model.</p>
",0,-1,162,2020-08-17 14:16:22,https://stackoverflow.com/questions/63452739/ai-bias-in-the-sentiment-analysis
Calculate sentiment of each row in a big dataset using R,"<p>I having trouble calculating average sentiment of each row in a relatively big dataset (N=36140).
My dataset containts review data from an app on Google Play Store (each row represents one review) and I would like to calculate sentiment of each review using <code>sentiment_by()</code> function.
The problem is that this function takes a lot of time to calculate it.</p>
<p>Here is the link to my dataset in .csv format:</p>
<p><a href=""https://drive.google.com/drive/folders/1JdMOGeN3AtfiEgXEu0rAP3XIe3Kc369O?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1JdMOGeN3AtfiEgXEu0rAP3XIe3Kc369O?usp=sharing</a></p>
<p>I have tried using this code:</p>
<pre><code>library(sentimentr)
e_data = read.csv(&quot;15_06_2016-15_06_2020__Sygic.csv&quot;, stringsAsFactors = FALSE)
sentiment=sentiment_by(e_data$review)
</code></pre>
<p>Then I get the following warning message (After I cancel the process when 10+ minutes has passed):</p>
<pre><code>Warning message:
Each time `sentiment_by` is run it has to do sentence boundary disambiguation when a
raw `character` vector is passed to `text.var`. This may be costly of time and
memory.  It is highly recommended that the user first runs the raw `character`
vector through the `get_sentences` function. 
</code></pre>
<p>I have also tried to use the <code>get_sentences()</code> function with the following code, but the <code>sentiment_by()</code> function still needs a lot of time to execute the calculations</p>
<pre><code>e_sentences = e_data$review %&gt;%
  get_sentences() 
e_sentiment = sentiment_by(e_sentences)
</code></pre>
<p>I have datasets regarding the Google Play Store review data and I have used the sentiment_by() function for the past month and it worked very quickly when calculating the sentiment... I only started to run calculations for this long since yesterday.</p>
<p>Is there a way to quickly calculate sentiment for each row on a big dataset.</p>
","r, sentiment-analysis, sentimentr","<p>The algorithm used in <code>sentiment</code> appears to be O(N^2) once you get above 500 or so individual reviews, which is why it's suddenly taking a lot longer when you upped the size of the dataset significantly. Presumably it's comparing every pair of reviews in some way?</p>
<p>I glanced through the help file (<code>?sentiment</code>) and it doesn't seem to do anything which depends on pairs of reviews so that's a bit odd.</p>
<pre><code>library(data.table)
reviews &lt;- iconv(e_data$review, &quot;&quot;) # I had a problem with UTF-8, you may not need this
x1 &lt;- rbindlist(lapply(reviews[1:10],sentiment_by))
x1[,element_id:=.I]
x2 &lt;- sentiment_by(reviews[1:10])
</code></pre>
<p>produce effectively the same output which means that the <code>sentimentr</code> package has a bug in it causing it to be unnecessarily slow.</p>
<p>One solution is just to batch the reviews. This will break the 'by' functionality in <code>sentiment_by</code>, but I think you should be able to group them yourself before you send them in (or after as it doesnt seem to matter).</p>
<pre><code>batch_sentiment_by &lt;- function(reviews, batch_size = 200, ...) {
  review_batches &lt;- split(reviews, ceiling(seq_along(reviews)/batch_size))
  x &lt;- rbindlist(lapply(review_batches, sentiment_by, ...))
  x[, element_id := .I]
  x[]
}

batch_sentiment_by(reviews)
</code></pre>
<p>Takes about 45 seconds on my machine (and should be <code>O(N)</code> for bigger datasets.</p>
",1,0,969,2020-08-22 07:54:39,https://stackoverflow.com/questions/63533848/calculate-sentiment-of-each-row-in-a-big-dataset-using-r
Load and prepare a new dataset,"<p>I'm using tf to create a sentiment analysis model. Since I'm a noob of machine learning I followed a guide on the official documentation of Tensorflow to train and test a model with the IMDB_reviews dataset. It works pretty well but I wish I could train it with another dataset.</p>
<p>So I've downloaded this dataset: &quot;movie_review.csv&quot;. It contains various columns and I want to access text and tag (where the tag is a positive or negative value and text is the text of the review).</p>
<p>What I want to do is to prepare the CSV as a dataset, access text and tag, vectorize them, and feed them to the network. There is no division between test and train, so I have to divide the file too.
So, I want to know how to:</p>
<p><strong>0</strong>- Access the file I've downloaded and transform it into a dataset.<br />
<strong>1</strong>- Access text and tag in the file, maybe without using pandas. If pandas is recommended and there is a simple way to access the file and passing to a network using TensorFlow I'll be okay with the answer.<br />
<strong>2</strong>- Splitting the file in the test set and train set (I've already found a pandas solution for this actually).<br />
<strong>3</strong>- Vectorize my text and tag to feed my network.
If you have an entire guide on how to do this, it'll be fine, it just has to use TensorFlow.</p>
<p><strong>Questions 0 to 3 have been answered</strong></p>
<p>Ok so, I have used the file posted to load a dataset to train the model on short sentences, but I'm having trouble with the training.
When I followed the guide to build the model for text classification I came out with this code:</p>
<pre><code>dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)

train_dataset, test_dataset = dataset['train'], dataset['test']
encoder = info.features['text'].encoder

BUFFER_SIZE = 10000
BATCH_SIZE = 64

padded_shapes = ([None], ())

train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes = padded_shapes)

test_dataset = test_dataset.padded_batch(BATCH_SIZE, padded_shapes = padded_shapes)

model = tf.keras.Sequential([tf.keras.layers.Embedding(encoder.vocab_size, 64),
                            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
                            tf.keras.layers.Dense(64, activation='relu'),
                            tf.keras.layers.Dense(1, activation='sigmoid')])
model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4),
                metrics=['accuracy'])

history = model.fit(train_dataset, epochs = 1, validation_data = test_dataset, validation_steps=30, callbacks=[cp_callback])
</code></pre>
<p>So, I trained my model this way (Some parts are missing, I have included all the fundamental ones). After this, I wanted to train the model with another dataset, and thanks to Andrew I have accessed a dataset created by me this way:</p>
<pre><code>csv_dataset = tf.data.experimental.CsvDataset(filepath, default_values, header=header)

def reshape_dataset(txt, tag):
    txt = tf.reshape(txt, shape=(1,))
    tag = tf.reshape(tag, shape=(1,))
    return txt, tag

csv_dataset = csv_dataset.map(reshape_dataset)

training = csv_dataset.take(10) 
testing = csv_dataset.skip(10)

</code></pre>
<p>And my problem is to adapt the dataset to the model I already have. I have tried various solution, but I get errors on the shapes.
Can somebody be so gentle to explain me how to do this? Obviously the solution for step 3 has already been posted by Andrew in his file, but I'd like to use my model with the weights I have saved during training.</p>
","python, tensorflow, sentiment-analysis","<p>This sounds like a great place to use Tensorflow's Dataset API. <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer"">Here's</a> a notebook/tutorial that covers how to do some basic data input and preprocessing stuff, right from Tensorflow's website!</p>
<p>I have also made a notebook with a quick example, answering each of your questions with implementations. You can find that <a href=""https://colab.research.google.com/drive/18smWddd4oqIczVNTYQwxC42RcwyIyMFr?usp=sharing"" rel=""nofollow noreferrer"">here</a>.</p>
",1,0,284,2020-08-26 16:47:38,https://stackoverflow.com/questions/63602177/load-and-prepare-a-new-dataset
How to replace tokens if they are used together?,"<p>I would like to do a sentimental analysis on the topic COVID-19 using python. The problem arises that entries like &quot;positive tested&quot; receive a positive polarity, although this statement is a negative declaration. My current code is as follows:</p>
<pre><code>import nltk
from textblob import TextBlob
from nltk.stem import WordNetLemmatizer

# Setting the test string
test_string = &quot;He was tested positive on Covid-19&quot;

tokens = nltk.word_tokenize(test_string)

# Lemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

tokens_lem_list = []
for word in tokens:
    lem_tokens = wordnet_lemmatizer.lemmatize(word, pos=&quot;v&quot;)
    tokens_lem_list.append(lem_tokens)

# List to string
tokens_lem_str = ' '.join(tokens_lem_list)

# Print the polarity of the string
print(TextBlob(tokens_lem_str).sentiment.polarity)
</code></pre>
<p>With the following output:</p>
<pre><code>0.22727272727272727

Process finished with exit code 0
</code></pre>
<p>Therefore, I want to delete the tokens &quot;test&quot; and &quot;positive&quot;, if they are used together, and replace them with the word &quot;ill&quot;. Should I use a loop or would this only eat up my computing capacity with a large amount of text?</p>
<p>Thanks a lot for your help!</p>
","python, nlp, nltk, token, sentiment-analysis","<p>I have solved my problem as follows:</p>
<pre><code># Producing a loop which finds &quot;positive&quot; and &quot;negative&quot; tested string entries
matches_positive = [&quot;test&quot;, &quot;positive&quot;]
matches_negative = [&quot;test&quot;, &quot;negative&quot;]

replaced_testing_term_sentence = []
for sentence_lem in sentences_list_lem:
    # Constrain to replace &quot;positive tested&quot; by &quot;not healthy&quot;
    if all(x in sentence_lem for x in matches_positive):
        sentence_lem = [word.replace(&quot;positive&quot;, &quot;not healthy&quot;) for word in sentence_lem]
        sentence_lem.remove(&quot;test&quot;)
        replaced_testing_term_sentence.append(sentence_lem)
    # Constrain to replace &quot;negative tested&quot; by &quot;not ill&quot;
    elif all(x in sentence_lem for x in matches_negative):
        sentence_lem = [word.replace(&quot;negative&quot;, &quot;not ill&quot;) for word in sentence_lem]
        sentence_lem.remove(&quot;test&quot;)
        replaced_testing_term_sentence.append(sentence_lem)
    # Constrain to remain not matching sentences in the data sample
    else:
        replaced_testing_term_sentence.append(sentence_lem)
</code></pre>
<p>It does the job. The selected replacement terms are deliberately chosen. If anybody sees potential for optimization, I would appreciate a comment.</p>
",0,0,132,2020-09-01 14:29:37,https://stackoverflow.com/questions/63689735/how-to-replace-tokens-if-they-are-used-together
Get sentiment score of emoji #Python,"<pre><code>df
0        NaN
1        NaN
2         🤩🤩
3        NaN
4          ❤
        ... 
26368    NaN
26369    NaN
26370    NaN
26371     🔥👌
26372    NaN
Name: emojis, Length: 26373, dtype: object
</code></pre>
<p>From the df above, I would like to calculate the sentiment score of the emojis in each row.
If NaN, then return NaN.</p>
<pre><code>#!pip install emosent-py
from emosent import get_emoji_sentiment_rank
def emoji_sentiment(text):
    return get_emoji_sentiment_rank(text)[&quot;sentiment_score&quot;]

emoji_sentiment(&quot;😂&quot;)
--&gt; 0.221
</code></pre>
<p>Applying to the whole column</p>
<pre><code>df['emoji_sentiment'] = df['emojis'].apply(emoji_sentiment)
</code></pre>
<p>The code above returns <code>KeyError: nan</code></p>
<p>Expected result:</p>
<pre><code>          df             emoji_sentiment
0        NaN         |         NaN
1        NaN         |         NaN
2         🤩🤩      |  (a decimal number)
3        NaN         |         NaN
4          ❤        |   (a decimal number)
        ... 
26368    NaN         |         NaN
26369    NaN         |         NaN
26370    NaN         |         NaN
26371     🔥👌       |   (a decimal number)
26372    NaN         |         NaN
</code></pre>
","python, pandas, emoji, sentiment-analysis","<p>From your error, I'm guessing <code>get_emoji_sentiment_rank(text)[&quot;sentiment_score&quot;]</code> fails if text is <code>NaN</code>, so you can either apply the function and assign the update only to the rows that re non-nan (preferable, but you first need to crate the column <code>emoji_sentiment</code> with a default <code>NaN</code> value):</p>
<pre><code>df['emoji_sentiment'] = np.NaN # init the value for all rows
not_na_idx = ~df.emojis.isna()
df.loc[not_na_idx, 'emoji_sentiment'] = df.loc[not_na_idx, 'emojis'].apply(emoji_sentiment)
</code></pre>
<p>or you change the return of <code>emoji_sentiment()</code>:</p>
<pre><code>def emoji_sentiment(text):
    return get_emoji_sentiment_rank(text)[&quot;sentiment_score&quot;] if not pd.isna(text) else np.NaN
</code></pre>
<p>(uglier and less performant, but stll feasible)</p>
",0,0,1237,2020-09-09 08:31:57,https://stackoverflow.com/questions/63807765/get-sentiment-score-of-emoji-python
"I am trying to parse a website and generate positive, neutral, or negative sentiment analysis","<p>I am trying to get a very basic sentiment analysis from the CNBC site.  I put together this piece of code, and it works fine.</p>
<pre><code>from bs4 import BeautifulSoup
import urllib.request
from  pandas import DataFrame

resp = urllib.request.urlopen(&quot;https://www.cnbc.com/finance/&quot;)
soup = BeautifulSoup(resp, from_encoding=resp.info().get_param('charset'))
    
substring = 'https://www.cnbc.com/'

df = ['review']
for link in soup.find_all('a', href=True):
    print(link['href'])
    if (link['href'].find(substring) == 0): 
        # append
        df.append(link['href'])

#print(link['href'])


#list(df)
# convert list to data frame
df = DataFrame(df)
#type(df)
#list(df)

# add column name
df.columns = ['review']

# clean up
df['review'] = df['review'].str.replace('\d+', '')

# Get rid of special characters
df['review'] = df['review'].str.replace(r'[^\w\s]+', '')


from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()
df['sentiment'] = df['review'].apply(lambda x: sid.polarity_scores(x))
def convert(x):
    if x &lt; 0:
        return &quot;negative&quot;
    elif x &gt; .2:
        return &quot;positive&quot;
    else:
        return &quot;neutral&quot;
df['result'] = df['sentiment'].apply(lambda x:convert(x['compound']))
df['result']
</code></pre>
<p>When I run the code above I get positives and negatives, but these are not mapped to the original 'review'.  How can I show each sentiment, in a data frame, next to the language of each link?  Thanks!</p>
","python, python-3.x, machine-learning, sentiment-analysis","<p>Oh, man, I am totally losing it!  This was just a simple merge!!</p>
<pre><code>df_final = pd.merge(df['review'], df['result'], left_index=True, right_index=True)
df_final
</code></pre>
<p>Result:</p>
<pre><code>0                                              review  neutral
1                      https://www.cnbc.com/business/  neutral
2   https://www.cnbc.com/2020/09/15/stocks-making-...  neutral
3   https://www.cnbc.com/2020/09/15/stocks-making-...  neutral
4             https://www.cnbc.com/maggie-fitzgerald/  neutral
..                                                ...      ...
90                      https://www.cnbc.com/finance/  neutral
91  https://www.cnbc.com/2020/09/10/citi-ceo-micha...  neutral
92                https://www.cnbc.com/central-banks/  neutral
93  https://www.cnbc.com/2020/09/10/watch-ecb-pres...  neutral
94               https://www.cnbc.com/finance/?page=2  neutral
</code></pre>
",1,1,74,2020-09-15 02:42:23,https://stackoverflow.com/questions/63894296/i-am-trying-to-parse-a-website-and-generate-positive-neutral-or-negative-senti
Sentiment Analysis Feature Selection based on word to label correlation,"<p>In my sentiment analysis on a dataset of 194k review texts with labels (class 1-5), I am trying to reduce the features (words) based on a word to label correlation by which a classifier can be trained.</p>
<p>Using sklearn.feature_extraction.text.CountVectorizer with default parameterization, I get 86,7k features. When performing fit_transform, I got a CSR-sparse matrix which I tried to put into a data frame using toarray().</p>
<p>Unfortunately, an array of size (194439,86719) causes a Memory Error. I think I need it to be in the data frame in order to calculate the correlations with df.corr(). Below you find my coding:</p>
<pre><code>corpus = data['reviewText']
vectorizer = CountVectorizer(analyzer ='word')
X = vectorizer.fit_transform(corpus)
content = X.toarray()         # here comes the Memory Error
vocab = vectorizer.get_feature_names()
df = pd.DataFrame(data= X.toarray(), columns=vocab)
corr = pd.Series(df.corrwith(df['overall']) &gt; 0.6)
new_vocab = df2[corr[corr == True].index]    # should return features that we want to use
</code></pre>
<p>Is there a way to filter by correlation without having to change the format into a data frame?
Most posts that were going into the same direction of using correlation on df do not have to handle the large data amount.</p>
","python, correlation, sentiment-analysis","<p>I figured that there are other ways to implement a feature selection based on the correlation. With SelectKBest and the scoring function f_regression.</p>
",0,0,142,2020-09-15 16:04:44,https://stackoverflow.com/questions/63905775/sentiment-analysis-feature-selection-based-on-word-to-label-correlation
Sentiment analysis with IBM Watson,"<p>I am trying to build a sentiment analysis bot with IBM Watson for slack. <a href=""https://api.slack.com/tutorials/watson-sentiment#setting_up_your_request_url"" rel=""nofollow noreferrer"">https://api.slack.com/tutorials/watson-sentiment#setting_up_your_request_url</a></p>
<p>I got stuck quite early, as when running the node script of index.js, I keep getting this error:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>SyntaxError: Unexpected token 'const'
    at wrapSafe (internal/modules/cjs/loader.js:1053:16)
    at Module._compile (internal/modules/cjs/loader.js:1101:27)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1157:10)
    at Module.load (internal/modules/cjs/loader.js:985:32)
    at Function.Module._load (internal/modules/cjs/loader.js:878:14)
    at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:71:12)
    at internal/main/run_main_module.js:17:47
(base) ndaa-qtn3118-mbp:sentimentapp qtn3118$ nano index.js
(base) ndaa-qtn3118-mbp:sentimentapp qtn3118$ node index.js
/Users/qtn3118/sentimentapp/index.js:6
const server = app.listen(5000, () =&gt; {  
^^^^^

SyntaxError: Unexpected token 'const'
    at wrapSafe (internal/modules/cjs/loader.js:1053:16)
    at Module._compile (internal/modules/cjs/loader.js:1101:27)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1157:10)
    at Module.load (internal/modules/cjs/loader.js:985:32)
    at Function.Module._load (internal/modules/cjs/loader.js:878:14)
    at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:71:12)
    at internal/main/run_main_module.js:17:47</code></pre>
</div>
</div>
</p>
<p>I checked my code with the example github, and it doesn't look like I have any typos, here is the .js file:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const express = require('express');
const bodyParser = require('body-parser');
const app = express();
...

const server = app.listen(5000, () =&gt; {  
  console.log('Express server listening on port %d in %s mode', server.address().port, app.settings.env);});

app.post('/event', (req, res) =&gt; {
  if (req.body.type === 'url_verification') {
    res.send(req.body.challenge);
  }});</code></pre>
</div>
</div>
</p>
","node.js, express, ibm-watson, slack-api, sentiment-analysis","<p>the &quot;...&quot; on the 4th line is not supposed to be in your actual code. It's just a shorthand for you to delete, meaning that other libraries or variable definitions could appear there.</p>
<p>Just remove the &quot;...&quot;</p>
",0,0,66,2020-09-21 13:23:01,https://stackoverflow.com/questions/63993189/sentiment-analysis-with-ibm-watson
Using a Filter in searchTwitter package in R,"<p>I am using the code below to extract tweets with #walmart. I would like 1200 tweets but I want to exclude tweets that have #walmarthelp as a second hashtag.</p>
<pre><code>tweets_a &lt;- searchTwitter(&quot;#walmart&quot;, n = 1200, lang = &quot;en&quot;, since=&quot;2020-09-13&quot;)
</code></pre>
<p>A simple filter after the tweets are extracted will reduce the critical component of <code>n=1200</code>.</p>
","r, filter, twitter, sentiment-analysis, social-media","<p>Since this uses Twitter's search, the best bet would be using that to your advantage. Twitter allows you to exclude terms using <code>-</code>(minus) within the search query. In your case, <code>&quot;#walmart -#walmarthelp&quot;</code> should exclude the Walmart Help hashtag.</p>
",2,1,45,2020-09-23 13:48:56,https://stackoverflow.com/questions/64029450/using-a-filter-in-searchtwitter-package-in-r
"NLP, difference between using NLTK&#39;s sentiment analysis and using ML approach","<p>I recently started to learn NLP and ML using Python.
I started with Sentiment Analysis.
I'm having trouble understanding where machine learning comes in to play when doing sentiment analysis.</p>
<p>Let's say I'm analyzing tweets or news headlines using NLTK's SentimentIntensityAnalyzer and I'm loading a case relevant lexicons so I'm getting polarity and negativity, positivity, neutral scores.
Now what I don't understand is, in which case should I use code like in this article:</p>
<p><a href=""https://medium.com/dataseries/sentiment-classifier-using-tfidf-3ffce3f1cbd5"" rel=""nofollow noreferrer"">Sentiment with ML toturial</a></p>
<p>or just the built-in like in NLTK or even something like Google's BERT?</p>
<p>Any answer or link to Blog or tutorial would be welcomed!</p>
","python, machine-learning, nlp, nltk, sentiment-analysis","<p><code>SentimentIntensityAnalyzer</code> is a tool built specifically for analyzing sentiment, it is easy to use, but can miss some cases, for example:</p>
<pre><code>In [52]: from nltk.sentiment.vader import SentimentIntensityAnalyzer                                                

In [53]: sia = SentimentIntensityAnalyzer()                                                                         

In [54]: sia.polarity_scores(&quot;I am not going to miss using this product.&quot;)                                          
Out[54]: {'neg': 0.0, 'neu': 0.829, 'pos': 0.171, 'compound': 0.1139}
</code></pre>
<p>A Machine Learning approach, like the one outlined in your link more involved it focuses on creating features, often using TF-IDF, but certainly not limited to. And then a Machine Learning is used on top of that. This approach relies on availability of good enough and large enough training dataset. Often feature extraction is the more important part and a simple model, like Logistic Regression is chosen.</p>
<p>BERT is pretrained model, that can be fine tuned, thought it doesn't have to be I found that fine tuning helps in my experience.</p>
<p>The main advantages of BERT:</p>
<ol>
<li><p>With enough training data BERT can be very powerful, with enough training data it should be able to get an example in the beginning of my post correctly. And this is a huge advantage.</p>
</li>
<li><p>Since BERT is already pretrained it might require relatively small number of training samples to give good reasonable results.</p>
</li>
<li><p>Because BERT does not require (or require a lot less) feature engineering, it can be fast in terms of ML engineering work to get good initial results.</p>
</li>
</ol>
<p>The main limitations of BERT are:</p>
<ol>
<li><p>Learning curve, mostly conceptually understanding how it works. Using BERT is not very hard.</p>
</li>
<li><p>BERT is slow to train and predict. You pretty much have to use at least a moderate GPU even for a small dataset.</p>
</li>
<li><p>Lack of transparency. It is really hard to know why BERT based model is suggesting what it is suggesting.</p>
</li>
</ol>
",3,4,2798,2020-09-25 16:38:55,https://stackoverflow.com/questions/64068144/nlp-difference-between-using-nltks-sentiment-analysis-and-using-ml-approach
Deleting the rows from sqlite db still the values are not getting deleted and size is getting bigger,"<p>I am collecting the twitter stream and storing it in a sqlite db.Since the streams are coming and database is getting bigger i executed a command to delete the tweets that are older than a minute.But the tweets are there only and database is getting bigger.Please help since as I am new to sqlite</p>
<p>Here's the code</p>
<pre><code>class listener(StreamListener):

def on_data(self,data):
    try:
        data = json.loads(data)
        tweet = unidecode(data['text'])
        text = preprocess(tweet)
        score = predict(text)['score']
        
        created_at = data['created_at']
        

        
        

        c.execute('INSERT INTO sentiment (created_at,tweet,score) VALUES (?,?,?)',(created_at,tweet,score))
        conn.commit()

      
        
        c.execute('DELETE FROM sentiment WHERE created_at IN(SELECT created_at FROM(SELECT 
                  created_at, strftime(&quot;%s&quot;,&quot;now&quot;) - strftime(&quot;%s&quot;,created_at) AS passed_time FROM 
                sentiment WHERE passed_time &gt;=60))')
       
        conn.commit()

    except Exception as e:
        print(str(e))
</code></pre>
","python-3.x, sqlite, tweepy, sentiment-analysis","<p>You are testing IN subquery, which in turn has a subquery,
and you're complaining that this complex approach didn't work,
that IN found no matches
among your &quot;seconds since 1970&quot; timestamps.</p>
<p>Ok. Your spec is <em>much</em> simpler than that, you said you want</p>
<blockquote>
<p>a command to delete the tweets that are older than a minute</p>
</blockquote>
<p>Piece of cake. Just follow that English sentence and turn it into SQL:</p>
<pre><code>DELETE FROM sentiment WHERE created_at &lt; strftime('%s', 'now') - 60;
</code></pre>
<p>Current time minus sixty seconds is a minute ago,
and the WHERE clause asks for rows older than that.</p>
",1,0,54,2020-10-03 16:05:12,https://stackoverflow.com/questions/64186232/deleting-the-rows-from-sqlite-db-still-the-values-are-not-getting-deleted-and-si
Spacy&#39;s Dependency Parser,"<p>I was trying to play around with Spacy's Dependency Parser to extract Aspect for Aspect Based Sentiment Analysis.
I followed this link: <a href=""https://remicnrd.github.io/Aspect-based-sentiment-analysis/"" rel=""nofollow noreferrer"">https://remicnrd.github.io/Aspect-based-sentiment-analysis/</a></p>
<p>When I tried the following piece of the code on my data, I got an Error message.</p>
<pre><code>import spacy
nlp = spacy.load('en')

dataset.review = dataset.review.str.lower()

aspect_terms = []
for review in nlp.pipe(dataset.review):
    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']
    aspect_terms.append(' '.join(chunks))
dataset['aspect_terms'] = aspect_terms
dataset.head(10)
</code></pre>
<p>The Error message was:</p>
<blockquote>
<p>TypeError: object of type 'NoneType' has no len()</p>
</blockquote>
<p>The Error was in this line:</p>
<pre><code>for review in nlp.pipe(dataset.review):
</code></pre>
<p>Could someone please help me understand the issue here and how to resolve this. Thanks.</p>
","spacy, sentiment-analysis","<p>Writing the solution here incase it helps someone in future.
I was getting the Error because I had some empty rows for the column review.
I re-ran the code after removing the empty rows/rows with NaN values for the column reviews and it works fine :)</p>
",2,1,376,2020-10-06 19:52:05,https://stackoverflow.com/questions/64232902/spacys-dependency-parser
How can I handle InvalidArgumentError in keras while testing my model?,"<p>I'am doing sentiment analysis on <a href=""http://help.sentiment140.com/for-students"" rel=""nofollow noreferrer"">stanford140 dataset</a>.</p>
<p>I have designed a keras model with this architecture :</p>
<pre><code>    embedding_layer = layers.Embedding(12995, 300, weights=[embedding_vectors], 
    input_length= 50, trainable=False)
    model = keras.models.Sequential()
    model.add(embedding_layer)
    model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu'))
    model.add(layers.MaxPooling1D(pool_size=2,strides=2))
    model.add(layers.LSTM(100))
    model.add(layers.Dense(64))
    model.add(layers.Dense(1, activation='sigmoid'))

</code></pre>
<p>Function below shows how I created embedding_vectors (embedding_vectors is used in embedding_layer of previous piece of code)</p>
<pre><code>def _create_embedding_vectors(self):
    self.vocab_size = len(self.tokenizer.word_index) + 1
    embedding_vectors = np.zeros((self.vocab_size,300))
    for word,i in self.tokenizer.word_index.items():
      try :
        embedding_vectors[i] = self.w2v[word]
      except KeyError :
        embedding_vectors[i] = np.zeros(300)

    return embedding_vectors 

</code></pre>
<p><code>self.vocab_size</code> is 12995 and <code>self.tokenizer</code> has been imported from <code>tensorflow.keras.preprocessing.text</code> .
<code>self.w2v</code> is <strong>google news pretrained vectors</strong> .</p>
<p>I trained my model and every thing was ok during training process. So I pickled the model for further uses.</p>
<p>But in another file where I load model and pass some sentences to it in order to classifying them, the <em>Classifier class</em> raises <em>InvalidArgumentError</em>. Code below shows <em>Classifier class</em>.</p>
<pre><code>class Classifier :
  

  def __init__(self,path_to_model,path_to_tokenizer) :
    self.model = self.load_model(path_to_model)
    self.tokenizer = self.load_tokenizer(path_to_tokenizer)

  

  def text_preprocessor (self,tweet) :
    # remove user mentions 
    tweet =re.sub('\s*@[a-zA-Z0-9]*\s*',' ',tweet) 
    # remove signle character
    tweet =re.sub('\s+[a-zA-Z0-9]\s+',' ',tweet) 
    # remove hashtag sign 
    tweet = re.sub('#','',tweet) 
    # remove underline 
    tweet = re.sub('_',' ',tweet) 
    # remove dash
    tweet = re.sub('-',' ',tweet) 
    # translate &amp;
    tweet = re.sub('&amp;', ' and ' , tweet)
    # lower
    tweet = tweet.lower()
    # remove punctuation 
    tweet = ' '.join([token for token in nltk.word_tokenize(tweet) if token not in punctuation])
    return tweet


  def load_model(self,path_to_model) :
    return keras.models.load_model(path_to_model)

  def load_tokenizer(self,path_to_tokenizer) :
    with open(path_to_tokenizer, 'rb') as file_reader:
      tokenizer = pickle.load(file_reader)
    return tokenizer 

  def transform_tweets(self,tweets) :
    encoded_docs = self.tokenizer.texts_to_sequences(tweets)
    max_length = 50
    x_test = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
    return x_test

  def predict(self,tweets) :
    tweets = list(map(self.text_preprocessor,tweets)) 
    x_test = self.transform_tweets(tweets) 
    y_pred = self.model.predict(x_test)
    return np.round(y_pred)
</code></pre>
<p>I face this error when I pass some texts to model for predicting their sentiment.</p>
<pre><code>(0) Invalid argument: indices[3,16] = 12399 is not in [0, 12210)
 [[node sequential_111/embedding_111/embedding_lookup (defined at &lt;ipython-input-2-71e9b0ec6210&gt;:47) ]]
(1) Invalid argument: indices[3,16] = 12399 is not in [0, 12210)
 [[node sequential_111/embedding_111/embedding_lookup (defined at &lt;ipython-input-2-71e9b0ec6210&gt;:47) ]] [[sequential_111/embedding_111/embedding_lookup/_6]]
</code></pre>
<p><em><strong>Can anyone help me to avoid this error ?</strong></em></p>
","python, tensorflow, keras, sentiment-analysis","<p>I was scrimmaging this error about 3 weeks. Finally by omitting <code>input_length= 50</code> from piece of code for designing model, I avoid that error :)
code below shows how I created the model .</p>
<hr />
<pre><code>    embedding_layer = layers.Embedding(12995, 300, weights=[embedding_vectors], trainable=False)
    model = keras.models.Sequential()
    model.add(embedding_layer)
    model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu'))
    model.add(layers.MaxPooling1D(pool_size=2,strides=2))
    model.add(layers.LSTM(100))
    model.add(layers.Dense(64))
    model.add(layers.Dense(1, activation='sigmoid'))

</code></pre>
<p>HOPE TO BE USEFUL FOR SOMEONE ELSE WHO IS TIRED OF THIS ERROR.</p>
",0,0,211,2020-10-07 08:54:04,https://stackoverflow.com/questions/64240572/how-can-i-handle-invalidargumenterror-in-keras-while-testing-my-model
gridsearchcv with tfidf and count vectorizer,"<p>I want to use GridSearchCV for parameter tuning. Is it also possible to check with GridSearchCV whether CountVectorizer or TfidfVectorizer works best? My idea:</p>
<pre><code>pipeline = Pipeline([
           ('vect', TfidfVectorizer()),
           ('clf', SGDClassifier()),
])
parameters = {
'vect__max_df': (0.5, 0.75, 1.0),
'vect__max_features': (None, 5000, 10000, 50000),
'vect__ngram_range': ((1, 1), (1, 2), (1,3),  
'tfidf__use_idf': (True, False),
'tfidf__norm': ('l1', 'l2', None),
'clf__max_iter': (20,),
'clf__alpha': (0.00001, 0.000001),
'clf__penalty': ('l2', 'elasticnet'),
'clf__max_iter': (10, 50, 80),
}

grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)
</code></pre>
<p>My idea: CountVectorizer is the same as TfidfVectorizer with use_idf=False and normalize=None. If GridSearchCV gives this as the best result those parameters, then CountVectorizer is the best option. Is that correct?</p>
<p>Thank you in advance :)</p>
","python, machine-learning, scikit-learn, sentiment-analysis, gridsearchcv","<p>Once you've included a given step with its corresponding name in the <code>Pipeline</code>, you can access it from the parameter grid and add other parameters, or vectorizers in this case, in the grid. You can also have a list of grids in a single pipeline:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer

pipeline = Pipeline([
           ('vect', TfidfVectorizer()),
           ('clf', SGDClassifier()),
])
parameters = [{
    'vect__max_df': (0.5, 0.75, 1.0),
    'vect__max_features': (None, 5000, 10000, 50000),
    'vect__ngram_range': ((1, 1), (1, 2), (1,3),)  
    'tfidf__use_idf': (True, False),
    'tfidf__norm': ('l1', 'l2', None),
    'clf__max_iter': (20,),
    'clf__alpha': (0.00001, 0.000001),
    'clf__penalty': ('l2', 'elasticnet'),
    'clf__max_iter': (10, 50, 80)
},{
    'vect': (CountVectorizer(),)
    # count_vect_params...
    'clf__max_iter': (20,),
    'clf__alpha': (0.00001, 0.000001),
    'clf__penalty': ('l2', 'elasticnet'),
    'clf__max_iter': (10, 50, 80)
}]

grid_search = GridSearchCV(pipeline, parameters)
</code></pre>
",4,6,6518,2020-10-08 08:26:33,https://stackoverflow.com/questions/64258622/gridsearchcv-with-tfidf-and-count-vectorizer
TheGuardian API - Script crashes,"<pre><code>import json
import requests
from os import makedirs
from os.path import join, exists
from datetime import date, timedelta

ARTICLES_DIR = join('tempdata', 'articles')
makedirs(ARTICLES_DIR, exist_ok=True)

API_ENDPOINT = 'http://content.guardianapis.com/search'
my_params = {
    'q': 'coronavirus,stock,covid',
    'sectionID': 'business',
    'from-date': &quot;2019-01-01&quot;,
    'to-date': &quot;2020-09-30&quot;,
    'order-by': &quot;newest&quot;,
    'show-fields': 'all',
    'page-size': 300,
    'api-key': '### my cryptic key ###'
}


# day iteration from here:
# http://stackoverflow.com/questions/7274267/print-all-day-dates-between-two-dates
start_date = date(2019, 1, 1)
end_date = date(2020,9, 30)
dayrange = range((end_date - start_date).days + 1)
for daycount in dayrange:
    dt = start_date + timedelta(days=daycount)
    datestr = dt.strftime('%Y-%m-%d')
    fname = join(ARTICLES_DIR, datestr + '.json')
    if not exists(fname):
        # then let's download it
        print(&quot;Downloading&quot;, datestr)
        all_results = []
        my_params['from-date'] = datestr
        my_params['to-date'] = datestr
        current_page = 1
        total_pages = 1
        while current_page &lt;= total_pages:
            print(&quot;...page&quot;, current_page)
            my_params['page'] = current_page
            resp = requests.get(API_ENDPOINT, my_params)
            data = resp.json()
            all_results.extend(data['response']['results'])
            # if there is more than one page
            current_page += 1
            total_pages = data['response']['pages']

        with open(fname, 'w') as f:
            print(&quot;Writing to&quot;, fname)

            # re-serialize it for pretty indentation
            f.write(json.dumps(all_results, indent=2))

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-18-f04b4f0fe9ed&gt; in &lt;module&gt;
     49             resp = requests.get(API_ENDPOINT, my_params)
     50             data = resp.json()
---&gt; 51             all_results.extend(data['response']['results'])
     52             # if there is more than one page
     53             current_page += 1

KeyError: 'results'
</code></pre>
<blockquote>
<p>Same error occurs for 'pages'</p>
<p>At first there was no issues and was able to run it. Download crashed after 2020-03-24. Since then can't get the code running again.</p>
</blockquote>
<p>I'm referring to Line 51 and 54. At least at this point the codes crashes.
Not sure how to get rid of the issue. Any ideas?</p>
","python-3.x, api, sentiment-analysis","<p>Understanding the error message would be the first step - it compains about a missing key. Check if <code>data['response']['results']</code> is present (hint: it is not) and check what exactly the structure of your <code>data['response']</code> is.</p>
<p>Fortunately one can use the api parameter <code>'test'</code> so we can help using that key:</p>
<pre><code>my_params = {
    'q': 'coronavirus,stock,covid',
    'sectionID': 'business',
    'from-date': &quot;2019-01-01&quot;,
    'to-date': &quot;2020-09-30&quot;,
    'order-by': &quot;newest&quot;,
    'show-fields': 'all',
    'page-size': 300,
    'api-key': 'test'    # test key for that API
}
</code></pre>
<p>On running, I get the same exception, inspect <code>data['response']</code> and get:</p>
<p><a href=""https://i.sstatic.net/ShXnE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ShXnE.png"" alt=""number too big"" /></a></p>
<p>Lets see what parameters are given, shall we?</p>
<pre><code>my_params = {
    'q': 'coronavirus,stock,covid',
    'sectionID': 'business',
    'from-date': &quot;2019-01-01&quot;,
    'to-date': &quot;2020-09-30&quot;,
    'order-by': &quot;newest&quot;,
    'show-fields': 'all',
    'page-size': 300,      # TOO BIG
    'api-key': 'test'
}
</code></pre>
<p>Fix that to 200 and you'll get</p>
<pre><code>Downloading 2019-01-01
...page 1
Writing to tempdata\articles\2019-01-01.json
Downloading 2019-01-02
...page 1
Writing to tempdata\articles\2019-01-02.json
Downloading 2019-01-03
...page 1
Writing to tempdata\articles\2019-01-03.json
Downloading 2019-01-04
...page 1
Writing to tempdata\articles\2019-01-04.json
Downloading 2019-01-05
[snipp]
</code></pre>
",1,1,119,2020-10-10 08:19:22,https://stackoverflow.com/questions/64291397/theguardian-api-script-crashes
ValueError: Shapes are incompatible in LSTM model,"<p>I am creating an LSTM model based on the following parameters</p>
<pre><code>embed_dim = 128
lstm_out = 200
batch_size = 32

model = Sequential()
model.add(Embedding(2500, embed_dim,input_length = X.shape[1]))
model.add(Dropout(0.2))
model.add(LSTM(lstm_out))
model.add(Dense(2,activation='sigmoid'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

Xtrain, Xtest, ytrain, ytest = train_test_split(X, train['target'], test_size = 0.2, shuffle=True)
print(Xtrain.shape, ytrain.shape)
print(Xtest.shape, ytest.shape)

model.fit(Xtrain, ytrain, batch_size =batch_size, epochs = 1,  verbose = 5)
</code></pre>
<p>but I am receiving the following error</p>
<pre><code>ValueError: Shapes (32, 1) and (32, 2) are incompatible
</code></pre>
<p>Can you help me with this error?</p>
","python, tensorflow, keras, lstm, sentiment-analysis","<p>Your <code>y_train</code> is coming from a single column of a Pandas dataframe, which is a single column. This is suitable if your classification problem is a binary classification 0/1 problem. Then you only need a single neuron in the output layer.</p>
<pre><code>model = Sequential()
model.add(Embedding(2500, embed_dim,input_length = X.shape[1]))
model.add(Dropout(0.2))
model.add(LSTM(lstm_out))
# Only one neuron in the output layer
model.add(Dense(1,activation='sigmoid'))
</code></pre>
",0,1,179,2020-10-16 00:42:01,https://stackoverflow.com/questions/64381457/valueerror-shapes-are-incompatible-in-lstm-model
6 GB RAM Fails in Vectorizing text using Word2Vec,"<p>I'm trying to do one basic tweet sentiment analysis using word2vec and tfidf-score on a dataset consisting of 1,6M tweets but my 6 GB Gforce-Nvidia fails to do so. since this is my first practice project relating machine learning I'm wondering what I'm doing wrong because dataset is all text it shouldn't take this much RAM which makes my laptop froze in tweet2vec function or giving Memory Error in scaling part. below is part of my code that everything collapses.
the last thing is that I've tried with up to 1M data and it worked! so I'm curious what causes the problem</p>
<pre><code># --------------- calculating word weight for using later in word2vec model &amp; bringing words together ---------------
def word_weight(data):
    vectorizer = TfidfVectorizer(sublinear_tf=True, use_idf=True)
    d = dict()
    for index in tqdm(data, total=len(data), desc='Assigning weight to words'):
        # --------- try except caches the empty indexes ----------
        try:
            matrix = vectorizer.fit_transform([w for w in index])
            tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
            d.update(tfidf)
        except ValueError:
            continue
    print(&quot;every word has weight now\n&quot;
          &quot;--------------------------------------&quot;)
    return d


# ------------------- bringing tokens with weight to recreate tweets ----------------
def tweet2vec(tokens, size, tfidf):
    count = 0
    for index in tqdm(tokens, total=len(tokens), desc='creating sentence vectors'):
        # ---------- size is the dimension of word2vec model (200) ---------------
        vec = np.zeros(size)
        for word in index:
            try:
                vec += model[word] * tfidf[word]
            except KeyError:
                continue
        tokens[count] = vec.tolist()
        count += 1
    print(&quot;tweet vectors are ready for scaling for ML algorithm\n&quot;
          &quot;-------------------------------------------------&quot;)
    return tokens


dataset = read_dataset('training.csv', ['target', 't_id', 'created_at', 'query', 'user', 'text'])
dataset = delete_unwanted_col(dataset, ['t_id', 'created_at', 'query', 'user'])
dataset_token = [pre_process(t) for t in tqdm(map(lambda t: t, dataset['text']),
                                              desc='cleaning text', total=len(dataset['text']))]

print('pre_process completed, list of tweet tokens is returned\n'
      '--------------------------------------------------------')
X = np.array(tweet2vec(dataset_token, 200, word_weight(dataset_token)))
print('scaling vectors ...')
X_scaled = scale(X)
print('features scaled!')
</code></pre>
<p>the data given to word_weight function is a (1599999, 200) shaped list which each index is consisted of pre-processed tweet tokens.
I appreciate your time and answer in advance and of course I'm glad to hear better approaches for handling big datasets</p>
","python, machine-learning, bigdata, sentiment-analysis","<p>my problem was solved when i changed the code (tweet2vec function) to this
(w is word weight)</p>
<pre><code>def tweet2vec(tokens, size, tfidf):
    # ------------- size is the dimension of word2vec model (200) ---------------
    vec = np.zeros(size).reshape(1, size)
    count = 0
    for word in tokens:
        try:
            vec += model[word] * tfidf[word]
            count += 1
        except KeyError:
            continue
    if count != 0:
        vec /= count
    return vec

X = np.concatenate([tweet2vec(token, 200, w) for token in tqdm(map(lambda token: token, dataset_token),
                                                               desc='creating tweet vectors',
                                                               total=len(dataset_token))]
</code></pre>
<p>)</p>
<p>I have no idea why!!!!</p>
",0,0,137,2020-10-22 21:23:25,https://stackoverflow.com/questions/64490738/6-gb-ram-fails-in-vectorizing-text-using-word2vec
Python Sentiment Analysis given a dataset with Facebook Posts,"<p>I have a dataset containing raw facebook posts and comments. What I would like to do is to perform sentiment analysis with Python 3 (NTLK ?) in order to label each post and each comment against some categories (a sort of clustering in unsupervised mode). The problem is that I have no idea how to do something similar.</p>
<p>I accept suggestions
Thank you</p>
","python, label, nltk, data-mining, sentiment-analysis","<p>You can use <a href=""https://pypi.org/project/vaderSentiment/"" rel=""nofollow noreferrer"">vaderSentiment</a> which is a python package to perform unsupervised English Sentiment Analysis using dictionary and rules. There is some example on their <a href=""https://github.com/cjhutto/vaderSentiment#code-examples"" rel=""nofollow noreferrer"">github</a>. This option might be more effective than an unsupervised clustering.</p>
",0,1,442,2020-11-05 11:57:52,https://stackoverflow.com/questions/64696740/python-sentiment-analysis-given-a-dataset-with-facebook-posts
How can I make the sentiment analysis plot look nicer?,"<p>I am currently finishing up an assignment in my R programming class. I needed to plot the sentiment value vs the location in the book, the book in question was Jane Austen's &quot;Pride and Prejudice.&quot; I got everything plotted correctly, I just need to make it look nicer as it looks &quot;clustered&quot; and rather clunky.</p>
<p>This is what I have so far:
<a href=""https://i.sstatic.net/ntyoI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ntyoI.png"" alt=""enter image description here"" /></a></p>
<p>I need to use a smoother of some sort, as per the assignment, and I would like to get this plot to look a little better instead of this clustered mess. Any help is appreciated!</p>
","r, ggplot2, plot, r-markdown, sentiment-analysis","<pre><code> library(ggplot2)
 words_2 %&gt;% 
 ggplot(aes(x = word_num, y = value)) +
 geom_point(alpha = 0.2, show.legend = F) +
 geom_smooth() + 
 labs(x = &quot;Location in the book (in ascending order)&quot;, 
   y = &quot;Sentiment score&quot;)
</code></pre>
<p>After using this code, this is the plot that is generated:
<a href=""https://i.sstatic.net/SULF9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SULF9.png"" alt=""sentiment score plot"" /></a></p>
<p>Initially, I used the incorrect variables to plot. I fixed them and used some arguments to make it a little sharper. The next problem in the book was specifically asking to make the plot look more readable using the <code>group_by</code> function.</p>
",0,-1,127,2020-11-19 00:46:06,https://stackoverflow.com/questions/64903685/how-can-i-make-the-sentiment-analysis-plot-look-nicer
R how get afinn score in a new column,"<p>Here is my problem.
I want to put the &quot;AFINN&quot; score related to a column of single words into another column
Here my dataset</p>
<pre><code>head(service_df)
# A tibble: 6 x 3
  word1   word2        n
  &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt;
1 service delivery  8574
2 service covid     1163
3 service lockdown   541
4 service worker     389
5 service provider   370
6 service online     236
</code></pre>
<p>I want to obtain the score related to the column &quot;word2&quot; in order to have something like this</p>
<pre><code>   word1   word2        n      AFINN_word_2
      &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt;
    1 service delivery  8574    -1
    2 service covid     1163     0
    3 service lockdown   541     0  
    4 service worker     389     0.3
    5 service provider   370     0.1
    6 service online     236     1
</code></pre>
<p>(I just put random values under AFINN_word_2)</p>
<p>Thanks Everyone!</p>
","r, tidyverse, sentiment-analysis","<p>Unfortunately for you none of the records you provided in sample match with words ascribed an afinn sentiment score. You could try tokenising or recoding the words you have in words2.</p>
<pre><code># Install pacakges if they are not already installed: necessary_packages =&gt; vector
necessary_packages &lt;- c(&quot;tidytext&quot;, &quot;tidyverse&quot;)

# Create a vector containing the names of any packages needing installation:
# new_pacakges =&gt; vector
new_packages &lt;- necessary_packages[!(necessary_packages %in%
                                       installed.packages()[, &quot;Package&quot;])]

# If the vector has more than 0 values, install the new pacakges
# (and their) associated dependencies:
if(length(new_packages) &gt; 0){install.packages(new_packages, dependencies = TRUE)}

# Initialise the packages in the session: list of boolean =&gt; stdout (console)
lapply(necessary_packages, require, character.only = TRUE)

# data: service_df =&gt; data.frame
service_df &lt;- structure(list(word1 = c(&quot;service&quot;, &quot;service&quot;, &quot;service&quot;, &quot;service&quot;, 
&quot;service&quot;, &quot;service&quot;), word2 = c(&quot;delivery&quot;, &quot;covid&quot;, &quot;lockdown&quot;, 
&quot;worker&quot;, &quot;provider&quot;, &quot;online&quot;), n = c(8574L, 1163L, 541L, 389L, 
370L, 236L)), class = &quot;data.frame&quot;, row.names = c(NA, -6L))

# Use afinn to get sentiment of word: res =&gt; data.frame
res &lt;- service_df %&gt;%
  left_join(get_sentiments(&quot;afinn&quot;), by = c(&quot;word2&quot; = &quot;word&quot;)) %&gt;%
  select(word1, word2, n, AFINN_word_2 = value)
</code></pre>
",0,0,279,2020-11-19 06:38:50,https://stackoverflow.com/questions/64906569/r-how-get-afinn-score-in-a-new-column
"Applying a custom function to a data.table doesn&#39;t work, even though the function seems okay individually","<p>tl,dr: My function seems to work, but then I lapply it and it doesn't. Is it the function or the lapplying?</p>
<h3>The data</h3>
<p>I have a datatable that contains text which is already tokenised into a character vector:</p>
<pre><code>   id                   text
1:  1    c(&quot;sadness&quot;, &quot;joy&quot;)
2:  2   c(&quot;anger&quot;, &quot;scream&quot;)
3:  3 c(&quot;relief&quot;, &quot;sadness&quot;)
</code></pre>
<p>I want to annotate my tokenised texts with emotional values with a dictionary that has words and associated emotional values:</p>
<pre><code>     words emotion1 emotion2
1: sadness        1        5
2:   anger        2        6
3:  relief        3        7
</code></pre>
<h3>The ultimate goal</h3>
<p>I am expecting my search_function to output something similar to this:</p>
<pre><code>my_emotion_function(c(&quot;relief&quot;, &quot;sadness&quot;), lexicon_emotions)
   emotion1 emotion2
1:        2        6
my_emotion_function(c(&quot;relief&quot;, &quot;meh&quot;), lexicon_emotions)
   emotion1 emotion2
1:        3        7
my_emotion_function(c(&quot;meh&quot;, &quot;ugh&quot;), lexicon_emotions)
   emotion1 emotion2
1:       NA       NA
</code></pre>
<p>Applying this to the tokens, I would add new columns and fill them with the results.</p>
<pre><code>  id               text     emotion1 emotion2
1:  1  c(&quot;sadness&quot;, &quot;joy&quot;)         1        5
2:  2  c(&quot;anger&quot;, &quot;scream&quot;)        2        6
3:  3  c(&quot;relief&quot;, &quot;sadness&quot;)      2        6
</code></pre>
<h3>The function that half-works</h3>
<p>The function takes a character vector, subsets the (keyed) emotional dictionary for matching words and calculates the average score for each emotional dimension.</p>
<pre><code>my_emotion_function &lt;- function(characters, lexicon){
  return(lexicon[.(characters), lapply(.SD, mean, na.rm = TRUE), .SDcols = 2:3])
}
</code></pre>
<h3>What I don't understand</h3>
<p>What I am baffled by and can't understand is why this function seems to work well when tested on one character vector (the example above, testing it only on one vector, works well), but when I want to lapply it to a data.table, it doens't work.<br />
I am not sure whether the function is wrong in one aspect or my laplying of it to the data.table. I can't figure out why the single instance works, but not repeatedly on a data.table</p>
<p>If I execute the above code, with an equal number of tokens in each &quot;text&quot; row, then I will just get N.A for every cell, no matter the words.</p>
<pre><code>   id                   text emotion1 emotion2
1:  1    c(&quot;sadness&quot;, &quot;joy&quot;)      NaN      NaN
2:  2   c(&quot;anger&quot;, &quot;scream&quot;)      NaN      NaN
3:  3 c(&quot;relief&quot;, &quot;sadness&quot;)      NaN      NaN
</code></pre>
<p>If you test it out with an unequal number of tokens (say the first row), then every row contains the value for the first row.</p>
<pre><code>   id                   text emotion1 emotion2
1:  1                sadness        1        5
2:  2   c(&quot;anger&quot;, &quot;scream&quot;)        1        5
3:  3 c(&quot;relief&quot;, &quot;sadness&quot;)        1        5
</code></pre>
<p>I can't find a reason as to why I either get only the same result or NA's everywhere.</p>
<h3>Complete code for reproduction</h3>
<pre><code>library(data.table)
table_of_tokens &lt;- data.table(&quot;id&quot; = 1:3,
                              &quot;text&quot; = list(c(&quot;sadness&quot;, &quot;joy&quot;),
                                            c(&quot;anger&quot;, &quot;scream&quot;),
                                            c(&quot;relief&quot;, &quot;sadness&quot;)))
table_of_tokens[, &quot;text&quot; := as.character(text)]
#convert to character vector to use key-subsetting in data.table

lexicon_emotions &lt;-
  data.table(
    &quot;words&quot; = c(&quot;sadness&quot;, &quot;anger&quot;, &quot;relief&quot;),
    &quot;emotion1&quot; = 1:3,
    &quot;emotion2&quot; = 5:7
  )
setkey(lexicon_emotions, words)

my_emotion_function &lt;- function(characters, lexicon) {
  return(lexicon[.(characters), 
                 lapply(.SD, mean, na.rm = TRUE), .SDcols = 2:3])
}
table_of_tokens[, c(&quot;emotion1&quot;, &quot;emotion2&quot;) := 
                  my_emotion_function(text, lexicon_emotions)]
</code></pre>
<p><em>Credit: this is a basically a re-write of the <a href=""https://github.com/mjockers/syuzhet"" rel=""nofollow noreferrer"">syuzhet</a> R-package, which relies on data.frames and is therefore not flexible or efficient enough in my situation for a large dataset.</em></p>
","r, data.table, sentiment-analysis","<p>One of the most important aspects of code writing is debugging. Let's use a simple <code>print()</code> call to figure out what is happening during the function call:</p>
<pre class=""lang-r prettyprint-override""><code>my_emotion_function &lt;- function(characters, lexicon) {
  print(characters) ## for debugging
  return(lexicon[.(characters), 
                 lapply(.SD, mean, na.rm = TRUE), .SDcols = 2:3])
}

table_of_tokens[, c(&quot;emotion1&quot;, &quot;emotion2&quot;) := 
                  my_emotion_function(text, lexicon_emotions)]

## [1] &quot;c(\&quot;sadness\&quot;, \&quot;joy\&quot;)&quot;    &quot;c(\&quot;anger\&quot;, \&quot;scream\&quot;)&quot;   &quot;c(\&quot;relief\&quot;, \&quot;sadness\&quot;)&quot;
</code></pre>
<p>What that means is that we are are actually performing:</p>
<pre><code>lexicon[&quot;c(\&quot;sadness\&quot;, \&quot;joy\&quot;)&quot; ...]

## what we actually want for each token

lexicon[c(&quot;sadness&quot;, &quot;joy&quot;), lapply(.SD, mean, na.rm = TRUE), .SDcols = 2:3])
</code></pre>
<p>To do so, we <em>do <strong>not</strong> want to convert from a list to a character</em> as suggested by @IanCampbell. The other item is that we want to loop through each element, which means <code>lapply()</code> can be our friend:</p>
<pre class=""lang-r prettyprint-override""><code>table_of_tokens[, c(&quot;emotion1&quot;, &quot;emotion2&quot;) := 
                  rbindlist(lapply(text, my_emotion_function, lexicon_emotions))]
table_of_tokens

##    id           text emotion1 emotion2
## 1:  1    sadness,joy        1        5
## 2:  2   anger,scream        2        6
## 3:  3 relief,sadness        2        6
</code></pre>
<p>I am still uncertain what would happen if there were no matches.</p>
",0,1,471,2020-12-05 16:22:52,https://stackoverflow.com/questions/65159177/applying-a-custom-function-to-a-data-table-doesnt-work-even-though-the-functio
R: How can I repeat a line of code for all my rows?,"<p>I have a line of code,
tweet1 &lt;- userTimeline(&quot;@VALUE&quot;, n=100)
I want this to continue based on the rows in my column.
So say I have a dataframe with one column, &quot;Companies&quot; and rows, Optus, Telstra, Samsung.,
I want to have a function that will automatically do:</p>
<p>tweet1 &lt;- userTimeline(&quot;@Optus&quot;, n=100)</p>
<p>tweet2 &lt;- userTimeline(&quot;@Telstra&quot;, n=100)</p>
<p>tweet3 &lt;- userTimeline(&quot;@Samsung&quot;, n=100)</p>
<p>Thanks :)</p>
","r, function, dataframe, twitter, sentiment-analysis","<p>You can use <code>for</code> loop or <code>lapply</code>. If your dataframe is called <code>df</code> you can do :</p>
<pre><code>tweet_list &lt;- lapply(paste0('@', df$Companies), userTimeline, n = 100)
</code></pre>
",0,0,58,2020-12-09 02:55:57,https://stackoverflow.com/questions/65209977/r-how-can-i-repeat-a-line-of-code-for-all-my-rows
Unable to import process_tweets from utils,"<p>Thanks for looking into this, I have a python program for which I need to have <code>process_tweet</code> and <code>build_freqs</code> for some NLP task, <code>nltk</code> is installed already and <code>utils</code> <strong>wasn't</strong> so I installed it via <code>pip install utils</code> but the above mentioned two modules apparently weren't installed, the error I got is standard one here,</p>
<pre><code>ImportError: cannot import name 'process_tweet' from
'utils' (C:\Python\lib\site-packages\utils\__init__.py)
</code></pre>
<p>what have I done wrong or is there anything missing?
Also I referred <a href=""https://stackoverflow.com/questions/37096364/python-importerror-cannot-import-name-utils"">This stackoverflow answer</a> but it didn't help.</p>
","python, nlp, nltk, sentiment-analysis","<p>You can easily access any source code with ??, for example in this case: process_tweet?? (the code above from deeplearning.ai NLP course custome utils library):</p>
<pre><code>def process_tweet(tweet):
&quot;&quot;&quot;Process tweet function.
Input:
    tweet: a string containing a tweet
Output:
    tweets_clean: a list of words containing the processed tweet

&quot;&quot;&quot;
stemmer = PorterStemmer()
stopwords_english = stopwords.words('english')
# remove stock market tickers like $GE
tweet = re.sub(r'\$\w*', '', tweet)
# remove old style retweet text &quot;RT&quot;
tweet = re.sub(r'^RT[\s]+', '', tweet)
# remove hyperlinks
tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)
# remove hashtags
# only removing the hash # sign from the word
tweet = re.sub(r'#', '', tweet)
# tokenize tweets
tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                           reduce_len=True)
tweet_tokens = tokenizer.tokenize(tweet)

tweets_clean = []
for word in tweet_tokens:
    if (word not in stopwords_english and  # remove stopwords
            word not in string.punctuation):  # remove punctuation
        # tweets_clean.append(word)
        stem_word = stemmer.stem(word)  # stemming word
        tweets_clean.append(stem_word)
</code></pre>
",3,1,6382,2020-12-19 13:06:56,https://stackoverflow.com/questions/65370140/unable-to-import-process-tweets-from-utils
Can an except block of python have 2 conditions simultaneously?,"<p>I was trying to learn stock prediction with the help of this <a href=""https://github.com/Samar-080301/Stock-Predictor"" rel=""nofollow noreferrer"">github project</a>. but when I run the <code>main.py</code> file given in the repository, via the <code>cmd</code>. I encountered an error</p>
<pre><code>File &quot;/Stock-Predictor/src/tweetstream/streamclasses.py&quot;, line 101
    except urllib2.HTTPError, exception:
                            ^
SyntaxError: invalid syntax
</code></pre>
<p>The below given code is part of a PyPi module named <code>tweetstream</code>i.e. named as <code>tweetstream/streamclasses.py</code>. Which while implementing in a Twitter sentiment analysis project gave the error</p>
<pre><code>import time
import urllib
import urllib2
import socket
from platform import python_version_tuple
import anyjson

from . import AuthenticationError, ConnectionError, USER_AGENT 

class BaseStream(object):
    &quot;&quot;&quot;A network connection to Twitters streaming API
    :param username: Twitter username for the account accessing the API.
    :param password: Twitter password for the account accessing the API.
    :keyword count: Number of tweets from the past to get before switching to
      live stream.
    :keyword url: Endpoint URL for the object. Note: you should not
      need to edit this. It's present to make testing easier.
    .. attribute:: connected
        True if the object is currently connected to the stream.
    .. attribute:: url
        The URL to which the object is connected
    .. attribute:: starttime
        The timestamp, in seconds since the epoch, the object connected to the
        streaming api.
    .. attribute:: count
        The number of tweets that have been returned by the object.
    .. attribute:: rate
        The rate at which tweets have been returned from the object as a
        float. see also :attr: `rate_period`.
    .. attribute:: rate_period
        The amount of time to sample tweets to calculate tweet rate. By
        default 10 seconds. Changes to this attribute will not be reflected
        until the next time the rate is calculated. The rate of tweets vary
        with time of day etc. so it's useful to set this to something
        sensible.
    .. attribute:: user_agent
        User agent string that will be included in the request. NOTE: This can
        not be changed after the connection has been made. This property must
        thus be set before accessing the iterator. The default is set in
        :attr: `USER_AGENT`.
    &quot;&quot;&quot;

    def __init__(self, username, password, catchup=None, url=None):
        self._conn = None
        self._rate_ts = None
        self._rate_cnt = 0
        self._username = username
        self._password = password
        self._catchup_count = catchup
        self._iter = self.__iter__()

        self.rate_period = 10  # in seconds
        self.connected = False
        self.starttime = None
        self.count = 0
        self.rate = 0
        self.user_agent = USER_AGENT
        if url: self.url = url

    def __enter__(self):
        return self

    def __exit__(self, *params):
        self.close()
        return False

    def _init_conn(self):
        &quot;&quot;&quot;Open the connection to the twitter server&quot;&quot;&quot;
        headers = {'User-Agent': self.user_agent}

        postdata = self._get_post_data() or {}
        if self._catchup_count:
            postdata[&quot;count&quot;] = self._catchup_count

        poststring = urllib.urlencode(postdata) if postdata else None
        req = urllib2.Request(self.url, poststring, headers)

        password_mgr = urllib2.HTTPPasswordMgrWithDefaultRealm()
        password_mgr.add_password(None, self.url, self._username, self._password)
        handler = urllib2.HTTPBasicAuthHandler(password_mgr)
        opener = urllib2.build_opener(handler)

        try:
            self._conn = opener.open(req)

        except urllib2.HTTPError, exception:  #___________________________problem here
            if exception.code == 401:
                raise AuthenticationError(&quot;Access denied&quot;)
            elif exception.code == 404:
                raise ConnectionError(&quot;URL not found: %s&quot; % self.url)
            else:  # re raise. No idea what would cause this, so want to know
                raise
        except urllib2.URLError, exception:
            raise ConnectionError(exception.reason)
</code></pre>
","python-3.x, twitter, artificial-intelligence, sentiment-analysis, stock","<p>The second item in the <code>except</code> is an identifier used in the body of the exception to access the exception information.  The <code>try/except</code> syntax changed between Python 2 and Python 3 and your code is the Python 2 syntax.</p>
<p>Python 2 (<a href=""https://docs.python.org/2/reference/compound_stmts.html#the-try-statement"" rel=""nofollow noreferrer"" title=""Python 2 try/except"">language reference</a>):</p>
<pre class=""lang-py prettyprint-override""><code>try:
    ...
except &lt;expression&gt;, &lt;identifier&gt;:
    ...
</code></pre>
<p>Python 3 (<a href=""https://docs.python.org/3/reference/compound_stmts.html#the-try-statement"" rel=""nofollow noreferrer"" title=""Python 3 try/except"">language reference</a>, <a href=""https://www.python.org/dev/peps/pep-3110/"" rel=""nofollow noreferrer"" title=""PEP 3110"">rationale</a>):</p>
<pre><code>try:
    ...
except &lt;expression&gt; as &lt;identifier&gt;:
    ...
</code></pre>
<p>Note that  can be a single exception class or a tuple of exception classes to catch more than one type in a single <code>except</code> clause, so to answer your titled question you could use the following to handle more than one possible exception being thrown:</p>
<pre><code>try:
    x = array[5]  # NameError if array doesn't exist, IndexError if it is too short
except (IndexError,NameError) as e:
    print(e)  # which was it?
</code></pre>
",1,0,44,2020-12-31 07:49:34,https://stackoverflow.com/questions/65518086/can-an-except-block-of-python-have-2-conditions-simultaneously
Extract tweets with specific words in python list. sentimnt analysis,"<p>I have tweets stored in python list<br>
like</p>
<ol>
<li>COVID vaccine allergic reactions polyethylene glycol hypothesized possible culprit I kind skeptical science fit well VaccinesWork VaccinesSaveLives</li>
<li>COVIDー COVID MaskeAuf vaccine Impfstart Impfstoff</li>
<li>M people died hunger There vaccine called food It works straight away w one dose harmful side effects Let cure hunger worry virus w survival rate w vaccine vaccine covid hunger etc..</li>
</ol>
<p>and I have most comman words like Covid, Vaccine etc..</p>
<p>I need to get all tweets with specific words. assume I stored words in new list like</p>
<pre><code>word_= &quot;Covid&quot;
</code></pre>
<p>and I have new empty list named new_list to append the tweets</p>
<pre><code>for tweets in list_tweets:
    if word in list_tweets:
     new_list.append(list_tweets)
</code></pre>
<p>but I got nothing. any help , suggestions. I be so appreciated</p>
","python, list, nlp, sentiment-analysis","<p><code>if word in list_tweets</code> checks if one element of the <code>list_tweets</code> is <code>&quot;Covid&quot;</code> (or any value contained in <code>word</code>)</p>
<p>I think you want to check <code>if word in tweets</code> instead.</p>
<p>ps : naming your variable <code>tweet</code> instead of <code>tweets</code>would be a good idea, since it takes values from a list of tweets (thus is a single tweet)</p>
",0,-1,180,2020-12-31 13:18:03,https://stackoverflow.com/questions/65521409/extract-tweets-with-specific-words-in-python-list-sentimnt-analysis
Calculate the Polarity of Sentiwordnet,"<p>I'm trying to calculate to compare the score (on a scale from 1 to 5) from a review with the score extracted from the sentiment analysis of the text review. I'm using sentiwordnet, I managed to get the positive, negative, and objective scores but I can't get to iterate over the scores of every word in every sentence of the 42 reviews to calculate a score.</p>
<p>This is my code:</p>
<pre class=""lang-py prettyprint-override""><code>def preprocess_token(text): 

    lower_text=text.lower() 
    tokens = nltk.tokenize.word_tokenize(lower_text)
    return tokens

def penn_to_wn(tag):

    if tag.startswith('J'):
        return wn.ADJ
    elif tag.startswith('N'):
        return wn.NOUN
    elif tag.startswith('R'):
        return wn.ADV
    elif tag.startswith('V'):
        return wn.VERB
    return None

  lemmatizer = WordNetLemmatizer()

def get_sentiment(word,tag):
    
    wn_tag = penn_to_wn(tag)
    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):
        return []

    lemma = lemmatizer.lemmatize(word, pos=wn_tag)
    if not lemma:
        return []

    synsets = wn.synsets(word, pos=wn_tag)
    if not synsets:
        return []

    synset = synsets[0]
    swn_synset = swn.senti_synset(synset.name())

    return [swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]


data[&quot;text_n&quot;] = data['Text'].str.replace('[^\w\s]','')

data['tokens']=data['text_n'].apply(preprocess_token)


word_list=[]

for i in range(len(data['tokens'])):

    word_list.append([])

for i in range(len(data['tokens'])): 

    for word in data['tokens'][i]:

        if word[:].isalpha():

            word_list[i].append(word[:])



tagged_tokens=[]

for token in word_list:

    tagged_tokens.append(nltk.pos_tag(token))
    


senti_val=[]

for i in range(len(tagged_tokens)):

    t=tuple(get_sentiment(x,y) for x,y in tagged_tokens[i])

    values= [x for x in t if x]

    senti_val.append(values)

data['value']=senti_val

calc=[]
for i in range(len(senti_val)):
    r=senti_val[i][i][0] - senti_val[i][i][1]
    calc.append(r)
#This last chunk of code gives me error:
Traceback (most recent call last):

  File &quot;C:\Users\----\OneDrive\python\reviews.py&quot;, line 158, in &lt;module&gt;
    r=senti_val[i][i][0] - senti_val[i][i][1]

IndexError: list index out of range

#I want to get the result of subtracting the negative score from the positive score for each word of each sentence and get it in a list.
</code></pre>
<p>any recommendation on what's wrong or what should I do next will be deeply appreciated.</p>
<p>Thanks in advance</p>
","python, sentiment-analysis, wordnet, senti-wordnet","<pre class=""lang-py prettyprint-override""><code>calc=[]
#iterating over each sentence 
for i in range(len(senti_val)):
    # subtract positive and negative score of each token in the sentence 
    scores = [token_score[0] - token_score[1] for token_score in senti_val[i]]
    # if you want to sum up all the score within the sentence
    # scores = sum(scores) 
    print(scores)
    calc.append(scores)
</code></pre>
",1,0,871,2021-01-02 18:13:07,https://stackoverflow.com/questions/65542500/calculate-the-polarity-of-sentiwordnet
NotFittedError: CountVectorizer - Vocabulary wasn&#39;t fitted. while performing sentiment analysis,"<p>while performing sentiment analysis using data -</p>
<pre><code>http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
</code></pre>
<p>The dataset contains 25K training and testing data (12.5 Positive and 12.5 Negative reviews)
I'm constantly getting -</p>
<pre><code>NotFittedError: CountVectorizer - Vocabulary wasn't fitted.
</code></pre>
<p>Code -</p>
<p>(Required libraries and Variable names are initialized separately)</p>
<p>To create training and testing data -</p>
<pre><code>import glob
import os
import numpy as np
def load_texts_labels_from_folders(path, folders):
    texts,labels = [],[]
    for idx,label in enumerate(folders):
        for fname in glob.glob(os.path.join(path, label, '*.*')):
            texts.append(open(fname, 'r',encoding=&quot;utf8&quot;).read())
            labels.append(idx)
    # stored as np.int8 to save space 
    return texts, np.array(labels).astype(np.int8)

trn,trn_y = load_texts_labels_from_folders(f'{PATH}train',names)
val,val_y = load_texts_labels_from_folders(f'{PATH}test',names)

len(trn),len(trn_y),len(val),len(val_y)

len(trn_y[trn_y==1]),len(val_y[val_y==1])

np.unique(trn_y)
</code></pre>
<p>Count Vectorization -</p>
<pre><code>re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')
def tokenize(s): return re_tok.sub(r' \1 ', s).split()

#create term documetn matrix
veczr = CountVectorizer(tokenizer=tokenize)


trn_term_doc = veczr.fit_transform(trn)
val_term_doc = veczr.transform(val)

veczr = CountVectorizer(tokenizer=tokenize,ngram_range=(1,3), min_df=1,max_features=80000)
trn_term_doc
trn_term_doc[5] #83 stored elements
w0 = set([o.lower() for o in trn[5].split(' ')]); w0
len(w0)
vocab = loaded_vectorizer.get_feature_names()
print(len(vocab))
vocab[5000:5005]
</code></pre>
<p>Here i get Error -</p>
<pre><code>NotFittedError: CountVectorizer - Vocabulary wasn't fitted.
</code></pre>
","python, scikit-learn, nlp, sentiment-analysis, countvectorizer","<pre><code>vocab = loaded_vectorizer.get_feature_names()
</code></pre>
<p><code>loaded_vectorizer</code> is not defined anywhere in this code, so it's not surprising that it's not initialized.</p>
<p>Also why do you initialize <code>veczr</code> twice? Apparently you don't use it the second time.</p>
",1,0,225,2021-01-08 20:31:46,https://stackoverflow.com/questions/65636002/notfittederror-countvectorizer-vocabulary-wasnt-fitted-while-performing-sen
Adding colour by group to ggplot,"<p>I have analysed text, and am trying to visualise the frequency of words in three groups. I want to set the three groups a colour each, so all my graphs with the groups are easily comparable. Below is the structure of my data and the code I'm using to make the graph. I'm not sure how to assign each group its own colour and reproduce this in my script. At the moment it just gives produces varying shades of blue depending on the group.</p>
<p>Thanks</p>
<pre><code>structure(list(group = c(1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 
2, 2, 2, 2, 3, 3, 3), word = c(&quot;happy&quot;, &quot;dance&quot;, &quot;pain&quot;, &quot;pen&quot;, 
&quot;feel&quot;, &quot;head&quot;, &quot;football&quot;, &quot;year&quot;, &quot;asthma&quot;, &quot;contagious&quot;, &quot;flowers&quot;, 
&quot;lamp&quot;, &quot;calendar&quot;, &quot;phone&quot;, &quot;cereal&quot;, &quot;book&quot;, &quot;acne&quot;, &quot;low&quot;, 
&quot;pain&quot;), n = c(134, 138, 157, 195, 209, 213, 266, 414, 114, 114, 
126, 149, 182, 193, 205, 223, 103, 110, 118), row = c(1, 2, 3, 
4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19)), row.names = c(NA, 
-19L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
<p>and the code for the graph</p>
<pre><code># Colours for the three groups
my_colors &lt;- c(&quot;#FFDBCE&quot;, &quot;#8CAEAE&quot;, &quot;#beb6d7&quot;)


#Organise words by group 
wordsbygroup &lt;- script %&gt;% 
  group_by(group) %&gt;%
  count(word, group, sort = TRUE) %&gt;%
  slice(seq_len(8)) %&gt;%
  ungroup() %&gt;%
  arrange(group,n) %&gt;%
  mutate(row = row_number()) 

#Visualise words by group 

wordsbygroup %&gt;%
  ggplot(aes(row, n, fill = group)) +
  geom_col(show.legend = F) +
  labs(x = NULL, y = &quot;Word Count&quot;) +
  ggtitle(&quot;Frequent Words by group&quot;) + 
  facet_wrap(~group, scales = &quot;free_y&quot;,&quot;fixed_x&quot;) +
  scale_x_continuous(  # This handles replacement of row 
    breaks = wordsbygroup$row, # notice need to reuse data frame
    labels = wordsbygroup$word) +
  coord_flip()
</code></pre>
<p><a href=""https://i.sstatic.net/35vCm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/35vCm.png"" alt=""graph output"" /></a></p>
","r, ggplot2, graph, sentiment-analysis","<p>I turned your fill group into a factor to make the group discrete.</p>
<p>Then added <code>scale_fill_manual(values = my_colors)</code> to assign the fill colors.</p>
<pre><code>wordsbygroup %&gt;%
  ggplot(aes(row, n, fill = as.factor(group))) +
  geom_col(show.legend = F) +
  labs(x = NULL, y = &quot;Word Count&quot;) +
  ggtitle(&quot;Frequent Words by group&quot;) + 
  facet_wrap(~group, scales = &quot;free_y&quot;,&quot;fixed_x&quot;) +
  scale_x_continuous(  # This handles replacement of row 
    breaks = wordsbygroup$row, # notice need to reuse data frame
    labels = wordsbygroup$word) +
  scale_fill_manual(values = my_colors) + 
  coord_flip()
</code></pre>
<p><a href=""https://i.sstatic.net/THrTQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/THrTQ.png"" alt=""enter image description here"" /></a></p>
",1,1,282,2021-01-13 19:02:10,https://stackoverflow.com/questions/65708283/adding-colour-by-group-to-ggplot
Sentiment Analysis how to get the probability of the result?,"<p>So i have this simple sentiment analysis app here</p>
<p>so far i can only print its result in Positive/negative/neutral
but i want it to also print it's probability on why it's a Positive sentence.</p>
<p>like this one</p>
<pre><code> Positive 
 85.2%
</code></pre>
<p>Can anyone help me?</p>
<p>Here's my code</p>
<pre><code>import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import StandardScaler
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score
wordnet_lemmatizer = WordNetLemmatizer()

df = pd.read_csv('Tweets.csv')

def normalizer(comment):
      only_letters = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, comment)
      only_letters = only_letters.lower()
      only_letters = only_letters.split()
      filtered_result = [word for word in only_letters if word not in stopwords.words('english')]
      lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]
      lemmas = ' '.join(lemmas)
      return lemmas

df = shuffle(df)
y = df['airline_sentiment']
x = df.text.apply(normalizer)

vectorizer = CountVectorizer()
x_vectorized = vectorizer.fit_transform(x)

train_x,val_x,train_y,val_y = train_test_split(x_vectorized,y)


regressor = LogisticRegression(multi_class='multinomial', solver='newton-cg')
model = regressor.fit(train_x, train_y)

params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }
gs_clf = GridSearchCV(model, params, n_jobs=1, cv=5)
gs_clf = gs_clf.fit(train_x, train_y)
model = gs_clf.best_estimator_

#_f1 = f1_score(val_y, y_pred, average='micro')
#_confusion = confusion_matrix(val_y, y_pred)
#__precision = precision_score(val_y, y_pred, average='micro')
#_recall = recall_score(val_y, y_pred, average='micro')
#_statistics = {'f1_score': _f1,
#              'confusion_matrix': _confusion,
#              'precision': __precision,
#              'recall': _recall
#              }

y_pred = model.predict(val_x)
print(accuracy_score(val_y, y_pred))

test_feature = vectorizer.transform(['The Movie is good'])
print(model.predict(test_feature,))
</code></pre>
<p>and my current output is:</p>
<pre><code>0.7846994535519126
['positive']
</code></pre>
","python, linear-regression, sentiment-analysis","<pre><code>prediction_probablities = model.predict_proba(val_x)
</code></pre>
<p>for additional information:</p>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html</a></p>
",0,-1,200,2021-01-14 17:54:07,https://stackoverflow.com/questions/65724362/sentiment-analysis-how-to-get-the-probability-of-the-result
Pandas how to index a a list from JSON and put it into a dataframe?,"<p>How can I index a list inside a dataframe?</p>
<p>I have this code here that will get data from JSON and insert it into a dataframe</p>
<p>Here's what the JSON looks like</p>
<pre><code>{&quot;text_sentiment&quot;: &quot;positive&quot;, &quot;text_probability&quot;: [0.33917574607174916, 0.26495590980799744, 0.3958683441202534]}
</code></pre>
<p>Here's my code.</p>
<pre><code>input_c = pd.DataFrame(columns=['Comments','Result'])
for i in range(input_df.shape[0]):
    url = 'http://classify/?text='+str(input_df.iloc[i])
    r = requests.get(url)
    result = r.json()[&quot;text_sentiment&quot;]
    proba = r.json()[&quot;text_probability&quot;]
    input_c = input_c.append({'Comments': input_df.loc[i].to_string(index=False),'Result': result, 'Probability': proba}, ignore_index = True)
st.write(input_c)
</code></pre>
<p>Here's what the results look like
<a href=""https://i.sstatic.net/JbBym.png"" rel=""nofollow noreferrer"">result</a></p>
<pre><code>                                     Comments      Result                              Probability
0                This movie is good in my eyes.   neutral    [0.26361889609129974, 0.4879752378104797, 0.2484058660982205]
1            This is a bad movie it's not good.  negative   [0.5210904912792065, 0.22073131008688818, 0.25817819863390534]
2     One of the best performance in this year.  positive   [0.14644707145500369, 0.3581522311734714, 0.49540069737152503]
3                The best movie i've ever seen.  positive   [0.1772046003747405, 0.026468108571479156, 0.7963272910537804]
4                             The movie is meh.   neutral   [0.24349393167653663, 0.6820982528652574, 0.07440781545820596]
5  One of the best selling artist in the world.  positive    [0.07738688706903311, 0.3329095061233371, 0.5897036068076298]
</code></pre>
<p>The data in the Probability column is the one I want to index.</p>
<p>For example: If the value in Result is &quot;positive&quot; then I want the proba to index to 2,and If the result is &quot;neutral&quot; index to 1</p>
<p>Like this</p>
<pre><code>                                      Comments     Result        Probability
0                This movie is good in my eyes.   neutral    [0.4879752378104797]
1            This is a bad movie it's not good.  negative    [0.5210904912792065]
2     One of the best performance in this year.  positive   [0.49540069737152503]
3                The best movie i've ever seen.  positive    [0.7963272910537804]
4                             The movie is meh.   neutral    [0.6820982528652574]
5  One of the best selling artist in the world.  positive    [0.5897036068076298]
</code></pre>
<p>Are there any ways on how to do it?</p>
","python, json, pandas, numpy, sentiment-analysis","<p>In your code, you already decided the <code>Result</code> content, whether it's negative, neutral, or positive, so you need only to store the maximum value of the probability list in the data frame <code>input_c</code>.</p>
<p>This means, change <code>'Probability': proba</code> to <code>'Probability': max(proba)</code>, so modify:</p>
<pre><code> input_c = input_c.append({'Comments': input_df.loc[i].to_string(index=False),'Result': result, 'Probability': proba}, ignore_index = True)
</code></pre>
<p>to</p>
<pre><code> input_c = input_c.append({'Comments': input_df.loc[i].to_string(index=False),'Result': result, 'Probability': max(proba}, ignore_index = True)
</code></pre>
<p>then to set the index in <code>input_c </code> to <code>Probability</code> column, use</p>
<pre><code>input_c.set_index('Probability')
</code></pre>
",0,0,143,2021-01-16 12:38:15,https://stackoverflow.com/questions/65749742/pandas-how-to-index-a-a-list-from-json-and-put-it-into-a-dataframe
Unable to retrieve tweets from tweepy (No error/columns with no result in output),"<pre><code>apikey = '2238c8h8E25gSVU1WW28ti7fS7'
apisecretkey = 'ssLG9s4rt4QwLo6PFyMSpLVRT1IoQ3f1EwrrgzTg6TRJLUTeI5e'
accesstoken = '33347844103627698178-3HuOoCCFuMWHwLTmhswKUtJSvG22et'
accesstokensecret = '2s8tAcatrjTHgh81Oo7dw6rvWGGRFZoSrPDa5eInY22Q3c'

auth = tw.OAuthHandler(apikey,apisecretkey) #calling OAuthHandler required for authantication with Twitter
auth.set_access_token(accesstoken,accesstokensecret)

api = tw.API(auth,wait_on_rate_limit=True)

search_word = '#IndvsAus' or '#AusvsInd'
date_since = '2021-01-10'
date_until = '2021-01-11'

tweets = tw.Cursor(api.search,q = search_word+' -filter:retweets',\
                   lang ='en',tweet_mode='extended',since='date_since',until='date_until').items(100)

tweet_details = [[tweet.id,tweet.source,tweet.full_text,tweet.user.location,tweet.user.created_at,tweet.user.verified,tweet.created_at]for tweet in tweets]

import pandas as pd
tweet100_df = pd.DataFrame(data = tweet_details,columns=['tweet_id','source','Full_text','User_location','User_created_at','User_verified','tweet_timestamp',])
pd.set_option('max_colwidth',800)
tweet100_df.head(20)
</code></pre>
<p>Output: tweet_id    source  Full_text   User_location   User_created_at User_verified   tweet_timestamp</p>
<p><strong>The output is showing no tweets only columns headings. Where am I going wrong?</strong></p>
","python-3.x, pandas, twitter, tweepy, sentiment-analysis","<p>In order to dump the output of Tweepy's <em>Cursor</em> API into a Pandas DataFrame you need to pass <code>pd.DataFrame</code> a list of dictionaries and the fields you'd be interested in as column names.</p>
<p>Tweepy has methods for structuring the data from the <em>Cursor</em> <code>items()</code> method into a dictionary.</p>
<p>In your case:</p>
<pre><code>tweets = tw.Cursor(api.search,q = search_word+' -filter:retweets',\
                   lang ='en',tweet_mode='extended',since='date_since',until='date_until').items(100)


list_of_dicts = []
for each_json_tweet in tweets:
    list_of_dicts.append(tweets._json)
</code></pre>
<p>And then you can do:</p>
<pre><code>tweet100_df = pd.DataFrame(data=list_of_dicts,columns=['tweet_id','source','Full_text','User_location','User_created_at','User_verified','tweet_timestamp'])
</code></pre>
",0,0,72,2021-01-17 05:48:35,https://stackoverflow.com/questions/65757730/unable-to-retrieve-tweets-from-tweepy-no-error-columns-with-no-result-in-output
Sentiment Analysis By Date,"<p>I'm doing some very basic sentiment analysis on a pretty large set of data that continues to grow every day. I need to feed this data into a shiny app where I can adjust the date range. Rather than running the analysis over and over again, what I'd like to do is create a new CSV with the sum of each sentiment score by date.  I'm having trouble iterating over the date though.   Here's some sample data and the <code>lapply()</code> statement I tried that is not working.</p>
<pre><code>library(tidyverse)
library(syuzhet)
library(data.table)

df &lt;- data.frame(date = c(&quot;2021-01-18&quot;, &quot;2021-01-18&quot;, &quot;2021-01-18&quot;, &quot;2021-01-17&quot;,&quot;2021-01-17&quot;, &quot;2021-01-16&quot;, &quot;2021-01-15&quot;, &quot;2021-01-15&quot;, &quot;2021-01-15&quot;),
                 text = c(&quot;Some text here&quot;, &quot;More text&quot;, &quot;Some other words&quot;, &quot;Just making this up&quot;, &quot;as I go along&quot;, &quot;hope the example helps&quot;, &quot;thank you in advance&quot;, &quot;I appreciate the help&quot;, &quot;the end&quot;))

&gt; df
        date                   text
1 2021-01-18         Some text here
2 2021-01-18              More text
3 2021-01-18       Some other words
4 2021-01-17    Just making this up
5 2021-01-17          as I go along
6 2021-01-16 hope the example helps
7 2021-01-15   thank you in advance
8 2021-01-15  I appreciate the help
9 2021-01-15                the end


dates_scores_df &lt;- lapply(df, function(i){
  data &lt;- df %&gt;% 
    # Filter to the unique date
    filter(date == unique(df$date[i]))
  
  # Sentiment Analysis for each date
  sentiment_data &lt;- get_nrc_sentiment(df$text)
  
  # Convert to df
  score_df &lt;- data.frame(sentiment_data[,])
  
  # Transpose the data frame and adjust column names
  daily_sentiment_data &lt;- transpose(score_df)
  colnames(daily_sentiment_data) &lt;- rownames(score_df)

 # Add a date column
  daily_sentiment_data$date &lt;- df$date[i]

})

sentiment_scores_by_date &lt;- do.call(&quot;rbind.data.frame&quot;, dates_scores_df)
</code></pre>
<p>What I'd like to get to is something like this (data here is made up and will not match the example above)</p>
<pre><code>      date anger anticipation disgust fear joy sadness surprise trust negative positive
2021-01-18     1            2       0    1   2       0        2     1        1        2
2021-01-17     1            2       0    2   3       3        1     2        0        1   
</code></pre>
","r, group-by, sentiment-analysis, summarization","<p>You can try :</p>
<pre><code>library(dplyr)
library(purrr)
library(syuzhet)

df %&gt;%
  split(.$date) %&gt;%
  imap_dfr(~get_nrc_sentiment(.x$text) %&gt;% 
             summarise(across(.fns = sum)) %&gt;% 
             mutate(date = .y, .before = 1)) -&gt; result

result
</code></pre>
",1,1,507,2021-01-18 22:40:24,https://stackoverflow.com/questions/65783030/sentiment-analysis-by-date
Using XLNet for sentiment analysis - setting the correct reshape parameters,"<p>Following  <a href=""https://medium.com/swlh/using-xlnet-for-sentiment-classification-cfa948e65e85"" rel=""nofollow noreferrer"">this link</a>, I am trying to use my own data to do sentiment analysis. But I get this error:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;

&lt;ipython-input-41-5f2f35b7976e&gt; in train_epoch(model, data_loader, optimizer, device, scheduler, n_examples)
      7 
      8     for d in data_loader:
----&gt; 9         input_ids = d[&quot;input_ids&quot;].reshape(4,64).to(device)
     10         attention_mask = d[&quot;attention_mask&quot;].to(device)
     11         targets = d[&quot;targets&quot;].to(device)

RuntimeError: shape '[4, 64]' is invalid for input of size 64
</code></pre>
<p>When I try to run this code</p>
<pre><code>history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):
    print(f'Epoch {epoch + 1}/{EPOCHS}')
    print('-' * 10)

    train_acc, train_loss = train_epoch(
        model,
        train_data_loader,     
        optimizer, 
        device, 
        scheduler, 
        len(df_train)
    )

    print(f'Train loss {train_loss} Train accuracy {train_acc}')

    val_acc, val_loss = eval_model(
        model,
        val_data_loader, 
        device, 
        len(df_val)
    )

    print(f'Val loss {val_loss} Val accuracy {val_acc}')
    print()

    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['val_acc'].append(val_acc)
    history['val_loss'].append(val_loss)
</code></pre>
<p>I know this error has something to do with the shape of my data but I am not sure how to find the correct <code>reshape</code> parameters in order to make this work.</p>
","python, machine-learning, nlp, sentiment-analysis, huggingface-transformers","<p>Shape [4,64] in your example is actually [batch size, max_sequence_length]</p>
<p>So maybe you could replace them with your values...</p>
",1,1,276,2021-01-22 20:22:11,https://stackoverflow.com/questions/65852264/using-xlnet-for-sentiment-analysis-setting-the-correct-reshape-parameters
How to print valence score for each lexicon in vader?,"<p>I am trying to print the valence score for each lexicon (word) in a sentence using vader, but I am getting confused in the process. I am able to sort the words in a sentence as positive, negative and neutral using vader. I want to print the valence score as well. How to approach this?</p>
<pre><code>sid = SentimentIntensityAnalyzer()
pos_word_list=[]
neu_word_list=[]
neg_word_list=[]

for word in tokenized_sentence:
    if (sid.polarity_scores(word)['compound']) &gt;= 0.1:
        pos_word_list.append(word)
        sid.score_valence(word)
    elif (sid.polarity_scores(word)['compound']) &lt;= -0.1:
        neg_word_list.append(word)
    else:
      neu_word_list.append(word)                

print('Positive:',pos_word_list)        
print('Neutral:',neu_word_list)    
print('Negative:',neg_word_list) 
score = sid.polarity_scores(sentence)
print('\nScores:', score)
</code></pre>
<p>This is the code I saw <a href=""https://stackoverflow.com/questions/40325980/how-is-the-vader-compound-polarity-score-calculated-in-python-nltk"">here</a>. I want it to print as</p>
<pre><code>Positive: ['happy', 1.3]
Neutral: ['paper', 0, 'too', 0, 'much', 0]
Negative: ['missed', -1.2, 'stupid', -1.9]

Scores: {'neg': 0.491, 'neu': 0.334, 'pos': 0.175, 'compound': -0.5848}
</code></pre>
<p>thus showing the word 'happy' having 1.3 valence score in the sentence.</p>
","python, nlp, nltk, sentiment-analysis, vader","<p>It would be great if you could provide the sentence you had used for your code. However, I have provided a sentence which you can replace with your sentence.</p>
<p>Have a look at my source code:</p>
<pre><code>import nltk
from nltk.tokenize import word_tokenize, RegexpTokenizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
 
Analyzer = SentimentIntensityAnalyzer()
 
sentence = 'Make sure you stay happy and less doubtful'
 
tokenized_sentence = nltk.word_tokenize(sentence)
pos_word_list=[]
neu_word_list=[]
neg_word_list=[]
 
for word in tokenized_sentence:
    if (Analyzer.polarity_scores(word)['compound']) &gt;= 0.1:
        pos_word_list.append(word)
        pos_word_list.append(Analyzer.polarity_scores(word)['compound'])
    elif (Analyzer.polarity_scores(word)['compound']) &lt;= -0.1:
        neg_word_list.append(word)
        neg_word_list.append(Analyzer.polarity_scores(word)['compound'])
    else:
        neu_word_list.append(word)
        neu_word_list.append(Analyzer.polarity_scores(word)['compound'])

print('Positive:',pos_word_list)
print('Neutral:',neu_word_list)
print('Negative:',neg_word_list) 
score = Analyzer.polarity_scores(sentence)
print('\nScores:', score)
</code></pre>
<p>From what I perceive from your question, I guess you might be looking for output such as this. Let me know, if otherwise.</p>
<p><strong>OUTPUT:</strong></p>
<pre><code>Positive: ['sure', 0.3182, 'happy', 0.5719]
Neutral: ['Make', 0.0, 'you', 0.0, 'stay', 0.0, 'and', 0.0, 'less', 0.0]
Negative: ['doubtful', -0.34]

Scores: {'neg': 0.161, 'neu': 0.381, 'pos': 0.458, 'compound': 0.5984}
</code></pre>
",1,1,839,2021-01-23 23:23:15,https://stackoverflow.com/questions/65865554/how-to-print-valence-score-for-each-lexicon-in-vader
Is there an R function to clean via a custom dictionary,"<p>I would like to use a custom dictionary (upwards of 400,000 words) when cleaning my data in R. I already have the dictionary loaded as a large character list and I am trying to have it so that the content within my data (VCorpus) compromises of only the words in my dictionary.<br />
For example:</p>
<pre><code>#[1] &quot;never give up uouo cbbuk jeez&quot;  
</code></pre>
<p>would become</p>
<pre><code>#[1*] &quot;never give up&quot;  
</code></pre>
<p>as the words &quot;never&quot;,&quot;give&quot;,and &quot;up&quot; are all in the custom dictionary.
I have previously tried the following:</p>
<pre><code>#Reading the custom dictionary as a function
    english.words  &lt;- function(x) x %in% custom.dictionary
#Filtering based on words in the dictionary
    DF2 &lt;- DF1[(english.words(DF1$Text)),]
</code></pre>
<p>but my result is a character list with one word. Any advice?</p>
","r, text-mining, data-cleaning, sentiment-analysis","<p>You can split the sentences into words, keep only words that are part of your dictionary and paste them in one sentence again.</p>
<pre><code>DF1$Text1 &lt;- sapply(strsplit(DF1$Text, '\\s+'), function(x) 
                    paste0(Filter(english.words, x), collapse = ' '))
</code></pre>
<p>Here I have created a new column called <code>Text1</code> with only english words, if you want to replace the original column you can save the output in  <code>DF1$Text</code>.</p>
",2,0,258,2021-01-25 07:40:24,https://stackoverflow.com/questions/65880680/is-there-an-r-function-to-clean-via-a-custom-dictionary
How can I make sentiment analysis with new sentence on trained model?,"<p>I trained a model by using Naive Bayes. I have high accuracy, but now I want to give a sentence then I want to see it's sentiment. Here it is my code:</p>
<pre><code># data Analysis
import pandas as pd

# data Preprocessing and Feature Engineering
from textblob import TextBlob
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

# Model Selection and Validation
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import joblib

import warnings
import mlflow

warnings.filterwarnings(&quot;ignore&quot;)

train_tweets = pd.read_csv('data/train.csv')

tweets = train_tweets.tweet.values
labels = train_tweets.label.values

processed_features = []

for sentence in range(0, len(tweets)):
    # Remove all the special characters
    processed_feature = re.sub(r'\W', ' ', str(tweets[sentence]))

    # remove all single characters
    processed_feature= re.sub(r'\s+[a-zA-Z]\s+', ' ', processed_feature)

    # Remove single characters from the start
    processed_feature = re.sub(r'\^[a-zA-Z]\s+', ' ', processed_feature)

    # Substituting multiple spaces with single space
    processed_feature = re.sub(r'\s+', ' ', processed_feature, flags=re.I)

    # Removing prefixed 'b'
    processed_feature = re.sub(r'^b\s+', '', processed_feature)

    # Converting to Lowercase
    processed_feature = processed_feature.lower()

    processed_features.append(processed_feature)


vectorizer = TfidfVectorizer(max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))
processed_features = vectorizer.fit_transform(processed_features).toarray()

X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)

text_classifier = MultinomialNB()
text_classifier.fit(X_train, y_train)

predictions = text_classifier.predict(X_test)

print(confusion_matrix(y_test,predictions))
print(classification_report(y_test,predictions))
print(accuracy_score(y_test, predictions))


joblib.dump(text_classifier, 'model.pkl')
</code></pre>
<p>As you can see, I'm saving my model. Now, I want give an input like this:</p>
<pre><code>new_sentence = &quot;I am very happy today&quot;
model.predict(new_sentence)
</code></pre>
<p>And I want see something like this as an output:</p>
<pre><code>sentence = &quot;I am very happy today&quot;
sentiment = Positive
</code></pre>
<p>How can I do that?</p>
","machine-learning, scikit-learn, nlp, sentiment-analysis","<p>First, put the preprocessing in a function:</p>
<pre><code>def preproc(tweets):
    processed_features = []

    for sentence in range(0, len(tweets)):
        # Remove all the special characters
        processed_feature = re.sub(r'\W', ' ', str(tweets[sentence]))

        # remove all single characters
        processed_feature= re.sub(r'\s+[a-zA-Z]\s+', ' ', processed_feature)

        # Remove single characters from the start
        processed_feature = re.sub(r'\^[a-zA-Z]\s+', ' ', processed_feature)

        # Substituting multiple spaces with single space
        processed_feature = re.sub(r'\s+', ' ', processed_feature, flags=re.I)

        # Removing prefixed 'b'
        processed_feature = re.sub(r'^b\s+', '', processed_feature)

        # Converting to Lowercase
        processed_feature = processed_feature.lower()

        processed_features.append(processed_feature)

    return processed_features

processed_features = preproc(tweets)
vectorizer = TfidfVectorizer(max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))
processed_features = vectorizer.fit_transform(processed_features).toarray()
</code></pre>
<p>Then use it to preprocess the test string and feed it to the classifier using <code>transform</code>:</p>
<pre><code># feeding two 1-sentence tweets:
test = preproc([[&quot;I hate this book.&quot;], [&quot;I love this movie.&quot;]])
predictions = text_classifier.predict(vectorizer.transform(test).toarray())
print(predictions) 
</code></pre>
<p>Now, depending on what labels you have in the dataset and how <code>train_tweets.label.values</code> is coded, you will get different output that you can parse into a string. For example, if the labels in the dataset are coded as 1=positive and 0=negative, you might get [0,1].</p>
",1,2,376,2021-02-15 11:34:09,https://stackoverflow.com/questions/66207282/how-can-i-make-sentiment-analysis-with-new-sentence-on-trained-model
Receiving Key Error = 0 while calculating the polarity in Python,"<p>I have two columns - <code>text</code> and <code>title</code> for news articles.</p>
<p>Data looks fine, apologize for a printscreen, just to show the structure.</p>
<p><a href=""https://i.sstatic.net/GSoST.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GSoST.png"" alt="""" /></a></p>
<p>But it gives me a weird error when I try to calculate the polarity.</p>
<pre><code># Create
polarity = []

# Creare for loop for Text column only
for i in range(len(jordan_df['text'])):
    polarity.append(TextBlob(jordan_df['text'][i]).sentiment.polarity)

# Put data together    
polarity_data = {'article_text':jordan_df['text'], 'article_polarity': polarity}
</code></pre>
<p>The weird thing that this code works, when I change <code>jordan_df</code> to <code>some_df</code> with the same structure.</p>
<p>Error:</p>
<pre><code>KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, 
tolerance)
2897             try:
-&gt; 2898                 return self._engine.get_loc(casted_key)
2899             except KeyError as err:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

**KeyError: 0**

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
3 frames
&lt;ipython-input-186-edab50678cab&gt; in &lt;module&gt;()
  9 # Creare for loop for Text column only
 10 for i in range(len(jordan_df['text'])):
---&gt; 11     polarity.append(TextBlob(jordan_df['text'][i]).sentiment.polarity)
 12 
 13 # Put data together

/usr/local/lib/python3.7/dist-packages/pandas/core/series.py in __getitem__(self, key)
880 
881         elif key_is_scalar:
--&gt; 882             return self._get_value(key)
883 
884         if is_hashable(key):

/usr/local/lib/python3.7/dist-packages/pandas/core/series.py in _get_value(self, label, takeable)
988 
989         # Similar to Index.get_value, but we do not fall back to positional
--&gt; 990         loc = self.index.get_loc(label)
991         return self.index._get_values_for_loc(self, loc, label)
992 

/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, 
tolerance)
2898                 return self._engine.get_loc(casted_key)
2899             except KeyError as err:
-&gt; 2900                 raise KeyError(key) from err
2901 
2902         if tolerance is not None:
</code></pre>
","python, nlp, sentiment-analysis, keyerror","<p>Add this line in your code:</p>
<pre><code>polarity = []

jordan_df.reset_index(drop=True,inplace = True)  #add this line


# Creare for loop for Text column only
for i in range(len(jordan_df['text'])):
    polarity.append(TextBlob(jordan_df['text'][i]).sentiment.polarity)

# Put data together    
polarity_data = {'article_text':jordan_df['text'], 'article_polarity': polarity}
</code></pre>
<p>You have probably filtered out result, which have changed the index in your <code>jordan_df</code>. You can see in <code>head()</code> of your <code>jordan_df</code> that the index starts with 7.</p>
<p>And that's why you get <code>KeyError</code> on Key <code>0</code></p>
<p>i.e. when <code>i=0</code> in <code>jordan_df['text'][i]</code></p>
",1,0,594,2021-03-07 02:07:38,https://stackoverflow.com/questions/66512572/receiving-key-error-0-while-calculating-the-polarity-in-python
How to iterate the list and get the sentiments through pandas dataframe column?,"<pre><code>How to iterate the list and get the sentiments through pandas dataframe column?
</code></pre>
<p>I have one dataframe with only one column and only comments in that column.</p>
<pre><code>data.head()
</code></pre>
<p>Output :</p>
<pre><code>    Review
0   If you've ever been to Disneyland anywhere you...
1   Its been a while since d last time we visit HK...
2   Thanks God it wasn t too hot or too humid wh...
3   HK Disneyland is a great compact park. Unfortu...
4   the location is not in the city, took around 1...
</code></pre>
<p>I'm using hugging face sentiment classifier which returns the sentiment for the comment for example</p>
<pre><code>classifier(&quot;My name is mark&quot;)
</code></pre>
<p>Output is :</p>
<pre><code>[{'label': 'POSITIVE', 'score': 0.9953688383102417}]
</code></pre>
<p>To only get the label :</p>
<pre><code>basic_sentiment = [i['label'] for i in value if 'label' in i]
basic_sentiment
</code></pre>
<p>Output is :</p>
<pre><code>['POSITIVE']
</code></pre>
<p><strong>How to run all the given comments in the dataframe in the classifier and return the output?</strong></p>
<pre><code>sent = []

for i in text[:]:
  sentiment = classifier(i)
  sent.append(sentiment)
</code></pre>
<p>I tried the above code, but's it's returning error</p>
","python, pandas, list, dataframe, sentiment-analysis","<p>I understand your problem is iterating thorugh a pandas dataframe, not returning only <code>label</code> from the classifier.</p>
<p>If you want to return a list with the result of classifier() you'll need:</p>
<pre><code>sent = []

for i in range(len(data)):
    sentiment = classifier(data.iloc[i,0])
    sent.append(sentiment)
</code></pre>
<p><code>range(len(data))</code> iterates trough all rows of the dataframe. <code>data.iloc[i,0]</code> takes the value from the i-th row and the 0-th column (you only have one column and python is zero-indexed).</p>
<p>you could also save the result of classifier in another column so you have a dataframe with two columns, one contains the plain text, the other contains the sentiment:</p>
<pre><code># initialize new column
data['Sentiment'] = ''

for i in range(len(data)):
    data.iloc[i,1] = classifier(data.iloc[i,0])
</code></pre>
<p>Where <code>data.iloc[i,1]</code> targets the i-th row and the column with index 1 which is your second column (zero-indexing) <code>Sentiment</code>.</p>
<p>If you only want to save the <code>label</code> the you'll need insert that step into the code, however it's not really clear from you question what you want to save.</p>
",-1,1,436,2021-04-05 09:20:55,https://stackoverflow.com/questions/66950731/how-to-iterate-the-list-and-get-the-sentiments-through-pandas-dataframe-column
text mining preprocessing must be applied to test or to train set?,"<p>I'm doing some text-mining tasks and I have such a simple question and I still can't reach a conclusion.</p>
<p>I am applying pre-processing, such as tokenization and stemming to my training set so i can train my model.</p>
<p>Should I also apply this pre-processing to my test set?</p>
","python, nlp, text-mining, sentiment-analysis","<p>Yes, you should apply same things to your test set. Because you test set must represent your train set, that's why they should be from same distribution. Let's think intuitively:</p>
<p>You will enter an exam. In order you to prepare for exam and get a normal result, lecturer should ask from same subjects in the lectures. But if the lecturer ask questions from a totally different subjects that no one has seen, it is not possible to get a normal result.</p>
",1,0,245,2021-04-17 20:34:59,https://stackoverflow.com/questions/67142717/text-mining-preprocessing-must-be-applied-to-test-or-to-train-set
Spacy Dependency Parsing with Pandas dataframe,"<p>I would like to extract noun-adjective pair for Aspect Based Sentiment Analysis using Spacy's Dependency parser on my pandas dataframe. I was trying this code on Amazon fine food reviews dataset from Kaggle: <a href=""https://stackoverflow.com/questions/60967134/named-entity-recognition-in-aspect-opinion-extraction-using-dependency-rule-matc"">Named Entity Recognition in aspect-opinion extraction using dependency rule matching</a></p>
<p>However, something seems to be wrong the way I feed my pandas dataframe to spacy. My results are not the way I would expect them to be. Could someone help me debug this please. Thanks a lot.</p>
<pre><code>!python -m spacy download en_core_web_lg
import nltk
nltk.download('vader_lexicon')

import spacy
nlp = spacy.load(&quot;en_core_web_lg&quot;)

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()


def find_sentiment(doc):
    # find roots of all entities in the text
  for i in df['Text'].tolist():
    doc = nlp(i)
    ner_heads = {ent.root.idx: ent for ent in doc.ents}
    rule3_pairs = []
    for token in doc:
        children = token.children
        A = &quot;999999&quot;
        M = &quot;999999&quot;
        add_neg_pfx = False
        for child in children:
            if(child.dep_ == &quot;nsubj&quot; and not child.is_stop): # nsubj is nominal subject
                if child.idx in ner_heads:
                    A = ner_heads[child.idx].text
                else:
                    A = child.text
            if(child.dep_ == &quot;acomp&quot; and not child.is_stop): # acomp is adjectival complement
                M = child.text
            # example - 'this could have been better' -&gt; (this, not better)
            if(child.dep_ == &quot;aux&quot; and child.tag_ == &quot;MD&quot;): # MD is modal auxiliary
                neg_prefix = &quot;not&quot;
                add_neg_pfx = True
            if(child.dep_ == &quot;neg&quot;): # neg is negation
                neg_prefix = child.text
                add_neg_pfx = True
        if (add_neg_pfx and M != &quot;999999&quot;):
            M = neg_prefix + &quot; &quot; + M
        if(A != &quot;999999&quot; and M != &quot;999999&quot;):
            rule3_pairs.append((A, M, sid.polarity_scores(M)['compound']))
    return rule3_pairs
df['three_tuples'] = df['Text'].apply(find_sentiment) 
df.head()
</code></pre>
<p>My result is coming like this which clearly means something is wrong with my loop:
<a href=""https://i.sstatic.net/TXp2I.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TXp2I.png"" alt=""enter image description here"" /></a></p>
","python, pandas, nlp, spacy, sentiment-analysis","<p>If you call <code>apply</code> on <code>df['Text']</code>, then you are essentially looping over every value in that column and passing that value to a function.</p>
<p>Here, however, your function itself iterates over the same dataframe column that you are applying the function to while also overwriting the value that is passed to it early in the function.</p>
<p>So I would start by rewriting the function as follows and see if it produces the intended results. I can't say for sure, as you didn't post any sample data, but this should at least move the ball forward:</p>
<pre><code>def find_sentiment(text):
    doc = nlp(text)
    ner_heads = {ent.root.idx: ent for ent in doc.ents}
    rule3_pairs = []
    for token in doc:
        children = token.children
        A = &quot;999999&quot;
        M = &quot;999999&quot;
        add_neg_pfx = False
        for child in children:
            if(child.dep_ == &quot;nsubj&quot; and not child.is_stop): # nsubj is nominal subject
                if child.idx in ner_heads:
                    A = ner_heads[child.idx].text
                else:
                    A = child.text
            if(child.dep_ == &quot;acomp&quot; and not child.is_stop): # acomp is adjectival complement
                M = child.text
            # example - 'this could have been better' -&gt; (this, not better)
            if(child.dep_ == &quot;aux&quot; and child.tag_ == &quot;MD&quot;): # MD is modal auxiliary
                neg_prefix = &quot;not&quot;
                add_neg_pfx = True
            if(child.dep_ == &quot;neg&quot;): # neg is negation
                neg_prefix = child.text
                add_neg_pfx = True
        if (add_neg_pfx and M != &quot;999999&quot;):
            M = neg_prefix + &quot; &quot; + M
        if(A != &quot;999999&quot; and M != &quot;999999&quot;):
            rule3_pairs.append((A, M, sid.polarity_scores(M)['compound']))
    return rule3_pairs

</code></pre>
",2,0,749,2021-04-18 16:35:56,https://stackoverflow.com/questions/67150944/spacy-dependency-parsing-with-pandas-dataframe
Can&#39;t get update_polarity_table in Sentimentr to update polarity,"<p>I'm trying to do a sentiment analysis using hash_sentiment_socal_google in Sentimentr. Looking through the responses, I've noticed that one word responses of &quot;unsure&quot;, or &quot;unknown&quot;, get an average sentiment score of -.5. And &quot;yes&quot;, gets .8. I would like all of them to show up as 0, or neutral.</p>
<p>I don't actually see any of these words in hash_sentiment_socal_google, so I'm not sure why these responses are being assigned sentiment scores. But I just figured I could add to the key with the following code to set to 0:</p>
<pre><code>updated_socal_google &lt;- 
  sentimentr:::update_polarity_table(lexicon::hash_sentiment_socal_google,
  x = data.frame(words = c('yes', 'unsure', 'unknown'),
  polarity = c(0, 0, 0), stringsAsFactors = FALSE))
</code></pre>
<p>But after running the code below:</p>
<pre><code>sentiments_new &lt;- sentiment_by(text_sentences, by = NULL, 
                           averaging.function = average_mean,  
                           updated_socal_google, amplifier.weight = .5, 
                           n.before = 10, n.after = 4)
</code></pre>
<p>These one word responses are still getting assigned the same average sentiment scores as before, not 0. Can someone explain what I'm doing wrong?</p>
<p>Thank you!</p>
","r, sentiment-analysis, sentimentr","<p>Found out the answer, so wanted to update in case anyone else runs into this issue. I needed to specify that polarity_dt = updated_socal_google.</p>
<p>So instead of what I had above:</p>
<pre><code>sentiments_new &lt;- sentiment_by(text_sentences, by = NULL, 
                           averaging.function = average_mean,  
                           polarity_dt = updated_socal_google, amplifier.weight = .5, 
                           n.before = 10, n.after = 4)
</code></pre>
",0,0,50,2021-04-20 20:22:09,https://stackoverflow.com/questions/67185839/cant-get-update-polarity-table-in-sentimentr-to-update-polarity
VaderSentiment: emoji analyzer does not work in Jupyter Notebook,"<p>I am trying to do some sentiment analysis on r/wallstreetbets content and would also like to use the meaning of emojis.</p>
<p>Here is my code:</p>
<pre><code>from nltk.sentiment.vader import SentimentIntensityAnalyzer

wsb_lingo = {
    &quot;bullish&quot;: 4.0,
    &quot;bearish&quot;: -4.0,
    &quot;bagholder&quot;: -4.0,
    &quot;BTFD&quot;: 4.0,
    &quot;FD&quot;: 4.0,
    &quot;diamond hands&quot;: 0.0,
    &quot;paper hands&quot;: 0.0,
    &quot;DD&quot;: 4.0,
    &quot;GUH&quot;: -4.0,
    &quot;pump&quot;: 4.0,
    &quot;dump&quot;: -4.0,
    &quot;gem stone&quot;: 4.0, # emoji
    &quot;rocket&quot;: 4.0, # emoji
    &quot;andromeda&quot;: 0.0,
    &quot;to the moon&quot;: 4.0,
    &quot;stonks&quot;: -4.0,
    &quot;tendies&quot;: 4.0,
    &quot;buy&quot;: 4.0,
    &quot;sell&quot;: -4.0,
    &quot;hold&quot;: 4.0,
    &quot;short&quot;: 4.0,
    &quot;long&quot;: 4.0,
    &quot;overvalued&quot;: -4.0,
    &quot;undervalued&quot;: 4.0,
    &quot;calls&quot;: 4.0,
    &quot;call&quot;: 4.0,
    &quot;puts&quot;: -4.0,
    &quot;put&quot;: -4.0,
}

sid = SentimentIntensityAnalyzer()
sid.lexicon.update(wsb_lingo)

# Test
print(sid.polarity_scores('🚀'))
print(sid.polarity_scores('😄'))
</code></pre>
<p>The output is given below:</p>
<pre><code>{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}
{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}
</code></pre>
<p>How is it possible that it's unable to give any sentiment for emojis (e.g., due to Jupyter Notebook)? Am I forgetting something here? All libraries are up-to-date.</p>
","python, emoji, sentiment-analysis, vader","<p>If I use <code>vaderSentiment</code> instead of <code>nltk.sentiment.vader</code> it works for me</p>
<pre><code>from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

new = { &quot;rocket&quot;: 4.0 }
sia = SentimentIntensityAnalyzer()
sia.polarity_scores('🚀')
# Outputs: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}

sia.lexicon.update(new)
sia.polarity_scores('🚀')
# Outputs: {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.7184}
</code></pre>
<p>See also <a href=""https://github.com/cjhutto/vaderSentiment/issues/63"" rel=""nofollow noreferrer"">this issue</a></p>
",1,1,387,2021-04-22 14:41:33,https://stackoverflow.com/questions/67215471/vadersentiment-emoji-analyzer-does-not-work-in-jupyter-notebook
"I was working on a movie sentiment analysis but, code but i&#39;m facing issues in my code related to processing words","<p>My data set has 42,000 rows. This is the code I used to edit my text before vectorizing it. However the problem is it has a nested for loop which I guess makes it very slow and I'm not being able to use it with more than 1500 rows. Can someone please help out on a better way to do this?</p>
<pre><code>filtered = []
for i in range(2):
    rev = re.sub('[^a-zA-Z]', ' ', df['text'][i])
    rev = rev.lower()
    rev = rev.split()
    filtered =[]
    for word in rev:
        if word not in stopwords.words(&quot;english&quot;):
            word = PorterStemmer().stem(word)
            filtered.append(word)
    filtered = &quot; &quot;.join(filtered)
    corpus.append(filtered)
</code></pre>
","nlp, nltk, sentiment-analysis","<p>I've used <a href=""https://github.com/pyutils/line_profiler"" rel=""nofollow noreferrer"">line_profiler</a> to measure the speed of the code you posted.</p>
<p>The measurement results are as follows.</p>
<pre><code>Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     8                                           @profile
     9                                           def profile_nltk():
    10         1     435819.0 435819.0      0.3      df = pd.read_csv('IMDB_Dataset.csv')  # (50000, 2)
    11         1          1.0      1.0      0.0      filtered = []
    12         1        247.0    247.0      0.0      reviews = df['review'][:4000]
    13         1          0.0      0.0      0.0      corpus = []
    14      4001     216341.0     54.1      0.1      for i in range(len(reviews)):
    15      4000     221885.0     55.5      0.2          rev = re.sub('[^a-zA-Z]', ' ', df['review'][i])
    16      4000       3878.0      1.0      0.0          rev = rev.lower()
    17      4000      30209.0      7.6      0.0          rev = rev.split()
    18      4000       1097.0      0.3      0.0          filtered = []
    19    950808     235589.0      0.2      0.2          for word in rev:
    20    946808  115658060.0    122.2     78.2              if word not in stopwords.words(&quot;english&quot;):
    21    486614   30898223.0     63.5     20.9                  word = PorterStemmer().stem(word)
    22    486614     149604.0      0.3      0.1                  filtered.append(word)
    23      4000      11290.0      2.8      0.0          filtered = &quot; &quot;.join(filtered)
    24      4000       1429.0      0.4      0.0          corpus.append(filtered)
</code></pre>
<p>As @parsa-abbasi pointed out, the process of checking the stopword accounts for about 80% of the total.</p>
<p>The measurement results for the modified script are as follows. The same process has been reduced to about 1/100th of the processing time.</p>
<pre><code>Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     8                                           @profile
     9                                           def profile_nltk():
    10         1     441467.0 441467.0      1.4      df = pd.read_csv('IMDB_Dataset.csv')  # (50000, 2)
    11         1          1.0      1.0      0.0      filtered = []
    12         1        335.0    335.0      0.0      reviews = df['review'][:4000]
    13         1          1.0      1.0      0.0      corpus = []
    14         1       2696.0   2696.0      0.0      stopwords_set = stopwords.words('english')
    15      4001      59013.0     14.7      0.2      for i in range(len(reviews)):
    16      4000     186393.0     46.6      0.6          rev = re.sub('[^a-zA-Z]', ' ', df['review'][i])
    17      4000       3657.0      0.9      0.0          rev = rev.lower()
    18      4000      27357.0      6.8      0.1          rev = rev.split()
    19      4000        999.0      0.2      0.0          filtered = []
    20    950808     220673.0      0.2      0.7          for word in rev:
    21                                                       # if word not in stopwords.words(&quot;english&quot;):
    22    946808    1201271.0      1.3      3.8              if word not in stopwords_set:
    23    486614   29479712.0     60.6     92.8                  word = PorterStemmer().stem(word)
    24    486614     141242.0      0.3      0.4                  filtered.append(word)
    25      4000      10412.0      2.6      0.0          filtered = &quot; &quot;.join(filtered)
    26      4000       1329.0      0.3      0.0          corpus.append(filtered)
</code></pre>
<p>I hope this is helpful.</p>
",0,0,55,2021-05-01 20:11:23,https://stackoverflow.com/questions/67350459/i-was-working-on-a-movie-sentiment-analysis-but-code-but-im-facing-issues-in-m
How to perform Sentiment Analysis on Noun Phrases in Pandas?,"<p>I need your help as i tried every method but not able to perform sentiment analysis on my noun phrases, extracted from tweets in dataframe, using TextBlob. Also i think TextBlob.noun_phrases function is not producing the correct results. See for yourself in the image below. I am really new to Python, please help!!</p>
<p>So my code for extracting the Noun phrase from dataframe is:</p>
<pre><code>from textblob import TextBlob
nltk.download('wordnet')
nltk.download('brown')
nltk.download('punkt')

def blob(text):
  return TextBlob(text).noun_phrases

df['Noun_Phrases'] = df['Tweets'].apply(blob)

df
</code></pre>
<p><a href=""https://i.sstatic.net/wkIBu.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Next, my code for sentiment analysis is below, and i get the error as shown in below image:</p>
<pre><code>def getsubjectivity(text):
return TextBlob(text).sentiment.subjectivity 

df['Subjectivity'] = df['Noun_Phrases'].apply(getsubjectivity)
</code></pre>
<p>Error : TypeError: The <code>text</code> argument passed to <code>__init__(text)</code> must be a string, not &lt;class 'textblob.blob.WordList'&gt;</p>
<p><a href=""https://i.sstatic.net/gYWZO.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","python, pandas, twitter, sentiment-analysis, textblob","<p>not sure about your objective. in your <code>getsubjectivity</code> function the input need to be string, seems like you are feeding it a list.</p>
<p>if you make the below change, you will overcome the error.</p>
<pre><code>def getsubjectivity(text):
    text=''.join(text)
    return TextBlob(text).sentiment.subjectivity 
</code></pre>
",0,0,339,2021-05-08 00:10:58,https://stackoverflow.com/questions/67442929/how-to-perform-sentiment-analysis-on-noun-phrases-in-pandas
NLTK polarity_scores,"<p>I was wonder if i can get the Sentimentia.polarity_scores() function to print only the negative, positive only and I can get rid of the neutral  and compound results</p>
<pre><code>    i = 0
    while i &lt; len(Replaced_Data):
        Sentimentia = SentimentIntensityAnalyzer()
        print(Sentimentia.polarity_scores(data['Clean_TweetText'][i]))
        i = i + 1

output &gt;&gt;&gt; {'neg': 0.214, 'neu': 0.534, 'pos': 0.252, 'compound': 0.25}

desired output &gt;&gt;&gt; {'neg': 0.214, 'pos': 0.188}
</code></pre>
","python, nltk, sentiment-analysis","<p>Can filter the 'neg' and 'pos' from the dictionary using <a href=""https://www.python.org/dev/peps/pep-0274/"" rel=""nofollow noreferrer"">Dictionary Comprehensions</a></p>
<pre><code>ss = Sentimentia.polarity_scores(data['Clean_TweetText'][i])     
print({e:ss[e] for e in ss if e in ['neg','pos']})

</code></pre>
",1,1,602,2021-05-15 12:15:46,https://stackoverflow.com/questions/67546647/nltk-polarity-scores
Pandas associate or filter a date column between a range and groupby another column,"<p>I am new to pandas and I am trying to carry out some EDA on my twitter dataset. <a href=""https://i.sstatic.net/W2wWA.png"" rel=""nofollow noreferrer"">Dataset column</a></p>
<p>Link to Dataset : <a href=""https://www.kaggle.com/kaushiksuresh147/the-social-dilemma-tweets"" rel=""nofollow noreferrer"">https://www.kaggle.com/kaushiksuresh147/the-social-dilemma-tweets</a></p>
<p>Dataframe Sample : <a href=""https://i.sstatic.net/hemU4.png"" rel=""nofollow noreferrer"">Sample dataframe</a></p>
<p>I want to filter new users created (from the user_created column) between &quot;2020-09-08 and 2020-09-22&quot; and then group the results with the sentiment column. I also want to count the total number of tweets created from this new users within that period and compare it with the overall number of tweets from other users which are not in the selected range(2020-09-08 and 2020-09-22).</p>
<p>I have tried an approach and my code keeps giving me the error message : KeyError: 'user_created'<a href=""https://i.sstatic.net/6t7Ga.png"" rel=""nofollow noreferrer"">code snippet</a></p>
<p>I also tried this code which also gives me error message:KeyError: 'user_created'<a href=""https://i.sstatic.net/MGpDZ.png"" rel=""nofollow noreferrer"">2nd code </a></p>
<pre><code>df['user_created'] = pd.to_datetime(df['user_created'])
start = '2020-09-08'
end = '2020-09-20'
df[(df['user_created'] &gt;= start) &amp; (df['user_created'] &lt;= end)]
df[(df['user_created'] &gt;= '2020-09-08') &amp; (df['user_created'] &lt;= '2020-09-22')]
grouped_df = df.groupby(['user_name', 'Sentiment','user_created']).size().reset_index(name=&quot;Count&quot;)
print(grouped_df.to_string(header=False))
</code></pre>
<p>I have tried the df.get(user_created) to retrieve this column but it doesnt seem to work.</p>
","python, pandas, pandas-groupby, sentiment-analysis","<p>I think <code>start</code> and <code>end</code> should be in datetime format (<code>datetime.datetime</code>, <code>np.datetime64</code>, or <code>pd.Timestamp</code>), not in string format.</p>
<pre><code>from datetime import datetime

start = datetime.strptime('2020-09-08', '%Y-%m-%d')
end = datetime.strptime('2020-09-20', '%Y-%m-%d')
df[(df['user_created'] &gt;= start) &amp; (df['user_created'] &lt;= end)]
</code></pre>
",0,0,150,2021-05-24 04:05:27,https://stackoverflow.com/questions/67666542/pandas-associate-or-filter-a-date-column-between-a-range-and-groupby-another-col
"Update watsonplatform.net endpoint for sentiment analysis, getting 404","<p>I updated the Watson end-point as per IBM's email instructions (service credentials URL &amp; api) and when I execute the call, I get a &quot;404 Not Found&quot;.</p>
<p>Url is</p>
<pre><code>https://api.eu-gb.natural-language-understanding.watson.cloud.ibm.com/instances/2xxxxx43-52ac-4xx5-922d-cxxxxx5ea52b
</code></pre>
<p>EDIT:
This is the test I am doing (and the same code I used for analysing text in live environment):</p>
<pre><code>    function Analyse($text)
    {
        $body = '{
              &quot;text&quot;: &quot;' . addslashes($text) . '&quot;,
              &quot;features&quot;: {
                &quot;sentiment&quot;: {}
              }
            }';

        $url = &quot;https://api.eu-gb.natural-language-understanding.watson.cloud.ibm.com/instances/2xxxxx43-52ac-4xx5-922d-cxxxxx5ea52b&quot;;
        $key = &quot;ukLuEL.........qinV&quot;;

        $response = Request::post($url)
            -&gt;authenticateWith('apikey', $key)// authenticate with basic auth...
            -&gt;body($body)
            -&gt;sendsJson()
            -&gt;send();

        $body = $response-&gt;body; // &lt;&lt;&lt;&lt;&lt; THIS MESSAGE STATES &quot;NOT FOUND 404&quot;
        $sentiment = $body-&gt;sentiment-&gt;document;

        return [
            &quot;label&quot; =&gt; $sentiment-&gt;label,
            &quot;score&quot; =&gt; $sentiment-&gt;score
        ];
    }
</code></pre>
<p>If I substitute the old credentials back, it works and returns the sentiment. The old URL is:</p>
<pre><code>https://gateway-lon.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2018-03-19
</code></pre>
","ibm-cloud, ibm-watson, sentiment-analysis","<p>Make sure to not substitue the old full path with just the base URI. See the <a href=""https://cloud.ibm.com/apidocs/natural-language-understanding#introduction"" rel=""nofollow noreferrer"">API docs for IBM Watson Natural Language Understanding</a> for the base URI and the path for the individual API function.</p>
<p>Thus, this URI:</p>
<pre><code>https://gateway-lon.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2018-03-19
</code></pre>
<p>should become</p>
<pre><code>https://api.eu-gb.natural-language-understanding.watson.cloud.ibm.com/instances/2xxxxx43-52ac-4xx5-922d-cxxxxx5ea52b//v1/analyze?version=2021-03-25
</code></pre>
",0,0,96,2021-05-24 11:45:13,https://stackoverflow.com/questions/67671529/update-watsonplatform-net-endpoint-for-sentiment-analysis-getting-404
How to convert emoji unicode to emoji?,"<p>I have a <code>.json</code> file with over 70,000 tweets, with each tweet containing emojis. However, I am unsure how to convert the Unicode into the actual emojis, so that it can be used for sentiment analysis.</p>
<p>This is a sample of 5 tweets in my <code>.json</code> file:</p>
<pre><code>{&quot;text&quot;:&quot;The morning is going so fast Part 2 of #DiscoveryDay is in full swing \ud83d\ude01\n\nGreat Atmosphere in the room \n\n#BIGSocial\u2026 https:\/\/t.co\/P08qBoH6tv&quot;}
{&quot;text&quot;:&quot;Double kill! #XiuKai lives! I died. \ud83d\ude0c https:\/\/t.co\/QCyk3r2JCb&quot;}
{&quot;text&quot;:&quot;ALLTY \ud83d\udc94&quot;}
{&quot;text&quot;:&quot;Shouldn\u2019t be normal for a 24 year old to be this tiered \ud83d\udca4&quot;}
{&quot;text&quot;:&quot;@TheNames_BrieX Trust me! \ud83d\udcaf&quot;}
</code></pre>
<p>Now, how would I convert the unicode for all the tweets into the actual emoji? For instance, how would \ud83d\ude0c be converted into the actual emoji?</p>
<p>What methods can be used to convert the unicode into the actual emojis?</p>
","python, json, unicode, emoji, sentiment-analysis","<p>If this is your actual JSON file content:</p>
<pre class=""lang-json prettyprint-override""><code>{&quot;text&quot;:&quot;The morning is going so fast Part 2 of #DiscoveryDay is in full swing \ud83d\ude01\n\nGreat Atmosphere in the room \n\n#BIGSocial\u2026 https:\/\/xxx\/P08qBoH6tv&quot;}
{&quot;text&quot;:&quot;Double kill! #XiuKai lives! I died. \ud83d\ude0c https:\/\/xxx\/QCyk3r2JCb&quot;}
{&quot;text&quot;:&quot;ALLTY \ud83d\udc94&quot;}
{&quot;text&quot;:&quot;Shouldn\u2019t be normal for a 24 year old to be this tiered \ud83d\udca4&quot;}
{&quot;text&quot;:&quot;@TheNames_BrieX Trust me! \ud83d\udcaf&quot;}
</code></pre>
<p>Then that is <a href=""https://jsonlines.org/"" rel=""nofollow noreferrer"">JSON Lines</a> format, where each line is a complete JSON structure, and not a single valid JSON file.</p>
<p>Read it a line at a time like so:</p>
<pre class=""lang-py prettyprint-override""><code>import json
with open('test.json') as f:
    for line in f:
        print(json.loads(line))
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>{'text': 'The morning is going so fast Part 2 of #DiscoveryDay is in full swing 😁\n\nGreat Atmosphere in the room \n\n#BIGSocial… https://xxx/P08qBoH6tv'}
{'text': 'Double kill! #XiuKai lives! I died. 😌 https://xxx/QCyk3r2JCb'}
{'text': 'ALLTY 💔'}
{'text': 'Shouldn’t be normal for a 24 year old to be this tiered 💤'}
{'text': '@TheNames_BrieX Trust me! 💯'}
</code></pre>
<p>Note I had to change the tiny URLs from the original since SO disallows content with them.</p>
<p>If, as you say, that was only a sample of the JSON lines, and it is a fully formed, correct JSON file, then just read it with <code>json.load</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import json
with open('test.json') as f:
    print(json.load(f))
</code></pre>
",3,1,4178,2021-05-28 18:08:30,https://stackoverflow.com/questions/67743720/how-to-convert-emoji-unicode-to-emoji
Why does Transformer&#39;s BERT (for sequence classification) output depend heavily on maximum sequence length padding?,"<p>I am using Transformer's RobBERT (the dutch version of RoBERTa) for sequence classification - trained for sentiment analysis on the Dutch Book Reviews dataset.</p>
<p>I wanted to test how well it works on a similar dataset (also on sentiment analysis), so I made annotations for a set of text fragments and checked its accuracy. When I checked what kind of sentence are misclassified, I noticed that the output for a unique sentence depends heavily on the length of padding I give when tokenizing. See code below.</p>
<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch.nn.functional as F
import torch


model = RobertaForSequenceClassification.from_pretrained(&quot;pdelobelle/robBERT-dutch-books&quot;, num_labels=2)
tokenizer = RobertaTokenizer.from_pretrained(&quot;pdelobelle/robBERT-dutch-books&quot;, do_lower_case=True)

sent = 'De samenwerking gaat de laatste tijd beter'
max_seq_len = 64


test_token = tokenizer(sent,
                        max_length = max_seq_len,
                        padding = 'max_length',
                        truncation = True,
                        return_tensors = 'pt'
                        )

out = model(test_token['input_ids'],test_token['attention_mask'])

probs = F.softmax(out[0], dim=1).detach().numpy()
</code></pre>
<p>For the given sample text, which translates in English to &quot;The collaboration has been improving lately&quot;, there is a huge difference in output on classification depending on the max_seq_len. Namely, for <code>max_seq_len = 64</code> the output for <code>probs</code> is:</p>
<p>[[0.99149346 0.00850648]]</p>
<p>whilst for <code>max_seq_len = 9</code>, being the actual length including cls tokens:</p>
<p>[[0.00494814 0.9950519 ]]</p>
<p>Can anyone explain why this huge difference in classification is happening? I would think that the attention mask ensures that in the output there is no difference because of padding to the max sequence length.</p>
","sentiment-analysis, bert-language-model, huggingface-transformers, huggingface-tokenizers","<p>This is caused because your comparison isn't correct. The sentence <code>De samenwerking gaat de laatste tijd beter</code> has actually 16 tokens (+2 for the specialtokens) and not 9. You only counted the words which are not necessarily the tokens.</p>
<pre class=""lang-py prettyprint-override""><code>print(tokenizer.tokenize(sent))
print(len(tokenizer.tokenize(sent)))
</code></pre>
<p>Output:</p>
<pre><code>['De', 'Ġsam', 'en', 'wer', 'king', 'Ġga', 'at', 'Ġde', 'Ġla', 'at', 'ste', 'Ġt', 'ij', 'd', 'Ġbe', 'ter']
16
</code></pre>
<p>When you set the sequence length to 9 you are truncating the sentence to:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer.decode(tokenizer(sent,
                         max_length = 9,
                         padding = 'max_length',
                         truncation = True,
                         return_tensors = 'pt', 
                         add_special_tokens=False
                         )['input_ids'][0])
</code></pre>
<p>Output:</p>
<pre><code>'De samenwerking gaat de la'
</code></pre>
<p>And as final prove, the output when you set <code>max_length</code> to 52 is also [[0.99149346 0.00850648]].</p>
",3,2,1273,2021-05-31 09:33:47,https://stackoverflow.com/questions/67771257/why-does-transformers-bert-for-sequence-classification-output-depend-heavily
NLTK Vader SentimentIntensityAnalyzer Bigram,"<p>For the VADER SentimentIntensityAnalyzer within Python, is there a way to add a bigram rule? I tried updating the lexicon with a two word input, but it did not change the polarity score. Thanks in advance!</p>
<pre><code>from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyser = SentimentIntensityAnalyzer()

#returns a compound score of -0.296
print(analyser.polarity_scores('no issues'))

analyser.lexicon['no issues'] = 0.0
#still returns a compound score of -0.296
print(analyser.polarity_scores('no issues'))
</code></pre>
","python, nlp, nltk, sentiment-analysis, vader","<p>There is no straightforward way to add bigram to the vader lexicon. This is because vader considers individual tokens for sentiment analysis. However, one can do this using following steps:</p>
<ol>
<li>Create bigrams as tokens. For example, you can convert the bigram (&quot;no issues&quot;) into a token (&quot;noissues&quot;).</li>
<li>Maintain a dictionary of polarity of the newly
created tokens. {&quot;noissues&quot; : 2}</li>
<li>Then perform additional text processing before
passing the text for sentiment score calculation.</li>
</ol>
<p>Following code accomplishes the above:</p>
<pre><code>allowed_bigrams = {'noissues' : 2} #add more as per your requirement
    
def process_text(text):
    tokens = text.lower().split() # list of tokens
    bigrams = list(nltk.bigrams(tokens)) # create bigrams as tuples of tokens
    bigrams = list(map(''.join, bigrams)) # join each word without space to create new bigram
    bigrams.append('...') # make length of tokens and bigrams list equal
     
    #begin recreating the text
    final = ''
    for i, token in enumerate(tokens):
        b = bigrams[i]
        
        if b in allowed_bigrams:
          join_word = b # replace the word in text by bigram
          tokens[i+1] = '' #skip the next word
        else:
            join_word = token
        final += join_word + ' '
    return final
text  = 'Hello, I have no issues with you'
print (text)
print (analyser.polarity_scores(text))
final = process_text(text)
print (final)
print(analyser.polarity_scores(final))
</code></pre>
<p>The output :</p>
<pre><code>Hello, I have no issues with you
{'neg': 0.268, 'neu': 0.732, 'pos': 0.0, 'compound': -0.296}
hello, i have noissues  with you 
{'neg': 0.0, 'neu': 0.625, 'pos': 0.375, 'compound': 0.4588}
</code></pre>
<p>Notice in the output, how two words &quot;no&quot; and &quot;issues&quot; have been added together to form bigram &quot;noissues&quot;.</p>
",3,1,857,2021-06-02 03:28:43,https://stackoverflow.com/questions/67798527/nltk-vader-sentimentintensityanalyzer-bigram
(ValueError: Columns must be same length as key) from sentiment analysis,"<p>I have been executing a sentiment analysis, and have returned the positive and negative outcomes to my pd.DataFrame in the following manner.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>author</th>
<th>Text</th>
<th>Sentiment</th>
</tr>
</thead>
<tbody>
<tr>
<td>12323</td>
<td>this is text</td>
<td>(0.25, 0.35)</td>
</tr>
</tbody>
</table>
</div>
<p>However, when I want to split the sentiment column (which consists of Polarity and Subjectivity), I get the following error:</p>
<p><code>ValueError: Columns must be same length as key</code></p>
<p>I have tried multiple approaches:</p>
<ul>
<li>str.split</li>
<li>str.extract</li>
<li>rounding</li>
</ul>
<p>With the rounding approach I get an error mentioning the float NaN's could not be multiplied. So I suppose that there is a NaN in there somewhere. However, when I look for NaN's I get this answer:</p>
<pre><code>author_id       0
text_stemmed    0
sentiment       0
dtype: int64 
</code></pre>
<p>What is the best way to approach this problem?</p>
<p>Thanks!</p>
","python, split, sentiment-analysis","<p>Edit: changed case from <code>df['sentiment']</code> to <code>df['Sentiment']</code></p>
<p>The <code>string</code> methods won't work because it's not a string but a <code>set</code> stored in the cell.</p>
<p>You can do this to create a new column:</p>
<pre><code>df['sentiment_0'] = df['Sentiment'].apply(lambda x: x[0])
df['sentiment_1'] = df['Sentiment'].apply(lambda x: x[1])
</code></pre>
<p>or</p>
<pre><code>df['sentiment_0'], df['sentiment_1'] = df['Sentiment'].explode()
</code></pre>
",3,0,470,2021-06-04 08:07:08,https://stackoverflow.com/questions/67833669/valueerror-columns-must-be-same-length-as-key-from-sentiment-analysis
How to apply a defined function to many rows?,"<p>I would like to apply the defined function &quot;tokenization&quot; to all rows of the column &quot;Review Gast&quot; of the dataset &quot;reviews_english&quot;. How can i do that? Currenty i can only apply it to one row. Thanks! :)</p>
<pre><code>
def tokenization(text):
    # Normalize
    text = normalize(text)

    # Remove Punctuation
    text = remove_punctuation(text)

    # Tokenize
    tokens = text.split()

    # Remove Stopwords
    tokens = remove_stopwords(tokens)

    # Apply Bag-of-Words (set of tokens)
    bow = set(tokens)

    return bow

clean_reviews_english =tokenization(reviews_english[&quot;Review Gast&quot;][0])
print(clean_reviews_english)
</code></pre>
","python, function, sentiment-analysis, review","<p>Use a list comprehension</p>
<pre><code>clean_reviews_english = tokenization(review for review in reviews_english[&quot;Review Gast&quot;])
</code></pre>
<p>or <code>map</code>:</p>
<pre><code>clean_reviews_english = map(tokenization, reviews_english[&quot;Review Gast&quot;])
</code></pre>
",0,0,63,2021-06-04 09:28:45,https://stackoverflow.com/questions/67834847/how-to-apply-a-defined-function-to-many-rows
ValueError: Number of Coefficients does not match number of features (Mglearn visualization),"<p>I am trying to perform a sentiment analysis based on product reviews collected from various websites. I've been able to follow along with the below article until it gets to the model coefficient visualization step.</p>
<p><a href=""https://towardsdatascience.com/how-a-simple-algorithm-classifies-texts-with-moderate-accuracy-79f0cd9eb47"" rel=""nofollow noreferrer"">https://towardsdatascience.com/how-a-simple-algorithm-classifies-texts-with-moderate-accuracy-79f0cd9eb47</a></p>
<p>When I run my program, I get the following error:</p>
<pre><code>ValueError: Number of coefficients 6021 doesn't match number offeature names 6290.
</code></pre>
<p>Any advice on how to ensure the number of coefficients match the number of features in my dataset?</p>
<p>Below is my code:</p>
<pre><code>y = reviews['Review Type']
X = reviews['Review Comment']

#Split the data into training and test sets
from sklearn.model_selection import train_test_split
text_train, text_test, y_train, y_test = train_test_split(X, y, random_state=0)

#run the feature extraction on training &amp; test independent variables with bag of words
#changing the variable back to X_train after transforming it.

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
vect = CountVectorizer().fit(text_train)
X_train = vect.transform(text_train)
print(repr(X_train))

X_test = vect.transform(text_test)
print(repr(X_test))

feature_names = vect.get_feature_names()
print(len(feature_names))

#running a logistic regression model to predict whether a review is positive 
#or negative

from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(max_iter=10000, class_weight='balanced', random_state=0)
param_grid = {'C': [0.01, 0.1, 1, 10, 100]}


grid = GridSearchCV(logreg, param_grid, scoring= 'roc_auc', cv=5)
logreg_train = grid.fit(X_train, y_train)

pred_logreg = logreg_train.predict(X_test)
confusion = confusion_matrix(y_test, pred_logreg)
print(confusion)
print(&quot;Classification accuracy is: &quot;, (confusion[0][0] + confusion[1][1]) / np.sum(confusion))

from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt
import seaborn as sns; sns.set();

fpr, tpr, thresholds = roc_curve(y_test, grid.decision_function(X_test))
# find threshold closest to zero:
close_zero = np.argmin(np.abs(thresholds))
plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10, 
 label= 'threshold zero(default)', fillstyle= 'none', c='k', mew=2)
plt.plot([0,1], linestyle='-', lw=2, color='r', label='random', alpha=0.8)
plt.legend(loc=4)
plt.plot(fpr, tpr, label='ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (recall)')
plt.title('roc_curve');
from sklearn.metrics import auc
print('AUC score is: ', auc(fpr, tpr));


from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(\
                                                      y_test, logreg_train.decision_function(X_test))
close_zero = np.argmin(np.abs(thresholds))
plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10, 
         label=&quot;threhold zero&quot;, fillstyle=&quot;none&quot;, c=&quot;k&quot;, mew=2)
plt.plot(precision, recall, label=&quot;precision recall curve&quot;)
plt.xlabel(&quot;precision&quot;)
plt.ylabel(&quot;recall&quot;)
plt.title(&quot;Precision Recall Curve&quot;)
plt.legend(loc=&quot;best&quot;);

from sklearn.feature_extraction.text import TfidfVectorizer
logreg = LogisticRegression(max_iter=10000, class_weight=&quot;balanced&quot;, random_state=0)
pipe = make_pipeline(TfidfVectorizer(norm=None, stop_words='english'), logreg)
param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}
grid = GridSearchCV(pipe, param_grid, scoring=&quot;roc_auc&quot;, cv=5)
logreg_train = grid.fit(text_train, y_train)

fpr, tpr, thresholds = roc_curve(y_test, grid.decision_function(text_test))
pred_logreg = logreg_train.predict(text_test)
confusion = confusion_matrix(y_test, pred_logreg)
print(confusion)
print(&quot;Classification accuracy is: &quot;, (confusion[0][0] + confusion[1][1]) / np.sum(confusion)) 
print(&quot;Test AUC score is: &quot;, auc(fpr, tpr));

mglearn.tools.visualize_coefficients(grid.best_estimator_.named_steps[&quot;logisticregression&quot;].coef_,feature_names, n_top_features=25)
</code></pre>
","python, machine-learning, sentiment-analysis, valueerror","<p>You've defined <code>feature_names</code> in terms of the features from a <code>CountVectorizer</code> with the default <code>stop_words=None</code>, but your model in the last bit of code is using a <code>TfidfVectorizer</code> with <code>stop_words='english'</code>.  Use instead</p>
<pre class=""lang-py prettyprint-override""><code>feature_names = grid.best_estimator_.named_steps[&quot;tfidfvectorizer&quot;].get_feature_names()
</code></pre>
",0,-2,213,2021-06-09 16:01:46,https://stackoverflow.com/questions/67907780/valueerror-number-of-coefficients-does-not-match-number-of-features-mglearn-vi
Paysify sentiment api returning null,"<p>Good day fam. I have this sentiment analysis API from Paysify, which returns a JSON output but all of a sudden it is now returning <code>NULL</code> as output.
Please I help here, I have been trying to fix this for the past 4 hours, no headway.</p>
<pre><code>   function detect_sentiment($string){
      $string = urlencode($string);
      $api_key = &quot;&lt; api key &gt;&quot;;
      $url = 'https://api.paysify.com/sentiment?api_key='.$api_key.'&amp;string='.$string.'';
      $ch = curl_init();
      curl_setopt($ch, CURLOPT_URL, $url);
      curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);
      curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
      
      $result = curl_exec($ch);
      $response = json_decode($result,true);
      curl_close($ch);
      return $response;
   }
      
  print_r(detect_sentiment(&quot;I love this product&quot;));

</code></pre>
<p>Thanks</p>
","php, api, nlp, sentiment-analysis","<p>This issue is not related to your code. The service you use has an issue on their server. If you try to open your Chrome browser and navigate to <a href=""https://api.paysify.com/sentiment?api_key=24ed361e9d48b3e84a7170fff53609&amp;string=I%20love%20this%20product"" rel=""nofollow noreferrer"">https://api.paysify.com/sentiment?api_key={key}&amp;string=I+love+this+product</a> you will see that the page can not be opened for an SSL error.</p>
<p>Please contact your API provider to resolve.</p>
<p>PS: remove the API Key from your initial question or anyone can use it.</p>
",0,0,62,2021-06-13 13:47:43,https://stackoverflow.com/questions/67958953/paysify-sentiment-api-returning-null
What are the cons and potenzial problems of using TextBlob to perform sentiment analysis? How could they be solved?,"<p>I know that TextBlob ignore the words that it doesn’t know, and it will consider words and phrases that it can assign polarity to and averages to get the final score.</p>
<p>Are there any other problems and defects that I don't know about?
Also, I would like to know how it is possible to fix them.</p>
<p>Considering that we can use TextBlob both with a dictionary and through machine learning, I think a solution could be to use a larger dictionary and improve the train set.
Are my intuitions right?</p>
","python, sentiment-analysis, textblob","<p>Most of the Challenges in NLP sentiment analysis tasks are semantic ones like Irony and sarcasm ambiguity in th text,Multipolarity...
Thay why TextBlob may not yield the best resulat depending on your text and if it contains multiples languges , you can add new models or languages through extensions .</p>
",0,0,592,2021-06-14 20:48:18,https://stackoverflow.com/questions/67977030/what-are-the-cons-and-potenzial-problems-of-using-textblob-to-perform-sentiment
Perform Text Sentiment analysis and Keyphrase extraction from excel and store in Azure Blob storage,"<p>I want to perform Sentiment analysis and keyphrase extraction on text data stored in an excel format. The sentiments and the extracted keyphrases also need to be appended to the same excel and the final excel needs to be stored in Azure blob storage. Finally this needs be made into a flask app. Would be grateful if anyone can help me on this. Thanks in advance..</p>
","python, flask, azure-blob-storage, sentiment-analysis","<p>Your question scope is too wide, so I write a simple demo for you.</p>
<p>Just try the code below to read data from .csv and use Sentiment analysis and then write back to .csv and upload to blob, the only thing you need is to integrate the code with your flask app:</p>
<pre><code>from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
from azure.storage import blob
from azure.storage.blob import BlobClient
import pandas as pd

region = ''
key = ''

excelFilePath = &quot;&lt;local file path&gt;/test.csv&quot;

storageConnStr = '&lt;storage conn str&gt;'
containerName = '&lt;container name&gt;'
destBlob = 'test-upload.csv'

csv = pd.read_csv(excelFilePath,'rb')
data =csv['text']
documents = data.array

blob = BlobClient.from_connection_string(storageConnStr,containerName,destBlob)
credential = AzureKeyCredential(key)
text_analytics_client = TextAnalyticsClient(endpoint=&quot;https://&quot;+ region +&quot;.api.cognitive.microsoft.com/&quot;, credential=credential)

response = text_analytics_client.analyze_sentiment(documents, language=&quot;en&quot;)

sentiments = [res.sentiment for res in response ]
csv.insert(1, &quot;sentiment&quot;, sentiments)
csv.to_csv(excelFilePath, index=False)

blob.upload_blob(open(excelFilePath,'rb').read())
</code></pre>
<p>Result:</p>
<p>My .csv:</p>
<p><a href=""https://i.sstatic.net/syUQv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/syUQv.png"" alt=""enter image description here"" /></a></p>
<p>After running :</p>
<p><a href=""https://i.sstatic.net/KopHy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KopHy.png"" alt=""enter image description here"" /></a></p>
<p>and it has been uploaded to storage :</p>
<p><a href=""https://i.sstatic.net/xtYj7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xtYj7.png"" alt=""enter image description here"" /></a></p>
",0,1,145,2021-06-16 15:21:45,https://stackoverflow.com/questions/68005614/perform-text-sentiment-analysis-and-keyphrase-extraction-from-excel-and-store-in
How does this split of train and evaluation data ensure there is no overlap?,"<p>I am reading this sentiment classification tutorial from Tensorflow:</p>
<p><a href=""https://www.tensorflow.org/tutorials/keras/text_classification"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/keras/text_classification</a></p>
<p>The way it splits data into train and evaluate is the following code:</p>
<pre><code>batch_size = 32
seed = 42

raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)

raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='validation', 
    seed=seed)
</code></pre>
<p>Shouldn't a single call of the function text_dataset_from_directory generate the two sets? If it is called twice, does it ensure there will be no overlap between the two split sets?</p>
","tensorflow, sentiment-analysis, training-data","<p>You need to either set a seed or set <code>shuffle = False</code> in order to make sure that you have no overlap in two sets. Here's what happens under the hood:</p>
<p>When subset (train-val) is provided, seed or shuffle args are checked (<a href=""https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/preprocessing/dataset_utils.py#L219"" rel=""nofollow noreferrer"">Source</a>)</p>
<pre><code>if validation_split and shuffle and seed is None:
        raise ValueError(
            'If using `validation_split` and shuffling the data, you must provide '
            'a `seed` argument, to make sure that there is no overlap between the '
            'training and validation subset.')
</code></pre>
<p>Then, the data is reserved. (<a href=""https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/preprocessing/dataset_utils.py#L166"" rel=""nofollow noreferrer"">Source</a>)</p>
<pre><code>num_val_samples = int(validation_split * len(samples))
if subset == 'training':
 print('Using %d files for training.' % (len(samples) - num_val_samples,))
 samples = samples[:-num_val_samples]
 labels = labels[:-num_val_samples]
elif subset == 'validation':
 print('Using %d files for validation.' % (num_val_samples,))
    samples = samples[-num_val_samples:]
    labels = labels[-num_val_samples:]
</code></pre>
<p>With the last code samples &amp; labels restricted to the training or validation set. And since you specified <strong>seed</strong>, datasets is <strong>randomized</strong> in the <strong>same</strong> order.</p>
",1,0,1047,2021-06-16 21:27:47,https://stackoverflow.com/questions/68010225/how-does-this-split-of-train-and-evaluation-data-ensure-there-is-no-overlap
How can I make a word cloud based on a condition in python?,"<p>i try to make a word cloud of the column &quot;Review Gast&quot; based on a condition that the column &quot;sentiment&quot; needs to be &quot;negative&quot;. However the wordclouds shows all Reviews and not only the negative ones. What am I doing wrong?</p>
<pre><code>
for i in reviews_english[reviews_english['sentiment']=='negative']['Review Gast'].astype(str).values:
    vectorizerneg = CountVectorizer(ngram_range=(3, 3), stop_words=STOPWORDS and ['width', 'px', 'jpg', 'cdn', 'src', 'https'])
bag_of_words = vectorizerneg.fit_transform(reviews_english['Review Gast'])
vectorizer.vocabulary_
sum_words = bag_of_words.sum(axis=0) 
words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
print (words_freq[:100])
#Generating wordcloud and saving as jpg image
words_dict = dict(words_freq)
WC_height = 1000
WC_width = 1500
WC_max_words = 200
wordCloud = WordCloud(width = 800, height = 800,background_color=&quot;white&quot;,min_font_size = 10, max_words=50)
wordCloud.generate_from_frequencies(words_dict)
plt.title('Most frequently occurring bigrams connected by same colour and font size')
plt.imshow(wordCloud, interpolation='bilinear')
plt.axis(&quot;off&quot;)
plt.show()
wordCloud.to_file('wordcloud_bigram.jpg')
</code></pre>
","python, for-loop, conditional-statements, sentiment-analysis, word-cloud","<p>You're running <code>fit_transform</code> on the entire column <code>reviews_english['Review Gast']</code>. Try</p>
<pre><code>vectorizerneg = CountVectorizer(ngram_range=(3, 3), stop_words=STOPWORDS and ['width', 'px', 'jpg', 'cdn', 'src', 'https'])
bag_of_words = vectorizerneg.fit_transform(reviews_english[reviews_english['sentiment']=='negative']['Review Gast'].to_list())
</code></pre>
<p>You can remove the for loop because it does nothing but reload CountVectorizer.</p>
",0,-2,455,2021-06-24 21:04:32,https://stackoverflow.com/questions/68122742/how-can-i-make-a-word-cloud-based-on-a-condition-in-python
How to take just the score from HuggingFace Pipeline Sentiment Analysis,"<p>I'm quite new to the whole HuggingFace pipeline world, and I have stumbled upon something which I can't figure out. I have googled quite a bit for an answer, but haven't found anything yet, so any help would be great.
I am trying to get just the score from the HF pipeline sentiment classifier, not the label, as I want to apply the scores to a dataframe containing many cells of text.
I know how to achieve this on just a single sentence, namely like so:</p>
<pre><code>from transformers import pipeline
classifier = pipeline(&quot;sentiment-analysis&quot;)

result = classifier(&quot;This is a positive sentence&quot;)[0]
(result['score'])
</code></pre>
<p>This gives me the following output:</p>
<blockquote>
<p>0.9994597434997559</p>
</blockquote>
<p>I know how to apply the classifier to my dataframe. However, when I adapt the code above to the dataframe, like so:</p>
<pre><code>result = df['text'].apply(lambda x: classifier(x[:512]))[0]
df['sentiment'] = result['score']
</code></pre>
<p>My code fails on the second line, with the following error:</p>
<pre><code>TypeError: list indices must be integers or slices, not str
</code></pre>
<p>Does anyone know how to fix this? I have tried a few things, but I haven't been able to figure it out so far. Any help would be immensely appreciated!</p>
","python, pandas, sentiment-analysis, huggingface-transformers","<p>If your classifier output looks like this:</p>
<pre><code>[{'label': '1', 'score': 0.9999555349349976}]
</code></pre>
<p>then you could extract the score with the following:</p>
<pre><code>result['sentiment'] = df['text'].apply(lambda x: classifier(x[:512]).apply(
  lambda x: classifier(x)).str[0].str['score']
</code></pre>
<hr />
<p><strong>Alternatively:</strong></p>
<p>Get the classifier output:</p>
<pre><code>df['result'] = df['text'].apply(lambda x: classifier(x[:512]))
</code></pre>
<p>Extract the score from the output:</p>
<pre><code>df['sentiment'] = df['result'].str[0].str['score']
</code></pre>
",1,3,1884,2021-06-30 15:28:26,https://stackoverflow.com/questions/68197664/how-to-take-just-the-score-from-huggingface-pipeline-sentiment-analysis
How to tune sentiment analysis when one algorithm says positive and one negative?,"<p>I'm currently working on a project and I use [Google CNL] <a href=""https://cloud.google.com/natural-language"" rel=""nofollow noreferrer"">1</a> and [PyFeel] <a href=""https://github.com/AdilZouitine/pyFeel"" rel=""nofollow noreferrer"">2</a> to perform a sentiment analysis of a dataset.</p>
<p>I have read a lot of articles and tips on how to tune the classifier, but I'm more looking for some ideas on: what to do when technology 1 says positive and technology 2 says negative?</p>
<p>What can I do in this situation?</p>
",sentiment-analysis,"<p>In the general machine learning case, this is what ensembles are all about: taking the output of multiple models and making a decision. So there is a huge amount of literature you can read there.</p>
<p>When you have two models the choices will come down to:</p>
<ul>
<li>Sum their confidences</li>
<li>Trust one more</li>
<li>Get a 3rd opinion as a tie-breaker.</li>
<li>Make no decision/escalate to a human</li>
</ul>
<p>If the two models are binary classifiers then you cannot use the first idea (which would say if model A says it is +0.7 positive sentiment, and model B says it is -0.4 negative sentiment, then the sum is +0.3, so it is positive).</p>
<p>The second idea, with only two binary classifier models, makes your less trusted model pointless. But it is useful if you have scores to work with. Extending the previous example, if you trust model B twice as much, +0.7 + (-0.4 * 2) = -0.1.</p>
<p>I.e. because you trust model B more, you allow its -0.4 to override model A's +0.7, and decide it is negative.</p>
",1,1,86,2021-07-05 11:18:21,https://stackoverflow.com/questions/68255234/how-to-tune-sentiment-analysis-when-one-algorithm-says-positive-and-one-negative
Importing SpacyTextBlob shows Attributeerror,"<p>I am trying to import the <strong>SpacyTextBlob</strong> to do some unsupervised sentiment analysis. I installed and load the package as:</p>
<pre><code>pip install spacytextblob --user
from spacytextblob.spacytextblob import SpacyTextBlob
</code></pre>
<p>But it shows <code>AttributeError: type object 'Language' has no attribute 'factory'</code>.</p>
<p>I have searched the issue and found <a href=""https://github.com/R1j1t/contextualSpellCheck/issues/53"" rel=""nofollow noreferrer"">this</a>. I tried to do</p>
<pre><code>!pip install contextualSpellCheck
</code></pre>
<p>But that installation also failed. I am using python 3.8 and spacy 2.3.7. Is it any comapaitibility issue or the command has been changed? Is there anyone else who faced the same and know the workaround?</p>
","python, spacy, attributeerror, sentiment-analysis","<p>I found the workaround. The issue is the compatibility of spacy version with the spacytextblob. I was using spacy 2.3.7, so I downgraded my spacytextblob to a lower version using</p>
<pre><code>pip install spacytextblob==0.1.7
</code></pre>
",1,1,561,2021-07-08 07:33:38,https://stackoverflow.com/questions/68297377/importing-spacytextblob-shows-attributeerror
I&#39;m trying to make Front-end for my &quot;Sentiment Analysis&quot; project but can&#39;t figure out how to make my &quot;prediction_function&quot; work at the Front-end,"<p>I'm making a Sentiment Analysis project which can be used for any language. Here's how it works: At the end part of the code <strong>&quot;result&quot;</strong> translates a sentence into English. Then <strong>predict_function(result.text)</strong> classifies the English text as positive, negative or neutral.</p>
<p>The code works fine if I run it separately. Now I'm trying to make Front-end and the only issue is I can't figure out how to link <strong>prediction_function</strong> with it. Translation function is working there but the only thing left is to classify that translated text on Front-end. I'm new to this and I did make many changes but couldn't get it worked.</p>
<p>This is my whole code: (I guess no need to look at the whole code because I feel the issue is at the end part, after @app.route('/', methods=['POST']) line)</p>
<pre><code>from flask import Flask, request, render_template
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
import pandas as pd
import numpy as np
import seaborn as sns
import regex as re
import math

import googletrans

from googletrans import Translator
from nltk.tokenize import word_tokenize

app = Flask(__name__)

@app.route('/')
def my_form():
    return render_template('form.html')

df = pd.read_csv('C:/Users/path/file.csv')
df = df.rename(columns = {'clean_text':'Comment'})
df.head()

df.describe()

cat = []
for val in df['category'].values:
  if val not in cat:
    cat.append(val)
print(cat)

index_arr = []
for index, val in df.iterrows():
    if val['category'] not in [-1.0, 0.0, 1.0]:
        index_arr.append(index)
print(index_arr)
df.drop(index_arr, axis = 0, inplace = True)

sns.countplot(x='category',data=df)

def clean_comments(comment):
    comment = re.sub(r'\$\w*', '', str(comment))
    comment = re.sub(r'^RT[\s]+', '', str(comment))
    comment = re.sub(r'https?:\/\/.*[\r\n]*', '', str(comment))
    comment = re.sub(r'#', '', str(comment))
    comment = re.sub(r&quot;@[^\s]+[\s]?&quot;,'',comment)
    comment = re.sub('[^ a-zA-Z0-9]', '', comment)
    comment = re.sub('[0-9]', '', comment)
    return comment

df['Comment'] = df['Comment'].apply(clean_comments)
df.head()

nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = stopwords.words('english')

def removing_stopwords(words):
  cleaned_tokens = []
  for val in words.split(' '):
    val = val.lower()
    if val not in stop_words and val != '':
      cleaned_tokens.append(val)
  return(cleaned_tokens)

df['Comment'] = df['Comment'].apply(removing_stopwords) 
df.head()

from nltk.stem.porter import PorterStemmer

def stem_comments(words):
  ps = PorterStemmer()
  stemmed_review = []
  for review in  words:
    stemmed_review.append(ps.stem(review))
  return stemmed_review

df['Comment'] = df['Comment'].apply(stem_comments)   
df.head()

temp = df.iloc[:,0].values
X = [' '.join(ele) for ele in temp]
X = np.array(X)
Y = df.iloc[:,1].values

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(X).toarray()
print(X.shape)

print(Y[:5])
print(Y.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.01)

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

del X
del Y
del temp
del df

from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
print(&quot;Accuracy = &quot;, accuracy_score(y_pred, y_test))

import seaborn as sn
from matplotlib.figure import Figure
df_cm = pd.DataFrame(cm, index = [0,1,2],columns = [0,1,2])
f = Figure(figsize = (20,10))
sn.heatmap(df_cm, annot=True)

def predict_function(sentence):
    sentence = clean_comments(sentence)
    sentence = removing_stopwords(sentence)
    sentence = stem_comments(sentence)
    
    X = [' '.join([str(elem) for elem in sentence])]
    X = np.array(X)
    X = vectorizer.transform(X).toarray()
    
    result = classifier.predict(X)

    if result == -1.0:
        print(&quot;Negative&quot;)
    elif result == 0.0:
        print(&quot;Neutral&quot;)
    else:
        print(&quot;Positive&quot;)

@app.route('/', methods=['POST'])
def my_form_post():
    text1 = request.form['text1'].lower()

    translator = Translator(service_urls=['translate.googleapis.com'])
    result = translator.translate(text1, dest='en')
    senti=predict_function(result.text)

    return render_template('form.html', final=result.text, last=senti, text1=text1)



if __name__ == &quot;__main__&quot;:
    app.run(debug=True, host=&quot;127.0.0.1&quot;, port=5002, threaded=True)
</code></pre>
<p>HTML code for Front-end:</p>


<pre><code>&lt;body&gt;
    &lt;h1&gt;Welcome To Sentiment Analyzer&lt;/h1&gt;
    &lt;form method=&quot;POST&quot;&gt;
        &lt;textarea name=&quot;text1&quot; placeholder=&quot;Say Something: ....&quot; rows=&quot;10&quot; cols=&quot;109&quot;&gt;&lt;/textarea&gt;&lt;br&gt;&lt;br&gt;

        &lt;input class=&quot;example_a&quot; type=&quot;submit&quot;&gt;
    &lt;/form&gt;
    {% if final %}
    &lt;div&gt; 
        &lt;h2&gt;The Sentiment of&lt;/h2&gt; '{{ text1 }}' &lt;h2&gt;is {{ final }} &lt;/h2&gt; &lt;h2&gt;is {{ last }} &lt;/h2&gt;
        {% else %}
        &lt;p&gt;&lt;/p&gt;
        {% endif %}
    &lt;/div&gt;
&lt;/body&gt;
</code></pre>


","python, html, machine-learning, frontend, sentiment-analysis","<p>In your predict_function function you're not returning any value just printing whether it is positive or not. Try replacing those print statements at the end with return statements.</p>
",1,0,424,2021-07-08 22:05:19,https://stackoverflow.com/questions/68309112/im-trying-to-make-front-end-for-my-sentiment-analysis-project-but-cant-figur
R tidytext sentiment analysis- how to use the drop parameter,"<p>I recently asked a question about entries that are omitted after a sentiment analysis. The tweets that I analyse don't always contain words that are in the lexicon. I would like to know which ones can't be translated. So I would like to keep these even if zero words were scored. In my previous question, the drop parameter was given as a solution. However, I think I might be doing it wrong or missing something. This is my first time working with these techniques.</p>
<p>The following function takes a data frame and gives a new one in return, containing the amount of positive and negative words along with the sentiment.</p>
<p><strong>The input</strong> (with one text in Dutch on purpose so it can't be scored)</p>
<pre><code>id &lt;- c(1, 2, 3)
date &lt;- c(&quot;12-05-2021&quot;, &quot;12-06-2021&quot;, &quot;12-07-2021&quot;)
text &lt;- c(&quot;Dit is tekst in het Nederlands&quot;, &quot;I,m so happy that websites like this exsist&quot;, &quot;This icecream tastes terrible. It made me upset&quot;)

df &lt;- data.frame(id, date, text)
</code></pre>
<p>What i want as output is:</p>
<pre><code>sentiment     positive     negative
0             0            0
2             2            0
-2            0            2
</code></pre>
<p>But my function gives me something else:</p>
<pre><code>sentimentAnalysis &lt;- function(tweetData){
  
  sentimentDataframe &lt;- data.frame()
  
  for(row in 1:nrow(tweetData)){
    
    tekst &lt;- as.character(tweetData[row, &quot;text&quot;])
    
    positive &lt;- 0
    negative &lt;- 0
    
    tokens &lt;- tibble(text = tekst) %&gt;% unnest_tokens(word, text, drop = FALSE)
    
    sentiment &lt;- tokens %&gt;%
      inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% 
      count(sentiment) %&gt;% 
      spread(sentiment, n, fill = 0) %&gt;% 
      mutate(sentiment = positive - negative)
    
    
    sentimentDataframe &lt;- bind_rows(sentimentDataframe, sentiment)
  }
  
  sentimentDataframe[is.na(sentimentDataframe)] &lt;- 0
  return(sentimentDataframe)
  
}
</code></pre>
<p>This still returns a data frame with the unscored texts missing. As you can see, the first text is omitted:</p>
<pre><code>sentiment     positive     negative
2             2            0
-2            0            2
</code></pre>
","r, sentiment-analysis, tidytext","<p>If there are no rows returned after the join you can return a tibble with all 0 values. We can use an <code>if</code> condition to check this.</p>
<p>In cases when there is only positive or negative sentiment in a sentence, <code>complete</code> would create another row with opposite sentiment and assign it the value 0. Also replaced <code>spread</code> with <code>pivot_wider</code> since <code>spread</code> is now superseded.</p>
<pre><code>library(tidyverse)
library(tidytext)

map_df(df$text, ~{
  tibble(text = .x) %&gt;% 
    unnest_tokens(word, text, drop = FALSE) %&gt;%
    inner_join(get_sentiments(&quot;bing&quot;)) -&gt; tmp
  if(nrow(tmp) == 0) tibble(sentiment = 0, positive = 0, negative = 0)
  else {
  tmp %&gt;%
    count(sentiment) %&gt;% 
    complete(sentiment = c('positive', 'negative'), fill = list(n = 0)) %&gt;%
    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% 
    mutate(sentiment = positive - negative)
  }
}) -&gt; res

res
#  sentiment positive negative
#      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
#1         0        0        0
#2         2        2        0
#3        -2        0        2
</code></pre>
",1,1,115,2021-07-11 17:07:44,https://stackoverflow.com/questions/68338367/r-tidytext-sentiment-analysis-how-to-use-the-drop-parameter
Examine data after building Machine Learning model in python,"<p>I built a sentiment analysis model in Arabic; in Python; after building the model, how can I test it with external data and how to build the code for that?</p>
<p>When I fitting the model, I extracted the features via tf-idf, and the problem I faced is dealing with it when I want to test external data after I trained the model on the training and test data.</p>
<p><strong>summary :
After I trained the model and reached an accuracy of 88%, I want to build a code that tests the model with external data..</strong></p>
<pre><code># train = 3461 record  
# test = 61 record \
# combi = train + test - to apply tf-idf

train = pd.read_excel('Final_train.xlsx')

test = pd.read_excel('Testing.xlsx' , usecols=['Tweet'])


# merge train &amp; test to apply all function on it 
def combine(tr,te):
   global combi 
   combi = tr.append(te , ignore_index=True)


# this is script to removing all stop words based on NLTK ( Natural Language ToolKit )

def remove_stop(combi1):
    combi1['Tweet'] = combi1['Tweet'].apply(lambda x: &quot; &quot;.join(x for x in x.split() if x not in stop))
    return combi1


#
# this is script to returns Arabic root for the given token Provided by University of Nevada, Las Vegas, USA.

def steeming(combi2):
    st = ISRIStemmer()
    combi2['Tweet'] = combi2['Tweet'].apply(lambda x: &quot; &quot;.join([st.stem(word) for word in x.split()])) 
    return combi2


# This is a List contain many word not Related to our domain we need to remove it from our Dataset
# to make ML Model Work properly &amp; Accurate __ I built it Manually in order to to develop accuracy of model 

def remove_unneded_word(combi3):
    unword =  pd.read_excel('Final_train &amp; MCSA/Un_neededword.xlsx')
    unword = unword.squeeze()
    unword = list(unword)
    combi3['Tweet'] = combi3['Tweet'].apply(lambda x: &quot; &quot;.join(x for x in x.split() if x not in unword))
    return combi3

# Calling the Each function Alone 
combine(train,test)
combi = remove_stop(combi)
combi = steeming(combi)
combi = remove_unneded_word(combi)



#                  ______________________________________________
#                 | Term Frequency–Inverse Document Frequency    |
#
#   To Represent Each word as matrix of numbers 

tfidf_vectorizer = TfidfVectorizer(max_df=0.8,min_df=5, max_features=1600)
# TF-IDF feature matrix
tfidf = tfidf_vectorizer.fit_transform(combi['Tweet'])




from sklearn import svm #Import scikit-learn  to apply support victor machine Algorithm 
from sklearn.model_selection import train_test_split
from sklearn import metrics


train_bow = tfidf[:3641,:]
test_bow = tfidf[3641:,:]

# splitting data into training and validation set
Tr_D_bow, Te_D_bow, Tr_L_bow , Te_L_bow = train_test_split(train_bow, train['Class'], random_state=45, test_size=0.2)
# Create SVM classifer object
SVM = svm.SVC()
# Train SVM Classifer
SVM = SVM.fit(Tr_D_bow,Tr_L_bow)

#Predict the response for test dataset
SVM_pre = SVM.predict(Te_D_bow)



print('The Accuracy of SVMC is --&gt;',metrics.accuracy_score(Te_L_bow, SVM_pre))
</code></pre>
","python, machine-learning, nlp, nltk, sentiment-analysis","<p>In order to test the model (use the model to predict on unseen data), you should use the <code>.predict</code> or <code>.transform</code> function (with sklearn).</p>
<p>In your code you separated the preprocessing functions and the model training functions which is good. But the fact that you combine the test and the train data is not good. The test data should be your &quot;external data&quot;.
Also you tread the TfIdf as a preprocessing step applied to both the test and train data, but the <code>tfidf_vectorizer.transform</code> should only be applied to the train data !
If you fit all your data in the TfIdf then you will not know how your model will behave when it haven't seen some words of the input.</p>
<p>With sklearn I usually organize my code as follow:</p>
<pre class=""lang-py prettyprint-override""><code># Read data
train = pd.read_excel('Final_train.xlsx')

# Split data in test, valid, train
x_train, x_valid, y_train, y_valid = train_test_split(
    train, train['Class'], random_state=45, test_size=0.2)

# Define preprocessing functions
def preprocess(data):
    data = remove_stop(data)
    data = stemming(data)
    data = remove_unneeded_word(data)
    return data

# Define sklearn pipeline
tfidf_vectorizer = TfidfVectorizer(
    max_df=0.8,min_df=5, max_features=1600)
classifier = svm.SVC()

# Train classifier
x_train_preproc = preprocess(train)
x_train_bow = tfidf_vectorizer.fit_transform(x_train_preproc)
classifier.fit(x_train_preproc, y_train)

# Test pipeline
x_valid_preproc = preprocess(valid)
x_valid_bow = tfidf_vectorizer.transform(x_valid_preproc)
pred_valid = classifier.predict(x_valid_bow)
print('The Accuracy of SVMC is --&gt;',
      metrics.accuracy_score(y_valid, pred_valid))

# Save model
# from https://medium.datadriveninvestor.com/machine-learning-how-to-save-and-load-scikit-learn-models-d7b99bc32c27
with open('trained_tfidf.pkl', 'wb') as f:
    pickle.dump(tfidf_vectorizer, f)
with open('trained_classifier.pkl', 'wb') as f:
    pickle.dump(classifier, f)

# Load model
with open('trained_tfidf.pkl', 'wb') as f:
    tfidf_vectorizer = pickle.load(f)
with open('trained_classifier.pkl', 'rb') as f:
    classifier = pickle.load(f)

# Predict on unseen data
# Note how the test data have not been loaded until now !
test = pd.read_excel('Testing.xlsx', usecols=['Tweet'])
x_test_preproc = preprocess(test)
x_test_bow = tfidf_vectorizer.transform(x_test_preproc)
pred_test = classifier.predict(x_test_bow)
print(pred_test)
</code></pre>
<p>The main goal is to be sure to use the test data only when everything has been trained to evaluate the generalization of the model to unseen data.
To make it easier you can combine the TfIdfVectorizer and the SVC into a <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html?highlight=pipeline#sklearn.pipeline.Pipeline"" rel=""nofollow noreferrer"">pipeline</a>.
Please note that the code I wrote wasn't tested it is here to show the general steps.</p>
",0,0,751,2021-07-12 08:23:25,https://stackoverflow.com/questions/68344135/examine-data-after-building-machine-learning-model-in-python
Tensorflow hub-NNLM word embedding using sentiment140 data gives input shape error,"<p>I am using tensorflow hub &quot;https://tfhub.dev/google/nnlm-en-dim128/2&quot; word embedding for the sentiment analysis of Kaggle &quot;sentiment140&quot; dataset.</p>
<p>Data set : Kaggle(&quot;sentiment140&quot;) <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">https://www.kaggle.com/kazanova/sentiment140</a>
Tensorflow-Hub : <a href=""https://tfhub.dev/google/nnlm-en-dim128/2"" rel=""nofollow noreferrer"">https://tfhub.dev/google/nnlm-en-dim128/2</a></p>
<p>Here i am using keras sequential layer when i fit the model it gives value error</p>
<pre><code>ValueError: Python inputs incompatible with input_signature:
      inputs: (
        Tensor(&quot;IteratorGetNext:0&quot;, shape=(None, 128), dtype=float32))
      input_signature: (
        TensorSpec(shape=(None,), dtype=tf.string, name=None))
</code></pre>
<p>My code:</p>
<pre><code>    import pandas as pd
import tensorflow as tf
from sklearn.model_selection import  train_test_split
import seaborn as sns
import tensorflow_hub as hub
from tensorflow.keras import Sequential
import keras

tweet_df = pd.read_csv(&quot;training.1600000.processed.noemoticon.csv&quot;, names=['polarity', 'id', 'date', 'query', 'user', 'text'],encoding='latin-1')

tweet_df.info()

tweet_df.head()

&quot;&quot;&quot;#### 2.) Data Visualization&quot;&quot;&quot;

tweet_df['polarity'] = tweet_df['polarity'].replace(to_replace=4,value=1)

### Print two movies reviews from each class

print(&quot;Movie Review Polarity Negative class 0 :\n&quot;, tweet_df[tweet_df['polarity']==0]['text'].head(2) )

print(&quot;\n\nMovie Review Polarity Positive class 1 :\n&quot;, tweet_df['text'][tweet_df['polarity']==1].head(2) )

class_dist = tweet_df['polarity'].value_counts().rename_axis('Class Label').reset_index(name='Tweets')
#class_dist = class_dist['Class Label'].replace({0:'Negative',1:'Positve'})
class_dist

## Bar graph of Distribution of Classes
class_dist['class'] = ['Positive','Negative']
sns.set_theme(style='whitegrid')
sns.barplot(x='Class Label', y='Tweets', hue='class', data= class_dist)

### Train and test split 
X = tweet_df.iloc[:,5]
y = tweet_df.iloc[:,0]
X_train, X_test,y_train, y_test = train_test_split(X,y,random_state=5, test_size=0.2)

print(&quot;Training shape of X and y : &quot;, X_train.shape ,y_train.shape)
print(&quot;Testing shape of X and y : &quot;, X_test.shape ,y_test.shape)

&quot;&quot;&quot;#### 3.) Data Pre-processing&quot;&quot;&quot;

embed = hub.load(&quot;https://tfhub.dev/google/nnlm-en-dim128/2&quot;)
X_train_embed = embed(X_train)

y_train = tf.keras.utils.to_categorical(y_train,2)

X_train_embed.shape


X_sample = X_train_embed[:1000]
y_sample = y_train[:1000]
y_sample = tf.keras.utils.to_categorical(y_sample,2)


&quot;&quot;&quot;#### 4.) Model Building&quot;&quot;&quot;

hub_layer = hub.KerasLayer('https://tfhub.dev/google/nnlm-en-dim128/2',input_shape=[],dtype=tf.string,trainable=False)

model = Sequential()
model.add(hub_layer)
model.add(keras.layers.Dense(128, 'relu', name ='layer_1'))
model.add(keras.layers.Dense(64, 'relu', name = 'layer_2'))
model.add(keras.layers.Dense(2, activation='sigmoid', name='output'))

model.compile(optimizer='adam',loss= 'BinaryCrossentropy',  #'categorical_crossentropy' ,
              metrics=['accuracy'] )

NN_model = model.fit(X_sample, y_sample, epochs=20, validation_split=0.1, verbose=1)
</code></pre>
<p>Input shape:</p>
<pre><code>X_sample.shape
</code></pre>
<p>TensorShape([1000, 128])</p>
<pre><code>y_sample.shape
</code></pre>
<p>(1000, 2, 2)</p>
<pre><code>X_sample

&lt;tf.Tensor: shape=(1000, 128), dtype=float32, numpy=
array([[ 0.10381411,  0.07044576, -0.0282673 , ...,  0.08205549,
0.15822364, -0.10019408],
[-0.03332436, -0.00529242,  0.20348714, ..., -0.14174528,
0.05178985, -0.12599435],
[ 0.2461916 , -0.03084931,  0.05861813, ...,  0.07956063,
-0.03579932,  0.07493019],
[ 0.4102695 ,  0.15445013,  0.19045362, ...,  0.12681636,
0.12362286, -0.03969387],
[-0.0144283 , -0.05236297,  0.04851832, ...,  0.05562773,
0.01529189,  0.12605236],
[ 0.29280087,  0.05795274, -0.11779188, ..., -0.01890504,
0.02824693, -0.13629636]], dtype=float32)&gt;
</code></pre>
","keras, sentiment-analysis, word-embedding, tensorflow-hub, language-model","<p>As described on <a href=""https://tfhub.dev/google/nnlm-en-dim128/2"" rel=""nofollow noreferrer"">https://tfhub.dev/google/nnlm-en-dim128/2</a>, the model expects a vector of strings as input. You're basically calling the model twice since you're executing</p>
<pre><code>embed = hub.load(&quot;https://tfhub.dev/google/nnlm-en-dim128/2&quot;)
X_train_embed = embed(X_train)  # (n, 128) float matrix
</code></pre>
<p>and then passing that embedding to <code>model</code>, which actually takes strings as input since it starts with the NNLM KerasLayer.</p>
<p>I'd propose to remove <code>embed</code> and <code>X_train_embed</code> and just call <code>model.fit</code> with <code>X_train</code>:</p>
<pre><code>model.fit(np.array([&quot;Lyx is cool&quot;, &quot;Lyx is not cool&quot;]), np.array([1, 0]), epochs=20, validation_split=0.1, verbose=1)
</code></pre>
",1,0,409,2021-07-13 13:41:57,https://stackoverflow.com/questions/68363587/tensorflow-hub-nnlm-word-embedding-using-sentiment140-data-gives-input-shape-err
CUDA error: CUBLAS_STATUS_INVALID_VALUE error when training BERT model using HuggingFace,"<p>I am working on sentiment analysis on steam reviews dataset using BERT model where I have 2 labels: positive and negative. I have fine-tuned the model with 2 Linear layers and the code for that is as below.</p>
<pre><code> bert = BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;,
                                                 num_labels = len(label_dict),
                                                 output_attentions = False,
                                                 output_hidden_states = False)

 class bertModel(nn.Module):
   def __init__(self, bert):
     super(bertModel, self).__init__()
     self.bert = bert
     self.dropout1 = nn.Dropout(0.1)
     self.relu =  nn.ReLU()
     self.fc1 = nn.Linear(768, 512)
     self.fc2 = nn.Linear(512, 2)
     self.softmax = nn.LogSoftmax(dim = 1)

  def forward(self, **inputs):
     _, x = self.bert(**inputs)
    x = self.fc1(x)
    x = self.relu(x)
    x = self.dropout1(x)
    x = self.fc2(x)
    x = self.softmax(x)

  return x
</code></pre>
<p>This is my train function:</p>
<pre><code>def model_train(model, device, criterion, scheduler, optimizer, n_epochs):
  train_loss = []
  model.train()
 for epoch in range(1, epochs+1):
   total_train_loss, training_loss = 0,0 
  for idx, batch in enumerate(dataloader_train):
     model.zero_grad()
     data = tuple(b.to(device) for b in batch)
     inputs = {'input_ids':      data[0],'attention_mask': data[1],'labels':data[2]}
     outputs = model(**inputs)
     loss = criterion(outputs, labels)
     loss.backward()
     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
     #update the weights
     optimizer.step()
     scheduler.step()
     training_loss += loss.item()
     total_train_loss += training_loss
     if idx % 25 == 0:
        print('Epoch: {}, Batch: {}, Training Loss: {}'.format(epoch, idx, training_loss/10))
        training_loss = 0      
  #avg training loss
  avg_train_loss = total_train_loss/len(dataloader_train)
  #validation data loss
  avg_pred_loss = model_evaluate(dataloader_val)
  #print for every end of epoch
  print('End of Epoch {}, Avg. Training Loss: {}, Avg. validation Loss: {} \n'.format(epoch, avg_train_loss, avg_pred_loss))
</code></pre>
<p>I am running this code on Google Colab. When I run the train function, I get the following the error, I have tried with batch sizes 32, 256, 512.</p>
<pre><code>RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &amp;alpha, a, lda, b, ldb, &amp;beta, c, ldc)`
</code></pre>
<p>Can anyone please help me on this? Thank you.</p>
<p><strong>Update on the code:</strong> I tried running the code on the CPU and the error is in the matrix shapes mismatch. The input shape, shape after the self.bert is printed in the image. Since the first linear layer (fc1) is not getting executed, the shape after that is not printed.</p>
<p><a href=""https://i.sstatic.net/vhqjC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vhqjC.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/h7sgA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/h7sgA.png"" alt=""enter image description here"" /></a></p>
","python, pytorch, sentiment-analysis, bert-language-model","<p>I suggest trying out couple of things that can possibly solve the error.</p>
<p>As shown in this <a href=""https://discuss.pytorch.org/t/runtimeerror-cuda-error-cublas-status-invalid-value-when-calling-cublassgemm-handle-opa-opb-m-n-k-alpha-a-lda-b-ldb-beta-c-ldc/124544"" rel=""nofollow noreferrer"">forum</a>, one possible solution is to lower the batch size of how you load data. Since it might be a memory error.</p>
<p>If that does not work then I suggest as shown in this github <a href=""https://github.com/pytorch/pytorch/issues/56747"" rel=""nofollow noreferrer"">issue</a> to update to a new version of Pytorch cuda that fixes a matrix multiplication bug that releases this same error that your code could be doing. Hence, as shown in this <a href=""https://discuss.pytorch.org/t/cublas-status-execution-failed-when-calling-cublassgemm-handle-opa-opb-m-n-k-alpha-a-lda-b-ldb-beta-c-ldc/116740/9"" rel=""nofollow noreferrer"">forum</a> You can update Pytorch to the nightly pip wheel, or use the CUDA10.2 or conda binaries. You can find information on such installations on the pytorch home page where it mentions how to install pytorch.</p>
<p>If none of that works, then the best thing to do is to run a smaller version of the process on CPU and recreate the error. When running it on CPU instead of CUDA, you will get a more useful traceback that can solve your error.</p>
<h1 id=""edit-based-on-comments-wplj"">EDIT (Based on Comments):</h1>
<p>You have a matrix error in your model.
The problem stems in your forward func then</p>
<p>The model BERT outputs a tensor that has torch.size (64, 2) which means if you put it in the Linear layer you have it will error since that linear layer requires input of (?, 768) b/c you initialized it as <code>nn.Linear(768, 512)</code>. In order to make the error disappear you need to either do some transformation on the tensor or initialize another linear layer as shown below:</p>
<pre><code>somewhere defined in __init__: self.fc0 = nn.Linear(2, 768)
def forward(self, **inputs):
     _, x = self.bert(**inputs)
     
    x = self.fc0(x)
    x = self.fc1(x)
    x = self.relu(x)
    x = self.dropout1(x)
    x = self.fc2(x)
    x = self.softmax(x)

</code></pre>
<p>Sarthak Jain</p>
",2,2,6643,2021-07-14 18:47:33,https://stackoverflow.com/questions/68383634/cuda-error-cublas-status-invalid-value-error-when-training-bert-model-using-hug
Trying to analyze text and sentiments,"<p>I am trying to analyze text and sentiment data, but I don't want an extensive analysis. I just want a basic distribution of good, neutral and bad, and the percentage of each category.</p>
<p>can anyone direct me with some advice or suggestions?</p>
<p>thank you all!!</p>
","nlp, sentiment-analysis","<p><a href=""https://github.com/winkjs/wink-nlp"" rel=""nofollow noreferrer"">WinkNLP</a> can measure sentiment on a scale of -1 to +1 for the entire document or its sentences. Here is an <a href=""https://observablehq.com/@winkjs/how-to-perform-sentiment-analysis?collection=@winkjs/winknlp-recipes"" rel=""nofollow noreferrer"">Observable notebook</a> with a live example.</p>
",0,0,40,2021-07-29 20:07:38,https://stackoverflow.com/questions/68582263/trying-to-analyze-text-and-sentiments
Removing words from lemmatisation dictionary/updating lemma dictionary in textstem,"<p>I am using the textstem package to lemmatise words in some responses. However there is one word (spotting) which I do not wan't to be included, and reduced to &quot;spot&quot;. I want it to remain as spotting. How might I be able to do this? Do I need to make a custom dictionary? Currently doing:</p>
<pre><code>lemmatize_strings(df, dictionary = lexicon::hash_lemmas)
</code></pre>
","r, nlp, sentiment-analysis, stemming, lemmatization","<p>You can create your own dictionary where you remove the token <code>spotting</code></p>
<pre><code># hash_lemmas is a datatable, so you can use column name token instead hash_lemmas$token
my_lex &lt;- lexicon::hash_lemmas[!token == &quot;spotting&quot;, ]

df_lemmatized &lt;- lemmatize_strings(df, dictionary = my_lex)
</code></pre>
<p>Or if you want to do it without creating your own lexicon:</p>
<pre><code>df_lemmatized &lt;- lemmatize_strings(df, dictionary = lexicon::hash_lemmas[!token == &quot;spotting&quot;, ])
</code></pre>
",1,0,135,2021-08-04 15:59:30,https://stackoverflow.com/questions/68654498/removing-words-from-lemmatisation-dictionary-updating-lemma-dictionary-in-textst
Why is my sentiment analysis running so slow?,"<p>I'm trying to make a GUI app where you enter the twitter hashtag of two different things and it compares them using sentiment analysis (I'm using movies right now as an example). My code isn't finished yet as I only have one hashtag showing so far. The end result is supposed to be a graph that shows the polarity of tweets (so far, it only shows polarity of one movie). While running my code works and will pop up a graph, it takes FOREVER most of the time. Sometimes it will load up quick like I expect, but any other time it takes so long I get impatient and re run the program. Is the way the code arranged/the modules being used causing this? Or is sentiment analysis generally very slow? This is my first sentiment analysis project so I'm not really sure. Here is my code, I've taken out the twitter keys and tokens as I'm not sure I can leave those there:</p>
<pre><code>import tweepy as tw
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

consumer_key = ''
consumer_secret = ''
access_token = ''
access_token_secret = ''

# authenticate twitter
auth = tw.OAuthHandler(consumer_key,consumer_secret)
auth.set_access_token(access_token,access_token_secret)
api = tw.API(auth,wait_on_rate_limit= True)

# GET TWEETS HERE

hashtag = (&quot;#GreenKnight&quot;,)
query = tw.Cursor(api.search, q = hashtag).items(1000)
tweets = [{'Tweets':tweet.text, 'Timestamp':tweet.created_at}for tweet in query]
# put tweets in pandas dataframe
df = pd.DataFrame.from_dict(tweets)
df.head()

# green knight movie references
green_knight_references = [&quot;GreenKnight&quot;, &quot;Green Knight&quot;, &quot;green knight&quot;, &quot;greenknight&quot;, &quot;'The Green Knight'&quot;]
def identify_subject(tweet,refs):
    flag = 0
    for ref in refs:
        if tweet.find(ref) != - 1:
            flag = 1
        return flag

df['Green Knight'] = df['Tweets'].apply(lambda x: identify_subject(x, green_knight_references))

df.head(10)

# time for stop words, to clear out the language not needed
import nltk
from nltk.corpus import stopwords
from textblob import Word, TextBlob
stop_words = stopwords.words(&quot;english&quot;)
custom_stopwords = ['RT']

def preprocess_tweets(tweet,custom_stopwords):
    preprocessed_tweet = tweet
    preprocessed_tweet.replace('{^\w\s}',&quot;&quot;)
    preprocessed_tweet = &quot; &quot;.join(word for word in preprocessed_tweet.split() if word not in stop_words)
    preprocessed_tweet = &quot; &quot;.join(word for word in preprocessed_tweet.split() if word not in custom_stopwords)
    preprocessed_tweet = &quot; &quot;.join(Word(word).lemmatize() for word in preprocessed_tweet.split())
    return (preprocessed_tweet)


df['Processed Tweet'] = df['Tweets'].apply(lambda x: preprocess_tweets(x, custom_stopwords))
df.head()

#visualize

df['polarity'] = df['Processed Tweet'].apply(lambda x: TextBlob(x).sentiment[0])
df['subjectivity'] = df['Processed Tweet'].apply(lambda x: TextBlob(x).sentiment[1])
df.head()
(df[df['Green Knight']==1][['Green Knight','polarity','subjectivity']].groupby('Green Knight').agg([np.mean, np.max, np.min, np.median]))


green_knight = df[df['Green Knight']==1][['Timestamp', 'polarity']]
green_knight = green_knight.sort_values(by='Timestamp', ascending=True)
green_knight['MA Polarity'] = green_knight.polarity.rolling(10, min_periods=3).mean()

green_knight.head()

fig, axes = plt.subplots(2, 1, figsize=(13, 10))

axes[0].plot(green_knight['Timestamp'], green_knight['MA Polarity'])
axes[0].set_title(&quot;\n&quot;.join([&quot;Green Knight Tweets&quot;]))


fig.suptitle(&quot;\n&quot;.join([&quot;Movie tweet polarity&quot;]), y=0.98)

plt.show()
</code></pre>
","python, pandas, twitter, sentiment-analysis","<p>I've worked with <code>tweepy</code> before and the single-most slow thing was Twitter's API. It gets exhausted extremely quickly and without paying them, it's going to be frustrating :(  . <br />
The sentiment analysis using <code>TextBlob</code> shouldn't be slow.
However, your best bet is to use the <code>cProfile</code> option as @osint_alex mentioned in the comment, or for a simple solution just put some print statements in between the main 'blocks' of code.</p>
",1,0,577,2021-08-05 00:09:06,https://stackoverflow.com/questions/68659221/why-is-my-sentiment-analysis-running-so-slow
how to fix the error ValueError: could not convert string to float in a NLP project in python?,"<p>I am writing a python code using <strong>jupyter notebook</strong> that train and test  a dataset in order to return a correct sentiment.</p>
<p>The problem that when i try to predict the sentiment of the phrase the system crash and display the below  error :</p>
<blockquote>
<p>ValueError: could not convert string to float: 'this book was so
interstening it made me not happy'</p>
</blockquote>
<p>Note i have an imbalanced dataset so i use  <strong>SMOTE</strong> in order to over_sampling the dataset</p>
<h1>code:</h1>
<pre><code>import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE# for inbalance dataset
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
from sklearn.pipeline import Pipeline

df = pd.read_csv(&quot;data/Apple-Twitter-Sentiment-DFE.csv&quot;,encoding=&quot;ISO-8859-1&quot;)

df
# data is cleaned using preprocessing functions

# Solving inbalanced dataset using SMOTE 

vectorizer = TfidfVectorizer()
vect_df =vectorizer.fit_transform(df[&quot;clean_text&quot;])
oversample = SMOTE(random_state = 42)
x_smote,y_smote = oversample.fit_resample(vect_df, df[&quot;sentiment&quot;])
print(&quot;shape x before SMOTE: {}&quot;.format(vect_df.shape))
print(&quot;shape x after SMOTE: {}&quot;.format(x_smote.shape))
print(&quot;balance of targets feild %&quot;)
y_smote.value_counts(normalize = True)*100


# split the dataset into train and test 
x_train,x_test,y_train,y_test = train_test_split(x_smote,y_smote,test_size = 0.2,random_state =42)


logreg = Pipeline([
                ('tfidf', TfidfTransformer()),
                ('clf', LogisticRegression(n_jobs=1, C=1e5)),
               ])
logreg.fit(x_train, y_train)

y_pred = logreg.predict(x_test)

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred))

# Make prediction 
exl = &quot;this book was so interstening it made me not happy&quot;

logreg.predict(exl)
</code></pre>
","python, pandas, nlp, logistic-regression, sentiment-analysis","<p>You should define your variable <code>exl</code> as the following:</p>
<pre class=""lang-py prettyprint-override""><code>exl = vectorizer.transform([&quot;this book was so interstening it made me not happy&quot;])
</code></pre>
<p>and then do the prediction.</p>
<p>First, put the testing data in a list and then use <code>vectorizer</code> to use features extracted from your training data to do the prediction.</p>
",0,1,768,2021-08-24 09:48:43,https://stackoverflow.com/questions/68905349/how-to-fix-the-error-valueerror-could-not-convert-string-to-float-in-a-nlp-proj
OOM error when training the BERT Keras model,"<p>I am working on training the fine-tuned BERT model using Keras. However when I start the training on GPU, I am facing the OOM error. The below is the code of my model.</p>
<pre><code>  max_len = 256
  input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
  input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_mask&quot;)
  segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=&quot;segment_ids&quot;)
  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) 
  x = Dense(128, activation = &quot;relu&quot;)(sequence_output)
  x = Dropout(0.1)(x)
  out = Dense(1, activation = &quot;sigmoid&quot;)(x)

  model = Model(inputs = [input_word_ids, input_mask, segment_ids], outputs = out)
  model.compile(Adam(lr = 2e-6), loss = 'binary_crossentropy', metrics = ['accuracy'])
  model.summary()

  train_history = model.fit(train_input, train_labels, 
                      validation_split = 0.2,
                      epochs = 3,
                      batch_size = 16)
</code></pre>
<p>The error I get is</p>
<pre><code>   ---------------------------------------------------------------------------
   ResourceExhaustedError                    Traceback (most recent call last)
   &lt;timed exec&gt; in &lt;module&gt;

   /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py  in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split,     validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch,  validation_steps, validation_batch_size, validation_freq, max_queue_size, workers,  use_multiprocessing)
    1098                 _r=1):
    1099               callbacks.on_train_batch_begin(step)
 -&gt; 1100               tmp_logs = self.train_function(iterator)
    1101               if data_handler.should_sync:
    1102                 context.async_wait()

   /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in  __call__(self, *args, **kwds)
   826     tracing_count = self.experimental_get_tracing_count()
   827     with trace.Trace(self._name) as tm:
--&gt; 828       result = self._call(*args, **kwds)
   829       compiler = &quot;xla&quot; if self._experimental_compile else &quot;nonXla&quot;
   830       new_tracing_count = self.experimental_get_tracing_count()

   /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
   886         # Lifting succeeded, so variables are initialized and we can run the
   887         # stateless function.
--&gt; 888         return self._stateless_fn(*args, **kwds)
   889     else:
   890       _, _, _, filtered_flat_args = \

  /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
  2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)
  2942     return graph_function._call_flat(
-&gt; 2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  #  pylint: disable=protected-access
  2944 
  2945   @property

  /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
  1917       # No tape is watching; skip to running the function.
  1918       return self._build_call_outputs(self._inference_function.call(
-&gt; 1919           ctx, args, cancellation_manager=cancellation_manager))
  1920     forward_backward = self._select_forward_and_backward_functions(
  1921         args,

 /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
  558               inputs=args,
  559               attrs=attrs,
--&gt; 560               ctx=ctx)
  561         else:
  562           outputs = execute.execute_with_cancellation(

 /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
 58     ctx.ensure_initialized()
 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
 ---&gt; 60                                         inputs, attrs, num_outputs)
 61   except core._NotOkStatusException as e:
 62     if name is not None:

 ResourceExhaustedError:  OOM when allocating tensor with shape[16,160,1024] and type  float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
 [[{{node  model_1/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_model/StatefulPartitionedCall/encoder/layer_23/output_layer_norm/moments/SquaredDifference}}]]
 Hint: If you want to see a list of allocated tensors when OOM happens, add  report_tensor_allocations_upon_oom to RunOptions for current allocation info.
 [Op:__inference_train_function_105694]

 Function call stack:
 train_function
</code></pre>
<p>Also, when I run the code with only the last dense layer, then I don't seem to have the OOM error. It happens only when I add the Dense(units = 128) and Dropout(0.1) layers.</p>
<p>Can anyone please help me on this?
Thank you in advance.</p>
","python, keras, nlp, sentiment-analysis, bert-language-model","<p>A quick google search led me to <a href=""https://datascience.stackexchange.com/a/76871"">this discussion</a> where he states the splitting of the data with <code>validation_split</code> parameter can lead to this OOM Error and the resolution was to split the data before calling <code>model.fit()</code> by using <code>sklearn.preprocessing.train_test_split()</code> or any other way you prefer.</p>
",1,0,242,2021-09-15 21:25:43,https://stackoverflow.com/questions/69199961/oom-error-when-training-the-bert-keras-model
How to put spaces in between every emojis,"<p>I have a dataset of tweets where it contains at least one occurrence of emoji. But sometimes there are more. Emojis can be in the middle of the sentence, or it could be at the start or at the end. Hence for each tweet the case is different. I am having difficulties trying to split only the emojis in the sentence. If I loop through each word, the multiple emojis are also considered as one word.</p>
<pre><code>She is too hot for Congress.  Vote her out!  #sarcasm 😎😁😎😁😎😁
</code></pre>
<p>Expected output: <code>She is too hot for Congress.  Vote her out!  #sarcasm 😎 😁 😎 😁 😎 😁</code></p>
<pre><code>The Struggle is Real 😂😂😂 #struggle #struggleisreal #struggles #funny #humor #saying #sarcasm #lifestruggles #sarcastic #funnysaying #sayings #thestruggleisreal 
</code></pre>
<p>Expected output: <code>The Struggle is Real 😂 😂 😂 #struggle #struggleisreal #struggles #funny #humor #saying #sarcasm #lifestruggles #sarcastic #funnysaying #sayings #thestruggleisreal</code></p>
<pre><code>😂😂😂  For More Funny Post Follow
</code></pre>
<p>Expected output: <code>😂 😂 😂  For More Funny Post Follow</code></p>
<p><a href=""https://stackoverflow.com/questions/47375680/counter-for-words-and-emoji"">Counter for words and emoji</a></p>
<p>Answer from the above post gives me a list and toknized words for each tweet in the dataset which I don't want, it also does not solve my problem. I do not get space between the emojis.</p>
","python, nlp, sentiment-analysis","<p>Using emoji library <code>'v1.5.0'</code> it's an easy job.</p>
<pre><code>import emoji

def extract_emojis(s):
    return ''.join((' '+c+' ') if c in emoji.UNICODE_EMOJI['en'] else c for c in s)
</code></pre>
<p>test:</p>
<pre><code>s = &quot;🤔🙈 me así, se🤔🤔🤔😌ds 💕👭👙 hello 👩🏾‍🎓 emoji hello 👨‍👩‍👦 how are 😊 you today🙅🏽🙅🏽&quot;

extract_emojis(s)
</code></pre>
<p>output:</p>
<pre><code>' 🤔  🙈  me así, se 🤔  🤔  🤔  😌 ds  💕  👭  👙  hello  👩  🏾 \u200d 🎓  emoji hello  👨 \u200d 👩 \u200d 👦  how are  😊  you today 🙅  🏽  🙅  🏽 '
</code></pre>
",2,0,556,2021-10-03 09:36:31,https://stackoverflow.com/questions/69423621/how-to-put-spaces-in-between-every-emojis
Why is my new list outputting an empty one when I should be getting an answer,"<p>I'm having problems with my code where I'm trying to add to a new list all words that match a given list (in order at the end to have a basic sentiment analysis code). I have tried passing it through Python Tutor but have had no success. In short, I'm expecting an output of the word <strong>great</strong> but instead get an <strong>empty list</strong>.</p>
<pre class=""lang-py prettyprint-override""><code>def remove_punc_and_split (wordlist) : 
    punc = '.!?/:;,{}'  #the punctuation to be removed
    no_punc = &quot;&quot; 
    for char in wordlist :
        if char not in punc :
            no_punc = no_punc + char.lower()
    new_word_list = no_punc.split() #dividing the words into seperate strings in a list
    return new_word_list

def sentiment1 (wordlist1):
    positive_words = [&quot;good&quot;,&quot;awesome&quot;,&quot;excellent&quot;,&quot;great&quot;]
    wordlist1 = remove_punc_and_split (comment_1)
    pos_word_list = []                                                               
    for postive_words in wordlist1 :
        if wordlist1[0] == positive_words :
            pos_word_list.append(wordlist1)
            print(wordlist1)
    return pos_word_list
    
comment_1 = 'Good for the price, but poor Bluetooth connections.'
sentiment1(comment_1)
</code></pre>
","python, list, sentiment-analysis","<p>One approach can be to use <a href=""https://www.programiz.com/python-programming/methods/set/intersection"" rel=""nofollow noreferrer""><code>set.intersection</code></a> method for this to find common values between <code>positive_words</code> and the input list. I also updated it to pass the lowercased sentence <code>wordlist1.lower()</code> to the remove punc function, because you want to do a case-insensitive match against <code>positive_words</code>; so first you will need to lowercase all characters in the sentence before you can check if there are any positive words.</p>
<pre class=""lang-py prettyprint-override""><code>def sentiment1(wordlist1):
    positive_words = {&quot;good&quot;, &quot;awesome&quot;, &quot;excellent&quot;, &quot;great&quot;}
    wordlist1 = remove_punc_and_split(wordlist1.lower())
    pos_word_list = list(positive_words.intersection(wordlist1))
    return pos_word_list
</code></pre>
",1,1,33,2021-10-07 14:30:33,https://stackoverflow.com/questions/69482974/why-is-my-new-list-outputting-an-empty-one-when-i-should-be-getting-an-answer
"About LSTM structure for classification ( in this case, it&#39;s Sentiment Analysis)","<p>I'm trying to understand how LSTM can solve a Sentiment Analysis problem. Currently, I'm confused by this structure: <img src=""https://i.sstatic.net/9WONf.png"" alt=""LSTM unrolled architecture"" /></p>
<p>So here is my list of questions:</p>
<ul>
<li>After you feed the LSTM layer with vectors which represent each word of each sentence of the dataset, what does the LSTM layer do with them and what is the output?</li>
<li>What does the forget gate do? For example, there are 50 vectors to represent the word &quot;good&quot;, so it just forgets unnecessary vectors or something?</li>
<li>Why there are two LSTM layers?</li>
</ul>
","python, lstm, sentiment-analysis","<blockquote>
<p>After you fed LSTM layer with vectors which represented each words of each sentences of the dataset, what does LSTM layer do with them and what is the output?</p>
</blockquote>
<p>The final output of the architecture you show (up to the point it is fed into the softmax) is a sentence embedding, i.e. a single (high-dimensional) floating-point vector that represents the sentence. (You showed the unrolled version, but I find it useful to be simultaneously thinking of its real rolled up form, too.)</p>
<p>As far as this question goes, it is doing the same thing as a simple RNN, so make sure you understand that first. There are a lot of articles and videos explaining it. E.g. <a href=""https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45"" rel=""nofollow noreferrer"">https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45</a></p>
<blockquote>
<p>What does the forget gate do?</p>
</blockquote>
<p>It tries to learn which are the more important words. If being trained for sentiment analysis it will hopefully learn to give more weight to words that are emotive, and less weight to the words carrying no sentiment. You might also see it called a keep gate. When the weight is high it keeps the word, when the weight is low it forgets the word.</p>
<p>Say you had a long sentence like: &quot;This movie has some really funny characters and superb action scenes, and was directed by Tom Smith, produced by Dick Jones, and began production in 2019.&quot;  The problem here for an RNN doing sentiment analysis is that it maintains state one word at a time. Each step it loses a bit of what went before. By the end it has all but forgotten the positive early words.</p>
<p>You really want it seen as: &quot;really funny characters, superb action scenes.&quot;. That is a much shorter sentence to learn and understand. So this is what the forget gate in an LSTM is trying to do for you.</p>
<p>Spoiler alert: It does not do it that well, but it is distinctly better than not doing it at all.</p>
<blockquote>
<p>Why there are two LSTM layers?</p>
</blockquote>
<p>To allow more sophisticated understanding and better sentence embeddings. It is the same as adding another layer in a fully-connected neural network. LSTMs don't scale very well beyond two layers however.</p>
<blockquote>
<p>how the model know that sentence's sentiment base on that vector exactly?
Does that vector have some kind of features that make it positive or negative</p>
</blockquote>
<p>The answer to your second sub-question is &quot;maybe, but not always in a human-readable form&quot;. What the very last layer of your architecture is doing is trying to learn what kind of sentiment each element of the vector indicates, using a classic fully-connected neural net.</p>
<p>E.g. say you use vectors of dimension 100, and you want to classify into three classes: angry, happy, sad. Your training data then has sentences like &quot;Everything went smoothly today&quot; with a label of &quot;happy&quot;. Maybe that sentence has a high number in element 77 of the vector the LSTM produces. Maybe most of the &quot;happy&quot; sentences have a high value in that element, but the angry and sad sentences show no pattern for it. So it gives a high positive weight connecting element 77 to the happy output, and a zero weight connecting 77 to each of angry and sad.</p>
<p>If your goal is improving your intuition, it can be interesting to build models with just 2 or 3 dimensions, and just a few training sentences, and then deliberately overfit the model on your training data. Sometimes you get noise, but sometimes you get lucky and can follow through how the vectors for each word interact, to give a different pattern in the output, and how the final layer combines them to give a prediction.</p>
",2,1,394,2021-10-13 16:10:53,https://stackoverflow.com/questions/69558824/about-lstm-structure-for-classification-in-this-case-its-sentiment-analysis
How to create a function that scores ngrams before unigrams in Python?,"<p>Let's assume I would like to score <code>text</code> with a dictionary called <code>dictionary</code>:</p>
<pre><code>text = &quot;I would like to reduce carbon emissions&quot;

dictionary = pd.DataFrame({'text': [&quot;like&quot;,&quot;reduce&quot;,&quot;carbon&quot;,&quot;emissions&quot;,&quot;reduce carbon emissions&quot;],'score': [1,-1,-1,-1,1]})

</code></pre>
<p>I would like to write a function that adds up every term in <code>dictionary</code> that is in <code>text</code>. However, such a rule must have a nuance: prioritizing ngrams over unigrams.</p>
<p>Concretely, if I sum up the unigrams in <code>dictionary</code> that are in <code>text</code>, I get: <code>1+(-1)+(-1)+(-1)=-2</code> since <code>like =1, reduce=-1, carbon =-1,emissions=-1</code>. This is not what I want. The function must say the following things:</p>
<ol>
<li>consider first ngrams (<code>reduce carbon emissions</code> in the example), if there the set of ngrams is not empty, then attribute the corresponding value to it, otherwise if the the set of ngrams is empty then consider unigrams;</li>
<li>if the set of ngrams is non-empty, ignore those single words (unigrams) that are in the selected ngrams (e.g. ignore &quot;reduce&quot;, &quot;carbon&quot; and &quot;emissions&quot; that are already in &quot;reduce carbon emissions&quot;).</li>
</ol>
<p>Such a function should give me this output: <code>+2</code> since <code>like =1</code> + <code>reduce carbon emissions = 1</code>.</p>
<p>I am pretty new  to Python and I am stuck. Can anyone help me with this?</p>
<p>Thanks!</p>
","python, pandas, module, sentiment-analysis, vader","<p>I would sort the keywords descendingly by length, so it's guarantee that <code>re</code> would match ngrams before one-gram:</p>
<pre><code>import re

pat = '|'.join(sorted(dictionary.text, key=len, reverse=True))

found = re.findall(fr'\b({pat})\b', text)
</code></pre>
<p>Output:</p>
<pre><code>['like', 'reduce carbon emissions']
</code></pre>
<p>To get the expected output:</p>
<pre><code>scores = dictionary.set_index('text')['score']

scores.re_index(found).sum()
</code></pre>
",1,0,91,2021-10-14 15:55:19,https://stackoverflow.com/questions/69573775/how-to-create-a-function-that-scores-ngrams-before-unigrams-in-python
"Is it necessary to re-train BERT models, specifically RoBERTa model?","<p>I am looking for a sentiment analysis code with atleast 80%+ accuracy. I tried Vader and it I found it easy and usable, however it was giving accuracy of 64% only.</p>
<p>Now, I was looking at some BERT models and I noticed it needs to be re-trained? Is that correct? Isn't it pre-trained? is re-training necessary?</p>
","python, tensorflow, sentiment-analysis, bert-language-model, roberta-language-model","<p>You can use pre-trained models from  <code>HuggingFace</code>. There are plenty to choose from. Search for <code>emotion</code> or <code>sentiment</code> <a href=""https://huggingface.co/models?language=en&amp;pipeline_tag=text-classification&amp;sort=likes&amp;search=Emotion"" rel=""nofollow noreferrer"">models</a></p>
<p>Here is an example of a model with 26 emotions. The current implementation works but is very slow for large datasets.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline

tokenizer = RobertaTokenizerFast.from_pretrained(&quot;arpanghoshal/EmoRoBERTa&quot;)
model = TFRobertaForSequenceClassification.from_pretrained(&quot;arpanghoshal/EmoRoBERTa&quot;)


emotion = pipeline('sentiment-analysis', 
                    model='arpanghoshal/EmoRoBERTa')

# example data
DATA_URI = &quot;https://github.com/AFAgarap/ecommerce-reviews-analysis/raw/master/Womens%20Clothing%20E-Commerce%20Reviews.csv&quot;
dataf = pd.read_csv(DATA_URI, usecols=[&quot;Review Text&quot;,])

# This is super slow, I will find a better optimization ASAP


dataf = (dataf
         .head(50) # comment this out for the whole dataset
         .assign(Emotion = lambda d: (d[&quot;Review Text&quot;]
                                       .fillna(&quot;&quot;)
                                       .map(lambda x: emotion(x)[0].get(&quot;label&quot;, None))
                                  ),
             
            )
)

</code></pre>
<p>We could also refactor it a bit</p>
<pre class=""lang-py prettyprint-override""><code>...
# a bit faster than the previous but still slow

def emotion_func(text:str) -&gt; str:
    if not text:
        return None
    return emotion(text)[0].get(&quot;label&quot;, None)
    



dataf = (dataf
         .head(50) # comment this out for the whole dataset
         .assign(Emotion = lambda d: (d[&quot;Review Text&quot;]
                                        .map(emotion_func)
                                     ),

            )
)

</code></pre>
<h2>Results:</h2>
<pre class=""lang-sh prettyprint-override""><code>    Review Text Emotion
0   Absolutely wonderful - silky and sexy and comf...   admiration
1   Love this dress! it's sooo pretty. i happene... love
2   I had such high hopes for this dress and reall...   fear
3   I love, love, love this jumpsuit. it's fun, fl...   love
...
6   I aded this in my basket at hte last mintue to...   admiration
7   I ordered this in carbon for store pick up, an...   neutral
8   I love this dress. i usually get an xs but it ...   love
9   I'm 5&quot;5' and 125 lbs. i ordered the s petite t...   love
...
16  Material and color is nice. the leg opening i...    neutral
17  Took a chance on this blouse and so glad i did...   admiration
...
26  I have been waiting for this sweater coat to s...   excitement
27  The colors weren't what i expected either. the...   disapproval
...
31  I never would have given these pants a second ...   love
32  These pants are even better in person. the onl...   disapproval
33  I ordered this 3 months ago, and it finally ca...   disappointment
34  This is such a neat dress. the color is great ...   admiration
35  Wouldn't have given them a second look but tri...   love
36  This is a comfortable skirt that can span seas...   approval
...
40  Pretty and unique. great with jeans or i have ...   admiration
41  This is a beautiful top. it's unique and not s...   admiration
42  This poncho is so cute i love the plaid check ...   love
43  First, this is thermal ,so naturally i didn't ...   love

</code></pre>
",1,2,1517,2021-10-21 04:44:03,https://stackoverflow.com/questions/69655995/is-it-necessary-to-re-train-bert-models-specifically-roberta-model
how to format data using Pandas (format the data format of the results of sentiment analysis),"<p>I am doing sentiment analysis using BERT.
I want to convert the result to DataFrame format, but I don't know how.
If anyone knows, please let me know.</p>
<p>The related web pages are as follows
<a href=""https://huggingface.co/transformers/main_classes/pipelines.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/main_classes/pipelines.html</a></p>
<pre><code>&gt;&gt;&gt; pipe = pipeline (&quot;text-classification&quot;)
&gt;&gt;&gt; pipe ([&quot;This restaurant is awesome&quot;, &quot;This restaurant is aweful&quot;])
[{'label':'POSITIVE','score': 0.9998743534088135},
  {'label':'NEGATIVE','score': 0.9996669292449951}]
</code></pre>
<p>The output result is output in list format.</p>
<p>Therefore, I want to convert it to the data frame format as shown below. What kind of processing should I do?</p>
<p><a href=""https://i.sstatic.net/XtP1q.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XtP1q.png"" alt=""enter image description here"" /></a></p>
","python, pandas, dataframe, sentiment-analysis","<p>Try this:</p>
<pre><code>sentiment = pipe ([&quot;This restaurant is awesome&quot;, &quot;This restaurant is aweful&quot;])
df = pd.DataFrame(sentiment)
</code></pre>
",0,0,71,2021-10-22 01:34:45,https://stackoverflow.com/questions/69670480/how-to-format-data-using-pandas-format-the-data-format-of-the-results-of-sentim
"Predicting Sentiment of Raw Text using Trained BERT Model, Hugging Face","<p>I'm predicting sentiment analysis of Tweets with positive, negative, and neutral classes. I've trained a BERT model using Hugging Face. Now I'd like to make predictions on a dataframe of unlabeled Twitter text and I'm having difficulty.</p>
<p>I've followed the following tutorial (<a href=""https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"" rel=""nofollow noreferrer"">https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/</a>) and was able to train a BERT model using Hugging Face.</p>
<p>Here's an example of predicting on raw text however it's only one sentence and I would like to use a column of Tweets. <a href=""https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/#predicting-on-raw-text"" rel=""nofollow noreferrer"">https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/#predicting-on-raw-text</a></p>
<pre><code>review_text = &quot;I love completing my todos! Best app ever!!!&quot;

encoded_review = tokenizer.encode_plus(
  review_text,
  max_length=MAX_LEN,
  add_special_tokens=True,
  return_token_type_ids=False,
  pad_to_max_length=True,
  return_attention_mask=True,
  return_tensors='pt',
)

input_ids = encoded_review['input_ids'].to(device)
attention_mask = encoded_review['attention_mask'].to(device)
output = model(input_ids, attention_mask)
_, prediction = torch.max(output, dim=1)
print(f'Review text: {review_text}')
print(f'Sentiment  : {class_names[prediction]}')

Review text: I love completing my todos! Best app ever!!!
Sentiment  : positive

</code></pre>
<p>Bill's response works. Here's the solution.</p>
<pre><code>def predictionPipeline(text):
  encoded_review = tokenizer.encode_plus(
      text,
      max_length=MAX_LEN,
      add_special_tokens=True,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt',
    )

  input_ids = encoded_review['input_ids'].to(device)
  attention_mask = encoded_review['attention_mask'].to(device)

  output = model(input_ids, attention_mask)
  _, prediction = torch.max(output, dim=1)

  return(class_names[prediction])

df2['prediction']=df2['cleaned_tweet'].apply(predictionPipeline)
</code></pre>
","pytorch, sentiment-analysis, huggingface-transformers, pytorch-dataloader","<p>You can use the same code to predict texts from the dataframe column.</p>
<pre><code>model = ...
tokenizer = ...
    
def predict(review_text):
    encoded_review = tokenizer.encode_plus(
    review_text,
    max_length=MAX_LEN,
    add_special_tokens=True,
    return_token_type_ids=False,
    pad_to_max_length=True,
    return_attention_mask=True,
    return_tensors='pt',
    )

    input_ids = encoded_review['input_ids'].to(device)
    attention_mask = encoded_review['attention_mask'].to(device)
    output = model(input_ids, attention_mask)
    _, prediction = torch.max(output, dim=1)
    print(f'Review text: {review_text}')
    print(f'Sentiment  : {class_names[prediction]}')
    return class_names[prediction]


df = pd.DataFrame({
            'texts': [&quot;text1&quot;, &quot;text2&quot;, &quot;....&quot;]
        })

df_dataset[&quot;sentiments&quot;] = df.apply(lambda l: predict(l.texts), axis=1)
</code></pre>
",2,2,2616,2021-11-03 06:07:04,https://stackoverflow.com/questions/69820318/predicting-sentiment-of-raw-text-using-trained-bert-model-hugging-face
Convert pandas data frame to JSON with strings separated,"<p><strong>I have a pandas.dataframe named 'df' with the following format:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>group_name</th>
<th>Positive_Sentiment</th>
<th>Negative_Sentiment</th>
</tr>
</thead>
<tbody>
<tr>
<td>group1</td>
<td>helpful, great support</td>
<td>slow customer service, weak interface, bad management</td>
</tr>
</tbody>
</table>
</div>
<p><strong>I would like to convert this dataframe to a JSON file with the following format:</strong></p>
<pre><code>[{
&quot;Group Name&quot;: &quot;group1&quot;,
&quot;Postive Sentiment&quot;: [
&quot;helpful&quot;,
&quot;great support&quot;
],
&quot;Negative Sentiment&quot;: [
&quot;slow customer service&quot;,
&quot;weak interface&quot;,
&quot;bad management&quot;
]
}
]
</code></pre>
<p><strong>So far I have used this:</strong></p>
<pre><code>    import json
    b = []
    for i in range(len(df)):
        x={}
        x['Group Name']=df.iloc[i]['group_name']
        x['Positive Sentiment']= [df.iloc[i]['Positive_Sentiment']]
        x['Negative Sentiment']= [df.iloc[i]['Negative_Sentiment']]
        b.append(x)
    
    ##Export
    with open('AnalysisResults.json', 'w') as f:
        json.dump(b, f, indent = 2)
</code></pre>
<p><strong>This results in:</strong></p>
<pre><code>[{
&quot;Group Name&quot;: &quot;group1&quot;,
&quot;Postive Sentiment&quot;: [
&quot;helpful,
great support&quot;
],
&quot;Negative Sentiment&quot;: [
&quot;slow customer service,
weak interface,
bad UX&quot;
]
}
]
</code></pre>
<p><strong>You can see it is quite close. The crucial difference is the double-quotes around the ENTIRE contents of each row (e.g., &quot;helpful, great support&quot;) instead of each comma-separated string in the row (e.g., &quot;helpful&quot;, &quot;great support&quot;). I would like double-quotes around each string.</strong></p>
","python, json, pandas, nlp, sentiment-analysis","<p>You can apply <code>split(&quot;,&quot;)</code> to your columns:</p>
<pre><code>
from io import StringIO
import pandas as pd
import json

inp = StringIO(&quot;&quot;&quot;group_name    Positive_Sentiment  Negative_Sentiment
group1  helpful, great support  slow customer service, weak interface, bad management
group2  great, good support     interface meeeh, bad management&quot;&quot;&quot;)

df = pd.read_csv(inp, sep=&quot;\s{2,}&quot;)

def split_and_strip(sentiment):
         [x.strip() for x in sentiment.split(&quot;,&quot;)]

df[&quot;Positive_Sentiment&quot;] = df[&quot;Positive_Sentiment&quot;].apply(split_and_strip)
df[&quot;Negative_Sentiment&quot;] = df[&quot;Negative_Sentiment&quot;].apply(split_and_strip)

print(json.dumps(df.to_dict(orient=&quot;record&quot;), indent=4))

# to save directly to a file:
with open(&quot;your_file.json&quot;, &quot;w+&quot;) as f:
    json.dump(df.to_dict(orient=&quot;record&quot;), f, indent=4)
</code></pre>
<p>Output:</p>
<pre><code>[
    {
        &quot;group_name&quot;: &quot;group1&quot;,
        &quot;Positive_Sentiment&quot;: [
            &quot;helpful&quot;,
            &quot;great support&quot;
        ],
        &quot;Negative_Sentiment&quot;: [
            &quot;slow customer service&quot;,
            &quot;weak interface&quot;,
            &quot;bad management&quot;
        ]
    },
    {
        &quot;group_name&quot;: &quot;group2&quot;,
        &quot;Positive_Sentiment&quot;: [
            &quot;great&quot;,
            &quot;good support&quot;
        ],
        &quot;Negative_Sentiment&quot;: [
            &quot;interface meeeh&quot;,
            &quot;bad management&quot;
        ]
    }
]
</code></pre>
",0,1,882,2021-11-04 17:52:11,https://stackoverflow.com/questions/69843702/convert-pandas-data-frame-to-json-with-strings-separated
return tensorflow predict by string not an array,"<p>So i build a tensorflow for sentimen analysis with 3 multiple class with <code>y_label = {0:&quot;negative&quot;,1:&quot;neutral,2:&quot;positive&quot;)</code> . When I try to predict a sentences it returns an array,how to change it so it will return string based on predicted result?.In my understanding does the predicted value is the max of the array?</p>
<pre><code>sentence = [&quot;kurir keren&quot;]
sequences = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequences,maxlen=max_length,padding=padding_type,truncating=trunc_type)
predic = np.rint(model.predict(padded))
predic2 = model.predict(padded)
print(predic)
print(predic2)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>[[0. 0. 1.]]
[[0.3224687  0.45956263 0.5116373 ]]
</code></pre>
","python, tensorflow, nlp, sentiment-analysis","<p>It seems like you are using one-hot encoded labels where <code>[1. 0. 0.]</code> represents <code>negative </code>, <code>[0. 1. 0.]</code> represents <code>neutral</code>, and <code>[0. 0. 1.]</code> represents <code>positive</code>, so maybe just use <code>np.argmax</code> to get the index of the  highest prediction and use that integer to get the corresponding string:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

y_label = {0:&quot;negative&quot;,1:&quot;neutral&quot;,2:&quot;positive&quot;}

prediction = np.array([[0.3224687,  0.45956263, 0.5116373 ]])
print(y_label[np.argmax(prediction)])
</code></pre>
<pre><code>positive
</code></pre>
",1,0,897,2021-11-08 05:20:15,https://stackoverflow.com/questions/69878939/return-tensorflow-predict-by-string-not-an-array
Create graphs with limits on axis using seaborn/matplotlib,"<p>I have a graph that has values between 0-40 on the x-axis and 0-20,000 on the y-axis. If I am plotting the graph, this is what I see,</p>
<p><a href=""https://i.sstatic.net/oWWE4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oWWE4.png"" alt=""enter image description here"" /></a></p>
<p>Is it possible to plot the graph only till the points in the axis where data entries fall?</p>
","python, matplotlib, nlp, seaborn, sentiment-analysis","<p>I think you can put limits till where the data could be shown using:
x limit as <code>matplotlib.pyplot.xlim(0, 40)</code> and y limit as <code>matplotlib.pyplot.ylim(0,20000)</code></p>
",1,0,95,2021-11-16 07:06:09,https://stackoverflow.com/questions/69984999/create-graphs-with-limits-on-axis-using-seaborn-matplotlib
Loading Pandas Dataframe with skipped sentiment,"<p>I have this dataset for sentiment analysis, loading the data with this code:</p>
<pre><code>url = 'https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/amazon_cells_labelled.tsv'
df = pd.read_csv(url, sep='\t', names=[&quot;Sentence&quot;, &quot;Feeling&quot;])
</code></pre>
<p>The issue is the DataFrame is getting lines with NaN, but It's just part of the whole sentence.</p>
<p>The Output, right now is like this:</p>
<pre><code>sentence                      feeling
I do not like it.             NaN
I give it a bad score.        0
</code></pre>
<p>The Output should look like:</p>
<pre><code>sentence                                    feeling
I do not like it. I give it a bad score     0
</code></pre>
<p>Can you help me to concatenate or load the dataset based on the scores?</p>
","python, pandas, loading, sentiment-analysis","<p>Create virtual groups before <code>groupby</code> and <code>agg</code> rows:</p>
<pre><code>grp = df['Feeling'].notna().cumsum().shift(fill_value=0)
out = df.groupby(grp).agg({'Sentence': ' '.join, 'Feeling': 'last'})
print(out)

# Output:
                                                  Sentence  Feeling
Feeling                                                            
0        I try not to adjust the volume setting to avoi...      0.0
1                              Good case, Excellent value.      1.0
2        I thought Motorola made reliable products!. Ba...      1.0
3        When I got this item it was larger than I thou...      0.0
4                                        The mic is great.      1.0
...                                                    ...      ...
996      But, it was cheap so not worth the expense or ...      0.0
997      Unfortunately, I needed them soon so i had to ...      0.0
998      The only thing that disappoint me is the infra...      0.0
999      No money back on this one. You can not answer ...      0.0
1000     It's rugged. Well this one is perfect, at the ...      NaN

[1001 rows x 2 columns]
</code></pre>
",0,0,73,2021-11-24 15:17:12,https://stackoverflow.com/questions/70098605/loading-pandas-dataframe-with-skipped-sentiment
Create Bar plot in Highcharter library [R Programming],"<p>I have created a simple barplot from sentiment analysis in R, I was studying high charter and was trying to create the same barplot in the high charter. The dataset is simple as below</p>
<pre><code>  anger anticipation disgust fear joy sadness surprise trust negative positive
1     0            2       0    0   2       0        1     2        0        3
2     0            0       0    0   1       0        0     1        0        1
3     0            0       0    0   0       0        0     0        0        1
4     0            2       0    0   2       0        1     2        0        2
5     0            0       0    0   1       0        0     0        0        2
6     0            3       0    0   2       0        1     2        0        2
7     0            0       0    0   0       0        0     0        0        0
</code></pre>
<pre><code>barplot(colSums(s),
        las = 3,
        col = rainbow(10),
        ylab = 'Count',
        main = 'Sentiment Scores')
</code></pre>
<p>With the output below</p>
<p><a href=""https://i.sstatic.net/2yWxH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2yWxH.png"" alt=""enter image description here"" /></a></p>
<p>Any suggestions how I might be able to make the same reactive graph in highcharter? Thanks</p>
","r, visualization, sentiment-analysis, r-highcharter","<p>It might help if you create a simple summary data.frame including the sentiments and scores from your data.</p>
<pre><code>df &lt;- data.frame(sentiment = names(s), score = colSums(s))
</code></pre>
<p>Then you can reference this in <code>hchart</code> to create your barplot.</p>
<pre><code>library(highcharter)

hchart(df, 'column', hcaes(x = sentiment, y = score, color = rainbow(10)))
</code></pre>
",1,0,154,2021-12-02 10:42:21,https://stackoverflow.com/questions/70197966/create-bar-plot-in-highcharter-library-r-programming
How to determine &#39;did&#39; or &#39;did not&#39; on something,"<p>What's the straightforward way to distinguish between this two:</p>
<ol>
<li><code>the movie received critical acclaim</code></li>
<li><code>the movie did not attain critical acclaim</code>.</li>
</ol>
<p>Seems to me 'sentiment analysis' of nlp could do it for me. So I'm using <code>Textblob</code> <a href=""https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis"" rel=""nofollow noreferrer"">sentiment analysis</a>. But both sentences' <code>polarity</code> is <code>0.0</code>.</p>
","python, nlp, sentiment-analysis, textblob","<p>For <em>simple</em> in your case, you can use <strong>flair</strong> which is based on <strong>LSTM</strong> model, that takes sequences of words into account for prediction.</p>
<p><em>1. installing flair</em></p>
<pre><code>!pip3 install flair
</code></pre>
<p><em>2. code</em></p>
<pre><code>import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')

sentence1 = 'the movie received critical acclaim'
sentence2 = 'the movie did not attain critical acclaim'

s1 = flair.data.Sentence(sentence1)
flair_sentiment.predict(s1)
s1_sentiment = s1.labels
print(s1_sentiment)

s2 = flair.data.Sentence(sentence2)
flair_sentiment.predict(s2)
s2_sentiment = s2.labels
print(s2_sentiment)
</code></pre>
<p><em>3. result</em></p>
<pre><code>print(s1_sentiment)
[POSITIVE (0.9995)]

print(s2_sentiment)
[NEGATIVE (0.9985)]
</code></pre>
<p>For more details about <strong>flair</strong>, you can visit <a href=""https://github.com/flairNLP/flair"" rel=""nofollow noreferrer"">this github repo</a>.</p>
",2,1,70,2021-12-12 00:58:23,https://stackoverflow.com/questions/70320166/how-to-determine-did-or-did-not-on-something
rare misspelled words messes my fastText/Word-Embedding Classfiers,"<p>I'm currently trying to make a sentiment analysis on the IMDB review dataset as a part of homework assignment for my college, I'm required to firstly do some preprocessing e.g. : tokenization, stop words removal, stemming, lemmatization. then use different ways to convert this data to vectors to be classfied by different classfiers, Gensim FastText library was one of the required models to obtain word embeddings on the data I got from text pre-processing step.</p>
<p>the problem I faced with Gensim is that I firstly tried to train on my data using vectors of feature size (100,200,300) but yet they always fail at some point, I tried later to use many pre-trained Gensim data vectors, but none of them worked to find word embeddings for all of the words, they'd rather fail at some point with error</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-28-644253c70aa3&gt; in &lt;module&gt;()
----&gt; 1 model.most_similar(some-rare-word)

1 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py in word_vec(self, word, use_norm)
    450             return result
    451         else:
--&gt; 452             raise KeyError(&quot;word '%s' not in vocabulary&quot; % word)
    453 
    454     def get_vector(self, word):

KeyError: &quot;word some-rare-word not in vocabulary&quot;
</code></pre>
<p>the ones I've tried so far are :<br />
conceptnet-numberbatch-17-06-300 : doesn't contain &quot;glass&quot;<br />
word2vec-google-news-300 : ram insufficient in Google Colab<br />
glove-twitter-200 : doesn't contain &quot;5&quot;<br />
crawl-300d-2M : doesn't contain &quot;waltons&quot;<br />
wiki-news-300d-1M : doesn't contain &quot;waltons&quot;<br />
glove-wiki-gigaword-300 : doesn't contain &quot;riget&quot;</p>
<p>got their names from these sources, <a href=""https://github.com/RaRe-Technologies/gensim-data#readme"" rel=""nofollow noreferrer"">here</a> and <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">here</a></p>
<p>by inspecting the failing words, I found that even the largest libraries would usually fail because of the misspelled words that has no meaning like 'riget', 'waltons',...etc</p>
<p>Is their a way to classify and neglect this strange words before trying to inject them to Gensim and receiving this error ? or am I using Gensim very wrong and there's another way to use it ?</p>
<p>any snippet of code or some sort of lead on what to do would be appreciated</p>
<p>my code so far :</p>
<pre><code>import gensim.downloader as api
model = api.load(&quot;glove-wiki-gigaword-300&quot;) # this can be any vector-library of the previously mentioned ones
train_word_embeddings = []
# train_lemm is a vector of size (number of examples, number of words remaining in example sentence i after removal of stop words and lemmatization to nouns)
# they're mainly 25000 example review sentence while the second dimension is of random value depending on number of words
for i in range (len(train_lemm)): 
  train_word_embeddings.append(model.wv[train_lemm[i]])
</code></pre>
","python, gensim, sentiment-analysis, word-embedding","<p>If you train your own word-vector model, then it will contain vectors for all the words you told it to learn. If a word that was in your training data doesn't appear to have a vector, it likely did not appear the required <code>min_count</code> number of times. (These models tend to <em>improve</em> if you discard rare words who few example usages may not be suitably-informative, so the default <code>min_words=5</code> is a good idea.)</p>
<p>It's often reasonable for downstream tasks, like feature engineering using the text &amp; set of word-vectors, to simply ignore words with no vector. That is, if <code>some_rare_word in model.wv</code> is <code>False</code>, just don't try to use that word – &amp; its missing vector – for anything. So you don't necessarily need to find, or train, a set of word-vectors with <em>every</em> word you need. Just elide, rather than worry-about, the rare missing words.</p>
<p>Separate observations:</p>
<ul>
<li>Stemming/lemmatization &amp; stop-word removal aren't always worth the trouble, with all corpora/algorithms/goals. (And, stemming/lemmatization may wind up creating pseudowords that limit the model's interpretability &amp; easy application to any texts that don't go through identical preprocessing.) So if those are required parts of laerning exercise, sure, get some experience using them. But don't assume they're necessarily helping, or worth the extra time/complexity, unless you verify that rigrously.</li>
<li>FastText models will also be able to supply synthetic vectors for words that <em>aren't</em> known to the model, based on substrings. These are often pretty weak, but may better than nothing - especially when they give vectors for typos, or rare infelcted forms, similar to morphologically-related known words. (Since this deduced similarity, from many similarly-written tokens, provides some of the same value as stemming/lemmatization via a different path that <em>required</em> the original variations to all be present during initial training, you'd especially want to pay attention to whether FastText &amp; stemming/lemmatization mix well for your goals.) Beware, though: for very-short unknown words – for which the model learned no reusable substring vectors – FastText may still return an error or all-zeros vector.</li>
<li>FastText has a <code>supervised</code> classification mode, but it's not supported by Gensim. If you want to experiment with that, you'd need to use the Facebook FastText implementation. (You could still use a traditional, non-<code>supervised</code> FastText word vector model as a contributor of features for other possible representations.)</li>
</ul>
",1,0,756,2021-12-16 19:53:52,https://stackoverflow.com/questions/70384870/rare-misspelled-words-messes-my-fasttext-word-embedding-classfiers
My Naive Bayes classifier works for my model but will not accept user input on my application,"<p>I am trying to deploy my machine learning Naive Bayes sentiment analysis model onto a web application. The idea is that the user should type some text, which the application performs sentiment analysis on and then stores the text with the assigned sentiment in another column within the database, to be called as a list via html later.</p>
<p>While the model and vectorizer work fine on Google Colab, when I load the model to my application and try to run the user input through it it won't work. I have gotten many error codes depending on the different solutions I've tried.</p>
<p>The most recent is:</p>
<pre><code>ValueError: DataFrame constructor not properly called!
</code></pre>
<p>But when I try to fix this I get other error messages such as:</p>
<pre><code>'numpy.ndarray' object has no attribute 'lower'
</code></pre>
<p>or:</p>
<pre><code>ValueError: X has 1 features, but MultinomialNB is expecting 26150 features as input.
</code></pre>
<p>or:</p>
<pre><code>sklearn.exceptions.NotFittedError: Vocabulary not fitted or provided
</code></pre>
<p>BASICALLY I don't know what I'm doing and I've been trying to figure it out for weeks. My inclination is that the problem is either the format coming in from the user is not readable by the model, or the vectorizer is not working on the input.</p>
<p>OR maybe my whole approach is wrong, and there are steps I am missing. Any help with this would be massively appreciated.</p>
<p>My model code looks like this (after preprocessing):</p>
<pre><code>#Split into training and testing data
x = df['text']
y = df['sentiment']

df1 = df[df[&quot;text&quot;].notnull()]
x1 = df1['text']
y1 = df1['sentiment']

x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.2, random_state=30)

# Vectorize text
vec = CountVectorizer(stop_words='english')
x1 = vec.fit_transform(x1).toarray()
x1_test = vec.transform(x1_test).toarray()

df1 = df1.replace(r'^\s*$', np.nan, regex=True)

from sklearn.naive_bayes import MultinomialNB

sentiment_model = MultinomialNB()
sentiment_model.fit(x1, y1)
sentiment_model.score(x1_test, y1_test)

# Save model to disk
pickle.dump(sentiment_model, open('sentiment_model.pkl','wb'))
</code></pre>
<p>And my application code looks like this:</p>
<pre><code>@app.route('/journal', methods=['GET', 'POST'])
def entry():
    if request.method == 'POST':
        journals = request.form
        
        entry_date = journals['entry_date']
        journal_entry = journals['journal_entry']

        vec = CountVectorizer(stop_words='english')
        sdf = pd.DataFrame('journal_entry')
        sdf = vec.fit_transform(sdf).toarray()
        sdf = vec.transform(sdf).toarray()

        sentiment = sentiment_model.predict(sdf)
        journals['sentiment'] = sentiment

        cur = mysql.connection.cursor()
        #insert the values with sentiment attribute into database
        cur.execute(&quot;INSERT INTO journals(entry_date, journal_entry, sentiment) VALUES(%s, %s, %s)&quot;,(entry_date, journal_entry, sentiment))
        mysql.connection.commit()
   
    return render_template('journal.html')
</code></pre>
","python, classification, sentiment-analysis, naivebayes, countvectorizer","<p>So it seems to me that there are multiple issues here at play.</p>
<p>For one, <code>sdf = pd.DataFrame('journal_entry')</code> does not make sense -- you create the data frame from literal string 'journal_entry', not the actual contents of it? I suggest you get rid of the DataFrame in your <code>entry</code> function entirely, as it is not a required input structure for sklearn objects.</p>
<p>Secondly, you're duplicating functionality with calling <code>fit_transform</code> and then again <code>transform</code> in your <code>entry</code> function. It's sufficient to call <code>fit_transform</code> as it's doing two things: 1) it learns the dictionary 2) it transforms to document-term matrix.</p>
<p>Thirdly, you trained your model using a specific CountVectorizer model. This model will transform each document into vectors using the learned document-term matrix which acquires a fixed size at the time you call <code>fit</code> or <code>fit_transform</code> function. Then your Naive Bayes model is trained using this fixed sized vector. Hence, it complains when it gets a different sized vector at inference time -- this is because you're re-initializing <code>CountVectorizer</code> again at each <code>entry</code> call. You need to save the <code>CountVectorizer</code> as well if you want to preserve the feature size.</p>
<p>Also, I'd suggest making some check in your <code>entry</code> function that makes sure you get valid strings for your algorithm in the POST request.</p>
<pre class=""lang-py prettyprint-override""><code>
# load both CountVectorizer and the model 
vec = pickle.load(open(&quot;my_count_vec.pkl&quot;, &quot;rb&quot;))
sentiment_model = pickle.load(open(&quot;my_sentiment_model&quot;, &quot;rb&quot;))

@app.route('/journal', methods=['GET', 'POST'])
def entry():
    if request.method == 'POST':
        journals = request.form
        
        entry_date = journals['entry_date']
        journal_entry = journals['journal_entry']
        sdf = vec.transform([journal_entry]).reshape(1, -1)
        sentiment = sentiment_model.predict(sdf)
        ...
</code></pre>
<p><code>sdf = vec.transform([journal_entry]).reshape(1, -1)</code> assumes that journal entry is a single string and hence it needs reshaping for further processing.</p>
",1,1,4167,2021-12-23 11:46:47,https://stackoverflow.com/questions/70461730/my-naive-bayes-classifier-works-for-my-model-but-will-not-accept-user-input-on-m
Sentiment Analysis: Fitting a model result in value error (shapes incompatible?),"<p>I am doing a sentiment analysis on a set of reviews --&gt; predicting the rating (0-5) based on the text review. I have completed text pre-processing and tokenizing. I am using a pre-trained word vector embeddings (googlenews) and created the embedding_matrix.</p>
<p>I have built the model thus far:</p>
<pre><code>#defining X (padded) and y and completing train/test split
X = pad_sequences(sequences, maxlen= 1000)
y = df['rating']
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 1000)

y_train = to_categorical(y_train,6)

#building the model
sentiment_wv_model = Sequential()
embed_layer = Embedding(vocab_size, 100,weights = [embedding_matrix], input_length = 1000,trainable = True)

sentiment_wv_model.add(embed_layer)
sentiment_wv_model.add(Dense(100, activation = 'sigmoid'))
sentiment_wv_model.add(Dense(32, activation = 'sigmoid'))
sentiment_wv_model.add(Dense(1, activation='softmax'))


#compile model and fit to train data
sentiment_wv_model.compile(loss = 'categorical_crossentropy',optimizer = 'adam', metrics =['accuracy'])

sentiment_wv_model.summary
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_1 (Embedding)     (None, 1000, 100)         3631400   
                                                                 
 dense (Dense)               (None, 1000, 100)         10100     
                                                                 
 dense_1 (Dense)             (None, 1000, 32)          3232      
                                                                 
 dense_2 (Dense)             (None, 1000, 2)           66        
                                                                 
 dense_3 (Dense)             (None, 1000, 1)           3         
                                                                 
 dense_4 (Dense)             (None, 1000, 100)         200       
                                                                 
 dense_5 (Dense)             (None, 1000, 32)          3232      
                                                                 
 dense_6 (Dense)             (None, 1000, 1)           33        
                                                                 
=================================================================
Total params: 3,648,266
Trainable params: 3,648,266
Non-trainable params: 0
_________________________________________________________________


sentiment_wv_model.fit(X_train, y_train, batch_size = 32, epochs = 5, verbose =2)
</code></pre>
<p>Running this, I get the following error:</p>
<pre><code>ValueError: in user code:

    File &quot;C:\Users\tammy\Anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 878, in train_function  *
        return step_function(self, iterator)
    File &quot;C:\Users\tammy\Anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 867, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\tammy\Anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 860, in run_step  **
        outputs = model.train_step(data)
    File &quot;C:\Users\tammy\Anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 809, in train_step
        loss = self.compiled_loss(
    File &quot;C:\Users\tammy\Anaconda3\lib\site-packages\keras\engine\compile_utils.py&quot;, line 201, in __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    File &quot;C:\Users\tammy\Anaconda3\lib\site-packages\keras\losses.py&quot;, line 141, in __call__
        losses = call_fn(y_true, y_pred)
    File &quot;C:\Users\tammy\Anaconda3\lib\site-packages\keras\losses.py&quot;, line 245, in call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    File &quot;C:\Users\tammy\Anaconda3\lib\site-packages\keras\losses.py&quot;, line 1664, in categorical_crossentropy
        return backend.categorical_crossentropy(
    File &quot;C:\Users\tammy\Anaconda3\lib\site-packages\keras\backend.py&quot;, line 4994, in categorical_crossentropy
        target.shape.assert_is_compatible_with(output.shape)

    ValueError: Shapes (None, 6, 6) and (None, 1000, 1) are incompatible
</code></pre>
<p>I see that this type of question has been asked a few times, but I have tried other solutions such as putting y as 'to_categorical', changing the activation functions or switching to 'binary_crossentropy (the last two didn't make sense to me but I tried it anyway). Please advise!</p>
","python, tensorflow, keras, deep-learning, sentiment-analysis","<p>You are currently having a sparse tensor for your y-values:</p>
<pre><code>y_train = to_categorical(y_train,6)
</code></pre>
<p>This sould have the shape <code>[1000,6]</code> which you can check with <code>y_train.shape()</code>.</p>
<p>One thing that should be working is simply changing the size of your output layer to 6:</p>
<pre><code>sentiment_wv_model.add(Dense(6, activation='softmax'))
</code></pre>
<p>[Optional] After this you can also change your loss to sparse_categorical_crossentropy:</p>
<pre><code>sentiment_wv_model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy,optimizer = 'adam', metrics =['accuracy'])
</code></pre>
<p>Also, you should consider flattening your data after the <code>Embedding</code> layer, so that you get the output shape <code>(None, 6)</code> instead of <code>(None, 1000, 6)</code></p>
",0,1,137,2022-01-11 03:24:41,https://stackoverflow.com/questions/70661279/sentiment-analysis-fitting-a-model-result-in-value-error-shapes-incompatible
Span-Aste with allennlp - testing against new unseen and unlabeled data,"<p>I am trying to use this <a href=""https://colab.research.google.com/drive/1F9zW_nVkwfwIVXTOA_juFDrlPz5TLjpK?usp=sharing"" rel=""nofollow noreferrer"">colab</a> of this <a href=""https://github.com/xuuuluuu/Span-ASTE"" rel=""nofollow noreferrer"">github</a> page to extract the triplet [term, opinion, value] from a sentence from my custom dataset.</p>
<p>Here is an overview of the system architecture:
<a href=""https://i.sstatic.net/bReOT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bReOT.png"" alt=""Span-Aste system"" /></a></p>
<p>While I can use the sample offered in the colab and also train the model with my data, I don't know I should re-use this against an unlabeled sample.</p>
<p>If I try to run the colab as-is changing only the test and dev data with unlabeled data, I encounter this error:</p>
<pre><code>    DEVICE=0 {   &quot;names&quot;: &quot;sample&quot;,   &quot;seeds&quot;: [
        0   ],   &quot;sep&quot;: &quot;,&quot;,   &quot;name_out&quot;: &quot;results&quot;,   &quot;kwargs&quot;: {
        &quot;trainer__cuda_device&quot;: 0,
        &quot;trainer__num_epochs&quot;: 10,
        &quot;trainer__checkpointer__num_serialized_models_to_keep&quot;: 1,
        &quot;model__span_extractor_type&quot;: &quot;endpoint&quot;,
        &quot;model__modules__relation__use_single_pool&quot;: false,
        &quot;model__relation_head_type&quot;: &quot;proper&quot;,
        &quot;model__use_span_width_embeds&quot;: true,
        &quot;model__modules__relation__use_distance_embeds&quot;: true,
        &quot;model__modules__relation__use_pair_feature_multiply&quot;: false,
        &quot;model__modules__relation__use_pair_feature_maxpool&quot;: false,
        &quot;model__modules__relation__use_pair_feature_cls&quot;: false,
        &quot;model__modules__relation__use_span_pair_aux_task&quot;: false,
        &quot;model__modules__relation__use_span_loss_for_pruners&quot;: false,
        &quot;model__loss_weights__ner&quot;: 1.0,
        &quot;model__modules__relation__spans_per_word&quot;: 0.5,
        &quot;model__modules__relation__neg_class_weight&quot;: -1   },   &quot;root&quot;: &quot;aste/data/triplet_data&quot; } {   &quot;root&quot;: &quot;/content/Span-ASTE/aste/data/triplet_data/sample&quot;,   &quot;train_kwargs&quot;: {
        &quot;seed&quot;: 0,
        &quot;trainer__cuda_device&quot;: 0,
        &quot;trainer__num_epochs&quot;: 10,
        &quot;trainer__checkpointer__num_serialized_models_to_keep&quot;: 1,
        &quot;model__span_extractor_type&quot;: &quot;endpoint&quot;,
        &quot;model__modules__relation__use_single_pool&quot;: false,
        &quot;model__relation_head_type&quot;: &quot;proper&quot;,
        &quot;model__use_span_width_embeds&quot;: true,
        &quot;model__modules__relation__use_distance_embeds&quot;: true,
        &quot;model__modules__relation__use_pair_feature_multiply&quot;: false,
        &quot;model__modules__relation__use_pair_feature_maxpool&quot;: false,
        &quot;model__modules__relation__use_pair_feature_cls&quot;: false,
        &quot;model__modules__relation__use_span_pair_aux_task&quot;: false,
        &quot;model__modules__relation__use_span_loss_for_pruners&quot;: false,
        &quot;model__loss_weights__ner&quot;: 1.0,
        &quot;model__modules__relation__spans_per_word&quot;: 0.5,
        &quot;model__modules__relation__neg_class_weight&quot;: -1   },   &quot;path_config&quot;: &quot;/content/Span-ASTE/training_config/aste.jsonnet&quot;,   &quot;repo_span_model&quot;: &quot;/content/Span-ASTE&quot;,   &quot;output_dir&quot;: &quot;model_outputs/aste_sample_c7b00b66bf7ec669d23b80879fda043d&quot;,   &quot;model_path&quot;: &quot;models/aste_sample_c7b00b66bf7ec669d23b80879fda043d/model.tar.gz&quot;,   &quot;data_name&quot;: &quot;sample&quot;,   &quot;task_name&quot;: &quot;aste&quot; }
    # of original triplets:  11
    # of triplets for current setup:  11
    # of original triplets:  7
    # of triplets for current setup:  7 Traceback (most recent call last):   File &quot;/usr/lib/python3.7/pdb.py&quot;, line 1699, in main
        pdb._runscript(mainpyfile)   
File &quot;/usr/lib/python3.7/pdb.py&quot;, line 1568, in _runscript
        self.run(statement)   
File &quot;/usr/lib/python3.7/bdb.py&quot;, line 578, in run
        exec(cmd, globals, locals)   File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;   
File &quot;/content/Span-ASTE/aste/main.py&quot;, line 1, in &lt;module&gt;
        import json   
File &quot;/usr/local/lib/python3.7/dist-packages/fire/core.py&quot;, line 138, in Fire
        component_trace = _Fire(component, args, parsed_flag_args, context, name)   File &quot;/usr/local/lib/python3.7/dist-packages/fire/core.py&quot;, line 468, in
    _Fire
        target=component.__name__)   
File &quot;/usr/local/lib/python3.7/dist-packages/fire/core.py&quot;, line 672, in
    _CallAndUpdateTrace
        component = fn(*varargs, **kwargs)   File &quot;/content/Span-ASTE/aste/main.py&quot;, line 278, in main
        scores = main_single(p, overwrite=True, seed=seeds[i], **kwargs)   
File &quot;/content/Span-ASTE/aste/main.py&quot;, line 254, in main_single
        trainer.train(overwrite=overwrite)   
File &quot;/content/Span-ASTE/aste/main.py&quot;, line 185, in train
        self.setup_data()   
File &quot;/content/Span-ASTE/aste/main.py&quot;, line 177, in setup_data
        data.load()   
File &quot;aste/data_utils.py&quot;, line 214, in load
        opinion_offset=self.opinion_offset,   
File &quot;aste/evaluation.py&quot;, line 165, in read_inst
        o_output = line[2].split()  # opinion IndexError: list index out of range Uncaught exception. Entering post mortem debugging Running 'cont' or 'step' will restart the program
    &gt; /content/Span-ASTE/aste/evaluation.py(165)read_inst()
    -&gt; o_output = line[2].split()  # opinion (Pdb)
</code></pre>
<p>From my understanding, it seems that it is searching for the labels to start the evaluation. The problem is that I don't have those labels - although I have provided training set with similar data and labels associated.</p>
<p>I am new in deep learning and also allennlp so I am probably missing knowledge. I have tried to solve this for the past 2 weeks but I am still stuck, so here I am.</p>
","deep-learning, sentiment-analysis, bert-language-model, reproducible-research, allennlp","<p>KeyPi, this is a supervised learning model, it needs labelled data for your text corpus in the form sentence(ex: I charge it at night and skip taking the cord with me because of the good battery life .)  followed by '#### #### ####' as a separator and list of labels(include aspect/target word index in first list and the openion token index in the sentence followed by 'POS' for Positive and 'NEG' for negitive.) [([16, 17], [15], 'POS')]
16 and 17- battery life and in index 15, we have openion word &quot;good&quot;.
I am not sure if you have figures this out already and find some way to label the corpus.</p>
",0,0,93,2022-01-19 19:02:59,https://stackoverflow.com/questions/70776362/span-aste-with-allennlp-testing-against-new-unseen-and-unlabeled-data
Errors in counting + combining bing sentiment score variables in Tidytext?,"<p>I'm doing sentiment analysis on a large corpus of text. I'm using the bing lexicon in tidytext to get simple binary pos/neg classifications, but want to calculate the ratios of positive to total (positive &amp; negative) words within a document. I'm rusty with dplyr workflows, but I want to count the number of words coded as &quot;positive&quot; and divide it by the total count of words classified with a sentiment.</p>
<p>I tried this approach, using sample code and stand-in data . . .</p>
<pre><code>library(tidyverse)
library(tidytext)

#Creating a fake tidytext corpus
df_tidytext &lt;- data.frame(
  doc_id = c(&quot;Iraq_Report_2001&quot;, &quot;Iraq_Report_2002&quot;),
  text = c(&quot;xxxx&quot;, &quot;xxxx&quot;) #Placeholder for text
)

#Creating a fake set of scored words with bing sentiments 
#for each doc in corpus
df_sentiment_bing &lt;- data.frame(
  doc_id = c((rep(&quot;Iraq_Report_2001&quot;, each = 3)), 
             rep(&quot;Iraq_Report_2002&quot;, each = 3)),
  word = c(&quot;improve&quot;, &quot;democratic&quot;, &quot;violence&quot;,
           &quot;sectarian&quot;, &quot;conflict&quot;, &quot;insurgency&quot;),
  bing_sentiment = c(&quot;positive&quot;, &quot;positive&quot;, &quot;negative&quot;,
                &quot;negative&quot;, &quot;negative&quot;, &quot;negative&quot;) #Stand-ins for sentiment classification
)

#Summarizing count of positive and negative words
# (number of positive words out of total scored words in each doc)
df_sentiment_scored &lt;- df_tidytext %&gt;%
  left_join(df_sentiment_bing) %&gt;%
  group_by(doc_id) %&gt;%
  count(bing_sentiment) %&gt;%
  pivot_wider(names_from = bing_sentiment, values_from = n) %&gt;%
  summarise(bing_score = count(positive)/(count(negative) + count(positive)))

</code></pre>
<p>But I get the following error:</p>
<pre><code>&quot;Error: Problem with `summarise()` input `bing_score`.
x no applicable method for 'count' applied to an object of class &quot;c('integer', 'numeric')&quot;
ℹ Input `bing_score` is `count(positive)/(count(negative) + count(positive))`.
ℹ The error occurred in group 1: doc_id = &quot;Iraq_Report_2001&quot;.
</code></pre>
<p>Would love some insight into what I'm doing wrong with my summarizing workflow here.</p>
","r, dplyr, sentiment-analysis, tidytext","<p>I don't understand what is the point of counting there if the columns are numeric. By the way, that is also why you are having the error.</p>
<p>One solution could be:</p>
<pre><code>#Summarizing count of positive and negative words
# (number of positive words out of total scored words in each doc)
 df_tidytext %&gt;%
  left_join(df_sentiment_bing) %&gt;%
  group_by(doc_id) %&gt;%
  dplyr::count(bing_sentiment) %&gt;%
  pivot_wider(names_from = bing_sentiment, values_from = n) %&gt;%
  replace(is.na(.), 0) %&gt;%
  summarise(bing_score = sum(positive)/(sum(negative) + sum(positive)))
</code></pre>
<p>The result you should get its:</p>
<pre><code>Joining, by = &quot;doc_id&quot;
# A tibble: 2 × 2
  doc_id           bing_score
  &lt;fct&gt;                 &lt;dbl&gt;
1 Iraq_Report_2001      0.667
2 Iraq_Report_2002      0    
</code></pre>
",1,0,124,2022-02-01 23:42:00,https://stackoverflow.com/questions/70949018/errors-in-counting-combining-bing-sentiment-score-variables-in-tidytext
LSTM for sentiment analysis,"<p>I saw this tensorflow model which is used for telling if text is positive or negative, and I don't fully understand it. I know that LSTM saves the words and predict the next words based on the previous words, but how does this help network to distinguish emotions of the text?</p>
<pre><code>def tensorflow_based_model(): 
    inputs = Input(name='inputs',shape=[max_len])
    layer = Embedding(2000,50,input_length=max_len)(inputs) 
    layer = LSTM(64)(layer)
    layer = Dense(256,name='FC1')(layer) 
    layer = Activation('relu')(layer) 
    layer = Dropout(0.5)(layer)
    layer = Dense(1,name='out_layer')(layer) 
    layer = Activation('sigmoid')(layer) 
    model = Model(inputs=inputs,outputs=layer) 
    return model 
</code></pre>
","python, neural-network, nlp, lstm, sentiment-analysis","<p>Altough LSTM can be used in the text generation, the main use of LSTM (or any recurrent neural network layer) is to understand sequences. You can find more information in this blog posts:</p>
<p><a href=""https://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow noreferrer"">The Unreasonable Effectiveness of Recurrent Neural Networks</a></p>
<p><a href=""https://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow noreferrer"">Understanding LSTM Networks</a></p>
<p>In the case of sentiment analysis, Instead of helping to generate new word, LSTM helps us understand what was said. It basicaly reads over the string and keeps some important information about the previously said words.</p>
",2,1,272,2022-02-13 17:29:15,https://stackoverflow.com/questions/71103188/lstm-for-sentiment-analysis
The pandas dataframe do not get updated based on a condition,"<p>I have a dataframe and I need to update a column based on a condition (I'm trying to label text using Microsoft azure API and then save the label back to the original data frame so that later I can calculate the accuracy). But weirdly the data frame does not get updated!!</p>
<p>This is a sample code:</p>
<pre><code>from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient


key = &quot;key&quot;
endpoint = &quot;https://endpoint&quot;

text_analytics_client = TextAnalyticsClient(endpoint=endpoint,   credential=AzureKeyCredential(key))

df = pd.DataFrame({'id':[1,2,3], 'text': ['im ok', 'you arent ok', 'its fine'],
                   'Sentiment':['positive', 'negative', 'neutral']})
n = 10

for i in range(0, df.shape[0], n):
    result = text_analytics_client.analyze_sentiment(df.iloc[i:i + n].to_dict('records'))
######in case you do not have azure credentials to get this code run, the out of the result is like this:
######[AnalyzeSentimentResult(id=2, sentiment=negative, warnings= [], statistics=None, confidence_scores=SentimentConfidenceScores(positive=0.01, neutral=0.16, negative=0.83), sentences=[SentenceSentiment(text=you arent ok, sentiment=negative, confidence_scores=SentimentConfidenceScores(positive=0.01, neutral=0.16, negative=0.83), length=12, offset=0, mined_opinions=[])], is_error=False), AnalyzeSentimentResult(id=3, sentiment=positive, warnings=[], statistics=None, confidence_scores=SentimentConfidenceScores(positive=0.98, neutral=0.01, negative=0.01), sentences=[SentenceSentiment(text=its fine, sentiment=positive, confidence_scores=SentimentConfidenceScores(positive=0.98, neutral=0.01, negative=0.01), length=8, offset=0, mined_opinions=[])], is_error=False)]

    for idx, doc in enumerate(result):
        print(doc.sentiment) ##this will print out a value
        id_res = result[idx]['id']
        #print(id_res) this will print out the correct id
        df.loc[df.id == id_res, 'label'] = doc.sentiment
        print(df) ### but here when the dataframe is printed the label column is NAN
</code></pre>
<p>I have searched and find multiple links like <a href=""https://stackoverflow.com/questions/33274628/updating-values-of-pandas-dataframe-on-condition"">this</a>, <a href=""https://stackoverflow.com/questions/67368151/pandas-dataframe-doesnt-update-column-value-under-condition"">this</a> or <a href=""https://www.easytweaks.com/update-values-dataframe-pandas-python/"" rel=""nofollow noreferrer"">this</a>. In all three examples they are doing the same thing as me but my dataframe do not get updated and this is the result I get:</p>
<pre><code>   id          text   Sentiment label
0   1         im ok  positive   NaN
1   2  you arent ok  negative   NaN
2   3      its fine   neutral   NaN
</code></pre>
<p><strong>details</strong></p>
<p>Im adding some details so that it may help. As I commented in the code <code>res_result</code> has a correct id. When I replace this <code>df.loc[df.id == id_res, 'label']</code> with <code>df.loc[df.id == 1, 'label']</code> it successfully updated that rows but otherwise it does not get updated!!!!</p>
<p>Appreciate any input on how to fix this.</p>
","python, pandas, azure, dataframe, sentiment-analysis","<p>The issue is in this line here:</p>
<pre><code>df.loc[df.id == id_res, 'label'] = doc.sentiment
</code></pre>
<p><code>df.id</code> is type int and <code>id_res</code> is type string. If you convert <code>id_res</code> to int then this will be a valid comparison and you'll get the output you're looking for:</p>
<pre><code>df.loc[df.id == int(id_res), 'label'] = doc.sentiment
</code></pre>
<p>Output:</p>
<pre><code>   id          text Sentiment     label
0   1         im ok  positive   neutral
1   2  you arent ok  negative  negative
2   3      its fine   neutral  positive
</code></pre>
",1,0,95,2022-02-14 17:34:57,https://stackoverflow.com/questions/71115962/the-pandas-dataframe-do-not-get-updated-based-on-a-condition
How to remove unexpected parameter and attribute errors while importing data for sentiment analysis from twitter?,"<p>Q)
How to solve the following errors</p>
<p>1)Unexpected parameter: Lang</p>
<p>2)Unexpected parameter: tweet_node</p>
<p>3)line 25, in 
tweets = [tweet.full_text for tweet in tweet_cursor ]</p>
<p>AttributeError: 'Status' object has no attribute 'full_text'</p>
<p>CODE</p>
<pre><code>import tweepy
import textblob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re


api_key = 'xxxxx'
api_key_secret = 'xxxxxxx'
access_token = 'xxxxxxxxxxxxxxxx'
access_token_secret = 'xxxxxxxxxxxxxxxxxxxxxx'

authenticator = tweepy.OAuthHandler(api_key, api_key_secret)
authenticator.set_access_token(access_token, access_token_secret)

api= tweepy.API (authenticator, wait_on_rate_limit=True)

crypto_currency= &quot;Dogecoin&quot;

search= f'#(crypto currency) -filter: retweets'

tweet_cursor = tweepy.Cursor(api.search_tweets, q=search, Lang='en', `tweet_node='extended').items (100)`

tweets = [tweet.full_text for tweet in tweet_cursor ]

tweets_df = pd.DataFrame(tweets , columns = ['Tweets'])

for _, row in tweets_df.iterrows():
    row ['tweets'] = re.sub('https\S+','' , row['Tweets'])
    row['tweets'] = re.sub('#\S+', '', row['Tweets'])
    row['tweets'] = re.sub('@\S+', '', row['Tweets'])
    row['tweets'] = re.sub('\\n', '', row['Tweets'])

tweets_df['Polarity'] = tweets_df['Tweets'].map(lambda tweet:textblob.TextBlob(tweet).sentiment.polarity)
tweets_df['Result'] = tweets_df['Polarity'].map(lambda pol:'+' if pol &gt; 0 else '-')

positive = tweets_df[tweets_df.Result == '+'].count()['Tweets']
negative = tweets_df[tweets_df.Result == '-'].count()['Tweets']

plt.bar([0,1], [positive,negative], label=['Positive', 'Negative'], color=['green', 'red'])
plt.legend

plt.show()
</code></pre>
","python, twitter, tweepy, sentiment-analysis, textblob","<ol>
<li>lang=en should be inside of the value of <code>search</code>.</li>
<li>tweet_node should be <code>tweet_mode</code></li>
<li>The <code>full_text</code> will only exist if the <code>tweet_mode=extended</code> parameter is correct, and the Tweet is more than 140 characters in text length.</li>
</ol>
",0,0,368,2022-02-18 16:45:06,https://stackoverflow.com/questions/71177046/how-to-remove-unexpected-parameter-and-attribute-errors-while-importing-data-for
How to remove words from a sentence that carry no positive or negative sentiment?,"<p>Im trying a sentiment analysis based approach on youtube comments, but the comments many times have words like mrbeast, tiger/'s, lion/'s, pewdiepie, james, etc which do not add any feeling in the sentence. I've gone through nltk's average_perception_tagger but it didn't work well as it gave the results as</p>
<p>my input:</p>
<pre><code>&quot;mrbeast james lion tigers bad sad clickbait fight nice good&quot;
</code></pre>
<p>words that i need in my sentence:</p>
<pre><code>&quot;bad sad clickbait fight nice good&quot;
</code></pre>
<p>what i got using average_perception_tagger:</p>
<pre><code>[('mrbeast', 'NN'),
 ('james', 'NNS'),
 ('lion', 'JJ'),
 ('tigers', 'NNS'),
 ('bad', 'JJ'),
 ('sad', 'JJ'),
 ('clickbait', 'NN'),
 ('fight', 'NN'),
 ('nice', 'RB'),
 ('good', 'JJ')]

</code></pre>
<p>so as you can see if i remove mrbeast i.e NN the words like clickbait, fight will also get removed which than ultimately remove expressions from that sentence.</p>
","python, machine-learning, nlp, sentiment-analysis","<p>There are multiple ways of doing this like</p>
<ol>
<li><p>you can create a set of positive and negative words and for each word in your grammar you can check if it exists in your set, if it does you should keep the word, else delete it. This however would first require all positive and negative words dataset.</p>
</li>
<li><p>you can use something like textblob which can give you the sentiment score of a word or a sentence. so with a cutoff sentiment score you can filter out the words that you don't need.</p>
</li>
</ol>
",0,-2,1101,2022-02-27 10:59:01,https://stackoverflow.com/questions/71284177/how-to-remove-words-from-a-sentence-that-carry-no-positive-or-negative-sentiment
How can I show Label output only from Transformers Pipeline - Sentiment Analysis,"<p><a href=""https://i.sstatic.net/B4rVX.png"" rel=""nofollow noreferrer"">Picture of Output</a></p>
<p>From the picture, I've given the model a few sentences to analyse. The result from the analysis, I've added a new column to the dataframe called &quot;sentiment&quot;.</p>
<p>As you can see, it gives the results in a list format with labels and scores.</p>
<p>What i'm trying to do is only add the Label result. i.e: &quot;POSITIVE&quot;, &quot;NEGATIVE&quot;, etc.. Is there a way of doing this?</p>
<p>Many thanks in advance.</p>
","python, sentiment-analysis, huggingface-transformers","<p>I have no idea how works your <code>pipe</code> but you have <code>list</code> with <code>dict</code> and <code>pandas</code> has <code>.str</code> to work with string but <code>.str[index]</code> works also with <code>list</code> or <code>dict</code>.</p>
<pre><code>df['Sentiment'] = df['Sentiment'].str[0].str['label']
</code></pre>
<hr />
<p>Minimal working example</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
   'Sentiment': [
      [{'label':'Negative', 'score': 0.1}],
      [{'label':'Positive', 'score': 0.9}],
   ]      
}) 

print('\n--- before ---\n')
print(df)

df['Sentiment'] = df['Sentiment'].str[0].str['label']

print('\n--- after ---\n')
print(df)
</code></pre>
<p>Result:</p>
<pre><code>--- before ---

                               Sentiment
0  [{'label': 'Negative', 'score': 0.1}]
1  [{'label': 'Positive', 'score': 0.9}]

--- after ---

  Sentiment
0  Negative
1  Positive
</code></pre>
",1,-2,1001,2022-03-03 14:07:36,https://stackoverflow.com/questions/71338524/how-can-i-show-label-output-only-from-transformers-pipeline-sentiment-analysis
Sentiment Analysis: Is there a way to extract positive and negative aspects in reviews?,"<p>Currently, I'm working on a project where I need to extract the relevant aspects used in positive and negative reviews in real time.</p>
<p>For the notions of more negative and positive, it will be a question of contextualizing the word. Distinguish between a word that sounds positive in a negative context (consider irony).</p>
<p>Here is an example:
<em>Very nice welcome!!! We ate very well with traditional dishes as at home, the quality but also the quantity are in appointment!!!</em>*</p>
<p><strong>Positive aspects:</strong> welcome, traditional dishes, quality, quantity</p>
<p>Can anyone suggest to me some tutorials, papers or ideas about this topic?</p>
<p>Thank you in advance.</p>
","nlp, sentiment-analysis","<p>This task is called <strong>Aspect Based Sentiment Analysis</strong> (ABSA). Most popular is the format and dataset specified in the <a href=""https://alt.qcri.org/semeval2016/task5/"" rel=""nofollow noreferrer"">2014 Semantic Evaluation Workshop (Task 5)</a> and its updated versions in the following years.</p>
<p>Overview of model efficiencies over the years:</p>
<p><a href=""https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval"" rel=""nofollow noreferrer"">https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval</a></p>
<p>Good source for ressources and repositories on the topic (some are very advanced but there are some more starter friendly ressources in there too):</p>
<p><a href=""https://github.com/ZhengZixiang/ABSAPapers"" rel=""nofollow noreferrer"">https://github.com/ZhengZixiang/ABSAPapers</a></p>
<p>Just from my general experience in this topic a very powerful starting point that doesn't require advanced knowledge in machine learning model design is to prepare a Dataset (such as the one provided for the SemEval2014 Task) that is in a <a href=""https://huggingface.co/tasks/token-classification"" rel=""nofollow noreferrer"">Token Classification</a> Format and use it to fine-tune a pretrained transformer model such as BERT, RoBERTa or similar. Check out any tutorial on how to do fine-tuning on a token classification model like <a href=""https://huggingface.co/course/chapter7/2?fw=pt"" rel=""nofollow noreferrer"">this one in huggingface</a>. They usually use the popular task of Named Entity Recognition (NER) as the example task but for the ABSA-Task you basically do the same thing but with other labels and a different dataset.</p>
<p>Obviously an even easier approach would be to take more rule-based approaches or combine a rule-based approach with a trained sentiment analysis model/negation detection etc., but I think generally with a rule-based approach you can expect a much inferior performance compared to using state-of-the-art models as transformers.</p>
<p>If you want to go even more advanced than just fine-tuning the pretrained transformer models then check out the second and third link I provided and look at some of the machine learning model designs specifically designed for Aspect Based Sentiment Analysis.</p>
",0,0,829,2022-03-11 14:09:02,https://stackoverflow.com/questions/71439779/sentiment-analysis-is-there-a-way-to-extract-positive-and-negative-aspects-in-r
Azure Databricks Sentiment Analysis,"<p>I was following this tutorial regarding Databricks. In te final section, when calling the Language and Sentiment API, the sentiment column always return &quot;Couldn't Detect Language&quot;. I'm not familiar enough with Scala to solve this problem.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/databricks/scenarios/databricks-sentiment-analysis-cognitive-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/databricks/scenarios/databricks-sentiment-analysis-cognitive-services</a></p>
<p>This is the piece of code i'm running (The same as the tutorial):</p>
<pre><code>import java.io._
import java.net._
import java.util._

case class Language(documents: Array[LanguageDocuments], errors: Array[Any]) extends Serializable
case class LanguageDocuments(id: String, detectedLanguages: Array[DetectedLanguages]) extends Serializable
case class DetectedLanguages(name: String, iso6391Name: String, score: Double) extends Serializable

case class Sentiment(documents: Array[SentimentDocuments], errors: Array[Any]) extends Serializable
case class SentimentDocuments(id: String, score: Double) extends Serializable

case class RequestToTextApi(documents: Array[RequestToTextApiDocument]) extends Serializable
case class RequestToTextApiDocument(id: String, text: String, var language: String = &quot;&quot;) extends Serializable

import javax.net.ssl.HttpsURLConnection
import com.google.gson.Gson
import com.google.gson.GsonBuilder
import com.google.gson.JsonObject
import com.google.gson.JsonParser
import scala.util.parsing.json._

object SentimentDetector extends Serializable {

    // Cognitive Services API connection settings
    val accessKey = &quot;&lt;PROVIDE ACCESS KEY HERE&gt;&quot;
    val host = &quot;https://cognitive-docs.cognitiveservices.azure.com/&quot;
    val languagesPath = &quot;/text/analytics/v2.1/languages&quot;
    val sentimentPath = &quot;/text/analytics/v2.1/sentiment&quot;
    val languagesUrl = new URL(host+languagesPath)
    val sentimenUrl = new URL(host+sentimentPath)
    val g = new Gson

    def getConnection(path: URL): HttpsURLConnection = {
        val connection = path.openConnection().asInstanceOf[HttpsURLConnection]
        connection.setRequestMethod(&quot;POST&quot;)
        connection.setRequestProperty(&quot;Content-Type&quot;, &quot;text/json&quot;)
        connection.setRequestProperty(&quot;Ocp-Apim-Subscription-Key&quot;, accessKey)
        connection.setDoOutput(true)
        return connection
    }

    def prettify (json_text: String): String = {
        val parser = new JsonParser()
        val json = parser.parse(json_text).getAsJsonObject()
        val gson = new GsonBuilder().setPrettyPrinting().create()
        return gson.toJson(json)
    }

    // Handles the call to Cognitive Services API.
    def processUsingApi(request: RequestToTextApi, path: URL): String = {
        val requestToJson = g.toJson(request)
        val encoded_text = requestToJson.getBytes(&quot;UTF-8&quot;)
        val connection = getConnection(path)
        val wr = new DataOutputStream(connection.getOutputStream())
        wr.write(encoded_text, 0, encoded_text.length)
        wr.flush()
        wr.close()

        val response = new StringBuilder()
        val in = new BufferedReader(new InputStreamReader(connection.getInputStream()))
        var line = in.readLine()
        while (line != null) {
            response.append(line)
            line = in.readLine()
        }
        in.close()
        return response.toString()
    }

    // Calls the language API for specified documents.
    def getLanguage (inputDocs: RequestToTextApi): Option[Language] = {
        try {
            val response = processUsingApi(inputDocs, languagesUrl)
            // In case we need to log the json response somewhere
            val niceResponse = prettify(response)
            // Deserializing the JSON response from the API into Scala types
            val language = g.fromJson(niceResponse, classOf[Language])
            if (language.documents(0).detectedLanguages(0).iso6391Name == &quot;(Unknown)&quot;)
                return None
            return Some(language)
        } catch {
            case e: Exception =&gt; return None
        }
    }

    // Calls the sentiment API for specified documents. Needs a language field to be set for each of them.
    def getSentiment (inputDocs: RequestToTextApi): Option[Sentiment] = {
        try {
            val response = processUsingApi(inputDocs, sentimenUrl)
            val niceResponse = prettify(response)
            // Deserializing the JSON response from the API into Scala types
            val sentiment = g.fromJson(niceResponse, classOf[Sentiment])
            return Some(sentiment)
        } catch {
            case e: Exception =&gt; return None
        }
    }
}

// User Defined Function for processing content of messages to return their sentiment.
val toSentiment =
    udf((textContent: String) =&gt;
        {
            val inputObject = new RequestToTextApi(Array(new RequestToTextApiDocument(textContent, textContent)))
            val detectedLanguage = SentimentDetector.getLanguage(inputObject)
            detectedLanguage match {
                case Some(language) =&gt;
                    if(language.documents.size &gt; 0) {
                        inputObject.documents(0).language = language.documents(0).detectedLanguages(0).iso6391Name
                        val sentimentDetected = SentimentDetector.getSentiment(inputObject)
                        sentimentDetected match {
                            case Some(sentiment) =&gt; {
                                if(sentiment.documents.size &gt; 0) {
                                    sentiment.documents(0).score.toString()
                                }
                                else {
                                    &quot;Error happened when getting sentiment: &quot; + sentiment.errors(0).toString
                                }
                            }
                            case None =&gt; &quot;Couldn't detect sentiment&quot;
                        }
                    }
                    else {
                        &quot;Error happened when getting language&quot; + language.errors(0).toString
                    }
                case None =&gt; &quot;Couldn't detect language&quot;
            }
        }
    )


// Prepare a dataframe with Content and Sentiment columns
val streamingDataFrame = incomingStream.selectExpr(&quot;cast (body as string) AS Content&quot;).withColumn(&quot;Sentiment&quot;, toSentiment($&quot;Content&quot;))

// Display the streaming data with the sentiment
streamingDataFrame.writeStream.outputMode(&quot;append&quot;).format(&quot;console&quot;).option(&quot;truncate&quot;, false).start().awaitTermination()
</code></pre>
<p>Any thoughts?</p>
<p>Edited, 15/03/2022</p>
","azure, azure-databricks, sentiment-analysis, azure-cognitive-services","<p>Ok, error found. I was missing a character in the Cognitive Service's Endpoint.</p>
",0,0,297,2022-03-14 16:23:17,https://stackoverflow.com/questions/71471054/azure-databricks-sentiment-analysis
How to calculate accuracy of a sentiment analysis algorithm (Naive Bayes),"<p>I'm currently working on a Naive Bayes sentiment analysis program but I'm not quite sure how to determine it's accuracy. My code is:</p>
<pre><code>x = df[&quot;Text&quot;]
y = df[&quot;Mood&quot;]

test_size = 1785
x_train = x[:-test_size]
y_train = y[:-test_size]

x_test = x[-test_size:]
y_test = y[-test_size:]

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(x_train)
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
clf = MultinomialNB().fit(X_train_tfidf, y_train)

print(clf.predict(count_vect.transform([&quot;Random text&quot;])))
</code></pre>
<p>The prediction works just fine for a sentence that I give it, however I want to run it on 20% from my database (x_test and y_test) and calculate the accuracy. I'm not quite sure how to approach this. Any help would be appreciated.</p>
<p>I've also tried the following:</p>
<pre><code>predictions = clf.predict(x_test)

print(accuracy_score(y_test, predictions))
</code></pre>
<p>Which gives me the following error:</p>
<pre><code>ValueError: could not convert string to float: &quot;A sentence from the dataset&quot;
</code></pre>
","machine-learning, sentiment-analysis, naivebayes","<p>before usiing predictions = clf.predict(x_test) please convert the test set also to numeric</p>
<pre><code>x_test = count_vect.transform(x_test).toarray()
</code></pre>
<p>you can find step by step to do this<a href=""https://www.analyticsvidhya.com/blog/2021/07/performing-sentiment-analysis-with-naive-bayes-classifier/"" rel=""nofollow noreferrer""> [here]</a></p>
",0,0,260,2022-03-22 22:04:25,https://stackoverflow.com/questions/71579475/how-to-calculate-accuracy-of-a-sentiment-analysis-algorithm-naive-bayes
getting increase in val-loss and decrease in val-accuracy while running a deep learning model for test classification,"<p>I am trying to classify text with label 0,1 and doing it with Bi-lstm. Its giving me a bit good accuracy on training time but when it comes to validation the loss goes to increase and validation accuracy tends to decrease.. please suggest me some solution how I can I improve it.
shape of data: (1043708, 2)</p>
<p>here is my model</p>
<pre><code>model=tf.keras.Sequential([
    # add an embedding layer
    tf.keras.layers.Embedding(word_count, 16, input_length=max_len),
     # add dropout layer to prevent overfitting
    tf.keras.layers.Dropout(0.2),
    # add the bi-lstm layer
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),
    # add a dense layer
    tf.keras.layers.Dense(32, activation=tf.keras.activations.relu),
    tf.keras.layers.Dense(32, activation=tf.keras.activations.relu),
    tf.keras.layers.Dense(32, activation=tf.keras.activations.softmax),
    # add the prediction layer
    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid),
])

model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])

model.summary()
history = model.fit(XPAD_train, Y_train, validation_data=(XPAD_test, Y_test), epochs = 10, batch_size=batch_size, callbacks = [callback_func], verbose=1)
</code></pre>
","python, tensorflow, nlp, sentiment-analysis","<p>As you say in the comment if you want cross-validation. You can use <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn-model-selection-kfold"" rel=""nofollow noreferrer""><code>sklearn.model_selection.KFold</code></a> like below and train you model on each X_train, y_train, X_test, y_test like below:</p>
<pre><code>from sklearn.model_selection import KFold
import numpy as np
X = np.array([&quot;I'm good.&quot;, &quot;I'm very good&quot;, &quot;I'm bad&quot;, &quot;I'm very bad&quot;])
y = np.array([&quot;pos&quot;, &quot;pos&quot;, &quot;neg&quot;, &quot;neg&quot;])
k_fold = KFold(n_splits=4)
for train_idx, test_idx in k_fold.split(X):
    print(f'Train_idx: {train_idx} | Test_idx: {test_idx}')
    print(f'X_train : {X[train_idx]}')
    print(f'y_train : {y[train_idx]}')
    print(f'X_test  : {X[test_idx]}')
    print(f'y_test  : {y[test_idx]}')
    print()
</code></pre>
<p>Output:</p>
<pre><code>Train_idx: [1 2 3] | Test_idx: [0]
X_train : [&quot;I'm very good&quot; &quot;I'm bad&quot; &quot;I'm very bad&quot;]
y_train : ['pos' 'neg' 'neg']
X_test  : [&quot;I'm good.&quot;]
y_test  : ['pos']

Train_idx: [0 2 3] | Test_idx: [1]
X_train : [&quot;I'm good.&quot; &quot;I'm bad&quot; &quot;I'm very bad&quot;]
y_train : ['pos' 'neg' 'neg']
X_test  : [&quot;I'm very good&quot;]
y_test  : ['pos']

Train_idx: [0 1 3] | Test_idx: [2]
X_train : [&quot;I'm good.&quot; &quot;I'm very good&quot; &quot;I'm very bad&quot;]
y_train : ['pos' 'pos' 'neg']
X_test  : [&quot;I'm bad&quot;]
y_test  : ['neg']

Train_idx: [0 1 2] | Test_idx: [3]
X_train : [&quot;I'm good.&quot; &quot;I'm very good&quot; &quot;I'm bad&quot;]
y_train : ['pos' 'pos' 'neg']
X_test  : [&quot;I'm very bad&quot;]
y_test  : ['neg']
</code></pre>
",0,1,301,2022-03-31 17:14:05,https://stackoverflow.com/questions/71696697/getting-increase-in-val-loss-and-decrease-in-val-accuracy-while-running-a-deep-l
Pandas - Keyword count by Category,"<p>I am trying to get a count of the most occurring words in my df, grouped by another Columns values:</p>
<p>I have a dataframe like so:</p>
<pre><code>df=pd.DataFrame({'Category':['Red','Red','Blue','Yellow','Blue'],'Text':['this is very good ','good','dont like','stop','dont like']})
</code></pre>
<p><img src=""https://i.sstatic.net/7seVM.png"" alt=""enter image description here"" /></p>
<p>This is the way that I have counted the keywords in the Text column:</p>
<pre><code>from collections import Counter

top_N = 100


stopwords = nltk.corpus.stopwords.words('english')
# # RegEx for stopwords
RE_stopwords = r'\b(?:{})\b'.format('|'.join(stopwords))
# replace '|'--&gt;' ' and drop all stopwords
words = (df.Text
           .str.lower()
           .replace([r'\|', RE_stopwords], [' ', ''], regex=True)
           .str.cat(sep=' ')
           .split()
)

# generate DF out of Counter
df_top_words = pd.DataFrame(Counter(words).most_common(top_N),
                    columns=['Word', 'Frequency']).set_index('Word')
print(df_top_words)

</code></pre>
<p>Which produces this result:</p>
<p><a href=""https://i.sstatic.net/86ONi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/86ONi.png"" alt="""" /></a></p>
<p>However this just generates a list of all of the words in the data frame, what I am after is something along the lines of this:</p>
<p><a href=""https://i.sstatic.net/5ao3V.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5ao3V.png"" alt="""" /></a></p>
","python, pandas, nltk, sentiment-analysis","<p>Your <code>words</code> statement finds the words that you care about (removing stopwords) in the text of the whole column. We can change that a bit to apply the replacement on each row instead:</p>
<pre><code>df[&quot;Text&quot;] = (
    df[&quot;Text&quot;]
    .str.lower()
    .replace([r'\|', RE_stopwords], [' ', ''], regex=True)
    .str.strip()
    # .str.cat(sep=' ')
    .str.split()  # Previously .split()
)
</code></pre>
<p>Resulting in:</p>
<pre><code>  Category          Text
0      Red        [good]
1      Red        [good]
2     Blue  [dont, like]
3   Yellow        [stop]
4     Blue  [dont, like]
</code></pre>
<p>Now, we can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html"" rel=""nofollow noreferrer""><code>.explode</code></a> and then <code>.groupby</code> and <code>.size</code> to expand each list element to its own row and then count how many times does a word appear in the text of each (original) row:</p>
<pre><code>df.explode(&quot;Text&quot;).groupby([&quot;Category&quot;, &quot;Text&quot;]).size()
</code></pre>
<p>Resulting in:</p>
<pre><code>Category  Text
Blue      dont    2
          like    2
Red       good    2
Yellow    stop    1
</code></pre>
<p>Now, this does not match your output sample because in that sample you're not applying the <code>.replace</code> step from the original <code>words</code> statement (now used to calculate the new value of the &quot;Text&quot; column). If you wanted that result, you just have to comment out that <code>.replace</code> line (but I guess that's the whole point of this question)</p>
",1,2,444,2022-04-04 12:33:31,https://stackoverflow.com/questions/71737328/pandas-keyword-count-by-category
Extract Both Negation &amp; 3 Following Words ( Python/DataFrame),"<p>I'm currently trying to extract both the negation word and 3 words following the negation word.</p>
<p>i.e.)</p>
<p>&quot;I don't want to visit again. no sympathy.&quot; (from a column called ReviewText2)</p>
<p>what I want: [don't want to visit, no sympathy.]</p>
<p>what I get: [don't, no]</p>
<p>I used the following code, but I don't know how to tweak it to include the followings words as well.</p>
<pre><code>Negative_Reviews['ReviewText2'] =   Negative_Reviews['Review Text'].str.lower()   

keywords = [&quot;doesn't&quot;,&quot;don't&quot;,&quot;without&quot;,&quot;won't&quot;,&quot;not&quot;,&quot;never&quot;,&quot;no&quot;,&quot;wasn't&quot;,&quot;isn't&quot;,&quot;can't&quot;,&quot;shouldn't&quot;,&quot;wouldn't&quot;,&quot;couldn't&quot;,&quot;nobody&quot;,&quot;nothing&quot;,&quot;neighter&quot;,&quot;nowhere&quot;]

query = '|'.join(keywords)
Negative_Reviews['negation'] = Negative_Reviews['ReviewText2'] .str.findall(r'\b({})\b'.format(query))

</code></pre>
<p>I really appreciate your help!</p>
","python, regex, text, nlp, sentiment-analysis","<p>You can use</p>
<pre class=""lang-py prettyprint-override""><code>rx = r'\b(?:{})\b(?:\s+\w+){{0,3}}'.format(query)
Negative_Reviews['negation'] = Negative_Reviews['ReviewText2'].str.findall(rx)
</code></pre>
<p>The regex will look like</p>
<pre class=""lang-none prettyprint-override""><code>\b(?:doesn't|don't|without|won't|not|never|no|wasn't|isn't|can't|shouldn't|wouldn't|couldn't|nobody|nothing|neighter|nowhere)\b(?:\s+\w+){0,3}
</code></pre>
<p><em>Details</em>:</p>
<ul>
<li><code>\b</code> -a word boundary</li>
<li><code>(?:doesn't|don't|without|won't|not|never|no|wasn't|isn't|can't|shouldn't|wouldn't|couldn't|nobody|nothing|neighter|nowhere)</code> - one of the keywords</li>
<li><code>\b</code> -a word boundary</li>
<li><code>(?:\s+\w+){0,3}</code> - zero to three occurrences of one or more whitespaces and one or more word chars.</li>
</ul>
",0,2,98,2022-04-06 21:28:38,https://stackoverflow.com/questions/71773818/extract-both-negation-3-following-words-python-dataframe
"Can the f1, precision, accuracy and recall all have the same values?","<p>I've been trying to implement a support vector machine algorithm using scikit-learn and after doing some measurements all the scores provide the same values.</p>
<pre><code>x = df[&quot;Text&quot;]
y = df[&quot;Mood&quot;]

test_size = 5122

x_test = x[:-test_size]
y_test = y[:-test_size]

x_train = x[-test_size:]
y_train = y[-test_size:]

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(x_train)
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
x_test = count_vect.transform(x_test).toarray()

SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')
SVM.fit(X_train_tfidf, y_train)
predictions_SVM = SVM.predict(x_test)

print('Accuracy score is: ', accuracy_score(y_test, predictions_SVM))
print('F1 score is: ', f1_score(y_test, predictions_SVM, average='micro'))
print('Precission score is: ', precision_score(y_test, predictions_SVM, average ='micro'))
print('Recall score is: ', recall_score(y_test, predictions_SVM, average='micro'))
</code></pre>
<p>Output:</p>
<pre><code>Accuracy score is:  0.9687622022647403
F1 score is:  0.9687622022647403
Precission score is:  0.9687622022647403
Recall score is:  0.9687622022647403
</code></pre>
<p>Is this normal or have I made an error somewhere?</p>
","python, machine-learning, scikit-learn, svm, sentiment-analysis","<p>Looking at the documentation for these scores, it appears like they should all come out the same when you are using 'micro'.</p>
<p>They are all counting the fraction of times that you get the correct label.</p>
<p>See the examples:</p>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html</a></p>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html</a></p>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html</a></p>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html</a></p>
<p>In fact in the last three they all give the same example and of course get the same score.</p>
",4,1,3467,2022-04-08 14:57:41,https://stackoverflow.com/questions/71799168/can-the-f1-precision-accuracy-and-recall-all-have-the-same-values
Iterate Naive Bayes classifier over a list of strings,"<p>This is an NLP question that hopefully someone can help me with. Specifically trying to do sentiment analysis.</p>
<p>I have a Naive Bayes classifier that has been trained on the well-known data set of tweets that are labeled as either positive or negative:</p>
<pre><code>#convert tokens to a dictionary for NB classifier:
def get_tweets_for_model(cleaned_tokens_list):
    for tweet_tokens in cleaned_tokens_list:
        yield dict([token, True] for token in tweet_tokens)
    
pos_model_tokens = get_tweets_for_model(pos_clean_token)
neg_model_tokens = get_tweets_for_model(neg_clean_token)

#prepare training data
positive_dataset = [(tweet_dict, &quot;Positive&quot;)
                    for tweet_dict in pos_model_tokens]
negative_dataset = [(tweet_dict, &quot;Negative&quot;)
                    for tweet_dict in neg_model_tokens]

dataset = positive_dataset + negative_dataset

#shuffle so all positive tweets aren't first
random.shuffle(dataset) 

#set apart 7000 for training, 3000 for testing
train_data = dataset[:7000]  
test_data = dataset[7000:]

#train model
classifier = NaiveBayesClassifier.train(train_data)
</code></pre>
<p>Using this model, I want to iterate through a list of test data and increase a tally for each token whether it gets classified as positive or negative. The test data is a list of strings, which are taken from a data set of text messages.</p>
<pre><code>print(messages[-5:])
&gt;&gt;&gt;[&quot;I'm outside, waiting.&quot;, 'Have a great day :) See you soon!', &quot;I'll be at work so I can't make it, sry!&quot;, 'Are you doing anything this weekend?', 'Thanks for dropping that stuff off :)']
</code></pre>
<p>I can get the classification of a single message:</p>
<pre><code>print(classifier.classify(dict([message, True] for message in 
messages[65])))
&gt;&gt;&gt;&gt;Positive
</code></pre>
<p>I can return the boolean value of a classification being negative or positive:</p>
<pre><code>neg = (classifier.classify(dict([message, True] for message in messages[65])) == &quot;Negative&quot;)
</code></pre>
<p>That message in positive, so <code>neg</code> is set to <code>False</code>. So I want iterate over all the messages in the list of messages, and increase the tally of the positive counter if it's positive, and increase the tally of the negative counter if it's negative. But my attempts to do so either increase the positive counter by 1 only, or increase the positive counter only for the entire set of tokens, even though the classifier does return &quot;Negative&quot; on individual tokens. Here's what I tried:</p>
<pre><code>positive_tally = 0
negative_tally = 0

#increments positive_tally by 1
if (classifier.classify(dict([message, True] for message in messages)) == &quot;Positive&quot;) == True:
    positive_tally += 1
else:
    negative_tally += 1

#increments positive_tally by 3749 (length of messages list)
for token in tokens:
    if (classifier.classify(dict([message, True] for message in 
messages)) == &quot;Positive&quot;) == True:
        positive_tally += 1
    else:
        negative_tally += 1
</code></pre>
<p>Any ideas on this one? I'd really appreciate it. I can provide more info if needed.</p>
","python, nltk, sentiment-analysis, naivebayes","<p>Okay I got it, posting for posterity in case anyone else gets stuck on a similar problem.</p>
<p>Basically the classifier takes a string and evaluates each word in the string to make a classification. But I wanted to iterative over a list of strings. So instead of what I had been trying...</p>
<pre><code>#didn't get what I wanted
for message in messages:
    if (classifier.classify(dict([message, True] for message in messages))) == &quot;Positive&quot;:
        positive_tally += 1
    else: negative_tally += 1
</code></pre>
<p>...which tries (and fails) to classify each message i.e. the entire string, I had to ensure that it was checking each word within each message:</p>
<pre><code>#works and increases tally as desired!
for message in messages:
    if classifier.classify(dict([token, True] for token in message)) == &quot;Positive&quot;:
        us_pos_tally += 1
    else:
        us_neg_tally += 1
</code></pre>
<p>So you go from list level to string level in <code>for message in messages</code> and then string level to word level inside the call of the classifier: <code>dict([token, True] for token in message</code>.</p>
",0,1,219,2022-04-10 01:08:59,https://stackoverflow.com/questions/71813050/iterate-naive-bayes-classifier-over-a-list-of-strings
how to pass user defined string in tweet cursor search,"<p>Q)how to pass user defined string in tweet_cursor search i am trying to get tweets as per the user by taking input in a and passing variable a please help</p>
<p>currently it is searching for only a  literally instead of variable a
defined by user</p>
<p>`import textblob
import pandas as pd
import matplotlib.pyplot as plt
import re</p>
<pre><code>api_key = 'xxxxxxxxxxxx'
api_key_secret = 'xxxxxxxxxxx'
access_token = 'xxxxxxxxxxx'
access_token_secret = 'xxxxxxxxxxxxxxxxxxx'

authenticator = tweepy.OAuthHandler(api_key, api_key_secret)
authenticator.set_access_token(access_token, access_token_secret)

api = tweepy.API(authenticator, wait_on_rate_limit=True)

a=input(print(&quot;enter player name&quot;))

search= f'#(a) -filter:retweets lang:en'

tweet_cursor = tweepy.Cursor(api.search_tweets, q=search, tweet_mode='extended').items(100)

tweets = [tweet.full_text for tweet in tweet_cursor]

tweets_df = pd.DataFrame(tweets, columns=['Tweets'])

for _, row in tweets_df.iterrows():
    row['tweets'] = re.sub('https\S+', '', row['Tweets'])
    row['tweets'] = re.sub('#\S+', '', row['Tweets'])
    row['tweets'] = re.sub('@\S+', '', row['Tweets'])
    row['tweets'] = re.sub('\\n', '', row['Tweets'])
print(tweets_df)
print(tweets)


tweets_df['Polarity'] = tweets_df['Tweets'].map(lambda tweet: textblob.TextBlob(tweet).sentiment.polarity)
tweets_df['Result'] = tweets_df['Polarity'].map(lambda pol: '+' if pol &gt; 0 else '-')

positive = tweets_df[tweets_df.Result == '+'].count()['Tweets']
negative = tweets_df[tweets_df.Result == '-'].count()['Tweets']

print(positive)
print(negative)
langs = ['Positive', 'Negative']
students = [positive,negative]
a=plt.bar(langs,students)
a[0].set_color('g')
a[1].set_color('r')

plt.xlabel(&quot;Tweet Sentiment&quot;)
plt.ylabel(&quot;No. of Tweets&quot;)
plt.title(&quot;Sentiment Analysis&quot;)
plt.legend
</code></pre>
<p>plt.show()
`</p>
","python, string, twitter, tweepy, sentiment-analysis","<p>In Python f-Strings, you have to use curly braces around the variable.</p>
<pre class=""lang-py prettyprint-override""><code>search = f'#{a} -filter:retweets lang:en
</code></pre>
<p>But this will search for retweets containing the <strong>hashtag</strong> <code>a</code>.</p>
<p>If you want to search from the tweets of the <strong>user</strong> <code>a</code>, you should use:</p>
<pre class=""lang-py prettyprint-override""><code>search = f'from:{a} -filter:retweets lang:en
</code></pre>
<p>Please see the <a href=""https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/guides/standard-operators"" rel=""nofollow noreferrer"">Twitter API documentation</a> for all the search operators.</p>
",0,1,44,2022-04-15 12:14:23,https://stackoverflow.com/questions/71883919/how-to-pass-user-defined-string-in-tweet-cursor-search
How do I replace something that is on an append,"<p>I have to replace some words that are on a txt file, that I am reading. I separate them using an append, <code>[0]</code> is for the word that determinate the sentiment and <code>[1]</code> is for the rest of the text.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code># Opens the corpus
f = open(""analise-sentimentos-2000-noticias.txt"", ""r"", encoding=""utf-8-sig"")
linhas = f.readlines()

corpus_textos = []
corpus_rotulos = []

# Goes through 2000 lines
for linha in linhas:

  # Separate text and label/category/emotion
  item = linha.split("";;"")

  corpus_rotulos.append(item[0])
  corpus_textos.append(item[1])</code></pre>
</div>
</div>
</p>
<p>And I need to replace the words in append <code>[0]</code>, how can I do that? Here is some of the text I'm working with:</p>
<p><a href=""https://i.sstatic.net/j8YSg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/j8YSg.png"" alt=""link here to visualize some of the text"" /></a></p>
<p>I've highlighted as yellow the words that I need to replace, for example, 'alegria' should be replaced with 'positivo'. And in purple is the lines I need to exclude (if the line starts with the emotion 'surpresa', I need to exclude that). How can I do that?</p>
<p>For reference, I'm working with sentiment analysis using Google Colab. The txt was uploaded to my drive.</p>
","python, replace, google-colaboratory, sentiment-analysis","<p>First check for <code>surpresa</code> to skip those lines. Then you can do the replacement in the category when appending to the list.</p>
<pre><code>for linha in linhas:
    # Separate text and label/category/emotion
    rotulo, texto = linha.split(&quot;;;&quot;)

    if rotulo != 'surpresa':
        corpus_rotulos.append(rotulo.replace('alegria', 'positivo'))
        corpus_textos.append(texto)
</code></pre>
",1,0,397,2022-04-15 16:07:09,https://stackoverflow.com/questions/71886322/how-do-i-replace-something-that-is-on-an-append
How to add multiple layers to an RNN module for sentiment analysis? Pytorch,"<p>I am trying to create a sentiment analysis model with Pytorch (newbie)</p>
<pre><code>import torch.nn as nn

class RNN(nn.Module):
 def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, dropout):
    super().__init__() #to call the functions in the superclass
    self.embedding = nn.Embedding(input_dim, embedding_dim) #Embedding layer to create dense vector instead of sparse matrix
    self.rnn = nn.RNN(embedding_dim, hidden_dim) 
    self.fc = nn.Linear(hidden_dim, output_dim)
    self.dropout = nn.Dropout(dropout)
    
def forward(self, text):
    embedded = self.embedding(text)
    output, hidden = self.rnn(embedded)   
    hidden = self.dropout(hidden[-1,:,:])
    nn.Sigmoid()
    return self.fc(hidden)
</code></pre>
<p>However, the accuracy is below 50% and I would like to add an extra layer, maybe another linear before feeding it to the last linear to get the prediction. What kind of layers can I add after the RNN and before the last Linear? and also what should I feed it with?
I have tried simply adding another</p>
<pre><code>output, hidden= self.fc(hidden)
</code></pre>
<p>but I get</p>
<blockquote>
<p>ValueError: too many values to unpack (expected 2)</p>
</blockquote>
<p>Which I believe is because the output of the previous layer with activation and dropout is different. The help is greatly appreciated.</p>
<p>Thanks</p>
","machine-learning, pytorch, sentiment-analysis","<p>You were very close, just change your forward call to:</p>
<pre><code>import torch.nn.functional as F
class model_RNN(nn.Module):
 def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, dropout):
    super().__init__() #to call the functions in the superclass
    self.embedding = nn.Embedding(input_dim, embedding_dim) #Embedding layer to create dense vector instead of sparse matrix
    self.rnn = nn.RNN(embedding_dim, hidden_dim) 
    self.hidden_fc = nn.Linear(hidden_dim,hidden_dim)
    self.out_fc = nn.Linear(hidden_dim, output_dim)
    self.dropout = nn.Dropout(dropout)
    
def forward(self, text):
    embedded = self.embedding(text)
    output, hidden = self.rnn(embedded)   
    hidden = self.dropout(hidden[-1,:,:])
    hidden = F.relu(torch.self.hidden_fc(hidden))
    return self.out_fc(hidden)
</code></pre>
<p>Just a note, calling nn.Sigmoid() won't do anything to your model output because it will just create a sigmoid layer but won't call it on your data. What you want is probably <code>torch.sigmoid(self.fc(hidden))</code>. Although I would say it's not recommended to use an output activation because some common loss functions require the raw logits. Make sure you apply the sigmoid after the model call in eval mode though!</p>
",0,0,814,2022-04-18 20:45:18,https://stackoverflow.com/questions/71916899/how-to-add-multiple-layers-to-an-rnn-module-for-sentiment-analysis-pytorch
Out of memory training CNN-LSTM with GPU in Jupyter notebook,"<p>Currently, I want to compile my hybrid CNN-LSTM model for sentiment analysis, but I got the following error</p>
<pre><code>OOM when allocating tensor with shape[9051,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]
</code></pre>
<p>This my GPU list, where I want to use the RTX one:</p>
<pre><code>[name: &quot;/device:CPU:0&quot;
 device_type: &quot;CPU&quot;
 memory_limit: 268435456
 locality {
 }
 incarnation: 13057500645716466504,
 name: &quot;/device:GPU:0&quot;
 device_type: &quot;GPU&quot;
 memory_limit: 44957696
 locality {
   bus_id: 1
   links {
   }
 }
 incarnation: 6095838710984840352
 physical_device_desc: &quot;device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:05:00.0, compute capability: 8.6&quot;,
 name: &quot;/device:GPU:1&quot;
 device_type: &quot;GPU&quot;
 memory_limit: 10648354816
 locality {
   bus_id: 1
   links {
   }
 }
 incarnation: 10826802477734196135
 physical_device_desc: &quot;device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1&quot;]
</code></pre>
<p>this is my code:</p>
<pre><code># Build hybrid CNN-LSTM model
def build_cnn_lstm_model(num_words, embedding_vector_size, embedding_matrix, max_sequence_length):
    # Input layer
    input_layer = Input(shape=(max_sequence_length,))

    # Word embedding
    embedding_layer = Embedding(input_dim=num_words,
                              output_dim=embedding_vector_size,
                              weights=[embedding_matrix],
                              input_length=max_sequence_length)(input_layer)

    # CNN model
    # Bigrams extraction
    bigrams_convolution_layer = Conv1D(filters=256,
                                     kernel_size=2,
                                     strides=1,
                                     padding='valid',
                                     activation='relu')(embedding_layer)
    bigrams_max_pooling_layer = MaxPooling1D(pool_size=2,
                                           strides=1,
                                           padding='valid')(bigrams_convolution_layer)

    # Trigrams extraction
    trigrams_convolution_layer = Conv1D(filters=256,
                                     kernel_size=3,
                                     strides=1,
                                     padding='valid',
                                     activation='relu')(bigrams_max_pooling_layer)
    trigrams_max_pooling_layer = MaxPooling1D(pool_size=2,
                                           strides=1,
                                           padding='valid')(trigrams_convolution_layer)

    # Fourgrams extraction
    fourgrams_convolution_layer = Conv1D(filters=256,
                                      kernel_size=4,
                                      strides=1,
                                      padding='valid',
                                      activation='relu')(trigrams_max_pooling_layer)
    fourgrams_max_pooling_layer = MaxPooling1D(pool_size=2,
                                            strides=1,
                                            padding='valid')(fourgrams_convolution_layer)

    # Fivegrams extraction
    fivegrams_convolution_layer = Conv1D(filters=256,
                                      kernel_size=5,
                                      strides=1,
                                      padding='valid',
                                      activation='relu')(fourgrams_max_pooling_layer)
    fivegrams_max_pooling_layer = MaxPooling1D(pool_size=2,
                                            strides=1,
                                            padding='valid')(fivegrams_convolution_layer)

    # Dropout layer
    dropout_layer = Dropout(rate=0.5)(bigrams_max_pooling_layer)

    # LSTM model
    lstm_layer = LSTM(units=128,
                      activation='tanh',
                      return_sequences=False,
                      dropout=0.3,
                      return_state=False)(dropout_layer)

    # Batch normalization layer
    batch_norm_layer = BatchNormalization()(lstm_layer)

    # Classifier model
    dense_layer = Dense(units=10, activation='relu') (lstm_layer)
    output_layer = Dense(units=3, activation='softmax')(dense_layer)

    cnn_lstm_model = Model(inputs=input_layer, outputs=output_layer)

    return cnn_lstm_model

with tf.device('/device:GPU:0'):
    sinovac_cnn_lstm_model = build_cnn_lstm_model(SINOVAC_NUM_WORDS, 
                                                  SINOVAC_EMBEDDING_VECTOR_SIZE,
                                                  SINOVAC_EMBEDDING_MATRIX,
                                                  SINOVAC_MAX_SEQUENCE)
    sinovac_cnn_lstm_model.summary()

    sinovac_cnn_lstm_model.compile(loss='categorical_crossentropy',
                                   optimizer=Adam(lr=0.001),
                                   metrics=['accuracy'])
</code></pre>
<p>Strangely, I used the GPU:1 that is the GTX one, it worked
The GTX 1080Ti one is obviously has fewer memory than the RTX A6000 one, but why it produced Out Of Memory error when compiled and trained with the RTX A6000 ?
Any solution?</p>
","python, keras, conv-neural-network, lstm, sentiment-analysis","<p>Even though the <code>physical_device_desc</code> calls it <code>device: 0</code>, it is the name under that, <code>name: &quot;/device:GPU:1&quot;</code> entry that is used.  Therefore even though the 1080Ti calls itself <code>device: 1</code> in the <code>physical_device_desc</code> field, it is actually `&quot;/device:GPU:0&quot;.</p>
<p>In other words, use <code>with tf.device('/device:GPU:0'):</code> to use the 1080Ti, and <code>with tf.device('/device:GPU:1'):</code> to get the A6000.</p>
<p>That sounds potentially fragile, but I just had a poke around in Tensorflow docs, and there seems no built-in function to identify a GPU by model name. So you'd need to run through the list of devices, and match against that physical device name (or simply find the the one with most memory) to get the &quot;GPU:nnn&quot; name you need.</p>
",1,0,354,2022-04-20 11:17:14,https://stackoverflow.com/questions/71938687/out-of-memory-training-cnn-lstm-with-gpu-in-jupyter-notebook
Remove Stop words from multi-lingual Text,"<p>I am running textual and sentiment analysis on multi-lingual text files from the healthcare sector, and I want to remove stopwords from all the languages at once. I don't want to write the name of every language in the code to remove the stopwords. Is there any way I can do it fast?</p>
<p>Here is my code: The total number of files is 596</p>
<pre><code>files = list.files(path = getwd(), pattern = &quot;txt&quot;, all.files = FALSE,
                   full.names = TRUE, recursive = TRUE)
txt = {}
for (i in 1:596) 
  try( 
    {
      txt[[i]] &lt;- readLines(files[i], warn = FALSE) 
  
  filename &lt;- txt[[i]]
  filename &lt;- trimws(filename)
  corpus &lt;- iconv(filename, to = &quot;utf-8&quot;)
  corpus &lt;- Corpus(VectorSource(corpus))
  
  # Clean Text
  corpus &lt;- tm_map(corpus, removePunctuation)
  corpus &lt;- tm_map(corpus, removeNumbers)
  cleanset &lt;- tm_map(corpus, removeWords, stopwords(&quot;english&quot;))
  cleanset &lt;- tm_map(cleanset, removeWords, stopwords(&quot;spanish&quot;))
  cleanset &lt;- tm_map(cleanset, content_transformer(tolower))
  cleanset &lt;- tm_map(cleanset, stripWhitespace)
  
  # Remove spaces and newlines
  cleanset &lt;- tm_map(&quot;\n&quot;, &quot; &quot;, cleanset)
  cleanset &lt;- tm_map(&quot;^\\s+&quot;, &quot;&quot;, cleanset)
  cleanset &lt;- tm_map(&quot;\\s+$&quot;, &quot;&quot;, cleanset)
  cleanset &lt;- tm_map(&quot;[ |\t]+&quot;, &quot; &quot;, cleanset)

  }, silent = TRUE) 
</code></pre>
","r, nlp, sentiment-analysis, stop-words","<blockquote>
<p>I want to remove stopwords from all the languages at once.</p>
</blockquote>
<p>Merge the results of each <code>stopwords(cc)</code> call, and pass that to a single <code>tm_map(corpus, removeWords, allStopwords)</code> call.</p>
<blockquote>
<p>I don't want to write the name of every language in the code to remove the stopwords</p>
</blockquote>
<p>You could use <code>stopwords_getlanguages()</code> to get a list of all the supported languages, and do it as a loop.  See an example at <a href=""https://www.rdocumentation.org/packages/stopwords/versions/2.3"" rel=""nofollow noreferrer"">https://www.rdocumentation.org/packages/stopwords/versions/2.3</a></p>
<p>For what its worth, I think this (using stopwords of all languages) is a bad idea. What is a stop word in one language could be a high information word in another language. E.g. just skimming <a href=""https://github.com/stopwords-iso/stopwords-es/blob/master/stopwords-es.txt"" rel=""nofollow noreferrer"">https://github.com/stopwords-iso/stopwords-es/blob/master/stopwords-es.txt</a> I spotted &quot;embargo&quot;, &quot;final&quot;, &quot;mayor&quot;, &quot;salvo&quot;, &quot;sea&quot;, which are not in the English stopword list, and could carry information.</p>
<p>Of course it depends on what you are doing with the data once all these words have been stripped out.</p>
<p>But if something like searching for drug names, or other keywords, just do that on the original data, without removing stopwords.</p>
",2,3,948,2022-04-23 02:03:57,https://stackoverflow.com/questions/71976248/remove-stop-words-from-multi-lingual-text
"`logits` and `labels` must have the same shape, received ((None, 512, 768) vs (None, 1)) when using transformers","<p>I get the next error when im trying to fine tuning a bert model to predict sentiment analysis.</p>
<p>Im using as input:
X-A list of strings that contains tweets
y-a numeric list (0 - negative, 1 - positive)</p>
<p>I am trying to fine tuning a bert model to predict sentiment analysis but i always get the same error in logits and labels when im trying to fit the model. I load a pretrained model and then build the dataset but when i am trying to fit it, it is impossible.</p>
<p>The text used as input is a list of strings made of tweets and the labels used as input are a list of categories (negative and positive) but transformed to 0 and 1.</p>
<pre><code>from sklearn.preprocessing import MultiLabelBinarizer

#LOAD MODEL

hugging_face_model = 'distilbert-base-uncased-finetuned-sst-2-english'
batches = 32
epochs = 1 

tokenizer = BertTokenizer.from_pretrained(hugging_face_model)
model = TFBertModel.from_pretrained(hugging_face_model, num_labels=2)

#PREPARE THE DATASET

#create a list of strings (tweets)


lst = list(X_train_lower['lower_text'].values) 
encoded_input  = tokenizer(lst, truncation=True, padding=True, return_tensors='tf')

y_train['sentimentNumber'] = y_train['sentiment'].replace({'negative': 0, 'positive': 1})
label_list = list(y_train['sentimentNumber'].values) 

#CREATE DATASET

train_dataset = tf.data.Dataset.from_tensor_slices((dict(encoded_input), label_list))

#COMPILE AND FIT THE MODEL

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss=BinaryCrossentropy(from_logits=True),metrics=[&quot;accuracy&quot;])
model.fit(train_dataset.shuffle(len(df)).batch(batches),epochs=epochs,batch_size=batches) ```




ValueError                                Traceback (most recent call last)
&lt;ipython-input-158-e5b63f982311&gt; in &lt;module&gt;()
----&gt; 1 model.fit(train_dataset.shuffle(len(df)).batch(batches),epochs=epochs,batch_size=batches)

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1145           except Exception as e:  # pylint:disable=broad-except
   1146             if hasattr(e, &quot;ag_error_metadata&quot;):
-&gt; 1147               raise e.ag_error_metadata.to_exception(e)
   1148             else:
   1149               raise

ValueError: in user code:

    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1021, in train_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1010, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 1000, in run_step  **
        outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py&quot;, line 1000, in train_step
        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py&quot;, line 201, in __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/losses.py&quot;, line 141, in __call__
        losses = call_fn(y_true, y_pred)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/losses.py&quot;, line 245, in call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/losses.py&quot;, line 1932, in binary_crossentropy
        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),
    File &quot;/usr/local/lib/python3.7/dist-packages/keras/backend.py&quot;, line 5247, in binary_crossentropy
        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)

    ValueError: `logits` and `labels` must have the same shape, received ((None, 512, 768) vs (None, 1)).
</code></pre>
","tensorflow, machine-learning, keras, sentiment-analysis, bert-language-model","<p>As described in this <a href=""https://www.kaggle.com/code/dhruv1234/huggingface-tfbertmodel/notebook"" rel=""nofollow noreferrer"">kaggle notebook</a>, you must  build a custom Keras Model around the pre-trained BERT model to perform classification,</p>
<blockquote>
<p>The bare Bert Model transformer outputing raw hidden-states without
any specific head on top</p>
</blockquote>
<p>Here is a copy of a piece of code:</p>
<pre><code>def create_model(bert_model):
  input_ids = tf.keras.Input(shape=(60,),dtype='int32')
  attention_masks = tf.keras.Input(shape=(60,),dtype='int32')
  
  output = bert_model([input_ids,attention_masks])
  output = output[1]
  output = tf.keras.layers.Dense(32,activation='relu')(output)
  output = tf.keras.layers.Dropout(0.2)(output)

  output = tf.keras.layers.Dense(1,activation='sigmoid')(output)
  model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)
  model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])
  return model
</code></pre>
<p>Note: you might have to adapt this code and in particular modify the Input shape (60 to <strong>512</strong> seemingly from the error message, your tokenizer maximum length)</p>
<p>Load BERT model and build the classifier :</p>
<pre><code>from transformers import TFBertModel
bert_model = TFBertModel.from_pretrained(hugging_face_model)
model = create_model(bert_model)
model.summary()
</code></pre>
<p>Summary:</p>
<pre><code>Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 60)]         0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 60)]         0           []                               
                                                                                                  
 tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_1[0][0]',                
                                thPoolingAndCrossAt               'input_2[0][0]']                
                                tentions(last_hidde                                               
                                n_state=(None, 60,                                                
                                768),                                                             
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dense (Dense)                  (None, 32)           24608       ['tf_bert_model_1[0][1]']        
                                                                                                  
 dropout_74 (Dropout)           (None, 32)           0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 1)            33          ['dropout_74[0][0]']             
                                                                                                  
==================================================================================================
Total params: 109,506,881
Trainable params: 109,506,881
Non-trainable params: 0
</code></pre>
",0,0,598,2022-05-18 12:02:15,https://stackoverflow.com/questions/72288839/logits-and-labels-must-have-the-same-shape-received-none-512-768-vs-n
Huggingface Load_dataset() function throws &quot;ValueError: Couldn&#39;t cast&quot;,"<p>My goal is to train a classifier able to do sentiment analysis in Slovak language using loaded SlovakBert model and HuggingFace library. Code is executed on Google Colaboratory.</p>
<p>My test dataset is read from this csv file:
<a href=""https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_games.csv"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_games.csv</a></p>
<p>and train dataset:
<a href=""https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_accomodation.csv"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_accomodation.csv</a></p>
<p>Data has two columns: column of Slovak sentences and 2nd column of labels which indicate sentiment of the sentence. Labels have values -1, 0 or 1.</p>
<p>Load_dataset() function throws this error:</p>
<blockquote>
<p>ValueError: Couldn't cast
Vrtuľník je veľmi zraniteľný pri dobre mierenej streľbe zo zeme. Brániť sa, unikať, alebo vedieť zneškodniť nepriateľa je vecou sekúnd, ak nie stotín, kedy ide život. : string
-1: int64
-- schema metadata --
pandas: '{&quot;index_columns&quot;: [{&quot;kind&quot;: &quot;range&quot;, &quot;name&quot;: null, &quot;start&quot;: 0, &quot;' + 954
to
{'Priestorovo a vybavenim OK.': Value(dtype='string', id=None), '1': Value(dtype='int64', id=None)}
because column names don't match</p>
</blockquote>
<p>Code:</p>
<pre><code>!pip install transformers==4.10.0 -qqq
!pip install datasets -qqq

from re import M
import numpy as np
from datasets import load_metric, load_dataset, Dataset
from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding
import pandas as pd
from textblob import TextBlob
from textblob.sentiments import NaiveBayesAnalyzer

#links to dataset
test = 'https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_games.csv'
train = 'https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_accomodation.csv'


model_name = 'gerulata/slovakbert'


#Load data
dataset = load_dataset('csv', data_files={'train': train, 'test': test})
</code></pre>
<p>What is done wrong while loading the dataset?</p>
","machine-learning, nlp, sentiment-analysis, huggingface-tokenizers, huggingface","<p>The reason is since delimiter is used in first column multiple times the code fails to automatically determine number of columns ( some time segment a sentence into multiple columns as it cannot automatically determine <code>,</code> is a delimiter or a part of sentence.</p>
<p>But, the solution is simple: (just add column names)</p>
<pre class=""lang-py prettyprint-override""><code>dataset = load_dataset('csv', data_files={'train': train,'test':test},column_names=['sentence','label'])
</code></pre>
<p>output:</p>
<pre class=""lang-py prettyprint-override""><code>DatasetDict({
    train: Dataset({
        features: ['sentence', 'label'],
        num_rows: 89
    })
    test: Dataset({
        features: ['sentence', 'label'],
        num_rows: 91
    })
})
</code></pre>
",4,2,9813,2022-05-22 19:43:27,https://stackoverflow.com/questions/72340801/huggingface-load-dataset-function-throws-valueerror-couldnt-cast
Inbalanced Dataset for Classification Report?,"<p>I am trying to classify a model to deduce sentiment from text. My two labels are &quot;1&quot; for positive, and &quot;0&quot; for negative. When classification report is ran it produces this output:</p>
<pre><code>            precision    recall  f1-score   support

           0       0.39      1.00      0.57      1081
           1       0.00      0.00      0.00      1660

    accuracy                           0.39      2741
   macro avg       0.20      0.50      0.28      2741
weighted avg       0.16      0.39      0.22      2741
</code></pre>
<p>So by the looks of it, it doesn't seem to classify label 1. Looking at other Stack Overflow posts I thought it was an unbalanced dataset problem but it doesn't seem the case. To my understanding there seems to be more data for label 1 than label 0 so I am quite confused as to the issue here.</p>
<p>Below are the relevant code snippets</p>
<pre><code>import time
#Import the DecisionTreeeClassifier
from sklearn.tree import DecisionTreeClassifier
# Load from the filename
word2vec_df = pd.read_csv(word2vec_filename)
#Initialize the model
clf_decision_word2vec = DecisionTreeClassifier()

start_time = time.time()
# Fit the model
clf_decision_word2vec.fit(word2vec_df, Y_train['Sentiment'])
print(&quot;Time taken to fit the model with word2vec vectors: &quot; + str(time.time() - start_time))
</code></pre>
<pre><code>from sklearn.metrics import classification_report
test_features_word2vec = []
for index, row in X_test.iterrows():
    model_vector = np.mean([sg_w2v_model[token] for token in row['stemmed_tokens']], axis=0)
    if type(model_vector) is list:
        test_features_word2vec.append(model_vector)
    else:
        test_features_word2vec.append(np.array([0 for i in range(1000)]))
test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)
print(classification_report(Y_test['Sentiment'],test_predictions_word2vec))
for num in test_predictions_word2vec:
  print(num)
</code></pre>
<p>At the end of that code snippet I added a for loop to quickly test to see what data was in test_predictions_word2vec and it looks like all zeroes.</p>
<pre><code>0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
</code></pre>
<p>Not too sure what happened where all the 1s were left out (I only included a small subset here to show the 0s. Looking at the full output on my console there was no 1s present).</p>
<p>I'm assuming it is because of this line here:</p>
<pre><code>test_features_word2vec.append(np.array([0 for i in range(1000)]))
</code></pre>
<p>Where it looks like it just appending 0s. Any help in this issue will be greatly appreciated!</p>
<p>P.S snippet for test-train split and output:</p>
<pre><code>from sklearn.model_selection import train_test_split
# Train Test Split Function
def split_train_test(split_data, test_size=0.3, shuffle_state=True):
    X_train, X_test, Y_train, Y_test = train_test_split(split_data[['movie_title',  'critics_consensus',    'tomatometer_status',   'tokenized_text',   'stemmed_tokens']], 
                                                        split_data['Sentiment'], 
                                                        shuffle=shuffle_state,
                                                        test_size=test_size, 
                                                        random_state=42)
    print(&quot;Value counts for Train sentiments&quot;)
    print(Y_train.value_counts())
    print(&quot;Value counts for Test sentiments&quot;)
    print(Y_test.value_counts())
    print(type(X_train))
    print(type(Y_train))
    X_train = X_train.reset_index()
    X_test = X_test.reset_index()
    Y_train = Y_train.to_frame()
    Y_train = Y_train.reset_index()
    Y_test = Y_test.to_frame()
    Y_test = Y_test.reset_index()
    print(X_train.head())

    

    return X_train, X_test, Y_train, Y_test

# Call the train_test_split
X_train, X_test, Y_train, Y_test = split_train_test(split_data)
</code></pre>
<pre><code>Value counts for Train sentiments
1    3805
0    2588
Name: Sentiment, dtype: int64
Value counts for Test sentiments
1    1660
0    1081
</code></pre>
<p>EDIT: Adding output of 'word2vec_df'</p>
<pre><code>Time taken to fit the model with word2vec vectors: 18.75066113471985
             0         1         2         3         4         5         6  \
0     0.009097 -0.014559 -0.021197  0.060744 -0.019707  0.102395  0.032876   
1     0.008102 -0.003382 -0.014465  0.066731 -0.024593  0.085185  0.023677   
2     0.013941 -0.005870 -0.001550  0.071456 -0.013130  0.094142  0.043876   
3     0.010195 -0.012312 -0.006310  0.069745 -0.012042  0.091056  0.034140   
4     0.006570 -0.010348 -0.016157  0.063258 -0.029932  0.098463  0.034469   
...        ...       ...       ...       ...       ...       ...       ...   
6388  0.000616 -0.000732 -0.006287  0.063298 -0.024651  0.055185 -0.000368   
6389  0.010891 -0.007447 -0.025401  0.063245 -0.028681  0.100588  0.029031   
6390  0.009561 -0.007456 -0.017953  0.076449 -0.029962  0.092921  0.040811   
6391  0.012995 -0.008843 -0.013079  0.058345 -0.027885  0.095623  0.024361   
6392  0.007881  0.003228 -0.013990  0.065434 -0.017051  0.090314  0.031072   

             7         8         9  ...       990       991       992  \
0     0.068392  0.120006  0.038360  ... -0.009643 -0.062597 -0.027641   
1     0.073042  0.101701  0.030647  ... -0.016221 -0.058624 -0.030524   
2     0.061665  0.117775  0.014894  ... -0.017982 -0.065756 -0.044015   
3     0.057861  0.117489  0.015533  ... -0.016098 -0.065427 -0.039047   
4     0.071677  0.100755  0.029278  ... -0.022267 -0.050894 -0.030283   
...        ...       ...       ...  ...       ...       ...       ...   
6388  0.058975  0.085394  0.028661  ... -0.016373 -0.050449 -0.008869   
6389  0.066502  0.106864  0.035051  ... -0.019567 -0.069977 -0.039586   
6390  0.061507  0.120290  0.030399  ...  0.000696 -0.054154 -0.041237   
6391  0.081338  0.111422  0.034755  ... -0.019699 -0.060718 -0.032540   
6392  0.054831  0.125640  0.032965  ... -0.002751 -0.084193 -0.040441   

           993       994       995       996       997       998       999  
0     0.078252  0.034909 -0.007387  0.057867 -0.052527 -0.072866 -0.010007  
1     0.075942  0.039987 -0.012127  0.042507 -0.054933 -0.072949 -0.010296  
2     0.065845  0.057452  0.002048  0.057100 -0.048846 -0.097791 -0.007207  
3     0.059275  0.051354  0.000843  0.050823 -0.046350 -0.090028 -0.005206  
4     0.066598  0.034786 -0.000143  0.056494 -0.046227 -0.070975 -0.007705  
...        ...       ...       ...       ...       ...       ...       ...  
6388  0.061066  0.017348 -0.018751  0.041088 -0.042949 -0.049911 -0.019149  
6389  0.071031  0.043249 -0.002368  0.040806 -0.046722 -0.085424  0.005255  
6390  0.076632  0.065442 -0.000805  0.050374 -0.047395 -0.085746  0.006119  
6391  0.083535  0.030460 -0.004143  0.047868 -0.058123 -0.069077 -0.012215  
6392  0.077906  0.075460 -0.013605  0.056237 -0.059329 -0.093779 -0.009383  

[6393 rows x 1000 columns]
</code></pre>
","python, scikit-learn, sentiment-analysis, multilabel-classification","<p>You are correct:</p>
<pre class=""lang-py prettyprint-override""><code>np.array([0 for i in range(1000)])
</code></pre>
<p>Creates an array full of zeros.</p>
<p>You should try:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.metrics import classification_report
test_features_word2vec = []

averaged_test_vector = X_test['stemmed_tokens'].apply(
        lambda x: np.mean([sg_w2v_model[tok] for tok in x], axis=0) 
    ).tolist()

averaged_test_vector = np.vstack(averaged_test_vector)

test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)
print(classification_report(Y_test['Sentiment'],test_predictions_word2vec))
</code></pre>
<p>Generally speaking, I would use embeddings of lower dimension if available.
1000 is a lot for a small dataset.
And I wouldn't use <code>DecisionTreeClassifier</code> as it overfits quickly.
I would start with <code>LinearSVC</code> or <code>RandomForrestClassifier</code>.</p>
",0,0,107,2022-05-24 10:50:36,https://stackoverflow.com/questions/72361580/inbalanced-dataset-for-classification-report
"pytorch dataloader - RuntimeError: stack expects each tensor to be equal size, but got [157] at entry 0 and [154] at entry 1","<p>I am a beginner with pytorch. I am trying to do an aspect based sentiment analysis. I am facing the error mentioned in the subject. My code is as follows: I request help to resolve this error. Thanks in advance. I will share the entire code and the error stack.
<code>!pip install transformers</code></p>
<pre><code>import transformers
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup
import torch
import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
from textwrap import wrap
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
%matplotlib inline
%config InlineBackend.figure_format='retina'
sns.set(style='whitegrid', palette='muted', font_scale=1.2)
HAPPY_COLORS_PALETTE = [&quot;#01BEFE&quot;, &quot;#FFDD00&quot;, &quot;#FF7D00&quot;, &quot;#FF006D&quot;, &quot;#ADFF02&quot;, &quot;#8F00FF&quot;]
sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))
rcParams['figure.figsize'] = 12, 8
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
</code></pre>
<p><code>df = pd.read_csv(&quot;/Users/user1/Downloads/auto_bio_copy.csv&quot;)</code></p>
<p>I am importing a csv file which has content and label as shown below:</p>
<p><code>df.head()</code></p>
<pre><code>                     content                                      label
0   I told him I would leave the car and come back...   O O O O O O O O O O O O O O O O O O O O O O O ...
1   I had the ignition interlock device installed ...   O O O B-Negative I-Negative I-Negative O O O O...
2   Aug. 23 or 24 I went to Walmart auto service d...   O O O O O O O B-Negative I-Negative I-Negative...
3   Side note This is the same reaction I 'd gotte...   O O O O O O O O O O O O O O O O O O O O O O O ...
4   Locked out of my car . Called for help 215pm w...   O O O O O O O O O O O O O O O O O B-Negative O...
</code></pre>
<p><code>df.shape</code></p>
<p><code>(1999, 2)</code></p>
<p>I am converting the label values into integers as follows:
O=zero(0), B-Positive=1, I-Positive=2, B-Negative=3, I-Negative=4, B-Neutral=5, I-Neutral=6, B-Mixed=7, I-Mixed=8</p>
<pre><code>df['label'] = df.label.str.replace('O', '0')
df['label'] = df.label.str.replace('B-Positive', '1')
df['label'] = df.label.str.replace('I-Positive', '2')
df['label'] = df.label.str.replace('B-Negative', '3')
df['label'] = df.label.str.replace('I-Negative', '4')
df['label'] = df.label.str.replace('B-Neutral', '5')
df['label'] = df.label.str.replace('I-Neutral', '6')
df['label'] = df.label.str.replace('B-Mixed', '7')
df['label'] = df.label.str.replace('I-Mixed', '8')
</code></pre>
<p>Next, converting the string to integer list as follows:</p>
<pre><code>df['label'] = df['label'].str.split(' ').apply(lambda s: list(map(int, s)))
</code></pre>
<pre><code>df.head()
</code></pre>
<pre><code>                     content                                         label
0   I told him I would leave the car and come back...   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
1   I had the ignition interlock device installed ...   [0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
2   Aug. 23 or 24 I went to Walmart auto service d...   [0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 0, 0, 0, 0, ...
3   Side note This is the same reaction I 'd gotte...   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
4   Locked out of my car . Called for help 215pm w...   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
</code></pre>
<pre><code>PRE_TRAINED_MODEL_NAME = 'bert-base-cased'
</code></pre>
<pre><code>tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)
</code></pre>
<pre><code>token_lens = []
for txt in df.content:
  tokens = tokenizer.encode_plus(txt, max_length=512, add_special_tokens=True, truncation=True, return_attention_mask=True)
  token_lens.append(len(tokens))
MAX_LEN = 512
</code></pre>
<pre><code>class Auto_Bio_Dataset(Dataset):
    def __init__(self, contents, labels, tokenizer, max_len):
        self.contents = contents
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
    def __len__(self):
        return len(self.contents)
    def __getitem__(self, item):
        content = str(self.contents[item])
        label = self.labels[item]
        encoding = self.tokenizer.encode_plus(
          content,
          add_special_tokens=True,
          max_length=self.max_len,
          return_token_type_ids=False,
          #padding='max_length',
          pad_to_max_length=True,
          truncation=True,
          return_attention_mask=True,
          return_tensors='pt'
        )
        return {
          'content_text': content,
          'input_ids': encoding['input_ids'].flatten(),
          'attention_mask': encoding['attention_mask'].flatten(),
          'labels': torch.tensor(label)
        }
</code></pre>
<pre><code>df_train, df_test = train_test_split(
  df,
  test_size=0.1,
  random_state=RANDOM_SEED
)
df_val, df_test = train_test_split(
  df_test,
  test_size=0.5,
  random_state=RANDOM_SEED
)
</code></pre>
<pre><code>df_train.shape, df_val.shape, df_test.shape
</code></pre>
<pre><code>((1799, 2), (100, 2), (100, 2))
</code></pre>
<pre><code>def create_data_loader(df, tokenizer, max_len, batch_size):
    ds = Auto_Bio_Dataset(
        contents=df.content.to_numpy(),
        labels=df.label.to_numpy(),
        tokenizer=tokenizer,
        max_len=max_len
  )
    return DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=2
  )
</code></pre>
<pre><code>BATCH_SIZE = 16
train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)
</code></pre>
<pre><code>data = next(iter(train_data_loader))
data.keys()
</code></pre>
<p>Error is as follows:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-71-e0a71018e473&gt; in &lt;module&gt;
----&gt; 1 data = next(iter(train_data_loader))
      2 data.keys()

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)
    528             if self._sampler_iter is None:
    529                 self._reset()
--&gt; 530             data = self._next_data()
    531             self._num_yielded += 1
    532             if self._dataset_kind == _DatasetKind.Iterable and \

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)
   1222             else:
   1223                 del self._task_info[idx]
-&gt; 1224                 return self._process_data(data)
   1225 
   1226     def _try_put_index(self):

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _process_data(self, data)
   1248         self._try_put_index()
   1249         if isinstance(data, ExceptionWrapper):
-&gt; 1250             data.reraise()
   1251         return data
   1252 

~/opt/anaconda3/lib/python3.7/site-packages/torch/_utils.py in reraise(self)
    455             # instantiate since we don't know how to
    456             raise RuntimeError(msg) from None
--&gt; 457         raise exception
    458 
    459 

RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py&quot;, line 287, in _worker_loop
    data = fetcher.fetch(index)
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 52, in fetch
    return self.collate_fn(data)
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 157, in default_collate
    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 157, in &lt;dictcomp&gt;
    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 138, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [157] at entry 0 and [154] at entry 1
</code></pre>
<p>I found in some github post that this error can be because of batch size, so i changed the batch size to 8 and then the error is as follows:</p>
<pre><code>BATCH_SIZE = 8
train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)
</code></pre>
<pre><code>data = next(iter(train_data_loader))
data.keys()
</code></pre>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-73-e0a71018e473&gt; in &lt;module&gt;
----&gt; 1 data = next(iter(train_data_loader))
      2 data.keys()

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)
    528             if self._sampler_iter is None:
    529                 self._reset()
--&gt; 530             data = self._next_data()
    531             self._num_yielded += 1
    532             if self._dataset_kind == _DatasetKind.Iterable and \

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)
   1222             else:
   1223                 del self._task_info[idx]
-&gt; 1224                 return self._process_data(data)
   1225 
   1226     def _try_put_index(self):

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _process_data(self, data)
   1248         self._try_put_index()
   1249         if isinstance(data, ExceptionWrapper):
-&gt; 1250             data.reraise()
   1251         return data
   1252 

~/opt/anaconda3/lib/python3.7/site-packages/torch/_utils.py in reraise(self)
    455             # instantiate since we don't know how to
    456             raise RuntimeError(msg) from None
--&gt; 457         raise exception
    458 
    459 

RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py&quot;, line 287, in _worker_loop
    data = fetcher.fetch(index)
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 52, in fetch
    return self.collate_fn(data)
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 157, in default_collate
    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 157, in &lt;dictcomp&gt;
    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 137, in default_collate
    out = elem.new(storage).resize_(len(batch), *list(elem.size()))
RuntimeError: Trying to resize storage that is not resizable
</code></pre>
<p>I am not sure what is causing the first error(the one mentioned in subject). I am using padding and truncate in my code, yet the error.</p>
<p>Any help to resolve this issue is highly appreciated.</p>
<p>Thanks in advance.</p>
","pytorch, sentiment-analysis, pytorch-dataloader","<p>Quick answer: you need to implement your own <code>collate_fn</code> function when creating a <code>DataLoader</code>. See <a href=""https://discuss.pytorch.org/t/dataloader-gives-stack-expects-each-tensor-to-be-equal-size-due-to-different-image-has-different-objects-number/91941/7"" rel=""noreferrer"">the discussion from PyTorch forum</a>.</p>
<p>You should be able to pass the function object to <code>DataLoader</code> instantiation:</p>
<pre class=""lang-py prettyprint-override""><code>def my_collate_fn(data):
    # TODO: Implement your function
    # But I guess in your case it should be:
    return tuple(data)

return DataLoader(
    ds,
    batch_size=batch_size,
    num_workers=2,
    collate_fn=my_collate_fn
)
</code></pre>
<p>This should be the way to solving this, but as a temporary remedy in case anything is urgent or a quick test is nice, simply change <code>batch_size</code> to <code>1</code> to prevent torch from trying to stack things with different shapes up.</p>
",8,7,16020,2022-05-24 13:26:13,https://stackoverflow.com/questions/72363741/pytorch-dataloader-runtimeerror-stack-expects-each-tensor-to-be-equal-size-b
"ValueError: If the `request` argument is set, then none of the individual field arguments should be set","<p>I am using Google NLP API for Sentiment Analysis to extract sentiment score of dataframe with <code>review</code> as my text column. My code looks lik,</p>
<pre><code>def getSentiments(df):
inserts = 0
df1 = pd.DataFrame(columns = ['Content', 'Sentiment', 'Magnitude', 'ReviewId'])
for index, row in df.iterrows():
    time.sleep(0.5)
    print(index)
    text1 = row['review']
    reviewid = row['review_id']
    texts = []
    sentiments = []    
    magnitudes = []
    salience = []
    try:
        document = analyze_sentiment(text1)
        result = client.analyze_sentiment(document, encoding_type = &quot;UTF8&quot;)
        # Get overall sentiment of the input document
        print(u&quot;Document sentiment score: {}&quot;.format(result.document_sentiment.score))
       
        sentiments.append(result.document_sentiment.score)
        magnitudes.append(result.document_sentiment.magnitude)
        dfEntitySentiment = pd.DataFrame(columns=['Content','Sentiment','Magnitude', 'ReviewId'])
        dfEntitySentiment['Content'] = text1
        dfEntitySentiment['Sentiment'] = sentiments
        dfEntitySentiment['Magnitude'] = magnitudes
        dfEntitySentiment['ReviewId'] = reviewid
        df1 = df1.append(dfEntitySentiment, ignore_index = True)
        
    except:
        traceback.print_exc()
        continue
return df1
</code></pre>
<p>My <code>analyze_sentiment</code> function looks like,</p>
<pre><code>def analyze_sentiment(text_content):
client = language_v1.LanguageServiceClient.from_service_account_json(Alteryx.read(&quot;#2&quot;).iloc[0]['FullPath'])
type_ = language_v1.types.Document.Type.PLAIN_TEXT
language = &quot;en&quot;
document = {&quot;content&quot;: text_content, &quot;type&quot;: type_, &quot;language&quot;: language}
encoding_type = &quot;UTF8&quot;

return document
</code></pre>
<p>I am getting the above error and am not sure how to resolve this.</p>
<pre><code>from google.cloud.language_v1 import enums
from google.cloud.language_v1 import types
</code></pre>
<p>These 2 libraries I was using before but I had to update them. After updating the libraries I am getting the error.</p>
","python, google-cloud-platform, nlp, sentiment-analysis","<p>There is a library update, according to the <a href=""https://googleapis.dev/python/language/latest/UPGRADING.html#enums-and-types"" rel=""nofollow noreferrer"">Migration Guide</a>, the submodule <code>enums</code> and <code>types</code> have been removed.</p>
<p><code>request</code> parameter to <code>client.analyze_sentiment</code> needs to be following format,</p>
<pre><code>result = client.analyze_sentiment(request = {'document': document, 'encoding_type': encoding_type})
</code></pre>
<p>This helped me to resolve my error.</p>
",0,0,1285,2022-05-25 10:40:06,https://stackoverflow.com/questions/72376112/valueerror-if-the-request-argument-is-set-then-none-of-the-individual-field
Kernel keeps dying while using BERT-based sentiment analysis model,"<p>I'm trying to use german bert sentiment analysis on Jupyter Notebook. I have installed pytorch correctly but the Kernel keeps dying. I'm on a MacBook Pro '21 with MacOs Monterey 12.3.1. I've installed Python 10.3.4. PyTorch does not show up in the list of installed packages on that environment even though in the terminal it tells me that the 'requirement is already satisfied'.</p>
<p>I tried to run the example code postet on hugging face.
<a href=""https://huggingface.co/oliverguhr/german-sentiment-bert"" rel=""nofollow noreferrer"">https://huggingface.co/oliverguhr/german-sentiment-bert</a></p>
<pre><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer
from typing import List
import torch
import re

class SentimentModel():
    def __init__(self, model_name: str):
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        self.clean_chars = re.compile(r'[^A-Za-züöäÖÜÄß ]', re.MULTILINE)
        self.clean_http_urls = re.compile(r'https*\\S+', re.MULTILINE)
        self.clean_at_mentions = re.compile(r'@\\S+', re.MULTILINE)

    def predict_sentiment(self, texts: List[str])-&gt; List[str]:
        texts = [self.clean_text(text) for text in texts]
        # Add special tokens takes care of adding [CLS], [SEP], &lt;s&gt;... tokens in the right way for each model.
        encoded = self.tokenizer.batch_encode_plus(texts,padding=True, add_special_tokens=True,truncation=True, return_tensors=&quot;pt&quot;)
        encoded = encoded.to(self.device)
        with torch.no_grad():
                logits = self.model(**encoded)
        
        label_ids = torch.argmax(logits[0], axis=1)
        return [self.model.config.id2label[label_id.item()] for label_id in label_ids]

    def replace_numbers(self,text: str) -&gt; str:
            return text.replace(&quot;0&quot;,&quot; null&quot;).replace(&quot;1&quot;,&quot; eins&quot;).replace(&quot;2&quot;,&quot; zwei&quot;).replace(&quot;3&quot;,&quot; drei&quot;).replace(&quot;4&quot;,&quot; vier&quot;).replace(&quot;5&quot;,&quot; fünf&quot;).replace(&quot;6&quot;,&quot; sechs&quot;).replace(&quot;7&quot;,&quot; sieben&quot;).replace(&quot;8&quot;,&quot; acht&quot;).replace(&quot;9&quot;,&quot; neun&quot;)         

    def clean_text(self,text: str)-&gt; str:    
            text = text.replace(&quot;\n&quot;, &quot; &quot;)        
            text = self.clean_http_urls.sub('',text)
            text = self.clean_at_mentions.sub('',text)        
            text = self.replace_numbers(text)                
            text = self.clean_chars.sub('', text) # use only text chars                          
            text = ' '.join(text.split()) # substitute multiple whitespace with single whitespace   
            text = text.strip().lower()
            return text

texts = [&quot;Mit keinem guten Ergebniss&quot;,&quot;Das war unfair&quot;, &quot;Das ist gar nicht mal so gut&quot;,
        &quot;Total awesome!&quot;,&quot;nicht so schlecht wie erwartet&quot;, &quot;Das ist gar nicht mal so schlecht&quot;,
        &quot;Der Test verlief positiv.&quot;,&quot;Sie fährt ein grünes Auto.&quot;, &quot;Der Fall wurde an die Polzei übergeben.&quot;]

model = SentimentModel(model_name = &quot;oliverguhr/german-sentiment-bert&quot;)

print(model.predict_sentiment(texts))



</code></pre>
","python, jupyter-notebook, sentiment-analysis","<p>Can you please try this code. It's the same model in a simpler-to-use lib.</p>
<pre class=""lang-bash prettyprint-override""><code>pip install germansentiment
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from germansentiment import SentimentModel

model = SentimentModel()

texts = [
    &quot;Mit keinem guten Ergebniss&quot;,&quot;Das ist gar nicht mal so gut&quot;,
    &quot;Total awesome!&quot;,&quot;nicht so schlecht wie erwartet&quot;,
    &quot;Der Test verlief positiv.&quot;,&quot;Sie fährt ein grünes Auto.&quot;]
       
result = model.predict_sentiment(texts)
print(result)
</code></pre>
",1,1,2003,2022-05-26 18:26:10,https://stackoverflow.com/questions/72396420/kernel-keeps-dying-while-using-bert-based-sentiment-analysis-model
Scraping Reviews in R,"<p>I am working on a sentiment analysis project. My aim is to scrape all the reviews from the rotten tomatoes website of a particular movie. I have tried to scrape it but it is giving me illegal characters, not the reviews I want. Any suggestion will be highly appreciated.</p>
<p>I am using this function:</p>
<pre><code>dune_movie &lt;- read_html(&quot;https://www.rottentomatoes.com/m/dune_2021/reviews&quot;)
dune_movie
</code></pre>
<p>Output I am getting:</p>
<pre><code>{html_document}
&lt;html lang=&quot;en&quot; dir=&quot;ltr&quot; xmlns:fb=&quot;http://www.facebook.com/2008/fbml&quot; xmlns:og=&quot;http://opengraphprotocol.org/schema/&quot;&gt;
[1] &lt;head prefix=&quot;og: http://ogp.me/ns# flixstertomatoes: http://ogp.me/ns/ap ...
[2] &lt;body class=&quot;body no-touch&quot;&gt;\n\n        \n\n        &lt;div id=&quot;emptyPlaceho ...
</code></pre>
","r, web-scraping, sentiment-analysis","<p><code>read_html</code> gives you the html of the entire page in a navigable document tree. If you want to scrape the reviews, you have to find the elements on the page that contain the reviews:</p>
<pre class=""lang-r prettyprint-override""><code>library(rvest)

read_html(&quot;https://www.rottentomatoes.com/m/dune_2021/reviews&quot;) %&gt;%
  html_elements(xpath = &quot;//div[@class='the_review']&quot;) %&gt;% 
  html_text2()
#&gt;  [1] &quot;\r While Dune 2021 is a very well presented and styled gourmet sci-fi dish, for a platter stuffed with \&quot;spice\&quot;, it unfortunately lacks a ton of Dune flavor.\r&quot;                                                                                                  
#&gt;  [2] &quot;\r Feels more like an obscure, scattered conversation overheard on a long train ride, peaking early with Rampling’s natural mystique, and then hitting a downward spiral – all dense plot and mild tedium, a bounty of sensual imagery wasted on zero substance.\r&quot;
#&gt;  [3] &quot;\r Paul's family is evidently descended from a long line of matadors.\r&quot;                                                                                                                                                                                           
#&gt;  [4] &quot;\r The paramount attractions, the visuals, connect immediately. The landscapes, costuming, sets, makeup, creatures and wild special effects are stunning.\r&quot;                                                                                                       
#&gt;  [5] &quot;\r Duna works sometimes like a strange abstract opera in which we perceive more the intensities than the representation of the events, and sometimes not only do we not know what part of the story we are in, but what is concretely happening.\r&quot;                
#&gt;  [6] &quot;\r To promise a whole series of films might be dressed up as a gift for fans, but theres a lingering cynicism about this project a feeling that its essentially a way of maximising returns at the box office.\r&quot;                                                  
#&gt;  [7] &quot;\r This Dune toys with the idea of genocide, but it’s mostly a movie for people who like to memorize things. All these stupid names, one after another. \r&quot;                                                                                                        
#&gt;  [8] &quot;\r Dune might not be for everyone; but if you strap in, immerse yourself in the world and go along for the ride, Denis Villenueve delivers a blockbuster sci-fi epic that's regularly jaw-dropping.\r&quot;                                                             
#&gt;  [9] &quot;\r A dishwater war narrative masquerading as sci-fi. Pshaw.\r&quot;                                                                                                                                                                                                     
#&gt; [10] &quot;\r A blockbuster celebrating the awe-inspiring power of the big screen that everyone can get behind.\r&quot;                                                                                                                                                            
#&gt; [11] &quot;\r If nothing else, Dune wins its place as a masterpiece of adaptation, truncating roughly half the novel into its runtime to expand the books monomaniacal focus on Paul into a more ensemble narrative.\r&quot;                                                       
#&gt; [12] &quot;\r Mature yet juvenile, otherworldly yet pleasingly familiar, “Dune” demands to be experienced on the biggest screen you can find (sandworms!).\r&quot;                                                                                                                 
#&gt; [13] &quot;\r Dune: Part One will leave fans not only wanting, but hoping for more. The spice, in other words, must flow.\r&quot;                                                                                                                                                  
#&gt; [14] &quot;\r Dune is another triumph for Villeneuve and I can’t wait to see him finish his epic.\r&quot;                                                                                                                                                                          
#&gt; [15] &quot;\r Denis Villeneuve’s windswept epic is engrossing enough to maintain an audience with an intermission and a running time twice its length.\r&quot;                                                                                                                     
#&gt; [16] &quot;\r From the costumes to the enormous machinery and craft of mining spice the film presents a beautifully realised and consistent universe.\r&quot;                                                                                                                      
#&gt; [17] &quot;\r Dune falls under a high-brow take on science fiction. It is an entertaining cinematic feat to say the least. The franchise has a bright future.\r&quot;                                                                                                              
#&gt; [18] &quot;\r \&quot;Perhaps the best, or at least the most revealing, thing that can be said about Denis Villeneuves grandly mounted adaptation of Frank Herberts 'Dune' is that he makes it look easy.\&quot;\r&quot;                                                                      
#&gt; [19] &quot;\r Many people will encourage you to see new releases on as big a screen as possible. With a masterful spectacle like Dune, its practically a commandment.\r&quot;                                                                                                      
#&gt; [20] &quot;\r Its stateliness is both an asset and a detriment.\r&quot;
</code></pre>
<p><sup>Created on 2022-05-28 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v2.0.1)</sup></p>
",2,1,363,2022-05-28 20:50:06,https://stackoverflow.com/questions/72419167/scraping-reviews-in-r
NLP analysis advice,"<p>Working on an NLP project and would really benefit from any expert help.</p>
<p>I'm looking to narrow down my options and select the most appropriate analysis methods and techniques for a project I'm working on. My question relates to what I should do in relation to the data I have. Any help (for a newbie) is very appreciated.</p>
<p>My data: Open text, short string data responses to a survey question. I have multiple survey responses, each survey has a high number of respondents (3K+) although a relatively low number respond to the question (typically 50 per survey). The responses are short (typically one line/sentence response), but I have about 20 surveys, so a reasonable corpus to work with.</p>
<p>Here's what I was planning (high level): Preprocess and clean the data, run some descriptives on the text data itself (BOWS, word frequency, maybe tf-idf, word clouds), then attempt some Topic Modelling and maybe Sentiment Analysis.</p>
<p>My main questions as I work my way through this massive learning process:</p>
<p>Would this type of data set warrant any particular Topic Modelling or Sentiment Analysis techniques?
Are there any obvious or less obvious limitations or considerations I should keep in mind, as a result of the data I've got?
Are there any clear step by step guides you can recommend? (I've been dipping in and out of a lot course and reading, but any similar experiences or examples would be invaluable).</p>
<p>I appreciate this is a bit text heavy and asking a lot, but any help and support would be really fantastic.</p>
","database, methods, sentiment-analysis, survey, topic-modeling","<p>I'm a little bit late to the party, but in terms of topic modeling, the best starting point is LDA.  There are a bunch of implementations of it (the best is <a href=""https://mimno.github.io/Mallet/"" rel=""nofollow noreferrer"">MALLET</a>), and it is relatively easy to understand.  There are a bunch of topic models designed for short texts such as open-ended survey responses, including some that I helped design.  Our models can be found in the python package <a href=""https://github.com/GU-DataLab/gdtm"" rel=""nofollow noreferrer"">GDTM</a>.  Take a look at NLDA, which is designed for short texts, and Guided Topic Model (GTM), which is also designed for short texts, but which allows you to provide seed topics if you already know some important topics.  Have fun :)</p>
",0,0,267,2022-06-01 13:14:41,https://stackoverflow.com/questions/72462505/nlp-analysis-advice
Splitting Google sentiment analysis response into separate columns and generating `None` for cells with no value,"<p><strong>Goal</strong></p>
<p>I want to split the response from Google Sentiment Analysis into four columns, then merge with original content dataframe.</p>
<p><strong>Situation</strong></p>
<p>I'm running the Google sentiment analysis on a column of text in a python dataframe.<br />
Here's a sample for one of the returned rows. The column is 'sentiment':</p>
<p><code>magnitude: 0.6000000238418579\nscore: -0.6000000238418579</code></p>
<p>I then need to split that cell into four new columns, one for magnitude, one for it's returned value, one for score, and one for it's returned value.</p>
<p><strong>What I've tried</strong></p>
<p>Currently, I'm using this method to do that:</p>
<p><code>df02 = df01['sentiment'].astype(str).str.split(expand=True)</code></p>
<p>I'm then merging those four columns with the original dataframe that contains the analyzed <code>text</code> field and other values.</p>
<p>However, if sentiment returns no results, the <code>sentiment</code> cell is empty. And if all rows have empty <code>sentiment</code> cells, then it won't create four new columns. And that breaks my attempt to merge the two dataframes.</p>
<p>So I'm trying to understand how I can insert <code>None</code> into the new four column cells if the <code>sentiment</code> cell value is empty in the source dataframe. That way, at least I'll have four columns, with the values for each of the four new cells being <code>None</code>.</p>
<p>I've received input that I should use <code>apply()</code> and <code>fillna</code>, but I'm not understanding how that should be handled in my instance, and the documentation isn't clear to me. It seems like the method above needs code added that inserts <code>None</code> if no value is detected, but I'm not familiar enough with Python or pandas to know where to start on that.</p>
<p><strong>EXAMPLE</strong></p>
<p>What the data returned looks like. If all rows have no entry, then it won't create the four columns, which is required for my next method of merging this dataframe back into the dataframe with the original text content.</p>
<pre><code>|index|0|1|2|3|
|---|---|---|---|---|
|0|||||
|1|||||
|2|||||
|3|||||
|4|||||
|5|magnitude:|0\.6000000238418579|score:|-0\.6000000238418579|
|6|magnitude:|0\.10000000149011612|score:|0\.10000000149011612|
|7|magnitude:|0\.10000000149011612|score:|-0\.10000000149011612|
|8|magnitude:|0\.699999988079071|score:|-0\.699999988079071|
|9|magnitude:|0\.699999988079071|score:|-0\.30000001192092896|
|10|magnitude:|0\.699999988079071|score:|-0\.30000001192092896|
</code></pre>
","python, pandas, sentiment-analysis, google-natural-language","<p>As mentioned by @dsx, the responses from <a href=""https://cloud.google.com/natural-language/docs/sentiment-tutorial"" rel=""nofollow noreferrer"">Google Sentiment Analysis</a> can be split into four columns by using the below code :</p>
<pre><code>pd.DataFrame(df['sentiment'].apply(sentiment_pass).tolist(),columns=['magnitude', 'score'], index=df.index)
</code></pre>
<p>Sentiment Analysis is used to identify the prevailing emotions within the text using natural language processing. For more information, you can check this <a href=""https://cloud.google.com/natural-language/docs/analyzing-sentiment"" rel=""nofollow noreferrer"">link</a>.</p>
",1,0,178,2022-06-02 03:36:07,https://stackoverflow.com/questions/72470368/splitting-google-sentiment-analysis-response-into-separate-columns-and-generatin
AttributeError: &#39;Series&#39; object has no attribute &#39;encode&#39; in SentimentIntensityAnalyzer,"<p>I have scraped tweets using snscrape library in python. I was willing to get the sentiment for each of the tweets. In order to do That, I have used SentimentIntensityAnalyzer() from nltk and the following error popped up.</p>
<pre><code>AttributeError: 'Series' object has no attribute 'encode'
</code></pre>
<p>I went back to the dataset resulted from the scraping and it shows the type of the Text column of the dataset as the following</p>
<pre><code>type(data['Text'])
Out[42]: pandas.core.series.Series
</code></pre>
<p>I tried to change the data type and do other operations but the results were not positive. What approach should I take?</p>
<p>Thank you!</p>
","python, sentiment-analysis","<p>Passing data['Text'] through pandas.apply() function and then using SentimentIntensityAnalyzer() does the trick. The fact is the function is not capable of handling the series object. Passing one by one element to SentimentIntensityAnalyzer() using a looping function does the job in this case.</p>
",0,0,4147,2022-06-08 11:53:45,https://stackoverflow.com/questions/72545249/attributeerror-series-object-has-no-attribute-encode-in-sentimentintensitya
Machine Learning Model Only Predicting Mode in Data Set,"<p>I am trying to do sentiment analysis for text. I have 909 phrases commonly used in emails, and I scored them out of ten for how angry they are, when isolated. </p> Now, I upload this .csv file to a Jupyter Notebook, where I import the following modules:</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
</code></pre>
<p>Now, I define both columns as 'phrases' and 'anger':</p>
<pre><code>df=pd.read_csv('Book14.csv', names=['Phrase', 'Anger'])
df_x = df['Phrase']
df_y = df['Anger']
</code></pre>
<p>Subsequently, I split this data such that 20% is used for testing and 80% is used for training:</p>
<pre><code>x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=4)
</code></pre>
<p>Now, I convert the words in <code>x_train</code> to numerical data using TfidfVectorizer:</p>
<pre><code>tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words='en')
x_traincv = tfidfvectorizer.fit_transform(x_train.astype('U'))
</code></pre>
<p>Now, I convert <code>x_traincv</code> to an array:</p>
<pre><code>a = x_traincv.toarray()
</code></pre>
<p>I also convert <code>x_testcv</code> to a numerical array:</p>
<pre><code>x_testcv=tfidfvectorizer.fit_transform(x_test)
x_testcv = x_testcv.toarray()
</code></pre>
<p>Now, I have</p>
<pre><code>mnb = MultinomialNB()
b=np.array(y_test)
error_score = 0
b=np.array(y_test)
for i in range(len(x_test)):
    mnb.fit(x_testcv,y_test)
    testmessage=x_test.iloc[i]
    predictions = mnb.predict(x_testcv[i].reshape(1,-1))
    error_score = error_score + (predictions-int(b[i]))**2
    print(testmessage)
    print(predictions)
print(error_score/len(x_test))
</code></pre>
<p>However, an example of the results I get are:</p>
<blockquote>
<p>Bring it back
[0]
It is greatly appreciatd when
[0]
Apologies in advance
[0]
Can you please
[0]
See you then
[0]
I hope this email finds you well.
[0]
Thanks in advance
[0]
I am sorry to inform
[0]
You’re absolutely right
[0]
I am deeply regretful
[0]
Shoot me through
[0]
I’m looking forward to
[0]
As I already stated
[0]
Hello
[0]
We expect all students
[0]
If it’s not too late
[0]</p>
</blockquote>
<p>and this repeats on a large scale, even for phrases that are obviously very angry. When I removed all data containing a '0' from the .csv file, the now modal value (a 10) is the only prediction for my sentences. </p> Why is this happening? Is it some weird way to minimise error? Are there any inherent flaws in my code? Should I take a different approach?</p>
","python, machine-learning, scikit-learn, jupyter-notebook, sentiment-analysis","<p>Two things, you are fitting The MultinomialNB with the test set. In your loop you have <code>mnb.fit(x_testcv,y_test)</code> but you should do <code>mnb.fit(x_traincv,y_train)</code></p>
<p>Second, when performing pre-processing you should call the <code>fit_transform</code> only on the training data while on the test you should call only the <code>transform</code> method.</p>
",0,0,51,2022-06-23 12:23:13,https://stackoverflow.com/questions/72730095/machine-learning-model-only-predicting-mode-in-data-set
Finding the scores for each tweet with a BERT-based sentiment analysis model,"<p>I am doing a sentiment analysis of twitter posts and I have a question regarding “German Sentiment Classification with Bert”:</p>
<p>I would like to display the sentiment score (positive, negative, neutral) for each tweet like it is shown on the models card on <a href=""https://huggingface.co/oliverguhr/german-sentiment-bert"" rel=""nofollow noreferrer"">huggingface</a>(screenshot)
I tried to go through the implementation of the mode by stepping into every line of code but could not figure out how to find the scores.</p>
<p><a href=""https://i.sstatic.net/PPa99.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PPa99.png"" alt=""enter image description here"" /></a></p>
<p>My code is based on the following code:</p>
<pre><code>
model = SentimentModel()

texts = [
    &quot;Mit keinem guten Ergebniss&quot;,&quot;Das ist gar nicht mal so gut&quot;,
    &quot;Total awesome!&quot;,&quot;nicht so schlecht wie erwartet&quot;,
    &quot;Der Test verlief positiv.&quot;,&quot;Sie fährt ein grünes Auto.&quot;]
       
result = model.predict_sentiment(texts)
print(result)
</code></pre>
","python, twitter, sentiment-analysis, huggingface-transformers, bert-language-model","<p>you can inherit from the model's class and define a function to output the scores:</p>
<pre><code>from typing import List
import torch
from germansentiment import SentimentModel


class SentimentModel(SentimentModel):
    def __init__(self):
        super().__init__()
        
    def predict_sentiment_proba(self, texts: List[str])-&gt; List[str]:
        texts = [self.clean_text(text) for text in texts]
        # Add special tokens takes care of adding [CLS], [SEP], &lt;s&gt;... tokens in the right way for each model.
        # truncation=True limits number of tokens to model's limitations (512)
        encoded = self.tokenizer.batch_encode_plus(texts, padding=True, add_special_tokens=True,truncation=True, return_tensors=&quot;pt&quot;)
        
        encoded = encoded.to(self.device)
        with torch.no_grad():
                logits = self.model(**encoded)
        
        #label_ids = torch.argmax(logits[0], axis=1)
        return torch.nn.Softmax(dim=1)(logits[0]), self.model.config.id2label
   
texts = [&quot;Mit keinem guten Ergebniss&quot;,&quot;Das ist gar nicht mal so gut&quot;,
    &quot;Total awesome!&quot;,&quot;nicht so schlecht wie erwartet&quot;,
    &quot;Der Test verlief positiv.&quot;,&quot;Sie fährt ein grünes Auto.&quot;]

model = SentimentModel()
scores, ids = model.predict_sentiment_proba(texts)

scores
&gt;tensor([[1.1602e-03, 9.9877e-01, 6.8676e-05],
        [8.8440e-04, 9.9909e-01, 2.3437e-05],
        [9.8738e-01, 1.2542e-02, 7.6997e-05],
        [9.7940e-01, 2.0516e-02, 8.2444e-05],
        [4.1755e-04, 4.6088e-04, 9.9912e-01],
        [2.1236e-05, 5.3932e-05, 9.9992e-01]])
ids
&gt;{0: 'positive', 1: 'negative', 2: 'neutral'}

scores.argmax(dim=-1)
&gt;tensor([1, 1, 0, 0, 2, 2]) #negative, negative, positive, positive, neutral, neutral
</code></pre>
",2,2,1231,2022-07-12 16:46:08,https://stackoverflow.com/questions/72955752/finding-the-scores-for-each-tweet-with-a-bert-based-sentiment-analysis-model
Scrape Only English Tweets using Twitter Api in R,"<p>I have been working on a project related to Sentiment Analysis of Emojis. I have used this simple code to scrape tweets specifically on the topic &quot;Uber&quot; but I only want tweets in English and the code I am using gives me the tweets from all the languages. How can I scrape or customise my code So I would only get them in English?</p>
<p>My code:</p>
<pre><code>library(lubridate)
library(tidyverse)
library(rtweet)
library(igraph)
library(here)

get_token() # Connects with Twitter API

Uber &lt;- search_tweets(&quot;uber&quot;, n = 50)
</code></pre>
","r, web-scraping, sentiment-analysis, tweets","<p>From the documentation <code>?search_tweets</code>:</p>
<blockquote>
<p>...<br />
Further arguments passed as query parameters in request sent to Twitter's REST API. To return only English language tweets, for example, use lang = &quot;en&quot;. For more options see Twitter's API documentation.</p>
</blockquote>
<p>So:</p>
<pre><code>Uber &lt;- search_tweets(&quot;uber&quot;, n = 50, lang = &quot;en&quot;)
</code></pre>
",2,1,417,2022-07-14 21:27:13,https://stackoverflow.com/questions/72986833/scrape-only-english-tweets-using-twitter-api-in-r
Vader Sentiment Analysis in R,"<p>I am using Vader Sentiment Analysis on Tweets for a project of mine with 3000 tweets. Everything is working fine when I run the sentiment for only 1 tweet. That gives me all the scores for that tweet but when I run the loop command for all the tweets, I only get the final results as the overall combined score for Vader. I am interested to get the final results as the first one which is giving all the scores. Any help will be highly appreciated.</p>
<p>Sample data:</p>
<pre><code>dput(data_sample$text)
c(&quot;Need DoorDash or Uber method asap😭 cause I be starving😭😭&quot;, 
&quot;I’m such a real ahh niqq cuz I be having myself weak asl😂&quot;, 
&quot;This shii made me laugh so fuccin hard bro😂😂😂😂&quot;, 
&quot;Kevin Hart and Will Ferrell made a Gem in Get hard fr😂😂😂&quot;, 
&quot;@_big_emmy @NigerianAmazon Chill🤣😭&quot;, &quot;Ts so bomedy 😂😂😂&quot;, 
&quot;So is that ass Gotdam😂😂😂&quot;, 
&quot;This wild😂😂😂&quot;, &quot;Idc them late night DoorDash’s be goin crazy🤣&quot;, 
&quot;Video of the week😂😂😂😂&quot;)
</code></pre>
<p>Code:</p>
<pre><code>get_vader(data_sample$text[1])
</code></pre>
<p>I need this result for all 10 tweets from that loop below:</p>
<pre><code>word_scores                            compound                                 pos 
&quot;{0, 0, 0, 0, 0, 0, 0, 0, 0, -1.8}&quot;                            &quot;-0.421&quot;                                 &quot;0&quot; 
                                neu                                 neg                           but_count 
                            &quot;0.763&quot;                             &quot;0.237&quot;                                 &quot;0&quot; 
</code></pre>
<p>Not like this:</p>
<pre><code>for (i in 1:length(data_sample$text)){
  Loop_Error &lt;- F
  tryCatch({ 
    get_vader(data_sample$text[i]) %&gt;%
      as.numeric(unlist(.)) %&gt;%
      .[length(.)-4] -&gt;data_sample$score_vader[i]
  }, error = function(e){
    Loop_Error &lt;&lt;- T})
  if (Loop_Error){
    data_sample$score_vader[i] &lt;- &quot;Error&quot;
  }
}

vader_data



data_sample$score_vader
1                   -0.421
2                   -0.440
3                    0.444
4                   -0.103
5                    0.000
6                    0.000
7                   -0.581
8                    0.000
9                   -0.340
10                   0.000
</code></pre>
","r, sentiment-analysis, vader","<p>I have this idea to get all the outputs of <code>data_sample</code> given by <code>get_vader()</code>, but you will need to modify your code a bit to use <code>vader_df()</code>:</p>
<pre><code>allvals &lt;- NULL
for (i in 1:length(data_sample)){
outs &lt;-  vader_df(data_sample[i])
allvals &lt;- rbind(allvals,outs)
}
</code></pre>
",1,0,1097,2022-07-20 04:43:28,https://stackoverflow.com/questions/73046093/vader-sentiment-analysis-in-r
Emoji Sentiment Analysis in R,"<p>I am working on a project where I have used tweets with Emojis and Emoticons. My main goal is to get the combined sentiment score of the tweets( text + Emoticons ) and as we know these emoticons are probably the most meaningful part of the data and that's they can not be neglected. I have converted the encoding structure of the emojis and emoticons via iconv but I am only getting the sentiment score for the text, not the emojis. I am using Vader sentiment in this process but if there is another Sentiment library/Lexicon that can be used which will give me the senti score for all the emojis too it will be a lot helpful and highly appreciated.</p>
<p>Tweets:</p>
<pre><code>dput(df_emoji$Description)
c(&quot;DoorDash or Uber method asap&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;ad&gt; cause I be starving&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;ad&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;ad&gt;&quot;, 
&quot;such a real ahh niqq cuz I be having myself weak asl&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&quot;, 
&quot;shii made me laugh so fuccin hard bro&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&quot;, 
&quot;Hart and Will Ferrell made a Gem in Get hard fr&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&quot;, 
&quot;@NigerianAmazon Chill&lt;f0&gt;&lt;9f&gt;&lt;a4&gt;&lt;a3&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;ad&gt;&quot;, &quot;so bomedy &lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&quot;, 
&quot;is that ass Gotdam&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&quot;, 
&quot;wild&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&quot;, 
&quot;them late night DoorDash&lt;e2&gt;&lt;80&gt;&lt;99&gt;s be goin crazy&lt;f0&gt;&lt;9f&gt;&lt;a4&gt;&lt;a3&gt;&quot;, 
&quot;of the week&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&lt;82&gt;&quot;
)
</code></pre>
<p>Code:</p>
<pre><code>emoji_senti &lt;- data.frame(text = iconv(data_sample$text, &quot;latin1&quot;, &quot;ASCII&quot;, &quot;byte&quot;), 
                      stringsAsFactors = FALSE)
column1 &lt;- separate(emoji_senti, text, into = c(&quot;Bytes&quot;, &quot;Description&quot;), sep = &quot;\\ &quot;)
column2 &lt;- separate(emoji_senti, text, into = c(&quot;Bytes&quot;, &quot;Description&quot;), sep = &quot;^[^\\s]*\\s&quot;)
df_emoji &lt;- data.frame(Bytes = column1$Bytes, Description = column2$Description)

allvals_emoji &lt;- NULL
for (i in 1:length(df_emoji$Description)){
  outs &lt;-  vader_df(df_emoji$Description[i])
  allvals_emoji &lt;- rbind(allvals_emoji,outs)
}
allvals_emoji
</code></pre>
<p>See this that the first tweet has only 9 English words which have their scores but it misses the score for converted Unicode for emojis.</p>
<pre><code># word_scores compound   pos   neu   neg but_count
    # 1                 {0, 0, 0, 0, 0, 0, 0, 0, 0}    0.000 0.000 1.000 0.000         0
    # 2  {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1.9, 0, 0}   -0.440 0.000 0.805 0.195         0
    # 3        {0, 0, 0, 2.6, 0, 0, -0.67835, 0, 0}    0.444 0.293 0.570 0.137         0
    # 4        {0, 0, 0, 0, 0, 0, 0, 0, 0, -0.4, 0}   -0.103 0.000 0.877 0.123         0
    # 5                                      {0, 0}    0.000 0.000 1.000 0.000         0
    # 6                                {0, 0, 0, 0}    0.000 0.000 1.000 0.000         0
    # 7                          {0, 0, -2.5, 0, 0}   -0.542 0.000 0.533 0.467         0
    # 8                                      {0, 0}    0.000 0.000 1.000 0.000         0
    # 9                       {0, 0, 0, 0, 0, 0, 0}    0.000 0.000 1.000 0.000         0
    # 10                               {0, 0, 0, 0}    0.000 0.000 1.000 0.000         0
</code></pre>
","r, encoding, utf-8, emoji, sentiment-analysis","<p>Check this discussion: <a href=""https://stackoverflow.com/q/63249001/19565329"">VaderSentiment: unable to update emoji sentiment score</a></p>
<blockquote>
<p>&quot;Vader transforms emojis to their word representation prior to extracting sentiment&quot;</p>
</blockquote>
<p>Basically from what I tested out emoji's values are hidden but part of the score and can influence it. If you need the score for a specific emoji you can check <code>library(lexicon)</code> and run <code>data.frame(hash_emojis_identifier)</code> (dataframe that contains identifiers for emojis and matches them to a lexicon format) and <code>data.frame(hash_sentiment_emojis)</code> to get each emoji sentiment value. It is not possible though to determine from that what was the impact of a series of emojis over the total message score without knowing how vader calculates their cumulative impact on the score itself using libraries such as vader, lexicon.</p>
<p>You can evaluate the impact of the emoji though by doing a simple difference between the total score value of the message with emojis and the score without it:</p>
<pre><code>allvals &lt;- NULL
for (i in 1:length(data_sample)){
outs &lt;-  vader_df(data_sample[i])
allvals &lt;- rbind(allvals,outs)
}
allvalswithout &lt;- NULL
for (i in 1:length(data_samplewithout)){
outs &lt;-  vader_df(data_samplewithout[i])
allvalswithout &lt;- rbind(allvalswithout,outs)
}

emojiscore &lt;- allvals$compound-allvalswithout$compound
</code></pre>
<p>Then:</p>
<pre><code>allvals &lt;- cbind(allvals,emojiscore) 
</code></pre>
<p>Now for large datasets it would be ideal to automate the process of removing emojis out of texts. Here i just removed it manually to propose this kind of approach to the problem.</p>
",1,1,632,2022-07-20 16:02:44,https://stackoverflow.com/questions/73054815/emoji-sentiment-analysis-in-r
How to batch sentiment with PYABSA,"<p>I followed <a href=""https://colab.research.google.com/drive/1QViqvAE-oG4TYOmID4jchAsZyjSN9Wzu?usp=sharing#scrollTo=F3P3j_q-SYV-"" rel=""nofollow noreferrer"">this tutorial</a> to do sentiment inference. I could run the code with a list of sentences (in the section Aspect Sentiment Inference of the Colab Notebook). However, I don't know how to modify the following code (in the section Batch Sentiment Inference) to infer sentiment for a file of my own (containing just 2 lines, each has two sentences).</p>
<pre><code># inference_sets = ABSADatasetList.Phone # original code
inference_sets = 'test.dat.apc' # this is my own file that I want to infer sentiment for each sentence
results = sent_classifier.batch_infer(target_file=inference_sets,
                     print_result=True,
                     save_result=True,
                     ignore_error=False,
                     )
</code></pre>
<p>Running the modified code caused the following error</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
Input In [56], in &lt;cell line: 2&gt;()
      1 test = 'test.dat.apc'
----&gt; 2 results = sent_classifier.batch_infer(target_file=test,
      3                      print_result=False,
      4                      save_result=True,
      5                      ignore_error=False,
      6                      )

File ~\Anaconda3\envs\spacy\lib\site-packages\pyabsa-1.16.15-py3.9.egg\pyabsa\core\apc\prediction\sentiment_classifier.py:197, in SentimentClassifier.batch_infer(self, target_file, print_result, save_result, ignore_error, clear_input_samples)
    193     self.clear_input_samples()
    195 save_path = os.path.join(os.getcwd(), 'apc_inference.result.json')
--&gt; 197 target_file = detect_infer_dataset(target_file, task='apc')
    198 if not target_file:
    199     raise FileNotFoundError('Can not find inference datasets!')

File ~\Anaconda3\envs\spacy\lib\site-packages\pyabsa-1.16.15-py3.9.egg\pyabsa\functional\dataset\dataset_manager.py:302, in detect_infer_dataset(dataset_path, task)
    300     if os.path.isdir(dataset_path.dataset_name):
    301         print('No inference set found from: {}, unrecognized files: {}'.format(dataset_path, ', '.join(os.listdir(dataset_path.dataset_name))))
--&gt; 302     raise RuntimeError(
    303         'Fail to locate dataset: {}. If you are using your own dataset, you may need rename your dataset according to {}'.format(
    304             dataset_path,
    305             'https://github.com/yangheng95/ABSADatasets#important-rename-your-dataset-filename-before-use-it-in-pyabsa')
    306     )
    307 if len(dataset_path) &gt; 1:
    308     print(colored('Please DO NOT mix datasets with different sentiment labels for training &amp; inference !', 'yellow'))

RuntimeError: Fail to locate dataset: ['test.dat.apc']. If you are using your own dataset, you may need rename your dataset according to https://github.com/yangheng95/ABSADatasets#important-rename-your-dataset-filename-before-use-it-in-pyabsa
</code></pre>
<p>What did I do wrong?</p>
","python, sentiment-analysis","<p>You need to rename your dataset file name, by ending with .inference</p>
",1,0,502,2022-09-09 04:31:07,https://stackoverflow.com/questions/73657355/how-to-batch-sentiment-with-pyabsa
Problem completing BERT model for sentiment classification,"<p>I am trying to figure out sentiment classification on movie reviews using BERT, transformers and tensorflow. This is the code I currently have:</p>
<pre><code>def read_dataset(filename, model_name=&quot;bert-base-uncased&quot;):
    &quot;&quot;&quot;Reads a dataset from the specified path and returns sentences and labels&quot;&quot;&quot;

    tokenizer = BertTokenizer.from_pretrained(model_name)
    with open(filename, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
        lines = f.readlines()
        # preallocate memory for the data
        sents, labels = list(), np.empty((len(lines), 1), dtype=int)

        for i, line in enumerate(lines):
            text, str_label, _ = line.split(&quot;\t&quot;)
            labels[i] = int(str_label.split(&quot;=&quot;)[1] == &quot;POS&quot;)
            sents.append(text)
    return dict(tokenizer(sents, padding=True, truncation=True, return_tensors=&quot;tf&quot;)), labels


class BertMLP(tf.keras.Model):
    def __init__(self, embed_batch_size=100, model_name=&quot;bert-base-cased&quot;):
        super(BertMLP, self).__init__()
        self.bs = embed_batch_size
        self.model = TFBertModel.from_pretrained(model_name)
        self.classification_head = tf.keras.models.Sequential(
            layers = [
                tf.keras.Input(shape=(self.model.config.hidden_size,)),
                tf.keras.layers.Dense(350, activation=&quot;tanh&quot;),
                tf.keras.layers.Dense(200, activation=&quot;tanh&quot;),
                tf.keras.layers.Dense(50, activation=&quot;tanh&quot;),
                tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;, use_bias=False)
            ]
        )

    def call(self, inputs):
        outputs = self.model(inputs)
        return outputs

def evaluate(model, inputs, labels, loss_func):
    mean_loss = tf.keras.metrics.Mean(name=&quot;train_loss&quot;)
    accuracy = tf.keras.metrics.BinaryAccuracy(name=&quot;train_accuracy&quot;)

    predictions = model(inputs)
    mean_loss(loss_func(labels, predictions))
    accuracy(labels, predictions)

    return mean_loss.result(), accuracy.result() * 100


if __name__ == &quot;__main__&quot;:
    train = read_dataset(&quot;datasets/rt-polarity.train.vecs&quot;)
    dev = read_dataset(&quot;datasets/rt-polarity.dev.vecs&quot;)
    test = read_dataset(&quot;datasets/rt-polarity.test.vecs&quot;)

    mlp = BertMLP()
    mlp.compile(tf.keras.optimizers.SGD(learning_rate=0.01), loss='mse')
    dev_loss, dev_acc = evaluate(mlp, *dev, tf.keras.losses.MeanSquaredError())
    print(&quot;Before training:&quot;, f&quot;Dev Loss: {dev_loss}, Dev Acc: {dev_acc}&quot;)
    mlp.fit(*train, epochs=10, batch_size=10)
    dev_loss, dev_acc = evaluate(mlp, *dev, tf.keras.losses.MeanSquaredError())
    print(&quot;After training:&quot;, f&quot;Dev Loss: {dev_loss}, Dev Acc: {dev_acc}&quot;)
</code></pre>
<p>However, when I run this code, I get an error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;C:\Users\home\anaconda3\lib\site-packages\spyder_kernels\py3compat.py&quot;, line 356, in compat_exec
    exec(code, globals, locals)

  File &quot;c:\users\home\downloads\mlp.py&quot;, line 60, in &lt;module&gt;
    dev_loss, dev_acc = evaluate(mlp, *dev, tf.keras.losses.MeanSquaredError())

  File &quot;c:\users\home\downloads\mlp.py&quot;, line 46, in evaluate
    predictions = model(inputs)

  File &quot;C:\Users\home\anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File &quot;c:\users\home\downloads\mlp.py&quot;, line 39, in call
    outputs = self.model(inputs)

  File &quot;C:\Users\home\anaconda3\lib\site-packages\transformers\modeling_tf_utils.py&quot;, line 409, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)

  File &quot;C:\Users\home\anaconda3\lib\site-packages\transformers\models\bert\modeling_tf_bert.py&quot;, line 1108, in call
    outputs = self.bert(

  File &quot;C:\Users\home\anaconda3\lib\site-packages\transformers\modeling_tf_utils.py&quot;, line 409, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)

  File &quot;C:\Users\home\anaconda3\lib\site-packages\transformers\models\bert\modeling_tf_bert.py&quot;, line 781, in call
    embedding_output = self.embeddings(

  File &quot;C:\Users\home\anaconda3\lib\site-packages\transformers\models\bert\modeling_tf_bert.py&quot;, line 203, in call
    inputs_embeds = tf.gather(params=self.weight, indices=input_ids)

InvalidArgumentError: Exception encountered when calling layer &quot;embeddings&quot; (type TFBertEmbeddings).

indices[1174,8] = 29550 is not in [0, 28996) [Op:ResourceGather]

Call arguments received:
  • input_ids=tf.Tensor(shape=(1599, 73), dtype=int32)
  • position_ids=None
  • token_type_ids=tf.Tensor(shape=(1599, 73), dtype=int32)
  • inputs_embeds=None
  • past_key_values_length=0
  • training=False
</code></pre>
<p>I googled for a while, and I can't find anything conclusive. I am pretty sure it has something to do with this part:</p>
<pre><code>def call(self, inputs):
        outputs = self.model(inputs)
        return outputs
</code></pre>
<p>But again, I have tried a lot of different things, including limiting dataset size and installing different versions of transformers and tensorflow, but to no avail. Please let me know what I'm doing wrong. Thank you!</p>
","python, tensorflow, keras, sentiment-analysis, bert-language-model","<p>OP was using <code>bert-base-cased</code> for their model, and <code>bert-base-uncased</code> for their tokenizer, causing issues during training when the vocab size of the model and the tokenized data differed.</p>
",1,0,566,2022-10-05 15:46:48,https://stackoverflow.com/questions/73963008/problem-completing-bert-model-for-sentiment-classification
PHP sentiment score of descriptions from CSV file,"<p>I am trying to get sentiment scores of random product descriptions from a CSV file, I'm facing a problem with what I think is the API response time, not sure if I'm traversing through the CSV using the API incorrectly / un-efficiently but it is taking a long time to get results for all the 300+ entries in the CSV and whenever I want to push new changes to my codebase I need to wait for the API to re-evaluate the entries every time, here is my code I made for loading in the CSV file and for getting the sentiment scores</p>
<pre><code>    &lt;?php

set_time_limit(500); // extended timeout due to slow / overwhelmed API response

function extract_file($csv) { // CSV to array function

    $file = fopen($csv, 'r');

    while (!feof($file)) {
        $lines[] = fgetcsv($file, 1000, ',');
    }

    fclose($file);
    return $lines;

}

$the_file = 'dataset.csv';
$csv_data = extract_file($the_file);



$response_array = []; // array container to hold returned sentiment values from among prduct descriptions

for($x = 1; $x &lt; count($csv_data) - 1; $x++) { // loop through all descriptions
    echo $x; // show iteration
    $api_text = $csv_data[$x][1];
    $api_text = str_replace('&amp;', ' and ', $api_text); // removing escape sequence characters, '&amp;' breaks the api :)
    $api_text = str_replace(&quot; &quot;, &quot;%20&quot;, $api_text); // serializing string
    $text = 'text=';
    $text .=$api_text; // serializing string further for the API
    //echo 'current text1: ', $api_text;
    $curl = curl_init(); // API request init

    curl_setopt_array($curl, [
        CURLOPT_URL =&gt; &quot;https://text-sentiment.p.rapidapi.com/analyze&quot;,
        CURLOPT_RETURNTRANSFER =&gt; true,
        CURLOPT_FOLLOWLOCATION =&gt; true,
        CURLOPT_ENCODING =&gt; &quot;&quot;,
        CURLOPT_MAXREDIRS =&gt; 10,
        CURLOPT_TIMEOUT =&gt; 30,
        CURLOPT_HTTP_VERSION =&gt; CURL_HTTP_VERSION_1_1,
        CURLOPT_CUSTOMREQUEST =&gt; &quot;POST&quot;,
        CURLOPT_POSTFIELDS =&gt; $text,
        CURLOPT_HTTPHEADER =&gt; [
            &quot;X-RapidAPI-Host: text-sentiment.p.rapidapi.com&quot;,
            &quot;X-RapidAPI-Key: &lt;snip&gt;&quot;,
            &quot;content-type: application/x-www-form-urlencoded&quot;
        ],
    ]);

    $response = curl_exec($curl);
    $err = curl_error($curl);

    curl_close($curl);

    if ($err) {
        echo &quot;cURL Error #:&quot; . $err;
    } else {
        echo $response;
    }


    $json = json_decode($response, true); // convert response to JSON format
    
    if(isset($json[&quot;pos&quot;]) == false) { // catching response error 100, makes array faulty otherwise
        continue;
    }
    else {
        array_push($response_array, array($x, &quot;+&quot; =&gt; $json[&quot;pos&quot;], &quot;-&quot; =&gt; $json[&quot;neg&quot;])); // appends array with sentiment values at current index
    }
    
}

echo &quot;&lt;br&gt;&quot;;
echo &quot;&lt;br&gt; results: &quot;;

echo &quot;&lt;p&gt;&quot;;
for ($y = 0; $y &lt; count($response_array); $y++){ // prints out all the sentiment values
    echo &quot;&lt;br&gt;&quot;;
    echo print_r($response_array[$y]);
    echo &quot;&lt;br&gt;&quot;;
}
echo &quot;&lt;/p&gt;&quot;;

echo &quot;&lt;br&gt;the most negative description: &quot;;
$max_neg = array_keys($response_array, max(array_column($response_array, '-')));
//$max_neg = max(array_column($response_array, '-'));
echo print_r($csv_data[$max_neg[0]]);

echo &quot;&lt;br&gt;the most positive description: &quot;;
$max_pos = array_keys($response_array, max(array_column($response_array, '+')));
echo print_r($csv_data[$max_pos[0]]);


?&gt;
</code></pre>
<p>What this code snippet aims to do is find the most negative and most positive sentiment among the description column in the csv and print them out according to their index, I'm only interested in finding descriptions with the highest amount of positive and negative sentiment word number not the percentage of the overall sentiment</p>
<p>The file can be found in this <a href=""https://github.com/samiljimari/sentiment/blob/main/dataset.csv"" rel=""nofollow noreferrer"">git repo</a></p>
<p>Thanks for any suggestions</p>
","php, api, curl, sentiment-analysis","<p>This can be achieved by creating a cache file.</p>
<p>This solution creates a file <code>cache.json</code> that contains the results from the API, using the product name as the key for each entry.</p>
<p>On subsequent calls, it will use the cache value if it exists.</p>
<pre><code>set_time_limit(500);

function file_put_json($file, $data)
{
    $json = json_encode($data, JSON_PRETTY_PRINT);
    file_put_contents($file, $json);
}

function file_get_json($file, $as_array=false)
{
    return json_decode(file_get_contents($file), $as_array);
}

function file_get_csv($file, $header_row=true)
{
    $handle = fopen($file, 'r');
    
    if ($header_row === true)
        $header = fgetcsv($handle);

    $array = [];
    while ($row = fgetcsv($handle)) {
        if ($header_row === true) {
            $array[] = array_combine($header, array_map('trim', $row));
        } else {
            $array[] = array_map('trim', $row);
        }
    }
    fclose($handle);
    return $array;
}

function call_sentiment_api($input)
{
    $text = 'text=' . $input;
    $curl = curl_init();

    curl_setopt_array($curl, [
        CURLOPT_URL =&gt; &quot;https://text-sentiment.p.rapidapi.com/analyze&quot;,
        CURLOPT_RETURNTRANSFER =&gt; true,
        CURLOPT_FOLLOWLOCATION =&gt; true,
        CURLOPT_ENCODING =&gt; &quot;&quot;,
        CURLOPT_MAXREDIRS =&gt; 10,
        CURLOPT_TIMEOUT =&gt; 30,
        CURLOPT_HTTP_VERSION =&gt; CURL_HTTP_VERSION_1_1,
        CURLOPT_CUSTOMREQUEST =&gt; &quot;POST&quot;,
        CURLOPT_POSTFIELDS =&gt; $text,
        CURLOPT_HTTPHEADER =&gt; [
            &quot;X-RapidAPI-Host: text-sentiment.p.rapidapi.com&quot;,
            &quot;X-RapidAPI-Key: &lt;snip&gt;&quot;,
            &quot;content-type: application/x-www-form-urlencoded&quot;
        ],
    ]);

    $response = curl_exec($curl);
    $err = curl_error($curl);

    curl_close($curl);

    if ($err) {
        throw new Exception(&quot;cURL Error #:&quot; . $err);
    }

    return $response;
}

$csv_data = file_get_csv('dataset.csv');

if (file_exists('cache.json')) {
    $cache_data = file_get_json('cache.json', true);
} else {
    $cache_data = [];
}

$cache_names = array_keys($cache_data);

$output = [];

foreach ($csv_data as $csv) {
    $product_name = $csv['name'];
    echo $product_name . '...';

    if (in_array($product_name, $cache_names)) {
        echo 'CACHED...' . PHP_EOL;

        continue;
    }

    $description = urlencode(str_replace('&amp;', ' and ', $csv['description']));

    $response = call_sentiment_api($description);
    
    echo 'API...' . PHP_EOL;

    $json = json_decode($response, true);

    $cache_data[$product_name] = $json;
}

file_put_json('cache.json', $cache_data);

echo 'SAVE CACHE!' . PHP_EOL . PHP_EOL;

$highest_pos = 0;
$highest_neg = 0;

$pos = [];
$neg = [];

foreach ($cache_data as $name =&gt; $cache) {
    if (!isset($cache['pos']) || !isset($cache['neg'])) {
        continue;
    }
    if ($cache['pos'] &gt; $highest_pos) {
        $pos = [$name =&gt; $cache];
        $highest_pos = $cache['pos'];
    }
    if ($cache['pos'] === $highest_pos) {
        $pos[$name] = $cache;
    }
    if ($cache['neg'] &gt; $highest_neg) {
        $neg = [$name =&gt; $cache];
        $highest_neg = $cache['neg'];
    }
    if ($cache['neg'] === $highest_neg) {
        $neg[$name] = $cache;
    }
}

echo &quot;Most Positive Sentiment: &quot; . $highest_pos . PHP_EOL;
foreach ($pos as $name =&gt; $pos_) {
    echo &quot;\t&quot; . $name . PHP_EOL;
}
echo PHP_EOL;

echo &quot;Most Negative Sentiment: &quot; . $highest_neg . PHP_EOL;
foreach ($neg as $name =&gt; $neg_) {
    echo &quot;\t&quot; . $name . PHP_EOL;
}
</code></pre>
<p>Results in:</p>
<pre><code>Most Positive Sentiment: 4
        X-Grip Lifting Straps - GymBeam
        Beta Carotene - GymBeam
        Chelated Magnesium - GymBeam
        Creatine Crea7in - GymBeam
        L-carnitine 1000 mg - GymBeam - 20 tabs
        Resistance Band Set - GymBeam

Most Negative Sentiment: 2
        Calorie free Ketchup sauce 320 ml - GymBeam
        ReHydrate Hypotonic Drink 1000 ml - GymBeam
        Vitamin E 60 caps - GymBeam
        Vitamin B-Complex 120 tab - GymBeam
        Zero Syrup Hazelnut Choco 350 ml - GymBeam
        Bio Psyllium - GymBeam
        Zero calorie Vanilla Syrup - GymBeam
</code></pre>
",1,0,197,2022-10-07 23:36:52,https://stackoverflow.com/questions/73993389/php-sentiment-score-of-descriptions-from-csv-file
Using &quot;ifelse&quot; with negative values - R,"<p>I did a sentiment analysis using VADER and now want to classify the values with negative, positive and neutral.</p>
<p>Positive when compound score is &gt; 0.05</p>
<p>Negative when its &lt; - 0.05
neutral when in between -0.05 and 0.05</p>
<pre><code>df_polarity$VADER_Sent = ifelse(df_polarity$VADER_Sent &gt; 0.05, &quot;pos&quot;, 
                            ifelse (df_polarity$VADER_Sent &lt; -0.05, &quot;neg&quot;, 
                                    ifelse (between(df_polarity$VADER_Sent, -0.05, 0.05) , &quot;neu&quot;, &quot;NA&quot;)
                            ) 
)
</code></pre>
<p>When running this code, even values with - 0.4XXX will be classified as neutral and not as negative.</p>
<p>For some reason this won't work. There is anything I am missing... but I can figure out what it is...</p>
<p>I couldn't find any helpful tipps by googling it.</p>
<p>I hope someone of you can help me with this one!</p>
<p>Output from str(df_polarity):</p>
<blockquote>
<p>$ VADER_Sent   : chr  &quot;0.0&quot; &quot;-0.4939&quot; &quot;0.7717&quot; &quot;0.7096&quot;</p>
</blockquote>
<p>After further looking into my data, it seems that the &quot;-&quot; sign is not recognized in the context of a negative number.</p>
<p>Thanks to everyone who tried to help me! Really appreciated it!!!</p>
","r, if-statement, sentiment-analysis, vader","<p>The problem is because the <code>VADER_Sent</code> column is character. The comparisons <code>&lt;</code> and <code>&gt;</code> are checking alphabetically instead of numerically.</p>
<p>Example:</p>
<pre><code>&gt; -0.4939 &lt; -0.05
[1] TRUE

&gt; &quot;-0.4939&quot; &lt; &quot;-0.05&quot;
[1] FALSE
</code></pre>
<p>Try using <code>as.numeric(df_polarity$VADER_Sent)</code> in your <code>ifelse()</code> statements to get around this.</p>
",2,0,900,2022-10-13 10:31:06,https://stackoverflow.com/questions/74054276/using-ifelse-with-negative-values-r
Inner Join not working for R sentiment analysis,"<p>I'm trying to conduct sentiment analysis on a Simpsons' episode using the afinn library but for some reason when I inner join sentiment with my tidy_text and filter for words labelled -5, it returns words in the afinn library that are not in my dataframe.</p>
<p>I double checked my df ('tidy_text') and it definitely is a subset of words from the episode.</p>
<p>Any idea what I'm doing wrong here?</p>
<pre><code>afinn &lt;- tidy_text %&gt;%
  inner_join(get_sentiments(&quot;afinn&quot;)) %&gt;%
  filter(value == -5) %&gt;%
  count(word, value, sort = TRUE)


</code></pre>
","r, text-mining, sentiment-analysis","<p>Sounds like an interesting project.
Try adding , by = c(&quot;word&quot; = &quot;word&quot;)) %&gt;%</p>
<pre><code>afinn &lt;- tidy_text %&gt;%
  inner_join(get_sentiments(&quot;afinn&quot;), by = c(&quot;word&quot; = &quot;word&quot;)) %&gt;%
  filter(value == -5) %&gt;%
  count(word, value, sort = TRUE)
</code></pre>
",2,-1,60,2022-12-02 17:51:42,https://stackoverflow.com/questions/74659657/inner-join-not-working-for-r-sentiment-analysis
Sentiment Analysis in R for German language,"<p>I'm trying to conduct the sentiment analysis in German in R. However, the output does not seem promising as I could not find a way to make it in German language.</p>
<p>Would you have any suggestions for me?</p>
<pre><code>#libraries
library(tidyverse)
library(tokenizers)
library(stopwords)
library(sentimentr)

#load data
data &lt;- tribble(
  ~content, 
  &quot;Nimmt euch in Acht✌️#tage #periode #blu #hände #rot #blute #wald #fy #viral&quot;,
  &quot;ich liebe uns #wortwitze #Periode #Tage #couplegoals&quot;,
  &quot;Mit KadeZyklus bei Krämpfen gibt es jetzt endlich ein pflanzliches Helferlein gegen leichte Unterleibskrämpfe!&quot;,
  &quot;Es ist wie es ist Jungs&quot;
)

# count freq of words
words_as_tokens &lt;- setNames(lapply(sapply(data$content, 
                                          tokenize_words, 
                                          stopwords = stopwords(language = &quot;en&quot;, source = &quot;smart&quot;)), 
                                   function(x) as.data.frame(sort(table(x), TRUE), stringsAsFactors = F)), data$content) 

# tidyverse's job
stop_german &lt;- data.frame(word = stopwords::stopwords(&quot;de&quot;), stringsAsFactors = FALSE)
df &lt;- words_as_tokens %&gt;%
  bind_rows(, .id = &quot;content&quot;) %&gt;%
  rename(word = x) %&gt;% 
  anti_join(stop_german, by = c(&quot;word&quot;))

#sentiment
df$sentiment_score &lt;- sapply(df$content, function(x) 
  mean(sentiment(x)$sentiment))
</code></pre>
","r, dplyr, sentiment-analysis","<p>You have specified the wrong source for stopwords and the wrong language.  <code>smart</code> as <code>source</code> does not contain <code>de</code> as language. If you do <code>stopwords_getsources()</code> you get all available sources for <code>stopwords</code>. With <code>stopwords_getlanguages(source = 'snowball')</code> you'll see that this contains <code>de</code>.</p>
<p>Change your <code>stopwords</code> accordingly and it will work.</p>
<pre><code># count freq of words
words_as_tokens &lt;- setNames(lapply(
  sapply(data$content,
    tokenize_words,
    stopwords = stopwords(language = &quot;de&quot;, source = &quot;snowball&quot;)
  ),
  function(x) as.data.frame(sort(table(x), TRUE), stringsAsFactors = F)
), data$content)
</code></pre>
",2,1,732,2022-12-03 21:07:46,https://stackoverflow.com/questions/74670729/sentiment-analysis-in-r-for-german-language
Checking the values in a list against Pandas DataFrame,"<p>I have a Pandas DataFrame with a list of words and their associated emotions (each can have several emotions attached to it). Something like this:</p>
<p><a href=""https://i.sstatic.net/NR5C8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NR5C8.png"" alt=""enter image description here"" /></a></p>
<p>I have also extracted the tokens of a text, using Spacy, into a list. something like ['study', 'Maths', 'easy', 'great', 'study',...]</p>
<p>In order to match the tokens in the tokenList to the associated emotions in the emotion dataframe (df_lexicon) I have tried the following:
`</p>
<pre><code>emotions = []

// adding the token to emotions list if it exists in the emotion dataframe

for i in tokensList:
  if i in df_lexicon['word'].values:
    emotions.append(i)

// printing the row including the word and emotion

for i in emotions:
  print(df_lexicon[df_lexicon['word']==i])
</code></pre>
<p><code>But that gives me:</code></p>
<pre><code>       word   emotion
10215  ban  negative
       word   emotion
10220  mad    negative
       mad    fear
.
//(and many more)
</code></pre>
<p>I don't know how to add the results to a new DataFrame instead of just printing them. Appreciate your help.</p>
","pandas, dataframe, data-analysis, sentiment-analysis","<p>You can use <code>.isin()</code> to compare your dataframe with the values in the list:</p>
<pre><code>s = df_lexicon['word'].isin(tokenList)

new_df = df_lexicon[s]
</code></pre>
",3,3,43,2023-01-03 15:05:14,https://stackoverflow.com/questions/74995119/checking-the-values-in-a-list-against-pandas-dataframe
Token indices sequence length warning while using pretrained Roberta model for sentiment analysis,"<p>I am presently using a pretrained Roberta model to identify the sentiment scores and categories for my dataset. I am truncating the length to 512 but I still get the warning. What is going wrong here? I am using the following code to achieve this:</p>
<pre><code>from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax
model = f&quot;j-hartmann/sentiment-roberta-large-english-3-classes&quot;
tokenizer = AutoTokenizer.from_pretrained(model, model_max_length=512,truncation=True)
automodel = AutoModelForSequenceClassification.from_pretrained(model)
</code></pre>
<p>The warning that I am getting here:</p>
<pre><code>Token indices sequence length is longer than the specified maximum sequence length for this model (627 &gt; 512). Running this sequence through the model will result in indexing errors
</code></pre>
","python, sentiment-analysis, roberta-language-model, roberta","<p>You have not shared the code where you use tokenizer to encode/tokenize the inputs, so I'm taking my own example to explain how you can achieve this.</p>
<pre><code>tokenizer = RobertaTokenizer.from_pretrained(model_path,model_max_length=512)
</code></pre>
<p>example usage:</p>
<pre><code>text = &quot;hello &quot;*513 # example text with more than 512 words

tokenizer(text, max_length=512, truncation=True, padding='max_length')

# you may use tokenizer.encode_plus() or tokenizer.encode() based on your need with same parameters to get similar length tokens i.e 512 
</code></pre>
<p>These above parameters will tokenize any string into <code>max_length</code> tokens by padding (if number of tokens is &lt; <code>max_length</code>)  or truncating (for tokens count &gt; <code>max_length</code>).</p>
<p><strong>Note</strong>: <code>max_length</code> cannot be greater than <code>512</code> for RoBERTa model.</p>
",2,0,2547,2023-01-19 12:08:53,https://stackoverflow.com/questions/75172022/token-indices-sequence-length-warning-while-using-pretrained-roberta-model-for-s
How to print words that are not in the list,"<p>I have 2 files, the first one is a list of tweets. And the second one is a list of standard words which looks like this:</p>
<pre><code>acoustics
acquaint
acquaintable
tbc....
</code></pre>
<p>I want to iterate through the list of tweets and print the words that are not found in the standard words list.</p>
<p>This is what I tried:</p>
<pre><code>dk = open('wordslist.txt','r')
dlist = []
for x in dk.readlines():
    dlist.append(x.replace('\n',''))

dlist
</code></pre>
<pre><code>length = len(tokenized_tweets)
for i in range(length):
    print(tokenized_tweets[i])
</code></pre>
<pre><code>for x in range(len(tokenized_tweets)):
    if x[0] not in dlist:
        print(tokenized_tweets[x])
</code></pre>
<p>and I got this error : <code>'int' object is not subscriptable</code></p>
","python, loops, iteration, sentiment-analysis, tweets","<p>Read and follow the error message then you'll figure out what the problem is.</p>
<p>In traceback you would see an arrow pointing to line <code>for x in (len(tokenized_tweets)):</code>. The error message says: <code>'int' object is not iterable</code>. What is your iterable in that <code>for</code> loop? <code>(len(tokenized_tweets))</code> Is this really an iterable? No it's an <code>int</code>. The output of <code>len()</code> is always an <code>int</code>(unless you overwrite it).</p>
<p>You supposed to pass the length of the <code>tokenized_tweetes</code> to the <code>range()</code> object. It <em>is</em> an iterable.</p>
<h4>extra tip:</h4>
<p>Since you're finding the words for every tweet, make a <code>set</code> out of your words. Set's membership testing is much more faster than list. (O(1) &gt; O(n))</p>
<p>It also removes duplicates if there are any.</p>
<h2>Solution:</h2>
<pre class=""lang-py prettyprint-override""><code>with open(&quot;wordslist.txt&quot;) as f:
    words_list = {word.removesuffix(&quot;\n&quot;) for word in f}

with open(&quot;tweets.txt&quot;) as g:
    for tweete in g:
        for word in tweete.split():
            if word not in words_list:
                print(word)
</code></pre>
",0,0,92,2023-01-23 06:33:33,https://stackoverflow.com/questions/75206270/how-to-print-words-that-are-not-in-the-list
"How can I optimize KNN, GNB nd SVC sklearn algorithms to reduce exec time?","<p>I'm currently evaluating which classifier have the best performance for movie reviews sentiment analysis task. So far I have evaluate Logistic Regression, Linear Regression, Random Forest and Decision tree but I also want to consider KNN, GNB and SVC models as well. The problem is that each execution of those algorithms (particulary KNN) has a large exec time. Even using RandomizedSearch in KNN I have to wait about 1 hour with 10 iterations. Here are some snippets:</p>
<p>KNN Classifier</p>
<pre><code> #KNearestNeighbors X -&gt; large execution time
    knn=KNeighborsClassifier()
    k_range=list(range(1,50))
    options=['uniform', 'distance']
    param_grid = dict(n_neighbors=k_range, weights=options)
    rand_knn = RandomizedSearchCV(knn, param_grid, cv=10, scoring='accuracy', n_iter=10, random_state=0)
    rand_knn.fit(x_train_bow, y_train)
    print(rand_knn.best_score_)
    print(rand_knn.best_params_)
    confm_knn = confusion_matrix(y_test, y_pred_knn)
    print_confm(confm_knn)
    print(&quot;=============K NEAREST NEIGHBORS============&quot;)
    print_metrics(y_test,y_pred_knn)
    print(&quot;============================================&quot;)
</code></pre>
<p>I waited for the execution of the code above for about 85 minutes but it never finished and I had to cut the execution. In order to get any result (at least anything) I try to choose the best k manually with a for loop but still each iteration takes over 12 - 17 minutes.</p>
<pre><code>def testing_k_neighbors(x_train_bow,y_train,x_test_bow,y_test):
    accuracy_hist = []
    for i in range (1,21):
        knn=KNeighborsClassifier(n_neighbors=i)
        knn.fit(x_train_bow, y_train)
        yi_pred_knn = knn.predict(x_test_bow)
        acc_i = accuracy_score(y_test, yi_pred_knn)
        accuracy_hist.append(acc_i)
        print(f&quot;K: {i}, accuracy: {acc_i}&quot;)
    print(accuracy_hist)
</code></pre>
<p>output:</p>
<pre><code>K: 1, accuracy: 0.7384384634613782
K: 2, accuracy: 0.7435213732188984
K: 3, accuracy: 0.7574368802599784
K: 4, accuracy: 0.7678526789434214
K: 5, accuracy: 0.7681859845012916
K: 6, accuracy: 0.7745187901008249
K: 7, accuracy: 0.7729355887009416
K: 8, accuracy: 0.7774352137321889
K: 9, accuracy: 0.7742688109324223
K: 10, accuracy: 0.7810182484792934
K: 11, accuracy: 0.7776851929005916
K: 12, accuracy: 0.7854345471210732
K: 13, accuracy: 0.783101408215982
K: 14, accuracy: 0.7866844429630864
K: 15, accuracy: 0.784934588784268
K: 16, accuracy: 0.78860094992084
K: 17, accuracy: 0.7873510540788268
K: 18, accuracy: 0.7893508874260479
K: 19, accuracy: 0.7856011999000083
K: 20, accuracy: 0.7916006999416715
</code></pre>
<p>Also SVC and GNB takes similar time to get any result:</p>
<pre><code>    #Support Vector Macine  X -&gt; large execution time
    #svc=SVC(C = 100, kernel = 'linear', random_state=123)
    #svc.fit(x_train_bow,y_train)
    #y_pred_svc = svc.predict(x_test_bow)
    #print(&quot;=============SUPPORT VECTOR MACHINE============&quot;)
    #print_metrics(y_test,y_pred_svc)
    #print(&quot;============================================&quot;)   
</code></pre>
<pre><code>    #Gaussian Naive Bayes
    gnbc=GaussianNB()
    gnbc.fit(x_train_bow.toarray(),y_train)
    y_pred_gnbc = gnbc.predict(x_test_bow)
    print(&quot;=============GAUSSIAN NAIVE BAYES============&quot;)
    print_metrics(y_test,y_pred_gnbc)
    print(&quot;============================================&quot;)   
</code></pre>
<p>Is there any way to tune my code reduce execution time and mantain or improve models performance?</p>
<p>Im expecting to tune my code prioritzing both efficiency and performance</p>
","python, machine-learning, nlp, sentiment-analysis, sklearn-pandas","<p>i try your code:
then i print &quot;x_train_bow&quot;:</p>
<pre><code>&lt;28000x122447 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
    with 2796291 stored elements in Compressed Sparse Row format&gt;
</code></pre>
<p>you have 122447 columns then used TfidfVectorizer,
This is a problem of dimension, which is why it takes a lot of time.
There is no solution(KNN, SVC, trees). you need to reduce the dimension. You need to extract the corresponding words and then use TfidfVectorizer.</p>
",0,0,111,2023-02-10 19:06:37,https://stackoverflow.com/questions/75415183/how-can-i-optimize-knn-gnb-nd-svc-sklearn-algorithms-to-reduce-exec-time
Implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW,"<p>How to fix this deprecated AdamW model?</p>
<p>I tried to use the BERT model to perform a sentiment analysis on the hotel reviews, when I run this piece of code, it prompts the following warning. I am still studying the transformers and I don't want the code to be deprecated very soon. I searched on the web and I can't find the solution yet.</p>
<p>I found this piece of information, but I don't know how to apply it to my code.</p>
<blockquote>
<p>To switch optimizer, put optim=&quot;adamw_torch&quot; in your TrainingArguments
(the default is &quot;adamw_hf&quot;)</p>
</blockquote>
<p>could anyone kindly help with this?</p>
<pre><code>from transformers import BertTokenizer, BertForSequenceClassification
import torch_optimizer as optim
from torch.utils.data import DataLoader
from transformers import AdamW
import pandas as pd
import torch
import random
import numpy as np
import torch.nn as nn
from torch.nn import CrossEntropyLoss
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from tqdm.notebook import tqdm
import json
from collections import OrderedDict
import logging
from torch.utils.tensorboard import SummaryWriter
</code></pre>
<p>skip some code...</p>
<pre><code>param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [{
    'params':
    [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
    'weight_decay_rate':
    0.01
}, {
    'params':
    [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
    'weight_decay_rate':
    0.0
}]

#
optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)           ##deprecated 
#optimizer = optim.AdamW(optimizer_grouped_parameters, lr=1e-5)      ##torch.optim.AdamW  (not working)

step = 0
best_acc = 0
epoch = 10
writer = SummaryWriter(log_dir='model_best')
for epoch in tqdm(range(epoch)):
    for idx, batch in tqdm(enumerate(train_loader),
                           total=len(train_texts) // batch_size,
                           leave=False):
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs[0]  # Calculate Loss
        logging.info(
            f'Epoch-{epoch}, Step-{step}, Loss: {loss.cpu().detach().numpy()}')
        step += 1
        loss.backward()
        optimizer.step()
        writer.add_scalar('train_loss', loss.item(), step)
    logging.info(f'Epoch {epoch}, present best acc: {best_acc}, start evaluating.')
    accuracy, precision, recall, f1 = eval_model(model, eval_loader)  # Evaluate Model
    writer.add_scalar('dev_accuracy', accuracy, step)
    writer.add_scalar('dev_precision', precision, step)
    writer.add_scalar('dev_recall', recall, step)
    writer.add_scalar('dev_f1', f1, step)
    if accuracy &gt; best_acc:
        model.save_pretrained('model_best')  # Save Model
        tokenizer.save_pretrained('model_best')
        best_acc = accuracy
</code></pre>
","python, pytorch, huggingface-transformers, sentiment-analysis","<p>If you comment out both these lines:</p>
<pre><code>import torch_optimizer as optim
from transformers import AdamW
</code></pre>
<p>and then use:</p>
<pre><code>optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=1e-5)
</code></pre>
<p>does it work? If not, what is the error?</p>
<blockquote>
<p>To switch optimizer, put optim=&quot;adamw_torch&quot; in your TrainingArguments (the default is &quot;adamw_hf&quot;)</p>
</blockquote>
<p>This is referring to Huggingface <a href=""https://huggingface.co/docs/transformers/main_classes/trainer"" rel=""noreferrer""><code>Trainer</code></a>, which is configured with a <a href=""https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.TrainingArguments"" rel=""noreferrer""><code>TrainingArguments</code></a> instance. But as you are using your own training loop, this is not applicable to you.</p>
",5,5,6226,2023-02-22 16:37:58,https://stackoverflow.com/questions/75535679/implementation-of-adamw-is-deprecated-and-will-be-removed-in-a-future-version-u
Stemming texts separates words into letters,"<p>I am trying to process my text using tokenization, stemming, normalization and stop-word/punctuation removal, etc.
When I use <strong>snowball stemming technique</strong>, my text gets separated into letters with commas in between.</p>
<pre><code>def processed_tweets(text):

  punctuate_text= str.maketrans('', '', string.punctuation+string.digits)
  text = text.translate(punctuate_text)

  tokens = word_tokenize(text)

  stop_words = set(stopwords.words('english'))
  filtered_words = [w for w in tokens if not w in stop_words]

  #applying stemming 
  snow_stemmer = SnowballStemmer(language='english')
  text = [snow_stemmer.stem(word) for word in text]

  return text


tweet_df['processed_tweets'] = tweet_df['Tweet Body'].apply(processed_tweets)
tweet_df.head()
</code></pre>
<p>This is the output I am getting:</p>
<p><a href=""https://i.sstatic.net/ijcKZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ijcKZ.png"" alt=""Output"" /></a></p>
<p>Following is the output for <code>print(tokens)</code></p>
<p><a href=""https://i.sstatic.net/USNTX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/USNTX.png"" alt=""enter image description here"" /></a></p>
<p>This is not the case when using lemmatization though. Is there an issue on how I am writing my code or the technique I am using (stemming vs lemmatization)?</p>
","python, sentiment-analysis, stemming, lemmatization","<p>Pretty much a very small misunderstanding on the use of tokenize on my part. Editing how I apply stemming to tokenized words instead of the 'text' string works:</p>
<pre><code>text = [snow_stemmer.stem(word) for word in filtered_words]
</code></pre>
",0,0,366,2023-02-24 22:05:39,https://stackoverflow.com/questions/75561777/stemming-texts-separates-words-into-letters
How to labeling text based on aspect term and sentiment,"<p>I have coded to label text data by term aspect then sentiment with vader lexicon. But the result is only output -1 which means negative and 1 which means positive, where there should be 3 classes of positive, negative and neutral.</p>
<p>Here is the code :</p>
<pre><code>import pandas as pd
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Define the aspect keywords
system_keywords = ['server', 'bug', 'error', 'sinyal', 'jaringan', 'login', 'update', 
                   'perbaruan', 'loading', 'aplikasi', 'fitur', 'UI/UX' , 'tampilan', 
                   'data', 'otp', 'keamanan']
layanan_keywords = ['customer service', 'cs', 'call center', 'telepon', 'email', 'beli', 
                    'pertalite', 'bbm', 'topup']
transaksi_keywords = ['cash', 'cashless', 'debit', 'tunai', 'scan', 'e-wallet', 
                      'linkaja', 'link', 'bayar', 'ovo', 'transaksi', 'pembayaran', 
                      'cashback', 'struk', 'tunai', 'nontunai']
subsidi_keywords = ['verifikasi', 'data', 'form', 'formulir', 'daftar', 'subsidi', 
                    'pendaftaran', 'subsidi', 'kendaraan', 'formulir', 'stnk', 'ktp', 
                    'nopol', 'no', 'kendaraan', 'nomor', 'polisi', 'foto', 'kendaraan', 
                    'alamat', 'provinsi', 'kota', 'kabupaten', 'kecamatan']
kebermanfaatan_keywords = ['bagus', 'mantap', 'recommend', 'oke', 'mudah', 'berguna', 
                           'membantu', 'simple', 'guna', 'bantu']

# Define a function to label the aspect based on the aspect keywords
def label_aspect(text):
    aspect_labels = [0] * 5 # Initialize all aspect labels to 0
    for i, keywords in enumerate([system_keywords, layanan_keywords, transaksi_keywords, 
           subsidi_keywords, kebermanfaatan_keywords]):
        for keyword in keywords:
            if keyword in text:
                aspect_labels[i] = 1
                break
    return aspect_labels

# Load the data into a DataFrame
data = {'content': ['Sejak menggunakan aplikasi mypertamina beli pertalite jadi lebih simple dan mudah karena aplikasi ini bener bener membantu untuk meringankan penjual dan pembeli recomend bisa bayar pakai tunai atau nontunai mantepp', 
                    'sering ada bug, aplikasi tidak user friendly. bingung dalam menginput data untuk subsidi. tidak ada notifikasi apakah data inputan sudah masuk atau belum. Tolong diperbaiki',
                    'Bagus juga aplikasi, kalo ada promo seperti ini kan para pemakai premium bisa jadi beralih ke pertalite bahkan pertamax. Coba ada promo2 lainnya seperti kerja sama dg situs belanja online ya min. Pertahankan min',
                    'kadang sulit di akses terakhir ada perintah update MyPertamina, saya ikuti, setelah update, jadi sulit masuk seolah data tidak ada, malah QR code tidak bisa muncul, dan belum sempat saya print',
                    'buruk, sudah coba daftar berkali kali tetap gak bisa. Mau beli bbm harus ada barcode, daftar susah ah bukan nya memudahkan rakyat malah tambah mempersulit']}
df = pd.DataFrame(data)

# Utilize nltk VADER to use custom lexicon
vader_lexicon = SentimentIntensityAnalyzer()

# Add the aspect columns to the DataFrame and label them
aspect_labels = df['content'].apply(label_aspect)
df['sistem'], df['layanan'], df['transaksi'], df['pendaftaran subsidi'], df['kebermanfaatan'] = zip(*aspect_labels)

# Apply Vader sentiment analysis to label the aspect columns
for col in ['sistem', 'layanan', 'transaksi', 'pendaftaran subsidi', 'kebermanfaatan']:
    df[col] = df['content'].apply(lambda x: 1 if vader_lexicon.polarity_scores(x) 
              ['compound'] &gt;= 0.05 and df[col][0] == 1 else (-1 if 
              vader_lexicon.polarity_scores(x)['compound'] &lt;= -0.05 and df[col][0] == 1 
              else 0))

# Display the resulting DataFrame
df
</code></pre>
<p>Here is the output
<a href=""https://i.sstatic.net/sh6K3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sh6K3.png"" alt=""enter image description here"" /></a></p>
<p>The output results are still not correct. As in the example data :</p>
<ul>
<li>&quot;Sejak menggunakan aplikasi mypertamina beli pertalite jadi lebih simple dan mudah karena aplikasi ini bener bener membantu untuk meringankan penjual dan pembeli recomend bisa bayar pakai tunai atau nontunai mantepp&quot;. In this sentence there are no words contained in the subsidi_keywords aspect, but the results in the &quot;pendaftaran subsidi&quot; column contain a value of is 1, should contain the value is 0</li>
<li>&quot;sering ada bug, aplikasi tidak user friendly. bingung dalam menginput data untuk subsidi. tidak ada notifikasi apakah data inputan sudah masuk atau belum. Tolong diperbaiki&quot;. In this sentence there are no words contained in the transaksi_keywords, layanan_keywords, and kebermanfaatan_keywords aspect, but the results in the &quot;transaksi&quot; column, &quot;layanan&quot; column, and &quot;kebermanfaatan&quot; column contain a value of is 1, should contain the value is 0</li>
</ul>
","python, pandas, sentiment-analysis, aspect, vader","<p>Your issue is that you are always using <code>df[col][0]</code> to test against <code>0</code> or <code>1</code>, where you should be using the appropriate row for the content. You can work around that using <code>np.where</code> to do the computation. Note that the result from Vader that you are testing is a constant (doesn't vary per column) so you can compute it outside the loop:</p>
<pre class=""lang-py prettyprint-override""><code>compound = df['content'].apply(lambda x: 1 if vader_lexicon.polarity_scores(x)['compound'] &gt;= 0.05 else -1)
for col in ['sistem', 'layanan', 'transaksi', 'pendaftaran subsidi', 'kebermanfaatan']:
    df[col] = np.where(df[col] == 0, 0, compound)
</code></pre>
",1,3,469,2023-03-10 07:49:15,https://stackoverflow.com/questions/75693447/how-to-labeling-text-based-on-aspect-term-and-sentiment
I need advice on Sentimental analysis and ML,"<p>I'm using snscrape to scrape tweets about EURUSD and combining machine learning to predict if the price of of EURUSD will go up or down the following day using sentiments of those tweets that have been scraped. The problem I have with this project is how I would plan and structure my code, like for example should I use those tweets as features for the ML model or should I average the the sentiments of those tweets for that particular day and use them as features for the model to use. I will appreciate any advice from people that have worked on similar projects like these.</p>
","python, pandas, machine-learning, sentiment-analysis","<p>Provided that you have the tokens necessary you can test to do something like this:</p>
<pre><code>import pandas as pd
import snscrape.modules.twitter as sntwitter
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


tweets = []
for i, tweet in enumerate(sntwitter.TwitterSearchScraper('EURUSD').get_items()):
    if i &gt; 1000:
        break
    tweets.append([tweet.date, tweet.content])
df_tweets = pd.DataFrame(tweets, columns=['date', 'text'])
df_tweets.to_csv('tweets.csv', index=False)


def get_sentiment(text):
    sentiment = TextBlob(text).sentiment.polarity
    if sentiment &gt; 0:
        return 1
    elif sentiment &lt; 0:
        return -1
    else:
        return 0
df_tweets['sentiment'] = df_tweets['text'].apply(get_sentiment)


df_features = df_tweets.groupby('date').agg({'sentiment': 'mean'})
df_features.reset_index(inplace=True)


X_train, X_test, y_train, y_test = train_test_split(df_features.drop('date', axis=1), df_features['price_direction'], test_size=0.2, random_state=42)


rfc = RandomForestClassifier(n_estimators=100, random_state=42)
rfc.fit(X_train, y_train)


y_pred = rfc.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)


last_day_sentiment = df_features.iloc[-1]['sentiment']
next_day_direction = rfc.predict([[last_day_sentiment]])[0]
print('Next day direction:', next_day_direction)
</code></pre>
",-1,-3,54,2023-03-12 09:04:29,https://stackoverflow.com/questions/75712050/i-need-advice-on-sentimental-analysis-and-ml
UnimplementedError in python for ANN implementation,"<p>I have try to implement ANN implementation on Python. But iam getting below error message. How can I solve this issue?</p>
<pre><code>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)


from sklearn.preprocessing import StandardScaler
sc = StandardScaler(with_mean=False)
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)


import tensorflow as tf

ann = tf.keras.models.Sequential()

#adding first hidden layer
ann.add(tf.keras.layers.Dense(units=6,activation=&quot;relu&quot;))

#adding second hidden layer
ann.add(tf.keras.layers.Dense(units=6,activation=&quot;relu&quot;))

ann.add(tf.keras.layers.Dense(units=1,activation=&quot;sigmoid&quot;))
ann.compile(optimizer=&quot;adam&quot;,loss=&quot;binary_crossentropy&quot;,metrics=['accuracy'])
ann.fit(X_train,Y_train,batch_size=32,epochs = 100)
</code></pre>
<p>Iam getting this error message when I try to implement the ANN implementation on python.</p>
<pre><code>WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.
    Epoch 1/100
    ---------------------------------------------------------------------------
    UnimplementedError                        Traceback (most recent call last)
    Cell In[37], line 2
          1 #fitting ANN
    ----&gt; 2 ann.fit(X_train,Y_train,batch_size=32,epochs = 100)
</code></pre>
","python, jupyter-notebook, neural-network, sentiment-analysis","<p>Input data for <code>model.fit</code> could be</p>
<pre><code>A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).
A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs).
A dict mapping input names to the corresponding array/tensors, if the model has named inputs.
A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights).
A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights).
A tf.keras.utils.experimental.DatasetCreator, which wraps a callable that takes a single argument of type tf.distribute.InputContext, and returns a tf.data.Dataset. DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset. See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include sample_weights as a third component, note that sample weighting applies to the weighted_metrics argument but not the metrics argument in compile(). If using tf.distribute.experimental.ParameterServerStrategy, only DatasetCreator type is supported for x.
</code></pre>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">source</a></p>
<p>Looks like a problem with datatypes of <code>x,y</code> you are passing to <code>ann.fit</code> method.</p>
",1,0,81,2023-04-09 18:55:19,https://stackoverflow.com/questions/75972320/unimplementederror-in-python-for-ann-implementation
aspect sentiment analysis using Hugging face,"<p>I am new to transformers models and trying to extract aspect and sentiment for a sentence but having issues</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = &quot;yangheng/deberta-v3-base-absa-v1.1&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
text = &quot;The food was great but the service was terrible.&quot;
inputs = tokenizer(text, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)


</code></pre>
<p>I am able to get the tensor what I need is the output to extract the aspect and sentiment for the overall sentence</p>
<p>I tried this however getting error</p>
<pre><code>sentiment_scores = outputs.logits.softmax(dim=1)
aspect_scores = sentiment_scores[:, 1:-1]

aspects = [tokenizer.decode([x]) for x in inputs[&quot;input_ids&quot;].squeeze()][1:-1]
sentiments = ['Positive' if score &gt; 0.5 else 'Negative' for score in aspect_scores.squeeze()]

for aspect, sentiment in zip(aspects, sentiments):
    print(f&quot;{aspect}: {sentiment}&quot;)
</code></pre>
<p>I am looking for below o/p or similar o/p</p>
<p>I am unable to write the logic as to how extract aspect and sentiment</p>
<pre><code>text -The food was great but the service was terrible

aspect- food ,sentiment positive
aspect - service, sentiment negative


or at overall level

aspect - food, sentiment positive

</code></pre>
","python, nlp, huggingface-transformers, sentiment-analysis","<p>The model you are trying to use predicts the sentiment for a given aspect based on a text. That means, it requires <code>text</code> and <code>aspect</code> to perform a prediction. It was not trained to extract aspects from a text. You could use a keyword extraction model to extract aspects (compare <a href=""https://stackoverflow.com/a/76337458/6664872"">this SO answer</a>).</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = &quot;yangheng/deberta-v3-base-absa-v1.1&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

aspects = [&quot;food&quot;, &quot;service&quot;]
text = &quot;The food was great but the service was terrible.&quot;
sentiment_aspect = {}
for aspect in aspects:
  inputs = tokenizer(text, aspect, return_tensors=&quot;pt&quot;)

  with torch.inference_mode():
    outputs = model(**inputs)

  scores = F.softmax(outputs.logits[0], dim=-1)
  label_id = torch.argmax(scores).item()
  sentiment_aspect[aspect] = (model.config.id2label[label_id], scores[label_id].item())

print(sentiment_aspect)
</code></pre>
<p>Output:</p>
<pre><code>{'food': ('Positive', 0.9973154664039612), 'service': ('Negative', 0.9935430288314819)}
</code></pre>
",1,3,1481,2023-05-07 19:50:01,https://stackoverflow.com/questions/76195972/aspect-sentiment-analysis-using-hugging-face
How to generate sentiment scores using predefined aspects with deberta-v3-base-absa-v1.1 Huggingface model?,"<p>I have a dataframe , where there is text in 1st column and predefine aspect in another column however there is no aspects defined for few text  ,for example row 2.</p>
<pre><code>data = {
    'text': [
        &quot;The camera quality of this phone is amazing.&quot;,
        &quot;The belt is poor quality&quot;,
        &quot;The battery life could be improved.&quot;,
        &quot;The display is sharp and vibrant.&quot;,
        &quot;The customer service was disappointing.&quot;
    ],
    'aspects': [
        [&quot;camera&quot;, &quot;phone&quot;],
        [],
        [&quot;battery&quot;, &quot;life&quot;],
        [&quot;display&quot;],
        [&quot;customer service&quot;]
    ]
}

df = pd.DataFrame(data)

</code></pre>
<p>I want to generate two things</p>
<ol>
<li>using pre define aspect for the text, generate sentiment score</li>
<li>using text generate aspect and also the sentiment score from the package</li>
</ol>
<p>Note: This package yangheng/deberta-v3-base-absa-v1.1</p>
<p>1)generate sentiment score based on predefine aspects</p>
<p>2)generate both aspect and it's respective sentiments</p>
<p><strong>Note Row 2 does not have predefine aspect</strong></p>
<p><strong>I tried and getting error</strong></p>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import pandas as pd

# Load the ABSA model and tokenizer
model_name = &quot;yangheng/deberta-v3-base-absa-v1.1&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)




# Generate aspects and sentiments
aspects = []
sentiments = []

for index, row in df.iterrows():
    text = row['text']
    row_aspects = row['aspects']
    
    aspect_sentiments = []
    
    for aspect in row_aspects:
        inputs = tokenizer(text, aspect, return_tensors=&quot;pt&quot;)
        
        with torch.inference_mode():
            outputs = model(**inputs)
        
        predicted_sentiment = torch.argmax(outputs.logits).item()
        sentiment_label = model.config.id2label[predicted_sentiment]
        
        aspect_sentiments.append(f&quot;{aspect}: {sentiment_label}&quot;)
    
    aspects.append(row_aspects)
    sentiments.append(aspect_sentiments)

# Add the generated aspects and sentiments to the DataFrame
df['generated_aspects'] = aspects
df['generated_sentiments'] = sentiments

# Print the updated DataFrame
print(df)



</code></pre>
<p><strong>generic example to use the package</strong></p>
<pre><code>import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = &quot;yangheng/deberta-v3-base-absa-v1.1&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

aspects = [&quot;food&quot;, &quot;service&quot;]
text = &quot;The food was great but the service was terrible.&quot;
sentiment_aspect = {}
for aspect in aspects:
  inputs = tokenizer(text, aspect, return_tensors=&quot;pt&quot;)

  with torch.inference_mode():
    outputs = model(**inputs)

  scores = F.softmax(outputs.logits[0], dim=-1)
  label_id = torch.argmax(scores).item()
  sentiment_aspect[aspect] = (model.config.id2label[label_id], scores[label_id].item())

print(sentiment_aspect)

</code></pre>
<p><strong>Desired Output</strong></p>
<p><a href=""https://i.sstatic.net/Fe9Lq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fe9Lq.png"" alt=""enter image description here"" /></a></p>
","python, nlp, huggingface-transformers, sentiment-analysis, large-language-model","<p>Specific to the <code>yangheng/deberta-v3-base-absa-v1.1</code> model this is the usage and you have to loop through the model one time per aspect:</p>
<pre><code># Load the ABSA model and tokenizer
model_name = &quot;yangheng/deberta-v3-base-absa-v1.1&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

classifier = pipeline(&quot;text-classification&quot;, model=model, tokenizer=tokenizer)


for aspect in ['camera', 'phone']:
   print(aspect, classifier('The camera quality of this phone is amazing.',  text_pair=aspect))
</code></pre>
<p>[out]:</p>
<pre><code>camera [{'label': 'Positive', 'score': 0.9967294931411743}]
phone [{'label': 'Neutral', 'score': 0.9472787380218506}]
</code></pre>
<hr />
<p>To get the zero-shot classification scores in general, try using <code>pipeline</code>:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import pipeline


# Load the ABSA model and tokenizer
model_name = &quot;yangheng/deberta-v3-base-absa-v1.1&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)


pipe = pipeline(&quot;zero-shot-classification&quot;, model=model, tokenizer=tokenizer)

pipe(&quot;The camera quality of this phone is amazing.&quot;, candidate_labels=[&quot;camera&quot;, &quot;phone&quot;])
</code></pre>
<p>[out]:</p>
<pre><code>{'sequence': 'The camera quality of this phone is amazing.',
 'labels': ['camera', 'phone'],
 'scores': [0.9036691784858704, 0.09633082151412964]}
</code></pre>
<hr />
<p>Depending on what &quot;text generated aspect&quot; means, perhaps it's keyword extraction, and if so, doing a search on <a href=""https://huggingface.co/models?search=keyword"" rel=""nofollow noreferrer"">https://huggingface.co/models?search=keyword</a>, gives this as the top downloaded model, <a href=""https://huggingface.co/yanekyuk/bert-uncased-keyword-extractor"" rel=""nofollow noreferrer"">https://huggingface.co/yanekyuk/bert-uncased-keyword-extractor</a></p>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer2 = AutoTokenizer.from_pretrained(&quot;yanekyuk/bert-uncased-keyword-extractor&quot;)
model2 = AutoModelForTokenClassification.from_pretrained(&quot;yanekyuk/bert-uncased-keyword-extractor&quot;)



def extract_aspect(text):
    extractor = pipeline(&quot;ner&quot;, model=model2, tokenizer=tokenizer2)
    phrasesids = []
    for tag in extractor(text):
        if tag['entity'].startswith('B'):
            phrasesids.append([tag['start'], tag['end']])
        if tag['entity'].startswith('I'):
            phrasesids[-1][-1] = tag['end']
    phrases = [text[p[0]:p[1]] for p in phrasesids]
    return phrases

text = &quot;The camera quality of this phone is amazing.&quot;

extract_aspect(text)
</code></pre>
<p>[out]:</p>
<pre><code>camera
</code></pre>
<p>Putting the extractor and classifier together:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification
from transformers import pipeline


# Load the ABSA model and tokenizer
model_name = &quot;yangheng/deberta-v3-base-absa-v1.1&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

classifier = pipeline(&quot;zero-shot-classification&quot;, model=model, tokenizer=tokenizer)

tokenizer2 = AutoTokenizer.from_pretrained(&quot;yanekyuk/bert-uncased-keyword-extractor&quot;)
model2 = AutoModelForTokenClassification.from_pretrained(&quot;yanekyuk/bert-uncased-keyword-extractor&quot;)


def extract_aspect(text):
    extractor = pipeline(&quot;ner&quot;, model=model2, tokenizer=tokenizer2)
    phrasesids = []
    for tag in extractor(text):
        if tag['entity'].startswith('B'):
            phrasesids.append([tag['start'], tag['end']])
        if tag['entity'].startswith('I'):
            phrasesids[-1][-1] = tag['end']
    phrases = [text[p[0]:p[1]] for p in phrasesids]
    return phrases

text = &quot;The camera quality of this phone is amazing.&quot;

pipe(text, candidate_labels=extract_aspect(text))
</code></pre>
<p>[out]:</p>
<pre><code>{'sequence': 'The camera quality of this phone is amazing.',
 'labels': ['camera'],
 'scores': [0.9983300566673279]}
</code></pre>
<hr />
<h3>Q: But the extracted keywords is not &quot;right&quot; or doesn't match the pre-defined ones?</h3>
<p>A: No model is perfect and the model example above is a keyword extractor not a product aspect extractor. YMMV.</p>
<h3>Q: Why isn't the zero-shot classifier giving me negative / positive labels?</h3>
<p>A: The zero-shot classifier is labelling the data based on the extracted labels. Not a sentiment classifier.</p>
",2,1,1032,2023-05-26 01:06:19,https://stackoverflow.com/questions/76337058/how-to-generate-sentiment-scores-using-predefined-aspects-with-deberta-v3-base-a
How to get the logits of the model with a text classification pipeline from HuggingFace?,"<p>I need to use <code>pipeline</code> in order to get the tokenization and inference from the <code>distilbert-base-uncased-finetuned-sst-2-english</code> model over my dataset.</p>
<p>My data is a list of sentences, for recreation purposes we can assume it is:</p>
<p><code>texts = [&quot;this is the first sentence&quot;, &quot;of my data.&quot;, &quot;In fact, thats not true,&quot;, &quot;but we are going to assume it&quot;, &quot;is&quot;]</code></p>
<p>Before using <code>pipeline</code>, I was getting the logits from the model outputs like this:</p>
<pre><code>with torch.no_grad():
     logits = model(**tokenized_test).logits
</code></pre>
<p>Now I have to use pipeline, so this is the way I'm getting the model's output:</p>
<pre><code> selected_model = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;
 tokenizer = AutoTokenizer.from_pretrained(selected_model)
 model = AutoModelForSequenceClassification.from_pretrained(selected_model, num_labels=2)
 classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
 print(classifier(text))
</code></pre>
<p>which gives me:</p>
<p><code>[{'label': 'POSITIVE', 'score': 0.9746173024177551}, {'label': 'NEGATIVE', 'score': 0.5020197629928589}, {'label': 'NEGATIVE', 'score': 0.9995120763778687}, {'label': 'NEGATIVE', 'score': 0.9802979826927185}, {'label': 'POSITIVE', 'score': 0.9274746775627136}]</code></p>
<p>And I cant get the 'logits' field anymore.</p>
<p>Is there a way to get the <code>logits</code> instead of the <code>label</code> and <code>score</code>? Would a custom pipeline be the best and/or easiest way to do it?</p>
","python, huggingface-transformers, sentiment-analysis, huggingface, large-language-model","<p>When you use the default <code>pipeline</code>, the postprocess function will usually take the softmax, e.g.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')
model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')


text = ['hello this is a test',
 'that transforms a list of sentences',
 'into a list of list of sentences',
 'in order to emulate, in this case, two batches of the same lenght',
 'to be tokenized by the hf tokenizer for the defined model']

classifier(text, batch_size=2, truncation=&quot;only_first&quot;)
</code></pre>
<p>[out]:</p>
<pre><code>[{'label': 'NEGATIVE', 'score': 0.9379090666770935},
 {'label': 'POSITIVE', 'score': 0.9990271329879761},
 {'label': 'NEGATIVE', 'score': 0.9726701378822327},
 {'label': 'NEGATIVE', 'score': 0.9965035915374756},
 {'label': 'NEGATIVE', 'score': 0.9913086891174316}]
</code></pre>
<p>So what you want is to overload the postprocess logic by inheriting from the pipeline.</p>
<p>To check which pipeline the classifier inherits do this:</p>
<pre><code>classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
type(classifier)
</code></pre>
<p>[out]:</p>
<pre><code>transformers.pipelines.text_classification.TextClassificationPipeline
</code></pre>
<p>Now that you know the parent class of the task pipeline you want to use, now you can do this and still enjoy the perks of the precoded batching from <code>TextClassificationPipeline</code>:</p>
<pre><code>from transformers import TextClassificationPipeline

class MarioThePlumber(TextClassificationPipeline):
    def postprocess(self, model_outputs):
        best_class = model_outputs[&quot;logits&quot;]
        return best_class

pipe = MarioThePlumber(model=model, tokenizer=tokenizer)

pipe(text, batch_size=2, truncation=&quot;only_first&quot;)
</code></pre>
<p>[out]:</p>
<pre><code>[tensor([[ 1.5094, -1.2056]]),
 tensor([[-3.4114,  3.5229]]),
 tensor([[ 1.8835, -1.6886]]),
 tensor([[ 3.0780, -2.5745]]),
 tensor([[ 2.5383, -2.1984]])]
</code></pre>
",10,8,4405,2023-06-08 17:26:56,https://stackoverflow.com/questions/76434311/how-to-get-the-logits-of-the-model-with-a-text-classification-pipeline-from-hugg
Emoji count and analysis using python pandas,"<p>I am working on a sentiment analysis topic and there are a lot of comments with emojis.</p>
<p>I would like to know if my code is correct or is there a way to optimize it as well?</p>
<p><strong>Code to do smiley count</strong></p>
<pre><code>import pandas as pd
import regex as re
import emoji

# Assuming your DataFrame is called 'df' and the column with comments is 'Document'
comments = df['Document']

# Initialize an empty dictionary to store smiley counts and types
smiley_data = {'Smiley': [], 'Count': [], 'Type': []}

# Define a regular expression pattern to match smileys
pattern = r'([\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF])'

# Iterate over the comments
for comment in comments:
    # Extract smileys and their types from the comment
    smileys = re.findall(pattern, comment)
    
    # Increment the count and store the smileys and their types
    for smiley in smileys:
        if smiley in smiley_data['Smiley']:
            index = smiley_data['Smiley'].index(smiley)
            smiley_data['Count'][index] += 1
        else:
            smiley_data['Smiley'].append(smiley)
            smiley_data['Count'].append(1)
            smiley_data['Type'].append(emoji.demojize(smiley))
            
# Create a DataFrame from the smiley data
smiley_df = pd.DataFrame(smiley_data)

# Sort the DataFrame by count in descending order
smiley_df = smiley_df.sort_values(by='Count', ascending=False)

# Print the smiley data
smiley_df

</code></pre>
<p>I am majorly not sure if my below code block is getting all the smileys</p>
<pre><code># Define a regular expression pattern to match smileys
pattern = r'([\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF])'
</code></pre>
<p>would like to know what can I do with this analysis. something else on top of it - some charts maybe?</p>
<p>I am also sharing a test dataset that will generate similar smiley counts as those available in my real data. Please note that the test dataset only has known smileys if there is something else. it won't be there like in a real dataset.</p>
<p><strong>Test Dataset</strong></p>
<pre><code>import random
import pandas as pd

smileys = ['👍', '👌', '😍', '🏻', '😊', '🙂', '👎', '😃', '🏼', '💩']

# Additional smileys to complete the required count
additional_smileys = ['😄', '😎', '🤩', '😘', '🤗', '😆', '😉', '😋', '😇', '🥳', '🙌', '🎉', '🔥', '🥰', '🤪', '😜', '🤓',
                      '😚', '🤭', '🤫', '😌', '🥱', '🥶', '🤮', '🤡', '😑', '😴', '🙄', '😮', '🤥', '😢', '🤐', '🙈', '🙊',
                      '👽', '🤖', '🦄', '🐼', '🐵', '🦁', '🐸', '🦉']

# Combine the required smileys and additional smileys
all_smileys = smileys + additional_smileys

# Set a random seed for reproducibility
random.seed(42)

# Generate a single review
def generate_review(with_smiley=False):
    review = &quot;This movie&quot;
    if with_smiley:
        review += &quot; &quot; + random.choice(all_smileys)
    review += &quot; is &quot;
    review += random.choice([&quot;amazing&quot;, &quot;excellent&quot;, &quot;fantastic&quot;, &quot;brilliant&quot;, &quot;great&quot;, &quot;good&quot;, &quot;okay&quot;, &quot;average&quot;,
                             &quot;mediocre&quot;, &quot;disappointing&quot;, &quot;terrible&quot;, &quot;awful&quot;, &quot;horrible&quot;])
    review += random.choice([&quot;!&quot;, &quot;!!&quot;, &quot;!!!&quot;, &quot;.&quot;, &quot;..&quot;, &quot;...&quot;]) + &quot; &quot;
    review += random.choice([&quot;Highly recommended&quot;, &quot;Definitely worth watching&quot;, &quot;A must-see&quot;, &quot;I loved it&quot;,
                             &quot;Not worth your time&quot;, &quot;Skip it&quot;]) + random.choice([&quot;!&quot;, &quot;!!&quot;, &quot;!!!&quot;])
    return review

# Generate the random dataset
def generate_dataset():
    dataset = []
    review_count = 5000

    # Generate reviews with top smileys
    for smiley, count, _ in top_smileys:
        while count &gt; 0:
            review = generate_review(with_smiley=True)
            if smiley in review:
                dataset.append(review)
                count -= 1

    # Generate reviews with additional smileys
    additional_smileys_count = len(additional_smileys)
    additional_smileys_per_review = review_count - len(dataset)
    additional_smileys_per_review = min(additional_smileys_per_review, additional_smileys_count)

    for _ in range(additional_smileys_per_review):
        review = generate_review(with_smiley=True)
        dataset.append(review)

    # Generate reviews without smileys
    while len(dataset) &lt; review_count:
        review = generate_review()
        dataset.append(review)

    # Shuffle the dataset
    random.shuffle(dataset)
    return dataset

# List of top smileys and their counts
top_smileys = [
    ('👍', 331, ':thumbs_up:'),
    ('👌', 50, ':OK_hand:'),
    ('😍', 41, ':smiling_face_with_heart-eyes:'),
    ('🏻', 38, ':light_skin_tone:'),
    ('😊', 35, ':smiling_face_with_smiling_eyes:'),
    ('🙂', 14, ':slightly_smiling_face:'),
    ('👎', 12, ':thumbs_down:'),
    ('😃', 12, ':grinning_face_with_big_eyes:'),
    ('🏼', 10, ':medium-light_skin_tone:'),
    ('💩', 10, ':pile_of_poo:')
]

# Generate the dataset
dataset = generate_dataset()

# Create a data frame with 'Document' column
df = pd.DataFrame({'Document': dataset})

# Display the DataFrame
df

</code></pre>
<p>Thank you in advance!</p>
","python, pandas, emoji, sentiment-analysis, emoticons","<p><strong>Update</strong></p>
<p>If you prefer to use <code>emoji</code> package, you can do:</p>
<pre><code>import emoji

text = df['Document'].str.cat(sep='\n')
out = (pd.DataFrame(emoji.emoji_list(text)).value_counts('emoji')
         .rename_axis('Smiley').rename('Count').reset_index()
         .assign(Type=lambda x: x['Smiley'].apply(emoji.demojize)))
</code></pre>
<p>Output:</p>
<pre><code>&gt;&gt;&gt; out
   Smiley  Count                              Type
0       👍    331                       :thumbs_up:
1       👌     50                         :OK_hand:
2       🏻     41                 :light_skin_tone:
3       😍     41    :smiling_face_with_heart-eyes:
4       😊     35  :smiling_face_with_smiling_eyes:
5       🙂     15           :slightly_smiling_face:
6       👎     14                     :thumbs_down:
7       😃     13     :grinning_face_with_big_eyes:
8       🏼     10          :medium-light_skin_tone:
9       💩     10                     :pile_of_poo:
10      😜      3        :winking_face_with_tongue:
11      🦉      3                             :owl:
12      🤖      2                           :robot:
13      😑      2             :expressionless_face:
14      👽      2                           :alien:
15      🤫      2                   :shushing_face:
16      😢      2                     :crying_face:
17      🤪      2                       :zany_face:
18      🙈      2              :see-no-evil_monkey:
19      🙊      2            :speak-no-evil_monkey:
20      😇      1          :smiling_face_with_halo:
21      🤮      1                   :face_vomiting:
22      🤭      1       :face_with_hand_over_mouth:
23      🤡      1                      :clown_face:
24      🤗      1    :smiling_face_with_open_hands:
25      🙄      1          :face_with_rolling_eyes:
26      😆      1         :grinning_squinting_face:
27      🐸      1                            :frog:
28      😮      1            :face_with_open_mouth:
29      🐼      1                           :panda:
30      😚      1   :kissing_face_with_closed_eyes:
31      😎      1    :smiling_face_with_sunglasses:
32      😘      1             :face_blowing_a_kiss:
</code></pre>
<hr />
<p>You can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extractall.html"" rel=""nofollow noreferrer""><code>str.extractall</code></a> to avoid a loop then use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html"" rel=""nofollow noreferrer""><code>value_counts</code></a> to count the number of occurences. Finally, <em>&quot;demojize&quot;</em> each smiley (the slowest part):</p>
<pre><code>out = (df['Document'].str.extractall(pattern).value_counts()
                     .rename_axis('Smiley').rename('Count').reset_index()
                     .assign(Type=lambda x: x['Smiley'].apply(emoji.demojize)))
</code></pre>
<p>Output:</p>
<pre><code>&gt;&gt;&gt; out
   Smiley  Count                              Type
0       👍    331                       :thumbs_up:
1       👌     50                         :OK_hand:
2       🏻     41                 :light_skin_tone:
3       😍     41    :smiling_face_with_heart-eyes:
4       😊     35  :smiling_face_with_smiling_eyes:
5       🙂     15           :slightly_smiling_face:
6       👎     14                     :thumbs_down:
7       😃     13     :grinning_face_with_big_eyes:
8       💩     10                     :pile_of_poo:
9       🏼     10          :medium-light_skin_tone:
10      😜      3        :winking_face_with_tongue:
11      😑      2             :expressionless_face:
12      🙈      2              :see-no-evil_monkey:
13      😢      2                     :crying_face:
14      🙊      2            :speak-no-evil_monkey:
15      👽      2                           :alien:
16      😎      1    :smiling_face_with_sunglasses:
17      😘      1             :face_blowing_a_kiss:
18      😚      1   :kissing_face_with_closed_eyes:
19      🐸      1                            :frog:
20      😇      1          :smiling_face_with_halo:
21      😮      1            :face_with_open_mouth:
22      😆      1         :grinning_squinting_face:
23      🙄      1          :face_with_rolling_eyes:
24      🐼      1                           :panda:
</code></pre>
<blockquote>
<p>The pattern part is correct? I am not missing out on any emoticons?</p>
</blockquote>
<p>Your pattern is not right. I don't know the full list you want to extract but below you have a code to debug it:</p>
<pre><code>#     add latin1 codes --v
pattern2 = '([\\U00000000-\\U000000FF\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF])'

other = df['Document'].str.replace(pattern2, '', regex=True)
print(other[other != ''])

# Output / Missed emojis
1149    🤗
1238    🦉
1305    🤫
1424    🤫
1978    🤭
2611    🤮
2623    🦉
2959    🤡
3717    🤪
4045    🦉
4067    🤖
4699    🤖
4975    🤪
Name: Document, dtype: object
</code></pre>
",3,1,834,2023-06-27 09:30:34,https://stackoverflow.com/questions/76563480/emoji-count-and-analysis-using-python-pandas
Deleting certain segments of strings in R?,"<p>In R, suppose I have the following string:</p>
<pre><code>x&lt;-&quot;The bank is going after bank one and pizza and corn.&quot;
</code></pre>
<p>I would like to delete all segments of the string before the <strong>FINAL</strong> time <strong>bank</strong> appears in the sentence, thus obtaining the string &quot;one and pizza and corn.&quot; More generally, if I want to delete all text before the <strong>final time a specific word appears</strong> in a string, if there a way to do this?</p>
","r, string, text, text-mining, sentiment-analysis","<p>You can use group cature regex as shown below:</p>
<pre><code>words &lt;- &quot;bank&quot;
pat &lt;- sprintf(&quot;^.*?(\\b%s\\b).*\\1 ?&quot;, paste0(words, collapse = &quot;|&quot;))
sub(pat, &quot;&quot;, x, perl = TRUE)
</code></pre>
<p>Explanation <code>^</code> from begining the sentence, match <code>.*?</code> anything 0 or many times lazily until the first appearance of the bounded <code>word</code>. From here match greedily everything until you meet the word the last time. Replace everything with a empty string <code>''</code></p>
",2,0,35,2023-07-12 21:05:19,https://stackoverflow.com/questions/76674439/deleting-certain-segments-of-strings-in-r
Web scrape hyperlinked text in R?,"<p><a href=""https://www.nber.org/papers?page=1&amp;perPage=50&amp;sortBy=public_date"" rel=""nofollow noreferrer"">https://www.nber.org/papers?page=1&amp;perPage=50&amp;sortBy=public_date</a></p>
<p>The above webpage consists of a series of academic papers. The titles of these papers (e.g, <em>Sparse Modeling Under Grouped Heterogeneity with an Application to Asset Pricing</em>) are hyperlinked to pages with more detail on them; so, if you click on these titles (hyperlinked text) it directs you to pages with more detail.</p>
<p>Is there any way to scrape all these links in R? I would like all the <strong>links attached to the titles of the academic papers</strong>, not hyperlinks related to other things like people's names. I do not want the titles themselves, just the links they are attached to.</p>
","html, r, web-scraping, text-mining, sentiment-analysis","<p>The abstracts and links are loaded dynamically onto the page using an xhr call which fetches a JSON file to populate the html. If you want to get the links quickly and efficiently, you can download the json directly and parse it. You will find the json url using your browser's console.</p>
<p>Here's a full reprex:</p>
<pre class=""lang-r prettyprint-override""><code>urls &lt;- &quot;https://www.nber.org/api/v1/working_page_listing/contentType/&quot; |&gt;
  paste0(&quot;working_paper/_/_/search?page=1&amp;perPage=50&amp;sortBy=public_date&quot;) |&gt;
  httr::GET() |&gt;
  httr::content(&quot;parsed&quot;) |&gt;
  getElement(&quot;results&quot;) |&gt;
  sapply(function(x) x$url)
</code></pre>
<p>If you want the complete urls, rather than relative ones, simply paste the domain on in front.</p>
<pre class=""lang-r prettyprint-override""><code>paste0(&quot;https://www.nber.org&quot;, urls)
#&gt;  [1] &quot;https://www.nber.org/papers/w31388&quot; &quot;https://www.nber.org/papers/w31424&quot;
#&gt;  [3] &quot;https://www.nber.org/papers/w31482&quot; &quot;https://www.nber.org/papers/w31477&quot;
#&gt;  [5] &quot;https://www.nber.org/papers/w31478&quot; &quot;https://www.nber.org/papers/w31479&quot;
#&gt;  [7] &quot;https://www.nber.org/papers/w31480&quot; &quot;https://www.nber.org/papers/w31481&quot;
#&gt;  [9] &quot;https://www.nber.org/papers/w31490&quot; &quot;https://www.nber.org/papers/w31502&quot;
#&gt; [11] &quot;https://www.nber.org/papers/w31486&quot; &quot;https://www.nber.org/papers/w31483&quot;
#&gt; [13] &quot;https://www.nber.org/papers/w31484&quot; &quot;https://www.nber.org/papers/w31485&quot;
#&gt; [15] &quot;https://www.nber.org/papers/w31494&quot; &quot;https://www.nber.org/papers/w31489&quot;
#&gt; [17] &quot;https://www.nber.org/papers/w31496&quot; &quot;https://www.nber.org/papers/w31491&quot;
#&gt; [19] &quot;https://www.nber.org/papers/w31493&quot; &quot;https://www.nber.org/papers/w31488&quot;
#&gt; [21] &quot;https://www.nber.org/papers/w31495&quot; &quot;https://www.nber.org/papers/w31497&quot;
#&gt; [23] &quot;https://www.nber.org/papers/w31498&quot; &quot;https://www.nber.org/papers/w31499&quot;
#&gt; [25] &quot;https://www.nber.org/papers/w31500&quot; &quot;https://www.nber.org/papers/w31501&quot;
#&gt; [27] &quot;https://www.nber.org/papers/w31487&quot; &quot;https://www.nber.org/papers/w31503&quot;
#&gt; [29] &quot;https://www.nber.org/papers/w31476&quot; &quot;https://www.nber.org/papers/w31492&quot;
#&gt; [31] &quot;https://www.nber.org/papers/w31450&quot; &quot;https://www.nber.org/papers/w31449&quot;
#&gt; [33] &quot;https://www.nber.org/papers/w31448&quot; &quot;https://www.nber.org/papers/w31453&quot;
#&gt; [35] &quot;https://www.nber.org/papers/w31451&quot; &quot;https://www.nber.org/papers/w31452&quot;
#&gt; [37] &quot;https://www.nber.org/papers/w31454&quot; &quot;https://www.nber.org/papers/w31455&quot;
#&gt; [39] &quot;https://www.nber.org/papers/w31465&quot; &quot;https://www.nber.org/papers/w31458&quot;
#&gt; [41] &quot;https://www.nber.org/papers/w31459&quot; &quot;https://www.nber.org/papers/w31460&quot;
#&gt; [43] &quot;https://www.nber.org/papers/w31461&quot; &quot;https://www.nber.org/papers/w31472&quot;
#&gt; [45] &quot;https://www.nber.org/papers/w31473&quot; &quot;https://www.nber.org/papers/w31475&quot;
#&gt; [47] &quot;https://www.nber.org/papers/w31474&quot; &quot;https://www.nber.org/papers/w31470&quot;
#&gt; [49] &quot;https://www.nber.org/papers/w31462&quot; &quot;https://www.nber.org/papers/w31471&quot;
</code></pre>
<p>These are all the complete links to the articles on the first page. They are not in the order they appear on the page; I'm unsure whether these are just randomized.</p>
<p><sup>Created on 2023-07-24 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.0.2</a></sup></p>
",3,0,58,2023-07-24 20:25:36,https://stackoverflow.com/questions/76757814/web-scrape-hyperlinked-text-in-r
How can I implement real-time sentiment analysis on live audio streams using Python?,"<p>I'm currently working on a project where I need to perform real-time sentiment analysis on live audio streams using Python. The goal is to analyze the sentiment expressed in the spoken words and provide insights in real-time. I've done some research and found resources on text-based sentiment analysis, but I'm unsure about how to adapt these techniques to audio streams.</p>
<p><strong>Context and Efforts:</strong></p>
<p>Research: I've researched various libraries and tools for sentiment analysis, such as Natural Language Processing (NLP) libraries like NLTK and spaCy. However, most resources I found focus on text data rather than audio.</p>
<p>Audio Processing: I'm familiar with libraries like pyaudio and soundfile in Python for audio recording and processing. I've successfully captured live audio streams using these libraries.</p>
<p>Text-to-Speech Conversion: I've experimented with converting the spoken words from the audio streams into text using libraries like SpeechRecognition to prepare the data for sentiment analysis.</p>
<p><strong>Challenges:</strong></p>
<p>Sentiment Analysis: My main challenge is adapting the sentiment analysis techniques to audio data. I'm not sure if traditional text-based sentiment analysis models can be directly applied to audio, or if there are specific approaches for this scenario.</p>
<p>Real-Time Processing: I'm also concerned about the real-time aspect of the analysis. How can I ensure that the sentiment analysis is performed quickly enough to provide insights in real-time without introducing significant delays?</p>
<p><strong>Question:</strong></p>
<p>I'm seeking guidance on the best approach to implement real-time sentiment analysis on live audio streams using Python. Are there any specialized libraries or techniques for audio-based sentiment analysis that I should be aware of? How can I effectively process the audio data and perform sentiment analysis in real-time? Any insights, code examples, or recommended resources would be greatly appreciated.</p>
","python, speech-recognition, real-time, sentiment-analysis, audio-processing","<p>The solution I found here by following these steps:</p>
<ul>
<li>Adding middleware to make text from video</li>
<li>Running the sentiment analysis for each <strong>complete sentence</strong> (Using Whisper as @doneforaiur suggested)</li>
</ul>
<p><strong>So, the latency here depends on the length of the words of the sentence. It's a trade-off for getting the analysis for the full context. Making chunks doesn't reflect the whole sentence thus the complete context.</strong></p>
<p>There is a possible way to extend this by adding multiple sentence feeds to the analysis system to get deeper context. I'm thinking about it. Sometimes, we describe a scenario with multiple sentences.</p>
<p>Thanks, <strong>@doneforaiur</strong> for setting me in the right direction.</p>
",0,4,2463,2023-08-16 19:15:06,https://stackoverflow.com/questions/76916457/how-can-i-implement-real-time-sentiment-analysis-on-live-audio-streams-using-pyt
Combing Search Terms in gtrendsR package?,"<p>Within R, I am using the package gtrendsR. On the web version of google trends, one can &quot;combine&quot; search terms by putting plus signs between them as follows:
<a href=""https://i.sstatic.net/fRbpL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fRbpL.png"" alt=""enter image description here"" /></a></p>
<p>I believe this effectively makes the search term apple+orange a kind of sum of both of these search terms. I know one could write multiple separate search terms within gtrendsR, such as <code>gtrends(c(&quot;apple&quot;, &quot;orange&quot;)</code>, but this is not what I want to do.</p>
<p>Is there any way to perform the same operation shown in the picture (apple+orange) with the gtrendsR package? I have been unable to figure out how to do this.</p>
","r, sentiment-analysis, google-search, google-trends","<p>What have you tried?</p>
<pre><code>&gt; library(gtrendsR)
&gt; res &lt;- gtrends(&quot;apple+orange&quot;)
&gt; plot(res)
</code></pre>
<p>yields pretty much your answer (modulo different time resolution which you can adjust).</p>
<p><a href=""https://i.sstatic.net/apF62.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/apF62.png"" alt=""enter image description here"" /></a></p>
",1,0,54,2023-08-28 22:24:22,https://stackoverflow.com/questions/76996341/combing-search-terms-in-gtrendsr-package
"Will using arguments - max_length, truncate, and padding in tranformers pipeline affect the output?","<p>Hello so I was checking sentiment of a text using transformers pretrained model ,but doing so gave me error</p>
<blockquote>
<p>RuntimeError: The size of tensor a (1954) must match the size of tensor b (512) at non-singleton dimension 1</p>
</blockquote>
<p>I went through few post which suggested that setting <code>max_length</code> as 512 will sort the error.
It did resolve the error, but I want to know how it affects the quality of output. Does it truncate my text? For example, if the length of my text is 1195 will it process till 512, something like text[:512]?</p>
","python, nlp, huggingface-transformers, sentiment-analysis","<p>Yes. It means the sentiment will be based on the first 512 <em>tokens</em>, and any tokens after that will not influence the result.</p>
<p>Note that this is tokens, not characters. If <code>text</code> was your raw string, and if we assume that on average each token is 2.5 characters, then truncating at 512 tokens would be the same as <code>text[:1280]</code>.</p>
<p>(The characters per token can vary a lot based on the model, the tokenizer, the language, the domain, but mainly how unusual the string is compared to the text used to train the tokenizer.)</p>
<p>By the way, according to <a href=""https://huggingface.co/docs/transformers/pad_truncation"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/pad_truncation</a> if you don't specify <code>truncation</code> then no truncation is applied; and if you do, but don't specify <code>max_length</code> then it will default to the maximum supported by the model. So setting <code>max_length</code> and not changing anything else shouldn't have fixed it. (I've not tested anything, or read the code, that is just based on my understanding of the documentation.)</p>
",0,0,864,2023-08-30 18:08:05,https://stackoverflow.com/questions/77010524/will-using-arguments-max-length-truncate-and-padding-in-tranformers-pipeline
How can i get the first content of a python synsets list?,"<p><a href=""https://i.sstatic.net/gN3qc.png"" rel=""nofollow noreferrer"">enter image description here</a>I have a scrapped text stored under the variable &quot;message&quot;.
I have removed the StopWords and stored the result with the variable &quot;without_stop_words&quot;.
I want to loop through each words in the 'without_stop_words' and get their MEANINGS AND PRONOUNS.</p>
<p>Currently i am trying to get the MEANINGS but I'm getting an error: &quot;IndexError: list index out of range&quot;</p>
<p><a href=""https://i.sstatic.net/V55ve.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<pre><code>   
 for writeup in writeups:
        message = writeup.text
        #Stop Words
        stop_words = set(stopwords.words('english'))
        #print(stop_words)

        tokenized_words = word_tokenize(message)
        #Filtering Stop Words
        without_stop_words = []
        for word in tokenized_words:
            if word not in stop_words:
                without_stop_words.append(word)            
                #Word Meanings
        word_meanings = []
        for each_word in without_stop_words:
            sync_words = wordnet.synsets(each_word)
            meaning = sync_words[0].definition()
            print(meaning)

</code></pre>
<p>I want to get the MEANING of each word in the &quot;without_stop_words&quot;.</p>
","python, nlp, nltk, sentiment-analysis, synset","<p>The error comes from this line</p>
<pre><code>meaning = sync_words[0].definition()
</code></pre>
<p>And it indicates that <code>sync_words</code> is empty</p>
<pre><code>for each_word in without_stop_words:
    sync_words = wordnet.synsets(each_word)
    if sync_words:
        meaning = sync_words[0].definition()
        word_meanings.append(meaning)
    else:
        # Whatever you want to do if it's empty
</code></pre>
<p>This will stop the error, but you should try to find out why <code>sync_words</code> is empty in the first place.</p>
",1,1,42,2024-03-23 14:53:18,https://stackoverflow.com/questions/78211318/how-can-i-get-the-first-content-of-a-python-synsets-list
Capitalized words in sentiment analysis,"<p>I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive .
A common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and  distinction between sentiments as good , very good than just positive to include the importance of this upper case words .</p>
<p>this is my current code :</p>
<pre><code>from itertools import chain

def is_upper_case(text):
  return [word for word in text.split() if word.isupper() and word != 'I']

unique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))
print(unique_upper_words)
</code></pre>
","nlp, sentiment-analysis, bert-language-model, data-preprocessing","<p>If you are using a BERT-based model (or any other LLM) to do the actual classification I would recommend to not use any preprocessing at all (at least when it comes to capitalization), as these models were pre-trained on non-preprocessed data.</p>
<p>If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group n-grams and to simplify the analysis.</p>
<p>If you are thinking about having multiple classes to have a better distinction between the prediction, I think it would make most sense if you switch to a sentiment regression instead of a classification, where you predict a value in a continuous range. This comes somewhat natural to the fine-tuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax, so for your needs you can just skip that last step and directly use the model output. Many python ML frameworks for fine-tuning or using language models have their own classes for regression tasks, check out <a href=""https://github.com/EliasK93/transformer-model-comparison-for-review-sentiment-regression"" rel=""nofollow noreferrer"">this repository</a> as an example.</p>
",0,-1,128,2024-08-30 13:49:56,https://stackoverflow.com/questions/78932356/capitalized-words-in-sentiment-analysis
How can one obtain the &quot;correct&quot; embedding layer in BERT?,"<p>I want to utilize BERT to assess the similarity between two pieces of text:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
import numpy as np

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-chinese&quot;)
model = AutoModel.from_pretrained(&quot;bert-classifier&quot;)

def calc_similarity(s1, s2):
    inputs = tokenizer(s1, s2, return_tensors='pt', padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

    cosine_similarity = F.cosine_similarity(embeddings[0], embeddings[1])
    return cosine_similarity
</code></pre>
<p>The similarity presented here is derived from a BERT sentiment classifier, which is a model fine-tuned based on the BERT architecture.</p>
<p>My inquiry primarily revolves around this line of code：</p>
<pre class=""lang-py prettyprint-override""><code>embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
</code></pre>
<p>I have observed at least three different implementations regarding this line; in addition to the aforementioned version that retrieves the first row, there are two other variations:</p>
<pre class=""lang-py prettyprint-override""><code>embeddings = outputs.last_hidden_state.mean(axis=1).cpu().numpy()
</code></pre>
<p>and</p>
<pre class=""lang-py prettyprint-override""><code>embeddings = model.bert.pooler(outputs.last_hidden_state.cpu().numpy())
</code></pre>
<p>In fact, vector <code>outputs.last_hidden_state</code> is a 9*768 tensor, and the three aforementioned methods can transform it into a 1*768 vector, thereby providing a basis for subsequent similarity calculations. From my perspective, the first approach is not appropriate within the semantic space defined by the classification task, as our objective is not to predict the next word. What perplexes me is the choice between the second and third methods, specifically whether to employ a simple average or to utilize the pooling layer of the model itself.</p>
<p>Any assistance would be greatly appreciated!</p>
","pytorch, sentiment-analysis, similarity, bert-language-model","<p>1st approach is not a good choice because leveraging the [CLS] token embedding directly might not be the best approach, in case if the BERT  was fine tuned for a task other than similarity matching.</p>
<ul>
<li><strong>Task-Specific Embeddings</strong>: The [CLS] token embedding is affected by the task the bert model was trained on.</li>
<li><strong>Averaging</strong> : Taking the mean of all token embeddings, we can get a more general representation of the input. This method balances out the representation by considering the contextual embeddings of all tokens.</li>
</ul>
<p>Consider taking average or pooling (passing through another dense layer) will work.</p>
",1,3,36,2024-12-04 03:15:26,https://stackoverflow.com/questions/79249787/how-can-one-obtain-the-correct-embedding-layer-in-bert
Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged,"<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>
<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>
<p>This is several information about the dataset:</p>
<p>Embedding size: (41151, 100)</p>
<p>Maximum sequence length: 731</p>
<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>
<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>
<p>Total x training set (padded): (140997, 200)</p>
<p>Total x validation set (padded): (17625, 200)</p>
<p>Total x testing set (padded): (17625, 200)</p>
<p>Total y training set (one hot): (140997, 3)</p>
<p>Total y validation set (one hot): (17625, 3)</p>
<p>Total y testing set (one hot): (17625, 2003</p>
<p>This is my full code:
<a href=""https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model"" rel=""nofollow noreferrer"">enter link description here</a></p>
<p>This is my highlight code for this issue:</p>
<pre><code>lstm_model = Sequential()
lstm_model.add(Input(shape=(max_len,)))
lstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))
lstm_model.add(LSTM(256, return_sequences=True))
lstm_model.add(LSTM(128, return_sequences=True))
lstm_model.add(LSTM(64))
lstm_model.add(Dense(128, activation='relu'))
lstm_model.add(Dense(units=3, activation='softmax'))

lstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

lstm_model.summary()
</code></pre>
<p><a href=""https://i.sstatic.net/T6vCZ9Jj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/T6vCZ9Jj.png"" alt=""enter image description here"" /></a></p>
","keras, deep-learning, nlp, lstm, sentiment-analysis","<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>
<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>
<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.
This is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.
(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>
",0,-1,45,2025-02-07 02:48:25,https://stackoverflow.com/questions/79419884/underfitting-pre-trained-glove-lstm-model-accurcacy-unchanged
