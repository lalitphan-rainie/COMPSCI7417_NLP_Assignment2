&quot;pre-built&quot; matrices for latent semantic analysis,"<p>I want to use Latent Semantic Analysis for a small app I'm building, but I don't want to build up the matrices myself. (Partly because the documents I have wouldn't make a very good training collection, because they're kinda short and heterogeneous, and partly because I just got a new computer and I'm finding it a bitch to install the linear algebra and such libraries I would need.)</p>

<p>Are there any ""default""/pre-built LSA implementations available? For example, things I'm looking for include:</p>

<ul>
<li>Default U,S,V matrices (i.e., if D is a term-document matrix from some training set, then D = U S V^T is the singular value decomposition), so that given any query vector q, I can use these matrices to compute the LSA projection of q myself.</li>
<li>Some black-box LSA algorithm that, given a query vector q, returns the LSA projection of q.</li>
</ul>
","nlp, machine-learning, latent-semantic-indexing, gensim","<p>You'd probably be interested in the <a href=""http://nlp.fi.muni.cz/projekty/gensim/"" rel=""nofollow"">Gensim</a> framework for Python; notably, it has <a href=""http://nlp.fi.muni.cz/projekty/gensim/wiki.html"" rel=""nofollow"">an example on building the appropriate matrices from English Wikipedia</a>.</p>
",2,2,490,2010-11-06 04:59:22,https://stackoverflow.com/questions/4111979/pre-built-matrices-for-latent-semantic-analysis
LSI using gensim in python,"<p>I'm using Python's gensim library to do latent semantic indexing.  I followed the tutorials on the website, and it works pretty well.  Now I'm trying to modify it a bit; I want to be run the lsi model each time a document is added.</p>

<p>Here is my code:</p>

<pre><code>stoplist = set('for a of the and to in'.split())
num_factors=3
corpus = []

for i in range(len(urls)):
 print ""Importing"", urls[i]
 doc = getwords(urls[i])
 cleandoc = [word for word in doc.lower().split() if word not in stoplist]
 if i == 0:
  dictionary = corpora.Dictionary([cleandoc])
 else:
  dictionary.addDocuments([cleandoc])
 newVec = dictionary.doc2bow(cleandoc)
 corpus.append(newVec)
 tfidf = models.TfidfModel(corpus)
 corpus_tfidf = tfidf[corpus]
 lsi = models.LsiModel(corpus_tfidf, numTopics=num_factors, id2word=dictionary)
 corpus_lsi = lsi[corpus_tfidf]
</code></pre>

<p>geturls is function I wrote that returns the contents of a website as a string.  Again, it works if I wait until I process all of the documents before doing tfidf and lsi, but that's not what I want.  I want to do it on each iteration.  Unfortunately, I get this error:</p>

<pre><code>    Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""streamlsa.py"", line 51, in &lt;module&gt;
    lsi = models.LsiModel(corpus_tfidf, numTopics=num_factors, id2word=dictionary)
  File ""/Library/Python/2.6/site-packages/gensim-0.7.8-py2.6.egg/gensim/models/lsimodel.py"", line 303, in __init__
    self.addDocuments(corpus)
  File ""/Library/Python/2.6/site-packages/gensim-0.7.8-py2.6.egg/gensim/models/lsimodel.py"", line 365, in addDocuments
    self.printTopics(5) # TODO see if printDebug works and remove one of these..
  File ""/Library/Python/2.6/site-packages/gensim-0.7.8-py2.6.egg/gensim/models/lsimodel.py"", line 441, in printTopics
    self.printTopic(i, topN = numWords)))
  File ""/Library/Python/2.6/site-packages/gensim-0.7.8-py2.6.egg/gensim/models/lsimodel.py"", line 433, in printTopic
    return ' + '.join(['%.3f*""%s""' % (1.0 * c[val] / norm, self.id2word[val]) for val in most])
  File ""/Library/Python/2.6/site-packages/gensim-0.7.8-py2.6.egg/gensim/corpora/dictionary.py"", line 52, in __getitem__
    return self.id2token[tokenid] # will throw for non-existent ids
KeyError: 1248
</code></pre>

<p>Usually the error pops up on the second document.  I think I understand what it's telling me (the dictionary indices are bad), I just can't figure out WHY.  I've tried lots of different things and nothing seems to work.  Does anyone know what's going on?</p>

<p>Thanks!</p>
","python, latent-semantic-indexing, gensim","<p>This was a bug in gensim, where the reverse id->word mapping gets cached, but the cache didn't get updated after <code>addDocuments()</code>.</p>

<p>It got fixed in this commit in 2011: <a href=""https://github.com/piskvorky/gensim/commit/b88225cfda8570557d3c72b0820fefb48064a049"" rel=""nofollow"">https://github.com/piskvorky/gensim/commit/b88225cfda8570557d3c72b0820fefb48064a049</a> .</p>
",4,4,7790,2011-06-09 02:20:59,https://stackoverflow.com/questions/6287411/lsi-using-gensim-in-python
Eclipse + PyDev ImportError,"<p>I am having trouble getting <strong>PyDev on Eclipse</strong> to recognize installed modules (<strong>gensim</strong>), which work fine in IDLE. I am using Windows Vista, 32-bit. Python 2.7.</p>

<p>I have found this question asked: <a href=""https://stackoverflow.com/questions/6070423/adding-python-modules-to-pydev-in-eclipse-results-in-import-error"">here</a>, <a href=""https://stackoverflow.com/questions/2590435/eclipse-pydev-eclipse-telling-me-that-this-is-an-invalid-import"">here</a>, <a href=""https://stackoverflow.com/questions/4631377/unresolved-import-issues-with-pydev-and-eclipse"">here</a>, and <a href=""https://stackoverflow.com/questions/2983088/unresolved-import-models"">here</a>.</p>

<p>The recommended solution is to go to <strong>preferences > pydev > interpreter - python</strong>, and remove and re-add (w/ Auto Config) the python interpreter. I have done this, and have restarted Eclipse. In <code>PYTHONPATH</code>, <code>C:\Python27\lib\site-packages\gensim-0.8.0-py2.7.egg</code>, appears, but I still run into the import error. My code is:</p>

<pre><code>from gensim import corpora, models, similarities
</code></pre>

<p>And this yields:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\Jpan\Documents\workspace\FirstProject\src\gensim.py"", line 1, in &lt;module&gt;
    from gensim import corpora, models, similarities
  File ""C:\Users\Jpan\Documents\workspace\FirstProject\src\gensim.py"", line 1, in &lt;module&gt;
    from gensim import corpora, models, similarities
ImportError: cannot import name corpora
</code></pre>

<p>Another recommended solution is to manually add the folder by clicking ""New Folder"" in the bottom part of the interpreter - python screen and navigating to the location where gensim installed. I have also done this, and added <code>C:\Python27\lib\site-packages\gensim-0.8.0-py2.7.egg\gensim</code>, which has all the necessary <code>\__init__.py</code> files. But, I still get the <code>ImportError</code>.</p>

<p>Any suggestions for what else I could try?</p>
","python, eclipse, import, pydev, gensim","<p>This is independent of Eclipse/PyDev. You'll get the same error running the code in any other way. Your module imports <code>gensim</code>. The first entry on the <code>PYTHONPATH</code> is the current directory, and your module is called <code>gensim.py</code>, so your module attempts to import iteself. Because imports are cached, you don't get into infinite recursion but get a reference to a module containing... nothing, especially not the things you expected from the ""real"" <code>gensim</code> module.</p>

<p>The error message should mention this possibility, it's incredibly common. The solution is to rename your file.</p>
",6,1,4824,2011-07-07 18:48:15,https://stackoverflow.com/questions/6615569/eclipse-pydev-importerror
How to use gensim for lda on news articles?,"<p>I'm trying to retrieve list of topics from a large corpus of news articles, I'm planning to use gensim to extract a topic distribution for each document using LDA. I want to know the format of processed articles required by gensim implementation of lda and how to convert raw articles to that format. I saw this link about using lda on wikipedia dump but I found the corpus to be in a processed state whose format was not mentioned anywhere</p>
","machine-learning, lda, gensim","<p>There is an offline learning step and an online feature creation step.</p>

<p><strong>Offline Learning</strong></p>

<p>Assume you have a big corpus such as Wikipedia or downloaded a bunch of news articles.</p>

<p>For each article/document:</p>

<ol>
<li>You get the raw text</li>
<li>You lemmatize it. Gensim has utils.lemmatize</li>
<li>You create a dictionary </li>
<li>You create a bag of word representation</li>
</ol>

<p>Then you train the TF-IDF model and convert the whole corpus to the TF-IDF space.
Finally, you train the LDA model on the ""TF-IDF corpus"".</p>

<p><strong>Online</strong></p>

<p>With an incoming news article you do almost the same:</p>

<ol>
<li>Lemmatize it</li>
<li>Create a bag of word representaiton using the dictionary.</li>
<li>Convert it to TF-IDF space using the TF-IDF model</li>
<li>Convert it to LDA space.</li>
</ol>
",3,3,2436,2012-04-02 00:31:23,https://stackoverflow.com/questions/9969599/how-to-use-gensim-for-lda-on-news-articles
Extracting a word plus 20 more from a section (python),"<p>Jep still playing around with Python. </p>

<p>I decided to try out Gensim, a tool to find out topics for a choosen word &amp; context. </p>

<p>So I wondered how to find a word in a section of text and extract 20 words together with it (as in 10 words before that spectic word and 10 words after that specific word) then to save it together with other such extractions so Gensim could be run on it.</p>

<p>What seems to be hard for me is to find a way to extract the 10 before and after words when the choosen word is found. I played with nltk before and by just tokenizing the text into words or sentences it was easy to get hold of the sentences. Still getting those words or the sentences before and after that specific sentence seems hard for me to figure out how to do.</p>

<p>For those who are confused (it's 1am here so I may be confusing) I'll show it with an example:</p>

<blockquote>
  <p>As soon as it had finished, all her blood rushed to her heart, for she
  was so angry to hear that Snow-White was yet living. ""But now,""
  thought she to herself, ""will I make something which shall destroy her
  completely."" Thus saying, she made a poisoned comb by arts which she
  understood, and then, disguising herself, she took the form of an old
  widow. She went over the seven hills to the house of the seven Dwarfs,
  and[15] knocking at the door, called out, ""Good wares to sell to-day!""</p>
</blockquote>

<p>If we say the word is Snow-White then I'd want to get this part extracted:</p>

<blockquote>
  <p>her heart, for she was so angry to hear that Snow-White was yet living. ""But now,""
  thought she to herself, ""will</p>
</blockquote>

<p>10 word before and after Snow-White. </p>

<p>It is also cool enough to instead get the sentence before and after the sentence Snow-White appeared in if this can be done in nltk and is easier. </p>

<p>I mean whatever works best I shall be happy with one of the two solutions if someone could help me.</p>

<p>If this can be done with Gensim too...and that is easier, then I shall be happy with that too. So any of the 3 ways will be fine...I just want to try and see how this can be done because atm my head is blank.</p>
","python, nltk, extract, gensim","<pre><code>text = """"""
As soon as it had finished, all her blood rushed to her heart, for she was so angry to hear that Snow-White was yet living. ""But now,"" thought she to herself, ""will I make something which shall destroy her completely."" Thus saying, she made a poisoned comb by arts which she understood, and then, disguising herself, she took the form of an old widow. She went over the seven hills to the house of the seven Dwarfs, and[15] knocking at the door, called out, ""Good wares to sell to-day!""
""""""
spl = text.split()

def ans(word):
    for ind, x in enumerate(spl):
       if x.strip("",'\"".!"") == word:
           break
    return "" "".join(spl[ind-10:ind] + spl[ind:ind+11])


&gt;&gt;&gt; ans('Snow-White')
her heart, for she was so angry to hear that Snow-White was yet living. ""But now,"" thought she to herself, ""will
</code></pre>
",7,5,1993,2012-05-11 23:00:15,https://stackoverflow.com/questions/10559591/extracting-a-word-plus-20-more-from-a-section-python
Finding topics of an unseen document via Gensim,"<p>I am using Gensim to do some large-scale topic modeling. I am having difficulty understanding how to determine predicted topics for an unseen (non-indexed) document. For example: I have 25 million documents which I have converted to vectors in LSA (and LDA) space. I now want to figure out the topics of a new document, lets call it x.</p>

<p>According to the Gensim documentation, I can use:</p>

<pre><code>topics = lsi[doc(x)]
</code></pre>

<p>where doc(x) is a function that converts x into a vector.</p>

<p>The problem is, however, that the above variable, topics, returns a vector. The vector is useful if I am comparing x to additional documents because it allows me to find the cosine similarity between them, but I am unable to actually return specific words that are associated with x itself.</p>

<p>Am I missing something, or does Gensim not have this capability?</p>

<p>Thank you,</p>

<p><strong>EDIT</strong></p>

<p>Larsmans has the answer.</p>

<p>I was able to show the topics by using:</p>

<pre><code>for t in topics:
    print lsi.show_topics(t[0])
</code></pre>
","python, nlp, latent-semantic-indexing, gensim","<p>The vector returned by <code>[]</code> on an LSI model is actually a list of <code>(topic, weight)</code> pairs. You can inspect a topic by means of the method <a href=""http://radimrehurek.com/gensim/models/lsimodel.html#gensim.models.lsimodel.LsiModel.show_topic"" rel=""noreferrer""><code>LsiModel.show_topic</code></a></p>
",6,7,11481,2012-07-13 13:22:10,https://stackoverflow.com/questions/11471376/finding-topics-of-an-unseen-document-via-gensim
Is stemming used when gensim creates a dictionary for tf-idf model?,"<p>I am using Gensim python toolkit to build tf-idf model for documents. So I need to create a dictionary for all documents first. However, I found Gensim does not use stemming before creating the dictionary and corpus. Am I right ? </p>
","python, nlp, gensim","<p>You are correct. Gensim doesn't do anything special other than convert what you give it into different models.</p>

<p>Here is the relevant quote and the link that it is from:</p>

<blockquote>
  <p>The ways to process documents are so varied and application- and
  language-dependent that I decided to not constrain them by any
  interface. Instead, a document is represented by the features
  extracted from it, not by its “surface” string form: how you get to
  the features is up to you.</p>
</blockquote>

<p><a href=""http://radimrehurek.com/gensim/tut1.html#from-strings-to-vectors"" rel=""nofollow"">From Strings to Vectors</a></p>
",3,2,1038,2013-01-22 21:11:09,https://stackoverflow.com/questions/14468078/is-stemming-used-when-gensim-creates-a-dictionary-for-tf-idf-model
Why did the tf-idf model in `gensim` throws away the terms and counts after i transform the corpus?,"<p>Why did the tf-idf model in <code>gensim</code> throws away the terms and counts after i transform the corpus?</p>

<p>My code:</p>

<pre><code>from gensim import corpora, models, similarities

# Let's say you have a corpus made up of 2 documents.
doc0 = [(0, 1), (1, 1)]
doc1 = [(0,1)]
doc2 = [(0, 1), (1, 1)]
doc3 = [(0, 3), (1, 1)]

corpus = [doc0,doc1,doc2,doc3]

# Train a tfidf model using the corpus
tfidf = models.TfidfModel(corpus)

# Now if you print the corpus, it still remains as the flat frequency counts.
for d in corpus:
  print d
print 

# To convert the corpus into tfidf, re-initialize the corpus 
# according to the model to get the normalized frequencies.
corpus = tfidf[corpus]

for d in corpus:
  print d
</code></pre>

<p>Outputs:</p>

<pre><code>[(0, 1.0), (1, 1.0)]
[(0, 1.0)]
[(0, 1.0), (1, 1.0)]
[(0, 3.0), (1, 1.0)]

[(1, 1.0)]
[]
[(1, 1.0)]
[(1, 1.0)]
</code></pre>
","python, nlp, information-retrieval, tf-idf, gensim","<p>IDF is obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. In your case, all the documents has term0, so IDF for term0 is log(1), equal to 0. So in your doc-term matrix, the column for term0 is all zeros. </p>

<p>A term that appears in all documents has zero weight, it carries absolutely no information. </p>
",6,2,2129,2013-02-23 01:40:55,https://stackoverflow.com/questions/15036048/why-did-the-tf-idf-model-in-gensim-throws-away-the-terms-and-counts-after-i-tr
LDA model generates different topics everytime i train on the same corpus,"<p>I am using python <code>gensim</code> to train an Latent Dirichlet Allocation (LDA) model from a small corpus of 231 sentences. However, each time i repeat the process, it generates different topics. </p>

<p><strong>Why does the same LDA parameters and corpus generate different topics everytime?</strong></p>

<p><strong>And how do i stabilize the topic generation?</strong></p>

<p>I'm using this corpus (<a href=""http://pastebin.com/WptkKVF0"">http://pastebin.com/WptkKVF0</a>) and this list of stopwords (<a href=""http://pastebin.com/LL7dqLcj"">http://pastebin.com/LL7dqLcj</a>) and here's my code:</p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models import hdpmodel, ldamodel
from itertools import izip
from collections import defaultdict
import codecs, os, glob, math

stopwords = [i.strip() for i in codecs.open('stopmild','r','utf8').readlines() if i[0] != ""#"" and i != """"]

def generateTopics(corpus, dictionary):
    # Build LDA model using the above corpus
    lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)
    corpus_lda = lda[corpus]

    # Group topics with similar words together.
    tops = set(lda.show_topics(50))
    top_clusters = []
    for l in tops:
        top = []
        for t in l.split("" + ""):
            top.append((t.split(""*"")[0], t.split(""*"")[1]))
        top_clusters.append(top)

    # Generate word only topics
    top_wordonly = []
    for i in top_clusters:
        top_wordonly.append("":"".join([j[1] for j in i]))

    return lda, corpus_lda, top_clusters, top_wordonly

####################################################################### 

# Read textfile, build dictionary and bag-of-words corpus
documents = []
for line in codecs.open(""./europarl-mini2/map/coach.en-es.all"",""r"",""utf8""):
    lemma = line.split(""\t"")[3]
    documents.append(lemma)
texts = [[word for word in document.lower().split() if word not in stopwords]
             for document in documents]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda, corpus_lda, topic_clusters, topic_wordonly = generateTopics(corpus, dictionary)

for i in topic_wordonly:
    print i
</code></pre>
","python, nlp, lda, topic-modeling, gensim","<blockquote>
  <p>Why does the same LDA parameters and corpus generate different topics everytime?</p>
</blockquote>

<p>Because LDA uses randomness in both training and inference steps.</p>

<blockquote>
  <p>And how do i stabilize the topic generation?</p>
</blockquote>

<p>By resetting the <code>numpy.random</code> seed to the same value every time a model is trained or inference is performed, with <code>numpy.random.seed</code>:</p>

<pre><code>SOME_FIXED_SEED = 42

# before training/inference:
np.random.seed(SOME_FIXED_SEED)
</code></pre>

<p>(This is ugly, and it makes Gensim results hard to reproduce; consider submitting a patch. I've already opened an <a href=""https://github.com/piskvorky/gensim/issues/113"">issue</a>.)</p>
",32,19,19852,2013-02-25 13:08:28,https://stackoverflow.com/questions/15067734/lda-model-generates-different-topics-everytime-i-train-on-the-same-corpus
Which gensim corpora class should I use to load an LDA transformed corpus? - Python,"<p><strong>How do I load an LDA transformed corpus from python's <code>gensim</code> ?</strong> What i've tried:</p>

<pre><code>from gensim import corpora, models
import numpy.random
numpy.random.seed(10)

doc0 = [(0, 1), (1, 1)]
doc1 = [(0,1)]
doc2 = [(0, 1), (1, 1)]
doc3 = [(0, 3), (1, 1)]

corpus = [doc0,doc1,doc2,doc3]
dictionary = corpora.Dictionary(corpus)

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
corpus_tfidf.save('x.corpus_tfidf')

# To access the tfidf fitted corpus i've saved i used corpora.MmCorpus.load()
corpus_tfidf = corpora.MmCorpus.load('x.corpus_tfidf')

lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)
corpus_lda = lda[corpus]
corpus_lda.save('x.corpus_lda')

for i,j in enumerate(corpus_lda):
  print j, corpus[i]
</code></pre>

<p>The above code will output:</p>

<pre><code>[(0, 0.54259038344543631), (1, 0.45740961655456358)] [(0, 1), (1, 1)]
[(0, 0.56718063124157458), (1, 0.43281936875842542)] [(0, 1)]
[(0, 0.54255407573666647), (1, 0.45744592426333358)] [(0, 1), (1, 1)]
[(0, 0.75229707773868093), (1, 0.2477029222613191)] [(0, 3), (1, 1)]

# [(&lt;topic_number_from x.corpus_lda model&gt;, 
#   &lt;probability of this topic for this document&gt;), 
#  (&lt;topic# from lda model&gt;, &lt;prob of this top for this doc&gt;)] [&lt;document[i] from corpus&gt;]
</code></pre>

<p><strong>If i want to load the saved LDA transformed corpus, which class from <code>gensim</code> should i be using to load?</strong></p>

<p>I have tried using <code>corpora.MmCorpus.load()</code>, it doesn't give me the same output of the transformed corpus as shown above:</p>

<pre><code>&gt;&gt;&gt; lda_corpus = corpora.MmCorpus.load('x.corpus_lda')
&gt;&gt;&gt; for i,j in enumerate(lda_corpus):
...   print j, corpus[i]
... 
[(0, 0.55087839240547309), (1, 0.44912160759452685)] [(0, 1), (1, 1)]
[(0, 0.56715974584850259), (1, 0.43284025415149735)] [(0, 1)]
[(0, 0.54275680271070581), (1, 0.45724319728929413)] [(0, 1), (1, 1)]
[(0, 0.75233330695720912), (1, 0.24766669304279079)] [(0, 3), (1, 1)]
</code></pre>
","python, nlp, corpus, lda, gensim","<p>There are more problems in your code.</p>

<p>To save a corpus in MatrixMarket format, you'd </p>

<pre><code>corpora.MmCorpus.serialize('x.corpus_lda', corpus_lda)
</code></pre>

<p>The docs are <a href=""http://radimrehurek.com/gensim/tut1.html#corpus-formats"" rel=""nofollow"">here</a>.</p>

<p>You're training on <code>corpus_tfidf</code>, but then transforming only <code>lda[corpus]</code> (no tfidf). Either use tfidf or plain bag-of-words, but use it consistently.</p>
",4,4,3303,2013-03-03 10:20:55,https://stackoverflow.com/questions/15184655/which-gensim-corpora-class-should-i-use-to-load-an-lda-transformed-corpus-pyt
Gensim topic printing errors/issues,"<p>All,</p>

<p>This is a re-post to what I responded to over in <a href=""https://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python"">this thread</a>. I am getting some totally screwy results with trying to print LSI topics in gensim. Here is my code:</p>

<pre><code>try:
    from gensim import corpora, models
except ImportError as err:
    print err

class LSI:
    def topics(self, corpus):
        tfidf = models.TfidfModel(corpus)
        corpus_tfidf = tfidf[corpus]
        dictionary = corpora.Dictionary(corpus)
        lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)
        print lsi.show_topics()

if __name__ == '__main__':
    data = '../data/data.txt'
    corpus = corpora.textcorpus.TextCorpus(data)
    LSI().topics(corpus)
</code></pre>

<p>This prints the following to the console.</p>

<pre><code>-0.804*""(5, 1)"" + -0.246*""(856, 1)"" + -0.227*""(145, 1)"" + ......
</code></pre>

<p>I would like to be able to print out the topics like @2er0 did <a href=""https://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python"">over here</a> but I am getting results like these. See below and note that the second item that is printed is a tuple and I have no idea where it came from. data.txt is a text file with several paragraphs in it. That is all.</p>

<p>Any thoughts on this would be fantastic! Adam</p>
","python, topic-modeling, gensim","<p>To answer why your LSI topics are tuples instead of words, check your input corpus. </p>

<p><strong>is it created from a list of documents that is converted into corpus through <code>corpus = [dictionary.doc2bow(text) for text in texts]</code> ?</strong></p>

<p>Because if it isn't and you just read it from serialized corpus without reading a dictionary, then you wont get the words in the topic outputs.</p>

<p>Below my code works and prints out the topics with weighted words:</p>

<pre><code>import gensim as gs

documents = [""Human machine interface for lab abc computer applications"",
             ""A survey of user opinion of computer system response time"",
             ""The EPS user interface management system"",
             ""System and human system engineering testing of EPS"",
             ""Relation of user perceived response time to error measurement"",
             ""The generation of random binary unordered trees"",
             ""The intersection graph of paths in trees"",
             ""Graph minors IV Widths of trees and well quasi ordering"",
             ""Graph minors A survey""]

texts = [[word for word in document.lower().split()] for document in documents]
dictionary = gs.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

tfidf = gs.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

lsi = gs.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)
lsi.print_topics()

for i in lsi.print_topics():
    print i
</code></pre>

<p>The above outputs: </p>

<pre><code>-0.331*""system"" + -0.329*""a"" + -0.329*""survey"" + -0.241*""user"" + -0.234*""minors"" + -0.217*""opinion"" + -0.215*""eps"" + -0.212*""graph"" + -0.205*""response"" + -0.205*""time""
-0.330*""minors"" + 0.313*""eps"" + 0.301*""system"" + -0.288*""graph"" + -0.274*""a"" + -0.274*""survey"" + 0.268*""management"" + 0.262*""interface"" + 0.208*""human"" + 0.189*""engineering""
0.282*""trees"" + 0.267*""the"" + 0.236*""in"" + 0.236*""paths"" + 0.236*""intersection"" + -0.233*""time"" + -0.233*""response"" + 0.202*""generation"" + 0.202*""unordered"" + 0.202*""binary""
-0.247*""generation"" + -0.247*""unordered"" + -0.247*""random"" + -0.247*""binary"" + 0.219*""minors"" + -0.214*""the"" + -0.214*""to"" + -0.214*""error"" + -0.214*""perceived"" + -0.214*""relation""
0.333*""machine"" + 0.333*""for"" + 0.333*""lab"" + 0.333*""abc"" + 0.333*""applications"" + 0.258*""computer"" + -0.214*""system"" + -0.194*""eps"" + -0.191*""and"" + -0.188*""testing""
</code></pre>
",4,0,1012,2013-03-07 00:24:44,https://stackoverflow.com/questions/15260864/gensim-topic-printing-errors-issues
How do you initialize a gensim corpus variable with a csr_matrix?,"<p>I have X as a csr_matrix that I obtained using scikit's tfidf vectorizer, and y which is an array</p>

<p>My plan is to create features using LDA, however, I failed to find how to initialize a gensim's corpus variable with X as a csr_matrix. In other words, I don't want to download a corpus as shown in gensim's documentation nor convert X to a dense matrix, since it would consume a lot of memory and the computer could hang.</p>

<p>In short, my questions are the following,</p>

<ol>
<li>How do you initialize a gensim corpus given that I have a csr_matrix (sparse) representing the whole corpus?</li>
<li>How do you use LDA to extract features?</li>
</ol>
","python, scikit-learn, document-classification, lda, gensim","<p>Gensim has a semi-well-hidden function that can kind of do this for you:</p>

<p><a href=""http://radimrehurek.com/gensim/matutils.html#gensim.matutils.Sparse2Corpus"">http://radimrehurek.com/gensim/matutils.html#gensim.matutils.Sparse2Corpus</a></p>

<p>""class gensim.matutils.Sparse2Corpus(sparse, documents_columns=True)
    Convert a matrix in scipy.sparse format into a streaming gensim corpus.""</p>

<p>I've had some success with it using a corpus extracted with CountVectorizer, then loaded into gensim.</p>
",10,7,1959,2013-03-27 22:12:52,https://stackoverflow.com/questions/15670525/how-do-you-initialize-a-gensim-corpus-variable-with-a-csr-matrix
Can we use a self made corpus for training for LDA using gensim?,"<p>I have to apply LDA (Latent Dirichlet Allocation) to get the possible topics from a data base of 20,000 documents that I collected.</p>

<p>How can I use these documents rather than the other corpus available like the Brown Corpus or English Wikipedia as training corpus ?</p>

<p>You can refer <a href=""http://radimrehurek.com/gensim/models/ldamodel.html"">this</a> page.</p>
","python, lda, gensim","<p>After going through the documentation of the Gensim package, I found out that there are total 4 ways of transforming a text repository into a corpus.</p>

<p>There are total 4 formats for the corpus:</p>

<ol>
<li>Market Matrix (.mm)</li>
<li>SVM Light (.svmlight)</li>
<li>Blie Format (.lad-c)</li>
<li>Low Format (.low)</li>
</ol>

<p>In this problem, as mentioned above there are total of 19,188 documents in the database.
One has to read each document and remove stopwords and punctuations from the sentences, which can be done using <code>nltk</code>.</p>

<pre><code>import gensim
from gensim import corpora, similarities, models

##
##Text Preprocessing is done here using nltk
##

##Saving of the dictionary and corpus is done here
##final_text contains the tokens of all the documents

dictionary = corpora.Dictionary(final_text)
dictionary.save('questions.dict');
corpus = [dictionary.doc2bow(text) for text in final_text]
corpora.MmCorpus.serialize('questions.mm', corpus)
corpora.SvmLightCorpus.serialize('questions.svmlight', corpus)
corpora.BleiCorpus.serialize('questions.lda-c', corpus)
corpora.LowCorpus.serialize('questions.low', corpus)

##Then the dictionary and corpus can be used to train using LDA

mm = corpora.MmCorpus('questions.mm')
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=dictionary, num_topics=100, update_every=0, chunksize=19188, passes=20)
</code></pre>

<p>This way one can transform his dataset to a corpus that can be trained for topic modelling using LDA using gensim package.</p>
",16,9,7628,2013-04-27 16:05:52,https://stackoverflow.com/questions/16254207/can-we-use-a-self-made-corpus-for-training-for-lda-using-gensim
How to predict the topic of a new query using a trained LDA model using gensim?,"<p>I have trained a corpus for LDA topic modelling using gensim.</p>

<p>Going through the tutorial on the gensim website (this is not the whole code):</p>

<pre><code>question = 'Changelog generation from Github issues?';

temp = question.lower()
for i in range(len(punctuation_string)):
    temp = temp.replace(punctuation_string[i], '')

words = re.findall(r'\w+', temp, flags = re.UNICODE | re.LOCALE)
important_words = []
important_words = filter(lambda x: x not in stoplist, words)
print important_words
dictionary = corpora.Dictionary.load('questions.dict')
ques_vec = []
ques_vec = dictionary.doc2bow(important_words)
print dictionary
print ques_vec
print lda[ques_vec]
</code></pre>

<p>This is the output that I get:</p>

<pre><code>['changelog', 'generation', 'github', 'issues']
Dictionary(15791 unique tokens)
[(514, 1), (3625, 1), (3626, 1), (3627, 1)]
[(4, 0.20400000000000032), (11, 0.20400000000000032), (19, 0.20263215848547525), (29, 0.20536784151452539)]
</code></pre>

<p>I don't know how the last output is going to help me find the possible topic for the <code>question</code> !!!</p>

<p>Please help!</p>
","python, nlp, lda, topic-modeling, gensim","<p>I have written a function in python that gives the possible topic for a new query:</p>

<pre><code>def getTopicForQuery (question):
    temp = question.lower()
    for i in range(len(punctuation_string)):
        temp = temp.replace(punctuation_string[i], '')

    words = re.findall(r'\w+', temp, flags = re.UNICODE | re.LOCALE)

    important_words = []
    important_words = filter(lambda x: x not in stoplist, words)

    dictionary = corpora.Dictionary.load('questions.dict')

    ques_vec = []
    ques_vec = dictionary.doc2bow(important_words)

    topic_vec = []
    topic_vec = lda[ques_vec]

    word_count_array = numpy.empty((len(topic_vec), 2), dtype = numpy.object)
    for i in range(len(topic_vec)):
        word_count_array[i, 0] = topic_vec[i][0]
        word_count_array[i, 1] = topic_vec[i][1]

    idx = numpy.argsort(word_count_array[:, 1])
    idx = idx[::-1]
    word_count_array = word_count_array[idx]

    final = []
    final = lda.print_topic(word_count_array[0, 0], 1)

    question_topic = final.split('*') ## as format is like ""probability * topic""

    return question_topic[1]
</code></pre>

<p>Before going through this do refer <a href=""https://stackoverflow.com/questions/16254207/can-we-use-a-self-made-corpus-for-training-for-lda-using-gensim"">this</a> link!</p>

<p>In the initial part of the code, the query is being pre-processed so that it can be stripped off stop words and unnecessary punctuations. </p>

<p>Then, the dictionary that was made by using our own database is loaded. </p>

<p>We, then, we convert the tokens of the new query to bag of words and then the topic probability distribution of the query is calculated by <code>topic_vec = lda[ques_vec]</code> where <code>lda</code> is the trained model as explained in the link referred above.</p>

<p>The distribution is then sorted w.r.t the probabilities of the topics. The topic with the highest probability is then displayed by <code>question_topic[1]</code>.</p>
",7,11,17388,2013-04-28 10:39:43,https://stackoverflow.com/questions/16262016/how-to-predict-the-topic-of-a-new-query-using-a-trained-lda-model-using-gensim
Gensim ImportError in PyCharm: No module named scipy.sparse,"<p>I am on Mac OS X 10.8.3 (Mountain Lion) and am trying to run a script in PyCharm.  Python 2.7.2 is installed, I have installed Canopy and Gensim.  I just do not understand what could be causing the error that I'm getting.</p>

<pre><code>scipy.__version__ 
</code></pre>

<p>shows that v 0.11 is installed.</p>

<p>Here is the entirety of my output following a run of the script:</p>

<pre><code>/System/Library/Frameworks/Python.framework/Versions/2.7/bin/python ""/util/LSA/Base LSA.py""

Traceback (most recent call last):

File ""/util/LSA/Base LSA.py"", line 8, in &lt;module&gt;
    from gensim import corpora, models, similarities, matutils
File ""/Library/Python/2.7/site-packages/gensim-0.8.6-py2.7.egg/gensim/__init__.py"", line 7, in &lt;module&gt;
    import utils, matutils, interfaces, corpora, models, similarities
File ""/Library/Python/2.7/site-packages/gensim-0.8.6-py2.7.egg/gensim/matutils.py"", line 20, in &lt;module&gt;
    import scipy.sparse
ImportError: No module named scipy.sparse

Process finished with exit code 1
</code></pre>
","python, scipy, pycharm, lda, gensim","<p>I'd suggest using the <a href=""http://www.python.org/download/"" rel=""noreferrer"">Python.org</a> version of Python, not the one that came with OSX, as there are some issues that are most easily overcome by installing the latest version - 2.7.4 in the case of the 2.x branch. Don't worry about breaking anything, both versions will happily coexist together. Once you have that, you can install the latest <a href=""http://sourceforge.net/projects/numpy/files/NumPy/1.7.1/"" rel=""noreferrer"">NumPy</a> and <a href=""http://sourceforge.net/projects/scipy/files/scipy/0.12.0/"" rel=""noreferrer"">SciPy</a> binaries (get the 10.6 <code>dmg</code> files). NumPy is required for SciPy to work. </p>

<p>Make sure you set up PyCharm to work with the new version of Python, and double-check that your modules are installed in the right <code>site-packages</code> directory (it should be <code>/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/</code>). You can always copy all of the files in your <code>/Library/Python/2.7/site-packages/</code> directory to the one I just mentioned, as the major.minor version of Python (2.7) is still the same. Then, you should be good to go. You will likely want to symlink <code>/usr/local/bin/python</code> to <code>/Library/Frameworks/Python.framework/Versions/2.7/bin/python</code> (it may be already) to make an easier shebang line, and don't forget to put <code>/usr/local/bin</code> in front of <code>/usr/bin</code> in your <code>PATH</code> for when you do command-line work, and for <code>#!/usr/bin/env python</code> shebangs. Good luck!</p>
",5,1,6712,2013-05-14 21:35:04,https://stackoverflow.com/questions/16553252/gensim-importerror-in-pycharm-no-module-named-scipy-sparse
Document topical distribution in Gensim LDA,"<p>I've derived a LDA topic model using a toy corpus as follows:</p>

<pre><code>documents = ['Human machine interface for lab abc computer applications',
             'A survey of user opinion of computer system response time',
             'The EPS user interface management system',
             'System and human system engineering testing of EPS',
             'Relation of user perceived response time to error measurement',
             'The generation of random binary unordered trees',
             'The intersection graph of paths in trees',
             'Graph minors IV Widths of trees and well quasi ordering',
             'Graph minors A survey']

texts = [[word for word in document.lower().split()] for document in documents]
dictionary = corpora.Dictionary(texts)

id2word = {}
for word in dictionary.token2id:    
    id2word[dictionary.token2id[word]] = word
</code></pre>

<p>I found that when I use a small number of topics to derive the model, Gensim yields a full report of topical distribution over all potential topics for a test document. E.g.:</p>

<pre><code>test_lda = LdaModel(corpus,num_topics=5, id2word=id2word)
test_lda[dictionary.doc2bow('human system')]

Out[314]: [(0, 0.59751626959781134),
(1, 0.10001902477790173),
(2, 0.10001375856907335),
(3, 0.10005453508763221),
(4, 0.10239641196758137)]
</code></pre>

<p>However when I use a large number of topics, the report is no longer complete:</p>

<pre><code>test_lda = LdaModel(corpus,num_topics=100, id2word=id2word)

test_lda[dictionary.doc2bow('human system')]
Out[315]: [(73, 0.50499999999997613)]
</code></pre>

<p>It seems to me that topics with a probability less than some threshold (I observed 0.01 to be more specific) are omitted form the output.</p>

<p>I'm wondering if this behaviour is due to some aesthetic considerations? And how can I get the distribution of the probability mass residual over all other topics?</p>

<p>Thank you for your kind answer! </p>
","python, lda, gensim","<p>Read the <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/ldamodel.py"" rel=""noreferrer"">source</a> and it turns out that topics with probabilities smaller than a threshold are ignored. This threshold is with a default value of 0.01.</p>
",8,17,11590,2013-06-26 03:13:39,https://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda
"Gensim: How to save LDA model&#39;s produced topics to a readable format (csv,txt,etc)?","<p>last parts of the code:   </p>

<pre><code>lda = LdaModel(corpus=corpus,id2word=dictionary, num_topics=2)
print lda
</code></pre>

<p>bash output:</p>

<pre><code>INFO : adding document #0 to Dictionary(0 unique tokens)
INFO : built Dictionary(18 unique tokens) from 5 documents (total  20 corpus positions)
INFO : using serial LDA version on this node
INFO : running online LDA training, 2 topics, 1 passes over the supplied corpus of 5 documents, updating model once every 5 documents
WARNING : too few updates, training might not converge; consider increasing the number of passes to improve accuracy
INFO : PROGRESS: iteration 0, at document #5/5
INFO : 2/5 documents converged within 50 iterations
INFO : topic #0: 0.079*cute + 0.076*broccoli + 0.070*adopted + 0.069*yesterday + 0.069*eat + 0.069*sister + 0.068*kitten + 0.068*kittens + 0.067*bananas + 0.067*chinchillas
INFO : topic #1: 0.082*broccoli + 0.079*cute + 0.071*piece + 0.070*munching + 0.069*spinach + 0.068*hamster + 0.068*ate + 0.067*banana + 0.066*breakfast + 0.066*smoothie
INFO : topic diff=0.470477, rho=1.000000
&lt;gensim.models.ldamodel.LdaModel object at 0x10f1f4050&gt;
</code></pre>

<p>So I'm wondering i'm able to save the resulting topics that it generated, to a readable format. I've tried the <code>.save()</code> methods, but it always outputs something unreadable. </p>
","python, lda, gensim","<p>you just need to use <code>lda.show_topics(topics=-1)</code> or any number of topics you want to have (topics=10, topics=15, topics=1000....). I am usually doing just: </p>

<pre><code>logfile = open('.../yourfile.txt', 'a')
print&gt;&gt;logfile, lda.show_topics(topics=-1, topn=10)
</code></pre>

<p>All these parameters and others are available in gensim <a href=""http://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow"">documentation</a>. </p>
",3,9,30106,2013-06-27 22:39:49,https://stackoverflow.com/questions/17354417/gensim-how-to-save-lda-models-produced-topics-to-a-readable-format-csv-txt-et
How to print out the full distribution of words in an LDA topic in gensim?,"<p>The <code>lda.show_topics</code> module from the following code only prints the distribution of the top 10 words for each topic, how do i print out the full distribution of all the words in the corpus?</p>

<pre><code>from gensim import corpora, models

documents = [""Human machine interface for lab abc computer applications"",
""A survey of user opinion of computer system response time"",
""The EPS user interface management system"",
""System and human system engineering testing of EPS"",
""Relation of user perceived response time to error measurement"",
""The generation of random binary unordered trees"",
""The intersection graph of paths in trees"",
""Graph minors IV Widths of trees and well quasi ordering"",
""Graph minors A survey""]

stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)

for i in lda.show_topics():
    print i
</code></pre>
","python, lda, topic-modeling, gensim","<p>There is a variable call <code>topn</code> in <code>show_topics()</code> where you can specify the number of top N words you require from the words distribution over each topic. see <a href=""http://radimrehurek.com/gensim/models/ldamodel.html"">http://radimrehurek.com/gensim/models/ldamodel.html</a></p>

<p>So instead of the default <code>lda.show_topics()</code>. You can use the <code>len(dictionary)</code> for the full word distributions for each topic:</p>

<pre><code>for i in lda.show_topics(topn=len(dictionary)):
    print i
</code></pre>
",8,8,7897,2013-07-15 20:06:08,https://stackoverflow.com/questions/17662916/how-to-print-out-the-full-distribution-of-words-in-an-lda-topic-in-gensim
python gensim: indices array has non-integer dtype (float64),"<p>I am using this <a href=""http://radimrehurek.com/gensim/tut3.html#similarity-interface"" rel=""nofollow"">gensim</a> tutorial to find similarities between texts. Here is the code </p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models import hdpmodel, ldamodel
from itertools import izip

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

'''
documents = [""Human machine interface for lab abc computer applications"",
              ""bags loose tea water second ingredient tastes water"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey"",
              ""red cow butter oil""]
'''
documents = [""Human machine interface for lab abc computer applications"",
              ""bags loose tea water second ingredient tastes water""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once]
         for text in texts]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

#print corpus

tfidf = models.TfidfModel(corpus)

#print tfidf

corpus_tfidf = tfidf[corpus]

#print corpus_tfidf

lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)
lsi.print_topics(1)

lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)
lda.print_topics(1)

corpora.MmCorpus.serialize('dict.mm', corpus)
corpus = corpora.MmCorpus('dict.mm')
#print corpus

lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
doc = ""human computer interaction""
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lsi = lsi[vec_bow]
#print vec_lsi

index = similarities.MatrixSimilarity(lsi[corpus])
index.save('dict.index')
index = similarities.MatrixSimilarity.load('dict.index')

sims = index[vec_lsi]
#print list(enumerate(sims))

sims = sorted(enumerate(sims),key=lambda item: -item[1])
for sim in sims:
  print documents[sim[0]], "" ==&gt; "", sim[1]
</code></pre>

<p>There are two documents here. One has 10 texts and another has 2. One is commented out. If I use the first documents list everything goes fine and generates meaningful output. If I use the second document list(having 2 texts) an error occured. Here is it </p>

<pre><code>/usr/lib/python2.7/dist-packages/scipy/sparse/compressed.py:122: UserWarning: indices array has non-integer dtype (float64)
% self.indices.dtype.name )
</code></pre>

<p>What is the reason behind this error and how can I fix it?
I am using a 64bit machine.</p>
","python, gensim","<p>It could be caused by the fact that your second list will be <code>[[], ['water']]</code> by the time that you have removed singletons, trying to do matrix operations on matrices with dimensions of 0 and 1 could cause all sorts of issues.</p>

<p>Having a play with your code:</p>

<pre><code>&gt;&gt;&gt; corpus = [dictionary.doc2bow(text) for text in texts]
&gt;&gt;&gt; corpus
[[], [(0, 2)]]
&gt;&gt;&gt; tfidf = models.TfidfModel(corpus)
2013-07-21 09:23:31,415 : INFO : collecting document frequencies
2013-07-21 09:23:31,415 : INFO : PROGRESS: processing document #0
2013-07-21 09:23:31,415 : INFO : calculating IDF weights for 2 documents and 1 features (1 matrix non-zeros)
&gt;&gt;&gt; corpus = [[(1,)], [(0,2)]]
&gt;&gt;&gt; tfidf = models.TfidfModel(corpus)
2013-07-21 09:24:16,452 : INFO : collecting document frequencies
2013-07-21 09:24:16,452 : INFO : PROGRESS: processing document #0
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/tfidfmodel.py"", line 96, in __init__
    self.initialize(corpus)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/tfidfmodel.py"", line 119, in initialize
    for termid, _ in bow:
ValueError: need more than 1 value to unpack
&gt;&gt;&gt; corpus = [[(1,3)], [(0,2)]]
&gt;&gt;&gt; tfidf = models.TfidfModel(corpus)
2013-07-21 09:24:26,892 : INFO : collecting document frequencies
2013-07-21 09:24:26,892 : INFO : PROGRESS: processing document #0
2013-07-21 09:24:26,892 : INFO : calculating IDF weights for 2 documents and 2 features (2 matrix non-zeros)
&gt;&gt;&gt; 
</code></pre>

<p>As I said above you need to ensure that <code>corpus</code> does <strong>not</strong> have any empty lists before calling <code>models.TfidfModel(corpus)</code> on it.</p>
",2,2,1178,2013-07-20 18:45:27,https://stackoverflow.com/questions/17765509/python-gensim-indices-array-has-non-integer-dtype-float64
Gensim Dictionary Implementation,"<p>I was just curious about the gensim dictionary implementation. I have the following code:</p>

<pre><code>    def build_dictionary(documents):
        dictionary = corpora.Dictionary(documents)
        dictionary.save('/tmp/deerwester.dict') # store the dictionary
        return dictionary    
</code></pre>

<p>and I looked inside the file deerwester.dict and it looks like this:</p>

<pre><code>8002 6367 656e 7369 6d2e 636f 7270 6f72
612e 6469 6374 696f 6e61 7279 0a44 6963
7469 6f6e 6172 790a 7101 2981 7102 7d71
0328 5508 6e75 6d5f 646f 6373 7104 4b09
5508 ...
</code></pre>

<p>the following code, however,</p>

<pre><code>my_dict = dictionary.load('/tmp/deerwester.dict') 
print my_dict.token2id #view dictionary
</code></pre>

<p>yields this:</p>

<pre><code>{'minors': 30, 'generation': 22, 'testing': 16, 'iv': 29, 'engineering': 15, 'computer': 2, 'relation': 20, 'human': 3, 'measurement': 18, 'unordered': 25, 'binary': 21, 'abc': 0, 'ordering': 31, 'graph': 26, 'system': 10, 'machine': 6, 'quasi': 32, 'random': 23, 'paths': 28, 'error': 17, 'trees': 24, 'lab': 5, 'applications': 1, 'management': 14, 'user': 12, 'interface': 4, 'intersection': 27, 'response': 8, 'perceived': 19, 'widths': 34, 'well': 33, 'eps': 13, 'survey': 9, 'time': 11, 'opinion': 7}
</code></pre>

<p>So my question is, since I don't see the actual words inside the .dict file, what are all of the hexadecimal values stored there? Is this some kind of super compressed format? I'm curious because I feel like if it is, I should consider using it from now on.</p>
","python, nlp, topic-modeling, gensim","<p>Given the example:</p>

<pre><code>&gt;&gt;&gt; from gensim import corpora
&gt;&gt;&gt; docs = [""this is a foo bar"", ""you are a foo""]
&gt;&gt;&gt; texts = [[i for i in doc.lower().split()] for doc in docs]
&gt;&gt;&gt; print texts
[['this', 'is', 'a', 'foo', 'bar'], ['you', 'are', 'a', 'foo']]

&gt;&gt;&gt; dictionary = corpora.Dictionary(texts)
&gt;&gt;&gt; dictionary.save('foobar.txtdic')
</code></pre>

<p>If you use the <code>gensim.corpora.dictionary.save_as_text()</code> (see <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/dictionary.py"" rel=""noreferrer"">https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/dictionary.py</a>), you should have got the below text file:</p>

<pre><code>0   a   2
5   are 1
1   bar 1
2   foo 2
3   is  1
4   this    1
6   you 1
</code></pre>

<p>If you use the default <code>gensim.corpora.dictionary.save()</code>, it saves into a pickled binary file. See <code>class SaveLoad(object)</code> in <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/utils.py"" rel=""noreferrer"">https://github.com/piskvorky/gensim/blob/develop/gensim/utils.py</a></p>

<p>For information on <code>pickle</code>, see <a href=""http://docs.python.org/2/library/pickle.html#pickle-example"" rel=""noreferrer"">http://docs.python.org/2/library/pickle.html#pickle-example</a></p>
",11,6,9777,2013-08-12 09:38:20,https://stackoverflow.com/questions/18183810/gensim-dictionary-implementation
How to generate pertinent text?,"<p>What I want to do is, get a text training set (natural language) and increase this set with automatically created text that tries to mimic the text content. I'm using a bag-of-words assumption, sequence doesn't matter, syntax doesn't matter, I just want to create text that contains words that is pertinent with the general topic of the base.</p>

<p>Right now I'm using <strong>Latent Dirichlet Allocation</strong> to classify my documents in topics distributions, average the topic distribution of my set, and generate documents from these topic distribution.</p>

<p>I want to know two things:</p>

<blockquote>
  <p>1- Is there a better way to do that?</p>
  
  <p>2- Can I train LDA with texts that are not of the domain of my set,
  without tainting my topics: Eg. The set that I want to increase has
  texts about politics. Can I train my model with any kind of text
  (cars, fashion, musics) and classificates my base of politics text get its topics distributions and generates similar text from this distribution.</p>
</blockquote>

<p>I'm using python 2.7 and gensim.</p>
","algorithm, language-agnostic, nlp, probability-theory, gensim","<p><a href=""https://stackoverflow.com/questions/18391602/what-does-generate-do-when-using-nltk-in-python"">NLTK's generate() function</a> may be what you're looking for.</p>
<p>From <a href=""http://nltk.org/api/nltk.html?highlight=generate#nltk.text.Text.generate"" rel=""nofollow noreferrer"">the docs</a>:</p>
<blockquote>
<p><code>generate(length=100)</code></p>
<p>Print random text, generated using a
trigram language model.</p>
<p>Parameters:</p>
<p>length (int) – The length of text to generate (default=100)</p>
</blockquote>
",0,-1,139,2013-09-17 02:50:55,https://stackoverflow.com/questions/18840537/how-to-generate-pertinent-text
How to resolve the unpicklingerror in loading gensim corpus? - python,"<p>I can save a serialized corpus into <code>foobar.mm</code> but when i try to load it, it gives <code>UnpicklingError</code>. Loading the dictionary seems fine though. <strong>Anyone knows how to resolve this? And why does this occur?</strong></p>

<pre><code>&gt;&gt;&gt; from gensim import corpora
&gt;&gt;&gt; docs = [""this is a foo bar"", ""you are a foo""]
&gt;&gt;&gt; texts = [[i for i in doc.lower().split()] for doc in docs]
&gt;&gt;&gt; print texts
[['this', 'is', 'a', 'foo', 'bar'], ['you', 'are', 'a', 'foo']]

&gt;&gt;&gt; dictionary = corpora.Dictionary(texts)
&gt;&gt;&gt; dictionary.save('foobar.dic')
&gt;&gt;&gt; print dictionary
Dictionary(7 unique tokens)
&gt;&gt;&gt; corpora.Dictionary.load('foobar.dic')
&lt;gensim.corpora.dictionary.Dictionary object at 0x329f910&gt;

&gt;&gt;&gt; corpus = [dictionary.doc2bow(text) for text in texts]
&gt;&gt;&gt; corpora.MmCorpus.serialize('foobar.mm', corpus)
&gt;&gt;&gt; corpus = corpora.MmCorpus.load('foobar.mm')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.8.6-py2.7.egg/gensim/utils.py"", line 166, in load
    return unpickle(fname)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.8.6-py2.7.egg/gensim/utils.py"", line 492, in unpickle
    return cPickle.load(open(fname, 'rb'))
cPickle.UnpicklingError: invalid load key, '%'.
</code></pre>
","python, lda, topic-modeling, gensim","<p>See the documentation at <a href=""http://radimrehurek.com/gensim/tut1.html#corpus-formats"" rel=""nofollow"">http://radimrehurek.com/gensim/tut1.html#corpus-formats</a></p>

<p>What you're trying to do is store the corpus in MatrixMarket format (=a text format) and then load it using the <a href=""http://radimrehurek.com/gensim/utils.html#gensim.utils.SaveLoad"" rel=""nofollow"">save/load binary interface</a>.</p>

<p>To load a serialized MatrixMarket corpus, simply <code>corpus = corpora.MmCorpus('foobar.mm')</code></p>
",4,2,3154,2013-09-18 08:39:47,https://stackoverflow.com/questions/18867516/how-to-resolve-the-unpicklingerror-in-loading-gensim-corpus-python
Transposed parameter in Matrix Market Format of gensim - python,"<p>In the <code>gensim</code> library, there is a <code>MmReader</code> class that converts a <a href=""http://bickson.blogspot.de/2012/02/matrix-market-format.html"" rel=""nofollow"">matrix market format</a> file into a python object. Sometimes it is necessary to <a href=""https://en.wikipedia.org/wiki/Transpose"" rel=""nofollow"">transpose the matrix</a>, hence the transposed parameter was introduced in the <code>MmReader</code>.</p>

<p>However, I am confused about why is it that at lines <code>525-526</code> and <code>567-568</code> of <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/matutils.py"" rel=""nofollow"">https://github.com/piskvorky/gensim/blob/develop/gensim/matutils.py</a> , the inversion of term-document values and id happens when <code>transposed == False</code>.</p>

<p>Anyone familiar with term-document matrices in information retrieval care to enlighten me?</p>

<pre><code>class MmReader(object):
    """"""
    Wrap a term-document matrix on disk (in matrix-market format), and present it
    as an object which supports iteration over the rows (~documents).

    Note that the file is read into memory one document at a time, not the whole
    matrix at once (unlike scipy.io.mmread). This allows us to process corpora
    which are larger than the available RAM.
    """"""
    def __init__(self, input, transposed=True):
        """"""
        Initialize the matrix reader.

        The `input` refers to a file on local filesystem, which is expected to
        be in the sparse (coordinate) Matrix Market format. Documents are assumed
        to be rows of the matrix (and document features are columns).

        `input` is either a string (file path) or a file-like object that supports
        `seek()` (e.g. gzip.GzipFile, bz2.BZ2File).
        """"""
        logger.info(""initializing corpus reader from %s"" % input)
        self.input, self.transposed = input, transposed
        if isinstance(input, basestring):
            input = open(input)
        header = input.next().strip()
        if not header.lower().startswith('%%matrixmarket matrix coordinate real general'):
            raise ValueError(""File %s not in Matrix Market format with coordinate real general; instead found: \n%s"" %
                             (self.input, header))
        self.num_docs = self.num_terms = self.num_nnz = 0
        for lineno, line in enumerate(input):
            if not line.startswith('%'):
                self.num_docs, self.num_terms, self.num_nnz = map(int, line.split())
                if not self.transposed: ## line 525
                    self.num_docs, self.num_terms = self.num_terms, self.num_docs
                break
        logger.info(""accepted corpus with %i documents, %i features, %i non-zero entries"" %
                     (self.num_docs, self.num_terms, self.num_nnz))

    def __len__(self):
        return self.num_docs

    def __str__(self):
        return (""MmCorpus(%i documents, %i features, %i non-zero entries)"" %
                (self.num_docs, self.num_terms, self.num_nnz))

    def skip_headers(self, input_file):
        """"""
        Skip file headers that appear before the first document.
        """"""
        for line in input_file:
            if line.startswith('%'):
                continue
            break

    def __iter__(self):
        """"""
        Iteratively yield vectors from the underlying file, in the format (row_no, vector),
        where vector is a list of (col_no, value) 2-tuples.

        Note that the total number of vectors returned is always equal to the
        number of rows specified in the header; empty documents are inserted and
        yielded where appropriate, even if they are not explicitly stored in the
        Matrix Market file.
        """"""
        if isinstance(self.input, basestring):
            fin = open(self.input)
        else:
            fin = self.input
            fin.seek(0)
        self.skip_headers(fin)

        previd = -1
        for line in fin:
            docid, termid, val = line.split()
            if not self.transposed:
                termid, docid = docid, termid
            docid, termid, val = int(docid) - 1, int(termid) - 1, float(val) # -1 because matrix market indexes are 1-based =&gt; convert to 0-based
            assert previd &lt;= docid, ""matrix columns must come in ascending order""
            if docid != previd:
                # change of document: return the document read so far (its id is prevId)
                if previd &gt;= 0:
                    yield previd, document

                # return implicit (empty) documents between previous id and new id
                # too, to keep consistent document numbering and corpus length
                for previd in xrange(previd + 1, docid):
                    yield previd, []

                # from now on start adding fields to a new document, with a new id
                previd = docid
                document = []

            document.append((termid, val,)) # add another field to the current document

        # handle the last document, as a special case
        if previd &gt;= 0:
            yield previd, document

        # return empty documents between the last explicit document and the number
        # of documents as specified in the header
        for previd in xrange(previd + 1, self.num_docs):
            yield previd, []


    def docbyoffset(self, offset):
        """"""Return document at file offset `offset` (in bytes)""""""
        # empty documents are not stored explicitly in MM format, so the index marks
        # them with a special offset, -1.
        if offset == -1:
            return []
        if isinstance(self.input, basestring):
            fin = open(self.input)
        else:
            fin = self.input

        fin.seek(offset) # works for gzip/bz2 input, too
        previd, document = -1, []
        for line in fin:
            docid, termid, val = line.split()
            if not self.transposed: ## line 567
                termid, docid = docid, termid
            docid, termid, val = int(docid) - 1, int(termid) - 1, float(val) # -1 because matrix market indexes are 1-based =&gt; convert to 0-based
            assert previd &lt;= docid, ""matrix columns must come in ascending order""
            if docid != previd:
                if previd &gt;= 0:
                    return document
                previd = docid

            document.append((termid, val,)) # add another field to the current document
        return document
#endclass MmReader
</code></pre>
","python, matrix, information-retrieval, tf-idf, gensim","<p>Apparently, the <code>transposed</code> parameter is never used in the latest version of <code>gensim</code> where the format of the <code>mmreader</code> and <code>mmwriter</code> is the same.</p>

<p>For more details, the developer explained in <a href=""https://groups.google.com/forum/?hl=en#!topic/gensim/XC7Q_q3WcyQ"" rel=""nofollow"">https://groups.google.com/forum/?hl=en#!topic/gensim/XC7Q_q3WcyQ</a></p>
",1,0,571,2013-09-24 18:07:35,https://stackoverflow.com/questions/18988886/transposed-parameter-in-matrix-market-format-of-gensim-python
Build Dictionary without Loading All Texts,"<p>I am new to Python and Gensim.  I am currently working through one of the tutorials on <code>gensim</code> (<a href=""http://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">http://radimrehurek.com/gensim/tut1.html</a>).  I have two question about this line of code:</p>

<pre><code># collect statistics about all tokens
&gt;&gt;&gt; dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))
</code></pre>

<p>1) Is the file <code>mycorpus.txt</code> fully loaded into memory before the Dictionary starts to get built?    The tutorial explicitly says no:</p>

<pre><code>Similarly, to construct the dictionary without loading all texts into memory
</code></pre>

<p>but when I monitor RAM usage in my Activity Monitor, the Python process hits 1 gig for a 3 gig file (I killed the process midway).  This is strange, as I assumed the dictionary for my 3 gig text file would be MUCH smaller.  Can someone clarify this point for me?</p>

<p>2) How can I recode this line so that I can do stuff between each line read?  I want to print to screen to see the progress.  Here is my attempt:</p>

<pre><code>i = 1

for line in f:
    if i % 1000 == 0:
        print i
    dictionary = corpora.Dictionary([line.lower().split()])
    i += 1
</code></pre>

<p>This doesn't work because dictionary is being reinitialized for every line.</p>

<p>I realize these are very n00b questions - appreciate your help and patience.</p>
","python, dictionary, gensim","<p>1) No, they are passing a generator object which will yield only one line at a time to the dictionary constructor.  Other than some caching done by python internally, it only reads basically 1 line at a time.</p>

<p>After the dictionary is built, it will probably take almost the same amount of memory as the original file -- After all, it's probably storing all that information.</p>

<p>2) As far as recoding it, you can make a new generator which does your action and yields the lines as it did before:</p>

<pre><code>def generator(f)
    for i, line in enumerate(f):
        if i % 1000 == 0:
            print i
        yield line

with open('mycorpus.txt') as f:
    dictionary = corpora.Dictionary(line.lower().split() for line in generator(f))
</code></pre>
",3,2,166,2013-10-20 05:46:31,https://stackoverflow.com/questions/19474333/build-dictionary-without-loading-all-texts
Can&#39;t install gensim,"<p>When trying to install gensim (with pip install and setup install), it gives me this error:</p>

<pre><code>Traceback (most recent call last):
  File ""setup.py"", line 19, in &lt;module&gt;
    import ez_setup
  File ""C:\Users\User\Desktop\gensim-0.8.7\ez_setup.py"", line 106
    except pkg_resources.VersionConflict, e:
                                        ^
SyntaxError: invalid syntax
</code></pre>

<p>How can I solve this</p>
","python, gensim","<p>I've never worked in Gensim, but I'm pretty sure the problem is that you have incompatible versions of it and Python.  The below code uses Python 2.x. syntax.</p>

<pre><code>except pkg_resources.VersionConflict, e:
</code></pre>

<p>In Python 3.x. however, you use <code>as</code> instead of <code>,</code>:</p>

<pre><code>except pkg_resources.VersionConflict as e:
</code></pre>

<p>Below is a demonstration written in Python 3.x.:</p>

<pre><code>&gt;&gt;&gt; try:
...     1/0
... except ZeroDivisionError, e:
  File ""&lt;stdin&gt;"", line 3
    except ZeroDivisionError, e:
                            ^
SyntaxError: invalid syntax
&gt;&gt;&gt;
&gt;&gt;&gt; try:
...     1/0
... except ZeroDivisionError as e:
...     print(e)
...
division by zero
&gt;&gt;&gt;
</code></pre>

<p>As you can see, it is your exact same error.</p>

<p>So, to fix the problem, you need to fix the versions.  Either set Python to version 2.x. or get a version of Gensim that runs with Python 3.x.</p>

<hr>

<p>I'm going to move the comment into my post because it is important.  <a href=""https://github.com/samantp/gensimPy3"" rel=""nofollow"">Here</a> is the link to Gensim for Python 3.x.</p>
",3,0,3434,2013-10-22 15:34:33,https://stackoverflow.com/questions/19522258/cant-install-gensim
Topic models evaluation in Gensim,"<p>I've been experimenting with LDA topic modelling using <a href=""http://radimrehurek.com/gensim/"" rel=""nofollow"">Gensim</a>. I couldn't seem to find any topic model evaluation facility in Gensim, which could report on the perplexity of a topic model on held-out evaluation texts thus facilitates subsequent fine tuning of LDA parameters (e.g. number of topics). It would be greatly appreciated if anyone could shed some light on how I can perform topic model evaluation in Gensim. This question has also been posted on <a href=""http://metaoptimize.com/qa/questions/14332/topic-models-evaluation-in-gensim"" rel=""nofollow"">metaoptimize</a>.</p>
","lda, gensim","<p>Found the <a href=""https://groups.google.com/forum/#!topic/gensim/LM619SB57zM"" rel=""nofollow"">answer</a> on the <a href=""https://groups.google.com/forum/#!forum/gensim"" rel=""nofollow"">gensim mailing list</a>.</p>

<p>In short, the bound() method of LdaModel computes a lower bound on perplexity, based on a held-out corpus. </p>
",2,2,6018,2013-10-27 08:11:23,https://stackoverflow.com/questions/19615951/topic-models-evaluation-in-gensim
Understanding LDA implementation using gensim,"<p>I am trying to understand how gensim package in Python implements Latent Dirichlet Allocation. I am doing the following:</p>

<p>Define the dataset</p>

<pre><code>documents = [""Apple is releasing a new product"", 
             ""Amazon sells many things"",
             ""Microsoft announces Nokia acquisition""]             
</code></pre>

<p>After removing stopwords, I create the dictionary and the corpus:</p>

<pre><code>texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
</code></pre>

<p>Then I define the LDA model.</p>

<pre><code>lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, update_every=1, chunksize=10000, passes=1)
</code></pre>

<p>Then I print the topics:</p>

<pre><code>&gt;&gt;&gt; lda.print_topics(5)
['0.181*things + 0.181*amazon + 0.181*many + 0.181*sells + 0.031*nokia + 0.031*microsoft + 0.031*apple + 0.031*announces + 0.031*acquisition + 0.031*product', '0.077*nokia + 0.077*announces + 0.077*acquisition + 0.077*apple + 0.077*many + 0.077*amazon + 0.077*sells + 0.077*microsoft + 0.077*things + 0.077*new', '0.181*microsoft + 0.181*announces + 0.181*acquisition + 0.181*nokia + 0.031*many + 0.031*sells + 0.031*amazon + 0.031*apple + 0.031*new + 0.031*is', '0.077*acquisition + 0.077*announces + 0.077*sells + 0.077*amazon + 0.077*many + 0.077*nokia + 0.077*microsoft + 0.077*releasing + 0.077*apple + 0.077*new', '0.158*releasing + 0.158*is + 0.158*product + 0.158*new + 0.157*apple + 0.027*sells + 0.027*nokia + 0.027*announces + 0.027*acquisition + 0.027*microsoft']
2013-12-03 13:26:21,878 : INFO : topic #0: 0.181*things + 0.181*amazon + 0.181*many + 0.181*sells + 0.031*nokia + 0.031*microsoft + 0.031*apple + 0.031*announces + 0.031*acquisition + 0.031*product
2013-12-03 13:26:21,880 : INFO : topic #1: 0.077*nokia + 0.077*announces + 0.077*acquisition + 0.077*apple + 0.077*many + 0.077*amazon + 0.077*sells + 0.077*microsoft + 0.077*things + 0.077*new
2013-12-03 13:26:21,880 : INFO : topic #2: 0.181*microsoft + 0.181*announces + 0.181*acquisition + 0.181*nokia + 0.031*many + 0.031*sells + 0.031*amazon + 0.031*apple + 0.031*new + 0.031*is
2013-12-03 13:26:21,881 : INFO : topic #3: 0.077*acquisition + 0.077*announces + 0.077*sells + 0.077*amazon + 0.077*many + 0.077*nokia + 0.077*microsoft + 0.077*releasing + 0.077*apple + 0.077*new
2013-12-03 13:26:21,881 : INFO : topic #4: 0.158*releasing + 0.158*is + 0.158*product + 0.158*new + 0.157*apple + 0.027*sells + 0.027*nokia + 0.027*announces + 0.027*acquisition + 0.027*microsoft
&gt;&gt;&gt; 
</code></pre>

<p>I'm not able to understand much out of this result. Is it providing with a probability of the occurrence of each word? Also, what's the meaning of topic #1, topic #2 etc? I was expecting something more or less like the most important keywords.</p>

<p>I already checked the <a href=""http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""noreferrer"">gensim tutorial</a> but it didn't really help much.</p>

<p>Thanks.</p>
","python, gensim, lda, topic-modeling, dirichlet","<p>The answer you're looking for is in the <a href=""http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""noreferrer"">gensim tutorial</a>.  <code>lda.printTopics(k)</code> prints the most contributing words for <code>k</code> randomly selected topics.  One can assume that this is (partially) the distribution of words over each of the given topics, meaning the probability of those words appearing in the topic to the left. </p>

<p>Usually, one would run LDA on a large corpus. Running LDA on a ridiculously small sample won't give the best results.</p>
",22,30,31637,2013-12-03 11:31:04,https://stackoverflow.com/questions/20349958/understanding-lda-implementation-using-gensim
How to load sentences into Python gensim?,"<p>I am trying to use the <a href=""http://radimrehurek.com/gensim/models/word2vec.html""><code>word2vec</code></a> module from <code>gensim</code> natural language processing library in Python.</p>

<p>The docs say to initialize the model:</p>

<pre class=""lang-python prettyprint-override""><code>from gensim.models import word2vec
model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>What format does <code>gensim</code> expect for the input sentences?  I have raw text</p>

<pre><code>""the quick brown fox jumps over the lazy dogs""
""Then a cop quizzed Mick Jagger's ex-wives briefly.""
etc.
</code></pre>

<p>What additional processing do I need to post into <code>word2fec</code>?</p>

<hr>

<p><strong>UPDATE:</strong> Here is what I have tried.  When it loads the sentences, I get nothing.</p>

<pre><code>&gt;&gt;&gt; sentences = ['the quick brown fox jumps over the lazy dogs',
             ""Then a cop quizzed Mick Jagger's ex-wives briefly.""]
&gt;&gt;&gt; x = word2vec.Word2Vec()
&gt;&gt;&gt; x.build_vocab([s.encode('utf-8').split( ) for s in sentences])
&gt;&gt;&gt; x.vocab
{}
</code></pre>
","python, nlp, gensim","<p><a href=""http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""noreferrer"">A list of <code>utf-8</code> sentences</a>. You can also stream the data from the disk.</p>

<p>Make sure it's <code>utf-8</code>, and split it:</p>

<pre><code>sentences = [ ""the quick brown fox jumps over the lazy dogs"",
""Then a cop quizzed Mick Jagger's ex-wives briefly."" ]
word2vec.Word2Vec([s.encode('utf-8').split() for s in sentences], size=100, window=5, min_count=5, workers=4)
</code></pre>
",14,16,14002,2013-12-03 22:25:56,https://stackoverflow.com/questions/20362993/how-to-load-sentences-into-python-gensim
What does &quot;word for word&quot; syntax mean in Python?,"<p>I see the following script snippet from the <a href=""http://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">gensim tutorial page</a>.</p>

<p>What's the syntax of <strong>word for word</strong> in below Python script?</p>

<pre><code>&gt;&gt; texts = [[word for word in document.lower().split() if word not in stoplist]
&gt;&gt;          for document in documents]
</code></pre>
","python, gensim","<p>This is a <a href=""http://docs.python.org/3.3/tutorial/datastructures.html#list-comprehensions"" rel=""noreferrer"">list comprehension</a>.  The code you posted loops through every element in <code>document.lower.split()</code> and creates a new list that contains only the elements that meet the <code>if</code> condition.  It does this for each document in <code>documents</code>.</p>

<p>Try it out...</p>

<pre><code>elems = [1, 2, 3, 4]
squares = [e*e for e in elems]  # square each element
big = [e for e in elems if e &gt; 2]  # keep elements bigger than 2
</code></pre>

<p>As you can see from your example, list comprehensions can be nested.</p>
",6,1,5444,2014-01-06 15:21:46,https://stackoverflow.com/questions/20953143/what-does-word-for-word-syntax-mean-in-python
Topic distribution: How do we see which document belong to which topic after doing LDA in python,"<p>I am able to run the LDA code from gensim and got the top 10 topics with their respective keywords.</p>

<p>Now I would like to go a step further to see how accurate the LDA algo is by seeing which document they cluster into each topic. Is this possible in gensim LDA?</p>

<p>Basically i would like to do something like this, but in python and using gensim.</p>

<p><a href=""https://stackoverflow.com/questions/14875493/lda-with-topicmodels-how-can-i-see-which-topics-different-documents-belong-to"">LDA with topicmodels, how can I see which topics different documents belong to?</a></p>
","python, nltk, lda, gensim","<p>Using the probabilities of the topics, you can try to set some threshold and use it as a clustering baseline, but i am sure there are better ways to do clustering than this 'hacky' method.</p>

<pre><code>from gensim import corpora, models, similarities
from itertools import chain

"""""" DEMO """"""
documents = [""Human machine interface for lab abc computer applications"",
             ""A survey of user opinion of computer system response time"",
             ""The EPS user interface management system"",
             ""System and human system engineering testing of EPS"",
             ""Relation of user perceived response time to error measurement"",
             ""The generation of random binary unordered trees"",
             ""The intersection graph of paths in trees"",
             ""Graph minors IV Widths of trees and well quasi ordering"",
             ""Graph minors A survey""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once] for text in texts]

# Create Dictionary.
id2word = corpora.Dictionary(texts)
# Creates the Bag of Word corpus.
mm = [id2word.doc2bow(text) for text in texts]

# Trains the LDA models.
lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=3, \
                               update_every=1, chunksize=10000, passes=1)

# Prints the topics.
for top in lda.print_topics():
  print top
print

# Assigns the topics to the documents in corpus
lda_corpus = lda[mm]

# Find the threshold, let's set the threshold to be 1/#clusters,
# To prove that the threshold is sane, we average the sum of all probabilities:
scores = list(chain(*[[score for topic_id,score in topic] \
                      for topic in [doc for doc in lda_corpus]]))
threshold = sum(scores)/len(scores)
print threshold
print

cluster1 = [j for i,j in zip(lda_corpus,documents) if i[0][1] &gt; threshold]
cluster2 = [j for i,j in zip(lda_corpus,documents) if i[1][1] &gt; threshold]
cluster3 = [j for i,j in zip(lda_corpus,documents) if i[2][1] &gt; threshold]

print cluster1
print cluster2
print cluster3
</code></pre>

<p><code>[out]</code>:</p>

<pre><code>0.131*trees + 0.121*graph + 0.119*system + 0.115*user + 0.098*survey + 0.082*interface + 0.080*eps + 0.064*minors + 0.056*response + 0.056*computer
0.171*time + 0.171*user + 0.170*response + 0.082*survey + 0.080*computer + 0.079*system + 0.050*trees + 0.042*graph + 0.040*minors + 0.040*human
0.155*system + 0.150*human + 0.110*graph + 0.107*minors + 0.094*trees + 0.090*eps + 0.088*computer + 0.087*interface + 0.040*survey + 0.028*user

0.333333333333

['The EPS user interface management system', 'The generation of random binary unordered trees', 'The intersection graph of paths in trees', 'Graph minors A survey']
['A survey of user opinion of computer system response time', 'Relation of user perceived response time to error measurement']
['Human machine interface for lab abc computer applications', 'System and human system engineering testing of EPS', 'Graph minors IV Widths of trees and well quasi ordering']
</code></pre>

<hr>

<p>Just to make it clearer:</p>

<pre><code># Find the threshold, let's set the threshold to be 1/#clusters,
# To prove that the threshold is sane, we average the sum of all probabilities:
scores = []
for doc in lda_corpus
    for topic in doc:
        for topic_id, score in topic:
            scores.append(score)
threshold = sum(scores)/len(scores)
</code></pre>

<p>The above code is sum the score of all words and in all topics for all documents.
Then normalize the sum by the number of scores.</p>
",34,32,27586,2014-01-08 00:30:08,https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi
Unsupervised Clustering of Words in a document semantically,"<p>I want to cluster words based on their semantic similarity. Currently I have a list of documents with detected noun phrases in them. I want to make cluster out of these obtained nouns within the documents and unsupervisedly cluster them semantically?</p>

<p>I have looked at wordnet and gensim libraries. Any suggestions as to which can really help in getting the required cluster of words based on their semantic similarity?</p>
","python, cluster-analysis, semantics, wordnet, gensim","<p>For similarity based on phrase co-occurrence (phrases appearing more often together in documents will be more similar), you can use gensim.</p>

<p>Check out the Latent Semantic Analysis and Latent Dirichlet Allocation there: <a href=""http://radimrehurek.com/gensim/tut2.html#available-transformations"" rel=""nofollow"">http://radimrehurek.com/gensim/tut2.html#available-transformations</a></p>

<p>Depending on what exactly you want your clusters to do, you can either use the LSI/LDA topics directly as clusters. Or cluster the obtained latent phrase vectors etc.</p>
",1,2,1998,2014-01-28 11:04:12,https://stackoverflow.com/questions/21403839/unsupervised-clustering-of-words-in-a-document-semantically
Latent semantic analysis in finding topics,"<p>I am learning Latent semantic analysis (LSA) and I am able to construct term-document matrix and find its SVD decomposition. How can I get the topics from that decomposition?</p>

<p>For example, in gensim:</p>

<pre><code>topic #0(332.762): 0.425*""utc"" + 0.299*""talk"" + 0.293*""page"" + 0.226*""article"" + 0.224*""delete"" + 0.216*""discussion"" + 0.205*""deletion"" + 0.198*""should"" + 0.146*""debate"" + 0.132*""be""
topic #1(201.852): 0.282*""link"" + 0.209*""he"" + 0.145*""com"" + 0.139*""his"" + -0.137*""page"" + -0.118*""delete"" + 0.114*""blacklist"" + -0.108*""deletion"" + -0.105*""discussion"" + 0.100*""diff""
topic #2(191.991): -0.565*""link"" + -0.241*""com"" + -0.238*""blacklist"" + -0.202*""diff"" + -0.193*""additions"" + -0.182*""users"" + -0.158*""coibot"" + -0.136*""user"" + 0.133*""he"" + -0.130*""resolves""
</code></pre>
","algorithm, svd, gensim","<p>You can get the U, S and V matrices of your SVD decomposition:
<a href=""https://github.com/piskvorky/gensim/wiki/Recipes-&amp;-FAQ#wiki-q4-how-do-you-output-the-u-s-vt-matrices-of-lsi"" rel=""nofollow"">https://github.com/piskvorky/gensim/wiki/Recipes-&amp;-FAQ#wiki-q4-how-do-you-output-the-u-s-vt-matrices-of-lsi</a></p>

<p><strong>EDIT</strong> Answering the question from the comment:</p>

<p>The printed topics are simply vectors from the matrix U (=the left singular vectors), normalized to unit length.</p>

<p>Perhaps the tutorial at <a href=""http://radimrehurek.com/gensim/tut2.html#transforming-vectors"" rel=""nofollow"">http://radimrehurek.com/gensim/tut2.html#transforming-vectors</a> may help.</p>

<p>What is actually printed are the top-N words that contribute the most to that particular topic (default=print top 10 words).</p>

<p>You can see the exact way these topics are computed here, it's rather straightforward:
<a href=""https://github.com/piskvorky/gensim/blob/0.8.9/gensim/models/lsimodel.py#L447"" rel=""nofollow"">https://github.com/piskvorky/gensim/blob/0.8.9/gensim/models/lsimodel.py#L447</a></p>
",3,2,688,2014-01-29 18:57:53,https://stackoverflow.com/questions/21440132/latent-semantic-analysis-in-finding-topics
Using scikit-learn vectorizers and vocabularies with gensim,"<p>I am trying to recycle scikit-learn vectorizer objects with gensim topic models. The reasons are simple: first of all, I already have a great deal of vectorized data; second, I prefer the interface and flexibility of scikit-learn vectorizers; third, even though topic modelling with gensim is very fast, computing its dictionaries (<code>Dictionary()</code>) is relatively slow in my experience.</p>

<p>Similar questions have been asked before, <a href=""https://stackoverflow.com/questions/19504898/use-sikit-tfidf-with-gensim-lda"">especially here</a> and <a href=""https://stackoverflow.com/questions/15670525/how-do-you-initialize-a-gensim-corpus-variable-with-a-csr-matrix"">here</a>, and the bridging solution is gensim's <code>Sparse2Corpus()</code> function which transforms a Scipy sparse matrix into a gensim corpus object.</p>

<p>However, this conversion does not make use of the <code>vocabulary_</code> attribute of sklearn vectorizers, which holds the mapping between words and feature ids. This mapping is necessary in order to print the discriminant words for each topic (<code>id2word</code> in gensim topic models, described as ""a a mapping from word ids (integers) to words (strings)"").</p>

<p>I am aware of the fact that gensim's <code>Dictionary</code> objects are much more complex (and slower to compute) than scikit's <code>vect.vocabulary_</code> (a simple Python <code>dict</code>)...</p>

<p>Any ideas to use <code>vect.vocabulary_</code> as <code>id2word</code> in gensim models?</p>

<p>Some example code:</p>

<pre><code># our data
documents = [u'Human machine interface for lab abc computer applications',
        u'A survey of user opinion of computer system response time',
        u'The EPS user interface management system',
        u'System and human system engineering testing of EPS',
        u'Relation of user perceived response time to error measurement',
        u'The generation of random binary unordered trees',
        u'The intersection graph of paths in trees',
        u'Graph minors IV Widths of trees and well quasi ordering',
        u'Graph minors A survey']

from sklearn.feature_extraction.text import CountVectorizer
# compute vector space with sklearn
vect = CountVectorizer(min_df=1, ngram_range=(1, 1), max_features=25000)
corpus_vect = vect.fit_transform(documents)
# each doc is a scipy sparse matrix
print vect.vocabulary_
#{u'and': 1, u'minors': 20, u'generation': 9, u'testing': 32, u'iv': 15, u'engineering': 5, u'computer': 4, u'relation': 28, u'human': 11, u'measurement': 19, u'unordered': 37, u'binary': 3, u'abc': 0, u'for': 8, u'ordering': 23, u'graph': 10, u'system': 31, u'machine': 17, u'to': 35, u'quasi': 26, u'time': 34, u'random': 27, u'paths': 24, u'of': 21, u'trees': 36, u'applications': 2, u'management': 18, u'lab': 16, u'interface': 13, u'intersection': 14, u'response': 29, u'perceived': 25, u'in': 12, u'widths': 40, u'well': 39, u'eps': 6, u'survey': 30, u'error': 7, u'opinion': 22, u'the': 33, u'user': 38}

import gensim
# transform sparse matrix into gensim corpus
corpus_vect_gensim = gensim.matutils.Sparse2Corpus(corpus_vect, documents_columns=False)
lsi = gensim.models.LsiModel(corpus_vect_gensim, num_topics=4)
# I instead would like something like this line below
# lsi = gensim.models.LsiModel(corpus_vect_gensim, id2word=vect.vocabulary_, num_topics=2)
print lsi.print_topics(2)
#['0.622*""21"" + 0.359*""31"" + 0.256*""38"" + 0.206*""29"" + 0.206*""34"" + 0.197*""36"" + 0.170*""33"" + 0.168*""1"" + 0.158*""10"" + 0.147*""4""', '0.399*""36"" + 0.364*""10"" + -0.295*""31"" + 0.245*""20"" + -0.226*""38"" + 0.194*""26"" + 0.194*""15"" + 0.194*""39"" + 0.194*""23"" + 0.194*""40""']
</code></pre>
","python, scikit-learn, topic-modeling, gensim","<p>Gensim doesn't require <code>Dictionary</code> objects. You can use your plain <code>dict</code> as input to <code>id2word</code> directly, as long as it maps ids (integers) to words (strings).</p>

<p>In fact anything dict-like will do (including <code>dict</code>, <code>Dictionary</code>, <code>SqliteDict</code>...).</p>

<p>(Btw gensim's <code>Dictionary</code> is a simple Python <code>dict</code> underneath.
Not sure where your remarks on <code>Dictionary</code> performance come from, you can't get a mapping much faster than a plain <code>dict</code> in Python. Maybe you're confusing it with text preprocessing (not part of gensim), which can indeed be slow.)</p>
",12,21,10271,2014-02-04 12:25:15,https://stackoverflow.com/questions/21552518/using-scikit-learn-vectorizers-and-vocabularies-with-gensim
How to calculate the sentence similarity using word2vec model of gensim with python,"<p>According to the <a href=""http://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">Gensim Word2Vec</a>, I can use the word2vec model in gensim package to calculate the similarity between 2 words.</p>

<p>e.g.</p>

<pre><code>trained_model.similarity('woman', 'man') 
0.73723527
</code></pre>

<p>However, the word2vec model fails to predict the sentence similarity. I find out the LSI model with sentence similarity in gensim, but, which doesn't seem that can be combined with word2vec model. The length of corpus of each sentence I have is not very long (shorter than 10 words).  So, are there any simple ways to achieve the goal?</p>
","python, gensim, word2vec","<p>This is actually a pretty challenging problem that you are asking. Computing sentence similarity requires building a grammatical model of the sentence, understanding equivalent structures (e.g. ""he walked to the store yesterday"" and ""yesterday, he walked to the store""), finding similarity not just in the pronouns and verbs but also in the proper nouns, finding statistical co-occurences / relationships in lots of real textual examples, etc.</p>

<p>The simplest thing you could try -- though I don't know how well this would perform and it would certainly not give you the optimal results -- would be to first remove all ""stop"" words (words like ""the"", ""an"", etc. that don't add much meaning to the sentence) and then run word2vec on the words in both sentences, sum up the vectors in the one sentence, sum up the vectors in the other sentence, and then find the difference between the sums. By summing them up instead of doing a word-wise difference, you'll at least not be subject to word order. That being said, this will fail in lots of ways and isn't a good solution by any means (though good solutions to this problem almost always involve some amount of NLP, machine learning, and other cleverness).</p>

<p>So, short answer is, no, there's no easy way to do this (at least not to do it well).</p>
",99,145,135950,2014-03-02 16:04:53,https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt
Word2Vec: Effect of window size used,"<p>I am trying to train a word2vec model on very short phrases (5 grams). Since each sentence or example is very short, I believe the window size I can use can atmost be 2. I am trying to understand what the implications of such a small window size are on the quality of the learned model, so that I can understand whether my model has learnt something meaningful or not. I tried training a word2vec model on 5-grams but it appears the learnt model does not capture semantics etc very well.</p>

<p>I am using the following test to evaluate the accuracy of model:
<a href=""https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt"" rel=""noreferrer"">https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt</a></p>

<p>I used gensim.Word2Vec to train a model and here is a snippet of my accuracy scores (using a window size of 2)</p>

<pre><code>[{'correct': 2, 'incorrect': 304, 'section': 'capital-common-countries'},
 {'correct': 2, 'incorrect': 453, 'section': 'capital-world'},
 {'correct': 0, 'incorrect': 86, 'section': 'currency'},
 {'correct': 2, 'incorrect': 703, 'section': 'city-in-state'},
 {'correct': 123, 'incorrect': 183, 'section': 'family'},
 {'correct': 21, 'incorrect': 791, 'section': 'gram1-adjective-to-adverb'},
 {'correct': 8, 'incorrect': 544, 'section': 'gram2-opposite'},
 {'correct': 284, 'incorrect': 976, 'section': 'gram3-comparative'},
 {'correct': 67, 'incorrect': 863, 'section': 'gram4-superlative'},
 {'correct': 41, 'incorrect': 951, 'section': 'gram5-present-participle'},
 {'correct': 6, 'incorrect': 1089, 'section': 'gram6-nationality-adjective'},
 {'correct': 171, 'incorrect': 1389, 'section': 'gram7-past-tense'},
 {'correct': 56, 'incorrect': 936, 'section': 'gram8-plural'},
 {'correct': 52, 'incorrect': 705, 'section': 'gram9-plural-verbs'},
 {'correct': 835, 'incorrect': 9973, 'section': 'total'}]
</code></pre>

<p>I also tried running the demo-word-accuracy.sh script outlined here with a window size of 2 and get poor accuracy as well:</p>

<pre><code>Sample output:
    capital-common-countries:
    ACCURACY TOP1: 19.37 %  (98 / 506)
    Total accuracy: 19.37 %   Semantic accuracy: 19.37 %   Syntactic accuracy: -nan % 
    capital-world:
    ACCURACY TOP1: 10.26 %  (149 / 1452)
    Total accuracy: 12.61 %   Semantic accuracy: 12.61 %   Syntactic accuracy: -nan % 
    currency:
    ACCURACY TOP1: 6.34 %  (17 / 268)
    Total accuracy: 11.86 %   Semantic accuracy: 11.86 %   Syntactic accuracy: -nan % 
    city-in-state:
    ACCURACY TOP1: 11.78 %  (185 / 1571)
    Total accuracy: 11.83 %   Semantic accuracy: 11.83 %   Syntactic accuracy: -nan % 
    family:
    ACCURACY TOP1: 57.19 %  (175 / 306)
    Total accuracy: 15.21 %   Semantic accuracy: 15.21 %   Syntactic accuracy: -nan % 
    gram1-adjective-to-adverb:
    ACCURACY TOP1: 6.48 %  (49 / 756)
    Total accuracy: 13.85 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 6.48 % 
    gram2-opposite:
    ACCURACY TOP1: 17.97 %  (55 / 306)
    Total accuracy: 14.09 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 9.79 % 
    gram3-comparative:
    ACCURACY TOP1: 34.68 %  (437 / 1260)
    Total accuracy: 18.13 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 23.30 % 
    gram4-superlative:
    ACCURACY TOP1: 14.82 %  (75 / 506)
    Total accuracy: 17.89 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 21.78 % 
    gram5-present-participle:
    ACCURACY TOP1: 19.96 %  (198 / 992)
    Total accuracy: 18.15 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 21.31 % 
    gram6-nationality-adjective:
    ACCURACY TOP1: 35.81 %  (491 / 1371)
    Total accuracy: 20.76 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.14 % 
    gram7-past-tense:
    ACCURACY TOP1: 19.67 %  (262 / 1332)
    Total accuracy: 20.62 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 24.02 % 
    gram8-plural:
    ACCURACY TOP1: 35.38 %  (351 / 992)
    Total accuracy: 21.88 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.52 % 
    gram9-plural-verbs:
    ACCURACY TOP1: 20.00 %  (130 / 650)
    Total accuracy: 21.78 %   Semantic accuracy: 15.21 %   Syntactic accuracy: 25.08 % 
    Questions seen / total: 12268 19544   62.77 % 
</code></pre>

<p>However the word2vec site claims its possible to obtain an accuracy of ~60% on these tasks.
Hence I would like to gain some insights into the effect of these hyperparameters like window size and how they affect quality of learnt models.</p>
","gensim, word2vec","<p>To your question: ""I am trying to understand what the implications of such a small window size are on the quality of the learned model"".</p>

<p>For example ""stackoverflow great website for programmers"" with 5 words (suppose we save the stop words great and for here)
if the window size is 2 then the vector of word ""stackoverflow"" is directly affected by the word ""great"" and ""website"", if the window size is 5 ""stackoverflow"" can be directly affected by two more words ""for"" and ""programmers"". The 'affected' here means it will pull the vector of two words closer.</p>

<p>So it depends on the material you are using for training, if the window size of 2 can capture the context of a word, but 5 is chosen, it will decrease the quality of the learnt model, and vise versa.</p>
",16,25,35531,2014-03-08 17:07:51,https://stackoverflow.com/questions/22272370/word2vec-effect-of-window-size-used
LDA for Html Documents in Genism,"<p>I have bunch of html documents 10-15 on which i have to apply LDA algorithm in gensim
I am stuck on creating the corpus as i don't understand how i design a corpus for a collection of html documents. The example on the site shows the creation of them on wikipedia compressed file .xml.bz</p>

<p>Anyone please guide me how can i apply LDA on bunch of html documents.
Thanks in advance</p>
","python, gensim","<p>Check out HTML processing libraries, like <code>lxml</code> or <code>beautifulsoup</code>.</p>

<p>For higher level processing (removal of boilerplate, extracting plain text from HTML), have a look at e.g. Honza Pomikalek's <a href=""https://github.com/miso-belica/jusText"" rel=""nofollow"">jusText</a> package.</p>

<p>Once you have plain text documents, you can proceed as per <a href=""http://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow"">gensim's tutorials</a>.</p>
",1,1,317,2014-03-12 19:06:18,https://stackoverflow.com/questions/22361438/lda-for-html-documents-in-genism
Python Gensim: how to calculate document similarity using the LDA model?,"<p>I've got a trained LDA model and I want to calculate the similarity score between two documents from the corpus I trained my model on.
After studying all the Gensim tutorials and functions, I still can't get my head around it. Can somebody give me a hint? Thanks!</p>
","python, nlp, lda, gensim","<p>Depends what similarity metric you want to use.</p>

<p><a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""noreferrer"">Cosine similarity</a> is universally useful &amp; <a href=""http://radimrehurek.com/gensim/matutils.html#gensim.matutils.cossim"" rel=""noreferrer"">built-in</a>:</p>

<pre><code>sim = gensim.matutils.cossim(vec_lda1, vec_lda2)
</code></pre>

<p><a href=""http://en.wikipedia.org/wiki/Hellinger_distance"" rel=""noreferrer"">Hellinger distance</a> is useful for similarity between probability distributions (such as LDA topics):</p>

<pre><code>import numpy as np
dense1 = gensim.matutils.sparse2full(lda_vec1, lda.num_topics)
dense2 = gensim.matutils.sparse2full(lda_vec2, lda.num_topics)
sim = np.sqrt(0.5 * ((np.sqrt(dense1) - np.sqrt(dense2))**2).sum())
</code></pre>
",36,34,29695,2014-03-16 06:51:25,https://stackoverflow.com/questions/22433884/python-gensim-how-to-calculate-document-similarity-using-the-lda-model
Gensim get topic for a document (seen document),"<p>I know that after training the lda model for gensim, we can get the topic for an unseen document by:</p>

<pre><code>lda = LdaModel(corpus, num_topics=10)
doc_lda = lda[doc_bow]
</code></pre>

<p>But how about the documents that are already used for training? I mean is there a way to get the topic for a document in corpus that was used in training without treating it like a new document?</p>
","python, lda, gensim","<p>No.</p>

<p>Information from individual documents is distilled into the model, then forgotten. 
No per-document information is kept (more generally: no information that would require <code>O(#docs)</code> memory is kept).</p>
",8,8,1843,2014-04-12 15:59:46,https://stackoverflow.com/questions/23032745/gensim-get-topic-for-a-document-seen-document
tf-idf and LDA on Google App Engine,"<p>I have a python code that uses sklearn and gensim libraries for tf-idf and LDA(Latent Dirichlet Allocation). Now that I want to migrate to Google app engine I can't use any of these two libraries because they are not supported yet. Is there any service already included in Google app engine that I can use instead of these two libraries to do tf-idf and LDA?</p>
","google-app-engine, scikit-learn, tf-idf, lda, gensim","<p>if you are not using any of GAE-specific tools, try deploying your app on Heroku. It let's you deploy a whole virtual environment with all the installed libraries on it.</p>
",1,1,500,2014-04-19 21:58:42,https://stackoverflow.com/questions/23176061/tf-idf-and-lda-on-google-app-engine
Python MySQLdb change string encoding,"<p>I believe my issue is that python does not play nicely with the character encoding of a column in a SQL table:</p>

<pre><code>| column | varchar(255) | latin1_swedish_ci | YES  |     | NULL              |                             | select,insert,update,references |    | 
</code></pre>

<p>The above shows the output for this column. It has type <code>varchar(255)</code> and has encoding <code>latin1_swedish_ci.</code> </p>

<p>Now when I try to make python play with this data, I am getting the following error: </p>

<pre><code> dictionary = gs.corpora.Dictionary(tweets)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 50, in __init__
    self.add_documents(documents)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 97, in add_documents
    _ = self.doc2bow(document, allow_update=True) # ignore the result, here we only care about updating token ids
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 121, in doc2bow
    document = sorted(utils.to_utf8(token) for token in document)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/corpora/dictionary.py"", line 121, in &lt;genexpr&gt;
    document = sorted(utils.to_utf8(token) for token in document)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.9.1-py2.7.egg/gensim/utils.py"", line 164, in any2utf8
    return unicode(text, encoding, errors=errors).encode('utf8')
  File ""/usr/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x96 in position 0: invalid start byte
</code></pre>

<p><code>gs</code> is the <a href=""http://radimrehurek.com/gensim/"" rel=""nofollow"">gensim</a> topic modeling library. I believe that the problem is that gensim requires unicode encodings. </p>

<ol>
<li>How can I change the character encoding (collation?) for this column in my database?</li>
<li>Is there an alternative solution?</li>
</ol>

<p>Thanks for all the help!</p>
","python, mysql, encoding, collation, gensim","<p>I think that your MYSQLdb python library doesn't know it's supposed to encode to utf8</p>

<p>and is encoding to the default python system-defined charset latin1.</p>

<p>When you connect() to your database, pass the  <code>charset='utf8'</code></p>

<p>parameter. This should also make a manual <code>SET NAMES</code></p>
",3,0,1027,2014-04-28 18:41:57,https://stackoverflow.com/questions/23348819/python-mysqldb-change-string-encoding
Topic Modelling and finding similarity in topics,"<p>Problem statement: I have several documents(20k documents). I need to apply Topic modelling to find similar documents and then analyze those similar documents to find how those are different from each other. 
Q: Could anyone suggest me any Topic modelling package through which I can achieve this. I am exploring Mallet and Gensim Python. Not sure which would best fit in my requirement. </p>

<p>Any help would be highly appreciated. </p>
","topic-modeling, gensim, mallet","<p>I don't know Gensim Python, but MALLET could be a solution. Assuming you have Java expertise, it shouldn't be too difficult. </p>

<p>Create a <code>cc.mallet.types.InstanceList</code> with your data and fit a <code>cc.mallet.topics.SimpleLDA</code> model. Then, for each <code>cc.mallet.types.Instance</code> (Instances are your documents), compute a divergence metric to each other <code>Instance</code>. For this, you will need to compute the probability of each topic within each <code>Instance</code>, which is slightly tricky. In <code>SimpleLDA</code>, there is an <code>ArrayList&lt;TopicAssignment&gt; data</code> object that holds <code>Instances</code> and their <code>cc.mallet.topics.TopicAssignment</code>. A <code>TopicAssignment</code> contains a <code>cc.mallet.types.LabelSequence</code> called <code>topicSequence</code>, which holds the the topic assignment for each word. You will need to loop through this to get counts for each topic. Then, the the probability of topic i in document j is simply (#words assigned to topic i in doc j) / (total words in doc j). Store these probabilities and use them to compute the divergence metric of your choice (e.g., KL divergence).</p>
",2,0,1033,2014-05-05 13:34:04,https://stackoverflow.com/questions/23473844/topic-modelling-and-finding-similarity-in-topics
Gensim train word2vec on wikipedia - preprocessing and parameters,"<p>I am trying to train the word2vec model from <code>gensim</code> using the Italian wikipedia
""<a href=""http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2</a>""</p>

<p>However, I am not sure what is the best preprocessing for this corpus.</p>

<p><code>gensim</code> model accepts a list of tokenized sentences.
My first try is to just use the standard <code>WikipediaCorpus</code> preprocessor from <code>gensim</code>. This extract each article, remove punctuation and split words on spaces. With this tool each sentence would correspond to an entire model, and I am not sure of the impact of this fact on the model.</p>

<p>After this I train the model with default parameters. Unfortunately after training it seems that I do not manage to obtain very meaningful similarities.</p>

<p>What is the most appropriate preprocessing on the Wikipedia corpus for this task? (if this questions are too broad please help me by pointing to a relevant tutorial / article )</p>

<p>This the code of my first trial:</p>

<pre><code>from gensim.corpora import WikiCorpus
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
corpus = WikiCorpus('itwiki-latest-pages-articles.xml.bz2',dictionary=False)
max_sentence = -1

def generate_lines():
    for index, text in enumerate(corpus.get_texts()):
        if index &lt; max_sentence or max_sentence==-1:
            yield text
        else:
            break

from gensim.models.word2vec import BrownCorpus, Word2Vec
model = Word2Vec() 
model.build_vocab(generate_lines()) #This strangely builds a vocab of ""only"" 747904 words which is &lt;&lt; than those reported in the literature 10M words
model.train(generate_lines(),chunksize=500)
</code></pre>
","nlp, gensim, word2vec","<p>Your approach is fine.</p>

<pre><code>model.build_vocab(generate_lines()) #This strangely builds a vocab of ""only"" 747904 words which is &lt;&lt; than those reported in the literature 10M words
</code></pre>

<p>This could be because of pruning infrequent words (the default is <code>min_count=5</code>).</p>

<p>To speed up computation, you can consider ""caching"" the preprocessed articles as a plain <code>.txt.gz</code> file, one sentence (document) per line, and then simply using <a href=""http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence"">word2vec.LineSentence</a> corpus. This saves parsing the bzipped wiki XML on every iteration.</p>

<p>Why word2vec doesn't produce ""meaningful similarities"" for Italian wiki, I don't know. English wiki seems to work fine. See also <a href=""https://groups.google.com/forum/#!topic/gensim/MJWrDw_IvXw"">here</a>.</p>
",9,17,9432,2014-05-19 10:37:21,https://stackoverflow.com/questions/23735576/gensim-train-word2vec-on-wikipedia-preprocessing-and-parameters
python IndexError using gensim for LDA Topic Modeling,"<p><a href=""https://stackoverflow.com/questions/21313493/indexerror-while-using-gensim-package-for-lda-topic-modelling"">Another thread</a> has a similar question to mine but leaves out reproducible code. </p>

<p>The goal with the script in question is to create a process that is as memory efficient as possible. So I tried to write a the class <code>corpus()</code> to take advantage of gensims' capabilities. However, I am running into an IndexError that I'm not sure how to resolve when creating <code>lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=checker.dictionary, num_topics=int(options.number_of_topics))</code>. </p>

<p>The documents that I am using are the same as used in the gensim tutorial, which I placed into tutorial_example.txt:</p>

<pre><code>$ cat tutorial_example.txt 
Human machine interface for lab abc computer applications
A survey of user opinion of computer system response time
The EPS user interface management system
System and human system engineering testing of EPS
Relation of user perceived response time to error measurement
The generation of random binary unordered trees
The intersection graph of paths in trees
Graph minors IV Widths of trees and well quasi ordering
Graph minors A survey
</code></pre>

<h2>Error received</h2>

<pre><code>$./gensim_topic_modeling.py -mn2 -w'english' -l1 tutorial_example.txt 
Traceback (most recent call last):
  File ""./gensim_topic_modeling.py"", line 98, in &lt;module&gt;
    lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=checker.dictionary, num_topics=int(options.number_of_topics))
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 306, in __init__
    self.update(corpus)
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 543, in update
    self.log_perplexity(chunk, total_docs=lencorpus)
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 454, in log_perplexity
    perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 630, in bound
    gammad, _ = self.inference([doc])
  File ""/Users/me/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 366, in inference
    expElogbetad = self.expElogbeta[:, ids]
IndexError: index 7 is out of bounds for axis 1 with size 7
</code></pre>

<p>Below is the <code>gensim_topic_modeling.py</code> script:</p>

<pre><code>##gensim_topic_modeling.py

#!/usr/bin/env python
# -*- coding: UTF-8 -*-
import sys
import re
import codecs
import logging
import fileinput
from operator import *
from itertools import *
from sklearn.cluster import KMeans
from gensim import corpora, models, similarities, matutils
import argparse
from nltk.corpus import stopwords

reload(sys)
sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
sys.stdin = codecs.getreader('utf-8')(sys.stdin)


##defs

def stop_word_gen():
    nltk_langs=['danish', 'dutch', 'english', 'french', 'german', 'italian','norwegian', 'portuguese', 'russian', 'spanish', 'swedish']
    stoplist = []
    for lang in options.stop_langs.split("",""):
        if lang not in nltk_langs:
            sys.stderr.write('\n'+""Language {0} not supported"".format(lang)+'\n')
            continue
        stoplist.extend(stopwords.words(lang))
    return stoplist


def clean_texts(texts):
    # remove tokens that appear only once
    all_tokens = sum(texts, [])
    tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
    return [[word for word in text if word not in tokens_once] for text in texts]

##class

class corpus(object):
    """"""sparse vector matrix and dictionary""""""
    def __iter__(self):
        first=True
        for line in fileinput.FileInput(options.input, openhook=fileinput.hook_encoded(""utf-8"")):
            # assume there's one document per line; tokenizer option determines how to split
            if options.space_tokenizer:
                rl = re.compile('\s+', re.UNICODE).split(unicode(line,'utf-8'))
            else:
                rl = re.compile('\W+', re.UNICODE).split(tagRE.sub(' ',line)) 
            # create dictionary
            tokens=[token.strip().lower() for token in rl if token != '' and token.strip().lower() not in stoplist]
            if first:
                first=False
                self.dictionary=corpora.Dictionary([tokens])
            else:
                self.dictionary.add_documents([tokens])
                self.dictionary.compactify
            yield self.dictionary.doc2bow(tokens)


##main 

if __name__ == '__main__':
    ##parser
    parser = argparse.ArgumentParser(
                description=""Topic model from a column of text.  Each line is a document in the corpus"")
    parser.add_argument(""input"", metavar=""args"")
    parser.add_argument(""-l"", ""--document-frequency-limit"", dest=""doc_freq_limit"", default=1,
                help=""Remove all tokens less than or equal to limit (default 1)"")
    parser.add_argument(""-m"", ""--create-model"", dest=""create_model"", default=False, action=""store_true"",
                help=""Create and save a model from existing dictionary and input corpus."")
    parser.add_argument(""-n"", ""--number-of-topics"", dest=""number_of_topics"", default=2,
                help=""Number of topics (default 2)"")
    parser.add_argument(""-t"", ""--space-tokenizer"", dest=""space_tokenizer"", default=False, action=""store_true"", 
                help=""Use alternate whitespace tokenizer"")
    parser.add_argument(""-w"", ""--stop-word-languages"", dest=""stop_langs"", default=""danish,dutch,english,french,german,italian,norwegian,portuguese,russian,spanish,swedish"",
                help=""Desired languages for stopword lists"")
    options = parser.parse_args()

    ##globals

    stoplist=set(stop_word_gen())  
    tagRE = re.compile(r'&lt;.*?&gt;', re.UNICODE)    # Remove xml/html tags
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO, filename=""topic-modeling-log"")
    logr = logging.getLogger(""topic_model"")
    logr.info(""#""*15 + "" started "" + ""#""*15)

    ##instance of class 

    checker=corpus()
    logr.info(""#""*15 + "" SPARSE MATRIX (pre-filter)"" + ""#""*15)

    ##view sparse matrix and dictionary

    for vector in checker: 
        logr.info(vector)
    logr.info(""#""*15 + "" DICTIONARY (pre-filter)"" + ""#""*15)
    logr.info(checker.dictionary)
    logr.info(checker.dictionary.token2id)
    #filter
    checker.dictionary.filter_extremes(no_below=int(options.doc_freq_limit)+1)
    logr.info(""#""*15 + "" DICTIONARY (post-filter)"" + ""#""*15)
    logr.info(checker.dictionary)
    logr.info(checker.dictionary.token2id)

    ##Create lda model

    if options.create_model:     
        tfidf = models.TfidfModel(checker,normalize=False)
        print tfidf
        logr.info(""#""*15 + "" corpus_tfidf "" + ""#""*15)
        corpus_tfidf = tfidf[checker]
        logr.info(""#""*15 + "" lda "" + ""#""*15)
        lda = models.ldamodel.LdaModel(corpus_tfidf, id2word=checker.dictionary, num_topics=int(options.number_of_topics))
        logr.info(""#""*15 + "" corpus_lda "" + ""#""*15)
        corpus_lda = lda[corpus_tfidf] 

        ##Evaluate topics based on threshold

        scores = list(chain(*[[score for topic,score in topic] \
                      for topic in [doc for doc in corpus_lda]]))
        threshold = sum(scores)/len(scores)
        print ""threshold:"",threshold
        print
        cluster1 = [j for i,j in zip(corpus_lda,documents) if i[0][1] &gt; threshold]
        cluster2 = [j for i,j in zip(corpus_lda,documents) if i[1][1] &gt; threshold]
        cluster3 = [j for i,j in zip(corpus_lda,documents) if i[2][1] &gt; threshold]
</code></pre>

<p>The resulting <code>topic-modeling-log</code> file is below. Thanks in advance for any help!</p>

<h1>topic-modeling-log</h1>

<pre><code>2014-05-25 02:58:50,482 : INFO : ############### started ###############
2014-05-25 02:58:50,483 : INFO : ############### SPARSE MATRIX (pre-filter)###############
2014-05-25 02:58:50,483 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,483 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,483 : INFO : [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]
2014-05-25 02:58:50,483 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,483 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,483 : INFO : [(2, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]
2014-05-25 02:58:50,483 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(4, 1), (10, 1), (12, 1), (13, 1), (14, 1)]
2014-05-25 02:58:50,484 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(3, 1), (10, 2), (13, 1), (15, 1), (16, 1)]
2014-05-25 02:58:50,484 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)]
2014-05-25 02:58:50,484 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,484 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,484 : INFO : [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)]
2014-05-25 02:58:50,485 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,485 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,485 : INFO : [(24, 1), (26, 1), (27, 1), (28, 1)]
2014-05-25 02:58:50,485 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,485 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,485 : INFO : [(24, 1), (26, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]
2014-05-25 02:58:50,485 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,485 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
2014-05-25 02:58:50,485 : INFO : [(9, 1), (26, 1), (30, 1)]
2014-05-25 02:58:50,485 : INFO : ############### DICTIONARY (pre-filter)###############
2014-05-25 02:58:50,485 : INFO : Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,485 : INFO : {'minors': 30, 'generation': 22, 'testing': 16, 'iv': 29, 'engineering': 15, 'computer': 2, 'relation': 20, 'human': 3, 'measurement': 18, 'unordered': 25, 'binary': 21, 'abc': 0, 'ordering': 31, 'graph': 26, 'system': 10, 'machine': 6, 'quasi': 32, 'random': 23, 'paths': 28, 'error': 17, 'trees': 24, 'lab': 5, 'applications': 1, 'management': 14, 'user': 12, 'interface': 4, 'intersection': 27, 'response': 8, 'perceived': 19, 'widths': 34, 'well': 33, 'eps': 13, 'survey': 9, 'time': 11, 'opinion': 7}
2014-05-25 02:58:50,486 : INFO : keeping 12 tokens which were in no less than 2 and no more than 4 (=50.0%) documents
2014-05-25 02:58:50,486 : INFO : resulting dictionary: Dictionary(12 unique tokens: ['minors', 'graph', 'system', 'trees', 'eps']...)
2014-05-25 02:58:50,486 : INFO : ############### DICTIONARY (post-filter)###############
2014-05-25 02:58:50,486 : INFO : Dictionary(12 unique tokens: ['minors', 'graph', 'system', 'trees', 'eps']...)
2014-05-25 02:58:50,486 : INFO : {'minors': 0, 'graph': 1, 'system': 2, 'trees': 3, 'eps': 4, 'computer': 5, 'survey': 6, 'user': 7, 'human': 8, 'time': 9, 'interface': 10, 'response': 11}
2014-05-25 02:58:50,486 : INFO : collecting document frequencies
2014-05-25 02:58:50,486 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,486 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,486 : INFO : PROGRESS: processing document #0
2014-05-25 02:58:50,486 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,486 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,486 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,487 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,487 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,488 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,488 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,488 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,488 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
2014-05-25 02:58:50,488 : INFO : calculating IDF weights for 9 documents and 34 features (51 matrix non-zeros)
2014-05-25 02:58:50,488 : INFO : ############### corpus_tfidf ###############
2014-05-25 02:58:50,488 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,488 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,489 : INFO : ############### lda ###############
2014-05-25 02:58:50,489 : INFO : using symmetric alpha at 0.5
2014-05-25 02:58:50,489 : INFO : using serial LDA version on this node
2014-05-25 02:58:50,489 : WARNING : input corpus stream has no len(); counting documents
2014-05-25 02:58:50,489 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,489 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,489 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,489 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,489 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,490 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,490 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,491 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,491 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,491 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,491 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
2014-05-25 02:58:50,491 : INFO : running online LDA training, 2 topics, 1 passes over the supplied corpus of 9 documents, updating model once every 9 documents, evaluating perplexity every 9 documents, iterating 50 with a convergence threshold of 0
2014-05-25 02:58:50,491 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
2014-05-25 02:58:50,491 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-05-25 02:58:50,491 : INFO : built Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...) from 1 documents (total 7 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(7 unique tokens: ['abc', 'lab', 'machine', 'applications', 'computer']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...) from 2 documents (total 14 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(13 unique tokens: ['abc', 'system', 'lab', 'machine', 'applications']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...) from 3 documents (total 19 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(15 unique tokens: ['abc', 'management', 'system', 'lab', 'eps']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...) from 4 documents (total 25 corpus positions)
2014-05-25 02:58:50,492 : INFO : adding document #0 to Dictionary(17 unique tokens: ['abc', 'testing', 'management', 'system', 'lab']...)
2014-05-25 02:58:50,492 : INFO : built Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...) from 5 documents (total 32 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(21 unique tokens: ['measurement', 'perceived', 'abc', 'testing', 'management']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 6 documents (total 37 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(26 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...) from 7 documents (total 41 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(29 unique tokens: ['generation', 'testing', 'engineering', 'computer', 'relation']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 8 documents (total 49 corpus positions)
2014-05-25 02:58:50,493 : INFO : adding document #0 to Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...)
2014-05-25 02:58:50,493 : INFO : built Dictionary(35 unique tokens: ['minors', 'generation', 'testing', 'iv', 'engineering']...) from 9 documents (total 52 corpus positions)
</code></pre>
","python, lda, topic-modeling, gensim","<p>This is caused by using a <strong>corpus</strong> and <strong>dictionary</strong> that don't have the same id-to-word mapping.  It can happen if you prune your dictionary and call <code>dictionary.compactify()</code> at the wrong time.</p>

<p>A simple example will make it clear.  Let's make a dictionary:</p>

<pre><code>from gensim.corpora.dictionary import Dictionary
documents = [
    ['here', 'is', 'one', 'document'],
    ['here', 'is', 'another', 'document'],
]
dictionary = Dictionary()
dictionary.add_documents(documents)
</code></pre>

<p>This dictionary now has entries for these words and maps them to integer id's. It's useful to turn documents into vectors of <code>(id, count)</code> tuples (which we'd want to do before passing them into a model):</p>

<pre><code>vectorized_corpus = [dictionary.doc2bow(doc) for doc in corpus]
</code></pre>

<p>Sometimes you'll want to alter your dictionary.  For example, you might want to remove very rare, or very common words:</p>

<pre><code>dictionary.filter_extremes(no_below=2, no_above=0.5, keep_n=100000)
dictionary.compactify()
</code></pre>

<p>Removing words creates gaps in the dictionary, but calling <code>dictionary.compactify()</code> re-assigns ids to fill in the gaps.  But that means our <code>vectorized_corpus</code> from above doesn't use the same id's as the <code>dictionary</code> any more, and if we pass them into a model, we'll get an <code>IndexError</code>.</p>

<p><strong>Solution</strong>: make your vector representation using the dictionary <em>after</em> making changes and calling <code>dictionary.compactify()</code>!</p>
",6,2,2584,2014-05-25 09:23:37,https://stackoverflow.com/questions/23853828/python-indexerror-using-gensim-for-lda-topic-modeling
word2vec lemmatization of corpus before training,"<p>Word2vec seems to be mostly trained on raw corpus data. However, lemmatization is a standard preprocessing for many semantic similarity tasks. I was wondering if anybody had experience in lemmatizing the corpus before training word2vec and if this is a useful preprocessing step to do.</p>
","nlp, word2vec, gensim, lemmatization","<p>I think it really matters about what you want to solve with this. It depends on the task. </p>

<p>Essentially by lemmatization, you make the input space sparser, which can help if you don't have enough training data. </p>

<p>But since Word2Vec is fairly big, if you have big enough training data, lemmatization shouldn't gain you much. </p>

<p>Something more interesting is, how to do tokenization with respect to the existing diction of words-vectors inside the W2V (or anything else).  Like ""Good muffins cost $3.88\nin New York."" needs to be tokenized to ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New York.'] Then you can replace it with its vectors from W2V. The challenge is that some tokenizers my tokenize ""New York"" as ['New' 'York'], which doesn't make much sense. (For example, NLTK is making this mistake <a href=""https://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html"" rel=""noreferrer"">https://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html</a>) This is a problem when you have many multi-word phrases.</p>
",9,32,17221,2014-05-26 20:35:36,https://stackoverflow.com/questions/23877375/word2vec-lemmatization-of-corpus-before-training
Python- Closing bunch of text files opened at the same time with list comprehensions,"<p>I am working on an LDA model with gensim. For this, I am basically opening text files, building a dictionary, and then running the model. </p>

<p>To open the files I use this: </p>

<pre><code>files = [codecs.open(infile, 'r', 'utf-16', 'ignore') for infile in sample_list] 
</code></pre>

<p>in which sample_list is a list of paths to files. I need to use codecs.open because the texts are in a different language (and I haven't updated Python). </p>

<p>My problem is that I don't know how to close all the files after using them. Any ideas? I've tried a couple of things. I cannot use a regular loop here because of my following step is:</p>

<pre><code>texts = ["" "".join(file.readlines()[0:]) for file in files]
</code></pre>

<p>When I use over 5,000 files I get the error '' IOError: [Errno 24] Too many open files '' I am thinking that I could open a number of files at a time,  join them, close them, and repeat. Also, keeping the files open is just bad. 
Thank you!</p>
","python, loops, text, gensim","<pre><code>def read_contents(filename):
    with codecs.open(filename, 'r', 'utf-16', 'ignore') as infile:
        return ' '.join(infile)

texts = [read_contents(filename) for filename in sample_list]
</code></pre>

<h3>Using <code>with</code> is equivalent to doing:</h3>

<pre><code>def read_contents(filename):
    try:
        infile = codecs.open(filename, 'r', 'utf-16', 'ignore')
        return ' '.join(infile)
    finally:
        infile.close()
</code></pre>

<p>The <code>finally</code> keyword ensures that <code>close()</code> gets executed no matter what, even if the script throws an error inside of <code>try</code>.</p>
",4,2,92,2014-06-13 19:09:45,https://stackoverflow.com/questions/24212368/python-closing-bunch-of-text-files-opened-at-the-same-time-with-list-comprehens
Load file with django manage.py runserver,"<p>I'm using Django for a search engine. Requests are made by POST, the server treats them and answers with JSON format. In order to be fast, I need to have an index file loaded at the beginning (with manage.py runserver) and a way to access it when a view is called.</p>

<p>Does anyone know how to do it ?</p>

<p>Thanks in advance !</p>
","python, django, gensim","<p>Loading the index file at process startup means you'll have to restart your process(es) each time the index file changes. You'd be better running a dedicated process as the ""search engine"" and have your Django application communicate with this process (hint: you probably want a json document database, really). </p>

<p>But anyway: to have your index loaded at process startup, the simplest way is to do it in one of your models files - then your view can just import it from the models file.</p>
",0,-1,620,2014-07-21 09:03:08,https://stackoverflow.com/questions/24861239/load-file-with-django-manage-py-runserver
retrieve topic-word array &amp; document-topic array from lda gensim,"<p><strong>Situation:</strong></p>

<p>I have a numpy term-document matrix 
example: [[0,1,0,0....],....[......0,0,0,0]].</p>

<p>I have plugged in the above matrix to the ldamodel method of the gensim. And it is working fine with the lad method <code>lda = LdaModel(corpus, num_topics=10)</code>. 
<code>corpus</code> is my term-document matrix mentioned above.
I needed two intermediate matrices( <strong>topic-word array &amp; document-topic array</strong>) for research purpose.</p>

<blockquote>
  <p>1) per document-topic probability matrix (p_d_t)</p>
  
  <p>2) per topic-word probability matrix (p_w_t)</p>
</blockquote>

<p><strong>Question:</strong></p>

<p>How to get those array from the gensim <code>LdaModel()</code> function.? Kindly help me with getting those matrices.</p>
","lda, gensim","<p>1.Per-document topic probability matrix: </p>

<p>Apply a <a href=""http://radimrehurek.com/gensim/tut2.html"">transformation</a> to your corpus. </p>

<pre><code>docTopicProbMat = lda[corpus]
</code></pre>

<ol start=""2"">
<li>Per-topic word probability matrix: </li>
</ol>

<p><code>
K = lda.num_topics
topicWordProbMat = lda.print_topics(K)
</code></p>
",10,5,5561,2014-09-12 07:46:07,https://stackoverflow.com/questions/25803267/retrieve-topic-word-array-document-topic-array-from-lda-gensim
Why the similarity beteween two bag-of-words in gensim.word2vec calculated this way?,"<pre><code>def n_similarity(self, ws1, ws2):
    v1 = [self[word] for word in ws1]
    v2 = [self[word] for word in ws2]
    return dot(matutils.unitvec(array(v1).mean(axis=0)), matutils.unitvec(array(v2).mean(axis=0)))
</code></pre>

<p>This is the code I excerpt from gensim.word2Vec, I know that two single words' similarity can be calculated by cosine distances, but what about two word sets? The code seems to use the mean of each wordvec and then calculated on the two mean vectors' cosine distance. I know few in word2vec, is there some foundations of such process?</p>
","nlp, gensim, word2vec","<p>Taking the mean of all word vectors is the simplest way of reducing them to a single vector so cosine similarity can be used. The intuition is that by adding up all word vectors you get a bit of all of them (the meaning) in the result. You then divide by the number of vectors so that larger bag of words don't end up with longer vectors (not that it matters for cosine similarity anyway).</p>

<p>There are other ways to reduce an entire sentence to a single vector is a complex one. I wrote a bit about it in a <a href=""https://stackoverflow.com/a/15173821/419338"">related question</a> on SO. Since then a bunch of new algorithms have been proposed. One of the more accessible ones is <a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">Paragraph Vector</a>, which you shouldn't have problems understanding if you are familiar with <code>word2vec</code>.</p>
",3,3,3355,2014-09-24 07:08:51,https://stackoverflow.com/questions/26010645/why-the-similarity-beteween-two-bag-of-words-in-gensim-word2vec-calculated-this
BleiCorpus and Associated Press dataset in Gensim: IO Error,"<p>I am trying to follow the tutorial on topic modelling / Latent Dirichlet Allocation (LDA) in the book Building Machine Learning Systems"" with Python.</p>

<p>I have not gone too far in this book, and the the first part of topic modelling returns errors for me:</p>

<pre><code>from gensim import corpora, models, similarities
corpus = corpora.BleiCorpus('./data/ap/ap.dat', './data/ap/vocab.txt')
</code></pre>

<p>Error:</p>

<pre><code>     63 
     64         self.fname = fname
---&gt; 65         with utils.smart_open(fname_vocab) as fin:
     66             words = [utils.to_unicode(word).rstrip() for word in fin]
     67         self.id2word = dict(enumerate(words))

/Users/user/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/gensim/utils.pyc in smart_open(fname, mode)
    659         from gzip import GzipFile
    660         return make_closing(GzipFile)(fname, mode)
--&gt; 661     return open(fname, mode)
    662 
    663 

IOError: [Errno 2] No such file or directory: './data/ap/vocab.txt'
</code></pre>

<p>The vocab.txt file does not exists, but switching to the directory where it is supposed to be, I find the following:</p>

<blockquote>
  <p>$ ls
  download_ap.sh        download_wp.sh      preprocess-wikidata.sh</p>
</blockquote>

<p>It looks like the ap data needs to be downloaded separately (not mentioned in the book), so by doing this:</p>

<pre><code>sh download_ap.sh
</code></pre>

<p>I get this:</p>

<pre><code>download_ap.sh: line 2: wget: command not found
tar: Error opening archive: Failed to open 'ap.tgz'
</code></pre>

<p>Does anybody knows how to solve this issue?</p>

<p>Thank you </p>
","python, enthought, lda, topic-modeling, gensim","<p>There is nothing wrong with the code or your dev environment. The most likely problem is that you don't have wget. The same functionality can be achieved with CURL, in case you want to try it. You can also download the Associated Press corpus directly from some other source (do a Google search) and place it in the directory that Gensim is using for the tutorial.</p>

<p>If you want to follow the tutorials exactly as in the book, you probably need to install wget, which for OS X (I assume that's your system), requires a little bit of configuration. To add and install wget to OS X you need to download the source files, compile the code and make an install. To actually compile the code you need a compiler, unfortunately it doesn’t come with OS X by default. First you need to install xcode suite from Apple which includes the GCC compiler.</p>

<p>This <a href=""http://coolestguidesontheplanet.com/install-and-configure-wget-on-os-x/"" rel=""nofollow"">post</a> explains how to do it step by step.</p>

<p>Hope this works.</p>
",2,1,1627,2014-10-01 16:20:48,https://stackoverflow.com/questions/26145937/bleicorpus-and-associated-press-dataset-in-gensim-io-error
Installing gensim in windows 7,"<p>I am trying to install gensim python library. However I am facing some dependencies errors. I ve isntalled schipy and numpy throught canopy. Next step to use pip install gensim in order to get gensim package. However I am getting error messages. I have installed python 2.7.4. I ve got visual studio 2010 installed on my machine.</p>

<p><img src=""https://i.sstatic.net/2dPne.jpg"" alt=""enter image description here"">
<img src=""https://i.sstatic.net/7R2g3.jpg"" alt=""enter image description here""></p>
","python, gensim","<p>After some minutes googling, thanksfully I think I found a solution. Basically I read this post in :<a href=""https://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat"">here</a>. What I did is to re-install scipy. Afterthat I install libpython from <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/"" rel=""nofollow noreferrer"">here</a>. </p>
",3,5,4200,2014-10-08 07:45:48,https://stackoverflow.com/questions/26251674/installing-gensim-in-windows-7
Python:: IOError: [Errno 2] No such file or directory: &#39;models/dictionary.dict&#39;,"<p>I am using gensim package for topic modelling in python.</p>

<p>I am trying to train the topic model using gensim. Below is the train.py module:</p>

<pre><code>class Corpus(object):
    def __init__(self, cursor, reviews_dictionary, corpus_path):
        self.cursor = cursor
        self.reviews_dictionary = reviews_dictionary
        self.corpus_path = corpus_path

    def __iter__(self):
        self.cursor.rewind()
        for review in self.cursor:
            yield self.reviews_dictionary.doc2bow(review[""words""])

    def serialize(self):
        BleiCorpus.serialize(self.corpus_path, self, id2word=self.reviews_dictionary)

        return self


class Dictionary(object):
    def __init__(self, cursor, dictionary_path):
        self.cursor = cursor
        self.dictionary_path = dictionary_path

    def build(self):
        self.cursor.rewind()
        dictionary = corpora.Dictionary(review[""words""] for review in self.cursor)
        dictionary.filter_extremes(keep_n=10000)
        dictionary.compactify()
        corpora.Dictionary.save(dictionary, self.dictionary_path)

        return dictionary


class Train:
    def __init__(self):
        pass

    @staticmethod
    def run(lda_model_path, corpus_path, num_topics, id2word):
        corpus = corpora.BleiCorpus(corpus_path)
        lda = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=id2word)
        lda.save(lda_model_path)

        return lda
</code></pre>

<p>I am getting the below error when I run this module:</p>

<pre><code>&gt; Traceback (most recent call last):


    File ""train.py"", line 74, in &lt;module&gt;
    main()

    File ""train.py"", line 68, in main
    dictionary = Dictionary(reviews_cursor, dictionary_path).build()
    File ""train.py"", line 38, in build
    corpora.Dictionary.save(dictionary, self.dictionary_path)
    File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 288, in save
    pickle(self, fname)
    File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 666, in pickle
    with smart_open(fname, 'wb') as fout: # 'b' for binary, needed on Windows
    File ""/usr/local/lib/python2.7/dist-packages/gensim/utils.py"", line 661, in smart_open
    return open(fname, mode)
    IOError: [Errno 2] No such file or directory: 'models/dictionary.dict'
</code></pre>

<p>Could anyone please help me figure out the issue?</p>
","python, gensim","<p>When ""No such file or directory"" occurs during a save operation, it usually means the directory path that you have specified as the container for the output file does not exist.  In this case you have clearly given it  <code>self.dictionary_path = ""models/dictionary.dict""</code> which is a relative path. An error saving to this path presumably means a file cannot be saved inside ""models"" because the directory ""models"" does not exist relative to the current working directory.  </p>

<p>To find out the current working directory you can use <code>os.getcwd</code> .  To test whether a directory exists you can use <code>os.path.isdir</code>.  To create a directory you can use <code>os.mkdir</code>.</p>
",3,-1,4838,2014-10-09 19:12:16,https://stackoverflow.com/questions/26286206/python-ioerror-errno-2-no-such-file-or-directory-models-dictionary-dict
Index Error when running LDA in gensim,"<p>I read the docs I have</p>

<pre><code>corpusObj.readDocsSample(sampleFile)
</code></pre>

<p>Next,</p>

<pre><code>dictionary = corpusObj.buildDictionary()
</code></pre>

<p>Then I build a corpus:</p>

<pre><code>corpus = corpusObj.buildCorpus()
</code></pre>

<p>Definition of buildDictionary and buildCorpus:</p>

<pre><code>def buildDictionary(self):

         texts = [[word for word in self.docs[i]] for i in self.docs]
         self.dictionary = corpora.Dictionary(texts)
         return self.dictionary


def buildCorpus(self):
         return [self.dictionary.doc2bow(words) for words in self.docs.itervalues()]
</code></pre>

<p>Then I do stop words stuff:</p>

<pre><code>stop = corpus.readStopWords()
stopids = [dictionary.token2id[stopword] for stopword in stop
         if stopword in dictionary.token2id]
dictionary.filter_tokens(stopids)
dictionary.compactify()
</code></pre>

<p>Then I call:</p>

<pre><code> lda = gensim.models.ldamodel.LdaModel(corpus=corp, id2word=dictionary, num_topics=100, update_every=1, chunksize=1000, passes=1)
</code></pre>

<p>Here is the error:</p>

<pre><code> Traceback (most recent call last):
File ""/Users/jsuit/PycharmProjects/MyGensimPlaything/GensimPlayToy.py"", line 33, in &lt;module&gt;
lda = gensim.models.ldamodel.LdaModel(corpus=corp, id2word=dictionary, num_topics=100,     update_every=1, chunksize=1000, passes=1)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 313, in __init__
self.update(corpus)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 553, in update
self.log_perplexity(chunk, total_docs=lencorpus)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 464, in log_perplexity
perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 639, in bound
gammad, _ = self.inference([doc])
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.py"", line 376, in inference
expElogbetad = self.expElogbeta[:, ids]
IndexError: index 46979 is out of bounds for axis 1 with size 46979
</code></pre>

<p>The logging information below shows that it gets started but then crashes.</p>

<pre><code>2014-11-07 19:31:56,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
2014-11-07 19:32:00,458 : INFO : built Dictionary(47445 unique tokens: [u'Szczecin', u'pro-Soviet', u'Negroponte', u'1,800', u'woods']...) from 2250 documents (total 1050902 corpus positions)
2014-11-07 19:32:08,192 : DEBUG : rebuilding dictionary, shrinking gaps
2014-11-07 19:32:08,237 : INFO : using symmetric alpha at 0.01
2014-11-07 19:32:08,237 : INFO : using serial LDA version on this node
2014-11-07 19:32:08,856 : INFO : running online LDA training, 100 topics, 1 passes over   
the supplied corpus of 2250 documents, updating model once every 1000 documents,           
evaluating perplexity every 2250 documents, iterating 50x with a convergence threshold of 
0.001000
2014-11-07 19:32:08,856 : WARNING : too few updates, training might not converge; 
consider increasing the number of passes or iterations to improve accuracy
2014-11-07 19:32:08,931 : INFO : PROGRESS: pass 0, at document #1000/2250
2014-11-07 19:32:08,931 : DEBUG : performing inference on a chunk of 1000 documents
2014-11-07 19:32:15,414 : DEBUG : 22/1000 documents converged within 50 iterations
2014-11-07 19:32:15,432 : DEBUG : updating topics
2014-11-07 19:32:15,476 : INFO : merging changes from 1000 documents into a model of 2250  
documents
2014-11-07 19:32:16,222 : INFO : topic #60 (0.010): 0.057*Neeman + 0.042*woods + 0.039*needed + 0.024*timeout + 0.020*reggae + 0.020*Shocked + 0.019*Dexter + 0.015*nonsensical + 0.014*3-to-1 + 0.011*Mauritius
2014-11-07 19:32:16,237 : INFO : topic #45 (0.010): 0.049*needed + 0.047*Neeman + 0.042*woods + 0.024*reggae + 0.023*timeout + 0.022*Dexter + 0.022*Shocked + 0.019*nonsensical + 0.012*3-to-1 + 0.011*mid-week
2014-11-07 19:32:16,251 : INFO : topic #86 (0.010): 0.049*needed + 0.048*Neeman + 0.047*woods + 0.029*Shocked + 0.023*timeout + 0.017*nonsensical + 0.016*reggae + 0.016*3-to-1 + 0.014*Dexter + 0.014*Mauritius
2014-11-07 19:32:16,265 : INFO : topic #92 (0.010): 0.017*Neeman + 0.016*needed + 0.014*woods + 0.011*Dexter + 0.010*timeout + 0.009*reggae + 0.006*Shocked + 0.006*nonsensical + 0.005*22-month-old + 0.004*3-to-1
2014-11-07 19:32:16,279 : INFO : topic #95 (0.010): 0.045*needed + 0.041*woods + 0.032*Shocked + 0.028*Neeman + 0.022*timeout + 0.020*nonsensical + 0.018*reggae + 0.017*Dexter + 0.013*Mauritius + 0.010*3-to-1
2014-11-07 19:32:16,294 : INFO : topic #30 (0.010): 0.054*needed + 0.052*Neeman + 0.033*woods + 0.024*timeout + 0.022*nonsensical + 0.021*Dexter + 0.021*Shocked + 0.016*reggae + 0.013*Mauritius + 0.012*3-to-1
2014-11-07 19:32:16,307 : INFO : topic #51 (0.010): 0.000*expands + 0.000*Promotion + 0.000*Arnold + 0.000*1,320.75 + 0.000*credits + 0.000*tuition + 0.000*_Or + 0.000*Hunt + 0.000*Futrell + 0.000*stagecoaches
2014-11-07 19:32:16,321 : INFO : topic #41 (0.010): 0.045*Neeman + 0.032*needed + 0.031*woods + 0.016*Dexter + 0.013*nonsensical + 0.013*Shocked + 0.013*timeout + 0.011*reggae + 0.009*peux + 0.009*Mauritius
2014-11-07 19:32:16,336 : INFO : topic #28 (0.010): 0.052*Neeman + 0.046*needed + 0.040*woods + 0.030*timeout + 0.026*Shocked + 0.019*nonsensical + 0.018*Dexter + 0.014*reggae + 0.011*3-to-1 + 0.010*crouch
2014-11-07 19:32:16,351 : INFO : topic #11 (0.010): 0.046*Neeman + 0.044*woods + 0.037*needed + 0.031*Shocked + 0.021*Dexter + 0.021*reggae + 0.017*nonsensical + 0.017*timeout + 0.012*3-to-1 + 0.010*Mauritius
2014-11-07 19:32:16,365 : INFO : topic #20 (0.010): 0.067*Neeman + 0.036*woods + 0.035*needed + 0.028*timeout + 0.020*reggae + 0.018*Dexter + 0.016*Mauritius + 0.015*Shocked + 0.015*nonsensical + 0.014*mid-week
2014-11-07 19:32:16,379 : INFO : topic #31 (0.010): 0.001*Neeman + 0.001*woods + 0.001*timeout + 0.001*reggae + 0.000*needed + 0.000*Dexter + 0.000*3-to-1 + 0.000*Shocked + 0.000*romped + 0.000*1,800
2014-11-07 19:32:16,393 : INFO : topic #80 (0.010): 0.043*woods + 0.042*Neeman + 0.037*needed + 0.029*timeout + 0.024*Shocked + 0.017*nonsensical + 0.015*reggae + 0.014*Dexter + 0.011*1,800 + 0.011*3-to-1
2014-11-07 19:32:16,407 : INFO : topic #58 (0.010): 0.029*Neeman + 0.027*needed + 0.019*woods + 0.019*timeout + 0.013*Dexter + 0.010*Shocked + 0.008*nonsensical + 0.008*mid-week + 0.007*reggae + 0.007*3-to-1
 2014-11-07 19:32:16,421 : INFO : topic #79 (0.010): 0.002*woods + 0.002*needed + 0.002*sustaining + 0.001*Neeman + 0.001*timeout + 0.001*Godchaux + 0.001*Dexter + 0.001*dozen + 0.001*rumor + 0.001*Miami-based
 2014-11-07 19:32:16,437 : INFO : topic diff=78.067282, rho=1.000000  
 2014-11-07 19:32:16,523 : INFO : PROGRESS: pass 0, at document #2000/2250
 2014-11-07 19:32:16,523 : DEBUG : performing inference on a chunk of 1000 documents
 2014-11-07 19:32:22,841 : DEBUG : 38/1000 documents converged within 50 iterations
 2014-11-07 19:32:22,862 : DEBUG : updating topics
 2014-11-07 19:32:22,919 : INFO : merging changes from 1000 documents into a model of     
 2250 documents
 2014-11-07 19:32:23,640 : INFO : topic #63 (0.010): 0.017*Neeman + 0.016*needed + 0.014*autobiography + 0.013*teacher + 0.012*woods + 0.011*Mauritius + 0.010*Shocked + 0.009*timeout + 0.007*mid-week + 0.007*CFC
 2014-11-07 19:32:23,654 : INFO : topic #8 (0.010): 0.028*Neeman + 0.027*woods + 0.024*Shocked + 0.023*needed + 0.016*timeout + 0.013*Dexter + 0.010*reggae + 0.010*nonsensical + 0.007*Mauritius + 0.007*65-plus
 2014-11-07 19:32:23,669 : INFO : topic #85 (0.010): 0.054*needed + 0.041*woods + 0.036*Neeman + 0.036*Shocked + 0.031*timeout + 0.023*nonsensical + 0.016*reggae + 0.014*Dexter + 0.011*3-to-1 + 0.011*crouch
 2014-11-07 19:32:23,683 : INFO : topic #18 (0.010): 0.017*needed + 0.013*woods + 0.011*Neeman + 0.010*timeout + 0.009*Shocked + 0.008*reggae + 0.008*Guttierez + 0.006*livid + 0.006*Vermont + 0.006*Dexter
  2014-11-07 19:32:23,697 : INFO : topic #49 (0.010): 0.028*needed + 0.028*Neeman + 0.026*woods + 0.024*timeout + 0.019*Dexter + 0.017*nonsensical + 0.012*reggae + 0.009*Shocked + 0.007*3-to-1 + 0.007*crouch
 2014-11-07 19:32:23,712 : INFO : topic #53 (0.010): 0.035*mid-week + 0.034*Mauritius + 0.028*Neeman + 0.028*needed + 0.027*woods + 0.024*Tourism + 0.023*macho + 0.014*Shocked + 0.013*nonsensical + 0.012*timeout
 2014-11-07 19:32:23,726 : INFO : topic #32 (0.010): 0.071*Neeman + 0.031*woods + 0.022*needed + 0.015*timeout + 0.013*Shocked + 0.012*Dexter + 0.012*Mauritius + 0.009*nonsensical + 0.009*mid-week + 0.007*reggae
 2014-11-07 19:32:23,740 : INFO : topic #78 (0.010): 0.040*needed + 0.039*Neeman + 0.039*woods + 0.019*timeout + 0.019*Shocked + 0.017*Dexter + 0.017*reggae + 0.016*nonsensical + 0.015*3-to-1 + 0.011*mid-week
 2014-11-07 19:32:23,754 : INFO : topic #94 (0.010): 0.002*needed + 0.002*Neeman + 0.001*woods + 0.001*timeout + 0.001*Dexter + 0.001*nonsensical + 0.001*Shocked + 0.001*3-to-1 + 0.001*reggae + 0.000*dozen
 2014-11-07 19:32:23,768 : INFO : topic #17 (0.010): 0.023*needed + 0.022*woods + 0.018*Neeman + 0.018*timeout + 0.012*1,800 + 0.011*reggae + 0.011*Shocked + 0.010*Dexter + 0.008*Anderson + 0.007*Bovek
 2014-11-07 19:32:23,781 : INFO : topic #73 (0.010): 0.001*woods + 0.001*timeout + 0.001*Neeman + 0.001*Shocked + 0.001*reggae + 0.001*Falcon + 0.001*Dexter + 0.001*needed + 0.001*dozes + 0.001*dozen
 2014-11-07 19:32:23,796 : INFO : topic #96 (0.010): 0.049*Neeman + 0.048*needed + 0.043*woods + 0.025*timeout + 0.019*nonsensical + 0.018*Shocked + 0.015*Dexter + 0.013*3-to-1 + 0.011*reggae + 0.010*crouch
 2014-11-07 19:32:23,810 : INFO : topic #46 (0.010): 0.042*needed + 0.035*Neeman + 0.033*woods + 0.025*Shocked + 0.019*3-to-1 + 0.016*reggae + 0.016*timeout + 0.013*Dexter + 0.012*nonsensical + 0.011*Mauritius
 2014-11-07 19:32:23,824 : INFO : topic #39 (0.010): 0.002*needed + 0.002*Neeman + 0.001*woods + 0.001*Shocked + 0.001*Dexter + 0.001*mid-week + 0.001*timeout + 0.001*18th + 0.001*nonsensical + 0.001*Mauritius
 2014-11-07 19:32:23,838 : INFO : topic #66 (0.010): 0.036*Neeman + 0.032*woods + 0.021*needed + 0.020*timeout + 0.019*Shocked + 0.019*Blank + 0.014*cares + 0.013*Dexter + 0.011*reggae + 0.010*nonsensical
 2014-11-07 19:32:23,855 : INFO : topic diff=5.923197, rho=0.707107
 2014-11-07 19:32:24,260 : DEBUG : bound: at document #0
</code></pre>

<p>And then we get the error message I posted above.</p>
","python, lda, topic-modeling, gensim","<p>It looks like you created a corpus with a dictionary, then modified the dictionary, so the indices don't align. Try removing stopwords first, then build the corpus and finally apply the LDA model.</p>
",4,2,3786,2014-11-08 01:13:02,https://stackoverflow.com/questions/26812617/index-error-when-running-lda-in-gensim
what does the vector of a word in word2vec represents?,"<p><a href=""https://code.google.com/p/word2vec/"" rel=""noreferrer"">word2vec</a> is a open source tool by Google: </p>

<ul>
<li><p>For each word it provides a vector of float values, what exactly do they represent?</p></li>
<li><p>There is also a paper on <a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""noreferrer"">paragraph vector</a> can anyone explain how they are using word2vec in order to obtain fixed length vector for a paragraph.</p></li>
</ul>
","machine-learning, nlp, neural-network, gensim","<p><strong>TLDR</strong>: Word2Vec is building word projections (<em>embeddings</em>) in a <strong>latent space</strong> of N dimensions, (N being the size of the word vectors obtained). The float values represents the coordinates of the words in this N dimensional space.</p>

<p>The major idea behind latent space projections, putting objects in a different and continuous dimensional space, is that your objects will have a representation (a vector) that has more interesting calculus characteristics than basic objects. </p>

<p>For words, what's useful is that you have a <strong>dense</strong> vector space which encodes <strong>similarity</strong> (i.e tree has a vector which is more similar to wood than from dancing). This opposes to classical <strong>sparse</strong> one-hot or ""bag-of-word"" encoding which treat each word as one dimension making them <strong>orthogonal</strong> by design (i.e tree,wood and dancing all have the same distance between them)</p>

<p>Word2Vec algorithms do this:</p>

<p>Imagine that you have a sentence: </p>

<blockquote>
  <p>The dog has to go ___ for a walk in the park.</p>
</blockquote>

<p>You obviously want to fill the blank with the word ""outside"" but you could also have ""out"". The w2v algorithms are inspired by this idea. You'd like all words that fill in the blanks near, because they belong together - This is called the <strong>Distributional Hypothesis</strong> - Therefore the words ""out"" and ""outside"" will be closer together whereas a word like ""carrot"" would be farther away. </p>

<p>This is sort of the ""intuition"" behind word2vec. For a more theorical explanation of what's going on i'd suggest reading:</p>

<ul>
<li><a href=""http://nlp.stanford.edu/pubs/glove.pdf"" rel=""nofollow noreferrer"">GloVe: Global Vectors for Word Representation</a></li>
<li><a href=""http://www.cs.bgu.ac.il/~yoavg/publications/conll2014analogies.pdf"" rel=""nofollow noreferrer"">Linguistic Regularities in Sparse and Explicit Word Representations</a> </li>
<li><a href=""http://www.cs.bgu.ac.il/~yoavg/publications/nips2014pmi.pdf"" rel=""nofollow noreferrer"">Neural Word Embedding as Implicit Matrix Factorization</a></li>
</ul>

<p>For paragraph vectors, the idea is the same as in w2v. Each paragraph can be represented by its words. Two models are presented in the paper. </p>

<ol>
<li>In a ""Bag of Word"" way (the pv-dbow model) where one <strong>fixed length</strong> paragraph vector is used to predict its words.</li>
<li>By adding a <strong>fixed length</strong> paragraph token in word contexts (the pv-dm model). By retropropagating the gradient they get ""a sense"" of what's missing, bringing paragraph with the same words/topic ""missing"" close together.</li>
</ol>

<p><a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">Bits from the article</a>:</p>

<blockquote>
  <p>The
  paragraph vector and word vectors are averaged or concatenated
  to predict the next word in a context.
  [...]
  The paragraph token can be thought of as another word. It
  acts as a memory that remembers what is missing from the
  current context – or the topic of the paragraph</p>
</blockquote>

<p>For full understanding on how these vectors are built you'll need to learn how neural nets are built and how the backpropagation algorithm works. (i'd suggest starting by <a href=""http://youtu.be/q0pm3BrIUFo"" rel=""nofollow noreferrer"">this video</a> and Andrew NG's Coursera class)</p>

<p><strong>NB:</strong> Softmax is just a fancy way of saying classification, each word in w2v algorithms is considered as a class. Hierarchical softmax/negative sampling are tricks to speed up softmax and handle a lot of classes.</p>
",37,22,12141,2014-11-20 05:40:50,https://stackoverflow.com/questions/27032517/what-does-the-vector-of-a-word-in-word2vec-represents
Load PreComputed Vectors Gensim,"<p>I am using the Gensim Python package to learn a neural language model, and I know that you can provide a training corpus to learn the model. However, there already exist many precomputed word vectors available in text format (e.g. <a href=""http://www-nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">http://www-nlp.stanford.edu/projects/glove/</a>). Is there some way to initialize a Gensim Word2Vec model that just makes use of some precomputed vectors, rather than having to learn the vectors from scratch?</p>

<p>Thanks! </p>
","python, nlp, gensim, word2vec","<p>You can download pre-trained word vectors from here (get the file 'GoogleNews-vectors-negative300.bin'):
<a href=""https://code.google.com/p/word2vec/"" rel=""noreferrer"">word2vec</a></p>

<p>Extract the file and then you can load it in python like:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec.load_word2vec_format(os.path.join(os.path.dirname(__file__), 'GoogleNews-vectors-negative300.bin'), binary=True)

model.most_similar('dog')
</code></pre>

<p>EDIT (May 2017):
As the above code is now deprecated, this is how you'd load the vectors now:</p>

<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(os.path.dirname(__file__), 'GoogleNews-vectors-negative300.bin'), binary=True)
</code></pre>
",24,25,20415,2014-11-26 01:35:41,https://stackoverflow.com/questions/27139908/load-precomputed-vectors-gensim
Passing Term-Document Matrix to Gensim LDA Model,"<p>My term-document matrix is in a numpy matrix format, and I have a dictionary to represent the  of the term-document matrix.</p>

<p>Is there any way I can easily pass these two into Gensim's LDA model?</p>

<pre><code>tdMatrix = np.load('tdmatrix.npy')
dictionary = cPickle.load(open('dictionary.p', 'r')) # stores term represented by each column
</code></pre>

<p>Can I pass this somewhow to gensim.models.ldamodel.LDA?</p>
","python, numpy, machine-learning, nlp, gensim","<p>I believe Gensim uses pretty much the same structure to represent a bag of words corpus, but I don't think a default dictionary or numpy array would be compatible.
Gensim's API lists a few ""corpusreaders"" that can accommodate various formats, but those seem to be built for importing data from other tool kits.
So maybe in your case the easiest solution would be to reconstruct the documents using your matrix and dictionary as a list of separated strings. Then convert your list to Gensim's bag of word corpus and finally to LDA as shown in the <a href=""http://radimrehurek.com/gensim/tut1.html#corpus-streaming-one-document-at-a-time"" rel=""nofollow"">tutorials</a>.</p>

<p>This approach has the added benefit that you can apply Gensim's preprocessing functions and filter words with low/high frequencies.</p>
",1,4,1890,2014-12-01 02:40:15,https://stackoverflow.com/questions/27220927/passing-term-document-matrix-to-gensim-lda-model
A mistake in installing gensim,"<p>I can't install gensim successfully through many ways.For I'm  a freshman in coding,it's difficult for me to understand the following information.</p>

<hr>

<pre><code>**C:\Python27\Lib\site-packages\gensim-0.10.3&gt;python setup.py install
Traceback (most recent call last):
  File ""setup.py"", line 22, in &lt;module&gt;
    from setuptools import setup, find_packages, Extension
  File ""C:\Python27\lib\site-packages\setuptools\__init__.py"", line 12, in &lt;modu
le&gt;
    from setuptools.extension import Extension
  File ""C:\Python27\lib\site-packages\setuptools\extension.py"", line 7, in &lt;modu
le&gt;
    from setuptools.dist import _get_unpatched
  File ""C:\Python27\lib\site-packages\setuptools\dist.py"", line 16, in &lt;module&gt;
    from setuptools.compat import numeric_types, basestring
  File ""C:\Python27\lib\site-packages\setuptools\compat.py"", line 19, in &lt;module
&gt;
    from SimpleHTTPServer import SimpleHTTPRequestHandler
  File ""C:\Python27\lib\SimpleHTTPServer.py"", line 27, in &lt;module&gt;
    class SimpleHTTPRequestHandler(BaseHTTPServer.BaseHTTPRequestHandler):
  File ""C:\Python27\lib\SimpleHTTPServer.py"", line 208, in SimpleHTTPRequestHand
ler
    mimetypes.init() # try to read system mime.types
  File ""C:\Python27\lib\mimetypes.py"", line 358, in init
    db.read_windows_registry()
  File ""C:\Python27\lib\mimetypes.py"", line 258, in read_windows_registry
    for subkeyname in enum_types(hkcr):
  File ""C:\Python27\lib\mimetypes.py"", line 249, in enum_types
    ctype = ctype.encode(default_encoding) # omit in 3.x!
UnicodeDecodeError: 'ascii' codec can't decode byte 0xb6 in position 6: ordinal
not in range(128)**
</code></pre>

<hr>

<p>Thanks for help!</p>
","python, lda, gensim","<p>If you are new to Python I recommend <a href=""https://store.continuum.io/cshop/anaconda/"" rel=""nofollow noreferrer"">Anaconda</a>, a Python distribution that comes with many useful pacakges (including Gensim, IIRC) ready to use. </p>

<p>Alternatively try </p>

<pre><code>pip install Gensim
</code></pre>

<p>If you don't have pip installed yet look at <a href=""https://stackoverflow.com/questions/4750806/how-to-install-pip-on-windows"">this</a> answer.</p>
",0,1,282,2014-12-04 10:05:22,https://stackoverflow.com/questions/27291145/a-mistake-in-installing-gensim
Topic Modeling Using Gensim in Python,"<p>I have a list of bag of words for two classes. Say <strong><em>n</em></strong> items in class <strong><em>A</em></strong> and <strong><em>m</em></strong> items in class <strong><em>B</em></strong>. I want to use the topic modeling with gensim package (for LDA) in python in order to train a model for class A vs class B. Meanwhile I am new to both <strong>Topic Modeling</strong> and <strong>Python</strong>. Does anyone know how should I do this? I mean, should I merge all the bags for each class and the use gensim or should I use bag for each item seperately? Thanks!</p>
","python, machine-learning, nlp, lda, gensim","<p>If I understand you correctly you want to compare documents from two sources. </p>

<p>One way to do this with Gensim would be:</p>

<ul>
<li>create bag of words corpus from all documents (A and B) (~convert texts to a n X n matrix of ones and zeroes)</li>
<li>train LDA model on your corpus (~ find the topics)</li>
<li>convert corpus to LDA space (~ determine which topics are relevant for the documents)</li>
</ul>

<p>Now you can see topics distributions for each documents and determine how similar two documents are using Gensim's similarity methods.</p>

<p>For details take a look at Gensim's <a href=""http://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow"">tutorials</a>. The only modification you'd need to make would be to combine your documents from A and B into one bigger document and save the indices somewhere so that you can compare them easily later.</p>

<p>However, depending on your data and your goal, other forms of LDA (such as correlated topics models) may be more suitable.</p>
",1,1,2727,2014-12-05 03:10:43,https://stackoverflow.com/questions/27308118/topic-modeling-using-gensim-in-python
Convert word2vec bin file to text,"<p>From the <a href=""https://code.google.com/p/word2vec/"">word2vec</a> site I can download GoogleNews-vectors-negative300.bin.gz.  The .bin file (about 3.4GB) is a binary format not useful to me.  Tomas Mikolov <a href=""https://groups.google.com/d/msg/word2vec-toolkit/lxbl_MB29Ic/g4uEz5rNV08J"">assures us</a> that ""It should be fairly straightforward to convert the binary format to text format (though that will take more disk space). Check the code in the distance tool, it's rather trivial to read the binary file.""  Unfortunately, I don't know enough C to understand <a href=""http://word2vec.googlecode.com/svn/trunk/distance.c"">http://word2vec.googlecode.com/svn/trunk/distance.c</a>.</p>

<p>Supposedly <a href=""http://radimrehurek.com/2014/02/word2vec-tutorial/"">gensim</a> can do this also, but all the tutorials I've found seem to be about converting <em>from</em> text, not the other way.</p>

<p>Can someone suggest modifications to the C code or instructions for gensim to emit text?</p>
","python, c, gensim, word2vec","<p>On the word2vec-toolkit mailing list Thomas Mensink has provided an <a href=""https://groups.google.com/forum/#!topic/word2vec-toolkit/5Qh-x2O1lV4"" rel=""noreferrer"">answer</a> in the form of a small C program that will convert a .bin file to text.  This is a modification of the distance.c file.  I replaced the original distance.c with Thomas's code below and rebuilt word2vec (make clean; make), and renamed the compiled distance to readbin.  Then <code>./readbin vector.bin</code> will create a text version of vector.bin.</p>

<pre><code>//  Copyright 2013 Google Inc. All Rights Reserved.
//
//  Licensed under the Apache License, Version 2.0 (the ""License"");
//  you may not use this file except in compliance with the License.
//  You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
//  Unless required by applicable law or agreed to in writing, software
//  distributed under the License is distributed on an ""AS IS"" BASIS,
//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//  See the License for the specific language governing permissions and
//  limitations under the License.

#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;math.h&gt;
#include &lt;malloc.h&gt;

const long long max_size = 2000;         // max length of strings
const long long N = 40;                  // number of closest words that will be shown
const long long max_w = 50;              // max length of vocabulary entries

int main(int argc, char **argv) {
  FILE *f;
  char file_name[max_size];
  float len;
  long long words, size, a, b;
  char ch;
  float *M;
  char *vocab;
  if (argc &lt; 2) {
    printf(""Usage: ./distance &lt;FILE&gt;\nwhere FILE contains word projections in the BINARY FORMAT\n"");
    return 0;
  }
  strcpy(file_name, argv[1]);
  f = fopen(file_name, ""rb"");
  if (f == NULL) {
    printf(""Input file not found\n"");
    return -1;
  }
  fscanf(f, ""%lld"", &amp;words);
  fscanf(f, ""%lld"", &amp;size);
  vocab = (char *)malloc((long long)words * max_w * sizeof(char));
  M = (float *)malloc((long long)words * (long long)size * sizeof(float));
  if (M == NULL) {
    printf(""Cannot allocate memory: %lld MB    %lld  %lld\n"", (long long)words * size * sizeof(float) / 1048576, words, size);
    return -1;
  }
  for (b = 0; b &lt; words; b++) {
    fscanf(f, ""%s%c"", &amp;vocab[b * max_w], &amp;ch);
    for (a = 0; a &lt; size; a++) fread(&amp;M[a + b * size], sizeof(float), 1, f);
    len = 0;
    for (a = 0; a &lt; size; a++) len += M[a + b * size] * M[a + b * size];
    len = sqrt(len);
    for (a = 0; a &lt; size; a++) M[a + b * size] /= len;
  }
  fclose(f);
  //Code added by Thomas Mensink
  //output the vectors of the binary format in text
  printf(""%lld %lld #File: %s\n"",words,size,file_name);
  for (a = 0; a &lt; words; a++){
    printf(""%s "",&amp;vocab[a * max_w]);
    for (b = 0; b&lt; size; b++){ printf(""%f "",M[a*size + b]); }
    printf(""\b\b\n"");
  }  

  return 0;
}
</code></pre>

<p>I removed the ""\b\b"" from the <code>printf</code>.  </p>

<p>By the way, the resulting text file still contained the text word and some unnecessary whitespace which I did not want for some numerical calculations.  I removed the initial text column and the trailing blank from each line with bash commands.</p>

<pre><code>cut --complement -d ' ' -f 1 GoogleNews-vectors-negative300.txt &gt; GoogleNews-vectors-negative300_tuples-only.txt
sed 's/ $//' GoogleNews-vectors-negative300_tuples-only.txt
</code></pre>
",20,68,56245,2014-12-05 20:39:00,https://stackoverflow.com/questions/27324292/convert-word2vec-bin-file-to-text
GENSIM Error in Canopy Express,"<p>I am trying to run the GENSIM Topic modeling example in Canopy Express and get the following error on Sum() line.</p>

<pre><code>from gensim import corpora, models, similarities
from itertools import chain

"""""" DEMO """"""
documents = [""Human machine interface for lab abc computer applications"",
         ""A survey of user opinion of computer system response time"",
         ""The EPS user interface management system"",
         ""System and human system engineering testing of EPS"",
         ""Relation of user perceived response time to error measurement"",
         ""The generation of random binary unordered trees"",
         ""The intersection graph of paths in trees"",
         ""Graph minors IV Widths of trees and well quasi ordering"",
         ""Graph minors A survey""]

# remove common words and tokenize
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
     for document in documents]

# remove words that appear only once
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once] for text in texts]
</code></pre>

<p>The error I get is TypeError: an integer is required.  It seems to be ok in regular Python but Canopy has an issue.  It seems it is how Canopy treats the sum statement but I'm not sure how to work around it.  Any ideas as I'm just getting started with Python and text analysis.</p>
","python-2.7, canopy, gensim","<p>Canopy Python itself <em>is</em> ""regular Python 2.7"". However, the Python pane in the Canopy GUI is an IPython QTConsole, which adds a layer of functionality, mostly for better, but on rare occasion for worse. By default it starts in Pylab mode, which can be confusing to beginners  (see <a href=""https://support.enthought.com/entries/25750190-Modules-are-already-available-in-Canopy-s-Python-PyLab-prompt-but-not-in-a-script"" rel=""nofollow"">https://support.enthought.com/entries/25750190-Modules-are-already-available-in-Canopy-s-Python-PyLab-prompt-but-not-in-a-script</a>).</p>

<p>You don't describe what you are doing with any precision, but from the symptom that you describe, it sounds as if you are running your commands one-by-one at the IPython prompt, either by copy-paste or by selecting your commands in the text editor and doing ""Run Selection"". In the IPython prompt, because Pylab does an implicit <code>from numpy import *</code>, the <code>sum</code> function refers to numpy's sum, rather than the built-in Python sum, which would account for the error message that you report.</p>

<p>Three different solutions (out of many):</p>

<p>1) If you simply run your script (rather than ""Run Selection"" or copy/paste commands), it should act as expected. This is the most robust, flexible solution.</p>

<p>2) Disable Pylab mode in Canopy preferences; then you can run your commands either way.</p>

<p>3) (Not a great solution but instructive). Do <code>del sum</code> at the IPython prompt. This will delete the numpy <code>sum</code> from the IPython namespace, uncovering the original built-in <code>sum</code> and allowing your code to run either way.</p>
",0,-1,188,2014-12-15 03:51:47,https://stackoverflow.com/questions/27477084/gensim-error-in-canopy-express
How to download subset of Amazon CommonCrawel (only the text (WET files?) is needed),"<p>For research purposes, I want a large (~100K) set of web pages, though I am only interested in their text. I plan to use them for gensim LDA topic model. CommonCrawler seems like a good place to start, but I am not sure how to do it.
Could someone point the way how to download 100K text files or how to access them (if it's easier than downloading them)?</p>
","download, lda, gensim, common-crawl","<p>It seems it is possible to download only parts of the DataSet (you can just select the month you want), and you can download only the text (called WET files).
for example, you can download the August 2014 Crawl Data from: <a href=""http://blog.commoncrawl.org/2014/09/august-2014-crawl-data-available/"" rel=""nofollow"">http://blog.commoncrawl.org/2014/09/august-2014-crawl-data-available/</a> and an explanation about the file format can be found here: <a href=""http://blog.commoncrawl.org/2014/04/navigating-the-warc-file-format/"" rel=""nofollow"">http://blog.commoncrawl.org/2014/04/navigating-the-warc-file-format/</a></p>
",3,1,388,2014-12-17 20:09:12,https://stackoverflow.com/questions/27533977/how-to-download-subset-of-amazon-commoncrawel-only-the-text-wet-files-is-nee
How should I train gensim on Brown corpus,"<p>I am trying to use gensim word2vec. I am unable to train the model based on Brown Corpus. Here is my code.</p>

<pre><code>from gensim import models

model = models.Word2Vec([sentence for sentence in models.word2vec.BrownCorpus(""E:\\nltk_data\\"")],workers=4)
model.save(""E:\\data.bin"")
</code></pre>

<p>I downloaded nltk_data using <code>nltk.download()</code>.  I am getting the error below.</p>

<pre><code>C:\Python27\lib\site-packages\gensim-0.10.1-py2.7.egg\gensim\models\word2vec.py:401: UserWarning: Cython compilation failed, training will be slow. Do you have Cython installed? `pip install cython`
  warnings.warn(""Cython compilation failed, training will be slow. Do you have Cython installed? `pip install cython`"")
Traceback (most recent call last):
  File ""E:\eclipse_workspace\Python_files\Test\Test.py"", line 8, in &lt;module&gt;
    model = models.Word2Vec([sentence for sentence in models.word2vec.BrownCorpus(""E:\\nltk_data\\"")],workers=4)
  File ""C:\Python27\lib\site-packages\gensim-0.10.1-py2.7.egg\gensim\models\word2vec.py"", line 276, in __init__
    self.train(sentences)
  File ""C:\Python27\lib\site-packages\gensim-0.10.1-py2.7.egg\gensim\models\word2vec.py"", line 407, in train
    raise RuntimeError(""you must first build vocabulary before training the model"")
RuntimeError: you must first build vocabulary before training the model
</code></pre>

<p>What am I doing wrong?</p>
","python, gensim","<p>Maybe you create the sentences in the wrong way.<br>
Try this, it works for me.</p>

<pre><code>import gensim
import logging
from nltk.corpus import brown    

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
sentences = brown.sents()
model = gensim.models.Word2Vec(sentences, min_count=1)
model.save('/tmp/brown_model')
</code></pre>

<p>The logging part is not necessary, and you can change the params in <code>Word2Vec()</code> as you own need.</p>
",10,4,3897,2014-12-24 06:17:47,https://stackoverflow.com/questions/27632404/how-should-i-train-gensim-on-brown-corpus
"corpus2dense requires two arguments, but tutorial example only uses one","<p>I posted this issue on github (<a href=""https://github.com/piskvorky/gensim/issues/274"" rel=""nofollow"">https://github.com/piskvorky/gensim/issues/274</a>)</p>

<p>However, I need help with how to actually use the compatibility with numpy that gensim has.</p>

<p>I tried passing in None, <code>len(corpus)</code>, and 0-2 all failing.</p>

<p>The following is the corpus:</p>

<pre><code>[(0, 1.0), (1, 1.0), (2, 1.0)]
[(0, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0)]
[(2, 1.0), (5, 1.0), (7, 1.0), (8, 1.0)]
[(1, 1.0), (5, 2.0), (8, 1.0)]
[(3, 1.0), (6, 1.0), (7, 1.0)]
[(9, 1.0)]
[(9, 1.0), (10, 1.0)]
[(9, 1.0), (10, 1.0), (11, 1.0)]
[(4, 1.0), (10, 1.0), (11, 1.0)]
</code></pre>

<p>This is the code which doesn't work in my iPython notebook:</p>

<pre><code>from gensim import matutils
corpus = corpora.MmCorpus('/tmp/corpus.mm')
import numpy
numpy_matrix = matutils.corpus2dense(corpus)
</code></pre>

<p>Which throws IndexErrors</p>
","python, numpy, gensim","<p>As stated in my comment, it should be <code>2*len(corpus)</code> instead of <code>len(corpus)</code>.</p>
",3,0,673,2015-01-05 22:52:13,https://stackoverflow.com/questions/27789298/corpus2dense-requires-two-arguments-but-tutorial-example-only-uses-one
"Extract array (column name, data) from Pandas DataFrame","<p>This is my first question at Stack Overflow.</p>

<p>I have a DataFrame of Pandas like this.</p>

<pre><code>        a   b   c   d
one     0   1   2   3
two     4   5   6   7
three   8   9   0   1
four    2   1   1   5
five    1   1   8   9
</code></pre>

<p>I want to extract the pairs of column name and data whose data is 1 and each index is separate at array.</p>

<pre><code>[ [(b,1.0)], [(d,1.0)], [(b,1.0),(c,1.0)], [(a,1.0),(b,1.0)] ]
</code></pre>

<p>I want to use gensim of python library which requires corpus as this form.</p>

<p>Is there any smart way to do this or to apply gensim from pandas data?</p>
","python, pandas, gensim","<p>Many gensim functions accept numpy arrays, so there may be a better way...</p>

<pre><code>In [11]: is_one = np.where(df == 1)

In [12]: is_one
Out[12]: (array([0, 2, 3, 3, 4, 4]), array([1, 3, 1, 2, 0, 1]))

In [13]: df.index[is_one[0]], df.columns[is_one[1]]
Out[13]:
(Index([u'one', u'three', u'four', u'four', u'five', u'five'], dtype='object'),
 Index([u'b', u'd', u'b', u'c', u'a', u'b'], dtype='object'))
</code></pre>

<p>To groupby each row, you could use iterrows:</p>

<pre><code>from itertools import repeat

In [21]: [list(zip(df.columns[np.where(row == 1)], repeat(1.0)))
          for label, row in df.iterrows()
          if 1 in row.values]  # if you don't want empty [] for rows without 1
Out[21]:
[[('b', 1.0)],
 [('d', 1.0)],
 [('b', 1.0), ('c', 1.0)],
 [('a', 1.0), ('b', 1.0)]]
</code></pre>

<p><em>In python 2 the <code>list</code> is not required since zip returns a list.</em></p>
",1,2,1752,2015-01-15 05:04:05,https://stackoverflow.com/questions/27957112/extract-array-column-name-data-from-pandas-dataframe
Gensim not installing - Windows 8.1 - Python,"<p>I am trying to install <a href=""http://radimrehurek.com/gensim/index.html"" rel=""nofollow"">Gensim</a> but I am getting the errors displayed below. I am running Anaconda 2.1.0 with Python 2.7.8 and NumPy 1.9.0 on a Windows 8.1 machine. I already have the Windows SDK 8.1.</p>

<p>It says something about a deprecated NumPy version 1.7, which seems odd because I am running NumPy 1.9.0.</p>

<p>I also have a Anaconda3 installation with Python 3.4, though I removed those from my PATH in order to be able to run Python 2 in cmd, because I need to work on a project in Python 2. Running <code>python --version</code> returns ""Python 2.7.8"".</p>

<pre><code>In [9]: %run setup.py install
running install
running bdist_egg
running egg_info
writing requirements to gensim.egg-info\requires.txt
writing gensim.egg-info\PKG-INFO
writing top-level names to gensim.egg-info\top_level.txt
writing dependency_links to gensim.egg-info\dependency_links.txt
reading manifest file 'gensim.egg-info\SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no files found matching '*.sh' under directory '.'
no previously-included directories found matching 'docs\src*'
writing manifest file 'gensim.egg-info\SOURCES.txt'
installing library code to build\bdist.win-amd64\egg
running install_lib
running build_py
running build_ext
building 'gensim.models.word2vec_inner' extension
C:\Program Files (x86)\Haskell Platform\2013.2.0.0\mingw\bin\gcc.exe -DMS_WIN64
-mdll -O -Wall -IC:\Users\Robert-Jan\Downloads\gensim-0.10.3\gensim-0.10.3\gensi
m\models -IC:\Anaconda\include -IC:\Anaconda\PC -IC:\Anaconda\lib\site-packages\
numpy\core\include -c ./gensim/models/word2vec_inner.c -o build\temp.win-amd64-2
.7\Release\.\gensim\models\word2vec_inner.o
In file included from C:\Anaconda\lib\site-packages\numpy\core\include/numpy/nda
rraytypes.h:1804:0,
                 from C:\Anaconda\lib\site-packages\numpy\core\include/numpy/nda
rrayobject.h:17,
                 from C:\Anaconda\lib\site-packages\numpy\core\include/numpy/arr
ayobject.h:4,
                 from ./gensim/models/word2vec_inner.c:232:
C:\Anaconda\lib\site-packages\numpy\core\include/numpy/npy_1_7_deprecated_api.h:
12:9: note: #pragma message: C:\Anaconda\lib\site-packages\numpy\core\include/nu
mpy/npy_1_7_deprecated_api.h(12) : Warning Msg: Using deprecated NumPy API, disa
ble it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
./gensim/models/word2vec_inner.c: In function '__Pyx_RaiseArgtupleInvalid':
./gensim/models/word2vec_inner.c:9761:18: warning: unknown conversion type chara
cter 'z' in format
./gensim/models/word2vec_inner.c:9761:18: warning: format '%.1s' expects type 'c
har *', but argument 5 has type 'Py_ssize_t'
./gensim/models/word2vec_inner.c:9761:18: warning: unknown conversion type chara
cter 'z' in format
./gensim/models/word2vec_inner.c:9761:18: warning: too many arguments for format

./gensim/models/word2vec_inner.c: In function '__Pyx_RaiseTooManyValuesError':
./gensim/models/word2vec_inner.c:10235:18: warning: unknown conversion type char
acter 'z' in format
./gensim/models/word2vec_inner.c:10235:18: warning: too many arguments for forma
t
./gensim/models/word2vec_inner.c: In function '__Pyx_RaiseNeedMoreValuesError':
./gensim/models/word2vec_inner.c:10241:18: warning: unknown conversion type char
acter 'z' in format
./gensim/models/word2vec_inner.c:10241:18: warning: format '%.1s' expects type '
char *', but argument 3 has type 'Py_ssize_t'
./gensim/models/word2vec_inner.c:10241:18: warning: too many arguments for forma
t
./gensim/models/word2vec_inner.c: At top level:
C:\Anaconda\lib\site-packages\numpy\core\include/numpy/__multiarray_api.h:1629:1
: warning: '_import_array' defined but not used
C:\Anaconda\lib\site-packages\numpy\core\include/numpy/__ufunc_api.h:241:1: warn
ing: '_import_umath' defined but not used
./gensim/models/word2vec_inner.c: In function '__pyx_pf_5trunk_6gensim_6models_1
4word2vec_inner_train_sentence_sg':
./gensim/models/word2vec_inner.c:5271:59: warning: '__pyx_v_syn1' may be used un
initialized in this function
./gensim/models/word2vec_inner.c:5274:59: warning: '__pyx_v_syn1neg' may be used
 uninitialized in this function
./gensim/models/word2vec_inner.c:5275:28: warning: '__pyx_v_table' may be used u
ninitialized in this function
./gensim/models/word2vec_inner.c:5276:25: warning: '__pyx_v_table_len' may be us
ed uninitialized in this function
./gensim/models/word2vec_inner.c:5277:25: warning: '__pyx_v_next_random' may be
used uninitialized in this function
./gensim/models/word2vec_inner.c: In function '__pyx_pf_5trunk_6gensim_6models_1
4word2vec_inner_2train_sentence_cbow':
./gensim/models/word2vec_inner.c:6080:59: warning: '__pyx_v_syn1' may be used un
initialized in this function
./gensim/models/word2vec_inner.c:6083:59: warning: '__pyx_v_syn1neg' may be used
 uninitialized in this function
./gensim/models/word2vec_inner.c:6084:28: warning: '__pyx_v_table' may be used u
ninitialized in this function
./gensim/models/word2vec_inner.c:6085:25: warning: '__pyx_v_table_len' may be us
ed uninitialized in this function
./gensim/models/word2vec_inner.c:6086:25: warning: '__pyx_v_next_random' may be
used uninitialized in this function
writing build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.def
C:\Program Files (x86)\Haskell Platform\2013.2.0.0\mingw\bin\dllwrap.exe -DMS_WI
N64 -mdll -static --entry _DllMain@12 --output-lib build\temp.win-amd64-2.7\Rele
ase\.\gensim\models\libword2vec_inner.a --def build\temp.win-amd64-2.7\Release\.
\gensim\models\word2vec_inner.def -s build\temp.win-amd64-2.7\Release\.\gensim\m
odels\word2vec_inner.o -LC:\Anaconda\libs -LC:\Anaconda\PCbuild\amd64 -lpython27
 -lmsvcr90 -o build\lib.win-amd64-2.7\gensim\models\word2vec_inner.pyd
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x23fb): undefined reference to `_imp__PyExc_TypeError'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2406): undefined reference to `_imp__PyErr_Format'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2447): undefined reference to `_imp__PyDict_Next'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x246f): undefined reference to `_imp__PyString_Type'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x24aa): undefined reference to `_imp___PyString_Eq'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x251f): undefined reference to `_imp___PyString_Eq'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2555): undefined reference to `_imp__PyUnicodeUCS2_Compare'
build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o:word2vec_inner
.c:(.text+0x2561): undefined reference to `_imp__PyErr_Occurred'
[...]
</code></pre>

<p>...and it goes on throwing up undefined references.</p>
","python, numpy, windows-8.1, anaconda, gensim","<p>Turns out the problem was that the C libraries that Gensim needs are being compiled using a program called gcc.dll. Anaconda has this program built in, but so does the Haskell platform. As I've been using Haskell a while ago the Python installer was using the Haskell version of gcc.dll, instead of Anaconda's gcc.bat.</p>

<p>Removing my Haskell installation fixed the problem.</p>
",0,0,912,2015-01-19 22:35:40,https://stackoverflow.com/questions/28034688/gensim-not-installing-windows-8-1-python
Gensim Word2vec storing attribute syn0norm,"<p>I am trying to use <em>word2vec</em> for a project and after training I get:</p>

<pre><code>INFO : not storing attribute syn0norm
</code></pre>

<p>Is there any way I could save the <code>syn0norm</code>.</p>

<p>How can I do so?</p>
","python, gensim, word2vec","<p>This is fine -- you shouldn't need to store the array syn0norm.</p>

<p>It's computed in the init_sims procedure, and only on an as needed basis.  After training, it's actually not defined so there's nothing to train.</p>

<p>When you query the model (such as most_similar), it will call init_sims which checks to see if syn0norm is defined.  If not it assigns it with the following line:</p>

<pre><code>self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)
</code></pre>

<p>EDIT:</p>

<p>After looking through code (for other things) I see that you can specify if you want to save syn0norm -- there's an ignore setting which defaults to ['syn0norm'], so the following will save all:</p>

<pre><code>In [239]: model.save('test',ignore=[])
2015-03-17 09:07:54,733 : INFO : saving Word2Vec object under test, separately None
2015-03-17 09:07:54,734 : INFO : storing numpy array 'syn0' to test.syn0.npy
2015-03-17 09:08:15,908 : INFO : storing numpy array 'table' to test.table.npy
2015-03-17 09:08:17,908 : INFO : storing numpy array 'syn1neg' to test.syn1neg.npy
2015-03-17 09:08:35,037 : INFO : storing numpy array 'syn1' to test.syn1.npy
2015-03-17 09:09:03,766 : INFO : storing numpy array 'syn0norm' to test.syn0norm.npy
</code></pre>

<p>The problem is, it usually will take less time to calculate than save and reload.</p>
",3,3,4251,2015-02-13 20:53:48,https://stackoverflow.com/questions/28508548/gensim-word2vec-storing-attribute-syn0norm
"Tweet analysis, Python error when making dictionary for LDA","<p>I've downloaded Tweets about Amsterdam, in UTF-8 using the Twitter API for python.
Now i'm trying to make a dictionary for LDA, using this code (just a part of the code, but this is the part that causes the error):</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file))
</code></pre>

<p>which always gives me an error, depending on which txt file I choose as input, either: </p>

<pre><code>UnicodeDecodeError: 'utf8' codec can't decode byte 0xf1 in position 2: invalid continuation byte
</code></pre>

<p>or</p>

<pre><code> UnicodeDecodeError: 'utf8' codec can't decode byte xxxx in position 175-176: unexpected end of data
</code></pre>

<p>I expect the reason for this to be characters which are unknown in UTF-8 (some smilies used in Tweets maybe) and after Googling tried to replace the code by:</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, errors='ignore'))
</code></pre>

<p>with error message:</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, errors='ignore'))
TypeError: 'errors' is an invalid keyword argument for this function
</code></pre>

<p>or </p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, 'ignore'))
</code></pre>

<p>with error message:</p>

<pre><code>dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, 'ignore'))
ValueError: mode string must begin with one of 'r', 'w', 'a' or 'U', not 'ignore'
</code></pre>

<p>Does anyone have a solution? Thanks</p>
","python, dictionary, lda, gensim","<p>If you know the data you're reading is utf-8, you can import the codecs module and use <code>codecs.open()</code> instead of <code>open()</code></p>

<p><a href=""https://docs.python.org/2/howto/unicode.html#reading-and-writing-unicode-data"" rel=""nofollow"">Reading and Writing Unicode Data</a></p>

<p>Give this a try:</p>

<pre><code>import codecs
dictionary = corpora.Dictionary(line.lower().split() for line in codecs.open(input_file, mode='r', encoding='utf-8', errors='ignore'))
</code></pre>
",0,0,425,2015-03-31 13:01:20,https://stackoverflow.com/questions/29369317/tweet-analysis-python-error-when-making-dictionary-for-lda
Gensim word2vec augment or merge pre-trained vectors,"<p>I am loading pre-trained vectors from a binary file generated from the word2vec C code with something like:</p>

<pre><code>model_1 = Word2Vec.load_word2vec_format('vectors.bin', binary=True)
</code></pre>

<p>I am using those vectors to generate vector representations of sentences that contain words that may not have already existing vectors in <code>vectors.bin</code>. For example, if <code>vectors.bin</code> has no associated vector for the word ""yogurt"", and I try</p>

<pre><code>yogurt_vector = model_1['yogurt']
</code></pre>

<p>I get <code>KeyError: 'yogurt'</code>, which makes good sense. What I want is to be able to take the sentence words that do not have corresponding vectors and add representations for them to <code>model_1</code>. I am aware from <a href=""https://stackoverflow.com/questions/22121028/update-gensim-word2vec-model"">this post</a> that you cannot continue to train the C vectors. Is there then a way to train a new model, say <code>model_2</code>, for the words with no vectors and merge <code>model_2</code> with <code>model_1</code>?</p>

<p>Alternatively, is there a way to test if the model contains a word before I actually try to retrieve it, so that I can at least avoid the KeyError?</p>
","python, gensim, keyerror, word2vec","<p>Avoiding the key error is easy:</p>

<pre><code>[x for x in 'this model hus everything'.split() if x in model_1.vocab]
</code></pre>

<p>The more difficult problem is merging a new word to an existing model.  The problem is that word2vec calculates the likelihood of 2 words being next to each other, and if the word 'yogurt' wasn't in the first body that the model was trained on it's not next to any of those words, so the second model would not correlate to the first.</p>

<p>You can look at the internals when a model is saved (uses numpy.save)  and I would be interested in working with you to come up with code to allow adding vocabulary.</p>
",5,4,3736,2015-04-12 16:14:54,https://stackoverflow.com/questions/29591581/gensim-word2vec-augment-or-merge-pre-trained-vectors
Missing sentences from the Doc2vec representation,"<p>I am using the Doc2vec class from the gensim framework to compute the vectorial representation of each document in a corpus.</p>

<p>The corpus contains very short sentences, they can have even one word. I observed that for many sentences, especially the short ones, Doc2vec does not provide any representations. Could someone explain the reasons for this?</p>
","gensim, word2vec","<p>I had this same problem. I solved it by setting the parameter min_count=1.</p>

<pre><code>model = doc2vec.Doc2Vec(size=100)
</code></pre>

<p>became</p>

<pre><code>model = doc2vec.Doc2Vec(size=100, min_count=1)
</code></pre>

<p>Made my problem go away!</p>

<p>I found my answer in the comments of the doc2vec tutorial <a href=""http://radimrehurek.com/2014/12/doc2vec-tutorial/"" rel=""nofollow"">http://radimrehurek.com/2014/12/doc2vec-tutorial/</a></p>
",2,1,323,2015-04-16 13:35:30,https://stackoverflow.com/questions/29676413/missing-sentences-from-the-doc2vec-representation
What is the difference between models.ldamodel.LdaModel and models.LdaModel?,"<p>What is the difference between 
<code>gensim.models.ldamodel.LdaModel(...)</code> and <code>gensim.models.LdaModel(...)</code>?</p>

<p>The <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow"">docs</a> use <code>gensim.models.ldamodel.LdaModel(...)</code>.</p>

<p>However, <a href=""https://www.google.com/search?q=%22models.LdaModel%22&amp;oq=%22models.LdaModel%22&amp;aqs=chrome..69i57j0.9506j0j7&amp;sourceid=chrome&amp;es_sm=119&amp;ie=UTF-8#q=%22models.LdaModel(corpus%22&amp;tbas=0"" rel=""nofollow"">many people</a> seem to use <code>gensim.models.LdaModel(...)</code>.</p>
","python, lda, gensim","<p>They're the same. If you look at the source, the <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/__init__.py"" rel=""nofollow""><code>__init__.py</code></a> in the <code>models</code> package has the line:</p>

<pre><code># bring model classes directly into package namespace, to save some typing
from .ldamodel import LdaModel
</code></pre>

<p>Which basically just makes it so that <code>gensim.models.ldamodel.LdaModel</code> is the same thing as <code>gensim.models.LdaModel</code> -- just to save some typing, as helpfully pointed out by the comment.</p>
",4,1,331,2015-04-24 02:13:02,https://stackoverflow.com/questions/29837563/what-is-the-difference-between-models-ldamodel-ldamodel-and-models-ldamodel
Word2Vec and Gensim parameters equivalence,"<p>Gensim is an optimized python port of Word2Vec (see <a href=""http://radimrehurek.com/2013/09/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow"">http://radimrehurek.com/2013/09/deep-learning-with-word2vec-and-gensim/</a>)</p>

<p>I am currently using these vectors: <a href=""http://clic.cimec.unitn.it/composes/semantic-vectors.html"" rel=""nofollow"">http://clic.cimec.unitn.it/composes/semantic-vectors.html</a></p>

<p>I am going to rerun the model training with gensim because there was some noisy tokens in their models. So i would like to find out what are some equivalent parameters for <code>word2vec</code> in <code>gensim</code></p>

<p>And the parameters they used from <code>word2vec</code> are:</p>

<ul>
<li>2-word context window, PMI weighting, no compression, 300K dimensions</li>
</ul>

<p>What is the gensim equivalence when i train a Word2Vec model?</p>

<p>Is it:</p>

<pre><code>&gt;&gt;&gt; model = Word2Vec(sentences, size=300000, window=2, min_count=5, workers=4)
</code></pre>

<p><strong>Is there a PMI weight option in gensim?</strong></p>

<p><strong>What is the default min_count used in word2vec?</strong></p>

<p>There's another set of parameters from word2vec as such:</p>

<ul>
<li>5-word context window, 10 negative samples, subsampling, 400 dimensions.</li>
</ul>

<p><strong>Is there a negative samples parameter in gensim?</strong></p>

<p><strong>What is the parameter equivalence of subsampling in gensim?</strong></p>
","python, nlp, neural-network, gensim, word2vec","<ol>
<li><p>The paper you link to compares word embeddings from a number of schemes, including Continuous Bag of Words (CBOW). CBOW is one of the models implemented in Gensim's ""word2vec"" model. The paper also discusses word embeddings obtained from Singular Value Decomposition with various weighting schemes, some involving PMI. There is no equivalence between SVD and word2vec, but if you want to do an SVD in gensim, it's called ""LSA"" or ""Latent Semantic Analysis"" when done in natural language processing.</p></li>
<li><p>The <code>min_count</code> parameter is set to 5 by default, as can be seen <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow"">here</a>. </p></li>
<li><p>Negative Sampling and Hierarchical Softmax are two approximate inference methods for estimating a probability distribution over a discrete space (used when a normal softmax is too computationally expensive). Gensim's <code>word2vec</code> implements both. It uses hierarchical softmax by default, but you can use negative sampling by setting the hyperparameter <code>negative</code> to be greater than zero. This is documented in comments in gensim's code <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow"">here</a> as well. </p></li>
</ol>
",3,3,4841,2015-04-29 09:44:22,https://stackoverflow.com/questions/29939984/word2vec-and-gensim-parameters-equivalence
Continue training a Doc2Vec model,"<p>Gensim's <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""noreferrer"">official tutorial</a> explicitly states that it is possible to continue training a (loaded) model. I'm aware that according to the documentation it is not possible to continue training a model that was loaded from the <code>word2vec</code> format. But even when one generates a model from scratch and then tries to call the <code>train</code> method, it is not possible to access the newly created labels for the <code>LabeledSentence</code> instances supplied to <code>train</code>.</p>

<pre><code>&gt;&gt;&gt; sentences = [LabeledSentence(['first', 'sentence'], ['SENT_0']), LabeledSentence(['second', 'sentence'], ['SENT_1'])]
&gt;&gt;&gt; model = Doc2Vec(sentences, min_count=1)
&gt;&gt;&gt; print(model.vocab.keys())
dict_keys(['SENT_0', 'SENT_1', 'sentence', 'first', 'second'])
&gt;&gt;&gt; sentence = LabeledSentence(['third', 'sentence'], ['SENT_2'])
&gt;&gt;&gt; model.train([sentence])
&gt;&gt;&gt; print(model.vocab.keys())

# At this point I would expect the key 'SENT_2' to be present in the vocabulary, but it isn't
dict_keys(['SENT_0', 'SENT_1', 'sentence', 'first', 'second'])
</code></pre>

<p>Is it at all possible to continue the training of a Doc2Vec model in Gensim with new sentences? If so, how can this be achieved?</p>
","neural-network, gensim","<p>My understand is that this is not possible for any new labels. We can only continue training when the new data has the same labels as the old data. As a result, we are training or retuning the weights of the already learned vocabulary, but are not able to learn a new vocabulary.</p>
<p>There is a similar question for adding new labels/words/sentences during training: <a href=""https://groups.google.com/forum/#!searchin/word2vec-toolkit/online$20word2vec/word2vec-toolkit/L9zoczopPUQ/_Zmy57TzxUQJ"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!searchin/word2vec-toolkit/online$20word2vec/word2vec-toolkit/L9zoczopPUQ/_Zmy57TzxUQJ</a></p>
<p>Also, you might want to keep an eye on this discussion:
<a href=""https://groups.google.com/forum/#!topic/gensim/UZDkfKwe9VI"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/gensim/UZDkfKwe9VI</a></p>
<p>Update: If you want to add new words to an already trained model, take a look at online word2vec here:
<a href=""https://rutumulkar.com/ml-notes/word2vec/representation%20learning/2015/08/22/word2vec.html"" rel=""nofollow noreferrer"">https://rutumulkar.com/ml-notes/word2vec/representation%20learning/2015/08/22/word2vec.html</a></p>
",5,5,4071,2015-05-10 19:04:33,https://stackoverflow.com/questions/30155506/continue-training-a-doc2vec-model
How to check if a key exists in a word2vec trained model or not,"<p>I have trained a word2vec model using a corpus of documents with Gensim. Once the model is training, I am writing the following piece of code to get the raw feature vector of a word say ""view"".</p>

<pre><code>myModel[""view""]
</code></pre>

<p>However, I get a KeyError for the word which is probably because this doesn't exist as a key in the list of keys indexed by word2vec. How can I check if a key exits in the index before trying to get the raw feature vector?</p>
","python, gensim, word2vec","<p>convert the model into vectors with </p>

<pre><code>word_vectors = model.wv
</code></pre>

<p>then we can use </p>

<pre><code>if 'word' in word_vectors.vocab
</code></pre>
",36,45,57000,2015-05-18 11:24:45,https://stackoverflow.com/questions/30301922/how-to-check-if-a-key-exists-in-a-word2vec-trained-model-or-not
Doc2vec MemoryError,"<p>I am using the doc2vec model from teh gensim framework to represent a corpus of 15 500 000  short documents (up to 300 words): </p>

<pre><code>gensim.models.Doc2Vec(sentences, size=400, window=10, min_count=1, workers=8 )
</code></pre>

<p>After creating the vectors there are  more than  18 000 000 vectors representing words and documents. </p>

<p>I want to find the most similar items (words or documents) for a given item: </p>

<pre><code> similarities = model.most_similar(‘uid_10693076’)
</code></pre>

<p>but I get a MemoryError when the similarities are computed:</p>

<pre><code>Traceback (most recent call last):

   File ""article/test_vectors.py"", line 31, in &lt;module&gt; 
    similarities = model.most_similar(item) 
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 639, in most_similar 
    self.init_sims() 
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py"", line 827, in init_sims 
    self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL) 
</code></pre>

<p>I have a Ubuntu machine  with 60GB Ram and 70GB swap . I checked the memory allocation (in htop) and I observed that never the memory was completely used. I also set to unlimited the the maximum address space which may be locked in memory in python:</p>

<pre><code>resource.getrlimit(resource.RLIMIT_MEMLOCK)
</code></pre>

<p>Could someone explain the reason for this MemoryError? In my opinion the available memory should be enough for doing this computations. Could be some memory limits in python or OS?</p>

<p>Thanks in advance!</p>
","python, memory, gensim, word2vec","<p>18M vectors * 400 dimensions * 4 bytes/float = 28.8GB for the model's syn0 array (trained vectors)</p>

<p>The syn1 array (hidden weights) will also be 28.8GB – even though syn1 doesn't really need entries for doc-vectors, which are never target-predictions during training.</p>

<p>The vocabulary structures (vocab dict and index2word table) will likely add another GB or more. So that's all your 60GB RAM. </p>

<p>The syn0norm array, used for similarity calculations, will need another 28.8GB, for a total usage of around 90GB. It's the syn0norm creation where you're getting the error. But even if syn0norm creation succeeded, being that deep into virtual memory would likely ruin performance. </p>

<p>Some steps that might help:</p>

<ul>
<li><p>Use a min_count of at least 2: words appearing once are unlikely to contribute much, but likely use a lot of memory. (But since words are a tiny portion of your syn0, this will only save a little.)</p></li>
<li><p>After training but before triggering init_sims(), discard the the syn1 array. You won't be able to train more, but your existing word/doc vectors remain accessible. </p></li>
<li><p>After training but before calling most_similar(), call init_sims() yourself with a replace=True parameter, to discard the non-normalized syn0 and replace it with the syn0norm. Again you won't be able to train more, but you'll save the syn0 memory. </p></li>
</ul>

<p>In-progress work separating out the doc and word vectors, which will appear in gensim past verstion 0.11.1, should also eventually offer some relief. (It'll shrink the syn1 to only include word entries, and allow doc-vectors to come from a file-backed (memmap'd) array.)</p>
",15,6,4215,2015-05-27 16:53:24,https://stackoverflow.com/questions/30488695/doc2vec-memoryerror
Latent Dirichlet Allocation using Gensim on more than one corpus,"<p>I have two questions related to the usage of <strong>gensim</strong> for LDA. </p>

<p>1) How can I create a model using one corpus, save it and perhaps extend it later on another corpus by training the model on it ? Is it possible ?</p>

<p>2) Can LDA be used to classify an unseen document, or the model needs to be created again by including it in the corpus ? Is there an online way to do it and see the changes on the fly ?</p>

<p>I have a fairly basic understanding of LDA and have used it for Topic modeling on simple corpus using <strong>lda</strong> and <strong>gensim</strong> libraries.  Please point out any conceptual inconsistencies in the question. Thanks !</p>
","python, lda, topic-modeling, gensim","<p>I found this to be helpful. Gensim does allow for an extra corpus to be added(updated) to an existing LDA model. This module allows both LDA model estimation from a training corpus and inference of topic distribution on new, unseen documents. This is described here - </p>

<p><a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow"">https://radimrehurek.com/gensim/models/ldamodel.html</a></p>

<p>Additionally, the algorithm is streamed and can process corpora larger than the RAM. It also has a multicore implementation to speed up the process.</p>

<pre><code>lda = LdaModel(corpus, num_topics=10)

lda.update(other_corpus)
</code></pre>

<p>This is how  the model can be updated.</p>
",2,0,1257,2015-05-31 22:25:55,https://stackoverflow.com/questions/30563361/latent-dirichlet-allocation-using-gensim-on-more-than-one-corpus
How to save gensim LDA topics output to csv along with the scores?,"<p>How to save the output? I am using the following code:    </p>

<pre><code>%time lda1 = models.LdaModel(corpus1, num_topics=20, id2word=dictionary1, update_every=5, chunksize=10000, passes=100)
</code></pre>
","python-2.7, gensim","<p>To export topic mixtures of each document to a csv file:</p>

<pre><code>import pandas as pd

mixture = [dict(lda_model[x]) for x in corpus1]
pd.DataFrame(mixture).to_csv(""topic_mixture.csv"")
</code></pre>

<p>To export the top words for each topic to a csv file:</p>

<pre><code>top_words_per_topic = []
for t in range(lda_model.num_topics):
    top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 5)])

pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(""top_words.csv"")
</code></pre>

<p>The CSV file will have the following format</p>

<pre><code>Topic Word  P  
0     w1    0.004437  
0     w2    0.003553  
0     w3    0.002953  
0     w4    0.002866  
0     w5    0.008813  
1     w6    0.003393  
1     w7    0.003289  
1     w8    0.003197 
... 
</code></pre>
",6,3,4707,2015-06-08 20:27:31,https://stackoverflow.com/questions/30718471/how-to-save-gensim-lda-topics-output-to-csv-along-with-the-scores
Gensim LDA - Default number of iterations,"<p>I wish to know the default number of iterations in <strong>gensim</strong>'s LDA (Latent Dirichlet Allocation) algorithm. I don't think the documentation talks about this. (Number of iterations is denoted by the parameter <strong>iterations</strong> while initializing the <strong>LdaModel</strong> ). Thanks !</p>
","python, topic-modeling, gensim","<p>Checked the module's files in the <em>python/Lib/site-packages</em> directory. The constructor is something like this - </p>

<pre><code>def __init__(self, corpus=None, num_topics=100, id2word=None,
                 distributed=False, chunksize=2000, passes=1, update_every=1,
                 alpha='symmetric', eta=None, decay=0.5, offset=1.0,
                 eval_every=10, iterations=50, gamma_threshold=0.001)
</code></pre>

<p>So, the default number of iterations stands at 50.</p>
",8,5,7777,2015-06-10 00:37:51,https://stackoverflow.com/questions/30745184/gensim-lda-default-number-of-iterations
Gensim with MinGW,"<p>I seem to be one of the many people struggling to install gensim on windows. I have trawled through countless forums but the errors poster there never appear to match my errors. So hopefully someone can point me in the right direction! </p>

<p>I am running Windows Server 2012 R2 Standard 64-bit. I have installed MinGW &amp; Anaconda 2.2.0 (64-bit), which comes with Python 2.7.9. </p>

<p>I have added a file distutils.cfg into C:\Users\Sam\Anaconda\Lib\distutils  with the contents:</p>

<pre><code>[build]
compiler=mingw32
</code></pre>

<p>I have added C:\MinGW\bin to my Environment variables. </p>

<p>If I install gensim using pip I do not get any errors, until I try to run Word2Vec when I get the error:</p>

<pre><code>C:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\lib\site-packages\gensim\models\word2vec.py:459: UserWarning: C extension com
pilation failed, training will be slow. Install a C compiler and reinstall gensim for fast training.
</code></pre>

<p>So I have uninstalled gensim and tried to re-install using the mingw32 compiler, but this gives me this error:</p>

<pre><code>python setup.py build --compiler=mingw32
c:\users\sam.passmore\appdata\local\continuum\anaconda\lib\site-packages\setuptools-14.3-py2.7.egg\setuptools\dist.py:282: UserWarni
ng: Normalizing '0.11.1-1' to '0.11.1.post1'
running build
running build_ext
building 'gensim.models.word2vec_inner' extension
C:\MinGW\bin\gcc.exe -DMS_WIN64 -mdll -O -Wall -Igensim\models -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\include -IC:
\Users\sam.passmore\AppData\Local\Continuum\Anaconda\PC -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\lib\site-packages\n
umpy\core\include -c ./gensim/models/word2vec_inner.c -o build\temp.win-amd64-2.7\Release\.\gensim\models\word2vec_inner.o
gcc: error: ./gensim/models/word2vec_inner.c: No such file or directory
gcc: fatal error: no input files
compilation terminated.
command 'C:\\MinGW\\bin\\gcc.exe' failed with exit status 1
setup.py:82: UserWarning:
********************************************************************
WARNING: %s could not
be compiled. No C extensions are essential for gensim to run,
although they do result in significant speed improvements for some modules.
%s

Here are some hints for popular operating systems:

If you are seeing this message on Linux you probably need to
install GCC and/or the Python development package for your
version of Python.

Debian and Ubuntu users should issue the following command:

    $ sudo apt-get install build-essential python-dev

RedHat, CentOS, and Fedora users should issue the following command:

    $ sudo yum install gcc python-devel

If you are seeing this message on OSX please read the documentation
here:

http://api.mongodb.org/python/current/installation.html#osx
********************************************************************
The gensim.models.word2vec_inner extension moduleThe output above this warning shows how the compilation failed.
  ""The output above this warning shows how the compilation failed."")
building 'gensim.models.doc2vec_inner' extension
C:\MinGW\bin\gcc.exe -DMS_WIN64 -mdll -O -Wall -Igensim\models -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\include -IC:
\Users\sam.passmore\AppData\Local\Continuum\Anaconda\PC -IC:\Users\sam.passmore\AppData\Local\Continuum\Anaconda\lib\site-packages\n
umpy\core\include -c ./gensim/models/doc2vec_inner.c -o build\temp.win-amd64-2.7\Release\.\gensim\models\doc2vec_inner.o
gcc: error: ./gensim/models/doc2vec_inner.c: No such file or directory
gcc: fatal error: no input files
compilation terminated.
command 'C:\\MinGW\\bin\\gcc.exe' failed with exit status 1
setup.py:82: UserWarning:
********************************************************************
WARNING: %s could not
be compiled. No C extensions are essential for gensim to run,
although they do result in significant speed improvements for some modules.
%s

Here are some hints for popular operating systems:

If you are seeing this message on Linux you probably need to
install GCC and/or the Python development package for your
version of Python.

Debian and Ubuntu users should issue the following command:

    $ sudo apt-get install build-essential python-dev

RedHat, CentOS, and Fedora users should issue the following command:

    $ sudo yum install gcc python-devel

If you are seeing this message on OSX please read the documentation
here:

http://api.mongodb.org/python/current/installation.html#osx
********************************************************************
The gensim.models.doc2vec_inner extension moduleThe output above this warning shows how the compilation failed.
  ""The output above this warning shows how the compilation failed.""
</code></pre>

<p>I have exhausted all options I can think of or find, so if anyone could give some advice it would be much appreciated. </p>
","python, windows, python-2.7, gensim","<p>I managed to solve this after using conda install for gensim, rather than pip. </p>

<pre><code>conda install gensim
</code></pre>

<p>I am not sure what other steps I have included above have contributed to the answer, but this was the last thing I did before I no longer was getting the 'Install a C compiler and reinstall gensim for fast training.' message. </p>

<p>During my research in trying to solve this problem I saw that the most common methods were adding the lines </p>

<pre><code>[build]
compiler=mingw32
</code></pre>

<p>to the distutils.cfg file as well as ensuring MinGW is in your path. Also ensuring that the MinGW bit version is the same as your python version. </p>
",2,0,2078,2015-06-22 03:55:48,https://stackoverflow.com/questions/30971935/gensim-with-mingw
AttributeError: &#39;numpy.ndarray&#39; object has no attribute &#39;A&#39;,"<p>I am trying to perform tfidf on a matrix. I would like to use gensim, but <code>models.TfidfModel()</code> only works on a corpus and therefore returns a list of lists of varying lengths (I want a matrix).</p>

<p>The options are to somehow fill in the missing values of the list of lists, or just convert the corpus to a matrix </p>

<pre><code>numpy_matrix = gensim.matutils.corpus2dense(corpus, num_terms=number_of_corpus_features)
</code></pre>

<p>Choosing the latter, I then try to convert this count matrix to a tf-idf weighted matrix:</p>

<pre><code>def TFIDF(m):
    #import numpy
    WordsPerDoc = numpy.sum(m, axis=0)
    DocsPerWord = numpy.sum(numpy.asarray(m &gt; 0, 'i'), axis=1)
    rows, cols = m.shape
    for i in range(rows):
        for j in range(cols):
            amatrix[i,j] = (amatrix[i,j] / WordsPerDoc[j]) * log(float(cols) /     DocsPerWord[i])
</code></pre>

<p>But, I get the error <code>AttributeError: 'numpy.ndarray' object has no attribute 'A'</code></p>

<p>I copied the function above from another script. It was:</p>

<pre><code>def TFIDF(self):
    WordsPerDoc = sum(self.A, axis=0)        
    DocsPerWord = sum(asarray(self.A &gt; 0, 'i'), axis=1)
    rows, cols = self.A.shape
    for i in range(rows):
       for j in range(cols):
          self.A[i,j] = (self.A[i,j] / WordsPerDoc[j]) * log(float(cols) / DocsPerWord[i])
</code></pre>

<p>Which I believe is where it's getting the <code>A</code> from. However, I re-imported the function. </p>

<p>Why is this happening?</p>
","python, numpy, matrix, gensim","<p><code>self.A</code> is either an <code>np.matrix</code> or <code>sparse</code> matrix.  For both <code>A</code> means, return a copy that is a <code>np.ndarray</code>.  In other words, it converts the 2d matrix to a regular numpy array.  If <code>self</code> is already an array, it would produce your error.</p>

<p>It looks like you have corrected that with your own version of <code>TFIDF</code> - except that uses 2 variables, <code>m</code> and <code>amatrix</code> instead of <code>self.A</code>.</p>

<p>I think you need to look more at the error message and stack, to identify where that <code>.A</code> is.  Also make sure you understand where the code expects a matrix, especially a sparse one.  And whether your own code differs in that regard.</p>

<p>I recall from other SO questions that one of the learning packages had switched to using sparse matrices, and that required adding <code>.todense()</code> to some of their code (which expected dense ones).</p>
",0,0,19970,2015-06-22 06:41:44,https://stackoverflow.com/questions/30973503/attributeerror-numpy-ndarray-object-has-no-attribute-a
Access key value pairs in gensim dictionary,"<p>I am working with the gensim dictionary. For example, you can print <code>print(dictionary.token2id)</code>, as shown here <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">https://radimrehurek.com/gensim/tut1.html</a>. I can also <code>print dictionary</code>:</p>

<p><code>Dictionary(7 unique tokens: [u'nra', u'canon', u'deuterium', u'ion', u'facebook']...)</code></p>

<p>How do I access the key value pairs of the dictionary object, however? </p>
","python, gensim","<p>Token2id is a standard python dict. You can iterate like a standard dict:</p>

<p>Python 2:</p>

<pre><code>for k, v in dictionary.token2id.iteritems():
    print k, v
</code></pre>

<p>For Python 3 use <code>items()</code>:</p>

<pre><code>for k, v in dictionary.token2id.items():
    print(k, v)
</code></pre>
",7,2,3884,2015-06-23 18:48:58,https://stackoverflow.com/questions/31011061/access-key-value-pairs-in-gensim-dictionary
Unable to find words when using freebase with word2vec,"<p>I am trying to use freebase along with gensim's word2vec to find similarity score between vectors of two word using following code.</p>

<pre><code>model = gensim.models.Word2Vec()
model = models.Word2Vec.load_word2vec_format('freebase-vectors-skipgram1000-en.bin.gz', binary=True)
</code></pre>

<p>after creating a model based on freebase my code is giving me key error for any word.</p>

<pre><code>model.similarity('microsoft', 'apple')
</code></pre>

<p>This is giving me <code>KeyError: 'microsoft'</code></p>

<p>But when I use googlenews instead of freebase it works fine. Any idea why?</p>
","python-2.7, freebase, gensim, google-news","<p><code>model.similarity('/en/microsoft', '/en/apple')</code></p>
",2,1,629,2015-07-01 15:56:07,https://stackoverflow.com/questions/31166218/unable-to-find-words-when-using-freebase-with-word2vec
Python34 word2vec.Word2Vec OverFlowError,"<p>I'm studying word2vec, but when I use word2vec to train text data, occur OverFlowError with Numpy.</p>

<p>the message is,</p>

<pre><code>model.vocab[w].sample_int &gt; model.random.randint(2**32)]
Warning (from warnings module):
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 636
    warnings.warn(""C extension not loaded for Word2Vec, training will be slow. ""
UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""C:\Python34\lib\threading.py"", line 920, in _bootstrap_inner
    self.run()
  File ""C:\Python34\lib\threading.py"", line 868, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 675, in worker_loop
    if not worker_one_job(job, init):
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 666, in worker_one_job
    job_words = self._do_train_job(items, alpha, inits)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 623, in _do_train_job
    tally += train_sentence_sg(self, sentence, alpha, work)
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 112, in train_sentence_sg
    word_vocabs = [model.vocab[w] for w in sentence if w in model.vocab and
  File ""C:\Python34\lib\site-packages\gensim\models\word2vec.py"", line 113, in &lt;listcomp&gt;
    model.vocab[w].sample_int &gt; model.random.randint(2**32)]
  File ""mtrand.pyx"", line 935, in mtrand.RandomState.randint (numpy\random\mtrand\mtrand.c:9520)
OverflowError: Python int too large to convert to C long
</code></pre>

<p>Can you tell me the cases?</p>

<p>My machine is x64 and OS is windows 7, but python34 is 32bit. numpy and scipy are also 32bit.</p>
","python-3.x, windows-7-x64, integer-overflow, gensim, word2vec","<p>I get this as well. It looks like gensim has a potential workaround in the dev branch.</p>

<p><a href=""https://github.com/piskvorky/gensim/commit/726102df66000f2afcea82d95634b055e6521dc8"" rel=""nofollow"">https://github.com/piskvorky/gensim/commit/726102df66000f2afcea82d95634b055e6521dc8</a></p>

<p>This doesn't solve the core issue of navigating between different hardware and install int sizes, but I think it should alleviate issues with this particular line.</p>

<p>The necessary change involves switching out</p>

<p><code>model.vocab[w].sample_int &gt; model.random.randint(2**32)</code></p>

<p>for</p>

<p><code>model.vocab[w].sample_int &gt; model.random.rand() * 2**32</code></p>

<p>This avoids the 64 bit / 32 bit int issue created in randint.</p>

<p>UPDATE: I manually incorporated that change into my gensim install and it prevents the error.</p>
",1,2,1537,2015-07-08 07:48:57,https://stackoverflow.com/questions/31286574/python34-word2vec-word2vec-overflowerror
Gensim - Timeslice Data Format?,"<p>I'd like to use gensim's Python wrapper for Dynamic Topic Models. Essentially, it is a topic modeling approach that slices the corpus by date (i.e. years) and looks at how topics evolve over time. However, I am finding nothing online that specifies how <a href=""https://radimrehurek.com/gensim/models/dtmmodel.html#gensim.models.dtmmodel.DtmModel.ftimeslices"" rel=""nofollow"">my_timeslices</a> should be formatted. Does anyone have an example of a file and/or preparation?</p>
","python, python-2.7, gensim","<p>So I just noticed this. The time_slices is basically a list of integers. Every integer outlines how many of the documents are in each time slice.</p>

<p>Artyom</p>
",2,1,203,2015-07-10 21:21:53,https://stackoverflow.com/questions/31350481/gensim-timeslice-data-format
Getting UnicodeDecodeError when installing gensim on Ubuntu,"<p>I am trying to install <code>gensim</code> lib on Ubuntu using:</p>

<pre><code>pip install --upgrade gensim
</code></pre>

<p>However, I got an error like this:</p>

<pre><code>Requirement already up-to-date: gensim in /usr/local/lib/python3.4/dist-packages/gensim-0.12.0-py3.4-linux-x86_64.egg
Collecting numpy&gt;=1.3 (from gensim)
Downloading numpy-1.9.2.tar.gz (4.0MB)
100% |################################| 4.0MB 146kB/s 
Collecting scipy&gt;=0.7.0 (from gensim)
Downloading scipy-0.15.1.tar.gz (11.4MB)
100% |################################| 11.4MB 55kB/s
Requirement already up-to-date: six&gt;=1.2.0 in /usr/lib/python3/dist-packages (from gensim)
Collecting smart-open&gt;=1.2.1 (from gensim)
Downloading smart_open-1.2.1.tar.gz
Complete output from command python setup.py egg_info:
File ""/tmp/pip-build-_nbem_oq/smart-open/setup.py"", line 28, in &lt;module&gt;
    long_description = read('README.rst'),
  File ""/tmp/pip-build-_nbem_oq/smart-open/setup.py"", line 21, in read
    return open(os.path.join(os.path.dirname(__file__), fname)).read()
  File ""/usr/lib/python3.4/encodings/ascii.py"", line 26, in decode
    return codecs.ascii_decode(input, self.errors)[0]
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc5 in position 4344: ordinal not in range(128)
----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-_nbem_oq/smart-open
</code></pre>

<p>Does anyone know how to fix it? </p>
","ubuntu, pip, gensim","<p>It turned out that I can manually install the <code>gensim</code> lib by downloading and unzipping the tar.gz source for <code>gensim</code>, then run:</p>

<pre><code>python setup.py install
</code></pre>
",1,0,531,2015-07-13 13:38:52,https://stackoverflow.com/questions/31384947/getting-unicodedecodeerror-when-installing-gensim-on-ubuntu
"Gensim: ValueError: failed to create intent(cache|hide)|optional array-- must have defined dimensions but got (0,)","<p>I am trying to emulate streaming for some documents and update the LSI on additional documents streamed-in. I find this error:</p>

<pre><code>Traceback (most recent call last):
  File ""gensimStreamGen_tutorial5.py"", line 57, in &lt;module&gt;
    for vector in corpus_memory_friendly: # load one vector into memory at a time
  File ""gensimStreamGen_tutorial5.py"", line 44, in __iter__
    lsi = models.LsiModel(corpus, num_topics=10) # initialize an LSI transformation
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 331, in __init__
    self.add_documents(corpus)
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 388, in add_documents
    update = Projection(self.num_terms, self.num_topics, job, extra_dims=self.extra_samples, power_iters=self.power_iters)
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 126, in __init__
    extra_dims=self.extra_dims)
  File ""/Users/Desktop/gensim-0.12.0/gensim/models/lsimodel.py"", line 677, in stochastic_svd
    q, _ = matutils.qr_destroy(y) # orthonormalize the range
  File ""/Users/Desktop/gensim-0.12.0/gensim/matutils.py"", line 398, in qr_destroy
    qr, tau, work, info = geqrf(a, lwork=-1, overwrite_a=True)
ValueError: failed to create intent(cache|hide)|optional array-- must have defined dimensions but got (0,)
</code></pre>

<p>The code for streaming documents and updating LSI model:</p>

<pre><code>class MyCorpus(object):
    def __iter__(self):
        for document in documents:
            # Stream-in documents and build TF-IDF model to construct new_vec
            yield new_vec
            corpus.append(new_vec)
            tfidf = models.TfidfModel(corpus)
            corpus_tfidf = tfidf[corpus]
            lsi = models.LsiModel(corpus_tfidf,  num_topics=2)
            corpus_lsi = lsi[corpus_tfidf]
            lsi.print_topics(2)
            for doc in corpus_lsi:
                print(doc)

corpus_memory_friendly = MyCorpus()
for vector in corpus_memory_friendly:
    print(vector)
</code></pre>

<p>The corpus gets a new new_vec every iteration. The new_vec on each yield for different iterations:</p>

<pre><code>[]
[(0, 1)]
[(1, 1), (2, 1), (3, 1)]
[(3, 2), (4, 1), (5, 1)]
[(2, 1), (6, 1), (7, 1)]
[]
[(8, 1)]
[(8, 1), (9, 1)]
[(9, 1), (10, 1), (11, 1)]
</code></pre>

<p>The error appears on the first iteration (first line in expected new_vec). The rest is the expected output from new_vec.</p>
","python, gensim, latent-semantic-indexing","<p>I think because of your data in documents have blank
try add </p>

<pre><code>if(document!=[]and document!=[[]])
</code></pre>
",2,2,1633,2015-07-20 09:21:39,https://stackoverflow.com/questions/31512853/gensim-valueerror-failed-to-create-intentcachehideoptional-array-must-ha
Hierarchical Dirichlet Process Gensim topic number independent of corpus size,"<p>I am using the Gensim HDP module on a set of documents. </p>

<pre><code>&gt;&gt;&gt; hdp = models.HdpModel(corpusB, id2word=dictionaryB)
&gt;&gt;&gt; topics = hdp.print_topics(topics=-1, topn=20)
&gt;&gt;&gt; len(topics)
150
&gt;&gt;&gt; hdp = models.HdpModel(corpusA, id2word=dictionaryA)
&gt;&gt;&gt; topics = hdp.print_topics(topics=-1, topn=20)
&gt;&gt;&gt; len(topics)
150
&gt;&gt;&gt; len(corpusA)
1113
&gt;&gt;&gt; len(corpusB)
17
</code></pre>

<p>Why is the number of topics independent of corpus length?</p>
","python, nlp, lda, gensim","<p>@user3907335 is exactly correct here: HDP will calculate as many topics as the assigned truncation level. <em>However</em>, it may be the case that many of these topics have basically zero probability of occurring. To help with this in my own work, I wrote a handy little function that performs a rough estimate of the probability weight associated with each topic. Note that this is a rough metric only: <em>it does not account for the probability associated with each word</em>. Even so, it provides a pretty good metric for which topics are meaningful and which aren't: </p>

<pre><code>import pandas as pd
import numpy as np 

def topic_prob_extractor(hdp=None, topn=None):
    topic_list = hdp.show_topics(topics=-1, topn=topn)
    topics = [int(x.split(':')[0].split(' ')[1]) for x in topic_list]
    split_list = [x.split(' ') for x in topic_list]
    weights = []
    for lst in split_list:
        sub_list = []
        for entry in lst: 
            if '*' in entry: 
                sub_list.append(float(entry.split('*')[0]))
        weights.append(np.asarray(sub_list))
    sums = [np.sum(x) for x in weights]
    return pd.DataFrame({'topic_id' : topics, 'weight' : sums})
</code></pre>

<p>I assume that you already know how to calculate an HDP model. Once you have an hdp model calculated by gensim you call the function as follows: </p>

<pre><code>topic_weights = topic_prob_extractor(hdp, 500)
</code></pre>
",4,14,12656,2015-07-21 15:34:47,https://stackoverflow.com/questions/31543542/hierarchical-dirichlet-process-gensim-topic-number-independent-of-corpus-size
Configure Web2Py to use Anaconda Python,"<p>I am new to Web2Py and Python stack. I need to use a module in my Web2Py application which uses ""gensim"" and ""nltk"" libraries. I tried installing these into my Python 2.7 on a Windows 7 environment but came across several errors due to some issues with ""numpy"" and ""scipy"" installations on Windows 7. Then I ended up resolving those errors by uninstalling Python 2.7 and instead installing Anaconda Python which successfully installed the required ""gensim"" and ""nltk"" libraries. </p>

<p>So, at this stage I am able to see all these ""gensim"" and ""nltk"" libraries resolving properly without any error in ""Spyder"" and ""PyCharm"". However, when I run my application in Web2Py, it still complains about ""gensim"" and gives this error: <code>&lt;type 'exceptions.ImportError'&gt; No module named gensim</code></p>

<p>My guess is if I can configure Web2Py to use the Anaconda Python then this issue would be resolved. </p>

<p>I need to know if it's possible to configure Web2Py to use Anaconda Python and if it is then how do I do that? </p>

<p>Otherwise, if someone knows of some other way resolve that ""gensim"" error in Web2Py kindly share your thoughts.</p>

<p>All your help would be highly appreciated.</p>
","python-2.7, web2py, anaconda, gensim","<p>The Windows binary includes it's own Python interpreter and will therefore not see any packages you have in your local Python installation.</p>

<p>If you already have Python installed, you should instead run web2py from source.</p>
",2,0,772,2015-07-28 19:05:06,https://stackoverflow.com/questions/31685048/configure-web2py-to-use-anaconda-python
Combining Doc2Vec sentences into paragraph vectors,"<p>In Gensim's Doc2Vec, how do you combine sentence vectors to make a single vector for a paragraph?  I realise you can train on the entire paragraph, but it would obviously be better to train on individual sentences, for context, etc. (I think...?)</p>

<p>Any advice or normal use case?</p>

<p>Also, how would I retrieve sentence/paragraph vectors from the model?</p>
","gensim, word2vec","<p>Doc2Vec's architecture itself doesn't involve any parsing and it makes sense to train/test on the entire paragraph. </p>

<p>In original <a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow"">paper</a>, author shows results with just treating entire paragraph as one sentence, outperforming existing techniques.</p>
",2,1,678,2015-08-05 08:47:26,https://stackoverflow.com/questions/31827623/combining-doc2vec-sentences-into-paragraph-vectors
How to install Gensim on Windows 7,"<p>I have Windows 7 and WinPython 3.4.3.2; trying to install Gensim from <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/#gensim"" rel=""nofollow"">http://www.lfd.uci.edu/~gohlke/pythonlibs/#gensim</a></p>

<p>I'm getting this error:</p>

<pre><code>C:\Program Files (x86)\PowerCmd&gt;pip install e:\Python\gensim-0.12.1-cp34-none-win_amd64.whl
You are using pip version 6.0.8, however version 7.1.0 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Processing e:\python\gensim-0.12.1-cp34-none-win_amd64.whl
Requirement already satisfied (use --upgrade to upgrade): six&gt;=1.2.0 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages (from gensim==0.12.1)
Requirement already satisfied (use --upgrade to upgrade): numpy&gt;=1.3 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages (from gensim==0.12.1)
Collecting smart-open&gt;=1.2.1 (from gensim==0.12.1)
  Using cached smart_open-1.2.1.tar.gz
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 20, in &lt;module&gt;
  File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 28, in &lt;module&gt;
    long_description = read('README.rst'),
  File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 21, in read
    return open(os.path.join(os.path.dirname(__file__), fname)).read()
  File ""M:\WinPython-64bit-3.4.3.2\python-3.4.3.amd64\lib\encodings\cp1251.py"", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 4345: character maps to &lt;undefined&gt;
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 20, in &lt;module&gt;
      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 28, in &lt;module&gt;
        long_description = read('README.rst'),
      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 21, in read
        return open(os.path.join(os.path.dirname(__file__), fname)).read()
      File ""M:\WinPython-64bit-3.4.3.2\python-3.4.3.amd64\lib\encodings\cp1251.py"", line 23, in decode
        return codecs.charmap_decode(input,self.errors,decoding_table)[0]
    UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 4345: character maps to &lt;undefined&gt;
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

      File ""&lt;string&gt;"", line 20, in &lt;module&gt;

      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 28, in &lt;module&gt;

        long_description = read('README.rst'),

      File ""C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open\setup.py"", line 21, in read

        return open(os.path.join(os.path.dirname(__file__), fname)).read()

      File ""M:\WinPython-64bit-3.4.3.2\python-3.4.3.amd64\lib\encodings\cp1251.py"", line 23, in decode

        return codecs.charmap_decode(input,self.errors,decoding_table)[0]

    UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 4345: character maps to &lt;undefined&gt;

    ----------------------------------------
    Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\Joomler\AppData\Local\Temp\pip-build-49b17fh6\smart-open
</code></pre>
","python, gensim","<p>I installed <a href=""https://pypi.python.org/pypi/smart_open"" rel=""nofollow"">https://pypi.python.org/pypi/smart_open</a> manually from setup.py.</p>

<p>There was a problem file <code>README.rst</code> - I erased all data in it, to ensure that no bad characters would be printed.</p>

<p>Now I need to manually install Gensim, because it downloads <code>smart_open</code> automatically. Unfortunately, I am not able to <code>import gensim</code>. I'm having the following error:</p>

<pre><code>ImportError: No module named 'gensim'
</code></pre>

<p>But if I execute <code>pip install -U gensim</code> I'm able to see that all requirements are already satisfied:</p>

<pre><code>Requirement already up-to-date: gensim in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages\gensim-0.12.1-py3.4-win-amd64.egg
Requirement already up-to-date: numpy&gt;=1.3 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages (from gensim)
Requirement already up-to-date: scipy&gt;=0.7.0 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages (from gensim)
Requirement already up-to-date: six&gt;=1.5.0 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages (from gensim)
Requirement already up-to-date: smart-open&gt;=1.2.1 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages\smart_open-1.2.1-py3.4.egg (from gensim)
Requirement already up-to-date: boto&gt;=2.32 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages\boto-2.38.0-py3.4.egg (from smart-open&gt;=1.2.1-&gt;gensim)
Requirement already up-to-date: httpretty==0.8.6 in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages\httpretty-0.8.6-py3.4.egg (from smart-open&gt;=1.2.1-&gt;gensim)
Requirement already up-to-date: bz2file in m:\winpython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-packages\bz2file-0.98-py3.4.egg (from smart-open&gt;=1.2.1-&gt;gensim)
</code></pre>

<p>Any hints?</p>

<p>Edit 15/08/2015: Install was successful. I just had to clean up older attemps and reinstall.</p>
",0,0,3542,2015-08-13 19:28:39,https://stackoverflow.com/questions/31996843/how-to-install-gensim-on-windows-7
Error while loading Word2Vec model in gensim,"<p>I'm getting an <code>AttributeError</code> while loading the gensim model available at word2vec repository:</p>

<pre><code>from gensim import models
w = models.Word2Vec()
w.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print w[""queen""]

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-3-8219e36ba1f6&gt; in &lt;module&gt;()
----&gt; 1 w[""queen""]

C:\Anaconda64\lib\site-packages\gensim\models\word2vec.pyc in __getitem__(self, word)
    761 
    762         """"""
--&gt; 763         return self.syn0[self.vocab[word].index]
    764 
    765 

AttributeError: 'Word2Vec' object has no attribute 'syn0'
</code></pre>

<p>Is this a known issue ?</p>
","python, gensim, word2vec","<p>Fixed the problem with:</p>

<pre><code>from gensim import models
w = models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print w[""queen""]
</code></pre>
",11,10,33011,2015-08-19 17:17:41,https://stackoverflow.com/questions/32101795/error-while-loading-word2vec-model-in-gensim
how to remove numbers and symbols from output of LDA while using Gensim package?,"<p>how to remove these numbers from output of LDA while using Gensim package?</p>

<p>2015-08-25 15:26:20,439 : INFO : topic #8 (0.100): 0.038*watch + 0.020*water + 0.014*strap + 0.011*analog + 0.011*resistance + 0.010*atm + 0.010*coloured + 0.010*timepiece + 0.010*5 + 0.009*classy</p>

<p>so that output will be  watch,water,strap...etc</p>
","python, lda, gensim","<p>you can use <a href=""https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.show_topic"" rel=""nofollow"">show_topic</a> which returns a list of tuples (prob, word) and then print the words using for example :</p>

<pre><code>topic_tuple = lda.show_topic(0, 10)

print ','.join([str(word_tuple[1]) for word_tuple in topic_tuple])
</code></pre>

<p>Based on <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/ldamodel.py#L741"" rel=""nofollow"">gensim source code</a></p>
",0,0,403,2015-08-25 10:38:47,https://stackoverflow.com/questions/32201795/how-to-remove-numbers-and-symbols-from-output-of-lda-while-using-gensim-package
How to abstract bigram topics instead of unigrams using Latent Dirichlet Allocation (LDA) in python- gensim?,"<h1>LDA Original Output</h1>

<ul>
<li><p>Uni-grams </p>

<ul>
<li><p>topic1 -scuba,water,vapor,diving</p></li>
<li><p>topic2 -dioxide,plants,green,carbon</p></li>
</ul></li>
</ul>

<h1>Required Output</h1>

<ul>
<li><p>Bi-gram topics</p>

<ul>
<li><p>topic1 -scuba diving,water vapor</p></li>
<li><p>topic2 -green plants,carbon dioxide</p></li>
</ul></li>
</ul>

<p>Any idea?</p>
","nlp, text-mining, lda, gensim","<p>You can use word2vec to get most similar terms from the top n topics abstracted using LDA.</p>

<p>LDA Output</p>

<p>Create a dictionary of bi-grams using topics abstracted  (for ex:-san_francisco)</p>

<p>check <a href=""http://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/"" rel=""nofollow"">http://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/</a></p>

<p>Then, do word2vec to get most similar words (uni-grams,bi-grams etc)  </p>

<p><strong>Word</strong>      and           <strong>Cosine distance</strong>  </p>

<p>los_angeles               (0.666175)<br>
golden_gate               (0.571522)<br>
oakland                   (0.557521)  </p>

<p>check <a href=""https://code.google.com/p/word2vec/"" rel=""nofollow"">https://code.google.com/p/word2vec/</a>  (From words to phrases and beyond)</p>
",3,5,13427,2015-09-09 09:51:02,https://stackoverflow.com/questions/32476336/how-to-abstract-bigram-topics-instead-of-unigrams-using-latent-dirichlet-allocat
"Gensim: TypeError: doc2bow expects an array of unicode tokens on input, not a single string","<p>I am starting with some python task, I am facing a problem while using gensim. I am trying to load files from my disk and process them (split them and lowercase() them)</p>

<p>The code I have is below:</p>

<pre><code>dictionary_arr=[]
for file_path in glob.glob(os.path.join(path, '*.txt')):
    with open (file_path, ""r"") as myfile:
        text=myfile.read()
        for words in text.lower().split():
            dictionary_arr.append(words)
dictionary = corpora.Dictionary(dictionary_arr)
</code></pre>

<p>The list (dictionary_arr) contains the list of all words across all the file, I then use gensim corpora.Dictionary to process the list. However I face a error.</p>

<pre><code>TypeError: doc2bow expects an array of unicode tokens on input, not a single string
</code></pre>

<p>I cant understand whats a problem, A little guidance would be appreciated.</p>
","python, gensim","<p>In dictionary.py, the initialize function is:</p>

<pre><code>def __init__(self, documents=None):
    self.token2id = {} # token -&gt; tokenId
    self.id2token = {} # reverse mapping for token2id; only formed on request, to save memory
    self.dfs = {} # document frequencies: tokenId -&gt; in how many documents this token appeared

    self.num_docs = 0 # number of documents processed
    self.num_pos = 0 # total number of corpus positions
    self.num_nnz = 0 # total number of non-zeroes in the BOW matrix

    if documents is not None:
        self.add_documents(documents)
</code></pre>

<p>Function add_documents Build dictionary from a collection of documents. Each document is a list
        of tokens:</p>

<pre><code>def add_documents(self, documents):

    for docno, document in enumerate(documents):
        if docno % 10000 == 0:
            logger.info(""adding document #%i to %s"" % (docno, self))
        _ = self.doc2bow(document, allow_update=True) # ignore the result, here we only care about updating token ids
    logger.info(""built %s from %i documents (total %i corpus positions)"" %
                 (self, self.num_docs, self.num_pos))
</code></pre>

<p>So ,if you initialize Dictionary in this way, you must pass documents but not a single document. For example,</p>

<pre><code>dic = corpora.Dictionary([a.split()])
</code></pre>

<p>is OK.</p>
",19,16,40634,2015-10-20 06:20:23,https://stackoverflow.com/questions/33229360/gensim-typeerror-doc2bow-expects-an-array-of-unicode-tokens-on-input-not-a-si
results from an sqlachemy query as iterator,"<p>I am struggling to create an iterator from a query from sqlalchemy. </p>

<p>Here is what I tried so far </p>

<p><strong>create a table</strong> </p>

<pre><code>from sqlalchemy import create_engine, Column, MetaData, Table , Integer, String
engine = create_engine('sqlite:///test90.db')
conn = engine.connect()
metadata = MetaData()
myTable = Table('myTable', metadata,
     Column('Doc_id', Integer, primary_key=True),
     Column('Doc_Text', String))
metadata.create_all(engine)

conn.execute(myTable.insert(), [{'Doc_id': 1, 'Doc_Text' : 'first sentence'},
          {'Doc_id': 2, 'Doc_Text' : 'second sentence'},
          {'Doc_id': 3, 'Doc_Text' : 'third sentence'},
          {'Doc_id': 4, 'Doc_Text' : 'fourth sentence'}
          ])
</code></pre>

<p>I read everything I could on iterator but do not get it. 
Here the class I created to get an iterator but it does not work 
(it overflows although I specify a break) </p>

<pre><code>from sqlalchemy import create_engine

class RecordsIterator:
def __init__(self, xDB, xSQL):
    self.engine = create_engine(xDB)
    self.conn = self.engine.connect()
    self.xResultCollection = self.conn.execute(xSQL)
def __iter__(self):
    return self 
def next (self):
    while self.xResultCollection.closed is False:
        xText = (self.xResultCollection.fetchone())[1]
        xText = xText.encode('utf-8')
        yield xText.split()
        if not self.xResultCollection:
            break


x1 = RecordsIterator(xDB = 'sqlite:///test91.db', xSQL = 'select * from myTable')
</code></pre>

<p>In case you are wondering why I am not just using a <strong>generator</strong> . 
I need to feed the iterator in gensim.Word2Vec and unfortunately, it does not take a generator </p>

<pre><code>   import gensim
   gensim.models.Word2Vec(x1)
</code></pre>

<p>Thanks in advance  </p>
","python, iterator, sqlalchemy, gensim","<p>Your check <code>if not self.xResultCollection</code> will always return <code>False</code>, as the <a href=""https://docs.python.org/2/library/stdtypes.html#truth-value-testing"" rel=""nofollow"">truth value</a> of the result object will always be <code>True</code>.</p>

<p>In your <code>next</code> method you have a for and a while loop, which shouldn't really be needed, the <code>next</code> method should just return one element, there's no need for a loop there.</p>

<p>As <code>self.xResultCollection</code> is itself an iterable you could just do:</p>

<pre><code>class RecordsIterator:  
    def __init__(self, xDB, xSQL):
        self.engine = create_engine(xDB)
        self.conn = self.engine.connect()
        self.resultIterator = iter(self.conn.execute(xSQL))
    def __iter__(self):
        return self 
    def next (self):
        return next(self.resultIterator)[1].encode('utf-8').split()
</code></pre>
",1,0,400,2015-11-10 20:24:43,https://stackoverflow.com/questions/33638915/results-from-an-sqlachemy-query-as-iterator
"import gensim imports a file in an active module, not the root site-packages folder","<p>I'm running Anaconda Python 2.7 on Windows. I've installed gensim and pyLDAvis to do some topic modeling. (Note installing pyLDAvis on python 2.7 in windows is a little tricky as you have to make sure you are not using scikit-bio which doesn't appear to compile on Windows 2.7... I think I have a workaround for this, but I can't try it because of reasons to be outlined below!)</p>

<p>So I got pyLDAvis to install. However when running, it seems to have a problem with an import statement. </p>

<p>pyLDAvis is installed in this folder....</p>

<pre><code>C:\Anaconda2\Lib\site-packages\pyLDAvis-1.3.2-py2.7.egg\pyLDAvis
</code></pre>

<p><code>sys.path</code> returns this:</p>

<pre><code>['',
'C:\\Anaconda2\\lib\\site-packages\\pyldavis-1.3.2-py2.7.egg',
'C:\\Anaconda2\\lib\\site-packages\\joblib-0.9.3-py2.7.egg',
'C:\\Anaconda2\\python27.zip',
'C:\\Anaconda2\\DLLs',
'C:\\Anaconda2\\lib',
'C:\\Anaconda2\\lib\\plat-win',
'C:\\Anaconda2\\lib\\lib-tk',
'C:\\Anaconda2',
'C:\\Anaconda2\\Library\\bin',
'c:\\anaconda2\\lib\\site-packages\\sphinx-1.3.1-py2.7.egg',
'c:\\anaconda2\\lib\\site-packages\\setuptools-18.4-py2.7.egg',
'C:\\Anaconda2\\lib\\site-packages',
'C:\\Anaconda2\\lib\\site-packages\\cryptography-1.0.2-py2.7-win-amd64.egg',
'C:\\Anaconda2\\lib\\site-packages\\win32',
'C:\\Anaconda2\\lib\\site-packages\\win32\\lib',
'C:\\Anaconda2\\lib\\site-packages\\Pythonwin',
'C:\\Anaconda2\\lib\\site-packages\\IPython\\extensions']
</code></pre>

<p>What is happening is that when I try to run <code>pyLDAvis</code>, the library calls <code>import gensim</code>. However, <code>gensim</code> is both a folder in the <code>site-packages</code> and a file (<code>gensim.py</code>) inside <code>pyLDAvis</code>. </p>

<p>So when python tries to <code>import gensim</code> inside the <code>pyLDAvis</code> module, it imports the <code>gensim.py</code> file within the <code>pyLDAvis</code> module, not the ``gensim<code>folder inside</code>site-packages`. </p>

<p>How do I go about fixing this? </p>

<p>Thanks. </p>
","python, python-2.7, gensim","<p>File an issue report on <a href=""https://github.com/bmabey/pyLDAvis/issues"" rel=""nofollow"">pyLDAvis's GitHub</a>. It looks like a <a href=""https://github.com/bmabey/pyLDAvis/commit/848b247b77bb24b1e674aa0cf7c0d5f9b4f26a9d"" rel=""nofollow"">recent change</a> broke Python 2 compatibility by assuming Python 3 absolute import behavior for <code>import gensim</code>.</p>

<p>In the meantime, I believe the bug isn't present in the 1.3.1 release, so you could use that. Alternatively, you could edit <code>pyLDAvis/gensim.py</code> and add <code>from __future__ import absolute_import</code> at the top. That'd <em>probably</em> work as a temporary fix, but I didn't try it.</p>
",3,0,1031,2015-11-13 21:59:05,https://stackoverflow.com/questions/33702450/import-gensim-imports-a-file-in-an-active-module-not-the-root-site-packages-fol
Is there a limit in Gensim&#39;s Doc2Vec most_similar documents result set?,"<p>I have been experimenting with the doc2vec module for sometime now. I can train my model and have the trained model output similar documents for a given document as follows :</p>

<pre><code>import re
modelloaded=Doc2Vec.load(""model_all_doc_dm_1"")

st = 'long description of a document as string'
doc = re.sub('[^a-zA-Z]', ' ', st).lower().split() 

new_doc_vec = modelloaded.infer_vector(doc)

modelloaded.docvecs.most_similar([new_doc_vec])
</code></pre>

<p>This works well, and gives me 10 results. Is there a way to get more than 10 results or is that the limit?</p>
","python-3.x, gensim","<p>I found it:</p>

<pre><code>modelloaded.docvecs.most_similar([new_doc_vec], topn=N)
</code></pre>

<p>the <code>topn=N</code> handle can be used to get more than 10 results.</p>
",5,5,787,2015-11-18 20:16:25,https://stackoverflow.com/questions/33789541/is-there-a-limit-in-gensims-doc2vec-most-similar-documents-result-set
K means Clustering on n dimensional vectors.,"<p>I'm applying TFIDF on text documents where I get varied length n dimensional vectors each corresponding to a document. </p>

<pre><code>    texts = [[token for token in text if frequency[token] &gt; 1] for text in texts]
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    lda = models.ldamodel.LdaModel(corpus, num_topics=100, id2word=dictionary)
    tfidf = models.TfidfModel(corpus)   
    corpus_tfidf = tfidf[corpus]
    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)
    corpus_lsi = lsi[corpus_tfidf]
    corpus_lda=lda[corpus]
    print ""TFIDF:""
    print corpus_tfidf[1]
    print ""__________________________________________""
    print corpus_tfidf[2]
</code></pre>

<p>The output to this is:</p>

<pre><code>TFIDF:
Vec1:    [(19, 0.06602704727889631), (32, 0.360417819987515), (33, 0.3078487494326974), (34, 0.360417819987515), (35, 0.2458968255872351), (36, 0.23680107692707422), (37, 0.29225639811281434), (38, 0.31741275088103), (39, 0.28571949457481044), (40, 0.32872456368129543), (41, 0.3855741727557306)]
    __________________________________________
Vec2:    [(5, 0.05617283528623041), (6, 0.10499864499395724), (8, 0.11265354901199849), (16, 0.028248249837939252), (19, 0.03948130674177094), (29, 0.07013501129200184), (33, 0.18408018239985235), (42, 0.14904146984986072), (43, 0.20484144632880313), (44, 0.215514203535732), (45, 0.15836501876891904), (46, 0.08505477582234795), (47, 0.07138425858136686), (48, 0.127695955436003), (49, 0.18408018239985235), (50, 0.2305566099597365), (51, 0.20484144632880313), (52, 0.2305566099597365), (53, 0.2305566099597365), (54, 0.053099690797234665), (55, 0.2305566099597365), (56, 0.2305566099597365), (57, 0.2305566099597365), (58, 0.0881162347543671), (59, 0.20484144632880313), (60, 0.16408387627386525), (61, 0.08256873616398946), (62, 0.215514203535732), (63, 0.2305566099597365), (64, 0.16731192344738707), (65, 0.2305566099597365), (66, 0.2305566099597365), (67, 0.07320703902661252), (68, 0.17912628269786976), (69, 0.12332630621892736)]
</code></pre>

<p>The vector points not represented are 0. Which means say (18, ....) does not exist in the vector, then it is 0. </p>

<p>I want to apply K means clustering on these vectors (Vec1 and Vec2)</p>

<p>Scikit's K means clustering needs vectors in equal dimension and in matrix format. What should be done about this? </p>
","python, scikit-learn, k-means, gensim","<p>So after looking at the source code, it looks like gensim manually creates a sparse vector for each document (which is just a list of tuples). This makes the error make sense, since scikit-learn's kMeans algorithm allows for sparse scipy matrices, but it doesn't know how to interpret the gensim sparse vector. You can turn each of these individual lists into a scipy csr_matrix with the following (it would be better to convert all docs at once, but this is a quick fix).</p>

<pre><code>rows = [0] * len(corpus_tfidf[1])
cols = [tup[0] for tup in corpus_tfidf[1]]
data = [tup[1] for tup in corpus_tfidf[1]]
sparse_vec = csr_matrix((data, (rows, cols)))
</code></pre>

<p>You should be able to make use of this <code>sparse_vec</code>, but if it throws errors, you can turn it into a dense numpy array with <code>.toarray()</code> or numpy matrix with <code>.todense()</code>.</p>

<p>EDIT: Turns out that Gensim provides some nifty utility functions, including one that takes the streamed corpus object format and returns a csc matrix. Here's a full example of how your code might work (connected to sklearn's kMeans clustering algorithm)</p>

<pre><code>from gensim import corpora, models, matutils
from sklearn.cluster import KMeans

texts = [[token for token in text] for text in texts]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

print ""TFIDF:""
corpus_tfidf = matutils.corpus2csc(corpus_tfidf).transpose()
print corpus_tfidf
print ""__________________________________________""

kmeans = KMeans(n_clusters=2)
print kmeans.fit_predict(corpus_tfidf)
</code></pre>

<p>You should calculate and pass the additional parameters that go into corpus2csc, as it could save you cycles depending on the size of your corpus. We transpose the matrix as gensim puts the documents in the columns and the terms in the rows. You can turn the scipy sparse matrix into the myriad of other types, depending on your use case (besides just the kmeans clustering). </p>
",1,0,5363,2015-11-20 13:47:58,https://stackoverflow.com/questions/33828304/k-means-clustering-on-n-dimensional-vectors
gensim LdaMulticore not multiprocessing?,"<p>When I run gensim's <code>LdaMulticore</code> model on a machine with 12 cores, using:</p>

<pre><code>lda = LdaMulticore(corpus, num_topics=64, workers=10)
</code></pre>

<p>I get a logging message that says </p>

<pre><code>using serial LDA version on this node  
</code></pre>

<p>A few lines later, I see another loging message that says </p>

<pre><code>training LDA model using 10 processes
</code></pre>

<p>When I run top, I see 11 python processes have been spawned, but 9 are sleeping, I.e. only one worker is active.  The machine has 24 cores, and is not overwhelmed by any means.  Why isn't LdaMulticore running in parallel mode?</p>
","python, multiprocessing, lda, gensim","<p>First, make sure you <a href=""https://radimrehurek.com/gensim/distributed.html"" rel=""noreferrer"">have installed a fast BLAS library</a>, because most of the time consuming stuff is done inside low-level routines for linear algebra. </p>

<p>On my machine the <a href=""https://radimrehurek.com/gensim/models/ldamulticore.html"" rel=""noreferrer""><code>gensim.models.ldamodel.LdaMulticore</code></a> can use up all the 20 cpu cores with <code>workers=4</code> during training. Setting workers larger than this didn't speed up the training. <a href=""https://github.com/piskvorky/gensim/issues/288"" rel=""noreferrer"">One reason might be the <code>corpus</code> iterator is too slow to use LdaMulticore effectively</a>.</p>

<p>You can try to use <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/sharded_corpus.py"" rel=""noreferrer""><code>ShardedCorpus</code></a> to serialize and replace the <code>corpus</code>, which should be much faster to read/write. Also, simply zipping your large <code>.mm</code> file so it takes up less space (=less I/O) may help too. E.g.,</p>

<pre><code>mm = gensim.corpora.MmCorpus(bz2.BZ2File('enwiki-latest-pages-articles_tfidf.mm.bz2'))
lda = gensim.models.ldamulticore.LdaMulticore(corpus=mm, id2word=id2word, num_topics=100, workers=4)
</code></pre>
",15,9,7533,2015-11-26 02:31:03,https://stackoverflow.com/questions/33929680/gensim-ldamulticore-not-multiprocessing
Python: gensim: RuntimeError: you must first build vocabulary before training the model,"<p>I know that this question has been asked already, but I was still not able to find a solution for it. </p>

<p>I would like to use gensim's <code>word2vec</code> on a custom data set, but now I'm still figuring out in what format the dataset has to be. I had a look at <a href=""http://streamhacker.com/2014/12/29/word2vec-nltk/"">this post</a> where the input is basically a list of lists (one big list containing other lists that are tokenized sentences from the NLTK Brown corpus). So I thought that this is the input format I have to use for the command <code>word2vec.Word2Vec()</code>. However, it won't work with my little test set and I don't understand why.</p>

<p>What I have tried:</p>

<p><strong>This worked</strong>:</p>

<pre><code>from gensim.models import word2vec
from nltk.corpus import brown
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

brown_vecs = word2vec.Word2Vec(brown.sents())
</code></pre>

<p><strong>This didn't work</strong>:</p>

<pre><code>sentences = [ ""the quick brown fox jumps over the lazy dogs"",""yoyoyo you go home now to sleep""]
vocab = [s.encode('utf-8').split() for s in sentences]
voc_vec = word2vec.Word2Vec(vocab)
</code></pre>

<p>I don't understand why it doesn't work with the ""mock"" data, even though it has the same data structure as the sentences from the Brown corpus:</p>

<p><strong>vocab</strong>:</p>

<pre><code>[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs'], ['yoyoyo', 'you', 'go', 'home', 'now', 'to', 'sleep']]
</code></pre>

<p><strong>brown.sents()</strong>: (the beginning of it)</p>

<pre><code>[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', ""Atlanta's"", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', ""''"", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', ""''"", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]
</code></pre>

<p>Can anyone please tell me what I'm doing wrong? </p>
","python, gensim, word2vec","<p>Default <code>min_count</code> in gensim's Word2Vec is set to 5. If there is no word in your vocab with frequency greater than 4, your vocab will be empty and hence the error. Try</p>

<pre><code>voc_vec = word2vec.Word2Vec(vocab, min_count=1)
</code></pre>
",80,33,40313,2015-11-30 00:30:37,https://stackoverflow.com/questions/33989826/python-gensim-runtimeerror-you-must-first-build-vocabulary-before-training-th
importing gensim in mac,"<p>I'm having a problem when trying to import gensim in python. When typing:</p>

<blockquote>
  <p>import gensim</p>
</blockquote>

<p>I got the following error:</p>

<p>Traceback (most recent call last):
  File """", line 1, in 
  File ""/Library/Python/2.7/site-packages/gensim/<strong>init</strong>.py"", line 6, in 
    from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
ImportError: cannot import name parsing</p>

<p>Also, when I view ""<strong>init</strong>.py"" it contains only the following lines:</p>

<blockquote>
  <h1>bring model classes directly into package namespace, to save some typing</h1>
  
  <p>from .summarizer import summarize, summarize_corpus</p>
  
  <p>from .keywords import keywords</p>
</blockquote>

<p>Any idea on how to solve this problem is highly appreciated.</p>

<p>I'm using:
MAC 10.10.5 and Python 2.7</p>

<p>Thank you</p>
","python, gensim","<p>I had a similar error. I used pip to update itself, then uninstall, reinstall, and update gensim. I also pip installed Theano (b/c mine was unable to import something related to it). </p>

<pre><code>pip install --upgrade pip
pip uninstall gensim
pip install --upgrade gensim 
pip install Theano 
</code></pre>

<p>Then I needed to close and restart a new terminal python shell, and it worked! </p>

<p>One other note-- if you look at the error message, you can see the filepath to the .py files in the /gensim folder and the line in that .py file causing the error. Then you can try to manually run each import that is causing an error (after cd-ing to the appropriate folder). This might help you find what packages are causing the problem. </p>
",1,0,2159,2015-12-01 13:15:05,https://stackoverflow.com/questions/34021351/importing-gensim-in-mac
How to resolve error when installing gensim?,"<p>I'm trying to install gensim on Windows 7, with Python 3.4. According to <a href=""http://radimrehurek.com/gensim/install.html"" rel=""nofollow"">gensim official installation tutorial</a>, gensim depends on NumPy and SciPy, so I went to <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/"" rel=""nofollow"">here</a> to download .whl files for NumPy and SciPy installation. But when I used pip to install them, it gave me these errors:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;pyshell#2&gt;"", line 1, in &lt;module&gt;
import gensim
File ""C:\Python34\lib\site-packages\gensim\__init__.py"", line 6, in &lt;module&gt;
from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
File ""C:\Python34\lib\site-packages\gensim\matutils.py"", line 21, in &lt;module&gt;
import scipy.linalg
File ""C:\Python34\lib\site-packages\scipy\linalg\__init__.py"", line 172, in &lt;module&gt;
from .misc import *
File ""C:\Python34\lib\site-packages\scipy\linalg\misc.py"", line 5, in &lt;module&gt;
from .blas import get_blas_funcs
File ""C:\Python34\lib\site-packages\scipy\linalg\blas.py"", line 155, in &lt;module&gt;
from scipy.linalg import _fblas
ImportError: DLL load failed: 找不到指定的模块。
</code></pre>

<p>""找不到指定的模块""means ""Cannot find the designated module"".
How can I resolve this?</p>
","python, gensim","<p>Numpy and Scipy can be pretty difficult to install, because they rely on some classic implementations of algorithms in older programming languages like Fortran, which have complicated dependency chains.  </p>

<p>Would you be amenable to installing <a href=""https://www.continuum.io/why-anaconda"" rel=""nofollow"">Anaconda</a>, a fantastic Python distribution that includes Numpy and Scipy? If you install Anaconda's python installation, you should find it easy to use <code>pip install gensim</code> because you'll already have the most difficult-to-install dependencies installed. </p>

<p>There are Python 2.7 and 3.5 versions <a href=""https://www.continuum.io/downloads#_windows"" rel=""nofollow"">here</a> if you'd like to give it a try. As someone who regularly uses Numpy and Scipy, I regularly install Anaconda on new machines and it saves lots of heartache!</p>
",2,0,16108,2015-12-03 03:28:22,https://stackoverflow.com/questions/34057374/how-to-resolve-error-when-installing-gensim
"All of the words, those I use to train the word2vec model, must be in model.vocab, aren&#39;t they?","<p>I use the next code to train the model:</p>

<pre><code>norms_train = [ [''], [ u'word', u'to', u'learn', ... ], ...]
model = word2vec.Word2Vec(norms_train, size=100, window=10)
</code></pre>

<p>With procedure to check the results:</p>

<pre><code>i, j = 0, 0
for text in norms_train:
    j += len(text)
    for word in text:
        if word not in model.vocab:
            i += 1
print i, '/', j
</code></pre>

<p>13129 / 185379</p>
","python, gensim, training-data, word2vec","<p>All words that you have used to train the Word2Vec model should be in model.vocab. There may be a threshold on the minimum number of occurrences of a word, that have to be present for it to be included in the model vocabulary.</p>

<p>I suppose the argument <code>min_count</code> is set to 5 by default i.e. if a word has occurred less than 5 times in the training data, that word would not be present in the model.vocab.</p>
",1,0,308,2015-12-08 10:39:55,https://stackoverflow.com/questions/34153708/all-of-the-words-those-i-use-to-train-the-word2vec-model-must-be-in-model-voca
Generator is not an iterator?,"<p>I have an generator (a function that yields stuff), but when trying to pass it to <code>gensim.Word2Vec</code> I get the following error:</p>

<blockquote>
  <p>TypeError: You can't pass a generator as the sentences argument. Try an iterator.</p>
</blockquote>

<p>Isn't a generator a kind of iterator? If not, how do I make an iterator from it?</p>

<p>Looking at the library code, it seems to simply iterate over sentences like <code>for x in enumerate(sentences)</code>, which works just fine with my generator. What is causing the error then?</p>
","python, gensim, word2vec","<p>Generator is <strong>exhausted</strong> after one loop over it. Word2vec simply needs to traverse sentences multiple times (and probably get item for a given index, which is not possible for generators which are just a kind of stacks where you can only pop), thus requiring something more solid, like a list.</p>

<p>In particular in their code they call two different functions, both iterate over sentences (thus if you use generator, the second one would run on an empty set)</p>

<pre><code>self.build_vocab(sentences, trim_rule=trim_rule)
self.train(sentences)
</code></pre>

<p>It should work with anything implementing <code>__iter__</code>  which is not <code>GeneratorType</code>. So wrap your function in an iterable interface and make sure that you can traverse it multiple times, meaning that</p>

<pre><code>sentences = your_code
for s in sentences:
  print s
for s in sentences:
  print s
</code></pre>

<p>prints your collection twice</p>
",15,26,8018,2015-12-08 21:28:53,https://stackoverflow.com/questions/34166369/generator-is-not-an-iterator
Cannot run pyLDAvis. Getting Error : ImportError: cannot import name PCoA,"<p>I have created LDA model using gensim. Now, I wanted to visualise it using pyLDAvis library but getting :</p>

<pre><code>ImportError: cannot import name PCoA 
</code></pre>

<p>Can anyone help me with this or suggest some alternatives.</p>

<p>Thanks in advance.</p>
","python, scikit-learn, gensim, skbio","<p>You must check the scikit-bio python package. It must be &lt; than 0.4.x. From the version 0.4.x, the method has a different name.</p>

<p>You have to install the right version in the following way:</p>

<blockquote>
  <p>sudo pip install scikit-bio==0.2.X</p>
</blockquote>

<p>Cheers</p>
",1,0,1678,2015-12-16 10:23:10,https://stackoverflow.com/questions/34309428/cannot-run-pyldavis-getting-error-importerror-cannot-import-name-pcoa
gensim installation on yosemite using anaconda,"<p>I've installed gensim on my MacBookPro (Yosemite 10.10.5 ) and I'm using anconda. The installation with <code>pip install --upgrade gensim</code> was working without error message. 
When I tried to run the code of the tutorials, there appears an error when calling serialization:
<code>corpora.MmCorpus.serialize('/temp/deerwester.mm', corpus)</code></p>

<p>Complete error message:</p>

<p><code>File ""/Users/sage/Desktop/gensim/test_gensim.py"", line 39, in &lt;module&gt;
    corpora.MmCorpus.serialize('/temp/deerwester.mm', corpus)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/corpora/indexedcorpus.py"", line 94, in serialize
    offsets = serializer.save_corpus(fname, corpus, id2word, metadata=metadata)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/corpora/mmcorpus.py"", line 49, in save_corpus
    return matutils.MmWriter.write_corpus(fname, corpus, num_terms=num_terms, index=True, progress_cnt=progress_cnt, metadata=metadata)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/matutils.py"", line 486, in write_corpus
    mw = MmWriter(fname)
  File ""/System/Library/anaconda/lib/python2.7/site-packages/gensim-0.12.3-py2.7-macosx-10.5-x86_64.egg/gensim/matutils.py"", line 436, in __init__
    self.fout = utils.smart_open(self.fname, 'wb+') # open for both reading and writing
  File ""/System/Library/anaconda/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 111, in smart_open
    raise NotImplementedError('unknown file mode %s' % mode)
NotImplementedError: unknown file mode wb+</code></p>

<p>When I downloaded the tar files and performed <code>python setup.py test</code>,
the error <code>NotImplementedError: unknown file mode wb+</code> occurred too. </p>

<p>How can I fix this?</p>
","python, python-2.7, anaconda, gensim","<p>It looks like an incompatability between <code>gensim</code> and the <code>smart_open</code> library.</p>

<p>I resolved the same issue (on a linux box) with</p>

<pre><code>pip uninstall smart_open
</code></pre>

<p><code>gensim</code> then falls back to using the filesystem directly, which was fine for me.</p>
",1,2,969,2015-12-21 12:59:25,https://stackoverflow.com/questions/34396300/gensim-installation-on-yosemite-using-anaconda
&#39;utf-8&#39; decode error when loading a word2vec module,"<p>I have to use a word2vec module containing tons of Chinese characters. The module was trained by my coworkers using Java and is saved as a bin file. </p>

<p>I installed <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">gensim</a> and tries to load the module, but following error occurred: </p>

<pre><code>In [1]: import gensim  

In [2]: model = gensim.models.Word2Vec.load_word2vec_format('/data5/momo-projects/user_interest_classification/code/word2vec/vectors_groups_1105.bin', binary=True)

UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 96-97: unexpected end of data
</code></pre>

<p>I tried to load the module both in python 2.7 and 3.5, failed in the same way. So how can I load the module in gensim? Thanks.</p>
","python, nlp, gensim, word2vec","<p>The module was tons of Chinese characters trained by Java. I cannot figure out the encoding format of the original corpus. The error can be solved as the description in gensim <a href=""https://github.com/piskvorky/gensim/wiki/Recipes-&amp;-FAQ#q10-loading-a-word2vec-model-fails-with-unicodedecodeerror-utf-8-codec-cant-decode-bytes-in-position-"" rel=""noreferrer"">FAQ</a>, </p>

<p>Using load_word2vec_format with a flag for ignoring the character decoding errors:</p>

<pre><code>In [1]: import gensim

In [2]: model = gensim.models.Word2Vec.load_word2vec_format('/data5/momo-projects/user_interest_classification/code/word2vec/vectors_groups_1105.bin', binary=True, unicode_errors='ignore')
</code></pre>

<p>But I've no idea whether it matters when ignoring the encoding errors.</p>
",6,5,8572,2015-12-23 02:24:53,https://stackoverflow.com/questions/34427678/utf-8-decode-error-when-loading-a-word2vec-module
stopword removing when using the word2vec,"<p>I have been trying word2vec for a while now using the gensim's word2vec library. My question is do I have to remove stopwords from my input text?  Because, based on my initial experimental results, I could see words like 'of', 'when'.. (stopwords) popping up when I do a <code>model.most_similar('someword')</code>..?</p>

<p>But I didn't see anywhere referring that stop word removal is necessary with word2vec? Does the word2vec is supposed to handle stop words even if you don't remove them?</p>

<p>What are the must do pre processing things (like for topic modeling, it's almost a must that you should do stopword removal)?</p>
","nlp, gensim, word2vec","<p>Personaly I think, removal of stop word will give better results, check <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""noreferrer"">link</a></p>

<p>Also for topic modeling, you shlould perform preprocessing on the text, following things you must do,</p>

<ol>
<li>Remove of stop words.</li>
<li><a href=""http://www.nltk.org/book/ch03.html"" rel=""noreferrer"">Tokenization.</a></li>
<li><a href=""http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"" rel=""noreferrer"">Stemming and Lemmatization</a>.</li>
</ol>
",12,27,21645,2016-01-11 12:49:10,https://stackoverflow.com/questions/34721984/stopword-removing-when-using-the-word2vec
Ensure the gensim generate the same Word2Vec model for different runs on the same data,"<p>In <a href=""https://stackoverflow.com/questions/15067734/lda-model-generates-different-topics-everytime-i-train-on-the-same-corpus"">LDA model generates different topics everytime i train on the same corpus</a> , by setting the <code>np.random.seed(0)</code>, the LDA model will always be initialized and trained in exactly the same way. </p>

<p><strong>Is it the same for the Word2Vec models from <code>gensim</code>? By setting the random seed to a constant, would the different run on the same dataset produce the same model?</strong></p>

<p>But strangely, it's already giving me the same vector at different instances. </p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
&gt;&gt;&gt; exit()
alvas@ubi:~$ python
Python 2.7.11 (default, Dec 15 2015, 16:46:19) 
[GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; sentences = brown.sents()[:100]
&gt;&gt;&gt; model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4)
&gt;&gt;&gt; word0 = sentences[0][0]
&gt;&gt;&gt; model[word0]
array([ 0.04985042,  0.02882229, -0.03625415, -0.03165979,  0.06049283,
        0.01207791,  0.04722737,  0.01984878, -0.03026265,  0.04485954], dtype=float32)
&gt;&gt;&gt; model = Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model[word0]
array([ 0.02596745,  0.01475067, -0.01839622, -0.01587902,  0.03079717,
        0.00586761,  0.02367715,  0.00930568, -0.01521437,  0.02213679,
        0.01043982, -0.00625582,  0.00173071, -0.00235749,  0.01309298,
        0.00710233, -0.02270884, -0.01477827,  0.01166443,  0.00283862], dtype=float32)
</code></pre>

<p><strong>Is it true then that the default random seed is fixed?</strong> If so, what is the default random seed number? Or is it because I'm testing on a small dataset? </p>

<p>If it's true that the the random seed is fixed and different runs on the same data returns the same vectors, a link to a canonical code or documentation would be much appreciated.  </p>
","python, random, gensim, word2vec, word-embedding","<p>Yes, default random seed is fixed to <code>1</code>, as described by the author in <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a>. Vectors for each word are initialised using a hash of the concatenation of word + str(seed).</p>
<p>Hashing function used, however, is Python’s rudimentary built in hash function and can produce different results if two machines differ in</p>
<ul>
<li>32 vs 64 bit, <a href=""https://stackoverflow.com/questions/16452252/does-pythons-hash-function-remain-identical-across-different-versions"">reference</a></li>
<li>python versions, <a href=""https://stackoverflow.com/questions/34058947/hashing-tuple-in-python-causing-different-results-in-different-systems"">reference</a></li>
<li>different Operating Systems/ Interpreters, <a href=""https://stackoverflow.com/questions/17192418/hash-function-in-python"">reference1</a>, <a href=""https://stackoverflow.com/questions/793761/built-in-python-hash-function"">reference2</a></li>
</ul>
<p>Above list is not exhaustive. Does it cover your question though?</p>
<p><strong>EDIT</strong></p>
<p>If you want to ensure consistency, you can provide your own hashing function as an argument in word2vec</p>
<p>A very simple (and bad) example would be:</p>
<pre><code>def hash(astring):
   return ord(astring[0])

model = Word2Vec(sentences, size=10, window=5, min_count=5, workers=4, hashfxn=hash)

print model[sentences[0][0]]
</code></pre>
",11,15,9923,2016-01-16 20:05:51,https://stackoverflow.com/questions/34831551/ensure-the-gensim-generate-the-same-word2vec-model-for-different-runs-on-the-sam
How can I run this gensim code? Do I need some text files?,"<p>I was going through this website yesterday (<a href=""http://rutumulkar.com/blog/2015/word2vec/"" rel=""nofollow"">http://rutumulkar.com/blog/2015/word2vec/</a>)  and the author made use of the file <code>text8-queen</code>. In his script I noticed that she did not specify the location of the file and I was wondering how was he able to run it? I am unable to run it? Is there a way to run this file? Thank you.</p>

<p>The script is as follows:</p>

<pre><code>import gensim.models
import time
time1 = time.time()

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)


modelbase = gensim.models.Word2Vec()
sentences2 = gensim.models.word2vec.Sentences(""text8-queen"")
modelbase.build_vocab(sentences2)
modelbase.train(sentences2)
modelbase.save_word2vec_format(""wordvectors/model-text8-queen-only"")
modelbase.accuracy(""questions-words.txt"")

model = gensim.models.Word2Vec()
sentences = gensim.models.word2vec.Sentences(""text8-rest"")
model.build_vocab(sentences)
model.train(sentences)
model.save_word2vec_format(""model-text8-rest"")
model.accuracy(""questions-words.txt"")

sentences2 = gensim.models.word2vec.Sentences(""text8-queen"")
model.update_vocab(sentences2)
model.train(sentences2)
model.save_word2vec_format(""wordvectors/model-text8-queen"")
model.accuracy(""questions-words.txt"")

model1 = gensim.models.Word2Vec()
sentences = gensim.models.word2vec.Sentences(""text8-all"")
model1.build_vocab(sentences)
model1.train(sentences)
model1.save_word2vec_format(""wordvectors/model-text8-all"")
model1.accuracy(""questions-words.txt"")
print (""total time: %s"" % (time.time() - time1))
</code></pre>

<p>My question is in the in the line:  </p>

<pre><code>sentences = gensim.models.word2vec.Sentences(""text8-rest"")
</code></pre>

<p>how did the author call <code>text8-rest</code> and <code>text8-queen</code>? where should I put these text file (<code>text8-rest</code>, <code>text8-queen</code>) ? Do I have to specify the location of the text file or is python able to detect it?</p>
","python, gensim","<p>If you carefully read that tutorial, it says </p>

<blockquote>
  <p>NOTE: text8-rest, and text8-queen, and text8-all can be downloaded here: <a href=""http://rutumulkar.com/data/onlinew2v/text8-files.zip"" rel=""nofollow"">http://rutumulkar.com/data/onlinew2v/text8-files.zip</a>.</p>
</blockquote>
",2,0,408,2016-02-01 01:33:53,https://stackoverflow.com/questions/35121779/how-can-i-run-this-gensim-code-do-i-need-some-text-files
Print Wikipedia Article Title from Gensim WikiCorpus,"<p>I believe my question is easy, but I'm very new to python and I think that is blinding me a bit.</p>

<p>I've downloaded a Wikipedia dump as explained under ""Preparing the Corpus"" here: <a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow"">https://radimrehurek.com/gensim/wiki.html</a>.  Then I ran the following lines of code:</p>

<pre><code>import gensim

# these next two lines take around 16 hours
wikiDocs = gensim.corpora.wikicorpus.WikiCorpus('enwiki-latest-pages-articles.xml.bz2')
gensim.corpora.MmCorpus.serialize('wiki_en_vocab200k', wikiDocs)
</code></pre>

<p>These lines of code are taken from the link above.  Now, in a separate script I've done some text analysis.  The result of that text analysis is a number representing the index of a particular article in the wikiDocs corpus.  The problem, I don't know how to print out the text of that article.  The obvious thing to try is:</p>

<pre><code>wikiDocs[index_of_article]
</code></pre>

<p>but that returns the error</p>

<pre><code>TypeError: 'WikiCorpus' object does not support indexing
</code></pre>

<p>I've tried a few other things but I'm stuck.  Thanks for any help.</p>
","python, nlp, wikipedia, gensim, text-analysis","<p>It's not actually such an easy quesion, the reason why it didn't work is that <code>WikiCorpus</code> isn't an iterator, it's just a class with a few functions for saving and loading. You can see the functions buy typing <code>WikiCorpus.</code> and pressing TAB into IPython (this shows the options for TAB-completion:</p>

<pre><code>In [8]: wikiDocs = gensim.corpora.wikicorpus.WikiCorpus.
gensim.corpora.wikicorpus.WikiCorpus.get_texts    gensim.corpora.wikicorpus.WikiCorpus.load         gensim.corpora.wikicorpus.WikiCorpus.save_corpus
gensim.corpora.wikicorpus.WikiCorpus.getstream    gensim.corpora.wikicorpus.WikiCorpus.save
</code></pre>

<p>It looks like we want <code>get_texts</code>, this will probably return an iterator rather than a list though, (iterators don't directly support indexing either) so you'll have to use</p>

<pre><code>list(wikidocs.get_texts())[i]
</code></pre>

<p>or </p>

<pre><code>from itertools import islice
next(islice(wikidocs.get_texts(),i,i+1))
</code></pre>
",3,2,1487,2016-02-24 01:22:29,https://stackoverflow.com/questions/35591567/print-wikipedia-article-title-from-gensim-wikicorpus
gensim word2vec: Find number of words in vocabulary,"<p>After training a word2vec model using python <a href=""http://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">gensim</a>, how do you find the number of words in the model's vocabulary?</p>
","python, neural-network, nlp, gensim, word2vec","<p>In recent versions, the <code>model.wv</code> property holds the words-and-vectors, and can itself can report a length – the number of words it contains. So if <code>w2v_model</code> is your <code>Word2Vec</code> (or <code>Doc2Vec</code> or <code>FastText</code>) model, it's enough to just do:</p>
<pre class=""lang-py prettyprint-override""><code>vocab_len = len(w2v_model.wv)
</code></pre>
<p>If your model is just a raw set of word-vectors, like a <code>KeyedVectors</code> instance rather than a full <code>Word2Vec</code>/etc model, it's just:</p>
<pre class=""lang-py prettyprint-override""><code>vocab_len = len(kv_model)
</code></pre>
<p>Other useful internals in Gensim 4.0+ include <code>model.wv.index_to_key</code>, a plain list of the key (word) in each index position, and <code>model.wv.key_to_index</code>, a plain dict mapping keys (words) to their index positions.</p>
<p>In pre-4.0 versions, the vocabulary was in the <code>vocab</code> field of the Word2Vec model's <code>wv</code> property, as a dictionary, with the keys being each token (word). So there it was just the usual Python for getting a dictionary's length:</p>
<pre><code>len(w2v_model.wv.vocab)
</code></pre>
<p>In very-old gensim versions before 0.13 <code>vocab</code> appeared directly on the model. So way back then you would use <code>w2v_model.vocab</code> instead of <code>w2v_model.wv.vocab</code>.</p>
<p>But if you're still using anything from before Gensim 4.0, you should definitely upgrade! There are big memory &amp; performance improvements, and the changes required in calling code are relatively small – some renamings &amp; moves, covered in the <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes"" rel=""noreferrer"">4.0 Migration Notes</a>.</p>
",110,55,97308,2016-02-24 07:39:48,https://stackoverflow.com/questions/35596031/gensim-word2vec-find-number-of-words-in-vocabulary
Memory efficient LDA training using gensim library,"<p>Today I just started writing an script which trains LDA models on large corpora (minimum 30M sentences) using gensim library.
Here is the current code that I am using:</p>

<pre><code>from gensim import corpora, models, similarities, matutils

def train_model(fname):
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    dictionary = corpora.Dictionary(line.lower().split() for line in open(fname))
    print ""DOC2BOW""
    corpus = [dictionary.doc2bow(line.lower().split()) for line in open(fname)]

    print ""running LDA""
    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100, update_every=1, chunksize=10000, asses=1)
</code></pre>

<p>running this script on a small corpus (2M sentences) I realized that it needs about 7GB of RAM.
And when I try to run it on the larger corpora, it fails because of the memory issue.
The problem is obviously due to the fact that I am loading the corpus using this command:</p>

<pre><code>corpus = [dictionary.doc2bow(line.lower().split()) for line in open(fname)]
</code></pre>

<p>But, I think there is no other way because I would need it for calling the LdaModel() method:</p>

<pre><code>lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100, update_every=1, chunksize=10000, asses=1)
</code></pre>

<p>I searched for a solution to this problem but I could not find anything helpful.
I would imagine that it should be a common problem since we mostly train the models on very large corpora (usually wikipedia documents). So, it should be already a solution for it.</p>

<p>Any ideas about this issue and the solution for it?</p>
","python, nlp, gensim, lda, topic-modeling","<p>Consider wrapping your <code>corpus</code> up as an iterable and passing that instead of a list (a generator will not work).</p>

<p>From <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-tutorial"" rel=""nofollow noreferrer"">the tutorial</a>:</p>

<pre><code>class MyCorpus(object):
    def __iter__(self):
       for line in open(fname):
            # assume there's one document per line, tokens separated by whitespace
            yield dictionary.doc2bow(line.lower().split())

corpus = MyCorpus()
lda = gensim.models.ldamodel.LdaModel(corpus=corpus, 
                                      id2word=dictionary,
                                      num_topics=100,
                                      update_every=1,
                                      chunksize=10000,
                                      passes=1)
</code></pre>

<p>Additionally, Gensim has several different corpus formats readily available, which can be found in the <a href=""http://radimrehurek.com/gensim/apiref.html"" rel=""nofollow noreferrer"">API reference</a>.  You might consider using <code>TextCorpus</code>, which should fit your format nicely already:</p>

<pre><code>corpus = gensim.corpora.TextCorpus(fname)
lda = gensim.models.ldamodel.LdaModel(corpus=corpus, 
                                      id2word=corpus.dictionary, # TextCorpus can build the dictionary for you
                                      num_topics=100,
                                      update_every=1,
                                      chunksize=10000,
                                      passes=1)
</code></pre>
",3,3,1756,2016-02-24 17:36:39,https://stackoverflow.com/questions/35609171/memory-efficient-lda-training-using-gensim-library
Doc2Vec and PySpark: Gensim Doc2vec over DeepDist,"<p>I am looking at the <code>DeepDist</code> (<a href=""http://deepdist.com"" rel=""nofollow noreferrer"">link</a>) module and thinking to combine it with <code>Gensim</code>'s <code>Doc2Vec</code> API to train paragraph vectors on <code>PySpark</code>. The link actually provides with the following clean example for how to do it for <code>Gensim</code>'s <code>Word2Vec</code> model:</p>

<pre class=""lang-py prettyprint-override""><code>from deepdist import DeepDist
from gensim.models.word2vec import Word2Vec
from pyspark import SparkContext

sc = SparkContext()
corpus = sc.textFile('enwiki').map(lambda s: s.split())

def gradient(model, sentences):  # executes on workers
    syn0, syn1 = model.syn0.copy(), model.syn1.copy()   # previous weights
    model.train(sentences)
    return {'syn0': model.syn0 - syn0, 'syn1': model.syn1 - syn1}

def descent(model, update):      # executes on master
    model.syn0 += update['syn0']
    model.syn1 += update['syn1']

with DeepDist(Word2Vec(corpus.collect()) as dd:
    dd.train(corpus, gradient, descent)
    print dd.model.most_similar(positive=['woman', 'king'], negative=['man']) 
</code></pre>

<p>To my understanding, <code>DeepDist</code> is distributing the work of gradient descent into workers in batches, and the recombining them and updating at master. If I replace <code>Word2Vec</code> with <code>Doc2Vec</code>, there should be the document vectors that are being trained with the word vectors.</p>

<p>So I looked into the source code of <code>gensim.models.doc2vec</code> (<a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/doc2vec.py"" rel=""nofollow noreferrer"">link</a>). There are the following fields in the <code>Doc2Vec</code> model instance:</p>

<ol>
<li><code>model.syn0</code></li>
<li><code>model.syn0_lockf</code></li>
<li><code>model.docvecs.doctag_syn0</code></li>
<li><code>model.docvecs.doctag_syn0_lockf</code></li>
</ol>

<p>Comparing with the source code of <code>gensim.models.word2vec</code> (<a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">link</a>), the following fields went missing in <code>Doc2Vec</code> model:</p>

<ol start=""3"">
<li><code>model.syn1</code></li>
<li><code>model.syn1neg</code></li>
</ol>

<p>I think I do not touch the <code>lockf</code> vectors because they seem to be used after the training is done when new data points come in. Therefore my code should be something like </p>

<pre class=""lang-py prettyprint-override""><code>from deepdist import DeepDist
from gensim.models.doc2vec import Doc2Vec, LabeledSentence
from pyspark import SparkContext

sc = SparkContext()

# assume my dataset is in format 10-char-id followed by doc content
# 1 line per doc
corpus = sc.textFile('data_set').map(
    lambda s: LabeledSentence(words=s[10:].split(),labels=s[:10])
)

def gradient(model, sentence):  # executes on workers
    syn0, doctag_syn0 = model.syn0.copy(), model.docvecs.doctag_syn0.copy()   # previous weights
    model.train(sentence)
    return {'syn0': model.syn0 - syn0, 'doctag_syn0': model.docvecs.doctag_syn0 - doctag_syn0}

def descent(model, update):      # executes on master
    model.syn0 += update['syn0']
    model.docvecs.doctag_syn0 += update['doctag_syn0']

with DeepDist(Doc2Vec(corpus.collect()) as dd:
    dd.train(corpus, gradient, descent)
    print dd.model.most_similar(positive=['woman', 'king'], negative=['man']) 
</code></pre>

<p>Am I missing anything important here? For example:</p>

<ol>
<li>Should I care about <code>model.syn1</code> at all? What do they mean after all?</li>
<li>Am I right that <code>model.*_lockf</code> is the locked matrices after training?</li>
<li>Is it ok that I use <code>lambda s: LabeledSentence(words=s[10:].split(),labels=s[:10]</code> to parse my dataset, assuming I have each document in one line, prefixed by a 0-padded 10-digit id?</li>
</ol>

<p>Any suggestion/contribution are very appreciated. I will write up a blog post to summarize the result, mentioning contributors here, potentially to help others train Doc2Vec models on scaled distributed systems without spending much dev time trying to solve what I am solving now.</p>

<p>Thanks</p>

<hr>

<p>Update 06/13/2018</p>

<p>My apologies as I did not get to implement this. But there are better options nowaday, and <code>DeepDist</code> haven't been maintained for awhile now. Please read comment below.</p>

<p>If you insist on trying out my idea at the moment, be reminded you are proceeding with your own risk. Also, if someone knows that <code>DeepDist</code> still works, please report back in comments. It would help other readers. </p>
","apache-spark, pyspark, gensim, word2vec","<p>To avoid this question from remaining shown as open, here is how the asker resolved the situation:</p>

<blockquote>
  <p>I did not get to implement this, until it's too late that I didn't think it would work. DeepDist uses Flask app in backend to interact with Spark web interface. Since it wasn't maintained anymore, Spark's update very likely broke it already. If you are looking for Doc2Vec training in Spark, just go for Deeplearning4J(deeplearning4j.org/doc2vec#)</p>
</blockquote>
",1,11,3921,2016-02-25 00:40:27,https://stackoverflow.com/questions/35616088/doc2vec-and-pyspark-gensim-doc2vec-over-deepdist
Gensim word2vec on predefined dictionary and word-indices data,"<p>I need to train a word2vec representation on tweets using gensim. Unlike most tutorials and code I've seen on gensim my data is not raw, but has already been preprocessed. I have a dictionary in a text document containing 65k words (incl. an ""unknown"" token and a EOL token) and the tweets are saved as a numpy matrix with indices into this dictionary. A simple example of the data format can be seen below:</p>

<p><strong>dict.txt</strong></p>

<pre><code>you
love
this
code
</code></pre>

<p><strong>tweets (5 is unknown and 6 is EOL)</strong></p>

<pre><code>[[0, 1, 2, 3, 6],
 [3, 5, 5, 1, 6],
 [0, 1, 3, 6, 6]]
</code></pre>

<p>I'm unsure how I should handle the indices representation. An easy way is just to convert the list of indices to a list of strings (i.e. [0, 1, 2, 3, 6] -> ['0', '1', '2', '3', '6']) as I read it into the word2vec model. However, this must be inefficient as gensim then will try to look up the internal index used for e.g. '2'.</p>

<p>How do I load this data and create the word2vec representation in an efficient manner using gensim? </p>
","python, nlp, gensim, word2vec","<p>The normal way to initialize a <code>Word2Vec</code> model in <code>gensim</code> is [1]</p>

<pre><code>model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
</code></pre>

<p>The question is, what is <code>sentences</code>? <code>sentences</code> is supposed to be an iterator of iterables of words/tokens. It is just like the numpy matrix you have, but each row can be of different lengths.</p>

<p>If you look at the documentation for <code>gensim.models.word2vec.LineSentence</code>, it gives you a way of loading a text files as sentences directly. As a hint, according to the documentation, it takes</p>

<blockquote>
  <p>one sentence = one line; words already preprocessed and separated by whitespace.</p>
</blockquote>

<p>When it says <code>words already preprocessed</code>, it is referring to lower-casing, stemming, stopword filtering and all other text cleansing processes. In your case you wouldn't want <code>5</code> and <code>6</code> to be in your list of sentences, so you do need to filter them out.</p>

<p>Given that you already have the numpy matrix, assuming each row is a sentence, it is better to then cast it into a 2d array and filter out all <code>5</code> and <code>6</code>. The resultant 2d array can be used directly as the <code>sentences</code> argument to initialize the model. The only catch is that when you want to query the model after training, you need to input the indices instead of the tokens.</p>

<p>Now one question you have is if the model takes integer directly. In the <code>Python</code> version it doesn't check for type, and just passes the unique tokens around. Your unique indices in that case will work fine. But most of the time you would want to use the C-Extended routine to train your model, which is a big deal because it can give 70x performance. [2] I imagine in that case the C code may check for string type, which means there is a string-to-index mapping stored.</p>

<p>Is this inefficient? I think not, because the strings you have are numbers, which are in generally much shorter than the real token they represent (assuming they are compact indices from <code>0</code>). Therefore models will be smaller in size, which will save some effort in serialization and deserialization of the model at the end. You essentially have encoded the input tokens in a shorter string format and separated it from the <code>word2vec</code> training, and <code>word2vec</code> model do not and need not know this encoding happened before training.</p>

<p>My philosophy is <code>try the simplest way first</code>. I would just throw a sample test input of integers to the model and see what can go wrong. Hope it helps.</p>

<p>[1] <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a></p>

<p>[2] <a href=""http://rare-technologies.com/word2vec-in-python-part-two-optimizing/"" rel=""noreferrer"">http://rare-technologies.com/word2vec-in-python-part-two-optimizing/</a></p>
",9,11,3972,2016-03-01 11:20:22,https://stackoverflow.com/questions/35721503/gensim-word2vec-on-predefined-dictionary-and-word-indices-data
word2vec how to get words from vectors?,"<p>I use ANN to predict words from words. The input and output are all words vectors. I do not know how to get words from the output of ANN. By the way,  it's gensim I am using</p>
","machine-learning, gensim, word2vec","<p>You can find cosine similarity of the vector with all other word-vectors to find the nearest neighbors of your vector.</p>

<p>The nearest neighbor search on an n-dimensional space, can be brute force, or you can use libraries like FLANN, Annoy, scikit-kdtree to do it more efficiently.</p>

<p><strong>update</strong></p>

<p>Sharing a gist demonstrating the same:
<a href=""https://gist.github.com/kampta/139f710ca91ed5fabaf9e6616d2c762b"" rel=""nofollow"">https://gist.github.com/kampta/139f710ca91ed5fabaf9e6616d2c762b</a></p>
",1,3,1609,2016-03-10 10:48:09,https://stackoverflow.com/questions/35914287/word2vec-how-to-get-words-from-vectors
&#39;from gensim import test&#39; is not importing successfully,"<p>I installed <a href=""http://radimrehurek.com/gensim"" rel=""nofollow"">gensim</a>, Python library.
I executed the command </p>

<pre><code>Import gensim
</code></pre>

<p>It executed without any error. 
Then I tried to import test from gensim using the command </p>

<pre><code>from gensim import test
</code></pre>

<p>and it showed the following error</p>

<blockquote>
  <p>Traceback (most recent call last):
    File """", line 1, in 
      from gensim import test
  ImportError: cannot import name 'test'</p>
</blockquote>

<p>Python site-packages had gensim folder in that. 
Any help would be highly appreciated. </p>
","python, python-3.4, gensim","<p>As it says: <code>cannot import name 'test'</code><br>
That means that <code>test</code> is not in <code>gensim</code> package.</p>

<p>You can see package's modules with:</p>

<pre><code>import gensim
print(gensim.__all__) # when package provide a __all__
</code></pre>

<p>or</p>

<pre><code>import gensim
import pkgutil
modules = pkgutil.iter_modules(gensim.__path__)
for module in modules:
    print(module[1])
</code></pre>

<p>Edit:</p>

<blockquote>
  <p>How can I verify gensim installation ?</p>
</blockquote>

<pre><code>try:
    import gensim
except NameError:
    print('gensim is not installed')
</code></pre>

<p>Side note: if you have a file or package named gensim, this will ne imported instead of the real package.</p>
",3,0,6330,2016-03-14 11:07:17,https://stackoverflow.com/questions/35985851/from-gensim-import-test-is-not-importing-successfully
GenSim Word2Vec unexpectedly pruning,"<p>My objective is to find a vector representation of phrases. Below is the code I have, that works partially for bigrams using the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">Word2Vec</a> model provided by the <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow"">GenSim</a> library.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import word2vec

def bigram2vec(unigrams, bigram_to_search):
    bigrams = Phrases(unigrams)
    model = word2vec.Word2Vec(sentences=bigrams[unigrams], size=20, min_count=1, window=4, sg=1, hs=1, negative=0, trim_rule=None)
    if bigram_to_search in model.vocab.keys():
        return model[bigram_to_search]
    else:
        return None
</code></pre>

<p>The problem is that the Word2Vec model is seemingly doing automatic pruning of some of the bigrams, i.e. <code>len(model.vocab.keys()) != len(bigrams.vocab.keys())</code>. I've tried adjusting various parameters such as <code>trim_rule</code>, <code>min_count</code>, but they don't seem to affect the pruning.</p>

<p>PS - I am aware that bigrams to look up need to be represented using underscore instead of space, i.e. proper way to call my function would be <code>bigram2vec(unigrams, 'this_report')</code></p>
","gensim, word2vec","<p>Thanks to further clarification at the <a href=""https://groups.google.com/d/msg/gensim/lsDU42VksJA/ZasFdb21HAAJ"" rel=""nofollow"">GenSim support forum</a>, the solution is to set the appropriate <code>min_count</code> and <code>threshold</code> values for the <code>Phrases</code> being generated (see <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow"">documentation</a> for details about these parameters in the <code>Phrases</code> class). The corrected solution code is below.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import word2vec, Phrases

def bigram2vec(unigrams, bigram_to_search):
    bigrams = Phrases(unigrams, min_count=1, threshold=0.1)
    model = word2vec.Word2Vec(sentences=bigrams[unigrams], size=20, min_count=1, trim_rule=None)
    if bigram_to_search in model.vocab.keys():
        return model[bigram_to_search]
    else:
        return []
</code></pre>
",0,0,417,2016-03-15 13:47:16,https://stackoverflow.com/questions/36013137/gensim-word2vec-unexpectedly-pruning
What meaning does the length of a Word2vec vector have?,"<p>I am using Word2vec through <a href=""https://radimrehurek.com/gensim/"" rel=""noreferrer""><em>gensim</em></a> with Google's pretrained vectors trained on Google News. I have noticed that the word vectors I can access by doing direct index lookups on the <code>Word2Vec</code> object are not unit vectors:</p>

<pre><code>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; w2v = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
&gt;&gt;&gt; king_vector = w2v['king']
&gt;&gt;&gt; numpy.linalg.norm(king_vector)
2.9022589
</code></pre>

<p>However, in the <a href=""https://github.com/piskvorky/gensim/blob/0.12.4/gensim/models/word2vec.py#L1153-L1213"" rel=""noreferrer""><code>most_similar</code></a> method, these non-unit vectors are not used; instead, normalised versions are used from the undocumented <code>.syn0norm</code> property, which contains only unit vectors:</p>

<pre><code>&gt;&gt;&gt; w2v.init_sims()
&gt;&gt;&gt; unit_king_vector = w2v.syn0norm[w2v.vocab['king'].index]
&gt;&gt;&gt; numpy.linalg.norm(unit_king_vector)
0.99999994
</code></pre>

<p>The larger vector is just a scaled up version of the unit vector:</p>

<pre><code>&gt;&gt;&gt; king_vector - numpy.linalg.norm(king_vector) * unit_king_vector
array([  0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,
         0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,
        -7.45058060e-09,   0.00000000e+00,   3.72529030e-09,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
        ... (some lines omitted) ...
        -1.86264515e-09,  -3.72529030e-09,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)
</code></pre>

<p>Given that word similarity comparisons in Word2Vec are done by <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""noreferrer"">cosine similarity</a>, it's not obvious to me what the lengths of the non-normalised vectors mean - although I assume they mean <em>something</em>, since gensim exposes them to me rather than only exposing the unit vectors in <code>.syn0norm</code>.</p>

<p>How are the lengths of these non-normalised Word2vec vectors generated, and what is their meaning? For what calculations does it make sense to use the normalised vectors, and when should I use the non-normalised ones?</p>
","python, nlp, gensim, word2vec","<p>I think the answer you are looking for is described in the 2015 paper <a href=""https://arxiv.org/pdf/1508.02297.pdf"" rel=""noreferrer"">Measuring Word Significance
using
Distributed Representations of Words</a> by Adriaan Schakel and Benjamin Wilson. The key points:</p>

<blockquote>
  <p>When a word appears
  in different contexts, its vector gets moved in
  different directions during updates. The final vector
  then represents some sort of weighted average
  over the various contexts. Averaging over vectors
  that point in different directions typically results in
  a vector that gets shorter with increasing number
  of different contexts in which the word appears.
  For words to be used in many different contexts,
  they must carry little meaning. Prime examples of
  such insignificant words are high-frequency stop
  words, which are indeed represented by short vectors
  despite their high term frequencies ...</p>
</blockquote>

<hr>

<blockquote>
  <p>For given term frequency,
  the vector length is seen to take values only in a
  narrow interval. That interval initially shifts upwards
  with increasing frequency. Around a frequency
  of about 30, that trend reverses and the interval
  shifts downwards.</p>
  
  <p>...</p>
  
  <p>Both forces determining the length of a word
  vector are seen at work here. Small-frequency
  words tend to be used consistently, so that the
  more frequently such words appear, the longer
  their vectors. This tendency is reflected by the upwards
  trend in Fig. 3 at low frequencies. High-frequency
  words, on the other hand, tend to be
  used in many different contexts, the more so, the
  more frequently they occur. The averaging over
  an increasing number of different contexts shortens
  the vectors representing such words. This tendency
  is clearly reflected by the downwards trend
  in Fig. 3 at high frequencies, culminating in punctuation
  marks and stop words with short vectors at
  the very end.</p>
  
  <p>...</p>
  
  <p><a href=""https://i.sstatic.net/NI9je.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/NI9je.png"" alt=""Graph showing the trend described in the previous excerpt""></a></p>
  
  <p>Figure 3: Word vector length <em>v</em> versus term frequency
  <em>tf</em> of all words in the hep-th vocabulary.
  Note the logarithmic scale used on the frequency
  axis. The dark symbols denote bin means with the
  <i>k</i>th bin containing the frequencies in the interval
  [2<sup><i>k−1</i></sup>, 2<sup><i>k</i></sup> − 1] with <em>k</em> = 1, 2, 3, . . .. These means
  are included as a guide to the eye. The horizontal
  line indicates the length <em>v</em> = 1.37 of the mean
  vector</p>
</blockquote>

<hr>

<blockquote>
  <h3>4 Discussion</h3>
  
  <p>Most applications of distributed representations of
  words obtained through word2vec so far centered
  around semantics. A host of experiments have
  demonstrated the extent to which the direction of
  word vectors captures semantics. In this brief report,
  it was pointed out that not only the direction,
  but also the length of word vectors carries important
  information. Specifically, it was shown that
  word vector length furnishes, in combination with
  term frequency, a useful measure of word significance. </p>
</blockquote>
",35,32,11703,2016-03-16 11:31:27,https://stackoverflow.com/questions/36034454/what-meaning-does-the-length-of-a-word2vec-vector-have
what is (&#39;/tmp/text8&#39;) gensim,"<p>I am implementing the tutorial of gensim <a href=""http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/"" rel=""nofollow"">http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/</a> that includes the line 
    sentences = word2vec.Text8Corpus('/tmp/text8')
however when I run the program I get the error that text8 does not exist. Looking through the code I see that Text8Corpus is a method that accepts argument type object. The instructions indicate that it should be passed</p>

<p><a href=""http://mattmahoney.net/dc/text8.zip"" rel=""nofollow"">http://mattmahoney.net/dc/text8.zip</a></p>

<p>When I manually download this file and attempt to pass the resulting imbd uncompressed data set I am told that permissions denied. Does anyone have any insight into this problem ? Am I suppose to have downloaded the imdb dataset myself or was there suppose to be some pointers in the code that do it automatically ? </p>
","python, gensim","<p>Like he says in his description, you need to download and unzip the file to /tmp directory. you can do this like this:</p>

<pre><code> wget http://mattmahoney.net/dc/text8.zip -P /tmp
 unzip text8.zip
</code></pre>

<p>Now you should be ok ;)</p>
",5,0,4718,2016-03-22 16:43:32,https://stackoverflow.com/questions/36160322/what-is-tmp-text8-gensim
KeyError in Doc2Vec model even when min_count set to 1 during training,"<p>I'm using doc2vec with a corpus of about 1 million titles. To train the corpus, I'm using the following code: </p>

<pre><code>model = gensim.models.Doc2Vec(min_count=1, window=10, size=300, workers=4)
model.build_vocab(corpus)
for epoch in range(10):
    model.train(corpus)
</code></pre>

<p>Everything seems to train properly and I am able to infer a vector using titles.most_similar. </p>

<p>I encounter a problem, however, when I try to use the vectors. It seems as though some documents are missing from the final model! I.e.: </p>

<pre><code>model.docvecs['SENT_157000']
</code></pre>

<blockquote>
  <p>KeyError: 'SENT_157000'</p>
</blockquote>

<p>I checked the gensim forum and stackoverflow and the only suggestion I could find was to ensure that the min_count = 1. I did that but I'm still having this issue.</p>
","python, gensim, word2vec","<p>From <code>gensim</code>'s <code>Doc2Vec</code> <a href=""http://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow noreferrer"">documentation</a>, input to <code>Doc2Vec</code>  should be an iterator of <code>LabeledSentence</code> objects.</p>

<p>Your <code>corpus</code> variable needs to be constructed as follows:</p>

<pre><code>class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename

    def __iter__(self):
        for uid, line in enumerate(open(self.filename)):
            yield LabeledSentence(words=line.split(), labels=['SENT_%s' % uid])


corpus = LabeledLineSentence(filename)
</code></pre>

<p>followed by</p>

<pre><code>model.train(corpus)
</code></pre>
",0,0,726,2016-03-25 16:25:17,https://stackoverflow.com/questions/36223864/keyerror-in-doc2vec-model-even-when-min-count-set-to-1-during-training
How to map the word in data frame to integer ID with python-pandas and gensim?,"<p>Given such a data frame, including the item and corresponding review texts:</p>

<pre><code>item_id          review_text
B2JLCNJF16       i was attracted to this...
B0009VEM4U       great snippers...
</code></pre>

<p>I want to map the top <code>5000</code> most frequent word in <code>review_text</code>, so the resulting data frame should be like:</p>

<pre><code>item_id            review_text
B2JLCNJF16         1  2  3  4  5...
B0009VEM4U         6... #as the word ""snippers""  is out of the top 5000 most frequent word
</code></pre>

<p>Or, a bag-of-word vector is highly preferred:</p>

<pre><code>item_id            review_text
B2JLCNJF16         [1,1,1,1,1....]
B0009VEM4U         [0,0,0,0,0,1....] 
</code></pre>

<p>How can I do that? Thanks a lot!</p>

<p>EDIT:
I have tried @ayhan 's answer. Now I have successfully changed the review text to a <code>doc2bow</code> form:</p>

<pre><code>item_id            review_text
B2JLCNJF16         [(123,2),(130,3),(159,1)...]
B0009VEM4U         [(3,2),(110,2),(121,5)...]
</code></pre>

<p>It denotes the word of ID <code>123</code> has occurred <code>2</code> times in that document. Now I'd like to  transfer it to a vector like:</p>

<pre><code>[0,0,0,.....,2,0,0,0,....,3,0,0,0,......1...]
        #123rd         130th        159th
</code></pre>

<p>Do you how to do that? Thank you in advance!</p>
","python, pandas, gensim","<p>First, to get a list of words in every row:</p>

<pre><code>df[""review_text""] = df[""review_text""].map(lambda x: x.split(' '))
</code></pre>

<p>Now you can pass <code>df[""review_text""]</code> to gensim's Dictionary:</p>

<pre><code>from gensim import corpora
dictionary = corpora.Dictionary(df[""review_text""])
</code></pre>

<p>For the 5000 most frequent words, use filter_extremes method:</p>

<pre><code>dictionary.filter_extremes(no_below=1, no_above=1, keep_n=5000)
</code></pre>

<p>doc2bow method will get you the bag of words representation (word_id, frequency):</p>

<pre><code>df[""bow""] = df[""review_text""].map(dictionary.doc2bow)

0     [(1, 2), (3, 1), (5, 1), (11, 1), (12, 3), (18...
1     [(0, 3), (24, 1), (28, 1), (30, 1), (56, 1), (...
2     [(8, 1), (15, 1), (18, 2), (29, 1), (36, 2), (...
3     [(69, 1), (94, 1), (115, 1), (123, 1), (128, 1...
4     [(2, 1), (18, 4), (26, 1), (32, 1), (55, 1), (...
5     [(6, 1), (18, 1), (30, 1), (61, 1), (71, 1), (...
6     [(0, 5), (13, 1), (18, 6), (31, 1), (42, 1), (...
7     [(0, 10), (5, 1), (18, 1), (35, 1), (43, 1), (...
8     [(0, 24), (1, 4), (4, 2), (7, 1), (10, 1), (14...
9     [(0, 7), (18, 3), (30, 1), (32, 1), (34, 1), (...
10    [(0, 5), (9, 1), (18, 3), (19, 1), (21, 1), (2...
</code></pre>

<p>After getting the bag of words representation, you can concat the series in each row (probably not very efficient):</p>

<pre><code>df2 = pd.concat([pd.DataFrame(s).set_index(0) for s in df[""bow""]], axis=1).fillna(0).T.set_index(df.index)


    0   1   2   3   4   5   6   7   8   9   ... 728 729 730 731 732 733 734 735 736 737
0   0   2   0   1   0   1   0   0   0   0   ... 0   0   0   0   0   0   0   0   0   0
1   3   0   0   0   0   0   0   0   0   0   ... 0   0   0   0   0   0   0   0   0   0
2   0   0   0   0   0   0   0   0   1   0   ... 0   0   0   0   0   1   1   0   0   0
3   0   0   0   0   0   0   0   0   0   0   ... 0   0   0   0   0   0   0   0   0   0
4   0   0   1   0   0   0   0   0   0   0   ... 0   0   0   0   0   1   0   0   1   0
5   0   0   0   0   0   0   1   0   0   0   ... 0   0   0   1   0   0   0   0   0   0
6   5   0   0   0   0   0   0   0   0   0   ... 0   0   0   1   0   0   0   0   0   0
7   10  0   0   0   0   1   0   0   0   0   ... 0   0   0   0   0   0   0   1   0   0
8   24  4   0   0   2   0   0   1   0   0   ... 1   1   2   0   1   3   1   0   1   0
9   7   0   0   0   0   0   0   0   0   0   ... 0   0   0   0   0   0   0   0   0   1
10  5   0   0   0   0   0   0   0   0   1   ... 0   0   0   0   0   0   0   0   0   0
</code></pre>
",8,4,2951,2016-03-27 17:20:41,https://stackoverflow.com/questions/36250297/how-to-map-the-word-in-data-frame-to-integer-id-with-python-pandas-and-gensim
ImportError: cannot import name BytesIO on eclipse,"<p>I am getting the following error and am just not able to figure out why gensim cant be imported. I tried reimporting gensim again by creating virtual environment but that didnt work as well. 
I am new to python, please be generous.</p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\Tejasvi\workspace\major project\Tag Recommendation\test.py"", line 6, in &lt;module&gt;
import gensim
File ""C:\Python27\lib\site-packages\gensim\__init__.py"", line 6, in &lt;module&gt;
from gensim import parsing, matutils, interfaces, corpora, models, similarities, summarization
File ""C:\Python27\lib\site-packages\gensim\models\__init__.py"", line 18, in &lt;module&gt;
from . import wrappers
File ""C:\Python27\lib\site-packages\gensim\models\wrappers\__init__.py"", line 5, in &lt;module&gt;
from .ldamallet import LdaMallet
File ""C:\Python27\lib\site-packages\gensim\models\wrappers\ldamallet.py"", line 40, in &lt;module&gt;
from smart_open import smart_open
File ""C:\Python27\lib\site-packages\smart_open\__init__.py"", line 1, in &lt;module&gt;
from .smart_open_lib import *
File ""C:\Python27\lib\site-packages\smart_open\smart_open_lib.py"", line 34, in &lt;module&gt;
from boto.compat import BytesIO, urlsplit, six
ImportError: cannot import name BytesIO
</code></pre>

<p>This is my code:</p>

<pre><code>import string
import re
import gensim
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
stop_words.update(['.', ',', '""', ""'"", '?', '!', ':', ';', '(', ')', '[',    ']',
'{', '}', 'lt', 'gt','xA', '/', 'lt'])

outFile = open('C:/Users/Tejasvi/Desktop/major/preprocessed/c#.txt', 'w')

with open('C:/Users/Tejasvi/Desktop/major/3/c#.txt') as f:
    for line in f:
        new_str = re.sub('[^a-zA-Z0-9\n\.]', ' ', line)
        new_string = ' '.join([w for w in new_str.split() if len(w)&gt;2])
        for c in string.punctuation:
            new_string=new_string.replace(c,"""")
            for w in new_string.split():
                if w.lower() not in stop_words:
                    outFile.write(w)
                    outFile.write("" "")
outFile.close()

from gensim import corpora
with open ('C:/Users/Tejasvi/Desktop/major/preprocessed/c#.txt', 'r') as f:
    for line in f:
        dictionary = corpora.Dictionary(line.strip().split())
</code></pre>

<p>here is the output of pip freeze</p>

<pre><code>-curses==2.2
alabaster==0.7.6
alembic==0.7.6
astroid==1.3.6
astropy==1.0.3
Babel==1.3
backports.datetime-timestamp==1.0.2.dev0
backports.functools-lru-cache==1.0.2.dev0
backports.method-request==1.0.1.dev0
backports.shutil-get-terminal-size==1.0.0
backports.ssl-match-hostname==3.4.0.2
Beaker==1.7.0
BeautifulSoup==3.2.1
beautifulsoup4==4.3.2
blinker==1.4.dev0
blosc==1.2.7
bloscpack==0.7.2
boto==2.24.0
Bottleneck==1.0.0
bz2file==0.98
CacheControl==0.11.5
cchardet==0.3.5
cdecimal==2.3
certifi==2015.4.28
cffi==1.1.2
chardet==2.3.0
colorama==0.3.3
configobj==5.0.6+xy.1
configparser==3.5.0b2
cov-core==1.15.0
coverage==3.7.1
cryptography==1.0.dev1
cssselect==0.9.1
cx-Freeze==4.3.4
cyordereddict==0.2.3.dev7
datrie==0.7.1.dev37
decorator==3.4.0
distlib==0.2.0
docutils==0.12
ecdsa==0.13.1.dev0
ed25519ll==0.6
enum34==1.0.4
faulthandler==2.4
formlayout==1.0.15
funcsigs==0.4
futures==3.0.3
gensim==0.12.4
gevent==1.0.2
gevent-websocket==0.9.5
GraphLab-Create==0.9.1
greenlet==0.4.7
grin==1.2.1+xy1
guidata==1.6.1
guiqwt==2.3.2
h5py==2.5.0
html5lib==0.99999
httpretty==0.8.10
idna==2.1.dev1
ipaddress==1.0.7
ipdb==0.8.1
ipdbplugin==1.4.2
ipython==2.4.1
jaraco.apt==1.0
jaraco.classes==1.2
jaraco.collections==1.1
jaraco.context==1.3
jaraco.functools==1.3
jaraco.structures==1.0
jaraco.text==1.4
jaraco.ui==1.3.1
jaraco.windows==3.4
jedi==0.9.0
Jinja2==2.7.3
keyring==5.3
lda==1.0.3
libnacl==1.4.3
librato-metrics==0.4.9
linecache2==1.0.0
lockfile==0.10.2.post7
logilab-common==0.63.2
lxml==3.4.4
mahotas==1.3.0
Mako==1.0.1
MarkupSafe==0.23
matplotlib==1.4.3
mixpanel-py==3.1.1
mock==1.0.1
modernize==0.4
more-itertools==2.3.dev0
ndg-httpsclient==0.4.0
netifaces==0.10.4
nltk==3.1
nose==1.3.7
nose-cov==1.6
nose-fixes==1.3
numexpr==2.4.3
numpy==1.10.4
numpydoc==0.6.dev0
oauthlib==0.7.3.dev0
objgraph==2.0.1.dev0
packaging==15.2
pandas==0.16.2
paramiko==1.15.2
pathlib==1.0.1
patsy==0.3.0
pbr==1.8.1
pep8==1.6.2
Pillow==2.8.2
ply==3.6
prettytable==0.7.2
psutil==1.1.3
psycopg2==2.6.1
py==1.4.30
py2exe==0.6.9
pyasn1==0.1.8
pyasn1-modules==0.0.6
PyAudio==0.2.8
pycparser==2.14
pycrypto==2.6.1
pyemf==2.0.0
pyflakes==0.9.2
Pygments==2.0.2
PyICU==1.9.2+xy.1
PyJWT==1.3.1.dev2
pylint==1.4.3
pyMinuit==1.2.1
PyOpenGL==3.1.0
PyOpenGL-accelerate==3.1.0
pyOpenSSL==0.15.1
pyparsing==2.0.3
PyQt4==4.11.3
pyreadline==2.0.6+xy.1
PyStemmer==1.3.0
python-dateutil==2.4.2
pytz==2015.4
pywin==0.3.1
pywin32==219
PyYAML==3.11
pyzmq==14.7.0
reportlab==3.2.0
requests==2.8.1
requests-oauthlib==0.5.0
rope==0.10.2
sampy==1.2.1
scandir==1.1.1.dev7
scikit-learn==0.17
scipy==0.15.1 
scp==0.10.2
singledispatch==3.4.0.3
six==1.9.0
smart-open==1.3.2
snowballstemmer==1.2.1.dev1
Sphinx==1.3.2
sphinx-rtd-theme==0.1.8
sphinxcontrib-plantuml==0.6
spyder==2.3.5.2
SQLAlchemy==1.0.6
statsmodels==0.6.1
stop-words==2015.2.23.1
tables==3.2.0
Tempita==0.5.2
textmining==1.0
tornado==3.2.1
traceback2==1.4.0
ujson==1.33
unittest2==1.0.1
urllib3==1.10.4
veusz==1.23.1
virtualenv==13.0.3
virtualenvwrapper-win==1.2.1
ViTables==2.2a1
wheel==0.24.0
Whoosh==2.7.0
wincertstore==0.2
wsaccel==0.6.2
wxPython==2.8.12.1
wxPython-common==2.8.12.1
yappi==0.94
yg.lockfile==2.0
zc.lockfile==1.1.0
</code></pre>

<p>I also checked if I had my own version of io.py but it doesn't exist.</p>
","python, lda, gensim, topic-modeling","<p>I had a similar error message with this version of boto:</p>

<pre><code>&gt;&gt;&gt; import boto
&gt;&gt;&gt; boto.__version__
'2.20.1'
</code></pre>

<p>After solving the issue (removing the old version that was hiding the new one) I managed to load gensim:</p>

<pre><code>&gt;&gt;&gt; import boto
&gt;&gt;&gt; boto.__version__
'2.39.0'
&gt;&gt;&gt; import gensim
&gt;&gt;&gt; 
</code></pre>

<p>I suggest you try and update boto to a more recent version.</p>

<pre><code>pip install boto --upgrade
</code></pre>
",3,3,2165,2016-03-31 08:39:37,https://stackoverflow.com/questions/36328261/importerror-cannot-import-name-bytesio-on-eclipse
Can doc2vec be used if my text data is incrementally increasing?,"<p>I am new to Doc2vec use. In case I could get some advice before I start on it, it will save a LOT of time.
My data is an stream of text data (such as tweets) continuously coming in time. For clustering these tweets, I was thinking of using doc2vec to reduce the text content into a fixed size vector and use that to compare between documents. 
So in this case, the text data is getting accumulated over time, can this be still used with Doc2Vec, I may have to learn the model again and again (may be!) or could I use some large corpus such as Wikipedia or a large newscorpus to train the Doc2Vec model.</p>

<p>Any suggestions will help!</p>

<p>Thanks in Advance.</p>
","twitter, text, gensim, word2vec","<p>The gensim Doc2Vec class does <em>not</em> support adjusting the model with new documents, but it <em>can</em> 'infer' and report a vector for new documents, based on the model learned from an earlier bulk training. </p>

<p>So, you can use that new inferred vector to compare the new document to older ones, or feed it to a trained classifier, etc. </p>

<p>If new documents continue to arrive, and especially if the balance of topics/meaning in your documents drifts over time, you would likely at some point want to discard a model based on older data, and create a new model based on your larger (or more recent) data.</p>

<p>(Note that vectors from the old model and new model <em>won't</em> be directly comparable. Training sessions involve a lot of randomness, and the meanings of dimensions/directions in any one model are somewhat arbitrary. It's the relative positions of vectors, from within the same model, that has some interpretive power.)</p>
",1,0,790,2016-04-01 15:39:02,https://stackoverflow.com/questions/36360367/can-doc2vec-be-used-if-my-text-data-is-incrementally-increasing
Is it possible to use gensim doc2vec for classification,"<p>I have some training sentences generally of warning nature.  Now my goal is to predict weather incoming sentence is a warning message or not. I have gone through <a href=""https://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow"">Sentiment Analysis Using Doc2Vec</a> but according to my understanding it have not considered newly arriving sentence to predict if its positive or negative. </p>

<p>According to my experience I found that the output vector in <code>gensim.doc2vec</code> for each sentence is dependent on other sentences as well, which means we can not directly use the model to generate vector for newly arriving sentence. Please anyone help me with this. Thanks.</p>
","nlp, gensim, word2vec","<p>One way to generate new vectors is using the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec.infer_vector"" rel=""nofollow"">infer_vector()</a> function, which will generate a new vector based on a trained model. Since the model is frozen when you use this function, the new vector will be based on the existing sentence vectors, but not change them.</p>
",4,1,1477,2016-04-04 08:29:35,https://stackoverflow.com/questions/36397798/is-it-possible-to-use-gensim-doc2vec-for-classification
"An easy tutorial for a tool that supports text classification, clustering and topic modeling","<p>What is a text mining tool with some easy tutorials and active community? I found some popular but not sure which one to start with.  </p>
","weka, text-mining, gensim, topic-modeling, mallet","<p>I suggest <a href=""https://de.dariah.eu/tatom-intro"" rel=""nofollow"">TAToM</a> by Allan Riddell.</p>

<p>And there is a portal to more tutorials called  <a href=""https://fedora.clarin-d.uni-saarland.de/hub/kwsearch/topic%20model/0/"" rel=""nofollow"">TeLeMaCo</a> at the CLARIN centre at Saarland University.</p>
",1,1,241,2016-04-08 03:35:44,https://stackoverflow.com/questions/36491071/an-easy-tutorial-for-a-tool-that-supports-text-classification-clustering-and-to
Why Gensim doc2vec give AttributeError: &#39;list&#39; object has no attribute &#39;words&#39;?,"<p>I am trying to experiment gensim doc2vec, by using following code. As far as  I understand from tutorials, it should work. However it gives <strong>AttributeError: 'list' object has no attribute 'words'.</strong></p>

<pre><code>from gensim.models.doc2vec import LabeledSentence, Doc2Vec
document = LabeledSentence(words=['some', 'words', 'here'], tags=['SENT_1']) 
model = Doc2Vec(document, size = 100, window = 300, min_count = 10, workers=4)
</code></pre>

<p>So what did I do wrong? Any help please. Thank you. I am using python 3.5 and gensim 0.12.4</p>
","python-3.x, gensim, word2vec","<p>Input to <code>gensim.models.doc2vec</code> should be an <strong>iterator</strong> over the <code>LabeledSentence</code> (say a list object). Try:</p>

<pre><code>model = Doc2Vec([document], size = 100, window = 1, min_count = 1, workers=1)
</code></pre>

<p>I have reduced the <code>window</code> size, and <code>min_count</code> so that they make sense for the given input. Also go through this nice tutorial on <a href=""http://rare-technologies.com/doc2vec-tutorial/"" rel=""nofollow"">Doc2Vec</a>, if you haven't already.</p>
",4,10,7497,2016-04-08 21:55:54,https://stackoverflow.com/questions/36509957/why-gensim-doc2vec-give-attributeerror-list-object-has-no-attribute-words
"Word2Vec, most_similar(word1) returns same output on different runs","<p>I'm doing a small experiment where I have 2000 tweets as my input document. I train word2vec on this input tweets and then find the top 10 most similar words to a particular word - <code>w1</code>.</p>

<p>My concern is if I run word2vec 10 times (with same parameters) and inspect the top 10 most similar words to <code>w1</code>, gives me the same set of words (weights are also the same). </p>

<p>Now AFAIK word2vec initializes random weights at the beginning so why it's giving me the same output at different runs?</p>
","python, gensim, word2vec","<p>The default <code>seed</code> of gensim's word2vec is set to 1 as described in <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow"">documentation</a>. For different outputs, try varying the seed variable</p>

<pre><code>from gensim.models.word2vec import Word2Vec

sentences = ['a quick brown fox jumps over a dog'.split()]
model = Word2Vec(sentences, min_count=1, size=5, seed=1)
print model['fox']

Out: array([ 0.0629408 ,  0.07386301, -0.00788937, -0.09287976,  0.05859811], dtype=float32)

model = Word2Vec(sentences, min_count=1, size=5, seed=2)
print model['fox']

Out: array([-0.05617044,  0.00294411, -0.09557492,  0.02923537, -0.08322554], dtype=float32)
</code></pre>
",0,0,1128,2016-04-21 00:13:12,https://stackoverflow.com/questions/36757409/word2vec-most-similarword1-returns-same-output-on-different-runs
How to load pre-trained model with in gensim and train doc2vec with it?,"<p>I am having a ready to go word2vec model that I already trained. I have serialized it as a CSV file:</p>

<pre><code>word,  v0,     v1,     ..., vN
house, 0.1234, 0.4567, ..., 0.3461
car,   0.456,  0.677,  ..., 0.3461
</code></pre>

<p>What I'd like to know is how I can load that word vector model in <code>gensim</code> and use that to train a paragraph or doc2vec model.</p>

<p>This <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow"">Doc2Vec tutorial</a> says I can load a model in form of a ""<code># C text format</code>"" but I have no idea what that actually means. What is ""C text format"" in the first place but more important: </p>

<ul>
<li>How can I load my word2vec model and use it for doc2vec training?</li>
</ul>

<p>How do I build the vocabulary from my word2vec model?</p>
","python, gensim, word2vec, doc2vec","<p>Doc2Vec does not need word-vectors as an input: it will create any word-vectors that are needed during its own training. (And some modes, like pure DBOW – <code>dm=0, dbow_words=0</code> – don't use or train word-vectors at all.)</p>

<p>Seeding a Doc2Vec model with word-vectors might help or hurt; there's not much theory or published results to offer guidance. There's an experimental method on Word2Vec, <code>intersect_word2vec_format()</code>, that can merge word2vec-c-format vectors into a model with an existing vocabulary, but you'd need to review the source to really understand its assumptions:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/51753b95415bbc344ea6af671818277464905ea2/gensim/models/word2vec.py#L1140"" rel=""nofollow"">https://github.com/RaRe-Technologies/gensim/blob/51753b95415bbc344ea6af671818277464905ea2/gensim/models/word2vec.py#L1140</a></p>
",1,1,2766,2016-04-23 18:52:33,https://stackoverflow.com/questions/36815038/how-to-load-pre-trained-model-with-in-gensim-and-train-doc2vec-with-it
How to interpret gensim topics properly?,"<p>I thought this may have been discussed before, but somehow I couldn't find answers, so here it is.</p>

<p>Below are the topics generated using gensim lsi from some customer survey. My questions are:</p>

<ol>
<li>what does the minus and plus signs in front of the words mean?</li>
<li>here I generated 5 topics and I could have generated more. how do I determine what might be the optimal number of topics? for example, maybe statistically after the third topic everything else will just be trivial.</li>
</ol>

<p>Any suggestions are appreciated.</p>

<p>0.527*""interest"" + 0.475*""lower"" + 0.376*""rates"" + 0.338*""rate"" + 0.324*""good"" + 0.257*""service""
0.671*""good"" + 0.586*""service"" + -0.254*""interest"" + -0.251*""lower"" + -0.159*""rate"" + -0.150*""rates""
0.600*""great"" + 0.351*""easy"" + 0.337*""rewards"" + 0.242*""use"" + -0.167*""service"" + 0.160*""like""
-0.503*""rates"" + 0.499*""rate"" + -0.39*""great"" + 0.364*""high"" + -0.289*""lower"" + 0.167*""easy""
-0.608*""great"" + 0.362*""easy"" + -0.303*""rate"" + 0.275*""rates"" + 0.244*""use"" + -0.227*""high""</p>
",gensim,"<p>The main mechanism behind LSI is singular value decomposition (SVD) on the term-document matrix (TDM).  I won't go into very much detail here but you can read about <a href=""https://en.wikipedia.org/wiki/Singular_value_decomposition"" rel=""nofollow noreferrer"">SVD on wikipedia</a> if you like.</p>

<p>The topics generated are linear combinations of terms.  These linear combinations are chosen (using SVD) to create a 'low-rank approximation' of the TDM.</p>

<p>The magnitude of the weights on the words can be thought of as importance: how much they matter in approximating the original TDM.  Or, more loosely, how important the topic is in describing the corpus upon which the TDM is based.  </p>

<p>The signs of the weights are only important relative to one another (you could, for example, multiply everything by -1 and if you correctly re-interpret the linear combinations, you'll arrive at the same interpretation).  If each document can be rated on the degree to which it has each topic, then the sign tells you which way the associated word pushes the document.  For example, in the output you provided, documents with many appearances of the words 'interest' and 'rates' should be low in the second topic.  Documents with many appearances of 'good' and 'service' on the other hand, should be high in the second topic.</p>

<p>As for determining the optimal number of topics, it is context specific, but it depends mostly on the size of corpus.  Here are some general guidelines (taken from <a href=""https://stackoverflow.com/a/9759218/2019896"">this answer</a>):</p>

<blockquote>
  <p>As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts. The actual number of dimensions that can be used is limited by the number of documents in the collection. Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents). However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.</p>
</blockquote>
",2,2,907,2016-04-27 20:35:46,https://stackoverflow.com/questions/36900356/how-to-interpret-gensim-topics-properly
How to get vocabulary word count from gensim word2vec?,"<p>I am using gensim word2vec package in python. I know how to get the vocabulary from the trained model. But how to get the word count for each word in vocabulary?</p>
","gensim, word2vec","<p>Each word in the vocabulary has an associated vocabulary object, which contains an index and a count.</p>

<pre><code>vocab_obj = w2v.vocab[""word""]
vocab_obj.count
</code></pre>

<p>Output for google news w2v model: 2998437</p>

<p>So to get the count for each word, you would iterate over all words and vocab objects in the vocabulary.</p>

<pre><code>for word, vocab_obj in w2v.vocab.items():
  #Do something with vocab_obj.count
</code></pre>
",32,16,34091,2016-05-12 15:12:23,https://stackoverflow.com/questions/37190989/how-to-get-vocabulary-word-count-from-gensim-word2vec
How to get word vectors from a gensim Doc2Vec?,"<p>I trained a gensim.models.doc2vec.Doc2Vec model<br>
d2v_model = Doc2Vec(sentences, size=100, window=8, min_count=5, workers=4)
and I can get document vectors by
docvec = d2v_model.docvecs[0]</p>

<p>How can I get word vectors from trained model ?</p>
","gensim, word2vec, doc2vec","<p>Doc2Vec inherits from Word2Vec, and thus you can access word vectors the same as in Word2Vec, directly by indexing the model:</p>

<pre><code>wv = d2v_model['apple']
</code></pre>

<p>Note, however, that a Doc2Vec training mode like pure DBOW (<code>dm=0</code>) doesn't need or create word vectors. (Pure DBOW still works pretty well and fast for many purposes!) If you do access word vectors from such a model, they'll just be the automatic randomly-initialized vectors, with no meaning. </p>

<p>Only when the Doc2Vec mode itself co-trains word-vectors, as in the DM mode (default <code>dm=1</code>) or when adding optional word-training to DBOW (<code>dm=0, dbow_words=1</code>), are word-vectors and doc-vectors both learned simultaneously. </p>
",14,4,9324,2016-05-19 23:49:10,https://stackoverflow.com/questions/37335842/how-to-get-word-vectors-from-a-gensim-doc2vec
computing the weight of LDA topic for all the documents in the corpus,"<p>I computed my LDA model, I retrieved my topics and now I am looking for the way to compute the weight/percentage of each topic on the corpus. Surprisingly I cannot find the way to do this, so far my code looks like:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

## Tokenizing
tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = stopwords.words('english')

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

import json
import nltk
import re
import pandas

appended_data = []

#for i in range(20014,2016):
#    df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
#    appended_data.append(df0)

for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)


appended_data = pandas.concat(appended_data)
# doc_set = df1.body

doc_set = appended_data.body

# list for tokenized documents in loop
texts = []

# loop through document list
for i in doc_set:

    # clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    # add tokens to list
    texts.append(stopped_tokens)

# turn our tokenized documents into a id &lt;-&gt; term dictionary
dictionary = corpora.Dictionary(texts)

# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

# generate LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=50)
ldamodel.save(""model.lda0"")  
</code></pre>

<p>So far, what I have seen in other forums is to do the following:</p>

<pre><code>from itertools import chain
print(type(doc_set))
print(len(doc_set))

for top in ldamodel.print_topics():
  print(top)
print

# Assinging the topics to the document in corpus
lda_corpus = ldamodel[corpus]
#print(lda_corpus)

# Find the threshold, let's set the threshold to be 1/#clusters,
# To prove that the threshold is sane, we average the sum of all probabilities:
scores = list(chain(*[[score for topic_id,score in topic] \
                     for topic in [doc for doc in lda_corpus]]))
print(sum(scores))
print(len(scores))
threshold = sum(scores)/len(scores)
print(threshold)

cluster1 = [j for i,j in zip(lda_corpus,doc_set) if i[0][1] &gt; threshold]
cluster2 = [j for i,j in zip(lda_corpus,doc_set) if i[1][1] &gt; threshold]
cluster3 = [j for i,j in zip(lda_corpus,doc_set) if i[2][1] &gt; threshold] 
</code></pre>

<p>However I get the error in the cluster two: <code>IndexError: list index out of range</code>. Any idea why?</p>
","python, lda, gensim, corpus","<p>You need to state a minimum probability to zero in the lda function:</p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=50, minimum_probability=0)
</code></pre>

<p>Moreover, you can just get the topic-distribution for all articles by:</p>

<pre><code>for i in range(len(doc_set)):
    print(ldamodel[corpus[i]]) 
</code></pre>
",4,0,4218,2016-05-27 15:40:47,https://stackoverflow.com/questions/37487504/computing-the-weight-of-lda-topic-for-all-the-documents-in-the-corpus
How to monitor convergence of Gensim LDA model?,"<p>I can't seem to find it or probably my knowledge on statistics and its terms are the problem here but I want to achieve something similar to the graph found on the bottom page of the <a href=""http://pythonhosted.org/lda/getting_started.html"" rel=""noreferrer"" title=""LDA"">LDA lib from PyPI</a> and observe the uniformity/convergence of the lines. How can I achieve this with <a href=""https://radimrehurek.com/gensim/models/ldamulticore.html#module-gensim.models.ldamulticore"" rel=""noreferrer"" title=""Gensim LDA MultiCore"">Gensim LDA</a>?</p>
","python, lda, gensim, convergence","<p>You are right to wish to plot the convergence of your model fitting. 
Gensim unfortunately does not seem to make this very straight forward. </p>

<ol>
<li><p>Run the model in such a way that you will be able to analyze the output of the model fitting function. I like to setup a log file.</p>

<pre><code>import logging
logging.basicConfig(filename='gensim.log',
                    format=""%(asctime)s:%(levelname)s:%(message)s"",
                    level=logging.INFO)
</code></pre></li>
<li><p>Set the <code>eval_every</code> parameter in <code>LdaModel</code>. The lower this value is the better resolution your plot will have. However, computing the perplexity can slow down your fit a lot!  </p>

<pre><code>lda_model = 
LdaModel(corpus=corpus,
         id2word=id2word,
         num_topics=30,
         eval_every=10,
         pass=40,
         iterations=5000)
</code></pre></li>
<li><p>Parse the log file and make your plot.</p>

<pre><code>import re
import matplotlib.pyplot as plt
p = re.compile(""(-*\d+\.\d+) per-word .* (\d+\.\d+) perplexity"")
matches = [p.findall(l) for l in open('gensim.log')]
matches = [m for m in matches if len(m) &gt; 0]
tuples = [t[0] for t in matches]
perplexity = [float(t[1]) for t in tuples]
liklihood = [float(t[0]) for t in tuples]
iter = list(range(0,len(tuples)*10,10))
plt.plot(iter,liklihood,c=""black"")
plt.ylabel(""log liklihood"")
plt.xlabel(""iteration"")
plt.title(""Topic Model Convergence"")
plt.grid()
plt.savefig(""convergence_liklihood.pdf"")
plt.close()
</code></pre></li>
</ol>
",16,16,7611,2016-06-01 13:50:52,https://stackoverflow.com/questions/37570696/how-to-monitor-convergence-of-gensim-lda-model
Different models with gensim Word2Vec on python,"<p>I am trying to apply the word2vec model implemented in the library gensim in python. I have a list of sentences (each sentences is a list of words).</p>

<p>For instance let us have:</p>

<pre><code>sentences=[['first','second','third','fourth']]*n
</code></pre>

<p>and I implement two identical models:</p>

<pre><code>model = gensim.models.Word2Vec(sententes, min_count=1,size=2)
model2=gensim.models.Word2Vec(sentences, min_count=1,size=2)
</code></pre>

<p>I realize that the models sometimes are the same, and sometimes are different, depending on the value of n. </p>

<p>For instance, if n=100 I obtain</p>

<pre><code>print(model['first']==model2['first'])
True
</code></pre>

<p>while, for n=1000:</p>

<pre><code>print(model['first']==model2['first'])
False
</code></pre>

<p>How is it possible?</p>

<p>Thank you very much!</p>
","python, nlp, gensim, word2vec","<p>Looking at the <code>gensim</code> <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow"">documentation</a>, there is some randomization when you run <code>Word2Vec</code>:</p>

<blockquote>
  <p><code>seed</code> = for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed). Note that for a fully deterministically-reproducible run, you must also limit the model to a single worker thread, to eliminate ordering jitter from OS thread scheduling.</p>
</blockquote>

<p>Thus if you want to have reproducible results, you will need to set the seed:</p>

<pre><code>In [1]: import gensim

In [2]: sentences=[['first','second','third','fourth']]*1000

In [3]: model1 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2)

In [4]: model2 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2)

In [5]: print(all(model1['first']==model2['first']))
False

In [6]: model3 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2, seed = 1234)

In [7]: model4 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2, seed = 1234)

In [11]: print(all(model3['first']==model4['first']))
True
</code></pre>
",4,1,1393,2016-06-10 09:55:13,https://stackoverflow.com/questions/37745250/different-models-with-gensim-word2vec-on-python
How to install Gensim version 0.11.1 on Windows 10 Machine?,"<p>I have anaconda and Pip installed. I tried doing</p>

<pre><code>conda install -c anaconda gensim-0.11.1
</code></pre>

<p>but it couldn't find the package and the following msg was thrown on the PowerShell.</p>

<pre><code>Using Anaconda Cloud api site https://api.anaconda.org
Fetching package metadata: ......
Solving package specifications: .
Error:  Package missing in current win-64 channels:
  - gensim-0.11.1

You can search for this package on anaconda.org with

    anaconda search -t conda gensim-0.11.1
</code></pre>

<p>Any help would be appreciated. Thanks!</p>

<p>--Conda works well with the machine but even help with Pip would be appreciated.</p>
","python, pip, conda, gensim","<p>Any reason you need the <code>0.11.1</code> version?  That version hasn't been specifically built as a conda package apparently.</p>

<p>At the moment, <code>conda install gensim</code> will easily give you version <code>0.12.4</code>.</p>

<p>If you really need version <code>0.11.1</code>, first</p>

<pre><code>conda install scipy pip
</code></pre>

<p>then</p>

<pre><code>pip install gensim==0.11.1
</code></pre>
",2,1,847,2016-06-10 13:39:30,https://stackoverflow.com/questions/37749777/how-to-install-gensim-version-0-11-1-on-windows-10-machine
How to get the Document Vector from Doc2Vec in gensim 0.11.1?,"<p>Is there a way to get the document vectors of unseen and seen documents from Doc2Vec in the gensim 0.11.1 version? </p>

<ul>
<li><p>For example, suppose I trained the model on 1000 thousand - Can I get
the doc vector for those 1000 docs?     </p></li>
<li><p>Is there a way to get document vectors of unseen documents composed<br>
from the same vocabulary?</p></li>
</ul>
","python, gensim, word2vec, doc2vec","<p>For the first bullet point, you can do it in gensim 0.11.1</p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import LabeledSentence

documents = []
documents.append( LabeledSentence(words=[u'some', u'words', u'here'], labels=[u'SENT_1']) )
documents.append( LabeledSentence(words=[u'some', u'people', u'words', u'like'], labels=[u'SENT_2']) )
documents.append( LabeledSentence(words=[u'people', u'like', u'words'], labels=[u'SENT_3']) )


model = Doc2Vec(size=10, window=8, min_count=0, workers=4)
model.build_vocab(documents)
model.train(documents)

print(model[u'SENT_3'])
</code></pre>

<p>Here SENT_3 is a known sentence.</p>

<p>For the second bullet point, you can NOT do it in gensim 0.11.1, you have to update it to 0.12.4. This latest version has infer_vector function which can generate a vector for an unseen document.</p>

<pre><code>documents = []
documents.append( LabeledSentence([u'some', u'words', u'here'], [u'SENT_1']) )
documents.append( LabeledSentence([u'some', u'people', u'words', u'like'], [u'SENT_2']) )
documents.append( LabeledSentence([u'people', u'like', u'words'], [u'SENT_3']) )


model = Doc2Vec(size=10, window=8, min_count=0, workers=4)
model.build_vocab(documents)
model.train(documents)

print(model.docvecs[u'SENT_3']) # generate a vector for a known sentence
print(model.infer_vector([u'people', u'like', u'words'])) # generate a vector for an unseen sentence
</code></pre>
",9,6,4975,2016-06-11 12:45:03,https://stackoverflow.com/questions/37763883/how-to-get-the-document-vector-from-doc2vec-in-gensim-0-11-1
Is Doc2Vec suited for Sentiment Analysis?,"<p>I have been reading more modern posts about sentiment classification (analysis) such as <a href=""http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis"" rel=""nofollow"">this</a>.</p>

<p>Taking the IMDB dataset as an example I find that I get a similar accuracy percentage using Doc2Vec (88%), <strong>however a far better result using a simple tfidf vectoriser with tri-grams for feature extraction (91%)</strong>. I think this is similar to Table 2 in <a href=""http://arxiv.org/pdf/1412.5335v7.pdf"" rel=""nofollow"">Mikolov's 2015 paper</a>.</p>

<p>I thought that by using a bigger data-set this would change. So I re-ran my experiment using a breakdown of 1mill training and 1 mill test from <a href=""http://jmcauley.ucsd.edu/data/amazon/"" rel=""nofollow"">here</a>. Unfortunately, in that case my tfidf vectoriser feature extraction method increased to 93% but doc2vec fell to 85%.</p>

<p><strong>I was wondering if this is to be expected and that others find tfidf to be superior to doc2vec even for a large corpus?</strong></p>

<p>My data-cleaning is simple:</p>

<pre><code>def clean_review(review):
    temp = BeautifulSoup(review, ""lxml"").get_text()
    punctuation = """""".,?!:;(){}[]""""""
    for char in punctuation
        temp = temp.replace(char, ' ' + char + ' ')
    words = "" "".join(temp.lower().split()) + ""\n""
    return words
</code></pre>

<p>And I have tried using 400 and 1200 features for the Doc2Vec model:</p>

<pre><code>model = Doc2Vec(min_count=2, window=10, size=model_feat_size, sample=1e-4, negative=5, workers=cores)
</code></pre>

<p>Whereas my tfidf vectoriser has 40,000 max features:</p>

<pre><code>vectorizer = TfidfVectorizer(max_features = 40000, ngram_range = (1, 3), sublinear_tf = True)
</code></pre>

<p>For classification I experimented with a few linear methods, however found simple logistic regression to do OK...</p>
","machine-learning, sentiment-analysis, gensim, word2vec, doc2vec","<p>The example code Mikolov once posted (<a href=""https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ"" rel=""nofollow"">https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ</a>) used options <code>-cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1</code> – which in gensim would be similar to <code>dm=0, dbow_words=1, size=100, window=10, hs=0, negative=5, sample=1e-4, iter=20, min_count=1, workers=cores</code>.</p>

<p>My hunch is that optimal values might involve a smaller <code>window</code> and higher <code>min_count</code>, and maybe a <code>size</code> somewhere between 100 and 400, but it's been a while since I've run those experiments. </p>

<p>It can also sometimes help a little to re-infer vectors on the final model, using a larger-than-the-default <code>passes</code> parameter, rather than re-using the bulk-trained vectors. Still, these may just converge on similar performance to Tfidf – they're all dependent on the same word-features, and not very much data. </p>

<p>Going to a semi-supervised approach, where some of the document-tags represent sentiments where known, sometimes also helps. </p>
",3,1,2710,2016-07-12 09:01:53,https://stackoverflow.com/questions/38324328/is-doc2vec-suited-for-sentiment-analysis
How can I use a trained GloVe/word2vec model to extract keywords from articles?,"<p>I have trained a GloVe with ~5M <strong>spanish</strong> articles. I know how to load this GloVe in gensim and use it as if it was a word2vec model.
Now I am facing  the problem of topic modelling and keywords extraction from news articles (also in spanish) so I was wondering how could I use the trained model to do so.</p>

<p>How could I do it?</p>
","nlp, gensim, word2vec","<p>Your question on how to use a word2vec model is very general so my answer is likewise.</p>

<p>What word2vec allows you to do is to provide a generally ""better"" representation of words. So perhaps if you are using ""bag of words"" as a feature in topic modelling you can replace that with a ""bag of word vectors"" from word2vec which hopefully will give you better semantic similarity. Perhaps better keywords too.</p>
",1,0,1445,2016-07-21 15:09:39,https://stackoverflow.com/questions/38507935/how-can-i-use-a-trained-glove-word2vec-model-to-extract-keywords-from-articles
gensim can not be imported because ImportError: No module named queue?,"<p>I need to do some experiments on text files using gensim on mac Yosemite.</p>

<p>I've already installed <code>numpy</code> and <code>scipy</code> but when I want to import <code>gensim</code>.</p>

<p>I'm facing this error:</p>

<pre><code>from six.moves.queue import Queue as _Queue
ImportError: No module named queue
</code></pre>

<p>I upgraded <code>numpy</code> and <code>scipy</code> to latest version and Python is 2.7.10.</p>

<p>I read that the problem may be solved by hacking the <code>gensim</code> code to <code>from Queue import Queue as _Queue</code> but I don't know how!</p>

<p>Is there any other way?</p>
","python-2.7, queue, gensim","<p>According to the website <code>genesis</code> should work with python 2.7, however, I still think you can simply solve your issue by using it with python 3 instead.</p>
",0,0,4525,2016-07-24 20:31:38,https://stackoverflow.com/questions/38556496/gensim-can-not-be-imported-because-importerror-no-module-named-queue
Removing documents in Gensim,"<p>I'm using Gensim for an NLP task and currently I have a corpus which includes empty documents.  I don't want to rerun my code, although that is an option, and would just like to remove the documents that don't have any content.  The documents are already saved as TF-IDF corpora and was wondering if there was a way to remove these documents that are empty.  I can figure out which documents are empty but the corpora file is an iterator and not any type of data structure ie list.  Thanks,</p>

<p>Cameron</p>
","python, python-2.7, nlp, gensim","<p>You might try converting the corpus to a numpy matrix, like so:</p>

<pre><code>numpy_matrix = gensim.matutils.corpus2dense(corpus, num_terms=number_of_corpus_features)
</code></pre>

<p>Then remove the appropriate columns (those with all zero entries).  Then convert back to a gensim corpus to continue:</p>

<pre><code>corpus = gensim.matutils.Dense2Corpus(numpy_matrix)
</code></pre>

<p>If you plan on building any more corpora in your current context, it might be a good idea to modify the corpus creation process so you don't have to do this every time, but I'm sure you've thought of that.</p>
",2,0,571,2016-07-27 18:00:35,https://stackoverflow.com/questions/38620124/removing-documents-in-gensim
Matching words and vectors in gensim Word2Vec model,"<p>I have had the <a href=""https://radimrehurek.com/gensim/"" rel=""noreferrer"">gensim</a> <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""noreferrer"">Word2Vec</a> implementation compute some word embeddings for me. Everything went quite fantastically as far as I can tell; now I am clustering the word vectors created, hoping to get some semantic groupings.</p>
<p>As a next step, I would like to look at the words (rather than the vectors) contained in each cluster. I.e. if I have the vector of embeddings <code>[x, y, z]</code>, I would like to find out which actual word this vector represents. I can get the words/Vocab items by calling <code>model.vocab</code> and the word vectors through <code>model.syn0</code>. But I could not find a location where these are explicitly matched.</p>
<p>This was more complicated than I expected and I feel I might be missing the obvious way of doing it. Any help is appreciated!</p>
<h3>Problem:</h3>
<p>Match words to embedding vectors created by <code>Word2Vec ()</code> -- how do I do it?</p>
<h3>My approach:</h3>
<p>After creating the model (code below*), I would now like to match the indexes assigned to each word (during the <code>build_vocab()</code> phase) to the vector matrix outputted as <code>model.syn0</code>.
Thus</p>
<pre><code>for i in range (0, newmod.syn0.shape[0]): #iterate over all words in model
    print i
    word= [k for k in newmod.vocab if newmod.vocab[k].__dict__['index']==i] #get the word out of the internal dicationary by its index
    wordvector= newmod.syn0[i] #get the vector with the corresponding index
    print wordvector == newmod[word] #testing: compare result of looking up the word in the model -- this prints True
</code></pre>
<ul>
<li><p>Is there a better way of doing this, e.g. by feeding the vector into the model to match the word?</p>
</li>
<li><p>Does this even get me correct results?</p>
</li>
</ul>
<p>*My code to create the word vectors:</p>
<pre><code>model = Word2Vec(size=1000, min_count=5, workers=4, sg=1)
        
model.build_vocab(sentencefeeder(folderlist)) #sentencefeeder puts out sentences as lists of strings

model.save(&quot;newmodel&quot;)
</code></pre>
<p>I found <a href=""https://stackoverflow.com/questions/35914287/word2vec-how-to-get-words-from-vectors"">this question</a> which is similar but has not really been answered.</p>
","python, vector, machine-learning, gensim, word2vec","<p>So I found an easy way to do this, where <code>nmodel</code> is the name of your model. </p>

<pre><code>#zip the two lists containing vectors and words
zipped = zip(nmodel.wv.index2word, nmodel.wv.syn0)

#the resulting list contains `(word, wordvector)` tuples. We can extract the entry for any `word` or `vector` (replace with the word/vector you're looking for) using a list comprehension:
wordresult = [i for i in zipped if i[0] == word]
vecresult = [i for i in zipped if i[1] == vector]
</code></pre>

<p>This is based on the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""noreferrer"">gensim code</a>. For older versions of gensim, you might need to drop the <code>wv</code> after the model.  </p>
",5,21,18260,2016-07-29 18:40:54,https://stackoverflow.com/questions/38665556/matching-words-and-vectors-in-gensim-word2vec-model
How to install gensim on windows,"<p>Not able to install gensim on windows.Please help me I need to gensim Immediately and tell me installation steps with More details and other software that needs to be installed before it. thanks</p>
","python, gensim","<p><code>gensim</code> depends on <code>scipy</code> and <code>numpy</code>.You must have them installed prior to installing <code>gensim</code>. Simple way to install <code>gensim</code> in windows is, open cmd and type</p>

<pre><code>pip install -U gensim
</code></pre>

<p>Or download <code>gensim</code> for windows from</p>

<p><a href=""https://pypi.python.org/pypi/gensim"" rel=""nofollow noreferrer"">https://pypi.python.org/pypi/gensim</a> </p>

<p>then run  </p>

<pre><code>python setup.py test  
python setup.py install
</code></pre>
",5,1,56357,2016-08-03 09:11:31,https://stackoverflow.com/questions/38739250/how-to-install-gensim-on-windows
python - Yield improperly usage,"<p>Im pretty sure im using yield improperly:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-

import logging
from gensim import corpora, models, similarities
from collections import defaultdict
from pprint import pprint  # pretty-printer
from six import iteritems
import openpyxl
import string
from operator import itemgetter

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

#Creating a stoplist from file
with open('stop-word-list.txt') as f:
    stoplist = [x.strip('\n') for x in f.readlines()]

corpusFileName = 'content_sample_en.xlsx'
corpusSheetName = 'content_sample_en'

class MyCorpus(object):
    def __iter__(self):
        wb = openpyxl.load_workbook(corpusFileName)
        sheet = wb.get_sheet_by_name(corpusSheetName)
        for i in range(1, (sheet.max_row+1)/2):
            title = str(sheet.cell(row = i, column = 4).value.encode('utf-8'))
            summary = str(sheet.cell(row = i, column = 5).value.encode('utf-8'))
            content = str(sheet.cell(row = i, column = 10).value.encode('utf-8'))
            yield reBuildDoc(""{} {} {}"".format(title, summary, content))


def removeUnwantedPunctuations(doc):
    ""change all (/, \, &lt;, &gt;) into ' ' ""
    newDoc = """"
    for l in doc:
        if  l == ""&lt;"" or l == ""&gt;"" or l == ""/"" or l == ""\\"":
            newDoc += "" ""
        else:
            newDoc += l
    return newDoc

def reBuildDoc(doc):
    """"""
    :param doc:
    :return: document after being dissected to our needs.
    """"""
    doc = removeUnwantedPunctuations(doc).lower().translate(None, string.punctuation)
    newDoc = [word for word in doc.split() if word not in stoplist]
    return newDoc

corpus = MyCorpus()

tfidf = models.TfidfModel(corpus, normalize=True)
</code></pre>

<p>In the following example you can see me trying to create a corpus from an xlsx file. Im reading from the xlsx file 3 lines which are title summary and content and appending them into a big string. my <code>reBuildDoc()</code> and <code>removeUnwantedPunctuations()</code> functions then adjust the text to my needs and in the end returns a big list of words. (for ex: <code>[hello, piano, computer, etc... ]</code>) in the end I yield the result but I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Eran/PycharmProjects/tfidf/docproc.py"", line 101, in &lt;module&gt;
    tfidf = models.TfidfModel(corpus, normalize=True)
  File ""C:\Anaconda2\lib\site-packages\gensim-0.13.1-py2.7-win-amd64.egg\gensim\models\tfidfmodel.py"", line 96, in __init__
    self.initialize(corpus)
  File ""C:\Anaconda2\lib\site-packages\gensim-0.13.1-py2.7-win-amd64.egg\gensim\models\tfidfmodel.py"", line 119, in initialize
    for termid, _ in bow:
ValueError: too many values to unpack
</code></pre>

<p>I know the error is from the yield line because I had a different yield line that worked. It looked like this: </p>

<pre><code> yield [word for word in dictionary.doc2bow(""{} {} {}"".format(title, summary, content).lower().translate(None, string.punctuation).split()) if word not in stoplist]
</code></pre>

<p>It was abit messy and hard to put functionallity to it so I've changed it as you can see in the first example.</p>
","python, parsing, yield, gensim","<p>the problem is not the <code>yield</code> per se, is what is yielded, the error said is from <code>for termid, _ in bow</code> this line said that you expect that <code>bow</code> contain a list of tuples or any other object containing exactly 2 element like <code>(1,2),[1,2],""12"",...</code> but as it end giving to it the result of <code>MyCorpus</code> which is a string with obviously more than 2 element, hence the error, to fix this do either <code>for termid in bow</code> or in <code>MyCorpus</code> do <code>yield reBuildDoc(""{} {} {}"".format(title, summary, content)), None</code> so you yield a tuple of 2 object</p>

<p>to illustrate this check this example</p>

<pre><code>&gt;&gt;&gt; def fun(obj):
        for _ in range(2):
            yield obj


&gt;&gt;&gt; for a,b in fun(""xyz""):
        print(a,b)


Traceback (most recent call last):
  File ""&lt;pyshell#11&gt;"", line 1, in &lt;module&gt;
    for a,b in fun(""xyz""):
ValueError: too many values to unpack (expected 2)
&gt;&gt;&gt; for a,b in fun(""xy""):
        print(a,b)


x y
x y
&gt;&gt;&gt; for a,b in fun((""xy"",None)):
        print(a,b)


xy None
xy None
&gt;&gt;&gt; 
</code></pre>
",1,0,236,2016-08-09 13:23:48,https://stackoverflow.com/questions/38852074/python-yield-improperly-usage
How to use doc2vec with phrases?,"<p>i want to have phrases in doc2vec and i use gensim.phrases. in doc2vec we need tagged document to train the model and i cannot tag the phrases. how i can do this?</p>

<p>here is my code</p>

<pre><code>text = phrases.Phrases(text)
for i in range(len(text)):
    string1 = ""SENT_"" + str(i)

    sentence = doc2vec.LabeledSentence(tags=string1, words=text[i])
    text[i]=sentence

print ""Training model...""
model = Doc2Vec(text, workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling)
</code></pre>
","python, nlp, gensim, phrases, doc2vec","<p>The invocation of <code>Phrases()</code> trains a phrase-creating-model. You later use that model on text to get back phrase-combined text. </p>

<p>Don't replace your original <code>text</code> with the trained model, as on your code's first line. Also, don't try to assign into the Phrases model, as happens in your current loop, nor access the Phrases model by integers.</p>

<p>The <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow"">gensim docs for the Phrases class</a> has examples of the proper use of the <code>Phrases</code> class; if you follow that pattern you'll do well. </p>

<p>Further, note that <code>LabeledSentence</code> has been replaced by <code>TaggedDocument</code>, and its <code>tags</code> argument should be a list-of-tags. If you provide a string, it will see that as a list-of-one-character tags (instead of the one tag you intend). </p>
",0,0,1189,2016-08-16 06:53:20,https://stackoverflow.com/questions/38968353/how-to-use-doc2vec-with-phrases
Why does gensim Doc2Vec give me different vectors for the same sentence?,"<p>I am training on two identical sentences (documents) using from <code>gensim.models.doc2vec import Doc2Vec</code> and when checking out the vectors for each sentence they are completely different. Does the Neural Network have a different random initialisation per sentence?</p>

<pre><code># imports
from gensim.models.doc2vec import LabeledSentence
from gensim.models.doc2vec import Doc2Vec
from gensim import utils

# Document iteration class (turns many documents in to sentences
# each document being once sentence)
class LabeledDocs(object):
    def __init__(self, sources):
        self.sources = sources
        flipped = {}
        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')

    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                # print fin.read().strip(r""\n"")
                yield LabeledSentence(utils.to_unicode(fin.read()).split(),
                                      [prefix])

    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                #print fin, fin.read()
                self.sentences.append(
                    LabeledSentence(utils.to_unicode(fin.read()).split(),
                                    [prefix]))
        return self.sentences

# play and play3 are names of identical documents (diff gives nothing)
inp = LabeledDocs({""play"":""play"", ""play3"":""play3""})
model = Doc2Vec(size=20, window=8, min_count=2, workers=1, alpha=0.025,
                min_alpha=0.025, batch_words=1)
model.build_vocab(inp.to_array())
for epoch in range(10):
    model.train(inp)

# post to this model.docvecs[""play""] is very different from
# model.docvecs[""play3""]
</code></pre>

<p>Why is this ? Both <code>play</code> and <code>play3</code> contain :</p>

<pre class=""lang-none prettyprint-override""><code>foot ball is a sport
played with a ball where
teams of 11 each try to
score on different goals
and play with the ball
</code></pre>
","python, neural-network, gensim","<p><strong>Yes</strong>, each sentence vector is initialized differently.</p>

<p>In particular in the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/doc2vec.py#L366"" rel=""nofollow""><code>reset_weights</code></a> method. The code initializing the sentence vectors randomly is this:</p>

<pre><code>for i in xrange(length):
    # construct deterministic seed from index AND model seed
    seed = ""%d %s"" % (model.seed, self.index_to_doctag(i))
    self.doctag_syn0[i] = model.seeded_vector(seed)
</code></pre>

<p>Here you can see that each sentence vector is initialized using the random seed of the model and the tag of the sentence. Therefore it makes sense that in your example <code>play</code> and <code>play3</code> result in different vectors.</p>

<p>However if you train the model properly I would expect both vectors to end up very close to each other.</p>
",3,2,1129,2016-08-16 22:27:47,https://stackoverflow.com/questions/38985470/why-does-gensim-doc2vec-give-me-different-vectors-for-the-same-sentence
How to convert a set of features to a count matrix in pandas,"<p>Given a matrix </p>

<p>----<code>d1 d2 d3
 a: v1  0  v2
 b: v1  v3  0</code></p>

<p>I want </p>

<p>----<code>v1 v2 v3
 a: 1  1   0
 b: 1  0   1</code></p>

<p>I remember vaguely that this can be done with <code>Gensim</code>...but there must also be some module in pandas? I have tried to do <code>for v in v: for el in [a,b]</code>(happy to post the code, but I think that the example is clear enough) but it is very slow, and I imagine this must have been solved before.</p>
","python, pandas, gensim","<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html"" rel=""nofollow""><code>pandas.get_dummies</code></a>, e.g.</p>

<pre><code>import pandas as pd

# create your dataframe
df = pd.DataFrame(index=['a', 'b'],
                  data={'d1': ['v1', 'v1'],
                        'd2': [None, 'v3'],
                        'd3': ['v2', None]})

# perform one-hot encoding
df = pd.get_dummies(df, prefix_sep='=')

# rename if you so wish
df.rename(columns={c: c.split('=')[1] for c in df.columns}, inplace=True)

# sort columns by name (not really necessary)
df.sort_index(axis=1, inplace=True)

# have a look
print df
</code></pre>

<p>which yields</p>

<pre><code>   v1  v2  v3
a   1   1   0
b   1   0   1
</code></pre>
",3,1,286,2016-08-17 21:05:21,https://stackoverflow.com/questions/39006270/how-to-convert-a-set-of-features-to-a-count-matrix-in-pandas
score_cbow_pair in word2vec (gensim),"<p>I wanted to output the log-probability during learning of the word and doc vectors in gensim. I have taken a look at the implementation of the score function in the ""slow plain numpy"" version.</p>

<pre class=""lang-py prettyprint-override""><code>def score_cbow_pair(model, word, word2_indices, l1):
    l2a = model.syn1[word.point]  # 2d matrix, codelen x layer1_size
    sgn = (-1.0)**word.code  # ch function, 0-&gt; 1, 1 -&gt; -1
    lprob = -log(1.0 + exp(-sgn*dot(l1, l2a.T)))
    return sum(lprob)
</code></pre>

<p>The score function should make use of the parameters learned during hierarchical softmax training. But in the calculation of the log-probability there is supposed to be a sigmoid function( <a href=""http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf"" rel=""nofollow"">word2vec Parameter Learning Explained equation (45)</a>).
So does gensim really calculate the log-probability in <code>lprob</code> or is it just a score for comparison purposes.</p>

<p>I would have calculated the log-probability as follows:
<code>-log(1.0/(1.0+exp(-sgn*dot(l1, l2a.T))))</code></p>

<p>Is this equation not used because it explodes for values close to zero or is it in general wrong?</p>
","python, numpy, probability, gensim, word2vec","<p>I've overlooked that the logarithm of the sigmoid function can be rewritten: <code>log(1.0/(1.0+exp(-sgn*dot(l1, l2a.T)))) = log(1)-log(1.0+exp(-sgn*dot(l1, l2a.T))) = -log(1.0+exp(-sgn*dot(l1, l2a.T)))</code></p>

<p>So the code does compute the log-likelihood.</p>
",0,0,390,2016-08-31 10:45:53,https://stackoverflow.com/questions/39247496/score-cbow-pair-in-word2vec-gensim
updates of the document vectors in doc2vec (PV-DM) in gensim,"<p>I'm trying to understand the PV-DM implementation with averaging in gensim.
In the function <code>train_document_dm</code> in <code>doc2vec.py</code> the return value (""errors"") of <code>train_cbow_pair</code> is in the case of averaging (<code>cbow_mean=1</code>) not divided by the number of input vectors (<code>count</code>).
According to this explanation there should be a division by the number of documents in the case of averaging the input vectors: <a href=""http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf"" rel=""nofollow"">word2vec Parameter Learning Explained, equation (23)</a>.
Here is the code from <code>train_document_dm</code>:</p>

<pre class=""lang-py prettyprint-override""><code>l1 = np_sum(word_vectors[word2_indexes], axis=0)+np_sum(doctag_vectors[doctag_indexes], axis=0)  
count = len(word2_indexes) + len(doctag_indexes)  
if model.cbow_mean and count &gt; 1:  
    l1 /= count  
neu1e = train_cbow_pair(model, word, word2_indexes, l1, alpha,
                                learn_vectors=False,  learn_hidden=learn_hidden)  
if not model.cbow_mean and count &gt; 1:  
    neu1e /= count  
if learn_doctags:  
    for i in doctag_indexes:  
        doctag_vectors[i] += neu1e * doctag_locks[i]  
if learn_words:  
    for i in word2_indexes:  
        word_vectors[i] += neu1e * word_locks[i]  
</code></pre>
","python, numpy, gensim, word2vec, doc2vec","<p>Let's say V is defined as the average of <code>A</code>, <code>B</code>, and <code>C</code>: </p>

<p>V = (A + B + C) / 3</p>

<p>Let's set <code>A = 5</code>, <code>B = 6</code>, and <code>C = 10</code>. And let's say we want <code>V</code> equal 10.</p>

<p>We run the calculation (forward propagation), and the value of <code>V</code>, the average of the three numbers, is 7. Thus the correction needed for V is +3. </p>

<p>To apply this correction to A, B, and C, do we also <em>divide</em> that correction by 3, to get +1 against each? In that case <code>A = 6</code>, <code>B = 7</code>, and <code>C = 11</code> – and now <code>V</code> is just 8. It'd still need another +2 to match the target. </p>

<p>So, no. The proper correction to all of the components of <code>V</code>, in the case where <code>V</code> is an average, is the same as the correction to <code>V</code> – in this case, +3. If we were apply that, we'd reach our proper target value of 10:</p>

<pre><code>A = 8, B = 9, C = 13
V = (8 + 9 + 13) / 3 = 10
</code></pre>

<p>The same thing is happening the gensim backpropagation. In the case of averaging, the full corrective value (times the learning-rate <code>alpha</code>) is applied to each of the constituent vectors. </p>

<p>(If using a sum-of-vectors to create V instead, <em>then</em> the error would need to be divided by the count of constituent vectors - to split the error over them all, and not apply it redundantly.)</p>
",0,0,244,2016-08-31 14:51:26,https://stackoverflow.com/questions/39252860/updates-of-the-document-vectors-in-doc2vec-pv-dm-in-gensim
Tokenizing a corpus composed of articles into sentences Python,"<p>I will like to analyze my first deep learning model using Python and in order to do so I have to first split my corpus (8807 articles) into sentences. My corpus is built as follows:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

import json
import nltk
import re
import pandas


appended_data = []


#for i in range(20014,2016):
#    df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
#    appended_data.append(df0)

for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)


appended_data = pandas.concat(appended_data)
# doc_set = df1.body

doc_set = appended_data.body
</code></pre>

<p>I am trying to use the function <code>Word2Vec.load_word2vec_format</code> from the library <code>gensim.models</code> but I have to first split my corpus (<code>doc_set</code>) into sentences.</p>

<pre><code>from gensim.models import word2vec
model = Word2Vec.load_word2vec_format(doc_set, binary=False)
</code></pre>

<p>Any recommendations? </p>

<p>cheers</p>
","python, deep-learning, gensim, word2vec","<p>So, Gensim's <code>Word2Vec</code> requires this format for its training input: <code>sentences = [['first', 'sentence'], ['second', 'sentence']]</code>.</p>

<p>I assume your documents contain more than one sentence. You should first split by sentences, you can do that with <a href=""http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt"" rel=""nofollow"">nltk</a> (you might need to download the model first). Then tokenize each sentence and put everything together in a list.</p>

<pre><code>sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
sentenized = doc_set.body.apply(sent_detector.tokenize)
sentences = itertools.chain.from_iterable(sentenized.tolist()) # just to flatten

result = []
for sent in sentences:
    result += [nltk.word_tokenize(sent)]
gensim.models.Word2Vec(result)
</code></pre>

<p>Unfortunately I am not good enough with Pandas to perform all the operations in a ""pandastic"" way.</p>

<p>Pay a lot of attention to the parameters of <code>Word2Vec</code> picking them right can make a huge difference.</p>
",2,4,4512,2016-09-01 15:27:12,https://stackoverflow.com/questions/39275547/tokenizing-a-corpus-composed-of-articles-into-sentences-python
LdaModel - random_state parameter not recognized - gensim,"<p>I'm using <code>gensim</code>'s <code>LdaModel</code>, which, according to the <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow"">documentation</a>, has the parameter <code>random_state</code>. However, I'm getting an error that says:</p>

<pre><code> TypeError: __init__() got an unexpected keyword argument 'random_state'
</code></pre>

<p>Without the <code>random_state</code> parameter, the function works as expected. So, the workflow looks like this for those that want to know what else is happening...</p>

<pre><code>from gensim import corpora, models
import numpy as np

# pseudo code of text pre-processing all on ""comments"" variable
# stop words
# remove punctuation (optional)
# keep alpha only
# stemming
# get bigrams and integrate with corpus (gensim makes this very easy)


dictionary = corpora.Dictionary(comments)
corpus = [dictionary.doc2bow(comm) for comm in comments]
tfidf = models.TfidfModel(corpus) # change weights
corp_tfidf = tfidf[corpus] # apply them to corpus

# set random seed
random_seed = 135
state = np.random.RandomState(random_seed)

# train model
num_topics = 3
lda_mod = models.LdaModel(corp_tfidf, # corpus
                          num_topics=num_topics, # number of topics we want back
                          id2word=dictionary, # our id-word map
                          passes=10, # how many passes to take over the data
                          random_state=state) # reproduce the results
</code></pre>

<p>Which results in the error message above...</p>

<pre><code>TypeError: __init__() got an unexpected keyword argument 'random_state'
</code></pre>

<p>I'd like to be able to recreate my results, if possible.</p>
","python-3.x, numpy, gensim, topic-modeling","<p>According to <a href=""https://github.com/RaRe-Technologies/gensim/commit/2e0ed26c03bfa997c8748d03fafe313a05cd9741"" rel=""nofollow"">this</a>, <code>random_state</code> parameter was added in the latest version (0.13.2). You can update your gensim installation with <code>pip install gensim --upgrade</code>. You might need to update <code>scipy</code> first, because it caused me problems. </p>
",1,0,1614,2016-09-02 14:33:13,https://stackoverflow.com/questions/39294921/ldamodel-random-state-parameter-not-recognized-gensim
Graphical plot of words similarity given by Word2Vec,"<p>I will like to plot in a simple vector space graph the similarity between different words. I have calculated them using the model <code>word2vec</code> given by gensim but I cannot find any graphical examples in the literature. My code is as follows:</p>

<pre><code>## Libraries to download
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

import json
import nltk
import re
import pandas


appended_data = []


#for i in range(20014,2016):
#    df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
#    appended_data.append(df0)

for i in range(2005,2016):
    if i &gt; 2013:
        df0 = pandas.DataFrame([json.loads(l) for l in open('SDM_%d.json' % i)])
        appended_data.append(df0)
    df1 = pandas.DataFrame([json.loads(l) for l in open('Scot_%d.json' % i)])
    df2 = pandas.DataFrame([json.loads(l) for l in open('APJ_%d.json' % i)])
    df3 = pandas.DataFrame([json.loads(l) for l in open('TH500_%d.json' % i)])
    df4 = pandas.DataFrame([json.loads(l) for l in open('DRSM_%d.json' % i)])
    appended_data.append(df1)
    appended_data.append(df2)
    appended_data.append(df3)
    appended_data.append(df4)


appended_data = pandas.concat(appended_data)
# doc_set = df1.body

doc_set = appended_data.body

## Building the deep learning model
import itertools

sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
sentenized = doc_set.apply(sent_detector.tokenize)
sentences = itertools.chain.from_iterable(sentenized.tolist()) # just to flatten

from gensim.models import word2vec


result = []
for sent in sentences:
    result += [nltk.word_tokenize(sent)]

model = gensim.models.Word2Vec(result)
</code></pre>

<p>In a simple vector space graph, I will like to place the following words: bank, finance, market, property, oil, energy, business and economy. I can easily calculate the similarity of these pairs of words with the function:</p>

<pre><code>model.similarity('bank', 'property')
0.25089364531360675
</code></pre>

<p>Thanks a lot</p>
","python, graph, deep-learning, gensim, word2vec","<p>For plotting all the word-vectors in your Word2Vec model, you need to perform Dimensionality reduction. You can use TSNE tool from python's sklearn to visualise multi-dimensional vectors in 2-D space.    </p>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"" rel=""noreferrer"">t-distributed Stochastic Neighbor Embedding</a>.</p>

<pre><code>import sklearn.manifold.TSNE

tsne = sklearn.manifold.TSNE(n_components = 0 , random_state = 0)
all_vector_matrix = model.syn0
all_vector_matrix_2d = tsne.fit_transform(all_vector_matrix)
</code></pre>

<p>This will give you a 2-D similarity matrix which you can further parse through pandas and then plot using seaborn and matplotlib's pyplot function.</p>
",5,2,4415,2016-09-02 16:02:43,https://stackoverflow.com/questions/39296592/graphical-plot-of-words-similarity-given-by-word2vec
How to load a pre-trained Word2vec MODEL File and reuse it?,"<p>I want to use a pre-trained <code>word2vec</code> model, but I don't know how to load it in python.</p>

<p>This file is a MODEL file (703 MB).
It can be downloaded here:<br>
<a href=""http://devmount.github.io/GermanWordEmbeddings/"" rel=""noreferrer"">http://devmount.github.io/GermanWordEmbeddings/</a></p>
","python, file, model, word2vec, gensim","<p>just for loading</p>

<pre><code>import gensim

# Load pre-trained Word2Vec model.
model = gensim.models.Word2Vec.load(""modelName.model"")
</code></pre>

<p>now you can train the model as usual. also, if you want to be able to save it and retrain it multiple times, here's what you should do</p>

<pre><code>model.train(//insert proper parameters here//)
""""""
If you don't plan to train the model any further, calling
init_sims will make the model much more memory-efficient
If `replace` is set, forget the original vectors and only keep the normalized
ones = saves lots of memory!
replace=True if you want to reuse the model
""""""
model.init_sims(replace=True)

# save the model for later use
# for loading, call Word2Vec.load()

model.save(""modelName.model"")
</code></pre>
",30,19,48836,2016-09-17 16:40:54,https://stackoverflow.com/questions/39549248/how-to-load-a-pre-trained-word2vec-model-file-and-reuse-it
doc2vec - Input Format for doc2vec training and infer_vector() in python,"<p>In gensim, when I give a string as input for training doc2vec model,  I get this error : </p>

<blockquote>
  <p>TypeError('don\'t know how to handle uri %s' % repr(uri))</p>
</blockquote>

<p>I referred to this question <a href=""https://stackoverflow.com/questions/36780138/doc2vec-taggedlinedocument"">Doc2vec : TaggedLineDocument()</a>
but still have a doubt about the input format. </p>

<p><code>documents = TaggedLineDocument('myfile.txt')</code></p>

<p>Should the myFile.txt have tokens as list of lists or separate list in each line for each document or a string? </p>

<p><code>For eg</code> - I have 2 documents.</p>

<p>Doc 1 : Machine learning is a subfield of computer science that evolved from the study of pattern recognition.</p>

<p>Doc 2 :  Arthur Samuel defined machine learning as a ""Field of study that gives computers the ability to learn"".</p>

<p>So, what should the <code>myFile.txt</code> look like?</p>

<p>Case 1 : simple text of each document in each line</p>

<p>Machine learning is a subfield of computer science that evolved from the study of pattern recognition</p>

<p>Arthur Samuel defined machine learning as a Field of study that gives computers the ability to learn</p>

<p>Case 2 : a list of lists having tokens of each document</p>

<p><code>[ [""Machine"", ""learning"", ""is"", ""a"", ""subfield"", ""of"", ""computer"", ""science"", ""that"", ""evolved"", ""from"", ""the"", ""study"", ""of"", ""pattern"", ""recognition""]</code>,</p>

<pre><code>[""Arthur"", ""Samuel"", ""defined"", ""machine"", ""learning"", ""as"", ""a"", ""Field"", ""of"", ""study"", ""that"", ""gives"", ""computers"" ,""the"", ""ability"", ""to"", ""learn""] ]
</code></pre>

<p>Case 3 : list of tokens of each document in a separate line</p>

<pre><code>[""Machine"", ""learning"", ""is"", ""a"", ""subfield"", ""of"", ""computer"", ""science"", ""that"", ""evolved"", ""from"", ""the"", ""study"", ""of"", ""pattern"", ""recognition""]

[""Arthur"", ""Samuel"", ""defined"", ""machine"", ""learning"", ""as"", ""a"", ""Field"", ""of"", ""study"", ""that"", ""gives"", ""computers"" ,""the"", ""ability"", ""to"", ""learn""]
</code></pre>

<p>And when I am running it on the test data, what should be the format of the sentence which i want to predict the doc vector for? Should it be like case 1 or case 2 below or something else?</p>

<p><code>model.infer_vector(testSentence, alpha=start_alpha, steps=infer_epoch)</code></p>

<p>Should the testSentence be :</p>

<p>Case 1 : string</p>

<pre><code>testSentence = ""Machine learning is an evolving field""
</code></pre>

<p>Case 2 : list of tokens</p>

<pre><code>testSentence = [""Machine"", ""learning"", ""is"", ""an"", ""evolving"", ""field""]
</code></pre>
","python, gensim, word2vec, doc2vec","<p><code>TaggedLineDocument</code> is a convenience class that expects its source file (or file-like object) to be space-delimited tokens, one per line. (That is, what you refer to as 'Case 1' in your 1st question.)</p>

<p>But you can write your own iterable object to feed to gensim <code>Doc2Vec</code> as the <code>documents</code> corpus, as long as this corpus (1) iterably-returns <code>next()</code> objects that, like TaggedDocument, have <code>words</code> and <code>tags</code> lists; and (2) can be iterated over multiple times, for the multiple passes <code>Doc2Vec</code> requires for both the initial vocabulary-survey and then <code>iter</code> training passes. </p>

<p>The <code>infer_vector()</code> method takes lists-of-tokens, similar to the <code>words</code> attribute of individual <code>TaggedDocument</code>-like objects. (That is, what you refer to as 'Case 2' in your 2nd question.)</p>
",5,2,2170,2016-09-21 11:33:05,https://stackoverflow.com/questions/39615420/doc2vec-input-format-for-doc2vec-training-and-infer-vector-in-python
Fails to fix the seed value in LDA model in gensim,"<p>When using LDA model, I get different topics each time and I want to replicate the same set. I have searched for the similar question in Google such as <a href=""https://groups.google.com/forum/#!topic/gensim/s1EiOUsqT8s"" rel=""nofollow"">this</a>.</p>

<p>I fix the seed as shown in the article by <code>num.random.seed(1000)</code> but it doesn't work. I read the <code>ldamodel.py</code> and find the code below:</p>

<pre><code>def get_random_state(seed):

    """"""
    Turn seed into a np.random.RandomState instance.
    Method originally from maciejkula/glove-python, and written by @joshloyal
    """"""
     if seed is None or seed is numpy.random:
         return numpy.random.mtrand._rand
     if isinstance(seed, (numbers.Integral, numpy.integer)):
         return numpy.random.RandomState(seed)
     if isinstance(seed, numpy.random.RandomState):
        return seed
     raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
                      ' instance' % seed)
</code></pre>

<p>So I use the code:</p>

<pre><code>lda = models.LdaModel(
    corpus_tfidf,
    id2word=dic,
    num_topics=2,
    random_state=numpy.random.RandomState(10)
)
</code></pre>

<p>But it's still not working.</p>
","python, numpy, gensim","<p>The dictionary generated by <code>corpora.Dictionary</code> may be different to the same corpus(such as same words but different order).So one should fix the dictionary as well as seed to get tht same topic each time.The code below may help to fix the dictionary:</p>

<pre><code>dic = corpora.Dictionary(corpus)
dic.save(""filename"")
dic=corpora.Dictionary.load(""filename"")
</code></pre>
",2,0,2939,2016-09-21 11:33:45,https://stackoverflow.com/questions/39615436/fails-to-fix-the-seed-value-in-lda-model-in-gensim
Rules to set hyper-parameters alpha and theta in LDA model,"<p>I will like to know more about whether or not there are any rule to set the hyper-parameters alpha and theta in the LDA model. I run an LDA model given by the library <code>gensim</code>:</p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=30, id2word = dictionary, passes=50, minimum_probability=0)
</code></pre>

<p>But I have my doubts on the specification of the hyper-parameters. From what I red in the library documentation, both hyper-parameters are set to 1/number of topics. Given that my model has 30 topics, both hyper-parameters are set to a common value 1/30. I am running the model in news-articles that describe the economic activity. For this reason, I expect that the document-topic distribution (theta) to be high (similar topics in documents),while the topic-word distribution (alpha) be high as well (topics sharing many words in common, or, words not being so exclusive for each topic). For this reason, and given that my understanding of the hyper-parameters is correct, is 1/30 a correct specification value?</p>
","lda, gensim","<p>I'll assume you expect theta and phi (document-topic proportion and topic-word proportion) to be closer to equiprobable distributions instead of sparse ones, with exclusive topics/words.</p>

<p>Since alpha and beta are parameters to a symmetric Dirichlet prior, they have a direct influence on what you want. A Dirichlet distribution outputs probability distributions. When the parameter is 1, all possible distributions are equally liked to outcome (for K=2, [0.5,0.5] and [0.99,0.01] have the same chances). When parameter>1, this parameter behaves as a pseudo-counter, as a prior belief. For a high value, equiprobable output is preferred (P([0.5,0.5])>P([0.99,0.01]). Parameter&lt;1 has the opposite behaviour. For big vocabularies you don't expect topics with probability in all words, that's why beta tends to be under 1 (the same for alpha).</p>

<p>However, since you're using Gensim, you can let the model learn alpha and beta values for you, allowing to learn asymmetric vectors (see <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">here</a>), where it stands </p>

<blockquote>
  <p>alpha can be set to an explicit array = prior of your choice. It also
  support special values of ‘asymmetric’ and ‘auto’: the former uses a
  fixed normalized asymmetric 1.0/topicno prior, the latter learns an
  asymmetric prior directly from your data.</p>
</blockquote>

<p>The same for eta (which I call beta).</p>
",3,4,7930,2016-09-22 16:48:13,https://stackoverflow.com/questions/39644667/rules-to-set-hyper-parameters-alpha-and-theta-in-lda-model
Word2Vec: Using Gensim and Google-News dataset- Very Slow Execution Time,"<p>The Code is in python. I loaded up the binary model into gensim on python, &amp; used the ""init_sims"" option to make the execution faster. The OS is OS X.
It takes almost 50-60 seconds to load it up. And an equivalent time to find ""most_similar"". Is this normal? Before using the init_sims option, it took almost double the time! I have a feeling it might be an OS RAM allocation issue.</p>

<pre><code>model=Word2Vec.load_word2vec_format('GoogleNewsvectorsnegative300.bin',binary=True)
model.init_sims(replace=True)
model.save('SmallerFile')
#MODEL SAVED INTO SMALLERFILE &amp; NEXT LOAD FROM IT
model=Word2Vec.load('SmallerFile',mmap='r')
#GIVE RESULT SER!
print model.most_similar(positive=['woman', 'king'], negative=['man'])
</code></pre>
","python, gensim, word2vec","<p>Note that the memory-saving effect of <code>init_sims(replace=True)</code> doesn't persist across save/load cycles, because saving always saves the 'raw' vectors (from which the unit-normalized vectors can be recalculated). So, even after your re-load, when you call <code>most_similar()</code> for the 1st time, <code>init_sims()</code> will be called behind the scenes, and the memory usage will be doubled.</p>

<p>And, the GoogleNews dataset is quite large, taking 3+ GB to load even before the unit-normalization possibly doubles the memory usage. So depending on what else you've got running and the machine's RAM, you might be using swap memory by the time the <code>most_similar()</code> calculations are running – which is very slow for the calculate-against-every-vector-and-sort-results similarity ops. (Still, any <code>most_similar()</code> checks after the 1st won't need to re-fill the unit-normalized vector cache, so should go faster than the 1st call.)</p>

<p>Given that you've saved the model after <code>init_sims(replace=True)</code>, its raw vectors are already unit-normalized. So you can manually-patch the model to skip the recalculation, just after your <code>load()</code>:</p>

<pre><code>model.syn0norm = model.syn0
</code></pre>

<p>Then even your first <code>most_similar()</code> will just consult the (single, memory-mapped) set of vectors, without triggering an <code>init_sims()</code>.</p>

<p>If that's still too slow, you may need more memory or to trim the vectors to a subset. The GoogleNews vectors seem to be sorted to put the most-frequent words earliest, so throwing out the last 10%, 50%, even 90% may still leave you with a useful set of the most-common words. (You'd need to perform this trimming yourself by looking at the model object and source code.)</p>

<p>Finally, you can use a nearest-neighbors indexing to get faster top-N matches, but at a cost of extra memory and approximate results (that may miss some of the true top-N matches). There's an IPython notebook tutorial in recent gensim versions at <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/annoytutorial.ipynb"" rel=""noreferrer"">annoytutorial.ipynb</a> IPython notebook of the demo IPython notebooks, in the gensim <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/"" rel=""noreferrer""><code>docs/notebooks</code></a> directory.</p>
",6,5,2587,2016-09-23 09:24:33,https://stackoverflow.com/questions/39657215/word2vec-using-gensim-and-google-news-dataset-very-slow-execution-time
Siri-like app: calculating similarities between a query and a predefined set of control phrases,"<p>I am trying to make a Apple Siri-like application in python in which you give it vocal commands or questions through a microphone, it determines the text version of the inputted audio, and then determines the appropriate action to take based on the meaning of the command/question. I am going to be using the Speech Recognition library to accept microphone input and convert from speech to text (via the IBM Watson Speech to Text API).</p>

<p>The main problem I have with it right now is that when I define an action for the app to execute when the appropriate command is given/question is asked, I don't know how to determine if the said command/question is denoting that action. Let me clarify what I mean by that with an example:</p>

<p>Say we have a action called <code>hello</code>. There are multiple ways for somebody to say ""hello"" to another person (or in this case, my application), such as:</p>

<ul>
<li>""Hello""</li>
<li>""Hi""</li>
<li>""Howdy""</li>
<li>...Etcetera...</li>
</ul>

<p>Of course, I want all of these ways of saying ""hello"" to be classified under the action of <code>hello</code>. That is, when someone says ""hello"", ""hi"", or ""howdy"", the response for the action <code>hello</code> should be executed (most likely just the app saying ""hello"" back in this case).</p>

<p>My first thought on how to solve this was to supply the app with all of or the most common ways to say a certain command/question. So, if I follow the previous example, I would tell the computer that ""hello"", ""hi"", and ""howdy"" all meant the same thing: the <code>hello</code> action. However, this method has a couple flaws. First off, it simply wouldn't understand ways of saying ""hello"" that weren't hardcoded in, such as ""hey"". Second off, once the responses for new commands/questions start getting coded in, it would become very tedious entering all the ways to say a certain phrase.</p>

<p>So then, because of the aforementioned problems, I started looking into ways to calculate the similarities between a group of sentences, and a single query. I eventually came across the Gensim library for python. I looked into it and found some very promising information on complex processes such as latent semantic indexing/analysis (LSI/LSA) and Tf-idf. However, it seemed to me like these things were mainly for comparing documents with large word counts as they rely on the frequency of certain terms. Assuming this is true, these processes wouldn't really provide me with accurate results as the commands/questions given to my app will probably be about eight words on average. I could be completely wrong, after all I know very little about these processes.</p>

<p>I also discovered WordNet, and how to work with it in python using the Natural Language Toolkit (NLTK). It looks like it could be useful, but I'm not sure how.</p>

<p>So, finally, I guess my real question here is what would be the best solution to the problem I've mentioned? Should I use one of the methods I've mentioned? Or is there a better way to do what I want that I don't know about?</p>

<p>Any help at all would be greatly appreciated. Thanks in advance.</p>

<p>P.S. Sorry for the wordy explanation; I wanted to be sure I was clear :P</p>
","python, nlp, nltk, wordnet, gensim","<p>This is a hard problem. It is also the subject of <a href=""http://alt.qcri.org/semeval2017/task11"" rel=""nofollow"">Task 11</a> of this year's set of Semantic evaluation challenges (<a href=""http://alt.qcri.org/semeval2017/"" rel=""nofollow"">Semeval 2017</a>). So take a look at the <a href=""http://alt.qcri.org/semeval2017/task11/index.php?id=task-description"" rel=""nofollow"">task description</a>, which will give you a road map for how this problem can be solved. The task also comes with a suite of training data, which is essential for approaching a problem like this. The challenge is still ongoing, but eventually you'll be able to learn from the solutions as well. </p>

<p>So the short answer to ""how do I determine if some command/question is denoting a certain action"" is: Use the training data from Semeval2017 (or your own of course), and write a classifier. The <a href=""http://nltk.org/book"" rel=""nofollow"">nltk book</a> can help you get up to speed with writing classifiers.</p>
",3,1,203,2016-09-28 04:54:24,https://stackoverflow.com/questions/39738327/siri-like-app-calculating-similarities-between-a-query-and-a-predefined-set-of
How can I tell if Gensim Word2Vec is using the C compiler?,"<p>I am trying to use Gensim's Word2Vec implementation. Gensim warns that if you don't have a C compiler, the training will be 70% slower.  Is there away to verify that Gensim is correctly using the C Compiler I have installed?</p>

<p>I am using Anaconda Python 3.5 on Windows 10.</p>
","python, compilation, installation, gensim, word2vec","<p>Apparently gensim offers a variable to detect this:</p>

<pre><code>assert gensim.models.doc2vec.FAST_VERSION &gt; -1
</code></pre>

<p>I found this line in this tutorial:
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb</a></p>
",20,15,7179,2016-09-30 00:09:37,https://stackoverflow.com/questions/39781812/how-can-i-tell-if-gensim-word2vec-is-using-the-c-compiler
Gensim LDA topic assignment,"<p>I am hoping to assign each document to one topic using LDA. Now I realise that what you get is a distribution over topics from LDA. However as you see from the last line below I assign it to the most probable topic.</p>

<p>My question is this. I have to run <code>lda[corpus]</code> for somewhat the second time in order to get these topics. Is there some other builtin gensim function that will give me this topic assignment vectors directly? Especially since the LDA algorithm has passed through the documents it might have saved these topic assignments?</p>

<pre class=""lang-py prettyprint-override""><code>    # Get the Dictionary and BoW of the corpus after some stemming/ cleansing
    texts = [[stem(word) for word in document.split() if word not in STOPWORDS] for document in cleanDF.text.values]
    dictionary = corpora.Dictionary(texts)
    dictionary.filter_extremes(no_below=5, no_above=0.9)
    corpus = [dictionary.doc2bow(text) for text in texts]

    # The actual LDA component
    lda = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=30, chunksize=10000, passes=10,workers=4) 

    # Assign each document to most prevalent topic
    lda_topic_assignment = [max(p,key=lambda item: item[1]) for p in lda[corpus]]
</code></pre>
","gensim, lda, topic-modeling","<p>There is no other builtin Gensim function that will give the topic assignment vectors directly.</p>

<p>Your question is valid that LDA algorithm has passed through the documents but implementation of LDA is working by updating the model in chunks (based on value of <code>chunksize</code> parameter), hence it will not keep the entire corpus in-memory. </p>

<p>Hence you have to use <code>lda[corpus]</code> or use the method <code>lda.get_document_topics()</code></p>
",4,12,3180,2016-10-11 03:07:18,https://stackoverflow.com/questions/39969919/gensim-lda-topic-assignment
How to get the key value pairs in numpy.ndarray? (Gensim Word2vec),"<p>I am trying to get the keys as well as the vectors in the vector <code>model.syn0</code> which gives vectors by <code>model.syn0[""word""]</code> which gives an n-dim vector. Is there a better way to create a list of all the words in the model in the same order as the the vectors of <code>syn0</code> than this? I have 350000 words and this would take too long.</p>

<pre><code>from gensim.models import word2vec as wv
model = wv.Word2Vec.load('model')
lab=[]
for i in model.syn0:
    lab.append(model.similar_by_vector(i)[0])

print(type(model.syn0))
    &lt;type 'numpy.ndarray'&gt;
</code></pre>
","python, performance, gensim, word2vec","<p>At the direction of <a href=""https://groups.google.com/forum/#!searchin/gensim/$20List$20of$20words$20in$20the$20vocabulary$20(syn0$20word$20list)$20%7Csort:relevance/gensim/qkEZ-MFf-NI/jBHqH14pBwAJ"" rel=""nofollow noreferrer"">Gordon Mohr</a>, I found that the key value pairs are stored in <code>model.index2word</code>. </p>

<p>So the key-value pairs list can easily be obtained using:</p>

<pre><code>lab=model.index2word
</code></pre>
",0,1,1845,2016-10-21 16:59:07,https://stackoverflow.com/questions/40181943/how-to-get-the-key-value-pairs-in-numpy-ndarray-gensim-word2vec
gensim Generating LSI model causes &quot;Python has stopped working&quot;,"<p>So I am trying to use gensim to generate an LSI model along with corpus_lsi following <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow"">this</a> tutorial.</p>

<p>I start with a corpus and a dictionary that I generated myself.
The list of documents are too small (9 lines = 9 documents), which is the sample list provided in <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow"">gensim</a> tutorials</p>

<p>However, pythos just crashes when it reaches the line for generating LSI_model.
You can see below my code along with the generated output</p>

<p><strong>Code</strong></p>

<pre><code>#!/usr/bin/env python
import os
from gensim import corpora, models, similarities
import logging

#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

if __name__ == '__main__':
    if (os.path.exists(""tmp\dictionary.dict"")):
        dictionary = corpora.Dictionary.load('tmp\dictionary.dict')
        corpus = corpora.MmCorpus('tmp\corpus.mm')
        print(""Used files generated Dataset Generator"")
    else:
        print(""Please run dataset generator"")

print (""generating tf-idf model ..."")
tfidf = models.TfidfModel(corpus)   # Generate tfidf matrix (tf-idf model)
print (""generating corpus_tf-idf model ..."")
corpus_tfidf = tfidf[corpus]    #use the model to transform vectors

print (""generating LSI model ..."")
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation
print (""generating corpus_lsi model ..."")
corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow-&gt;tfidf-&gt;fold-in-lsi

lsi.print_topics(2)
</code></pre>

<p><strong>Output</strong></p>

<pre><code>Used files generated Dataset Generator
generating tf-idf model ...
generating corpus_tf-idf model ...
generating LSI model ...
</code></pre>

<p>After printing ""generating LSI model"" it crashes</p>

<p>Any suggestions ?</p>

<p><strong>Other things I tried</strong></p>

<ul>
<li>Changing python version to python 2.6</li>
<li>Removing gensim and installing it again from github (instead of conda) </li>
</ul>
","python, python-3.x, gensim, latent-semantic-indexing, latent-semantic-analysis","<p>It seems that the issue was the function used in the tutorial (maybe downgraded or something)</p>

<p>so I changed the line </p>

<pre><code>lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation
</code></pre>

<p>To </p>

<pre><code>lsi = LsiModel(corpus_tfidf,num_topics=2)
</code></pre>

<p>And it actually worked fine</p>
",4,2,798,2016-10-23 17:23:45,https://stackoverflow.com/questions/40205725/gensim-generating-lsi-model-causes-python-has-stopped-working
gensim doc2vec documents not found by id,"<p>Here is my code for training my doc2vec model</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
from FileDocIterator import FileDocIterator

doc_file_name = 'doc_6million.txt'
docs = FileDocIterator(doc_file_name)
print ""Fitting started""
model = Doc2Vec(docs, size=100, window=5, min_count=5, negative=20, workers=6, iter=4)
print ""Saving model""
model.save(""doc2vec_model"")
print ""model saved""
</code></pre>

<p>Now lets take a look at <code>FileDocIterator</code></p>

<pre><code>import json

from gensim.models.doc2vec import TaggedDocument
from gensim.models import Phrases

class FileDocIterator(object):
    def __init__(self, fileName):
        self.fileName = fileName
        self.phrase = Phrases.load(""phrases"")

    def __iter__(self):
        for line in open(self.fileName):
            jsData = json.loads(line)
            yield TaggedDocument(words=jsData[""data""], tags=jsData[""id""])
</code></pre>

<p>Now I do understand that phrases isn't being used in this implementation, but bear with me here, lets take a look at how the data looks like. Here is the first data point</p>

<pre><code>{""data"":[""strategic"",""and"",""analytical"",""technical"",""program"",""director"",""and"",""innovator"",""who"",""inspires"",""calculated"",""risk-taking"",""in"",""emerging"",""technologies"","","",""such"",""as"",""cyber"",""security"","","",""risk"","","",""analytics"","","",""big"",""data"","","",""cloud"","","",""mobility"",""and"",""3d"",""printing"",""."",""known"",""for"",""growing"",""company"",""profit"",""through"",""innovative"",""thinking"",""aimed"",""at"",""improving"",""employee"",""productivity"",""and"",""providing"",""solutions"",""to"",""private"",""industry"",""and"",""government"",""customers"",""."",""recognized"",""for"",""invigorating"",""creative"",""thinking"",""and"",""collaboration"",""within"",""large"",""companies"",""to"",""leverage"",""their"",""economies"",""of"",""scale"",""to"",""capture"",""market"",""share"",""."",""successful"",""in"",""managing"",""the"",""risk"",""and"",""uncertainty"",""throughout"",""the"",""innovation"",""lifecycle"",""by"",""leveraging"",""an"",""innovation"",""management"",""framework"",""to"",""overcome"",""barriers"",""."",""track"",""record"",""of"",""producing"",""results"",""in"",""competitive"","","",""rapidly"",""changing"",""environments"",""where"",""innovation"",""and"",""customer"",""satisfaction"",""is"",""the"",""business"",""."",""competencies"",""include"","":"",""innovation"",""management"",""cyber"","","",""risk"","","",""analytics"","","",""cloud"",""computing"",""and"",""mobility"",""technology"",""development"",""security"",""compliance"","":"",""dod/ic"",""("",""nispom"","","",""icd"",""503"","","",""fedramp"","")"",""commercial"",""("",""iso/iec"",""27002"","","",""pci"",""dss"","")"",""relationship"",""management"","":"",""dod"","","",""public"",""sector"",""and"",""intelligence"",""community"",""change"",""management"",""it"",""security"",""&amp;"",""risk"",""management"",""("",""cissp"","")"",""program"","","",""product"",""&amp;"",""portfolio"",""management"",""("",""pmp"","")"",""data"",""analytics"",""management"",""("",""cchd"","")"",""itil"",""service"",
""management"",""("",""itilv3-expert"","")""],
""id"":""55c37f730d03382935e12767""}
</code></pre>

<p>My understanding is that the id, <code>55c37f730d03382935e12767</code> should be the id of the document, so doing the following ought to give me back a docVector.</p>

<pre><code>model.docvecs[""55c37f730d03382935e12767""]
</code></pre>

<p>Instead, this is what is outputed. </p>

<pre><code>&gt;&gt;&gt; model.docvecs[""55c37f730d03382935e12767""]
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/doc2vec.py"", line 341, in __getitem__
    return self.doctag_syn0[self._int_index(index)]
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/doc2vec.py"", line 315, in _int_index
    return self.max_rawint + 1 + self.doctags[index].offset
KeyError: '55c37f730d03382935e12767'
</code></pre>

<p>Trying to get most similar gives the following back</p>

<pre><code>&gt;&gt;&gt; model.docvecs.most_similar(""55c37f730d03382935e12767"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/doc2vec.py"", line 450, in most_similar
    raise KeyError(""doc '%s' not in trained set"" % doc)
KeyError: ""doc '55c37f730d03382935e12767' not in trained set""
</code></pre>

<p>What I'm trying to understand is how are doc vectors saved and what id's are used. What part of my approach isn't working above? </p>

<p>Now here's something interesting, if I do the following I get back similar doc vectors but they have no meaning to me. </p>

<pre><code>&gt;&gt;&gt; model.docvecs.most_similar(str(1))
[(u'8', 0.9000369906425476), (u'3', 0.8878246545791626), (u'7', 0.886141836643219), (u'2', 0.8834314942359924), (u'e', 0.8812381029129028), (u'a', 0.8648831248283386), (u'd', 0.8587037920951843), (u'0', 0.8413013219833374), (u'4', 0.8385311365127563), (u'c', 0.8290119767189026)]
</code></pre>
","python, gensim, doc2vec","<p><code>TaggedDocument.tags</code> should be a list of tags, not a string. By providing a string, the library sees it as a list-of-characters, so the single-characters are interpreted as the document-tags. Change your line:</p>

<pre><code>            yield TaggedDocument(words=jsData[""data""], tags=jsData[""id""])
</code></pre>

<p>...to...</p>

<pre><code>            yield TaggedDocument(words=jsData[""data""], tags=[jsData[""id""]])
</code></pre>

<p>...and you will likely see the expected results.</p>
",2,1,927,2016-10-25 02:41:08,https://stackoverflow.com/questions/40230532/gensim-doc2vec-documents-not-found-by-id
Printing topic distribution after LDA using gensim,"<p>I have made a sample program for getting topic distribution per document after doing LDA using gensim</p>

<pre><code>documents = [""Apple is releasing a new product"", 
             ""Amazon sells many things"",
             ""Microsoft announces Nokia acquisition""]   

stoplist=[""is"",""are"",""am"",""were"",""a"",""me"",""I""]

texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, update_every=1, chunksize=10000, passes=1)
lda.print_topics(2)
</code></pre>

<p>But the program is not printing anything.. Any changes required?</p>
","python, lda, gensim","<p>In case someone needs , found the answer  .. logger has to be used before </p>

<pre><code>logging.basicConfig(format='%(message)s', level=logging.INFO)
</code></pre>
",0,1,346,2016-10-29 11:42:13,https://stackoverflow.com/questions/40318719/printing-topic-distribution-after-lda-using-gensim
gensim LDA module : Always getting uniform topical distribution while predicting,"<p>I have a set of documents and I want to know the topic distribution for each document (for different values of number of topics). I have taken a toy program from <a href=""https://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda"">this question</a>.
I have first used LDA provided by gensim and then I am again giving test data as my training data itself to get the topic distribution of each doc in training data . But I am getting uniform topic distribution always.</p>

<p>Here is the toy code I used</p>

<pre><code>import gensim
import logging
logging.basicConfig(filename=""logfile"",format='%(message)s', level=logging.INFO)


def get_doc_topics(lda, bow):
    gamma, _ = lda.inference([bow])
    topic_dist = gamma[0] / sum(gamma[0])  # normalize distribution

documents = ['Human machine interface for lab abc computer applications',
             'A survey of user opinion of computer system response time',
             'The EPS user interface management system',
             'System and human system engineering testing of EPS',
             'Relation of user perceived response time to error measurement',
             'The generation of random binary unordered trees',
             'The intersection graph of paths in trees',
             'Graph minors IV Widths of trees and well quasi ordering',
             'Graph minors A survey']

texts = [[word for word in document.lower().split()] for document in documents]
dictionary = gensim.corpora.Dictionary(texts)
id2word = {}
for word in dictionary.token2id:    
    id2word[dictionary.token2id[word]] = word
mm = [dictionary.doc2bow(text) for text in texts]
lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=2, update_every=1, chunksize=10000, passes=1,minimum_probability=0.0)

newdocs=[""human system""]
print lda[dictionary.doc2bow(newdocs)]

newdocs=[""Human machine interface for lab abc computer applications""] #same as 1st doc in training
print lda[dictionary.doc2bow(newdocs)]
</code></pre>

<p>Here is the output:</p>

<pre><code>[(0, 0.5), (1, 0.5)]
[(0, 0.5), (1, 0.5)]
</code></pre>

<p>I have checked with some more examples but all ended up giving the same equiprobable result.</p>

<p>Here is the logfile generated(i.e output of logger)</p>

<pre><code>adding document #0 to Dictionary(0 unique tokens: [])
built Dictionary(42 unique tokens: [u'and', u'minors', u'generation', u'testing', u'iv']...) from 9 documents (total 69 corpus positions)
using symmetric alpha at 0.5
using symmetric eta at 0.5
using serial LDA version on this node
running online LDA training, 2 topics, 1 passes over the supplied corpus of 9 documents, updating model once every 9 documents, evaluating perplexity every 9 documents, iterating 50x with a convergence threshold of 0.001000
too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
-5.796 per-word bound, 55.6 perplexity estimate based on a held-out corpus of 9 documents with 69 words
PROGRESS: pass 0, at document #9/9
topic #0 (0.500): 0.057*""of"" + 0.043*""user"" + 0.041*""the"" + 0.040*""trees"" + 0.039*""interface"" + 0.036*""graph"" + 0.030*""system"" + 0.027*""time"" + 0.027*""response"" + 0.026*""eps""
topic #1 (0.500): 0.088*""of"" + 0.061*""system"" + 0.043*""survey"" + 0.040*""a"" + 0.036*""graph"" + 0.032*""trees"" + 0.032*""and"" + 0.032*""minors"" + 0.031*""the"" + 0.029*""computer""
topic diff=0.539396, rho=1.000000
</code></pre>

<p>It says ' too few updates, training might not converge' so I have tried increasing no of passes to 1000 but the output is still same.
(though it is not related to convergence , I have also tried increasing no of topics)</p>
","python, lda, gensim","<p>The problem is in transforming the variable <code>newdocs</code> into a gensim document. <code>dictionary.doc2bow()</code> does indeed expect a list but a list of words. You provide a list of documents so it interprets ""human system"" as a word <em>but</em> there is no such word in the training set so it ignores it. To make my point clearer see the output of the following code</p>

<pre><code>import gensim
documents = ['Human machine interface for lab abc computer applications',
             'A survey of user opinion of computer system response time',
             'The EPS user interface management system',
             'System and human system engineering testing of EPS',
             'Relation of user perceived response time to error measurement',
             'The generation of random binary unordered trees',
             'The intersection graph of paths in trees',
             'Graph minors IV Widths of trees and well quasi ordering',
             'Graph minors A survey']

texts = [[word for word in document.lower().split()] for document in documents]
dictionary = gensim.corpora.Dictionary(texts)

print dictionary.doc2bow(""human system"".split())
print dictionary.doc2bow([""human system""])
print dictionary.doc2bow([""human""])
print dictionary.doc2bow([""foo""])
</code></pre>

<p>So to correct the above code all you have to do is change <code>newdocs</code> according to the following</p>

<pre><code>newdocs = ""human system"".lower().split()
newdocs = ""Human machine interface for lab abc computer applications"".lower().split()
</code></pre>

<p>Oh, by the way the behaviour you observe, getting the same probabilities, is simply the topic distribution of the empty document, a uniform distribution that is.</p>
",2,3,847,2016-11-01 08:44:08,https://stackoverflow.com/questions/40356631/gensim-lda-module-always-getting-uniform-topical-distribution-while-predicting
gensim word2vec accessing in/out vectors,"<p>In the word2vec model, there are two linear transforms that take a word in vocab space to a hidden layer (the ""in"" vector), and then back to the vocab space (the ""out"" vector). Usually this out vector is discarded after training. I'm wondering if there's an easy way of accessing the out vector in gensim python? Equivalently, how can I access the out matrix?</p>

<p>Motivation: I would like to implement the ideas presented in this recent paper: <a href=""https://arxiv.org/pdf/1602.01137v1.pdf"" rel=""noreferrer"">A Dual Embedding Space Model for Document Ranking</a></p>

<p>Here are more details. From the reference above we have the following word2vec model:</p>

<p><a href=""https://i.sstatic.net/OpupG.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/OpupG.png"" alt=""enter image description here""></a></p>

<p>Here, the input layer is of size $V$, the vocabulary size, the hidden layer is of size $d$, and an output layer of size $V$. The two matrices are W_{IN} and W_{OUT}. <em>Usually</em>, the word2vec model keeps only the W_IN matrix. This is what is returned where, after training a word2vec model in gensim, you get stuff like:</p>

<blockquote>
  <p>model['potato']=[-0.2,0.5,2,...] </p>
</blockquote>

<p>How can I access, or retain W_{OUT}? This is likely quite computationally expensive, and I'm really hoping for some built in methods in gensim to do this because I'm afraid that if I code this from scratch, it would not give good performance.</p>
","python, gensim","<p>While this might not be a proper answer (can't comment yet) and noone pointed this out, take a look <a href=""https://groups.google.com/forum/#!searchin/gensim/access$20input$20embeddings%7Csort:relevance/gensim/TzlqaVdZ_FA/y89Tt6kPGwAJ"" rel=""noreferrer"">here</a>. The creator seems to answer a similar question. Also that's the place where you have a higher chance for a valid answer. </p>

<p>Digging around in the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1643"" rel=""noreferrer"">link</a> he posted in the word2vec source code you could change the syn1 deletion to suit your needs. Just remember to delete it after you're done, since it proves to be a memory hog.</p>
",10,18,5609,2016-11-07 06:03:00,https://stackoverflow.com/questions/40458742/gensim-word2vec-accessing-in-out-vectors
word vector and paragraph vector query,"<p>I am trying to understand relation between word2vec and doc2vec vectors in Gensim's implementation. In my application, I am tagging multiple documents with same label (topic), I am training a doc2vec model on my corpus using dbow_words=1 in order to train word vectors as well. I have been able to obtain similarities between word and document vectors in this fashion which does make a lot of sense
For ex. getting documents labels similar to a word-
doc2vec_model.docvecs.most_similar(positive = [doc2vec_model[""management""]], topn = 50))</p>

<p>My question however is about theoretical interpretation of computing similarity between word2vec and doc2vec  vectors. Would it be safe to assume that when trained on the same corpus with same dimensionality (d = 200), word vectors and document vectors can always be compared to find similar words for a document label or similar document labels for a word. Any suggestion/ideas are most welcome.</p>

<p>Question 2: My other questions is about impact of high/low frequency of a word in final word2vec model. If wordA and wordB have similar contexts in a particular doc label(set) of documents but wordA has much higher frequency than wordB, would wordB have higher similarity score with the corresponding doc label or not. I am trying to train multiple word2vec models by sampling corpus in a temporal fashion and want to know if the hypothesis that as words get more and more frequent, assuming context relatively stays similar, similarity score with a document label would also increase. Am I wrong to make this assumption? Any suggestions/ideas are very welcome.</p>

<p>Thanks,
Manish</p>
","similarity, gensim, word2vec, temporal, doc2vec","<p>In a training mode where word-vectors and doctag-vectors are interchangeably used during training, for the same surrounding-words prediction-task, they tend to be meaningfully comparable. (Your mode, DBOW with interleaved skip-gram word-training, fits this and is the mode used by the paper '<a href=""https://arxiv.org/abs/1507.07998"" rel=""nofollow noreferrer"">Document Embedding with Paragraph Vectors</a>'.)</p>

<p>Your second question is abstract and speculative; I think you'd have to test those ideas yourself. The Word2Vec/Doc2Vec processes train the vectors to be good at certain mechanistic word-prediction tasks, subject to the constraints of the model and tradeoffs with other vectors' quality. That the resulting spatial arrangement happens to be then useful for other purposes – ranked/absolute similarity, similarity along certain conceptual lines, classification, etc. – is then just an observed, pragmatic benefit. It's a 'trick that works', and might yield insights, but many of the ways models change in response to different parameter choices or corpus characteristics haven't been theoretically or experimentally worked-out. </p>
",1,0,633,2016-11-07 18:30:20,https://stackoverflow.com/questions/40472070/word-vector-and-paragraph-vector-query
"python gensim TypeError: coercing to Unicode: need string or buffer, list found","<p>So I believe despite this being a common issue with many similar questions (especially on stackoverflow), the main reason behind this issue varies in each case</p>

<p>In my case I have a method named <code>readCorpus</code> (<strong>find code below</strong>) it reads a list of 21 files, extract docs from each file then yield them</p>

<p>The yield operation occurs at the end of reading each file</p>

<p>I have another method named <code>uploadCorpus</code> (<strong>find code below</strong>). The main aim of this method is to upload that corpus. </p>

<p>Obviously the main reason behind using yield is that the corpus can be very large and I only need to read it once.</p>

<p>Once I run the method <code>uploadCorpus</code> I receive the error below</p>

<p><code>TypeError: coercing to Unicode: need string or buffer, list found</code></p>

<p>The erros occurs at the line <code>self.readCorpus()])</code>. </p>

<p>Reading similar problems I came to understand that it happens when a list is misplaced .. I tried to uplate the line of question here to <code>docs for docs in self.readCorpus()])</code> but I ended with the same issue</p>

<p><strong>My code (uploadCorpus)</strong></p>

<pre><code>def uploadCorpus(self):
        #convert docs to corpus
        print ""uploading""

        utils.upload_chunked(
            self.service,
            [{'id': 'doc_%i' % num, 'tokens': utils.simple_preprocess(doc)}
            for num, doc in enumerate([ 
                self.readCorpus()])
                ],
            chunksize=1000) # send 1k docs at a time
</code></pre>

<p><strong>My code readCorpus()</strong></p>

<pre><code>def readCorpus(self):
    path = '../data/reuters'
    doc=''
    docs = []
    docStart=False

    fileCount=0

    print 'Reading Corpus'
    for name in glob.glob(os.path.join(path, '*.sgm')):
        print 'Reading File| ' + name
        docCount=0
        for line in open(name):
            if(len(re.findall(r'&lt;BODY&gt;', line)) &gt; 0 ): 
                docStart = True
                pattern = re.search(r'&lt;BODY&gt;.*', line)
                doc+= pattern.group()[6:]

            if(len(re.findall(r'&lt;/BODY&gt;\w*', line)) &gt; 0 ):
                docStart = False
                docs.append(doc)
                doc=''
                docCount+=1
                continue
                #break
            if(docStart):
                doc += line

        fileCount+=1
        print 'docuemnt[%d][%d]'%(fileCount,docCount)
        yield docs
        docs = []
</code></pre>
","python, python-2.7, typeerror, iterable, gensim","<p>The line below is expecting an iterable object .. where the <code>readCorpus</code> function was supposed to be a generator using the keyword <code>yield</code></p>

<pre><code>self.readCorpus()
</code></pre>

<p>However the <code>readCorpus</code> function was not behaving the way a generator is supposed to be because of a poor implementation of the <code>yield</code> keyword. </p>

<p>The current implementation yield an array of items every 1000 loop iterations while the correct way is yield item by item.</p>

<p>Hence the readCorpus needs to be modified as following</p>

<pre><code>def readCorpus(self):
        path = '../data/reuters'
        doc=''
        docStart=False

        fileCount=0

        print 'Reading Corpus'
        for name in glob.glob(os.path.join(path, '*.sgm')):
            print 'Reading File| ' + name
            docCount=0
            for line in open(name):
                if(len(re.findall(r'&lt;BODY&gt;', line)) &gt; 0 ): 
                    docStart = True
                    pattern = re.search(r'&lt;BODY&gt;.*', line)
                    doc+= pattern.group()[6:]

                if(len(re.findall(r'&lt;/BODY&gt;\w*', line)) &gt; 0 ):
                    docStart = False
                    #docs.append(doc)
                    yield doc
                    doc=''
                    docCount+=1
                    continue
                    #break
                if(docStart):
                    doc += line

            fileCount+=1
            print 'docuemnt[%d][%d]'%(fileCount,docCount)
</code></pre>
",0,0,640,2016-11-16 21:55:53,https://stackoverflow.com/questions/40643082/python-gensim-typeerror-coercing-to-unicode-need-string-or-buffer-list-found
use a.all() or a.any() error while trying to use gensim word2vec,"<p>I've been trying to run an example of how t use word2vec from the gensim library of python but I keep getting this error </p>

<pre><code>    ValueError: The truth value of an array with more than one element is   ambiguous. Use a.any() or a.all()
</code></pre>

<p>This is my code, it's just a simple example :</p>

<pre><code>    from gensim.models import Word2Vec
    sentences = [['first', 'sentence'], ['second', 'sentence']]
    # train word2vec on the two sentences
    model = Word2Vec(sentences, min_count=1)
</code></pre>

<p>Note: I've made sure that gensim is installed with all its dependencies.</p>
","python-2.7, gensim, word2vec","<p>I had the same exact problem too, what i did was installing python-dev package then re-installing gensim, somehow that worked, i'm on ubuntu so this is what i did :</p>

<pre><code>sudo apt-get install python-dev
sudo pip uninstall gensim
sudo pip install gensim
</code></pre>

<p>when i run this :</p>

<pre><code>model = gensim.models.Word2Vec(sentences=listSentence,min_count=2,window=3,size=20,workers=1)
print model['Brasil']
</code></pre>

<p>it worked and i got the result vector :</p>

<pre><code>[-0.01635483  0.02224622 -0.01865266  0.02168317 -0.01231722 -0.0207897
 -0.0014509   0.00264822 -0.01889374 -0.02109174 -0.00244757  0.00024959
 -0.00898884 -0.01826199 -0.01361686 -0.01770178 -0.02431025 -0.01903439
 -0.00775641  0.02353667]
</code></pre>
",0,2,1898,2016-11-18 06:53:42,https://stackoverflow.com/questions/40671057/use-a-all-or-a-any-error-while-trying-to-use-gensim-word2vec
ValueError: cannot compute LDA over an empty collection (no terms),"<p>Getting this error in python when trying to compute lda for a smaller size of corpus but works fine in other cases.</p>

<p>The size of corpus is 15 and I tried setting the number of topic to 5 then reduced it to 2 but it still gives the same error : <strong>ValueError: cannot compute LDA over an empty collection (no terms)</strong></p>

<p>getting error at this line :     <code>lda = models.LdaModel(corpus, num_topics=topic_number, id2word=dictionary, passes=passes)</code></p>

<p>where corpus is <code>corpus = [dictionary.doc2bow(text) for a, id, text, s_date, e_date, qd, qd_perc in texts]</code></p>

<p>Why is it giving no terms?</p>
","python, gensim, lda, topic-modeling","<p>Finally figured it out. The issue with small documents is that if you try to filter the extremes from dictionary, you might end up with empty lists in corpus.<code>corpus = [dictionary.doc2bow(text)]</code>.</p>
<p>So the values of parameters in <code>dictionary.filter_extremes(no_below=2, no_above=0.1)</code> needs to be selected accordingly and carefully before <code>corpus = [dictionary.doc2bow(text)]</code></p>
<p>I just removed the filter extremes and lda model runs fine now. Though I will change the parameter values in filter extreme and use it later.</p>
",4,2,7323,2016-11-28 09:18:15,https://stackoverflow.com/questions/40840731/valueerror-cannot-compute-lda-over-an-empty-collection-no-terms
Rename gensim Word2Vec words with mapping,"<p>I want to replace the words of my gensim Word2Vec model with a mapping.</p>

<p><strong>Example</strong></p>

<p>My current model has the word <code>'foo'</code> that maps to a vector: </p>

<pre><code>&gt;&gt;&gt; model['foo']
[1.0 0.0]
</code></pre>

<p>I have the mapping: <code>d = {'foo': 'bar', ...}</code></p>

<p>How can I rebuild the model with this new mapping such that </p>

<pre><code>&gt;&gt;&gt; model['bar']  # in place of 'foo'
[1.0 0.0]
</code></pre>
","python, gensim, word2vec","<p>One solution is to save the model in the C-based word2vec format and replace the original words with a mapping of the new words using <code>awk</code>.</p>

<p>Assume we have a file mapping of the form:</p>

<pre><code>$ cat map.txt
foo:bar
...
</code></pre>

<p>We can recreate the model via: </p>

<pre><code>import subprocess as sp
import shlex

from gensim.models import Word2Vec

model.save_word2vec_format('embeddings.txt', binary=False)

CMD = r""""""
awk -F'[ ]|:' 'FNR==NR {a[$1]=$2; next} FNR==1{print $0} FNR!=1{$1=a[$1]; print $0}' map.txt embeddings.txt
""""""

with open('new_embeddings.txt', 'w') as f:
    p = sp.Popen(shlex.split(CMD), stdout=f)

new_model = Word2Vec.load_word2vec_format('new_embeddings.txt')

new_model.create_binary_tree()
</code></pre>

<p>As an aside my mapping was actually an array where I was training on the index of the word in some array <code>arr</code>. I created the map file using numpy:</p>

<pre><code>import numpy as np

np.savetxt('map.txt', np.c_[np.arange(arr.size), arr], '%d:%s')
</code></pre>
",1,2,393,2016-12-02 15:56:07,https://stackoverflow.com/questions/40936197/rename-gensim-word2vec-words-with-mapping
Downloading the image produced by LDAvis library,"<p>I am using the topic visualization library LDAvis:</p>

<pre><code>## visualization of the topics
import pyLDAvis
import pyLDAvis.gensim
pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
</code></pre>

<p>which produces an image of the Principal Components of the topics unveiled by the LDA (Latent Dirichlet Allocation) model. I will like to download the image but I am stuck. Any help much appreciated it!</p>
","python, ipython, gensim, lda","<p>You can save the HTML file like this in Python: </p>

<pre><code>vis_data = pyLDAvis.gensim.prepare(lda, corpus, dictionary)
pyLDAvis.save_html(vis_data, 'output_filename.html')
</code></pre>

<p>Or you can use SVG Crowbar to download the SVG node from the page it's already loaded in.</p>
",6,0,3052,2016-12-12 12:55:05,https://stackoverflow.com/questions/41101424/downloading-the-image-produced-by-ldavis-library
Gensim Word2Vec model: Cut dimensions,"<p>I have a trained word2vec models in geinsim with 300 dimensions and would like to cut the dimensions to 100 (simply drop the last 200 dimensions). What is the easiest and most efficient way using python?</p>
","python, python-3.x, gensim, word2vec","<p>You could save the output model in the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.save_word2vec_format"" rel=""nofollow noreferrer"">word2vec format</a>. Make sure to save it as a text file (.txt). The word2vec format is as follows</p>

<p>First line is <code>&lt;vocabulary_size&gt; &lt;embedding_size&gt;</code>. In your case the <code>&lt;embedding_size&gt;</code> will be <code>300</code>.
Rest of the lines will be <code>&lt;word&gt;&lt;TAB&gt;&lt;300 floating point numbers space separated&gt;</code>. Now you can easily parse this file in python and discard the last 200 floating points from each of the lines. Make sure to update the <code>&lt;embedding_size&gt;</code> in your first line. Save this as a new file (optional). Now you can load this new file as a fresh word2vec model using <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.load_word2vec_format"" rel=""nofollow noreferrer"">load_word2vec_format()</a>. </p>
",5,2,1576,2016-12-13 20:23:03,https://stackoverflow.com/questions/41129933/gensim-word2vec-model-cut-dimensions
Get weight matrices from gensim word2Vec,"<p>I am using gensim word2vec package in python.
I would like to retrieve the <code>W</code> and <code>W'</code> weight matrices that have been learn during the skip-gram learning.</p>
<p>It seems to me that <code>model.syn0</code> gives me the first one but I am not sure how I can get the other one. Any idea?</p>
<p>I would actually love to find any exhaustive documentation on models accessible attributes because the official one does not seem to be precise (for instance <code>syn0</code> is not described as an attribute)</p>
","python, machine-learning, nlp, word2vec, gensim","<p>The <code>model.wv.syn0</code> contains the <em>input</em> embedding matrix. <em>Output</em> embedding is stored in <code>model.syn1</code> when it's trained with <a href=""http://building-babylon.net/2017/08/01/hierarchical-softmax/"" rel=""noreferrer"">hierarchical softmax</a> (<code>hs=1</code>) or in <code>model.syn1neg</code> when it uses negative sampling (<code>negative&gt;0</code>). That's it! When both hierarchical softmax and negative sampling are not enabled, <code>Word2Vec</code> uses a single weight matrix <code>model.wv.syn0</code> for training.</p>

<p>See also a related discussion <a href=""https://stackoverflow.com/q/42554289/712995"">here</a>.</p>
",14,16,9101,2016-12-15 11:19:11,https://stackoverflow.com/questions/41162876/get-weight-matrices-from-gensim-word2vec
What is the difference between gensim LabeledSentence and TaggedDocument,"<p>Please help me in understanding the difference between how <code>TaggedDocument</code> and <code>LabeledSentence</code> of <code>gensim</code> works. My ultimate goal is Text Classification using <code>Doc2Vec</code> model and any classifier. I am following this <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""noreferrer"">blog</a>!</p>

<pre><code>class MyLabeledSentences(object):
    def __init__(self, dirname, dataDct={}, sentList=[]):
        self.dirname = dirname
        self.dataDct = {}
        self.sentList = []
    def ToArray(self):       
        for fname in os.listdir(self.dirname):            
            with open(os.path.join(self.dirname, fname)) as fin:
                for item_no, sentence in enumerate(fin):
                    self.sentList.append(LabeledSentence([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no]))
        return sentList


class MyTaggedDocument(object):
    def __init__(self, dirname, dataDct={}, sentList=[]):
        self.dirname = dirname
        self.dataDct = {}
        self.sentList = []
    def ToArray(self):       
        for fname in os.listdir(self.dirname):            
            with open(os.path.join(self.dirname, fname)) as fin:
                for item_no, sentence in enumerate(fin):
                    self.sentList.append(TaggedDocument([w for w in sentence.lower().split() if w in stopwords.words('english')], [fname.split('.')[0].strip() + '_%s' % item_no]))
        return sentList

sentences = MyLabeledSentences(some_dir_name)
model_l = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5,     workers=7)
sentences_l = sentences.ToArray()
model_l.build_vocab(sentences_l )
for epoch in range(15): # 
    random.shuffle(sentences_l )
    model.train(sentences_l )
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model_l.alpha 

sentences = MyTaggedDocument(some_dir_name)
model_t = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5, workers=7)
sentences_t = sentences.ToArray()
model_l.build_vocab(sentences_t)
for epoch in range(15): # 
    random.shuffle(sentences_t)
    model.train(sentences_t)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model_l.alpha
</code></pre>

<p>My question is <code>model_l.docvecs['some_word']</code> is same as <code>model_t.docvecs['some_word']</code>?
Can you provide me weblink of good sources to get a grasp on how <code>TaggedDocument</code> or <code>LabeledSentence</code> works.</p>
","gensim, text-classification, word2vec, doc2vec","<p><code>LabeledSentence</code> is an older, deprecated name for the same simple object-type to encapsulate a text-example that is now called <code>TaggedDocument</code>. Any objects that have <code>words</code> and <code>tags</code> properties, each a list, will do. (<code>words</code> is always a list of strings; <code>tags</code> can be a mix of integers and strings, but in the common and most-efficient case, is just a list with a single id integer, starting at 0.)</p>

<p><code>model_l</code> and <code>model_t</code> will serve the same purposes, having trained on the same data with the same parameters, using just different names for the objects. But the vectors they'll return for individual word-tokens (<code>model['some_word']</code>) or document-tags (<code>model.docvecs['somefilename_NN']</code>) will likely be different – there's randomness in Word2Vec/Doc2Vec initialization and training-sampling, and introduced by ordering-jitter from multithreaded training.  </p>
",7,8,4641,2016-12-16 10:33:16,https://stackoverflow.com/questions/41182372/what-is-the-difference-between-gensim-labeledsentence-and-taggeddocument
Encoding issue in python while using w2v,"<p>I'm writing my first app in python to use word2vec model.
Here is my simple code</p>

<pre><code>import gensim, logging
import sys
import warnings
from gensim.models import Word2Vec

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

def main(): 
    ####LOAD MODEL
    model = Word2Vec.load_word2vec_format('models/vec-cbow.txt', binary=False)  
    model.similarity('man', 'women')

if __name__ == '__main__':
    with warnings.catch_warnings():
        warnings.simplefilter(""error"")
        #warnings.simplefilter(""ignore"")
    main()
</code></pre>

<p>I getting this the following error:</p>

<pre><code>UnicodeDecodeError: 'utf8' codec can't decode bytes in position 96-97: invalid continuation byte 
</code></pre>

<p>I tried solving it by adding these two lines, but I'm still getting the error. </p>

<pre><code>reload(sys)  # Reload does the trick!
sys.setdefaultencoding('UTF8') #UTF8 #latin-1
</code></pre>

<p>The w2v model was trained on English sentences.</p>

<p>EDIT: Here is the full stack:</p>

<pre><code>**%run ""...\getSimilarity.py""**
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
**...\getSimilarity.py in &lt;module&gt;()**
     64         warnings.simplefilter(""error"")
     65         #warnings.simplefilter(""ignore"")
---&gt; 66     main()

**...\getSimilarity.py in main()**
     30     ####LOAD MODEL
---&gt; 31     model = Word2Vec.load_word2vec_format('models/vec-cbow.txt', binary=False)  # C binary format
     32     model.similarity('man', 'women')

**...\AppData\Local\Enthought\Canopy\User\lib\site-packages\gensim-0.12.4-py2.7-win-amd64.egg\gensim\models\word2vec.pyc in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors)**
   1090             else:
   1091                 for line_no, line in enumerate(fin):
-&gt; 1092                     parts = utils.to_unicode(line.rstrip(), encoding=encoding, errors=unicode_errors).split("" "")
   1093                     if len(parts) != vector_size + 1:
   1094                         raise ValueError(""invalid vector on line %s (is this really the text format?)"" % (line_no))

**...\AppData\Local\Enthought\Canopy\User\lib\site-packages\gensim-0.12.4-py2.7-win-amd64.egg\gensim\utils.pyc in any2unicode(text, encoding, errors)**
    215     if isinstance(text, unicode):
    216         return text
--&gt; 217     return unicode(text, encoding, errors=errors)
    218 to_unicode = any2unicode
    219 

**...\AppData\Local\Enthought\Canopy\App\appdata\canopy-1.6.2.3262.win-x86_64\lib\encodings\utf_8.pyc in decode(input, errors)**
     14 
     15 def decode(input, errors='strict'):
---&gt; 16     return codecs.utf_8_decode(input, errors, True)
     17 
     18 class IncrementalEncoder(codecs.IncrementalEncoder):

**UnicodeDecodeError: 'utf8' codec can't decode bytes in position 96-97: invalid continuation byte** 
</code></pre>

<p>Any hints how to solve the problem?
Thanks in advance.</p>
","python, gensim, word2vec","<p>I found the solution simply by reading this <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ#q10-loading-a-word2vec-model-fails-with-unicodedecodeerror-utf-8-codec-cant-decode-bytes-in-position-"" rel=""nofollow noreferrer"">FAQ</a> page. 
""The strings (words) stored in your model are not valid utf8. By default, gensim decodes the words using the strict encoding settings, which results in the above exception whenever an invalid utf8 sequence is encountered.""</p>
",0,1,3575,2017-01-02 16:55:15,https://stackoverflow.com/questions/41430565/encoding-issue-in-python-while-using-w2v
Is there any way to get the vocabulary size from doc2vec model?,"<p>I am using gensim <code>doc2vec</code>. I want know if there is any efficient way to know the vocabulary size from doc2vec. One crude way is to count the total number of words, but if the data is huge(1GB or more) then this won't be an efficient way.</p>
","gensim, word2vec, doc2vec","<p>If <code>model</code> is your trained Doc2Vec model, then the number of unique word tokens in the surviving vocabulary after applying your <code>min_count</code> is available from:</p>

<pre><code>len(model.wv.vocab)
</code></pre>

<p>The number of trained document tags is available from:</p>

<pre><code>len(model.docvecs)
</code></pre>
",14,8,7167,2017-01-12 08:07:07,https://stackoverflow.com/questions/41607976/is-there-any-way-to-get-the-vocabulary-size-from-doc2vec-model
gensim Getting Started Error: No such file or directory: &#39;text8&#39;,"<p>I am learning about word2vec and GloVe model in python so I am going through this available <a href=""http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python"" rel=""noreferrer"">here</a>. </p>

<p>After I compiled these code step by step in Idle3:</p>

<pre><code>&gt;&gt;&gt;from gensim.models import word2vec
&gt;&gt;&gt;import logging
&gt;&gt;&gt;logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
&gt;&gt;&gt;sentences = word2vec.Text8Corpus('text8')
&gt;&gt;&gt;model = word2vec.Word2Vec(sentences, size=200)
</code></pre>

<p>I am getting this error : </p>

<pre><code>2017-01-13 11:15:41,471 : INFO : collecting all words and their counts
Traceback (most recent call last):
  File ""&lt;pyshell#4&gt;"", line 1, in &lt;module&gt;
    model = word2vec.Word2Vec(sentences, size=200)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 469, in __init__
    self.build_vocab(sentences, trim_rule=trim_rule)
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 533, in build_vocab
    self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 545, in scan_vocab
    for sentence_no, sentence in enumerate(sentences):
  File ""/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py"", line 1536, in __iter__
    with utils.smart_open(self.fname) as fin:
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 127, in smart_open
    return file_smart_open(parsed_uri.uri_path, mode)
  File ""/usr/local/lib/python3.5/dist-packages/smart_open-1.3.5-py3.5.egg/smart_open/smart_open_lib.py"", line 558, in file_smart_open
    return open(fname, mode)
FileNotFoundError: [Errno 2] No such file or directory: 'text8'
</code></pre>

<p>How do I rectify this ?
Thanks in advance for your help.</p>
","python, python-3.x, error-handling, gensim, word2vec","<p>It seems you're missing the file used here. Specifically, it is trying to open <code>text8</code> and can't find it (hence the <code>FileNotFoundError</code>).</p>

<p>You could download the file itself from <a href=""http://mattmahoney.net/dc/text8.zip"" rel=""nofollow noreferrer"">here</a> as is stated <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Text8Corpus"" rel=""nofollow noreferrer"">in the documentation for <code>Text8Corpus</code></a>:</p>

<pre><code>Docstring:      
Iterate over sentences from the ""text8"" corpus, unzipped from http://mattmahoney.net/dc/text8.zip .
</code></pre>

<p>and make it available. <em>Extract it</em> and then supply it as an argument to <code>Text8Corpus</code>:</p>

<pre><code>sentences = word2vec.Text8Corpus('/path/to/text8')
</code></pre>
",6,7,4729,2017-01-13 06:46:13,https://stackoverflow.com/questions/41628856/gensim-getting-started-error-no-such-file-or-directory-text8
Chunkize warning while installing gensim,"<p>I have installed gensim (through pip) in Python. After the installation is over I get the following warning:</p>

<blockquote>
  <p><strong>C:\Python27\lib\site-packages\gensim\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")</strong></p>
</blockquote>

<p>How can I rectify this? </p>

<p>I am unable to import word2vec from gensim.models due to this warning.</p>

<p>I have the following configurations: Python 2.7, gensim-0.13.4.1, numpy-1.11.3, scipy-0.18.1, pattern-2.6.</p>
","python, gensim","<p>You can suppress the message with this code <em>before</em> importing gensim:</p>

<pre><code>import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')

import gensim
</code></pre>
",34,16,24323,2017-01-15 06:43:50,https://stackoverflow.com/questions/41658568/chunkize-warning-while-installing-gensim
How to use word2vec with keras CNN (2D) to do text classification?,"<p>There is Convolution1D example <a href=""https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py"" rel=""nofollow noreferrer"">https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py</a> without word2vec.</p>

<p>Currently, I am using gensim to train word2vec model.</p>

<p>I want to use word2vec and keras cnn(2D not 1D) to do document classifacation(Chinese Text). I learned the basic flow of text classification in cnn and want to do a test.</p>

<h2>For example(the steps I imagine):</h2>

<ol>
<li><p>Use a good Cinese Tokenized Text Set to train word2vec model</p>

<pre><code>model = gensim.models.Word2Vec(new_sentences, workers=10, size=200, min_count=2)
</code></pre></li>
<li><p>Tokenize my sentences dataset to words lists dataset(the longest sentence has over 8000 words, shortest is less 50)</p>

<pre><code>1     ['你们', '好', '今天', '天气', '真', '好']
2     ['嗯', '对的']
...
9999  ['好', '就', '这样']
</code></pre></li>
<li><p>Use a method to transform words lists dataset to word2vec dataset</p>

<p>transform every word in every sencence to a vec by trained model.</p>

<pre><code>1     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
2     [[word2vec size=200], [word2vec size=200]]
...
9999  [[word2vec size=200], [word2vec size=200], [word2vec size=200]]
</code></pre></li>
<li><p>Pad  word2vec dataset (with size=200 zero array)</p>

<pre><code>1     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
2     [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
....
9999  [[word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200], [word2vec size=200]]
</code></pre></li>
<li><p>go to the  CNN (using Convolution2D)</p></li>
</ol>

<hr>

<p>I search for a long time, but can't find any way to do the step 3 ( after step 3,  the parameter and layers setting in step 5 is hard to understand too). </p>
","neural-network, deep-learning, keras, gensim, word2vec","<p><strong>Transforming a single sentence to a 2D vector</strong></p>

<p>Assuming you have a list of words and a model you can do:</p>

<pre><code>import numpy as np
sentence_vec = None
for word in sentence:
    word_vec = np.expand_dims(model[word], axis=0)
    if sentence_vec is None:
        sentence_vec = word_vec
    else:
        sentence_vec = np.concatenate((sentence_vec, word_vec), axis=0)
</code></pre>

<p>As for step 5 - it would be helpful if you listed what you are having trouble with. Basically you only need to do is change both 1D operations (Convolution1D, GlobalMaxPooling1D) to their 2D counter-parts .</p>
",1,3,2109,2017-01-17 07:05:39,https://stackoverflow.com/questions/41690885/how-to-use-word2vec-with-keras-cnn-2d-to-do-text-classification
What is gensim&#39;s &#39;docvecs&#39;?,"<p><a href=""https://i.sstatic.net/ofJqR.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/ofJqR.png"" alt=""Doc2Vec Figure 2""></a></p>

<p>The above picture is from <a href=""https://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""noreferrer"">Distributed Representations of Sentences and Documents</a>, the paper introducing Doc2Vec. I am using Gensim's implementation of Word2Vec and Doc2Vec, which are great, but I am looking for clarity on a few issues.</p>

<ol>
<li>For a given doc2vec model <code>dvm</code>, what is <code>dvm.docvecs</code>? My impression is that it is the averaged or concatenated vector that includes all of the word embedding <em>and</em> the paragraph vector, <code>d</code>. Is this correct, or is it d?</li>
<li>Supposing <code>dvm.docvecs</code> is not <code>d</code>, can one access d by itself? How?</li>
<li>As a bonus, how is <code>d</code> calculated? The paper only says:</li>
</ol>

<blockquote>
  <p>In our Paragraph Vector framework (see Figure 2), every
  paragraph is mapped to a unique vector, represented by a
  column in matrix D and every word is also mapped to a
  unique vector, represented by a column in matrix W.</p>
</blockquote>

<p>Thanks for any leads!</p>
","python, nlp, gensim, doc2vec","<p>The <code>docvecs</code> property of the Doc2Vec model holds all trained vectors for the 'document tags' seen during training. (These are also referred to as 'doctags' in the source code.)</p>

<p>In the most simple case, analogous to the Paragraph Vectors paper, each text example (paragraph) just has a serial number integer ID as its 'tag', starting at 0. This would be an index into the <code>docvecs</code> object – and the <code>model.docvecs.doctag_syn0</code> numpy array is essentially the same thing as the (capital) <em>D</em> in your excerpt from the Paragraph Vectors paper. </p>

<p>(Gensim also supports using string tokens as document tags, and multiple tags per document, and repeating tags across many of the training documents. For string tags, if any, they're mapped to indexes near the end of the <code>docvecs</code> by the dict <code>model.docvecs.doctags</code>.)</p>
",5,5,8278,2017-01-18 00:15:02,https://stackoverflow.com/questions/41709318/what-is-gensims-docvecs
How to filter out words in a corpus from a constrained vocabulary with gensim?,"<p>I am using gensim for topic modeling. I've created a corpus using </p>

<pre><code>wordDict = corpora.Dictionary(trimmedTextTokens)

gsCorpus = [wordDict.doc2bow(text) for text in trimmedTextTokens]
</code></pre>

<p>where trimmedTextTokens are the result of removing stop words. Now I want to filter out the terms from the corpus that are not in a list of a restricted or constructed vocabulary. Any ideas? Thank you!!</p>
","python, nlp, gensim, topic-modeling","<p>Assuming your restricted vocabulary list is in a variable named <code>restrictedVocabularyList</code> you could do:</p>

<pre><code>wordDict = corpora.Dictionary(trimmedTextTokens)

gsCorpus = [wordDict.doc2bow(text) for text in trimmedTextTokens if text in restrictedVocabularyList]
</code></pre>
",0,0,918,2017-01-18 20:56:50,https://stackoverflow.com/questions/41729287/how-to-filter-out-words-in-a-corpus-from-a-constrained-vocabulary-with-gensim
"Python, LDA : How to get the id of keywords instead of the keywords themselves with Gensim?","<p>I am applying the LDA method using Gensim to extract keywords from documents.
I can extract topics, and then assign these topics and key words associated to the documents.</p>

<p>I would like to have the ids of these terms (or key words) instead of the terms themselves. I know that <code>corpus[i]</code> extract a list of couples  [(term_id, term_frequency) ...] of document <code>i</code> but I can't see how could I use this in my code to extract only the ids and assign it to my results.</p>

<p>My code is as follows :</p>

<pre><code>ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=passes, minimum_probability=0)

# Assinging the topics to the document in corpus
lda_corpus = ldamodel[corpus]

# Find the threshold, let's set the threshold to be 1/#clusters,
# To prove that the threshold is sane, we average the sum of all probabilities:
scores = list(chain(*[[score for topic_id,score in topic] \
                     for topic in [doc for doc in lda_corpus]]))

threshold = sum(scores)/len(scores)
print(threshold)

for t in range(len(topic_tuple)):  

    key_words.append([topic_tuple[t][j][0] for j in range(num_words)])
    df_key_words = pd.DataFrame({'key_words' : key_words})

    documents_corpus.append([j for i,j in zip(lda_corpus,doc_set) if i[t][1] &gt; threshold])
    df_documents_corpus = pd.DataFrame({'documents_corpus' : documents_corpus})

    documents_corpus_id.append([i for d,i in zip(lda_corpus, doc_set_id) if d[t][1] &gt; threshold])
    df_documents_corpus_id = pd.DataFrame({'documents_corpus_id' : documents_corpus_id})


result.append(pd.concat([df_key_words, df_documents_corpus, df_documents_corpus_id ], axis=1))
</code></pre>

<p>Thank you in advance and ask me if more information are needed.</p>
","python, gensim, lda","<p>In case someone has the same issue that I had, here is the answer for a reverse map :</p>

<pre><code>reverse_map = dict((ldamodel.id2word[id],id) for id in ldamodel.id2word)
</code></pre>

<p>Thanks to bigdeeperadvisors</p>
",0,2,1096,2017-01-20 14:28:54,https://stackoverflow.com/questions/41765951/python-lda-how-to-get-the-id-of-keywords-instead-of-the-keywords-themselves-w
AttributeError: &#39;list&#39; object has no attribute &#39;lower&#39; gensim,"<p>I have a list of 10k words in a text file like so:</p>

<p>G15
KDN
C30A
Action Standard
Air Brush
Air Dilution</p>

<p>I am trying to convert them into lower cased tokens using this code for subsequent processing with GenSim:</p>

<pre><code>data = [line.strip() for line in open(""C:\corpus\TermList.txt"", 'r')]
texts = [[word for word in data.lower().split()] for word in data]
</code></pre>

<p>and I get the following callback:</p>

<pre><code>AttributeErrorTraceback (most recent call last)
&lt;ipython-input-84-33bbe380449e&gt; in &lt;module&gt;()
      1 data = [line.strip() for line in open(""C:\corpus\TermList.txt"", 'r')]
----&gt; 2 texts = [[word for word in data.lower().split()] for word in data]
      3 
AttributeError: 'list' object has no attribute 'lower'
</code></pre>

<p>Any suggestions on what I am doing wrong and how to correct it would be greatly appreciated!!! Thank you!!</p>
","python, string, split, gensim","<p>try:</p>

<pre><code>data = [line.strip() for line in open(""C:\corpus\TermList.txt"", 'r')]
texts = [[word.lower() for word in text.split()] for text in data]
</code></pre>

<p>you were trying to apply .lower() to data, which is a list. <br> .lower() can only be applied to strings.</p>
",21,6,134713,2017-01-24 13:21:52,https://stackoverflow.com/questions/41829323/attributeerror-list-object-has-no-attribute-lower-gensim
Export pyLDAvis graphs as standalone webpage,"<p>i am analysing text with topic modelling and using Gensim and pyLDAvis for that. Would like to share the results with distant colleagues, without a need for them to install python and all required libraries. 
Is there a way to export interactive graphs as HTML/JS files that could be uploaded to any web server?
I've found something mentioned in documentation, but have no idea how to implement it:
<strong><a href=""https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/_display.py"" rel=""noreferrer"">https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/_display.py</a></strong></p>
","python, gensim, lda, topic-modeling","<p>pyLDAvis.save_html should work:</p>

<pre><code>p = pyLDAvis.gensim.prepare(topic_model, corpus, dictionary)
pyLDAvis.save_html(p, 'lda.html')
</code></pre>
",53,24,21921,2017-01-30 13:10:29,https://stackoverflow.com/questions/41936775/export-pyldavis-graphs-as-standalone-webpage
how to get tf-id from w2v on gensim,"<p>I have a textual dataset on which I trained a <code>gensim</code> w2v model. Now I want to use those vectors to recive the tf-idf values for the words and documents in my data set. What is the right way to do it? I tried to followe the <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">tutorial</a> on gensim's site.</p>

<p>I expect something like <code>models.tfidfmodel(model.wv[model.wv.index2word])</code>
 but this fail since </p>

<p><code>File ""&lt;ipython-input-229-7946418f8a82&gt;"", line 1, in &lt;module&gt;
    models.tfidfmodel(model.wv[model.wv.index2word])
TypeError: 'module' object is not callable</code></p>

<p>does what I want makes since? Is BOW the only way to do that?</p>
","python-3.x, machine-learning, nlp, gensim, word2vec","<p>The tutorial you have linked to the model is given the corpus, i.e. the text (or transformed text) as a whole.</p>

<p>What you have <strong>tried</strong> to do is give the model the dictionary that the w2v model learned.</p>

<p>If what you want is </p>

<blockquote>
  <p>to recive the tf-idf values for the words and documents in my data
  set.</p>
</blockquote>

<p>Then you should simply pass it as such:</p>

<pre><code>tfidf = models.TfidfModel(corpus)
</code></pre>

<p>If what you actually want is to run the TF-IDF model on the <em>transformed</em> corpus, then you should first use your w2v to transform the corpus and then pass the transformed corpus to the tfidfmodel.</p>

<hr>

<p>Note that as the tfidf model simply calculates the word frequency there is nothing to be gained by giving it the transformed corpus and not the original one.</p>
",3,0,1374,2017-01-31 14:31:00,https://stackoverflow.com/questions/41960099/how-to-get-tf-id-from-w2v-on-gensim
How to get cython and gensim to work with pyspark,"<p>I'm running a Lubuntu 16.04 Machine with <code>gcc</code> installed. I'm not getting <code>gensim</code> to work with <code>cython</code> because when I train a <code>doc2vec model</code>, it is only ever trained with one worker which is dreadfully slow. </p>

<p>As I said <code>gcc</code> was installed from the start. I then maybe made the mistake and installed <code>gensim</code> before <code>cython</code>. I corrected that by forcing a reinstall of <code>gensim</code> via <code>pip</code>. With no effect still just one worker.</p>

<p>The machine is setup as a <code>spark</code> master and I interface with <code>spark</code> via <code>pyspark</code>.  It works something like this, <code>pyspark</code> uses <code>jupyter</code> and <code>jupyter</code> uses python 3.5. This way I get a <code>jupyter</code> interface to my cluster. Now I have no idea if this is the reason why i cant get <code>gensim</code> to work with <code>cython</code>. I don't execute any gensim code on the cluster, it is just more convenient to fire up <code>jupyter</code> to also do <code>gensim</code>. </p>
","python, python-3.x, pyspark, cython, gensim","<p>After digging deeper and trying things like loading the whole corpus into memory executing gensim in a different environment etc. all with no effect. It seems it is a problem with gensim that the code is only partial parallelized. This results in the workers not being able to fully utilize the CPU. See the issues on github <a href=""https://github.com/RaRe-Technologies/gensim/issues/532"" rel=""nofollow noreferrer"">link</a>.</p>
",0,0,1309,2017-02-04 11:45:01,https://stackoverflow.com/questions/42039964/how-to-get-cython-and-gensim-to-work-with-pyspark
load pre-trained word2vec model for doc2vec,"<p>I'm using gensim to extract feature vector from a document.
I've downloaded the pre-trained model from Google named <code>GoogleNews-vectors-negative300.bin</code> and I loaded that model using the following command:</p>

<pre><code>model = models.Doc2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>My purpose is to get a feature vector from a document. For a word, it's very easy to get the corresponding vector:</p>

<pre><code>vector = model[word]
</code></pre>

<p>However, I don't know how to do it for a document. Could you please help?</p>
","machine-learning, nlp, gensim, word2vec, doc2vec","<p>A set of word vectors (such as <code>GoogleNews-vectors-negative300.bin</code>) is neither necessary nor sufficient for the kind of text vectors (Le/Mikolov 'Paragraph Vectors') created by the Doc2Vec class. It instead expects to be trained with example texts to learn per-document vectors. Then, also, the trained model can be used to 'infer' vectors for other new documents.</p>

<p>(The Doc2Vec class only supports the <code>load_word2vec_format()</code> method because it inherits from the Word2Vec class – not because it needs that functionality.)</p>

<p>There's another simple kind of text vector that can be created by simply averaging all the words in the document, perhaps also according to some per-word significance weighting. But that's not what Doc2Vec provides.</p>
",1,3,1870,2017-02-08 16:58:43,https://stackoverflow.com/questions/42119237/load-pre-trained-word2vec-model-for-doc2vec
"Training wordvec in Tensorflow, importing to Gensim","<p>I am training a word2vec model from the tensorflow tutorial.</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a></p>

<p>After training I get the embedding matrix. I would like to save this and import it as a trained model in gensim.</p>

<p>To load a model in gensim, the command is:</p>

<pre><code>model = Word2Vec.load_word2vec_format(fn, binary=True)
</code></pre>

<p>But how do I generate the <code>fn</code> file from Tensorflow?</p>

<p>Thanks</p>
","python, machine-learning, tensorflow, gensim","<p>One way to is save the file in the non-binary Word2Vec format, which essentially looks like this:</p>

<pre><code>num_words vector_size  # this is the header
label0 x00 x01 ... x0N
label1 x10 x11 ... x1N
...
</code></pre>

<p>Example:</p>

<pre><code>2 3
word0 -0.000737 -0.002106 0.001851
word1 -0.000878 -0.002106 0.002834
</code></pre>

<p>Save the file and then load with kwarg <code>binary=False</code>:</p>

<pre><code>model = Word2Vec.load_word2vec_format(filename, binary=False)

print(model['word0'])
</code></pre>

<h2>Update</h2>

<p>New way to load model is:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

model = KeyedVectors.load_word2vec_format(model_path, binary=False)
</code></pre>
",2,3,1150,2017-02-12 10:38:17,https://stackoverflow.com/questions/42186543/training-wordvec-in-tensorflow-importing-to-gensim
Python tf-idf: fast way to update the tf-idf matrix,"<p>I have a dataset of several thousand rows of text, my target is to calculate the tfidf score and then cosine similarity between documents, this is what I did using gensim in Python followed the tutorial:</p>

<pre><code>dictionary = corpora.Dictionary(dat)
corpus = [dictionary.doc2bow(text) for text in dat]

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
index = similarities.MatrixSimilarity(corpus_tfidf)
</code></pre>

<p>Let's say we have the tfidf matrix and similarity built, when we have a new document come in, I want to query for its most similar document in our existing dataset.</p>

<p>Question: is there any way we can update the tf-idf matrix so that I don't have to append the new text doc to the original dataset and recalculate the whole thing again? </p>
","python, nlp, tf-idf, gensim, cosine-similarity","<p>I'll post my solution since there are no other answers. Let's say we are in the following scenario:</p>

<pre><code>import gensim
from gensim import models
from gensim import corpora
from gensim import similarities
from nltk.tokenize import word_tokenize
import pandas as pd

# routines:
text = ""I work on natural language processing and I want to figure out how does gensim work""
text2 = ""I love computer science and I code in Python""
dat = pd.Series([text,text2])
dat = dat.apply(lambda x: str(x).lower()) 
dat = dat.apply(lambda x: word_tokenize(x))


dictionary = corpora.Dictionary(dat)
corpus = [dictionary.doc2bow(doc) for doc in dat]
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]


#Query:
query_text = ""I love icecream and gensim""
query_text = query_text.lower()
query_text = word_tokenize(query_text)
vec_bow = dictionary.doc2bow(query_text)
vec_tfidf = tfidf[vec_bow]
</code></pre>

<p>if we look at:</p>

<pre><code>print(vec_bow)
[(0, 1), (7, 1), (12, 1), (15, 1)]
</code></pre>

<p>and:</p>

<pre><code>print(tfidf[vec_bow])
[(12, 0.7071067811865475), (15, 0.7071067811865475)]
</code></pre>

<p>FYI id and doc:</p>

<pre><code>print(dictionary.items())

[(0, u'and'),
 (1, u'on'),
 (8, u'processing'),
 (3, u'natural'),
 (4, u'figure'),
 (5, u'language'),
 (9, u'how'),
 (7, u'i'),
 (14, u'code'),
 (19, u'in'),
 (2, u'work'),
 (16, u'python'),
 (6, u'to'),
 (10, u'does'),
 (11, u'want'),
 (17, u'science'),
 (15, u'love'),
 (18, u'computer'),
 (12, u'gensim'),
 (13, u'out')]
</code></pre>

<p>Looks like the query only picked up existing terms and using pre-calculated weights to give you the tfidf score. So my workaround is to rebuild the model weekly or daily since it is fast to do so.</p>
",2,8,6002,2017-02-13 19:54:33,https://stackoverflow.com/questions/42212423/python-tf-idf-fast-way-to-update-the-tf-idf-matrix
Doc2Vec: Differentiate Sentence and Document,"<p>I am just playing around with Doc2Vec from gensim, analysing stackexchange dump to analyze semantic similarity of questions to identify duplicates.</p>

<p>The tutorial on <a href=""https://rare-technologies.com/doc2vec-tutorial/"" rel=""noreferrer"">Doc2Vec-Tutorial</a> seems to describe the input as tagged sentences.</p>

<p>But the original paper: <a href=""https://arxiv.org/pdf/1405.4053.pdf"" rel=""noreferrer"">Doc2Vec-Paper</a> claims that the method can be used to infer fixed length vectors of paragraphs/documents.</p>

<p>Can someone explain the difference between a sentence and a document in this context, and how i would go about inferring paragraph vectors.</p>

<p>Since a question can sometimes span multiple sentences,
I thought, during training i will give sentences arising from the same question the same tags, but then how would i do this to infer_vector on unseen questions? </p>

<p>And this notebook : <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""noreferrer"">Doc2Vec-Notebook</a></p>

<p>seems to be training vectors on TRAIN and TEST docs, can someone explain the rationale behind this and should i do the same?</p>
","python, gensim, doc2vec","<p>Gensim's Doc2Vec expects you to provide text examples of the same object-shape as the example TaggedDocument class: having both a <code>words</code> and a <code>tags</code> property. </p>

<p>The <code>words</code> are an ordered sequence of string tokens of the text – they might be a single sentence worth, or a paragraph, or a long document, it's up to you. </p>

<p>The <code>tags</code> are a list of tags to be learned from the text – such as plain ints, or string-tokens, that somehow serve to name the corresponding texts. In the original 'Paragraph Vectors' paper, they were just unique IDs for each text – such as integers monotonically increasing from 0. (So the first TaggedDocument might have a <code>tags</code> of just <code>[0]</code>, the next <code>[1]</code>, etc.)</p>

<p>The algorithm just works on chunks of text, without any idea of what a sentence/paragraph/document etc might be. (Just consider them all 'documents' for the purpose of Doc2Vec, with you deciding what's the right kind of 'document' from your corpus.) It's even common for the tokenization to retain punctuation, such as the periods between sentences, as standalone tokens. </p>

<p>Inference occurs via the <code>infer_vector()</code> method, which takes a mandatory parameter <code>doc_words</code>, which should be a list-of-string-tokens just like those that were supplied as text <code>words</code> during training. </p>

<p>You don't supply any tags on inferred text: Doc2Vec just gives you back a raw vector that, within the relationships learned by the model, fits the text well. (That is: the vector is good at predicting the text's words, in the same way that the vectors and internal model weights learned during bulk training were good at prediction the training texts' words.)</p>

<p>Note that many have found better results from inference by increasing the optional <code>steps</code> parameter (and possibly decreasing the inference starting <code>alpha</code> to be more like the bulk-training starting alpha, 0.025 to 0.05). </p>

<p>The doc2vec-IMDB demo notebook tries to reproduce one of the experiments from the original Paragraph Vectors paper, so it's following what's described there, and a demo script that one of the authors (Mikolov) once released. Since 'test' documents (withoout their target-labels/known-sentiments) may still be available, at training time, to help improve the text-modelling, it can be reasonable to include their raw texts during the unsupervised Doc2Vec training. (Their known-labels are <em>not</em> used when training the classifier which uses the doc-vectors.)</p>

<p>(Note that at the moment, February 2017, the doc2vec-IMDB demo notebook is a little out-of-date compared to the current gensim Doc2Vec defaults &amp; best-practices – in particular the models aren't given the right explicit <code>iter=1</code> value to make the later manual loop-and-<code>train()</code> do just the right umber of training passes.)</p>
",5,6,2359,2017-02-15 06:55:42,https://stackoverflow.com/questions/42242521/doc2vec-differentiate-sentence-and-document
Extract topic word probability matrix in gensim LdaModel,"<p>I have the LDA model and the document-topic probabilities.</p>

<pre><code># build the model on the corpus
ldam = LdaModel(corpus=corpus, num_topics=20, id2word=dictionary) 
# get the document-topic probabilities
theta, _ = ldam.inference(corpus)
</code></pre>

<p>I also need the distribution of words for all the topics i.e. a topic-word probability matrix. Is there a way to extract this information? </p>

<p>Thanks!</p>
","python, gensim, lda, topic-modeling","<p>The topics-term matrix (lambda) is accessible via : </p>

<pre><code>topics_terms = ldam.state.get_lambda()
</code></pre>

<p>If you want a probability distribution just normalize it : </p>

<pre><code>topics_terms_proba = np.apply_along_axis(lambda x: x/x.sum(),1,topics_terms)
</code></pre>
",6,4,2677,2017-02-17 05:09:35,https://stackoverflow.com/questions/42289858/extract-topic-word-probability-matrix-in-gensim-ldamodel
SVD using Scikit-Learn and Gensim with 6 million features,"<p>I am trying to classify paragraphs based on their sentiments. I have training data of 600 thousand documents. When I convert them to <code>Tf-Idf</code> vector space with words as analyzer and ngram range as 1-2 there are almost 6 million features. So I have to do Singular value decomposition (SVD) to reduce features.</p>

<p>I have tried gensim and sklearn's SVD feature. Both work fine for feature reduction till 100 but as soon as I try for 200 features they throw memory error. </p>

<p>Also I have not used entire document (600 thousand) as training data, I have taken 50000 documents only. So essentially my training matrix is:
50000 * 6 million and want to reduce it to  50000 * (100 to 500) </p>

<p>Is there any other way I can implement it in python, or do I have to implement sparks mllib SVD(written for only java and scala) ? If Yes, how much faster will it be?</p>

<p>System specification: 32 Gb RAM with 4 core processors on ubuntu 14.04</p>
","python, scikit-learn, gensim, svd","<p>I don't really see why using sparks mllib SVD would improve performance or avoid memory errors. You simply exceed the size of your RAM. You have some options to deal with that:</p>

<ul>
<li>Reduce the dictionary size of your tf-idf (playing with max_df and min_df parameters of scikit-learn for example).</li>
<li>Use a hashing vectorizer instead of tf-idf.</li>
<li>Get more RAM (but at some point tf-idf + SVD is not scalable).</li>
</ul>

<p>Also you should show your code sample, you might do something wrong in your python code. </p>
",1,1,1129,2017-02-18 14:30:56,https://stackoverflow.com/questions/42316431/svd-using-scikit-learn-and-gensim-with-6-million-features
Python NLP British English vs American English,"<p>I'm currently working on NLP in python. However, in my corpus, there are both British and American English(realize/realise) I'm thinking to convert British to American. However, I did not find a good tool/package to do that. Any suggestions?</p>
","python, nlp, nltk, gensim, linguistics","<p>I've not been able to find a package either, but try this:</p>

<p>(Note that I've had to trim the us2gb dictionary substantially for it to fit within the Stack Overflow character limit - you'll have to rebuild this yourself).</p>

<pre><code># Based on Shengy's code:
# http://stackoverflow.com/questions/18840640/python-2-7-find-and-replace-from-text-file-using-dictionary-to-new-text-file

def replace_all(text, mydict):
    for gb, us in mydict.items():
        text = text.replace(us, gb)
    return text

# List of d differences taken from:
# http://www.tysto.com/uk-us-spelling-list.html
#
# key/value pairs can be added if required
us2gb = {'accessorize': 'accessorise',
 'accessorized': 'accessorised',
 'accessorizes': 'accessorises',
 'accessorizing': 'accessorising',
 'acclimatization': 'acclimatisation',
 'acclimatize': 'acclimatise',
 'acclimatized': 'acclimatised',
 'acclimatizes': 'acclimatises',
 'acclimatizing': 'acclimatising',
 'accouterments': 'accoutrements',
 'aerogram': 'aerogramme',
 'aerograms': 'aerogrammes',
 'aggrandizement': 'aggrandisement',
 'aging': 'ageing',
 'agonize': 'agonise',
 'agonized': 'agonised',
 'agonizes': 'agonises',
 'agonizing': 'agonising',
 'agonizingly': 'agonisingly',
 'airplane': 'aeroplane',
 'airplanes ': 'aeroplanes ',
 'almanac': 'almanack',
 'almanacs': 'almanacks',
 'aluminum': 'aluminium',
 'amortizable': 'amortisable',
 'amortization': 'amortisation',
 'amortizations': 'amortisations',
 'amortize': 'amortise',
 'amortized': 'amortised',
 'amortizes': 'amortises',
 'amortizing': 'amortising',
 'amphitheater': 'amphitheatre',
 'amphitheaters': 'amphitheatres',
 'analog': 'analogue',
 'analogs': 'analogues',
 'analyze': 'analyse',
 'analyzed': 'analysed',
 'analyzes': 'analyses',
 'analyzing': 'analysing',
 'anemia': 'anaemia',
 'anemic': 'anaemic',
 'anesthesia': 'anaesthesia',
 'anesthetic': 'anaesthetic',
 'anesthetics': 'anaesthetics',
 'anesthetist': 'anaesthetist',
 'anesthetists': 'anaesthetists',
 'anesthetize': 'anaesthetize',
 'anesthetized': 'anaesthetized',
 'anesthetizes': 'anaesthetizes',
 'anesthetizing': 'anaesthetizing',
 'anglicize': 'anglicise',
 'anglicized': 'anglicised',
 'anglicizes': 'anglicises',
 'anglicizing': 'anglicising',
 'annualized': 'annualised',
 'antagonize': 'antagonise',
 'antagonized': 'antagonised',
 'antagonizes': 'antagonises',
 'antagonizing': 'antagonising',
 'apologize': 'apologise',
 'apologized': 'apologised',
 'apologizes': 'apologises',
 'apologizing': 'apologising',
 'appall': 'appal',
 'appalls': 'appals',
 'appetizer': 'appetiser',
 'appetizers': 'appetisers',
 'appetizing': 'appetising',
 'appetizingly': 'appetisingly',
 'arbor': 'arbour',
 'arbors': 'arbours',
 'archeological': 'archaeological',
 'archeologically': 'archaeologically',
 'archeologist': 'archaeologist',
 'archeologists': 'archaeologists',
 'archeology': 'archaeology',
 'ardor': 'ardour',
 'armor': 'armour',
 'armored': 'armoured',
 'armorer': 'armourer',
 'armorers': 'armourers',
 'armories': 'armouries',
 'armory': 'armoury',
 'artifact': 'artefact',
 'artifacts': 'artefacts',
 'authorize': 'authorise',
 'authorized': 'authorised',
 'authorizes': 'authorises',
 'authorizing': 'authorising',
 'ax': 'axe',
 'backpedaled': 'backpedalled',
 'backpedaling': 'backpedalling',
 'balk': 'baulk',
 'balked': 'baulked',
 'balking': 'baulking',
 'balks': 'baulks',
 'banister': 'bannister',
 'banisters': 'bannisters',
 'baptize': 'baptise',
 'baptized': 'baptised',
 'baptizes': 'baptises',
 'baptizing': 'baptising',
 'bastardize': 'bastardise',
 'bastardized': 'bastardised',
 'bastardizes': 'bastardises',
 'bastardizing': 'bastardising',
 'battleax': 'battleaxe',
 'bedeviled': 'bedevilled',
 'bedeviling': 'bedevilling',
 'behavior': 'behaviour',
 'behavioral': 'behavioural',
 'behaviorism': 'behaviourism',
 'behaviorist': 'behaviourist',
 'behaviorists': 'behaviourists',
 'behaviors': 'behaviours',
 'behoove': 'behove',
 'behooved': 'behoved',
 'behooves': 'behoves',
 'bejeweled': 'bejewelled',
 'belabor': 'belabour',
 'belabored': 'belaboured',
 'belaboring': 'belabouring',
 'belabors': 'belabours',
 'beveled': 'bevelled',
 'bevies': 'bevvies',
 'bevy': 'bevvy',
 'biased': 'biassed',
 'biasing': 'biassing',
 'binging': 'bingeing',
 'bougainvillea': 'bougainvillaea',
 'bougainvilleas': 'bougainvillaeas',
 'bowdlerize': 'bowdlerise',
 'bowdlerized': 'bowdlerised',
 'bowdlerizes': 'bowdlerises',
 'bowdlerizing': 'bowdlerising',
 'breathalyze': 'breathalyse',
 'breathalyzed': 'breathalysed',
 'breathalyzer': 'breathalyser',
 'breathalyzers': 'breathalysers',
 'breathalyzes': 'breathalyses',
 'breathalyzing': 'breathalysing',
 'brutalize': 'brutalise',
 'brutalized': 'brutalised',
 'brutalizes': 'brutalises',
 'brutalizing': 'brutalising',
 'busses': 'buses',
 'bussing': 'busing',
 'caliber': 'calibre',
 'calibers': 'calibres',
 'caliper': 'calliper',
 'calipers': 'callipers',
 'calisthenics': 'callisthenics',
 'canalize': 'canalise',
 'canalized': 'canalised',
 'canalizes': 'canalises',
 'canalizing': 'canalising',
 'cancelation': 'cancellation',
 'cancelations': 'cancellations',
 'canceled': 'cancelled',
 'canceling': 'cancelling',
 'candor': 'candour',
 'cannibalize': 'cannibalise',
 'cannibalized': 'cannibalised',
 'cannibalizes': 'cannibalises',
 'cannibalizing': 'cannibalising',
 'canonize': 'canonise',
 'canonized': 'canonised',
 'canonizes': 'canonises',
 'canonizing': 'canonising',
 'capitalize': 'capitalise',
 'capitalized': 'capitalised',
 'capitalizes': 'capitalises',
 'capitalizing': 'capitalising',
 'caramelize': 'caramelise',
 'caramelized': 'caramelised',
 'caramelizes': 'caramelises',
 'caramelizing': 'caramelising',
 'carbonize': 'carbonise',
 'carbonized': 'carbonised',
 'carbonizes': 'carbonises',
 'carbonizing': 'carbonising',
 'caroled': 'carolled',
 'caroling': 'carolling',
 'catalog': 'catalogue',
 'cataloged': 'catalogued',
 'cataloging': 'cataloguing',
 'catalogs': 'catalogues',
 'catalyze': 'catalyse',
 'catalyzed': 'catalysed',
 'catalyzes': 'catalyses',
 'catalyzing': 'catalysing',
 'categorize': 'categorise',
 'categorized': 'categorised',
 'categorizes': 'categorises',
 'categorizing': 'categorising',
 'cauterize': 'cauterise',
 'cauterized': 'cauterised',
 'cauterizes': 'cauterises',
 'cauterizing': 'cauterising',
 'caviled': 'cavilled',
 'caviling': 'cavilling',
 'center': 'centre',
 'centered': 'centred',
 'centerfold': 'centrefold',
 'centerfolds': 'centrefolds',
 'centerpiece': 'centrepiece',
 'centerpieces': 'centrepieces',
 'centers': 'centres',
 'centigram': 'centigramme',
 'centigrams': 'centigrammes',
 'centiliter': 'centilitre',
 'centiliters': 'centilitres',
 'centimeter': 'centimetre',
 'centimeters': 'centimetres',
 'centralize': 'centralise',
 'centralized': 'centralised',
 'centralizes': 'centralises',
 'centralizing': 'centralising',
 'cesarean': 'caesarean',
 'cesareans': 'caesareans',
 'channeled': 'channelled',
 'channeling': 'channelling',
 'characterize': 'characterise',
 'characterized': 'characterised',
 'characterizes': 'characterises',
 'characterizing': 'characterising',
 'check': 'cheque',
 'checkbook': 'chequebook',
 'checkbooks': 'chequebooks',
 'checkered': 'chequered',
 'checks': 'cheques',
 'chili': 'chilli',
 'chimera': 'chimaera',
 'chimeras': 'chimaeras',
 'chiseled': 'chiselled',
 'chiseling': 'chiselling',
 'cipher': 'cypher',
 'ciphers': 'cyphers',
 'circularize': 'circularise',
 'circularized': 'circularised',
 'circularizes': 'circularises',
 'circularizing': 'circularising',
 'civilize': 'civilise',
 'civilized': 'civilised',
 'civilizes': 'civilises',
 'civilizing': 'civilising',
 'clamor': 'clamour',
 'clamored': 'clamoured',
 'clamoring': 'clamouring',
 'clamors': 'clamours',
 'clangor': 'clangour',
 'clarinetist': 'clarinettist',
 'clarinetists': 'clarinettists',
 'collectivize': 'collectivise',
 'collectivized': 'collectivised',
 'collectivizes': 'collectivises',
 'collectivizing': 'collectivising',
 'colonization': 'colonisation',
 'colonize': 'colonise',
 'colonized': 'colonised',
 'colonizer': 'coloniser',
 'colonizers': 'colonisers',
 'colonizes': 'colonises',
 'colonizing': 'colonising',
 'color': 'colour',
 'colorant': 'colourant',
 'colorants': 'colourants',
 'colored': 'coloured',
 'coloreds': 'coloureds',
 'colorful': 'colourful',
 'colorfully': 'colourfully',
 'coloring': 'colouring',
 'colorize': 'colourize',
 'colorized': 'colourized',
 'colorizes': 'colourizes',
 'colorizing': 'colourizing',
 'colorless': 'colourless',
 'colors': 'colours',
 'commercialize': 'commercialise',
 'commercialized': 'commercialised',
 'commercializes': 'commercialises',
 'commercializing': 'commercialising',
 'compartmentalize': 'compartmentalise',
 'compartmentalized': 'compartmentalised',
 'compartmentalizes': 'compartmentalises',
 'compartmentalizing': 'compartmentalising',
 'computerize': 'computerise',
 'computerized': 'computerised',
 'computerizes': 'computerises',
 'computerizing': 'computerising',
 'conceptualize': 'conceptualise',
 'conceptualized': 'conceptualised',
 'conceptualizes': 'conceptualises',
 'conceptualizing': 'conceptualising',
 'connection': 'connexion',
 'connections': 'connexions',
 'contextualize': 'contextualise',
 'contextualized': 'contextualised',
 'contextualizes': 'contextualises',
 'contextualizing': 'contextualising',
 'councilor': 'councillor',
 'councilors': 'councillors',
 'counseled': 'counselled',
 'counseling': 'counselling',
 'counselor': 'counsellor',
 'counselors': 'counsellors',
 'cozier': 'cosier',
 'cozies': 'cosies',
 'coziest': 'cosiest',
 'cozily': 'cosily',
 'coziness': 'cosiness',
 'cozy': 'cosy',
 'crenelated': 'crenellated',
 'criminalize': 'criminalise',
 'criminalized': 'criminalised',
 'criminalizes': 'criminalises',
 'criminalizing': 'criminalising',
 'criticize': 'criticise',
 'criticized': 'criticised',
 'criticizes': 'criticises',
 'criticizing': 'criticising',
 'crueler': 'crueller',
 'cruelest': 'cruellest',
 'crystallization': 'crystallisation',
 'crystallize': 'crystallise',
 'crystallized': 'crystallised',
 'crystallizes': 'crystallises',
 'crystallizing': 'crystallising',
 'cudgeled': 'cudgelled',
 'cudgeling': 'cudgelling',
 'customize': 'customise',
 'customized': 'customised',
 'customizes': 'customises',
 'customizing': 'customising',
 'decentralization': 'decentralisation',
 'decentralize': 'decentralise',
 'decentralized': 'decentralised',
 'decentralizes': 'decentralises',
 'decentralizing': 'decentralising',
 'decriminalization': 'decriminalisation',
 'decriminalize': 'decriminalise',
 'decriminalized': 'decriminalised',
 'decriminalizes': 'decriminalises',
 'decriminalizing': 'decriminalising',
 'defense': 'defence',
 'defenseless': 'defenceless',
 'defenses': 'defences',
 'dehumanization': 'dehumanisation',
 'dehumanize': 'dehumanise',
 'dehumanized': 'dehumanised',
 'dehumanizes': 'dehumanises',
 'dehumanizing': 'dehumanising',
 'demeanor': 'demeanour',
 'demilitarization': 'demilitarisation',
 'demilitarize': 'demilitarise',
 'demilitarized': 'demilitarised',
 'demilitarizes': 'demilitarises',
 'demilitarizing': 'demilitarising',
 'demobilization': 'demobilisation',
 'demobilize': 'demobilise',
 'demobilized': 'demobilised',
 'demobilizes': 'demobilises',
 'demobilizing': 'demobilising',
 'democratization': 'democratisation',
 'democratize': 'democratise',
 'democratized': 'democratised',
 'democratizes': 'democratises',
 'democratizing': 'democratising',
 'demonize': 'demonise',
 'demonized': 'demonised',
 'demonizes': 'demonises',
 'demonizing': 'demonising',
 'demoralization': 'demoralisation',
 'demoralize': 'demoralise',
 'demoralized': 'demoralised',
 'demoralizes': 'demoralises',
 'demoralizing': 'demoralising',
 'denationalization': 'denationalisation',
 'denationalize': 'denationalise',
 'denationalized': 'denationalised',
 'denationalizes': 'denationalises',
 'denationalizing': 'denationalising',
 'deodorize': 'deodorise',
 'deodorized': 'deodorised',
 'deodorizes': 'deodorises',
 'deodorizing': 'deodorising',
 'depersonalize': 'depersonalise',
 'depersonalized': 'depersonalised',
 'depersonalizes': 'depersonalises',
 'depersonalizing': 'depersonalising',
 'deputize': 'deputise',
 'deputized': 'deputised',
 'deputizes': 'deputises',
 'deputizing': 'deputising',
 'desensitization': 'desensitisation',
 'desensitize': 'desensitise',
 'desensitized': 'desensitised',
 'desensitizes': 'desensitises',
 'desensitizing': 'desensitising',
 'destabilization': 'destabilisation',
 'destabilize': 'destabilise',
 'destabilized': 'destabilised',
 'destabilizes': 'destabilises',
 'destabilizing': 'destabilising',
 'dialed': 'dialled',
 'dialing': 'dialling',
 'dialog': 'dialogue',
 'dialogs': 'dialogues',
 'diarrhea': 'diarrhoea',
 'digitize': 'digitise',
 'digitized': 'digitised',
 'digitizes': 'digitises',
 'digitizing': 'digitising',
 'discolor': 'discolour',
 'discolored': 'discoloured',
 'discoloring': 'discolouring',
 'discolors': 'discolours',
 'disemboweled': 'disembowelled',
 'disemboweling': 'disembowelling',
 'disfavor': 'disfavour',
 'disheveled': 'dishevelled',
 'passivizes': 'passivises',
 'passivizing': 'passivising',
 'pasteurization': 'pasteurisation',
 'pasteurize': 'pasteurise',
 'pasteurized': 'pasteurised',
 'pasteurizes': 'pasteurises',
 'pasteurizing': 'pasteurising',
 'patronize': 'patronise',
 'patronized': 'patronised',
 'patronizes': 'patronises',
 'patronizing': 'patronising',
 'patronizingly': 'patronisingly',
 'pedaled': 'pedalled',
 'pedaling': 'pedalling',
 'pederast': 'paederast',
 'pederasts': 'paederasts',
 'pedestrianization': 'pedestrianisation',
 'pedestrianize': 'pedestrianise',
 'pedestrianized': 'pedestrianised',
 'pedestrianizes': 'pedestrianises',
 'pedestrianizing': 'pedestrianising',
 'pediatric': 'paediatric',
 'pediatrician': 'paediatrician',
 'pediatricians': 'paediatricians',
 'pediatrics': 'paediatrics',
 'pedophile': 'paedophile',
 'pedophiles': 'paedophiles',
 'pedophilia': 'paedophilia',
 'penalize': 'penalise',
 'penalized': 'penalised',
 'penalizes': 'penalises',
 'penalizing': 'penalising',
 'penciled': 'pencilled',
 'penciling': 'pencilling',
 'personalize': 'personalise',
 'personalized': 'personalised',
 'personalizes': 'personalises',
 'personalizing': 'personalising',
 'pharmacopeia': 'pharmacopoeia',
 'pharmacopeias': 'pharmacopoeias',
 'philosophize': 'philosophise',
 'philosophized': 'philosophised',
 'philosophizes': 'philosophises',
 'philosophizing': 'philosophising',
 'phony ': 'phoney ',
 'pizzazz': 'pzazz',
 'plagiarize': 'plagiarise',
 'plagiarized': 'plagiarised',
 'plagiarizes': 'plagiarises',
 'plagiarizing': 'plagiarising',
 'plow': 'plough',
 'plowed': 'ploughed',
 'plowing': 'ploughing',
 'plowman': 'ploughman',
 'plowmen': 'ploughmen',
 'plows': 'ploughs',
 'plowshare': 'ploughshare',
 'plowshares': 'ploughshares',
 'polarization': 'polarisation',
 'polarize': 'polarise',
 'polarized': 'polarised',
 'polarizes': 'polarises',
 'polarizing': 'polarising',
 'politicization': 'politicisation',
 'politicize': 'politicise',
 'politicized': 'politicised',
 'politicizes': 'politicises',
 'politicizing': 'politicising',
 'popularization': 'popularisation',
 'popularize': 'popularise',
 'popularized': 'popularised',
 'popularizes': 'popularises',
 'popularizing': 'popularising',
 'pouf': 'pouffe',
 'poufs': 'pouffes',
 'practice': 'practise',
 'practiced': 'practised',
 'practices': 'practises',
 'practicing ': 'practising ',
 'presidium': 'praesidium',
 'presidiums ': 'praesidiums ',
 'pressurization': 'pressurisation',
 'pressurize': 'pressurise',
 'pressurized': 'pressurised',
 'pressurizes': 'pressurises',
 'pressurizing': 'pressurising',
 'pretense': 'pretence',
 'pretenses': 'pretences',
 'primeval': 'primaeval',
 'prioritization': 'prioritisation',
 'prioritize': 'prioritise',
 'prioritized': 'prioritised',
 'prioritizes': 'prioritises',
 'prioritizing': 'prioritising',
 'privatization': 'privatisation',
 'privatizations': 'privatisations',
 'privatize': 'privatise',
 'privatized': 'privatised',
 'privatizes': 'privatises',
 'privatizing': 'privatising',
 'professionalization': 'professionalisation',
 'professionalize': 'professionalise',
 'professionalized': 'professionalised',
 'professionalizes': 'professionalises',
 'professionalizing': 'professionalising',
 'program': 'programme',
 'programs': 'programmes',
 'prolog': 'prologue',
 'prologs': 'prologues',
 'propagandize': 'propagandise',
 'propagandized': 'propagandised',
 'propagandizes': 'propagandises',
 'propagandizing': 'propagandising',
 'proselytize': 'proselytise',
 'proselytized': 'proselytised',
 'proselytizer': 'proselytiser',
 'proselytizers': 'proselytisers',
 'proselytizes': 'proselytises',
 'proselytizing': 'proselytising',
 'psychoanalyze': 'psychoanalyse',
 'psychoanalyzed': 'psychoanalysed',
 'psychoanalyzes': 'psychoanalyses',
 'psychoanalyzing': 'psychoanalysing',
 'publicize': 'publicise',
 'publicized': 'publicised',
 'publicizes': 'publicises',
 'publicizing': 'publicising',
 'pulverization': 'pulverisation',
 'pulverize': 'pulverise',
 'pulverized': 'pulverised',
 'pulverizes': 'pulverises',
 'pulverizing': 'pulverising',
 'pummel': 'pummelled',
 'pummeled': 'pummelling',
 'quarreled': 'quarrelled',
 'quarreling': 'quarrelling',
 'radicalize': 'radicalise',
 'radicalized': 'radicalised',
 'radicalizes': 'radicalises',
 'radicalizing': 'radicalising',
 'rancor': 'rancour',
 'randomize': 'randomise',
 'randomized': 'randomised',
 'randomizes': 'randomises',
 'randomizing': 'randomising',
 'rationalization': 'rationalisation',
 'rationalizations': 'rationalisations',
 'rationalize': 'rationalise',
 'rationalized': 'rationalised',
 'rationalizes': 'rationalises',
 'rationalizing': 'rationalising',
 'raveled': 'ravelled',
 'raveling': 'ravelling',
 'realizable': 'realisable',
 'realization': 'realisation',
 'realizations': 'realisations',
 'realize': 'realise',
 'realized': 'realised',
 'realizes': 'realises',
 'realizing': 'realising',
 'recognizable': 'recognisable',
 'recognizably': 'recognisably',
 'recognizance': 'recognisance',
 'recognize': 'recognise',
 'recognized': 'recognised',
 'recognizes': 'recognises',
 'recognizing': 'recognising',
 'reconnoiter': 'reconnoitre',
 'reconnoitered': 'reconnoitred',
 'reconnoitering': 'reconnoitring',
 'reconnoiters': 'reconnoitres',
 'refueled': 'refuelled',
 'refueling': 'refuelling',
 'regularization': 'regularisation',
 'regularize': 'regularise',
 'regularized': 'regularised',
 'regularizes': 'regularises',
 'regularizing': 'regularising',
 'remodeled': 'remodelled',
 'remodeling': 'remodelling',
 'remold': 'remould',
 'remolded': 'remoulded',
 'remolding': 'remoulding',
 'remolds': 'remoulds',
 'reorganization': 'reorganisation',
 'reorganizations': 'reorganisations',
 'reorganize': 'reorganise',
 'reorganized': 'reorganised',
 'reorganizes': 'reorganises',
 'reorganizing': 'reorganising',
 'reveled': 'revelled',
 'reveler': 'reveller',
 'revelers': 'revellers',
 'reveling': 'revelling',
 'revitalize': 'revitalise',
 'revitalized': 'revitalised',
 'revitalizes': 'revitalises',
 'revitalizing': 'revitalising',
 'revolutionize': 'revolutionise',
 'revolutionized': 'revolutionised',
 'revolutionizes': 'revolutionises',
 'revolutionizing': 'revolutionising',
 'rhapsodize': 'rhapsodise',
 'rhapsodized': 'rhapsodised',
 'rhapsodizes': 'rhapsodises',
 'rhapsodizing': 'rhapsodising',
 'rigor': 'rigour',
 'rigors': 'rigours',
 'ritualized': 'ritualised',
 'rivaled': 'rivalled',
 'rivaling': 'rivalling',
 'romanticize': 'romanticise',
 'romanticized': 'romanticised',
 'romanticizes': 'romanticises',
 'romanticizing': 'romanticising',
 'rumor': 'rumour',
 'rumored': 'rumoured',
 'rumors': 'rumours',
 'saber': 'sabre',
 'sabers': 'sabres',
 'saltpeter': 'saltpetre',
 'sanitize': 'sanitise',
 'sanitized': 'sanitised',
 'sanitizes': 'sanitises',
 'sanitizing': 'sanitising',
 'satirize': 'satirise',
 'satirized': 'satirised',
 'satirizes': 'satirises',
 'satirizing': 'satirising',
 'savior': 'saviour',
 'saviors': 'saviours',
 'savor': 'savour',
 'savored': 'savoured',
 'savories': 'savouries',
 'savoring': 'savouring',
 'savors': 'savours',
 'savory': 'savoury',
 'scandalize': 'scandalise',
 'scandalized': 'scandalised',
 'scandalizes': 'scandalises',
 'scandalizing': 'scandalising',
 'scepter': 'sceptre',
 'scepters': 'sceptres',
 'scrutinize': 'scrutinise',
 'scrutinized': 'scrutinised',
 'scrutinizes': 'scrutinises',
 'scrutinizing': 'scrutinising',
 'secularization': 'secularisation',
 'secularize': 'secularise',
 'secularized': 'secularised',
 'secularizes': 'secularises',
 'secularizing': 'secularising',
 'sensationalize': 'sensationalise',
 'sensationalized': 'sensationalised',
 'sensationalizes': 'sensationalises',
 'sensationalizing': 'sensationalising',
 'sensitize': 'sensitise',
 'sensitized': 'sensitised',
 'sensitizes': 'sensitises',
 'sensitizing': 'sensitising',
 'sentimentalize': 'sentimentalise',
 'sentimentalized': 'sentimentalised',
 'sentimentalizes': 'sentimentalises',
 'sentimentalizing': 'sentimentalising',
 'sepulcher': 'sepulchre',
 'sepulchers ': 'sepulchres',
 'serialization': 'serialisation',
 'serializations': 'serialisations',
 'serialize': 'serialise',
 'serialized': 'serialised',
 'serializes': 'serialises',
 'serializing': 'serialising',
 'sermonize': 'sermonise',
 'sermonized': 'sermonised',
 'sermonizes': 'sermonises',
 'sermonizing': 'sermonising',
 'sheik ': 'sheikh ',
 'shoveled': 'shovelled',
 'shoveling': 'shovelling',
 'shriveled': 'shrivelled',
 'shriveling': 'shrivelling',
 'signaled': 'signalled',
 'signaling': 'signalling',
 'signalize': 'signalise',
 'signalized': 'signalised',
 'signalizes': 'signalises',
 'signalizing': 'signalising',
 'siphon': 'syphon',
 'siphoned': 'syphoned',
 'siphoning': 'syphoning',
 'siphons': 'syphons',
 'skeptic': 'sceptic',
 'skeptical': 'sceptical',
 'skeptically': 'sceptically',
 'skepticism': 'scepticism',
 'skeptics': 'sceptics',
 'smolder': 'smoulder',
 'smoldered': 'smouldered',
 'smoldering': 'smouldering',
 'smolders': 'smoulders',
 'sniveled': 'snivelled',
 'sniveling': 'snivelling',
 'snorkeled': 'snorkelled',
 'snorkeling': 'snorkelling',
 'snowplow': 'snowploughs',
 'socialization': 'socialisation',
 'socialize': 'socialise',
 'socialized': 'socialised',
 'socializes': 'socialises',
 'socializing': 'socialising',
 'sodomize': 'sodomise',
 'sodomized': 'sodomised',
 'sodomizes': 'sodomises',
 'sodomizing': 'sodomising',
 'solemnize': 'solemnise',
 'solemnized': 'solemnised',
 'solemnizes': 'solemnises',
 'solemnizing': 'solemnising',
 'somber': 'sombre',
 'specialization': 'specialisation',
 'specializations': 'specialisations',
 'specialize': 'specialise',
 'specialized': 'specialised',
 'specializes': 'specialises',
 'specializing': 'specialising',
 'specter': 'spectre',
 'specters': 'spectres',
 'spiraled': 'spiralled',
 'spiraling': 'spiralling',
 'splendor': 'splendour',
 'splendors': 'splendours',
 'squirreled': 'squirrelled',
 'squirreling': 'squirrelling',
 'stabilization': 'stabilisation',
 'stabilize': 'stabilise',
 'stabilized': 'stabilised',
 'stabilizer': 'stabiliser',
 'stabilizers': 'stabilisers',
 'stabilizes': 'stabilises',
 'stabilizing': 'stabilising',
 'standardization': 'standardisation',
 'standardize': 'standardise',
 'standardized': 'standardised',
 'standardizes': 'standardises',
 'standardizing': 'standardising',
 'stenciled': 'stencilled',
 'stenciling': 'stencilling',
 'sterilization': 'sterilisation',
 'sterilizations': 'sterilisations',
 'sterilize': 'sterilise',
 'sterilized': 'sterilised',
 'sterilizer': 'steriliser',
 'sterilizers': 'sterilisers',
 'sterilizes': 'sterilises',
 'sterilizing': 'sterilising',
 'stigmatization': 'stigmatisation',
 'stigmatize': 'stigmatise',
 'stigmatized': 'stigmatised',
 'stigmatizes': 'stigmatises',
 'stigmatizing': 'stigmatising',
 'stories': 'storeys',
 'story': 'storey',
 'subsidization': 'subsidisation',
 'subsidize': 'subsidise',
 'subsidized': 'subsidised',
 'subsidizer': 'subsidiser',
 'subsidizers': 'subsidisers',
 'subsidizes': 'subsidises',
 'subsidizing': 'subsidising',
 'succor': 'succour',
 'succored': 'succoured',
 'succoring': 'succouring',
 'succors': 'succours',
 'sulfate': 'sulphate',
 'sulfates': 'sulphates',
 'sulfide': 'sulphide',
 'sulfides': 'sulphides',
 'sulfur': 'sulphur',
 'sulfurous': 'sulphurous'}

gb_text = ""I realise I can see the world in colour but I can't vocalise its splendour""
us_text = replace_all(gb_text, us2gb)
print (""GB:"", gb_text)
print (""----------------------------------------------------------------------------"")
print (""US:"", us_text)
</code></pre>

<p>This will output:</p>

<pre><code># OUTPUT
# &gt;&gt;&gt; GB: I realise I can see the world in colour but I can't vocalise its splendour
# &gt;&gt;&gt; ----------------------------------------------------------------------------
# &gt;&gt;&gt; US: I realize I can see the world in color but I can't vocalize its splendor
</code></pre>

<p>Bear in mind that the Oxford variety of British English also uses -ize suffixes, so it's not simply a case of -ize being American and -ise being British.</p>

<p>There's a good article here that explores the intricacies:</p>

<p><a href=""http://blog.oxforddictionaries.com/2011/03/ize-or-ise/"" rel=""noreferrer"">http://blog.oxforddictionaries.com/2011/03/ize-or-ise/</a></p>
",10,10,9499,2017-02-19 16:28:49,https://stackoverflow.com/questions/42329766/python-nlp-british-english-vs-american-english
gensim word2vec - array dimensions in updating with online word embedding,"<p>Word2Vec from gensim 0.13.4.1 to update the word vectors on the fly does not work.</p>

<pre><code>model.build_vocab(sentences, update=False)
</code></pre>

<p>works fine;  however, </p>

<pre><code>model.build_vocab(sentences, update=True)
</code></pre>

<p>does not.</p>

<hr>

<p>I am using <a href=""http://rutumulkar.com/blog/2015/word2vec"" rel=""noreferrer"">this website</a> to try and emulate what they have done; hence I use the following script at some point:</p>

<pre><code>model = gensim.models.Word2Vec()
sentences = gensim.models.word2vec.LineSentence(""./text8/text8"")
model.build_vocab(sentences, keep_raw_vocab=False, trim_rule=None, progress_per=10000, update=False)
model.train(sentences)
</code></pre>

<p>However while this runs with <code>update=False</code>, using <code>update=True</code> gives me the following traceback:</p>

<pre><code>Traceback (most recent call last):
  File ""word2vecAttempt.py"", line 34, in &lt;module&gt;
    model.build_vocab(sentences, progress_per=10000, update=True)
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 535, in build_vocab
    self.finalize_vocab(update=update)  # build tables &amp; arrays
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 708, in finalize_vocab
    self.update_weights()
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 1070, in update_weights
    self.wv.syn0 = vstack([self.wv.syn0, newsyn0])
  File ""/home/brownc/anaconda3/lib/python3.5/site-packages/numpy/core/shape_base.py"", line 230, in vstack
    return _nx.concatenate([atleast_2d(_m) for _m in tup], 0)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
</code></pre>
","python, numpy, gensim","<p>I was able to reproduce your error. I think you're calling <code>update=True</code> when the model is not trained yet. You should only call it when it has been pre-trained.</p>

<p>This works:</p>

<pre><code>import gensim

model = gensim.models.Word2Vec()
sentences = gensim.models.word2vec.LineSentence(""text8"")
model.build_vocab(sentences, update=False)
model.train(sentences)

model.build_vocab(sentences, update=True)
model.train(sentences)
</code></pre>

<p>But this will fail:</p>

<pre><code>import gensim

model = gensim.models.Word2Vec()
sentences = gensim.models.word2vec.LineSentence(""text8"")
model.build_vocab(sentences, update=True)
model.train(sentences)

ValueError: all the input array dimensions except for the concatenation axis must match exactly
</code></pre>

<p>Using the latest version of gensim 0.13.4.1.</p>
",13,8,3928,2017-02-21 02:35:31,https://stackoverflow.com/questions/42357678/gensim-word2vec-array-dimensions-in-updating-with-online-word-embedding
AttributeError: type object &#39;Word2Vec&#39; has no attribute &#39;load_word2vec_format&#39;,"<p>I am trying to implement word2vec model and getting Attribute error </p>

<blockquote>
  <p>AttributeError: type object 'Word2Vec' has no attribute 'load_word2vec_format'</p>
</blockquote>

<p>Below is the code :</p>

<pre><code>wv = Word2Vec.load_word2vec_format(""GoogleNews-vectors-negative300.bin.gz"", binary=True)
wv.init_sims(replace=True)
</code></pre>

<p>Please let me know the issue ?</p>
","python, nlp, gensim, word2vec","gojomo's answer is right

<p><code>gensim.models.KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin.gz"", binary=True)</code></p>

<p>try to upgrade all dependencies of gensim(e.g. smart_open), if you still have errors as follows</p>

<p><code>pip install --upgrade gensim</code></p>

<p><i>File ""/home/liangn/PythonProjects/DeepRecommendation/Algorithm/Word2Vec.py"", line 18, in <strong>init</strong>
    self.model = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)</p>

<p>File ""/home/liangn/PythonProjects/venvLiang/lib/python2.7/site-packages/gensim/models/keyedvectors.py"", line 191, in load_word2vec_format with utils.smart_open(fname) as fin:</p>

<p>File ""/home/liangn/PythonProjects/venvLiang/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 138, in smart_open
    return file_smart_open(parsed_uri.uri_path, mode)</p>

<p>File ""/home/liangn/PythonProjects/venvLiang/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 642, in file_smart_open
    return compression_wrapper(open(fname, mode), fname, mode)</p>

<p>File ""/home/liangn/PythonProjects/venvLiang/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 630, in compression_wrapper
    return make_closing(GzipFile)(file_obj, mode)</p>

<p>File ""/usr/lib64/python2.7/gzip.py"", line 94, in <strong>init</strong>
    fileobj = self.myfileobj = <strong>builtin</strong>.open(filename, mode or 'rb')</p>

<p><strong>TypeError: coercing to Unicode: need string or buffer, file found</strong></i></p>
",4,7,16424,2017-02-21 09:49:33,https://stackoverflow.com/questions/42363897/attributeerror-type-object-word2vec-has-no-attribute-load-word2vec-format
How build Doc2Vec model by useing an &#39;iterable&#39; object,"<p>My code is running out of memory because of the question I asked in <a href=""https://groups.google.com/forum/#!topic/gensim/g19d2xXJ9VY"" rel=""nofollow noreferrer"">this page</a>. Then, I wrote the second code to have an iterable <code>alldocs</code>, not an all-in-memory <code>alldocs</code>. I changed my code based on the explanation of <a href=""https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/"" rel=""nofollow noreferrer"">this page</a>. I am not familiar with stream concept and I could not solve the error I got.</p>

<p>This code read all files of all folders of given path.The context of each file is consist of a document name and its context in two lines.For instance:</p>

<blockquote>
  <p>clueweb09-en0010-07-00000</p>
  
  <p>dove   gif clipart pigeon  clip    art picture image   hiox    free    birds   india   web icons   clipart add stumble upon    </p>
  
  <p>clueweb09-en0010-07-00001</p>
  
  <p>google bookmarks   yahoo   bookmarks   php script  java    script  jsp script  licensed    scripts html    tutorials   css tutorials</p>
</blockquote>

<p>First code:</p>

<pre><code># coding: utf-8
 import string
 import nltk
 import nltk.tokenize 
 from nltk.corpus import stopwords
 import re
 import os, sys 

 import MySQLRepository

 from gensim import utils
 from gensim.models.doc2vec import Doc2Vec
 import gensim.models.doc2vec
 from gensim.models.doc2vec import LabeledSentence
 from boto.emr.emrobject import KeyValue


 def readAllFiles(path):
    dirs = os.listdir( path )
    for file in dirs:
        if os.path.isfile(path+""/""+file):
           prepareDoc2VecSetting(path+'/'+file)
       else:
           pf=path+""/""+file
           readAllFiles(pf)      

def prepareDoc2VecSetting (fname):
    mapDocName_Id=[]
    keyValues=set()
   with open(fname) as alldata:
        a= alldata.readlines()
        end=len(a)
        label=0
        tokens=[]
        for i in range(0,end):
            if a[i].startswith('clueweb09-en00'):
               mapDocName_Id.insert(label,a[i])
               label=label+1
               alldocs.append(LabeledSentence(tokens[:],[label]))
               keyValues |= set(tokens)
               tokens=[]
           else:
               tokens=tokens+a[i].split()  

   mydb.insertkeyValueData(keyValues) 

   mydb.insertDocId(mapDocName_Id)


   mydb=MySQLRepository.MySQLRepository()

  alldocs = [] 
  pth='/home/flr/Desktop/newInput/tokens'
  readAllFiles(ipth)

  model = Doc2Vec(alldocs, size = 300, window = 5, min_count = 2, workers = 4)
  model.save(pth+'/my_model.doc2vec')
</code></pre>

<p>Second code:(I did not consider parts related to DB)</p>

<pre><code>import gensim
import os


from gensim.models.doc2vec import Doc2Vec
import gensim.models.doc2vec
from gensim.models.doc2vec import LabeledSentence



class prepareAllDocs(object):

    def __init__(self, top_dir):
        self.top_dir = top_dir

    def __iter__(self):
    mapDocName_Id=[]
    label=1
    for root, dirs, files in os.walk(top_directory):
        for fname in files:
            print fname
            inputs=[]
            tokens=[]
            with open(os.path.join(root, fname)) as f:
                for i, line in enumerate(f):          
                    if line.startswith('clueweb09-en00'):
                        mapDocName_Id.append(line)
                        if tokens:
                            yield LabeledSentence(tokens[:],[label])
                            label+=1
                            tokens=[]
                    else:
                        tokens=tokens+line.split()
                yield LabeledSentence(tokens[:],[label])

pth='/home/flashkar/Desktop/newInput/tokens/'
allDocs = prepareAllDocs('/home/flashkar/Desktop/newInput/tokens/')
for doc in allDocs:
    model = Doc2Vec(allDocs, size = 300, window = 5, min_count = 2, workers = 4)
model.save(pth+'/my_model.doc2vec')
</code></pre>

<p>This is the error:</p>

<blockquote>
  <p>Traceback (most recent call last):   File
  ""/home/flashkar/git/doc2vec_annoy/Doc2Vec_Annoy/KNN/testiterator.py"",
  line 44, in 
      model = Doc2Vec(allDocs, size = 300, window = 5, min_count = 2, >workers = 4)   File
  ""/home/flashkar/anaconda/lib/python2.7/site->packages/gensim/models/doc2vec.py"",
  line 618, in <strong>init</strong>
      self.build_vocab(documents, trim_rule=trim_rule)   File >""/home/flashkar/anaconda/lib/python2.7/site->packages/gensim/models/word2vec.py"",
  line 523, in build_vocab
      self.scan_vocab(sentences, progress_per=progress_per, >trim_rule=trim_rule)  # initial survey   File
  ""/home/flashkar/anaconda/lib/python2.7/site->packages/gensim/models/doc2vec.py"",
  line 655, in scan_vocab
      for document_no, document in enumerate(documents):   File >""/home/flashkar/git/doc2vec_annoy/Doc2Vec_Annoy/KNN/testiterator.py"",
  line 40, in <strong>iter</strong>
      yield LabeledSentence(tokens[:],tpl<a href=""https://groups.google.com/forum/#!topic/gensim/g19d2xXJ9VY"" rel=""nofollow noreferrer"">1</a>) IndexError: list index out of range</p>
</blockquote>
","python, iterator, gensim, doc2vec","<p>You are using a generator function because you don't want to store all of your documents, but you are still storing all of your documents in <code>alldocs</code>. You can just <code>yield LabeledSentence(tokens[:], tpl[1]]))</code>.</p>

<p>What is currently happening is you are appending to a list and returning the list. this is why you are getting the AttributeError. Additionally, on each iteration you are appending to the list, which means that on each iteration, i, you are returning i and all documents that came before i!</p>
",1,0,883,2017-02-21 16:07:53,https://stackoverflow.com/questions/42372346/how-build-doc2vec-model-by-useing-an-iterable-object
Interpreting negative Word2Vec similarity from gensim,"<p>E.g. we train a word2vec model using <code>gensim</code>:</p>

<pre><code>from gensim import corpora, models, similarities
from gensim.models.word2vec import Word2Vec

documents = [""Human machine interface for lab abc computer applications"",
              ""A survey of user opinion of computer system response time"",
              ""The EPS user interface management system"",
              ""System and human system engineering testing of EPS"",
              ""Relation of user perceived response time to error measurement"",
              ""The generation of random binary unordered trees"",
              ""The intersection graph of paths in trees"",
              ""Graph minors IV Widths of trees and well quasi ordering"",
              ""Graph minors A survey""]

texts = [[word for word in document.lower().split()] for document in documents]
w2v_model = Word2Vec(texts, size=500, window=5, min_count=1)
</code></pre>

<p>And when we query the similarity between words, we find negative similarity scores:</p>

<pre><code>&gt;&gt;&gt; w2v_model.similarity('graph', 'computer')
0.046929569156789336
&gt;&gt;&gt; w2v_model.similarity('graph', 'system')
0.063683518562347399
&gt;&gt;&gt; w2v_model.similarity('survey', 'generation')
-0.040026775040430063
&gt;&gt;&gt; w2v_model.similarity('graph', 'trees')
-0.0072684112978664561
</code></pre>

<p><strong>How do we interpret the negative scores?</strong> </p>

<p>If it's a cosine similarity shouldn't the range be <code>[0,1]</code>?</p>

<p><strong>What is the upper bound and lower bound of the <code>Word2Vec.similarity(x,y)</code> function?</strong> There isn't much written in the docs: <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.similarity</a> =(</p>

<p>Looking at the Python wrapper code, there isn't much too: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L1165</a></p>

<p>(If possible, please do point me to the <code>.pyx</code> code of where the similarity function is implemented.)</p>
","python, nlp, similarity, gensim, word2vec","<p>Cosine similarity ranges from -1 to 1, same as a regular cosine wave.</p>

<p><a href=""https://i.sstatic.net/5hD3g.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/5hD3g.png"" alt=""Cosine Wave""></a></p>

<p>As for the source:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/ba1ce894a5192fc493a865c535202695bb3c0424/gensim/models/word2vec.py#L1511"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/ba1ce894a5192fc493a865c535202695bb3c0424/gensim/models/word2vec.py#L1511</a></p>

<pre><code>def similarity(self, w1, w2):
    """"""
    Compute cosine similarity between two words.
    Example::
      &gt;&gt;&gt; trained_model.similarity('woman', 'man')
      0.73723527
      &gt;&gt;&gt; trained_model.similarity('woman', 'woman')
      1.0
    """"""
    return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2])
</code></pre>
",14,21,11700,2017-02-22 03:00:46,https://stackoverflow.com/questions/42381902/interpreting-negative-word2vec-similarity-from-gensim
What is the `null_word` parameter in gensim Word2Vec?,"<p>The <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">Word2Vec</a> object in <code>gensim</code> has a <code>null_word</code> parameter that isn't explained in the docs. </p>

<blockquote>
  <p>class gensim.models.word2vec.Word2Vec(sentences=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000)</p>
</blockquote>

<p><strong>What is the <code>null_word</code> parameter used for?</strong></p>

<p>Checking the code at <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L680"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L680</a>, it states:</p>

<pre><code>    if self.null_word:
        # create null pseudo-word for padding when using concatenative L1 (run-of-words)
        # this word is only ever input – never predicted – so count, huffman-point, etc doesn't matter
        word, v = '\0', Vocab(count=1, sample_int=0)
        v.index = len(self.wv.vocab)
        self.wv.index2word.append(word)
        self.wv.vocab[word] = v
</code></pre>

<p><strong>What is ""concatenative L1""?</strong></p>
","python, null, deep-learning, gensim, word2vec","<p>The <code>null_word</code> is only used if using the PV-DM with concatenation mode – parameters <code>dm=1, dm_concat=1</code> in model initialization. </p>

<p>In this non-default mode, the doctag-vector and the vectors of the neighboring words within <code>window</code> positions of a target word are <em>concatenated</em> into a very-wide input layer, rather than the more typical averaging. </p>

<p>Such models are much larger and slower than other modes. In the case of target words near the beginning or end of a text example, there might not be enough neighboring words to create this input layer – but the model requires values for those slots. So the <code>null_word</code> is essentially used as padding. </p>

<p>While the original <code>Paragraph Vectors</code> paper mentioned using this mode in some of their experiments, this mode is not sufficient to reproduce their results. (No one that I know of has been able to reproduce those results, and other comments from one of the authors imply that the original paper has some error or omission in its process.)</p>

<p>Additionally, I haven't found cases where this mode offers a clear benefit to justify the added time/memory. (It might require very-large datasets or very-long training times to show any benefit.)</p>

<p>So you shouldn't be too concerned about this model property unless you're doing advanced experiments with this less-common mode – in which case you can review the source for all the fine details about how it's used as padding.</p>
",1,0,1836,2017-02-22 03:32:50,https://stackoverflow.com/questions/42382207/what-is-the-null-word-parameter-in-gensim-word2vec
Gensim data parsing,"<p>Okay, this is a specific question about what data structure is required when providing training data to the Gensim python library. In particular, there must be an implicit understanding of what constitutes a document in any data that it is provided (otherwise it wouldn't, for instance, be able to find the tf-idf).</p>

<p>For a specific example, the wikipedia dump is used in the tutorials for the library for training purposes. The wikipedia dump is provided in XML. What gives gensim an understanding of separate documents? Is this understanding predicated on the nesing of xml elements? </p>
","python, gensim","<p>This is answered in the first two Gensim tutorials, <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow noreferrer"">Corpora and Vector Spaces Tutorial</a> and <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">Corpora and Vector Spaces</a>. They walk you through all the steps with code examples. </p>

<p>They start with a <code>documents</code> object (a list of strings), show how to create a dictionary and a corpus out of it, and how to use the dictionary and corpus to create models such as LDA and LSI.</p>

<p>As can be seen in the <a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">Experiments on the English Wikipedia</a> tutorial example code, the dictionary and the corpus are read from serialized files. I recommend to go through all the <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow noreferrer"">tutorials</a> and the example code.</p>
",1,0,410,2017-02-22 11:16:52,https://stackoverflow.com/questions/42389748/gensim-data-parsing
Save gensim Word2vec model in binary format .bin with save_word2vec_format,"<p>I'm training my own word2vec model using different data. To implement the resulting model into my classifier and compare the results with the original pre-trained Word2vec model I need to save the model in binary extension .bin. Here is my code, <em>sentences</em> is a list of short messages.</p>

<pre><code>import gensim, logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
sentences = gensim.models.word2vec.LineSentence('dati.txt')
model = gensim.models.Word2Vec(
sentences, size=300, window=5, min_count=5, workers=5,
sg=1, hs=1, negative=0
)
model.save_word2vec_format('model.bin', binary=True)
</code></pre>

<p>The last method, save_word2vec_format, gives me this error:</p>

<p><code>
AttributeError: 'Word2Vec' object has no attribute 'save_word2vec_format'
</code></p>

<p>What am I missing here? I've read the documentation of gensim and other forums. This <a href=""https://github.com/devmount/GermanWordEmbeddings/blob/c2b603a07d968146995ee9dde54a25fd0aa8586a/training.py#L56"" rel=""noreferrer"">repo on github</a> uses almost the same configuration so I cannot understand what's wrong. I've tried to switch from skipgram to cbow and from hierarchical softmax to negative sampling with no results.</p>

<p>Thank you in advance!</p>
","python, attributes, nlp, gensim, word2vec","<p>Are you using a pre-release release candidate version of gensim, or code directly from the <code>develop</code> branch?</p>

<p>In those versions <code>save_word2vec_format()</code> has moved to a utility class called <code>KeyedVectors</code>. </p>

<p>You won't yet (as of February 2017) get these versions from the usual way of installing gensim, <code>pip install gensim</code> – and it's likely that by the time this change is in the official distribution, the error message for trying the older call will be improved.</p>

<p>I recommend using the version that comes via plain <code>pip install gensim</code> unless you are a relatively expert user who is also carefully following the project <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/CHANGELOG.md"" rel=""noreferrer"">CHANGELOG.md</a>.</p>
",6,5,18734,2017-02-22 18:33:14,https://stackoverflow.com/questions/42399565/save-gensim-word2vec-model-in-binary-format-bin-with-save-word2vec-format
Reduce Google&#39;s Word2Vec model with Gensim,"<p>Loading the complete pre-trained word2vec model by <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"" rel=""noreferrer"">Google</a> is time intensive and tedious, therefore I was wondering if there is a chance to remove words below a certain frequency to bring the <code>vocab</code> count down to e.g. 200k words.</p>

<p>I found Word2Vec methods in the <code>gensim</code> package to determine the word frequency and to re-save the model again, but I am not sure how to <code>pop</code>/<code>remove</code> vocab from the pre-trained model before saving it again. I couldn't find any hint in the <code>KeyedVector class</code> and the <code>Word2Vec class</code> for such an operation?</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py</a>
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py</a></p>

<p><strong>How can I select a subset of the vocabulary of the pre-trained word2vec model?</strong></p>
","nlp, gensim, word2vec","<p>The GoogleNews word-vectors file format doesn't include frequency info. But, it does seem to be sorted in roughly more-frequent to less-frequent order. </p>

<p>And, <code>load_word2vec_format()</code> offers an optional <code>limit</code> parameter that only reads that many vectors from the given file. </p>

<p>So, the following should do roughly what you've requested:</p>

<pre><code>goognews_wordecs = KeyedVectors.load_word2vec_format(`GoogleNews-vectors-negative300.bin.gz`, binary=True, limit=200000)
</code></pre>
",8,9,4123,2017-02-25 17:38:06,https://stackoverflow.com/questions/42459373/reduce-googles-word2vec-model-with-gensim
Gensim word2vec in python3 missing vocab,"<p>I'm using gensim implementation of Word2Vec. I have the following code snippet:</p>

<pre><code>print('training model')
model = Word2Vec(Sentences(start, end))
print('trained model:', model)
print('vocab:', model.vocab.keys())
</code></pre>

<p>When I run this in python2, it runs as expected. The final print is all the words in the vocabulary.</p>

<p>However, if I run it in python3, I get an error:</p>

<pre><code>trained model: Word2Vec(vocab=102, size=100, alpha=0.025)
Traceback (most recent call last):
  File ""learn.py"", line 58, in &lt;module&gt;
    train(to_datetime('-4h'), to_datetime('now'), 'model.out')
  File ""learn.py"", line 23, in train
    print('vocab:', model.vocab.keys())
AttributeError: 'Word2Vec' object has no attribute 'vocab'
</code></pre>

<p>What is going on? Is gensim word2vec not compatible with python3?</p>
","python, gensim, word2vec","<p>Are you using the same version of gensim in both places? Gensim 1.0.0 moves <code>vocab</code> to a helper object, so whereas in pre-1.0.0 versions of gensim (in Python 2 or 3), you can use:</p>

<pre><code>model.vocab
</code></pre>

<p>...in gensim 1.0.0+ you should instead use (in Python 2 or 3)...</p>

<pre><code>model.wv.vocab
</code></pre>
",40,20,20974,2017-02-28 19:43:33,https://stackoverflow.com/questions/42517435/gensim-word2vec-in-python3-missing-vocab
Topic Modelling using gensim,"<p>I am making SMS categorizer. For this I want to classify my messages into different topics. So I want to use gensim for that. 
Can anybody provide me the source of any tutorial that can help me to begin topic modelling using gensim?</p>
","gensim, topic-modeling","<p>Radim Řehůřek, the author of gensim, provides tutorials on his website, which I found quite helpful: <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/tutorial.html</a></p>

<p>He also provides some tutorials on his github that are not that easy to find, so digging for a while in there might bring some great notebooks to the surface.
For example, these two (for LDA): 
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/lda_training_tips.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/lda_training_tips.ipynb</a></p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_methods.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_methods.ipynb</a></p>

<p>I personally found it very thrilling to start with the English Wikipedia, for which Řehůřek also provides a tutorial. Even though it takes quite a long time to train the model, you will find yourself with very ""natural"" seeming topics, which make it easy to test gensim's functionality.  </p>
",2,0,705,2017-03-01 10:38:55,https://stackoverflow.com/questions/42529467/topic-modelling-using-gensim
Got EOFError during loading doc2vec model,"<p>I could not load a doc2vec model on my computer and I got the following error. But, when I load that model on other computers, I can use that model.Therefore, I know the model was built correctly.</p>

<p>what should I do.</p>

<p>This is the code:</p>

<pre><code># coding: utf-8
from gensim.models.doc2vec import Doc2Vec
import gensim.models.doc2vec
from gensim.models.doc2vec import LabeledSentence
import os
import pickle
pth='/home/fatemeh/Step2/input-output/model/iterator'
model= Doc2Vec.load(pth+'/my_model.doc2vec')
</code></pre>

<p>This is the error:</p>

<pre><code>    Traceback (most recent call last):
  File ""CreateAnnoyIndex.py"", line 16, in &lt;module&gt;
    model= Doc2Vec.load(pth+'/my_model.doc2vec')
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.13.3-py2.7-linux-x86_64.egg/gensim/models/word2vec.py"", line 1762, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.13.3-py2.7-linux-x86_64.egg/gensim/utils.py"", line 248, in load
    obj = unpickle(fname)
  File ""/usr/local/lib/python2.7/dist-packages/gensim-0.13.3-py2.7-linux-x86_64.egg/gensim/utils.py"", line 912, in unpickle
    return _pickle.loads(f.read())
EOFError
</code></pre>
","python-2.7, pickle, gensim, doc2vec","<p>I think your model causes the problem. Are you check with same model? I mean build in a same way. please see <a href=""https://github.com/RaRe-Technologies/gensim/issues/860"" rel=""nofollow noreferrer"">this page</a></p>
",1,1,369,2017-03-01 18:34:51,https://stackoverflow.com/questions/42539384/got-eoferror-during-loading-doc2vec-model
How can I access output embedding(output vector) in gensim word2vec?,"<p>I want to use output embedding of word2vec such as in <a href=""http://www2016.net/proceedings/companion/p83.pdf"" rel=""noreferrer"">this paper (Improving document ranking with dual word embeddings)</a>.</p>

<p>I know input vectors are in syn0, output vectors are in syn1 and syn1neg if negative sampling.</p>

<p>But when I calculated most_similar with output vector, I got same result in some ranges because of removing syn1 or syn1neg.</p>

<p>Here is what I got.</p>

<pre><code>IN[1]: model = Word2Vec.load('test_model.model')

IN[2]: model.most_similar([model.syn1neg[0]])

OUT[2]: [('of', -0.04402521997690201),
('has', -0.16387106478214264),
('in', -0.16650712490081787),
('is', -0.18117375671863556),
('by', -0.2527652978897095),
('was', -0.254993200302124),
('from', -0.2659570872783661),
('the', -0.26878535747528076),
('on', -0.27521973848342896),
('his', -0.2930959463119507)]
</code></pre>

<p>but another syn1neg numpy vector is already similar output.</p>

<pre><code>IN[3]: model.most_similar([model.syn1neg[50]])

OUT[3]: [('of', -0.07884830236434937),
('has', -0.16942456364631653),
('the', -0.1771494299173355),
('his', -0.2043554037809372),
('is', -0.23265135288238525),
('in', -0.24725285172462463),
('by', -0.27772971987724304),
('was', -0.2979024648666382),
('time', -0.3547973036766052),
('he', -0.36455872654914856)]
</code></pre>

<p>I want to get output numpy arrays(negative or not) with preserved during training.</p>

<p>Let me know how can I access pure syn1 or syn1neg, or code, or some word2vec module can get output embedding.</p>
","python, numpy, gensim, word2vec","<p>With negative-sampling, <code>syn1neg</code> weights are per-word, and in the same order as <code>syn0</code>. </p>

<p>The mere fact that your two examples give similar results doesn't necessarily indicate anything is wrong. The words are by default sorted by frequency, so the early words (including those in position 0 and 50) are very-frequent words with very-generic cooccurrence-based meanings (that may all be close to each other).</p>

<p>Pick a medium-frequency word with a more distinct meaning, and you may get more meaningful results (if your corpus/settings/needs are sufficiently like those of the 'dual word embeddings' paper). For example, you might want to compare:</p>

<pre><code>model.most_similar('cousin')
</code></pre>

<p>...with...</p>

<pre><code>model.most_similar(positive=[model.syn1neg[model.vocab['cousin'].index])
</code></pre>

<p>However, in all cases the existing <code>most_similar()</code> method only looks for similar-vectors in <code>syn0</code> – the 'IN' vectors of the paper's terminology. So I believe the above code would only really be computing what the paper might call 'OUT-IN' similarity: a list of which IN vectors are most similar to a given OUT vector. They actually seem to tout the reverse, 'IN-OUT' similarity, as something useful. (That'd be the OUT vectors most similar to a given IN vector.)</p>

<p>The latest versions of gensim introduce a <code>KeyedVectors</code> class for representing a set of word-vectors, keyed by string, separate from the specific Word2Vec model or other training method. You could potentially create an extra <code>KeyedVectors</code> instance that replaces the usual <code>syn0</code> with <code>syn1neg</code>, to get lists of OUT vectors similar to a target vector (and thus calculate top-n 'IN-OUT' similarities or even 'OUT-OUT' similarities). </p>

<p>For example, this <em>might</em> work (I haven't tested it): </p>

<pre><code>outv = KeyedVectors()
outv.vocab = model.wv.vocab  # same
outv.index2word = model.wv.index2word  # same
outv.syn0 = model.syn1neg  # different
inout_similars = outv.most_similar(positive=[model['cousin']])
</code></pre>

<p><code>syn1</code> only exists when using hierarchical-sampling, and it's less clear what an ""output embedding"" for an individual word would be there. (There are multiple output nodes corresponding to predicting any one word, and they all need to be closer to their proper respective 0/1 values to predict a single word. So unlike with `syn1neg, there's no one place to read a vector that means a single word's output. You might have to calculate/approximate some set of hidden->output weights that would drive those multiple output nodes to the right values.)</p>
",8,7,8099,2017-03-02 11:31:07,https://stackoverflow.com/questions/42554289/how-can-i-access-output-embeddingoutput-vector-in-gensim-word2vec
Gensim: KeyedVectors.train(),"<p>I downloaded Wikipedia word vectors from <a href=""https://github.com/clips/dutchembeddings"" rel=""nofollow noreferrer"">here</a>. I loaded the vectors with:</p>

<pre><code>model_160 = KeyedVectors.load_word2vec_format(wiki_160_path, binary=False)
</code></pre>

<p>and then want to train them with:</p>

<pre><code>model_160.train()
</code></pre>

<p>I get the error back:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-11-22a9f6312119&gt; in &lt;module&gt;()
----&gt; 1 model.train()

AttributeError: 'KeyedVectors' object has no attribute 'train'
</code></pre>

<p>My question is now:
It seems like KeyedVectors has no train function, but I want to continue training the vectors on my personal sentences, instead of just using the Wikipedia vectors. How is this possible?</p>

<p>Thanks in advance, Jan</p>
","python-3.x, gensim","<p>You can't use KeyedVectors for that.
From <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">the documentation</a>:</p>

<blockquote>
  <p>Word vector storage and similarity look-ups.<br>
  The word vectors are considered read-only in this class.</p>
</blockquote>

<p>And <a href=""https://archive.is/3WugV#selection-305.0-445.157"" rel=""nofollow noreferrer"">also</a>:</p>

<blockquote>
  <p>The word vectors can also be instantiated from an existing file on disk in the word2vec C format as a KeyedVectors instance.<br>
  [...]<br>
  NOTE: <strong>It is impossible to continue training the vectors loaded from the C format</strong>
  because hidden weights, vocabulary frequency and the binary tree is
  missing.</p>
</blockquote>
",4,4,2874,2017-03-06 12:59:32,https://stackoverflow.com/questions/42626287/gensim-keyedvectors-train
Gensim - Memory error using GoogleNews-vector model,"<p>I  getting memory error, when I use <strong>GoogleNews-vectors-negative300.bin</strong> or try to train a model with Gensim with wikipedia dataset corpus.(1 GB). I have 4GB RAM in my system. Is there any way to bypass this.</p>

<p>Can we host it on cloud service like AWS to get better speed ?</p>
","nlp, gensim, word2vec","<p>4GB is very tight for that vector set; you should have 8GB or more to load the full set. Alternatively you could use the optional <code>limit</code> argument to <code>load_word2vec_format()</code> to just load some of the vectors. For example, <code>limit=500000</code> would load just the first 500,000 (instead of the full 3 million). As the file appears to put the more-frequently-appearing tokens first, that may be sufficient for many purposes. </p>
",6,2,1961,2017-03-08 14:11:29,https://stackoverflow.com/questions/42673590/gensim-memory-error-using-googlenews-vector-model
Create a dictionary with &#39;word groups&#39;,"<p>I would like to do some text analysis on job descriptions and was going to use nltk. I can build a dictionary and remove the stopwords, which is part of what I want. However in addition to the single words and their frequencies I would like to keep meaningful 'word groups' and count them as well. </p>

<p>For example in job descriptions containing 'machine learning' I don't want to consider 'machine' and 'learning' separately but keep retain the word group in my dictionary if it frequently occurs together. What is the most efficient method to do that? (I think I wont need to go beyond word groups containing 2 or words). And: At which point should I do the stopword removal?</p>

<p>Here is an example:</p>

<pre><code>    text = 'As a Data Scientist, you will focus on machine 
            learning and Natural Language Processing'
</code></pre>

<p>The dictionary I would like to have is:</p>

<pre><code>     dict = ['data scientist', 'machine learning', 'natural language processing', 
             'data', 'scientist', 'focus', 'machine', 'learning', 'natural' 
             'language', 'processing']
</code></pre>
","python, nltk, gensim","<p>Sounds like what you want do is use <code>collocations</code> from <a href=""http://www.nltk.org/howto/collocations.html"" rel=""nofollow noreferrer"">nltk</a>.</p>
",1,2,1938,2017-03-12 19:55:27,https://stackoverflow.com/questions/42752356/create-a-dictionary-with-word-groups
Doc2Vec Get most similar documents,"<p>I am trying to build a document retrieval model that returns most documents ordered by their relevancy with respect to a query or a search string. For this I trained a doc2vec model using the <code>Doc2Vec</code> model in gensim. My dataset is in the form of a pandas dataset which has each document stored as a string on each line. This is the code I have so far</p>

<pre><code>import gensim, re
import pandas as pd

# TOKENIZER
def tokenizer(input_string):
    return re.findall(r""[\w']+"", input_string)

# IMPORT DATA
data = pd.read_csv('mp_1002_prepd.txt')
data.columns = ['merged']
data.loc[:, 'tokens'] = data.merged.apply(tokenizer)
sentences= []
for item_no, line in enumerate(data['tokens'].values.tolist()):
    sentences.append(LabeledSentence(line,[item_no]))

# MODEL PARAMETERS
dm = 1 # 1 for distributed memory(default); 0 for dbow 
cores = multiprocessing.cpu_count()
size = 300
context_window = 50
seed = 42
min_count = 1
alpha = 0.5
max_iter = 200

# BUILD MODEL
model = gensim.models.doc2vec.Doc2Vec(documents = sentences,
dm = dm,
alpha = alpha, # initial learning rate
seed = seed,
min_count = min_count, # ignore words with freq less than min_count
max_vocab_size = None, # 
window = context_window, # the number of words before and after to be used as context
size = size, # is the dimensionality of the feature vector
sample = 1e-4, # ?
negative = 5, # ?
workers = cores, # number of cores
iter = max_iter # number of iterations (epochs) over the corpus)

# QUERY BASED DOC RANKING ??
</code></pre>

<p>The part where I am struggling is in finding documents that are most similar/relevant to the query. I used the <code>infer_vector</code> but then realised that it considers the query as a document, updates the model and returns the results. I tried using the <code>most_similar</code> and <code>most_similar_cosmul</code> methods but I get words along with a similarity score(I guess) in return. What I want to do is when I enter a search string(a query), I should get the documents (ids) that are most relevant along with a similarity score(cosine etc). How do I get this part done?</p>
","python, nlp, gensim, doc2vec","<p>You need to use <code>infer_vector</code> to get a document vector of the new text - which does not alter the underlying model.</p>

<p>Here is how you do it:</p>

<pre><code>tokens = ""a new sentence to match"".split()

new_vector = model.infer_vector(tokens)
sims = model.docvecs.most_similar([new_vector]) #gives you top 10 document tags and their cosine similarity
</code></pre>

<p>Edit: </p>

<p>Here is an example of how the underlying model does not change after <code>infer_vec</code> is called.</p>

<pre><code>import numpy as np

words = ""king queen man"".split()

len_before =  len(model.docvecs) #number of docs

#word vectors for king, queen, man
w_vec0 = model[words[0]]
w_vec1 = model[words[1]]
w_vec2 = model[words[2]]

new_vec = model.infer_vector(words)

len_after =  len(model.docvecs)

print np.array_equal(model[words[0]], w_vec0) # True
print np.array_equal(model[words[1]], w_vec1) # True
print np.array_equal(model[words[2]], w_vec2) # True

print len_before == len_after #True
</code></pre>
",55,44,40318,2017-03-14 08:43:26,https://stackoverflow.com/questions/42781292/doc2vec-get-most-similar-documents
Gensim 1.0.1 on Python 3.5 TypeError: object of type &#39;map&#39; has no len()?,"<p>I installed Python 3.5 using Anaconda and gensim 1.0.1 (supports Python 3) using pip. I got the following error when running gensim:</p>

<pre><code>Exception in thread Thread-61:
Traceback (most recent call last):
  File ""/Users/mac/anaconda/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/Users/mac/anaconda/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/Users/mac/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 838, in job_producer
    sentence_length = self._raw_word_count([sentence])
  File ""/Users/mac/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 755, in _raw_word_count
    return sum(len(sentence) for sentence in job)
  File ""/Users/mac/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 755, in &lt;genexpr&gt;
    return sum(len(sentence) for sentence in job)
TypeError: object of type 'map' has no len()
</code></pre>

<p>The code causing this error is from <a href=""https://github.com/aditya-grover/node2vec"" rel=""nofollow noreferrer"">node2vec</a>. I am porting it to Python 3 but got this error.</p>

<p>I know that in Python 3, len(map) causes error, does it mean Gensim 1.0.1 does not support Python 3 although <a href=""https://pypi.python.org/pypi/gensim"" rel=""nofollow noreferrer"">pip website</a> says it supports? Or are there some hidden settings?</p>

<p>Anyone has any idea what is wrong? Thanks.</p>
","python, python-3.x, anaconda, gensim","<p>Gensim supports Python 3, of course. It is your (or node2vec's) responsibility to supply <code>Word2Vec()</code> with an iterable of your sentences.</p>

<p>In this case, you have to pass it an iterable that contains walks - where each walk is a list of vertices:</p>

<pre><code>walks = [list(map(str, walk)) for walk in walks] # convert each vertex id to a string
model = Word2Vec(walks, ...)
</code></pre>
",6,3,2353,2017-03-16 14:32:11,https://stackoverflow.com/questions/42836992/gensim-1-0-1-on-python-3-5-typeerror-object-of-type-map-has-no-len
How to get the topic-word probabilities of a given word in gensim LDA?,"<p>As I understand, if i'm training a LDA model over a corpus where the size of the dictionary is say 1000 and no of topics (K) = 10, for each word in the dictionary I should have a vector of size 10 where each position in the vector is the probability of that word belongs to that particular topic, right? </p>

<p>So my question is given a word, what is the probability of that word belongs to to topic k where k could be from 1 to 10, how do I get this value in the gensim lda model?</p>

<p>I was using <code>get_term_topics</code> method but it doesn't output all the probabilities for all the topics. For eg.,</p>

<pre><code>lda_model1.get_term_topics(""fun"")
[(12, 0.047421702085626238)],
</code></pre>

<p>but I want to see what is the prob that ""fun"" could be in all the other topics as well?</p>
","gensim, lda, topic-modeling","<p>For someone who is looking for the ans, i found it.</p>

<p>These prob values are in the <code>xx.expElogbeta</code> numpy array. Number of rows in this matrix is equivalent to the number of topics and the no of columns is the size of your dictionary (words). So if you get the values for a particular column, you get the prob of that word belonging to all the topics. </p>

<p>e.g.,</p>

<pre><code>&gt;&gt;&gt; data = np.load(""model.expElogbeta.npy"")
&gt;&gt;&gt; data.shape
(20, 6481) # i have trained with 20 topics == no of rows
&gt;&gt;&gt; dict = corpora.Dictionary.load(dictf)
&gt;&gt;&gt; len(dict.keys())
6481 #columns of the npy array is the words in my dict
</code></pre>

<p>src = <a href=""https://groups.google.com/forum/?fromgroups=#!searchin/gensim/lda"" rel=""nofollow noreferrer"">https://groups.google.com/forum/?fromgroups=#!searchin/gensim/lda</a>$20topic-word$20matrix/gensim/Qoj7Agkx3qE/r9lyfihC4b4J</p>
",4,2,1862,2017-03-17 07:41:18,https://stackoverflow.com/questions/42851859/how-to-get-the-topic-word-probabilities-of-a-given-word-in-gensim-lda
How to train Word2Vec model on Wikipedia page using gensim?,"<p>After reading <a href=""https://codesachin.wordpress.com/2015/10/09/generating-a-word2vec-model-from-a-block-of-text-using-gensim-python/"" rel=""nofollow noreferrer"">this article</a>, I start to train my own model. The problem is that the author does not make it clear what the <code>sentences</code> in  <code>Word2Vec</code> should be like. </p>

<p>I download the text from a Wikipedia page, as it is written is the article, and I make a list of sentences from it:</p>

<pre><code>sentences = [word for word in wikipage.content.split('.')]
</code></pre>

<p>So, for example, <code>sentences[0]</code> looks like:</p>

<pre><code>'Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed'
</code></pre>

<p>Then I try to train a model with this list:</p>

<pre><code>model = Word2Vec(sentences, min_count=2, size=50, window=10,  workers=4)
</code></pre>

<p>But the dictionary of the model consists of letters! For example, the output of <code>model.wv.vocab.keys()</code> is:</p>

<pre><code>dict_keys([',', 'q', 'D', 'B', 'p', 't', 'o', '(', ')', '0', 'V', ':', 'j', 's', 'R', '{', 'g', '-', 'y', 'c', '9', 'I', '}', '1', 'M', ';', '`', '\n', 'i', 'r', 'a', 'm', '–', 'v', 'N', 'h', '/', 'P', 'F', '8', '""', '’', 'W', 'T', 'u', 'U', '?', ' ', 'n', '2', '=', 'w', 'C', 'O', '6', '&amp;', 'd', '4', 'S', 'J', 'E', 'b', 'L', '$', 'l', 'e', 'H', '≈', 'f', 'A', ""'"", 'x', '\\', 'K', 'G', '3', '%', 'k', 'z'])
</code></pre>

<p>What am I doing wrong? Thanks in advance!</p>
","python, nlp, gensim, word2vec","<p>The input to the <code>Word2Vec</code> model object could be a list of list of words, using the tokenization function in <code>nltk</code>:</p>

<pre><code>&gt;&gt;&gt; import wikipedia
&gt;&gt;&gt; from nltk import sent_tokenize, word_tokenize
&gt;&gt;&gt; page = wikipedia.page('machine learning')
&gt;&gt;&gt; sentences = [word_tokenize(sent) for sent in sent_tokenize(page.content)]
&gt;&gt;&gt; sentences[0]
['Machine', 'learning', 'is', 'the', 'subfield', 'of', 'computer', 'science', 'that', 'gives', 'computers', 'the', 'ability', 'to', 'learn', 'without', 'being', 'explicitly', 'programmed', '.']
</code></pre>

<p>And feed it in:</p>

<pre><code>&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt; model = Word2Vec(sentences, min_count=2, size=50, window=10,  
&gt;&gt;&gt; list(model.wv.vocab.keys())[:10]
['sparsely', '(', 'methods', 'their', 'typically', 'information', 'assessment', 'False', 'often', 'problems']
</code></pre>

<p>But in general, an generator (of sentence) that contains a generator (of words) would work too, i.e.:</p>

<pre><code>&gt;&gt;&gt; from gensim.utils import tokenize
&gt;&gt;&gt; paragraphs = map(tokenize, page.content.split('\n')) # paragraphs
&gt;&gt;&gt; model = Word2Vec(paragraphs, min_count=2, size=50, window=10,  workers=4)
&gt;&gt;&gt; list(model.wv.vocab.keys())[:10]
['sparsely', 'methods', 'their', 'typically', 'information', 'assessment', 'False', 'often', 'problems', 'symptoms']
</code></pre>
",6,1,1655,2017-03-23 13:05:13,https://stackoverflow.com/questions/42976912/how-to-train-word2vec-model-on-wikipedia-page-using-gensim
How to speed up Gensim Word2vec model load time?,"<p>I'm building a chatbot so I need to vectorize the user's input using Word2Vec. </p>

<p>I'm using a pre-trained model with 3 million words by Google (GoogleNews-vectors-negative300).</p>

<p>So I load the model using Gensim:</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>The problem is that it takes about 2 minutes to load the model. I can't let the user wait that long.</p>

<p>So what can I do to speed up the load time?</p>

<p>I thought about putting each of the 3 million words and their corresponding vector into a MongoDB database. That would certainly speed things up but intuition tells me it's not a good idea.</p>
","python, nlp, gensim, word2vec","<p>In recent gensim versions you can load a subset starting from the front of the file using the optional <code>limit</code> parameter to <code>load_word2vec_format()</code>. (The GoogleNews vectors seem to be in roughly most- to least- frequent order, so the first N are usually the N-sized subset you'd want. So use <code>limit=500000</code> to get the most-frequent 500,000 words' vectors – still a fairly large vocabulary – saving 5/6ths of the memory/load-time.)</p>

<p>So that may help a bit. But if you're re-loading for every web-request, you'll still be hurting from loading's IO-bound speed, and the redundant memory overhead of storing each re-load. </p>

<p>There are some tricks you can use in combination to help. </p>

<p>Note that after loading such vectors in their original word2vec.c-originated format, you can re-save them using gensim's native <code>save()</code>. If you save them uncompressed, and the backing array is large enough (and the GoogleNews set is definitely large enough), the backing array gets dumped in a separate file in a raw binary format. That file can later be memory-mapped from disk, using gensim's native <code>[load(filename, mmap='r')][1]</code> option.</p>

<p>Initially, this will make the load seem snappy – rather than reading all the array from disk, the OS will just map virtual address regions to disk data, so that some time later, when code accesses those memory locations, the necessary ranges will be read-from-disk. So far so good!</p>

<p>However, if you are doing typical operations like <code>most_similar()</code>, you'll still face big lags, just a little later. That's because this operation requires both an initial scan-and-calculation over all the vectors (on first call, to create unit-length-normalized vectors for every word), and then another scan-and-calculation over all the normed vectors (on every call, to find the N-most-similar vectors). Those full-scan accesses will page-into-RAM the whole array – again costing the couple-of-minutes of disk IO. </p>

<p>What you want is to avoid redundantly doing that unit-normalization, and to pay the IO cost just once. That requires keeping the vectors in memory for re-use by all subsequent web requestes (or even multiple parallel web requests). Fortunately memory-mapping can also help here, albeit with a few extra prep steps.</p>

<p>First, load the word2vec.c-format vectors, with <code>load_word2vec_format()</code>. Then, use <code>model.init_sims(replace=True)</code> to force the unit-normalization, destructively in-place (clobbering the non-normalized vectors). </p>

<p>Then, save the model to a new filename-prefix: model.save('GoogleNews-vectors-gensim-normed.bin'`. (Note that this actually creates multiple files on disk that need to be kept together for the model to be re-loaded.)</p>

<p>Now, we'll make a short Python program that serves to both memory-map load the vectors, <em>and</em> force the full array into memory. We also want this program to hang until externally terminated (keeping the mapping alive), <em>and</em> be careful not to re-calculate the already-normed vectors. This requires another trick because the loaded KeyedVectors actually don't know that the vectors are normed. (Usually only the raw vectors are saved, and normed versions re-calculated whenever needed.)</p>

<p>Roughly the following should work:</p>

<pre><code>from gensim.models import KeyedVectors
from threading import Semaphore
model = KeyedVectors.load('GoogleNews-vectors-gensim-normed.bin', mmap='r')
model.syn0norm = model.syn0  # prevent recalc of normed vectors
model.most_similar('stuff')  # any word will do: just to page all in
Semaphore(0).acquire()  # just hang until process killed
</code></pre>

<p>This will still take a while, but only needs to be done once, before/outside any web requests. While the process is alive, the vectors stay mapped into memory. Further, unless/until there's other virtual-memory pressure, the vectors should stay loaded in memory. That's important for what's next.</p>

<p>Finally, in your web request-handling code, you can now just do the following:</p>

<pre><code>model = KeyedVectors.load('GoogleNews-vectors-gensim-normed.bin', mmap='r')
model.syn0norm = model.syn0  # prevent recalc of normed vectors
# … plus whatever else you wanted to do with the model
</code></pre>

<p>Multiple processes can share read-only memory-mapped files. (That is, once the OS knows that file X is in RAM at a certain position, every other process that also wants a read-only mapped version of X will be directed to re-use that data, at that position.). </p>

<p>So this web-reqeust <code>load()</code>, <em>and any subsequent accesses</em>, can all re-use the data that the prior process already brought into address-space and active-memory. Operations requiring similarity-calcs against every vector will still take the time to access multiple GB of RAM, and do the calculations/sorting, but will no longer require extra disk-IO and redundant re-normalization. </p>

<p>If the system is facing other memory pressure, ranges of the array may fall out of memory until the next read pages them back in. And if the machine lacks the RAM to ever fully load the vectors, then every scan will require a mixing of paging-in-and-out, and performance will be frustratingly bad not matter what. (In such a case: get more RAM or work with a smaller vector set.)</p>

<p>But if you do have enough RAM, this winds up making the original/natural load-and-use-directly code ""just work"" in a quite fast manner, without an extra web service interface, because the machine's shared file-mapped memory functions as the service interface. </p>
",66,28,30074,2017-03-23 20:30:58,https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time
Gensim: Unable to train the LDA model,"<p>I have a list of sentences, and I follow the instructions at the <a href=""https://radimrehurek.com/gensim/tut1.html"" rel=""nofollow noreferrer"">tutorial</a> to make a corpora from it:</p>

<pre><code>texts = [[word for word in document.lower().split() if word.isalpha()] for document in documents]
corpus = corpora.Dictionary(texts)
</code></pre>

<p>I want to train a LDA model on this corpora and extract the topics keywords.</p>

<pre><code>lda = models.LdaModel(corpus, num_topics=10)
</code></pre>

<p>However, I receive an error while training: <code>TypeError: 'int' object is not iterable</code>. What am I doing wrong? What the format of a corpus should be?</p>
","nlp, gensim, lda, corpus","<p>After making a corpora you should make a single corpus with <code>doc2bow</code> which makes hashes from words (so-called <a href=""https://en.wikipedia.org/wiki/Feature_hashing"" rel=""nofollow noreferrer"">'hashing trick'</a>):</p>

<pre><code>texts = [[word for word in document.lower().split() if word.isalpha()] for document in documents]
corpus = corpora.Dictionary(texts)
hashed_corpus = [corpora.doc2bow(text) for text in texts]
</code></pre>

<p>And after that you can train your model with <code>hashed_corpus</code>:</p>

<pre><code>lda = models.LdaModel(corpus, id2word=corpus, num_topics=5) 
</code></pre>

<p><code>id2word</code> maps your topics from hashes to words, and using of it makes it possible to get topics as words, not numbers.</p>
",0,1,393,2017-03-25 17:15:18,https://stackoverflow.com/questions/43019447/gensim-unable-to-train-the-lda-model
Gensim docvecs.most_similar returns Id&#39;s that dont exist,"<p>I'm trying create an algorithm that's capable of show the top n documents similar to a specific document.
For that i used the gensim doc2vec. The code is bellow:</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(size=400, window=8, min_count=5, workers = 11, 
dm=0,alpha = 0.025, min_alpha = 0.025, dbow_words = 1)

model.build_vocab(train_corpus)

for x in xrange(10):
    model.train(train_corpus)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
    model.train(train_corpus)

model.save('model_EN_BigTrain')

sims = model.docvecs.most_similar([408], topn=10)
</code></pre>

<p>The sims var should give me 10 tuples, being the first element the id of the doc and the second the score.
The problem is that some id's do not correspond to any document in my training data.</p>

<p>I've been trying for some time now to make sense out of the ids that aren't in my training data but i don't see any logic.</p>

<p>Ps: This is the code that i used to create my train_corpus</p>

<pre><code>def readData(train_corpus, jData):

print(""The response contains {0} properties"".format(len(jData)))
print(""\n"")
for i in xrange(len(jData)):
    print ""&gt; Reading offers from Aux array""
    if i % 10 == 0: 
        print ""&gt;&gt;"", i, ""offers processed...""

      train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(jData[i][1]), tags=[jData[i][0]]))
print ""&gt; Finished processing offers""
</code></pre>

<p>Being each position of the aux array one array in witch the position 0 is an int (that i want to be the id) and the position 1 a description</p>

<p>Thanks in advance.</p>
","python, gensim, doc2vec","<p>Are you using plain integer IDs as your <code>tags</code>, but not using exactly all of the integers from 0 to whatever your <code>MAX_DOC_ID</code> is? </p>

<p>If so, that could explain the appearance of tags within that range. When you use plain ints, gensim Doc2Vec avoids creating a dict mapping provided tags to index-positions in its internal vector-array – and just uses the ints themselves. </p>

<p>Thus that internal vector-array must be allocated to include <code>MAX_DOC_ID + 1</code> rows. Any rows corresponding to unused IDs are still initialized as random vectors, like all the positions, but won't receive any of the training from actual text examples to push them into meaningful relative positions. It's thus possible these random-initialized-but-untrained vectors could appear in later <code>most_similar()</code> results. </p>

<p>To avoid that, either use only contiguous ints from 0 to the last ID you need. Or, if you can afford the memory cost of the string-to-index mapping, use string tags instead of plain ints. Or, keep an extra record of the valid IDs and manually filter the unwanted IDs from results. </p>

<p>Separately: by not specifying <code>iter=1</code> in your Doc2Vec model initialization, the default of <code>iter=5</code> will be in effect, meaning each call to <code>train()</code> does 5 iterations over your data. Oddly, also, your <code>xrange(10)</code> for-loop includes two separate calls to <code>train()</code> each iteration (and the 1st is just using whatever alpha/min_alpha was already in place). So you're actually doing 10 * 2 * 5 = 100 passes over the data, with an odd learning-rate schedule. </p>

<p>I suggest instead if you want 10 passes to just set <code>iter=10</code>, leave default <code>alpha</code>/<code>min_alpha</code> untouched, and then call <code>train()</code> only once. The model will do 10 passes, smoothly managing alpha from its starting to ending values. </p>
",3,3,1327,2017-03-27 16:37:53,https://stackoverflow.com/questions/43051902/gensim-docvecs-most-similar-returns-ids-that-dont-exist
Python Gensim word2vec vocabulary key,"<p>I want to make word2vec with gensim. I heard that vocabulary corpus should be unicode so I converted it to unicode.</p>

<pre><code># -*- encoding:utf-8 -*-
# !/usr/bin/env python
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
from gensim.models import Word2Vec
import pprint

with open('parsed_data.txt', 'r') as f:
    corpus = map(unicode, f.read().split('\n'))

model = Word2Vec(size=128, window=5, min_count=5, workers=4)
model.build_vocab(corpus,keep_raw_vocab=False)
model.train(corpus)
model.save('w2v')

pprint.pprint(model.most_similar(u'너'))
</code></pre>

<p>Above is my source code. It seems like work well. However there are problem with vocabulary key. I want to make korean word2vec which use unicode. For example word <code>사과</code> which means apology in english and it's unicode is <code>\xC0AC\xACFC</code> If I try to find <code>사과</code> in word2vec, key error occur...<br>
Instead of <code>\xc0ac\xacfc</code> <code>\xc0ac</code> and <code>\xacfc</code> stores separately. 
What's the reason and how to solve it?</p>
","python, unicode, gensim, word2vec","<p>Word2Vec requires text examples that are broken into word-tokens. It appears you are simply providing strings to Word2Vec, so when it iterates over them, it will only be seeing single-characters as words. </p>

<p>Does Korean use spaces to delimit words? If so, break your texts by spaces before handing the list-of-words as a text example to Word2Vec. </p>

<p>If not, you'll need to use some external word-tokenizer (not part of gensim) before passing your sentences to Word2Vec.</p>
",3,1,1765,2017-03-28 09:32:41,https://stackoverflow.com/questions/43065843/python-gensim-word2vec-vocabulary-key
Gensim HDP topic model: How to train on multiple passes of corpus?,"<p>Gensim's HDP model for topic modeling (gensim.models.hdpmodel.HdpModel) has a constructor that takes an argument called <code>max_chunks</code>.</p>

<p>On the documentation, it says <code>max_chunks</code> is the number of chunks the model will go over, and if that is larger than the number of chunks in supplied corpus, the training will wrap around the corpus.</p>

<p>Since I was warned by INFO logs that the likelihood function has been decreasing, I figure I may need multiple passes on corpus to converge.</p>

<p>LDA model provides with the <code>passes</code> argument the functionality to train on corpus for multiple iterations. I have difficulty figuring out how <code>max_chunks</code> in HDP maps to <code>passes</code> in LDA.</p>

<p>For example, let say my corpus has 1000000 documents. what <code>max_chunks</code> needs to be exactly in order to train, say, 3 passes on my corpus.</p>

<p>Any suggestion? Many many thanks</p>
","nlp, gensim, lda, topic-modeling","<p><code>class gensim.models.hdpmodel.HdpModel(corpus, id2word, max_chunks=None, max_time=None, chunksize=256, kappa=1.0, tau=64.0, K=15, T=150, alpha=1, gamma=1, eta=0.01, scale=1.0, var_converge=0.0001, outputdir=None, random_state=None)
</code></p>

<p>I think if you have 1000000 documents then if you use the default chunksize of 256 you'll need to have max_chunks=100000/256*3 to force 3 passes.</p>

<p>I'm also getting the <code>WARNING : likelihood is decreasing!</code> message, and I think that my corpus is too small (608 short texts) and too uniform to find topics in.</p>
",0,1,1905,2017-03-28 21:29:10,https://stackoverflow.com/questions/43080291/gensim-hdp-topic-model-how-to-train-on-multiple-passes-of-corpus
Saving output (context) embeddings in word2vec (gensim implementation) as a final model,"<p>I have studied <code>word2vec</code> implementation in gensim, I am aware that input vectors are in <code>syn0</code>, output vectors are in <code>syn1</code> and <code>syn1neg</code> if negative sampling.</p>

<p>I know I can access similarity between input and output embeddings like this:</p>

<pre><code>outv = KeyedVectors()
outv.vocab = model.wv.vocab
outv.index2word = model.wv.index2word  
outv.syn0 = model.syn1neg 
inout_similars = outv.most_similar(positive=[model['cousin']])
</code></pre>

<p>My question is, if it is possible to save output embeddings (from <code>syn1</code> or <code>syn1neg</code> matrix) as final model. For example, when <code>model.save()</code>, so that it outputs output embeddings (or where exactly in the code of <code>word2vec.py</code> I could access and modify that). I need this in order to use these output embeddings as input to classifier. I have done it previously in brute-force approach, so I would like to access output embeddings easily.</p>
","python, gensim, word2vec, word-embedding","<p>Your object <code>outv</code>, as an instance of <code>KeyedVectors</code>, has its own <code>save()</code> method (inherited from the <code>SaveLoad</code> superclass defined in <code>gensim/utils.py</code>) and <code>save_word2vec_format()</code> method. Each would save them in a manner you could reload into Python code again later. </p>
",2,1,1609,2017-03-29 15:57:20,https://stackoverflow.com/questions/43098535/saving-output-context-embeddings-in-word2vec-gensim-implementation-as-a-fina
Index2word in Gensim&#39;s Doc2vec raises an Attribute error,"<p>I trained a doc2vec (gensim.models.Doc2Vec) model and now I'm using this line:</p>

<pre><code>print(dict([(model.index2word[i], similarity) for i, similarity in enumerate(model.similar_by_word('igdumd32.dll@0x', topn=False))])['igdumd64.dll@0x'])
</code></pre>

<p>but it yields this error:
AttributeError: 'Doc2Vec' object has no attribute 'index2word'</p>

<p>I am using gensim 1.0.1</p>

<p>Can you help?</p>
","python, gensim","<p>The <code>index2word</code> list of word-vectors has moved to the <code>wv</code> property of the model in recent gensim versions, so where you would say <code>model.index2word</code> you must now use <code>model.wv.index2word</code>. </p>

<p>(Note that this is still just word-vectors, which are only trained by the ""DM"" <code>dm=1</code> Doc2Vec modes. Doc-vectors are in the <code>model.docvecs</code> object, and you can see a list of the string tags to which doc-vectors may be associated in <code>model.docvecs.offset2doctag</code>.)</p>
",6,3,4892,2017-03-31 17:01:18,https://stackoverflow.com/questions/43146077/index2word-in-gensims-doc2vec-raises-an-attribute-error
Gensim: error while loading pretrained doc2vec model?,"<p>I'm loading pretrained Doc2Vec model using:</p>

<pre><code>from gensim.models import Doc2Vec
model = Doc2Vec.load('/path/to/pretrained/model')
</code></pre>

<p>I'm getting the following error:</p>

<blockquote>
  <p>AttributeError: 'module' object has no attribute 'call_on_class_only'</p>
</blockquote>

<p>Does anyone know how to fix it. The model was trained with gensim 0.13.3 and I'm using  gensim 0.12.4. </p>
","python, gensim, doc2vec","<p>Gensim will generally try to support loading of models saved from <em>older</em> versions into <em>newer</em> versions – but the reverse is a much harder problem, and will only work sometimes. </p>

<p>So upgrade the environment where you want to load the model to 0.13.3, to match where it was trained. (Or try the most-recent version, 1.0.1.) But don't try to move models backwards. </p>
",2,0,792,2017-03-31 17:24:05,https://stackoverflow.com/questions/43146420/gensim-error-while-loading-pretrained-doc2vec-model
Word2Vec: Is it possible to train with respect to weight in NLP?,"<p>I used <code>Gensim</code>'s <code>Word2Vec</code> for training most similar words.</p>

<p>My dataset is all posts from my college community site.</p>

<p>Each dataset consists of like this:</p>

<pre><code>(title) + (contents) + (all comments)  // String
</code></pre>

<p>For example, </p>

<pre><code>data[0] =&gt; ""This is title. Contents is funny. What so funny?. Not funny for me""
</code></pre>

<p>So, I have around 400,000 datas like above and make them as a vector and try to train these data via <code>Word2Vec</code>. </p>

<p>I wonder that whether it is possible to make <code>Word2Vec</code> consider WEIGHT, which means, if I give an weight to certain data vector, <code>Word2Vec</code> train this data in a way that each word in this data vector has more strong relationship(similarity).</p>

<p>For example, If I gave a weight 5 to dataset, <code>I like Pizza, Chicken</code>, the word <code>Pizza</code> and <code>Chicken</code> (or <code>like</code> and <code>Pizza</code> etc) has strong relations than other data vector's words.</p>

<p>Would that be possible?</p>

<p>Sorry for poor explanation but I'm not native english speaker. If need more detailed info, please post comment.</p>
","nlp, gensim, word2vec","<p>There's no such configurable weighting in the definition of the word2vec algorithm, or the gensim implementation. </p>

<p>You could try repeating those text examples that you want to have more influence. (Ideally, such repetitions wouldn't be back-to-back, but shuffled among the entire dataset.) </p>

<p>As a result, those examples will affect the underlying model's training more often, for a greater proportion of the total training time – shifting the relative positioning of the involved words, compared to less-repeated examples. That <em>might</em> have the end result you're seeking. </p>
",2,2,1405,2017-04-02 07:25:25,https://stackoverflow.com/questions/43165724/word2vec-is-it-possible-to-train-with-respect-to-weight-in-nlp
What is relation between tsne and word2vec?,"<p>As I know of, <code>tsne</code> is reducing dimension of word vector. </p>

<p><code>Word2vec</code> is generate word embedding model with huge amount of data.</p>

<p>What is the relation between two?</p>

<p>Does <code>Word2vec</code> use <code>tsne</code> inside? </p>

<p>(I use <code>Word2vec</code> from <code>Gensim</code>)</p>
","nlp, gensim, word2vec","<p>Internally they both use <em>gradient-descent</em> to reach their final optimized states. And both can be considered dimensionality-reduction operations. But, word2vec does not internally use t-SNE (or vice-versa). </p>

<p><strong>t-SNE</strong> (""t-distributed stochastic neighbor embedding"") typically reduces many-dimensional data to 2- or 3-dimensions, for the purposes of plotting a visualization. It involves learning a mapping from the original dimensionality, to the fewer dimensions, which still keeps similar points near each other.  </p>

<p><strong>word2vec</strong> takes many text examples and learns a shallow neural-network that's good at predicting words from nearby words. A particular layer of that neural-network's weights, which represent individual words, then becomes the learned N-dimensional word-vectors, with the value of N often 100 to 600. </p>

<p>(There's an alternative way to create word-vectors called <em>GLoVE</em> that works a little more like t-SNE, in that it trains directly from the high-dimensional co-occurrence matrix of words, rather than from the many in-context co-occurrence examples. But it's still not t-SNE itself.)</p>

<p>You could potentially run t-SNE with a target dimensionality of 100-400. But since that end-result wouldn't yet yield nice plots, the maintenance of 'nearness' that's central to t-SNE won't have delivered its usual intended benefit. </p>

<p>You could potentially learn word2vec (or GLoVE) vectors of just 2- or 3-dimensions, but most of the useful similarities/arrangements that people seek from word-vectors would be lost in the crowding. And in a plot, you'd probably not see as strong visual 'clumping' of related-word categories, because t-SNE's specific high-to-low dimensionality nearness-preservation goal wasn't applied. </p>
",3,3,1030,2017-04-02 09:29:02,https://stackoverflow.com/questions/43166762/what-is-relation-between-tsne-and-word2vec
can anyone tell me about the model (skipgram/ CBOW ) used by Gensim?,"<p>word2vec uses either of the model for distributed representation of words. I was checking out the codes of gensim but it is not defined about the model used by gensim .</p>
","python, nlp, semantics, gensim, word2vec","<p><a href=""http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">From the gensim documentation:</a></p>

<blockquote>
  <p><code>sg</code> defines the training algorithm. By default (<code>sg=0</code>), CBOW is used. Otherwise (<code>sg=1</code>), skip-gram is employed.</p>
</blockquote>
",1,2,411,2017-04-02 17:55:28,https://stackoverflow.com/questions/43171573/can-anyone-tell-me-about-the-model-skipgram-cbow-used-by-gensim
Pyspark - Load trained model word2vec,"<p>I want to use word2vec with PySpark to process some data.
I was previously using Google trained model <code>GoogleNews-vectors-negative300.bin</code> with <code>gensim</code> in Python.</p>

<p>Is there a way I can load this bin file with <code>mllib.word2vec</code> ?
Or does it make sense to export the data as a dictionary from Python <code>{word : [vector]}</code> (or <code>.csv</code> file) and then load it in <code>PySpark</code>? </p>

<p>Thanks</p>
","python, load, pyspark, gensim, word2vec","<p><a href=""https://spark.apache.org/docs/latest/sql-data-sources-binaryFile.html"" rel=""nofollow noreferrer"">Binary import is supported in Spark 3.x</a>:</p>
<pre><code>spark.read.format(&quot;binaryFile&quot;).option(&quot;pathGlobFilter&quot;, &quot;*.png&quot;).load(&quot;/path/to/data&quot;)
</code></pre>
<p>However, this would require processing the binary data. Hence a <code>gensim</code> <a href=""https://stackoverflow.com/questions/49471037/save-results-of-word2vec-model-query-in-a-csv-file"">export</a> is rather recommended:</p>
<pre><code># Save gensim model
filename = &quot;stored_model.csv&quot; 
trained_model.save(filename) 
</code></pre>
<p>Then <a href=""https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html"" rel=""nofollow noreferrer"">load the model</a> in pyspark:</p>
<pre><code>df = spark.read.load(&quot;stored_model.csv&quot;,
                     format=&quot;csv&quot;, 
                     sep=&quot;;&quot;, 
                     inferSchema=&quot;true&quot;, 
                     header=&quot;true&quot;)
</code></pre>
",2,8,1313,2017-04-06 08:27:18,https://stackoverflow.com/questions/43249717/pyspark-load-trained-model-word2vec
Does Doc2Vec learn representations for the tags?,"<p>I'm using the Doc2Vec tags as an unique identifier for my documents, each document has a different tag and no semantic meaning. I'm using the tags to find specific documents so I can calculate the similarity between them. </p>

<p>Do the tags influence the results of my model? </p>

<p>In this <a href=""https://rare-technologies.com/doc2vec-tutorial/"" rel=""noreferrer"">tutorial</a> they talk about a parameter <code>train_lbls=false</code>, with this set to false there are no representations learned for the labels (tags). </p>

<p>That tutorial is somewhat dated and I guess the parameter does no longer exist, how does Doc2Vec handle tags? </p>
","gensim, doc2vec","<p>For gensim's Doc2Vec, your text examples must be objects similar to the example <code>TaggedDocument</code> class: with <code>words</code> and <code>tags</code> properties. The <code>tags</code> property should be a list of 'tags', which serve as keys to the doc-vectors that will be learned from the corresponding text. </p>

<p>In the classic/original case, each document has a single tag – essentially a unique ID for that one document. (Tags can be strings, but for very large corpuses, Doc2Vec will use somewhat less memory if you instead use tags that are plain Python ints, starting from 0, with no skipped values.)</p>

<p>The tags are used to look-up the learned vectors after training. If you had a document during training with the single tag <code>'mars'</code>, you'd look-up the learned vector with:</p>

<pre><code>model.docvecs['mars']
</code></pre>

<p>If you were do a <code>model.docvecs.most_similar['mars']</code> call, the results will be reported by their tag keys, as well. </p>

<p>The tags are <em>just</em> keys into the doc-vectors collection – they have no semantic meaning, and even if a string is repeated from the word-tokens in the text, there's no necessary relation between this tag key and the word. </p>

<p>That is, if you have a document whose single ID tag is 'mars', there's no essential relationship between the learned doc-vector accessed via that key (<code>model.docvecs['mars']</code>), and any learned word-vector accessed with the same string key (<code>model.wv['mars']</code>) – they're coming from separate collections-of-vectors.</p>
",15,6,5652,2017-04-21 13:16:57,https://stackoverflow.com/questions/43543762/does-doc2vec-learn-representations-for-the-tags
Is it possible to use gensim word2vec model in deeplearning4j.word2vec?,"<p>I'm new to deeplearning4j, i want to make sentence classifier using words vector as input for the classifier. 
I was using python before, where the vector model was generated using gensim, and i want to use that model for this new classifier. 
Is it possible to use gensim's word2vec model in deeplearning4j.word2vec and how i can do that?</p>
","java, gensim, word2vec, deeplearning4j","<p>Yes, it's possible since Word2Vec implementation defines a standard to structure its model.</p>

<p>To do this:</p>

<ol>
<li><p>Using <em>gensim</em>, save the model <strong>compatible with Word2Vec implementation</strong>:</p>

<pre><code>w2v_model.wv.save_word2vec_format(""path/to/w2v_model.bin"", binary=True)
</code></pre></li>
<li><p>From <em>DL4J</em>, load the same pre-trained model:</p>

<pre><code>Word2Vec w2vModel = WordVectorSerializer.readWord2VecModel(""path/to/w2v_model.bin"");
</code></pre></li>
</ol>

<p>In fact, you could test the model in both codes and you should see the same results, for instance:</p>

<p>With gensim:</p>

<pre><code>print(w2v_model.most_similar(""love""))
print(w2v_model.n_similarity([""man""], [""king""]))
</code></pre>

<p>And with DL4J:</p>

<pre><code>System.out.println(w2vModel.wordsNearest(""love"", 10));
System.out.println(w2vModel.similarity(""man"", ""king""));
</code></pre>
",10,6,3893,2017-04-26 11:37:02,https://stackoverflow.com/questions/43633092/is-it-possible-to-use-gensim-word2vec-model-in-deeplearning4j-word2vec
How to print gensim dictionary and corpus,"<p>I am unable to understand how to print the output for the below code</p>

<pre><code># make gensim dictionary and corpus
dictionary = gensim.corpora.Dictionary(boc_texts)
corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]
tfidf = gensim.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
</code></pre>

<p>I want to print the keyphrases and their tfidf scores</p>

<p>Thank you</p>
","python, nlp, gensim","<p>I was working with the same code found on a blog post and had the same problem as you.</p>

<p>Here is the entire code: 
<a href=""https://gist.github.com/bbengfort/efb311aaa1b52814c284d3b21ae752d6"" rel=""nofollow noreferrer"">https://gist.github.com/bbengfort/efb311aaa1b52814c284d3b21ae752d6</a></p>

<p>Basically you just need to add</p>

<pre><code>if __name__ == '__main__':
tfidfs, id2word = score_keyphrases_by_tfidf(texts)
fileids = texts.fileids()

# Print top keywords by TF-IDF
for idx, doc in enumerate(tfidfs):
    print(""Document '{}' key phrases:"".format(fileids[idx]))
    # Get top 20 terms by TF-IDF score
    for wid, score in heapq.nlargest(20, doc, key=itemgetter(1)):
        print(""{:0.3f}: {}"".format(score, id2word[wid]))

    print("""")
</code></pre>
",3,2,2249,2017-04-27 21:28:16,https://stackoverflow.com/questions/43668207/how-to-print-gensim-dictionary-and-corpus
How to refactor repeated code,"<p>I have those 2 functions which differs in only 1 line, so to avoid code duplication, I want to create a base class with a general form of those functions then inherit it for each class.</p>

<p>function 1:</p>

<pre><code>def top_similar_traces(self, stack_trace, top=10):
        words_to_test = StackTraceProcessor.preprocess(stack_trace)
        words_to_test_clean = [w for w in np.unique(words_to_test).tolist() if w in model]

        # Cos-similarity
        all_distances = np.array(1.0 - np.dot(model.wv.syn0norm, model.wv.syn0norm[
            [model.wv.vocab[word].index for word in words_to_test_clean]].transpose()), dtype=np.double)

        for i, (doc_id, rwmd_distance) in enumerate(distances):

            doc_words_clean = [w for w in self.corpus[doc_id] if w in model]
            wmd = self.wmdistance(model, words_to_test_clean, doc_words_clean, all_distances)

        return sorted(similarities, key=lambda v: v[1])[:top]
</code></pre>

<p>function 2:</p>

<pre><code>def top_similar_traces(self, stack_trace, top=10):
        words_to_test = StackTraceProcessor.preprocess(stack_trace)
        words_to_test_clean = [w for w in np.unique(words_to_test).tolist() if w in model]

        # Cos-similarity
        all_distances = np.array(1.0 - np.dot(model.wv.syn0norm, model.wv.syn0norm[
            [model.wv.vocab[word].index for word in words_to_test_clean]].transpose()), dtype=np.double)

        for i, (doc_id, rwmd_distance) in enumerate(distances):

            doc_words_clean = [w for w in self.corpus[doc_id].words if w in model]
            wmd = self.wmdistance(model, words_to_test_clean, doc_words_clean, all_distances)

        return sorted(similarities, key=lambda v: v[1])[:top]
</code></pre>

<p>You can see the only difference is at</p>

<pre><code>        doc_words_clean = [w for w in self.corpus[doc_id].words if w in model]
        doc_words_clean = [w for w in self.corpus[doc_id] if w in model]
</code></pre>
","python, gensim","<p>Simply extract the changing part into a separate method. That way, bases classes can just overwrite that part and affect the original method without having to duplicate the whole code.</p>

<p>Something like this:</p>

<pre><code># Base class
def top_similar_traces(self, stack_trace, top=10):
    words_to_test = StackTraceProcessor.preprocess(stack_trace)
    words_to_test_clean = [w for w in np.unique(words_to_test).tolist() if w in model]

    # Cos-similarity
    all_distances = np.array(1.0 - np.dot(model.wv.syn0norm, model.wv.syn0norm[
        [model.wv.vocab[word].index for word in words_to_test_clean]].transpose()), dtype=np.double)

    for i, (doc_id, rwmd_distance) in enumerate(distances):
        # call another method here
        doc_words_clean = self.top_similar_traces_filter_words(doc_id)
        wmd = self.wmdistance(model, words_to_test_clean, doc_words_clean, all_distances)

    return sorted(similarities, key=lambda v: v[1])[:top]

# Subclass A
def top_similar_traces_filter_words(self, doc_id):
    return [w for w in self.corpus[doc_id].words if w in model]

# Subclass B
def top_similar_traces_filter_words(self, doc_id):
    return [w for w in self.corpus[doc_id] if w in model]
</code></pre>

<p>Btw. I don’t know where your <code>model</code> comes from, but it appears to be a global variable. You should probably avoid that and put it inside your class instead (or pass it in).</p>
",1,-1,440,2017-04-30 23:25:09,https://stackoverflow.com/questions/43712401/how-to-refactor-repeated-code
Visualise word2vec generated from gensim using t-sne,"<p>I have trained a doc2vec and corresponding word2vec on my own corpus using gensim. I want to visualise the word2vec using t-sne with the words. As in, each dot in the figure has the ""word"" also with it.</p>

<p>I looked at a similar question here : <a href=""https://stackoverflow.com/questions/40581010/how-to-run-tsne-on-word2vec-created-from-gensim"">t-sne on word2vec</a></p>

<p>Following it, I have this code : </p>

<p>import gensim
import gensim.models as g</p>

<pre><code>from sklearn.manifold import TSNE
import re
import matplotlib.pyplot as plt

modelPath=""/Users/tarun/Desktop/PE/doc2vec/model3_100_newCorpus60_1min_6window_100trainEpoch.bin""
model = g.Doc2Vec.load(modelPath)

X = model[model.wv.vocab]
print len(X)
print X[0]
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X[:1000,:])

plt.scatter(X_tsne[:, 0], X_tsne[:, 1])
plt.show()
</code></pre>

<p>This gives a figure with dots but no words. That is I don't know which dot is representative of which word. How can I display the word with the dot?</p>
","scikit-learn, data-visualization, gensim, word2vec","<p>Two parts to the answer: how to get the word labels, and how to plot the labels on a scatterplot.</p>
<p><strong>Word labels in gensim's word2vec</strong></p>
<p><code>model.wv.vocab</code> is a dict of {word: object of numeric vector}. To load the data into <code>X</code> for t-SNE, I made one change.</p>
<pre><code>vocab = list(model.wv.key_to_index)
X = model.wv[vocab]
</code></pre>
<p>This accomplishes two things: (1) it gets you a standalone <code>vocab</code> list for the final dataframe to plot, and (2) when you index <code>model</code>, you can be sure that you know the order of the words.</p>
<p>Proceed as before with</p>
<pre><code>tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)
</code></pre>
<p>Now let's put <code>X_tsne</code> together with the <code>vocab</code> list. This is easy with pandas, so <code>import pandas as pd</code> if you don't have that yet.</p>
<pre><code>df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])
</code></pre>
<p>The vocab words are the <em>indices</em> of the dataframe now.</p>
<p>I don't have your dataset, but in the <a href=""https://stackoverflow.com/questions/40581010/how-to-run-tsne-on-word2vec-created-from-gensim"">other SO</a> you mentioned, an example <code>df</code> that uses sklearn's newsgroups would look something like</p>
<pre><code>                        x             y
politics    -1.524653e+20 -1.113538e+20
worry        2.065890e+19  1.403432e+20
mu          -1.333273e+21 -5.648459e+20
format      -4.780181e+19  2.397271e+19
recommended  8.694375e+20  1.358602e+21
arguing     -4.903531e+19  4.734511e+20
or          -3.658189e+19 -1.088200e+20
above        1.126082e+19 -4.933230e+19
</code></pre>
<p><strong>Scatterplot</strong></p>
<p>I like the object-oriented approach to matplotlib, so this starts out a little different.</p>
<pre><code>fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

ax.scatter(df['x'], df['y'])
</code></pre>
<p>Lastly, the <code>annotate</code> method will label coordinates. The first two arguments are the text label and the 2-tuple. Using <code>iterrows()</code>, this can be very succinct:</p>
<pre><code>for word, pos in df.iterrows():
    ax.annotate(word, pos)
</code></pre>
<p>[Thanks to Ricardo in the comments for this suggestion.]</p>
<p>Then do <code>plt.show()</code> or <code>fig.savefig()</code>. Depending on your data, you'll probably have to mess with <code>ax.set_xlim</code> and <code>ax.set_ylim</code> to see into a dense cloud. This is the newsgroup example without any tweaking:</p>
<p><a href=""https://i.sstatic.net/OSKOJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OSKOJ.png"" alt=""scatterplot"" /></a></p>
<p>You can modify dot size, color, etc., too. Happy fine-tuning!</p>
",49,22,22551,2017-05-04 07:31:22,https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim-using-t-sne
RuntimeError: release unlocked lock while training doc2vec,"<p>I'm getting the following error when training a doc2vec model in a Jupyter notebook on OS X. The error is reproducible (although the specific thread in which it occurs changes) for my current dataset, although I have successfully trained models on other datasets. </p>

<pre><code>Exception in thread Thread-82:
Traceback (most recent call last):
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
self.run()
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 862, in run
self._target(*self._args, **self._kwargs)
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 822, in worker_loop
tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/doc2vec.py"", line 717, in _do_train_job
doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
File ""gensim/models/doc2vec_inner.pyx"", line 428, in gensim.models.doc2vec_inner.train_document_dm (./gensim/models/doc2vec_inner.c:5455)
File ""mtrand.pyx"", line 1266, in mtrand.RandomState.randint (numpy/random/mtrand/mtrand.c:15836)
RuntimeError: release unlocked lock
Exception in thread Thread-77:
Traceback (most recent call last):
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
self.run()
File ""/Users/kevinyang/anaconda/lib/python3.5/threading.py"", line 862, in run
self._target(*self._args, **self._kwargs)
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py"", line 822, in worker_loop
tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))
File ""/Users/kevinyang/anaconda/lib/python3.5/site-packages/gensim/models/doc2vec.py"", line 717, in _do_train_job
doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)
File ""gensim/models/doc2vec_inner.pyx"", line 458, in gensim.models.doc2vec_inner.train_document_dm (./gensim/models/doc2vec_inner.c:5963)
File ""mtrand.pyx"", line 1266, in mtrand.RandomState.randint (numpy/random/mtrand/mtrand.c:15836)
RuntimeError: release unlocked lock
</code></pre>
","gensim, doc2vec","<p>Most likely a Numpy issue. See discussion in <a href=""https://github.com/RaRe-Technologies/gensim/issues/1311#issuecomment-301622734"" rel=""nofollow noreferrer"">gensim bug tracker</a>. </p>
",1,1,724,2017-05-08 19:01:26,https://stackoverflow.com/questions/43855348/runtimeerror-release-unlocked-lock-while-training-doc2vec
Gensim saved dictionary has no id2token,"<p>I have saved a Gensim dictionary to disk. When I load it, the <code>id2token</code> attribute dict is not populated.</p>

<p>A simple piece of the code that saves the dictionary:</p>

<pre><code>dictionary = corpora.Dictionary(tag_docs)
dictionary.save(""tag_dictionary_lda.pkl"")
</code></pre>

<p>Now when I load it (I'm loading it in an jupyter notebook), it still works fine for mapping tokens to IDs, but <code>id2token</code> does not work (I cannot map IDs to tokens) and in fact <code>id2token</code> is not populated at all.</p>

<pre><code>&gt; dictionary = corpora.Dictionary.load(""../data/tag_dictionary_lda.pkl"")
&gt; dictionary.token2id[""love""]
Out: 1613

&gt; dictionary.doc2bow([""love""])
Out: [(1613, 1)]

&gt; dictionary.id2token[1613]
Out: 
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input&gt; in &lt;module&gt;()
----&gt; 1 dictionary.id2token[1613]

KeyError: 1613

&gt; list(dictionary.id2token.keys())
Out: []
</code></pre>

<p>Any thoughts? </p>
","python, nlp, gensim","<p>You don't need the <code>dictionary.id2token[1613]</code> as you can use <code>dictionary[1613]</code> directly.</p>

<p>Note, that if you check the <code>dictionary.id2token</code> afterwards, it won't be empty any more. That's because the <code>dictionary.id2token</code> is formed only on request to save memory (as is stated during the init of Dictionary class).</p>
",30,16,9530,2017-05-09 19:26:59,https://stackoverflow.com/questions/43878332/gensim-saved-dictionary-has-no-id2token
inconsistent similarity betwen inferred and trained vectors in doc2vec,"<p>I had trained a paragraph vector model from gensim by using a considerable amount text data. I did the next test: I verified the index of any sentence and then inferred a vector for it</p>

<pre><code>&gt;&gt;&gt; x=m.docvecs[18638]
&gt;&gt;&gt; g=m.infer_vector(""The seven OxyR target sequences analyzed previously and two new sites grxA at position 207 in GenBank entry M13449 and a second Mu phage mom site at position 59 in GenBank entry V01463 were used to generate an individual information weight matrix"".split())
</code></pre>

<p>When I computed the cosine similarity, it was very low (the opposite is expected). </p>

<pre><code>&gt;&gt;&gt; 1 - spatial.distance.cosine(g, x)
0.20437437837633066
</code></pre>

<p>Can someone tell me if I'm doing something wrong, please?</p>

<p>Thanks</p>
","python, gensim, doc2vec","<p>Some thoughts: </p>

<p>If your initial training did any extra preprocessing on text examples – like say case-flattening – you should do that, too, to the tokens as fed to <code>infer_vector()</code>. </p>

<p>The gensim defaults for the optional parameters of <code>infer_vector()</code>, including <code>steps=5</code> and <code>alpha=0.1</code>, are wild guesses that may be insufficient for many models/training modes. Many have reported better results with much higher <code>steps</code> (into the hundreds), or a lower starting <code>alpha</code> (more like the training default of <code>0.025</code>).  </p>

<p>When the model itself returns <code>most_similar()</code> results, it does all of its cosine-similarity calculations on unit-length normalized doc-vectors. 
 – that is, those in the generated-when-needed <code>model.docvecs.doctag_syn0norm</code> array. However, the vector returned by <code>infer_vector()</code> will just be the raw, unnormalized inferred vector – analogous to the raw vectors in the <code>model.docvecs.doctag_syn0</code> array. If computing your own cosine-similarities, be sure to account for this. (I think <code>spatial.distance.cosine()</code> accounts for this.)</p>

<p>In general re-inferring a vector for the same text as was trained a doc-vector during bulk-training should result in a very-similar (but not identical) vector. So if in fact <code>m.docvecs[18638]</code> was for the exact same text as you're re-inferring here, the distance should be quite small. (This can be a good 'sanity check' on whether a training process and then later inference is havong the desired effect.) If this expected similarity isn't achieved, you should re-check that the right preprocessing occurred during training, that the model parameters are causing real training to occur, that you're referring to the right trained vector (18638) without any off-by-one/etc errors, and so forth. </p>
",0,1,679,2017-05-10 00:49:37,https://stackoverflow.com/questions/43881924/inconsistent-similarity-betwen-inferred-and-trained-vectors-in-doc2vec
Gensim find topics in sentences,"<p>I have trained an LDA algorithm on a corpus , and what I'd like to do is getting for each sentence the topic on which it corresponds, in order, to make a comparison between what the algorithm finds and the labels I have.</p>

<p>I have tried with the code below, but the results are quite bad I find a great deal of topic 17 (maybe 25% of the volume, it should be closer to 5%)</p>

<p>Thanks for your help</p>

<pre><code># text lemmatized: list of string lemmatized
dico = Dictionary(texts_lemmatized)
corpus_lda = [dico.doc2bow(text) for text in texts_lemmatized]

lda_ = LdaModel(corpus_lda, num_topics=18)

df_ = pd.DataFrame([])
data = []

# theme_commentaire = label of the string
for i in range(0, len(theme_commentaire)):
     # lda_.get_document_topics() gives the distribution of all topic for a specific sentence
     algo = max(lda_.get_document_topics(corpus_lda[i]))[0]
     human = theme_commentaire[i]
     data.append([str(algo), human])

cols = ['algo', 'human']
df_ = pd.DataFrame(data, columns=cols)
df_.head()
</code></pre>
","python, gensim","<p>Resolved in comments:  </p>

<blockquote>
  <p>I've found my problem though,  It's the max() function, it operates on the key value of my list of tuple 
  [(num_topics, probability)] so basically I'll get 17 most of the time because it's the biggest key. – glouis</p>
</blockquote>
",0,0,500,2017-05-10 14:59:56,https://stackoverflow.com/questions/43896195/gensim-find-topics-in-sentences
gensim file not found error,"<p>I am executing the following line:</p>

<pre><code>id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')
</code></pre>

<p>This code is available at <strong>""<a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a>""</strong>. I downloaded the wikipedia corpus and generated the required files and wiki_en_wordids.txt is one of those files. This file is available in the following location:</p>

<pre><code>~/gensim/results/wiki_en
</code></pre>

<p>So when i execute the code mentioned above I get the following error:</p>

<pre><code>Traceback (most recent call last):
          File ""~\Python\Python36-32\temp.py"", line 5, in &lt;module&gt;
            id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')
          File ""~\Python\Python36-32\lib\site-packages\gensim\corpora\dictionary.py"", line 344, in load_from_text
            with utils.smart_open(fname) as f:
          File ""~\Python\Python36-32\lib\site-packages\smart_open\smart_open_lib.py"", line 129, in smart_open
            return file_smart_open(parsed_uri.uri_path, mode)
          File ""~\Python\Python36-32\lib\site-packages\smart_open\smart_open_lib.py"", line 613, in file_smart_open
            return open(fname, mode)
          FileNotFoundError: [Errno 2] No such file or directory: 'wiki_en_wordids.txt'
</code></pre>

<p>Even though the file is available in the required location I get that error. Should I place the file in any other location? How do I determine what the right location is?</p>
","python, python-3.x, gensim","<p>The code requires an absolute path here. Relative path should be used when entire operation is carried out in the same directory location, but in this case, the file name is passed as argument to some other function which is located at different location.</p>

<p>One way to handle this situation is using <em>abspath</em> -</p>

<pre><code>    import os
    id2word = gensim.corpora.Dictionary.load_from_text(os.path.abspath('wiki_en_wordids.txt'))
</code></pre>
",3,2,1559,2017-05-12 16:48:23,https://stackoverflow.com/questions/43942790/gensim-file-not-found-error
Is there a way to load the wiki-fasttext model faster with load_word2vec_format,"<p>Loading the wiki-fasttext model with the gensim library takes <strong>six</strong> minutes. </p>

<p>I'm aware of ways to cache the model but I'm looking for ways to speedup the initial model loading. The specific api is below:</p>

<pre><code>en_model = KeyedVectors.load_word2vec_format(os.path.join(root_dir, model_file))
</code></pre>

<p>Granted, wiki-fasttext a very large model, however I have load the same model in many languages.  </p>
","nlp, stanford-nlp, gensim, fasttext","<p>You can try to use <code>limit=vector_num</code> argument for load <code>vector_num</code> word vectors from a file. You will not load all the vectors, but you can speed up the loading process.</p>
",2,1,1439,2017-05-15 17:07:37,https://stackoverflow.com/questions/43985180/is-there-a-way-to-load-the-wiki-fasttext-model-faster-with-load-word2vec-format
What is different between doc2vec models when the dbow_words is set to 1 or 0?,"<p>I read this <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""noreferrer"">page</a> but I do not understand what is different between models which are built based on the following codes.
I know when dbow_words is 0, training of doc-vectors is faster.</p>

<p>First model</p>

<pre><code>model = doc2vec.Doc2Vec(documents1, size = 100, window = 300, min_count = 10, workers=4)
</code></pre>

<p>Second model</p>

<pre><code>model = doc2vec.Doc2Vec(documents1, size = 100, window = 300, min_count = 10, workers=4,dbow_words=1)
</code></pre>
","gensim, doc2vec","<p>The <code>dbow_words</code> parameter only has effect when training a DBOW model –
 that is, with the non-default <code>dm=0</code> parameter. </p>

<p>So, between your two example lines of code, which both leave the default <code>dm=1</code> value unchanged, there's no difference. </p>

<p>If you instead switch to DBOW training, <code>dm=0</code>, then with a default <code>dbow_words=0</code> setting, the model is pure PV-DBOW as described in the original 'Paragraph Vectors' paper. Doc-vectors are trained to be predictive of text example words, but <em>no</em> word-vectors are trained. (There'll still be some randomly-initialized word-vectors in the model, but they're not used or improved during training.) This mode is fast and still works pretty well. </p>

<p>If you add the <code>dbow_words=1</code> setting, then skip-gram word-vector training will be added to the training, in an interleaved fashion. (For each text example, both doc-vectors over the whole text, then word-vectors over each sliding context window, will be trained.) Since this adds more training examples, as a function of the <code>window</code> parameter, it will be significantly slower. (For example, with <code>window=5</code>, adding word-training will make training about 5x slower.) </p>

<p>This has the benefit of placing both the DBOW doc-vectors and the word-vectors into the ""same space"" - perhaps making the doc-vectors more interpretable by their closeness to words. </p>

<p>This mixed training might serve as a sort of corpus-expansion – turning each context-window into a mini-document – that helps improve the expressiveness of the resulting doc-vector embeddings. (Though, especially with sufficiently large and diverse document sets, it may be worth comparing against pure-DBOW with more passes.)</p>
",12,7,5472,2017-05-16 21:15:43,https://stackoverflow.com/questions/44011706/what-is-different-between-doc2vec-models-when-the-dbow-words-is-set-to-1-or-0
Unpickling Error while using Word2Vec.load(),"<p>I am trying to load a binary file using <code>gensim.Word2Vec.load(fname)</code> but I get the error:</p>

<blockquote>
  <p>File ""file.py"", line 24, in 
      model = gensim.models.Word2Vec.load('ammendment_vectors.model.bin')   </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 1396, in load
      model = super(Word2Vec, cls).load(*args, **kwargs) </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 271, in load
      obj = unpickle(fname)  </p>
  
  <p>File ""/home/hp/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 933, in unpickle
      return _pickle.load(f, encoding='latin1')</p>
  
  <p>_pickle.UnpicklingError: could not find MARK</p>
</blockquote>

<p>I googled but I am unable to figure out why this error is coming up. Please let me know if any other information is required.</p>
","python, gensim, word2vec","<p>This would normally work, if the file was created by gensim's native <code>.save()</code>. </p>

<p>Are you sure the file <code>'ammendment_vectors.model.bin'</code> is complete and uncorrupted? </p>

<p>Was it created using the same Python/gensim versions as in use where you're trying to <code>load()</code> it? </p>

<p>Can you try re-creating the file? </p>
",4,2,7060,2017-05-17 10:23:45,https://stackoverflow.com/questions/44022180/unpickling-error-while-using-word2vec-load
Gensim Word2Vec: poor training performance.,"<p>that might actually be a dumb question but I just can't figure out why my script with gensim.models.word2vec is not working. Here is the thing, I'm using the stanford sentiment analysis databank dataset (~11000 reviews), and i'm trying to build word2vec using gensim, this is my script: </p>

<pre><code>import gensim as gs 
import sys 

# open the datas
sentences = gs.models.word2vec.LineSentence('../processedWords.txt')
print(""size in RAM of the sentences: {}"".format(sys.getsizeof(sentences)))

# transform them
# bigram_transformer = gs.models.Phrases(sentences)

model = gs.models.word2vec.Word2Vec(sentences, min_count=10, size=100, window=5)
model.save('firstModel')
print(model.similarity('film', 'test'))
print(model.similarity('film', 'movie'))
</code></pre>

<p>Now, my problem is that the script runs in 2s, and gives only huge similarity between every pair of words. In addition, some words which are in the sentences are not in the built vocabulary. </p>

<p>I must be doing something obviously wrong, but can't figure what. </p>

<p>Thank you for your help. </p>
","python-3.x, dataset, text-mining, gensim, word2vec","<p>I'm almost certain that this is because you haven't specified a number of training iterations; I think <code>iter</code> defaults to 1, which is basically useless for training a neural net. Add the <code>iter=&lt;int&gt;</code> flag to your model declaration, e.g. <code>model = gs.models.word2vec.Word2Vec(sentences, min_count=10, size=100, window=5, iter=1000)</code>.
Kind of a face-palmer but I did the same exact thing.</p>
",3,3,1355,2017-05-20 20:26:35,https://stackoverflow.com/questions/44090503/gensim-word2vec-poor-training-performance
Code for gensim Word2vec as an HTTP service &#39;KeyedVectors&#39; Attribute error,"<p>I am using the <a href=""https://github.com/RaRe-Technologies/w2v_server_googlenews"" rel=""nofollow noreferrer"">w2v_server_googlenews</a> code from the word2vec HTTP server running at <a href=""https://rare-technologies.com/word2vec-tutorial/#bonus_app"" rel=""nofollow noreferrer"">https://rare-technologies.com/word2vec-tutorial/#bonus_app</a>. I changed the loaded file to a file of vectors trained with the original C version of word2vec. I load the file with </p>

<pre><code>gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)
</code></pre>

<p>and it seems to load without problems. But when I test the HTTP service with, let's say </p>

<pre><code>curl 'http://127.0.0.1/most_similar?positive%5B%5D=woman&amp;positive%5B%5D=king&amp;negative%5B%5D=man' 
</code></pre>

<p>I got an empty result with only the execution time.</p>

<pre><code>{""taken"": 0.0003361701965332031, ""similars"": [], ""success"": 1}
</code></pre>

<p>I put a <code>traceback.print_exc()</code> on the except part of the related method, which is in this case <code>def most_similar(self, *args, **kwargs):</code> and I got: </p>

<pre><code>Traceback (most recent call last):
  File ""./w2v_server.py"", line 114, in most_similar
    topn=5)
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py"", line 304, in most_similar
    self.init_sims()
  File ""/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py"", line 817, in init_sims
    self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)
AttributeError: 'KeyedVectors' object has no attribute 'syn0'
</code></pre>

<p>Any idea on why this might happens? </p>

<p>Note: I use python 2.7 and I installed gensim using pip, which gave me gensim 2.1.0.</p>
","python, gensim, word2vec","<p>FYI that demo code was baed on gensim 0.12.3 (from 2015, as listed in its <code>requirements.txt</code>), and would need updating to work with the latest gensim. </p>

<p>It might be sufficient to add a line to <code>w2v_server.py</code> at line 70 (just after the <code>load_word2vec_format()</code>), to force the creation of the needed <code>syn0norm</code> property (which in older gensims was auto-created on load), before deleting the raw <code>syn0</code> values. Specifically:</p>

<pre><code>self.model.init_sims(replace=True)
</code></pre>

<p>(You would leave out the <code>replace=True</code> if you were going to be doing operations other than <code>most_similar()</code>, that might require raw vectors.)</p>

<p>If this works to fix the problem for you, a pull-request to the <a href=""https://github.com/RaRe-Technologies/w2v_server_googlenews"" rel=""nofollow noreferrer"">w2v_server_googlenews</a> repo would be favorably received!</p>
",2,2,515,2017-05-23 19:26:36,https://stackoverflow.com/questions/44143441/code-for-gensim-word2vec-as-an-http-service-keyedvectors-attribute-error
How does doc2vec.infer_vector combine across words?,"<p>I trained a doc2vec model using train(..) with default settings. That worked, but now I'm wondering how infer_vector combines across input words, is it just the average of the individual word vectors?</p>

<pre><code>model.random.seed(0)
model.infer_vector(['cat', 'hat'])
model.random.seed(0)
model.infer_vector(['cat'])
model.infer_vector(['hat']) #doesn't average up to the ['cat', 'hat'] vector
model.random.seed(0)
model.infer_vector(['hat'])
model.infer_vector(['cat']) #doesn't average up to the ['cat', 'hat'] vector
</code></pre>

<p>Those don't add up, so I'm wondering what I'm misunderstanding. </p>
","python, gensim, doc2vec","<p><code>infer_vector()</code> doesn't combine the vectors for your given tokens – and in some modes doesn't consider those tokens' vectors at all.</p>

<p>Rather, it considers the entire Doc2Vec model as being frozen against internal changes, and then assumes the tokens you've provided are an example text, with a previously untrained tag. Let's call this implied but unnamed tag <em>X</em>. </p>

<p>Using a training-like process, it tries to find a good vector for <em>X</em>. That is, it starts with a random vector (as it did for all tags in original training), then sees how well that vector as model-input predicts the text's words (by checking the model neural-network's predictions for input <em>X</em>). Then via incremental gradient descent it makes that candidate vector for <em>X</em> better and better at predicting the text's words. </p>

<p>After enough such inference-training, the vector will be about as good (given the rest of the frozen model) as it possibly can be at predicting the text's words. So even though you're providing that text as an ""input"" to the method, inside the model, what you've provided is used to pick target ""outputs"" of the algorithm for optimization. </p>

<p>Note that:</p>

<ul>
<li>tiny examples (like one or a few words) aren't likely to give very meaningful results – they are sharp-edged corner cases, and the essential value of these sorts of dense embedded representations usually arises from the marginal balancing of many word-influences </li>
<li>it will probably help to do far more training-inference cycles than the <code>infer_vector()</code> default <code>steps=5</code> – some have reported tens or hundreds of <code>steps</code> work best for them, and it may be especially valuable to use more <code>steps</code> with short texts</li>
<li>it may also help to use a starting <code>alpha</code> for inference more like that used in bulk training (<code>alpha=0.025</code>), rather than the <code>infer_vector()</code> default (<code>alpha=0.1</code>)</li>
</ul>
",2,1,649,2017-05-24 16:24:45,https://stackoverflow.com/questions/44163836/how-does-doc2vec-infer-vector-combine-across-words
Problems accessing docvectors with gensim,"<p>I'm trying to use gensim's (ver 1.0.1) <code>doc2vec</code> to get the cosine similarities of documents. This should be relatively simple, but I'm having problems retrieving the vector of the documents so I can do cosine similarity. When I try to retrieve a document by the label I gave it in training, I get a key error. </p>

<p>For example, 
<code>print(model.docvecs['4_99.txt'])</code> 
will tell me that there is no such key as <code>4_99.txt</code>.</p>

<p>However if I print <code>print(model.docvecs.doctags)</code> I see things like this:
<code>'4_99.txt_3': Doctag(offset=1644, word_count=12, doc_count=1)</code></p>

<p>So it appears that for every document, <code>doc2vec</code> is saving each sentence as the ""document name underscore number""</p>

<p>So I'm either 
A) training incorrectly or
B) Don't understand how to retrieve the doc vector so that I can do <code>similarity(d1, d2)</code></p>

<p>Can anyone help me out here? </p>

<p>Here is how I train my doc2vec:</p>

<pre><code>#Obtain txt abstracts and txt patents 
filedir = os.path.abspath(os.path.join(os.path.dirname(__file__)))
files = os.listdir(filedir)

#Doc2Vec takes [['a', 'sentence'], 'and label']
docLabels = [f for f in files if f.endswith('.txt')]

sources = {}  #{'2_139.txt': '2_139.txt'}
for lable in docLabels:
    sources[lable] = lable
sentences = LabeledLineSentence(sources)


model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)
model.build_vocab(sentences.to_array())
for epoch in range(10):
    model.train(sentences.sentences_perm())

model.save('./a2v.d2v')
</code></pre>

<p>This uses this class </p>

<p><code>class LabeledLineSentence(object):</code></p>

<pre><code>def __init__(self, sources):
    self.sources = sources

    flipped = {}

    # make sure that keys are unique
    for key, value in sources.items():
        if value not in flipped:
            flipped[value] = [key]
        else:
            raise Exception('Non-unique prefix encountered')

def __iter__(self):
    for source, prefix in self.sources.items():
        with utils.smart_open(source) as fin:
            for item_no, line in enumerate(fin):
                yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])

def to_array(self):
    self.sentences = []
    for source, prefix in self.sources.items():
        with utils.smart_open(source) as fin:
            for item_no, line in enumerate(fin):
                self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))
    return self.sentences

def sentences_perm(self):
    shuffle(self.sentences)
    return self.sentences
</code></pre>

<p>I got this class from a web tutorial (<a href=""https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1"" rel=""nofollow noreferrer"">https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1</a>) to help me get around Doc2Vec's weird data formatting requirements and I don't completely understand it to be honest. It does look like this class written here is adding the <code>_n</code> for each sentence, but in the tutorial it seems that they still retrieve the document vector with just giving it the filename... So what am I doing wrong here?</p>
","gensim, doc2vec","<p>The gensim Doc2Vec class uses exactly the document 'tags' you've passed it during training as keys to the doc-vectors. </p>

<p>And yes, that <code>LabeledLineSentence</code> class is adding <code>_n</code> to the document-tags. Specifically, those appear to be the line-numbers from the associated files. </p>

<p>So you'll have to request vectors using those same keys that were provided during training, with the <code>_n</code> – if what you really want is a vector-per-line. </p>

<p>If you instead want each file to be its own document, you'll need to change the corpus class to use the whole file as a document. Looking at the tutorial you reference, it appears they have a second <code>LabeledLineSentence</code> class that <em>isn't</em> line-oriented (but still is named that way), but you're not using that variant. </p>

<p>Separately, you don't need to loop and call <code>train()</code> multiple times, and manually adjust the <code>alpha</code>. That's almost certainly not doing what you intend, in any recent version of gensim, where <code>train()</code> already iterates over the corpus multiple times. In the most recent versions of gensim there will even be an error if you call it that way, since many outdated examples on the web encourage this mistake.  </p>

<p>Just call <code>train()</code> once – it will iterate over your corpus the number of times specified when the model was constructed. (That's a default of 5, but controllable with the <code>iter</code> initialization parameter. And, 10 or more is common with Doc2Vec corpuses.)</p>
",2,1,562,2017-05-29 00:37:15,https://stackoverflow.com/questions/44233296/problems-accessing-docvectors-with-gensim
how to get a bound or perplexity value of the new unseen document on trained model?,"<p>I would like to find outliers in my dataset by using LDA. In order to specify outliers, For this case, I am planning to use a bound or perplexity value of the new unseen document on trained model?
After that, I will sort the values in ascending order to check whether it is the outlier or not? 
My issue is that I could not get a bound/perplex value of individual doc, the model throws me <strong>""TypeError: 'int' object is not subscriptable""</strong> error.</p>

<p>I would appreciate if you help me to solve my case?</p>

<p>Just in case, I am attaching my code : </p>

<pre><code>tokenized_corpora = dictionary.doc2bow(_acc[2])
total_number_of_words_tokenized_corpora = len(tokenized_corpora)
bound_corpora = ldaModel.bound(tokenized_corpora)
per_word_perplex_corpora = np.exp2(-bound_corpora / 
total_number_of_words_tokenized_corpora)
</code></pre>

<p>Thanks in advance. </p>
","python, gensim","<p>According to my research, in order to get log perplexity of single document, the following command can be used : </p>

<pre><code>ldaModel.log_perplexity([bow])
</code></pre>
",1,1,220,2017-05-29 19:32:17,https://stackoverflow.com/questions/44249358/how-to-get-a-bound-or-perplexity-value-of-the-new-unseen-document-on-trained-mod
LDA/LSI Topic modelling in Gensim with predefined list of topics,"<p>I have a set of documents. I also have the title of topics based on which I want to categorize documents. My preference is to use LDA in Gensim. is there any way to feed my own list of topics in the topic modeling algorithm ?</p>
","gensim, lda, topic-modeling","<p>As far as I know, the idea of LDA is to do topic modelling in unsupervised manner which means no predefined topics is needed to be fed to the model to predict topic(s) of a given document.</p>
",0,1,461,2017-06-01 07:23:07,https://stackoverflow.com/questions/44301061/lda-lsi-topic-modelling-in-gensim-with-predefined-list-of-topics
What “information” in document vectors makes sentiment prediction work?,"<p>Sentiment prediction based on document vectors works pretty well, as examples show:
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb</a>
<a href=""http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow noreferrer"">http://linanqiu.github.io/2015/10/07/word2vec-sentiment/</a></p>

<p>I wonder what pattern is in the vectors making that possible. I thought it should be similarity of vectors making that somehow possible. Gensim similarity measures rely on cosine similarity. Therefore, I tried the following:</p>

<p>Randomly initialised a fix “compare” vector, get cosine similarity of the “compare” vector with all other vectors in training and test set, use the similarities and the labels of the train set to estimate a logistic regression model, evaluate the model with the test set.</p>

<p>Looks like this, where train/test_arrays contain document vectors and train/test_labels labels either 0 or 1. (Notice, document vectors are obtained from genism doc2vec and are well trained, predicting the test set 80% right if directly used as input for the logistic regression):</p>

<pre><code>fix_vec = numpy.random.rand(100,1)
def cos_distance_to_fix(x):
    return scipy.spatial.distance.cosine(fix_vec, x)

train_arrays_cos =  numpy.reshape(numpy.apply_along_axis(cos_distance_to_fix, axis=1, arr=train_arrays), newshape=(-1,1))
test_arrays_cos = numpy.reshape(numpy.apply_along_axis(cos_distance_to_fix, axis=1, arr=test_arrays), newshape=(-1,1))

classifier = LogisticRegression()
classifier.fit(train_arrays_cos, train_labels)
classifier.score(test_arrays_cos, test_labels)
</code></pre>

<p>It turns out, that this approach does not work, predicting the test set only to 50%....
So, my question is, what “information” is in the vectors, making the prediction based on vectors work, if it is not the similarity of vectors? Or is my approach simply not possible to capture similarity of vectors correct?</p>
","machine-learning, sentiment-analysis, gensim, feature-selection, doc2vec","<p>This is less a question about Doc2Vec than about machine-learning principles with high-dimensional data. </p>

<p>Your approach is collapsing 100-dimensions to a single dimension – the distance to your <em>random</em> point. Then, you're hoping that single dimension can still be predictive. </p>

<p>And roughly all LogisticRegression can do with that single-valued input is try to pick a threshold-number that, when your distance is on one side of that threshold, predicts a class – and on the other side, predicts not-that-class. </p>

<p>Recasting that single-threshold-distance back to the original 100-dimensional space, it's essentially trying to find a hypersphere, around your random point, that does a good job collecting all of a single class either inside or outside its volume. </p>

<p>What are the odds your randomly-placed center-point, plus one adjustable radius, can do that well, in a complex high-dimensional space? My hunch is: not a lot. And your results, no better than random guessing, seems to suggest the same. </p>

<p>The LogisticRegression with access to the full 100-dimensions finds a discriminating-frontier for assigning the class that's described by 100 coefficients and one intercept-value – and all of those 101 values (free parameters) can be adjusted to improve its classification performance. </p>

<p>In comparison, your alternative LogisticRegression with access to only the one 'distance-from-a-random-point' dimension can pick just one coefficient (for the distance) and an intercept/bias. It's got 1/100th as much information to work with, and only 2 free parameters to adjust. </p>

<p>As an analogy, consider a much simpler space: the surface of the Earth. Pick a 'random' point, like say the South Pole. If I then tell you that you are in an unknown place 8900 miles from the South Pole, can you answer whether you are more likely in the USA or China? Hardly – both of those 'classes' of location have lots of instances 8900 miles from the South Pole. </p>

<p>Only in the extremes will the distance tell you for sure which class (country) you're in – because there are parts of the USA's Alaska and Hawaii further north and south than parts of China. But even there, you can't manage well with just a single threshold: you'd need a rule which says, ""less than X <em>or</em> greater than Y, in USA; otherwise unknown"". </p>

<p>The 100-dimensional space of Doc2Vec vectors (or other rich data sources) will often only be sensibly divided by far more complicated rules. And, our intuitions about distances and volumes based on 2- or 3-dimensional spaces will often lead us astray, in high dimensions. </p>

<p>Still, the Earth analogy does suggest a way forward: there are <em>some reference points</em> on the globe that will work way better, when you know the distance to them, at deciding if you're in the USA or China. In particular, a point at the center of the US, or at the center of China, would work really well. </p>

<p>Similarly, you may get somewhat better classification accuracy if rather than a random <code>fix_vec</code>, you pick either (a) any point for which a class is already known; or (b) some average of all known points of one class. In either case, your <code>fix_vec</code> is then likely to be ""in a neighborhood"" of similar examples, rather than some random spot (that has no more essential relationship to your classes than the South Pole has to northern-Hemisphere temperate-zone countries). </p>

<p>(Also: alternatively picking N multiple random points, and then feeding the N distances to your regression, will preserve more of the information/shape of the original Doc2Vec data, and thus give the classifier a better chance of finding a useful separating-threshold. Two would likely do better than your one distance, and 100 might approach or surpass the 100 original dimensions.)</p>

<p>Finally, some comment about the Doc2Vec aspect:</p>

<p>Doc2Vec optimizes vectors that are somewhat-good, within their constrained model, at predicting the words of a text. Positive-sentiment words tend to occur together, as do negative-sentiment words, and so the trained doc-vectors tend to arrange themselves in similar positions when they need to predict similar-meaning-words. So there are likely to be 'neighborhoods' of the doc-vector space that correlate well with predominantly positive-sentiment or negative-sentiment words, and thus positive or negative sentiments. </p>

<p>These won't necessarily be two giant neighborhoods, 'positive' and 'negative', separated by a simple boundary –or even a small number of neighborhoods matching our ideas of 3-D solid volumes. And many subtleties of communication – such as sarcasm, referencing a not-held opinion to critique it, spending more time on negative aspects but ultimately concluding positive, etc – mean incursions of alternate-sentiment words into texts. A fully-language-comprehending human agent could understand these to conclude the 'true' sentiment, while these word-occurrence based methods will still be confused. </p>

<p>But with an adequate model, and the right number of free parameters, a classifier might capture some generalizable insight about the high-dimensional space. In that case, you can achieve reasonably-good predictions, using the Doc2Vec dimensions – as you've seen with the ~80%+ results on the full 100-dimensional vectors.</p>
",1,1,194,2017-06-01 11:20:24,https://stackoverflow.com/questions/44306123/what-information-in-document-vectors-makes-sentiment-prediction-work
Why are almost all cosine similarities positive between word or document vectors in gensim doc2vec?,"<p>I have calculated document similarities using Doc2Vec.docvecs.similarity() in gensim. Now, I would either expect the cosine similarities to lie in the range [0.0, 1.0] if gensim used the absolute value of the cosine as the similarity metric, or roughly half of them to be negative if it does not.</p>

<p>However, what I am seeing is that <em>some</em> similarities are negative, but they are very rare – less than 1% of pairwise similarities in my set of 30000 documents.</p>

<p>Why are almost all of the similarities positive?</p>
","python, gensim, word2vec, doc2vec","<p>There's no inherent guarantee in Word2Vec/Doc2Vec that the generated set of vectors is symmetrically distributed around the origin point. They could be disproportionately in some directions, which would yield the results you've seen. </p>

<p>In a few tests I just did on the toy-sized dataset ('lee corpus') used in the bundled gensim <code>docs/notebooks/doc2vec-lee.ipynb</code> notebook, checking the cosine-similarities of all documents against the first document, it vaguely seems that:</p>

<ol>
<li>using hierarchical-softmax rather than negative sampling (<code>hs=1, negative=0</code>) yields a balance between >0.0 and &lt;0.0 cosine-similarities that is closer-to (but not yet quite) half and half</li>
<li>using a smaller number of negative samples (such as <code>negative=1</code>) yields a more balanced set of results; using a larger number (such as <code>negative=10</code>) yields relatively more >0.0 cosine-similarities</li>
</ol>

<p>While not conclusive, this is mildly suggestive that the arrangement of vectors may be influenced by the <code>negative</code> parameter. Specifically, typical negative-sampling parameters, such as the default <code>negative=5</code>, mean words will be trained more times as non-targets, than as positive targets. That <em>might</em> push the preponderance of final coordinates in one direction. (More testing on larger datasets and modes, and more analysis of how the model setup could affect final vector positions, would be necessary to have more confidence in this idea.)</p>

<p>If for some reason you wanted a more balanced arrangement of vectors, you could consider transforming their positions, post-training. </p>

<p>There's an interesting recent paper in the word2vec space, <a href=""https://arxiv.org/abs/1702.01417"" rel=""nofollow noreferrer"">""All-but-the-Top: Simple and Effective Postprocessing for Word Representations""</a>, that found sets of trained word-vectors don't necessarily have a 0-magnitude mean – they're on average in one direction from the origin. And further, this paper reports that subtracting the common mean (to 're-center' the set), and also removing a few other dominant directions, can improve the vectors' usefulness for certain tasks. </p>

<p>Intuitively, I suspect this 'all-but-the-top' transformation might serve to increase the discriminative 'contrast' in the resulting vectors. </p>

<p>A similar process <em>might</em> yield similar benefits for doc-vectors – and would likely make the full set of cosine-similarities, to any doc-vector, more balanced between >0.0 and &lt;0.0 values.</p>
",4,3,1885,2017-06-03 15:29:31,https://stackoverflow.com/questions/44345576/why-are-almost-all-cosine-similarities-positive-between-word-or-document-vectors
Importing word vectors from tensorflow into gensim,"<p>I want to import word vecters created from tensorflow and utilize it at gensim.</p>

<p>there is a method <code>gensim.models.KeyedVectors.load_word2vec_format</code></p>

<p>so I tried this method by following exactly the same way in <a href=""https://stackoverflow.com/questions/42186543/training-wordvec-in-tensorflow-importing-to-gensim"">Training wordvec in Tensorflow, importing to Gensim</a></p>

<p>Example:</p>

<blockquote>
  <p>2 3</p>
  
  <p>word0 -0.000737 -0.002106 0.001851</p>
  
  <p>word1 -0.000878 -0.002106 0.002834</p>
</blockquote>

<p>Save the file and then load with kwarg binary=False:</p>

<pre><code>model = Word2Vec.load_word2vec_format(filename, binary=False)
</code></pre>

<p>but error like</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    model=gensim.models.KeyedVectors.load_word2vec_format('test.w2v')
  File ""C:\Users\cbj\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py"", line 243, in load_word2vec_format
    raise EOFError(""unexpected end of input; is count incorrect or file otherwise damaged?"")
EOFError: unexpected end of input; is count incorrect or file otherwise damaged?
</code></pre>

<p>raised</p>

<p>how can I solve this problem?</p>
","python, tensorflow, gensim, word2vec, word-embedding","<p>This error is raised when the number of vector data doesn't match the number you provided at the first line.</p>

<p>If the first line wrote <code>2 3</code>, you should have exactly <code>2</code> lines below. Make sure that there's no empty line at the end of your file and of course some where in your file.</p>
",1,1,2488,2017-06-05 15:00:06,https://stackoverflow.com/questions/44371835/importing-word-vectors-from-tensorflow-into-gensim
Getting error while using gensim model in python,"<p>I have made doc2vec file by training data using gensim model now while processing it. I am getting an error.
I am running the below code:-</p>

<p>model = Doc2Vec.load('sentiment140.d2v')</p>

<pre><code>if len(sys.argv) &lt; 4:
    print (""Please input train_pos_count, train_neg_count and classifier!"")
    sys.exit()

train_pos_count = int(sys.argv[1])
train_neg_count = int(sys.argv[2])
test_pos_count = 144
test_neg_count = 144

print (train_pos_count)
print (train_neg_count)

vec_dim = 100

print (""Build training data set..."")
train_arrays = numpy.zeros((train_pos_count + train_neg_count, vec_dim))
train_labels = numpy.zeros(train_pos_count + train_neg_count)

for i in range(train_pos_count):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    train_arrays[i] = model.docvecs[prefix_train_pos]
    train_labels[i] = 1

for i in range(train_neg_count):
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    train_arrays[train_pos_count + i] = model.docvecs[prefix_train_neg]
    train_labels[train_pos_count + i] = 0


print (""Build testing data set..."")
test_arrays = numpy.zeros((test_pos_count + test_neg_count, vec_dim))
test_labels = numpy.zeros(test_pos_count + test_neg_count)

for i in range(test_pos_count):
    prefix_test_pos = 'TEST_POS_' + str(i)
    test_arrays[i] = model.docvecs[prefix_test_pos]
    test_labels[i] = 1

for i in range(test_neg_count):
    prefix_test_neg = 'TEST_NEG_' + str(i)
    test_arrays[test_pos_count + i] = model.docvecs[prefix_test_neg]
    test_labels[test_pos_count + i] = 0


print (""Begin classification..."")
classifier = None
if sys.argv[3] == '-lr':
    print (""Logistic Regressions is used..."")
    classifier = LogisticRegression()
elif sys.argv[3] == '-svm':
    print (""Support Vector Machine is used..."")
    classifier = SVC()
elif sys.argv[3] == '-knn':
    print (""K-Nearest Neighbors is used..."")
    classifier = KNeighborsClassifier(n_neighbors=10)
elif sys.argv[3] == '-rf':
    print (""Random Forest is used..."")
    classifier = RandomForestClassifier()

classifier.fit(train_arrays, train_labels)

print (""Accuracy:"", classifier.score(test_arrays, test_labels))
</code></pre>

<p>I am getting a Keyerror - ""TEST_POS_72""<a href=""https://i.sstatic.net/z9TEW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/z9TEW.png"" alt=""ERROR""></a></p>

<p>I want to know what I am doing wrong.</p>
","python, gensim, word2vec","<p>The error means quite literally that no doc-vector with the key ('tag') <code>TEST_POS_72</code> is part of the model. There mustn't have been any documents with that tag presented during training. </p>

<p>You can see a list of all known doc-tags in the model in <code>model.docvecs.offset2doctag</code>. If <code>TEST_POS_72</code> isn't there, you can't access a doc-vector via <code>model.docvecs['TEST_POS_72']</code>. (If that list is empty, then the doc-vectors were trained to be accessed by plain int keys – and <code>model.docvecs[72]</code> would be a more appropriate way to access a doc-vector.)</p>

<p>(Separately, Doc2Vec won't work well with tiny corpuses of a few hundred documents, and the warning in your screenshot ""Slow version of gensim.models.doc2vec is being used"" means that gensim's optimized C-compiled routines weren't part of the installation, and training will be 100x or more slower.)</p>
",0,2,250,2017-06-09 04:05:11,https://stackoverflow.com/questions/44449132/getting-error-while-using-gensim-model-in-python
Gensim: Slow version of gensim.models.doc2vec is being used,"<p>My setup is as follows:</p>

<p>Python version: 3.6.0</p>

<p>Numpy version: 1.13.0</p>

<p>Scipy version: 0.19.0</p>

<p>Gensim version: 2.1.0</p>

<p>GCC Compiler version: 5.3.0</p>

<p>System: Windows 7, 64bit</p>

<p>I get the following error with the setup above</p>

<pre><code>import gensim
&gt;&gt;&gt;Slow version of gensim.models.doc2vec is being used
</code></pre>

<p>This makes the run time far too slow when training models on gensim. I feel there is some problem with the package versions I am using or how I installed them because: I had to install numpy using <code>pip</code>; I had to install scipy using <code>conda</code>; and I had to install gensim using <code>pip</code> again. The reason for this setup, is because if I try to install scipy using <code>pip</code>, I get the error</p>

<pre><code>&gt;&gt;&gt;ImportError: DLL load failed: The specified procedure could not be found.
</code></pre>

<p>So I had to install scipy via <code>conda</code>. Also, if I try to install gensim using</p>

<p><code>conda install gensim</code></p>

<p>or</p>

<p><code>conda update gensim</code></p>

<p>it only installs version 1 - I have tried <code>conda install -c anaconda gensim=2.1.0</code> but I get the error</p>

<pre><code>PackageNotFoundError: Package missing in current win-64 channels:
- gensim 2.1.0*
</code></pre>

<p>Numpy and Scipy work fine independently when I import them into a script - that is, they import fine and I can use all their functionality. However, when they are being used by Gensim, clearly there is a problem and I don't know why.</p>

<p><strong>Would anyone be able to advise possible fixes? Ideally I would like to keep all the latest versions of these packages if possible. Thank you in advance</strong></p>

<p><strong>NOTE: Gensim works fine with the ""fast"" version when I have Gensim version 1 installed and with the same versions of the dependencies above!</strong></p>
","python, numpy, scipy, pip, gensim","<p>The problem is to do with the some underlying packages not being up to date. <a href=""http://The%20problem%20is%20to%20do%20with%20the%20some%20underlying%20packages%20not%20being%20up%20to%20date."" rel=""nofollow noreferrer"">Here</a> I found the answer which work for me, which is in short:</p>

<p>Uninstall Gensim  </p>

<pre><code>sudo pip3 uninstall gensim
</code></pre>

<p>Install python3-dev build-essential  </p>

<pre><code>sudo apt-get install python3-dev build-essential  
</code></pre>

<p>Re-Install Gensim  </p>

<pre><code>sudo pip3 install --upgrade gensim
</code></pre>

<p>Notes:</p>

<p>Instructions above are for systems where pip and apt-get are used to manage packages<br>
pip3 is the python3 version of pip</p>
",1,0,1655,2017-06-09 15:32:12,https://stackoverflow.com/questions/44461656/gensim-slow-version-of-gensim-models-doc2vec-is-being-used
Tf-idf calculation using gensim,"<p>I have one tf-idf example from an ISI paper. I’m trying to validate my code by this example. But I get different result from my code.I don’t know what the reason is!</p>

<p>Term-document matrix from paper:  </p>

<pre><code>acceptance     [ 0 1 0 1 1 0
information      0 1 0 1 0 0
media            1 0 1 0 0 2
model            0 0 1 1 0 0
selection        1 0 1 0 0 0 
technology       0 1 0 1 1 0]
</code></pre>

<p>Tf-idf matrix from paper: </p>

<pre><code>acceptance     [ 0   0.4   0   0.3   0.7  0
information      0   0.7   0   0.5   0    0
media            0.3  0   0.2   0    0    1
model            0    0   0.6   0.5  0    0
selection        0.9  0   0.6   0    0    0 
technology       0   0.4   0   0.3   0.7  0]
</code></pre>

<p>My tf-idf matrix:</p>

<pre><code>acceptance     [ 0   0.4   0   0.3   0.7  0
information      0   0.7   0   0.5   0    0
media            0.5  0   0.4   0    0    1
model            0    0   0.6   0.5  0    0
selection        0.8  0   0.6   0    0    0 
technology       0   0.4   0   0.3   0.7  0]
</code></pre>

<p>My code:</p>

<pre><code>tfidf = models.TfidfModel(corpus)   
corpus_tfidf=tfidf[corpus]
</code></pre>

<p>I’ve tried another code like this:</p>

<pre><code>transformer = TfidfTransformer()
tfidf=transformer.fit_transform(counts).toarray() ##counts is term-document matrix
</code></pre>

<p>But I didn’t get appropriate answer</p>
","python, tf-idf, gensim","<p>The reason of this difference between results as you mentioned is that there are many methods to calculate TF-IDF in papers. if you read <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">Wikipedia TF-IDF page</a> it mentioned that TF-IDF is calculated as</p>

<blockquote>
  <p>tfidf(t,d,D) = tf(t,d) . idf(t,D)</p>
</blockquote>

<p>and both of tf(t,d) and idf(t,D) can be calculated with different functions that will change last result of TF_IDF value. Actually functions are different for their usage in different applications.</p>

<p><a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html"" rel=""nofollow noreferrer"">Gensim TF-IDF Model</a> can calculate any function for tf(t,d) and idf(t,D) as it mentioned in it's documentation.</p>

<blockquote>
  <p>Compute tf-idf by multiplying a local component (term frequency) with
  a global component (inverse document frequency), and normalizing the
  resulting documents to unit length. Formula for unnormalized weight of
  term i in document j in a corpus of D documents:</p>
  
  <p><code>weight_{i,j} = frequency_{i,j} * log_2(D / document_freq_{i})</code> </p>
  
  <p>or, more generally:</p>
  
  <p><code>weight_{i,j} = wlocal(frequency_{i,j}) * wglobal(document_freq_{i}, D)</code></p>
  
  <p>so you can plug in your own custom wlocal and wglobal functions.</p>
  
  <p>Default for wlocal is identity (other options: math.sqrt, math.log1p,
  ...) and default for wglobal is log_2(total_docs / doc_freq), giving
  the formula above.</p>
</blockquote>

<p>Now if you want to reach exactly the paper result, you must know what functions it used for calculating TF-IDF matrix.</p>

<p>Also there is a good example in <a href=""https://groups.google.com/forum/#!topic/gensim/ml_gBOZRxYU"" rel=""nofollow noreferrer"">Gensim google group</a> that shows how you can use custom function for calculating TF-IDF.</p>
",2,1,3115,2017-06-13 08:37:05,https://stackoverflow.com/questions/44516295/tf-idf-calculation-using-gensim
How do I use the word vector returned by word2vec as features?,"<p>I am planning to use Multi Layer Perceptron Classifier from Scikit Learn for this purpose.<br>
Output is the Gender of that word which shall be represented in a one-hot encoding like [1,0,0] for male, [0, 1, 0] for female and [0, 0, 1] for female. 
Now one of the inputs is the word vector for the word. Each of these vectors has 20 dimensions.
The other features are it's Part Of Speech Tags and Singularity(0)/Plurality(1) state. 
My question is how do I use the word vector which is an array as a feature in MLPClassifier?</p>
","python, scikit-learn, neural-network, gensim, word2vec","<p>Your w2v vector captures some semantic similarity with respect to the word. This vector must be considered a whole - it is a feature in itself. </p>

<p>One nice attribute of neural networks are their capability of extracting and learning patterns on their own. As input, you could consider concatenating the word vector along with a vectorised/numerical equivalent of the POS tag, and finally the singularity state:</p>

<pre><code>------------------- ----  -   
\_________________/ \__/  |     } ------ 25d vector input to the MLP (assuming your POS takes 4 spaces)
     w2v vector      POS state
</code></pre>

<p>As long as you follow a consistent scheme with the training, testing, and unseen data, your MLP will use the entire input to automatically extract features from the input as it learns.   </p>
",1,0,800,2017-06-14 19:34:58,https://stackoverflow.com/questions/44553278/how-do-i-use-the-word-vector-returned-by-word2vec-as-features
probabilities returned by gensim&#39;s get_document_topics method doesn&#39;t add up to one,"<p>Sometimes it returns probabilities for all topics and all is fine, but  sometimes it returns probabilities for just a few topics and they don't add up to one, it seems it depends on the document. Generally when it returns few topics, the probabilities add up to more or less 80%, so is it returning just the most relevant topics? Is there a way to force it to return all probabilities?</p>

<p>Maybe I'm missing something but I can't find any documentation of the method's parameters.</p>
","text-mining, gensim, lda, topic-modeling","<p>I had the same problem and solved it by including the argument <code>minimum_probability=0</code> when calling the <code>get_document_topics</code> method of <code>gensim.models.ldamodel.LdaModel</code> objects. </p>

<pre class=""lang-py prettyprint-override""><code>    topic_assignments = lda.get_document_topics(corpus,minimum_probability=0)
</code></pre>

<p>By default, <strong>gensim doesn't output probabilities below 0.01</strong>, so for any document in particular, if there are any topics assigned probabilities under this threshold the sum of topic probabilities for that document will not add up to one.</p>

<p>Here's an example:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import common_texts
from gensim.corpora.dictionary import Dictionary
from gensim.models.ldamodel import LdaModel

# Create a corpus from a list of texts
common_dictionary = Dictionary(common_texts)
common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]

# Train the model on the corpus.
lda = LdaModel(common_corpus, num_topics=100)

# Try values of minimum_probability argument of None (default) and 0
for minimum_probability in (None, 0):
    # Get topic probabilites for each document
    topic_assignments = lda.get_document_topics(common_corpus,minimum_probability=minimum_probability)
    probabilities = [ [entry[1] for entry in doc] for doc in topic_assignments ]
    # Print output
    print(f""Calculating topic probabilities with minimum_probability argument = {str(minimum_probability)}"")
    print(f""Sum of probabilites:"")
    for i, P in enumerate(probabilities):
        sum_P = sum(P)
        print(f""\tdoc {i} = {sum_P}"")
</code></pre>

<p>And the output would be:</p>

<pre><code>Calculating topic probabilities with minimum_probability argument = None
Sum of probabilities:
    doc 0 = 0.6733324527740479
    doc 1 = 0.8585712909698486
    doc 2 = 0.7549994885921478
    doc 3 = 0.8019999265670776
    doc 4 = 0.7524996995925903
    doc 5 = 0
    doc 6 = 0
    doc 7 = 0
    doc 8 = 0.5049992203712463
Calculating topic probabilities with minimum_probability argument = 0
Sum of probabilites:
    doc 0 = 1.0000000400468707
    doc 1 = 1.0000000337604433
    doc 2 = 1.0000000079162419
    doc 3 = 1.0000000284053385
    doc 4 = 0.9999999937135726
    doc 5 = 0.9999999776482582
    doc 6 = 0.9999999776482582
    doc 7 = 0.9999999776482582
    doc 8 = 0.9999999930150807
</code></pre>

<p>This default behaviour is not very clearly stated in the documentation. The default value for <code>minimum_probability</code> for the <code>get_document_topics</code> method is <code>None</code>, however this does not set the probability to zero. Instead the value of <code>minimum_probability</code> is set to the value of <code>minimum_probability</code> of the <code>gensim.models.ldamodel.LdaModel</code> object, which by default is 0.01 as you can see in the <a href=""https://github.com/RaRe-Technologies/gensim/blob/996801bb3fb8c4e10a84eefa70f5e2ac738dd47b/gensim/models/ldamodel.py#L347"" rel=""nofollow noreferrer"">source code</a>:</p>

<pre class=""lang-py prettyprint-override""><code>def __init__(self, corpus=None, num_topics=100, id2word=None,
             distributed=False, chunksize=2000, passes=1, update_every=1,
             alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10,
             iterations=50, gamma_threshold=0.001, minimum_probability=0.01,
             random_state=None, ns_conf=None, minimum_phi_value=0.01,
             per_word_topics=False, callbacks=None, dtype=np.float32):
</code></pre>
",5,5,2549,2017-06-15 15:36:38,https://stackoverflow.com/questions/44571617/probabilities-returned-by-gensims-get-document-topics-method-doesnt-add-up-to
How can I improve the cosine similarity of two documents(sentences) in doc2vec model?,"<p>I am building a NLP chat application in Python using <code>gensim</code> library through <code>doc2vec</code> model. I have hard coded documents and given a set of training examples, I am testing the model by throwing a user question and then finding most similar documents as a first step. In this case my test question is an exact copy of a document from training example. </p>

<pre><code>import gensim
from gensim import models
sentence = models.doc2vec.LabeledSentence(words=[u'sampling',u'what',u'is',u'tell',u'me',u'about'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'eligibility',u'what',u'is',u'my',u'limit',u'how',u'much',u'can',u'I',u'claim'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'eligibility',u'I',u'am',u'retiring',u'how',u'much',u'can',u'claim',u'have', u'resigned'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'what',u'is',u'my',u'eligibility',u'post',u'my',u'promotion'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'what',u'is', u'my',u'eligibility' u'post',u'my',u'promotion'], tags=[""SENT_4""])
sentences = [sentence, sentence1, sentence2, sentence3, sentence4]
class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename
    def __iter__(self):
        for uid, line in enumerate(open(filename)):
            yield LabeledSentence(words=line.split(), labels=['SENT_%s' % uid])
model = models.Doc2Vec(alpha=0.03, min_alpha=.025, min_count=2)
model.build_vocab(sentences)
for epoch in range(30):
    model.train(sentences, total_examples=model.corpus_count, epochs = model.iter)
    model.alpha -= 0.002  # decrease the learning rate`
    model.min_alpha = model.alpha  # fix the learning rate, no decay
model.save(""my_model.doc2vec"")
model_loaded = models.Doc2Vec.load('my_model.doc2vec')
print (model_loaded.docvecs.most_similar([""SENT_4""]))
</code></pre>

<p>Result:</p>

<pre><code>[('SENT_1', 0.043695494532585144), ('SENT_2', 0.0017897281795740128), ('SENT_0', -0.018954679369926453), ('SENT_3', -0.08253869414329529)]
</code></pre>

<p>Similarity of <code>SENT_4</code> and <code>SENT_3</code> is only <code>-0.08253869414329529</code> when it should be 1 since they are exactly same. How should I improve this accuracy? Is there a specific way of training documents and I am missing something out?  </p>
","python, nlp, gensim, word2vec, doc2vec","<p>Word2Vec/Doc2Vec don't work well on toy-sized examples (such as few texts, short texts, and few total words). Many of the desirable properties are only reliably achieved with training sets of millions of words, or tens-of-thousands of documents. </p>

<p>In particular, with only 5 examples, and only a dozen or two words, but 100-dimensions of modeling vectors, the training isn't forced to do the main thing which makes word-vectors/doc-vectors useful: compress representations into dense embeddings, where similar items <em>need</em> to be incrementally nudged near each other in vector space, because there's no way to retain all the original variation in a sort-of-giant-lookup-table.  With more dimensions than corpus variation, your identical-tokens <code>SENT_3</code> and <code>SENT_4</code> can adopt wildly different doc-vectors, and the model is still large enough to do great on its training task (essentially, 'overfit'), without the desired end-state of similar-texts having similar-vectors being forced. </p>

<p>You can sometimes squeeze a little more meaning out of small datasets with more training iterations, and a much-smaller model (in terms of vector <code>size</code>), but really: these vectors need large, varied datasets to become meaningful.</p>

<p>That's the main issue. Some other inefficiencies or errors in your example code:</p>

<ul>
<li><p>Your code doesn't use the class <code>LabeledLineSentence</code>, so there's no need to include it here – it's irrelevant boilerplate. (Also, <code>TaggedDocument</code> is the preferred name for the <code>words</code>+<code>tags</code> document class in recent gensim versions, rather than <code>LabeledSentence</code>.)</p></li>
<li><p>Your custom-management of <code>alpha</code> and <code>min_alpha</code> is unlikely to do anything useful. These are best left at their defaults unless you already have something working, understand the algorithm well, and then want to try subtle optimizations. </p></li>
<li><p><code>train()</code> will do its own iterations, so you don't need to call it many times in an outer loop. (This code as written does in its first loop 5 <code>model.iter</code> iterations at <code>alpha</code> values gradually descending from 0.03 to 0.025, then 5 iterations at a fixed alpha of 0.028, then 5 more at 0.026, then 27 more sets of 5 iterations at decreasing alpha, ending on the 30th loop at a fixed alpha of -0.028. That's a nonsense ending value – the learning-rate should never be negative – at the end of a nonsense progression. Even with a big dataset, these 150 iterations, about half happening at negative <code>alpha</code> values, would likely yield weird results.)</p></li>
</ul>
",1,1,2362,2017-06-16 06:07:00,https://stackoverflow.com/questions/44581914/how-can-i-improve-the-cosine-similarity-of-two-documentssentences-in-doc2vec-m
Natural language processing keywords for building search engine,"<p>I'm recently interested in NLP, and would like to build up search engine for product recommendation. (Actually I'm always wondering about how search engine for Google/Amazon is built up)</p>

<p>Take Amazon product as example, where I could access all ""word"" information about one product:</p>

<pre><code>Product_Name    Description      ReviewText
""XXX brand""    ""Pain relief""    ""This is super effective""
</code></pre>

<p>By applying <code>nltk</code> and <code>gensim</code> packages I could easily compare similarity of different products and make recommendations.</p>

<p>But here's another question I feel very vague about:
How to build a search engine for such products? </p>

<p>For example, if I feel pain and would like to search for medicine online, I'd like to type-in <code>""pain relief""</code> or <code>""pain""</code>, whose searching results should include <code>""XXX brand""</code>.</p>

<p>So this sounds more like keyword extraction/tagging question? How should this be done in NLP? I know <strong>corpus</strong> should contain <strong>all</strong> but <strong>single</strong> words, so it's like:</p>

<pre><code>[""XXX brand"" : (""pain"", 1),(""relief"", 1)]
</code></pre>

<p>So if I typed in either <code>""pain""</code> or <code>""relief""</code> I could get <code>""XXX brand""</code>; but what about I searched <code>""pain relief""</code>?</p>

<p>I could come up with idea that directly call python in my javascript for calculate similarities of input words <code>""pain relief""</code> on browser-based server and make recommendation; but that's kind of do-able? </p>

<p>I still prefer to build up very big lists of keywords at backends, stored in datasets/database and directly visualized in web page of search engine.</p>

<p>Thanks!</p>
","nlp, nltk, search-engine, gensim, corpus","<p>Even though this does not provide a full how-to answer, there are two things that might be helpful.</p>

<p>First, it's important to note that Google does <strong>not</strong> only treat singular words but also ngrams. 
More or less every NLP problem and therefore also information retrieval from text needs to tackle ngrams. This is because <em>phrases</em> carry way more expressiveness and information than singular tokens.</p>

<p>That's also why so called <em>NGramAnalyzers</em> are popular in search engines, be it Solr or elastic. Since both are based on Lucene, you should take a look <a href=""https://lucene.apache.org/core/6_1_0/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Relying on either framework, you can use a synonym analyser that adds for each word the synonyms you provide.
For example, you could add <code>relief = remedy</code> (and vice versa if you wish) to your synonym mapping. Then, both engines would retrieve relevant documents regardless if you search for ""pain relief"" or ""pain remedy"". However, you should probably also read <a href=""http://opensourceconnections.com/blog/2013/10/27/why-is-multi-term-synonyms-so-hard-in-solr/"" rel=""nofollow noreferrer"">this post</a> about the issues you might encounter, especially when aiming for phrase synonyms.</p>
",1,1,1594,2017-06-17 14:15:59,https://stackoverflow.com/questions/44605649/natural-language-processing-keywords-for-building-search-engine
how to improve topic model of gensim,"<p>I want to extract topics from articles, the test article is ""<a href=""https://julien.danjou.info/blog/2017/announcing-scaling-python"" rel=""nofollow noreferrer"">https://julien.danjou.info/blog/2017/announcing-scaling-python</a>"".</p>

<p>It's an aticle about python and scalling. I've tried lsi and lda, most of time , lda seems works better. But the output of both of them isn't stable. </p>

<p>Of course, the first three or five keywords seem to hit the target. ""python"", ""book"", 'project' ( I don't think 'project' should be an useful topic and will drop it in stopwords list.) , scaling or scalable or openstack should be in keywords list, but not stable at all.</p>

<p>Topic list and stopwords list might improve the results, but it's not scalable. I have to maintain different list for different domain.</p>

<p>So the question here, is there any better solution to improve the algorithm?</p>

<pre><code>num_topics = 1
num_words = 10
passes = 20
</code></pre>

<h3>lda model demo code, code of lsi is the same.</h3>

<pre><code>for topic in lda.print_topics(num_words=num_words):
    termNumber = topic[0]
    print(topic[0], ':', sep='')
    listOfTerms = topic[1].split('+')
    for term in listOfTerms:
        listItems = term.split('*')
        print('  ', listItems[1], '(', listItems[0], ')', sep='')
        lda_list.append(listItems[1])
</code></pre>

<h3>Test Result 1</h3>

<pre><code>Dictionary(81 unique tokens: ['dig', 'shoot', 'lot', 'world', 'possible']...)
# lsi result
0:
  ""python"" (0.457)
  ""book"" ( 0.391)
  ""project"" ( 0.261)
  ""like"" ( 0.196)
  ""application"" ( 0.130)
  ""topic"" ( 0.130)
  ""new"" ( 0.130)
  ""openstack"" ( 0.130)
  ""way"" ( 0.130)
  ""decided""( 0.130)

# lda result
0:
  ""python"" (0.041)
  ""book"" ( 0.036)
  ""project"" ( 0.026)
  ""like"" ( 0.021)
  ""scalable"" ( 0.015)
  ""turn"" ( 0.015)
  ""working"" ( 0.015)
  ""openstack"" ( 0.015)
  ""scaling"" ( 0.015)
  ""different""( 0.015)
</code></pre>

<h3>Test Result 2</h3>

<pre><code>Dictionary(81 unique tokens: ['happy', 'idea', 'tool', 'new', 'shoot']...)
# lsi result
0:
  ""python"" (0.457)
  ""book"" ( 0.391)
  ""project"" ( 0.261)
  ""like"" ( 0.196)
  ""scaling"" ( 0.130)
  ""application"" ( 0.130)
  ""turn"" ( 0.130)
  ""working"" ( 0.130)
  ""openstack"" ( 0.130)
  ""topic""( 0.130)
# lda result
0:
  ""python"" (0.041)
  ""book"" ( 0.036)
  ""project"" ( 0.026)
  ""like"" ( 0.021)
  ""decided"" ( 0.015)
  ""different"" ( 0.015)
  ""turn"" ( 0.015)
  ""writing"" ( 0.015)
  ""working"" ( 0.015)
  ""application""( 0.015)
</code></pre>
","python, gensim, topic-modeling","<p>If I understand correctly, you have an article and want your model to explain to you what it is about. </p>

<p>But if I didn't misunderstand something you train your LDA model on that one single document with one topic. So afterall, you are not really extracting the topics since you only have one topic. I don't think that's how LDA was intented to be used. Generally you will want to train your model on a large corpus (collection of documents), like all English Wikipedia articles or all articles from a journal from the past 60 years using some two or three digit topic number. That's typically when LDA starts to gain power. </p>

<p>Often when I try to ""understand"" a document by understanding its topic distribution I will train the model on a large corpus, not necessarily directly connected to the document I am trying to query. That is especially useful in cases when your documents are few and/or short, like in your case. </p>

<p>If you expect your document to be diverse in topics, you could train LDA on the English Wikipedia (that gives your topics from ['apple', 'banana',...] to ['regression', 'probit',...]). <br>
If you know that all documents you want to query lie in a particular field, maybe training LDA on a corpus from this field will lead to a lot better results, because the topics related to the field will be much more precisely separated. In your case, you could train a LDA model on several dozen/hundreds Python related books and articles. But that all depends on your goals. </p>

<p>Then you can always play around with the number of topics. For very large corpora you can try 100, 200 and even 1000 topics. For smaller ones maybe 5 or 10. </p>
",1,0,1140,2017-06-17 16:17:55,https://stackoverflow.com/questions/44606735/how-to-improve-topic-model-of-gensim
How to extract a word vector from the Google pre-trained model for word2vec?,"<p>The file <code>GoogleNews-vectors-negative300.bin</code> contains 300 million word-vectors. I think (not sure) this file is loaded when the following line is written:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
</code></pre>

<p>I want to download the vectors for words that I give externally in a list called <code>words</code>. This is my code to do this:</p>

<pre><code>import math
import sys
import gensim
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')

from gensim.models.keyedvectors import KeyedVectors

words = ['access', 'aeroway', 'airport', 'amenity', 'area', 'atm', 'barrier', 'bay', 'bench', 'boundary', 'bridge', 'building', 'bus', 'cafe', 'car', 'coast', 'continue', 'created', 'defibrillator', 'drinking', 'ele', 'embankment', 'entrance', 'ferry', 'foot', 'fountain', 'fuel', 'gate', 'golf', 'gps', 'grave', 'highway', 'horse', 'hospital', 'house', 'landuse', 'layer', 'leisure', 'man', 'manmade', 'market', 'marketplace', 'maxheight', 'name', 'natural', 'noexit', 'oneway', 'park', 'parking', 'pgs', 'place', 'worship', 'playground', 'police', 'police station', '', 'post', 'post box or mail', 'power', 'powerstation', 'private', 'public', 'railway', 'ref', 'residential', 'restaurant', 'road', 'route', 'school', 'shelter', 'shop', 'source', 'sport', 'toilet', 'toilets', 'tourism', 'unknown', 'vehicle', 'vending', 'vending machine', 'village', 'wall', 'waste', 'water', 'waterway', 'worship'];

model = gensim.models.KeyedVectors.load_word2vec_format(words, binary=True)

M = len(words)
count = 0
for i in range(1,M):
    wi = id2word[words[i]]
    if wi in word2vec.vocab:
        vector[:,count] = model[:,i]
        count = count+1

f = open('word_vectors.csv', 'w')
print(vector, file=f)
f.close()
</code></pre>

<p>But when I run the code, it just freezes up my system. Is it because it is loading the whole of the binary file before searching for the words in <code>words</code>? If yes, how do I get around this issue? I think of this as I get the following warning, which is why I use the <code>warning</code> package to suppress it:</p>

<pre><code>c:\Python35\lib\site-packages\gensim\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
</code></pre>

<p>And the error it gives is:</p>

<pre><code>Traceback (most recent call last):
  File ""word2vec.py"", line 18, in &lt;module&gt;
    model = gensim.models.KeyedVectors.load_word2vec_format(topic, binary=True) 
  File ""c:\Python35\lib\site-packages\gensim\models\keyedvectors.py"", line 196, in load_word2vec_format
    with utils.smart_open(fname) as fin:
  File ""c:\Python35\lib\site-packages\smart_open\smart_open_lib.py"", line 208, in smart_open
    raise TypeError('don\'t know how to handle uri %s' % repr(uri))
TypeError: don't know how to handle uri [['access'], ['aeroway'], ['airport'], ['amenity'], ['area'], ['atm'], ['barrier'], ['bay'], ['bench'], ['boundary'], ['bridge'], ['building'], ['bus'], ['cafe'], ['car'], ['coast'], ['continue'], ['created'], ['defibrillator'], ['drinking'], ['ele'], ['embankment'], ['entrance'], ['ferry'], ['foot'], ['fountain'], ['fuel'], ['gate'], ['golf'], ['gps'], ['grave'], ['highway'], ['horse'], ['hospital'], ['house'], ['landuse'], ['layer'], ['leisure'], ['man'], ['manmade'], ['market'], ['marketplace'], ['maxheight'], ['name'], ['natural'], ['noexit'], ['oneway'], ['park'], ['parking'], ['pgs'], ['place'], ['worship'], ['playground'], ['police'], ['police station'], [''], ['post'], ['post box or mail'], ['power'], ['powerstation'], ['private'], ['public'], ['railway'], ['ref'], ['residential'], ['restaurant'], ['road'], ['route'], ['school'], ['shelter'], ['shop'], ['source'], ['sport'], ['toilet'], ['toilets'], ['tourism'], ['unknown'], ['vehicle'], ['vending'], ['vending machine'], ['village'], ['wall'], ['waste'], ['water'], ['waterway'], ['worship']]
</code></pre>

<p>This I guess means that the program is not able to search for the words in the binary file. So, how to solve it? </p>
","python, file-handling, gensim, word2vec","<p>Use the following code to extract the word vector from the Google trained model for word2vec:</p>

<pre><code>import math
import sys
import gensim
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')

# this line doesn't load the trained model 
from gensim.models.keyedvectors import KeyedVectors

words = ['access', 'aeroway', 'airport']

# this is how you load the model
model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)

# to extract word vector
print(model[words[0]])  #access
</code></pre>

<p>Result vector:</p>

<pre><code>[ -8.74023438e-02  -1.86523438e-01 .. ]
</code></pre>

<p>Your system is freezing because of the large size of model. Try using system with more memory or you can limit the size of model you are loading. </p>

<p><strong>Limit model size while loading</strong></p>

<pre><code>model = KeyedVectors.load_word2vec_format(path_to_model, binary=True, limit=20000)
</code></pre>
",7,2,4014,2017-06-22 07:44:56,https://stackoverflow.com/questions/44693241/how-to-extract-a-word-vector-from-the-google-pre-trained-model-for-word2vec
How to remove stop words from documents in gensim?,"<p>I'm building a NLP chat application using Doc2Vec technique in Python using its <code>gensim</code> package. I have already done tokenizing and stemming. I want to remove the stop words (to test if it works better) from both the training set as well as the question which user throws. </p>

<p>Here is my code.</p>

<pre><code>import gensim
import nltk
from gensim import models
from gensim import utils
from gensim import corpora
from nltk.stem import PorterStemmer
ps = PorterStemmer()

sentence0 = models.doc2vec.LabeledSentence(words=[u'sampl',u'what',u'is'],tags=[""SENT_0""])
sentence1 = models.doc2vec.LabeledSentence(words=[u'sampl',u'tell',u'me',u'about'],tags=[""SENT_1""])
sentence2 = models.doc2vec.LabeledSentence(words=[u'elig',u'what',u'is',u'my'],tags=[""SENT_2""])
sentence3 = models.doc2vec.LabeledSentence(words=[u'limit', u'what',u'is',u'my'],tags=[""SENT_3""])
sentence4 = models.doc2vec.LabeledSentence(words=[u'claim',u'how',u'much',u'can',u'I'],tags=[""SENT_4""])
sentence5 = models.doc2vec.LabeledSentence(words=[u'retir',u'i',u'am',u'how',u'much',u'can',u'elig',u'claim'],tags=[""SENT_5""])
sentence6 = models.doc2vec.LabeledSentence(words=[u'resign',u'i',u'have',u'how',u'much',u'can',u'i',u'claim',u'elig'],tags=[""SENT_6""])
sentence7 = models.doc2vec.LabeledSentence(words=[u'promot',u'what',u'is',u'my',u'elig',u'post',u'my'],tags=[""SENT_7""])
sentence8 = models.doc2vec.LabeledSentence(words=[u'claim',u'can,',u'i',u'for'],tags=[""SENT_8""])
sentence9 = models.doc2vec.LabeledSentence(words=[u'product',u'coverag',u'cover',u'what',u'all',u'are'],tags=[""SENT_9""])
sentence10 = models.doc2vec.LabeledSentence(words=[u'hotel',u'coverag',u'cover',u'what',u'all',u'are'],tags=[""SENT_10""])
sentence11 = models.doc2vec.LabeledSentence(words=[u'onlin',u'product',u'can',u'i',u'for',u'bought',u'through',u'claim',u'sampl'],tags=[""SENT_11""])
sentence12 = models.doc2vec.LabeledSentence(words=[u'reimburs',u'guidelin',u'where',u'do',u'i',u'apply',u'form',u'sampl'],tags=[""SENT_12""])
sentence13 = models.doc2vec.LabeledSentence(words=[u'reimburs',u'procedur',u'rule',u'and',u'regul',u'what',u'is',u'the',u'for'],tags=[""SENT_13""])
sentence14 = models.doc2vec.LabeledSentence(words=[u'can',u'i',u'submit',u'expenditur',u'on',u'behalf',u'of',u'my',u'friend',u'and',u'famili',u'claim',u'and',u'reimburs'],tags=[""SENT_14""])
sentence15 = models.doc2vec.LabeledSentence(words=[u'invoic',u'bills',u'procedur',u'can',u'i',u'submit',u'from',u'shopper stop',u'claim'],tags=[""SENT_15""])
sentence16 = models.doc2vec.LabeledSentence(words=[u'invoic',u'bills',u'can',u'i',u'submit',u'from',u'pantaloon',u'claim'],tags=[""SENT_16""])
sentence17 = models.doc2vec.LabeledSentence(words=[u'invoic',u'procedur',u'can',u'i',u'submit',u'invoic',u'from',u'spencer',u'claim'],tags=[""SENT_17""])

# User asks a question.

document = input(""Ask a question:"")
tokenized_document = list(gensim.utils.tokenize(document, lowercase = True, deacc = True))
#print(type(tokenized_document))
stemmed_document = []
for w in tokenized_document:
    stemmed_document.append(ps.stem(w))
sentence19 = models.doc2vec.LabeledSentence(words= stemmed_document, tags=[""SENT_19""])

# Building vocab.
sentences = [sentence0,sentence1,sentence2,sentence3, sentence4, sentence5,sentence6, sentence7, sentence8, sentence9, sentence10, sentence11, sentence12, sentence13, sentence14, sentence15, sentence16, sentence17, sentence19]

#I tried to remove the stop words but it didn't work out as LabeledSentence object has no attribute lower.
stoplist = set('for a of the and to in'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
          for document in sentences]
..
</code></pre>

<p>Is there a way I can remove stop words from <code>sentences</code> directly and get a new set of vocab without stop words ?</p>
","python, nlp, gensim, word2vec, doc2vec","<p>Your <code>sentences</code> object is a already a list of <code>LabeledSentence</code> objects. You construct these above; they include a list-of-strings in <code>words</code> and a list-of-strings in <code>tags</code>. </p>

<p>So each item in that list (<code>document</code> in your list-comprehension) can't have a string method like <code>.lower()</code> applied to it. (Nor would it need to be <code>.split()</code>, as its <code>words</code> are already separate tokens.)</p>

<p>The cleanest approach would be to remove stop-words from the lists-of-words <em>before</em> they're used to construct <code>LabeledSentence</code> objects. For example, you could make a function <code>without_stopwords()</code>, defined at the top. Then your lines creating <code>LabeledSentence</code> objects could instead be like:</p>

<pre><code>sentence0 = LabeledSentence(words=remove_stopwords([u'sampl', u'what', u'is']), 
                            tags=[""SENT_0""])
</code></pre>

<p>Alternatively, you could <em>mutate</em> the existing <code>LabeledSentence</code> objects so that each of their <code>words</code> attributes now lack stop-words. This would replace your last line with something more like:</p>

<pre><code>for doc in sentences:
    doc.words = [word for word in doc.words if word not in stoplist]
texts = sentences
</code></pre>

<p>Separately, things you didn't ask but should know:</p>

<ul>
<li><p><code>TaggedDocument</code> is now the preferred example-class name for Doc2Vec text objects – but in fact any object that has the two required properties <code>words</code> and <code>tags</code> will work fine.</p></li>
<li><p>Doc2Vec doesn't show many of the desired properties on tiny, toy-sized datasets – don't be surprised if a model built on dozens of sentences does not do anything useful, or misleads about what preprocessing/meta-parameter options are best. (Tens of thousands of texts, and texts at least tens-of-words long, are much better for meaningful results.)</p></li>
<li><p>Much Word2Vec/Doc2Vec work doesn't bother with stemming or stop-word removal, but it may sometimes be helpful.</p></li>
</ul>
",4,1,8646,2017-06-22 12:02:19,https://stackoverflow.com/questions/44698910/how-to-remove-stop-words-from-documents-in-gensim
Word2vec saved model is not UTF-8 encoded but the sentence input to the Word2vec model is UTF-8 encoded,"<p>I trained a word2vec model using gensim package and saved it with the following name. </p>

<pre><code>model_name = ""300features_1minwords_10context""
model.save(model_name)
</code></pre>

<p>I got these log message info. while the model was getting trained and saved.</p>

<pre><code>INFO : not storing attribute syn0norm
INFO : not storing attribute cum_table
</code></pre>

<p>Then, I tried to load the model using this, </p>

<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load(""300features_1minwords_10context"")
</code></pre>

<p>I got the following error. </p>

<pre><code>2017-06-22 21:27:14,975 : INFO : loading Word2Vec object from 300features_1minwords_10context
2017-06-22 21:27:15,496 : INFO : loading wv recursively from 300features_1minwords_10context.wv.* with mmap=None
2017-06-22 21:27:15,497 : INFO : setting ignored attribute syn0norm to None
2017-06-22 21:27:15,498 : INFO : setting ignored attribute cum_table to None
2017-06-22 21:27:15,499 : INFO : loaded 300features_1minwords_10context
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-25-9d90db0f07c0&gt; in &lt;module&gt;()
      1 from gensim.models import Word2Vec
      2 model = Word2Vec.load(""300features_1minwords_10context"")
----&gt; 3 model.syn0.shape

AttributeError: 'Word2Vec' object has no attribute 'syn0'
</code></pre>

<p>Also, in the file ""300features_1minwords_10context"", it shows that </p>

<pre><code>""300features_1minwords_10context"" is not UTF-8 encoded
Saving disabled.
Open console for more details 
</code></pre>

<p>To fix the above attribute error, I have also tried the following from the google forum, </p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format(""300features_1minwords_10context"")
model.syn0.shape
</code></pre>

<p>It resulted in another error which is </p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>The model is trained with UTF-8 encoded sentences. I am not sure why is it throwing this error ?</p>

<p>More info : </p>

<pre><code>df = pd.read_csv('UNSPSCdataset.csv',encoding='mac_roman',low_memory=False)
features = ['MaterialDescription']
temp_features = df[features]
temp_features.to_csv('materialDescription', encoding='UTF-8')
X = pd.read_csv('materialDescription',encoding='UTF-8')
</code></pre>

<p>Here, I had to use 'mac_roman' encoding in order to access it using pandas dataframe. Since the text in the dataframe has to be in UTF-8 while training the model, I have saved that particular feature in a separate csv file by encoding it with UTF-8 and later, I have the accessed that particular column.</p>

<p>Any help is appreciable </p>
","python-3.x, utf-8, nlp, gensim, word2vec","<p>Are you using the latest gensim? If not, be sure to try it – there have sometimes been <code>save()</code>/<code>load()</code> bugs in older versions. </p>

<p>The INFO ""not storing"" log lines are normal – they're not indicative of any problem (and thus could be deleted from your question.) </p>

<p>Are you getting the ""has no attribute"" error directly upon the <code>load()</code>? (A full error stack here would be useful, and clarify this.)</p>

<hr>

<p><strong>UPDATE:</strong> From the now-shown error-stack, the error is <em>not</em> occurring in the <code>load()</code> line, but on the following line, when you attempt to access <code>model.syn0.shape</code>. Recent versions of gensim no longer have a <code>syn0</code> as a property of Word2Vec class objects – that info is moved to a constituent <code>KeyedVectors</code> object, in the <code>wv</code> property. So <code>model.wv.syn0.shape</code> is likely to access what you're seeking, without an error. </p>

<hr>

<p>When your model is largish, <code>save()</code> can generate multiple files on the side, with extra extensions, for the model's large array properties (like <code>syn0</code>). These files must be kept alongside the main filename for the model to be re-<code>load()</code>ed. Is it possible you've moved the <code>300features_1minwords_10context</code> file, but not any such accompanying files, to a new location where the <code>load()</code> is then incomplete? </p>

<p>You can't <code>load_word2vec_format()</code> a file that was native-gensim <code>save()</code>d – their different formats entirely, so the encoding error is just an artifact of trying to read a binary Python pickle file (from <code>save()</code>) as another format entirely. </p>
",2,1,2064,2017-06-22 23:06:43,https://stackoverflow.com/questions/44710644/word2vec-saved-model-is-not-utf-8-encoded-but-the-sentence-input-to-the-word2vec
"Docker unable to install numpy, scipy, or gensim","<p>I am trying to build a Docker application that uses Python's gensim library, version 2.1.0, which is being installed via pip from a requirements.txt file.</p>

<p>However, Docker seems to have trouble installing numpy, scipy, and gensim. I googled the error messages and found other users who experienced the same problem, but in other environments. Many of their solutions do not seem to work in Docker.</p>

<p>The following is the error message:</p>

<pre><code>&lt;pre&gt; Step 4 : RUN pip install -r requirements.txt
 ---&gt; Running in a86d07e229d7
Collecting Flask==0.12 (from -r requirements.txt (line 1))
  Downloading Flask-0.12-py2.py3-none-any.whl (82kB)
Collecting requests==2.17.3 (from -r requirements.txt (line 2))
  Downloading requests-2.17.3-py2.py3-none-any.whl (87kB)
Collecting numpy==1.12.1 (from -r requirements.txt (line 3))
  Downloading numpy-1.12.1.zip (4.8MB)
Collecting nltk==3.2.2 (from -r requirements.txt (line 4))
  Downloading nltk-3.2.2.tar.gz (1.2MB)
Collecting scipy==0.19.0 (from -r requirements.txt (line 5))
  Downloading scipy-0.19.0.zip (15.3MB)
    Complete output from command python setup.py egg_info:
    /bin/sh: svnversion: not found
    /bin/sh: svnversion: not found
    non-existing path in 'numpy/distutils': 'site.cfg'
    Could not locate executable gfortran
    Could not locate executable f95
    Could not locate executable ifort
    Could not locate executable ifc
    Could not locate executable lf95
    Could not locate executable pgfortran
    Could not locate executable f90
    Could not locate executable f77
    Could not locate executable fort
    Could not locate executable efort
    Could not locate executable efc
    Could not locate executable g77
    Could not locate executable g95
    Could not locate executable pathf95
    don't know how to compile Fortran code on platform 'posix'
    Running from numpy source directory.
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py:367: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates
      run_build = parse_setuppy_commands()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Atlas (http://math-atlas.sourceforge.net/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [atlas]) or by setting
        the ATLAS environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Blas (http://www.netlib.org/blas/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [blas]) or by setting
        the BLAS environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Blas (http://www.netlib.org/blas/) sources not found.
        Directories to search for the sources can be specified in the
        numpy/distutils/site.cfg file (section [blas_src]) or by setting
        the BLAS_SRC environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Lapack (http://www.netlib.org/lapack/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [lapack]) or by setting
        the LAPACK environment variable.
      self.calc_info()
    /tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/system_info.py:572: UserWarning:
        Lapack (http://www.netlib.org/lapack/) sources not found.
        Directories to search for the sources can be specified in the
        numpy/distutils/site.cfg file (section [lapack_src]) or by setting
        the LAPACK_SRC environment variable.
      self.calc_info()
    /usr/local/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'define_macros'
      warnings.warn(msg)
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 158, in save_modules
        yield saved
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 199, in setup_context
        yield
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 254, in run_setup
        _execfile(setup_script, ns)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 48, in _execfile
        exec(code, globals, locals)
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 392, in &lt;module&gt;
        # higher up in this file.
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 384, in setup_package
        if ""--force"" in sys.argv:
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/core.py"", line 169, in setup
      File ""/usr/local/lib/python3.5/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/bdist_egg.py"", line 152, in run
        self.run_command(""egg_info"")
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/egg_info.py"", line 26, in run
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 148, in run
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 159, in build_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 294, in build_library_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 377, in generate_sources
      File ""numpy/core/setup.py"", line 674, in get_mathlib_info
    RuntimeError: Broken toolchain: cannot link a simple C program

    During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/pip-build-j8py_tat/scipy/setup.py"", line 416, in &lt;module&gt;
        setup_package()
      File ""/tmp/pip-build-j8py_tat/scipy/setup.py"", line 412, in setup_package
        setup(**metadata)
      File ""/usr/local/lib/python3.5/distutils/core.py"", line 108, in setup
        _setup_distribution = dist = klass(attrs)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/dist.py"", line 320, in __init__
        self.fetch_build_eggs(attrs['setup_requires'])
      File ""/usr/local/lib/python3.5/site-packages/setuptools/dist.py"", line 377, in fetch_build_eggs
        replace_conflicting=True,
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 852, in resolve
        dist = best[req.key] = env.best_match(req, ws, installer)
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1124, in best_match
        return self.obtain(req, installer)
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1136, in obtain
        return installer(requirement)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/dist.py"", line 445, in fetch_build_egg
        return cmd.easy_install(req)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 673, in easy_install
        return self.install_item(spec, dist.location, tmpdir, deps)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 699, in install_item
        dists = self.install_eggs(spec, download, tmpdir)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 880, in install_eggs
        return self.build_and_install(setup_script, setup_base)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1119, in build_and_install
        self.run_setup(setup_script, setup_base, args)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1105, in run_setup
        run_setup(setup_script, args)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 257, in run_setup
        raise
      File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
        self.gen.throw(type, value, traceback)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 199, in setup_context
        yield
      File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
        self.gen.throw(type, value, traceback)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 170, in save_modules
        saved_exc.resume()
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 145, in resume
        six.reraise(type, exc, self._tb)
      File ""/usr/local/lib/python3.5/site-packages/pkg_resources/_vendor/six.py"", line 685, in reraise
        raise value.with_traceback(tb)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 158, in save_modules
        yield saved
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 199, in setup_context
        yield
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 254, in run_setup
        _execfile(setup_script, ns)
      File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 48, in _execfile
        exec(code, globals, locals)
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 392, in &lt;module&gt;
        # higher up in this file.
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/setup.py"", line 384, in setup_package
        if ""--force"" in sys.argv:
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/core.py"", line 169, in setup
      File ""/usr/local/lib/python3.5/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/local/lib/python3.5/site-packages/setuptools/command/bdist_egg.py"", line 152, in run
        self.run_command(""egg_info"")
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/egg_info.py"", line 26, in run
      File ""/usr/local/lib/python3.5/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 148, in run
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 159, in build_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 294, in build_library_sources
      File ""/tmp/easy_install-ocgjhe9m/numpy-1.13.0/numpy/distutils/command/build_src.py"", line 377, in generate_sources
      File ""numpy/core/setup.py"", line 674, in get_mathlib_info
    RuntimeError: Broken toolchain: cannot link a simple C program
    /bin/sh: gcc: not found
    /bin/sh: gcc: not found

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-j8py_tat/scipy/
Removing intermediate container a86d07e229d7
The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1 &lt;/pre&gt;
</code></pre>

<p>I'm using the <code>python:3.5-alpine</code> image. The versions of the packages are <code>numpy==1.12.1</code>, <code>scipy==0.19.0</code>, and <code>gensim==2.1.0</code>.</p>
","python, numpy, docker, scipy, gensim","<p>To install numpy, scipy, or gensim, the following lines need to be added to the Dockerfile before <code>RUN pip install -r requirements.txt</code>:</p>

<pre><code>RUN apt-get -y install libc-dev
RUN apt-get -y install build-essential
RUN pip install -U pip
</code></pre>

<p>The first two lines install the C build tools required for the libraries. The tihrd line upgrades pip to the newest available version.</p>
",12,10,13156,2017-06-24 02:56:11,https://stackoverflow.com/questions/44732303/docker-unable-to-install-numpy-scipy-or-gensim
Necessary to apply TF-IDF to new documents in gensim LDA model?,"<p>I'm following the 'English Wikipedia' gensim tutorial at <a href=""https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation"" rel=""noreferrer"">https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation</a></p>

<p>where it explains that tf-idf is used during training (at least for LSA, not so clear with LDA).</p>

<p>I expected to apply a tf-idf transformer to new documents, but instead, at the end of the tut, it suggests to simply input a bag-of-words.</p>

<pre><code>doc_lda = lda[doc_bow]
</code></pre>

<p>Does LDA require bag-of-words vectors only?</p>
",gensim,"<p>TL;DR: Yes, LDA only needs a bag-of-word vector.</p>
<p>Indeed, in the Wikipedia example of the gensim tutorial, Radim Rehurek uses the TF-IDF corpus generated in the preprocessing step.</p>
<pre><code>mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')
</code></pre>
<p>I believe the reason for that is only that this matrix is sparse and easy to handle (and already exists anyways due to the preprocessing step).</p>
<p>LDA does not necessarily need to be trained on a TF-IDF corpus. The model works just fine if you use the corpus shown in the gensim tutorial <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html"" rel=""noreferrer"">Corpora and Vector Spaces</a>:</p>
<pre><code>from gensim import corpora, models
texts = [['human', 'interface', 'computer'],
         ['survey', 'user', 'computer', 'system', 'response', 'time'],
         ['eps', 'user', 'interface', 'system'],
         ['system', 'human', 'system', 'eps'],
         ['user', 'response', 'time'],
         ['trees'],
         ['graph', 'trees'],
         ['graph', 'minors', 'trees'],
         ['graph', 'minors', 'survey']]

dictionary = corpora.Dictionary(texts)

corpus = [dictionary.doc2bow(text) for text in texts]

lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, update_every=1, chunksize =10000, passes=1)
</code></pre>
<p>Notice that <code>texts</code> is a bag-of-word vector. As you pointed out correctly, that is the center piece of the LDA model. TF-IDF does not play any role in it at all.</p>
<p>In fact, Blei (who developed LDA), points out in the introduction of the paper of 2003 (entitled &quot;Latent Dirichlet Allocation&quot;) that LDA addresses the shortcomings of the TF-IDF model and leaves this approach behind. LSA is compeltely algebraic and generally (but not necessarily) uses a TF-IDF matrix, while LDA is a probabilistic model that tries to estimate probability distributions for topics in documents and words in topics. The weighting of TF-IDF is not necessary for this.</p>
",23,12,17493,2017-06-27 13:04:51,https://stackoverflow.com/questions/44781047/necessary-to-apply-tf-idf-to-new-documents-in-gensim-lda-model
Word2vec gensim - Calculating similarity between word isn&#39;t working when using phrases,"<p>Using <code>gensim</code> <code>word2vec</code> model in order to calculate similarities between two words. Training the model with a 250mb Wikipedia text gave a good result - about 0.7-0.8 similarity score for a related pair of words.</p>

<p>The problem is when I am using the <code>Phraser</code> model to add up phrases the similarity score drops to nearly zero for the same exact words.</p>

<p><strong>Results with the phrase model:</strong></p>

<pre><code>speed - velocity - 0.0203503432178
high - low - -0.0435703782446
tall - high - -0.0076987978333
nice - good - 0.0368784716958
computer - computational - 0.00487748035808
</code></pre>

<p>That probably means I am not using the Phraser model correctly.</p>

<p><strong>My Code:</strong></p>

<pre><code>    data_set_location = **
    sentences = SentenceIterator(data_set_location)

    # Train phrase locator model
    self.phraser = Phraser(Phrases(sentences))

    # Renewing the iterator because its empty
    sentences = SentenceIterator(data_set_location)

    # Train word to vector model or load it from disk
    self.model = Word2Vec(self.phraser[sentences], size=256, min_count=10, workers=10)



class SentenceIterator(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname), 'r', encoding='utf-8', errors='ignore'):
                yield line.lower().split()
</code></pre>

<p>Trying the pharser model alone looks like it worked fine:</p>

<p><code>&gt;&gt;&gt;vectorizer.phraser['new', 'york', 'city', 'the', 'san', 'francisco']
['new_york', 'city', 'the', 'san_francisco']</code></p>

<p>What can cause such behavior?</p>

<p><strong>Trying to figure out the solution:</strong></p>

<p>according to gojomo answer, I've tried to create a <code>PhraserIterator</code>:</p>

<pre><code>import os

class PhraseIterator(object):
def __init__(self, dirname, phraser):
    self.dirname = dirname
    self.phraser = phraser

def __iter__(self):
    for fname in os.listdir(self.dirname):
        for line in open(os.path.join(self.dirname, fname), 'r', encoding='utf-8', errors='ignore'):
            yield self.phraser[line.lower()]
</code></pre>

<p>using this iterator I've tried to train my <code>Word2vec</code> model.</p>

<pre><code>phrase_iterator = PhraseIterator(text_dir, self.phraser)
self.model = Word2Vec(phrase_iterator, size=256, min_count=10, workers=10
</code></pre>

<p>Word2vec training log:</p>

<pre><code>    Using TensorFlow backend.
2017-06-30 19:19:05,388 : INFO : collecting all words and their counts
2017-06-30 19:19:05,456 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types
2017-06-30 19:20:30,787 : INFO : collected 6227763 word types from a corpus of 28508701 words (unigram + bigrams) and 84 sentences
2017-06-30 19:20:30,793 : INFO : using 6227763 counts as vocab in Phrases&lt;0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000&gt;
2017-06-30 19:20:30,793 : INFO : source_vocab length 6227763
2017-06-30 19:21:46,573 : INFO : Phraser added 50000 phrasegrams
2017-06-30 19:22:22,015 : INFO : Phraser built with 70065 70065 phrasegrams
2017-06-30 19:22:23,089 : INFO : saving Phraser object under **/Models/word2vec/phrases_model, separately None
2017-06-30 19:22:23,441 : INFO : saved **/Models/word2vec/phrases_model
2017-06-30 19:22:23,442 : INFO : collecting all words and their counts
2017-06-30 19:22:29,347 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2017-06-30 19:33:06,667 : INFO : collected 143 word types from a corpus of 163438509 raw words and 84 sentences
2017-06-30 19:33:06,677 : INFO : Loading a fresh vocabulary
2017-06-30 19:33:06,678 : INFO : min_count=10 retains 95 unique words (66% of original 143, drops 48)
2017-06-30 19:33:06,679 : INFO : min_count=10 leaves 163438412 word corpus (99% of original 163438509, drops 97)
2017-06-30 19:33:06,683 : INFO : deleting the raw counts dictionary of 143 items
2017-06-30 19:33:06,683 : INFO : sample=0.001 downsamples 27 most-common words
2017-06-30 19:33:06,683 : INFO : downsampling leaves estimated 30341972 word corpus (18.6% of prior 163438412)
2017-06-30 19:33:06,684 : INFO : estimated required memory for 95 words and 256 dimensions: 242060 bytes
2017-06-30 19:33:06,685 : INFO : resetting layer weights
2017-06-30 19:33:06,724 : INFO : training model with 10 workers on 95 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2017-06-30 19:33:14,974 : INFO : PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0
2017-06-30 19:33:23,229 : INFO : PROGRESS: at 0.24% examples, 607 words/s, in_qsize 0, out_qsize 0
2017-06-30 19:33:31,445 : INFO : PROGRESS: at 0.48% examples, 810 words/s, 
...
2017-06-30 20:19:00,864 : INFO : PROGRESS: at 98.57% examples, 1436 words/s, in_qsize 0, out_qsize 1
2017-06-30 20:19:06,193 : INFO : PROGRESS: at 99.05% examples, 1437 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:11,886 : INFO : PROGRESS: at 99.29% examples, 1437 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:17,648 : INFO : PROGRESS: at 99.52% examples, 1438 words/s, in_qsize 0, out_qsize 0
2017-06-30 20:19:22,870 : INFO : worker thread finished; awaiting finish of 9 more threads
2017-06-30 20:19:22,908 : INFO : worker thread finished; awaiting finish of 8 more threads
2017-06-30 20:19:22,947 : INFO : worker thread finished; awaiting finish of 7 more threads
2017-06-30 20:19:22,947 : INFO : PROGRESS: at 99.76% examples, 1439 words/s, in_qsize 0, out_qsize 8
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 6 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 5 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 4 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 3 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 2 more threads
2017-06-30 20:19:22,948 : INFO : worker thread finished; awaiting finish of 1 more threads
2017-06-30 20:19:22,949 : INFO : worker thread finished; awaiting finish of 0 more threads
2017-06-30 20:19:22,949 : INFO : training on 817192545 raw words (4004752 effective words) took 2776.2s, 1443 effective words/s
2017-06-30 20:19:22,950 : INFO : saving Word2Vec object under **/Models/word2vec/word2vec_model, separately None
2017-06-30 20:19:22,951 : INFO : not storing attribute syn0norm
2017-06-30 20:19:22,951 : INFO : not storing attribute cum_table
2017-06-30 20:19:22,958 : INFO : saved **/Models/word2vec/word2vec_model
</code></pre>

<p>After this training - any of two similarity calculation produce zero:</p>

<pre><code>speed - velocity - 0
high - low - 0
</code></pre>

<p>So it seems that the iterator is not working well so I've checked it using gojomo trick:</p>

<pre><code>print(sum(1 for _ in s))
1

print(sum(1 for _ in s))
1
</code></pre>

<p>And its working. </p>

<p>What may be the problem?</p>
","python, deep-learning, gensim, word2vec, phrases","<p>First, if your iterable class is working properly – and it looks OK to me – you won't need to ""renew the iterator because it's empty"". Rather, it will be capable of being iterated over multiple times. You can test if it's working properly as an iterable-object, rather than a single iteration, with code like:</p>

<pre><code>sentences = SentencesIterator(mypath)
print(sum(1 for _ in sentences))
print(sum(1 for _ in sentences))
</code></pre>

<p>If the same length prints twice, congratulations, you have a true iterable object. (You might want to update the class name to reflect that.) If the second length is <code>0</code>, you've only got an iterator: it can be consumed once, and then is empty on subsequent attempts. (If so, adjust the class code so that each call to <code>__iter__()</code> starts fresh. But as noted above, I think your code is already correct.)</p>

<p>That digression was important, because what's the true cause of your problem is that <code>self.phraser[sentences]</code> is just returning a one-time iterator object, <em>not</em> a repeatable iterable object. Thus, Word2Vec's 1st vocabulary-discovery step consumes the whole corpus in its one pass, then all training passes just see nothing – and no training occurs. (If you have INFO-level logging on, this should be evident in the output showing instant training over no examples.)</p>

<p>Try making a <code>PhraserIterable</code> class, which takes a <code>phraser</code> and a <code>sentences</code>, and upon each call to <code>__iter__()</code> starts a new, fresh pass over the setences. Supply a (confirmed-restartable) instance of that as the corpus for Word2Vec. You should see training take longer as it does its default 5 passes – and then see real results on later token-comparisons. </p>

<p>Separately: the on-the-fly upgrading of original <code>sentences</code> unigrams to phraser-calculated bigrams can be computationally expensive. The approach suggested above means that happens 6 times – the vocabulary-scan then the 5 training passes. Where running-time is a concern, it can be beneficial to perform the phraser-combination once, saving the results to an in-memory object (if your corpus easily fits in RAM) or a new simply-space-delimited interim-results file, then use that file as input to the Word2Vec model. </p>
",1,0,1067,2017-06-29 17:25:57,https://stackoverflow.com/questions/44831480/word2vec-gensim-calculating-similarity-between-word-isnt-working-when-using-p
pip install pyemd error?,"<p>I'm trying to install <code>pyemd</code> package in Python through <code>pip</code> and getting following error:</p>

<pre><code>C:\Users\dipanwita.neogy&gt;pip install pyemd
Collecting pyemd
  Using cached pyemd-0.4.3.tar.gz
Requirement already satisfied: numpy&lt;2.0.0,&gt;=1.9.0 in c:\users\dipanwita.neogy\a
naconda3\lib\site-packages (from pyemd)
Building wheels for collected packages: pyemd
  Running setup.py bdist_wheel for pyemd ... error
  Complete output from command C:\Users\dipanwita.neogy\Anaconda3\python.exe -u
-c ""import setuptools, tokenize;__file__='C:\\Users\\DIPANW~1.NEO\\AppData\\Loca
l\\Temp\\pip-build-nk13uh5b\\pyemd\\setup.py';f=getattr(tokenize, 'open', open)(
__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __fil
e__, 'exec'))"" bdist_wheel -d C:\Users\DIPANW~1.NEO\AppData\Local\Temp\tmpngn2np
rmpip-wheel- --python-tag cp36:
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win32-3.6
  creating build\lib.win32-3.6\pyemd
  copying pyemd\__about__.py -&gt; build\lib.win32-3.6\pyemd
  copying pyemd\__init__.py -&gt; build\lib.win32-3.6\pyemd
  running build_ext
  building 'pyemd.emd' extension
  error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual C+
+ Build Tools"": http://landinghub.visualstudio.com/visual-cpp-build-tools

  ----------------------------------------
  Failed building wheel for pyemd
  Running setup.py clean for pyemd
Failed to build pyemd
Installing collected packages: pyemd
  Running setup.py install for pyemd ... error
    Complete output from command C:\Users\dipanwita.neogy\Anaconda3\python.exe -
u -c ""import setuptools, tokenize;__file__='C:\\Users\\DIPANW~1.NEO\\AppData\\Lo
cal\\Temp\\pip-build-nk13uh5b\\pyemd\\setup.py';f=getattr(tokenize, 'open', open
)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __f
ile__, 'exec'))"" install --record C:\Users\DIPANW~1.NEO\AppData\Local\Temp\pip-e
rihhtfj-record\install-record.txt --single-version-externally-managed --compile:

    running install
    running build
    running build_py
    creating build
    creating build\lib.win32-3.6
    creating build\lib.win32-3.6\pyemd
    copying pyemd\__about__.py -&gt; build\lib.win32-3.6\pyemd
    copying pyemd\__init__.py -&gt; build\lib.win32-3.6\pyemd
    running build_ext
    building 'pyemd.emd' extension
    error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual
C++ Build Tools"": http://landinghub.visualstudio.com/visual-cpp-build-tools

    ----------------------------------------
Command ""C:\Users\dipanwita.neogy\Anaconda3\python.exe -u -c ""import setuptools,
 tokenize;__file__='C:\\Users\\DIPANW~1.NEO\\AppData\\Local\\Temp\\pip-build-nk1
3uh5b\\pyemd\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read(
).replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install
 --record C:\Users\DIPANW~1.NEO\AppData\Local\Temp\pip-erihhtfj-record\install-r
ecord.txt --single-version-externally-managed --compile"" failed with error code
1 in C:\Users\DIPANW~1.NEO\AppData\Local\Temp\pip-build-nk13uh5b\pyemd\
</code></pre>

<p>I cannot find anything regarding this error. Please suggest me what should I do? </p>
","python, nlp, pip, gensim","<p>The error you are receiving is: <code>error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual C++ Build Tools"": http://landinghub.visualstudio.com/visual-cpp-build-tools</code>. You need to read the error message carefully. </p>

<p>You just need to go to the link they have provided for you and follow the instructions: <a href=""http://landinghub.visualstudio.com/visual-cpp-build-tools"" rel=""nofollow noreferrer"">http://landinghub.visualstudio.com/visual-cpp-build-tools</a></p>
",2,1,6700,2017-07-06 08:58:52,https://stackoverflow.com/questions/44944249/pip-install-pyemd-error
Gensim: Loss of Words/Tokens while Training,"<p>I have a corpus built out of Wikimedia Dump files stored at <strong><em>sentences.txt</em></strong>
I have a sentence say 'नीरजः हाँ माता जी! स्कूल ख़त्म होते सीधा घर आऊँगा'</p>

<p>Now when I try to extract the word vectors there is always one or two words which have been missed out while training (despite being included in the list to be trained upon) and I get the KeyError.
Is there any way to improve the training so that it doesn't miss out words that frequently? </p>

<p>Here is a proof that it does happen. <code>tok.wordtokenize</code> is a word tokenizer. <code>sent.drawlist()</code> as well as <code>sents.drawlist()</code> returns a list of sentences from the corpus stored inside <strong><em>sentences.txt</em></strong>. </p>

<hr>

<pre><code>&gt;&gt;&gt; sentence = 'नीरजः हाँ माता जी! स्कूल ख़त्म होते सीधा घर आऊँगा'
&gt;&gt;&gt; sentence = tok.wordtokenize(sentence) #tok.wordtokenize() is simply a word tokenizer.
&gt;&gt;&gt; sentences = sent.drawlist()
&gt;&gt;&gt; sentences = [tok.wordtokenize(i) for i in sentences]
&gt;&gt;&gt; sentences2 = sents.drawlist()
&gt;&gt;&gt; sentences2 = [tok.wordtokenize(i) for i in sentences2]
&gt;&gt;&gt; sentences = sentences2 + sentences + sentence
&gt;&gt;&gt; ""नीरजः"" in sentences #proof that the word is present inside sentences
True
&gt;&gt;&gt; sentences[0:10] #list of tokenized sentences.
[['विश्व', 'भर', 'में', 'करोड़ों', 'टीवी', 'दर्शकों', 'की', 'उत्सुकता', 'भरी', 'निगाह', 'के', 'बीच', 'मिस', 'ऑस्ट्रेलिया', 'जेनिफर', 'हॉकिंस', 'को', 'मिस', 'यूनिवर्स-२००४', 'का', 'ताज', 'पहनाया', 'गया'], ['करीब', 'दो', 'घंटे', 'चले', 'कार्यक्रम', 'में', 'विभिन्न', 'देशों', 'की', '८०', 'सुंदरियों', 'के', 'बीच', '२०', 'वर्षीय', 'हॉकिंस', 'को', 'सर्वश्रेष्ठ', 'आंका', 'गया'], ['मिस', 'अमेरिका', 'शैंडी', 'फिनेजी', 'को', 'प्रथम', 'उप', 'विजेता', 'और', 'मिस', 'प्यूरेटो', 'रिको', 'अल्बा', 'रेइज', 'द्वितीय', 'उप', 'विजेता', 'चुनी', 'गई'], ['भारत', 'की', 'तनुश्री', 'दत्ता', 'अंतिम', '१०', 'प्रतिभागियों', 'में', 'ही', 'स्थान', 'बना', 'पाई'], ['हॉकिंस', 'ने', 'कहा', 'कि', 'जीत', 'के', 'बारे', 'में', 'उसने', 'सपने', 'में', 'भी', 'नहीं', 'सोचा', 'था'], ['सौंदर्य', 'की', 'यह', 'शीर्ष', 'प्रतियोगिता', 'क्विटो', 'के', 'कन्वेंशन', 'सेंटर', 'में', 'मंगलवार', 'देर', 'रात', 'शुरू', 'हुई'], ['करीब', '७५००', 'विशिष्ट', 'दर्शकों', 'की', 'मौजूदगी', 'में', 'विश्व', 'की', 'सर्वश्रेष्ठ', 'सुंदरी', 'के', 'चयन', 'की', 'कवायद', 'शुरू', 'हुई'], ['हर', 'चरण', 'के', 'बाद', 'लोगों', 'की', 'सांसे', 'थमने', 'लगतीं'], ['टीवी', 'पर', 'लुत्फ', 'उठा', 'रहे', 'दर्शक', 'अपने', 'देश', 'व', 'क्षेत्र', 'की', 'सुंदरी', 'की', 'प्रतियोगिता', 'में', 'स्थिति', 'के', 'बारे', 'में', 'व्यग्र', 'रहे'], ['फाइनल', 'में', 'पहुंचने', 'वाली', 'पांच', 'प्रतिभागियों', 'में', 'मिस', 'पेराग्वे', 'यानिना', 'गोंजालेज', 'और', 'मिस', 'त्रिनिदाद', 'व', 'टोबैगो', 'डेनियल', 'जोंस', 'भी', 'शामिल', 'थीं']]
&gt;&gt;&gt; model = gensim.models.Word2Vec(sentences, size =10,  min_count=1) 
&gt;&gt;&gt; pred = []
&gt;&gt;&gt; for word in sentence:
...         pred.append(model.wv[word].tolist())
... 
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 2, in &lt;module&gt;
  File ""/home/djokester/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 574, in __getitem__
    return self.word_vec(words)
  File ""/home/djokester/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py"", line 273, in word_vec
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word 'नीरजः' not in vocabulary""
</code></pre>

<hr>

<p>As you can see, I check for the word ""नीरजः"" inside the list of tokenized sentences. It is present in the list that I feed into the Word2Vec trainer and yet after training it is not in the vocabulary. </p>
","python, gensim, word2vec","<p>It should never 'miss' words that were included in the tokenized corpus, and had at least <code>min_count</code> occurrences. So if you get a <code>KeyError</code>, you can be confident that the associated word-token was never supplied during training. </p>

<p>In your example code to reproduce, take a close look at:</p>

<pre><code>sentence = ""Jack and Jill went up the Hill""
sentence = [word_tokenize(i) for i in sentence]
</code></pre>

<p><code>i in sentence</code> will be each <em>character</em> of the string. It's unlikely your unshown <code>word_tokenize()</code> function does anything useful with the individual characters <code>['J', 'a', 'c', 'k', ' ', ...]</code> - probably just leaving them as a list of letters. Then <code>+</code>-appending that to your other <code>sentences</code> makes <code>sentences</code> 30 items longer, rather than the single extra tokenized example you expect.</p>

<p>I suspect your real issue is different but related: something wrong with tokenization and composition. So check every step individually for the expected results and nested-types. (Using unique variables per step, like <code>sentences_tokenized</code> or <code>sentence_tokenized</code> instead of clobber-reusing variables like <code>sentences</code> and <code>sentence</code>, can help debug.)</p>

<p><strong>Update as you suggested as edit:</strong> The issue with your latest code is that the line where you <code>+</code>-append is still wrong; it's appending each word in sentence as if it were its own new sentence. Looking at the results of each step – in the variable contents and lengths – should help make this clear. Also I again recommend not reusing variables for multiple steps while debugging. The line <code>""नीरजः"" in sentences #proof that the word is present inside sentences</code> is actually proving sentences is wrong; that single word should not be an item in <code>sentences</code>, but in its single last list-of-tokens, <code>sentences[-1]</code>.</p>
",1,0,1023,2017-07-07 06:56:08,https://stackoverflow.com/questions/44964380/gensim-loss-of-words-tokens-while-training
&#39;Word2Vec&#39; object has no attribute &#39;compute_loss&#39;,"<p>I want to know the loss for my w2v model and I upgrade <code>gensim</code> to the latest version, but still can't use the argument <code>compute_loss</code>, am I miss something??  </p>

<p><a href=""https://i.sstatic.net/Io83T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Io83T.png"" alt=""enter image description here""></a></p>
",gensim,"<p>No released version of gensim (through 2.2.0 of June 2017) yet has that feature. It is a work-in-progress in the <code>develop</code> branch, which should appear in a future release. </p>
",1,0,441,2017-07-10 09:05:57,https://stackoverflow.com/questions/45007902/word2vec-object-has-no-attribute-compute-loss
"After loading a pretrained Word2Vec model, how do I get word2vec representations of new sentences?","<p>I loaded a word2vec model using Google News dataset. Now I want to get the Word2Vec representations of a list of sentences that I wish to cluster. After going through the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">documentation</a> I found this <code>gensim.models.word2vec.LineSentence</code>but I'm not sure this is what I am looking for. </p>

<p>There should be a way to get word2vec representations of a list of sentences from a pretrained model right? None of the links I searched had anything about it. Any leads would be appreciated. </p>
","cluster-analysis, gensim, word2vec","<p>Word2Vec only offers vector representations for words, not sentences. </p>

<p>One crude but somewhat effective (for some purposes) way to go from word-vectors to vectors for longer texts (like sentences) is to average all the word-vectors together. This isn't a function of the gensim <code>Word2Vec</code> class; you have to code this yourself. </p>

<p>For example, with the word-vectors already loaded as <code>word_model</code>, you'd roughly do:</p>

<pre><code>import numpy as np

sentence_tokens = ""I do not like green eggs and ham"".split()
sum_vector = np.zeros(word_model.vector_size)
for token in sentence_tokens:
    sum_vector += word_model[token]
sentence_vector = sum_vector / len(sentence_tokens)
</code></pre>

<p>Real code might add handling for when the tokens aren't all known to the model, or other ways of tokenizing/filtering the text, and so forth.  </p>

<p>There are other more sophisticated ways to get the vector for a length-of-text, such as the 'Paragraph Vectors' algorithm implemented by gensim's <code>Doc2Vec</code> class. These don't necessarily start with pretrained word-vectors, but can be trained on your own corpus of texts. </p>
",1,0,544,2017-07-13 00:47:01,https://stackoverflow.com/questions/45069715/after-loading-a-pretrained-word2vec-model-how-do-i-get-word2vec-representations
python word2vec context similarity using surrounding words,"<p>I would like to use embeddings made by w2v in order to obtain the most likely substitute words GIVEN a context (surrounding words), rather than supplying an individual word.</p>

<p>Example:
sentence = 'I would like to go to the park tomorrow after school'</p>

<p>If I want to find candidates similar to ""park"", typically I would just leverage the similarity function from the Gensim model</p>

<pre><code>model.most_similar('park')
</code></pre>

<p>and obtain semantically similar words. However this could give me similar words to the verb 'park' instead of the noun 'park', which I was after.</p>

<p>Is there any way to query the model and give it surrounding words as context to provide better candidates?</p>
","python, gensim, word2vec, word-embedding","<p>Word2vec is not, primarily, a word-prediction algorithm. Internally it tries to do semi-predictions, to train its word-vectors, but usually these training-predictions aren't the end-use for which word-vectors are wanted.</p>

<p>That said, recent versions of gensim added a <code>predict_output_word()</code> method that (for some model modes) approximates the predictions done during training. It might be useful for your purposes. </p>

<p>Alternatively, checking for the words <code>most_similar()</code> to your initial target word that are <em>also</em> somewhat-similar to the context words might help. </p>

<p>There have been some research papers about ways to disambiguate multiple word senses (like 'to /park/ a car' versus 'walk in a /park/') during word-vector training, but I haven't seen them implemented in open source libraries. </p>
",3,1,855,2017-07-14 17:05:14,https://stackoverflow.com/questions/45108291/python-word2vec-context-similarity-using-surrounding-words
word2vec models consist of characters instead of words,"<p>I am trying to make a word2vec model by Gensim on Persian language which has ""space"" as the character delimiter, I use python 3.5. The problem that I encounter was I gave a text file as input and it returns a model which only consists of each character separately instead of words. I also gave the input as a list of words which is recommended on :</p>

<p><a href=""https://stackoverflow.com/questions/43065843/python-gensim-word2vec-vocabulary-key]"">Python Gensim word2vec vocabulary key</a></p>

<p>It doesn't work for me and I think it doesn't consider sequence of words in a sentence so it wouldn't be correct.</p>

<p>I did some preprocessing on my input which consist of:</p>

<p>collapse multiple whitespaces into a single one<br>
tokenize by splitting on whitespace<br>
remove words less than 3 characters long
remove stop words</p>

<p>I gave the text to word2vec which gave me result correctly, but I need it on python so my choice is limited to use Gensim.</p>

<p>Also I tried to load the model which made by word2vec source on gensim I get error so I need create the word2vec model by Gensim.</p>

<p>my code is:</p>

<pre><code>  wfile = open('aggregate.txt','r')    
  wfileRead = wfile.read()    
  model = word2vec.Word2Vec(wfileRead , size=100)   
  model.save('Word2Vec.txt')
</code></pre>
","gensim, word2vec","<p>The gensim Word2Vec model does not expect <em>strings</em> as its text examples (sentences), but <em>lists-of-tokens</em>. Thus, it's up to your code to tokenize your text, before passing it to Word2Vec. </p>

<p>Your code as shown just passes raw data from 'aggregate.txt' file into Word2Vec as <code>wFileRead</code>.</p>

<p>Look at examples in the gensim documentation, including the <code>LineSentence</code> class included with gensim, for ideas </p>
",8,3,4898,2017-07-18 06:59:03,https://stackoverflow.com/questions/45159693/word2vec-models-consist-of-characters-instead-of-words
Gensim Word2Vec Error: ValueError: missing section header before line #0,"<p>I am new to Gensim Word2Vec. I was trying to use Word2Vec to build word vectors for some raw html files. So I first convert the html file into txt file.</p>

<h3>My First Question:</h3>

<p>When I train the word2vec model, everything is fine. But when I want to test the accuracy of the model by doing</p>

<pre><code>model.accuracy(file_name)
</code></pre>

<p>it produced error: </p>

<pre><code>Traceback (most recent call last):
  File ""build_w2v.py"", line 82, in &lt;module&gt;
    main()
  File ""build_w2v.py"", line 77, in main
    gen_w2v_model()
  File ""build_w2v.py"", line 71, in gen_w2v_model
    accuracy = model.accuracy(target)
  File ""/home/k/shankai/app/anaconda2/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 1330, in accuracy
    return self.wv.accuracy(questions, restrict_vocab, most_similar, case_insensitive)
  File ""/home/k/shankai/app/anaconda2/lib/python2.7/site-packages/gensim/models/keyedvectors.py"", line 679, in accuracy
    raise ValueError(""missing section header before line #%i in %s"" % (line_no, questions))
ValueError: missing section header before line #0
</code></pre>

<p>Below is the sample file:</p>

<pre><code>zGR='ca-about-health_js';var ziRfw=0;zobt="" Vision Ads"";zOBT="" Ads"";function zIpSS(u){zpu(0,u,280,375,""ssWin"")}function zIlb(l,t,f){zT(l,'18/1Pp/wX')}


zWASL=1;zGRH=1
#rs{margin:0 0 10px}#rs #n5{font-weight:bold}#rs a{padding:7px;text-transform:capitalize}Poking Eyelashes - Poking Eyelashes Problem


&lt;!--
zGOW=0;xd=0;zap="""";zAth='25752';zAthG='25752';zTt='11';zir='';zBTS=0;zBT=0;zSt='';zGz=''
ch='health';gs='vision';xg=""Vision"";zcs=''
zFDT='0'
zFST='0'
zOr='BA15WT26OkWA0O1b';zTbO=zRQO=1;zp0=zp1=zp2=zp3=zfs=0;zDc=1;
zSm=zSu=zhc=zpb=zgs=zdn='';zFS='BA110BA0110B00101';zFD='BA110BA0110B00101'
zDO=zis=1;zpid=zi=zRf=ztp=zpo=0;zdx=20;zfx=100;zJs=0;
zi=1;zz=';336280=2-1-1299;72890=2-1-1299;336155=2-1-12-1;93048=2-1-12-1;30050=2-1-12-1';zx='100';zde=15;zdp=1440;zds=1440;zfp=0;zfs=66;zfd=100;zdd=20;zaX=new Array(11, new Array(100,1051,8192,2,'336,300'),7, new Array(100,284,8196,12,'336,400'));zDc=1;;zDO=1;;zD336=1;zhc='';;zGTH=1;
zGo=0;zG=17;zTac=2;zDot=0;
zObT=""Vision"";zRad=5;var tp="" primedia_""+(zBT?"""":""non_"")+""site_targeting"";if(!this.zGCID)zGCID=tp
else zGCID+=tp;
if(zBT&gt;0){zOBR=1}
if(!this.uy)uy='about.com';if(typeof document.domain!=""undefined"")document.domain=uy;//--&gt;


function zob(p){if(!this.zOfs)return;var a=zOfs,t,i=0,l=a.length;if(l){w('&lt;div id=""oF""&gt;&lt;b&gt;'+(this.zobt?zobt:xg+' Ads')+'&lt;/b&gt;&lt;ul&gt;');while((i&lt;l)&amp;&amp;i&lt;zRad){t=a[i++].line1;w('&lt;li&gt;&lt;a href=""/z/js/o'+(p?p:'')+'.htm?k='+zUriS(t.toLowerCase())+(this.zobr?zobr:'')+'&amp;d='+zUriS(t)+'&amp;r='+zUriS(zWl)+'"" target=""_'+(this.zOBNW?'new'+zr(9999):'top')+'""&gt;'+t+'&lt;/a&gt;&lt;/li&gt;');}w('&lt;/ul&gt;&lt;/div&gt;')}}function rb600(){if(gEI('bb'))gEI('bb').height=600}zJs=10
zJs=11
zJs=12
zJs=13
zc(5,'jsc',zJs,9999999,'')
zDO=0
</code></pre>

<p>So This file actually begins with many (I don't know) space or \n. When I open in the vim.<a href=""https://i.sstatic.net/jOz4T.png"" rel=""nofollow noreferrer"">It looks like this</a>.</p>

<p><strong>So what is the problem here?</strong></p>

<h3>My second question:</h3>

<p>Also, I am doing text classification of some biomedical papers. The files I was given are all raw html files in either Japanese or English. After I do the ascii conversion and some stop_words cleaning, there are still many HTML code left in the file. </p>

<p>When I try to clean these files and restrict the characters to [a-zA-Z0-9], I found some medical terms like [4protein...] or something get not properly cleaned as well.</p>

<p><strong>Are there any suggestions in how to clean up these files?</strong></p>
","python, html, nlp, gensim, word2vec","<p>The argument to <code>accuracy()</code> should be a set of analogies to test the model against, in the format of the <code>questions-words.txt</code> file available from the original word2vec.c distribution. (It should <em>not</em> be your own file.)</p>
",0,0,741,2017-07-18 10:36:34,https://stackoverflow.com/questions/45164340/gensim-word2vec-error-valueerror-missing-section-header-before-line-0
How to make word2vec model&#39;s loading time and memory use more efficient?,"<p>I want to use Word2vec in a web server (production) in two different variants where I fetch two sentences from the web and compare it in real-time. For now, I am testing it on a local machine which has 16GB RAM. </p>

<p>Scenario:
w2v = load w2v model </p>

<pre><code>If condition 1 is true:
   if normalize:
      reverse normalize by w2v.init_sims(replace=False) (not sure if it will work)
   Loop through some items:
   calculate their vectors using w2v
else if condition 2 is true:
   if not normalized:
       w2v.init_sims(replace=True)
   Loop through some items:
   calculate their vectors using w2v
</code></pre>

<p>I have already read the solution about reducing the vocabulary size to a small size but I would like to use all the vocabulary. </p>

<p>Are there new workarounds on how to handle this? Is there a way to initially load a small portion of the vocabulary for first 1-2 minutes and in parallel keep loading the whole vocabulary? </p>
","python, nlp, nltk, gensim, word2vec","<p>As a one-time delay that you should be able to schedule to happen before any service-requests, I would recommend against worrying too much about the first-time <code>load()</code> time. (It's going to inherently take a lot of time to load a lot of data from disk to RAM – but once there, if it's being kept around and shared between processes well, the cost is not spent again for an arbitrarily long service-uptime.)</p>

<p>It doesn't make much sense to ""load a small portion of the vocabulary for first 1-2 minutes and in parallel keep loading the whole vocabulary"" – as soon as any similarity-calc is needed, the whole set of vectors need to be accessed for any top-N results. (So the ""half-loaded"" state isn't very useful.) </p>

<p>Note that if you do <code>init_sims(replace=True)</code>, the model's original raw vector magnitudes are clobbered with the new unit-normed (all-same-magnitude) vectors. So looking at your pseudocode, the only difference between the two paths is the explicit <code>init_sims(replace=True)</code>. But if you're truly keeping the same shared model in memory between requests, as soon as <code>condition 2</code> occurs, the model is normalized, and thereafter calls under <code>condition 1</code> are also occurring with normalized vectors. And further, additional calls under <code>condition 2</code> just redundantly (and expensively) re-normalize the vectors in-place. So if normalized-comparisons are your only focus, best to do one in-place <code>init_sims(replace=True)</code> at service startup - not at the mercy of order-of-requests. </p>

<p>If you've saved the model using gensim's native <code>save()</code> (rather than <code>save_word2vec_format()</code>), and as uncompressed files, there's the option to 'memory-map' the files on a future re-load. This means rather than immediately copying the full vector array into RAM, the file-on-disk is simply marked as providing the addressing-space. There are two potential benefits to this: (1) if you only even access some limited ranges of the array, only those are loaded, on demand; (2) many separate processes all using the same mapped files will automatically reuse any shared ranges loaded into RAM, rather than potentially duplicating the same data. </p>

<p>(1) isn't much of an advantage as soon as you need to do a full-sweep over the whole vocabulary – because they're all brought into RAM then, and further at the moment of access (which will have more service-lag than if you'd just pre-loaded them). But (2) is still an advantage in multi-process webserver scenarios. There's a lot more detail on how you might use memory-mapped word2vec models efficiently in a prior answer of mine, at <a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">How to speed up Gensim Word2vec model load time?</a></p>
",2,1,2450,2017-07-19 09:16:06,https://stackoverflow.com/questions/45186094/how-to-make-word2vec-models-loading-time-and-memory-use-more-efficient
Gensim errors after updating Python version with conda,"<p>I recently updated a conda environment from python=3.4 to python 3.6. The environment is made for a project using <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow noreferrer"">gensim</a> which worked perfectly on 3.4. After this update, using the library generates multiple errors such as:</p>
<blockquote>
<p>TypeError: object of type 'itertools.chain' has no len()</p>
</blockquote>
<p>or</p>
<blockquote>
<p>AssertionError: decomposition not initialized yet</p>
</blockquote>
<p>Do you guys know why this happens while gensim explicitly says Python 3.5 and 3.6 are supported?</p>
<p>The used code:</p>
<pre><code># Create Texts
texts = src.data.raw.extract_clean_merge_titles_abstracts(papers)
src.data.raw.train_phraser(texts)
texts = src.data.raw.tokenize_stream(texts)

print(&quot;Size of corpus: &quot;, len(texts)) # ERROR 1 HERE

# Create Dictionary
dictionary = gensim.corpora.dictionary.Dictionary(texts, prune_at=None)
dictionary.filter_extremes(no_below=3 ,no_above=0.1, keep_n=None)
dictionary.compactify()
print(dictionary)
dictionary.save(config.paths.PATH_DATA_GENSIM_TEMP_DICTIONARY)

# Create corpus
corpus = [dictionary.doc2bow(text) for text in texts]
#gensim.corpora.MmCorpus.serialize(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS, corpus)
corpus_index = gensim.similarities.docsim.Similarity(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_INDEX, corpus, num_features=len(dictionary))
corpus_index.save(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_INDEX)

# tf-idf
tfidf = gensim.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]    #gensim.corpora.MmCorpus.serialize(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_TFIDF, corpus_tfidf)
tfidf.save(config.paths.PATH_DATA_GENSIM_TEMP_TFIDF)
corpus_tfidf_index = gensim.similarities.docsim.Similarity(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_TFIDF_INDEX, corpus_tfidf, num_features=len(dictionary))
corpus_tfidf_index.save(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_TFIDF_INDEX)

# lsa
lsa_num_topics = 100
lsa = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=lsa_num_topics)
corpus_lsa = lsa[corpus_tfidf] # ERROR 2 HERE
#gensim.corpora.MmCorpus.serialize(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_LSA, corpus_lsa)
lsa.save(config.paths.PATH_DATA_GENSIM_TEMP_LSA)
corpus_lsa_index = gensim.similarities.docsim.Similarity(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_LSA_INDEX, corpus_lsa, num_features=lsa_num_topics)
corpus_lsa_index.save(config.paths.PATH_DATA_GENSIM_TEMP_CORPUS_LSA_INDEX)
</code></pre>
<p>Here is the list of the packages installed:</p>
<pre><code>bkcharts                  0.2                      py36_0  
bokeh                     0.12.6                   py36_0  
boto                      2.47.0                   py36_0  
bz2file                   0.98                     py36_0  
cycler                    0.10.0                   py36_0  
dbus                      1.8.20                        1    ostrokach
decorator                 4.0.11                   py36_0  
expat                     2.1.0                         0    ostrokach
fontconfig                2.12.1                        3  
freetype                  2.5.5                         2  
gensim                    2.2.0               np113py36_0  
gettext                   0.19.5                        2    ostrokach
glib                      2.48.2                        0    ostrokach
gst-plugins-base          1.8.0                         0  
gstreamer                 1.8.0                         0  
icu                       54.1                          0    ostrokach
jinja2                    2.9.6                    py36_0  
jpeg                      9b                            0  
libffi                    3.2.1                         8    ostrokach
libgcc                    5.2.0                         0  
libgfortran               3.0.0                         1  
libiconv                  1.14                          0  
libpng                    1.6.27                        0  
libsigcpp                 2.4.1                         3    ostrokach
libxcb                    1.12                          1  
libxml2                   2.9.4                         0  
markupsafe                0.23                     py36_2  
matplotlib                2.0.2               np113py36_0  
mkl                       2017.0.3                      0  
networkx                  1.11                     py36_0  
nltk                      3.2.4                    py36_0  
numpy                     1.13.1                   py36_0  
openssl                   1.0.2l                        0  
pcre                      8.39                          1  
pip                       9.0.1                    py36_1  
pymysql                   0.7.9                    py36_0  
pyparsing                 2.1.4                    py36_0  
pyqt                      5.6.0                    py36_2  
python                    3.6.1                         2  
python-dateutil           2.6.0                    py36_0  
pytz                      2017.2                   py36_0  
pyyaml                    3.12                     py36_0  
qt                        5.6.2                         4  
readline                  6.2                           2  
requests                  2.14.2                   py36_0  
scikit-learn              0.18.2              np113py36_0  
scipy                     0.19.1              np113py36_0  
setuptools                27.2.0                   py36_0  
sip                       4.18                     py36_0  
six                       1.10.0                   py36_0  
smart_open                1.5.3                    py36_0  
sqlite                    3.13.0                        0  
system                    5.8                           2  
tk                        8.5.18                        0  
tornado                   4.5.1                    py36_0  
wheel                     0.29.0                   py36_0  
xz                        5.2.2                         1  
yaml                      0.1.6                         0  
zlib                      1.2.8                         3  
</code></pre>
","python-3.x, conda, gensim","<p>My bad, it came from the Phraser: </p>

<pre><code>def tokenize_stream(stream, max_num_words = 3):
    tokens_stream = [gensim.utils.simple_preprocess(t, min_len=2, max_len=50) for t in stream]
    for i,tokens in enumerate(tokens_stream):
        tokens_stream[i] = [j for j in tokens if j not in stop_words]
    phrases = gensim.models.phrases.Phrases.load(config.paths.PATH_DATA_GENSIM_PHRASES)
    grams = gensim.models.phrases.Phraser(phrases)
    tokens_stream = list(grams[tokens_stream]) ## HERE LIST IS IMPORTANT
    return tokens_stream
</code></pre>

<p>For some reason, with python 3.4, not using ""list(grams[...])"" did work in my code, and returns an itertool.chain instance which leads to an empty corpus with python 3.6. </p>
",0,0,480,2017-07-19 14:26:08,https://stackoverflow.com/questions/45193550/gensim-errors-after-updating-python-version-with-conda
Gensim Doc2Vec generating huge file for model,"<p>I am trying to run doc2vec library from gensim package. My problem is that when I am training and saving the model the model file is rather large(2.5 GB) I tried using this line :</p>

<pre><code>model.estimate_memory()
</code></pre>

<p>But it didn't change anything. I also have tried to change max_vocab_size to decrease the space. But there was not luck. Can somebody help me with this matter?</p>
","python, semantics, gensim, word2vec, doc2vec","<p>Doc2Vec models can be large. In particular, any word-vectors in use will use 4 bytes per dimension, times two layers of the model. So a 300-dimension model with a 200,000 word vocabulary will use just for the vectors array itself:</p>

<pre><code>200,000 vectors * 300 dimensions * 4 bytes/float * 2 layers = 480MB
</code></pre>

<p>(There will be additional overhead for the dictionary storing vocabulary information.)</p>

<p>Any doc-vectors will also use 4 bytes per dimnsion. So if you train a vectors for a million doc-tags, the model will use just for the doc-vectors array:</p>

<pre><code>1,000,000 vectors * 300 dimensions * 4 bytes/float = 2.4GB
</code></pre>

<p>(If you're using arbitrary string tags to name the doc-vectors, there'll be additional overhead for that.)</p>

<p>To use less memory when loaded (which will also result in a smaller store file), you can use a smaller vocabulary, train fewer doc-vecs, or use a smaller vector size. </p>

<p>If you'll only need the model for certain narrow purposes, there may be other parts you can throw out after training – but that requires knowledge of the model internals/source-code, and your specific needs, and will result in a model that's broken (and likely to throw errors) for many other usual operations.</p>
",7,5,1155,2017-07-19 15:37:04,https://stackoverflow.com/questions/45195169/gensim-doc2vec-generating-huge-file-for-model
Why doesn&#39;t gensim&#39;s Word2Vec recognize &#39;compute_loss&#39; keyword?,"<p>According to the <strong>gensim.models.Word2Vec</strong> <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">API reference</a>, ""compute_loss"" is a valid keyword. However, I get an error that says it's an <code>unexpected keyword</code>.</p>

<p><strong>UPDATE</strong>:</p>

<p>The Word2Vec class on GitHub <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">does have</a> the 'compute_loss' keyword, but my local library does not.
I see that the gensim documentation and library deviate from each other.
I found that the <code>win-64/gensim-2.2.0-np113py35_0.tar.bz2</code>-file in <a href=""https://anaconda.org/anaconda/gensim/files"" rel=""nofollow noreferrer"">conda repository</a> is not up to date.</p>

<p>However after uninstalling gensim with conda, <code>pip install gensim</code> did not change anything as it still doesn't work.</p>

<p>Apparently, the source on GitHub and the distributed library are different, but the tutorial seems to assume code is as on GitHub.</p>

<p><strong>/END OF UPDATE</strong></p>

<p>I followed and downloaded the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb"" rel=""nofollow noreferrer"">tutorial notebook on Word2Vec</a>.</p>

<p>In input [25], first cell after ""Training Loss Computation"" headline, I get an error in the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Word2Vec</a> class' initializer. </p>

<p>Input:</p>

<pre><code># instantiating and training the Word2Vec model
model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, 
compute_loss=True, hs=0, sg=1, seed=42)

# getting the training loss value
training_loss = model_with_loss.get_latest_training_loss()
print(training_loss)
</code></pre>

<p>Output:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-25-c2933abf4b08&gt; in &lt;module&gt;()
      1 # instantiating and training the Word2Vec model
----&gt; 2 model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42)
      3 
      4 # getting the training loss value
      5 training_loss = model_with_loss.get_latest_training_loss()

TypeError: __init__() got an unexpected keyword argument 'compute_loss'
</code></pre>

<p>I have gensim 2.2.0 installed via conda and have a new new clone from the gensim repository (with the tutorial notebook). I'm using 64-bit Python 3.5.3 on windows 10. (Anaconda)</p>

<p>I've tried to search for others with same encounter, but I haven't been successful. </p>

<p>Do you know the reason for this, and how to fix this? Apparently, the source on GitHub and the distributed library are different, but the tutorial seems to assume code is as on GitHub.</p>

<p>I've also previously <a href=""https://groups.google.com/forum/#!topic/gensim/J1J2zTZwD7Q"" rel=""nofollow noreferrer"">posted the question</a> in the official mailing list.</p>
","python, gensim, word2vec","<p><strong>UPDATE:</strong> <code>compute_loss</code> was added in version 2.3.0, on July 25th. <strong>/UPDATE</strong></p>

<p>The notebook referenced in the question is on the <strong>develop</strong> branch. The <strong>master</strong> branch has a <a href=""https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/word2vec.ipynb"" rel=""nofollow noreferrer"">notebook</a> that is consistent with the latest distribution.</p>

<p>The <code>compute_loss</code> parameter was added in <a href=""https://github.com/RaRe-Technologies/gensim/commit/cdc5944a15dc3fecbd80eb61145b7a0ef838c7fd"" rel=""nofollow noreferrer"">this commit</a>, June 19. The <a href=""https://pypi.python.org/pypi/gensim"" rel=""nofollow noreferrer"">last upload</a> to PYPI was June 21, only two days later. (As of today). The <code>compute_loss</code> is not included in the distribution. (Last commit in v2.2.0 is <a href=""https://github.com/RaRe-Technologies/gensim/commit/dfd1f8ea084179a56080b2bfc53b6a6f30a9cc59"" rel=""nofollow noreferrer"">this</a>.)</p>

<p>I assume that the solution is to wait for the next version of gensim, and download code from repository in the mean time.</p>

<p>However, this might cause challenges to get gensim FAST version to work, at least on Windows. See <a href=""https://stackoverflow.com/questions/44490739/using-gensim-shows-slow-version-of-gensim-models-doc2vec-being-used"">Using Gensim shows &quot;Slow version of gensim.models.doc2vec being used&quot;</a>.</p>

<p>How to install gensim from GitHub is explained in their <a href=""https://radimrehurek.com/gensim/install.html#install-gensim"" rel=""nofollow noreferrer"">install documentation</a>.</p>
",2,3,1796,2017-07-24 08:47:11,https://stackoverflow.com/questions/45276029/why-doesnt-gensims-word2vec-recognize-compute-loss-keyword
Using a Word2Vec model pre-trained on wikipedia,"<p>I need to use gensim to get vector representations of words, and I figure the best thing to use would be a word2vec module that's pre-trained on the english wikipedia corpus. Does anyone know where to download it, how to install it, and how to use gensim to create the vectors?</p>
","wikipedia, gensim, word2vec","<p>@imanzabet provided useful links with pre-trained vectors, but if you want to train the models yourself using genism than you need to do two things:</p>
<ol>
<li><p>Acquire the Wikipedia data, which you can access <a href=""https://dumps.wikimedia.org/"" rel=""nofollow noreferrer"">here</a>.  Looks like the most recent snapshot of English Wikipedia was on the 20th, and it can be found <a href=""https://dumps.wikimedia.org/enwiki/20170720/"" rel=""nofollow noreferrer"">here</a>. I believe the other English-language &quot;wikis&quot; e.g. <a href=""https://en.wikiquote.org"" rel=""nofollow noreferrer"">quotes</a> are captured separately, so if you want to include them you'll need to download those as well.</p>
</li>
<li><p>Load the data and use it to generate the models. That's a fairly broad question, so I'll just link you to the excellent <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">genism documentation</a> and <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">word2vec tutorial</a>.</p>
</li>
</ol>
",2,18,21396,2017-07-25 17:51:48,https://stackoverflow.com/questions/45310409/using-a-word2vec-model-pre-trained-on-wikipedia
How to get a complete topic distribution for a document using gensim LDA?,"<p>When I train my lda model as such</p>

<pre><code>dictionary = corpora.Dictionary(data)
corpus = [dictionary.doc2bow(doc) for doc in data]
num_cores = multiprocessing.cpu_count()
num_topics = 50
lda = LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, 
workers=num_cores, alpha=1e-5, eta=5e-1)
</code></pre>

<p>I want to get a full topic distribution for all <code>num_topics</code> for each and every document. That is, in this particular case, I want each document to have 50 topics contributing to the distribution <strong><em>and</em></strong> I want to be able to access all 50 topics' contribution. This output is what LDA should do if adhering strictly to the mathematics of LDA. However, gensim only outputs topics that exceed a certain threshold as shown <strong><a href=""https://stackoverflow.com/questions/23509699/understanding-lda-transformed-corpus-in-gensim/37708396?noredirect=1#comment77429460_37708396"">here</a></strong>. For example, if I try</p>

<pre><code>lda[corpus[89]]
&gt;&gt;&gt; [(2, 0.38951721864890398), (9, 0.15438596408262636), (37, 0.45607443684895665)]
</code></pre>

<p>which shows only 3 topics that contribute most to document 89. I have tried the solution in the link above, however this does not work for me. I still get the same output:</p>

<pre><code>theta, _ = lda.inference(corpus)
theta /= theta.sum(axis=1)[:, None]
</code></pre>

<p>produces the same output i.e. only 2,3 topics per document.</p>

<p>My question is how do I change this threshold so I can access the <strong><em>FULL</em></strong> topic distribution for <strong><em>each</em></strong> document? How can I access the full topic distribution, no matter how insignificant the contribution of a topic to a document? The reason I want the full distribution is so I can perform a <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""noreferrer"">KL similarity</a> search between documents' distribution.</p>

<p>Thanks in advance</p>
","python, gensim, lda","<p>It doesnt seem that anyone has replied yet, so I'll try and answer this as best I can given the gensim <a href=""https://radimrehurek.com/gensim/models/ldamulticore.html"" rel=""noreferrer"">documentation</a>.</p>

<p>It seems you need to set a parameter <code>minimum_probability</code> to 0.0 when training the model to get the desired results:</p>

<pre><code>lda = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary, workers=num_cores, alpha=1e-5, eta=5e-1,
              minimum_probability=0.0)

lda[corpus[233]]
&gt;&gt;&gt; [(0, 5.8821799358842424e-07),
 (1, 5.8821799358842424e-07),
 (2, 5.8821799358842424e-07),
 (3, 5.8821799358842424e-07),
 (4, 5.8821799358842424e-07),
 (5, 5.8821799358842424e-07),
 (6, 5.8821799358842424e-07),
 (7, 5.8821799358842424e-07),
 (8, 5.8821799358842424e-07),
 (9, 5.8821799358842424e-07),
 (10, 5.8821799358842424e-07),
 (11, 5.8821799358842424e-07),
 (12, 5.8821799358842424e-07),
 (13, 5.8821799358842424e-07),
 (14, 5.8821799358842424e-07),
 (15, 5.8821799358842424e-07),
 (16, 5.8821799358842424e-07),
 (17, 5.8821799358842424e-07),
 (18, 5.8821799358842424e-07),
 (19, 5.8821799358842424e-07),
 (20, 5.8821799358842424e-07),
 (21, 5.8821799358842424e-07),
 (22, 5.8821799358842424e-07),
 (23, 5.8821799358842424e-07),
 (24, 5.8821799358842424e-07),
 (25, 5.8821799358842424e-07),
 (26, 5.8821799358842424e-07),
 (27, 0.99997117731831464),
 (28, 5.8821799358842424e-07),
 (29, 5.8821799358842424e-07),
 (30, 5.8821799358842424e-07),
 (31, 5.8821799358842424e-07),
 (32, 5.8821799358842424e-07),
 (33, 5.8821799358842424e-07),
 (34, 5.8821799358842424e-07),
 (35, 5.8821799358842424e-07),
 (36, 5.8821799358842424e-07),
 (37, 5.8821799358842424e-07),
 (38, 5.8821799358842424e-07),
 (39, 5.8821799358842424e-07),
 (40, 5.8821799358842424e-07),
 (41, 5.8821799358842424e-07),
 (42, 5.8821799358842424e-07),
 (43, 5.8821799358842424e-07),
 (44, 5.8821799358842424e-07),
 (45, 5.8821799358842424e-07),
 (46, 5.8821799358842424e-07),
 (47, 5.8821799358842424e-07),
 (48, 5.8821799358842424e-07),
 (49, 5.8821799358842424e-07)]
</code></pre>
",13,15,11028,2017-07-25 18:21:31,https://stackoverflow.com/questions/45310925/how-to-get-a-complete-topic-distribution-for-a-document-using-gensim-lda
gensim.interfaces.TransformedCorpus - How use?,"<p>I'm relative new in the world of Latent Dirichlet Allocation.
I am able to generate a LDA Model following the Wikipedia tutorial and I'm able to generate a LDA model with my own documents.
My step now is try understand how can I use a previus generated model to classify unseen documents.
I'm saving my ""lda_wiki_model"" with</p>

<pre><code>id2word =gensim.corpora.Dictionary.load_from_text('ptwiki_wordids.txt.bz2')

    mm = gensim.corpora.MmCorpus('ptwiki_tfidf.mm')

    lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)
    lda.save('lda_wiki_model.lda')
</code></pre>

<p>And I'm loading the same model with:</p>

<pre><code>new_lda = gensim.models.LdaModel.load(path + 'lda_wiki_model.lda') #carrega o modelo
</code></pre>

<p>I have a ""new_doc.txt"", and I turn my document into a id&lt;-> term dictionary and converted this tokenized document to ""document-term matrix""</p>

<p>But when I run <code>new_topics = new_lda[corpus]</code> I receive a 
<strong><em>'gensim.interfaces.TransformedCorpus object at 0x7f0ecfa69d50'</em></strong></p>

<p>how can I extract topics from that?</p>

<p>I already tried </p>

<pre><code>`lsa = models.LdaModel(new_topics, id2word=dictionary, num_topics=1, passes=2)
corpus_lda = lsa[new_topics]
print(lsa.print_topics(num_topics=1, num_words=7)
</code></pre>

<p>and</p>

<p><code>print(corpus_lda.print_topics(num_topics=1, num_words=7</code>)
`</p>

<p>but that return topics not relationed to my new document.
Where is my mistake? I'm miss understanding something?</p>

<p>**If a run a new model using the dictionary and corpus created above, I receive the correct topics, my point is: how re-use my model? is correctly re-use that wiki_model?</p>

<p>Thank you.</p>
","gensim, lda","<p>I was facing the same problem. This code will solve your problem:</p>

<pre><code>new_topics = new_lda[corpus]

for topic in new_topics:

      print(topic)
</code></pre>

<p>This will give you a list of tuples of form (topic number, probability)</p>
",12,9,8897,2017-07-26 03:54:12,https://stackoverflow.com/questions/45317151/gensim-interfaces-transformedcorpus-how-use
Gensim&#39;s similarity: how does it work?,"<p>I wonder how the <a href=""https://radimrehurek.com/gensim/tut3.html"" rel=""nofollow noreferrer"">similarity</a> works with gensim ? How the different shards are created and does it increase performance when looking only for top-N similar document ? More generally, is there a documentation about the internal structures of gensim ?</p>
","python, nlp, gensim","<p>The documentation of the internals of gensim is the full source code:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim</a></p>

<p>With high-dimensional data like this, finding the <em>exact</em> top-N most-similar vectors generally requires an exhaustive search against all candidates. That is, there's no simple sharding that allows most vectors to be ignored as too-far away and still gives precise results.</p>

<p>There <em>are</em> approximate indexing techniques, like <a href=""https://github.com/spotify/annoy"" rel=""nofollow noreferrer"">ANNOY</a>, that can speed searches... but they tend to miss some of the true top-N results. Gensim includes a <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/annoytutorial.ipynb"" rel=""nofollow noreferrer"">demo notebook of using ANNOY-indexing with gensim's word2vec support</a>. (It should be possible to do something similar with other text-vectors, like the bag-of-words representations in the tutorial you link.)</p>
",1,1,391,2017-07-27 09:10:01,https://stackoverflow.com/questions/45346233/gensims-similarity-how-does-it-work
Issues installing gensim on Ubuntu,"<p>I am trying to install gensim in Python on my Ubuntu. I tried with  easy_install but getting errors. Could someone help with identifying what is going wrong?</p>

<p><strong>easy_install</strong></p>

<pre><code>easy_install -U gensim

Running scipy-0.19.1/setup.py -q bdist_egg --dist-dir /tmp/easy_install-    QXO1dA/scipy-0.19.1/egg-dist-tmp-AxijnA
/tmp/easy_install-QXO1dA/scipy-0.19.1/setup.py:323: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates 

warnings.warn(""Unrecognized setuptools command, proceeding with ""                                                                 /usr/local/lib/python2.7/dist-packages/numpy/distutils/system_info.py:572: UserWarning: Atlas (http://math-atlas.sourceforge.net/) libraries not found.                                                                     
Directories to search for the libraries can be specified in the                                                                     numpy/distutils/site.cfg file (section [atlas]) or by setting                                                                       the ATLAS environment variable.    

self.calc_info()                                                                                                                    
/usr/local/lib/python2.7/dist-packages/numpy/distutils/system_info.py:572: UserWarning:                                                 
Lapack (http://www.netlib.org/lapack/) libraries not found.                                                                          
Directories to search for the libraries can be specified in the                                                                     
numpy/distutils/site.cfg file (section [lapack]) or by setting                                                                      
the LAPACK environment variable.                                                                                                  
self.calc_info()                                                                                                                  
/usr/local/lib/python2.7/dist-packages/numpy/distutils
/system_info.py:572: UserWarning:                                                   
Lapack (http://www.netlib.org/lapack/) sources not found.                                                                           
Directories to search for the sources can be specified in the                                                                       
numpy/distutils/site.cfg file (section [lapack_src]) or by setting                                                                  
the LAPACK_SRC environment variable.                                                                                              
self.calc_info()                                                                                                                  
Running from scipy source directory.                                                                                                
non-existing path in 'scipy/integrate': 'quadpack.h'                                                                                
Warning: Can't read registry to find the necessary compiler setting                                                                 
Make sure that Python modules _winreg, win32api or win32con are  installed.                                                          
error: no lapack/blas resources found
</code></pre>

<p>Thank you</p>
","python, python-2.7, gensim","<p>Had to upgrade Scipy...followed the solution given in the answer by josteinb in the following thread:</p>

<p><a href=""https://stackoverflow.com/questions/17925381/cant-upgrade-scipy"">Can&#39;t upgrade Scipy</a></p>

<p>Was able to upgrade scipy as follows:</p>

<pre><code>sudo apt-get build-dep python-scipy
sudo pip install --upgrade scipy
</code></pre>

<p>easy_install of gensim worked smoothly after this</p>
",0,1,943,2017-07-27 13:39:51,https://stackoverflow.com/questions/45352522/issues-installing-gensim-on-ubuntu
AttributeError module &#39;Pyro4&#39; has no attribute &#39;expose&#39; while running gensim distributed LSI,"<p>So I am trying to run the demo from gensim for distributed LSI (You can find it <a href=""https://radimrehurek.com/gensim/dist_lsi.html"" rel=""nofollow noreferrer"">here</a>) Yet whenever I run the code I get the error</p>

<p><code>AttributeError: module 'Pyro4' has no attribute 'expose'</code></p>

<p>I have checked similar issues here on stackoverflow, and usually they are caused through misuse of the library.</p>

<p>However I am not using Pyro4 directly, I am using Distributed LSI introduced by gensim. So there is no room for mistakes on my side (or so I believe)</p>

<p>My code is really simple you can find it below</p>

<pre><code>from gensim import corpora, models, utils
import logging, os, Pyro4
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
os.environ[""PYRO_SERIALIZERS_ACCEPTED""] =  'pickle'
os.environ[""PYRO_SERIALIZER""] = 'pickle'

corpus = corpora.MmCorpus('wiki_corpus.mm') # load a corpus of nine documents, from the Tutorials
id2word = corpora.Dictionary.load('wiki_dict.dict')

lsi = models.LsiModel(corpus, id2word=id2word, num_topics=200, chunksize=1, distributed=True) # run distributed LSA on nine documents
</code></pre>
","python-2.7, gensim, latent-semantic-indexing, pyro4","<p><code>Pyro4.expose</code> was added in Pyro4 version 4.27 from august 2014.
It looks to me that you have a <em>very</em> old Pyro4 version installed from before this date, and that your gensim requires a more recent one.</p>

<p>Check using:</p>

<pre><code>$ python -m Pyro4.configuration | head -3
</code></pre>

<p>You should probably upgrade your Pyro4 library...
Pay attention though, I believe gensim doesn't support the most recent versions of Pyro4 so you should probably check its manual for the correct version that you need.  You can always try to install the latest (4.61 right now) and see how it goes.</p>

<p><em>edit</em> I suppose you could also try to find gensim specific support? <a href=""https://radimrehurek.com/gensim/support.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/support.html</a></p>
",1,0,729,2017-07-30 19:48:11,https://stackoverflow.com/questions/45404027/attributeerror-module-pyro4-has-no-attribute-expose-while-running-gensim-dis
Why is Doc2Vec.scale_vocab(...)[&#39;memory&#39;][&#39;vocab&#39;] divided by 700 to obtain vocabulary size?,"<p>From the Doc2Vec wikipedia tutorial at <a href=""https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>

<pre><code>for num in range(0, 20):
    print('min_count: {}, size of vocab: '.format(num), 
           pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/700)
</code></pre>

<p>Output is:</p>

<pre><code>min_count: 0, size of vocab: 8545782.0
min_count: 1, size of vocab: 8545782.0
min_count: 2, size of vocab: 4227783.0
min_count: 3, size of vocab: 3008772.0
min_count: 4, size of vocab: 2439367.0
min_count: 5, size of vocab: 2090709.0
min_count: 6, size of vocab: 1856609.0
min_count: 7, size of vocab: 1681670.0
min_count: 8, size of vocab: 1546914.0
min_count: 9, size of vocab: 1437367.0
min_count: 10, size of vocab: 1346177.0
min_count: 11, size of vocab: 1267916.0
min_count: 12, size of vocab: 1201186.0
min_count: 13, size of vocab: 1142377.0
min_count: 14, size of vocab: 1090673.0
min_count: 15, size of vocab: 1043973.0
min_count: 16, size of vocab: 1002395.0
min_count: 17, size of vocab: 964684.0
min_count: 18, size of vocab: 930382.0
min_count: 19, size of vocab: 898725.0
</code></pre>

<blockquote>
  <p>In the original paper, they set the vocabulary size 915,715. It seems similar size of vocabulary if we set min_count = 19. (size of vocab = 898,725)</p>
</blockquote>

<p><code>700</code> seems rather arbitrary, and I don't see any mentioning of this in the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">docs</a>. </p>
","gensim, doc2vec","<p>It doesn't make sense, but here's the reason:</p>

<p><code>scale_vocab()</code> (via the use of an internal <code>estimate_memory()</code> function) returns a dict with a bunch of roughly-estimated values of the amount of memory the model will need, in bytes, for the given <code>min_count</code>. Those estimates are based on the idea each word in the model's <code>vocab</code> dict will take about 700 bytes in an HS model (where it includes some extra huffman-coding information), or just 500 bytes in a negative-sampling model. See:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/word2vec.py#L1343"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/word2vec.py#L1343</a></p>

<p>(These are very rough estimates based on a series of ad-hoc tests I ran, and might vary a lot in other environments – but usually the <code>vocab</code> isn't the biggest factor of model memory-use, so precision here isn't that important.)</p>

<p>It appears this notebook is attempting to back-calculate what the exact retained vocab-size was calculated to be, given the <code>dry_run=True</code> trial numbers, from the memory estimate. </p>

<p>But, it really doesn't have to do that. The same dict-of-results from <code>scale_vocab()</code> that includes the memory-estimates also includes, at a top-level <code>retain_total</code> key, the exact calculated retained vocab-size. See:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/word2vec.py#L723"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f630816f8cde46c8408244fb9d3bdf7359ae4c2/gensim/models/word2vec.py#L723</a></p>

<p>So, the notebook could be improved. </p>
",1,0,246,2017-07-31 14:48:32,https://stackoverflow.com/questions/45419049/why-is-doc2vec-scale-vocab-memoryvocab-divided-by-700-to-obtain-voca
Gensim: KeyError: &quot;word not in vocabulary&quot;,"<p>I have a trained Word2vec model using Python's Gensim Library. I have a tokenized list as below. The vocab size is 34 but I am just giving few out of 34:</p>

<pre><code>b = ['let',
 'know',
 'buy',
 'someth',
 'featur',
 'mashabl',
 'might',
 'earn',
 'affili',
 'commiss',
 'fifti',
 'year',
 'ago',
 'graduat',
 '21yearold',
 'dustin',
 'hoffman',
 'pull',
 'asid',
 'given',
 'one',
 'piec',
 'unsolicit',
 'advic',
 'percent',
 'buy']
</code></pre>

<p><strong>Model</strong></p>

<pre><code>model = gensim.models.Word2Vec(b,min_count=1,size=32)
print(model) 
### prints: Word2Vec(vocab=34, size=32, alpha=0.025) ####
</code></pre>

<p>if I try to get the similarity score by doing <code>model['buy']</code> of one the words in the list, I get the </p>

<blockquote>
  <p>KeyError: ""word 'buy' not in vocabulary""</p>
</blockquote>

<p>Can you guys suggest me what I am doing wrong and what are the ways to check the model which can be further used to train PCA or t-sne in order to visualize similar words forming a topic? Thank you. </p>
","python, nlp, gensim, word2vec, topic-modeling","<p>The first parameter passed to <code>gensim.models.Word2Vec</code> is an iterable of sentences. Sentences themselves are a list of words. From the docs:</p>

<blockquote>
  <p>Initialize the model from an iterable of <code>sentences</code>. Each sentence is a
  list of words (unicode strings) that will be used for training.</p>
</blockquote>

<p>Right now, it thinks that each word in your list <code>b</code> is a sentence and so it is doing <code>Word2Vec</code> for each <strong>character</strong> in each word, as opposed to each word in your <code>b</code>. Right now you can do:</p>

<pre><code>model = gensim.models.Word2Vec(b,min_count=1,size=32)

print(model['a'])
array([  7.42487283e-03,  -5.65282721e-03,   1.28707094e-02, ... ]
</code></pre>

<p>To get it to work for words, simply wrap <code>b</code> in another list so that it is interpreted correctly:</p>

<pre><code>model = gensim.models.Word2Vec([b],min_count=1,size=32)

print(model['buy'])
array([-0.01331611,  0.00496594, -0.00165093, -0.01444992,  0.01393849, ... ]
</code></pre>
",39,26,45918,2017-07-31 15:59:08,https://stackoverflow.com/questions/45420466/gensim-keyerror-word-not-in-vocabulary
Python: What is the &quot;size&quot; parameter in Gensim Word2vec model class,"<p>I have been struggling to understand the use of <code>size</code> parameter in the <code>gensim.models.Word2Vec</code></p>

<p>From the Gensim documentation, <code>size</code> is the dimensionality of the vector. Now, as far as my knowledge goes, word2vec creates a vector of the probability of closeness with the other words in the sentence for each word. So, suppose if my <code>vocab</code> size is 30 then how does it create a vector with the dimension greater than 30? Can anyone please brief me on the optimal value of <code>Word2Vec</code> size? </p>

<p>Thank you.</p>
","python, gensim, word2vec","<p><code>size</code> is, as you note, the dimensionality of the vector.</p>

<p>Word2Vec needs large, varied text examples to create its 'dense' embedding vectors per word. (It's the competition between many contrasting examples during training which allows the word-vectors to move to positions that have interesting distances and spatial-relationships with each other.)</p>

<p>If you only have a vocabulary of 30 words, word2vec is unlikely an appropriate technology. And if trying to apply it, you'd want to use a vector size much lower than your vocabulary size – ideally <em>much</em> lower. For example, texts containing many examples of each of tens-of-thousands of words might justify 100-dimensional word-vectors.</p>

<p>Using a higher dimensionality than vocabulary size would more-or-less guarantee 'overfitting'. The training could tend toward an idiosyncratic vector for each word – essentially like a 'one-hot' encoding – that would perform better than any other encoding, because there's no cross-word interference forced by representing a larger number of words in a smaller number of dimensions. </p>

<p>That'd mean a model that does about as well as possible on the Word2Vec internal nearby-word prediction task – but then awful on other downstream tasks, because there's been no generalizable relative-relations knowledge captured. (The cross-word interference is what the algorithm <em>needs</em>, over many training cycles, to incrementally settle into an arrangement where similar words <em>must</em> be similar in learned weights, and contrasting words different.) </p>
",21,9,21079,2017-08-01 18:12:40,https://stackoverflow.com/questions/45444964/python-what-is-the-size-parameter-in-gensim-word2vec-model-class
Gensim Doc2vec model clustering into K-means,"<p>I'm new to doc2vec and I hope some one of you can help me with this issue.
I've asked a number of people about this issue, but nobody knows the solution.</p>

<p>What I wanto to do is cluster Doc2vec result into k-means. Please see below the code.</p>

<pre><code>mbk = MiniBatchKMeans(n_clusters=3, init_size=400, batch_size=300, verbose=1).fit(model_dm.docvecs[range([2000])                                                                                                 
MiniBatchKMeans.predict(mbk,mbk.labels_ )
</code></pre>

<p>I'm getting this Error.</p>

<pre><code>TypeErrorTraceback (most recent call last)
&lt;ipython-input-19-fbc57a13bf4b&gt; in &lt;module&gt;()
      6 
      7 
----&gt; 8 mbk = MiniBatchKMeans(n_clusters=3, init_size=400, batch_size=300, verbose=1).fit(model_dm.docvecs[:2000])
      9 
     10 #model_dm.docvecs.doctag_syn0[2000]

/usr/local/lib64/python2.7/site-packages/gensim/models/doc2vec.pyc in __getitem__(self, index)
    351             return self.doctag_syn0[self._int_index(index)]
    352 
--&gt; 353         return vstack([self[i] for i in index])
    354 
    355     def __len__(self):

TypeError: 'slice' object is not iterable
</code></pre>
","python, k-means, gensim, doc2vec","<p>You are trying to cluster a single document vector (2001th vector to be precise) on this part of your code:</p>

<pre><code>.fit(model_dm.docvecs[2000]) 
</code></pre>

<p>I assume you want the first 2000 vectors? </p>

<p><strong>Edit:</strong></p>

<p>After looking at the gensim documentation couldn't see a way to get a slice of document vectors. But looking at the <a href=""https://github.com/RaRe-Technologies/gensim/blob/fbe6db0e29636cab4a919e8909df61091ca4c4c5/gensim/models/doc2vec.py#L342"" rel=""nofollow noreferrer"">source code</a> DocvecsArray accepts a single key (int or str) or a list of keys. With that you can get the first 2000 vectors using:</p>

<pre><code>.fit(model_dm.docvecs[range(2000)])
</code></pre>

<p>It doesn't look satisfying so I will fix my answer if I can find another way later.</p>

<p>Also bear in mind these are <em>not the first</em> 2000 vectors since gensim seems to store docvecs as a key:value pair and dictionaries are not ordered.</p>

<p><strong>Second Edit:</strong></p>

<p>K-means part of the code also needs to be fixed, you are calling <code>MiniBatchKMeans</code> class' <code>predict</code> function. And give the class instance (mbk) as an argument. You need to call the class instance's (in which case it is mbk) <code>predict</code> function if you need to predict anything else. Which I assume you don't.</p>

<p>You can get the assigned labels using the code below.</p>

<pre><code>mbk = MiniBatchKMeans(n_clusters=3, init_size=400, batch_size=300, verbose=1).fit(model_dm.docvecs[range(2000])
mbk.labels_
</code></pre>
",0,1,1106,2017-08-02 07:38:26,https://stackoverflow.com/questions/45454433/gensim-doc2vec-model-clustering-into-k-means
Gensim: word vectors encoding problems,"<p>After creating word vectors in Gensim 2.2.0 from plain English text files with IMDB movie ratings:</p>

<pre><code>import gensim, logging
import smart_open, os
from nltk.tokenize import RegexpTokenizer

VEC_SIZE = 300 
MIN_COUNT = 5
WORKERS = 4
data_path = './data/'
vectors_path = 'vectors.bin.gz'

class AllSentences(object):
    def __init__(self, dirname):
        self.dirname = dirname
        self.read_err_cnt = 0
        self.tokenizer = RegexpTokenizer('[\'a-zA-Z]+', discard_empty=True)

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            print(fname)
            for line in open(os.path.join(self.dirname, fname)):
                words = []     
                try:
                    for word in self.tokenizer.tokenize(line):
                        words.append(word)
                    yield words
                except:
                    self.read_err_cnt += 1

sentences = AllSentences(data_path) 
</code></pre>

<p>Training and saving model:</p>

<pre><code>model = gensim.models.Word2Vec(sentences, size=VEC_SIZE, 
                               min_count=MIN_COUNT, workers=WORKERS)
word_vectors = model.wv
word_vectors.save(vectors_path)
</code></pre>

<p>And then trying to load it back:</p>

<pre><code>vectors = KeyedVectors.load_word2vec_format(vectors_path,
                                                    binary=True,
                                                    unicode_errors='ignore')
</code></pre>

<p>I get '<strong><em>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0</em></strong>' exception (see below). I tried different combinations of 'encoding' parameters including <strong><em>'ISO-8859-1'</em></strong> and <strong><em>'Latin1'</em></strong>. Also different combinations of <strong><em>'binary=True/False'</em></strong>. Nothing helps - the same exception, no matter what parameters are used. What is wrong? How to make loading vectors work?</p>

<p>Exception:</p>

<pre><code>UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-64-f353fa49685c&gt; in &lt;module&gt;()
----&gt; 1 w2v = get_w2v_vectors()

&lt;ipython-input-63-cbbe0a76e837&gt; in get_w2v_vectors()
      3     vectors = KeyedVectors.load_word2vec_format(word_vectors_path,
      4                                                     binary=True,
----&gt; 5                                                     unicode_errors='ignore')
      6 
      7                                                 #unicode_errors='ignore')

D:\usr\anaconda\lib\site-packages\gensim\models\keyedvectors.py in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)
    204         logger.info(""loading projection weights from %s"", fname)
    205         with utils.smart_open(fname) as fin:
--&gt; 206             header = utils.to_unicode(fin.readline(), encoding=encoding)
    207             vocab_size, vector_size = map(int, header.split())  # throws for invalid file format
    208             if limit:

D:\usr\anaconda\lib\site-packages\gensim\utils.py in any2unicode(text, encoding, errors)
    233     if isinstance(text, unicode):
    234         return text
--&gt; 235     return unicode(text, encoding, errors=errors)
    236 to_unicode = any2unicode
    237 

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
","python, gensim","<p>If you save vectors using gensim's native <code>save()</code> method, you should load them with the native <code>load()</code> method. </p>

<p>If you want to load vectors using <code>load_word2vec_format()</code>, you'll need to save them with <code>save_word2vec_format()</code>. (You'll lose some information this way, such as the exact occurrence counts that would otherwise be inside the <code>KeyedVectors.vocab</code> dictionary items.)</p>
",3,2,1725,2017-08-02 10:37:59,https://stackoverflow.com/questions/45458493/gensim-word-vectors-encoding-problems
Gensim Doc2Vec model only generates a limited number of vectors,"<p>I am using gensim <strong>Doc2Vec</strong> model to generate my feature vectors. Here is the code I am using (I have explained what my problem is in the code):</p>

<pre><code>cores = multiprocessing.cpu_count()

# creating a list of tagged documents
training_docs = []

# all_docs: a list of 53 strings which are my documents and are very long (not just a couple of sentences)
for index, doc in enumerate(all_docs):
    # 'doc' is in unicode format and I have already preprocessed it
    training_docs.append(TaggedDocument(doc.split(), str(index+1)))

# at this point, I have 53 strings in my 'training_docs' list 

model = Doc2Vec(training_docs, size=400, window=8, min_count=1, workers=cores)

# now that I print the vectors, I only have 10 vectors while I should have 53 vectors for the 53 documents that I have in my training_docs list.
print(len(model.docvecs))
# output: 10
</code></pre>

<p>I am just wondering if I am doing a mistake or if there is any other parameter that I should set?</p>

<blockquote>
  <p><strong>UPDATE</strong>: I was playing with the <strong><em>tags</em></strong> parameter in <strong><em>TaggedDocument</em></strong>, and when I changed it to a mixture of text and numbers like: <em>Doc1, Doc2, ...</em> I see a different number for the count of generated vectors, but still I do not have the same number of feature vectors as expected.</p>
</blockquote>
","python, nlp, gensim, doc2vec","<p>Look at the actual tags it has discovered in your corpus:</p>

<pre><code>print(model.docvecs.offset2doctag)
</code></pre>

<p>Do you see a pattern?</p>

<p>The <code>tags</code> property of each document should be a <em>list of tags</em>, not a single tag. If you supply a simple string-of-an-integer, it will see it as a list-of-digits, and thus only learn the tags <code>'0'</code>, <code>'1'</code>, ..., <code>'9'</code>.</p>

<p>You could replace <code>str(index+1)</code> with <code>[str(index+1)]</code> and get the behavior you were expecting. </p>

<p>But, since your document IDs are just ascending integers, you can also just use plain Python ints as your doctags. This will save some memory, buy avoiding the creation of a lookup dict from string-tag to array-slot (int). To do this, replace the <code>str(index+1)</code> with <code>[index]</code>. (This starts the doc-IDs from <code>0</code> – which is a teensy bit more Pythonic, and also avoids wasting an unused <code>0</code> position in the raw array that holds the trained vectors.)</p>
",1,0,317,2017-08-02 17:46:00,https://stackoverflow.com/questions/45467699/gensim-doc2vec-model-only-generates-a-limited-number-of-vectors
Using gensim&#39;s Word2Vec with custom word-context pairs,"<p>I would like to use Gensim's implemented Word2Vec with a list of context-word pairs as an input instead of sentences. I originally thought that entering the manually created context-word pairs as sentences would be equivalent to entering the raw sentences and setting the window parameter to 1, but the two approaches yield different results. How does Gensim's Word2Vec calculate the context-word pairs of sentences, and how should I enter my manually created pairs as an input to the function?</p>
","python, gensim","<p>Do you mean the ""different results"" happen between providing the original sentences with a long <code>window</code>, versus the pairs with a <code>window=1</code>? </p>

<p>Assuming you're using skip-gram and <code>window=1</code>, you <em>can</em> simulate the pairs from longer sentences as your own two-token texts. That is:</p>

<p>The sentence...</p>

<pre><code>['A', 'B', 'C', 'D']
</code></pre>

<p>...would generate the <code>window=1</code> training pairs...</p>

<pre><code>[('B', 'A'), 
 ('A', 'B'), ('C', 'B'), 
 ('B', 'C'), ('D', 'C'), 
 ('C', 'D')]
</code></pre>

<p>And, you would get the same training-pairs if you instead fed Word2Vec the <code>window=1</code> sentences:</p>

<pre><code>['A', 'B']
['B', 'C']
['C', 'D']
</code></pre>

<p>Now, however, note that the relative word-frequencies have changed – and word frequencies can affect both frequent-word downsampling (as controlled by the <code>sample</code> parameter) and negative-example selection (in the default negative-sampling mode). That alone would mean results wouldn't be identical between methods. </p>

<p>Additionally, both those aspects of the algorithm (down-sampling &amp; negative-sampling) involve intentional randomness – and even if you try to deterministically-seed the random-number generator, if using more than one worker, OS thread scheduling can mean differing progress between the workers from run-to-run, and thus a different order of pair-consideration and randomness-usage. So again, each run (even with the exact same parameters, but using many worker threads) can give slightly different results, by design. </p>

<p>Finally, if using a <code>window</code> value greater than 1, the algorithm actually chooses, for each target word, to use some other window-size randomly chosen from 1 to your value. (This is done to effectively give nearer words more weight. This approaches the same effect as if some scaling-factor were applied to more distant words – but by doing less work overall, and thus speeds training.) </p>

<p>Thus if you were trying to simulate the word-pairs of a <code>window</code> value greater-than-1, you couldn't approximate the long-sentence behavior by creating the naive pairings of all within-window words as your new sentences. You'd have to perform a similar random window shrinkage. (And, you'd still have the volatile effect on overall word-frequencies.)</p>

<p>Still, assuming you're interested in this path because your true data may not be real, word-ordered natural-language sentences, none of may matter. You could try supplying your own synthetic pairs as sentences, and still get acceptable results on whatever your end-goal is – you should try it and see. There's nothing magically justified about the windowing or weighting processes of the original word2vec algorithm – it just happens to do something interesting. Other choices may be just as good, or better, for other kinds of corpuses.</p>
",3,2,1067,2017-08-03 08:19:04,https://stackoverflow.com/questions/45478607/using-gensims-word2vec-with-custom-word-context-pairs
What is the difference between wmd (word mover distance) and wmd based similarity?,"<p>I am using WMD to calculate the similarity scale between sentences. For example:</p>

<pre><code>distance = model.wmdistance(sentence_obama, sentence_president)
</code></pre>

<p>Reference: <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>

<p>However, there is also WMD based similarity method <code>(WmdSimilarity).</code></p>

<p>Reference: 
<a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"" rel=""nofollow noreferrer"">https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html</a></p>

<p>What is the difference between the two except the obvious that one is distance and another similarity? </p>

<p><strong>Update:</strong> Both are exactly the same except with their different representation. </p>

<pre><code>n_queries = len(query)
result = []
for qidx in range(n_queries):
    # Compute similarity for each query.
    qresult = [self.w2v_model.wmdistance(document, query[qidx]) for document in self.corpus]
    qresult = numpy.array(qresult)
    qresult = 1./(1.+qresult)  # Similarity is the negative of the distance.

    # Append single query result to list of all results.
    result.append(qresult)
</code></pre>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/docsim.py</a></p>
","nlp, nltk, gensim, word2vec, word-embedding","<p>I think with the 'update' you'e more-or-less answered your own question. </p>

<p>That one is a distance, and the other a similarity, is the only difference between the two calculations. As the notebook you link notes in the <a href=""https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html#Part-2:-Similarity-queries-using-WmdSimilarity"" rel=""noreferrer"">relevant section</a>:</p>

<blockquote>
  <p>WMD is a measure of distance. The similarities in WmdSimilarity are simply the negative distance. Be careful not to confuse distances and similarities. Two similar documents will have a high similarity score and a small distance; two very different documents will have low similarity score, and a large distance.</p>
</blockquote>

<p>As the code you've excerpted shows, the similarity measure being used there is not exactly the 'negative' distance, but scaled so all similarity values are from 0.0 (exclusive) to 1.0 (inclusive). (That is, a zero distance becomes a 1.0 similarity, but ever-larger distances become ever-closer to 0.0.)</p>
",5,4,2262,2017-08-04 08:59:17,https://stackoverflow.com/questions/45502464/what-is-the-difference-between-wmd-word-mover-distance-and-wmd-based-similarit
Is there a semantic similarity method that outperforms word2vec approach for semantic accuracy?,"<p>I am looking at various semantic similarity methods such as word2vec, word mover distance (WMD), and fastText. fastText is not better than Word2Vec as for as semantic similarity is concerned. WMD and Word2Vec have almost similar results. </p>

<p>I was wondering if there is an alternative which has outperformed the Word2Vec model for semantic accuracy? </p>

<p><strong>My use case:</strong>
<em>Finding word embeddings for two sentences, and then use cosine similarity to find their similarity.</em> </p>
","nlp, nltk, gensim, word2vec, fasttext","<p>Whether any technique ""outperforms"" another will depend highly on your training data, the specific metaparameter options you choose, and your exact end-task. (Even ""semantic similarity"" may have many alternate aspects depending on the application.)</p>

<p>There's no one way to go from word2vec word-vectors to a sentence/paragraph vector. You could add the raw vectors. You could average the unit-normalized vectors. You could perform some other sort of weighted-average, based on other measures of word-significance. So your implied baseline is unclear. </p>

<p>Essentially you have to try a variety of methods and parameters, for your data and goal, with your custom evaluation. </p>

<p>Word Mover's Distance <em>doesn't</em> reduce each text to a single vector, and the pairwise calculation between two texts can be expensive, but it has reported very good performance on some semantic-similarity tasks. </p>

<p>FastText is essentially word2vec with some extra enhancements and new modes. Some modes with the extras turned off are exactly the same as word2vec, so using FastText word-vectors in some wordvecs-to-textvecs scheme should closely approximate using word2vec word-vectors in the same scheme. Some modes might help the word-vector quality for some purposes, but make the word-vectors less effective inside a wordvecs-to-textvecs scheme. Some modes might make the word-vector better for sum/average composition schemes – you should look especially at the 'classifier' mode, which trains word-vecs to be good, when averaged, at a classification task. (To the extent you may have any semantic labels for your data, this might make the word-vecs more composable for semantic-similarity tasks.)</p>

<p>You may also want to look at the 'Paragraph Vectors' technique (available in gensim as <code>Doc2Vec</code>), or other research results that go by the shorthand names 'fastSent' or 'sent2vec'. </p>
",4,2,1618,2017-08-08 10:05:15,https://stackoverflow.com/questions/45565389/is-there-a-semantic-similarity-method-that-outperforms-word2vec-approach-for-sem
Setting max length of char n-grams for fastText,"<p>I want to compare word2vec and fasttext model based on this comparison tutorial. 
<a href=""https://github.com/jayantj/gensim/blob/fast_text_notebook/docs/notebooks/Word2Vec_FastText_Comparison.ipynb"" rel=""nofollow noreferrer"">https://github.com/jayantj/gensim/blob/fast_text_notebook/docs/notebooks/Word2Vec_FastText_Comparison.ipynb</a></p>

<p>According to this, the semantic accuracy of fastText model increase when we set the max length of char n-grams to zero, such that fastText starts to behave almost like to word2vec. It ignores the ngrams. </p>

<p>However, I can not find any formation on how to set this parameter while loading a fastText model. Any ideas on how to do this?</p>
","nlp, nltk, gensim, word2vec, fasttext","<p>The parameter is set at training time – and then the model is built using that parameter, and dependent on that parameter for interpretation. So you wouldn't typically change it upon loading an already-trained model, and there's no API in gensim (or the original FastText) to change the setting on an already-trained model. </p>

<p>(By looking at the source and tampering with the loaded model state directly, you <em>might</em> be able to approximate the effect of ignoring char-ngrams that had been trained – but that'd be a novel mode, not at all like the no-ngrams-trained mode evaluated in the notebook you've linked. It might generate interesting, or awful, results – no way to tell without trying it.)</p>
",2,0,1255,2017-08-08 13:03:45,https://stackoverflow.com/questions/45569142/setting-max-length-of-char-n-grams-for-fasttext
Semantic Similarity across multiple languages,"<p>I am using word embeddings for finding similarity between two sentences. Using word2vec, I also get a similarity measure if one sentence is in English and the other one in Dutch (though not very good). </p>

<p>So I started wondering if it's possible to compute the similarity between two sentences in two different languages (without an explicit translation), especially if the languages have some similarities (Englis/Dutch)?</p>
","nlp, nltk, gensim, word2vec","<p>Let's assume that your sentence-similarity scheme uses only word-vectors as an input – as in simple word-vector averaging schemes, or Word Mover's Distance. </p>

<p>It should be possible to do what you've suggested, provided that:</p>

<ul>
<li>you have good sets of word-vectors for each language's words</li>
<li>the coordinate spaces of the word-vectors are compatible, meaning the words for the exact-same things in both languages have nearly-identical coordinates (and other words with similar meanings have close coordinates)</li>
</ul>

<p>That second quality is not automatically assured. In fact, given the random initialization of word2vec models, and other randomization introduced by the algorithm/implementation, even subsequent training runs on the exact same data won't place words into the exact same places. So word-vectors trained on totally-separate English/Dutch corpuses won't likely place equivalent words at the same coordinates. </p>

<p>But, you can learn an algebraic-transformation between two spaces, based on certain anchor/reference word-pairs (that you know should have similar vectors). You can then apply that transformation to all words in one of the two sets, which results in you having vectors for those 'foreign' words within the comparable coordinate-space of the 'canonical' word-set. </p>

<p>In fact this very idea was used in one of the first word2vec papers:</p>

<p>""<a href=""https://arxiv.org/abs/1309.4168"" rel=""nofollow noreferrer"">Exploiting Similarities among Languages for Machine Translation</a>""</p>

<p>If you were to apply a similar transformation on one of your language word-vector sets, then use those transformed vectors as inputs to your sentence-vector scheme, those sentence-vectors would likely have some useful comparability to sentence-vectors in the other language, bootstrapped from word-vectors in the same coordinate-space. </p>

<p><strong>Update:</strong> There's a very interesting <a href=""https://arxiv.org/abs/1601.02502"" rel=""nofollow noreferrer"">recent paper</a> that manages to train word-vectors in multiple languages simultaneously, using a corpus that includes both raw sentences in each single language, and a (smaller) set of aligned-sentences that are known to mean the same in both languages. Gensim doesn't yet support this mode, but there's <a href=""https://groups.google.com/d/msg/gensim/zksGwKHnIUA/7lde13FbAgAJ"" rel=""nofollow noreferrer"">discussion of supporting it</a> in a future refactor.  </p>
",7,3,3702,2017-08-08 14:38:45,https://stackoverflow.com/questions/45571295/semantic-similarity-across-multiple-languages
How to create a model using trained models?,"<p>I have created two models using gensim word2vec. Now I want to merge these two models in a way that I get the union of these two models.</p>

<p>Eg: </p>

<ul>
1. Model one has following vocabulary
</ul>

<pre><code>{""Hi"", ""Hello"", ""World""}
</code></pre>

<ul>
2. Model two has the following vocabulary
</ul>

<pre><code>{""Hi"", ""King"", ""Hello"", ""Human""}
</code></pre>

<p>Now I want to use these two models and create a new model which will have the following vocabulary</p>

<pre><code>{""Hi"", ""Hello"", ""World"", ""King"", ""Human""}
</code></pre>
","machine-learning, nlp, deep-learning, gensim, word2vec","<p>Common word2vec libraries, like gensim, do not provide such a facility for merging models. Inherently, the coordinates of words within-a-model are only comparable, in terms of distances and directions, with other words in-the-same-model – only by bring incrementally trained together do they get nudged to meaningful relative positions. </p>

<p>The most straightforward approach is, as @mujiga's answer suggests, to combine the two training corpuses which include all desired words, and train a new model on the combined texts. (And ideally, you'd shuffle the two corpuses together, rather than simply concatenate them, so that no words appear only in the beginning or end of the full set-of-texts.)</p>

<p>A more complicated approach is possible, when there are a lot of overlapping words. You can pick one of the two ""spaces"" as the coordinate-system you'd like to retain – probably the one with more words, having been trained on more texts. Call that the 'reference' model. </p>

<p>You would take a large number of the words (maybe all) that are shared between the two models, and learn a 'translation' operation that projects those words' coordinates in the smaller model to roughly the right places for those same words in the reference model. (This is itself typically a mathematical optimization problem.) Finally, you'd use that translation operation on the non-shared words, to convert coordinates of the smaller model into the reference-model coordinate space – and then construct a new data structure that includes all the original reference vectors, plus the translated vectors. </p>

<p>This is the technique used by <a href=""https://arxiv.org/abs/1309.4168"" rel=""nofollow noreferrer"">one of the original word2vec papers</a> for machine translation. It's also mentioned in <a href=""http://arxiv.org/abs/1506.06726"" rel=""nofollow noreferrer"">section 2.2 of the skip-thoughts paper</a> as a way to leverage words from another source when they don't appear in a local corpus. There's currently (August 2017) <a href=""https://github.com/RaRe-Technologies/gensim/pull/1434"" rel=""nofollow noreferrer"">some work-in-progress to add a facility for learning-the-translation</a> in gensim, but it's not yet fully-tested/documented or part of any formal release.</p>

<p>But really: the safe and straightforward thing is to train a new model on a common corpus. </p>
",2,2,103,2017-08-22 07:03:01,https://stackoverflow.com/questions/45810954/how-to-create-a-model-using-trained-models
embedded vectors doesn&#39;t converge in gensim,"<p>I am training a word2vec model using gensim on 800k browser useragent. My dictionary size is between 300 and 1000 depending on the word frequency limit.
I am looking at few embedding vectors and similarities to see if the algorithm has been converged.
here is my code:</p>

<pre><code>wv_sim_min_count_stat={}
window=7;min_count=50;worker=10;size=128
total_iterate=1000
from copy import copy
for min_count in [50,100,500]:
    print(min_count)

    wv_sim_min_count_stat[min_count]={}
    model=gensim.models.Word2Vec(size=size,window=window,min_count=min_count,iter=1,sg=1)
    model.build_vocab(ua_parsed)


    wv_sim_min_count_stat[min_count]['vocab_counts']=[len(ua_parsed),len(model.wv.vocab),len(model.wv.vocab)/len(ua_parsed)]
    wv_sim_min_count_stat[min_count]['test']=[]

    alphas=np.arange(0.025,0.001,(0.001-0.025)/(total_iterate+1))
    for i in range(total_iterate):
        model.train(ua_parsed,total_examples=model.corpus_count,
                    epochs=model.iter,start_alpha=alphas[i],end_alpha=alphas[i+1])

        wv_sim_min_count_stat[min_count]['test'].append(
        (copy(model.wv['iphone']),copy(model.wv['(windows']),copy(model.wv['mobile']),copy(model.wv['(ipad;']),copy(model.wv['ios']),
         model.similarity('(ipad;','ios')))
</code></pre>

<p>unfortunately even after 1000 epochs there is no sign of convergence in embedding vectors. for example I plot embedding of the first dimension of '(ipad''s embedding vector vs number of epochs below:</p>

<pre><code>for min_count in [50,100,500]:
    plt.plot(np.stack(list(zip(*wv_sim_min_count_stat[min_count]['test']))[3])[:,1],label=str(min_count))

plt.legend() 
</code></pre>

<p><a href=""https://i.sstatic.net/dfsJI.png"" rel=""nofollow noreferrer"">embedding of '(ipad' vs number of epochs</a></p>

<p>I looked at many blogs and papers and it seems nobody trained the word2vec beyond 100 epochs. What I am missing here?</p>
","python, gensim, convergence, word-embedding","<p>Your dataset, user-agent strings, may be odd for word2vec. It's not natural-language; it might not have the same variety of co-occurences that causes word2vec to do useful things for natural language. (Among other things, a dataset of 800k natural-language sentences/docs would tend to have a much larger vocabulary than just ~1,000 words.)</p>

<p>Your graphs do look like they're roughly converging, to me. In each case, as the learning-rate <code>alpha</code> decreases, the dimension magnitude is settling towards a final number. </p>

<p>There is no reason to expect the magnitude of a particular dimension, of a particular word, would reach the <em>same</em> absolute value in different runs. That is: you shouldn't expect the three lines you're plotting, under different model parameters, to all tend towards the same final value. </p>

<p>Why not? </p>

<p>The algorithm includes random initialization, randomization-during-training (in negative-sampling or frequent-word downsampling), and then in its multi-threading some arbitrary re-ordering of training-examples due to OS thread-scheduling jitter. As a result, even with exactly the same metaparameters, and the same training corpus, a single word could land at different coordinates in subsequent training runs. But, its distances and orientation with regard to other words <em>in the same run</em> should be about-as-useful. </p>

<p>With different metaparameters like <code>min_count</code>, and thus a different ordering of surviving words during initialization, and then wildly different random-initialization, the final coordinates per word could be especially different. There is no inherent set-of-best-final-coordinates for any word, even with regard to a particular fixed corpus or initialization. There's just coordinates that work increasingly well, through a particular randomized initialization/training session, balanced over all the other co-trained words/examples.</p>
",1,2,734,2017-08-22 19:28:32,https://stackoverflow.com/questions/45825532/embedded-vectors-doesnt-converge-in-gensim
Issue with gensim.models.Phrases,"<pre><code>from gensim.parsing import PorterStemmer
from gensim.models import Word2Vec, Phrases

class SentenceClass(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            with open(os.path.join(self.dirname,fname), 'r') as myfile:
                doc = myfile.read().replace('\n', ' ')
                for sent in tokenize.sent_tokenize(doc.lower()):
                    yield [Stemming.stem(word)\
                    for word in word_tokenize(re.sub(""[^A-Za-z]"", "" "",sent))\
                    if word not in stopwords]
</code></pre>

<p>Now of the two approaches:<br/>
1)</p>

<pre><code>model = Word2Vec(SentenceClass(data_dir_path), size=100, window=5, min_count=1, workers=4)
</code></pre>

<p>The above one runs really fine with no warning</p>

<p>2)</p>

<pre><code>bigram_transformer = Phrases(SentenceClass(data_dir_path), min_count=1)
model = Word2Vec(bigram_transformer[SentenceClass(data_dir_path)], size=100, window=5, min_count=1, workers=4)
</code></pre>

<p>produces the warning:</p>

<pre><code>WARNING:gensim.models.word2vec:train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).
WARNING:gensim.models.word2vec:supplied example count (0) did not equal expected count (30)
</code></pre>

<p>Now I do understand the difference between generator and iterators, and I am passing iterators, which is verified by printing below commands multiple times:</p>

<pre><code>print(list(SentenceClass(data_dir_path)))
print(list(SentenceClass(data_dir_path)))
print(list(bigram_transformer[SentenceClass(data_dir_path)]))
print(list(bigram_transformer[SentenceClass(data_dir_path)]))
</code></pre>

<p>And it prints thing fine, but I am still not sure why the warning of ""empty iterator"" for the second case, am I missing something here ? </p>
","python, nlp, gensim","<p>I realised that Phrases &amp; phrases are both making a generator only and the following class was required</p>

<pre><code>from gensim.models import Word2Vec, Phrases, phrases

class PhraseItertor(object):

    def __init__(self, my_phraser, data):
        self.my_phraser, self.data = my_phraser, data

    def __iter__(self):
        return self.my_phraser[self.data]

my_sentences = SentenceClass(data_dir_path)
my_phrases = Phrases(my_sentences, min_count=1)
bigram = phrases.Phraser(my_phrases)
my_corpus = PhraseItertor(bigram,my_sentences)

model = Word2Vec(my_corpus, size=100, window=5, min_count=1, workers=4)
</code></pre>
",1,2,940,2017-08-23 18:52:07,https://stackoverflow.com/questions/45847370/issue-with-gensim-models-phrases
Gensim word2vec WMD similarity dictionary,"<p>I'm using word2vec on a 1 million abstracts dataset (2 billion words). To find most similar documents, I use the <code>gensim.similarities.WmdSimilarity</code> class. When trying to retrieve the best match using <code>wmd_similarity_index[query]</code>, the calculation spends most of its time building a dictionary. Here is a piece of log:</p>

<pre><code>2017-08-25 09:45:39,441 : INFO : built Dictionary(127 unique tokens: ['empirical', 'model', 'estimating', 'vertical', 'concentration']...) from 2 documents (total 175 corpus positions)                                                        
2017-08-25 09:45:39,445 : INFO : adding document #0 to Dictionary(0 unique tokens: [])          
</code></pre>

<p>What does this part ? Is it dependent on the query ? Is there a way to do these calculations once for all ?</p>

<p><strong>EDIT:</strong> training and scoring phases in my code:</p>

<p>Training and saving to disk:</p>

<pre><code>w2v_size = 300
word2vec = gensim.models.Word2Vec(texts, size=w2v_size, window=9, min_count=5, workers=1, sg=1, hs=1, iter=20) # sg=1 means skip gram is used 
word2vec.save(utils.paths.PATH_DATA_GENSIM_WORD2VEC)
corpus_w2v_wmd_index = gensim.similarities.WmdSimilarity(texts, word2vec.wv)
corpus_w2v_wmd_index.save(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)
</code></pre>

<p>Loading and scoring:</p>

<pre><code>w2v = gensim.models.Word2Vec.load(utils.paths.PATH_DATA_GENSIM_WORD2VEC)
words = [t for t in proc_text if t in w2v.wv]
corpus_w2v_wmd_index = gensim.similarities.docsim.Similarity.load(utils.paths.PATH_DATA_GENSIM_CORPUS_WORD2VEC_WMD_INDEX)
scores_w2v = np.array(corpus_w2v_wmd_index[words])  
</code></pre>
","python, nlp, cpu-word, gensim, word2vec","<p>The ""Word Mover's Distance"" calculation is relatively expensive – for each pairwise document comparison, it searches for an optimal 'shifting' of semantic positions, and that shifting is itself dependent on the pairwise simple-distances between all words of each compared document. </p>

<p>That is, it involves far more calculation than a simple cosine-distance between two high-dimensional vectors, and it involves more calculation the longer the two documents are. </p>

<p>There isn't much that could be pre-calculated, from the <code>texts</code> corpus, until the query's words are known. (Each pairwise calculation depends on the query's words, and their simple-distances to each corpus document's words.)</p>

<p>That said, there are some optimizations the gensim <code>WmdSimilarity</code> class doesn't yet do. </p>

<p>The original WMD paper described a quicker calculation that could help eliminate corpus texts that couldn't possibly be in the top-N most-WMD-similar results. Theoretically, the gensim <code>WmdSimilarity</code> could also implement this optimization, and give quicker results, at least when initializing the <code>WmdSimilarity</code> with the <code>num_best</code> parameter. (Without it, every query returns all WMD-similarity-scores, so this optimization wouldn't help.)</p>

<p>Also, for now the <code>WmdSimilarity</code> class just calls <code>KeyedVectors.wmdistance(doc1, doc2)</code> for every query-to-corpus-document pair, as raw texts. Thus the pairwise simple-distances from all <code>doc1</code> words to <code>doc2</code> words will be recalculated each time, even if many pairs repeat across the corpus. (That is, if 'apple' is in the query and 'orange' is in every corpus doc, it will still calculate the 'apple'-to-'orange' distance repeatedly.) </p>

<p>So, some caching of interim values might help performance. For example, with a query of 1000 words, and a vocabulary of 100,000 words among all corpus documents, the <code>((1000 * 100,000) / 2)</code> 50 million pairwise word-distances could be precalculated once, using 200MB, then shared by all subsequent WMD-calculations. To add this optimization would require a cooperative refactoring of <code>WmdSimilarity.get_similarities()</code> and <code>KeyedVectors.wmdistance()</code>.</p>

<p>Finally, Word2Vec/Doc2Vec applications don't necessarily require or benefit much from stop-word removal or stemming. But because the expense of WMD calculation grows with document and vocabulary size, anything that shrinks effective document sizes could help performance. So various ways of discarding low-value words, or coalescing similar words, may be worth considering when using WMD on large document sets.  </p>
",8,4,4038,2017-08-25 07:51:09,https://stackoverflow.com/questions/45876711/gensim-word2vec-wmd-similarity-dictionary
Multi-dimensional documents with Gensim,"<p>I'm working on a document comparison engine / search engine. I'm currently using it as follows...</p>

<pre><code>search_doc = [""test search""]
documents = [""doc 1 text"", ""doc 2 text"", ""doc 3 text"", ""...""]
</code></pre>

<p>And then comparing the results.</p>

<p>What I would like to do (in the simplest terms possible) is have multi-dimensional documents (a document that has multiple dimensions, rather than just the ""document"")... for example..</p>

<pre><code>documents = {
                { ""doc 1 title"", ""doc 1 body"", ""doc 1 tags"" },
                { ""doc 2 title"", ""doc 2 body"", ""doc 2 tags"" },
                { ""doc 3 title"", ""doc 3 body"", ""doc 3 tags"" }
                { ... }
            }
</code></pre>

<p>And also be able to weight the results (for example, title is 0.6, body is 0.4, etc).</p>

<p>My question is... is there a way to do this within Gensim, or do I need to create a separate document for each meta item of the document (for example, comparing to each meta item (title, body, tags) as a separate document, and then combining weights after the fact using the document key/id? </p>

<p>I'm not sure i'm doing a good job of explaining this, but please let me know if I can improve my question.</p>

<p>Thank you.</p>
","python, machine-learning, tensorflow, gensim, tf-idf","<p>Gensim's major algorithms (like LDA or Doc2Vec) are trained from distinct individual documents, with no idea about sub-fields or composable sub-parts. So you'll have to model that yourself. </p>

<p>You should create a separate document per field, if your end-goals require the separate scoring per field. </p>

<p>You may also still want to create a full document composed of all fields. </p>

<p>You then might train a model on all full-docs and fields combined, or a separate model per type-of-doc. </p>

<p>How you might want to compare, deduplicate, or combine the scores of fields versus documents would be very dependent on your data and project goals, and your own experiments and custom evaluations. (For example, perhaps your users are most satisfied with body-matches, so any similarities in that field should have extra weight in your display – and so forth.)</p>
",1,1,127,2017-08-28 20:31:20,https://stackoverflow.com/questions/45926917/multi-dimensional-documents-with-gensim
Gensim Doc2vec finalize_vocab Memory Error,"<p>I am trying to train a Doc2Vec model using gensim with 114M unique documents/labels and vocab size of around 3M unique words. I have 115GB Ram linux machine on Azure.
When I run build_vocab, the iterator parses all files and then throws memory error as listed below.</p>

<pre><code>    Traceback (most recent call last):
  File ""doc_2_vec.py"", line 63, in &lt;module&gt;
    model.build_vocab(sentences.to_array())
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 579, in build_vocab
    self.finalize_vocab(update=update)  # build tables &amp; arrays
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/word2vec.py"", line 752, in finalize_vocab
    self.reset_weights()
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 662, in reset_weights
    self.docvecs.reset_weights(self)
  File ""/home/meghana/.local/lib/python2.7/site-packages/gensim/models/doc2vec.py"", line 390, in reset_weights
    self.doctag_syn0 = empty((length, model.vector_size), dtype=REAL)
MemoryError
</code></pre>

<p>My code-</p>

<pre><code>import parquet
import json
import collections
import multiprocessing


# gensim modules
from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec

class LabeledLineSentence(object):
    def __init__(self, sources):
        self.sources = sources   
        flipped = {}

    def __iter__(self):
        for src in self.sources:
            with open(src) as fo:
               for row in parquet.DictReader(fo, columns=['Id','tokens']):
                    yield LabeledSentence(utils.to_unicode(row['tokens']).split('\x01'), [row['Id']])

## list of files to be open ##
sources =  glob.glob(""/data/meghana_home/data/*"")
sentences = LabeledLineSentence(sources)

#pre = Doc2Vec(min_count=0)
#pre.scan_vocab(sentences)
""""""
for num in range(0, 20):
    print('min_count: {}, size of vocab: '.format(num), pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/700)
    print(""done"")
""""""

NUM_WORKERS = multiprocessing.cpu_count()
NUM_VECTORS = 300
model = Doc2Vec(alpha=0.025, min_alpha=0.0001,min_count=15, window=3, size=NUM_VECTORS, sample=1e-4, negative=10, workers=NUM_WORKERS) 
model.build_vocab(sentences)
print(""built vocab......."")
model.train(sentences,total_examples=model.corpus_count, epochs=10)
</code></pre>

<p>Memory usage as per top is-</p>

<p><a href=""https://i.sstatic.net/xarz0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xarz0.png"" alt=""enter image description here""></a></p>

<p>Can someone please tell me how much is the expected memory? What is better option- Adding swap space and slow the process or add more memory so that cost of cluster might eventually be equivalent.
What vectors gensim stores in memory? Any flag that i am missing for memory efficient usage.</p>
","python, nlp, gensim, doc2vec","<p>114 million doctags will require at least <code>114,000,000 doctags * 300 dimensions * 4 bytes/float = 136GB</code> just to store the raw doctag-vectors during training. </p>

<p>(If the doctag keys <code>row['Id']</code> are strings, there'll be extra overhead for remembering the string-to-int-index mapping dict. If the doctag keys are raw ints from 0 to 114 million, that will avoid filling that dict. If the doctag keys are raw ints, but include any int higher than 114 million, the model will attempt to allocate an array large enough to include a row for the largest int – even if many other lower ints are unused.)</p>

<p>The raw word-vectors and model output-layer (<code>model.syn1</code>) will require about another 8GB, and the vocabulary dictionary another few GB. </p>

<p>So you'd ideally want more addressable memory, or a smaller set of doctags.</p>

<p>You mention a 'cluster', but gensim <code>Doc2Vec</code> does not support multi-machine distribution.</p>

<p>Using swap space is generally a bad idea for these algorithms, which can involve a fair amount of random access and thus become very slow during swapping. But for the case of Doc2Vec, you <em>can</em> set its doctags array to be served by a memory-mapped file, using the <code>Doc2Vec.__init__()</code> optional parameter <code>docvecs_mapfile</code>. In the case of each document having a single tag, and those tags appearing in the same ascending order on each repeated sweep through the training texts, performance <em>may</em> be acceptable. </p>

<p>Separately:</p>

<p>Your management of training iterations and the <code>alpha</code> learning-rate is buggy. You're achieving 2 passes over the data, at <code>alpha</code> values of 0.025 and 0.023, even though each <code>train()</code> call is attempting a default 5 passes but then just getting a single iteration from your non-restartable <code>sentences.to_array()</code> object. </p>

<p>You should aim for more passes, with the model managing <code>alpha</code> from its initial-high to default final-tiny <code>min_alpha</code> value, in fewer lines of code. You need only call <code>train()</code> once unless you're absolutely certain you need to do extra steps between multiple calls. (Nothing shown here requires that.)</p>

<p>Make your <code>sentences</code> object a true iterable-object that can be iterated over multiple times, by changing <code>to_array()</code> to <code>__iter__()</code>, then passing the <code>sentences</code> alone (rather than <code>sentences.to_array()</code>) to the model. </p>

<p>Then call <code>train()</code> once with this multiply-iterable object, and let it do the specified number of iterations with a smooth <code>alpha</code> update from high-to-low. (The default inherited from <code>Word2Vec</code> is 5 iterations, but 10 to 20 are more commonly used in published <code>Doc2Vec</code> work. The default <code>min_alpha</code> of 0.0001 should hardly ever be changed.)</p>
",1,1,981,2017-08-29 16:13:47,https://stackoverflow.com/questions/45943832/gensim-doc2vec-finalize-vocab-memory-error
How is the Gensim doesnt_match function working?,"<p>Gensim implements a function called ""doesnt_match"" that return an outlier word from a list.</p>

<p>The function is called on a wordvector object.</p>

<p>model.wv.doesnt_match(""breakfast cereal dinner lunch"".split())
'cereal'</p>

<p>The documentation is not specifying how this function really work (what is the computation background)</p>

<p>Anyone knows ?</p>
","python, gensim, word2vec","<p>You can review the <a href=""https://github.com/RaRe-Technologies/gensim/blob/1a73e4f110e0f199d39f9e051cbc081c4e2bab46/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer""><code>doesnt_match()</code> method's source code</a> – it's only 11 lines –
for the full details.</p>

<p>But the gist of it is...</p>

<ul>
<li>take the mean of all the word-vectors – a sort of 'center' for all candidates</li>
<li>calculate the cosine-distance from that center to each word – this is the dot-product between unit-normalized versions of each relevant vector</li>
<li>return the single word with the highest cosine-distance from that mean</li>
</ul>
",4,2,2736,2017-08-29 21:22:30,https://stackoverflow.com/questions/45948533/how-is-the-gensim-doesnt-match-function-working
Optimizing Gensim word mover&#39;s distance function for speed (wmdistance),"<p>I am using <code>gensim</code> <code>wmdistance</code> for calculating similarity between a reference sentence and 1000 other sentences. </p>

<pre><code>    model = gensim.models.KeyedVectors.load_word2vec_format(
     'GoogleNews-vectors-negative300.bin', binary=True)
    model.init_sims(replace=True)  

    reference_sentence = ""it is a reference sentence""
    other_sentences = [1000 sentences]
    index = 0
    for sentence in other_sentences: 
      distance [index] = model.wmdistance(refrence_sentence, other_sentences)
      index = index + 1
</code></pre>

<p>According to <code>gensim</code> <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">source code</a>, <code>model.wmdistance</code> returns the following:</p>

<pre><code>emd(d1, d2, distance_matrix)
</code></pre>

<p>where </p>

<pre><code>d1 =  # Compute nBOW representation of reference_setence.
d2 =  # Compute nBOW representation of other_sentence (one by one).
distance_matrix = see the source code as its a bit too much to paste it here.
</code></pre>

<p>This code is inefficient in two ways for my use case. </p>

<p>1) For the reference sentence, it is repeatedly calculating d1 (1000 times) for the distance function <code>emd(d1, d2, distance_matrix)</code>.</p>

<p>2) This distance function is called by multiple users from different points which repeat this whole process of <code>model.wmdistance(doc1, doc2)</code> for the same other_sentences and it is computationally expensive. For this 1000 comparisons, it takes around 7-8 seconds. </p>

<p>Therefore, I would like to isolate the two tasks. The final calculation of distance: <code>emd(d1, d2, distance_matrix)</code> and the preparation of these inputs: d1, d2, and distance matrix. As distance matrix depends on both so at least its input preparation should be isolated from the final matrix calculation. </p>

<p>My initial plan is to create three customized functions: </p>

<pre><code>d1 = prepared1(reference_sentence)
d2 = prepared2(other_sentence)
distance_matrix inputs = prepare inputs 
</code></pre>

<p>Is it possible to do this with this <code>gensim</code> function or should I just go my own customized version? Any ideas and solutions to deal with this problem in a better way?</p>
","python, nlp, nltk, gensim, word2vec","<p>You are right to observe that this code could be refactored &amp; optimized to avoid doing repetitive operations, especially in the common case where one reference/query doc is evaluated against a larger set of documents. (Any such improvements would also be a welcome contribution back to gensim.)</p>

<p>Simply preparing single documents outside the calculation might not offer a big savings; in each case, all word-to-word distances between the two docs must be calculated. It might make sense to precalculate a larger <code>distance_matrix</code> (to the extent that the relevant vocabulary &amp; system memory allows) that includes all words needed for many pairwise WMD calculations. </p>

<p>(As tempting as it might be to precalculate all word-to-word distances, with a vocabulary of 3 million words like the <code>GoogleNews</code> vector-set, and mere 4-byte float distances, storing them all would take at least 18TB. So calculating distances for relevant words, on manageable batches of documents, may make more sense.)</p>

<p>A possible way to start would be to create a variant of <code>wmdistance()</code> that explicitly works on one document versus a set-of-documents, and can thus combine the creation of histograms/distance-matrixes for many comparisons at once. </p>

<p>For the common case of not needing <em>all</em> WMD values, but just wanting the top-N nearest results, there's an optimization described in the original WMD paper where another faster calculation (called there 'RWMD') can be used to deduce when there's no chance a document could be in the top-N results, and thus skip the full WMD calculation entirely for those docs.</p>
",2,3,2609,2017-08-30 12:38:55,https://stackoverflow.com/questions/45960671/optimizing-gensim-word-movers-distance-function-for-speed-wmdistance
gensim doc2vec &quot;intersect_word2vec_format&quot; command,"<p>Just reading through the doc2vec commands on the gensim page. </p>

<p>I am curious about  the command""intersect_word2vec_format"" . </p>

<p>My understanding of this command is it lets me inject vector values from a pretrained word2vec model into my doc2vec model and then train my doc2vec model using the pretrained word2vec values rather than generating the word vector values from my document corpus. The result is that I get a more accurate doc2vec model because I am using pretrained w2v values which was generated from a much larger corpus of data compared to my relatively small document corpus. </p>

<p>Is my understanding of this command correct or not even close?  ;-) </p>
","nlp, gensim, doc2vec","<p>Yes, the <code>intersect_word2vec_format()</code> will let you bring vectors from an external file into a model that's already had its own vocabulary initialized (as if by <code>build_vocab()</code>). That is, it will only load those vectors for which there are already words in the local vocabulary. </p>

<p>Additionally, it will by default <em>lock</em> those loaded vectors against any further adjustment during subsequent training, though other words in the pre-existing vocabulary may continue to update. (You can change this behavior by supplying a <code>lockf=1.0</code> value instead of the default 0.0.)</p>

<p>However, this is best considered an experimental function and what, if any, benefits it might offer will depend on lots of things specific to your setup. </p>

<p>The PV-DBOW Doc2Vec mode, corresponding to the <code>dm=0</code> parameter, is often a top-performer in speed and doc-vector quality, and doesn't use or train word-vectors at all – so any pre-loading of vectors won't have any effect. </p>

<p>The PV-DM mode, enabled by the default <code>dm=1</code> setting, trains any word-vectors it needs simultaneous with doc-vector training. (That is, there's no separate phase where word-vectors are created first, and thus for the same <code>iter</code> passes, PV-DM training takes the same amount of time whether word-vectors start with default random values, or are pre-loaded from elsewhere.) Pre-seeding the model with some word-vectors from elsewhere might help or hurt final quality – it's likely to depend on the specifics of your corpus, meta-parameters, and goals, and whether those external vectors represent word-meanings in sync with the current corpus/goal.</p>
",13,2,5280,2017-09-02 11:26:06,https://stackoverflow.com/questions/46013294/gensim-doc2vec-intersect-word2vec-format-command
How to find most similar terms/words of a document in doc2vec?,"<p>I have applied Doc2vec to convert documents into vectors.After that, I used the vectors in clustering and figured out the 5 nearest/most similar document to the centroid of each cluster. Now I need to find the most dominant or important terms of these documents so that I can figure out the characteristics of each cluster. 
My question is is there any way to figure out the most dominat or simlar terms/word of a document in Doc2vec . I am using python's gensim package for the Doc2vec implementaton </p>
","python, cluster-analysis, gensim, word2vec, doc2vec","<p>To find out the most dominant words of your clusters, you can use any of these two classic approaches. I personally found the second one very efficient and effective for this purpose. </p>

<ul>
<li><p>Latent Drichlet Allocation (LDA): A topic modelling algorithm that will give you a set of topic given a collection of documents. You can treat the set of similar documents in the clusters as one document and apply LDA to generate the topics and see topic distributions across documents.</p></li>
<li><p>TF-IDF: TF-IDF calculate the importance of a word to a document given a collection of documents. Therefore, to find the most important keywords/ngrams, you can calculate TF-IDF for every word that appears in the documents. The words with the highest TF-IDF then are you keywords. So: </p>

<ul>
<li>calculate IDF for every single word that appears in the documents based on the number of documents that contain that keyword </li>
<li>concatenate the text of the similar documents (I 'd call it a super-document) and then calculate TF for each word that appears in this super-document</li>
<li>calculate TF*IDF for every word... and then TA DAAA... you have your keywords associated with each cluster. </li>
</ul>

<p>Take a look at Section 5.1 here for more details on the use of <a href=""https://pdfs.semanticscholar.org/9f3f/6f65344da1bd61f1311ab134c1a7bfcd0741.pdf"" rel=""nofollow noreferrer"">TF-IDF</a>.</p></li>
</ul>
",1,0,3461,2017-09-05 05:23:46,https://stackoverflow.com/questions/46047506/how-to-find-most-similar-terms-words-of-a-document-in-doc2vec
word2vec training procedure clarification,"<p>I'm trying to learn the skip-gram model within word2vec, however I'm confused by some of the basic concepts.  To start, here is my current understanding of the model motivated with an example.  I am using Python <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim</a> as I go.</p>

<p>Here I have a corpus with three sentences.</p>

<pre><code>sentences = [
    ['i', 'like', 'cats', 'and', 'dogs'],
    ['i', 'like', 'dogs'],
    ['dogs', 'like', 'dogs']
]
</code></pre>

<p>From this, I can determine my vocabulary, <code>V = ['and', 'cats', 'dogs', 'i', 'like']</code>.</p>

<p>Following <a href=""https://arxiv.org/pdf/1310.4546.pdf"" rel=""nofollow noreferrer"">this paper</a> by Tomas Mikolov (and others)</p>

<blockquote>
  <p>The basic Skip-gram formulation defines p(w_t+j |w_t) using the softmax
  function:</p>
</blockquote>

<p><a href=""https://i.sstatic.net/hza7Q.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hza7Q.gif"" alt=""skipgram""></a></p>

<blockquote>
  <p>where v_w and v′_w are the “input” and “output” vector representations
  of w, and W is the number of words in the vocabulary.</p>
</blockquote>

<p>To my understanding, the skip-gram model involves two matrices (I'll call them <em>I</em> and <em>O</em>) which are the vector representations of ""input/center"" words and the vector representation of ""output/context"" words.  Assuming d = 2 (vector dimension or 'size' as its called in genism), <em>I</em> should be a 2x5 matrix and <em>O</em> should be a 5x2 matrix.  At the start of the training procedure, these matrices are filled with random values (yes?).  So we might have</p>

<pre><code>import numpy as np
np.random.seed(2017)

I = np.random.rand(5,2).round(2)  # 5 rows by 2 cols
[[ 0.02  0.77] # and
 [ 0.45  0.12] # cats
 [ 0.93  0.65] # dogs
 [ 0.14  0.23] # i
 [ 0.23  0.26]] # like

O = np.random.rand(2,5).round(2)  # 2 rows by 5 cols
  #and  #cats #dogs  #i   #like 
[[ 0.11  0.63  0.39  0.32  0.63]
 [ 0.29  0.94  0.15  0.08  0.7 ]]
</code></pre>

<p>Now if I want to calculate the probability that the word ""dogs"" appears in the context of ""cats"" I should do</p>

<p>exp([0.39, 0.15] * [0.45  0.12])/(...) = (0.1125)/(...)</p>

<p>A few questions on this:</p>

<ol>
<li>Is my understanding of the algorithm correct thus far?</li>
<li>Using genism, I can train a model on this data using</li>
</ol>

<p>&nbsp;</p>

<pre><code>import gensim
model = gensim.models.Word2Vec(sentences, sg = 1, size=2, window=1, min_count=1)
model.wv['dogs']  # array([ 0.06249372,  0.22618999], dtype=float32)
</code></pre>

<p>For the array given, is that the vector for ""dogs"" in the Input matrix or the Output matrix?  Is there a way to view both matrices in the final model?</p>

<ol start=""3"">
<li>Why does <code>model.wv.similarity('cats','cats')</code> = 1?  I thought this should be closer to 0, since the data would indicate that the word ""cats"" is unlikely to occur in the context of the word ""cats"".</li>
</ol>
","gensim, word2vec","<p>(1) Generally, yes, but:</p>

<p>The <em>O</em> output matrix – more properly understood as the weights from the neural-network's hidden layer, to a number of output nodes – is interpreted differently whether using 'negative sampling' ('NS') or 'hierarchical softmax' ('HS') training. </p>

<p>In practice in both <em>I</em> and <em>O</em> are <em>len(vocab)</em> rows and <em>vector-size</em> columns. (<em>I</em> is the <code>Word2Vec</code> <code>model</code> instance's <code>model.wv.syn0</code> array; <em>O</em> is its <code>model.syn1neg</code> array in NS or <code>model.syn1</code> in HS.)</p>

<p>I find NS a bit easier to think about: each predictable word corresponds to a single output node. For training data where (context)-indicates->(word), training tries to drive that word's node value toward 1.0, and the other randomly-chosen word node values toward 0.0. </p>

<p>In HS, each word is represented by a huffman-code of a small subset of the output nodes – those 'points' are driven to 1.0 or 0.0 to make the network more indicative of a single word after a (context)-indicates->(word) example. </p>

<p>Only the <em>I</em> matrix, initial word values, are randomized to low-magnitude vectors at the beginning. (The hidden-to-output weights <em>O</em> are left zeros.)</p>

<p>(2) Yes, that'll train things - just note that tiny toy-sized examples won't necessarily generate the useful constellations-of-vector-coordinates that are valued from word2vec.</p>

<p>(3) Note, <code>model.similarity('cats', 'cats')</code> is actually checking the cosine-similarity between the (input) vectors for those two words. Those are the same word, thus they definitionally have the same vector, and the similarity between identical vectors is 1.0. </p>

<p>That is, <code>similarity()</code> is <em>not</em> asking the model for a prediction, it's retrieving learned words by key and comparing those vectors. (Recent versions of gensim do have a <code>predict_output_word()</code> function, but it only works in NS mode, and making predictions isn't really the point of word2vec, and many implementations don't offer any prediction API at all. Rather, the point is using those attempted predictions during training to induce word-vectors that turn out to be useful for various other tasks.)</p>

<p>But even if you were reading predictions, 'cats' might still be a reasonable-although-bad prediction from the model in the context of 'cats'. The essence of forcing large vocabularies into the smaller dimensionality of 'dense' embeddings is compression – the model has no choice but to cluster related words together, because there's not enough internal complexity (learnable parameters) to simply memorize all details of the input. (And for the most part, that's a good thing, because it results in generalizable patterns, rather than just overfit idiosyncrasies of the training corpus.) </p>

<p>The word 'cats' will wind up close to 'dogs' and 'pets' – because they all co-occur with similar words, or each other. And thus the model will be forced to make similar output-predictions for each, because their input-vectors don't vary that much. And a few predictions that are nonsensical in logical language use – like a repeating word - may be made, but only because taking a larger error there still gives less error over the whole training set, compared to other weighting alternatives. </p>
",1,0,605,2017-09-06 00:00:34,https://stackoverflow.com/questions/46064892/word2vec-training-procedure-clarification
How does word embedding/ word vectors work/created?,"<p>How does word2vec create vectors for words? I trained two word2vec models using two different files (from commoncrawl website) but I am getting same word vectors for a given word from both models. </p>

<p>Actually, I have created multiple word2vec models using different text files from the commoncrawl website. Now I want to check which model is better among all. How can select the best model out of all these models and why I am getting same word vectors for different models?</p>

<p>Sorry, If the question is not clear. </p>
","neural-network, nlp, deep-learning, gensim, word2vec","<p>If you are getting identical word-vectors from models that you've prepared from different text corpuses, something is likely wrong in your process. You may not be performing any training at all, perhaps because of a problem in how the text iterable is provided to the <code>Word2Vec</code> class. (In that case, word-vectors would remain at their initial, randomly-initialized values.) </p>

<p>You should enable logging, and review the logs carefully to see that sensible counts of words, examples, progress, and incremental-progress are displayed during the process. You should also check that results for some superficial, ad-hoc checks look sensible after training. For example, does <code>model.most_similar('hot')</code> return other words/concepts somewhat like 'hot'? </p>

<p>Once you're sure models are being trained on varied corpuses – in which case their word-vectors should be very different from each other – deciding which model is 'best' depends on your specific goals with word-vectors.</p>

<p>You should devise a repeatable, quantitative way to evaluate a model against your intended end-uses. This might start crudely with a few of your own manual reviews of results, like looking over <code>most_similar()</code> results for important words for better/worse results – but should become more extensive. rigorous, and automated as your project progresses. </p>

<p>An example of such an automated scoring is the <code>accuracy()</code> method on gensim's word-vectors object. See:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/6d6f5dcfa3af4bc61c47dfdf5cdbd8e1364d0c3a/gensim/models/keyedvectors.py#L652"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/6d6f5dcfa3af4bc61c47dfdf5cdbd8e1364d0c3a/gensim/models/keyedvectors.py#L652</a></p>

<p>If supplied with a specifically-formatted file of word-analogies, it will check how well the word-vectors solve those analogies. For example, the <a href=""https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt"" rel=""nofollow noreferrer""><code>questions-words.txt</code></a> of Google's original <code>word2vec</code> code release includes the analogies they used to report vector quality. Note, though, that the word-vectors that are best for some purposes, like understanding text topics or sentiment, might not also be the best at solving this style of analogy, and vice-versa. If training your own word-vectors, it's best to choose your training corpus/parameters based on your own goal-specific criteria for what 'good' vectors will be. </p>
",0,2,337,2017-09-06 10:32:37,https://stackoverflow.com/questions/46072991/how-does-word-embedding-word-vectors-work-created
Word2Vec in Gensim using model.most_similar,"<p>I am new in 'Word2Vec' in Gensim. I want to build a Word2Vec model for the text (Extracted from Wikipedia: Machine Learning) and find <strong>most similar words</strong> to 'Machine Learning'.</p>

<p>My current code is as follows.</p>

<pre><code># import modules &amp; set up logging
from gensim.models import Word2Vec

sentences = ""Machine learning is the subfield of computer science that, according to Arthur Samuel, gives computers the ability to learn without being explicitly programmed.[1][2][verify] Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term machine learning in 1959 while at IBM. Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] – such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision.""
# train word2vec on the sentences
model = Word2Vec(sentences, min_count=1)
vocab = list(model.wv.vocab.keys())
print(vocab[:10])
</code></pre>

<p>However, for vocab I get one character output.</p>

<pre><code>['M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'l', 'r']
</code></pre>

<p>Please help me to get the most_similar_words by using using <strong>model.most_similar</strong></p>
","python, gensim, word2vec","<p>The <code>Word2Vec</code> class expects its corpus of <code>sentences</code> to be an iterable source of individual items which are each a list-of-word-tokens. </p>

<p>You're providing a single string. If it iterates over that, it gets individual characters. If it then tries to interpret those individual characters as a list-of-tokens, it still just gets a single-character – so the only 'words' it sees are single characters. </p>

<p>At the very least, you'd want your corpus to be constructed more like this:</p>

<pre><code>sentences = [
    ""Machine learning is the subfield of computer science that, according to Arthur Samuel, gives computers the ability to learn without being explicitly programmed.[1][2][verify] Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term machine learning in 1959 while at IBM. Evolved from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning explores the study and construction of algorithms that can learn from and make predictions on data[4] – such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character recognition (OCR),[7] learning to rank, and computer vision."".split(),
]
</code></pre>

<p>That's still just one 'sentence', but it'll be split-on-whitespace into word-tokens.</p>

<p>Note also that useful word2vec results require large, varied text samples – toy-sized examples won't usually show the kinds of word-similarities or word-relative-arrangements that word2vec is famous for creating. </p>
",1,0,981,2017-09-07 02:16:18,https://stackoverflow.com/questions/46086858/word2vec-in-gensim-using-model-most-similar
Get bigrams and trigrams in word2vec Gensim,"<p>I am currently using uni-grams in my word2vec model as follows.</p>

<pre><code>def review_to_sentences( review, tokenizer, remove_stopwords=False ):
    #Returns a list of sentences, where each sentence is a list of words
    #
    #NLTK tokenizer to split the paragraph into sentences
    raw_sentences = tokenizer.tokenize(review.strip())

    sentences = []
    for raw_sentence in raw_sentences:
        # If a sentence is empty, skip it
        if len(raw_sentence) &gt; 0:
            # Otherwise, call review_to_wordlist to get a list of words
            sentences.append( review_to_wordlist( raw_sentence, \
              remove_stopwords ))
    #
    # Return the list of sentences (each sentence is a list of words,
    # so this returns a list of lists
    return sentences
</code></pre>

<p>However, then I will miss important bigrams and trigrams in my dataset.</p>

<pre><code>E.g.,
""team work"" -&gt; I am currently getting it as ""team"", ""work""
""New York"" -&gt; I am currently getting it as ""New"", ""York""
</code></pre>

<p>Hence, I want to capture the important bigrams, trigrams etc. in my dataset and input into my word2vec model.</p>

<p>I am new to wordvec and struggling how to do it. Please help me.</p>
","python, tokenize, word2vec, gensim, n-gram","<p>First of all you should use gensim's class <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""noreferrer"">Phrases</a> in order to get bigrams, which works as pointed in the doc </p>

<pre><code>&gt;&gt;&gt; bigram = Phraser(phrases)
&gt;&gt;&gt; sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
&gt;&gt;&gt; print(bigram[sent])
[u'the', u'mayor', u'of', u'new_york', u'was', u'there']
</code></pre>

<p>To get trigrams and so on, you should use the bigram model that you already have and apply Phrases to it again, and so on. 
Example:</p>

<pre><code>trigram_model = Phrases(bigram_sentences)
</code></pre>

<p>Also there is a good notebook and video that explains how to use that .... <a href=""https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb"" rel=""noreferrer"">the notebook</a>, <a href=""https://www.youtube.com/watch?v=6zm9NC9uRkk&amp;list=PLw5BhoADa9fUtHRGND_rrdeZlv1XFM8In&amp;index=14&amp;t=3339s"" rel=""noreferrer"">the video</a></p>

<p>The most important part of it is how to use it in real life sentences which is as follows:</p>

<pre><code>// to create the bigrams
bigram_model = Phrases(unigram_sentences)

// apply the trained model to a sentence
 for unigram_sentence in unigram_sentences:                
            bigram_sentence = u' '.join(bigram_model[unigram_sentence])

// get a trigram model out of the bigram
trigram_model = Phrases(bigram_sentences)
</code></pre>

<p>Hope this helps you, but next time give us more information on what you are using and etc.</p>

<p>P.S: Now that you edited it, you are not doing anything in order to get bigrams just splitting it, you have to use Phrases in order to get words like New York as bigrams.</p>
",24,17,40015,2017-09-09 09:49:15,https://stackoverflow.com/questions/46129335/get-bigrams-and-trigrams-in-word2vec-gensim
Error in extracting phrases using Gensim,"<p>I am trying to get the bigrams in the sentences using Phrases in Gensim as follows.</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser
documents = [""the mayor of new york was there"", ""machine learning can be useful sometimes"",""new york mayor was present""]

sentence_stream = [doc.split("" "") for doc in documents]
#print(sentence_stream)
bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
bigram_phraser = Phraser(bigram)

for sent in sentence_stream:
    tokens_ = bigram_phraser[sent]
    print(tokens_)
</code></pre>

<p>Even though it catches ""new"", ""york"" as ""new york"", it does not catch ""machine"", learning as ""machine learning""</p>

<p>However, in the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">example shown in Gensim Website</a> they were able to catch the words ""machine"", ""learning"" as ""machine learning"".</p>

<p>Please let me know how to get ""machine learning"" as a bigram in the above example</p>
","python, data-mining, text-mining, word2vec, gensim","<p>The technique used by gensim <code>Phrases</code> is purely based on statistics of co-occurrences: how often words appear together, versus alone, in a formula also affected by <code>min_count</code> and compared against the <code>threshold</code> value. </p>

<p>It is only because your training set has 'new' and 'york' occur alongside each other twice, while other words (like 'machine' and 'learning') only occur alongside each other once, that 'new_york' becomes a bigram, and other pairings do not. What's more, even if you did find a combination of <code>min_count</code> and <code>threshold</code> that would promote 'machine_learning' to a bigram, it would <em>also</em> pair together every other bigram-that-appears-once – which is probably not what you want.</p>

<p>Really, to get good results from these statistical techniques, you need lots of varied, realistic data. (Toy-sized examples may superficially succeed, or fail, for superficial toy-sized reasons.) </p>

<p>Even then, they will tend to miss combinations a person would consider reasonable, and make combinations a person wouldn't. Why? Because our minds have much more sophisticated ways (including grammar and real-world knowledge) for deciding when clumps of words represent a single concept. </p>

<p>So even with more better data, be prepared for nonsensical n-grams. Tune or judge the model on whether it is overall improving on your goal, not any single point or ad-hoc check of matching your own sensibility.</p>

<p>(Regarding the referenced gensim documentation comment, I'm pretty sure that if you try <code>Phrases</code> on just the two sentences listed there, it won't find any of the desired phrases – not 'new_york' or 'machine_learning'. As a figurative example, the ellipses <code>...</code> imply the training set is larger, and the results indicate that the extra unshown texts are important. It's just because of the 3rd sentence you've added to your code that 'new_york' is detected. If you added similar examples to make 'machine_learning' look more like a statistically-outlying pairing, your code could promote 'machine_learning', too.)</p>
",6,3,1828,2017-09-10 05:27:14,https://stackoverflow.com/questions/46137572/error-in-extracting-phrases-using-gensim
Text similarity with gensim and cosine similarity,"<pre><code>from gensim import corpora, models, similarities

documents = [""This is a book about cars, dinosaurs, and fences""]

# remove common words and tokenize
stoplist = set('for a of the and to in - , is'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]

# Remove commas
texts[0] = [text.replace(',','') for text in texts[0]]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)

doc = ""I like cars and birds""
vec_bow = dictionary.doc2bow(doc.lower().split())

vec_lsi = lsi[vec_bow] 
index = similarities.MatrixSimilarity(lsi[corpus]) 

sims = index[vec_lsi] # perform a similarity query against the corpus
print(sims)
</code></pre>

<p>In the above code I am comparing how much ""This is a book about cars, dinosaurs, and fences"" is similar to ""I like cars and birds"" using the cosine similarity technique.</p>

<p>The two sentences have effectively 1 words in common, which is ""cars"", however when I run the code I get that they are 100% similar.  This does not make sense to me.</p>

<p>Can someone suggest how to improve my code so that I get a reasonable number?</p>
","python, gensim, cosine-similarity","<p>These topic-modelling techniques need varied, realistic data to achieve sensible results. Toy-sized examples of just one or a few text examples don't work well – and even if they do, it's often just good luck or contrived suitability. </p>

<p>In particular:</p>

<ul>
<li><p>a model with only one example can't sensibly create multiple topics, as there's no contrast-between-documents to model</p></li>
<li><p>a model presented with words it hasn't seen before ignores those words, so your test doc appears to it the same as the single word 'cars' – the only word it's seen before</p></li>
</ul>

<p>In this case, both your single training document, and the test document, get modeled by LSI as having <code>0</code> contribution from the 0th topic, and positive contribution (of different magnitudes) from the 1st topic. Since cosine-similarity merely compares angle, and not magnitude, both docs are along-the-same-line-from-the-origin, and so have no angle-of-difference, and thus similarity 1.0. </p>

<p>But if you had better training data, and more than a single-known-word test doc, you might start to get more sensible results. Even a few dozen training docs, and a test doc with several known words, might help... but hundreds or thousands or tens-of-thousands training-docs would be even better. </p>
",1,1,4386,2017-09-10 22:45:20,https://stackoverflow.com/questions/46146241/text-similarity-with-gensim-and-cosine-similarity
Error getting trigrams using gensim&#39;s Phrases,"<p>I want to extract all bigrams and trigrams of the given sentences. </p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""Human Computer Interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]

sentence_stream = [doc.split("" "") for doc in documents]
bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
trigram = Phrases(bigram(sentence_stream, min_count=1, threshold=2, delimiter=b' '))

for sent in sentence_stream:
    #print(sent)
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>The code works fine for bigrams and capture 'new york' and 'machine learning' ad bigrams.</p>

<p>However, I get the following error when I try to insert trigrams.</p>

<pre><code>TypeError: 'Phrases' object is not callable
</code></pre>

<p>Please let me know, how to correct my code.</p>

<p>I am following the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">example documentation</a> of gensim.</p>
","python, nlp, data-mining, text-mining, gensim","<p>According to the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">docs</a>, you can do:</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser 

phrases = Phrases(sentence_stream)
bigram = Phraser(phrases)
trigram = Phrases(bigram[sentence_stream])
</code></pre>

<p><code>bigram</code>, being a <code>Phrases</code> object, cannot be called again, as you are doing so.</p>
",1,0,1833,2017-09-11 01:14:35,https://stackoverflow.com/questions/46147013/error-getting-trigrams-using-gensims-phrases
Issues in getting trigrams using Gensim,"<p>I want to get bigrams and trigrams from the example sentences I have mentioned.</p>

<p>My code works fine for bigrams. However, it does not capture trigrams in the data (e.g., human computer interaction, which is mentioned in 5 places of my sentences)</p>

<p><strong>Approach 1</strong> Mentioned below is my code using Phrases in Gensim.</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=1, delimiter=b' ')
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p><strong>Approach 2</strong> I even tried to use Phraser and Phrases both, but it didn't work.</p>

<pre><code>from gensim.models import Phrases
from gensim.models.phrases import Phraser
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')
bigram_phraser = Phraser(bigram)
trigram = Phrases(bigram_phraser[sentence_stream])

for sent in sentence_stream:
    bigrams_ = bigram_phraser[sent]
    trigrams_ = trigram[bigrams_]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>Please help me to fix this issue of getting trigrams.</p>

<p>I am following the <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""noreferrer"">example documentation</a> of Gensim.</p>
","python, data-mining, text-mining, word2vec, gensim","<p>I was able to get bigrams and trigrams with a few modifications to your code:
</p>

<pre><code>from gensim.models import Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')
trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')

for sent in sentence_stream:
    bigrams_ = [b for b in bigram[sent] if b.count(' ') == 1]
    trigrams_ = [t for t in trigram[bigram[sent]] if t.count(' ') == 2]

    print(bigrams_)
    print(trigrams_)
</code></pre>

<p>I removed the <code>threshold = 1</code> parameter from the bigram <code>Phrases</code> because otherwise it seems to form weird digrams that allow the construction of weird trigrams (notice that <code>bigram</code> is used to build the trigram <code>Phrases</code>); this parameter would probably come useful when you have more data. For trigrams, the <code>min_count</code> parameter also needs to be specified because it defaults to 5 if not provided.</p>

<p>In order to retrieve the bigrams and trigrams of each document, you can use this list comprehension trick to filter elements that aren't formed by two or three words, respectively. </p>

<hr>

<p><strong>Edit</strong> - a few details about the <code>threshold</code> parameter:</p>

<p>This parameter is used by the estimator to determine if two words <em>a</em> and <em>b</em> form a phrase, and that is only if:
</p>

<pre><code>(count(a followed by b) - min_count) * N/(count(a) * count(b)) &gt; threshold
</code></pre>

<p>where <em>N</em> is the total vocabulary size. By default the parameter value is 10 (see <a href=""https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases"" rel=""noreferrer"">docs</a>). So, the higher the <code>threshold</code>, the harder the constraints for words to form phrases.</p>

<p>For example, in your first approach you were trying to use <code>threshold = 1</code>, so you would get <code>['human computer','interaction is']</code> as digrams of 3 out of your 5 sentences that begin with ""human computer interaction""; that weird second digram is a result of the more relaxed threshold. </p>

<p>Then, when you try to get trigrams with default <code>threshold = 10</code> you only get <code>['human computer interaction is']</code> for those 3 sentences, and nothing for the remaining two (filtered by threshold); and because that was a 4-gram instead of a trigram it would also be filtered by <code>if t.count(' ') == 2</code>. In case that, for example, you lower the trigram threshold to 1, you can get ['human computer interaction'] as trigram for the two remaining sentences. It doesn't seem easy to get a good combination of parameters, <a href=""https://stackoverflow.com/a/46143595/8507311"">here's</a> more about it. </p>

<p>I'm not an expert, so take this conclusion with a grain of salt: I think it's better to firstly get good digram results (not like 'interaction is') before moving on, as weird digrams can add confusion to further trigrams, 4-gram... </p>
",20,14,9082,2017-09-11 04:28:12,https://stackoverflow.com/questions/46148182/issues-in-getting-trigrams-using-gensim
Why Word2Vec&#39;s most_similar() function is giving senseless results on training?,"<p>I am running the gensim word2vec code on a corpus of resumes(stopwords removed) to identify similar context words in the corpus from a list of pre-defined keywords.</p>

<p>Despite several iterations with input parameters,stopword removal etc the similar context words are not at all making sense(in terms of distance or context)
Eg. correlation and matrix occurs in the same window several times yet matrix doesnt fall in the most_similar results for correlation </p>

<p>Following are the details of the system and codes
gensim 2.3.0 ,Running on Python 2.7 Anaconda
Training Resumes :55,418 sentences
Average words per sentence : 3-4 words(post stopwords removal)
Code :</p>

<pre><code>    wordvec_min_count=int()
    size = 50
    window=10
    min_count=5
    iter=50
    sample=0.001
    workers=multiprocessing.cpu_count()
    sg=1
    bigram = gensim.models.Phrases(sentences, min_count=10, threshold=5.0)
    trigram = gensim.models.Phrases(bigram[sentences], min_count=10, threshold=5.0)
    model=gensim.models.Word2Vec(sentences = trigram[sentences], size=size, alpha=0.005, window=window, min_count=min_count,max_vocab_size=None,sample=sample, seed=1, workers=workers, min_alpha=0.0001, sg=sg, hs=1, negative=0, cbow_mean=1,iter=iter)

model.wv.most_similar('correlation')
Out[20]: 
[(u'rankings', 0.5009744167327881),
 (u'salesmen', 0.4948525130748749),
 (u'hackathon', 0.47931140661239624),
 (u'sachin', 0.46358123421669006),
 (u'surveys', 0.4472047984600067),
 (u'anova', 0.44710394740104675),
 (u'bass', 0.4449636936187744),
 (u'goethe', 0.4413239061832428),
 (u'sold', 0.43735259771347046),
 (u'exceptional', 0.4313117265701294)]
</code></pre>

<p>I am lost as to why the results are so random ? Is there anyway to check the accuracy for word2vec ?</p>

<p>Also is there an alternative of word2vec for most_similar() function ? I read about gloVE but was not able to install the package.</p>

<p>Any information in this regard would be helpful</p>
","python-2.7, gensim, word2vec, word-embedding","<p>Enable INFO-level logging and make sure that it indicates real training is happening. (That is, you see incremental progress taking time over the expected number of texts, over the expected number of iterations.)</p>

<p>You may be hitting this <a href=""https://github.com/RaRe-Technologies/gensim/issues/1401"" rel=""nofollow noreferrer"">open bug issue in <code>Phrases</code></a>, where requesting the Phrase-promotion (as with <code>trigram[sentences]</code>) only offers a single-iteration, rather than the multiply-iterable collection object that <code>Word2Vec</code> needs. </p>

<p><code>Word2Vec</code> needs to pass over the corpus once for vocabulary-discovery, then <code>iter</code> times again for training. If <code>sentences</code> or the phrasing-wrappers only support single-iteration, only the vocabulary will be discovered – training will end instantly, and the model will appear untrained.</p>

<p>As you'll see in that issue, a workaround is to perform the Phrases-transformation and save the results into an in-memory list (if it fits) or to a separate text corpus on disk (that's already been phrase-combined). Then, use a truly restartable iterable on that – which will also save some redundant processing. </p>
",1,0,924,2017-09-11 14:19:26,https://stackoverflow.com/questions/46157937/why-word2vecs-most-similar-function-is-giving-senseless-results-on-training
Sentence matching with gensim word2vec: manually populated model doesn&#39;t work,"<p>I'm trying to solve a problem of sentence comparison using naive approach of summing up word vectors and comparing the results. My goal is to match people by interest, so the dataset consists of names and short sentences describing their hobbies. The batches are fairly small, few hundreds of people, so i wanted to give it a try before digging into doc2vec.</p>

<p>I prepare the data by cleaning it completely, removing stop words, tokenizing and lemmatizing. I use pre-trained model for word vectors which returns adequate results when finding similarities for some test words. Also tried summing up the sentence words to find similarities in the original model - the matches do make sense. The similarities would be around general sense of the phrase.</p>

<p>For sentence matching I'm trying the following: create an empty model</p>

<pre><code>b = gs.models.Word2Vec(min_count=1, size=300, sample=0, hs=0)
</code></pre>

<p>Build vocab out of names (or person id's), no training</p>

<pre><code>#first create vocab with an empty vector
test = [['test']]
b.build_vocab(test)
b.wv.syn0[b.wv.vocab['test'].index] = b.wv.syn0[b.wv.vocab['test'].index]*0

#populate vocab from an array
b.build_vocab([personIds], update=True)
</code></pre>

<p>Summ each sentence's word vectors and store the results into the model for a corresponding id</p>

<pre><code>#sentences are pulled from pandas dataset df. 'a' is a pre-trained model i use to get vectors for each word

def summ(phrase, start_model):
    '''
    vector addition function
    '''
    #starting with a vector of 0's
    sum_vec = start_model.word_vec(""cat_NOUN"")*0
    for word in phrase:
        sum_vec += start_model.word_vec(word)
    return sum_vec

for i, row in df.iterrows():
    try:
        personId = row[""ID""]
        summVec = summ(df.iloc[i,1],a)
        #updating syn0 for each name/id in vocabulary
        b.wv.syn0[b.wv.vocab[personId].index] = summVec
    except:
        pass
</code></pre>

<p>I understand that i shouldn't be expecting much accuracy here, but the t-SNE print doesn't show any clustering whatsoever. Finding similarities method also fails to find matches (&lt;0.2 similarity coefficient basically for everything). [<img src=""https://i.sstatic.net/w81nX.png"" alt=""]plot of the entire model[1]""></p>

<p>Wondering if anyone has an idea of where did i go wrong? Is my approach valid at all?</p>
","python, nlp, gensim, word2vec","<p>Your code, as shown, neither does any <code>train()</code> of word-vectors (using your local text), nor does it pre-load any vectors from elsewhere.  So any vectors which do exist – created by the <code>build_vocab()</code> calls – will still just be in their randomly-initialized starting locations, and be useless for any semantic purposes. </p>

<p>Suggestions:</p>

<ul>
<li>either (a) train your own vectors from your text, which makes sense if you have a good quantity of text; or (b) load vectors from elsewhere. But don't try to do both. (Or, in the case of the code above, neither.)</li>
<li>The <code>update=True</code> option for <code>build_vocab()</code> should be considered an expert, experimental option – only worth tinkering with if you've already had things working in simpler modes, and you're sure you need it and understand all the implications.</li>
<li>Normal use won't ever explicitly re-assign new values into the <code>Word2Vec</code> model's <code>syn0</code> property - those are managed by the class's training routines, so you never need to zero them out or modify them. You should tally up your own text summary vectors, based on word-vectors, outside the model in your own data structures. </li>
</ul>
",0,0,997,2017-09-12 05:00:47,https://stackoverflow.com/questions/46168239/sentence-matching-with-gensim-word2vec-manually-populated-model-doesnt-work
Using gensim doc2vec with Keras Conv1d. ValueError,"<p>Im currently trying to implement a convolutional lstm network using keras. Instead of using keras' embedding layer, I used Gensim's doc2vec embeddings and created input data from it.</p>

<p><strong>preprocessing</strong></p>

<pre><code>preprocessed_train = utils.preprocess_text(train_vect)
preprocessed_test = utils.preprocess_text(test_vect)

print preprocessed_train[0]

result: [u'snes_classic', u'preorders_open', u'later_month', u'ever_since', u'nintendo', u'announce', u'snes_classic', u'edition', u'earlier', u'fan', u'desperate', u'register', u'interest', u'ensure', u'come', u'launch', u'however', u'although', u'system', u'pre-orders', u'make', u'available', u'retailers', u'every', u'store', u'plan', u'sell', u'console', u'allow', u'people', u'place', u'pre-orders', u'yet', u'today', u'though', u'nintendo', u'confirm', u'snes_classic', u'edition', u'pre-orders', u'soon', u'available', u'fan', u'post_official', u'facebook', u'company', u'console', u'make', u'available_pre-order', u'various_retailers', u'late', u'month', u'nintendo', u'appreciate', u'incredible', u'anticipation', u'hardware', u'reference', u'fact', u'snes_classic', u'edition', u'already', u'sell', u'many', u'place', u'across_globe', u'unfortunately', u'nintendo', u'clarify', u'exactly', u'retailers', u'open', u'snes_classic', u'pre-orders', u'provide', u'exact_date', u'however', u'stand_reason', u'wal-mart', u'retailers', u'force', u'cancel_pre-orders', u'hardware', u'website', u'error', u'saw', u'go_live', u'prematurely', u'currently_unclear', u'wal-mart', u'help', u'cancel', u'reservations', u'sign-up', u'pre-orders', u'go_live', u'properly', u'month', u'appreciate', u'incredible', u'anticipation', u'exist', u'super_nintendo', u'entertainment_system', u'super_nes', u'classic', u'post', u'nintendo', u'tuesday_august', u'1', u'2017', u'post', u'nintendo', u'mention', u'ship', u'significant_amount', u'snes_classic', u'edition', u'units', u'retailers', u'launch', u'company', u'make', u'units', u'available', u'throughout', u'balance', u'calendar', u'snes_classic', u'edition', u'first', u'announce', u'nintendo', u'explain', u'make', u'units', u'nes_classic', u'constantly', u'sell', u'leave', u'many', u'glad', u'nintendo', u'offer_clarification', u'others', u'however', u'remain_unconvinced', u'nintendo', u'able', u'keep', u'demand', u'console', u'incredibly_hard', u'fan', u'place', u'legitimate', u'order', u'snes_classic', u'edition', u'end', u'even_harder', u'find', u'throughout', u'scalpers', u'place', u'pre-orders', u'pick', u'console', u'post-launch', u'order', u'sell', u'higher_price', u'later_date', u'retailers', u'like', u'ebay', u'enforce_rule', u'scalpers', u'unclear_whether', u'enough', u'snes_classic', u'edition', u'launch', u'september_29', u'2017_source', u'nintendo', u'facebook']
</code></pre>

<p><strong>data labels</strong></p>

<pre><code>y_test = [x for x in test_data['slabel']]
y_train = [x for x in train_data['slabel']]

y_test = keras.utils.to_categorical(y_test)
y_train = keras.utils.to_categorical(y_train)

result:
array([[ 0.,  0.,  0.,  0.,  1.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.]])
</code></pre>

<p><strong>load doc2vec model</strong></p>

<pre><code>doc2vec_model = gensim.models.Doc2Vec.load('./doc2vec-models/dmbbv_300_epoch_500_size_model')
</code></pre>

<p><strong>infer data and create input vectors</strong>. The <em>infer_vector</em> function creates the document embeddings based on the doc2vec model that I created. </p>

<pre><code>X_train = []
for text in preprocessed_train:
    inferred_vec = doc2vec_model.infer_vector(text)
    X_train.append(inferred_vec)

X_test = []
for text in preprocessed_test:
    inferred_vec = doc2vec_model.infer_vector(text)
    X_test.append(inferred_vec)
</code></pre>

<p><strong>reshape data</strong></p>

<pre><code>X_train = np.array(X_train)
X_test = np.array(X_test)
X_train = X_train.reshape((X_train.shape[0],1,X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0],1,X_test.shape[1]))
X_train.shape,X_test.shape

result: ((1476, 1, 500), (370, 1, 500))
</code></pre>

<p><strong>building model</strong></p>

<pre><code>model = Sequential()
model.add(Conv1D(filters = 128,
                 kernel_size = 5,
                 input_shape = (X_train.shape[1],X_train.shape[2]), 
                 padding = 'valid',
                 activation = 'relu'))
model.add(MaxPooling1D(2))
model.add(LSTM(X_train.shape[1],return_sequences = True, 
               implementation=2, 
               kernel_regularizer=regularizers.l1_l2(0.001),
               activity_regularizer=regularizers.l1(0.01)
              ))
model.add(Dropout(0.7))
model.add(Activation('relu'))
model.add(LSTM(256,return_sequences = True))
model.add(Activation('relu'))
model.add(LSTM(128))
model.add(Activation('relu'))
model.add(LSTM(64,return_sequences = True))
model.add(Activation('relu'))
model.add(LSTM(32,return_sequences = True))
model.add(Activation('relu'))
model.add(LSTM(16))
model.add(Activation('relu'))
model.add(Dense(5, activation = 'sigmoid'))
model.compile(loss=""categorical_crossentropy"", optimizer='adamax',metrics=['categorical_accuracy', 'accuracy'])
</code></pre>

<p>then I get this error </p>

<hr>

<pre><code>-----------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-488-b29db30c3ee7&gt; in &lt;module&gt;()
      5 #                  use_bias=True,
      6                  padding = 'valid',
----&gt; 7                  activation = 'relu'))
      8 model.add(MaxPooling1D(2))
      9 model.add(LSTM(X_train.shape[1],return_sequences = True, 

/usr/local/lib/python2.7/dist-packages/keras/models.pyc in add(self, layer)
    434                 # and create the node connecting the current layer
    435                 # to the input layer we just created.
--&gt; 436                 layer(x)
    437 
    438             if len(layer.inbound_nodes) != 1:

/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in __call__(self, inputs, **kwargs)
    594 
    595             # Actually call the layer, collecting output(s), mask(s), and shape(s).
--&gt; 596             output = self.call(inputs, **kwargs)
    597             output_mask = self.compute_mask(inputs, previous_mask)
    598 

/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.pyc in call(self, inputs)
    154                 padding=self.padding,
    155                 data_format=self.data_format,
--&gt; 156                 dilation_rate=self.dilation_rate[0])
    157         if self.rank == 2:
    158             outputs = K.conv2d(

/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc in conv1d(x, kernel, strides, padding, data_format, dilation_rate)
   3114         strides=(strides,),
   3115         padding=padding,
-&gt; 3116         data_format=tf_data_format)
   3117     return x
   3118 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in convolution(input, filter, padding, strides, dilation_rate, name, data_format)
    670         dilation_rate=dilation_rate,
    671         padding=padding,
--&gt; 672         op=op)
    673 
    674 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in with_space_to_batch(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)
    336       raise ValueError(""dilation_rate must be positive"")
    337     if np.all(const_rate == 1):
--&gt; 338       return op(input, num_spatial_dims, padding)
    339 
    340   # We have two padding contributions. The first is used for converting ""SAME""

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in op(input_converted, _, padding)
    662           data_format=data_format,
    663           strides=strides,
--&gt; 664           name=name)
    665 
    666     return with_space_to_batch(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in _non_atrous_convolution(input, filter, padding, data_format, strides, name)
    114           padding=padding,
    115           data_format=data_format_2d,
--&gt; 116           name=scope)
    117     elif conv_dims == 2:
    118       if data_format is None or data_format == ""NHWC"":

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc in conv1d(value, filters, stride, padding, use_cudnn_on_gpu, data_format, name)
   2011     result = gen_nn_ops.conv2d(value, filters, strides, padding,
   2012                                use_cudnn_on_gpu=use_cudnn_on_gpu,
-&gt; 2013                                data_format=data_format)
   2014     return array_ops.squeeze(result, [spatial_start_dim])
   2015 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.pyc in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)
    395                                 strides=strides, padding=padding,
    396                                 use_cudnn_on_gpu=use_cudnn_on_gpu,
--&gt; 397                                 data_format=data_format, name=name)
    398   return result
    399 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
    765         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    766                          input_types=input_types, attrs=attr_protos,
--&gt; 767                          op_def=op_def)
    768         if output_structure:
    769           outputs = op.outputs

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
   2630                     original_op=self._default_original_op, op_def=op_def)
   2631     if compute_shapes:
-&gt; 2632       set_shapes_for_outputs(ret)
   2633     self._add_op(ret)
   2634     self._record_op_seen_by_control_dependencies(ret)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)
   1909       shape_func = _call_cpp_shape_fn_and_require_op
   1910 
-&gt; 1911   shapes = shape_func(op)
   1912   if shapes is None:
   1913     raise RuntimeError(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in call_with_requiring(op)
   1859 
   1860   def call_with_requiring(op):
-&gt; 1861     return call_cpp_shape_fn(op, require_shape_fn=True)
   1862 
   1863   _call_cpp_shape_fn_and_require_op = call_with_requiring

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in call_cpp_shape_fn(op, require_shape_fn)
    593     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,
    594                                   input_tensors_as_shapes_needed,
--&gt; 595                                   require_shape_fn)
    596     if not isinstance(res, dict):
    597       # Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in _call_cpp_shape_fn_impl(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)
    657       missing_shape_fn = True
    658     else:
--&gt; 659       raise ValueError(err.message)
    660 
    661   if missing_shape_fn:

ValueError: Negative dimension size caused by subtracting 5 from 1 for 'conv1d_55/convolution/Conv2D' (op: 'Conv2D') with input shapes: [?,1,1,500], [1,5,500,128].
</code></pre>
","python, machine-learning, keras, gensim","<p>The problem is with the input shape. Instead of <code>(None, 1, 500)</code> you can try with <code>(None, 500, 1)</code>.</p>

<p>You can tell that by looking at the exception </p>

<pre><code>`Negative dimension size caused by subtracting 5 from 1 for 'conv1d'` 
</code></pre>

<p>the kernel (5) is bigger than the second dimension.</p>
",2,1,1127,2017-09-13 12:22:52,https://stackoverflow.com/questions/46197493/using-gensim-doc2vec-with-keras-conv1d-valueerror
How Word Mover&#39;s Distance (WMD) uses word2vec embedding space?,"<p>According to WMD <a href=""http://proceedings.mlr.press/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">paper</a>, it's inspired by word2vec model and use word2vec vector space for moving document 1 towards document 2 (in the context of Earth Mover Distance metric). From the paper:</p>

<pre><code>Assume we are provided with a word2vec embedding matrix
X ∈ Rd×n for a finite size vocabulary of n words. The 
ith column, xi ∈ Rd, represents the embedding of the ith
word in d-dimensional space. We assume text documents
are represented as normalized bag-of-words (nBOW) vectors,
d ∈ Rn. To be precise, if word i appears ci times in
the document, we denote di = ci/cj (for j=1 to n). An nBOW vector
d is naturally very sparse as most words will not appear in
any given document. (We remove stop words, which are
generally category independent.)
</code></pre>

<p>I understand the concept from the paper, however, I couldn't understand how wmd uses word2vec embedding space from the code in Gensim. </p>

<p><strong><em>Can someone explain it in a simple way? Does it calculate the word vectors in a different way because I couldn't understand where in this code word2vec embedding matrix is used?</em></strong> </p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">WMD Fucntion from Gensim:</a></p>

<pre><code>   def wmdistance(self, document1, document2):
    # Remove out-of-vocabulary words.
    len_pre_oov1 = len(document1)
    len_pre_oov2 = len(document2)
    document1 = [token for token in document1 if token in self]
    document2 = [token for token in document2 if token in self]

    dictionary = Dictionary(documents=[document1, document2])
    vocab_len = len(dictionary)

    # Sets for faster look-up.
    docset1 = set(document1)
    docset2 = set(document2)

    # Compute distance matrix.
    distance_matrix = zeros((vocab_len, vocab_len), dtype=double)
    for i, t1 in dictionary.items():
        for j, t2 in dictionary.items():
            if t1 not in docset1 or t2 not in docset2:
                continue
            # Compute Euclidean distance between word vectors.
            distance_matrix[i, j] = sqrt(np_sum((self[t1] - self[t2])**2))

    def nbow(document):
        d = zeros(vocab_len, dtype=double)
        nbow = dictionary.doc2bow(document)  # Word frequencies.
        doc_len = len(document)
        for idx, freq in nbow:
            d[idx] = freq / float(doc_len)  # Normalized word frequencies.
        return d

    # Compute nBOW representation of documents.
    d1 = nbow(document1)
    d2 = nbow(document2)

    # Compute WMD.
    return emd(d1, d2, distance_matrix)
</code></pre>
","nlp, nltk, gensim, word2vec, word-embedding","<p>For the purposes of WMD, a text is considered a bunch of 'piles' of meaning. Those piles are placed at the coordinates of the text's words – and that's why WMD calculation is dependent on a set of word-vectors from another source. Those vectors position the text's piles.</p>

<p>The WMD is then the minimal amount of work needed to shift one text's piles to match another text's piles. And the measure of the work needed to shift from one pile to another is the euclidean distance between those pile's coordinates. </p>

<p>You could just try a naive shifting of the piles: look at the first word from text A, shift it to the first word from text B, and so forth. But that's unlikely to be the <em>cheapest</em> shifting – which would likely try to match nearer words, to send the 'meaning' on the shortest possible paths. So actually calculating the WMD is an iterative optimization problem – significantly more expensive than just a simple euclidean-distance or cosine-distance between two points. </p>

<p>That optimization is done inside the <code>emd()</code> call in the code you excerpt. But what that optimization requires is the pairwise distances between all words in text A, and all words in text B – because those are all the candidate paths across which meaning-weight might be shifted. You can see those pairwise distances calculated in the code to populate the <code>distance_matrix</code>, using the word-vectors already loaded in the model and accessible via <code>self[t1]</code>, <code>self[t2]</code>, etc. </p>
",2,2,1652,2017-09-13 15:09:18,https://stackoverflow.com/questions/46201029/how-word-movers-distance-wmd-uses-word2vec-embedding-space
How to turn embeddings loaded in a Pandas DataFrame into a Gensim model?,"<p>I have a DataFrame in which the index are words and I have 100 columns with float number such that for each word I have its embedding as a 100d vector. I would like to convert my DataFrame object into a <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">gensim model object</a> so that I can use its methods; specially <code>gensim.models.keyedvectors.most_similar()</code> so that I can search for similar words within my subset.</p>

<p>Which is the preferred way of doing that?</p>

<p>Thanks</p>
","python, pandas, gensim","<p>Not sure what the ""preferred"" way of doing this is, but the format gensim expects is pretty easy to replicate:</p>

<pre><code>data = pd.DataFrame([[0.15941701, 0.84058299],
                     [0.12190033, 0.87809967],
                     [0.06293788, 0.93706212]],
                    index=[""these"", ""be"", ""words""])

np.savetxt('test.txt', data.reset_index().values, 
           delimiter="" "", 
           header=""{} {}"".format(len(data), len(data.columns)),
           comments="""",
           fmt=[""%s""] + [""%.18e""]*len(data.columns))
</code></pre>

<p>The header is 2 space separated integers, the number of words in the vocabulary and the length of the word vector. The first column of each row is the word itself. The rest of the columns are the elements of the word vector. The fmt weirdness is to have the first element formatted as a string, and the rest formatted as a float.</p>

<p>Then can load this in gensim and do whatever:</p>

<pre><code>import gensim

from gensim.models.keyedvectors import KeyedVectors
word_vectors = KeyedVectors.load_word2vec_format('test.txt', binary=False)

word_vectors.similarity('these', 'words')
</code></pre>
",8,3,5830,2017-09-19 10:10:57,https://stackoverflow.com/questions/46297740/how-to-turn-embeddings-loaded-in-a-pandas-dataframe-into-a-gensim-model
Understanding LDA / topic modelling -- too much topic overlap,"<p>I'm new to topic modelling / Latent Dirichlet Allocation and have trouble understanding how I can apply the concept to my dataset (or whether it's the correct approach).</p>

<p>I have a small number of literary texts (novels) and would like to extract some general topics using LDA.</p>

<p>I'm using the <code>gensim</code> module in Python along with some <code>nltk</code> features. For a test I've split up my original texts (just 6) into 30 chunks with 1000 words each. Then I converted the chunks into document-term matrices and ran the algorithm. This is the code (although I think it doesn't matter for the question) :</p>

<pre><code># chunks is a 30x1000 words matrix

dictionary = gensim.corpora.dictionary.Dictionary(chunks)
corpus = [ dictionary.doc2bow(chunk) for chunk in chunks ]
lda = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = dictionary,
    num_topics = 10)
topics = lda.show_topics(5, 5)
</code></pre>

<p>However the result is completely different from any example I've seen in that the topics are full of meaningless words that can be found in <em>all</em> source documents, e.g. ""I"", ""he"", ""said"", ""like"", ... example:</p>

<pre><code>[(2, '0.009*""I"" + 0.007*""\'s"" + 0.007*""The"" + 0.005*""would"" + 0.004*""He""'), 
(8, '0.012*""I"" + 0.010*""He"" + 0.008*""\'s"" + 0.006*""n\'t"" + 0.005*""The""'), 
(9, '0.022*""I"" + 0.014*""\'s"" + 0.009*""``"" + 0.007*""\'\'"" + 0.007*""like""'), 
(7, '0.010*""\'s"" + 0.009*""I"" + 0.006*""He"" + 0.005*""The"" + 0.005*""said""'), 
(1, '0.009*""I"" + 0.009*""\'s"" + 0.007*""n\'t"" + 0.007*""The"" + 0.006*""He""')]
</code></pre>

<p>I don't quite understand why that happens, or why it doesn't happen with the examples I've seen. How do I get the LDA model to find more distinctive topics with less overlap? Is it a matter of filtering out more common words first? How can I adjust how many times the model runs? Is the number of original texts too small?</p>
","python, nlp, gensim, lda, topic-modeling","<p>LDA is extremely dependent on the words used in a corpus and how frequently they show up. The words you are seeing are all stopwords - meaningless words that are the most frequent words in a language e.g. ""the"", ""I"", ""a"", ""if"", ""for"", ""said"" etc. and since these words are the most frequent, it will negatively impact the model.</p>

<p>I would use the <code>nltk</code> stopword corpus to filter out these words:</p>

<pre><code>from nltk.corpus import stopwords
stop_words = stopwords.words('english')
</code></pre>

<p>Then make sure your text does not contain any of the words in the <code>stop_words</code> list (by whatever pre processing method you are using) - an example is below</p>

<pre><code>text = text.split() # split words by space and convert to list
text = [word for word in text if word not in stop_words]
text = ' '.join(text) # join the words in the text to make it a continuous string again
</code></pre>

<p>You may also want to remove punctuation and other characters (""/"",""-"") etc.) then use regular expressions:</p>

<pre><code>import re
remove_punctuation_regex = re.compile(r""[^A-Za-z ]"") # regex for all characters that are NOT A-Z, a-z and space "" ""
text = re.sub(remove_punctuation_regex, """", text) # sub all non alphabetical characters with empty string """"
</code></pre>

<p>Finally, you may also want to filter on most frequent or least frequent words in your corpus, which you can do using nltk:</p>

<pre><code>from nltk import FreqDist
all_words = text.split() # list of all the words in your corpus
fdist = FreqDist(all_words) # a frequency distribution of words (word count over the corpus)
k = 10000 # say you want to see the top 10,000 words
top_k_words, _ = zip(*fdist.most_common(k)) # unzip the words and word count tuples
print(top_k_words) # print the words and inspect them to see which ones you want to keep and which ones you want to disregard
</code></pre>

<p>That should get rid of the stopwords and extra characters, but still leaves the vast problem of topic modelling (which I wont try to explain here but will leave some tips and links).</p>

<p>Assuming you know a little bit about topic modelling, lets start. LDA is a bag of words model, meaning word order doesnt matter. The model assigns a topic distribution (of a predetermined number of topics K) to each document, and a word distribution to each topic. A very insightful <a href=""https://www.youtube.com/watch?v=3mHy4OSyRf0"" rel=""noreferrer"">high level video explains this here</a>. If you want to see more of the mathematics, but still at an accessible level, check out <a href=""https://www.youtube.com/watch?v=HyuBTMaKFmU&amp;t=193s"" rel=""noreferrer"">this video</a>. The more documents the better, and usually longer documents (with more words) also fair better using LDA - <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.4032&amp;rep=rep1&amp;type=pdf"" rel=""noreferrer"">this paper</a> shows that LDA doesnt perform well with short texts (less than ~20 words). K is up to you to choose, and really depends on your corpus of documents (how large it is, what different topics it covers etc.). Usually a good value of K is between 100-300, but again this really depends on your corpus.</p>

<p>LDA has two hyperparamters, alpha and beta (alpha and eta in gemsim) - a higher alpha means each text will be represented by more topics (so naturally a lower alpha means each text will be represented by less topics). A high eta means each topic is represented by more words, and a low eta means each topic is represented by less words - so with a low eta you would get less ""overlap"" between topics.</p>

<p>There's many insights you could gain using LDA</p>

<ol>
<li><p>What are the topics in a corpus (naming topics may not matter to your application, but if it does this can be done by inspecting the words in a topic as you have done above)</p></li>
<li><p>What words contribute most to a topic</p></li>
<li><p>What documents in the corpus are most similar (using a <a href=""https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence"" rel=""noreferrer"">similarity metric</a>)</p></li>
</ol>

<p>Hope this has helped. I was new to LDA a few months ago but I've quickly gotten up to speed using stackoverflow and youtube!</p>
",35,13,10385,2017-09-20 15:30:07,https://stackoverflow.com/questions/46326173/understanding-lda-topic-modelling-too-much-topic-overlap
TFIDIF Model Creation TypeError in Gensim,"<p>TypeError: 'TfidfModel' object is not callable</p>

<p><strong>Why can I not compute the TFIDF Matrix for each Doc after initializing?</strong></p>

<p>I started with 999 <em>documents</em>: 999 paragraphs with about 5-15 sentences each.
After spaCy tokenizing everything, I created the <em>dictionary</em> (~16k unique tokens) and <em>corpus</em> (a list of lists of tuples)</p>

<p>Now I'm ready to create the tfidf matrix (and later LDA and w2V matricies) for some ML; however, after initializing the tfidf model with my corpus (for calculation of the 'IDF')
<code>tfidf = models.TfidfModel(corpus)</code> I get the following error message when trying to see the tfidf of each doc <code>tfidf(corpus[5])</code>
<strong>TypeError: 'TfidfModel' object is not callable</strong></p>

<p>I am able to create this model using a differnt corpus where i have four docs each comprised of only a sentence.
There I can confirm that the expected corpus fomat is a list of lists of tuples: 
[doc1[(word1, count),(word2, count),...], doc2[(word3, count),(word4,count),...]...]</p>

<pre><code>from gensim import corpora, models, similarities

texts = [['teenager', 'martha', 'moxley'...], ['ok','like','kris','usual',...]...]
dictionary = corpora.Dictionary(texts)
&gt;&gt;&gt; Dictionary(15937 unique tokens: ['teenager', 'martha', 'moxley']...)

corpus = [dictionary.doc2bow(text) for text in texts]
&gt;&gt;&gt; [[(0, 2),(1, 2),(2, 1)...],[(3, 1),(4, 1)...]...]

tfidf = models.TfidfModel(corpus)
&gt;&gt;&gt; TfidfModel(num_docs=999, num_nnz=86642)

tfidf(corpus[0])
&gt;&gt;&gt; TypeError: 'TfidfModel' object is not callable

corpus[0]
&gt;&gt;&gt; [(0, 2),(1, 2),(2, 1)...]

print(type(corpus),type(corpus[1]),type(corpus[1][3]))
&gt;&gt;&gt; &lt;class 'list'&gt; &lt;class 'list'&gt; &lt;class 'tuple'&gt;
</code></pre>
","python, nlp, gensim, tf-idf, language-features","<p>Expanding on @whs2k's answer, the square bracket syntax is used to form a transformation wrapper around the corpus, forming a kind of lazy processing pipeline.</p>

<p>I didn't get it until I read the note in this tutorial: <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/tut2.html</a></p>

<blockquote>
  <p>Calling model[corpus] only creates a wrapper around the old corpus
  document stream – actual conversions are done on-the-fly, during
  document iteration. We cannot convert the entire corpus at the time of
  calling corpus_transformed = model[corpus], because that would mean
  storing the result in main memory, and that contradicts gensim’s
  objective of memory-indepedence. If you will be iterating over the
  transformed corpus_transformed multiple times, and the transformation
  is costly, serialize the resulting corpus to disk first and continue
  using that.</p>
</blockquote>

<p>But I still don't feel I fully understand the underlying Python list magic.</p>
",0,1,1276,2017-09-22 15:55:37,https://stackoverflow.com/questions/46368720/tfidif-model-creation-typeerror-in-gensim
Text Processing - Word2Vec training after phrase detection (bigram model),"<p>I want to make a word2vec model with more n-grams that usual. As I found, Phrase class in gensim.models.phrase can find phrases that I want and it's possible to use phrases on corpus and use it's result model for word2vec train function.</p>

<p>So first of all I do something like below, exactly like sample codes in <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim documentation</a>.</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield word_tokenize(line)

sentences = MySentences('sentences_directory')

bigram = gensim.models.Phrases(sentences)

model = gensim.models.Word2Vec(bigram['sentences'], size=300, window=5, workers=8)
</code></pre>

<p>model has been created but without any good result in evaluation and a warning :</p>

<pre><code>WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable)
</code></pre>

<p>I searched for it and I found <a href=""https://groups.google.com/forum/#!topic/gensim/XWQ8fPMFSi0"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/gensim/XWQ8fPMFSi0</a> and changed my code:</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield word_tokenize(line)

class PhraseItertor(object):
    def __init__(self, my_phraser, data):
        self.my_phraser, self.data = my_phraser, data

    def __iter__(self):
        yield self.my_phraser[self.data]


sentences = MySentences('sentences_directory')

bigram_transformer = gensim.models.Phrases(sentences)

bigram = gensim.models.phrases.Phraser(bigram_transformer)

corpus = PhraseItertor(bigram, sentences)

model = gensim.models.Word2Vec(corpus, size=300, window=5, workers=8)
</code></pre>

<p>I get error:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/fatemeh/Desktop/Thesis/bigramModeler.py"", line 36, in &lt;module&gt;
    model = gensim.models.Word2Vec(corpus, size=300, window=5, workers=8)
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 478, in init
    self.build_vocab(sentences, trim_rule=trim_rule)
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 553, in build_vocab
    self.scan_vocab(sentences, progress_per=progress_per, trim_rule=trim_rule)  # initial survey
  File ""/home/fatemeh/.local/lib/python3.4/site-packages/gensim/models/word2vec.py"", line 575, in scan_vocab
    vocab[word] += 1
TypeError: unhashable type: 'list'
</code></pre>

<p>Now I want to know that what is wrong in my codes.</p>
","python, gensim, word2vec, text-processing","<p>I asked my question in <a href=""https://groups.google.com/forum/#!topic/gensim/hJ216c59TGM"" rel=""nofollow noreferrer"">Gensim GoogleGroup</a> and <a href=""https://www.linkedin.com/in/gojomo"" rel=""nofollow noreferrer"">Mr Gordon Mohr</a> answered me:</p>

<blockquote>
  <p>You typically wouldn't want an <code>__iter__()</code> method to do a single
  <code>yield</code>. It should return an iterator object (ready to return multiple
  objects via <code>next()</code> or a StopIteration exception). One way to effect
  a iterator is to use <code>yield</code> to have the method treated as a
  'generator' – but that would typically require the <code>yield</code> to be
  inside a loop. </p>
  
  <p>But I now see that my example code in the thread you reference does
  the wrong thing with <em>its</em> <code>__iter__()</code> return line: it should not be
  returning the raw phrasifier, but one that has already been
  started-as-an-iterator, by use of the <code>iter()</code> built-in method. That
  is, the example there should have read:</p>

<pre><code>class PhrasingIterable(object):
    def __init__(self, phrasifier, texts):
        self. phrasifier, self.texts = phrasifier, texts
    def __iter__():
        return iter(phrasifier[texts])
</code></pre>
  
  <p>Making a similar change in your variation may resolve the <code>TypeError:
  iter() returned non-iterator of type 'TransformedCorpus'</code> error.</p>
</blockquote>
",0,1,774,2017-09-26 08:47:19,https://stackoverflow.com/questions/46421771/text-processing-word2vec-training-after-phrase-detection-bigram-model
Import GoogleNews-vectors-negative300.bin,"<p>I am working on code using the gensim and having a tough time troubleshooting a ValueError within my code. I finally was able to zip GoogleNews-vectors-negative300.bin.gz file so I could implement it in my model. I also tried gzip which the results were unsuccessful. The error in the code occurs in the last line. I would like to know what can be done to fix the error. Is there any workarounds? Finally, is there a website that I could reference? </p>

<p>Thank you respectfully for your assistance! </p>

<pre><code>import gensim
from keras import backend
from keras.layers import Dense, Input, Lambda, LSTM, TimeDistributed
from keras.layers.merge import concatenate
from keras.layers.embeddings import Embedding
from keras.models import Mode

pretrained_embeddings_path = ""GoogleNews-vectors-negative300.bin""
word2vec = 
gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path, 
binary=True)

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-3-23bd96c1d6ab&gt; in &lt;module&gt;()
  1 pretrained_embeddings_path = ""GoogleNews-vectors-negative300.bin""
----&gt; 2 word2vec = 
gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path, 
binary=True)

C:\Users\green\Anaconda3\envs\py35\lib\site-
packages\gensim\models\keyedvectors.py in load_word2vec_format(cls, fname, 
fvocab, binary, encoding, unicode_errors, limit, datatype)
244                             word.append(ch)
245                     word = utils.to_unicode(b''.join(word), 
encoding=encoding, errors=unicode_errors)
--&gt; 246                     weights = fromstring(fin.read(binary_len), 
dtype=REAL)
247                     add_word(word, weights)
248             else:

ValueError: string size must be a multiple of element size
</code></pre>
","python, gensim","<p>Edit: The S3 url has stopped working. You can d<a href=""https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300"" rel=""nofollow noreferrer"">ownload the data from Kaggle</a> or use <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g"" rel=""nofollow noreferrer"">this Google Drive link</a> (be careful downloading files from Google Drive).</p>
<p>The below commands <strong>no longer work</strong> work.</p>
<pre><code>brew install wget

wget -c &quot;https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz&quot;
</code></pre>
<p>This downloads the GZIP compressed file that you can uncompress using:</p>
<pre class=""lang-bash prettyprint-override""><code>gzip -d GoogleNews-vectors-negative300.bin.gz
</code></pre>
<p>You can then use the below command to get <strong>wordVector</strong>.</p>
<pre><code>from gensim import models

w = models.KeyedVectors.load_word2vec_format(
    '../GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>
",47,29,84314,2017-09-26 18:51:02,https://stackoverflow.com/questions/46433778/import-googlenews-vectors-negative300-bin
Calculating similarity between Tfidf matrix and predicted vector causes memory overflow,"<p>I am have generated a tf-idf model on ~20,000,000 documents using the following code, which works well. The problem is when I try to calculate similarity scores when using linear_kernel the memory usage blows up:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

train_file = ""docs.txt""
train_docs = DocReader(train_file) #DocReader is a generator for individual documents

vectorizer = TfidfVectorizer(stop_words='english',max_df=0.2,min_df=5)
X = vectorizer.fit_transform(train_docs)

#predicting a new vector, this works well when I check the predictions
indoc = ""This is an example of a new doc to be predicted""
invec = vectorizer.transform([indoc])

#This is where the memory blows up
similarities = linear_kernel(invec, X).flatten()
</code></pre>

<p>Seems like this shouldn't take up much memory, doing a comparison of a 1-row-CSR to a 20mil-row-CSR should output a 1x20mil ndarray.</p>

<p>Justy FYI: X is a CSR matrix ~12 GB in memory (my computer only has 16). I have tried looking into gensim to replace this but I can't find a great example.</p>

<p>Any thoughts on what I am missing?</p>
","python, scikit-learn, gensim, tf-idf, csr","<p>You can do the processing in batches. Here is an example based on your code snippet but replacing the dataset to something in sklearn. For this smaller dataset, I compute it the original way as well to show that the results are equivalent. You can probably use a larger batchsize.</p>

<pre><code>import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from sklearn.datasets import fetch_20newsgroups

train_docs = fetch_20newsgroups(subset='train')

vectorizer = TfidfVectorizer(stop_words='english', max_df=0.2,min_df=5)
X = vectorizer.fit_transform(train_docs.data)

#predicting a new vector, this works well when I check the predictions
indoc = ""This is an example of a new doc to be predicted""
invec = vectorizer.transform([indoc])

#This is where the memory blows up
batchsize = 1024
similarities = []
for i in range(0, X.shape[0], batchsize):
    similarities.extend(linear_kernel(invec, X[i:min(i+batchsize, X.shape[0])]).flatten())
similarities = np.array(similarities)
similarities_orig = linear_kernel(invec, X)
print((similarities == similarities_orig).all())
</code></pre>

<p>Output:</p>

<pre><code>True
</code></pre>
",0,0,668,2017-09-26 20:25:36,https://stackoverflow.com/questions/46435220/calculating-similarity-between-tfidf-matrix-and-predicted-vector-causes-memory-o
Why do I get single letter vocabulary in Gensim word2vec?,"<p>I am building a word2vec model as follows.</p>

<pre><code>from gensim.models import word2vec, Phrases
documents = [""the mayor of new york was there"", ""human computer interaction and machine learning has now become a trending research area"",""human computer interaction is interesting"",""human computer interaction is a pretty interesting subject"", ""human computer interaction is a great and new subject"", ""machine learning can be useful sometimes"",""new york mayor was present"", ""I love machine learning because it is a new subject area"", ""human computer interaction helps people to get user friendly applications""]
sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')
trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')

for sent in sentence_stream:
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]

    print(bigrams_)
    print(trigrams_)


# Set values for various parameters
num_features = 10    # Word vector dimensionality                      
min_word_count = 1   # Minimum word count                        
num_workers = 4       # Number of threads to run in parallel
context = 5          # Context window size                                                                                    
downsampling = 1e-3   # Downsample setting for frequent words


model = word2vec.Word2Vec(trigrams_, workers=num_workers, \
            size=num_features, min_count = min_word_count, \
            window = context, sample = downsampling)

vocab = list(model.wv.vocab.keys())
print(vocab[:10])
</code></pre>

<p>However, the output I get for the model's vocabulary is single character as follows. </p>

<pre><code>['h', 'u', 'm', 'a', 'n', ' ', 'c', 'o', 'p', 't']
</code></pre>

<p>I am getting the bigrams and trigrams correctly. Hence, I am just confused where I make the code wrong. Please let me know what is the problem?</p>
","python, gensim, word2vec","<p>This solved my issue. I should pass list of lists to the word2vec model as follows.</p>

<pre><code>trigram_sentences_project = []


bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')
trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')


for sent in sentence_stream:
    #bigrams_ = [b for b in bigram[sent] if b.count(' ') == 1]
    #trigrams_ = [t for t in trigram[bigram[sent]] if t.count(' ') == 2]
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]
    trigram_sentences_project.append(trigrams_)
</code></pre>
",4,6,2286,2017-09-27 07:27:33,https://stackoverflow.com/questions/46441876/why-do-i-get-single-letter-vocabulary-in-gensim-word2vec
How to access topic words only in gensim,"<p>I built LDA model using Gensim and I want to get the topic words only How can I get the words of the topics only no probabilities and no IDs.words only </p>

<p>I tried print_topics() and show_topics() functions in gensim but I can't get clean words ! </p>

<p>This is the code I used</p>

<pre><code>dictionary = corpora.Dictionary(doc_clean)
doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(doc_term_matrix, num_topics=12, id2word = dictionary, passes = 100, alpha='auto', update_every=5)
x = ldamodel.print_topics(num_topics=12, num_words=5)
for i in x:
    print(i[1])
    #print('\n' + str(i))

0.045*تعرض + 0.045*الماضية + 0.045*السنوات + 0.045*وءسرته + 0.045*لءحمد
0.021*مصر + 0.021*الديمقراطية + 0.021*حرية + 0.021*باسم + 0.021*الحكومة
0.068*المواطنة + 0.068*الطاءفية + 0.068*وانهيارات + 0.068*رابطة + 0.005*طبول
0.033*عربية + 0.033*انكسارات + 0.033*رهابيين + 0.033*بحقوق + 0.033*ل
0.007*وحريات + 0.007*ممنهج + 0.007*قواءم + 0.007*الناس + 0.007*دراج
0.116*طبول + 0.116*الوطنية + 0.060*يكتب + 0.060*مصر + 0.005*عربية
0.064*قيم + 0.064*وهن + 0.064*عربيا + 0.064*والتعددية + 0.064*الديمقراطية
0.036*تضامنا + 0.036*الشخصية + 0.036*مع + 0.036*التفتيش + 0.036*الءخلاق
0.052*تضامنا + 0.052*كل + 0.052*محمد + 0.052*الخلوق + 0.052*مظلوم
0.034*بمواطنين + 0.034*رهابية + 0.034*لم + 0.034*عليهم + 0.034*يثبت
0.035*مع + 0.035*ومستشار + 0.035*يستعيدا + 0.035*ءرهقهما + 0.035*حريتهما
0.064*للقمع + 0.064*قريبة + 0.064*لا + 0.064*نهاية + 0.064*مصر
</code></pre>

<p>I tried show_topics and it gave the same output</p>

<pre><code>y = np.array(ldamodel.show_topics(num_topics=12, num_words=5))
for i in y[:,1]:
    #if i != '%d':
    #print([str(word) for word in i])
    print(i)
</code></pre>

<p>If I have the topic ID how can I access its words and other informations </p>

<p>Thanks in Advance</p>
","python, nlp, gensim, lda, topic-modeling","<p>I think the below code snippet should give you a list of tuples containing the each topic(tp) and corresponding list of words(wd) in that topic</p>

<pre><code>x=ldamodel.show_topics(num_topics=12, num_words=5,formatted=False)
topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]

#Below Code Prints Topics and Words
for topic,words in topics_words:
    print(str(topic)+ ""::""+ str(words))
print()

#Below Code Prints Only Words 
for topic,words in topics_words:
    print("" "".join(words))
</code></pre>
",7,10,9331,2017-10-03 01:58:24,https://stackoverflow.com/questions/46536132/how-to-access-topic-words-only-in-gensim
applying the Similar function in Gensim.Doc2Vec,"<p>I am trying to get the doc2vec function to work in python 3.
I Have the following code:</p>

<pre><code>tekstdata = [[ index, str(row[""StatementOfTargetFiguresAndPoliciesForTheUnderrepresentedGender""])] for index, row in data.iterrows()]
def prep (x):
    low = x.lower()
    return word_tokenize(low)

def cleanMuch(data, clean):
    output = []
    for x, y in data:
        z = clean(y)
        output.append([str(x), z])
    return output

tekstdata = cleanMuch(tekstdata, prep)

def tagdocs(docs):
    output = []    
    for x,y in docs:
        output.append(gensim.models.doc2vec.TaggedDocument(y, x))
    return output
    tekstdata = tagdocs(tekstdata)

    print(tekstdata[100])

vectorModel = gensim.models.doc2vec.Doc2Vec(tekstdata, size = 100, window = 4,min_count = 3, iter = 2)


ranks = []
second_ranks = []
for x, y in tekstdata:
 print (x)
 print (y)
 inferred_vector = vectorModel.infer_vector(y)
 sims = vectorModel.docvecs.most_similar([inferred_vector], topn=1001,   restrict_vocab = None)
rank = [docid for docid, sim in sims].index(y)
ranks.append(rank)
</code></pre>

<p>All works as far as I can understand until the rank function. 
The error I get is that there is no zero in my list e.g. the documents I am putting in does not have 10 in list:</p>

<pre><code>  File ""C:/Users/Niels Helsø/Documents/github/Speciale/Test/Data prep.py"", line 59, in &lt;module&gt;
rank = [docid for docid, sim in sims].index(y)

ValueError: '10' is not in list
</code></pre>

<p>It seems to me that it is the similar function that does not work. 
the model trains on my data (1000 documents) and build a vocab which is tagged.
The documentation I mainly have used is this: 
<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Gensim dokumentation</a>
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">Torturial</a></p>

<p>I hope that some one can help. If any additional info is need please let me know. 
best
Niels</p>
","python, gensim, doc2vec","<p>If you're getting <code>ValueError: '10' is not in list</code>, you can rely on the fact that <code>'10'</code> is not in the list. So have you looked at the list, to see what is there, and if it matches what you expect? </p>

<p>It's not clear from your code excerpts that <code>tagdocs()</code> is ever called, and thus unclear what form <code>tekstdata</code> is in when provided to <code>Doc2Vec</code>. The intent is a bit convoluted, and there's nothing to display what the data appears as in its raw, original form. </p>

<p>But perhaps the <code>tags</code> you are supplying to <code>TaggedDocument</code> are not the required list-of-tags, but rather a simple string, which will be interpreted as a list-of-characters. As a result, even if you're supplying a <code>tags</code> of <code>'10'</code>, it will be seen as <code>['1', '0']</code> – and <code>len(vectorModel.doctags)</code> will be just 10 (for the 10 single-digit strings). </p>

<p>Separate comments on your setup:</p>

<ul>
<li>1000 documents is pretty small for <code>Doc2Vec</code>, where most published results use tens-of-thousands to millions of documents</li>
<li>an <code>iter</code> of 10-20 is more common in <code>Doc2Vec</code> work (and even larger values might be helpful with smaller datasets)</li>
<li><code>infer_vector()</code> often works better with non-default values in its optional parameters, especially a <code>steps</code> that's much larger (20-200) or a starting <code>alpha</code> that's more like the bulk-training default (<code>0.025</code>)</li>
</ul>
",1,2,265,2017-10-04 08:09:53,https://stackoverflow.com/questions/46559980/applying-the-similar-function-in-gensim-doc2vec
Can we build word2vec model in a distributed way?,"<p>Currently I have 1.2tb text data to build gensim's word2vec model. It is almost taking 15 to 20 days to complete. </p>

<p>I want to build model for 5tb of text data, then it might take few months to create model. I need to minimise this execution time. Is there any way we can use multiple big systems to create model? </p>

<p>Please suggest any way which can help me in reducing the execution time.</p>

<p>FYI, I have all my data in S3 and I use smart_open module to stream the data.</p>
","nlp, deep-learning, distributed-computing, gensim, word2vec","<p>Training a model with a huge corpus will surely take a very long time because of a large number of weights involved.  Suppose your word vectors have 300 components and your vocabulary size is 10,000. The size of weight matrix would be 300*10000 = 3 million!</p>

<p>To build a model for huge datasets I would recommend you to first preprocess the dataset. Following preprocessing steps can be applied:</p>

<ul>
<li>Removing stop words.</li>
<li>Treating word pairs or phrases as single words, like new york as new_york, etc.</li>
<li>Subsampling frequent words to decrease the number of training examples.</li>
<li>Modifying the optimization objective with a technique they called “Negative Sampling”, which causes each training sample to update only a small percentage of the model’s weights. </li>
</ul>

<p>The above tasks were also done in official word2vec implementation released by Google. Gensim provides very beautiful high-level APIs to perform most of above tasks. Also, have a look at this <a href=""https://rare-technologies.com/word2vec-in-python-part-two-optimizing/"" rel=""nofollow noreferrer"">blog</a> for further optimizing techniques.</p>

<p>One more thing that can be done is instead of training your own model use the already trained <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"" rel=""nofollow noreferrer"">word2vec mode</a>l released by Google It’s 1.5GB and includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. </p>
",0,3,1011,2017-10-06 19:45:38,https://stackoverflow.com/questions/46612949/can-we-build-word2vec-model-in-a-distributed-way
How to manually change the vector dimensions of a word in Gensim Word2Vec,"<p>I have a Word2Vec model with a lot of word vectors. I can access a word vector as so. </p>

<pre><code>word_vectors = gensim.models.Word2Vec.load(wordspace_path)
print(word_vectors['boy'])
</code></pre>

<p><strong>Output</strong></p>

<pre><code>[ -5.48055351e-01   1.08748421e-01  -3.50534245e-02  -9.02988110e-03...]
</code></pre>

<p>Now I have a proper vector representation that I want to replace the word_vectors['boy'] with.</p>

<pre><code>word_vectors['boy'] = [ -7.48055351e-01   3.08748421e-01  -2.50534245e-02  -10.02988110e-03...]
</code></pre>

<p>But the following error is thrown</p>

<pre><code>TypeError: 'Word2Vec' object does not support item assignment
</code></pre>

<p>Is there any fashion or workaround to do this? That is manipulate word vectors manually once the model is trained? Is it possible in other platforms except Gensim? </p>
","python, vector, gensim, word2vec, vector-space","<p>Since word2vec vectors are typically only created by the iterative training process, then accessed, the gensim <code>Word2Vec</code> object does not support direct assignment of new values by its word indexes.</p>

<p>However, as it is in Python, all its internal structures are fully viewable/tamperable by you, and as it is open-source, you can view exactly how it does all of its existing functionality, and use that as a model for how to do new things. </p>

<p>Specifically, the raw word-vectors are (in recent versions of gensim) stored in a property of the <code>Word2Vec</code> object called <code>wv</code>, and this <code>wv</code> property is an instance of <code>KeyedVectors</code>. If you examine its source code, you can see accesses of word-vectors by string key (eg <code>'boy'</code>), including those by <code>[]</code>-indexing implemented by the <code>__getitem__()</code> method, go through its method <code>word_vec()</code>. You can view the source of that method either in your local installation, or at Github:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/c2201664d5ae03af8d90fb5ff514ffa48a6f305a/gensim/models/keyedvectors.py#L265"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/c2201664d5ae03af8d90fb5ff514ffa48a6f305a/gensim/models/keyedvectors.py#L265</a></p>

<p>There you'll see the word is actually converted to an integer-index (via <code>self.vocab[word].index</code>) then used to access an internal <code>syn0</code> or <code>syn0norm</code> array (depending on whether the user is accessing the raw or unit-normalized vector). If you look elsewhere where these are set up, or simply examine them in your own console/code (as if by <code>word_vectors.wv.syn0</code>), you'll see these are <code>numpy</code> arrays which <em>do</em> support direct assignment by index.</p>

<p>So, you <em>can</em> directly tamper with their values by integer index, as if by:</p>

<pre><code>word_vectors.wv.syn0[word_vectors.wv.vocab['boy'].index] = [ -7.48055351e-01   3.08748421e-01  -2.50534245e-02  -10.02988110e-03...]
</code></pre>

<p>And then, future accesses of <code>word_vectors.wv['boy']</code> will return your updated values. </p>

<p>Notes:</p>

<p>• If you want <code>syn0norm</code> to be updated, to have the proper unit-normed vectors (as are used in <code>most_similar()</code> and other operations), it'd likely be best to modify <code>syn0</code> first, then discard and recalculate <code>syn0norm</code>, via:</p>

<pre><code>word_vectors.wv.syn0norm = None
word_vectors.wv.init_sims()
</code></pre>

<p>• Adding new words would require more involved object-tampering, because it will require growing the <code>syn0</code> (replacing it with a larger array), and updating the <code>vocab</code> dict</p>
",11,5,3037,2017-10-09 13:41:42,https://stackoverflow.com/questions/46647945/how-to-manually-change-the-vector-dimensions-of-a-word-in-gensim-word2vec
Gensim doc2vec sentence tagging,"<p>Im trying to understand doc2vec and can I use it to solve my scenario. I want to label sentences with 1 or more tags using TaggedSentences([words], [tags]), but im unsure If my understanding is correct.</p>

<p>so basically, i need this to happen(or am I totally off the mark)</p>

<p>I create 2 TaggedDocuments</p>

<pre><code>TaggedDocument(words=[""the"", ""bird"", ""flew"", ""over"", ""the"", ""coocoos"", ""nest"", labels=[""animal"",""tree""])
TaggedDocument(words=[""this"", ""car"", ""is"", ""over"", ""one"", ""million"", ""dollars"", labels=[""motor"",""money""])
</code></pre>

<p>I build my model</p>

<pre><code>model = gensim.models.Doc2Vec(documents, dm=0, alpha=0.025, size=20, min_alpha=0.025, min_count=0)
</code></pre>

<p>Then I train my model</p>

<pre><code>model.train(documents, total_examples=len(documents), epochs=1)
</code></pre>

<p>So when I have all that done, what I expect is when I execute</p>

<pre><code>model.most_similar(positive=[""bird"", ""flew"", ""over"", ""nest])
</code></pre>

<p>is  [animal,tree], but I get</p>

<pre><code>[('the', 0.4732949137687683), 
('million', 0.34103643894195557),
('dollars', 0.26223617792129517),
('one', 0.16558100283145905),
('this', 0.07230066508054733),
('is', 0.012532509863376617),
('cocos', -0.1093338280916214),
('car', -0.13764989376068115)]
</code></pre>

<p>UPDATE:
when I infer</p>

<pre><code>vec_model = model.Word2Vec.load(os.path.join(""save"",""vec.w2v""))
infer = vec_model.infer_vector([""bird"", ""flew"", ""over"", ""nest""])
print(vec_model.most_similar(positive=[infer], topn=10))
</code></pre>

<p>I get</p>

<pre><code>[('bird', 0.5196993350982666),
('car', 0.3320297598838806), 
('the',  0.1573483943939209), 
('one', 0.1546170711517334), 
('million',  0.05099521577358246),
('over', -0.0021460093557834625), 
('is',  -0.02949431538581848),
('dollars', -0.03168443590402603), 
('flew', -0.08121247589588165),
('nest', -0.30139490962028503)]
</code></pre>

<p>So the elephant in the room, Is doc2vec what I need to accomplish the above scenario, or should I go back to bed and have a proper think about what Im trying to achieve in life :)</p>

<p>Any help greatly appreciated</p>
","python, machine-learning, data-science, gensim, doc2vec","<p>It's not clear what your goal is. </p>

<p>Your code examples are a bit muddled; there's no way the <code>TaggedDocument</code> constructions, as currently shown, will result in good text examples. (<code>words</code> needs to be a list of words, not a string with a bunch of comma-separated tokens.) </p>

<p>If you ask <code>model</code> for similarities, you'll get words – if you want doc-tags, you'll have to ask the model's <code>docvecs</code> sub-property. (That is, <code>model.docvecs.most_similar()</code>.) </p>

<p>Regarding your training parameters, there's no good reason to change the default <code>min_alpha</code> to be equal to the starting-alpha. A <code>min_count=0</code>, retaining all words, usually makes word2vec/doc2vec vectors worse. And the algorithm typically needs many passes over the data – usually 10 or more – rather than one. </p>

<p>But also, word2vec/doc2vec really needs bulk data to achieve its results – toy-sized tests rarely show the same beneficial properties that are possible with larger datasets. </p>
",1,0,1950,2017-10-10 19:37:00,https://stackoverflow.com/questions/46674609/gensim-doc2vec-sentence-tagging
Does gensim.corpora.Dictionary have term frequency saved?,"<p><strong>Does gensim.corpora.Dictionary have term frequency saved?</strong> </p>

<p>From <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""noreferrer""><code>gensim.corpora.Dictionary</code></a>, it's possible to get the document frequency of the words (i.e. how many document did a particular word occur in):</p>

<pre><code>from nltk.corpus import brown
from gensim.corpora import Dictionary

documents = brown.sents()
brown_dict = Dictionary(documents)

# The 100th word in the dictionary: 'these'
print('The word ""' + brown_dict[100] + '"" appears in', brown_dict.dfs[100],'documents')
</code></pre>

<p>[out]:</p>

<pre><code>The word ""these"" appears in 1213 documents
</code></pre>

<p>And there is the <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_n_most_frequent"" rel=""noreferrer""><code>filter_n_most_frequent(remove_n)</code></a> function that can remove the n-th most frequent tokens: </p>

<blockquote>
  <p><code>filter_n_most_frequent(remove_n)</code>
  Filter out the ‘remove_n’ most frequent tokens that appear in the documents.</p>
  
  <p>After the pruning, shrink resulting gaps in word ids.</p>
  
  <p>Note: Due to the gap shrinking, the same word may have a different word id before and after the call to this function!</p>
</blockquote>

<p><strong>Is the <code>filter_n_most_frequent</code> function removing the n-th most frequent based on the document frequency or term frequency?</strong> </p>

<p>If it's the latter, <strong>is there some way to access the term frequency of the words in the <code>gensim.corpora.Dictionary</code> object?</strong></p>
","python, dictionary, frequency, gensim, tf-idf","<p>No, <code>gensim.corpora.Dictionary</code> does not save term frequency. You can <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/dictionary.py"" rel=""noreferrer"">see the source code here</a>. The class only stores the following member variables:</p>

<pre><code>    self.token2id = {}  # token -&gt; tokenId
    self.id2token = {}  # reverse mapping for token2id; only formed on request, to save memory
    self.dfs = {}  # document frequencies: tokenId -&gt; in how many documents this token appeared

    self.num_docs = 0  # number of documents processed
    self.num_pos = 0  # total number of corpus positions
    self.num_nnz = 0  # total number of non-zeroes in the BOW matrix
</code></pre>

<p>This means everything in the class defines frequency as document frequency, never term frequency, as the latter is never stored globally. This applies to <code>filter_n_most_frequent(remove_n)</code> as well as every other method.</p>
",8,7,12495,2017-10-11 09:37:55,https://stackoverflow.com/questions/46684810/does-gensim-corpora-dictionary-have-term-frequency-saved
How to create gensim word2vec model using pre trained word vectors?,"<p>I have created word vectors using a distributed word2vec algorithm. Now I have words and their corresponding vectors. How to build a gensim word2vec model using these words and vectors? </p>
","nlp, gensim, word2vec, text-analysis, word-embedding","<p>I am not sure if you created word2vec model using <code>gensim</code> or some other tools but if understand your question correctly you want to just load the word2vec model using gensim. This is done in the following way:</p>

<pre><code>import gensim
w2v_file = codecs.open(WORD2VEC_PATH, encoding='utf-8')
model = gensim.models.KeyedVectors.load_word2vec_format(w2v_file, binary=True)  # or binary=False if the model is not compressed
</code></pre>

<p>If, however, what you want to do is to train word2vec model from scratch (i.e. from raw text) using purely <code>gensim</code> here is a <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">tutorial on how to train word2vec model using gensim</a>.</p>
",4,3,3336,2017-10-12 03:59:07,https://stackoverflow.com/questions/46701173/how-to-create-gensim-word2vec-model-using-pre-trained-word-vectors
gensim KeyedVectors object word count,"<p>I load a KeyedVectors model and the word frequency seems like word index</p>

<p>And I miss something?</p>

<p><a href=""https://i.sstatic.net/Fy0vC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fy0vC.png"" alt=""enter image description here""></a></p>
",gensim,"<p>The single-file format loaded by <code>load_word2vec_format()</code> doesn't include word counts – so they can't appear in the loaded object. </p>

<p>The usual convention is to put such files in most-frequent to least-frequent order, though. So in the absence of true count information, a plug value is used that decreases from the vocabulary-size to 1. (That's the number that's somewhat like the word index you're seeing.) </p>

<p>There's a way in some software to save extra info in a separate file – see the <code>fvocab</code> option of gensim's <code>save_word2vec_format()</code> and <code>load_word2vec_format()</code>. So perhaps that's already available with your vectors, and you can use that option. </p>
",3,-1,646,2017-10-16 03:06:11,https://stackoverflow.com/questions/46762366/gensim-keyedvectors-object-word-count
What are doc2vec training iterations?,"<p>I am new to doc2vec. I was initially trying to understand doc2vec and mentioned below is my code that uses Gensim. As I want I get a trained model and document vectors for the two documents.</p>

<p>However, I would like to know the benefits of retraining the model in several epoches and how to do it in Gensim? Can we do it using <code>iter</code> or <code>alpha</code> parameter or do we have to train it in a seperate <code>for loop</code>? Please let me know how I should change the following code to train the model for 20 epoches.</p>

<p>Also, I am interested in knowing is the multiple training iterations are needed for word2vec model as well.</p>

<pre><code># Import libraries
from gensim.models import doc2vec
from collections import namedtuple

# Load data
doc1 = [""This is a sentence"", ""This is another sentence""]

# Transform data
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for i, text in enumerate(doc1):
    words = text.lower().split()
    tags = [i]
    docs.append(analyzedDocument(words, tags))

# Train model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)

# Get the vectors
model.docvecs[0]
model.docvecs[1]
</code></pre>
","python, deep-learning, word2vec, gensim, doc2vec","<p><code>Word2Vec</code> and related algorithms (like 'Paragraph Vectors' aka <code>Doc2Vec</code>) usually make multiple training passes over the text corpus. </p>

<p>Gensim's <code>Word2Vec</code>/<code>Doc2Vec</code> allows the number of passes to be specified by the <code>iter</code> parameter, if you're also supplying the corpus in the object initialization to trigger immediate training. (Your code above does this by supplying <code>docs</code> to the <code>Doc2Vec(docs, ...)</code> constructor call.)</p>

<p>If unspecified, the default <code>iter</code> value used by gensim is 5, to match the default used by Google's original word2vec.c release. So your code above is already using 5 training passes. </p>

<p>Published <code>Doc2Vec</code> work often uses 10-20 passes. If you wanted to do 20 passes instead, you could change your <code>Doc2Vec</code> initialization to:</p>

<pre><code>model = doc2vec.Doc2Vec(docs, iter=20, ...)
</code></pre>

<p>Because <code>Doc2Vec</code> often uses unique identifier tags for each document, more iterations can be more important, so that every doc-vector comes up for training multiple times over the course of the training, as the model gradually improves. On the other hand, because the words in a <code>Word2Vec</code> corpus might appear anywhere throughout the corpus, each words' associated vectors will get multiple adjustments, early and middle and late in the process as the model improves – even with just a single pass. (So with a giant, varied <code>Word2Vec</code> corpus, it's thinkable to use fewer than the default-number of passes.)</p>

<p>You <em>don't</em> need to do your own loop, and most users shouldn't. If you do manage the separate <code>build_vocab()</code> and <code>train()</code> steps yourself, instead of the easier step of supplying the <code>docs</code> corpus in the initializer call to trigger immediate training, then you must supply an <code>epochs</code> argument to <code>train()</code> – and it will perform that number of passes, so you still only need one call to <code>train()</code>. </p>
",7,3,4703,2017-10-18 09:33:27,https://stackoverflow.com/questions/46807010/what-are-doc2vec-training-iterations
Doc2vec and word2vec with negative sampling,"<p>My current doc2vec code is as follows.</p>

<pre><code># Train doc2vec model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4, iter = 20)
</code></pre>

<p>I also have a word2vec code as below.</p>

<pre><code> # Train word2vec model
model = word2vec.Word2Vec(sentences, size=300, sample = 1e-3, sg=1, iter = 20)
</code></pre>

<p>I am interested in using both DM and DBOW in <strong>doc2vec</strong> AND both Skip-gram and CBOW in <strong>word2vec</strong>.</p>

<p>In Gensim I found the below mentioned sentence:
<strong>""Produce word vectors with deep learning via word2vec’s “skip-gram and CBOW models”, using either hierarchical softmax or negative sampling""</strong></p>

<p>Thus, I am confused either to use hierarchical softmax or negative sampling. Please let me know what are the <strong>differences</strong> in these two methods.</p>

<p>Also, I am interested in knowing <strong>what are the parameters that need to be changed</strong> to use <strong>hierarchical softmax</strong> AND/OR <strong>negative sampling</strong> with respect to <strong>dm, DBOW, Skip-gram and CBOW</strong>?</p>

<p>P.s. my application is a recommendation system :)</p>
","python, nlp, word2vec, gensim, doc2vec","<p>Skip-gram or CBOW are different ways to choose the <em>input contexts</em> for the neural-network. Skip-gram picks one nearby word, then supplies it as input to try to predict a target word; CBOW averages together a bunch of nearby words, then supplies that average as input to try to predict a target word.</p>

<p>DBOW is most similar to skip-gram, in that a single paragraph-vector for a whole text is used to predict individual target words, regardless of distance and without any averaging. It can mix well with simultaneous skip-gram training, where in addition to using the single paragraph-vector, individual nearby word-vectors are also used. The gensim option <code>dbow_words=1</code> will add skip-gram training to a DBOW <code>dm=0</code> training.</p>

<p>DM is most similar to CBOW: the paragraph-vector is averaged together with a number of surrounding words to try to predict a target word. </p>

<p>So in Word2Vec, you must choose between skip-gram (<code>sg=1</code>) and CBOW (<code>sg=0</code>) – they can't be mixed. In Doc2Vec, you must choose between DBOW (<code>dm=0</code>) and DM (<code>dm=1</code>) - they can't be mixed. But you can, when doing Doc2Vec DBOW, also add skip-gram word-training (with <code>dbow_words=1</code>). </p>

<p>The choice between hierarchical-softmax and negative-sampling is separate and independent of the above choices. It determines how target-word predictions are read from the neural-network. </p>

<p>With negative-sampling, every possible prediction is assigned a single output-node of the network. In order to improve what prediction a particular input context creates, it checks the output-nodes for the 'correct' word (of the current training example excerpt of the corpus), and for N other 'wrong' words (that don't match the current training example). It then nudges the network's internal weights and the input-vectors to make the 'correct' word output node activation a little stronger, and the N 'wrong' word output node activations a little weaker. (This is called a 'sparse' approach, because it avoids having to calculate <em>every</em> output node, which is very expensive in large vocabularies, instead just calculation N+1 nodes and ignoring the rest.)</p>

<p>You could set negative-sampling with 2 negative-examples with the parameter <code>negative=2</code> (in Word2Vec or Doc2Vec, with any kind of input-context mode). The default mode, if no <code>negative</code> specified, is <code>negative=5</code>, following the default in the original Google word2vec.c code. </p>

<p>With hierarchical-softmax, instead of every preictable word having its own output node, some pattern of multiple output-node activations is interpreted to mean specific words. Which nodes should be closer to 1.0 or 0.0 in order to represent a word is matter of the word's encoding, which is calculated so that common words have short encodings (involving just a few nodes), while rare words will have longer encodings (involving more nodes). Again, this serves to save calculation time: to check if an input-context is driving just the right set of nodes to the right values to predict the 'correct' word (for the current training-example), just a few nodes need to be checked, and nudged, instead of the whole set. </p>

<p>You enable hierarchical-softmax in gensim with the argument <code>hs=1</code>. By default, it is not used. </p>

<p>You should generally disable negative-sampling, by supplying <code>negative=0</code>, if enabling hierarchical-softmax – typically one or the other will perform better for a given amount of CPU-time/RAM. </p>

<p>(However, following the architecture of the original Google word2vec.c code, it is possible but not recommended to have them both active at once, for example <code>negative=5, hs=1</code>. This will result in a larger, slower model, which might appear to perform better since you're giving it more RAM/time to train, but it's likely that giving equivalent RAM/time to just one or the other would be better.)</p>

<p>Hierarchical-softmax tends to get slower with larger vocabularies (because the average number of nodes involved in each training-example grows); negative-sampling does not (because it's always N+1 nodes). Projects with larger corpuses tend to trend towards preferring negative-sampling. </p>
",23,8,5132,2017-10-21 04:58:22,https://stackoverflow.com/questions/46860197/doc2vec-and-word2vec-with-negative-sampling
"How to create a DataFrame with the word2ve vectors as data, and the terms as row labels?","<p>I tried to follow this documentation:
nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb
Where I have the following code snippet:</p>

<pre><code>ordered_vocab = [(term, voc.index, voc.count)
             for term, voc in food2vec.vocab.iteritems()]

ordered_vocab = sorted(ordered_vocab, key=lambda (term, index, count): -count)

ordered_terms, term_indices, term_counts = zip(*ordered_vocab)

word_vectors = pd.DataFrame(food2vec.syn0norm[term_indices, :],
                        index=ordered_terms
</code></pre>

<p>To get it to run i have change it to following:</p>

<pre><code>ordered_vocab = [(term, voc.index, voc.count)
             for term, voc in word2vecda.wv.vocab.items()]
ordered_vocab = sorted(ordered_vocab)
ordered_terms, term_indices, term_counts = zip(*ordered_vocab)
word_vectorsda = pd.DataFrame(word2vecda.wv.syn0norm[term_indices,],index=ordered_terms)
word_vectorsda [:20]
</code></pre>

<p>But the last line before I print the DataFrame give me an error I cannot get my head around. It keeps return that the noneType object cannot be in this line. To me, it looks like it is Term_indices there tracking it, but I do not get why? </p>

<pre><code> TypeError: 'NoneType' object is not subscriptable
</code></pre>

<p>Can any help me with this? Any inputs are most welcome
Best Niels</p>
","python-3.x, pandas, word2vec, gensim","<p>Use the following code:</p>

<pre><code>ordered_vocab = [(term, voc.index, voc.count) for term, voc in model.wv.vocab.items()]
ordered_vocab = sorted(ordered_vocab, key=lambda k: k[2])
ordered_terms, term_indices, term_counts = zip(*ordered_vocab)
word_vectors = pd.DataFrame(model.wv.syn0[term_indices, :], index=ordered_terms)
</code></pre>

<p>Replace <code>model</code> with <code>food2vec</code>.<br>
Working on <code>python 3.6.1</code>, <code>gensim '3.0.0'</code></p>
",3,0,3063,2017-10-23 09:00:26,https://stackoverflow.com/questions/46885454/how-to-create-a-dataframe-with-the-word2ve-vectors-as-data-and-the-terms-as-row
"word2vec - what is best? add, concatenate or average word vectors?","<p>I am working on a recurrent language model. To learn word embeddings that can be used to initialize my language model, I am using gensim's word2vec model. 
After training, the word2vec model holds two vectors for each word in the vocabulary: the word embedding (rows of input/hidden matrix) and the context embedding (columns of hidden/output matrix).</p>

<p>As outlined in <a href=""https://stackoverflow.com/questions/36731784/wordvectors-how-to-concatenate-word-vectors-to-form-sentence-vector"">this post</a> there are at least three common ways to combine these two embedding vectors:</p>

<ol>
<li>summing the context and word vector for each word</li>
<li>summing &amp; averaging</li>
<li>concatenating the context and word vector</li>
</ol>

<p>However, I couldn't find proper papers or reports on the best strategy. So my questions are:</p>

<ol>
<li>Is there a common solution whether to sum, average or concatenate the vectors?</li>
<li>Or does the best way depend entirely on the task in question? If so, what strategy is best for a word-level language model?</li>
<li>Why combine the vectors at all? Why not use the ""original"" word embeddings for each word, i.e. those contained in the weight matrix between input and hidden neurons.</li>
</ol>

<p>Related (but unanswered) questions: </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/42119824/word2vec-summing-concatenate-inside-and-outside-vector?rq=1"">word2vec: Summing/concatenate inside and outside vector</a> </li>
<li><a href=""https://stackoverflow.com/questions/46065773/why-we-use-input-hidden-weight-matrix-to-be-the-word-vectors-instead-of-hidden-o?rq=1"">why we use input-hidden weight matrix to be the word vectors instead of hidden-output weight matrix?</a></li>
</ul>
","python, word2vec, gensim, word-embedding, language-model","<p>I have found an answer in the Stanford lecture ""Deep Learning for Natural Language Processing"" (Lecture 2, March 2016). It's available <a href=""https://www.youtube.com/watch?v=aRqn8t1hLxs"" rel=""noreferrer"">here</a>. In minute 46 Richard Socher states that the common way is to <strong>average</strong> the two word vectors.</p>
",8,21,20201,2017-10-23 12:44:40,https://stackoverflow.com/questions/46889727/word2vec-what-is-best-add-concatenate-or-average-word-vectors
Gensim Word2Vec uses too much memory,"<p>I want to train a word2vec model on a tokenized file of size 400MB. I have been trying to run this python code :
</p>

<pre><code>import operator
import gensim, logging, os
from gensim.models import Word2Vec
from gensim.models import *

class Sentences(object):
    def __init__(self, filename):
        self.filename = filename

    def __iter__(self):
        for line in open(self.filename):
            yield line.split()

def runTraining(input_file,output_file):
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    sentences = Sentences(input_file)
    model = gensim.models.Word2Vec(sentences, size=200)
    model.save(output_file)
</code></pre>

<p></p>

<p>When I call this function on my file, I get this :</p>

<pre><code>2017-10-23 17:57:00,211 : INFO : collecting all words and their counts
2017-10-23 17:57:04,071 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2017-10-23 17:57:16,116 : INFO : collected 4735816 word types from a corpus of 47054017 raw words and 1 sentences
2017-10-23 17:57:16,781 : INFO : Loading a fresh vocabulary
2017-10-23 17:57:18,873 : INFO : min_count=5 retains 290537 unique words (6% of original 4735816, drops 4445279)
2017-10-23 17:57:18,873 : INFO : min_count=5 leaves 42158450 word corpus (89% of original 47054017, drops 4895567)
2017-10-23 17:57:19,563 : INFO : deleting the raw counts dictionary of 4735816 items
2017-10-23 17:57:20,217 : INFO : sample=0.001 downsamples 34 most-common words
2017-10-23 17:57:20,217 : INFO : downsampling leaves estimated 35587188 word corpus (84.4% of prior 42158450)
2017-10-23 17:57:20,218 : INFO : estimated required memory for 290537 words and 200 dimensions: 610127700 bytes
2017-10-23 17:57:21,182 : INFO : resetting layer weights
2017-10-23 17:57:24,493 : INFO : training model with 3 workers on 290537 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2017-10-23 17:57:28,216 : INFO : PROGRESS: at 0.00% examples, 0 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:32,107 : INFO : PROGRESS: at 20.00% examples, 1314 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:36,071 : INFO : PROGRESS: at 40.00% examples, 1728 words/s, in_qsize 0, out_qsize 0
2017-10-23 17:57:41,059 : INFO : PROGRESS: at 60.00% examples, 1811 words/s, in_qsize 0, out_qsize 0
Killed
</code></pre>

<p>I know that word2vec needs a lot of space, but I still think there is a problem here. As you see the estimated memory for this model is of 600MB, while my computer has 16GB of RAM. Yet monitoring the process while the code runs shows that it occupies all of my memory and then gets killed.</p>

<p>As other posts advise I have tried to increase min_count and decrease size. But even with ridiculous values (min_count=50, size=10) the process stops at 60%.</p>

<p>I also tried to make python an exception to OOM so that the process doesn't get killed. When I do that, I have a MemoryError instead of the killing.</p>

<p>What is going on ?</p>

<p>(I use a recent laptop with Ubuntu 17.04, 16GB RAM and a Nvidia GTX 960M. I run python 3.6 from Anaconda and gensim 3.0, but it does'nt do better with gensim 2.3)</p>
","python-3.x, memory, word2vec, gensim","<p>Your file is a single line, as indicated by the log output:</p>

<pre><code>2017-10-23 17:57:16,116 : INFO : collected 4735816 word types from a corpus of 47054017 raw words and 1 sentences
</code></pre>

<p>It is doubtful that this is what you want; in particular the optimized cython code in gensim's <code>Word2Vec</code> can only handle sentences of 10,000 words before truncating them (and discarding the rest). So most of your data isn't being considered during training (even if it were to finish). </p>

<p>But the bigger problem is that single 47-million-word line will come into memory as one gigantic string, then be <code>split()</code> into a 47-million-entry list-of-strings. So your attempt to use a memory-efficient iterator isn't helping any – the full file is being brought into memory, twice over, for a single 'iteration'. </p>

<p>I still don't see that using a full 16GB RAM, but perhaps correcting that will resolve the issue, or make whatever remaining issues more evident. </p>

<p>If your tokenized data doesn't have natural line breaks around or below the 10,000-token sentence length, you can look how the example corpus class <code>LineSentence</code>, included in gensim to be able to work on the (also missing line breaks) <code>text8</code> or <code>text9</code> corpuses, limits each yielded sentence to 10,000 tokens:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/58b30d71358964f1fc887477c5dc1881b634094a/gensim/models/word2vec.py#L1620"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/58b30d71358964f1fc887477c5dc1881b634094a/gensim/models/word2vec.py#L1620</a></p>

<p>(It may not be a contributing factor but you may also want to use the <code>with</code> context-manager to ensure your <code>open()</code>ed file is promptly closed after the iterator is exhausted.)</p>
",3,4,4300,2017-10-23 21:48:12,https://stackoverflow.com/questions/46899062/gensim-word2vec-uses-too-much-memory
Doc2Vec model splits documents tags in symbols,"<p>I'm using <code>gensim 3.0.1</code>. </p>

<p>I have a list of <code>TaggedDocument</code> with unique labels of the form <code>""label_17""</code>, but when I train Doc2Vec model, it somehow splits the labels to symbols, so the output for <code>model.docvecs.doctags</code> is the following:
</p>

<pre><code>{'0': Doctag(offset=5, word_count=378, doc_count=40),
 '1': Doctag(offset=6, word_count=1330, doc_count=141),
 '2': Doctag(offset=7, word_count=413, doc_count=50),
 '3': Doctag(offset=8, word_count=365, doc_count=41),
 '4': Doctag(offset=9, word_count=395, doc_count=41),
 '5': Doctag(offset=10, word_count=420, doc_count=41),
 '6': Doctag(offset=11, word_count=408, doc_count=41),
 '7': Doctag(offset=12, word_count=426, doc_count=41),
 '8': Doctag(offset=13, word_count=385, doc_count=41),
 '9': Doctag(offset=14, word_count=376, doc_count=40),
 '_': Doctag(offset=4, word_count=2009, doc_count=209),
 'a': Doctag(offset=1, word_count=2009, doc_count=209),
 'b': Doctag(offset=2, word_count=2009, doc_count=209),
 'e': Doctag(offset=3, word_count=2009, doc_count=209),
 'l': Doctag(offset=0, word_count=4018, doc_count=418)}
</code></pre>

<p>but in the initial list of tagged document each document has its own unique label.</p>

<p>The code for model training is the following:
</p>

<pre><code>model = Doc2Vec(size=300, sample=1e-4, workers=2)
print('Building Vocabulary')
model.build_vocab(data)
print('Training...')
model.train(data, total_words=total_words_count, epochs=20)
</code></pre>

<p>Therefore I can't index my documents like <code>model.docvecs['label_17']</code> and get <code>KeyError</code>.</p>

<p>The same thing if I pass data to the constructor instead of building the vocabulary.</p>

<p>Why is this happening? Thanks.</p>
","python-3.x, gensim, doc2vec","<p><code>Doc2Vec</code> expects the text examples, objects of the shape <code>TaggedDocument</code>, to have a <code>tags</code> property that's a <em>list-of-tags</em>. </p>

<p>If you instead supply a string, like <code>'label_17'</code>, it is actually a <code>*list-of-characters*, so it's essentially saying that</code>TaggedDocument` has tags:</p>

<pre><code>['l', 'a', 'b', 'e', 'l', '_', '1', '7']
</code></pre>

<p>Make sure you make <code>tags</code> a list-of-one-tag, for example <code>tags=['label_17']</code>, and you should see results in terms of trained-tags more like what you expect. </p>

<p>Separately: it appears you have about 200 documents, of about 10 words each. Note <code>Word2Vec</code>/<code>Doc2Vec</code> need large, varied datasets to get good results. In particular with just 200 texts but 300 vector-dimensions, the training can get quite good at the training task (internal word prediction) with little more than memorizing the idiosyncracies of the training set, which is essentially 'overfitting' and does not result in vectors whose distances/arrangement represent generalizable knowledge that would transfer to other examples. </p>
",1,1,408,2017-10-24 15:31:58,https://stackoverflow.com/questions/46914513/doc2vec-model-splits-documents-tags-in-symbols
gensim - Word2vec online training - AttributeError: &#39;Word2Vec&#39; object has no attribute &#39;model_trimmed_post_training,"<p>I am trying to use a pre-trained model and add additional vocabulary to it. I have a csv file with 1 column of sentences in it.</p>

<pre><code>import gensim

existing_model_fr = gensim.models.Word2Vec.load('./fr/fr.bin')

new_sentences = gensim.models.word2vec.LineSentence('./data/french.csv')
existing_model_fr.build_vocab(new_sentences, update=True)

existing_model_fr.train(new_sentences, total_examples=existing_model_fr.corpus_count, epochs=5)
existing_model_fr.save('new_model_fr')
</code></pre>

<p>I get following error on existing_model_fr.train() line. What am I missing?</p>

<blockquote>
  <p>AttributeError Traceback (most recent call last) in ()</p>
  
  <p>/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py in
  train(self, sentences, total_examples, total_words, epochs,
  start_alpha, end_alpha, word_count, queue_factor, report_delay,
  compute_loss) 863 is only called once, the model's cached iter value
  should be supplied as epochs value. 864 """""" --> 865 if
  self.model_trimmed_post_training: 866 raise RuntimeError(""Parameters
  for training were discarded using model_trimmed_post_training method"")
  867 if FAST_VERSION &lt; 0:</p>
  
  <p>AttributeError: 'Word2Vec' object has no attribute
  'model_trimmed_post_training'</p>
</blockquote>
","nlp, word2vec, gensim","<p>It's likely you're loading a model from a earlier version of gensim where the property <code>model_trimmed_post_training</code> wasn't defined. You can likely work around the issue by setting the property yourself, after loading but before <code>train()</code>:</p>

<pre><code>existing_model_fr. model_trimmed_post_training = false
</code></pre>
",1,1,1932,2017-10-28 01:08:17,https://stackoverflow.com/questions/46985320/gensim-word2vec-online-training-attributeerror-word2vec-object-has-no-att
Warning message after importing gensim module in Windows,"<p>When I tried to import gensim module in Windows, I end up with below error.</p>

<blockquote>
  <p>c:\python27\lib\site-packages\gensim-3.0.1-py2.7-win-amd64.egg\gensim\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
    <strong>warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")</strong></p>
</blockquote>

<p>Is there any possibility to overcome this warning?</p>
","python, nlp, warnings, semantics, gensim","<p>If you want to suppress this warning just use the following code, before importing <code>gensim</code>:</p>

<pre><code>import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
import gensim
</code></pre>
",1,1,771,2017-10-30 18:46:21,https://stackoverflow.com/questions/47022246/warning-message-after-importing-gensim-module-in-windows
Optimizing gensim(C compilier and BLAS) in Window 7,"<p>I wants to optimize gensim to run doc2vec in Window7</p>
<p>[1] C compiler</p>
<p>I installed gensim by following this instruction: <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/install.html</a></p>
<pre><code>pip install --upgrade gensim
</code></pre>
<p>However, in this page(<a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a>), it is saying that C compiler is needed before installing gensim.</p>
<blockquote>
<p>Make sure you have a C compiler before installing gensim, to use optimized (compiled) doc2vec training (70x speedup [blog]).</p>
</blockquote>
<ol>
<li>Should I do something before using pip?</li>
</ol>
<p>[2] BLAS</p>
<p>In the tutorial, <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a> it is saying that</p>
<blockquote>
<p>Time to Train</p>
<p>If the BLAS library is being used, this should take no more than 3 seconds. If the BLAS library is not being used, this should take no more than 2 minutes, so use BLAS if you value your time.</p>
</blockquote>
<p>So it seems like I have to install BLAS for optimization,
but I have no idea what BLAS is and there are little and complex BLAS installation guides for window.</p>
<ol start=""2"">
<li>Which BLAS library should I install for running gensim in Window?</li>
<li>If I install BLAS library, will it be automatically linked to python code when I am running gensim doc2vec? or should I do something to link it to doc2vec code?</li>
</ol>
","python-2.7, word2vec, gensim, blas, doc2vec","<p>It's not just BLAS that gensim's optimized code needs, but native-compiled libraries based on Cython code. </p>

<p>If at all possible, this kind of work should be done on UNIX-like systems (Linux/MacOS), because that's where most of the open-source libraries are most developed, tested, and used. So you'll be closer to the system configurations of the primary developers, and larger user community – meaning default installation instructions are more likely to ""just work"", and any problems you run into are more likely to have existing answers in findable places. </p>

<p>But if you're trapped using Windows, the 'conda' distribution of Python generally does a good job of installing optimized versions of the key libraries on Windows, so it can be a good choice. I especially like to start with the '<a href=""https://conda.io/miniconda.html"" rel=""nofollow noreferrer"">miniconda</a>' variant, so that only the exact packages I explicitly need are installed into an environment. </p>

<p>The <a href=""https://conda.io/docs/user-guide/install/index.html"" rel=""nofollow noreferrer"">Miniconda installation instructions</a> and <a href=""https://conda.io/docs/user-guide/getting-started.html"" rel=""nofollow noreferrer"">getting-started-guide</a> are both quite good. Generally once you are in a <code>conda</code> environment you can <code>conda install PACKAGENAME</code> for major foundational packages like <code>numpy</code> or <code>scipy</code>, and still choose to <code>pip install PACKAGENAME</code> for anything that's not in the conda repositories, or not as up-to-date in the conda repositories. (Sometimes it makes sense to get <code>gensim</code> from <code>pip</code> even if otherwise using a <code>conda</code>-based environment.)</p>
",1,0,1436,2017-10-31 14:01:09,https://stackoverflow.com/questions/47037276/optimizing-gensimc-compilier-and-blas-in-window-7
gensim with jython,"<p>Im using Jython 2.7.1. It is working fine. I need to install gensim. Does this library works in jython?</p>

<p>Thank you</p>
","gensim, jython-2.7","<p>No, Jython does not support gensim.</p>

<p>Gensim has as dependencies SciPy and NumPy. <a href=""https://radimrehurek.com/gensim/install.html"" rel=""nofollow noreferrer"">Gensim Installation</a></p>

<p>Both libraries use native C extensions, which Jython does not support. There is a project whose goal is to allow the use of C extensions in Jython - <a href=""https://jyni.org/"" rel=""nofollow noreferrer"">JyNI</a>. As of 2015, JyNI did not adequately support NumPy. See <a href=""https://stackoverflow.com/questions/30215271/how-to-setup-numpy-in-jython"">This Question</a>. I have not tested later versions of JyNI.</p>
",3,2,192,2017-11-02 17:03:00,https://stackoverflow.com/questions/47080842/gensim-with-jython
How to get word2index from gensim,"<p>By doc we can use this to read a word2vec model with genism</p>

<pre><code>model = KeyedVectors.load_word2vec_format('word2vec.50d.txt', binary=False)
</code></pre>

<p>This is an index-to-word mapping, that is, e.g., <code>model.index2word[2]</code>, how to derive an inverted mapping (word-to-index) based on this?</p>
",gensim,"<p>The mappings from word-to-index are in the <code>KeyedVectors</code> <code>vocab</code> property, a dictionary with objects that include an <code>index</code> property. </p>

<p>For example:</p>

<pre><code>word = ""whatever""  # for any word in model
i = model.vocab[word].index
model.index2word[i] == word  # will be true
</code></pre>
",30,13,15646,2017-11-05 02:21:00,https://stackoverflow.com/questions/47117569/how-to-get-word2index-from-gensim
How do you link back topics generated by LDA model to actual document,"<p>The LDA code generates topics say from 0 to 5 . Is there a standard way (a norm) used to link the generated topics and the documents themselves. Eg: doc1 is of Topic0 , doc5 is of topic Topic1 etc. 
One way i can think of is to string search each of geenrated key words in each topic on the docs , is there a generic way or practice followed for this?</p>

<p>Ex LDA code - <a href=""https://github.com/manhcompany/lda/blob/master/lda.py"" rel=""nofollow noreferrer"">https://github.com/manhcompany/lda/blob/master/lda.py</a> </p>
","machine-learning, nlp, gensim, lda, topic-modeling","<p>I ""collected some code"", and this worked for me. Assuming you have a term frequency </p>

<pre><code>tf_vectorizer = CountVectorizer(""parameters of your choice"")
tf = tf_vectorizer.fit_transform(""your data)`
lda_model = LatentDirichletAllocation(""other parameters of your choice"")
lda_model.fit(tf)
</code></pre>

<p>create the topic-document matrix (the crucial step), and select the num_topic  most important topics </p>

<pre><code>doc_topic = lda_model.transform(tf)
num_most_important_topic = 2

dominant_topic = []
for ind_doc in range(doc_topic.shape[0]):
    dominant_topic.append(sorted(range(len(doc_topic[ind_doc])),
                          key=lambda ind_top: doc_topic[ind_doc][ind_top],
                          reverse=True)[:num_most_important_topic])
</code></pre>

<p>This should give you an array of the num_most_important_topic topics. Good luck!</p>
",0,0,386,2017-11-06 13:30:44,https://stackoverflow.com/questions/47138149/how-do-you-link-back-topics-generated-by-lda-model-to-actual-document
Given a word vector get the word of it in word2vec,"<p>I obtain word vectors from my code. e.g., </p>

<pre><code>array([ -3.09521449e-04,   2.73033947e-06,   2.15601496e-04, ...,
         5.12349070e-04,   5.04256517e-04,   8.16784304e-05], dtype=float32)
</code></pre>

<p>Now, I want to identify what is the word that represents this word vector in wor2vec genism.</p>

<p>I tried it using the below code. However it did not work.</p>

<pre><code>print(model.wv.index2word(kmeans_clustering.cluster_centers_))
</code></pre>

<p>Please help me.</p>
","python, word2vec, gensim, word-embedding, doc2vec","<p>The gensim <code>most_similar()</code> method will take a vector as an argument, as well, but you have to explicitly supply it as one item inside a list of <code>positive</code> examples – so that it's not misunderstood as a something else. </p>

<p>For example:</p>

<pre><code>wv = model.wv.['book']
similars = model.wv.most_similar(positive=[wv,])
</code></pre>

<p>Naturally, 'book' will be at the top of this list of words most-similar to its own vector. </p>
",0,1,2171,2017-11-07 10:21:23,https://stackoverflow.com/questions/47155414/given-a-word-vector-get-the-word-of-it-in-word2vec
Why are multiple model files created in gensim word2vec?,"<p>When I try to create a word2vec model (skipgram with negative sampling) I received 3 files as output as follows.</p>

<pre><code>word2vec (File)
word2vec.syn1nef.npy (NPY file)
word2vec.wv.syn0.npy (NPY file)
</code></pre>

<p>I am just worried why this happens as for my previous test examples in word2vec I only received one model(no npy files).</p>

<p>Please help me.</p>
","python, word2vec, gensim, word-embedding","<p>Models with larger internal vector-arrays can't be saved via Python 'pickle' to a single file, so beyond a certain threshold, the gensim <code>save()</code> method will store subsidiary arrays in separate files, using the more-efficient raw format of numpy arrays (<code>.npy</code> format). </p>

<p>You still <code>load()</code> the model by just specifying the root model filename; when the subsidiary arrays are needed, the loading code will find the side files – as long as they're kept beside the root file. So when moving a model elsewhere, be sure to keep all files with the same root filename together.  </p>
",33,19,5267,2017-11-08 07:07:15,https://stackoverflow.com/questions/47173538/why-are-multiple-model-files-created-in-gensim-word2vec
Suffixes being added to extra model files during save,"<p>When I execute the below code</p>

<pre><code>sim_model = gensim.similarities.MatrixSimilarity(corp)
sim_model.save(""sim_model.pkl"")
</code></pre>

<p>Instead of getting ""sim_model.pkl"" I get two files ""sim_model.pkl.index.npy"" and ""sim_model.pkl"" why is this behavior.</p>
","python-2.7, gensim","<p>For larger models, a single <code>save()</code> can result in multiple files being written, with extra suffixes. See <a href=""https://stackoverflow.com/questions/47173538/why-are-multiple-model-files-created-in-gensim-word2vec"">Why are multiple model files created in gensim word2vec?</a> for more details. </p>
",1,0,41,2017-11-11 06:39:27,https://stackoverflow.com/questions/47235153/suffixes-being-added-to-extra-model-files-during-save
How to obtain document vectors in doc2vec in gensim,"<p>I know to obtain a document vector for a given tag in doc2vec using <code>print(model.docvecs['recipe__11'])</code>.</p>

<p>My document vectors are either recipes (tags start with <code>recipe__</code>), newspapers (tags start with <code>news__</code>) or ingredients (tags start with <code>ingre__</code>)</p>

<p>Now I want to retrieve all the document vectors of recipes. The pattern of my recipe documents is <code>recipe__&lt;some number&gt;</code> (e.g., recipe__23, recipe__34). I am interested in knowing if it possible to obtain multiple document vectors using a pattern (e.g., tags starting with <code>recipe__</code>)</p>

<p>Please help me!</p>
","python, gensim, doc2vec","<p>There's no pattern-retrieval, but you can access the list of all known (string) doc-tags in <code>model.docvecs.offset2doctag</code>. You could then loop over that list to find all matches, and retrieve each individually.</p>

<p>Also, all the doc-vectors are in a large array <code>model.docvecs.doctag_syn0</code> And, if you've used exclusively string doc-tags, then the position of a tag in <code>offset2doctag</code> will be exactly the index of the corresponding vector in <code>doctag_syn0</code>. That would allow you to use numpy 'mask indexing' to grab a subset of vectors as a new array, like:</p>

<pre><code>recipes_mask = [tag.startswith('recipe_') for tag in model.dacvecs.offset2doctag]
recipes_vectors = model.docvecs.doctag_syn0[recipes_mask]
</code></pre>

<p>Of course, this array-of-vectors no longer has the recipes in the same positions as the original, so you'd need extra steps to know where (for example) the 'recipe__11' vector is in <code>recipes_vectors</code>. </p>
",6,2,2339,2017-11-15 06:09:06,https://stackoverflow.com/questions/47300490/how-to-obtain-document-vectors-in-doc2vec-in-gensim
How to adjust alpha parameter in gensim LdaModel,"<p>I have trained an topic model using a symmetric alpha in my lda distibution:</p>

<pre><code>model = gensim.models.ldamodel.LdaModel(bows, num_topics = 20, id2word = dictionary, passes = 100)
</code></pre>

<p>I can see that:</p>

<pre><code>model.alpha
array([ 0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,
    0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,
    0.05,  0.05])
</code></pre>

<p>where </p>

<pre><code>numpy.sum(model.alpha)
1.0000000000000002
</code></pre>

<p>I can't quite understand how gensim allows for lowering alpha parameter to allow each document to be a mixture of fewer topics?</p>
","python, alpha, gensim, topic-modeling","<p>Looking as the <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">docs</a></p>

<p>It seems like <code>gensim.models.ldamodel.LdaModel</code> takes an <code>alpha</code> parameter that defaults to <code>'symmetric'</code>. You can either explicitly provide it an array of alphas, or set it to <code>'auto'</code> and it will learn the priors from your data.</p>

<p>I suggest trying it with <code>alpha='auto'</code> to let it learn the priors.</p>
",1,0,2543,2017-11-15 23:25:04,https://stackoverflow.com/questions/47319033/how-to-adjust-alpha-parameter-in-gensim-ldamodel
Issues in doc2vec tags in Gensim,"<p>I am using gensim doc2vec as below.</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple
import re

my_d = {'recipe__001__1': 'recipe 1 details should come here',
 'recipe__001__2': 'Ingredients of recipe 2 need to be added'}
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for key, value in my_d.items():
    value = re.sub(""[^a-zA-Z]"","" "", value)
    words = value.lower().split()
    tags = key
    docs.append(analyzedDocument(words, tags))
model = doc2vec.Doc2Vec(docs, size = 300, window = 10, dm=1, negative=5, hs=0, min_count = 1, workers = 4, iter = 20)
</code></pre>

<p>However, when I check <code>model.docvecs.offset2doctag</code> I get <code>['r', 'e', 'c', 'i', 'p', '_', '0', '1', '2']</code> as the output. The real output should be `'recipe__001__1' and 'recipe__001__2'.</p>

<p>When I use <code>len(model.docvecs.doctag_syn0)</code> I get <code>9</code> as the output. But the real value should be <code>2</code> because I only have 2 recipes in my test dictionary.</p>

<p>Please let me know, why this happens?</p>
","python, gensim, doc2vec","<p>Try to change this line:</p>

<pre><code>tags = key
</code></pre>

<p>to</p>

<pre><code>tags = [key]
</code></pre>
",3,0,293,2017-11-16 14:28:15,https://stackoverflow.com/questions/47332205/issues-in-doc2vec-tags-in-gensim
Creating a wordvector model combining words from other models,"<p>I have two different word vector models created using word2vec algorithm . Now issue i am facing is few words from first model is not there in second model . I want to create a third model from two different word vectors models where i can use word vectors from both models without loosing meaning and the context of word vectors. </p>

<p>Can I do this, and if so, how?</p>
","machine-learning, nlp, word2vec, gensim","<p>You could potentially translate the vectors for the words only in one model to the other model's coordinate space, using other shared words to learn a translation-function. </p>

<p>There's a facility to do this in recent gensim versions – see the <a href=""https://radimrehurek.com/gensim/models/translation_matrix.html"" rel=""nofollow noreferrer"">TranslationMatrix</a> tool. There's a demo Jupyter notebook included in the <code>docs/notebooks</code> directory, viewable online at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb</a></p>

<p>You'd presumably take the <em>larger</em> model (or whichever one is thought to be <em>better</em>, perhaps because it was trained on more data), and translate the smaller number of words its missing into its space. You'd use as many common-reference 'anchor' words as is practical.</p>
",0,2,99,2017-11-27 09:01:16,https://stackoverflow.com/questions/47507091/creating-a-wordvector-model-combining-words-from-other-models
Gensim word2vec / doc2vec multi-threading parallel queries,"<p>I would like to call <code>model.wv.most_similar_cosmul</code>, on the same copy of <code>model</code> object, using <code>multiple cores</code>, on <code>batches of input pairs</code>.</p>

<p>The <code>multiprocessing</code> module requires multiple copies of <code>model</code>, which will require too much RAM because my <code>model</code> is 30+ GB in RAM.</p>

<p>I have tried to evaluate my query pairs. It took me ~12 hours for the first round. There may be more rounds coming. That's why I am looking for a threading solution. I understand Python has <code>Global Interpreter Lock</code> issue.</p>

<p>Any suggestions?</p>
","python, multithreading, word2vec, gensim, doc2vec","<p>Forking off processes using <code>multiprocessing</code> <em>after</em> your text-vector model is in memory and unchanging <em>might</em> work to let many processes share the same object-in-memory. </p>

<p>In particular, you'd want to be sure that the automatic generation of unit-normed vectors (into a <code>syn0norm</code> or <code>doctag_syn0norm</code>) has already happened. It'll be automatically triggered the first time it's needed by a <code>most_similar()</code> call, or you can force it with the <code>init_sims()</code> method on the relevant object. If you'll <em>only</em> be doing most-similar queries between unit-normed vectors, never needing the original raw vectors, use <code>init_sims(replace=True)</code> to clobber the raw mixed-magnitude <code>syn0</code> vectors in-place and thus save a lot of addressable memory.</p>

<p>Gensim also has options to use memory-mapped files as the sources of model giant arrays, and when multiple processes use the same read-only memory-mapped file, the OS will be smart enough to only map that file into physical memory once, providing both processes pointers to the shared array. </p>

<p>For more discussion of the tricky parts of using this technique in a similar-but-not-identical use case, see my answer at:</p>

<p><a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">How to speed up Gensim Word2vec model load time?</a></p>
",1,1,3447,2017-11-28 14:18:29,https://stackoverflow.com/questions/47533772/gensim-word2vec-doc2vec-multi-threading-parallel-queries
Does word2vec realization from gensim go beyond sentence level when examining context?,"<p>I found <a href=""https://stackoverflow.com/questions/36790867"">this question</a> which provides evidence that sentence order probably matters (but effect can be also a result of different random initialization).</p>

<p>I want to process <a href=""https://files.pushshift.io/reddit/comments/"" rel=""nofollow noreferrer"">Reddit comment dumps</a> for my project, but the strings extracted from json would be unsorted and belong to very different subreddits and topics, so I don't want to mess up contexts:</p>

<pre><code>{""gilded"":0,""author_flair_text"":""Male"",""author_flair_css_class"":""male"",""retrieved_on"":1425124228,""ups"":3,""subreddit_id"":""t5_2s30g"",""edited"":false,""controversiality"":0,""parent_id"":""t1_cnapn0k"",""subreddit"":""AskMen"",""body"":""I can't agree with passing the blame, but I'm glad to hear it's at least helping you with the anxiety. I went the other direction and started taking responsibility for everything. I had to realize that people make mistakes including myself and it's gonna be alright. I don't have to be shackled to my mistakes and I don't have to be afraid of making them. "",""created_utc"":""1420070668"",""downs"":0,""score"":3,""author"":""TheDukeofEtown"",""archived"":false,""distinguished"":null,""id"":""cnasd6x"",""score_hidden"":false,""name"":""t1_cnasd6x"",""link_id"":""t3_2qyhmp""}
</code></pre>

<p>So does the neighbor sentences matter for gensim word2vec and should I recover whole comment tree structure, or I can simply extract ""bag of sentences"" and train the model on it?</p>
","word2vec, gensim, word-embedding","<p>The corpus expected by gensim Word2Vec is an <em>iterable</em> of <em>lists-of-tokens</em>. (For example, a list of lists-of-tokens would work, but for larger corpuses you'd usually want to provide a restartable iterable that streams text examples from persistent storage, to avoid holding the whole corpus in memory.) </p>

<p>The word-vector training only considrs context <em>within individual text examples</em>. That is, within one list-of-tokens. So if two consecutive examples are...</p>

<pre><code>['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']
['Everybody', 'needs', 'a', 'thneed']
</code></pre>

<p>...there's no influence, in these examples between 'ham' and 'Everybody'. (The contexts are only within each example.)</p>

<p>Still, there could be subtle effects on quality if the ordering of examples clumps together all words or topics of a certain type. For example, you'd not want all examples of word X to happen in the beginning of the corpus, and all examples of word Y to happen late – that prevents the sort of interleaved variety-of-examples that achieves the best results.</p>

<p>So if your corpus comes in any kind of sorted order, clumped by topic or author or size or language, it is often beneficial to perform an initial shuffle to remove such clumping. (Re-shuffling any more, such as between training passes, usually has negligible additional benefit.)</p>
",1,0,175,2017-12-01 17:19:25,https://stackoverflow.com/questions/47598369/does-word2vec-realization-from-gensim-go-beyond-sentence-level-when-examining-co
Which formula of tf-idf does the LSA model of gensim use?,"<p>There are many different ways in which tf and idf can be calculated. I want to know which formula is used by gensim in its LSA model. I have been going through its source code <code>lsimodel.py</code>, but it is not obvious to me where the document-term matrix is created (probably because of memory optimizations).</p>

<p>In <a href=""http://www.ling.ohio-state.edu/~reidy.16/LSAtutorial.pdf"" rel=""nofollow noreferrer"">one LSA paper</a>, I read that each cell of the document-term matrix is the log-frequency of that word in that document, divided by the entropy of that word:</p>

<pre><code>tf(w, d) = log(1 + frequency(w, d))
idf(w, D) = 1 / (-Σ_D p(w) log p(w))
</code></pre>

<p>However, this seems to be a very unusual formulation of tf-idf. A more familiar form of tf-idf is:</p>

<pre><code>tf(w, d) = frequency(w, d)
idf(w, D) = log(|D| / |{d ∈ D: w ∈ d}|)
</code></pre>

<p>I also notice that there is a <a href=""https://stackoverflow.com/questions/9470479/how-is-tf-idf-implemented-in-gensim-tool-in-python"">question on how the <code>TfIdfModel</code> itself is implemented in gensim</a>. However, I didn't see <code>lsimodel.py</code> importing <code>TfIdfModel</code>, and therefore can only assume that <code>lsimodel.py</code> has its own implementation of tf-idf.</p>
","gensim, tf-idf, latent-semantic-indexing, latent-semantic-analysis","<p>As I understand, <code>lsimodel.py</code> does not preform the tf-idf encoding step. You may find some details in gensim's <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html"" rel=""nofollow noreferrer"">API documentation</a> - there's a dedicated tf-idf model, which can be employed to encode a text that can be later fed into the LSA model. From the <code>tfidfmodel.py</code> <a href=""https://github.com/RaRe-Technologies/gensim/blob/b6234e7d90ffe9aeda7fab4b39c484381d7c5930/gensim/models/tfidfmodel.py#L52"" rel=""nofollow noreferrer"">source code</a> it appears that the latter of two definitions of tf-idf you listed is followed.</p>
",1,0,990,2017-12-01 17:36:25,https://stackoverflow.com/questions/47598646/which-formula-of-tf-idf-does-the-lsa-model-of-gensim-use
How do I save a Gensim model while ensuring forwards compatibility?,"<p>I'm using the <a href=""https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phraser.save"" rel=""nofollow noreferrer"">save method</a> on the Gensim Phrases class to store a model for future use but if I update my version of Gensim, I have problems loading that model back in. For example, I get the following error when loading a model in Gensim 2.3.0 that was made in 2.2.0:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;()

~/Stuff/Sources/anaconda3/envs/nlp/lib/python3.6/site-packages/gensim/models/phrases.py in __init__(self, phrases_model)
    395         self.min_count = phrases_model.min_count
    396         self.delimiter = phrases_model.delimiter
--&gt; 397         self.scoring = phrases_model.scoring
    398         self.phrasegrams = {}
    399         corpus = pseudocorpus(phrases_model.vocab, phrases_model.delimiter)

AttributeError: 'Phrases' object has no attribute 'scoring'
</code></pre>

<p>Is there a better way to ensure forwards compatibility?</p>
","python, machine-learning, nlp, data-science, gensim","<p>I've used <code>gensim</code> only a couple times and is a newbie, but judging by the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/CHANGELOG.md#230-2017-07-25"" rel=""nofollow noreferrer"">Change Log</a>, the <a href=""https://github.com/RaRe-Technologies/gensim/pull/1464/files#diff-cb673cf37d57cdbaaa92b62f019a380dR171"" rel=""nofollow noreferrer""><code>scoring</code> attribute was introduced on a <code>Phrases</code>  class in 2.3.0</a>.</p>

<p>Now, from what I noticed in the github issues, when it comes to saving and loading the models, backwards compatibility is something that maintainers are trying to keep. It looks like the ""missing scoring"" attribute problem was <a href=""https://github.com/RaRe-Technologies/gensim/commit/a5872fabb069925c89dcdeaff5ff569b28813ef6"" rel=""nofollow noreferrer"">addressed in 3.1.0</a> - see the ""backwards scoring compatibility when loading a Phrases class"" comment and the related discussion in the <a href=""https://github.com/RaRe-Technologies/gensim/pull/1573"" rel=""nofollow noreferrer"">pull request</a>. The idea of the fix was basically to improve the <code>load()</code> method to <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/phrases.py#L485-L496"" rel=""nofollow noreferrer"">handle missing attributes and implicitly replacing them with the defaults</a> to avoid loading failures.</p>

<p>I think in 2.3.0 <code>gensim</code> had this <a href=""https://github.com/RaRe-Technologies/gensim/blob/2.3.0/gensim/utils.py#L249"" rel=""nofollow noreferrer"">generic <code>SaveLoad</code> class for pickling/unpickling models</a> - as you can see, it is pretty much straightforward, no model-specific logic here.</p>

<p>I am though not sure if and how you can keep the models compatible between 2.2.0 and 2.3.0. I would open a new issue at the <a href=""https://github.com/RaRe-Technologies/gensim/issues"" rel=""nofollow noreferrer""><code>gensim</code> issue tracker</a>.</p>
",1,1,586,2017-12-02 04:50:50,https://stackoverflow.com/questions/47604717/how-do-i-save-a-gensim-model-while-ensuring-forwards-compatibility
Using word2vec to classify words in categories,"<p><strong>BACKGROUND</strong></p>

<p>I have vectors with some sample data and each vector has a category name (Places,Colors,Names).</p>

<pre><code>['john','jay','dan','nathan','bob']  -&gt; 'Names'
['yellow', 'red','green'] -&gt; 'Colors'
['tokyo','bejing','washington','mumbai'] -&gt; 'Places'
</code></pre>

<p>My objective is to train a model that take a new input string and predict which category it belongs to. For example if a new input is ""purple"" then I should be able to predict 'Colors' as the correct category. If the new input is ""Calgary"" it should predict 'Places' as the correct category.</p>

<p><strong>APPROACH</strong></p>

<p>I did some research and came across <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">Word2vec</a>. This library has a ""similarity"" and ""mostsimilarity"" function which i can use. So one brute force approach I thought of is the following:</p>

<ol>
<li>Take new input.</li>
<li>Calculate it's similarity with each word in each vector and take an average.</li>
</ol>

<p>So for instance for input ""pink"" I can calculate its similarity with words in vector ""names"" take a average and then do that for the other 2 vectors also. The vector that gives me the highest similarity average would be the correct vector for the input to belong to.</p>

<p><strong>ISSUE</strong></p>

<p>Given my limited knowledge in NLP and machine learning I am not sure if that is the best approach and hence I am looking for help and suggestions on better approaches to solve my problem. I am open to all suggestions and also please point out any mistakes I may have made as I am new to machine learning and NLP world.</p>
","python, machine-learning, nlp, word2vec, gensim","<p>If you're looking for the simplest / fastest solution then I'd suggest you take the pre-trained word embeddings (Word2Vec or GloVe) and just build a simple query system on top of it. The vectors have been trained on a huge corpus and are likely to contain good enough approximation to your domain data.</p>

<p>Here's my solution below:</p>

<pre><code>import numpy as np

# Category -&gt; words
data = {
  'Names': ['john','jay','dan','nathan','bob'],
  'Colors': ['yellow', 'red','green'],
  'Places': ['tokyo','bejing','washington','mumbai'],
}
# Words -&gt; category
categories = {word: key for key, words in data.items() for word in words}

# Load the whole embedding matrix
embeddings_index = {}
with open('glove.6B.100d.txt') as f:
  for line in f:
    values = line.split()
    word = values[0]
    embed = np.array(values[1:], dtype=np.float32)
    embeddings_index[word] = embed
print('Loaded %s word vectors.' % len(embeddings_index))
# Embeddings for available words
data_embeddings = {key: value for key, value in embeddings_index.items() if key in categories.keys()}

# Processing the query
def process(query):
  query_embed = embeddings_index[query]
  scores = {}
  for word, embed in data_embeddings.items():
    category = categories[word]
    dist = query_embed.dot(embed)
    dist /= len(data[category])
    scores[category] = scores.get(category, 0) + dist
  return scores

# Testing
print(process('pink'))
print(process('frank'))
print(process('moscow'))
</code></pre>

<p>In order to run it, you'll have to download and unpack the pre-trained GloVe data from <a href=""http://nlp.stanford.edu/data/glove.6B.zip"" rel=""noreferrer"">here</a> (careful, 800Mb!). Upon running, it should produce something like this:</p>

<pre><code>{'Colors': 24.655489603678387, 'Names': 5.058711671829224, 'Places': 0.90213905274868011}
{'Colors': 6.8597321510314941, 'Names': 15.570847320556641, 'Places': 3.5302454829216003}
{'Colors': 8.2919375101725254, 'Names': 4.58830726146698, 'Places': 14.7840416431427}
</code></pre>

<p>... which looks pretty reasonable. And that's it! If you don't need such a big model, you can filter the words in <code>glove</code> according to their <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""noreferrer"">tf-idf</a> score. Remember that the model size only depends on the data you have and words you might want to be able to query.</p>
",32,19,14447,2017-12-06 04:16:35,https://stackoverflow.com/questions/47666699/using-word2vec-to-classify-words-in-categories
Updating training documents for gensim Doc2Vec model,"<p>I have an existing gensim Doc2Vec model, and I'm trying to do iterative updates to the training set, and by extension, the model.</p>

<p>I take the new documents, and perform preproecssing as normal:</p>

<pre><code>stoplist = nltk.corpus.stopwords.words('english')
train_corpus= []
for i, document in enumerate(corpus_update['body'].values.tolist()):
     train_corpus.append(gensim.models.doc2vec.TaggedDocument([word for word in gensim.utils.simple_preprocess(document) if word not in stoplist], [i]))
</code></pre>

<p>I then load the original model, update the vocabulary, and retrain:</p>

<pre><code>#### Original model
## model = gensim.models.doc2vec.Doc2Vec(dm=0, size=300, hs=1, min_count=10, dbow_words= 1, negative=5, workers=cores)

model = Doc2Vec.load('pvdbow_model_6_06_12_17.doc2vec')

model.build_vocab(train_corpus, update=True)

model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)
</code></pre>

<p>I then update the training set Pandas dataframe by appending the new data, and reset the index.</p>

<pre><code>corpus = corpus.append(corpus_update)
corpus = corpus.reset_index(drop=True)
</code></pre>

<p>However, when I try to use infer_vector() with the <em>updated</em> model:</p>

<pre><code>inferred_vector = model1.infer_vector(tokens)
sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
</code></pre>

<p>the result quality is poor, suggesting that the indices from the model and the training set dataframe no longer match. </p>

<p>When I compare it against the <em>non-updated</em> training set dataframe (again using the updated model) the results are fine - though, obviously I'm missing the new documents.</p>

<p>Is there anyway to have both updated, as I want to be able to make frequent updates to the model without a full retrain of the model?</p>
","gensim, doc2vec","<p>Gensim <code>Doc2Vec</code> doesn't yet have official support for expanding-the-vocabulary (via <code>build_vocab(..., update=True)</code>), so the model's behavior here is not defined to do anything useful. In fact, I think any existing doc-tags will be completely discarded and replaced with any in the latest corpus. (Additionally, there are outstanding unresolved reports of memory-fault process-crashes when trying to use <code>update_vocab()</code> with <code>Doc2Vec</code>, such as <a href=""https://github.com/RaRe-Technologies/gensim/issues/1019"" rel=""noreferrer"">this issue</a>.)</p>

<p>Even if that worked, there are a number of murky balancing issues to consider if ever continuing to call <code>train()</code> on a model with texts different than the initial training-set. In particular, each such training session will nudge the model to be better on the new examples, but lose value of the original training, possibly making the model worse for some cases or overall. </p>

<p>The most defensible policy with a growing corpus would be to occasionally retrain from scratch with all training examples combined into one corpus. Another outline of a possible process for rolling updates to a model was discussed in <a href=""https://groups.google.com/d/msg/gensim/7pLHl0D9br4/xIaoNvaXAwAJ"" rel=""noreferrer"">my recent post to the gensim discussion list</a>. </p>

<p>A few other comments on your setup:</p>

<ul>
<li><p>using both hierarchical-softmax (<code>hs=1</code>) and negative sampling (with <code>negative</code> > 0) increases the model size and training time, but may not offer any advantage compared to using just one mode with more iterations (or other tweaks) – so it's rare to have both modes active</p></li>
<li><p>by not specifying an <code>iter</code>, you're using the default-inherited-from-Word2Vec of '5', while published Doc2Vec work often uses 10-20 or more iterations</p></li>
<li><p>many report <code>infer_vector</code> working better with a much-higher value for its optional parameter <code>steps</code> (which has a default of only <code>5</code>), and/or with smaller values of <code>alpha</code> (which has a default of <code>0.1</code>)</p></li>
</ul>
",8,3,3418,2017-12-12 14:56:40,https://stackoverflow.com/questions/47775557/updating-training-documents-for-gensim-doc2vec-model
Improving Gensim Doc2vec results,"<p>I tried to apply doc2vec on 600000 rows of sentences: Code as below:</p>

<pre><code>from gensim import models
model = models.Doc2Vec(alpha=.025, min_alpha=.025, min_count=1, workers = 5)
model.build_vocab(res)
token_count = sum([len(sentence) for sentence in res])
token_count

%%time
for epoch in range(100):
    #print ('iteration:'+str(epoch+1))
    #model.train(sentences)
    model.train(res, total_examples = token_count,epochs = model.iter)
    model.alpha -= 0.0001  # decrease the learning rate`
    model.min_alpha = model.alpha  # fix the learning rate, no decay
</code></pre>

<p>I am getting very poor results with the above implementation. 
the change I made apart from what was suggested in the tutorial was change the below line:</p>

<pre><code>  model.train(sentences)
</code></pre>

<p>As:</p>

<pre><code> token_count = sum([len(sentence) for sentence in res])
model.train(res, total_examples = token_count,epochs = model.iter)
</code></pre>
","python, nlp, gensim, doc2vec","<p>Unfortunately, your code's a nonsensical mix of misguided practices. so <em>don't</em> follow whatever online example you're following! </p>

<p>Taking the problems in order, top to bottom:</p>

<p><em>Don't</em> make <code>min_alpha</code> the same as <code>alpha</code>. The stochastic-gradient-descent optimization process needs a gradual decline from a larger to smaller <code>alpha</code> learning-rate over the course of seeing many varied examples, and should generally end with a negligible near-zero value. (There are other problems with the code's attempt to explicitly decrement <code>alpha</code> in this way we'll get to below.) Only expert users who've already got a working setup, understand the algorithm well, and are performing experimental tweaks should be changing the <code>alpha</code>/<code>min_alpha</code> defaults. </p>

<p><em>Don't</em> set <code>min_count=1</code>. Rare words that only appear once, or a few times, are generally <em>not helpful</em> for Word2Vec/Doc2Vec training. Their few occurrences mean their own corresponding model weights don't get much training, and the few occurrences are more likely to be unrepresentative compared to the corresponding words' true meaning (as might be reflected in test data or later production data). So the model's representations of these individual rare words are unlikely to become very good. But in total, all those rare words compete a lot with other words that <em>do</em> have a chance to become meaningful – so the 'rough' rare words are mainly random interference against other words. Or perhaps, those words mean extra model vocabulary parameters which help the model become superficially better on training data, due to memorizing non-generalizable idiosyncrasies there, but worse on future test/production data. So, <code>min_count</code> is another default (5) that should only be changed once you have a working baseline - and if you rigorously meta-optimize this parameter later, on a good-sized dataset (like your 600K docs), you're quite likely to find that a <em>higher</em> <code>min_count</code> rather than <em>lower</em> improves final results. </p>

<p>Why make a <code>token_count</code>? There's no later place where a total token-count is needed. The <code>total_examples</code> parameter later expects a count of the text examples – that is, number of individual documents/sentences – <em>not</em> total words. By supplying the (much-larger) word-count, <code>train()</code> wouldn't be able to manage <code>alpha</code> correctly or estimate progress in logged-output.</p>

<p><em>Don't</em> call <code>train()</code> multiple times in a loop with your own explicit <code>alpha</code> management, unless you're positive you know what you're doing. Most people get it wrong. By supplying the default <code>model.iter</code> (which has a value of 5) as a parameter here, you're actually performing 500 total passes over your corpus, which is unlikely what you want. By decrementing an initial 0.025 <code>alpha</code> value by 0.0001 over 100 loops, you're winding up with a final <code>alpha</code> of 0.015 - less than half the starting value. Instead, call <code>train()</code> exactly once, with a correct <code>total_examples</code>, and a well-chosen <code>epochs</code> value (often 10 to 20 are used in Doc2Vec published work). Then it will do the exact right number of explicit iterations, and manage <code>alpha</code> intelligently, and print accurate progress estimation in logging. </p>

<p>Finally, this next thing isn't necessarily a problem in your code, because you don't show how your corpus <code>res</code> is constructed, but there is a common error to beware: make sure your corpus can be iterated over multiple times (as if it were an in-memory list, or a restartable <em>iterable</em> object over something coming from IO). Often people supply a single-use <em>iterator</em>, which after one pass through (as in the <code>build_vocab()</code>) returns nothing else - resulting in instant training and a uselessly-still-random-and-untrained model. (If you've enabled logging, and pay attention to logged output and timing of each step, it'll be obvious if this is a problem.)</p>
",28,9,6398,2017-12-19 15:20:20,https://stackoverflow.com/questions/47890052/improving-gensim-doc2vec-results
gensim doc2vec give non-determined result,"<p>I am using the Doc2Vec model in gensim python library.</p>

<p>Every time I feeds the model with the same sentences data and set the parameter:seed of Doc2Vec to a fixed number, the model gives different vectors after the model is built.</p>

<p>For tests purpose, I need a determined result every time I gave a unchanged input data. I searched a lot and does not find a way to keep the gensim's result unchanged. </p>

<p>Is there anything wrong in the way I use it? thanks for replying in advance.</p>

<p>Here is my code:</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
model = Doc2Vec(sentences, dm=1, dm_concat=1, size=100, window=5, hs=0, min_count=10, seed=64)
result = model.docvecs
</code></pre>
","python, nlp, gensim","<p>The <code>Doc2Vec</code> algorithm makes use of randomness in both initialization and training, and efficient multi-threaded training introduces more randomness because the batches across mutliple threads won't necessarily be trained-against in the same order from run-to-run. </p>

<p>If the model is training well, the jitter in results from run-to-run shouldn't be large, and the quality of downstream assessments shouldn't vary much. If the quality-of-results does vary a lot, there are likely other problems with the application of the algorithm to your data or training. </p>

<p>Separately: you almost certainly don't want to be using the non-default  <code>dm_concat=1</code> mode. It results in a much larger, much slower-to-train model, and there aren't any clear public examples of it being worth that extra cost. (I'd only try it if I had a strong baseline result from more simple modes, and lots and lots of data and time.)</p>
",1,0,288,2017-12-20 08:47:26,https://stackoverflow.com/questions/47901979/gensim-doc2vec-give-non-determined-result
Word embedding for OOV words,"<p>I have generated word vectors from a corpus, but I am facing out of vocabulary issues for many words. How can I generate word vectors for OOV words on the fly using existing word embedding?</p>
","machine-learning, nlp, word2vec, gensim","<p>A very late answer (not even the answer you are looking for) but, with <code>skip-gram</code> models what you ask is almost impossible because each word is a distinct entity in and of itself.</p>

<p>The feature you ask can be done with <a href=""https://github.com/facebookresearch/fastText/"" rel=""nofollow noreferrer"">FastText</a> out of the box. It generates OOV word vectors using it's <code>n-gram</code>s.</p>

<p>Gensim has a high-level <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">API</a> to use FastText.</p>
",2,4,5351,2017-12-28 14:47:23,https://stackoverflow.com/questions/48009532/word-embedding-for-oov-words
How to convert gensim Word2Vec model to FastText model?,"<p>I have a Word2Vec model which was trained on a huge corpus. While using this model for Neural network application I came across quite a few ""Out of Vocabulary"" words. Now I need to find word embeddings for these ""Out of Vocabulary"" words. So I did some googling and found that Facebook has recently released a FastText library for this. Now my question is how can I convert my existing word2vec model or Keyedvectors to FastText model?</p>
","nlp, word2vec, gensim, word-embedding, fasttext","<p>FastText is able to create vectors for subword fragments by including those fragments in the initial training, from the original corpus. Then, when encountering an out-of-vocabulary ('OOV') word, it constructs a vector for those words using fragments it recognizes. For languages with recurring word-root/prefix/suffix patterns, this results in vectors that are better than random guesses for OOV words. </p>

<p>However, the FastText process does <em>not</em> extract these subword vectors from final full-word vectors. Thus there's no simple way to turn full-word vectors into a FastText model that also includes subword vectors.</p>

<p>There might be workable way to approximate the same effect, for example by taking all known-words with the same subword fragment, and extracting some common average/vector-component to be assigned to the subword. Or modeling OOV words as some average of in-vocabulary words that are a short edit-distance from the OOV word. But these techniques wouldn't quite be FastText, just vaguely analogous to it, and how well they work, or could be made to work with tweaking, would be an experimental question. So, it's not a matter of grabbing an off-the-shelf library.</p>

<p>There are a couple of research papers with other OOV-bootstrapping ideas, mentioned in <a href=""http://ruder.io/word-embeddings-2017/index.html#oovhandling"" rel=""nofollow noreferrer"">this blog post by Sebastien Ruder</a>.</p>

<p>If you need the FastText OOV functionality, the best-grounded approach would be to train FastText vectors from scratch on the same corpus as was used for your traditional full-word-vectors.</p>
",3,3,3003,2017-12-29 04:18:46,https://stackoverflow.com/questions/48017343/how-to-convert-gensim-word2vec-model-to-fasttext-model
doc2vec/gensim - issue with shuffling sentences in the epochs,"<p>I am trying to get started with <code>word2vec</code> and <code>doc2vec</code> using the excellent tutorials, <a href=""http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"" rel=""nofollow noreferrer"">here</a> and <a href=""https://medium.com/@mishra.thedeepak/doc2vec-in-a-simple-way-fa80bfe81104"" rel=""nofollow noreferrer"">here</a> and trying to use the code samples. I only added in a <code>line_clean()</code> method to remove punctuation, stopwords etc. </p>

<p>But I am having trouble with the <code>line_clean()</code> method called in the training iterations.  I understand the call to the global method is messing it up, but I am not sure how to get past this problem.</p>

<pre><code>Iteration 1
Traceback (most recent call last):
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 96, in &lt;module&gt;
    train()
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 91, in train
    model.train(sentences.sentences_perm(),total_examples=model.corpus_count,epochs=model.iter)
  File ""/Users/santino/Dev/doc2vec_exp/doc2vec_exp_app/doc2vec/untitled.py"", line 61, in sentences_perm
    shuffled = list(self.sentences)
AttributeError: 'TaggedLineSentence' object has no attribute 'sentences'
</code></pre>

<p>My code is below:</p>

<pre><code>import gensim
from gensim import utils
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec
import os
import random
import numpy
from sklearn.linear_model import LogisticRegression
import logging
import sys
from nltk import RegexpTokenizer
from nltk.corpus import stopwords

tokenizer = RegexpTokenizer(r'\w+')
stopword_set = set(stopwords.words('english'))


def clean_line(line):
    new_str = unicode(line, errors='replace').lower() #encoding issues
    dlist = tokenizer.tokenize(new_str)
    dlist = list(set(dlist).difference(stopword_set))
    new_line = ' '.join(dlist)
    return new_line


class TaggedLineSentence(object):
    def __init__(self, sources):
        self.sources = sources

        flipped = {}

        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')

    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    yield TaggedDocument(utils.to_unicode(clean_line(line)).split(), [prefix + '_%s' % item_no])

    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    self.sentences.append(TaggedDocument(utils.to_unicode(clean_line(line)).split(), [prefix + '_%s' % item_no]))
        return(self.sentences)

    def sentences_perm(self):
        shuffled = list(self.sentences)
        random.shuffle(shuffled)
        return(shuffled)


def train():
    #create a list data that stores the content of all text files in order of their names in docLabels
    doc_files = [f for f in os.listdir('./data/') if f.endswith('.csv')]

    sources = {}
    for doc in doc_files:
        doc2 = os.path.join('./data',doc)
        sources[doc2] = doc.replace('.csv','')

    sentences = TaggedLineSentence(sources)


    # #iterator returned over all documents
    model = gensim.models.Doc2Vec(size=300, min_count=2, alpha=0.025, min_alpha=0.025)
    model.build_vocab(sentences)

    #training of model
    for epoch in range(10):
        #random.shuffle(sentences)
        print 'iteration '+str(epoch+1)
        #model.train(it)
        model.alpha -= 0.002
        model.min_alpha = model.alpha
        model.train(sentences.sentences_perm(),total_examples=model.corpus_count,epochs=model.iter)
    #saving the created model
    model.save('reddit.doc2vec')
    print ""model saved"" 

train()
</code></pre>
","python, word2vec, gensim, doc2vec","<p>Those aren't great tutorials for the latest versions of <code>gensim</code>. In particular, it's a bad idea to be calling <code>train()</code> multiple times in a loop with your own manual management of <code>alpha</code>/<code>min_alpha</code>. It's easy to mess up – the wrong things will happen in your code, for example – and offers no benefit for most users. Don't change <code>min_alpha</code> from the default, and call <code>train()</code> exactly once – it'll then do exactly <code>epochs</code> iterations, decaying the learning-rate <code>alpha</code> from its max to min values properly. </p>

<p>Your specific error is because your <code>TaggedLineSentence</code> class doesn't have a <code>sentences</code> property – at least not until after <code>to_array()</code> is called – and yet the code is trying to access that non-existent property. </p>

<p>The whole <code>to_array()</code>/<code>sentences_perm()</code> approach is a bit broken. The reason for using such an iterable class is typically to keep a large dataset out of main-memory, streaming it from disk. But <code>to_array()</code> then just loads everything, caching it <em>inside</em> the class - eliminating the iterable benefit. If you can afford that, because the full dataset easily fits in memory, you can just do...</p>

<pre><code>sentences = list(TaggedLineSentence(sources)
</code></pre>

<p>...to iterate-from-disk once, then keep the corpus in an in-memory list. </p>

<p>And shuffling repeatedly during training isn't usually needed. Only if the training data has some existing clumping – like all the examples with certain words/topics are stuck together at the top or bottom of the ordering – is the native ordering likely to cause training problems. And in that case, a single shuffle, before any training, should be enough to remove the clumping. So again assuming your data fits in memory, you can just do...</p>

<pre><code>sentences = random.shuffle(list(TaggedLineSentence(sources)
</code></pre>

<p>...once, then you've got a <code>sentences</code> that's fine to pass to <code>Doc2Vec</code> in both <code>build_vocab()</code> and <code>train()</code> (once) below. </p>
",7,2,1004,2017-12-31 17:55:45,https://stackoverflow.com/questions/48044670/doc2vec-gensim-issue-with-shuffling-sentences-in-the-epochs
How much data is actually required to train a doc2Vec model?,"<p>I have been using <strong>gensim's</strong> libraries to train a doc2Vec model. After experimenting with different datasets for training, I am fairly confused about what should be an ideal training data size for doc2Vec model?</p>

<p>I will be sharing my understanding here. Please feel free to correct me/suggest changes-</p>

<ol>
<li><strong>Training on a general purpose dataset-</strong> If I want to use a model trained on a general purpose dataset, in a specific use case, I need to train on a lot of data.</li>
<li><strong>Training on the context related dataset-</strong> If I want to train it on the data having the same context as my use case, usually the training data size can have a smaller size.</li>
</ol>

<p><em>But what are the number of words used for training, in both these cases?</em></p>

<p>On a general note, we stop training a ML model, when the error graph reaches an ""elbow point"", where further training won't help significantly in decreasing error. Has any study being done in this direction- where doc2Vec model's training is stopped after reaching an elbow ?</p>
","neural-network, gensim, doc2vec","<p>There are no absolute guidelines - it depends a lot on your dataset and specific application goals. There's some discussion of the sizes of datasets used in published <code>Doc2Vec</code> work at:</p>

<p><a href=""https://stackoverflow.com/questions/45959618/what-is-the-minimum-dataset-size-needed-for-good-performance-with-doc2vec"">what is the minimum dataset size needed for good performance with doc2vec?</a></p>

<p>If your general-purpose corpus doesn't match your domain's vocabulary – including the same words, or using words in the same senses – that's a problem that can't be fixed with just ""a lot of data"". More data could just 'pull' word contexts and representations more towards generic, rather than domain-specific, values. </p>

<p>You really need to have your own quantitative, automated evaluation/scoring method, so you can measure whether results with your specific data and goals are sufficient, or improving with more data or other training tweaks. </p>

<p>Sometimes parameter tweaks can help get the most out of thin data – in particular, more training iterations or a smaller model (fewer vector-dimensions) can slightly offset some issues with small corpuses, sometimes. But the <code>Word2Vec</code>/<code>Doc2Vec</code> really benefit from lots of subtly-varied, domain-specific data - it's the constant, incremental tug-of-war between all the text-examples during training that helps the final representations settle into a useful constellation-of-arrangements, with the desired relative-distance/relative-direction properties. </p>
",6,7,3296,2018-01-02 10:19:07,https://stackoverflow.com/questions/48059145/how-much-data-is-actually-required-to-train-a-doc2vec-model
How to create dataframe of top 5 close words to a particular word lists from a dictionary in pandas,"<p>I have a word2vec dictionary which gives a top similar words  to given word.</p>

<p>I want to pass the list of words for which similarity  needs to calculated  from  a file or list</p>

<p><strong>Input</strong> </p>

<pre><code>word_list =['wan,'floor','street']
</code></pre>

<p>Similarity of these words should be checked against the word2vec dictionary and  similar words to the input word_list must found and written to a dataframe in the below shown format.</p>

<pre><code>model.most_similar(""wan"")

[('wan.', 0.7509685754776001),
 ('want', 0.7326164245605469),
 ('aupuiwan', 0.7161564230918884),
 ('puiwan', 0.7119397521018982),
 ('wanstreet', 0.7096157073974609),
 ('woshing', 0.7046518921852112),
 ('futan', 0.6979573369026184),
 ('won', 0.696295440196991),
 ('fota', 0.6961145401000977),
 ('pul', 0.6921802759170532)]
</code></pre>

<p>I want create a dataframe with  two columns Word and Similar words. </p>

<p><strong>Output Dataframe</strong></p>

<pre><code>Word    Similar Words
wan     ('wan.', 'want','aupuiwan','puiwan','wanstreet')
floor   ('fl','flooor','flor','flr','gf')
street  ('st','rosestreet','stret','strt','str')
</code></pre>

<p>Any help is appreciated.</p>
","python, string, pandas, word2vec, gensim","<p>Try this:</p>

<pre class=""lang-py prettyprint-override""><code>words = ['wan', 'floor', 'street']
similar = [[item[0] for item in model.most_similar(word)[:5]] for word in words]
df = pd.DataFrame({'Word': words, 'Similar Words': similar})
</code></pre>
",0,2,226,2018-01-03 17:11:12,https://stackoverflow.com/questions/48082018/how-to-create-dataframe-of-top-5-close-words-to-a-particular-word-lists-from-a-d
How to write words having similarity above .6 to a specific word from a dictionary to a dataframe in pandas,"<p>I have a word2vec dictionary which has a list of similar words to given word.</p>

<p><strong>Example</strong></p>

<pre><code>model.most_similar(""ltd"")
[('limited', 0.7886955142021179),
 ('limi', 0.6512018442153931),
 ('limite', 0.6031635999679565),
 ('wilford', 0.5938706994056702),
 ('lt', 0.583463728427887),
 ('lighttech', 0.5828145146369934),
 ('rmc', 0.5821658372879028),
 ('tomoike', 0.5752800703048706),
 ('jd', 0.5751883387565613),
 ('nxp', 0.5725069046020508)]
</code></pre>

<p>I want to create  dataframe containing root and similar_words(having similarity above .6)</p>

<p>Currently I am able to write all the  similar words corresponding to root word</p>

<pre><code>words = y
similar = [[item[0] for item in model.most_similar(word)[:6]] for word in words]
similarity_matrix = pd.DataFrame({'Root_Word': words, 'Similar_Words': similar})
</code></pre>

<p><strong>Current Output</strong></p>

<pre><code>Root_Word    Similar_word
[st]         [st., sreet, rd;, yop, tseun, tsven] 
[limited]    [ltd, lt, wt, serial, (h.k., dk] 
[centre]     [cent, ct, cte, entre, ctr., ce]
</code></pre>

<p><strong>Expected output is have only Similar words which having similarity above .6.</strong></p>

<p>How can this be done</p>
","python, string, pandas, word2vec, gensim","<p>Based on your current method:</p>

<pre><code> similar = [[item[0] for item in model.most_similar(word) if item[1] &gt; 0.6] for word in words]
</code></pre>
",0,0,64,2018-01-04 05:24:03,https://stackoverflow.com/questions/48089141/how-to-write-words-having-similarity-above-6-to-a-specific-word-from-a-dictiona
AttributeError: &#39;list&#39; object has no attribute &#39;words&#39; in python gensim module,"<p>While training using <code>doc2vec</code>, I got this error:</p>

<pre><code>AttributeError: 'list' object has no attribute 'words' in python gensim module
</code></pre>

<p>This is my code:</p>

<pre><code># Extracting titles from csv to list
with open(query+'_titles.csv', 'rb') as f:
    reader = csv.reader(f)
    titlelist = list(reader)
# build
model = doc2vec.Doc2Vec(size=30, window=1, alpha=0.01, min_count=2, sample=1e-5, workers=100)
model.build_vocab(titlelist)
titlearray = np.asarray(titlelist)
print 'Training Model...'
</code></pre>

<p>I use python <em>2.7.11</em> and gensim version is <em>3.2.0</em> if that helps. There must be something I am really missing. </p>
","python, machine-learning, nlp, gensim, doc2vec","<p><code>Doc2Vec</code> requires not just the list of sentences, but the list of <em>tagged</em> sentences. From <a href=""https://datascience.stackexchange.com/q/10216/18375"">this discussion on DS.SE</a>:</p>

<blockquote>
  <p>In <code>word2vec</code> there is no need to label the words, because every word
  has their own semantic meaning in the vocabulary. But in case of
  <code>doc2vec</code>, there is a need to specify that how many number of words or
  sentences convey a semantic meaning, so that the algorithm could
  identify it as a single entity. For this reason, we are specifying
  labels or tags to sentence or paragraph depending on the level of
  semantic meaning conveyed.</p>
</blockquote>

<p>Consequently, Gensim expects the following input:</p>

<pre><code>sentences = [doc2vec.TaggedDocument(sentence, 'tag') for sentence in titlelist]
model.build_vocab(sentences)
</code></pre>

<p>Obviously, you might want to set different tags depending on the sentences to get meaningful vectors. By the way, are you sure you want to read CSV in binary mode?</p>
",3,1,9599,2018-01-04 07:15:54,https://stackoverflow.com/questions/48090426/attributeerror-list-object-has-no-attribute-words-in-python-gensim-module
Cosine Similarity and LDA topics,"<p>I want to compute Cosine Similarity between LDA topics. In fact, gensim function .matutils.cossim can do it but I dont know  which parameter (vector ) I can use for this function?</p>

<p>Here is a snap of  code :</p>

<pre><code>import numpy as np
import lda
from sklearn.feature_extraction.text import CountVectorizer

cvectorizer = CountVectorizer(min_df=4, max_features=10000, stop_words='english')
cvz = cvectorizer.fit_transform(tweet_texts_processed)

n_topics = 8
n_iter = 500
lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)
X_topics = lda_model.fit_transform(cvz)

n_top_words = 6
topic_summaries = []

topic_word = lda_model.topic_word_  # get the topic words
vocab = cvectorizer.get_feature_names()
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
    topic_summaries.append(' '.join(topic_words))
    print('Topic {}: {}'.format(i, ' '.join(topic_words)))

doc_topic = lda_model.doc_topic_
lda_keys = []
for i, tweet in enumerate(tweets):
    lda_keys += [X_topics[i].argmax()]

import gensim
from gensim import corpora, models, similarities
#Cosine Similarity between LDA topics
 **sim = gensim.matutils.cossim(LDA_topic[1], LDA_topic[2])** 
</code></pre>
","python, nlp, gensim, lda","<p>You can use word-topic distribution vector.
You need both topic vectors to be with the same dimension, and have first element of tuple to be int, and second - float.</p>
<p><code>vec1 (list of (int, float))</code></p>
<p>So first element is word_id, that you can find in id2word variable in model.
If you have two models, you need to union dictionaries.
Your vectors must be:</p>
<pre><code>[(1, 0.541223), (2, 0.44123)]
</code></pre>
<p>then you can compare them.</p>
",0,2,2324,2018-01-05 14:50:58,https://stackoverflow.com/questions/48115965/cosine-similarity-and-lda-topics
Gensim word embedding training with initial values,"<p>I have a dataset with documents separated into different years, and my objective is to train an embedding model for each year's data, while at the same time, the same word appearing in different years will have similar vector representations. Like this: for word 'compute', its vector in year 1 is</p>

<pre><code>[0.22, 0.33, 0.20]
</code></pre>

<p>and in year 2 it's something around:</p>

<pre><code>[0.20, 0.35, 0.18]
</code></pre>

<p>Is there a way to accomplish this? For example, train the model of year 2 with both initial values (if the word is trained already in year 1, modify its vector) and randomness (if this is a new word for the corpus).</p>
","machine-learning, nlp, word2vec, gensim, word-embedding","<p>I think the easiest solution is to <em>save</em> the embeddings after training on the first data set, then <em>load</em> the trained model and continue training for the second data set. This way you shouldn't expect the embeddings to drift away from the saved state much (unless your data sets are very different).</p>

<p>It would also make sense to create a single vocabulary from all documents: vocabulary words that aren't present in a particular document will get some random representation, but still it will be a working word2vec model.</p>

<p>Example from the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">documentation</a>:</p>

<pre><code>&gt;&gt;&gt; model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
&gt;&gt;&gt; model.save(fname)
&gt;&gt;&gt; model = Word2Vec.load(fname)  # continue training with the loaded model
</code></pre>
",0,1,181,2018-01-09 09:25:03,https://stackoverflow.com/questions/48164954/gensim-word-embedding-training-with-initial-values
How to generate word embeddings in Portuguese using Gensim?,"<p>I have the following problem:</p>

<p>In English language my code generates successful word embeddings with Gensim, and similar phrases are close to each other considering cosine distance:</p>

<p>The angle between ""Response time and error measurement"" and ""Relation of user perceived response time to error measurement"" is very small, thus they are the most similar phrases in the set.</p>

<p><a href=""https://i.sstatic.net/Seuzy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Seuzy.png"" alt=""enter image description here""></a></p>

<p>However, when I use the same phrases in Portuguese, it doesn't work:</p>

<p><a href=""https://i.sstatic.net/0l2Sz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0l2Sz.png"" alt=""enter image description here""></a></p>

<p>My code as follows:</p>

<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
import matplotlib.pyplot as plt
from gensim import corpora
documents = [""Interface máquina humana para aplicações computacionais de laboratório abc"",
          ""Um levantamento da opinião do usuário sobre o tempo de resposta do sistema informático"",
           ""O sistema de gerenciamento de interface do usuário EPS"",
           ""Sistema e testes de engenharia de sistemas humanos de EPS"",
           ""Relação do tempo de resposta percebido pelo usuário para a medição de erro"",
           ""A geração de árvores não ordenadas binárias aleatórias"",
           ""O gráfico de interseção dos caminhos nas árvores"",
           ""Gráfico de menores IV Largura de árvores e bem quase encomendado"",
           ""Gráficos menores Uma pesquisa""]

stoplist = set('for a of the and to in on'.split())
texts = [[word for word in document.lower().split() if word not in stoplist]
for document in documents]
texts

from collections import defaultdict
frequency = defaultdict(int)

for text in texts:
    for token in text:
        frequency[token] += 1
frequency

from nltk import tokenize  
texts=[tokenize.word_tokenize(documents[i], language='portuguese') for i in range(0,len(documents))]

from pprint import pprint
pprint(texts)

dictionary = corpora.Dictionary(texts)
dictionary.save('/tmp/deerwester.dict')
print(dictionary)

print(dictionary.token2id)


# VECTOR
new_doc = ""Tempo de resposta e medição de erro""
new_vec = dictionary.doc2bow(new_doc.lower().split())
print(new_vec)

## VETOR OF PHRASES
corpus = [dictionary.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  
print(corpus)

from gensim import corpora, models, similarities
tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model

### PHRASE COORDINATES
frase=tfidf[new_vec]
print(frase)

corpus_tfidf = tfidf[corpus]
for doc in corpus_tfidf:
    print(doc)

lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)
corpus_lsi = lsi[corpus_tfidf]

lsi.print_topics(2)

## TEXT COORDINATES
todas=[]
for doc in corpus_lsi:
    todas.append(doc)
todas

from gensim import corpora, models, similarities
dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')
corpus = corpora.MmCorpus('/tmp/deerwester.mm')
print(corpus)

lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)

doc = new_doc
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_lsi = lsi[vec_bow]
print(vec_lsi)

p=[]
for i in range(0,len(documents)):
    doc1 = documents[i]
    vec_bow2 = dictionary.doc2bow(doc1.lower().split())
    vec_lsi2 = lsi[vec_bow2]
    p.append(vec_lsi2)

p

index = similarities.MatrixSimilarity(lsi[corpus])

index.save('/tmp/deerwester.index')
index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')

sims = index[vec_lsi]
print(list(enumerate(sims)))

sims = sorted(enumerate(sims), key=lambda item: -item[1])
print(sims) 

#################

import gensim
import numpy as np
import matplotlib.colors as colors
import matplotlib.cm as cmx
import matplotlib as mpl

matrix1 = gensim.matutils.corpus2dense(p, num_terms=2)
matrix3=matrix1.T
matrix3[0]
ss=[]
for i in range(0,9):
    ss.append(np.insert(matrix3[i],0,[0,0]))
matrix4=ss
matrix4

matrix2 = gensim.matutils.corpus2dense([vec_lsi], num_terms=2)
matrix2=np.insert(matrix2,0,[0,0])
matrix2

DATA=np.insert(matrix4,0,matrix2)
DATA=DATA.reshape(10,4)
DATA

names=np.array(documents)
names=np.insert(names,0,new_doc)
new_doc
cmap = plt.cm.jet

cNorm  = colors.Normalize(vmin=np.min(DATA[:,3])+.2, vmax=np.max(DATA[:,3]))

scalarMap = cmx.ScalarMappable(norm=cNorm,cmap=cmap)
len(DATA[:,1])

plt.subplots()
plt.figure(figsize=(12,9))
plt.scatter(matrix1[0],matrix1[1],s=60)
plt.scatter(matrix2[2],matrix2[3],color='r',s=95)
for idx in range(0,len(DATA[:,1])):
    colorVal = scalarMap.to_rgba(DATA[idx,3])
    plt.arrow(DATA[idx,0],
          DATA[idx,1], 
          DATA[idx,2], 
          DATA[idx,3], 
          color=colorVal,head_width=0.002, head_length=0.001)
for i,names in enumerate (names):
    plt.annotate(names, (DATA[i][2],DATA[i][3]),va='top')
plt.title(""PHRASE SIMILARITY - WORD2VEC with GENSIM library"")
plt.xlim(min(DATA[:,2]-.2),max(DATA[:,2]+1))
plt.ylim(min(DATA[:,3]-.2),max(DATA[:,3]+.3))
plt.show()
</code></pre>

<p>My question is: is there any additional set up for Gensim to generate proper word embeddings in Portuguese language or Gensim does not support this language?</p>
","python, nlp, nltk, gensim","<p>One year and 10 months later, I got the response by myself: use BERT embeddings in PyTorch:</p>

<p>Phrases:</p>

<p><a href=""https://i.sstatic.net/NbYtr.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/NbYtr.png"" alt=""Phrases""></a></p>

<p>I adapted PyTorch extract_features.py at <a href=""https://github.com/ethanjperez/pytorch-pretrained-BERT/blob/master/examples/extract_features.py"" rel=""noreferrer"">https://github.com/ethanjperez/pytorch-pretrained-BERT/blob/master/examples/extract_features.py</a></p>

<pre><code>class Main:
    def main(self,input_file,output_file):
        self.input_file=input_file
        self.output_file=output_file
        self.bert_model='bert-base-multilingual-uncased'
        self.do_lower_case=True
        self.layers=""-1""
        self.max_seq_length=128
        self.batch_size=32
        self.local_rank=-1
        self.no_cuda=False

        if self.local_rank == -1 or self.no_cuda:
            device = torch.device(""cuda"" if torch.cuda.is_available() and not self.no_cuda else ""cpu"")
            n_gpu = torch.cuda.device_count()
        else:
            device = torch.device(""cuda"", self.local_rank)
            n_gpu = 1
            # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
            torch.distributed.init_process_group(backend='nccl')
        logger.info(""device: {} n_gpu: {} distributed training: {}"".format(device, n_gpu, bool(self.local_rank != -1)))

        layer_indexes = [int(x) for x in self.layers.split("","")]

        tokenizer = BertTokenizer.from_pretrained(self.bert_model, do_lower_case=self.do_lower_case)

        examples = read_examples(self.input_file)

        features = convert_examples_to_features(
            examples=examples, seq_length=self.max_seq_length, tokenizer=tokenizer)

        unique_id_to_feature = {}
        for feature in features:
            unique_id_to_feature[feature.unique_id] = feature

        model = BertModel.from_pretrained(self.bert_model)
        model.to(device)

        if self.local_rank != -1:
            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[self.local_rank],
                                                            output_device=self.local_rank)
        elif n_gpu &gt; 1:
            model = torch.nn.DataParallel(model)

        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)
        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)

        eval_data = TensorDataset(all_input_ids, all_input_mask, all_example_index)
        if self.local_rank == -1:
            eval_sampler = SequentialSampler(eval_data)
        else:
            eval_sampler = DistributedSampler(eval_data)
        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=self.batch_size)

        model.eval()
        with open(self.output_file, ""w"", encoding='utf-8') as writer:
            for input_ids, input_mask, example_indices in eval_dataloader:
                input_ids = input_ids.to(device)
                input_mask = input_mask.to(device)

                all_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)
                all_encoder_layers = all_encoder_layers

                for b, example_index in enumerate(example_indices):
                    feature = features[example_index.item()]
                    unique_id = int(feature.unique_id)
                    # feature = unique_id_to_feature[unique_id]
                    output_json = collections.OrderedDict()
                    output_json[""linex_index""] = unique_id
                    all_out_features = []
                    for (i, token) in enumerate(feature.tokens):
                        all_layers = []
                        for (j, layer_index) in enumerate(layer_indexes):
                            layer_output = all_encoder_layers[int(layer_index)].detach().cpu().numpy()
                            layer_output = layer_output[b]
                            layers = collections.OrderedDict()
                            layers[""index""] = layer_index
                            print(layer_output.shape)
                            layers[""values""] = [
                                round(x.item(), 6) for x in layer_output[i]
                            ]
                            all_layers.append(layers)
                        out_features = collections.OrderedDict()
                        out_features[""token""] = token
                        out_features[""layers""] = all_layers
                        all_out_features.append(out_features)
                    output_json[""features""] = all_out_features
                    writer.write(json.dumps(output_json) + ""\n"")
</code></pre>

<p>And then run:</p>

<pre><code>embeddings=extrair.Main()
embeddings.main(input_file='gensim.csv',output_file='gensim.json')
</code></pre>

<p>Parsing the JSON file:</p>

<pre><code>import json
from pprint import pprint
import numpy as np

data = [json.loads(line) for line in open('gensim.json', 'r')]

xx=[]
for parte in range(0,len(data)):
    xx.append(np.mean([data[parte]['features'][i]['layers'][0]['values'] for i in range(0,len(data[parte]['features']))],axis=0))

from scipy.spatial.distance import cosine as cos

for i in range(0,len(xx)):
    print(cos(xx[2],xx[i]))
</code></pre>

<p>Getting as output:</p>

<p><a href=""https://i.sstatic.net/UlIX6.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/UlIX6.png"" alt=""enter image description here""></a></p>
",9,3,2028,2018-01-12 11:56:34,https://stackoverflow.com/questions/48225845/how-to-generate-word-embeddings-in-portuguese-using-gensim
Gensim Doc2Vec.infer_vector() equivalent in KeyedVector,"<p>I have a working app using <code>doc2vec</code> from <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">gensim</a>. I know the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer""><code>KeyedVector</code></a> is now the recommended approach, and trying to port over however I am not sure what is the equivalent method for the <code>infer_vector</code> method in <code>Doc2Vec</code>?</p>

<p>Or better put, how do I obtain a document vector for an entire document using the <code>KeyedVector</code> model to write to my Annoy model?</p>
","machine-learning, nlp, word2vec, gensim, doc2vec","<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer""><code>KeyedVectors</code></a> doesn't replace <code>Doc2Vec</code>, it's a storage and index system for word vectors:</p>

<blockquote>
  <p>Word vector storage and similarity look-ups. Common code independent
  of the way the vectors are trained(Word2Vec, FastText, WordRank,
  VarEmbed etc)</p>
  
  <p>The word vectors are considered read-only in this class.</p>
</blockquote>

<p>This class doesn't know anything about tagged documents and it can't implement <code>infer_vector</code> or an equivalent because this procedure requires training and the idea of <code>KeyedVectors</code> is to abstract from the training method.</p>
",0,1,733,2018-01-12 22:00:38,https://stackoverflow.com/questions/48234595/gensim-doc2vec-infer-vector-equivalent-in-keyedvector
How can I get a vector after each training iter in word2vec?,"<p>I want to get a vector of words every few iter in <code>word2vec</code>, e.g., I would like to use the model below.</p>

<pre><code>embedding_model = Word2Vec(test_set, size=300, 
                           window=4, workers=6, 
                           iter=300, sg=1, min_count=10)
</code></pre>

<p>In this model, I want to get the 300-dimensional vectors learned for every 50 iterations, because I want to show continuous learning contents in html d3.</p>

<p>How can I do this?</p>
","python-3.x, nlp, word2vec, gensim, word-embedding","<p>You can call <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.train"" rel=""nofollow noreferrer""><code>train()</code></a> method iteratively 6 times, each with <code>epochs=50</code>:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec(size=300, window=4, workers=6, sg=1, 
                                        min_count=10)
model.build_vocab(sentences)
for i in range(6):
  model.train(sentences, total_examples=model.corpus_count, epochs=50)
  print(model.wv.word_vec('the'))  # get the intermediate vector(s)
</code></pre>
",2,2,285,2018-01-15 12:32:11,https://stackoverflow.com/questions/48263122/how-can-i-get-a-vector-after-each-training-iter-in-word2vec
doc2vec: any way to fetch closest matching terms for a given vector?,"<p>The use-case I have is to have a collection of ""upvoted"" documents and ""downvoted"" documents and using those to re-order a set of results in a search.</p>

<p>I am using gensim <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">doc2vec</a> and am able to run the <code>most_similar</code> queries for word(s) and fetch matching words. But how would I be able to fetch the matching keywords given a vector fetched by a vector sum of the above doc vectors? </p>
","word2vec, gensim, doc2vec","<p>Ohh silly me, I found the answer staring right in my face, posting here in case anyone else has the issue:</p>

<pre><code>similar_by_vector(vector, topn=10, restrict_vocab=None)
</code></pre>

<p>This is however found not in the Doc2Vec class, but in the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">KeyedVector</a> class.</p>
",0,0,579,2018-01-15 17:10:08,https://stackoverflow.com/questions/48267720/doc2vec-any-way-to-fetch-closest-matching-terms-for-a-given-vector
Python node2vec (Gensim Word2Vec) &quot;Process finished with exit code 134 (interrupted by signal 6: SIGABRT)&quot;,"<p>I am working on node2vec in Python, which uses Gensim's <code>Word2Vec</code> internally.</p>
<p>When I am using a small dataset, the code works well. But as soon as I try to run the same code on a large dataset, the code crashes:</p>
<blockquote>
<p>Error:  Process finished with exit code 134 (interrupted by signal 6: SIGABRT).</p>
</blockquote>
<p>The line which is giving the error is</p>
<pre><code>model = Word2Vec(walks, size=args.dimensions,
                 window=args.window_size, min_count=0, sg=1,
                 workers=args.workers, iter=args.iter)
</code></pre>
<p>I am using <a href=""https://en.wikipedia.org/wiki/PyCharm"" rel=""nofollow noreferrer"">PyCharm</a> and Python 3.5.</p>
<p>What is happening? I could not find any post which could solve my problem.</p>
","python, pycharm, word2vec, gensim","<p>You are almost certainly running out of memory – which causes the OS to abort your memory-using process with the <code>SIGABRT</code>.</p>
<p>In general, solving this means looking at how your code is using memory, leading up to and at the moment of failure. (The actual 'leak' of excessive bulk memory usage might, however, be arbitrarily earlier - with only the last small/proper increment triggering the error.)</p>
<p>Specifically with the usage of Python, and the <code>node2vec</code> tool which makes use of the Gensim <code>Word2Vec</code> class, some things to try include:</p>
<p>Watch a readout of the Python process size during your attempts.</p>
<p>Enable Python logging to at least the <code>INFO</code> level to see more about what's happening leading-up to the crash.</p>
<p>Further, be sure to:</p>
<ol>
<li>Optimize your <code>walks</code> iterable to <em>not</em> compose a large in-memory list. (Gensim's <code>Word2Vec</code> can work on a corpus of <em>any</em> length, iuncluding those far larger than RAM, as long as (a) the corpus is streamed from disk via a re-iterable Python sequence; and (b) the model's number of <em>unique</em> word/node tokens can be modeled within RAM.)</li>
<li>Ensure the number of unique words (tokens/nodes) in your model doesn't require a model larger than RAM allows. Logging output, once enabled, will show the raw sizes involved just before the main model-allocation (which is likely failing) happens. (If it fails, either: (a) use a system with more RAM to accomdate your full set of nodes; or (b) or use a higher <code>min_count</code> value to discard more less-important nodes.)</li>
</ol>
<p>If your <code>Process finished with exit code 134 (interrupted by signal 6: SIGABRT)</code> error does not involve Python, Gensim, &amp; <code>Word2Vec</code>, you should instead:</p>
<ol>
<li>Search for occurrences of that error combined with more specific details of <em>your</em> triggering situations - the tools/libraries and lines-of-code that create your error.</li>
<li>Look into general <em>memory-profiling</em> tools for your situation, to identify where (even long before the final error) your code might be consuming almost-all of the available RAM.</li>
</ol>
",5,14,55222,2018-01-16 21:51:28,https://stackoverflow.com/questions/48290403/python-node2vec-gensim-word2vec-process-finished-with-exit-code-134-interrup
Efficient transformation of gensim TransformedCorpus data to array,"<p>Is there a more direct or efficient method for getting the topic probabilities data from a gensim.interfaces.TransformedCorpus object into a numpy array (or alternatively, pandas dataframe) than the by-row method below?</p>

<pre><code>from gensim import models
import numpy as np

num_topics = 5
model = models.LdaMulticore(corpus, num_topics=num_topics, minimum_probability=0.0)

all_topics = model.get_document_topics(corpus)
num_docs = len(all_topics)

lda_scores = np.empty([num_docs, num_topics])

for i in range(0, num_docs):
    lda_scores[i] = np.array(all_topics[i]).transpose()[1]
</code></pre>
","python, numpy, gensim, lda","<p>Might be too late, but gensim has a helper function for converting to and from numpy/scipy arrays. </p>

<p>What you're looking for: </p>

<p><a href=""https://radimrehurek.com/gensim/matutils.html#gensim.matutils.corpus2csc"" rel=""noreferrer""><code>gensim.matutils.corpus2csc</code></a></p>

<p>You can then can convert the output to a numpy array or pandas df as you wish. </p>

<pre><code>import gensim
import numpy as np

all_topics_csr = gensim.matutils.corpus2csc(all_topics)
all_topics_numpy = all_topics_csr.T.toarray()
</code></pre>
",8,3,2769,2018-01-20 16:03:38,https://stackoverflow.com/questions/48358161/efficient-transformation-of-gensim-transformedcorpus-data-to-array
How to serialize gensim corpus in pyspark using apache-zeppelin notebook?,"<p>I am trying to create a gensim corpus and save it to arbitrary HDFS or regular FS path. I am using pyspark (2.2.1) and running a zeppelin notebook on a hadoop cluster. Here is my minimal example:</p>

<pre><code>from gensim import corpora
import os

path = ""/my/existing/hadoop/path""
corpus = [[(0,0), (1,2)]]
corpora.MmCorpus.serialize(os.path.join(path,""corpus.mm""), corpus)
</code></pre>

<p>This leads to error:</p>

<pre><code>[Errno 2] No such file or directory: '/my/existing/hadoop/path/corpus.mm' 
</code></pre>

<p>Although the path exists.</p>

<p>Running the following works.  </p>

<pre><code>corpora.MmCorpus.serialize(""corpus.mm"", corpus)
corpora.MmCorpus.serialize(os.path.join(""/tmp"",""corpus.mm""), corpus)
</code></pre>

<p>However, I can't find it. I checked <code>/tmp</code> and <code>hadoop fs -ls /tmp</code>
What kind of path is required when working with pyspark? </p>
","hadoop, serialization, pyspark, gensim, apache-zeppelin","<blockquote>
  <p>What kind of path is required when working with pyspark? </p>
</blockquote>

<p>Regular path is required when saving regular paths.
It is only necessary to ensure that zeppelin has rights to write to the required location. You also have to know which node of the cluster is the current session running on.</p>

<blockquote>
  <p>Running the following works.</p>
  
  <p>corpora.MmCorpus.serialize(""corpus.mm"", corpus)
     corpora.MmCorpus.serialize(os.path.join(""/tmp"",""corpus.mm""), corpus)</p>
</blockquote>

<p>Location can be found simply by </p>

<pre><code>import os
print(os.getcwd())
</code></pre>
",0,0,287,2018-01-23 14:18:59,https://stackoverflow.com/questions/48403942/how-to-serialize-gensim-corpus-in-pyspark-using-apache-zeppelin-notebook
NLP Challenge: Automatically removing bibliography/references?,"<p>I recently came across following problem: When applying a topic model on a bunch of parsed PDF files, I discovered that content of the references unfortunately also counts for the model. I.e. words within the references appear in the tokenized list of words.</p>

<p><strong>Is there any known ""best-practice"" to solve this problem?</strong></p>

<p>I thought about a search strategy where the python code automatically removes all content after the last mention of ""references"" or ""bibliography"". If I would go by the first, or a random mention of ""references"" or ""bibliography"" within the full text, the parser might not capture the true full content.</p>

<p>The input PDF are all from different journals and thus have a different page structure.</p>
","nlp, gensim, topic-modeling","<p>The syntax is what makes a bibliography entry distinct from a regular sentence.</p>

<p>Test for the pattern that coincides with whatever (or multiple) reference styles you are trying to remove.</p>

<p>Aka date, unquoted string, string, page numbers in a certain format.</p>

<p>I'd spend some time searching for a tool that already recognizes bibliography before doing this, as it will be unique to each style (MLA etc.)</p>
",2,0,255,2018-01-25 13:45:10,https://stackoverflow.com/questions/48444393/nlp-challenge-automatically-removing-bibliography-references
Can Word2Vec be used for information extraction?,"<p>I am using Gensim to train Word2Vec. I know word similarities are deteremined by if the words can replace each other and make sense in a sentence. But can word similarities be used to extract relationships between entities?</p>

<p>Example:
I have a bunch of interview documents and in each interview, the interviewee always says the name of their manager. If I wanted to extract the name of the manager from these interview transcripts could I just get a list of all human name's in the document (using nlp), and the name that is the most similar to the word ""manager"" using Word2Vec, is most likely the manager.</p>

<p>Does this thought process make any sense with Word2Vec? If it doesn't, would the ML solution to this problem then be to input my word embeddings into a sequence to sequence model?</p>
","machine-learning, word2vec, gensim, recurrent-neural-network, information-extraction","<p>Yes, word-vector similarities &amp; relative-arrangements can indicate relationships. </p>

<p>In the original Word2Vec paper, this was demonstrated by using word-vectors to solve word-analogies. The most famous example involves the analogy ""'man' is to 'king' as 'woman' is to ?"".  </p>

<p>By starting with the word-vector for 'king', then subtracting the vector for 'man', and adding the vector for 'woman', you arrive at a new point in the coordinate system. And then, if you look for other words close to that new point, often the closest word will be <code>queen</code>. Essentially, the directions &amp; distances have helped find a word that's related in a particular way – a gender-reversed equivalent. </p>

<p>And, in large news-based corpuses, famous names like 'Obama' or 'Bush' do wind up with vectors closer to their well-known job titles like 'president'. (There will be many contexts in such corpuses where the words appear immediately together – ""President Obama today signed…"" – or simply in similar roles – ""The President appointed…"" or ""Obama appointed…"", etc.)</p>

<p>However, I suspect that's less-likely to work with your 'manager' interview-transcripts example. Achieving meaningful word-to-word arrangements depends on lots of varied examples of the words in shared usage contexts. Strong vectors require large corpuses of millions to billions of words. So the transcripts with a single manager wouldn't likely be enough to get a good model – you'd need transcripts across many managers. </p>

<p>And in such a corpus each manager's name might not be strongly associated with just <code>manager</code>-like contexts. The same name(s) will be repeated when also mentioning other roles, and transcripts may not especially refer to managerial-action in helpful third-person ways that make specific name-vectors well-positioned. (That is, there won't be clean expository statements like, ""John_Smith called a staff meeting"", or ""John_Smith cancelled the project, alongside others like ""…manager John_Smith…"" or ""The manager cancelled the project"".)</p>
",1,4,1659,2018-01-26 04:12:40,https://stackoverflow.com/questions/48455703/can-word2vec-be-used-for-information-extraction
How to generate a topic from a list of titles using LDA (Python)?,"<p>I am new to natural language processing.
I have a list of blog titles, for example (Not real data, but you get the point):</p>

<pre><code>docs = [""Places to Eat"", ""Places to Visit"", ""Top 10 Things to Do in Singapore""]...
</code></pre>

<p>There are about 3000 over titles and I want to use LDA in Python to generate topics for each of this title. Assuming that I have already cleaned and tokenised these texts using nltk package and removed the stopwords, I will end up with:</p>

<pre><code>texts = [[""places"",""eat""],[""places"",""visit""]]...
</code></pre>

<p>I then proceed to convert these texts into Bag-of-words:</p>

<pre><code>from gensim import corpora, models
dictionary = corpora.Dictionary(texts)

corpus = [dictionary.doc2bow(text) for text in texts]
</code></pre>

<p>Corpus data looks like this:</p>

<pre><code>[(0, 1), (1, 1)]...
</code></pre>

<p>Model creation:</p>

<pre><code>import gensim
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=30, id2word = dictionary, passes=20)
</code></pre>

<p>How do I make use of this model to generate a list of topics - For example ""Eat"", ""Visit"", etc. for each of this titles? I understand that the output might contain probabilities but I would like to string them together with only the text.</p>
","python, nlp, nltk, gensim, lda","<p>You can retrieve a list of document topics from a gensim LDA with</p>

<pre><code>Ldamodel.show_topics()
</code></pre>

<p>and then classify a new document with</p>

<pre><code>Ldamodel.get_document_topics(doc)
</code></pre>

<p>where doc is a document bag-of-words vector.</p>

<p><a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a></p>
",3,2,2126,2018-02-04 08:50:48,https://stackoverflow.com/questions/48606331/how-to-generate-a-topic-from-a-list-of-titles-using-lda-python
ELKI Kmeans clustering Task failed error for high dimensional data,"<p>I have a 60000 documents which i processed in <code>gensim</code> and got a 60000*300 matrix. I exported this as a <code>csv</code> file. When i import this in <code>ELKI</code> environment and run <code>Kmeans</code> clustering, i am getting below error.</p>

<pre><code>Task failed
de.lmu.ifi.dbs.elki.data.type.NoSupportedDataTypeException: No data type found satisfying: NumberVector,field AND NumberVector,variable
Available types: DBID DoubleVector,variable,mindim=266,maxdim=300 LabelList
    at de.lmu.ifi.dbs.elki.database.AbstractDatabase.getRelation(AbstractDatabase.java:126)
    at de.lmu.ifi.dbs.elki.algorithm.AbstractAlgorithm.run(AbstractAlgorithm.java:81)
    at de.lmu.ifi.dbs.elki.workflow.AlgorithmStep.runAlgorithms(AlgorithmStep.java:105)
    at de.lmu.ifi.dbs.elki.KDDTask.run(KDDTask.java:112)
    at de.lmu.ifi.dbs.elki.application.KDDCLIApplication.run(KDDCLIApplication.java:61)
    at [...]
</code></pre>

<p>Below is the ELKI settings i have used
<a href=""https://i.sstatic.net/QdG3V.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QdG3V.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/ECMlW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ECMlW.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/XyMJu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XyMJu.jpg"" alt=""enter image description here""></a></p>
","cluster-analysis, k-means, gensim, doc2vec, elki","<p>This sounds strange, but i found the solution to this issue by opening the exported <code>CSV</code> file and doing <code>Save As</code> and saving again as a <code>CSV</code> file. While size of the original file is 437MB, the second file is 163MB. I used the numpy function <code>np.savetxt</code> for saving the <code>doc2vec</code> vector. So it seems to be a <code>Python</code> issue instead of being <code>ELKI</code> issue.</p>

<hr>

<p><strong>Edit</strong>: Above solution is not useful. I instead exported the <code>doc2vec</code> output which was created using <code>gensim</code> library and while exporting format of the values were decided explicitly as <code>%1.22e</code>. i.e. the values exported are in exponential format and values have length of 22. Below is the entire line of code.</p>

<pre><code>textVect = model.docvecs.doctag_syn0
np.savetxt('D:\Backup\expo22.csv',textVect,delimiter=',',fmt=('%1.22e'))
</code></pre>

<p><code>CSV</code> file thus created runs without any issue in ELKI environment.</p>
",1,0,244,2018-02-05 13:01:28,https://stackoverflow.com/questions/48623214/elki-kmeans-clustering-task-failed-error-for-high-dimensional-data
how to use build_vocab in gensim?,"<ol>
<li>Build_vocab extend my old vocabulary? </li>
</ol>

<p>For example, my idea is when I use doc2vec(s) to train a model, it just builds the vocabulary from the datasets.  If I want to extend it, I need to use build_vocab()</p>

<ol start=""2"">
<li>Where should I use it?  Should I put it after ""gensim.doc2vec()""?  </li>
</ol>

<p>For example:</p>

<pre><code>sentences = gensim.models.doc2vec.TaggedLineDocument(f_path)
dm_model = gensim.models.doc2vec.Doc2Vec(sentences, dm=1, size=300, window=8, min_count=5, workers=4)
dm_model.build_vocab()
</code></pre>
","nlp, word2vec, gensim, doc2vec","<p>You should follow working examples in gensim documentation/tutorials/notebooks or online tutorials to understand which steps are necessary and in what order. </p>

<p>In particular, <em>if</em> you provide your <code>sentences</code> corpus iterable on the <code>Doc2Vec()</code> initialization, it will <em>automatically</em> do both the vocabulary-discovery pass and all training – so you <em>don’t</em> then need to call either <code>build_vocab()</code> or <code>train()</code> yourself. And further, you would <em>never</em> call <code>build_vocab()</code> with no arguments. (No working example in docs or online will do what your code does – so don’t improvise new things until you’ve followed the examples and know why they do what they do.)</p>

<p>There is an optional <code>update</code> argument to <code>build_vocab()</code>, which purports to allow the expansion of a vocabulary from an earlier training session (in preparation for further training with the newer words). HOWEVER, it’s only been developed/tested with regard to <code>Word2Vec</code> models – there are reports it causes crashes when used with <code>Doc2Vec</code>. And even in <code>Word2Vec</code>, its overall effects and best-ways-to-use aren’t clear, across all training modes. So I don’t recommend its use except for experts who can read &amp; interpret the source code, and many involved tradeoffs, on their own. If you receive a chunk of new texts, with new words, the best-grounded course of action, and easiest to evaluate/reason-about, is to re-train from scratch, using a combined corpus of all text examples. </p>
",9,3,12168,2018-02-09 09:53:11,https://stackoverflow.com/questions/48703067/how-to-use-build-vocab-in-gensim
Why adding documents to gensim Dictionary gets slow when reaching 2 million words?,"<p>I noticed that when adding documents to a gensim Dictionary, execution time jumps from 0.2s to more than 6s when reaching 2 million words.</p>

<p>The code below is a quick example. I loop through int and add the number to the dictionary at each iteraion.</p>

<pre><code>from gensim import corpora
import time



dict_transcript = corpora.Dictionary()


for i in range(1,10000000):

    start_time = time.time()

    doc = [str(i)]

    dict_transcript.add_documents([doc])

    print(""Iter ""+str(i)+"" done in "" + str(time.time() - start_time) + ' w/ '+str(len(doc)) + ' words and dico size ' +
          str(len(dict_transcript)))
</code></pre>

<p>I do get the following output when reaching 2 million words:</p>

<pre><code>Iter 1999999 done in 0.0 w/ 1 words and dico size 1999999
Iter 2000000 done in 0.0 w/ 1 words and dico size 2000000
Iter 2000001 done in 0.0 w/ 1 words and dico size 2000001
Iter 2000002 done in 7.940511226654053 w/ 1 words and dico size 2000001
</code></pre>

<p>Is there any reason why? And does anyone know how to bypass that problem?
I'm using this dictionary on a big corpus that I tokenize into bigrams so I'm expecting the dictionary to be a few million rows.</p>

<p>Many thanks</p>
","python, dictionary, nlp, gensim","<p>Have a look at the <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""nofollow noreferrer"">gensim documentation</a>:</p>

<blockquote>
  <p><em>class</em> <code>gensim.corpora.dictionary.Dictionary(documents=None, prune_at=2000000)</code></p>
  
  <p><strong>prune_at</strong> (int, optional) – Total number of unique words. Dictionary will keep not more than prune_at words.</p>
</blockquote>

<p>Set <code>prune_at=None</code> or to a suitable integer for your use case.</p>
",4,4,425,2018-02-09 11:47:28,https://stackoverflow.com/questions/48705138/why-adding-documents-to-gensim-dictionary-gets-slow-when-reaching-2-million-word
Gensim: Word2Vec Recommender accuracy Improvement,"<p>I am trying to implement something similar in <a href=""https://arxiv.org/pdf/1603.04259.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1603.04259.pdf</a> using awesome gensim library however I am having trouble improving quality of results when I compare to Collaborative Filtering.</p>

<p>I have two models one built on Apache Spark and other one using gensim Word2Vec on grouplens 20 million ratings dataset. My apache spark model is hosted on AWS <a href=""http://sparkmovierecommender.us-east-1.elasticbeanstalk.com"" rel=""nofollow noreferrer"">http://sparkmovierecommender.us-east-1.elasticbeanstalk.com</a>
and I am running gensim model on my local. However when I compare the results I see superior results with CF model 9 out of 10 times(like below example more similar to searched movie - affinity towards Marvel movies) </p>

<p>e.g.:- If I search for Thor movie I get below results </p>

<p><strong><em>Gensim</em></strong></p>

<ul>
<li>Captain America: The First Avenger (2011) </li>
<li>X-Men: First Class (2011)</li>
<li>Rise of the Planet of the Apes (2011) </li>
<li>Iron Man 2 (2010) </li>
<li>X-Men Origins: Wolverine (2009) </li>
<li>Green Lantern (2011) </li>
<li>Super 8 (2011) </li>
<li>Tron:Legacy (2010) </li>
<li>Transformers: Dark of the Moon (2011)</li>
</ul>

<p><strong><em>CF</em></strong></p>

<ul>
<li>Captain America: The First Avenger</li>
<li>Iron Man 2</li>
<li>Thor: The Dark World</li>
<li>Iron Man</li>
<li>The Avengers</li>
<li>X-Men: First Class</li>
<li>Iron Man 3</li>
<li>Star Trek</li>
<li>Captain America: The Winter Soldier</li>
</ul>

<p>Below is my model configuration, so far I have tried playing with window, min_count and size parameter but not much improvement.</p>

<pre><code>word2vec_model = gensim.models.Word2Vec(
    seed=1,
    size=100, 
    min_count=50, 
    window=30)

word2vec_model.train(movie_list, total_examples=len(movie_list), epochs=10)
</code></pre>

<p>Any help in this regard is appreciated.</p>
","word2vec, gensim, recommendation-engine","<p>You don't mention what Collaborative Filtering algorithm you're trying, but maybe it's just better than <code>Word2Vec</code> for this purpose. (<code>Word2Vec</code> is not doing awful; why do you expect it to be better?) </p>

<p>Alternate meta-parameters might do better.</p>

<p>For example, the <code>window</code> is the max-distance between tokens that might affect each other, but the effective windows used in each target-token training randomly chosen from 1 to <code>window</code>, as a way to give nearby tokens more weight. Thus when some training-texts are much larger than the <code>window</code> (as in your example row), some of the correlations will be ignored (or underweighted). Unless ordering is very significant, a giant <code>window</code> (MAX_INT?) might do better, or even a related method where ordering is irrelevant (such as <code>Doc2Vec</code> in pure PV-DBOW <code>dm=0</code> mode, with every token used as a doc-tag).</p>

<p>Depending on how much data you have, your <code>size</code> might be too large or small. Different <code>min_count</code>, <code>negative</code> count, greater 'iter'/'epochs', or <code>sample</code> level might work much better. (And perhaps even things you've already tinkered with would only help after other changes are in place.)</p>
",3,1,1014,2018-02-10 06:32:46,https://stackoverflow.com/questions/48717970/gensim-word2vec-recommender-accuracy-improvement
&#39;gensim.models.doc2vec&#39; has no attribute &#39;LabeledSentence&#39;,"<p>I am using gensim in python 3 as shown within image below </p>

<p><a href=""https://i.sstatic.net/xMbap.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xMbap.png"" alt="" Image""></a></p>

<p>In line no 11 I am getting the following error:</p>

<p><a href=""https://i.sstatic.net/NoGO4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NoGO4.png"" alt=""Image""></a></p>
","python-3.x, sublimetext3, sentiment-analysis, gensim","<p>It's a deprecated import:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/issues/1886"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/1886</a></p>

<p>if you want to use LabeledSentenced you must import it from the deprecated section:</p>

<pre><code>from gensim.models.deprecated.doc2vec import LabeledSentence
</code></pre>

<p>So you have to do this:          </p>

<pre><code>LabeledSentence = gensim.models.deprecated.doc2vec.LabeledSentence
</code></pre>
",2,0,4001,2018-02-17 15:28:11,https://stackoverflow.com/questions/48842866/gensim-models-doc2vec-has-no-attribute-labeledsentence
gensim doc2vec train more documents from pre-trained model,"<p>I am trying to train with new labelled document(TaggedDocument) with the pre-trained model.</p>

<p>Pretrained model is the trained model with documents which the unique id with label1_index, for instance, Good_0, Good_1 to Good_999
And the total size of trained data is about 7000</p>

<p>Now, I want to train the pre-trained model with new documents which the unique id with label2_index, for instance, Bad_0, Bad_1... to Bad_1211
And the total size of trained data is about 1211</p>

<p>The train itself was successful without any error, but the problem is that whenever I try to use 'most_similar' it only suggests the similar document labelled with Good_... where I expect the labelled with Bad_.</p>

<p>If I train altogether from the beginning, it gives me the answers I expected - it infers a newly given document similar to either labelled with Good or Bad. </p>

<p>However, the practice above will not work as the one trained altogether from the beginning.</p>

<p>Is continuing train not working properly or did I make some mistake?</p>
","gensim, doc2vec, pre-trained-model, resuming-training","<p>The gensim <code>Doc2Vec</code> class can always be fed extra examples via <code>train()</code>, but it only discovers the working vocabulary of both word-tokens and document-tags during an initial <code>build_vocab()</code> step. So unless words/tags were available during the <code>build_vocab()</code>, they'll be ignored as unknown later. (The words get silently dropped from the text; the tags aren't trained or remembered inside the model.)</p>

<p>The <code>Word2Vec</code> superclass from which <code>Doc2Vec</code> borrows a lot of functionality has a newer, more-experimental parameter on its <code>build_vocab()</code> called <code>update</code>. If set true, that call to <code>build_vocab()</code> will add to, rather than replace, any prior vocabulary. However, as of February 2018, this option doesn't yet work with <code>Doc2Vec</code>, and indeed often causes memory-fault crashes. </p>

<p>But even if/when that can be made to work, providing incremental training examples isn't necessarily a good idea. By only updating parts of the model – those exercised by the new examples – the overall model can get worse, or its vectors made less self-consistent with each other. (The essence of these dense-embedding models is that the optimization over <em>all</em> varied examples results in generally-useful vectors. Training over just some subset causes the model to drift towards being good on just that subset, at likely cost to earlier examples.)</p>

<p>If you need new examples to also become part of the results for <code>most_similar()</code>, you might want to create your own separate set-of-vectors outside of <code>Doc2Vec</code>. When you infer new vectors for new texts, you could add those to that outside set, and then implement your own <code>most_similar()</code> (using the gensim code as a model) to search over this expanding set of vectors, rather than just the fixed set that is created by initial bulk <code>Doc2Vec</code> training. </p>
",3,2,1878,2018-02-21 04:45:14,https://stackoverflow.com/questions/48898325/gensim-doc2vec-train-more-documents-from-pre-trained-model
"TypeError: a bytes-like object is required, not &#39;str&#39; when converting gensim to tensorboard","<p>I am converting a <code>gensim</code> w2v file to a <code>Tensorboard</code> tsv file with this code:</p>

<pre><code>with open(outfiletsv, 'w+b') as file_vector:
    with open(outfiletsvmeta, 'w+b') as file_metadata:
        for word in model.index2word:
            file_metadata.write(gensim.utils.to_utf8(word) + gensim.utils.to_utf8('\n'))
            vector_row = '\t'.join(str(x) for x in model[word])
            file_vector.write(vector_row + '\n')
</code></pre>

<p>It results in this error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
~\_repos\special\word2vec2tensor.py in &lt;module&gt;()
     79 
     80     logger.info(""running %s"", ' '.join(sys.argv))
---&gt; 81     word2vec2tensor(args.input, args.output, args.binary)
     82     logger.info(""finished running %s"", os.path.basename(sys.argv[0]))

~\_repos\special\word2vec2tensor.py in word2vec2tensor(word2vec_model_path, tensor_filename, binary)
     61                 file_metadata.write(gensim.utils.to_utf8(word) + gensim.utils.to_utf8('\n'))
     62                 vector_row = '\t'.join(str(x) for x in model[word])
---&gt; 63                 file_vector.write(vector_row + '\n')
     64 
     65     logger.info(""2D tensor file saved to %s"", outfiletsv)

TypeError: a bytes-like object is required, not 'str'
</code></pre>

<p>I added <code>b</code> to the original <code>w+</code> in the open file pieces to counteract the opposite issue (<code>TypeError: write() argument must be str, not bytes</code>). </p>

<p>I tried adding `vector_row = vector_row.encode('UTF-8'), but this did not work. </p>

<p>How do I remedy this <code>TypeError</code>?</p>
","python, python-3.x, typeerror, gensim","<p>You can just convert your string back to bytes:</p>

<pre><code>file_vector.write(vector_row.encode() + b'\n')
</code></pre>

<p>However your code already reads the file as bytes and then you explicitly convert to str with this (i guess): <code>'\t'.join(str(x) for x in model[word])</code></p>

<p>So you might want to clean up and use bytes everywhere instead of going back and forth :)</p>
",1,0,574,2018-02-21 23:52:20,https://stackoverflow.com/questions/48917449/typeerror-a-bytes-like-object-is-required-not-str-when-converting-gensim-to
Can i build vocaburay in twice with gensim word2vec or doc2vec?,"<p>I have two different corpus and what i want is to train the model with both and to do it it I thought that it could be something like this:</p>

<pre><code>model.build_vocab(sentencesCorpus1)
model.build_vocab(sentencesCorpus2)
</code></pre>

<p>Would it be right?</p>
","python, word2vec, gensim, doc2vec","<p>No: each time you call <code>build_vocab(corpus)</code>, like that, it creates a fresh vocabulary from scratch – discarding any prior vocabulary. </p>

<p>You can provide an optional argument to <code>build_vocab()</code>, <code>update=True</code>, which tries to add to the existing vocabulary. However:</p>

<ul>
<li><p>it wasn't designed/tested with <code>Doc2Vec</code> in mind, and as of right now (February 2018), using it with <code>Doc2Vec</code> is unlikely to work and often causes memory-fault crashes. (See <a href=""https://github.com/RaRe-Technologies/gensim/issues/1019"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/1019</a>.)</p></li>
<li><p>it's still best to <code>train()</code> with all available data together - any sort of multiple-calls to <code>train()</code>, with differing data subsets each time, introduces other murky tradeoffs in model quality/correctness that are easy to get wrong. (And, when calling <code>train()</code>, be sure to provide correct values for its required parameters – the practices shown in most online examples are typically only correct for the case where <code>build_vocab()</code> was called once, with exactly the same texts as later calling <code>train()</code>.)</p></li>
</ul>
",0,0,123,2018-02-22 17:53:38,https://stackoverflow.com/questions/48934154/can-i-build-vocaburay-in-twice-with-gensim-word2vec-or-doc2vec
Gensim Doc2Vec Training,"<p>I am using <code>gensim</code> to train a <code>Doc2Vec</code> model on documents assigned to particular people. There are 10 million documents and 8,000 people. I don't care about all 8,000 people. I care about a specific group of people (say anywhere from 1 to 500). </p>

<p>The people I'm interested in could change day-to-day, but I will never need to look at the full population. The end goal is to have the resulting vectors of the people I am interested in. I am currently training the model each time on the documents assigned to the specific people.</p>

<p>Should I train the model on all 10 million documents? Or should I train the model on only the documents assigned to the people I'm interested in? If it's important to train it on all 10 million documents, how would I then get the vectors only for the people I'm interested in?</p>
","python, gensim, doc2vec","<p>It is a good idea to train on all the 10 million documents, that will help you capture the general essence of the words and not just with in the context of authors that you are interested in. Also, it will help you if the set of authors who you are interested in, changes tomorrow.</p>

<p>If you think Doc2Vec takes a lot of time, you could also use <a href=""https://fasttext.cc/"" rel=""nofollow noreferrer"">Fasttext</a> to learn WordEmbeddings and use a simple average or TF-IDF weighted average on the word vectors to construct your DocumentVector. You could leverage the power of hierarchical softmax (loss function) in Fasttext that will reduce your training time by 1000+ folds.</p>
",4,1,1364,2018-02-23 13:40:21,https://stackoverflow.com/questions/48949166/gensim-doc2vec-training
Gensim Doc2Vec Access Vectors by Document Author,"<p>I have three documents in a df:</p>

<pre><code>id    author    document
12X   john      the cat sat
12Y   jane      the dog ran
12Z   jane      the hippo ate
</code></pre>

<p>These documents are converted into a corpus of <code>TaggedDocuments</code> with the tags being the typical practice of semantically meaningless ints:</p>

<pre><code>def read_corpus(documents):
    for i, plot in enumerate(documents):
        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(plot, max_len=30), [i])

train_corpus = list(read_corpus(df.document))
</code></pre>

<p>This corpus is then used to train my <code>Doc2Vec</code> model:</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=55)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>

<p>The resulting vectors of the model are accessed like this:</p>

<pre><code>model.docvecs.vectors_docs
</code></pre>

<p>How would I tie the original df to the resulting vectors? Now that all the documents are trained and vectors are identified for each one, I want to query the set of vectors by author. For example, if I want to return a set of vectors only for Jane, how would I do so?</p>

<p>I think the basic idea is to identify the int tags that correspond to Jane and then do something like this to access them:</p>

<pre><code>from operator import itemgetter 
a = model.docvecs.vectors_docs
b = [1, 2]
itemgetter(*b)(a)
</code></pre>

<p>How would I identify the tags though? They are only meaningful to the model and the tagged documents, so they don't join back to my original df.</p>
","python, gensim, doc2vec","<p>I tried a simple example using Gensim. I think the approach here should work for you</p>

<pre><code>import gensim
training_sentences = ['This is some document from Author {}'.format(i) for i in range(1,10)]
def read_corpus():
    for i,line in enumerate(training_sentences):
        # lets use the tag to identify the document and author
        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), ['Doc{}_Author{}'.format(i,i)])
</code></pre>

<p>You could also directly prepare the training corpus from pandas_df like shown below</p>

<pre><code>data_df = pd.DataFrame({'doc':training_sentences,'doc_id':[i for i in range (1,10)],'author_id':[10+i for i in range (1,10)]})
data_df.head()
tagged_docs = data_df.apply(lambda x:gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(x.doc),['doc{}_auth{}'.format(x.doc_id,x.author_id)]),axis=1)
training_corpus = tagged_docs.values

&gt;&gt; array([ TaggedDocument(words=['this', 'is', 'some', 'document', 'from', 'author'], tags=['doc1_auth11']),
       TaggedDocument(words=['this', 'is', 'some', 'document', 'from', 'author'], tags=['doc2_auth12']),

# training
model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=55)
train_corpus = list(read_corpus())
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)
# indexing
model.docvecs.index2entity

&gt;&gt;
['Doc0_Author0',
 'Doc1_Author1',
 'Doc2_Author2',
 'Doc3_Author3',
 'Doc4_Author4',
 'Doc5_Author5',
 'Doc6_Author6',
 'Doc7_Author7',
 'Doc8_Author8']
</code></pre>

<p>Now, to access the vector corresponding to document1 of author1, you could do </p>

<pre><code>model.docvecs[model.docvecs.index2entity.index('Doc1_Author1')]

array([  8.08026362e-03,   4.27437993e-03,  -7.73820514e-03,
        -7.40669528e-03,   6.36066869e-03,   4.03292105e-03,
         9.60215740e-03,  -4.26750770e-03,  -1.34797185e-03,
        -9.02472902e-03,   6.25275355e-03,  -2.49505695e-03,
         3.18572600e-03,   2.56929174e-03,  -4.17032139e-03,
        -2.33384431e-03,  -5.10744564e-03,  -5.29057207e-03,
         5.41675789e-03,   5.83767192e-03,  -5.91145828e-03,
         5.91885624e-03,  -1.00465110e-02,   8.32535885e-03,
         9.72494949e-03,  -7.35746371e-03,  -1.86231872e-03,
         8.94813929e-05,  -4.11528209e-03,  -9.72509012e-03,
        -6.52212929e-03,  -8.83922912e-03,   9.46981460e-03,
        -3.90578934e-04,   6.74136635e-03,  -5.24599617e-03,
         9.73031297e-03,  -8.77021812e-03,  -5.55411633e-03,
        -7.21857697e-03,  -4.50362219e-03,  -4.06361837e-03,
         2.57276138e-03,   1.76626759e-06,  -8.08755495e-03,
        -1.48400548e-03,  -5.26673114e-03,  -7.78301107e-03,
        -4.24248137e-04,  -7.99000356e-03], dtype=float32)
</code></pre>

<p>yes this uses doc-author pair ordering, you could just use doc_id alone and maintain a separate index like <code>{doc_id:author_id}</code> in a python dict, if you want to filter by author then use <code>{author_id : [docids,...]}</code></p>
",3,1,1861,2018-02-23 18:08:41,https://stackoverflow.com/questions/48953871/gensim-doc2vec-access-vectors-by-document-author
How to find semantic similarity using gensim and word2vec in python,"<p>I have a list of words in my python programme. Now I need to iterate through this list and find out the semantically similar words and put them into another list. I have been trying to do this using gensim with word2vec but could find a proper solution.This is what I have implemeted up to now. I need a help on how to iterate through the list of words in the variable sentences and find the semantically similar words and save it in another list.</p>

<pre><code>import gensim, logging

import textPreprocessing, frequentWords , summarizer
from gensim.models import Word2Vec, word2vec

import numpy as np
from scipy import spatial

sentences = summarizer.sorteddict

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model = word2vec.Word2Vec(sentences, iter=10, min_count=5, size=300, workers=4)
</code></pre>
","python, machine-learning, nlp, word2vec, gensim","

<p>If you don't care about <em>proper</em> clusters, you can use this code:</p>

<pre class=""lang-py prettyprint-override""><code>similar = [[item[0] for item in model.most_similar(word)[:5]] for word in words]
</code></pre>

<hr>

<p>If you really want to clusterize the words, here are few notes:</p>

<ul>
<li>There can be several such clusters.</li>
<li>The number of clusters depends on a <em>hyperparameter</em>, some threshold. When the threshold is big, all of the words are similar and belong to the same cluster, when it's too small, none of them are.</li>
<li>Words can be naturally included <em>transitively</em> into a cluster, i.e. <code>A</code> is similar to <code>B</code> and <code>B</code> is similar to <code>C</code>, so all three should be in the same cluster. This means you'll have to implement some sort of graph traversal algorithm.</li>
<li>The performance greatly depends on the training corpus: only if it's large enough, gensim word2vec will be able to capture proper similarity. Gemnsim hyperparameters and text pre-processing thus also matter.</li>
</ul>

<p>Here's a naive and probably not very efficient algorithm and identifies clusters:</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.word2vec.Word2Vec(sentences, iter=10, min_count=5, size=300, workers=4)
vocab = model.wv.vocab.keys()

threshold = 0.9
clusters = {}
for word in vocab:
  for similar_word, distance in model.most_similar(word)[:5]:
    if distance &gt; threshold:
      cluster1 = clusters.get(word, set())
      cluster2 = clusters.get(similar_word, set())
      joined = set.union(cluster1, cluster2, {word, similar_word})
      clusters[word] = joined
      clusters[similar_word] = joined
</code></pre>
",2,3,738,2018-02-26 14:36:55,https://stackoverflow.com/questions/48990935/how-to-find-semantic-similarity-using-gensim-and-word2vec-in-python
Is there a way to save a Gensim doc2vec model as plain text (.txt)?,"<p>What I have achieved so far are models that can not be read by a person. I need to save the model as plain text to use it with a certain software, which requires that the model be this way.</p>

<p>I tried the following:</p>

<pre><code>model = models.doc2vec.Doc2Vec(size=300, min_count=0, alpha=0.025, min_alpha=0.025)
model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)
model.save('mymodel.txt')
</code></pre>

<p>But I get:</p>

<pre><code>Process finished with exit code -1073741571 (0xC00000FD)
</code></pre>

<p>I do not know if I should pass a specific parameter.</p>
","python, gensim, doc2vec","<p>The native gensim <code>save()</code> has no plain-text option: it makes use of Python core functionality like object-pickling, or writing large raw floating-point arrays (to secondary files with extra extensions <code>.npy</code>). Such files will include raw binary data – and merely specifying a <code>.txt</code> filename doesn't have any affect on what is written. </p>

<p>You can save <em>just</em> the word-vectors into the one-vector-per-line, plain-text format used by the original Google <code>word2vec.c</code> by using the alternate method <code>save_word2vec_format()</code>. Also, recent versions of gensim <code>Doc2Vec</code> add an optional <code>doctag_vec</code> option to this method. If you supply <code>doctag_vec=True</code>, the doctag vectors will also be saved to the file – with their tag-names distinguished from word-vectors by an extra prefix. See the method's doc-comment and source-code for more info:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/b000b4fa71386235ffa2b80a62bcccf73fa42c6e/gensim/models/doc2vec.py#L635"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/b000b4fa71386235ffa2b80a62bcccf73fa42c6e/gensim/models/doc2vec.py#L635</a></p>

<p>However, no variant of <code>save_word2vec_format()</code> saves the <em>entire</em> model, with internal model-weights and the vocabulary/doctag information (like relative frequencies) that are necessary for continued training. For that, you must use the native <code>save()</code>. If you need the full <code>Doc2Vec</code> model in a text format, you'll need to write that save code yourself, perhaps using the above method as a partial guide. (Additionally, I'm not aware of a preexisting convention for representing a whole model – so you'd have to find or devise that yourself, to match your needs wherever the full model is later-to-be-loaded.)</p>

<p>Separately regarding your <code>Doc2Vec</code> initialization parameters:</p>

<ul>
<li><p>a <code>min_count=0</code> is usually a bad idea: rare words make models worse, so the default of <code>min_count=5</code> usually improves models, and as your corpus gets larger, even larger <code>min_count</code> values discarding more low-frequency words tend to help model quality (as well as speeding training and shrinking the model's RAM/save sizes)</p></li>
<li><p>a <code>min_alpha</code> the same as <code>alpha</code> is usually a bad idea, and means that <code>train()</code> is no longer performing the linear-decay of the <code>alpha</code> learning-rate that's the usual and effective manner of performing stochastic-gradient-descent optimization of the model</p></li>
</ul>
",1,1,2498,2018-02-26 17:37:19,https://stackoverflow.com/questions/48994062/is-there-a-way-to-save-a-gensim-doc2vec-model-as-plain-text-txt
Export gensim doc2vec embeddings into separate file to use with keras Embedding layer later,"<p>I am a bit new to gensim and right now I am trying to solve the problem which involves using the doc2vec embeddings in keras. I wasn't able to find existing implementation of doc2vec in keras - as far as I see in all examples I found so far everyone just uses the gensim to get the document embeddings.</p>

<p>Once I trained my doc2vec model in gensim I need to export embeddings weights from genim into keras somehow and it is not really clear on how to do that. I see that</p>

<pre><code>model.syn0
</code></pre>

<p>Supposedly gives the word2vec embedding weights (according to <a href=""https://codekansas.github.io/blog/2016/gensim.html"" rel=""nofollow noreferrer"">this</a>). But it is unclear how to do the same export for document embeddings. Any advise?</p>

<p>I know that in general I can just get the embeddings for each document directly from gensim model but I want to fine-tune the embedding layer in keras later on, since doc embeddings will be used as a part of a larger task hence they might be fine-tuned a bit.</p>
","keras, gensim, word-embedding, doc2vec","<p>I figured this out.</p>

<p>Assuming you already trained the gensim model and used string tags as document ids:</p>

<pre><code>#get vector of doc
model.docvecs['2017-06-24AEON']
#raw docvectors (all of them)
model.docvecs.doctag_syn0
#docvector names in model
model.docvecs.offset2doctag
</code></pre>

<p>You can export this doc vectors into keras embedding layer as below, assuming your DataFrame df has all of the documents out there. Notice that in the embedding matrix you need to pass only integers as inputs. I use raw number in dataframe as the id of the doc for input. Also notice that embedding layer requires to not touch index 0 - it is reserved for masking, so when I pass the doc id as input to my network I need to ensure it is >0</p>

<pre><code>#creating embedding matrix
embedding_matrix = np.zeros((len(df)+1, text_encode_dim))
for i, row in df.iterrows():
    embedding = modelDoc2Vec.docvecs[row['docCode']]
    embedding_matrix[i+1] = embedding

#input with id of document
doc_input = Input(shape=(1,),dtype='int16', name='doc_input')
#embedding layer intialized with the matrix created earlier
embedded_doc_input = Embedding(output_dim=text_encode_dim, input_dim=len(df)+1,weights=[embedding_matrix], input_length=1, trainable=False)(doc_input)
</code></pre>

<h1>UPDATE</h1>

<p>After late 2017, with the introduction of Keras 2.0 API very last line should be changed to:</p>

<pre><code>embedded_doc_input = Embedding(output_dim=text_encode_dim, input_dim=len(df)+1,embeddings_initializer=Constant(embedding_matrix), input_length=1, trainable=False)(doc_input)
</code></pre>
",4,2,1744,2018-02-27 00:19:17,https://stackoverflow.com/questions/48999199/export-gensim-doc2vec-embeddings-into-separate-file-to-use-with-keras-embedding
Doc2vec: Only 10 docvecs in gensim doc2vec model?,"<p>I used gensim fit a doc2vec model, with tagged document (length>10) as training data. The target is to get doc vectors of all training docs, but only 10 vectors can be found in model.docvecs.</p>

<p>The example of training data (length>10)</p>

<pre><code>docs = ['This is a sentence', 'This is another sentence', ....]
</code></pre>

<p>with some pre-treatment</p>

<pre><code>doc_=[d.strip().split("" "") for d in doc]
doc_tagged = []
for i in range(len(doc_)):
  tagd = TaggedDocument(doc_[i],str(i))
  doc_tagged.append(tagd)
</code></pre>

<p>tagged docs</p>

<pre><code>TaggedDocument(words=array(['a', 'b', 'c', ..., ],
  dtype='&lt;U32'), tags='117')
</code></pre>

<p>fit a doc2vec model</p>

<pre><code>model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)
model.build_vocab(doc_tagged)
model.train(doc_tagged, total_examples= model.corpus_count, epochs= model.iter)
</code></pre>

<p>then i get the final model</p>

<pre><code>len(model.docvecs)
</code></pre>

<p>the result is 10...</p>

<p>I tried other datasets (length>100, 1000) and got same result of <code>len(model.docvecs)</code>.
So, my question is:
How to use model.docvecs to get full vectors? (without using <code>model.infer_vector</code>)
Is <code>model.docvecs</code> designed to provide all training docvecs?</p>
","machine-learning, nlp, word2vec, gensim, doc2vec","<p>The bug is in this line:</p>



<pre class=""lang-py prettyprint-override""><code>tagd = TaggedDocument(doc[i],str(i))
</code></pre>

<p>Gensim's <code>TaggedDocument</code> accepts a <strong>sequence of tags</strong> as a second argument. When you pass a string <code>'123'</code>, it's turned into <code>['1', '2', '3']</code>, because it's treated as a <em>sequence</em>. As a result, all of the documents are tagged with just 10 tags <code>['0', ..., '9']</code>, in various combinations.</p>

<p>Another issue: you're defining <code>doc_</code> and never actually using it, so your documents will be split incorrectly as well.</p>

<p>Here's the proper solution:</p>

<pre class=""lang-py prettyprint-override""><code>docs = [doc.strip().split(' ') for doc in docs]
tagged_docs = [doc2vec.TaggedDocument(doc, [str(i)]) for i, doc in enumerate(docs)]
</code></pre>
",11,6,975,2018-02-28 03:14:02,https://stackoverflow.com/questions/49021389/doc2vec-only-10-docvecs-in-gensim-doc2vec-model
gensim docvecs.doctags incorrect indices,"<p>I'm working with a large dataset of Yelp reviews for a machine learning research project. Gensim has worked well so far, however, when I build the vocabulary with <code>doc2vec.build_vocab()</code> on the over 5,000,000 documents I have...the indices appear to all be collected into a 64-key dictionary (which should certainly not be the case).</p>

<p>Below is the script I made for tagging the documents, building the vocabulary, and training the model.</p>

<pre><code>import os
import time
import pandas as pd
import numpy as np
from collections import namedtuple
from gensim.models.doc2vec import Doc2Vec
from keras.preprocessing.text import text_to_word_sequence

# keras helper function
def text2_word_seq(review):
  return text_to_word_sequence(review, 
       filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n', 
       lower=True, split="" "")

# instantiate the model
d2v = Doc2Vec(vector_size=300, 
  window=6, min_count=5, workers=os.cpu_count()-1)

chunksize = 5000
train_data = pd.read_json(""dataset/review.json"",
    chunksize=chunksize,
    lines=True)

Review = namedtuple('Review', 'words tags')
documents = list()
for i, data in enumerate(train_data):
    print(""Looked at %d chunks, %d documents"" % 
       (i, i*chunksize), end='\r', flush=True)
    users = data.user_id.values
    for j, review in enumerate(data.text):
        documents.append(Review(text2_word_seq(review), users[j]))

# build the vocabulary 
d2v.build_vocab(documents.__iter__(), update=False,
   progress_per=100000, keep_raw_vocab=False, trim_rule=None)

# train the model
d2v.train(documents, total_examples=len(documents), epochs=10)
d2v.save('d2v-model-v001')
</code></pre>

<p>After saving the model and loading it with <code>genim.models.Doc2Vec.load()</code>, the model's <code>docvecs.doctags</code> is of length 64. Each tag I am using when building the vocabulary is a user id. It is not necessarily unique, but there are thousands of users (not 64). Also, the tags appear as single characters - which is not expected...</p>

<pre><code>&gt;&gt;&gt; len(x.docvecs.doctags)
</code></pre>

<p>64</p>

<pre><code>&gt;&gt;&gt; x.docvecs.doctags

{'Y': Doctag(offset=27, word_count=195151634, doc_count=1727798), 
'j': Doctag(offset=47, word_count=198241878, doc_count=1739169), 
'4': Doctag(offset=17, word_count=195902251, doc_count=1728095), 
'J': Doctag(offset=50, word_count=197884244, doc_count=1741666), 
'W': Doctag(offset=41, word_count=198804200, doc_count=1741269), 
'O': Doctag(offset=23, word_count=196212468, doc_count=1728735), 
'o': Doctag(offset=9, word_count=194177928, doc_count=1709768), 
'n': Doctag(offset=3, word_count=193799059, doc_count=1714620), 
'3': Doctag(offset=34, word_count=197320036, doc_count=1725467), 
'F': Doctag(offset=10, word_count=195614702, doc_count=1729058) ...
</code></pre>

<p>What am I doing wrong here?  </p>
","python, gensim, doc2vec","<p>The <code>tags</code> property of your text examples should be a list-of-tags. (It can be a list containing just a single tag, but it must be a list.)</p>

<p>If you provide a string instead, that will look like a list-of-one-character-strings to the code expecting a lis. Thus you'll train just a small number of single-character-tags, one per unique character appearing in any of the <code>tags</code> strings you provided. </p>
",2,1,534,2018-03-08 22:36:26,https://stackoverflow.com/questions/49183643/gensim-docvecs-doctags-incorrect-indices
is possible to extract bow from gensim lda model,"<p>I trained <code>LDA</code> model on my preprocessed corpus (i forget to save preprocessed data which was in  form of list of list) Is this possible to recover this data from trained model or not ?</p>
","dataset, gensim","<p>You can use dictionary.id2token to return token from ID.</p>

<p>Following code would work:</p>

<p>dictionary[0]</p>

<p>bow=[(dictionary.id2token(item),count) for item,count in doc for doc in corpus]</p>
",1,0,162,2018-03-12 07:50:42,https://stackoverflow.com/questions/49230376/is-possible-to-extract-bow-from-gensim-lda-model
Word2Vec Python similarity,"<p>I made a word embedding with this code:</p>

<pre><code>with open(""text.txt"",'r') as longFile:
        sentences = []
        single= []
        for line in longFile:
            for word in line.split("" ""):
                single.append(word)
            sentences.append(single)
    model = Word2Vec(sentences,workers=4, window=5)
</code></pre>

<p>I want now to calculate the similarity between two word and see what are the neighbours of them.
What is the difference between <code>model[""word""]</code>,<code>model.wv.most_similar()</code>, <code>model.similar_by_vector()</code> and <code>model.similarity()</code>?
Which one should I use?</p>
","python, similarity, word2vec, gensim, word-embedding","<p>Edit: Maybe we should tag <code>gensim</code> here, because it is the library we are using</p>

<p>If you want to find the neighbours of both you can use
<code>model.wv.most_similar()</code> this will give you a dict (top n) for each word and its similarities for a given string (word). This method will calculate the cosine similarity between the word-vectors.</p>

<p>Note that the other methods you mentioned are deprecated in <code>3.4.0</code>, use <code>model.wv.similarity()</code> and <code>model.wv.similar_by_vector()</code> instead.</p>

<p>You can also use <code>model.wv.similar_by_vector()</code> to do the exact same thing but by passing a vector. Eg. <code>model[""woman""]</code> would give you such a vector. Actually if you look at the implementation, all the method does is call <code>most_similar()</code></p>

<pre><code>def similar_by_vector(self, vector, topn=10, restrict_vocab=None):
   return self.most_similar(positive=[vector], topn=topn, restrict_vocab=restrict_vocab)
</code></pre>

<p>Same goes for the <code>similar_by_word()</code> method. I actually don't know why these methods exist in the first place.</p>

<p>To find a similarity measure between exactly two words you can either use
<code>model.wv.similarity()</code> to find the cosine similarity or <code>model.wv.distance()</code> to find the cosine distance between the two.</p>

<p>To answer your actual question, I would simply compute the similarity between the two instead of comparing the results of <code>most_similar()</code>.</p>

<p>I hope this helps. Look at the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">docs</a> or the source files to get even more information, the code documentation is pretty good I think.</p>
",3,4,3373,2018-03-20 09:11:53,https://stackoverflow.com/questions/49380138/word2vec-python-similarity
Python LDA gensim &quot;DeprecationWarning: invalid escape sequence&quot;,"<p>I am new to stackoverflow and python so please bear with me.
I am trying to run an Latent Dirichlet Analysis on a text corpora with the gensim package in python using PyCharm editor. I prepared the corpora in R and exported it to a csv file using this R command:</p>
<pre><code>write.csv(testdf, &quot;C://...//test.csv&quot;, fileEncoding = &quot;utf-8&quot;) 
</code></pre>
<p>Which creates the following csv structure (though with much longer and already preprocessed texts):</p>
<pre><code>,&quot;datetimestamp&quot;,&quot;id&quot;,&quot;origin&quot;,&quot;text&quot;
1,&quot;1960-01-01&quot;,&quot;id_1&quot;,&quot;Newspaper1&quot;,&quot;Test text one&quot;
2,&quot;1960-01-02&quot;,&quot;id_2&quot;,&quot;Newspaper1&quot;,&quot;Another text&quot;
3,&quot;1960-01-03&quot;,&quot;id_3&quot;,&quot;Newspaper1&quot;,&quot;Yet another text&quot;
4,&quot;1960-01-04&quot;,&quot;id_4&quot;,&quot;Newspaper2&quot;,&quot;Four Five Six&quot;
5,&quot;1960-01-05&quot;,&quot;id_5&quot;,&quot;Newspaper2&quot;,&quot;Alpha Bravo Charly&quot;
6,&quot;1960-01-06&quot;,&quot;id_6&quot;,&quot;Newspaper2&quot;,&quot;Singing Dancing Laughing&quot;
</code></pre>
<p>I then try the following essential python code (based on the <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow noreferrer"">gensim tutorials</a>) to perform simple LDA analysis:</p>
<pre><code>import gensim
from gensim import corpora, models, similarities, parsing
import pandas as pd
from six import iteritems
import os
import pyLDAvis.gensim

class MyCorpus(object):
     def __iter__(self):
             for row in pd.read_csv('//mpifg.local/dfs/home/lu/Meine Daten/Imagined Futures and Greek State Bonds/Topic Modelling/Python/test.csv', index_col=False, header = 0 ,encoding='utf-8')['text']:
                 # assume there's one document per line, tokens separated by whitespace
                 yield dictionary.doc2bow(row.split())

if __name__ == '__main__':
    dictionary = corpora.Dictionary(row.split() for row in pd.read_csv(
        '//.../test.csv', index_col=False, encoding='utf-8')['text'])
    print(dictionary)
    dictionary.save(
        '//.../greekdict.dict')  # store the dictionary, for future reference

    ## create an mmCorpus
    corpora.MmCorpus.serialize('//.../greekcorpus.mm', MyCorpus())
    corpus = corpora.MmCorpus('//.../greekcorpus.mm')

    dictionary = corpora.Dictionary.load('//.../greekdict.dict')
    corpus = corpora.MmCorpus('//.../greekcorpus.mm')

    # train model
    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=50, iterations=1000)
</code></pre>
<p>I get the following error codes and the code exits:</p>
<blockquote>
<p>...\Python\venv\lib\site-packages\setuptools-28.8.0-py3.6.egg\pkg_resources_vendor\pyparsing.py:832: DeprecationWarning: invalid escape sequence \d</p>
<p>\...\Python\venv\lib\site-packages\setuptools-28.8.0-py3.6.egg\pkg_resources_vendor\pyparsing.py:2736: DeprecationWarning: invalid escape sequence \d</p>
<p>\...\Python\venv\lib\site-packages\setuptools-28.8.0-py3.6.egg\pkg_resources_vendor\pyparsing.py:2914: DeprecationWarning: invalid escape sequence \g</p>
<p>\...\Python\venv\lib\site-packages\pyLDAvis_prepare.py:387:
DeprecationWarning:
.ix is deprecated. Please use
.loc for label based indexing or
.iloc for positional indexing</p>
</blockquote>
<p>I cannot find any solution and to be honest neither have any clue where exactly the problem comes from. I spent hours making sure that the encoding of the csv is utf-8 and exported (from R) and imported (in python) correctly.</p>
<p>What am I doing wrong or where else could I look at? Cheers!</p>
","r, python-3.x, export-to-csv, gensim, deprecation-warning","<p><code>DeprecationWarining</code> is exactly that - warning about a feature being <em>deprecated</em> which is supposed to prompt the user to use some other functionality instead to maintain the compatibility in the future. So in your case I would just watch for the update of libraries that you use.</p>

<p>Starting with the last warning it look like it is originating from <code>pandas</code> and has been logged against <code>pyLDAvis</code> <a href=""https://github.com/bmabey/pyLDAvis/issues/96"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The remaining ones come from <code>pyparsing</code> module but it does not seem that you are importing it explicitly. Maybe one of the libraries you use has a dependency and uses some relatively old and deprecated functionality. To eradicate the warning for the start I would check if upgrading does not help. Good luck!</p>
",3,3,1084,2018-03-20 16:02:47,https://stackoverflow.com/questions/49388929/python-lda-gensim-deprecationwarning-invalid-escape-sequence
Applying word2vec to find all words above a similarity threshold,"<p>The command model.most_similar(positive=['france'], topn=100) gives the top 100 most similar words to ""france"". However, I would like to know if there is a method which will output the most similar words above a similarity threshold to a given word. Is there a method like the following?:
model.most_similar(positive=['france'], threshold=0.9)</p>
","word2vec, gensim","<p>No, you'd have to request a large number (or all, with <code>topn=0</code>) then apply the cutoff yourself. </p>

<p>What you request could theoretically be added as an option. </p>

<p>However, the cosine-similarity absolute magnitudes don't necessarily have a stable meaning, like ""90% similar"" across different model runs. Their distribution can vary based on model training parameters, such as the vector <code>size</code>, and they are most-often interpreted only in ranked-comparison to other pairwise values from the same model. </p>

<p>For example, the composition of the top-100 most-similar words for 'cold' may be very similar in models with different training parameters, but the range of absolute similarity values for the #1 to #100 words can be quite different. So if you were picking an absolute threshold, you'd likely want to vary the cutoff based on observing the model, or along with other model training metaparameters.</p>
",3,7,2146,2018-03-20 18:22:22,https://stackoverflow.com/questions/49391597/applying-word2vec-to-find-all-words-above-a-similarity-threshold
Gensim Word2Vec most similar different result python,"<p>I have the first Harry Potter book in txt format. From this, I created two new txt files: in the first, all the occurrencies of <code>Hermione</code> have been replaced with <code>Hermione_1</code>; in the second, all the occurrencies of <code>Hermione</code> have been replaced with <code>Hermione_2</code>. Then I concatenated these 2 text to create one long text and I used this as input for Word2Vec.
This is my code:</p>

<pre><code>import os
from gensim.models import Word2Vec
from gensim.models import KeyedVectors

with open(""HarryPotter1.txt"", 'r') as original, \
        open(""HarryPotter1_1.txt"", 'w') as mod1, \
        open(""HarryPotter1_2.txt"", 'w') as mod2:

    data=original.read()
    data_1 = data.replace(""Hermione"", 'Hermione_1')
    data_2 = data.replace(""Hermione"", 'Hermione_2')
    mod1.write(data_1 + r""\n"")
    mod2.write(data_2 + r""\n"")

with open(""longText.txt"",'w') as longFile:
    with open(""HarryPotter1_1.txt"",'r') as textfile:
        for line in textfile:
            longFile.write(line)
    with open(""HarryPotter1_2.txt"",'r') as textfile:
        for line in textfile:
            longFile.write(line)


model = """"
word_vectors = """"
modelName = ""ModelTest""
vectorName = ""WordVectorsTestst""

answer2 = raw_input(""Overwrite  embeddig? (yes or n)"")
if(answer2 == 'yes'):
    with open(""longText.txt"",'r') as longFile:
        sentences = []
        single= []
        for line in longFile:
            for word in line.split("" ""):
                single.append(word)
            sentences.append(single)

    model = Word2Vec(sentences,workers=4, window=5,min_count=5)

    model.save(modelName)
    model.wv.save_word2vec_format(vectorName+"".bin"",binary=True)
    model.wv.save_word2vec_format(vectorName+"".txt"", binary=False)
    model.wv.save(vectorName)

    word_vectors = model.wv

else:
    model = Word2Vec.load(modelName)
    word_vectors = KeyedVectors.load_word2vec_format(vectorName + "".bin"", binary=True)

    print(model.wv.similarity(""Hermione_1"",""Hermione_2""))
    print(model.wv.distance(""Hermione_1"",""Hermione_2""))
    print(model.wv.most_similar(""Hermione_1""))
    print(model.wv.most_similar(""Hermione_2""))
</code></pre>

<p>How is possible that <code>model.wv.most_similar(""Hermione_1"")</code> and <code>model.wv.most_similar(""Hermione_2"")</code> give me different output? 
Their neighbour are completely different. This is the output of the four print:</p>

<pre><code>0.00799602753634
0.992003972464
[('moments,', 0.3204237222671509), ('rose;', 0.3189219534397125), ('Peering', 0.3185565173625946), ('Express,', 0.31800806522369385), ('no...', 0.31678506731987), ('pushing', 0.3131707012653351), ('triumph,', 0.3116190731525421), ('no', 0.29974159598350525), ('them?""', 0.2927379012107849), ('first.', 0.29270970821380615)]
[('go?', 0.45812922716140747), ('magical', 0.35565727949142456), ('Spells.""', 0.3554503619670868), ('Scabbets', 0.34701400995254517), ('cupboard.""', 0.33982667326927185), ('dreadlocks', 0.3325180113315582), ('sickening', 0.32789379358291626), ('First,', 0.3245708644390106), ('met', 0.3223033547401428), ('built', 0.3218075931072235)]
</code></pre>
","python, string, word2vec, gensim, word-embedding","<p>Training word2Vec models is random to an extent. That is why you may get different results. Also, <code>Hermione_2</code> starts appearing in the second half of the text data. In my understanding over the course of processing the data when the <code>Hermione_1</code> context is already established and so is the vector for this word you introduce a second word in exactly the same context and the algorithm tries to find what differentiates the two.
Secondly, you use a very short vector which may under-represent the complexity of the conceptual space. Due to the simplifications you get two vectors without any overlap.</p>
",1,0,966,2018-03-21 09:08:39,https://stackoverflow.com/questions/49402113/gensim-word2vec-most-similar-different-result-python
How can I load Word2vec with Gensim without getting an AttributeError?,"<p>I am new to Gensim, and I am trying to load my given (pre-trained) Word2vec model. I have 2 files: <em>xxxx.model.wv</em> and a bigger one <em>xxxx.model.wv.syn0.npy</em>.</p>

<p>When I call the following line:</p>

<pre><code>gensim.models.Word2Vec.load('xxxx.model.wv')
</code></pre>

<p>I get the following error:</p>

<pre><code>AttributeError: 'EuclideanKeyedVectors' object has no attribute 'negative'
</code></pre>

<p>How can I solve this error?</p>
","python, word2vec, gensim, word-embedding","<p>Are you sure your <code>xxxx.model.wv</code> file was a saved full <code>Word2Vec</code> model object? </p>

<p>That error suggests it was instead a <code>EuclideanKeyedVectors</code> – just the vectors, and not a full model with all properties like <code>negative</code> – so you might need to load it as that instead. </p>
",1,2,1204,2018-03-22 13:29:27,https://stackoverflow.com/questions/49429971/how-can-i-load-word2vec-with-gensim-without-getting-an-attributeerror
word co-occurrence matrix from gensim,"<p>When building a python gensim word2vec <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">model</a>, is there a way to see a doc-to-word matrix?</p>

<p>With input of <code>sentences = [['first', 'sentence'], ['second', 'sentence']]</code> I'd see something like*:</p>

<pre><code>      first  second  sentence
doc0    1       0        1
doc1    0       1        1
</code></pre>

<p>*I've illustrated 'human readable', but I'm looking for a scipy (or other) matrix, indexed to <code>model.wv.index2word</code>.</p>

<p>And, can that be transformed into a word-to-word matrix (to see co-occurences)? Something like:</p>

<pre><code>          first  second  sentence
first       1       0        1
second      0       1        1  
sentence    1       1        2   
</code></pre>

<p>I've already implemented something like <a href=""https://stackoverflow.com/questions/35562789/word-word-co-occurrence-matrix"">word-word co-occurrence matrix</a> using CountVectorizer. It works well. However, I'm already using gensim in my pipeline and speed/code simplicity matter for my use-case. </p>
","python, nlp, gensim","<p>The doc-word to word-word transform turns out to be more complex (for me at least) than I'd originally supposed. <code>np.dot()</code> is a key to its solution, but I need to apply a mask first.  I've created a more complex example for testing...</p>

<p>Imagine a doc-word matrix</p>

<pre><code>#       word1  word2  word3
# doc0    3      4      2
# doc1    6      1      0
# doc3    8      0      4 
</code></pre>

<ul>
<li>in docs were word2 occurs, word1 occurs 9 times</li>
<li>in docs were word2 occurs, word2 occurs 5 times</li>
<li>in docs were word2 occurs, word3 occurs 2 times</li>
</ul>

<p>So, when we're done we should end up with something like the below (or it's inverse). Reading in columns, the word-word matrix becomes:</p>

<pre><code>#       word1  word2  word3
# word1  17      9     11
# word2   5      5      4
# word3   6      2      6
</code></pre>

<p>A straight <code>np.dot()</code> product yields:</p>

<pre><code>import numpy as np
doc2word = np.array([[3,4,2],[6,1,0],[8,0,4]])
np.dot(doc2word,doc2word.T)
# array([[29, 22, 32],
#        [22, 37, 48],
#        [32, 48, 80]])
</code></pre>

<p>which implies that word1 occurs with itself 29 times. </p>

<p>But if, instead of multiplying doc2word times itself, I first build a mask, I get closer. Then I need to reverse the order of the arguments:</p>

<pre><code>import numpy as np
doc2word = np.array([[3,4,2],[6,1,0],[8,0,4]])
# a mask where all values greater than 0 are true
# so when this is multiplied by the orig matrix, True = 1 and False = 0
doc2word_mask = doc2word &gt; 0  

np.dot(doc2word.T, doc2word_mask)
# array([[17,  9, 11],
#        [ 5,  5,  4],
#        [ 6,  2,  6]])
</code></pre>

<p>I've been thinking about this for too long....</p>
",0,4,4660,2018-03-22 14:29:51,https://stackoverflow.com/questions/49431270/word-co-occurrence-matrix-from-gensim
TypeError: &#39;&gt;&#39; not supported between instances of &#39;float&#39; and &#39;NoneType&#39;,"<p>I have trained an LDA model using gensim library and I am using it to extract topic vectors of a document and I am using the following code</p>

<pre><code>def clean_doc(data_string):    
    global en_stop
    tokenizer = RegexpTokenizer(r'\w+') #Create appropriate tokenizer
    p_stemmer = PorterStemmer() #Create object from Porter Stemmer
    #clean and tokenize document string
    raw = data_string.lower()
    tokens = tokenizer.tokenize(raw)
    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]
    # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
    return stemmed_tokens

def infer_lda_vector(s, dictionary, model, dimensions):
    #s = s.decode('utf-8')
    vector = [0.0]*dimensions
    s = clean_doc(s)
    bow_vector = dictionary.doc2bow(s)   
    lda_vector = model[bow_vector]            
    for i in lda_vector:
        vector[i[0]] = i[1]
    return vector
</code></pre>

<p>I call it as follows:</p>

<pre><code>text = ""this a test""
lda_vector = infer_lda_vector(text, dictionary, lda_model, 300)
</code></pre>

<p>This exact piece of code was working when I was using Python2.7 but when I updated my system to Python3.x, its throwing the following error:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-36-723f03d03620&gt; in &lt;module&gt;()
      1 text = ""this a a test""
----&gt; 2 lda_vector = infer_lda_vector(text, dictionary, lda_model, 300)
      3 lda_vector

&lt;ipython-input-34-885205b68d9e&gt; in infer_lda_vector(s, dictionary, model, dimensions)
     34     s = clean_doc(s)
     35     bow_vector = dictionary.doc2bow(s)
---&gt; 36     lda_vector = model[bow_vector]
     37     for i in lda_vector:
     38         vector[i[0]] = i[1]

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\ldamodel.py in __getitem__(self, bow, eps)
   1158             `(topic_id, topic_probability)` 2-tuples.
   1159         """"""
-&gt; 1160         return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
   1161 
   1162     def save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs):

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\ldamodel.py in get_document_topics(self, bow, minimum_probability, minimum_phi_value, per_word_topics)
    979         if minimum_probability is None:
    980             minimum_probability = self.minimum_probability
--&gt; 981         minimum_probability = max(minimum_probability, 1e-8)  # never allow zero values in sparse output
    982 
    983         if minimum_phi_value is None:

TypeError: '&gt;' not supported between instances of 'float' and 'NoneType'
</code></pre>

<p>What am I doing wrong?</p>
","python, gensim, lda","<p>Cleaning and re-installation in conda fixed it.</p>

<pre><code>conda clean -t
conda install gensim
</code></pre>

<p>I am guessing a corrupt version was installed and the clean command removed it before re-installation.</p>
",0,0,2137,2018-03-27 13:24:37,https://stackoverflow.com/questions/49514111/typeerror-not-supported-between-instances-of-float-and-nonetype
Extremely slow LDA training model with large corpora python gensim,"<p>I am currently working with 9600 documents and applying gensim LDA. For training part, the process seems to take forever to get the model. I've tried to use multicore function as well, but it seems not working. I ran whole almost 3-days and I still can not get the lda model. I've checked some features of my data and the codes. I read this question <a href=""https://stackoverflow.com/questions/33929680/gensim-ldamulticore-not-multiprocessing"">gensim LdaMulticore not multiprocessing?</a>, but still don't get the solutions.</p>

<pre><code>corpora.MmCorpus.serialize('corpus_whole.mm', corpus)
corpus = gensim.corpora.MmCorpus('corpus_whole.mm')
dictionary = gensim.corpora.Dictionary.load('dictionary_whole.dict')

dictionary.num_pos
12796870

print(corpus)
MmCorpus(5275227 documents, 44 features, 11446976 non-zero entries)

# lda model training codes
lda = models.LdaModel(corpus, num_topics=45, id2word=dictionary,\
 update_every=5, chunksize=10000,  passes=100)

ldanulti = models.LdaMulticore(corpus, num_topics=45, id2word=dictionary,\
                            chunksize=10000, passes=100, workers=3)
</code></pre>

<p>This is my config to check BLAS, which I am not sure I installed proper one.
One thing I struggled here is, I can not use the command apt-get to install packages on my mac. I've installed Xcode but it still gives me an error. </p>

<pre><code>python -c 'import scipy; scipy.show_config()'
lapack_mkl_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
lapack_opt_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
blas_opt_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
blas_mkl_info:
libraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']
library_dirs = ['/Users/misun/anaconda/lib']
include_dirs = ['/Users/misun/anaconda/include']
define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
</code></pre>

<p>I have poor understanding on how to use shardedcorpus in python with my dictionary and corpora, so any helps will be appreciated! I haven't slept for 3 days to figure this problem!! Thanks!!</p>
","python, machine-learning, multiprocessing, gensim, lda","<p>I cannot really reproduce your problem on my machine but to me it looks like your problem is not multiprocessing but rather your parameter <code>passes</code>, which seems way too high to me. 
Try something like 1 or 2, which should be a good parameter to start with. If your topics don't converge well you can still increase it. </p>

<pre><code>lda = models.LdaModel(corpus, num_topics=45, id2word=dictionary, update_every=5, chunksize=10000,  passes=1)
</code></pre>

<p>This should be through in a day at most, probably just some hours (depending on your machine). </p>
",3,0,5054,2018-03-29 20:15:54,https://stackoverflow.com/questions/49564330/extremely-slow-lda-training-model-with-large-corpora-python-gensim
Gensim Doc2Vec most_similar() method not working as expected,"<p>I am struggling with Doc2Vec and I cannot see what I am doing wrong.
I have a text file with sentences. I want to know, for a given sentence, what is the closest sentence we can find in that file.</p>

<p>Here is the code for model creation:</p>

<pre><code>sentences = LabeledLineSentence(filename)

model = models.Doc2Vec(size=300, min_count=1, workers=4, window=5, alpha=0.025, min_alpha=0.025)
model.build_vocab(sentences)
model.train(sentences, epochs=50, total_examples=model.corpus_count)
model.save(modelName)
</code></pre>

<p>For test purposes, here is my file:</p>

<pre><code>uduidhud duidihdd
dsfsdf sdf sddfv
dcv dfv dfvdf g fgbfgbfdgnb
i like dogs
sgfggggggggggggggggg ggfggg
</code></pre>

<p>And here is my test:</p>

<pre><code>test = ""i love dogs"".split()
print(model.docvecs.most_similar([model.infer_vector(test)]))
</code></pre>

<p>No matter what parameter for training, this should obviously tell me that the most similar sentence is the 4th one (SENT_3 or SENT_4, I don't know how their indexes work, but the sentence labels are this form). But here is the result:</p>

<pre><code>[('SENT_0', 0.15669342875480652),
 ('SENT_2', 0.0008485736325383186),
 ('SENT_4', -0.009077289141714573)]
</code></pre>

<p>What am I missing ? And if I try with the same sentence (I LIKE dogs), I have SENT_2, then 1 then 4... I really don't get it. And why such low numbers ? And when I run few times in a row with a load, I don't get the same results either.</p>

<p>Thanks for your help</p>
","python, nlp, gensim, doc2vec, sentence-similarity","<p><code>Doc2Vec</code> doesn't work well on toy-sized examples. (Published work uses tens-of-thousands to millions of texts, and even tiny unit tests inside <code>gensim</code> uses hundreds-of-texts, combined with a much-smaller vector <code>size</code> and many more <code>iter</code> epochs, to get just-barely reliable results.)</p>

<p>So, I would not expect your code to have consistent or meaningful results. This is especially the case when:</p>

<ul>
<li>maintaining a large vector <code>size</code> with tiny data (which allows severe model overfitting)</li>
<li>using a <code>min_count=1</code> (because words without many varied usage examples can't get good vectors)</li>
<li>changing the <code>min_alpha</code> to remain the same as the larger starting alpha (because the stochastic gradient descent learning algorithm's usually-beneficial behavior relies on a gradual decay of this update-rate)</li>
<li>using documents of just a few words (as the doc-vectors are trained in proportion to the number of words they contain)</li>
</ul>

<p>Finally, even if everything else was working, <code>infer_vector()</code> usually benefits from many more <code>steps</code> than the default 5 (to the tens or hundreds of), and sometimes a starting <code>alpha</code> less like its inference default (0.1) and more like the training value (0.025). </p>

<p>So: </p>

<ul>
<li>don't change <code>min_count</code> or <code>min_alpha</code></li>
<li>get much more data</li>
<li>if it's not tens-of-thousands of texts, use a smaller vector <code>size</code> and more <code>epochs</code> (but realize results may still be weak with small data sets)</li>
<li>if each text is tiny, use more <code>epochs</code> (but realize results may still be a weaker than with longer texts)</li>
<li>try other <code>infer_vector()</code> parameters, such as <code>steps=50</code> (or more, especially with small texts), and <code>alpha=0.025</code></li>
</ul>
",3,3,2055,2018-04-03 13:47:23,https://stackoverflow.com/questions/49631758/gensim-doc2vec-most-similar-method-not-working-as-expected
How to properly tag a list of documenta by Gensim TaggedDocument(),"<p>I would like to tag a list of documents by <code>Gensim TaggedDocument()</code>, and then pass these documents as in input of <code>Doc2Vec()</code>. </p>

<p>I have read the documentation about <code>TaggedDocument</code> <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument"" rel=""nofollow noreferrer"">here</a>, but I don' t have understood what exactly are the parameters <code>words</code> and <code>tags</code>.</p>

<p>I have tried:</p>

<pre><code>texts = [[word for word in document.lower().split()]
          for document in X.values]

texts = [[token for token in text]
          for text in texts]

model = gensim.models.Doc2Vec(texts, vector_size=200)
model.train(texts, total_examples=len(texts), epochs=10)
</code></pre>

<p>But I get the error <code>'list' object has no attribute 'words'</code>.</p>
","nlp, gensim, doc2vec","<p><code>Doc2Vec</code> expects an iterable collection of texts that are each (shaped like) the example <code>TaggedDocument</code> class, with both <code>words</code> and <code>tags</code> properties. </p>

<p>The <code>words</code> can be your tokenized text (as a list), but the <code>tags</code> should be a list of document-tags that should be receive learned vectors via the <code>Doc2Vec</code> algorithm. Most often, these are unique IDs, one per document. (You can just use plain int indexes, if that works as a way to refer to your documents elsewhere, or string IDs.) Note that <code>tags</code> must be a list-of-tags, even if you're only providing one per document. </p>

<p>You are simply providing a list of lists-of-words, thus generating the error. </p>

<p>Try instead just the single line to initialize <code>texts</code>:</p>

<pre><code>texts = [TaggedDocument(
             words=[word for word in document.lower().split()],
             tags=[i]
         ) for i, document in enumerate(X.values)]
</code></pre>

<p>Also, you don't need to call <code>train()</code> if you've supplied <code>texts</code> when the <code>Doc2Vec</code> was created. (By supplying the corpus at initialization, <code>Doc2Vec</code> will automatically do both an initial vocabulary-discovery scan and then your specified number of training passes.)</p>

<p>You should look at working examples for inspiration, such as the <code>doc2vec-lee.ipynb</code> runnable Jupyter notebook that's included with <code>gensim</code>. It will be your install directory, if you can find it, but you can also view a (static, non-runnable) version inside the <code>gensim</code> source code repository at: </p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a></p>
",7,2,1473,2018-04-03 16:50:21,https://stackoverflow.com/questions/49635325/how-to-properly-tag-a-list-of-documenta-by-gensim-taggeddocument
"UnpicklingError: invalid load key, &#39;3&#39;","<p>I am creating a chatbot. So, i need word2vec file in binary format.
When i am loading bin file then i am getting this type of error.</p>

<pre><code>import gensim

model = gensim.models.Word2Vec.load('GoogleNews-vectors-negative300.bin')

Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py"", line 975, in load
return super(Word2Vec, cls).load(*args, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 629, in load
model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py"", line 278, in load
return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 395, in load
obj = unpickle(fname)
File ""/home/surya/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 1302, in unpickle
return _pickle.load(f, encoding='latin1')_pickle.

UnpicklingError: invalid load key, '3'.
</code></pre>
","python-3.x, word2vec, gensim","<p>If it is a binary file you need to mention it like this:</p>

<pre><code>import gensim.models.keyedvectors as word2vec
model = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>
",16,9,9661,2018-04-05 15:21:59,https://stackoverflow.com/questions/49676060/unpicklingerror-invalid-load-key-3
PyTorch / Gensim - How do I load pre-trained word embeddings?,"<p>I want to load a pre-trained word2vec embedding with gensim into a PyTorch embedding layer.</p>
<p>How do I get the embedding weights loaded by gensim into the PyTorch embedding layer?</p>
","python, pytorch, neural-network, gensim, word-embedding","<p>I just wanted to report my findings about loading a gensim embedding with PyTorch.</p>

<hr>

<ul>
<li><h2>Solution for PyTorch <code>0.4.0</code> and newer:</h2></li>
</ul>

<p>From <code>v0.4.0</code> there is a new function <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained"" rel=""noreferrer""><code>from_pretrained()</code></a> which makes loading an embedding very comfortable.
Here is an example from the documentation.</p>

<pre><code>import torch
import torch.nn as nn

# FloatTensor containing pretrained weights
weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])
embedding = nn.Embedding.from_pretrained(weight)
# Get embeddings for index 1
input = torch.LongTensor([1])
embedding(input)
</code></pre>

<p>The weights from <a href=""https://radimrehurek.com/gensim/"" rel=""noreferrer""><em>gensim</em></a> can easily be obtained by:</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')
weights = torch.FloatTensor(model.vectors) # formerly syn0, which is soon deprecated
</code></pre>

<p>As noted by @Guglie: in newer gensim versions the weights can be obtained by <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer""><code>model.wv</code></a>:</p>

<pre><code>weights = model.wv
</code></pre>

<hr>

<ul>
<li><h2>Solution for PyTorch version <code>0.3.1</code> and older:</h2></li>
</ul>

<p>I'm using version <code>0.3.1</code> and <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained"" rel=""noreferrer""><code>from_pretrained()</code></a> isn't available in this version.</p>

<p>Therefore I created my own <code>from_pretrained</code> so I can also use it with <code>0.3.1</code>.</p>

<p><em>Code for <code>from_pretrained</code> for PyTorch versions <code>0.3.1</code> or lower:</em></p>

<pre><code>def from_pretrained(embeddings, freeze=True):
    assert embeddings.dim() == 2, \
         'Embeddings parameter is expected to be 2-dimensional'
    rows, cols = embeddings.shape
    embedding = torch.nn.Embedding(num_embeddings=rows, embedding_dim=cols)
    embedding.weight = torch.nn.Parameter(embeddings)
    embedding.weight.requires_grad = not freeze
    return embedding
</code></pre>

<p>The embedding can be loaded then just like this:</p>

<pre><code>embedding = from_pretrained(weights)
</code></pre>

<p>I hope this is helpful for someone.</p>
",85,52,61711,2018-04-07 18:21:06,https://stackoverflow.com/questions/49710537/pytorch-gensim-how-do-i-load-pre-trained-word-embeddings
Gensim: how to load precomputed word vectors from text file,"<p>I have a text file with my precomputed word vectors in the following format (example):</p>

<p><code>word -0.0762464299711 0.0128308048976 ... 0.0712385589283\n’</code></p>

<p>on each line for every word (with 297 extra floats in place of the <code>...</code>). I am trying to load these with Gensim as KeyedVectors, because I ultimately would like to compute the cosine similarity, find most similar words, etc. Unfortunately I have not worked with Gensim before and from the documentation it's not quite clear to me how to do this. I have tried the following which I found <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""noreferrer"">here</a>:</p>

<p><code>word_vectors = KeyedVectors.load_word2vec_format('/embeddings/word.vectors', binary=False)</code></p>

<p>However this gives the following error:</p>

<p><code>ValueError: invalid literal for int() with base 10: 'the'</code></p>

<p>'the' is the first word in the text file, so I suspect that the loading function is expecting something to be there that is not. But I can't find any information on what should be there. I would highly appreciate a pointer to such information or any other solution to my problem. Thanks!</p>
","python, python-3.x, nlp, gensim","<p>You can see <a href=""https://radimrehurek.com/gensim/scripts/glove2word2vec.html"" rel=""noreferrer"">here</a> an example of Word2Vec format.
The first line is supposed to contain the number of words you have in your file followed by the dimension of your vectors. This is probably why your script is returning you an error.</p>

<p>In your example :</p>

<pre><code>1 300
word -0.0762464299711 0.0128308048976 ... 0.0712385589283
</code></pre>
",8,9,10500,2018-04-10 09:30:19,https://stackoverflow.com/questions/49750112/gensim-how-to-load-precomputed-word-vectors-from-text-file
gensim.similarities.docsim.Similarity returns empty when queried,"<p>I seem to be getting all the correct results until the very last step. My array of results keeps coming back empty. </p>

<p>I'm trying to follow this tutorial to compare 6 sets of notes:</p>

<p><a href=""https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python"" rel=""nofollow noreferrer"">https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python</a></p>

<p>I have this so far:</p>

<pre><code>#tokenize an array of all text
raw_docs = [Notes_0, Notes_1, Notes_2, Notes_3, Notes_4, Notes_5]
gen_docs = [[w.lower() for w in word_tokenize(text)]
           for text in raw_docs]

#create dictionary
dictionary_interactions = gensim.corpora.Dictionary(gen_docs)
print(""Number of words in dictionary: "", len(dictionary_interactions))
#create a corpus
corpus_interactions = [dictionary_interactions.doc2bow(gen_docs) for gen_docs in gen_docs]
len(corpus_interactions)
#convert to tf-idf model
tf_idf_interactions = gensim.models.TfidfModel(corpus_interactions)
#check for similarities between docs
sims_interactions = gensim.similarities.Similarity('C:/Users/JNproject', tf_idf_interactions[corpus_interactions],
                               num_features = len(dictionary_interactions))

print(sims_interactions)
print(type(sims_interactions))
</code></pre>

<p>with the output:</p>

<pre><code>Number of words in dictionary:  46364
Similarity index with 6 documents in 0 shards (stored under C:/Users/Jeremy Bice/JNprojects/Company/Interactions/sim_interactions)
&lt;class 'gensim.similarities.docsim.Similarity'&gt;
</code></pre>

<p>That seems right so I continue with this:</p>

<pre><code>query_doc = [w.lower() for w in word_tokenize(""client is"")]
print(query_doc)
query_doc_bow = dictionary_interactions.doc2bow(query_doc)
print(query_doc_bow)
query_doc_tf_idf = tf_idf_interactions[query_doc_bow]
print(query_doc_tf_idf)

#check for similarities between docs
sims_interactions[query_doc_tf_idf]
</code></pre>

<p>and my output is this:</p>

<pre><code>['client', 'is']
[(335, 1), (757, 1)]
[]
array([ 0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)
</code></pre>

<p>How do I get an output here?</p>
","python-3.x, nltk, jupyter-notebook, gensim","<p>Depending on the content of your <code>raw_docs</code>, this can be the correct behaviour. </p>

<p>Your code returns an empty <code>tf_idf</code> although your query words appear in your original documents and your dictionary. <code>tf_idf</code> is computed by <code>term_frequency * inverse_document_frequency</code>. <code>inverse_document_frequency</code> is computed by <code>log(N/d)</code>, where <code>N</code> is your total number of documents and <code>d</code> is the number of documents a specific term occurs in. </p>

<p>My guess is that your query terms <code>['client', 'is']</code> occur in each document of yours, resulting in an <code>inverse_document_frequency</code> of <code>0</code> and an empty <code>tf_idf</code> list. You can check this behaviour with the documents I took and modified from the tutorial you mentioned:</p>

<pre><code># original: commented out
# added arbitrary words 'now' and 'the' where missing, so they occur in each document

#raw_documents = [""I'm taking the show on the road."",
raw_documents = [""I'm taking the show on the road now."",
#                 ""My socks are a force multiplier."",
                 ""My socks are the force multiplier now."",
#                 ""I am the barber who cuts everyone's hair who doesn't cut their own."",
                 ""I am the barber who cuts everyone's hair who doesn't cut their own now."",
#                 ""Legend has it that the mind is a mad monkey."",
                 ""Legend has it that the mind is a mad monkey now."",
#                 ""I make my own fun.""]
                 ""I make my own the fun now.""]
</code></pre>

<p>If you query for </p>

<pre><code>query_doc = [w.lower() for w in word_tokenize(""the now"")]
</code></pre>

<p>you get</p>

<pre><code>['the', 'now']
[(3, 1), (8, 1)]
[]
[0. 0. 0. 0. 0.]
</code></pre>
",2,1,796,2018-04-10 18:53:50,https://stackoverflow.com/questions/49761033/gensim-similarities-docsim-similarity-returns-empty-when-queried
Document similarity in Spacy vs Word2Vec,"<p>I have a niche corpus of ~12k docs, and I want to test near-duplicate documents with similar meanings across it - think article about the same event covered by different news organisations. </p>

<p>I have tried gensim's Word2Vec, which gives me terrible similarity score(&lt;0.3) even when the test document is <em>within</em> the corpus, and I have tried SpaCy, which gives me >5k documents with similarity > 0.9. I tested SpaCy's most similar documents, and it was mostly useless.</p>

<p>This is the relevant code.                                                              </p>

<pre><code>tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=40)
doc = preprocess(query)
vec_bow = dictionary.doc2bow(doc)
vec_lsi_tfidf = lsi[tfidf[vec_bow]] # convert the query to LSI space
index = similarities.Similarity(corpus = corpus, num_features = len(dictionary), output_prefix = ""pqr"")
sims = index[vec_lsi_tfidf] # perform a similarity query against the corpus
most_similar = sorted(list(enumerate(sims)), key = lambda x:x[1])

for mid in most_similar[-100:]:
    print(mid, file_list[mid[0]])
</code></pre>

<p>Using gensim I have found a decent approach, with some preprocessing, but the similarity score is still quite low. Has anyone faced such a problem, and are there are some resources or suggestions that could be useful?</p>
","python-3.x, nlp, gensim, spacy","<p>I would post a comment but I don't have enough reputation! In NLP it is easy to get caught up in the methods and forget about the preprocessing.</p>

<p>1) Remove Stopwords/most frequent words</p>

<p>2) Merge word pairs - Look at SpaCy's documentation</p>

<p>i.e. ""New York City"" becomes its own unique token instead of ""New"", ""York"", ""City""</p>

<p><a href=""https://spacy.io/usage/linguistic-features"" rel=""noreferrer"">https://spacy.io/usage/linguistic-features</a></p>

<p>3) Use Doc2Vec instead of Word2Vec (Since you are already using gensim, this shouldn't be too hard to figure out, they have their own implementation)</p>

<p>Then, once you have done all of these things, you will have document vectors, which will likely give you a better score. Also, keep in mind that the 12k docs that you have are a small amount of samples in the grand scheme of things.</p>
",14,3,9509,2018-04-11 05:37:30,https://stackoverflow.com/questions/49767270/document-similarity-in-spacy-vs-word2vec
Using a model to compare name and surname,"<p>I have names of employees saved in a text file. I processed the file and compared a name that already exist.
When I checked using most_similar method, I found that it returns totally unrelated name even if the exact same name exist in the corpus.</p>

<pre><code>import gensim

training_file='todel.txt'
mylist=list()
with open(training_file, encoding=""iso-8859-1"") as f:
    for i, line in enumerate(f):
        mylist.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]))

model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=55)
model.build_vocab(mylist)

inferred_vector=model.infer_vector(['aakash', 'prakash', 'patel'])

sims = model.docvecs.most_similar([inferred_vector])

' '.join(mylist[sims[0][0]].words)
</code></pre>

<p>How do I correctly train the data to return (closely) matching names?</p>
","machine-learning, gensim","<p>You define similarity in terms of <em>edit distance</em>, i.e. how similar two <strong>strings</strong> are. </p>

<p>x2vec models define similarity in terms of <em>semantic closeness</em>, i.e. how similar two <strong>meanings</strong> are, computed through machine learning and co-occurrence statistics. </p>

<p>In other words, you're using a sledgehammer to kill a fly. Look into <a href=""https://pypi.python.org/pypi/python-Levenshtein"" rel=""nofollow noreferrer"">tools for computing string distance</a> instead:</p>



<pre class=""lang-python prettyprint-override""><code>from Levenshtein import distance
string1 = 'aakash'
string2 = 'akash'
string3 = 'konstantinos'
print(distance(string1, string2))
1
print(distance(string1, string3))
11
</code></pre>
",2,0,111,2018-04-11 06:58:06,https://stackoverflow.com/questions/49768453/using-a-model-to-compare-name-and-surname
Python 3.6: ImportError: cannot import name &#39;config&#39; when trying to import gensim,"<p>I am running the latest version of Python:</p>

<pre><code>'3.6.5 |Anaconda custom (64-bit)| (default, Mar 29 2018, 13:14:23) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]'
</code></pre>

<p>Upon trying to import gensim like so:</p>

<pre><code>from gensim.corpora import Dictionary
import numpy as np
</code></pre>

<p>I get the following error:</p>

<pre><code>/anaconda/lib/python3.6/site-packages/boto/provider.py in &lt;module&gt;()
     32 
     33 import boto
---&gt; 34 from boto import config
     35 from boto.compat import expanduser
     36 from boto.pyami.config import Config

ImportError: cannot import name 'config'
</code></pre>

<p>I have tried updating Python, all of the packages and their dependencies, and so on.  Nothing seems to be working.</p>

<p>Any thoughts?</p>
","python, jupyter, boto, gensim","<p>I have recently bumped into a similar problem as following as well:</p>

<pre><code>/anaconda2/lib/python2.7/site-packages/boto3/session.py in &lt;module&gt;()
     16 
     17 import botocore.session
---&gt; 18 from botocore.client import Config
     19 from botocore.exceptions import DataNotFoundError, UnknownServiceError
     20 

ImportError: cannot import name Config
</code></pre>

<p>Based on my experience, it is rooted in the conflict of dependencies for packages in your conda environment. So the way I resolved it is as following:</p>

<ol>
<li>Remove Anaconda completely. (see <a href=""https://stackoverflow.com/a/42182997/1874449"">here</a>)</li>
<li>Reinstall Anaconda from scratch (see <a href=""https://www.anaconda.com/distribution/"" rel=""nofollow noreferrer"">here</a>)</li>
<li>Install <code>Gensim</code> library with <code>conda install -c anaconda gensim</code> command</li>
</ol>

<p>It is needful to say that to avoid future issues similar to this, you should try creating different environment variables for conda as it keeps packages separate from each other hence no conflict of packages. You can see <a href=""https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/"" rel=""nofollow noreferrer"">this post</a> that explains clearly how to create such an environment before starting your different projects</p>

<p>I hope that helps.</p>
",1,1,5005,2018-04-12 15:36:01,https://stackoverflow.com/questions/49800622/python-3-6-importerror-cannot-import-name-config-when-trying-to-import-gensi
gensim model return ids not related with input doc2vec,"<p>I created a model from mongodb db news and I tagged the documents by mongo collection id</p>

<pre><code>from gensim.models.doc2vec import TaggedDocument
i=0
docs=[]
for artical in lstcontent:
    doct = TaggedDocument(clean_str(artical), [lstids[i]])
    docs.append(doct)
    i+=1
</code></pre>

<p>after that I created the model by</p>

<pre><code>pretrained_emb='tweet_cbow_300/tweets_cbow_300'
saved_path = ""documentmodel/doc2vec_model.bin""
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, pretrained_emb=pretrained_emb, iter=train_epoch)
model.save(saved_path)
</code></pre>

<p>when I using the model by the code :</p>

<pre><code>import gensim.models as g
import codecs
model=""documentmodel/doc2vec_model.bin""
start_alpha=0.01
infer_epoch=1000
m = g.Doc2Vec.load(model)
sims = m.docvecs.most_similar(['5aa94578094b4051695eeb10'])
sims
</code></pre>

<p>the output is</p>

<pre><code>[('5aa944c1094b4051695eeaef', 0.9255372881889343),
('5aa945c1094b4051695eeb1d', 0.9222575426101685),
('5aa94584094b4051695eeb12', 0.9210859537124634),
('5aa945d2094b4051695eeb20', 0.9083569049835205),
('5aa945c7094b4051695eeb1e', 0.905883252620697),
('5aa9458f094b4051695eeb14', 0.9054019451141357),
('5aa944c7094b4051695eeaf0', 0.9019848108291626),
('5aa94589094b4051695eeb13', 0.9012798070907593),
('5aa945b1094b4051695eeb1a', 0.9000773429870605),
('5aa945bc094b4051695eeb1c', 0.8999895453453064)]
</code></pre>

<p>the ids not related with 5aa94578094b4051695eeb10
where is my proplem !?</p>
","word2vec, gensim, cosine-similarity, doc2vec","<p>It looks like you might be providing a string as the <code>words</code> of your <code>TaggedDocument</code> texts. It should be a list-of-words. (If you supply a string, it will see it as a list-of-single-character-words, and try to run the algorithm as character-to-character predictions – which won't lead to very good vectors.)</p>

<p>If you enable INFO level logging, and watch the output, you may see hints that this is the problem, in the form of a very small count of vocabulary words, dozens rather than tens-of-thousands. Or if that's not the problem, you may see other discrepancies that hint at what's going wrong. </p>

<p>Separate observations &amp; tips:</p>

<ul>
<li><p>you're using a 'pretrained_emb' argument that's not part of standard gensim. If you're using an unofficial variant, based on an older gensim, you might have other issues. Pretrained word embeddings are <em>not</em> necessary for <code>Doc2Vec</code> to work, and may not offer much benefit. (I would always try without any such extra complications, first, then only after you have a simple approach working as a baseline, try such added tweaks and always evaluate if they're really helping or not.)</p></li>
<li><p>it's unclear how many <code>iter</code> passes you're using, but 10-20 are typical values, perhaps more if your corpus is small and/or typical texts are short</p></li>
<li><p><code>dm=1, dm_concat=1</code> (PV-DM with a concatenative input layer) results in larger, slower models that may require much more data to become well-trained. It's not clear this <code>dm_concat=1</code> mode is ever worth the trouble. At best, it should be considered experimental. So I would get things working without it, before perhaps trying it as an advanced experiment. </p></li>
</ul>
",1,2,253,2018-04-14 09:10:00,https://stackoverflow.com/questions/49829787/gensim-model-return-ids-not-related-with-input-doc2vec
Clear approach for assigning semantic tags to each sentence (or short documents) in python,"<p>I am looking for a good approach using python libraries to tackle the following problem:</p>

<p>I have a dataset with a column that has product description. The values in this column can be very messy and would have a lot of other words that are not related to the product. I want to know which rows are about the same product, so I would need to tag each description sentence with its main topics. For example, if I have the following: 
""500 units shoe green sport tennis import oversea plastic"", I would like the tags to be something like: ""shoe"", ""sport"". So I am looking to build an approach for semantic tagging of sentences, not part of speech tagging. Assume I don't have labeled (tagged) data for training. </p>

<p>Any help would be appreciated.</p>
","python-2.7, nlp, nltk, gensim, semantic-analysis","<p>Lack of labeled data means you cannot apply any semantic classification method using word vectors, which would be the optimal solution to your problem. An alternative however could be to construct the document frequencies of your token n-grams and assume importance based on some smoothed variant of idf (i.e. words that tend to appear often in descriptions probably carry some semantic weight). You can then inspect your sorted-by-idf list of words and handpick(/erase) words that you deem important(/unimportant). The results won't be perfect, but it's a clean and simple solution given your lack of training data. </p>
",2,1,379,2018-04-19 19:44:29,https://stackoverflow.com/questions/49929066/clear-approach-for-assigning-semantic-tags-to-each-sentence-or-short-documents
Gensim build_vocab taking too long,"<p>I'm trying to train a doc2vec model using the gensim library on 50 million sentences of variable length.</p>

<p>Some tutorials (eg. <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a>) have a <code>model.build_vocab</code> step before the actual training process. This part has been running for 3 hours now without any updates.</p>

<p>Is this step necessary for the training process? Why could this step be taking so long since it's just a linear pass over the data?</p>

<p>Using gensim version 3.4.0 with python 3.6.0</p>
","python-3.x, nlp, word2vec, gensim","<p>The <code>build_vocab()</code> step to discover all words, then set-up the known-vocabulary structures, is required. (Though, if you supply your corpus as an argument to <code>Doc2Vec</code>, both the <code>build_vocab()</code> and <code>train()</code> will be done automatically.)</p>

<p>You should enable Python logging at the INFO level to see logged information about the progress of this, and other long-running gensim steps. This will help you see if progress is truly being made, or has stopped or slowed at some point. </p>

<p>If the vocabulary-discovery starts fast but then slows, perhaps your system has too little memory and has begun using very-slow virtual memory (swapping). If it seems to stop, perhaps there's a silent error in your method of reading the corpus. If it's just slow the whole way, perhaps there's something wrong with your method of reading the corpus. </p>
",5,3,2024,2018-04-22 05:18:53,https://stackoverflow.com/questions/49962749/gensim-build-vocab-taking-too-long
Correct way of using Phrases and preprocess_string gensim,"<p>What is the correct way to use gensim's Phrases and preprocess_string together ?, i am doing this way but it a little contrived.</p>

<pre><code>from gensim.models.phrases import Phrases
from gensim.parsing.preprocessing import preprocess_string
from gensim.parsing.preprocessing import strip_tags
from gensim.parsing.preprocessing import strip_short
from gensim.parsing.preprocessing import strip_multiple_whitespaces
from gensim.parsing.preprocessing import stem_text
from gensim.parsing.preprocessing import remove_stopwords
from gensim.parsing.preprocessing import strip_numeric
import re
from gensim import utils

# removed ""_"" from regular expression
punctuation = r""""""!""#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^`{|}~""""""

RE_PUNCT = re.compile(r'([%s])+' % re.escape(punctuation), re.UNICODE)


def strip_punctuation(s):
    """"""Replace punctuation characters with spaces in `s` using :const:`~gensim.parsing.preprocessing.RE_PUNCT`.

    Parameters
    ----------
    s : str

    Returns
    -------
    str
        Unicode string without punctuation characters.

    Examples
    --------
    &gt;&gt;&gt; from gensim.parsing.preprocessing import strip_punctuation
    &gt;&gt;&gt; strip_punctuation(""A semicolon is a stronger break than a comma, but not as much as a full stop!"")
    u'A semicolon is a stronger break than a comma  but not as much as a full stop '

    """"""
    s = utils.to_unicode(s)
    return RE_PUNCT.sub("" "", s)



my_filter = [
    lambda x: x.lower(), strip_tags, strip_punctuation,
    strip_multiple_whitespaces, strip_numeric,
    remove_stopwords, strip_short, stem_text
]


documents = [""the mayor of new york was there"", ""machine learning can be useful sometimes"",""new york mayor was present""]

sentence_stream = [doc.split("" "") for doc in documents]
bigram = Phrases(sentence_stream, min_count=1, threshold=2)
sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
test  = "" "".join(bigram[sent])


print(preprocess_string(test))
print(preprocess_string(test, filters=my_filter))
</code></pre>

<p>The result is:</p>

<pre><code>['mayor', 'new', 'york']
['mayor', 'new_york'] #correct
</code></pre>

<p>part of the code was taken from: <a href=""https://stackoverflow.com/questions/35716121/how-to-extract-phrases-from-corpus-using-gensim/35748858"">How to extract phrases from corpus using gensim</a></p>
","python, python-3.x, nlp, gensim","<p>I would recommend using <a href=""https://radimrehurek.com/gensim/utils.html#gensim.utils.tokenize"" rel=""nofollow noreferrer""><code>gensim.utils.tokenize()</code></a> instead of <a href=""https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.preprocess_string"" rel=""nofollow noreferrer""><code>gensim.parsing.preprocessing.preprocess_string()</code></a> for your example.</p>

<p>In many cases <code>tokenize()</code> does a very good job as it will only return sequences of alphabetic characters (no digits). This saves you the extra cleaning steps for punctuation etc.</p>

<p>However, <code>tokenize()</code> does not include removal of stopwords, short tokens nor stemming. This has to be cutomized anyway if you are dealing with other languages than English.</p>

<p>Here is some code for your (already clean) example documents which gives you the desired bigrams.</p>

<pre class=""lang-py prettyprint-override""><code>documents = [""the mayor of new york was there"",
             ""machine learning can be useful sometimes"",
             ""new york mayor was present""]

import gensim, pprint

# tokenize documents with gensim's tokenize() function
tokens = [list(gensim.utils.tokenize(doc, lower=True)) for doc in documents]

# build bigram model
bigram_mdl = gensim.models.phrases.Phrases(tokens, min_count=1, threshold=2)

# do more pre-processing on tokens (remove stopwords, stemming etc.)
# NOTE: this can be done better
from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, stem_text
CUSTOM_FILTERS = [remove_stopwords, stem_text]
tokens = [preprocess_string("" "".join(doc), CUSTOM_FILTERS) for doc in tokens]

# apply bigram model on tokens
bigrams = bigram_mdl[tokens]

pprint.pprint(list(bigrams))
</code></pre>

<p>Output:</p>

<pre class=""lang-py prettyprint-override""><code>[['mayor', 'new_york'],
 ['machin', 'learn', 'us'],
 ['new_york', 'mayor', 'present']]
</code></pre>
",3,1,6653,2018-04-24 18:58:29,https://stackoverflow.com/questions/50009030/correct-way-of-using-phrases-and-preprocess-string-gensim
How to use gensim&#39;s LDA to conduct text-retrievals from queries?,"<p>I am trying to understand how LDA can be used for text-retrieval, and I am currently using the gensim's LdaModel model for implementing LDA, here: <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamodel.html</a>. </p>

<p>I have managed to identify the k topics and their most-used words, and I understand that LDA is about probabilistic distributions of topics and how words are distributed within those topics in the documents, so that much makes sense. </p>

<p>That said, I do not understand how to use the LdaModel to retrieve the documents that are relevant to a string input of search query eg ""negative effects of birth control"". I have tried inferring topic distributions on the search query and finding similarities between the topic distribution on the search query and the topic distributions from the corpus using gensim's similarities.MatrixSimilarity to compute cosine similarity like so:</p>

<p><code>lda = LdaModel(corpus, num_topics=10)
 index = similarities.MatrixSimilarity(lda[corpus])
 query = lda[query_bow]
 sims = index[query]</code></p>

<p>But the performance isn't really good. What I figure is that finding the topic distribution of the search query is not too meaningful because there is usually only 1 topic in the search query. But I don't know how else I could implement this on the LdaModel on gensim. Any advice would be really appreciated, I am new to topic modeling and maybe I am missing something that's glaringly obvious to me? Thanks!</p>
","gensim, information-retrieval, lda, topic-modeling","<p>I believe your text query lengths are too small and/or your ratio of number of topics to length of query is too small for what you want to achieve.</p>

<p>If you want to use LDA to find similar topics to a given query, in most cases you will in deed need more than one topic per query to be able to present a specific document rather than a whole section of documents.</p>

<p>Your LDA model above has only 10 topics so your chances of finding more than one topic in a given sentence are very low. So already, I would suggest testing if training the model on 100 or 200 topics makes this a bit better. Now you have high chances of falling onto several topics in one sentence. </p>

<p>Here is an (oversimplified) example of why this could work:<br>
With <code>num_topics=10</code> you might have topics:</p>

<pre><code>topic_1: ""pizza"", ""pie"", ""fork"", dinner"", ""farm"",...
topic_2: ""pilot"", ""navy"", ""ocean"", ""air"", ""USA"", ...
...
</code></pre>

<p>Now if you query the sentence </p>

<pre><code>""Tonight at dinner I will eat pizza with a fork""
</code></pre>

<p>You will only get <code>topic_1</code> as a response</p>

<p>However, with <code>num_topics=200</code> your topics might be something like this</p>

<pre><code>topic_1: ""pizza"", ""margherita"", ""funghi"",...
topic_2: ""fork"", ""knife"", ""spoon"",...
topic_3: ""dinner"", ""date"", ""lunch"", ...
</code></pre>

<p>So the same sentence now covers <code>topic_1</code>, <code>topic_2</code>, <code>topic_3</code>.</p>

<p>Now it depends a lot on your corpus if increasing the number of topics that much will make the output good. For something as large as the English Wikipedia, 200 topics works. For a smaller corpus this is not clear. </p>

<p>And even with more topics, I believe it could still be the case that your query text is just too short. </p>
",4,3,791,2018-04-26 01:39:21,https://stackoverflow.com/questions/50033595/how-to-use-gensims-lda-to-conduct-text-retrievals-from-queries
Do gensim Doc2Vec distinguish between same Sentence with positive and negative context.?,"<p>While learning Doc2Vec library, I got stuck on the following question.</p>

<p><strong>Do gensim Doc2Vec distinguish between the same Sentence with positive and negative context?</strong></p>

<p>For Example:</p>

<p>Sentence A: ""I love Machine Learning""</p>

<p>Sentence B: ""I do not love Machine Learning""</p>

<p>If I train sentence A and B with doc2vec and find cosine similarity between their vectors:</p>

<ol>
<li>Will the model be able to distinguish the sentence and give a cosine similarity very less than 1 or negative?</li>
<li>Or Will the model represent both the sentences very close in vector space and give cosine similarity close to 1, as mostly all the words are same except the negative word (do not).</li>
</ol>

<p>Also, If I train only on sentence A and try to infer Sentence B, will both vectors be close to each other in vector space.?</p>

<p>I would request the NLP community and Doc2Vec experts for helping me out in understanding this.</p>

<p>Thanks in Advance !!</p>
","python, nlp, gensim, doc2vec","<p>Inherently, all that the 'Paragraph Vector' algorithm behind gensim <code>Doc2Vec</code> does is find a vector that (together with a neural-network) is good at predicting the words that appear in a text. So yes, texts with almost-identical words will have very close vectors. (There's no syntactic understanding that certain words, in certain places, have a big reversing-effect.)</p>

<p>However, even such vectors <em>may</em> be ok (though not state-of-the-art) at sentiment analysis. One of the ways the original 'Paragraph Vectors' paper evaluated the vector usability was estimating the sentiment of short movie reviews. (These were longer than a single sentence – into the hundreds of words.) When training a classifier on the doc-vectors, the classifier did a pretty good job, and better than other baseline techniques, at estimating the negativity/positivity of reviews. </p>

<p>Your single, tiny, contrived sentences could be harder – they're short with just a couple words' difference, so the vectors will be very close. But those different words (especially <code>'not'</code>) are often very indicative of sentiment – so the tiny difference might be enough to shift the vector from the 'positive' regions to the 'negative' regions. </p>

<p>So you'd have to try it, with a real training corpus of tens of thousands of varied text examples (because this technique doesn't work well on toy-sized datasets) and a post-vectorization classifier step. </p>

<p>Note also that in pure <code>Doc2Vec</code>, adding known labels (like 'positive' or 'negative') during training (alongside or instead of any unique document-ID based tags) can sometimes help the resulting vector-space be more sensitive to the distinction you want. And, other variant techniques like 'FastText' or 'StarSpace' more directly integrate known-labels into the vectorization in a way that might help. </p>

<p>The best results on short sentences, though, would probably take into account the relative ordering of words and grammatical parsing. You can see a demo of such a more-advanced technique at a page from Stanford's NLP research group: </p>

<p><a href=""http://nlp.stanford.edu:8080/sentiment/rntnDemo.html"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/sentiment/rntnDemo.html</a></p>

<p>Though look in the comments there for various examples of hard cases that it still struggles with.</p>
",3,4,988,2018-04-26 08:31:46,https://stackoverflow.com/questions/50038347/do-gensim-doc2vec-distinguish-between-same-sentence-with-positive-and-negative-c
joining sentences from a list in python3,"<p>I am trying to join a lists of appended sentences into a large a string text object so that I can use it as an input for the Gensim summarize module. However, when I try to do this, it says the input sentences are less than 2. But when I run a split on the text, I see multiple sentences but it counts each sentence once instead of the total of sentences together. And the variable r is a string type object. I would like to concatenate the sentences together into one large string so it can be read through the Gensim summarize module.</p>

<p><strong>Sample Code:</strong></p>

<pre><code>import re
ruling_corpora  = re.findall(""\.?([^\.].\*?I find[^\.]*\. |[^\.]*$In sum[^\.]*\. |[^\.]*$agree[^\.]*\.)"", tokenized, re.I |re.DOTALL |re.M)[1:-1]

for r in ruling_corpora:                                   
    print(type(r))
    rc= ''.join(r)
    print(summarize(rc))
</code></pre>

<p>SAMPLE OUTPUT:</p>

<pre><code>raise ValueError(""input must have more than one sentence"")
ValueError: input must have more than one sentence
</code></pre>

<p>Here is an example of my input I want to summarize with the Gensim summarizer. The numbers underneath each string represent the count of sentences ending in periods:</p>

<pre><code>####Beginning of File### LUMB65.BL23607963.xml
Background Content: ANDERSON INITIAL DECISIONOn January 13, 2015, the appellant filed this appeal arguing that the agency's decision not to renew his term limited appointment which expired on January 28, 2015, is in error.  

 For the reasons discussed below, this appeal is DISMISSED for lack of jurisdiction without a hearing.
1
There is nothing in the agreement that curtails the agency's ability not to extend the term appointment. 
 IdIn reviewing the appellant's arguments, the appellant fails to establish that the Board has jurisdiction to review the agency's decision not to renew his time-limited appointment at issue in this appeal.
 Following a review of the record evidence, I find that the appellant has failed to non-frivolously allege Board jurisdiction over this appeal on any basis.
 Accordingly, this appeal must be dismissed for lack of jurisdiction.
1
####End of File### LUMB65.BL23607963.xml
</code></pre>
","python, regex, list, join, gensim","<p>According to <a href=""https://radimrehurek.com/gensim/summarization/summariser.html#gensim.summarization.summarizer.summarize"" rel=""nofollow noreferrer"">the documentation</a> (emphasis mine):</p>

<blockquote>
  <p>The input should be a string, and must be longer than INPUT_MIN_LENGTH
  sentences for the summary to make sense. The text will be split into
  sentences using the split_sentences method in the
  gensim.summarization.texcleaner module. <strong>Note that newlines divide
  sentences.</strong></p>
</blockquote>

<p>Try using <code>rc = '\n'.join(r)</code>.  You could also debug by calling <code>gensim.summarization.texcleaner.split_sentences</code> to check the result.</p>

<p>Additionally your regular expression does not match your given input and even if it did, you discard the only two results with <code>[1:-1]</code>.  Try this instead:</p>

<pre><code>&gt;&gt;&gt; map(lambda x: x[0], re.findall('([^.]*?(I find|In sum|agree)[^.]*\.)', tokenized, re.I | re.DOTALL | re.M))
[""\n1\nThere is nothing in the agreement that curtails the agency's ability not to extend the term appointment."", '\n Following a review of the record evidence, I find that the appellant has failed to non-frivolously allege Board jurisdiction over this appeal on any basis.']
</code></pre>

<p>You may want to process out the stand-alone numbers first since they show up in the match.</p>
",0,-1,1007,2018-04-26 17:03:20,https://stackoverflow.com/questions/50048484/joining-sentences-from-a-list-in-python3
How do I find a synonym of a word or multi-word paraphrase using the gensim toolkit,"<p>Having loaded a pre-trained word2vec model with the gensim toolkit, I would like to find a synonym of a word given a context such as intelligent for 'she is a bright person'.</p>
","python, nlp, word2vec, gensim, word-sense-disambiguation","<p>There's a method <code>[most_similar()][1]</code> that will report the words of the closest vectors, by cosine-similarity in the model's coordinates, to a given word. For example:</p>

<pre><code>similars = loaded_w2v_model.most_similar('bright')
</code></pre>

<p>However, Word2vec won't find strictly synonyms – just words that were contextually-related in its training-corpus. These are often synonym-like, but also can be similar in other ways – such as used in the same topical domains, or able to replace each other functionally. (In that last respect, sometimes the highly-similar word-vectors are for <em>antonyms</em>, because words like 'hot' and 'cold' appear in the same places, referring the the same aspect of something.)</p>

<p>Plain word2vec also doesn't deal with polysemy (that a token like 'bright' is both a word for 'well-lit' and a word for 'smart') well. So the list of most-similar words for 'bright' will include a mix from its alternate senses.</p>
",6,2,3500,2018-05-05 15:44:52,https://stackoverflow.com/questions/50191231/how-do-i-find-a-synonym-of-a-word-or-multi-word-paraphrase-using-the-gensim-tool
IndexError when trying to update gensim&#39;s LdaModel,"<p>I am facing the following error when trying to update my gensim's <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">LdaModel</a>: </p>

<blockquote>
  <p>IndexError: index 6614 is out of bounds for axis 1 with size 6614</p>
</blockquote>

<p>I checked why were other people having this issue on <a href=""https://groups.google.com/forum/#!topic/gensim/T0GMxE7YZqM"" rel=""nofollow noreferrer"">this thread</a>, but I am using the same dictionary from the beginning to the end, which was their error.</p>

<p>As I have a big dataset, I am loading it chunk by chunk (using pickle.load). I am building the dictionary in this way, iteratively, thanks to this piece of code : </p>

<p> </p>

<pre><code> fr_documents_lda = open(""documents_lda_40_rails_30_ruby_full.dat"", 'rb')
 dictionary = Dictionary()
 chunk_no = 0
 while 1:
     try:
         t0 = time()
         documents_lda = pickle.load(fr_documents_lda)
         chunk_no += 1
         dictionary.add_documents(documents_lda)
         t1 = time()
         print(""Chunk number {0} took {1:.2f}s"".format(chunk_no, t1-t0))
     except EOFError:
         print(""Finished going through pickle"")
         break
</code></pre>

<p>Once built for the whole dataset, I am training the model in the same fashion, iteratively, this way :</p>

<pre><code>fr_documents_lda = open(""documents_lda_40_rails_30_ruby_full.dat"", 'rb')
first_iter = True
chunk_no = 0
lda_gensim = None
while 1:
    try:
        t0 = time()
        documents_lda = pickle.load(fr_documents_lda) 
        chunk_no += 1
        corpus = [dictionary.doc2bow(text) for text in documents_lda]
        if first_iter:
            first_iter = False
            lda_gensim = LdaModel(corpus, num_topics=no_topics, iterations=100, offset=50., random_state=0, alpha='auto')
        else:
            lda_gensim.update(corpus)
        t1 = time()
        print(""Chunk number {0} took {1:.2f}s"".format(chunk_no, t1-t0))
    except EOFError:
        print(""Finished going through pickle"")
        break
</code></pre>

<p>I also tried updating the dictionary at every chunk, i.e. having
 </p>

<pre><code>dictionary.add_documents(documents_lda)
</code></pre>

<p>right before
 </p>

<pre><code>corpus = [dictionary.doc2bow(text) for text in documents_lda]
</code></pre>

<p> in the last piece of code. Finally, I tried setting the allow_update argument of doc2bow to True. Nothing works.</p>

<p>FYI, the size of my final dictionary is 85k. The size of my dictionary built only from the first chunk is 10k. The error occurs on the second iteration, when it passes in the else condition, when calling the update method.</p>

<p>The error is raised by the line  <code>expElogbetad = self.expElogbeta[:, ids]</code>
, called by <code>gamma, sstats = self.inference(chunk, collect_sstats=True)</code>, itself called by <code>gammat = self.do_estep(chunk, other)</code>, itself called by <code>lda_gensim.update(corpus)</code>.</p>

<p>Is anyone having an idea on how to fix this, or what is happening ?</p>

<p>Thank you in advance.</p>
","python-3.x, gensim, lda, topic-modeling, index-error","<p>The solution is simply to initialize the LdaModel with the argument <code>id2word = dictionary</code>.</p>

<p>If you don't do that, it assumes that your vocabulary size is the vocabulary size of the first set of documents you train it on, and can't update it. In fact, it sets its <code>num_terms</code> value to the length of id2word once <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/ldamodel.py#L277"" rel=""nofollow noreferrer"">there</a>, and never updates it afterwards (you can verify in the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/ldamodel.py#L586"" rel=""nofollow noreferrer"">update</a> function).</p>
",2,3,1451,2018-05-07 13:02:05,https://stackoverflow.com/questions/50214899/indexerror-when-trying-to-update-gensims-ldamodel
gensim: &#39;Doc2Vec&#39; object has no attribute &#39;intersect_word2vec_format&#39; when I load the Google pre-trained word2vec model,"<p>I get this error when I load the google pre-trained word2vec to train doc2vec model with my own data. Here is part of my code:</p>

<pre><code>model_dm=doc2vec.Doc2Vec(dm=1,dbow_words=1,vector_size=400,window=8,workers=4)
model_dm.build_vocab(document)
model_dm.intersect_word2vec_format('home/xxw/Downloads/GoogleNews-vectors-negative300.bin',binary=True)
model_dm.train(document)
</code></pre>

<p>But I got this error:</p>

<blockquote>
  <p>'Doc2Vec' object has no attribute 'intersect_word2vec_format'</p>
</blockquote>

<p>Can you help me with the error? I get the google model from <a href=""https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz</a>, and my gensim is the latest version I think.</p>
","word2vec, gensim, doc2vec","<p>A recent refactor made <code>Doc2Vec</code> no longer share a superclass with this method. You might be able to call the method on your <code>model_dm.wv</code> object instead, but I'm not sure. Otherwise you could look at the source and mimic the code to achieve the same effect, if you really need that step.</p>

<p>But note that <code>Doc2Vec</code> doesn't need word-vectors as input: it can learn everything it needs from your own training data. Whether word-vectors from elsewhere will help will depend on a lot of factors – and the larger your own data is, or the more unique, the less preloaded vectors from elsewhere are likely to help, or even have any residual effect when your own training is done. </p>

<p>Other notes on your apparent setup:</p>

<ul>
<li><p><code>dbow_words=1</code> will have no effect in <code>dm=1</code> mode - that mode already inherently trains word-vectors. (It only has effect in <code>dm=0</code> DBOW mode, where it adds extra interleaved word-training, if you need word-vectors. Often plain DBOW, without word-vector training, is a fast and effective option.)</p></li>
<li><p>Recent versions of gensim require more arguments to train, and note that typical published work with this algorithm use 10-20 (or sometimes more) passes over the data (as can be specified to <code>train()</code> via the <code>epochs</code> argument), rather than the default (in some versions of gensim) of 5. </p></li>
</ul>
",2,2,7703,2018-05-08 15:28:24,https://stackoverflow.com/questions/50237247/gensim-doc2vec-object-has-no-attribute-intersect-word2vec-format-when-i-loa
Document similarity in production environment,"<p>We are having n number of documents. Upon submission of new document by user, our goal is to inform him about possible duplication of existing document (just like stackoverflow suggests questions may already have answer).</p>

<p>In our system, new document is uploaded every minute and mostly about the same topic (where there are more chance of duplication).</p>

<p>Our current implementation includes gensim doc2vec model trained on documents (tagged with unique document ids). We infer vector for new document and find most_similar docs (ids) with it. Reason behind choosing doc2vec model is that we wanted to take advantage of semantics to improve results. As far as we know, it does not support online training, so we might have to schedule a cron or something that periodically updates the model. But scheduling cron will be disadvantageous as documents come in a burst. User may upload duplicates while model is not yet trained for new data. Also given huge amount of data, training time will be higher.</p>

<p>So i would like to know how such cases are handled in big companies. Are there any better alternative? or better algorithm for such problem?</p>
","python, machine-learning, nlp, gensim, doc2vec","<p>You don't have to take the old model down to start training a new model, so despite any training lags, or new-document bursts, you'll always have a live model doing the best it can.</p>

<p>Depending on how much the document space changes over time, you might find retraining to have a negligible benefit. (One good model, built on a large historical record, might remain fine for inferring new vectors indefinitely.)</p>

<p>Note that tuning inference to use more <code>steps</code> (especially for short documents), or a lower starting <code>alpha</code> (more like the training default of 0.025) may give better results. </p>

<p>If word-vectors are available, there is also the ""Word Mover's Distance""  (WMD) calculation of document similarity, which might be ever better at identifying close duplicates. Note, though, it can be quite expensive to calculate – you might want to do it only against a subset of likely candidates, or have to add many parallel processors, to do it in bulk. There's another newer distance metric called 'soft cosine similarity' (available in recent gensim) that's somewhere between simple vector-to-vector cosine-similarity and full WMD in its complexity, that may be worth trying. </p>

<p>To the extent the vocabulary hasn't expanded, you can load an old <code>Doc2Vec</code> model, and continue to <code>train()</code> it – and starting from an already working model may help you achieve similar results with fewer passes. But note: it currently doesn't support learning any new words, and the safest practice is to re-train with a mix of <em>all</em> known examples interleaved. (If you only train on incremental new examples, the model may lose a balanced understanding of the older documents that aren't re-presented.)</p>

<p>(If you chief concern is documents that duplicate exact runs-of-words, rather than just similar fuzzy topics, you might look at mixing-in other techniques, such as breaking a document into a bag-of-character-ngrams, or 'shingleprinting' as in common in plagiarism-detection applications.)</p>
",2,3,460,2018-05-10 01:53:40,https://stackoverflow.com/questions/50264369/document-similarity-in-production-environment
Difference between most_similar and similar_by_vector in gensim word2vec?,"<p>I was confused with the results of most_similar and similar_by_vector from gensim's Word2vecKeyedVectors. They are supposed to calculate cosine similarities in the same way - however:</p>

<p>Running them with one word gives identical results, for example:
model.most_similar(['obama']) and similar_by_vector(model['obama'])</p>

<p>but if I give it an equation:</p>

<pre><code>model.most_similar(positive=['king', 'woman'], negative=['man'])
</code></pre>

<p>gives:</p>

<pre><code>[('queen', 0.7515910863876343), ('monarch', 0.6741327047348022), ('princess', 0.6713887453079224), ('kings', 0.6698989868164062), ('kingdom', 0.5971318483352661), ('royal', 0.5921063423156738), ('uncrowned', 0.5911505818367004), ('prince', 0.5909028053283691), ('lady', 0.5904011130332947), ('monarchs', 0.5884358286857605)]
</code></pre>

<p>while with:</p>

<pre><code>q = model['king'] - model['man'] + model['woman']
model.similar_by_vector(q)
</code></pre>

<p>gives:</p>

<pre><code>[('king', 0.8655095100402832), ('queen', 0.7673765420913696), ('monarch', 0.695580005645752), ('kings', 0.6929547786712646), ('princess', 0.6909604668617249), ('woman', 0.6528975963592529), ('lady', 0.6286187767982483), ('prince', 0.6222133636474609), ('kingdom', 0.6208546161651611), ('royal', 0.6090123653411865)]
</code></pre>

<p>There is a noticable difference in cosine distance of the words queen, monarch... etc. I'm wondering why?</p>

<p>Thanks!</p>
","nlp, word2vec, gensim","<p>The <code>most_similar</code> similar function retrieves the vectors corresponding to <code>""king""</code>, <code>""woman""</code> and <code>""man""</code>, and <em>normalizes</em> them before computing <code>king - man + woman</code> (<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#L426"" rel=""noreferrer"">source code</a>: <code>use_norm=True</code>).</p>

<p>The function call <code>model.similar_by_vector(v)</code> just calls <code>model.most_similar(positive=[v])</code>. So the difference is due to <code>most_similar</code> having a behaviour depending on the type of input (string or vector).</p>

<p>Finally, when <code>most_similar</code> has string inputs, it removes the words from the output (that is why ""king"" does not appear in the results).</p>

<p>A piece of code to see the differences:</p>

<pre><code>&gt;&gt;&gt; un = False
&gt;&gt;&gt; v = model.word_vec(""king"", use_norm=un) + model.word_vec(""woman"", use_norm=un) - model.word_vec(""man"", use_norm=un)
&gt;&gt;&gt; un = True
&gt;&gt;&gt; v2 = model.word_vec(""king"", use_norm=un) + model.word_vec(""woman"", use_norm=un) - model.word_vec(""man"", use_norm=un)
&gt;&gt;&gt; model.most_similar(positive=[v], topn=6)
[('king', 0.8449392318725586), ('queen', 0.7300517559051514), ('monarch', 0.6454660892486572), ('princess', 0.6156251430511475), ('crown_prince', 0.5818676948547363), ('prince', 0.5777117609977722)]
&gt;&gt;&gt; model.most_similar(positive=[v2], topn=6)
[('king', 0.7992597222328186), ('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133)]
&gt;&gt;&gt; model.most_similar(positive=[""king"", ""woman""], negative=[""man""], topn=6)
[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044)]
</code></pre>
",11,9,10927,2018-05-10 14:44:57,https://stackoverflow.com/questions/50275623/difference-between-most-similar-and-similar-by-vector-in-gensim-word2vec
Pipeline and GridSearch for Doc2Vec,"<p>I currently have following script that helps to find the best model for a doc2vec model. It works like this: First train a few models based on given parameters and then test against a classifier. Finally, it outputs the best model and classifier (I hope).</p>

<p><strong>Data</strong></p>

<p>Example data (data.csv) can be downloaded here: <a href=""https://pastebin.com/takYp6T8"" rel=""noreferrer"">https://pastebin.com/takYp6T8</a>
Note that the data has a structure that should make an ideal classifier with 1.0 accuracy.</p>

<p><strong>Script</strong></p>

<pre><code>import sys
import os
from time import time
from operator import itemgetter
import pickle
import pandas as pd
import numpy as np
from argparse import ArgumentParser

from gensim.models.doc2vec import Doc2Vec
from gensim.models import Doc2Vec
import gensim.models.doc2vec
from gensim.models import KeyedVectors
from gensim.models.doc2vec import TaggedDocument, Doc2Vec

from sklearn.base import BaseEstimator
from gensim import corpora

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


dataset = pd.read_csv(""data.csv"")

class Doc2VecModel(BaseEstimator):

    def __init__(self, dm=1, size=1, window=1):
        self.d2v_model = None
        self.size = size
        self.window = window
        self.dm = dm

    def fit(self, raw_documents, y=None):
        # Initialize model
        self.d2v_model = Doc2Vec(size=self.size, window=self.window, dm=self.dm, iter=5, alpha=0.025, min_alpha=0.001)
        # Tag docs
        tagged_documents = []
        for index, row in raw_documents.iteritems():
            tag = '{}_{}'.format(""type"", index)
            tokens = row.split()
            tagged_documents.append(TaggedDocument(words=tokens, tags=[tag]))
        # Build vocabulary
        self.d2v_model.build_vocab(tagged_documents)
        # Train model
        self.d2v_model.train(tagged_documents, total_examples=len(tagged_documents), epochs=self.d2v_model.iter)
        return self

    def transform(self, raw_documents):
        X = []
        for index, row in raw_documents.iteritems():
            X.append(self.d2v_model.infer_vector(row))
        X = pd.DataFrame(X, index=raw_documents.index)
        return X

    def fit_transform(self, raw_documents, y=None):
        self.fit(raw_documents)
        return self.transform(raw_documents)


param_grid = {'doc2vec__window': [2, 3],
              'doc2vec__dm': [0,1],
              'doc2vec__size': [100,200],
              'logreg__C': [0.1, 1],
}

pipe_log = Pipeline([('doc2vec', Doc2VecModel()), ('log', LogisticRegression())])

log_grid = GridSearchCV(pipe_log, 
                        param_grid=param_grid,
                        scoring=""accuracy"",
                        verbose=3,
                        n_jobs=1)

fitted = log_grid.fit(dataset[""posts""], dataset[""type""])

# Best parameters
print(""Best Parameters: {}\n"".format(log_grid.best_params_))
print(""Best accuracy: {}\n"".format(log_grid.best_score_))
print(""Finished."")
</code></pre>

<p>I do have following questions regarding my script (I combine them here to avoid three posts with the same code snippet):</p>

<ol>
<li>What's the purpose of <code>def __init__(self, dm=1, size=1, window=1):</code>? Can I possibly remove this part, somehow (tried unsuccessfully)?</li>
<li>How can I add a <code>RandomForest</code> classifier (or others) to the GridSearch workflow/pipeline?</li>
<li>How could a train/test data split added to the code above, as the current script only trains on the full dataset?</li>
</ol>
","scikit-learn, pipeline, gensim, grid-search","<p>1) <code>init()</code> lets you define the parameters you would like your class to take at initialization (equivalent to contructor in java). </p>

<p>Please look at these questions for more details:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/625083/python-init-and-self-what-do-they-do?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa"">Python __init__ and self what do they do?</a></li>
<li><a href=""https://stackoverflow.com/questions/8985806/python-constructors-and-init/8986413?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa"">Python constructors and __init__</a></li>
</ul>

<p>2) Why do you want to add the <code>RandomForestClassifier</code> and what will be its input?  </p>

<p>Looking at your other two questions, do you want to compare the output of RandomForestClassifier with <code>LogisticRegression</code> here? If so, you are doing good in <a href=""https://stackoverflow.com/questions/50272416/gridsearch-on-model-and-classifiers"">this question of yours</a>.</p>

<p>3) You have imported the <code>train_test_split</code>, just use it.</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(dataset[""posts""], dataset[""type""])

fitted = log_grid.fit(X_train, y_train)
</code></pre>
",2,7,2781,2018-05-10 17:52:21,https://stackoverflow.com/questions/50278744/pipeline-and-gridsearch-for-doc2vec
"Gensim: &quot;C extension not loaded, training will be slow.&quot;","<p>I am running gensim on Linux Suse. I can start my python program but on startup I get: </p>

<blockquote>
  <p>C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.</p>
</blockquote>

<p>GCC is installed. Does anyone know what I have to do?</p>
","pip, word2vec, gensim, opensuse, suse","<p>Try the following:</p>

<p><strong>Python 3.x</strong></p>

<pre><code>$ pip3 uninstall gensim    
$ apt-get install python3-dev build-essential      
$ pip3 install --upgrade gensim
</code></pre>

<p><strong>Python 2.x</strong></p>

<pre><code>$ pip uninstall gensim    
$ apt-get install python-dev build-essential      
$ pip install --upgrade gensim
</code></pre>
",4,3,7439,2018-05-12 13:22:37,https://stackoverflow.com/questions/50306710/gensim-c-extension-not-loaded-training-will-be-slow
AttributeError: &#39;LdaModel&#39; object has no attribute &#39;minimum_phi_value&#39;,"<p>As I was just experimenting with NLP then I was working on sarcasm detection but in meanwhile I had put this code. </p>

<p><strong>sarcasmextractor.py</strong></p>

<pre><code># coding: utf-8

# Importing the library

# In[2]:

import io
import sys
import os
import numpy as np
import pandas as pd
import nltk
import gensim
import csv, collections
from textblob import TextBlob
from sklearn.utils import shuffle
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
from sklearn.feature_extraction import DictVectorizer
import pickle
import replace_emoji


# Define a class to load the SentimentWordnet and write methods to calculate the scores

# In[4]:

class load_senti_word_net(object):
    """"""
    constructor to load the file and read the file as CSV
    6 columns - pos, ID, PosScore, NegScore, synsetTerms, gloss
    synsetTerms can have multiple similar words like abducting#1 abducent#1 and will read each one and calculaye the scores
    """"""

    def __init__(self):
        sent_scores = collections.defaultdict(list)
        with io.open(""SentiWordNet_3.0.0_20130122.txt"") as fname:
            file_content = csv.reader(fname, delimiter='\t',quotechar='""')

            for line in file_content:                
                if line[0].startswith('#') :
                    continue                    
                pos, ID, PosScore, NegScore, synsetTerms, gloss = line
                for terms in synsetTerms.split("" ""):
                    term = terms.split(""#"")[0]
                    term = term.replace(""-"","""").replace(""_"","""")
                    key = ""%s/%s""%(pos,term.split(""#"")[0])
                    try:
                        sent_scores[key].append((float(PosScore),float(NegScore)))
                    except:
                        sent_scores[key].append((0,0))

        for key, value in sent_scores.items():
            sent_scores[key] = np.mean(value,axis=0)

        self.sent_scores = sent_scores    

    """"""
    For a word,
    nltk.pos_tag([""Suraj""])
    [('Suraj', 'NN')]
    """"""

    def score_word(self, word):
        pos = nltk.pos_tag([word])[0][1]
        return self.score(word, pos)

    def score(self,word, pos):
        """"""
        Identify the type of POS, get the score from the senti_scores and return the score
        """"""

        if pos[0:2] == 'NN':
            pos_type = 'n'
        elif pos[0:2] == 'JJ':
            pos_type = 'a'
        elif pos[0:2] =='VB':
            pos_type='v'
        elif pos[0:2] =='RB':
            pos_type = 'r'
        else:
            pos_type =  0

        if pos_type != 0 :    
            loc = pos_type+'/'+word
            score = self.sent_scores[loc]
            if len(score)&gt;1:
                return score
            else:
                return np.array([0.0,0.0])
        else:
            return np.array([0.0,0.0])

    """"""
    Repeat the same for a sentence
    nltk.pos_tag(word_tokenize(""My name is Suraj""))
    [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Suraj', 'NNP')]    
    """"""    

    def score_sentencce(self, sentence):
        pos = nltk.pos_tag(sentence)
        print (pos)
        mean_score = np.array([0.0, 0.0])
        for i in range(len(pos)):
            mean_score += self.score(pos[i][0], pos[i][1])

        return mean_score

    def pos_vector(self, sentence):
        pos_tag = nltk.pos_tag(sentence)
        vector = np.zeros(4)

        for i in range(0, len(pos_tag)):
            pos = pos_tag[i][1]
            if pos[0:2]=='NN':
                vector[0] += 1
            elif pos[0:2] =='JJ':
                vector[1] += 1
            elif pos[0:2] =='VB':
                vector[2] += 1
            elif pos[0:2] == 'RB':
                vector[3] += 1

        return vector



# Now let's extract the features
# 
# ###Stemming and Lemmatization

# In[5]:

porter = nltk.PorterStemmer()
sentiments = load_senti_word_net()


# In[7]:

def gram_features(features,sentence):
    sentence_rep = replace_emoji.replace_reg(str(sentence))
    token = nltk.word_tokenize(sentence_rep)
    token = [porter.stem(i.lower()) for i in token]        

    bigrams = nltk.bigrams(token)
    bigrams = [tup[0] + ' ' + tup[1] for tup in bigrams]
    grams = token + bigrams
    #print (grams)
    for t in grams:
        features['contains(%s)'%t]=1.0



# In[8]:

import string
def sentiment_extract(features, sentence):
    sentence_rep = replace_emoji.replace_reg(sentence)
    token = nltk.word_tokenize(sentence_rep)    
    token = [porter.stem(i.lower()) for i in token]   
    mean_sentiment = sentiments.score_sentencce(token)
    features[""Positive Sentiment""] = mean_sentiment[0]
    features[""Negative Sentiment""] = mean_sentiment[1]
    features[""sentiment""] = mean_sentiment[0] - mean_sentiment[1]
    #print(mean_sentiment[0], mean_sentiment[1])

    try:
        text = TextBlob("" "".join([""""+i if i not in string.punctuation and not i.startswith(""'"") else i for i in token]).strip())
        features[""Blob Polarity""] = text.sentiment.polarity
        features[""Blob Subjectivity""] = text.sentiment.subjectivity
        #print (text.sentiment.polarity,text.sentiment.subjectivity )
    except:
        features[""Blob Polarity""] = 0
        features[""Blob Subjectivity""] = 0
        print(""do nothing"")


    first_half = token[0:int(len(token)/2)]    
    mean_sentiment_half = sentiments.score_sentencce(first_half)
    features[""positive Sentiment first half""] = mean_sentiment_half[0]
    features[""negative Sentiment first half""] = mean_sentiment_half[1]
    features[""first half sentiment""] = mean_sentiment_half[0]-mean_sentiment_half[1]
    try:
        text = TextBlob("" "".join([""""+i if i not in string.punctuation and not i.startswith(""'"") else i for i in first_half]).strip())
        features[""first half Blob Polarity""] = text.sentiment.polarity
        features[""first half Blob Subjectivity""] = text.sentiment.subjectivity
        #print (text.sentiment.polarity,text.sentiment.subjectivity )
    except:
        features[""first Blob Polarity""] = 0
        features[""first Blob Subjectivity""] = 0
        print(""do nothing"")

    second_half = token[int(len(token)/2):]
    mean_sentiment_sechalf = sentiments.score_sentencce(second_half)
    features[""positive Sentiment second half""] = mean_sentiment_sechalf[0]
    features[""negative Sentiment second half""] = mean_sentiment_sechalf[1]
    features[""second half sentiment""] = mean_sentiment_sechalf[0]-mean_sentiment_sechalf[1]
    try:
        text = TextBlob("" "".join([""""+i if i not in string.punctuation and not i.startswith(""'"") else i for i in second_half]).strip())
        features[""second half Blob Polarity""] = text.sentiment.polarity
        features[""second half Blob Subjectivity""] = text.sentiment.subjectivity
        #print (text.sentiment.polarity,text.sentiment.subjectivity )
    except:
        features[""second Blob Polarity""] = 0
        features[""second Blob Subjectivity""] = 0
        print(""do nothing"")  





# In[9]:

features = {}
sentiment_extract(features,""a long narrow opening"")


# In[11]:

def pos_features(features,sentence):
    sentence_rep = replace_emoji.replace_reg(sentence)
    token = nltk.word_tokenize(sentence_rep)
    token = [ porter.stem(each.lower()) for each in token]
    pos_vector = sentiments.pos_vector(token)
    for j in range(len(pos_vector)):
        features['POS_'+str(j+1)] = pos_vector[j]
    print (""done"")



# In[12]:

features = {}
pos_features(features,""a long narrow opening"")


# In[13]:

def capitalization(features,sentence):
    count = 0
    for i in range(len(sentence)):
        count += int(sentence[i].isupper())
    features['Capitalization'] = int(count &gt; 3)
    print (count)


# In[14]:

features = {}
capitalization(features,""A LoNg NArrow opening"")


# In[15]:

import topic
topic_mod = topic.topic(nbtopic=200,alpha='symmetric')


# In[16]:

topic_mod = topic.topic(model=os.path.join('topics.tp'),dicttp=os.path.join('topics_dict.tp'))


# In[17]:

def topic_feature(features,sentence,topic_modeler):    
    topics = topic_modeler.transform(sentence)    
    for j in range(len(topics)):
        features['Topic :'] = topics[j][1]



# In[18]:

topic_feature(features,""A LoNg NArrow opening"",topic_mod)


# In[19]:

def get_features(sentence, topic_modeler):
    features = {}
    gram_features(features,sentence)
    pos_features(features,sentence)
    sentiment_extract(features, sentence)
    capitalization(features,sentence)
    topic_feature(features, sentence,topic_modeler)
    return features


# In[20]:

df = pd.DataFrame()
df = pd.read_csv(""dataset_csv.csv"", header=0, sep='\t')
df.head()


# In[17]:

import re

for i in range(0,df.size):
    temp = str(df[""tweets""][i])
    temp = re.sub(r'[^\x00-\x7F]+','',temp)
    featureset.append((get_features(temp,topic_mod), df[""label""][i]))


# In[20]:

c = []
for i in range(0,len(featureset)):
    c.append(pd.DataFrame(featureset[i][0],index=[i]))

result = pd.concat(c)


# In[22]:

result.insert(loc=0,column=""label"",value='0')


# In[23]:

for i in range(0, len(featureset)):
    result[""label""].loc[i] = featureset[i][1]   



# In[25]:

result.to_csv('feature_dataset.csv')


# In[3]:

df = pd.DataFrame()
df = pd.read_csv(""feature_dataset.csv"", header=0)
df.head()


# In[4]:

get_ipython().magic('matplotlib inline')

import matplotlib as matplot 
import seaborn

result = df


# In[5]:

X = result.drop(['label','Unnamed: 0','Topic :'],axis=1).values


# In[6]:

Y = result['label']


# In[7]:

import pickle
import pefile
import sklearn.ensemble as ek
from sklearn import cross_validation, tree, linear_model
from sklearn.feature_selection import SelectFromModel
from sklearn.externals import joblib
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn import preprocessing
from sklearn import svm
from sklearn.linear_model import LinearRegression
import sklearn.linear_model as lm


# In[29]:

model = { ""DecisionTree"":tree.DecisionTreeClassifier(max_depth=10),
         ""RandomForest"":ek.RandomForestClassifier(n_estimators=50),
         ""Adaboost"":ek.AdaBoostClassifier(n_estimators=50),
         ""GradientBoosting"":ek.GradientBoostingClassifier(n_estimators=50),
         ""GNB"":GaussianNB(),
         ""Logistic Regression"":LinearRegression()   
}


# In[8]:

X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, Y ,test_size=0.2)


# In[9]:

X_train = pd.DataFrame(X_train)
X_train = X_train.fillna(X_train.mean())

X_test = pd.DataFrame(X_test)
X_test = X_test.fillna(X_test.mean())


# In[38]:

results_algo = {}
for algo in model:
    clf = model[algo]
    clf.fit(X_train,y_train.astype(int))
    score = clf.score(X_test,y_test.astype(int))
    print (""%s : %s "" %(algo, score))
    results_algo[algo] = score



# In[39]:

winner = max(results_algo, key=results_algo.get)


# In[40]:

clf = model[winner]
res = clf.predict(X_test)
mt = confusion_matrix(y_test, res)
print(""False positive rate : %f %%"" % ((mt[0][1] / float(sum(mt[0])))*100))
print('False negative rate : %f %%' % ( (mt[1][0] / float(sum(mt[1]))*100)))


# In[41]:

from sklearn import metrics
print (metrics.classification_report(y_test, res))


# In[34]:

test_data = ""public meetings are awkard for me as I can insult people but I choose not to and that is something that I find difficult to live with""


# In[101]:

test_data=""I purchased this product 4.47 billion years ago and when I opened it today, it was half empty.""


# In[82]:

test_data=""when people see me eating and ask me are you eating? No no I'm trying to choke myself to death #sarcastic""


# In[102]:

test_feature = []
test_feature.append((get_features(test_data,topic_mod)))


# In[104]:

test_feature


# In[105]:

c = []

c.append(pd.DataFrame(test_feature[0],index=[i]))

test_result = pd.concat(c)
test_result = test_result.drop(['Topic :'],axis=1).values


# In[106]:

res= clf.predict(test_result)
</code></pre>

<p>But it is giving me the following error:</p>

<pre><code>C:\ProgramData\Anaconda3\lib\site-packages\gensim\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
  warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")
[('a', 'DT'), ('long', 'JJ'), ('narrow', 'JJ'), ('open', 'JJ')]
[('a', 'DT'), ('long', 'JJ')]
[('narrow', 'JJ'), ('open', 'JJ')]
done
5
Traceback (most recent call last):
  File ""C:\shubhamprojectwork\sarcasm detection\SarcasmDetection-master\SarcasmDetection-master\Code\sarcasm-extraction.py"", line 276, in &lt;module&gt;
    topic_feature(features,""A LoNg NArrow opening"",topic_mod)
  File ""C:\shubhamprojectwork\sarcasm detection\SarcasmDetection-master\SarcasmDetection-master\Code\sarcasm-extraction.py"", line 268, in topic_feature
    topics = topic_modeler.transform(sentence)    
  File ""C:\shubhamprojectwork\sarcasm detection\SarcasmDetection-master\SarcasmDetection-master\Code\topic.py"", line 42, in transform
    return self.lda[corpus_sentence]     
  File ""C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\ldamodel.py"", line 1160, in __getitem__
    return self.get_document_topics(bow, eps, self.minimum_phi_value, self.per_word_topics)
AttributeError: 'LdaModel' object has no attribute 'minimum_phi_value'
</code></pre>

<p>Code for <strong>topic.py</strong>:</p>

<pre><code>from gensim import corpora, models, similarities
import nltk
from nltk.corpus import stopwords
import numpy as np
import pandas as pd
import replace_emoji

class topic(object):
    def __init__(self, nbtopic = 100, alpha=1,model=None,dicttp=None):
        self.nbtopic = nbtopic
        self.alpha = alpha
        self.porter = nltk.PorterStemmer()
        self.stop = stopwords.words('english')+['.','!','?','""','...','\\',""''"",'[',']','~',""'m"",""'s"",';',':','..','$']
        if model!=None and dicttp!=None:
            self.lda = models.ldamodel.LdaModel.load(model)
            self.dictionary =  corpora.Dictionary.load(dicttp)

    def fit(self,documents):

        documents_mod = documents
        tokens = [nltk.word_tokenize(sentence) for sentence in documents_mod]
        tokens = [[self.porter.stem(t.lower()) for t in sentence if t.lower() not in self.stop] for sentence in tokens]        

        self.dictionary = corpora.Dictionary(tokens)
        corpus = [self.dictionary.doc2bow(text) for text in tokens]
        self.lda = models.ldamodel.LdaModel(corpus,id2word=self.dictionary, num_topics=self.nbtopic,alpha=self.alpha)

        self.lda.save('topics.tp')
        self.dictionary.save('topics_dict.tp')

    def get_topic(self,topic_number):

        return self.lda.print_topic(topic_number)

    def transform(self,sentence):

        sentence_mod = sentence
        tokens = nltk.word_tokenize(sentence_mod)
        tokens = [self.porter.stem(t.lower()) for t in tokens if t.lower() not in self.stop] 
        corpus_sentence = self.dictionary.doc2bow(tokens)

        return self.lda[corpus_sentence]     
</code></pre>

<p>The overall code is found here <a href=""https://github.com/surajr/SarcasmDetection"" rel=""nofollow noreferrer"">overall code</a>.</p>
","python, tensorflow, nlp, gensim, topic-modeling","<p>The <code>minimum_phi_value</code> is a property of <code>LdaModel</code> that is set when an instance is created and for some reason it hasn't been serialized (which is pretty strange, probably a bug).</p>

<p>To workaround this particular issue you can add</p>

<pre><code>self.lda.minimum_phi_value = 0.01
</code></pre>

<p>... after <code>self.lda</code> loading or avoid saving/restoring the model if possible (i.e. always train it). </p>

<p>But I encourage you to examine the fields of <code>self.lda</code> before and after serialization to check they are identical.</p>
",1,1,6578,2018-05-14 08:38:13,https://stackoverflow.com/questions/50326147/attributeerror-ldamodel-object-has-no-attribute-minimum-phi-value
CSV Input in gensim LDA via corpora.csvcorpus,"<p>I wanna use the LDA in gensim for topic modeling over a few thousand documents.
Therefore I´m using a csv-File as Input in the format of a term-document-matrix.</p>

<p>Currently it occurs an error when running the following code:</p>

<pre><code>from gensim import corpora

import_path =""TDM.csv""

dictionary = corpora.csvcorpus(import_path, labels='true')
</code></pre>

<p>The error is the following:</p>

<pre><code>dictionary = corpora.csvcorpus(import_path, labels='true')

AttributeError: module 'gensim.corpora' has no attribute 'csvcorpus'
</code></pre>

<p>Am I using the module correctly and if so, where is my mistake?</p>

<p>Thanks in advance.</p>
","python-3.x, csv, gensim, lda, corpus","<p>This also bugged me for quite awhile.
It looks like csvcorpus is actually in the experimental stage as you can see in their github issue, <a href=""https://github.com/RaRe-Technologies/gensim/issues/1583"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/1583</a></p>

<p>I would recommend going by the old fashioned way of using the csv package to read your csv file instead.</p>

<p>Cheers.</p>
",1,0,701,2018-05-14 11:13:01,https://stackoverflow.com/questions/50328915/csv-input-in-gensim-lda-via-corpora-csvcorpus
gensim - Doc2Vec: Difference iter vs. epochs,"<p>When reading the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Doc2Vec documentation of gensim</a>, I get a bit confused about some options. For example, the constructor of Doc2Vec has a parameter <em>iter</em>:</p>

<blockquote>
  <p>iter (int) – Number of iterations (epochs) over the corpus.</p>
</blockquote>

<p>Why does the train method then also have a similar parameter called <em>epochs</em>?</p>

<blockquote>
  <p>epochs (int) – Number of iterations (epochs) over the corpus.</p>
</blockquote>

<p>What is the difference between both? There's one more paragraph on it in the docs:</p>

<blockquote>
  <p>To avoid common mistakes around the model’s ability to do multiple
  training passes itself, an explicit epochs argument MUST be provided.
  In the common and recommended case, where train() is only called once,
  the model’s cached iter value should be supplied as epochs value.</p>
</blockquote>

<p>But I do not really understand why the constructor needs a <em>iter</em> parameter and what exactly should be provided for it.</p>

<p><strong>EDIT</strong>:</p>

<p>I just saw that there is also the possibility to specify the corpus directly in the constructor rather than calling train() separately. So I think in this case, <em>iter</em> would be used and otherwise <em>epochs</em>. Is that correct?</p>

<p>If so, what is the difference between specifying the corpus in the constructor and calling train() manually? Why would one choose the one or other?</p>

<p><strong>EDIT 2</strong>:</p>

<p>Although not mentioned in the docs, <em>iter</em> is now depreciated as parameter of Doc2Vec. It was renamed to <em>epochs</em> to be consistent with the parameter of <em>train()</em>. Training seems to work with that, although I struggle with <a href=""https://stackoverflow.com/questions/50390455/gensim-doc2vec-memoryerror-when-training-on-english-wikipedia"">MemoryErrors</a>.</p>
","python, gensim, doc2vec","<p>The parameter in the constructor was originally called <code>iter</code>, and when doing everything via a single constructor call – supplying the corpus in the constructor – that value would just be used as the number of training passes. </p>

<p>When the parameters to <code>train()</code> were expanded and made mandatory to avoid common errors, the term <code>epochs</code> was chosen as more descriptive, and distinct from the <code>iter</code> value.</p>

<p>When you specify the corpus in the constructor, <code>build_vocab()</code> and <code>train()</code> will be called for you automatically, as part of the construction. For most simple uses, that's fine. </p>

<p>But, if you let those automatic calls happen, you lose the chance to time the steps separately, or to tamper with results of the vocabulary steps before starting training, or to call <code>train()</code> multiple times (which is usually a bad idea unless you're sure you know what you're doing). </p>
",2,4,1915,2018-05-17 11:41:46,https://stackoverflow.com/questions/50390582/gensim-doc2vec-difference-iter-vs-epochs
Gensim Doc2Vec: I&#39;m gettting different vectors from documents that are identical,"<p>I have the following code and I think I am getting the vectors in a wrong way, because for example the vectors of two documents that are 100% identical are not the same.</p>

<pre><code>def getDocs(corpusPath):
    """"""Function for processings documents as TaggedDocument""""""
    # Loop over all the files in corpus
    for file in glob.glob(os.path.join(corpusPath, '*.csv')):
        # getWords is a function that gets the words from the provided directory
        # os.path.basename(file) takes the filename from the complete path
        yield TaggedDocument(words=getWords(file), tags=[os.path.basename(file)])

def getModel(corpusPath, outputName):
    # Get documents words from path
    documents = getDocs(corpusPath)

    cores = multiprocessing.cpu_count()

    # Initialize the model
    model = models.doc2vec.Doc2Vec(vector_size=100, epochs=10, min_count=1, max_vocab_size=None, alpha=0.025, min_alpha=0.01, workers=cores)

    # Build Vocabulary
    model.build_vocab(documents)

    # Train the model
    model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)

    # Save the model as shown below
    model.save_word2vec_format(outputName, doctag_vec=True, word_vec=False, prefix="""")
</code></pre>

<p>And the output has to be like this:</p>

<pre><code>12571 100
134602.csv 0.00691074 0.157398 0.0921498 0.126362 0.158668 -0.0753151 -0.164655 0.0883756 0.0407546 0.15239 -0.0145177 0.061617 -0.0891562 -0.0417054 -0.0858589 0.00102948 0.0161595 2.13553e-05 -0.0668119 0.0450828 0.117537 -0.0729031 -0.0580456 -0.00258632 -0.104359 0.136366 -0.144994 -0.12065 -0.121757 0.0830929 -0.16462 -0.0151503 0.0399056 0.160027 -0.0787732 -0.00789994 -0.094897 0.00608254 -0.0661624 0.129721 0.163127 -0.0793746 -0.0964145 0.0606208 0.0875067 0.0161015 -0.132051 -0.0491245 -0.154828 0.133222 -0.0687664 0.120808 -0.111705 -0.053042 -0.0912231 -0.111089 0.0443708 -0.139493 0.0607425 -0.161168 0.0786498 0.150048 0.146688 -0.0837242 -0.0553738 -0.117545 0.0986267 -0.0923841 0.098877 -0.12193 -0.062616 -0.0845228 -0.0636123 0.0823107 -0.0826875 0.139011 -0.0923962 0.0288433 0.137355 0.121588 -0.145517 0.160373 0.0628389 -0.0764258 -0.107213 0.0421445 0.137447 -0.0658571 0.0424128 0.0672861 0.109817 -0.126953 -0.0453275 0.0834503 0.0974179 0.00825522 -0.165445 -0.0213084 -0.0292943 -0.162938
125202.csv 0.106642 0.167441 -0.0275412 0.130408 -0.107533 0.091452 0.0103496 -0.0214623 0.0873943 -0.0465384 -0.165227 -0.0540914 -0.00923723 0.175378 -0.051865 0.0107003 -0.179349 0.0683971 -0.159605 0.0644916 0.136338 0.111336 -0.0805002 0.00214934 -0.0490576 0.151279 -0.0397022 0.075442 -0.0278023 -0.0636982 0.174473 0.087985 -0.0714066 -0.0800442 -0.103995 -0.0228613 0.157171 -0.0678672 -0.161953 0.0839289 -0.155191 -0.00721683 0.0586751 -0.0474399 -0.122106 0.170611 0.157929 0.075531 -0.13505 0.093849 -0.119415 0.0386302 0.0139714 0.0756701 -0.0810199 -0.111754 0.112905 0.130293 -0.126257 -0.00654255 -0.0369909 -0.072449 0.0257127 0.0716955 0.103714 -0.0842208 -0.0534867 -0.095218 0.127797 -0.029322 0.161806 -0.177695 -0.0684089 0.0623551 0.06396 0.0828089 -0.0590939 0.0180832 -0.0591218 0.136139 -0.153984 0.108085 -0.127018 -0.0847872 -0.167081 0.0199622 0.0209045 0.0320618 0.0591803 0.0809688 0.0799196 0.15632 -0.0519707 0.0270171 -0.163197 -0.0846849 -0.176135 -0.0120047 -0.0697305 0.014441
116200.csv -0.0182099 -0.130409 -0.138414 -0.0310527 -0.0274882 -0.0711805 -0.0628653 -0.144249 -0.166021 -0.0242265 -0.130593 -0.141916 0.0119525 0.0500143 -0.147568 -0.036778 0.110357 0.0439302 -0.132496 -0.105203 0.0356234 0.0982645 0.134903 -0.0648039 -0.0566216 0.138991 -0.0467151 -0.140643 0.139711 0.0943256 0.0576583 0.0644239 0.00136725 -0.0296913 0.0612566 0.148131 0.067239 0.100442 0.0665155 0.104861 -0.0498524 0.0995954 -0.115922 -0.00524584 0.0491675 0.159028 0.132554 0.0479373 0.141164 0.173129 0.022317 -0.000446397 0.0867293 -0.155649 -0.0675728 -0.0981307 -0.0806008 -0.0107237 -0.103454 -0.0753868 -0.0551634 0.170743 0.0495554 0.11536 -0.0294355 0.061617 0.126016 -0.04804 -0.0315217 -0.169522 -0.0892494 -0.025444 0.0672556 0.166157 0.0647261 0.0944827 -0.0792354 0.0182105 0.118192 0.000124603 -0.10565 -0.155033 0.107355 0.150469 -0.104327 -0.162604 -0.0218357 0.145972 -0.145784 -0.00176559 0.153054 -0.16377 -0.11736 0.0892985 -0.0212026 0.0511168 -0.146278 -0.0134697 -0.0540684 0.0791529
148597.csv -0.15473 0.0955252 0.0432369 -0.0945614 0.136283 -0.102851 0.0847211 -0.0396431 -0.0467567 0.17154 0.153097 0.0693114 0.163837 0.135897 0.146128 -0.167215 -0.152268 -0.11602 0.0282252 -0.0779752 -0.0829204 0.018318 0.00621094 0.0707405 0.0968831 0.00652018 -0.0568833 0.0916579 -0.0400151 -0.0391421 -0.0548217 -0.173926 -0.110223 -0.0317329 -0.02952 -0.129147 0.0698902 -0.154276 -0.157658 -0.14261 0.032107 -0.0385964 -0.0587693 0.0212146 0.143626 0.142041 -0.0530896 -0.133748 0.131452 0.13672 0.148338 0.160325 -0.113424 0.0678939 -0.0229337 -0.170486 -0.156904 0.0710402 0.00277802 0.120395 0.0360002 -0.0593753 0.155915 -0.0620641 -0.112055 0.0153659 0.147731 -0.0249911 0.0360584 -0.0402479 0.022273 0.00174414 -0.0178126 -0.116679 0.0191754 -0.0089874 0.083151 -0.168562 -0.160357 -0.0659622 0.0248376 0.045583 0.127733 -0.0675122 -0.0734585 0.113653 0.166756 0.0723445 0.0554671 -0.0751338 0.0481711 -0.00127609 0.0560728 0.124651 -0.0495638 0.0985305 -0.110315 0.0672438 0.096637 0.104245
166916.csv 0.168698 0.0629846 0.0248923 -0.105248 0.172408 -0.0322083 0.174124 -0.113572 -0.0104922 0.0429484 -0.0306917 0.022368 -0.0584265 0.0337984 -0.0225754 0.143456 -0.121288 -0.133673 0.0677091 0.0583681 0.0390327 -0.141176 0.0694527 -0.0290526 -0.129707 -0.0765447 0.071578 0.146411 -0.112526 0.103688 -0.110703 0.0781341 0.0318269 0.105218 0.0177797 0.123248 0.158062 0.0370042 -0.137394 0.0246147 0.00653834 0.166063 -0.100149 -0.0479191 -0.0702838 0.0690037 0.114349 -0.0274343 0.014801 -0.0421596 0.0694873 0.0662955 -0.12477 -0.0088994 0.104959 0.149459 0.16611 0.0265376 -0.134808 0.101123 0.0431258 0.0584757 -0.0315779 0.121671 -0.0380923 -0.0897689 -0.0237933 0.110452 -0.0039647 0.106183 -0.165717 -0.16557 0.136988 0.121843 0.0722612 -0.00844494 0.175932 -0.0751714 0.152611 -0.0646956 0.105122 -0.108245 0.0583691 0.113012 0.171521 -0.0258976 0.0851889 -0.0941529 0.153386 0.0455267 -0.0259182 -0.0437207 -0.150415 0.132313 -0.143572 -0.0281547 -0.00231613 -0.00760185 -0.147233 -0.167408
148291.csv 0.00976907 0.168438 -0.0919878 -0.164332 -0.138181 -0.149775 -0.0394723 0.027946 0.0662307 -0.00850593 0.12174 0.106023 -0.11512 0.0694538 0.128228 0.066019 0.0805346 0.00220964 -0.0465066 0.0923588 0.121286 0.168551 0.0462572 0.0221805 -0.119831 0.00797117 -0.00709804 -0.0222688 0.0938169 0.100695 0.133902 0.15964 0.0544278 -0.0504766 -0.0539783 -0.0158389 0.0280565 -0.10531 0.112356 -0.0349924 0.155673 0.0491142 0.171533 -0.044268 0.0560867 -0.135758 0.114202 -0.120608 0.0373457 -0.0847815 0.0285375 -0.0101114 0.0169282 -0.00141743 -0.028344 -0.00979434 -0.0599551 0.0554465 -0.0583942 -0.169627 0.167471 -0.00661054 0.114252 -0.00489984 0.167312 0.144928 0.0376684 -0.118885 0.0426739 0.169052 0.00265325 0.146609 0.163534 -0.100965 -0.101386 0.127619 0.148285 -0.0881821 -0.100448 -0.044064 0.106071 0.0239426 0.0733384 -0.0962991 0.0939341 0.0659483 0.122844 -0.140426 -0.0485195 0.0645185 0.037179 0.0963829 -0.109955 -0.151168 -0.0413991 -0.0556731 -0.173456 -0.167728 -0.128145 0.150923
...
</code></pre>

<p>Where the first word of each line is the name of each file, and what follows is the corresponding vector for that file. I need to save the vectors in this way to use an external software.</p>
","python, gensim, word-embedding, doc2vec","<p>The algorithm ('Paragraph Vector') behind <code>Doc2Vec</code> makes use of randomness during initialization and training. Training also never reaches a point where all adjustments stop – just a point where it's believed that further updates will have negligible net value. </p>

<p>So, identical texts won't achieve identical vectors – they're each being updated, alongside the model's internals, with each training cycle, against a slightly-different base model, with slightly different randomization choices. If you have enough data, good parameters, and are engaging in enough training, they should become <em>very close</em>. And, your downstream evaluations/uses should be tolerant of such small variances. </p>

<p>Similarly, two runs on the same corpus won't result in identical end-vectors unless extreme care is taken to force determinism – for example by limiting training to a single worker thread, so that OS thread scheduling unpredictability doesn't slightly change the order of training examples. So vectors should only be compared if they were co-trained together, in the same model – and again, downstream applications should be tolerant of slight jitter from run to run or example to example.</p>

<p>Other notes about your setup:</p>

<ul>
<li><p><code>min_count=1</code> is almost always a bad choice - words with single (or few) examples just add noise to the training, making resulting vectors worse.</p></li>
<li><p>stochastic gradient descent optimization typically ends after the learning-rate <code>alpha</code> has been smoothly reduced to a tiny, near-zero value (such as <code>0.0001</code>) – you're using a final alpha (<code>0.01</code>) that's a full 40% of the starting alpha.</p></li>
<li><p>you may also want to save your models using gensim's native <code>.save()</code>, because <code>.save_word2vec_format()</code> discards most model internals, and squashes the doc-vectors into the same namespace as any word-vectors. </p></li>
</ul>
",2,1,872,2018-05-18 10:00:20,https://stackoverflow.com/questions/50408740/gensim-doc2vec-im-gettting-different-vectors-from-documents-that-are-identical
Dynamic Topic Modeling with Gensim / which code?,"<p>I want to use Dynamic Topic Modeling by Blei et al. (<a href=""http://www.cs.columbia.edu/~blei/papers/BleiLafferty2006a.pdf"" rel=""noreferrer"">http://www.cs.columbia.edu/~blei/papers/BleiLafferty2006a.pdf</a>) for a large corpus of nearly 3800 patent documents.
Does anybody has experience in using the DTM in the gensim package?
I identified two models: </p>

<ol>
<li>models.ldaseqmodel – Dynamic Topic Modeling in Python <a href=""https://radimrehurek.com/gensim/models/ldaseqmodel.html"" rel=""noreferrer"">Link</a></li>
<li>models.wrappers.dtmmodel – Dynamic Topic Models (DTM) <a href=""https://radimrehurek.com/gensim/models/wrappers/dtmmodel.html"" rel=""noreferrer"">Link</a></li>
</ol>

<p>Which one did you use, of if you used both, which one is ""better""? In better words, which one did/do you prefer?</p>
","python-3.x, gensim, lda","<p>Both packages work fine, and are pretty much functionally identical. Which one you might want to use depends on your use case. There are small differences in the functions each model comes with, and small differences in the naming, which might be a little confusing, but for most DTM use cases, it does not matter very much which you pick.</p>

<p><strong>Are the model outputs identical?</strong></p>

<p>Not exactly. They are however very, very close to being identical (98%+) - I believe most of the differences come from slightly different handling of the probabilities in the generative process. So far, I've not yet come across a case where a difference in the sixth or seventh digit after the decimal point has any significant meaning. Interpreting the topics your models finds matters much more than one version finding a higher topic loading for some word by 0.00002</p>

<p>The big difference between the two models: <code>dtmmodel</code> is a python wrapper for the original C++ implementation from <a href=""https://github.com/blei-lab/dtm"" rel=""nofollow noreferrer"">blei-lab</a>, which means python will run the binaries, while <code>ldaseqmodel</code> is fully written in python.</p>

<p><strong>Why use dtmmodel?</strong></p>

<ul>
<li>the C++ code is faster than the python implementation</li>
<li>supports the Document Influence Model from <a href=""http://www.cs.columbia.edu/~blei/papers/GerrishBlei2010.pdf"" rel=""nofollow noreferrer"">Gerrish/Blei 2010</a> (potentially interesting for your research, see <a href=""http://proceedings.mlr.press/v28/shalit13.pdf"" rel=""nofollow noreferrer"">this paper</a> for an implementation.</li>
</ul>

<p><strong>Why use ldaseqmodel?</strong></p>

<ul>
<li>easier to install (simple <code>import</code> statement vs downloading binaries)</li>
<li>can use <code>sstats</code> from a pretrained LDA model - useful with <code>LdaMulticore</code></li>
<li>easier to understand the workings of the code</li>
</ul>

<p>I mostly use <code>ldaseqmodel</code> but thats for convenience. Native DIM support would be great to have, though.</p>

<p><strong>What should you do?</strong></p>

<p>Try each of them out, say, on a small sample set and see what the models return. 3800 documents isn't a <em>huge</em> corpus (assuming the patents aren't hundreds of pages each), and I assume that after preprocessing (removing stopwords, images and metadata) your dictionary won't be too large either (lots of standard phrases and legalese in patents, I'd assume). Pick the one that works best for you or has the capabilities you need.</p>

<p>Full analysis might take hours anyway, if you let your code run overnight there is little practical difference, after all, do you care if it finishes at 3am or 5am? If runtime is critical, I would assume the <code>dtmmodel</code> will be more useful.</p>

<p>For implementation examples, you might want to take a look at these notebooks: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb"" rel=""nofollow noreferrer"">ldaseqmodel</a> and <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/dtm_example.ipynb"" rel=""nofollow noreferrer"">dtmmodel</a></p>
",4,5,4605,2018-05-18 14:02:29,https://stackoverflow.com/questions/50413059/dynamic-topic-modeling-with-gensim-which-code
"In spacy, how to use your own word2vec model created in gensim?","<p>I have trained my own word2vec model in gensim and I am trying to load that model in spacy. First, I need to save it in my disk and then try to load an init-model in spacy but unable to figure out exactly how.</p>

<pre><code>gensimmodel
Out[252]:
&lt;gensim.models.word2vec.Word2Vec at 0x110b24b70&gt;

import spacy
spacy.load(gensimmodel)

OSError: [E050] Can't find model 'Word2Vec(vocab=250, size=1000, alpha=0.025)'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
</code></pre>
","model, word2vec, gensim, spacy","<p>Train and save your model in plain-text format:</p>

<pre><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec

path = get_tmpfile(""./data/word2vec.model"")

model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
model.wv.save_word2vec_format(""./data/word2vec.txt"")
</code></pre>

<p>Gzip the text file:</p>

<pre><code>gzip word2vec.txt
</code></pre>

<p>Which produces a <code>word2vec.txt.gz</code> file.</p>

<p>Run the following command:</p>

<pre><code>python -m spacy init-model en ./data/spacy.word2vec.model --vectors-loc word2vec.txt.gz
</code></pre>

<p>Load the vectors using:</p>

<pre><code>nlp = spacy.load('./data/spacy.word2vec.model/')
</code></pre>
",26,19,12324,2018-05-22 11:32:20,https://stackoverflow.com/questions/50466643/in-spacy-how-to-use-your-own-word2vec-model-created-in-gensim
Visualize Gensim Word2vec Embeddings in Tensorboard Projector,"<p>I've only seen a few questions that ask this, and none of them have an answer yet, so I thought I might as well try. I've been using gensim's word2vec model to create some vectors. I exported them into text, and tried importing it on tensorflow's live model of the embedding projector. One problem. <em>It didn't work</em>. It told me that the tensors were improperly formatted. So, being a beginner, I thought I would ask some people with more experience about possible solutions.<br>
Equivalent to my code:  </p>

<pre><code>import gensim
corpus = [[""words"",""in"",""sentence"",""one""],[""words"",""in"",""sentence"",""two""]]
model = gensim.models.Word2Vec(iter = 5,size = 64)
model.build_vocab(corpus)
# save memory
vectors = model.wv
del model
vectors.save_word2vec_format(""vect.txt"",binary = False)
</code></pre>

<p>That creates the model, saves the vectors, and then prints the results out nice and pretty in a tab delimited file with values for all of the dimensions. I understand how to do what I'm doing, I just can't figure out what's wrong with the way I put it in tensorflow, as the documentation regarding that is pretty scarce as far as I can tell.<br>
One idea that has been presented to me is implementing the appropriate tensorflow code, but I don’t know how to code that, just import files in the live demo.  </p>

<p>Edit: I have a new problem now. The object I have my vectors in is non-iterable because gensim apparently decided to make its own data structures that are non-compatible with what I'm trying to do.<br>
  Ok. Done with that too! Thanks for your help!</p>
","python, tensorflow, gensim, tensorboard, word-embedding","<p>What you are describing is possible. What you have to keep in mind is that Tensorboard reads from saved tensorflow binaries which represent your variables on disk.</p>
<blockquote>
<p>More information on saving and restoring tensorflow graph and variables <a href=""https://www.tensorflow.org/programmers_guide/saved_model"" rel=""noreferrer"">here</a></p>
</blockquote>
<p><strong>The main task is therefore to get the embeddings as saved tf variables.</strong></p>
<blockquote>
<p>Assumptions:</p>
<ul>
<li><p>in the following code <code>embeddings</code> is a python dict <code>{word:np.array (np.shape==[embedding_size])}</code></p>
</li>
<li><p>python version is 3.5+</p>
</li>
<li><p>used libraries are <code>numpy as np</code>, <code>tensorflow as tf</code></p>
</li>
<li><p>the directory to store the tf variables is <code>model_dir/</code></p>
</li>
</ul>
</blockquote>
<hr />
<h1>Step 1: Stack the embeddings to get a single <code>np.array</code></h1>
<pre><code>embeddings_vectors = np.stack(list(embeddings.values(), axis=0))
# shape [n_words, embedding_size]
</code></pre>
<hr />
<h1>Step 2: Save the <code>tf.Variable</code> on disk</h1>
<pre><code># Create some variables.
emb = tf.Variable(embeddings_vectors, name='word_embeddings')

# Add an op to initialize the variable.
init_op = tf.global_variables_initializer()

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, initialize the variables and save the
# variables to disk.
with tf.Session() as sess:
   sess.run(init_op)

# Save the variables to disk.
   save_path = saver.save(sess, &quot;model_dir/model.ckpt&quot;)
   print(&quot;Model saved in path: %s&quot; % save_path)
</code></pre>
<blockquote>
<p><code>model_dir</code> should contain files <code>checkpoint</code>, <code>model.ckpt-1.data-00000-of-00001</code>, <code>model.ckpt-1.index</code>, <code>model.ckpt-1.meta</code></p>
</blockquote>
<hr />
<h1>Step 3: Generate a <code>metadata.tsv</code></h1>
<p>To have a beautiful labeled cloud of embeddings, you can provide tensorboard with metadata as Tab-Separated Values (tsv) (<em>cf.</em> <a href=""https://www.tensorflow.org/versions/r1.0/get_started/embedding_viz#metadata"" rel=""noreferrer"">here</a>).</p>
<pre><code>words = '\n'.join(list(embeddings.keys()))

with open(os.path.join('model_dir', 'metadata.tsv'), 'w') as f:
   f.write(words)

# .tsv file written in model_dir/metadata.tsv
</code></pre>
<hr />
<h1>Step 4: Visualize</h1>
<p>Run <code>$ tensorboard --logdir model_dir</code> -&gt; <strong>Projector</strong>.</p>
<p>To load metadata, the magic happens here:</p>
<p><a href=""https://i.sstatic.net/N81kM.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/N81kM.png"" alt=""load_meta"" /></a></p>
<hr />
<p>As a reminder, some <em>word2vec</em> embedding projections are also available on <a href=""http://projector.tensorflow.org/"" rel=""noreferrer"">http://projector.tensorflow.org/</a></p>
",17,13,9268,2018-05-23 15:50:18,https://stackoverflow.com/questions/50492676/visualize-gensim-word2vec-embeddings-in-tensorboard-projector
Why I get different length of vectors using gensim LSI model?,"<p>I'm trying to cluster some descriptions using LSI. As the dataset that I have is too long, I'm clustering based on the vectors obtained from the models instead of using the similarity matrix, which requires too much memory, and if I pick a sample, the matrix generated doesn't correspond to a square (this precludes the use of MDS).</p>

<p>However, after running the model and looking for the vectors I'm getting different vector's lengths in the descriptions. Most of them have a length of 300 (the num_topics argument in the model), but some few, with the same description, present a length of 299.</p>

<p>Why is this happening? Is there a way to correct it?</p>

<pre><code>dictionary = gensim.corpora.Dictionary(totalvocab_lemmatized)
dictionary.compactify()

corpus = [dictionary.doc2bow(text) for text in totalvocab_lemmatized]

###tfidf model
tfidf = gensim.models.TfidfModel(corpus, normalize = True)
corpus_tfidf = tfidf[corpus]

###LSI model
lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)
vectors =[]
for n in lemmatized[:100]:
    vec_bow = dictionary.doc2bow(n)
    vec_lsi = lsi[vec_bow]
    print(len(vec_lsi))
</code></pre>
","python, nlp, gensim","<p>Explicit zeros are omitted, which is why some vectors appear shorter. 
Source: <a href=""https://github.com/RaRe-Technologies/gensim/issues/2501"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/2501</a></p>
",0,1,456,2018-05-25 03:57:03,https://stackoverflow.com/questions/50521304/why-i-get-different-length-of-vectors-using-gensim-lsi-model
Gensim doc2vec most_similar equivalent to get full documents,"<p>In Gensim's doc2vec implementation, <code>gensim.models.keyedvectors.Doc2VecKeyedVectors.most_similar</code> returns the tags and cosine similarity of the documents most similar to the query document. What if I want the actual documents themselves and not the tags? Is there a way to do that directly without searching for the document associated with the tag returned by <code>most_similar</code>?</p>

<p>Also, is there documentation on this? I can't seem to find the documentation for half of Gensim's classes.</p>
","python-3.x, nlp, text-mining, gensim, doc2vec","<p>The <code>Doc2Vec</code> class doesn't serve as a full document database that stores the original documents in their original formats. That would require a lot of extra complexity and state. </p>

<p>Instead, you just present the docs, with their particular tags, in the tokenized format it needs for training, and the model only learns and retains their vector representations. </p>

<p>If you need to then look-up the original documents, you must maintain your own (tags -> documents) lookup – which many projects will already have as the original source of the docs. </p>

<p>The <code>Doc2Vec</code> class docs are at <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a> but it may also be helpful to look at the example Jupyter notebooks included in the <code>gensim</code> <code>docs/notebooks</code> directory but also viewable online at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks</a></p>

<p>The three notebooks related to <code>Doc2Vec</code> have filenames beginning <code>doc2vec-</code>. </p>
",2,2,1156,2018-05-25 13:50:44,https://stackoverflow.com/questions/50530747/gensim-doc2vec-most-similar-equivalent-to-get-full-documents
Gensim DOC2VEC trims and delete the vocabulary,"<p>I tried creating a simple Doc2Vec model:</p>

<pre><code> sentences = []
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'rosse', u'con', u'tacco'], tags=[1]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'blu'], tags=[2]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarponcini', u'Emporio', u'Armani'], tags=[3]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'marca', u'italiana'], tags=[4]))
 sentences.append(doc2vec.TaggedDocument(words=[u'scarpe', u'bianche', u'senza', u'tacco'], tags=[5]))

 model = Doc2Vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate
 model.build_vocab(sentences)  
</code></pre>

<p>But I end up with an empty vocabulary. With some debugging I saw that inside the build_vocab() function a dictionary is actually created by the vocabulary.scan_vocab() function, but it's being deleted by the following vocabulary.prepare_vocab() function. More deeply, this is the function that causes the problem:</p>

<pre><code>def keep_vocab_item(word, count, min_count, trim_rule=None):
    """"""Check that should we keep `word` in vocab or remove.

    Parameters
    ----------
    word : str
        Input word.
    count : int
        Number of times that word contains in corpus.
    min_count : int
        Frequency threshold for `word`.
    trim_rule : function, optional
        Function for trimming entities from vocab, default behaviour is `vocab[w] &lt;= min_reduce`.

    Returns
    -------
    bool
        True if `word` should stay, False otherwise.

    """"""
    default_res = count &gt;= min_count

    if trim_rule is None:
        return default_res # &lt;-- ALWAYS RETURNS FALSE
    else:
        rule_res = trim_rule(word, count, min_count)
        if rule_res == RULE_KEEP:
            return True
        elif rule_res == RULE_DISCARD:
            return False
        else:
            return default_res  
</code></pre>

<p>Does somebody understand the problem?</p>
","python, gensim, doc2vec, vocabulary","<p>I found the answer myself, the default value for min_count is 5 and I had no words with a counter of 5 or more.
I just had to change this line of code:</p>

<pre><code>model = Doc2Vec(min_count=0, alpha=0.025, min_alpha=0.025)
</code></pre>
",2,0,689,2018-05-28 14:59:16,https://stackoverflow.com/questions/50569110/gensim-doc2vec-trims-and-delete-the-vocabulary
UnicodeDecodeError error when loading word2vec,"<p><strong>Full Description</strong></p>

<p>I am starting to work with word embedding and found a great amount of information about it. I understand, this far, that I can train my own word vectors or use previously trained ones, such as Google's or Wikipedia's, which are available for the English language and aren't useful to me, since I am working with texts in <em>Brazilian Portuguese</em>. Therefore, I went on a hunt for pre-trained word vectors in Portuguese and I ended up finding <a href=""http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/"" rel=""nofollow noreferrer"">Hirosan's List of Pretrained Word Embeddings</a> which led me to Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a> from which I learned about Rami Al-Rfou's <a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a>. After downloading both, I unsuccessfully have been trying to simply load the word vectors.</p>

<p><strong>Short Description</strong></p>

<p>I can't load pre-trained word vectors; I am trying <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a> and <a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a>.</p>

<p><strong>Downloads</strong></p>

<ul>
<li><a href=""https://drive.google.com/open?id=0B0ZXk88koS2KRDcwcV9IVWFTeUE"" rel=""nofollow noreferrer"">Kyubyong's pre-trained word2vector format word vectors for Portuguese</a>;</li>
<li><a href=""https://doc-0g-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/c1ch6rdnp89glqmi8g81ev2somslu7cs/1527537600000/10341224892851088318/*/0B5lWReQPSvmGNEh0VTdmSHlHZ1k?e=download"" rel=""nofollow noreferrer"">Polyglot's pre-trained word vectors for Portuguese</a>;</li>
</ul>

<p><strong>Loading attempts</strong></p>

<p><em>Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a></em>
First attempt: using Gensim as suggested by <a href=""http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/"" rel=""nofollow noreferrer"">Hirosan</a>;</p>

<pre><code>from gensim.models import KeyedVectors
kyu_path = '.../pre-trained_word_vectors/kyubyong_pt/pt.bin'
word_vectors = KeyedVectors.load_word2vec_format(kyu_path, binary=True)
</code></pre>

<p>And the error returned:</p>

<pre><code>[...]
File ""/Users/luisflavio/anaconda3/lib/python3.6/site-packages/gensim/utils.py"", line 359, in any2unicode
return unicode(text, encoding, errors=errors)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>The zip downloaded also contains other files but all of them return similar errors.</p>

<p><em><a href=""https://sites.google.com/site/rmyeid/projects/polyglot"" rel=""nofollow noreferrer"">Polyglot</a></em>
First attempt: following <a href=""http://nbviewer.jupyter.org/gist/aboSamoor/6046170"" rel=""nofollow noreferrer"">Al-Rfous's instructions</a>;</p>

<pre><code>import pickle
import numpy
pol_path = '.../pre-trained_word_vectors/polyglot/polyglot-pt.pkl'
words, embeddings = pickle.load(open(pol_path, 'rb'))
</code></pre>

<p>And the error returned:</p>

<pre><code>File ""/Users/luisflavio/Desktop/Python/w2v_loading_tries.py"", line 14, in &lt;module&gt;
    words, embeddings = pickle.load(open(polyglot_path, ""rb""))

UnicodeDecodeError: 'ascii' codec can't decode byte 0xd4 in position 1: ordinal not in range(128)
</code></pre>

<p>Second attempt: using <a href=""https://polyglot.readthedocs.io/en/latest/Embeddings.html"" rel=""nofollow noreferrer"">Polyglot's word embedding load function</a>;</p>

<p>First, we have to install polyglot via pip:</p>

<pre><code>pip install polyglot
</code></pre>

<p>Now we can import it:</p>

<pre><code>from polyglot.mapping import Embedding
pol_path = '.../pre-trained_word_vectors/polyglot/polyglot-pt.pkl'
embeddings = Embedding.load(polyglot_path)
</code></pre>

<p>And the error returned:</p>

<pre><code>File ""/Users/luisflavio/anaconda3/lib/python3.6/codecs.py"", line 321, in decode
(result, consumed) = self._buffer_decode(data, self.errors, final)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p><strong>Extra Information</strong></p>

<p>I am using python 3 on MacOS High Sierra.</p>

<p><strong>Solutions</strong></p>

<p><em>Kyubyong's <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">WordVectors</a></em>
As pointed out by <a href=""https://stackoverflow.com/a/50579950?noredirect=1"">Aneesh Joshi</a>, the correct way to load Kyubyong's model is by calling the native load function of Word2Vec.</p>

<pre><code>from gensim.models import Word2Vec
kyu_path = '.../pre-trained_word_vectors/kyubyong_pt/pt.bin'
model = Word2Vec.load(kyu_path)
</code></pre>

<p>Even though I am more than grateful for Aneesh Joshi solution, polyglot seems to be a better model for working with Portuguese. Any ideas about that one?</p>
","python, word2vec, gensim, python-unicode, polyglot","<p>For Kyubyong's pre-trained word2vector .bin file:
it may have been saved using gensim's save function.</p>

<p>""load the model with <code>load()</code>. Not <code>load_word2vec_format</code> (that's for the C-tool compatibility).""</p>

<p>i.e., <code>model = Word2Vec.load(fname)</code></p>

<p>Let me know if that works.</p>

<p>Reference : <a href=""https://groups.google.com/forum/#!msg/gensim/gZ0BRnETMnw/m4WnXSyZFdcJ"" rel=""nofollow noreferrer"">Gensim mailing list</a></p>
",4,4,3791,2018-05-28 20:25:36,https://stackoverflow.com/questions/50573054/unicodedecodeerror-error-when-loading-word2vec
What is the default smartirs for gensim TfidfModel?,"<p>Using <code>gensim</code>:</p>

<pre><code>from gensim.models import TfidfModel
from gensim.corpora import Dictionary

sent0 = ""The quick brown fox jumps over the lazy brown dog ."".lower().split()
sent1 = ""Mr brown jumps over the lazy fox ."".lower().split()

dataset = [sent0, sent1]
vocab = Dictionary(dataset)
corpus = [vocab.doc2bow(sent) for sent in dataset] 
model = TfidfModel(corpus)

# To retrieve the same pd.DataFrame format.
documents_tfidf_lol = [{vocab[word_idx]:tfidf_value for word_idx, tfidf_value in sent} for sent in model[corpus]]
documents_tfidf = pd.DataFrame(documents_tfidf_lol)
documents_tfidf.fillna(0, inplace=True)

documents_tfidf
</code></pre>

<p>[out]:</p>

<pre><code>    dog mr  quick
0   0.707107    0.0 0.707107
1   0.000000    1.0 0.000000
</code></pre>

<p>If we do the TF-IDF computation manually, </p>

<pre><code>sent0 = ""The quick brown fox jumps over the lazy brown dog ."".lower().split()
sent1 = ""Mr brown jumps over the lazy fox ."".lower().split()

documents = pd.DataFrame.from_dict(list(map(Counter, [sent0, sent1])))
documents.fillna(0, inplace=True, downcast='infer')
documents = documents.apply(lambda x: x/sum(x))  # Normalize the TF.
documents.head()

# To compute the IDF for all words.
num_sentences, num_words = documents.shape

idf_vector = [] # Lets save an ordered list of IDFS w.r.t. order of the column names.

for word in documents:
  word_idf = math.log(num_sentences/len(documents[word].nonzero()[0]))
  idf_vector.append(word_idf)

# Compute the TF-IDF table.
documents_tfidf = pd.DataFrame(documents.as_matrix() * np.array(idf_vector), 
                               columns=list(documents))
documents_tfidf
</code></pre>

<p>[out]:</p>

<pre><code>    .   brown   dog fox jumps   lazy    mr  over    quick   the
0   0.0 0.0 0.693147    0.0 0.0 0.0 0.000000    0.0 0.693147    0.0
1   0.0 0.0 0.000000    0.0 0.0 0.0 0.693147    0.0 0.000000    0.0
</code></pre>

<p>If we use <code>math.log2</code> instead of <code>math.log</code>:</p>

<pre><code>    .   brown   dog fox jumps   lazy    mr  over    quick   the
0   0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
1   0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
</code></pre>

<p>It looks like <code>gensim</code>:</p>

<ul>
<li>remove the non-salient words from the TF-IDF model, it's evident when we <code>print(model[corpus])</code></li>
<li>maybe the log base seem to be different from the log_2</li>
<li>maybe there's some normalization going on. </li>
</ul>

<p>Looking at <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel</a> , the <code>smart</code> scheme difference would have output different values but it's not clear in the docs what is the default value.</p>

<p><strong>What is the default smartirs for gensim TfidfModel?</strong></p>

<p><strong>What are the other default parameters that've caused the difference between a natively implemented TF-IDF and gensim's?</strong></p>
","python, nlp, gensim, information-retrieval, tf-idf","<p>The default value of <code>smartirs</code> is None, but if you follow the code, it is equal to <strong>ntc</strong>. </p>

<p><em>But how?</em></p>

<p>First, when you call <code>model = TfidfModel(corpus)</code>, it calculates IDF of the corpus with a function called <code>wglobal</code> which explained in docs as:</p>

<p><code>wglobal</code> is function for global weighting, the default value is <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.df2idf"" rel=""noreferrer""><code>df2idf()</code></a>. <code>df2idf</code> is a function that computes IDF for a term with the given document frequency. The default arguman and formula for <code>df2idf</code> is:</p>

<pre><code>df2idf(docfreq, totaldocs, log_base=2.0, add=0.0)
</code></pre>

<p><a href=""https://i.sstatic.net/6EjBv.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/6EjBv.png"" alt=""df2idf formula""></a></p>

<p>which implemented as:</p>

<pre><code>idfs = add + np.log(float(totaldocs) / docfreq) / np.log(log_base)
</code></pre>

<p>One of the smartirs is determined:  document frequency weighting is inverse-document-frequency or <strong><code>idf</code></strong>.</p>

<hr>

<p><code>wlocals</code> by default is <a href=""https://radimrehurek.com/gensim/utils.html#gensim.utils.identity"" rel=""noreferrer""><code>identity</code></a> function. Term frequency of the corpus passed through the identify function which nothing happened, and the corpus itself return. Hence, another parameter of smartirs, term frequency weighing, is natural or <strong><code>n</code></strong>. Now that we have term frequency and inverse-document-frequency we can compute tfidf:</p>

<p><a href=""https://i.sstatic.net/INxLU.gif"" rel=""noreferrer""><img src=""https://i.sstatic.net/INxLU.gif"" alt=""tfidf formula""></a></p>

<hr>

<p><a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.smartirs_normalize"" rel=""noreferrer""><code>normalize</code></a> by default is true that means after computing TfIDF it normalizes the tfidf vectors. The normalization is done with <code>l2-norm</code> (Euclidean unit norm) which means our last smartirs is cosine or <strong><code>c</code></strong>. This part implemented as:</p>

<pre><code># vec(term_id, value) is tfidf result
length = 1.0 * math.sqrt(sum(val ** 2 for _, val in vec))
normalize_by_length = [(termid, val / length) for termid, val in vec]
</code></pre>

<hr>

<p>When you call <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel.__getitem__"" rel=""noreferrer""><code>model[corpus]</code></a> or <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel.__getitem__"" rel=""noreferrer""><code>model.__getitem__()</code></a> the following things happen:</p>

<p><code>__getitem__</code> has a <code>eps</code> argument which is a threshold value that will remove all entries that have tfidf-value less than <code>eps</code>. By default, this value is 1e-12. As a result, when you print the vectors only some of them appeared.</p>
",5,5,1587,2018-05-30 06:54:18,https://stackoverflow.com/questions/50598129/what-is-the-default-smartirs-for-gensim-tfidfmodel
Negative Values: Evaluate Gensim LDA with Topic Coherence,"<p>I´m currently trying to evaluate my topic models with gensim topiccoherencemodel:</p>

<pre><code>from gensim.models.coherencemodel import CoherenceModel
cm_u_mass = CoherenceModel(model = model1, corpus = corpus1, coherence = 'u_mass')
coherence_u_mass = cm_u_mass.get_coherence()

print('\nCoherence Score: ', coherence_u_mass)
</code></pre>

<p>The output is just negative values. Is this correct? Can anybody provide a formula or something how u_mass works?</p>
","python-3.x, gensim, evaluation, topic-modeling","<p>Having a quick look at the <a href=""http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf"" rel=""noreferrer"">original article</a> you can see that UMass coherence is calculated over the log of probabilities therefore it is negative.</p>

<p>About the formula you asked, it can be found as equation 4 <a href=""https://i.sstatic.net/szrtG.png"" rel=""noreferrer"">in the same article</a>. </p>

<p>I understand that as the value of UMass coherence approaches to 0 the topic coherence gets better.</p>

<p>Hope this helps. </p>
",12,6,8857,2018-05-30 14:34:49,https://stackoverflow.com/questions/50607378/negative-values-evaluate-gensim-lda-with-topic-coherence
Topic Similarity in one model to csv Matrix,"<p>I want to generate a Topic to Topic Matrix in order to find similar topic to generate internal clusters with the function <code>gensim.models.ldamodel.diff</code> from gensim LDA.
How can I save my generated data into a csv with topics over topics and the distances (in this case hellinger distance) in the cells?
This code is not working for me:</p>

<pre><code>from gensim import models
import pandas

dateiname_model1 = ""lda.model""
model1 =  models.LdaModel.load(dateiname_model1)

topic_over_topic = model1.diff(model1, annotation=True)

topic_over_topic_speicherpfad = ""topic_over_topic_similarity.csv""
pandas.DataFrame(topic_over_topic).to_csv(topic_over_topic_speicherpfad, sep=';')
</code></pre>
","python-3.x, export-to-csv, gensim","<p>It works with the code <code>topic_over_topic, annotation = model1.diff(model1, annotation=True)</code>:</p>

<pre><code>from gensim import models
import pandas

dateiname_model1 = ""lda.model""
model1 =  models.LdaModel.load(dateiname_model1)

topic_over_topic, annotation = model1.diff(model1, annotation=True)

topic_over_topic_speicherpfad = ""topic_over_topic_similarity.csv""
pandas.DataFrame(topic_over_topic).to_csv(topic_over_topic_speicherpfad, sep=';')
</code></pre>
",0,1,212,2018-05-31 14:45:48,https://stackoverflow.com/questions/50627026/topic-similarity-in-one-model-to-csv-matrix
Using gensim word2vec in scikit-learn pipeline,"<p>I am trying to use <code>word2vec</code> in a scikit-learn pipeline.</p>

<pre><code>from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np

class ItemSelector(BaseEstimator, TransformerMixin):
    def __init__(self, key):
        self.key = key

    def fit(self, x, y=None):
        return self

    def transform(self, data_dict):
        return data_dict[self.key]


from sklearn.pipeline import Pipeline
from gensim.sklearn_api import W2VTransformer
pipeline_word2vec = Pipeline([
                ('selector', ItemSelector(key='X')),
                ('w2v', W2VTransformer()),
            ])

pipeline_word2vec.fit(pd.DataFrame({'X':['hello world','is amazing']}), np.array([1,0]))
</code></pre>

<p>this gives me </p>

<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-11-9e2dd309d07c&gt; in &lt;module&gt;()
     23                 ('w2v', W2VTransformer()),
     24             ])
---&gt; 25 pipeline_word2vec.fit(pd.DataFrame({'X':['hello world','is amazing']}), np.array([1,0]))

/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    248         Xt, fit_params = self._fit(X, y, **fit_params)
    249         if self._final_estimator is not None:
--&gt; 250             self._final_estimator.fit(Xt, y, **fit_params)
    251         return self
    252 

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/sklearn_api/w2vmodel.py in fit(self, X, y)
     62             sg=self.sg, hs=self.hs, negative=self.negative, cbow_mean=self.cbow_mean,
     63             hashfxn=self.hashfxn, iter=self.iter, null_word=self.null_word, trim_rule=self.trim_rule,
---&gt; 64             sorted_vocab=self.sorted_vocab, batch_words=self.batch_words
     65         )
     66         return self

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py in __init__(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)
    525             batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window, seed=seed,
    526             hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,
--&gt; 527             fast_version=FAST_VERSION)
    528 
    529     def _do_train_job(self, sentences, alpha, inits):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in __init__(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)
    336             self.train(
    337                 sentences, total_examples=self.corpus_count, epochs=self.epochs, start_alpha=self.alpha,
--&gt; 338                 end_alpha=self.min_alpha, compute_loss=compute_loss)
    339         else:
    340             if trim_rule is not None:

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py in train(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)
    609             sentences, total_examples=total_examples, total_words=total_words,
    610             epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,
--&gt; 611             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)
    612 
    613     def score(self, sentences, total_sentences=int(1e6), chunksize=100, queue_factor=2, report_delay=1):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in train(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)
    567             sentences, total_examples=total_examples, total_words=total_words,
    568             epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,
--&gt; 569             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)
    570 
    571     def _get_job_params(self, cur_epoch):

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in train(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)
    239             epochs=epochs,
    240             total_examples=total_examples,
--&gt; 241             total_words=total_words, **kwargs)
    242 
    243         for callback in self.callbacks:

/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py in _check_training_sanity(self, epochs, total_examples, total_words, **kwargs)
    599 
    600         if not self.wv.vocab:  # should be set by `build_vocab`
--&gt; 601             raise RuntimeError(""you must first build vocabulary before training the model"")
    602         if not len(self.wv.vectors):
    603             raise RuntimeError(""you must initialize vectors before training the model"")

RuntimeError: you must first build vocabulary before training the model
</code></pre>

<p>in a jupyter notebook. Instead I seek a trained model. How can I fix this?</p>
","python, scikit-learn, word2vec, gensim","<p>The <code>W2VTransformer</code> has a parameter <code>min_count</code> and it is by default equal to 5. So the error is simply a result of the fact that you only feed 2 documents but require for each word in the vocabulary to appear at least in 5 documents.</p>

<p>Possible solutions:</p>

<ul>
<li><p>Decrease <code>min_count</code></p></li>
<li><p>Give the model more documents</p></li>
</ul>
",6,4,4672,2018-06-01 22:50:23,https://stackoverflow.com/questions/50651861/using-gensim-word2vec-in-scikit-learn-pipeline
gensim: pickle or not?,"<p>I have a question related to gensim. I like to know whether it is recommended or necessary to use pickle while saving or loading a model (or multiple models), as I find scripts on GitHub that do either.    </p>

<pre><code>mymodel = Doc2Vec(documents, size=100, window=8, min_count=5, workers=4)
      mymodel.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)
</code></pre>

<p>See <a href=""https://radimrehurek.com/gensim/models/deprecated/doc2vec.html"" rel=""noreferrer"">here</a></p>

<p><strong>Variant 1:</strong></p>

<pre><code>import pickle
# Save
mymodel.save(""mymodel.pkl"")  # Stores *.pkl file
# Load
mymodel = pickle.load(""mymodel.pkl"")
</code></pre>

<p><strong>Variant 2:</strong></p>

<pre><code># Save
model.save(mymodel) # Stores *.model file
# Load
model = Doc2Vec.load(mymodel)
</code></pre>

<p>In <code>gensim.utils</code>, it appears to me that there is a pickle function embedded: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/utils.py"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/utils.py</a></p>

<p>def save 
  ...
  try:
              _pickle.dump(self, fname_or_handle,   protocol=pickle_protocol)
  ...</p>

<p><strong>Goal of my question:</strong>
I would be glad to learn 1) whether I need pickle (for better memory management) and 2) in case, why it's better than loading *.model files.</p>

<p>Thank you!</p>
","memory, model, pickle, gensim","<p>Whenever you store a model using the built-in gensim function <code>save()</code>, pickle is being used regardless of the file extension. The <a href=""https://radimrehurek.com/gensim/utils.html#gensim.utils.SaveLoad"" rel=""noreferrer"">documentation for utils</a> tells us this:</p>

<blockquote>
  <p>class gensim.utils.SaveLoad</p>

<pre><code>Bases: object

Class which inherit from this class have save/load functions, which un/pickle them to disk.

Warning

This uses pickle for de/serializing, so objects must not contain unpicklable attributes, such as lambda functions etc.
</code></pre>
</blockquote>

<p>So gensim will use pickle to save any model as long as the model class inherits from the <code>gensim.utils.SaveLoad</code> class. In your case <code>gensim.models.doc2vec.Doc2Vec</code> inherits from <code>gensim.models.base_any2vec.BaseWordEmbeddingsModel</code> which in turn inherits from <code>gensim.utils.SaveLoad</code> which provides the actual <code>save()</code> function. </p>

<p>To answer your questions:</p>

<ol>
<li>Yes, you need pickle unless you want to write your own function for
storing your models to disk. Using pickle should not be problematic though since
it is in the standard library. You won't even notice it.</li>
<li>If you use the gensim <code>save()</code>
function you can chose any file extension: *.model, *.pkl, *.p,
*.pickle. The saved file will be pickled.</li>
</ol>
",6,10,6312,2018-06-02 09:26:26,https://stackoverflow.com/questions/50655405/gensim-pickle-or-not
How to initialize a pool of python multiprocessing workers with a shared state?,"<p>I am trying to execute in parallel <a href=""https://github.com/amirouche/wikimark/"" rel=""nofollow noreferrer"">some machine learning algorithm</a>.</p>

<p>When I use multiprocessing, it's slower than without. My wild guess is that the <code>pickle</code> serialization of the models I use slowing down the whole process. So the question is: <em>how can I initialize the pool's worker with an initial state so that I don't need to serialize/deserialize for every single call the models?</em></p>

<p>Here is my current code:</p>

<pre><code>import pickle
from pathlib import Path
from collections import Counter
from multiprocessing import Pool

from gensim.models.doc2vec import Doc2Vec

from wikimark import html2paragraph
from wikimark import tokenize


def process(args):
    doc2vec, regressions, filepath = args
    with filepath.open('r') as f:
        string = f.read()
    subcategories = Counter()
    for index, paragraph in enumerate(html2paragraph(string)):
        tokens = tokenize(paragraph)
        vector = doc2vec.infer_vector(tokens)
        for subcategory, model in regressions.items():
            prediction = model.predict([vector])[0]
            subcategories[subcategory] += prediction
    # compute the mean score for each subcategory
    for subcategory, prediction in subcategories.items():
        subcategories[subcategory] = prediction / (index + 1)
    # keep only the main category
    subcategory = subcategories.most_common(1)[0]
    return (filepath, subcategory)


def main():
    input = Path('./build')
    doc2vec = Doc2Vec.load(str(input / 'model.doc2vec.gz'))
    regressions = dict()
    for filepath in input.glob('./*/*/*.model'):
        with filepath.open('rb') as f:
            model = pickle.load(f)
        regressions[filepath.parent] = model

    examples = list(input.glob('../data/wikipedia/english/*'))

    with Pool() as pool:
        iterable = zip(
            [doc2vec] * len(examples),  # XXX!
            [regressions] * len(examples),  # XXX!
            examples
        )
        for filepath, subcategory in pool.imap_unordered(process, iterable):
            print('* {} -&gt; {}'.format(filepath, subcategory))


if __name__ == '__main__':
    main()
</code></pre>

<p>The lines marked with <code>XXX!</code> point to the data that serialized when I call <code>pool.imap_unodered</code>. There at least 200MB of data that is serialized.</p>

<p>How can I avoid serialization?</p>
","python, scikit-learn, nlp, data-science, gensim","<p>The solution as simple as using a global for both <code>doc2vec</code> and <code>regressions</code>. </p>
",1,0,385,2018-06-02 15:47:53,https://stackoverflow.com/questions/50658576/how-to-initialize-a-pool-of-python-multiprocessing-workers-with-a-shared-state
Evaluation of Dynamic Topic Models,"<p>I try to evaluate my dynamic topic models.
The model were generated with the gensim wrappers.
Are there any possible functions like perplexity or topic coherence equal to the ""normal"" topic modeling?</p>
","gensim, evaluation, lda","<p>Yeah there is topic coherence and perplexity for Gensim Wrapper:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code># Compute Coherence Score
coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=processed_docs, dictionary=dictionary, coherence='c_v')
coherence_ldamallet = coherence_model_ldamallet.get_coherence()
print('\nCoherence Score: ', coherence_ldamallet)</code></pre>
</div>
</div>
</p>

<p>You can check out this article for more information: <a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""nofollow noreferrer"">14</a></p>

<p>I hope this helps :)</p>
",1,1,374,2018-06-04 11:27:52,https://stackoverflow.com/questions/50679537/evaluation-of-dynamic-topic-models
Can doc2vec be useful if training on Documents and inferring on sentences only,"<p>I am training with some documents with gensim's Doc2vec.  </p>

<p>I have two types of inputs:  </p>

<ol>
<li>Whole English Wikipedia: Each article of Wikipedia text is considered as one 
document for doc2vec training. (Total around 5.5 million articles or documents)</li>
<li>Some documents related to my project that are manually prepared and collected from some websites. (around 15000 documents).<br>
Where each document the size is around 100 sentences.</li>
</ol>

<p>Further, I want to use this model to infer sentences of size (10~20 words).</p>

<p>I request some clarification on my approach.<br>
Is the method of training over documents(size of each document approx. 100 sentences each) and then inferring over new sentence correct. ?</p>

<p>Or, should I train over only sentences and not documents and then infer over the new sentence.?</p>
","python, gensim, training-data, doc2vec","<p>Every corpus and project goals are different. Your approach of training on larger docs but then inferring on shorter sentences could plausibly work, but you have to try it to see how well, and then iteratively test whether perhaps shorter training docs (as single sentences or groups-of-sentences) work better, for your specific goal. </p>

<p>Note that <code>gensim</code> <code>Doc2Vec</code> inference often gains from non-default parameters – especially more <code>steps</code> (than the tiny default 5) or a smaller starting <code>alpha</code> (more like the training default of <code>0.025</code>), especially on shorter documents. And, that inference also may work better or worse depending on original model metaparameters.</p>

<p>Note also that an implementation limit means that texts longer than 10,000 tokens are silently truncated in <code>gensim</code> <code>Word2Vec</code>/<code>Doc2Vec</code> training. (If you have longer docs, you can split them into less-than-10K-token subdocuments, but then repeat the <code>tags</code> for each subdocument, to closely simulate what effect training with a longer document would have had.)</p>
",1,0,422,2018-06-05 05:38:40,https://stackoverflow.com/questions/50692739/can-doc2vec-be-useful-if-training-on-documents-and-inferring-on-sentences-only
Small model from Google news Word2Vec model,"<p>I am using <code>GoogleNews-vectors-negative300.bin</code> model and <a href=""https://github.com/ian-beaver/pycontractions"" rel=""nofollow noreferrer"">pycontractions</a> library to determinate with machine learning the best option to expand contractions when there are ambiguous meanings like <code>I'd</code> with can be <code>I would</code> and <code>I had</code>. The size of this model is very large, around to 3.5Gb.</p>

<p>I think that 3.5Gb is a very large model to use for my purpose. Probably I'll never use all words representations in this model. Is there a way to reduce the size extracting only a subset of words representations that are useful to my purposes?</p>
","machine-learning, models, word2vec, gensim","<p>Truncating the set to the 1st N words is easy with an optional argument to <code>gensim</code>'s <code>load_word2vec_format()</code> method, <code>limit</code>. If present, only the given number of words will be loaded. For example, <code>limit=500000</code> reads only the 1st 500,000 words from the supplied file. </p>

<p>Since such files are usually sorted to put the most-frequent words first, you often don't lose much by discarding the 'long tail' of later words. (They'll appear less frequently in your texts, and their word-vectors were trained on fewer examples and thus of lower-quality, anyway.)</p>

<p>You could then re-<code>save_word2vec_format()</code> the truncated set, if you wanted a smaller file on disk. </p>

<p>You could also tamper with the file on disk to make it only include some <em>other</em> subset of words to retain. It might be easier to do so in the text (<code>binary=False</code>) format. Looking at the <code>gensim</code> source code for <code>load_word2vec_format()</code>/<code>save_word2vec_format()</code> could help you to understand what the file must look like to read back in. </p>
",4,0,1996,2018-06-05 21:41:04,https://stackoverflow.com/questions/50709355/small-model-from-google-news-word2vec-model
How is Word2Vec min_count applied,"<p>Say that I'm training a (Gensim) Word2Vec model with min_count=5. The documentation learns us what min_count does:</p>

<blockquote>
  <p>Ignores all words with total frequency lower than this.</p>
</blockquote>

<p>What is the effect of min_count on the context? Lets say that I have a sentence of frequent words (min_count > 5) and infrequent words (min_count &lt; 5), annotated with f and i:</p>

<blockquote>
  <p>This (f) is (f) a (f) test (i) sentence (i) which (f) is (f) shown (i) here (i)</p>
</blockquote>

<p>I just made up which word is frequently used and which word is not for demonstration purposes.</p>

<p>If I remove all infrequent words, we get a completely different context from which word2vec is trained. In this example, your sentence would be ""This is a which is"", which would then be a training sentence for Word2Vec. Moreover, if you have a lot of infrequent words, words that were originally very far away from each other are now placed within the same context.</p>

<p>Is this the correct interpretation of Word2Vec? Are we just assuming that you shouldn't have too many infrequent words in your dataset (or set a lower min_count threshold)?</p>
","python, word2vec, gensim","<p>Words below the <code>min_count</code> frequency are dropped before training occurs. So, the relevant context <code>window</code> is the word-distance among surviving words. </p>

<p>This de facto shrinking of contexts is usually a good thing: the infrequent words don't have enough varied examples to obtain good vectors for themselves. Further, while individually each infrequent word is rare, in total there are lots of them, so these doomed-to-poor-vector rare-words intrude on most other words' training, serving as a sort of noise that makes those word-vectors worse too.</p>

<p>(Similarly, when using the <code>sample</code> parameter to down-sample frequent words, the frequent words are randomly dropped – which also serves to essentially ""shrink"" the distances between surviving words, and often improves overall vector quality.)</p>
",10,4,10116,2018-06-06 14:46:50,https://stackoverflow.com/questions/50723303/how-is-word2vec-min-count-applied
Find the closest word to set of words,"<p>I would need to find something like the opposite of <code>model.most_similar()</code><br>
While <code>most_similar()</code> returns an array of words most similar to the one given as input, I need to find a sort of ""center"" of a list of words.</p>

<p>Is there a function in gensim or any other tool that could help me?</p>

<p>Example:<br>
Given <code>{'chimichanga', 'taco', 'burrito'}</code> the center would be maybe <code>mexico</code> or <code>food</code>, depending on the corpus that the model was trained on</p>
","python, nlp, word2vec, gensim","<p>If you supply a list of words as the <code>positive</code> argument to <code>most_similar()</code>, it will report words closest to their mean (which would seem to be one reasonable interpretation of the words' 'center'). </p>

<p>For example:</p>

<pre><code>sims = model.most_similar(positive=['chimichanga', 'taco', 'burrito'])
</code></pre>

<p>(I somewhat doubt the top result <code>sims[0]</code> here will be either 'mexico' or 'food'; it's most likely to be another mexican-food word. There isn't necessarily a ""more generic""/hypernym relation to be found either between word2vec words, or in certain directions... but some other embedding techniques, such as <a href=""https://dawn.cs.stanford.edu/2018/03/19/hyperbolics/"" rel=""nofollow noreferrer"">hyperbolic embeddings</a>, might provide that.)</p>
",3,2,1607,2018-06-06 15:11:38,https://stackoverflow.com/questions/50723841/find-the-closest-word-to-set-of-words
Understanding parameters in Gensim LDA Model,"<p>I am using <code>gensim.models.ldamodel.LdaModel</code> to perform LDA, but I do not understand some of the parameters and cannot find explanations in the documentation. If someone has experience working with this, I would love further details of what these parameters signify.
Specifically, I do not understand:</p>

<ul>
<li><code>random_state</code></li>
<li><code>update_every</code></li>
<li><code>chunksize</code></li>
<li><code>passes</code></li>
<li><code>alpha</code></li>
<li><code>per_word_topics</code></li>
</ul>

<p>I am working with a corpus of 500 documents which are roughly around 3-5 pages each (I unfortunately cannot share a snapshot of the data because of confidentiality reasons). Currently I have set </p>

<ul>
<li><code>num_topics = 10</code></li>
<li><code>random_state = 100</code></li>
<li><code>update_every = 1</code></li>
<li><code>chunksize = 50</code></li>
<li><code>passes = 10</code></li>
<li><code>alpha = 'auto'</code></li>
<li><code>per_word_topics = True</code></li>
</ul>

<p>but this is solely based off of an example I saw and I am not sure how generalizable that is to my data.</p>
","python, parameters, gensim, lda","<p>I wonder if you have seen <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""noreferrer"">this page</a>?</p>

<p>Either way, let me explain a few things for you. The number of documents you use is small for the method (it works much better when trained on a data source of the size of Wikipedia). Therefore the results will be rather crude and you have to be aware of that. This is why you should not aim for a large number of topics (you chose 10 which could maybe go sensibly up to 20 in your case).</p>

<p>As for the other parameters:</p>

<ul>
<li><p><code>random_state</code> - this serves as a seed (in case you wanted to repeat exactly the training process)</p></li>
<li><p><code>chunksize</code> - number of documents to consider at once (affects the memory consumption)</p></li>
<li><p><a href=""https://groups.google.com/forum/#!topic/gensim/ojySenxQHi4"" rel=""noreferrer""><code>update_every</code></a> - update the model every <code>update_every</code> <code>chunksize</code> chunks (essentially, this is for memory consumption optimization)</p></li>
<li><p><code>passes</code> - how many times the algorithm is supposed to pass over the whole corpus</p></li>
<li><p><code>alpha</code> - to cite the documentation:</p>

<blockquote>
  <p>can be set to an explicit array = prior of your choice. It also
  support special values of `‘asymmetric’ and ‘auto’: the former uses a
  fixed normalized asymmetric 1.0/topicno prior, the latter learns an
  asymmetric prior directly from your data.</p>
</blockquote></li>
<li><p><code>per_word_topics</code> - setting this to <code>True</code> allows for extraction of the most likely topics given a word. The training process is set in such a way that every word will be assigned to a topic. Otherwise, words that are not indicative are going to be omitted. <code>phi_value</code> is another parameter that steers this process - it is a threshold for a word treated as indicative or not.</p></li>
</ul>

<p>Optimal training process parameters are described particularly well in <a href=""https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation"" rel=""noreferrer"">M. Hoffman et al., Online Learning for Latent Dirichlet Allocation</a>.</p>

<p>For memory optimization of the training process or the model see <a href=""https://miningthedetails.com/blog/python/lda/GensimLDA/"" rel=""noreferrer"">this blog post</a>.</p>
",22,16,24856,2018-06-11 20:30:01,https://stackoverflow.com/questions/50805556/understanding-parameters-in-gensim-lda-model
Evaluation of ldaseqmodel in gensim,"<p>is there a possibility to evaluate the dynamic model (ldaseqmodel) like the ""normal"" lda model in values of perplexity and topic coherence?
I know that these values are printed into the logging.INFO, so another method would be to save the logging.INFO into a text file to search for these evaluation values after the simulation.
If method 1 (code to evaluate ldaseqmodel) doesnt exist, is it possible to save the logging.INFO into a text file?
Here is my code to generate the ldaseqmodel:</p>

<pre><code>from gensim import models, corpora
import csv
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

Anzahl_Topics1      = 10                

Zeitabschnitte      = [16, 19, 44, 51, 84, 122, 216, 290, 385, 441, 477, 375, 390, 408, 428, 192, 38]

TDM_dateipfad = './1gramm/TDM_1gramm_1998_2014.csv'

dateiname_corpus = ""./1gramm/corpus_DTM_1gramm.mm""

dateiname1_dtm  = ""./1gramm/DTM_1gramm_10.model""

ids = {} 
corpus = [] 

with open(TDM_dateipfad, newline='') as csvfile:
    reader = csv.reader(csvfile, delimiter=';', quotechar='|') 
    for rownumber, row in enumerate(reader): 
        for index, field in enumerate(row):
            if index == 0:
                if rownumber &gt; 0:
                    ids[rownumber-1] = field 
            else:
                if rownumber == 0:
                    corpus.append([])
                else:
                    corpus[index-1].append((rownumber-1, int(field))) 

corpora.MmCorpus.serialize(dateiname_corpus, corpus)

dtm1 = models.ldaseqmodel.LdaSeqModel(corpus=corpus, time_slice = Zeitabschnitte, id2word=ids, num_topics = Anzahl_Topics1, passes=1, chunksize=10000) 
dtm1.save(dateiname1_dtm)
</code></pre>
","python-3.x, gensim, lda","<p>You're asking two very different questions.</p>

<p><strong>Is it possible to save the logging.INFO into a text file?</strong></p>

<p>Yes. You can use this code to send your log to a file instead of the console. DEBUG level logging gives you more detailed information than INFO.</p>

<pre class=""lang-py prettyprint-override""><code>import logging
logging.basicConfig(level=logging.DEBUG, file='yourlogname.log')
</code></pre>

<p>You might also want to set up file handlers to have an INFO log in the console, and a DEBUG level log to a file. See the python documentation <a href=""https://docs.python.org/3/library/logging.html"" rel=""nofollow noreferrer"">here</a> for more info.</p>

<p><strong>Is there a possibility to evaluate the DTM using perplexity and topic coherence?</strong> </p>

<p>Yes, use <code>dtm_coherence</code> - see the <a href=""https://radimrehurek.com/gensim/models/ldaseqmodel.html"" rel=""nofollow noreferrer"">gensim documentation here</a> - coherence is generally a more useful measure (in terms of ""do humans understand this"") than perplexity. You will have to do so for each time slice separately though. My recommendation, if you want to compare two models, say a 10- vs. 20-topic model, would be to loop over the time slices for each model, and graph the coherence scores to see if one is consistently better, for example. There is a nice tutorial in this <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb"" rel=""nofollow noreferrer"">DTM example from the gensim devs</a>.</p>
",1,0,1168,2018-06-14 12:26:38,https://stackoverflow.com/questions/50857544/evaluation-of-ldaseqmodel-in-gensim
Is there a way to set min_df and max_df in gensim&#39;s tfidf model?,"<p>I am using gensim's tdidf model like so:</p>

<pre><code>from gensim import corpora, models

dictionary = corpora.Dictionary(some_corpus)
mapped_corpus = [dictionary.doc2bow(text)
                 for text in some_corpus]

tfidf = models.TfidfModel(mapped_corpus)
</code></pre>

<p>Now I'd like to apply thresholds to remove terms that appear too frequently (max_df) and too infrequently (min_df).  I know that scikit's CountVectorizer allows you to do this, but I can't seem to find how to set these thresholds in gensim's tfidf.  Could someone please help? </p>
","gensim, tf-idf","<p>You can filter your dictionary with </p>

<pre><code>dictionary.filter_extremes(no_below=min_df, no_above=rel_max_df)
</code></pre>

<p>Note that <code>no_below</code> expects the minimum number of documents in which tokens must appear, whereas <code>no_above</code> expects a maximum relative frequency, e.g. <code>0.5</code>. Afterwards you can then construct your corpus with the filtered dictionary. According to the <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html"" rel=""nofollow noreferrer"">gensim docs</a> it is also possible to construct a <code>TfidfModel</code> with only a dictionary.</p>
",1,0,1642,2018-06-14 14:10:47,https://stackoverflow.com/questions/50859540/is-there-a-way-to-set-min-df-and-max-df-in-gensims-tfidf-model
loading of fasttext pre trained german word embedding&#39;s .vec file throwing out of memory error,"<p>I am using gensim to load the fasttext's pre-trained word embedding</p>

<p><code>de_model = KeyedVectors.load_word2vec_format('wiki.de\wiki.de.vec')</code></p>

<p>But this gives me a memory error.</p>

<p>Is there any way I can load it?</p>
","nlp, gensim, word-embedding, fasttext","<p>Other than working on a machine with more memory, the <code>gensim</code> <code>load_word2vec_format()</code> methods have a <code>limit</code> option which can be given a count <em>n</em> of vectors to read. Only the first <em>n</em> vectors of the file will be loaded. </p>

<p>For example, to load just the 1st 100,000 words:</p>

<pre><code>de_model = KeyedVectors.load_word2vec_format('wiki.de\wiki.de.vec', limit=100000)
</code></pre>

<p>Since such files usually sort the more-frequent words first, and the 'long tail' of rarer words tend to be weaker vectors, many applications don't lose too much power by discarding rarer words. </p>
",6,4,2778,2018-06-18 13:08:57,https://stackoverflow.com/questions/50910287/loading-of-fasttext-pre-trained-german-word-embeddings-vec-file-throwing-out-o
Gensim Word2Vec select minor set of word vectors from pretrained model,"<p>I have a large pretrained Word2Vec model in gensim from which I want to use the pretrained word vectors for an embedding layer in my Keras model. </p>

<p>The problem is that the embedding size is enormous and I don't need most of the word vectors (because I know which words can occure as Input). So I want to get rid of them to reduce the size of my embedding layer.</p>

<p>Is there a way to just keep desired wordvectors (including the coresponding indices!), based on a whitelist of words?</p>
","python, keras, word2vec, gensim, word-embedding","<p>Thanks to <a href=""https://stackoverflow.com/a/54258997/7339624"">this answer</a> (I've changed the code a little bit to make it better). you can use this code for solving your problem.</p>

<p>we have all our minor set of words in <code>restricted_word_set</code>(it can be either list or set) and <code>w2v</code> is our model, so here is the function:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

def restrict_w2v(w2v, restricted_word_set):
    new_vectors = []
    new_vocab = {}
    new_index2entity = []
    new_vectors_norm = []

    for i in range(len(w2v.vocab)):
        word = w2v.index2entity[i]
        vec = w2v.vectors[i]
        vocab = w2v.vocab[word]
        vec_norm = w2v.vectors_norm[i]
        if word in restricted_word_set:
            vocab.index = len(new_index2entity)
            new_index2entity.append(word)
            new_vocab[word] = vocab
            new_vectors.append(vec)
            new_vectors_norm.append(vec_norm)

    w2v.vocab = new_vocab
    w2v.vectors = np.array(new_vectors)
    w2v.index2entity = np.array(new_index2entity)
    w2v.index2word = np.array(new_index2entity)
    w2v.vectors_norm = np.array(new_vectors_norm)
</code></pre>

<blockquote>
  <p><strong>WARNING:</strong> when you first create the model the <code>vectors_norm == None</code> so
  you will get an error if you use this function there. <code>vectors_norm</code>
  will get a value of the type <code>numpy.ndarray</code> after the first use. so
  before using the function try something like <code>most_similar(""cat"")</code> so
  that <code>vectors_norm</code> not be equal to <code>None</code>.</p>
</blockquote>

<p>It rewrites all of the variables which are related to the words based on the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py"" rel=""noreferrer"">Word2VecKeyedVectors</a>.</p>

<p>Usage:</p>

<pre><code>w2v = KeyedVectors.load_word2vec_format(""GoogleNews-vectors-negative300.bin.gz"", binary=True)
w2v.most_similar(""beer"")
</code></pre>

<blockquote>
  <p>[('beers', 0.8409687876701355),<br>
   ('lager', 0.7733745574951172),<br>
   ('Beer', 0.71753990650177),<br>
   ('drinks', 0.668931245803833),<br>
   ('lagers', 0.6570086479187012),<br>
   ('Yuengling_Lager', 0.655455470085144),<br>
   ('microbrew', 0.6534324884414673),<br>
   ('Brooklyn_Lager', 0.6501551866531372),<br>
   ('suds', 0.6497018337249756),<br>
   ('brewed_beer', 0.6490240097045898)]</p>
</blockquote>

<pre><code>restricted_word_set = {""beer"", ""wine"", ""computer"", ""python"", ""bash"", ""lagers""}
restrict_w2v(w2v, restricted_word_set)
w2v.most_similar(""beer"")
</code></pre>

<blockquote>
  <p>[('lagers', 0.6570085287094116),<br>
   ('wine', 0.6217695474624634),<br>
   ('bash', 0.20583480596542358),<br>
   ('computer', 0.06677375733852386),<br>
   ('python', 0.005948573350906372)]</p>
</blockquote>

<p>it can be used for removing some words either.</p>
",14,8,2822,2018-06-18 17:32:32,https://stackoverflow.com/questions/50914729/gensim-word2vec-select-minor-set-of-word-vectors-from-pretrained-model
How to get all the weight updates from Word2Vec,"<p>I am not only interested in the final W0 and W1 (also, to some known as W and W'), but all the variations of these two matrices during the learning.</p>

<p>For now, I am using the gensim implementation, but compared to sklearn, gensim's API is not very well organized in my mind. Hence, I am open to moving to tf if need be, given that getting access to these values would be possible/easier.</p>

<p>I know I can hack the main code; my question is whether there already is a function/variable for it.</p>
","tensorflow, gensim, word2vec","<p>There's no specific API for seeing individual training example updates, or interim weights mid-training. </p>

<p>But as you've intuited, instead of calling <code>train()</code> once, letting it run all epochs and all learning-rate-updates (as is recommended), you could call it one epoch at a time, providing it the right incremental <code>start_alpha</code> and <code>end_alpha</code> yourself each call, and between the calls look at the word-vectors (aka ""projection weights"") and hidden-to-output weights (<code>syn1neg</code> for default negative-sampling, or <code>syn1</code> for hierarchical-softmax). </p>

<p>If you needed more fine-grained reporting, you'd need to modify the source code to add the extra logging/callouts/etc you need.</p>
",0,0,490,2018-06-19 22:39:53,https://stackoverflow.com/questions/50937881/how-to-get-all-the-weight-updates-from-word2vec
Paragraph Vector or Doc2vec model size,"<p>I am using deeplearning4j java library to build paragraph vector model (doc2vec) of dimension 100. I am using a text file. It has around 17 million lines, and size of the file is 330 MB. 
I can train the model and calculate paragraph vector which gives reasonably good results.</p>

<p>The problem is that when I try to save  the model (by writing to disk) with WordVectorSerializer.writeParagraphVectors (dl4j method) it takes around 20 GB of space.  And around 30GB when I use native java serializer. </p>

<p>I'm thinking may be the model is size is too big for that much data. Is the model size 20GB reasonable for the text data of 300 MB?  </p>

<p>Comments are also welcome from people who have used doc2vec/paragraph vector in other library/language. </p>

<p>Thank you!</p>
","nlp, gensim, word-embedding, doc2vec, deeplearning4j","<p>I'm not familiar with the dl4j implementation, but model size is dominated by the number of unique word-vectors/doc-vectors, and the chosen vector size. </p>

<p>(330MB / 17 million) means each of your documents averages only 20 bytes – very small for <code>Doc2Vec</code>! </p>

<p>But if for example you're training up a 300-dimensional doc-vector for each doc, and each dimension is (as typical) a 4-byte float, then (17 million * 300 dims * 4 bytes/dim) = 20.4GB. And then there'd be more space for word-vectors and model inner-weights/vocabulary/etc, so the storage sizes you've reported aren't implausible.</p>

<p>With the sizes you've described, there's also a big risk of overfitting - if using 300-dimensions, you'd be modeling docs of &lt;20 bytes source material as (300*4=) 1200-byte doc-vectors. </p>

<p>To some extent, that makes the model tend towards a giant, memorized-inputs lookup table, and thus less-likely to capture generalizable patterns that help understand training docs, or new docs. Effective learning usually instead looks somewhat like compression: modeling the source materials as something smaller but more salient. </p>
",4,3,1199,2018-06-20 10:17:47,https://stackoverflow.com/questions/50945820/paragraph-vector-or-doc2vec-model-size
Doc2Vec input format,"<p>running gensim Doc2Vec over ubuntu</p>

<p>Doc2Vec rejects my input with the error</p>

<blockquote>
  <p>AttributeError: 'list' object has no attribute 'words'</p>
</blockquote>

<pre><code>    import gensim from gensim.models  
    import doc2vec as dtv
    from nltk.corpus import brown
    documents = brown.tagged_sents()
    d2vmodel = &gt; dtv.Doc2Vec(documents, size=100, window=1, min_count=1, workers=1)
</code></pre>

<p>I have tried already from 
<a href=""https://stackoverflow.com/questions/36509957/why-gensim-doc2vec-give-attributeerror-list-object-has-no-attribute-words"">this SO question</a> and many variations with the same result</p>

<p>documents = [brown.tagged_sents()}
adding a hash function</p>

<p>If corpus is a .txt file I <em>can</em> utilize </p>

<pre><code>    documents=TaggedLineDocument(documents)
</code></pre>

<p>but that is often not possible</p>
","gensim, doc2vec","<p>Gensim's <code>Doc2Vec</code> requires each document to be in the form of an object with a <code>words</code> property that is a list of string tokens, and a <code>tags</code> property that is a list of tags. These tags are usually strings, but expert users with large datasets can save a little memory by using plain-ints, starting from 0, instead. </p>

<p>A class <code>TaggedDocument</code> is included that is of the right 'shape', and used in most of the Gensim documentation/tutorial examples – but given Python's 'duck typing', any object with <code>words</code> and <code>tags</code> properties will do. </p>

<p>But a plain list won't. </p>

<p>And if I understand correctly, <code>brown.tagged_sents()</code> will return lists of (word, part-of-speech-tag) tuples, which isn't even the kind of list-of-word-tokens that would work as a <code>words</code>, and doesn't supply any of the full-document tags that are what <code>Doc2Vec</code> needs as keys to the doc-vectors that get trained.</p>

<p>Separately: it is unlikely you'd want to use <code>min_count=1</code>. Discarding very-low-frequency words usually makes retained <code>Word2Vec</code>/<code>Doc2Vec</code> vectors better.</p>
",1,0,802,2018-06-22 16:29:52,https://stackoverflow.com/questions/50992153/doc2vec-input-format
Hierarchical training for doc2vec: how would assigning same labels to sentences of the same document work?,"<p>What is the effect of assigning the same label to a bunch of sentences in doc2vec? I have a collection of documents that I want to learn vectors using gensim for a ""file"" classification task where file refers to a collection of documents for a given ID. I have several ways of labeling in mind and I want to know what would be the difference between them and which is the best - </p>

<ul>
<li><p>Take a document d1, assign label <code>doc1</code> to the tags and train. Repeat for others</p></li>
<li><p>Take a document d1, assign label <code>doc1</code> to the tags. Then tokenize document into sentences and assign label <code>doc1</code> to its tags and then train with both full document and individual sentences. Repeat for others</p></li>
</ul>

<p>For example (ignore that the sentence isn't tokenized) -</p>

<pre><code>Document -  ""It is small. It is rare"" 
TaggedDocument(words=[""It is small. It is rare""], tags=['doc1'])
TaggedDocument(words=[""It is small.""], tags=['doc1'])
TaggedDocument(words=[""It is rare.""], tags=['doc1'])
</code></pre>

<ul>
<li>Similar to above, but also assign a unique label for each sentence along with <code>doc1</code>. The full document has the all the sentence tags along with <code>doc1</code>.</li>
</ul>

<p>Example - </p>

<pre><code>Document -  ""It is small. It is rare"" 
TaggedDocument(words=[""It is small. It is rare""], tags=['doc1', 'doc1_sentence1', 'doc1_sentence2'])
TaggedDocument(words=[""It is small.""], tags=['doc1', 'doc1_sentence1'])
TaggedDocument(words=[""It is rare.""], tags=['doc1', 'doc1_sentence2'])
</code></pre>

<p>I also have some additional categorical tags that I'd be assigning. So what would be the best approach?</p>
","python, nlp, word2vec, gensim, doc2vec","<p>You can do all this! Assigning the same tag to multiple texts has <em>almost</em> the same effect as would combining those texts into one larger text, and assigning it that tag. The slight differences would be for <code>Doc2Vec</code> modes where there's a context-window – PV-DM (<code>dm=1</code>). With separate texts, there'd never be contexts stretching across the end/beginning of sentences.</p>

<p>In fact, as <code>gensim</code>'s optimized code paths have a 10,000-token limit to text sizes, splitting larger documents into subdocuments, but repeating their tags is sometimes necessary as a workaround.</p>

<p>What you've specifically proposed, training both the full-doc, and the doc-fragments, would work, but also have the effect of doubling the amount of text (and thus training-attention/individual-prediction-examples) for the <code>'doc1'</code> tags, compared to the narrower per-sentence tags. You might want that, or not - it could affect the relative quality of each. </p>

<p>What's best is unclear - it depends on your corpus, and end goals, so should be determined through experimentation, with a clear end-evaluation so that you can automate/systematize a rigorous search for what's best. </p>

<p>A few relevant notes, though:</p>

<ul>
<li><code>Doc2Vec</code> tends to works better with docs of at least a dozen or more words per document.</li>
<li>The <code>'words'</code> need to be tokenized - a list-of-strings, not a string.</li>
<li>It benefits from a lot of varied data, and in particular if you're training a larger model – more unique tags (including overlapping ones), and many-dimension vectors – you'll need more data to avoid overfitting.</li>
</ul>
",2,0,806,2018-06-24 22:25:09,https://stackoverflow.com/questions/51014463/hierarchical-training-for-doc2vec-how-would-assigning-same-labels-to-sentences
Normalizing bag of words data in Gensim,"<p>I am using gensim to create a bag of words model and I want to perform normalization. I found the documentation (<a href=""https://radimrehurek.com/gensim/models/normmodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/normmodel.html</a>), but I am confused as to how to implement that given the code I have. Conversations is a list of tokenized documents, so essentially a list of lists when each element is a document.</p>

<pre><code>id2word = corpora.Dictionary(conversations)
id2word.filter_extremes(keep_n=5000, keep_tokens=None) 
corpus = [id2word.doc2bow(text) for text in conversations]
norm_corpus = NormModel(corpus)
</code></pre>

<p>Corpus is a sparse matrix, I believe. For each document, it has the non-zero frequency terms and the corresponding counts: [[(0, 2), (1, 5), (2, 4)...(92, 2), (93, 3)],...].</p>

<p>The last line with <code>norm_corpus</code> does not work when I try to input it into the following: <code>models.LsiModel(norm_corpus, id2word=id2word, num_topics=12)</code>. I get the type error message, 'int' object is not iterable. However, the documentation says to pass in a corpus so I'm confused. I would appreciate any help -- thanks!</p>
","python, normalization, gensim, corpus, term-document-matrix","<p>I don't have a way to check at the moment but try this:</p>

<pre><code>norm_corpus = NormModel()
norm_corpus.normalize(text)
</code></pre>

<p>or</p>

<p><code>norm_corpus.normalize(id2word.doc2bow(text)</code> </p>

<p>In your original code you have</p>

<pre><code>`NormModel(iterable)`
</code></pre>

<p>but the documentation says you need to pass:</p>

<p><code>NormModel(iterable of iterable(int,number))</code></p>

<p>If this makes sense.</p>
",1,2,1102,2018-06-25 19:33:43,https://stackoverflow.com/questions/51030698/normalizing-bag-of-words-data-in-gensim
gensim function predict output words,"<p>I use the gensim library to create a word2vec model. It contains the function <code>predict_output_words()</code> which I understand as follows:</p>

<p>For example, I have a model that is trained with the sentence: ""Anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy.""</p>

<p>and then I use </p>

<p><code>model.predict_output_words(context_words_list=['Anarchism', 'does', 'not', 'offer', 'a', 'fixed', 'body', 'of', 'from', 'a', 'single', 'particular', 'world', 'view', 'instead', 'fluxing'], topn=10)</code>.</p>

<p>In this situation, could I get/predict the correct word or the omitted word 'doctrine'?</p>

<p>Is this the right way? Please explain this function in detail.</p>
","python, tensorflow, nlp, word2vec, gensim","<p>I am wondering if you have seen the documentation of <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer""><code>predict_output_word</code></a>?</p>

<blockquote>
  <p>Report the probability distribution of the center word given the
  context words as input to the trained model.</p>
</blockquote>

<p>To answer your specific question about the word 'doctrine' - it strongly depends if for the words you listed as your context one of the 10 most probable words is 'doctrine'. This means that it must occur relatively frequently in the corpus you use for training of the model. Also, since 'doctrine' does not seem to be one of the very often used words there is a high chance other words will have a higher probability of appearing in the context. Therefore, if you base only on the returned probability of the words given the context you may end up failing to predict 'doctrine' in this case.</p>
",1,3,5876,2018-06-29 16:10:47,https://stackoverflow.com/questions/51105753/gensim-function-predict-output-words
Computing top n word pair co-occurrences from document term matrix,"<p>I used gensim to create a bag of words model. Although it is much longer in reality, here is the format outputted when creating a bag of words document-term matrix on the tokenized texts using Gensim:</p>

<pre><code>id2word = corpora.Dictionary(texts)
corpus = [id2word.doc2bow(text) for text in texts]

[[(0, 2),
  (1, 1),
  (2, 1),
  (3, 1),
  (4, 11),
  (385, 1),
  (386, 2),
  (387, 3),
  (388, 1),
  (389, 1),
  (390, 1)],
 [(4, 31),
  (8, 2),
  (13, 2),
  (16, 2),
  (17, 2),
  (26, 1),
  (28, 4),
  (29, 1),
  (30, 1)]]
</code></pre>

<p>This is a sparse matrix representation, and from what I understand other libraries represent the document-term matrix in a similar fashion as well. If the document-term matrix is non-sparse (meaning the zero entries are there as well), I know that I just have to (A.T*A), since A is of dimension (num. of documents by num. of terms), so multiplying the two will give the term co-occurrences. Ultimately, I want to get the top n co-occurrences (so get the top n term pairs that occur together in the same texts). How would I achieve this? I am not attached to Gensim for creating the BOW model. If another library like sklearn can do it more easily, I am very open. I would appreciate any advice/help/code with this problem -- thanks!</p>
","python, matrix, scikit-learn, gensim, text-analysis","<p>Edit: Here is how you can achieve the matrix multiplication you asked about. Disclaimer: This might not be feasible for a very large corpus.</p>

<p>Sklearn:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer

Doc1 = 'Wimbledon is one of the four Grand Slam tennis tournaments, the others being the Australian Open, the French Open and the US Open.'
Doc2 = 'Since the Australian Open shifted to hardcourt in 1988, Wimbledon is the only major still played on grass'
docs = [Doc1, Doc2]

# Instantiate CountVectorizer and apply it to docs
cv = CountVectorizer()
doc_cv = cv.fit_transform(docs)

# Display tokens
cv.get_feature_names()

# Display tokens (dict keys) and their numerical encoding (dict values)
cv.vocabulary_

# Matrix multiplication of the term matrix
token_mat = doc_cv.toarray().T @ doc_cv.toarray()
</code></pre>

<p>Gensim:</p>

<pre><code>import gensim as gs
import numpy as np

cp = [[(0, 2),
  (1, 1),
  (2, 1),
  (3, 1),
  (4, 11),
  (7, 1),
  (11, 2),
  (13, 3),
  (22, 1),
  (26, 1),
  (30, 1)],
 [(4, 31),
  (8, 2),
  (13, 2),
  (16, 2),
  (17, 2),
  (26, 1),
  (28, 4),
  (29, 1),
  (30, 1)]]

# Convert to a dense matrix and perform the matrix multiplication
mat_1 = gs.matutils.sparse2full(cp[0], max(cp[0])[0]+1).reshape(1, -1)
mat_2 = gs.matutils.sparse2full(cp[1], max(cp[0])[0]+1).reshape(1, -1)
mat = np.append(mat_1, mat_2, axis=0)
mat_product = mat.T @ mat
</code></pre>

<p>For words that appear consecutively, you could prepare a list of bigrams for a set of documents and then use python's Counter to count the bigram occurrences. Here is an example using nltk.</p>

<pre><code>import nltk
from nltk.util import ngrams
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from collections import Counter

stop_words = set(stopwords.words('english'))

# Get the tokens from the built-in collection of presidential inaugural speeches
tokens = nltk.corpus.inaugural.words()

# Futher text preprocessing
tokens = [t.lower() for t in tokens if t not in stop_words]
word_l = WordNetLemmatizer()
tokens = [word_l.lemmatize(t) for t in tokens if t.isalpha()]

# Create bigram list and count bigrams
bi_grams = list(ngrams(tokens, 2)) 
counter = Counter(bi_grams)

# Show the most common bigrams
counter.most_common(5)
Out[36]: 
[(('united', 'state'), 153),
 (('fellow', 'citizen'), 116),
 (('let', 'u'), 99),
 (('i', 'shall'), 96),
 (('american', 'people'), 40)]

# Query the occurrence of a specific bigram
counter[('great', 'people')]
Out[37]: 7
</code></pre>
",2,1,1169,2018-07-03 17:47:44,https://stackoverflow.com/questions/51160354/computing-top-n-word-pair-co-occurrences-from-document-term-matrix
How to I get the similiarity between a word to a document in gensim,"<p>So I have started to learn gensim for both word2vec and doc2vec and it works. The similarity scores actually work really well. For an experiment, however, I wanted to optimize a key word based search algorithm by comparing a single word and getting how similar it is to a piece of text. </p>

<p>What is the best way to do this? I considered averaging the the word vectors of all words in the text (maybe remove fill and stop word first) and and comparing this to the search word? But this really is just intuition, what would be the best way to do this?</p>
","python, search, gensim, word2vec, doc2vec","<p>Averaging all the word-vectors of a longer text is one crude but somewhat effective way to get a single vector for the full text. The resulting vector might then be usefully comparable to single word-vectors. </p>

<p>The <code>Doc2Vec</code> modes that train word-vectors into the same 'space' as the doc-vectors – PV-DM (<code>dm=1</code>), or PV-DBOW if word-training is added (<code>dm=0, dbow_words=1</code>) – could be considered. The doc-vectors closest to a single word-vector might work for your purposes. </p>

<p>Another technique for calculating a 'closeness' of two sets-of-word-vectors is ""Word Mover's Distance"" ('WMD'). It's more expensive to calculate than those techniques that reduce a text to a single vector, because it's essentially considering many possible cost-minimizing ways of correlating the sets-of-vectors. I'm not sure how well it works in the degenerate case of one 'text' being just a single word (or very short phrase), but it could be worth trying. (The method <code>wmd_distance()</code> in gensim offers this.)</p>

<p>I've also seen mention of another calculation, called 'Soft Cosine Similarity', that may be more efficient that WMD but offer similar benefits. It's also now available in gensim; there's a <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb"" rel=""nofollow noreferrer"">Jupyter notebook intro tutorial as well</a>. </p>
",1,0,321,2018-07-04 21:18:46,https://stackoverflow.com/questions/51180848/how-to-i-get-the-similiarity-between-a-word-to-a-document-in-gensim
word2vec gensim multiple languages,"<p>This problem is going completely over my head. I am training a Word2Vec model using gensim. I have provided data in multiple languages i.e. English and Hindi. When I am trying to find the words closest to 'man', this is what I am getting:</p>

<pre><code>model.wv.most_similar(positive = ['man'])
Out[14]: 
[('woman', 0.7380284070968628),
 ('lady', 0.6933152675628662),
 ('monk', 0.6662989258766174),
 ('guy', 0.6513140201568604),
 ('soldier', 0.6491742134094238),
 ('priest', 0.6440571546554565),
 ('farmer', 0.6366012692451477),
 ('sailor', 0.6297377943992615),
 ('knight', 0.6290514469146729),
 ('person', 0.6288090944290161)]
--------------------------------------------
</code></pre>

<p>Problem is, these are all English words. Then I tried to find similarity between same meaning Hindi and English words, </p>

<pre><code>model.similarity('man', 'आदमी')
__main__:1: DeprecationWarning: Call to deprecated `similarity` (Method will 
be removed in 4.0.0, use self.wv.similarity() instead).
Out[13]: 0.078265618974427215
</code></pre>

<p>This accuracy should have been better than all the other accuracies. The Hindi corpus I have has been made by translating the English one. Hence the words appear in similar contexts. Hence they should be close.</p>

<p>This is what I am doing here:</p>

<pre><code>#Combining all the words together.
all_reviews=HindiWordsList + EnglishWordsList

#Training FastText model
cpu_count=multiprocessing.cpu_count()
model=Word2Vec(size=300,window=5,min_count=1,alpha=0.025,workers=cpu_count,max_vocab_size=None,negative=10)
model.build_vocab(all_reviews)
model.train(all_reviews,total_examples=model.corpus_count,epochs=model.iter)
model.save(""word2vec_combined_50.bin"")
</code></pre>
","python, nlp, artificial-intelligence, word2vec, gensim","<p>First of all, you should really use self.wv.similarity().</p>

<p>I'm assuming there are very close to no words that exist in both between your Hindi corpus and English corpus, since Hindi corpus is in Devanagari and English is in, well, English. Simply adding two corpuses together to make a model does not make sense. Corresponding words in the two languages co-occur in two versions of a document, but not in your word embeddings for Word2Vec to figure out most similar.</p>

<p>Eg. Until your model knows that </p>

<p>Man:Aadmi::Woman:Aurat, </p>

<p>from the word embeddings, it can never make out the </p>

<p>Raja:King::Rani:Queen </p>

<p>relation. And for that, you need <em>some</em> anchor between the two corpuses.
Here are a few suggestions that you can try out:</p>

<ol>
<li>Make an independent Hindi corpus/model</li>
<li>Maintain and lookup data of a few English->Hindi word pairs that you have will have to create manually.</li>
<li>Randomly replace input document words with their counterparts from the corresponding document while training</li>
</ol>

<p>These might be enough to give you an idea. You can also look into <a href=""https://github.com/google/seq2seq"" rel=""nofollow noreferrer"">seq2seq</a> if you want only want to do translations. You can also read the <a href=""https://www.learndatasci.com/tutorials/intro-to-word-embeddings-problems-theory/"" rel=""nofollow noreferrer"">Word2Vec theory</a> in detail to understand what it does.</p>
",3,4,7610,2018-07-08 15:46:00,https://stackoverflow.com/questions/51233632/word2vec-gensim-multiple-languages
What does epochs mean in Doc2Vec and train when I have to manually run the iteration?,"<p>I am trying to understand the <code>epochs</code> parameter in the <code>Doc2Vec</code> function and <code>epochs</code> parameter in the <code>train</code> function. </p>

<p>In the following code snippet, I manually set up a loop of 4000 iterations. Is it required or passing 4000 as epochs parameter in the Doc2Vec enough? Also how <code>epochs</code> in <code>Doc2Vec</code> is different from epochs in <code>train</code>?</p>

<pre><code>documents = Documents(train_set)

model = Doc2Vec(vector_size=100, dbow_words=1, dm=0, epochs=4000,  window=5,
                seed=1337, min_count=5, workers=4, alpha=0.001, min_alpha=0.025)

model.build_vocab(documents)

for epoch in range(model.epochs):
    print(""epoch ""+str(epoch))
    model.train(documents, total_examples=total_length, epochs=1)
    ckpnt = model_name+""_epoch_""+str(epoch)
    model.save(ckpnt)
    print(""Saving {}"".format(ckpnt))
</code></pre>

<p>Also, how and when are the weights updated?</p>
","python, gensim, doc2vec","<p>You don't have to manually run the iteration, and you <em>shouldn't</em> call <code>train()</code> more than once unless you're an expert who needs to do so for very specific reasons. If you've seen this technique in some online example you're copying, that example is likely outdated and misleading. </p>

<p>Call <code>train()</code> once, with your preferred number of passes as the <code>epochs</code> parameter. </p>

<p>Also, don't use a starting <code>alpha</code> learning-rate that is low (<code>0.001</code>) that then rises to a <code>min_alpha</code> value 25 times larger (<code>0.025</code>) - that's not how this is supposed to work, and most users shouldn't need to adjust the <code>alpha</code>-related defaults at all. (Again, if you're getting this from an online example somewhere - that's a bad example. Let them know they're giving bad advice.)</p>

<p>Also, 4000 training epochs is absurdly large. A value of 10-20 is common in published work, when dealing with tens-of-thousands to millions of documents. If your dataset is smaller, it may not work well with <code>Doc2Vec</code>, but sometimes more epochs (or smaller <code>vector_size</code>) can still learn something generalizable from tiny data - but still expect to use closer to dozens of epochs (not thousands). </p>

<p>A good intro (albeit with a tiny dataset that barely works with <code>Doc2Vec</code>) is the <code>doc2vec-lee.ipynb</code> Jupyter notebook that's bundled with gensim, and also viewable online at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a></p>

<p>Good luck!</p>
",12,8,7260,2018-07-09 12:32:10,https://stackoverflow.com/questions/51245689/what-does-epochs-mean-in-doc2vec-and-train-when-i-have-to-manually-run-the-itera
Doc2Vec gensim with supervised data predefined labels,"<p>I am trying to use gensim's doc2vec to create a model which will be trained on a set of documents and a set of labels. The labels were created manually and need to be put into the program to be trained on. So far I have 2 lists: a list of sentences, and a list of labels corresponding to that sentence. I need to use doc2vec specifically. Here is what I have tried so far.</p>

<pre><code>from gensim import utils
from gensim.models import Doc2Vec

tweets = [""A tweet"", ""Another tweet"", ""A third tweet"", ... , ""A thousandth-something tweet""]
labels_list = [1, 1, 3, ... , 16]

tagged_data = [tweets, labels_list]
model = Doc2Vec(size=20, alpha=0.025, min_alpha=0.00025, min_count=1, dm=1)
model.build_vocab(tagged_data)
for epoch in range(max_epochs):
    model.train(tagged_data, total_examples=model.corpus_count, 
epochs=model.iter)
    model.alpha -= 0.0002
    model.min_alpha = model.alpha
</code></pre>

<p>I am getting the error on the line with <code>model.build_vocab(tagged_data)</code> that there is an <code>AttributeError: 'list' object has no attribute 'words'</code>. I googled this and it says to put it into a labeled sentence object, but I am not sure if that will work if I have predefined labels. So does anyone know how to put pre-defined labels into doc2vec? Thanks in advance.</p>
","python, gensim, supervised-learning, doc2vec","<p>The corpus for <code>Doc2Vec</code> should be an <em>iterable</em> of objects that are similar to the <code>TaggedDocument</code> example class included with gensim: with a <code>words</code> list-of-string-tokens, and a <code>tags</code> list-of-tags. (Tags are the keys to the doc-vectors that are learned by training from each text, and are most often unique document IDs, but can also be known labels that repeat over multiple documents, or both IDs and labels.)</p>

<p>Your <code>tagged_data</code>, with one list of non-tokenized string, and one list of labels, is not at all like its expected format.</p>

<p>You should look at, and work through, some of the example Jupyter notebooks about <code>Doc2Vec</code> in the gensim <code>docs/notebooks</code> directory, such as <code>doc2vec-lee.ipynb</code> or <code>doc2vec-IMDB.ipynb</code>. These can also be viewed online, for example:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a></p>

<p>Also, you probably don't need to or want to call <code>train()</code> multiple times - it's easy to get wrong. (If you've copied that approach from an online example, that example is likely out-of-date.) Call it once, with your preferred number of training passes in the <code>epochs</code> parameter.</p>
",2,0,1234,2018-07-09 18:57:07,https://stackoverflow.com/questions/51252324/doc2vec-gensim-with-supervised-data-predefined-labels
Gensim n_similarity word not in vocabulary,"<p>I'm attempting to compare a tagged document consisting of a list of words to individual tags from a list of tags.</p>

<p>My code is as follows:</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
from gensim import similarities,corpora,models
import Load

documents = Load.get_doc('docs')

data = Doc2Vec.load('vectorised.model')

print('Data Loading finished')

tags = [['word1'],['word2'],['word3'],['word4'],['word5']]

tag_vectors = []

data.n_similarity(tags[0],documents[1])
</code></pre>

<p>The issue i'm having is running:</p>

<pre><code>data.n_similarity(tags[0],documents[1])
</code></pre>

<p>feeds back KeyError: ""word 'otherword' not in vocabulary</p>

<p>I want to get the similarity between the taggeddocument and the tag itself,
so my question is what do I need to change in my code so it checks correctly and gives back a similarity value?</p>

<p>n.b. I've replaced the actual words here with placeholders</p>
","python, gensim","<p>I think you should check if the ""word""(KeyError one) is in the 'vectorised.model' 
if the model do not have the word 
you can do some incremental training like </p>

<pre><code>model = Doc2Vec.load(your old model)
model.build_vocab(text, update=True) # update your vocab 
model.train 
</code></pre>
",1,1,454,2018-07-10 12:23:27,https://stackoverflow.com/questions/51265111/gensim-n-similarity-word-not-in-vocabulary
"Gensim Doc2vec trained, but not saved","<p>While I trained d2v on a large text corpus I received these 3 files: </p>

<pre><code>doc2vec.model.trainables.syn1neg.npy

doc2vec.model.vocabulary.cum_table.npy

doc2vec.model.wv.vectors.npy
</code></pre>

<p>Bun final model has not saved, because there was not enough free space available on the disk. </p>

<pre><code>OSError: 5516903000 requested and 4427726816 written
</code></pre>

<p>Is there a way to resave my model using these files in a shorter time, than all training time? </p>

<p>Thank you in advance! </p>
","model, save, gensim, word-embedding, doc2vec","<p>If you still have the model in RAM, in an environment (like a Jupyter notebook) where you can run new code, you could try to clear space (or attach a new volume) and then try a <code>.save()</code> again. That is, you don't need to re-train, just re-save what's already in RAM. </p>

<p>There's no routine for saving ""just what isn't already saved"". So even though the subfiles that <em>did</em> save could potentially be valuable if you were desperate to salvage anything from the 1st training run (perhaps via a process like in <a href=""https://stackoverflow.com/questions/51281241/gensim-word2vec-model-trained-but-not-saved"">my <code>Word2Vec</code> answer here</a>, though it's a bit more complicated with <code>Doc2Vec</code>), trying another save to the same place/volume would require getting those existing files out-of-the-way. (Maybe you could transfer them to remote storage in case they'll be needed, but delete them locally to free space?)</p>

<p>If you try to save to a filename that ends "".gz"", gensim will try to save everything compressed, which might help a little. (Unfortunately, the main vector arrays don't compress very well, so this might not be enough savings alone.)</p>

<p>There's no easy way to slim an already-trained model in memory, without potentially destroying some of its capabilities. (There are hard ways, but only if you're sure you can discard things a full model could do... and it's not yet clear you're in that situation.) </p>

<p>The major contributors to model size are the number of unique-words, and the number of unique doc-tags. </p>

<p>Specifying a larger <code>min_count</code> <em>before training</em> will discard more low-frequency words – and very-low-frequency words often just hurt the model anyway, so this trimming often improves three things simultaneously: faster training, smaller model, and higher-quality-results on downstream tasks. </p>

<p>If you're using plain-int doc-tags, the model will require vector space for all doc-tag ints from 0 to your highest number. So even if you trained just 2 documents, if they had plain-int doc-tags of <code>999998</code> and <code>999999</code>, it'd still need to allocate (and save) garbage vectors for 1 million tags, 0 to 999,999. So in some cases people's memory/disk usage is higher than expected because of that – and either using contiguous IDs starting from <code>0</code>, or switching to string-based doc-tags, reduces size a lot. (But, again, this has to be chosen before training.)</p>
",1,0,795,2018-07-17 08:05:51,https://stackoverflow.com/questions/51376241/gensim-doc2vec-trained-but-not-saved
Intel MKL FATAL ERROR: while trying to import gensim package,"<p>We have Anaconda 4.3.1 installed on our hosts and recently we have installed several packages for data science use. All the imports were fine except for gensim.</p>

<p>I am getting ""Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so."" and getting out of python shell.</p>

<p>It sounds like a duplicate but the weird part is, when I import tensorflow or seaborn before importing gensim, I am not getting that error and gensim is being imported. I would also like to know if there is any dependency between these packages. And I do have the latest version of numpy which is 1.14.5. I have looked at various solutions proposed about installing few packages and uninstalling few. I would like to know the reason why we should be doing it before actually doing it.</p>
","python, tensorflow, anaconda, seaborn, gensim","<p>Here is my theory on your question:</p>

<p><strong>Is there any dependency between gensim, tensoflow, seaborn and such packages?</strong>
 When you try to install these packages one by one using conda, you might have already seen conda prompting that some of the dependencies will be DOWNGRADED/UPDATED/INSTALLED. Hence there is dependency between the dependencies of these packages.</p>

<p><strong>Why import error is thrown only on certain cases?</strong>
Looks like a dependency issue. When you try to import gensim, it tries to load certain lib files, which its not able to find. However, when tensorflow or seaborn is imported the mentioned lib files might have already loaded, hence importing gensim did not show an error.</p>

<p><strong>Why installing few packages and uninstalling few, help to solve the problem?</strong>
This might help to have the correct dependencies for the packages to work properly.</p>

<p>Having said that, I tried to recreate the error that you got, however gensim is importing fine for me. If you could give the result of ""conda list"", will try to recreate the problem and would be able to give a better insight.</p>
",2,0,872,2018-07-17 19:14:21,https://stackoverflow.com/questions/51388707/intel-mkl-fatal-error-while-trying-to-import-gensim-package
How to check via callbacks if alpha is decreasing? + How to load all cores during training?,"<p>I'm training doc2vec, and using callbacks trying to see if alpha is decreasing over training time using this code:</p>

<pre><code>class EpochSaver(CallbackAny2Vec):
'''Callback to save model after each epoch.'''

    def __init__(self, path_prefix):
        self.path_prefix = path_prefix
        self.epoch = 0

        os.makedirs(self.path_prefix, exist_ok=True)

    def on_epoch_end(self, model):
        savepath = get_tmpfile(
            '{}_epoch{}.model'.format(self.path_prefix, self.epoch)
        )
        model.save(savepath)
        print(
            ""Model alpha: {}"".format(model.alpha), 
            ""Model min_alpha: {}"".format(model.min_alpha),
            ""Epoch saved: {}"".format(self.epoch + 1),
            ""Start next epoch""
        )
        self.epoch += 1


def train():

    workers = multiprocessing.cpu_count()*4
    model = Doc2Vec(
        DocIter(),
        vec_size=600, alpha=0.03, min_alpha=0.00025, epochs=20,
        min_count=10, dm=1, hs=1, negative=0, workers=workers,
        callbacks=[EpochSaver(""./checkpoints"")]
    )
    print(
        ""HS"", model.hs, ""Negative"", model.negative, ""Epochs"", 
         model.epochs, ""Workers: "", model.workers, ""Model alpha: 
         {}"".format(model.alpha)
    )  
</code></pre>

<p>And while training I see that alpha is not changing over time. On each callback I see alpha = 0.03.<br>
Is it possible to check if alpha is decreasing? Or it really not decreasing at all during training? </p>

<p>One more question: 
How can I benefit from all my cores while training doc2vec?</p>

<p><a href=""https://i.sstatic.net/Bo1uh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Bo1uh.jpg"" alt=""Loading of cores""></a></p>

<p>As we can see, each core is not loaded more than +-30%. </p>
","callback, gensim, multicore, word-embedding, doc2vec","<p>The <code>model.alpha</code> property only holds the initially-configured starting-<code>alpha</code> – it's not updated to the effective learning-rate through training. </p>

<p>So, even if the value is being decreased properly (and I expect that it is), you wouldn't see it in the logging you've added. </p>

<p>Separate observations about your code:</p>

<ul>
<li><p>in gensim versions at least through 3.5.0, maximum training throughput is most often reached with some value for <code>workers</code> between 3 and the number of cores – but usually not the full number of cores (if it's higher than 12) or larger. So <code>workers=multiprocessing.cpu_count()*4</code> is likely going to much slower than what you could achieve with a lower number. </p></li>
<li><p>if your corpus is large enough to support 600-dimensional vectors, and discarding words with fewer than <code>min_count=10</code> examples, negative sampling may work faster <em>and</em> get better results than the <code>hs</code> mode. (The pattern in published work seems to be to prefer negative-sampling with larger corpuses.)</p></li>
</ul>
",1,0,849,2018-07-19 08:45:36,https://stackoverflow.com/questions/51418154/how-to-check-via-callbacks-if-alpha-is-decreasing-how-to-load-all-cores-durin
No module named &#39;gensim.sklearn_api&#39;,"<p>I try to use</p>

<pre><code>from gensim.sklearn_api import W2VTransformer
</code></pre>

<p>and get</p>

<pre><code>ImportError: No module named 'gensim.sklearn_api'
</code></pre>

<p>I used </p>

<pre><code>import gensim
import sklearn
from sklearn.base import BaseEstimator, TransformerMixin
</code></pre>

<p>and get the same.
In <a href=""https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html"" rel=""nofollow noreferrer"">sklearn_api.w2vmodel – Scikit learn wrapper for word2vec model</a> I could find no advice. 
How to install <code>gensim.sklearn_api</code>?</p>
","python, scikit-learn, gensim","<p>@tursunWali</p>
<p>Class names seem to have changed from v1.0.5. Try calling TFIDF and other methods by prefixing them with <code>do_</code>, e.g. <code>hero.do_tfidf</code>.</p>
<p>Here's a minimal example:</p>
<p><code>df['pca']=(df['text'].pipe(hero.clean).pipe(hero.do_tfidf).pipe(hero.do_pca))</code></p>
<p>You can see all the class names in the package source code.</p>
",1,1,3122,2018-07-19 18:53:50,https://stackoverflow.com/questions/51429975/no-module-named-gensim-sklearn-api
How to install gensim without pip (firewall issues),"<p>All-</p>

<p>I would like to use the gensim library, but unfortunately I can't install it via pip due to the company's firewall. Any advice? Thank you in advance for any help or suggestions you can provide. </p>
","python, installation, firewall, gensim","<p>Tedious and only solution then is to download all the dependencies one by one and install them along with gensim.  So, go to pypi and download gensim first and install it, then it might raise errors saying some package is missing or trying to download it.Then, download that specific package and install it via pip by giving path to the downloaded whl or source file</p>
",0,-1,60,2018-07-20 20:22:20,https://stackoverflow.com/questions/51449841/how-to-install-gensim-without-pip-firewall-issues
Attribute mapping using Machine learning,"<p>I have several tables that have different column names which are mapped through ETL. There are a total of around 200 tables and 500 attributes, so the set is not massive.</p>

<p>Some column mappings are as follows:</p>

<pre><code>startDate EFT_DATE
startDate START_DATE
startDate entryDate 
</code></pre>

<p>As you can see the same column name can be mapped to different names across different tables. </p>

<p>I'm trying to solve the following problem :</p>

<p>Given two schemas I want to find matches between attribute names. </p>

<p>I was wondering if there is a way to leverage gensim to solve this problem similar to source-words from Google example. The challenge I'm facing is which dataset to use to train the model. Also I am wondering if there is another approach to solve the problem. </p>
","machine-learning, database-design, gensim","<p>You can apply basic <code>text analyzers</code> to this  by pre-processing each term.</p>

<ul>
<li>Split by any non-alphabetic character.

<ul>
<li>e.g. <code>EFT_DATE</code> becomes <code>[eft,date]</code></li>
</ul></li>
<li>Split by camelCase.

<ul>
<li>e.g. <code>startDate</code> becomes <code>[start,date]</code></li>
</ul></li>
<li>Lowercase each term</li>
<li>Apply a fuzzy dictionary lookup to each token

<ul>
<li>e.g. <code>startt</code> -> <code>start</code> (typo detection..)</li>
</ul></li>
<li>Apply stemming  

<ul>
<li>e.g. <code>starting</code> -> <code>start</code></li>
</ul></li>
<li>Maybe apply a synonym conversion . 

<ul>
<li>e.g.  <code>begin</code> -> <code>start</code></li>
</ul></li>
</ul>

<p>Optionally Sort the terms:</p>

<ul>
<li><code>dateStarted</code> ->  <code>[date,start]</code></li>
<li><code>startingDate</code> ->  <code>[start,date]</code> -> <code>[date,start]</code></li>
</ul>

<p>You can apply <code>set distance</code> operations now - which is <code>O(^2)</code>. Given your moderate cardinality that is fine.  If you had larger set of terms than scalable set-comparison approaches like the following can help reduce the complexity.</p>

<ul>
<li><code>LSH Forests</code>  

<ul>
<li>theory <a href=""http://infolab.stanford.edu/~bawa/Pub/similarity.pdf"" rel=""nofollow noreferrer"">http://infolab.stanford.edu/~bawa/Pub/similarity.pdf</a></li>
<li>python/sklearn  <a href=""http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.neighbors.LSHForest.html"" rel=""nofollow noreferrer"">http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.neighbors.LSHForest.html</a></li>
</ul></li>
<li><code>SimHash</code> / <code>MinHash</code>

<ul>
<li>theory <a href=""https://stackoverflow.com/a/46415603/1056563"">https://stackoverflow.com/a/46415603/1056563</a></li>
<li>python  

<ul>
<li>simhash <a href=""https://github.com/leonsim/simhash"" rel=""nofollow noreferrer"">https://github.com/leonsim/simhash</a>  </li>
<li>minhash/simhash/others <a href=""https://github.com/ekzhu/datasketch"" rel=""nofollow noreferrer"">https://github.com/ekzhu/datasketch</a></li>
</ul></li>
</ul></li>
</ul>
",3,0,628,2018-07-21 15:29:33,https://stackoverflow.com/questions/51457515/attribute-mapping-using-machine-learning
word2vec for dictionary of words,"<p>I need to generate word2vec array for a dictionary of words. The dictionary looks something like this </p>

<pre><code>test={0: 'tench, Tinca tinca',
 1: 'goldfish, Carassius auratus',
 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',
 3: 'tiger shark, Galeocerdo cuvieri',
 4: 'hammerhead, hammerhead shark'}
</code></pre>

<p>The loop should go through each line, check if the word exists in the model, if yes then store the vector in an array otherwise check the next word in the line. If none of the words are present in the gensim model, then it should do nothing (array is initialised with zeros)
However if a word doesn't exist in the pre trained model, then it raises this exception:</p>

<blockquote>
  <p>KeyError: ""word 'Galeocerdo cuvieri' not in vocabulary""</p>
</blockquote>

<p>What should be the ideal loop that also has the exception in order to bypass the error raised?
This is my starting code:</p>

<pre><code> import gensim
 model = gensim.models.KeyedVectors.load_word2vec_format('/home/shikhar /Downloads/GoogleNews-vectors-negative300.bin',binary=True) 
 array=np.zeros((4,300)) 
 for i in test:
     synonyms=test[i].split(',')
</code></pre>
","python, nlp, gensim, word2vec","<p>why don't try this</p>

<pre><code>vectors= list()
for i in test:
    flag=True
    synonyms=test[i].split(',')
    for k in synonyms:
        try:
            vectors.append(model[k]])
            flag = False
            break
        except KeyError as e:
            print(e)
            continue
    if flag:
        vectors.append(# Insert your array with zeroes here)               
</code></pre>

<p>I'm assuming that you need all the vectors in a list</p>
",1,2,1586,2018-07-25 08:57:25,https://stackoverflow.com/questions/51514825/word2vec-for-dictionary-of-words
Why is TFIDF seen as a model in gensim?,"<p>I am familiar with the tfidf vectorizer.</p>

<p>However, in gensim it seems like tfidf is treated as a model on itself, just like LDA, LSI and others.</p>

<p>Why is this the case? Can't tfidf not just be used to vectorize and then to input in an LDA model for example?</p>

<p>Link to documentation: <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/tut2.html</a></p>
","python, gensim","<p>TFIDF is not a static transformation.</p>

<p>The term frequencies need to be learned and stored (i.e. it is a model).</p>

<p>This means that you could learn the term frequencies with one corpus and transform another with it, so by making it a model in Gensim, it can be reused for multiple use cases.</p>

<p><a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html"" rel=""nofollow noreferrer"">Gensim TFIDF</a></p>

<p><a href=""https://rare-technologies.com/pivoted-document-length-normalisation/"" rel=""nofollow noreferrer"">Example on TFIDF</a></p>
",1,1,221,2018-07-25 13:23:54,https://stackoverflow.com/questions/51520031/why-is-tfidf-seen-as-a-model-in-gensim
gensim word2vec - update model data,"<p>I have an issue similar to the one discussed here - <a href=""https://stackoverflow.com/questions/40727093/gensim-word2vec-updating-word-embeddings-with-newcoming-data"">gensim word2vec - updating word embeddings with newcoming data</a></p>

<p>I have the following code that saves a model as <strong>text8_gensim.bin</strong></p>

<pre><code>sentences = word2vec.Text8Corpus('train/text8')
model = word2vec.Word2Vec(sentences, size=200, workers=12, min_count=5,sg=0, window=8, iter=15, sample=1e-4,alpha=0.05,cbow_mean=1,negative=25)
model.save(""./savedModel/text8_gensim.bin"")
</code></pre>

<p>Here is the code that adds more data to the saved model (after loading it)</p>

<pre><code>fname=""savedModel/text8_gensim.bin""
model = word2vec.Word2Vec.load(fname)
model.epochs=15

#Custom words
docs = [""start date"", ""end date"", ""eft date"",""termination date""]
model.build_vocab(docs, update=True)
model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)
model.wv.similarity('start','eft')
</code></pre>

<p>The model loads fine; however when I try to call <strong>model.wv.similarity</strong> function I get the following error</p>

<p><strong>KeyError: ""word 'eft' not in vocabulary""</strong></p>

<p>Am I missing something here?</p>
","gensim, word2vec, word-embedding","<p>Those <code>docs</code> aren't in the right format: each text should be a list-of-string-tokens, not a string. </p>

<p>And, the same <code>min_count</code> threshold will apply to incremental updates: words less frequent that that threshold will be ignored. (Since a <code>min_count</code> higher than 1 is almost always a good idea, a word that appears only once in any update will never be added to the model.)</p>

<p>Incrementally adding words introduces lots of murky issue with unclear proper choices with regard to model quality, balancing the effects of early-vs-late training, management of the <code>alpha</code> learning-rate, and so forth. It won't necessarily improve your model; with the wrong choices it could make it worse, by adjusting some words with your new texts in ways that move them out-of-compatible-alignment with earlier-batch-only words.</p>

<p>So be careful and always check with a repeatable automated quantitative quality check that your changes are helping. (The safest approach is to retrain with old and new texts in one combined corpus, so that all words get trained against one another equally on all data.)</p>
",2,1,1267,2018-07-26 20:49:50,https://stackoverflow.com/questions/51547315/gensim-word2vec-update-model-data
Gensim tagging documents with big numbers,"<p>I want to label my documents with tags mapped to id attribute in database.
The ids can be for example also like this:</p>

<p>documents[0] is for example</p>

<pre><code>TaggedDocument(words=['blabla', 'request'], tags=[225616076])
</code></pre>

<p>For some reason, it is not able to build_vocabulary. Although I have only 33382 unique ids/tags with higher values, it does not matter, gensim writes that I have '225616077 tags' (in the log).  </p>

<pre><code>2018-07-30 12:07:59,271 : INFO : collecting all words and their counts
2018-07-30 12:07:59,273 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2018-07-30 12:07:59,330 : INFO : PROGRESS: at example #1000, processed 7974 words (314086/s), 1975 word types, 225616077 tags
2018-07-30 12:07:59,343 : INFO : PROGRESS: at example #2000, processed 15882 words (701054/s), 2794 word types, 225616077 tags
...

...  
2018-07-30 12:14:56,454 : INFO : estimated required memory for 6765 words and 20 dimensions: 19793760900 bytes
2018-07-30 12:14:56,457 : INFO : resetting layer weights

---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
in &lt;module&gt;()
----&gt; 1 model.build_vocab(documents)
</code></pre>

<p>How can I solve this problem? I do not want to start from 0 and then map it to the higher numbers (uselessly used compute time). I also tried it to tag it as strings (so the documents[0] is TaggedDocument(words=['blabla', 'request'], tags=['225616076'])) but it does not work either.</p>

<p>I am inspecting gensim's code but can not get to solution on my own.</p>
","python, gensim, topic-modeling, doc2vec","<p>If you are using plain python <code>int</code> values as doc-tags, then the code assumes you want these to also be the raw int indexes into the underlying vector-array – and a vector-array large enough to hold your largest index will be allocated – even if many lower numbers go unused. </p>

<p>This is an optimization to allow the code to avoid building the usual tag-to-index mapping, for those people who have neatly identified texts, numbered from 0 up.</p>

<p>If your IDs aren't contiguous starting from 0, and can't easily be made to work that way, you can use string tags, which the code will recognize need to be mapped to unique index positions - and only a vector-array exactly the right size will be allocated. </p>

<p>For example, your <code>documents[0]</code> would then be:</p>

<pre><code>TaggedDocument(words=['blabla', 'request'], tags=[str(225616076)])
</code></pre>
",2,0,222,2018-07-30 12:27:15,https://stackoverflow.com/questions/51593971/gensim-tagging-documents-with-big-numbers
How I can extract matrixes WI and WO from gensim word2vec?,"<p>CBOW word2vec scheme look like this:</p>

<p><a href=""https://i.sstatic.net/gmgo6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gmgo6.png"" alt=""enter image description here""></a></p>

<p>How I can extract matrixes WI and WO from <code>gensim.models.word2vec.Word2Vec</code>?
I found only these fields in gensim w2v model:</p>

<p><code>gensim.models.word2vec.Word2Vec.trainables.syn1neg</code></p>

<p>and</p>

<p><code>gensim.models.word2vec.Word2Vec.vw.syn1neg.vectors</code></p>

<p>Can I make an assumption that <code>syn1neg</code> is WI, and WO = <code>vectors</code> - <code>syn1neg</code>?</p>

<p>Why this code</p>

<pre><code>sentences = [['car', 'tree', 'chip2'], ['chip1', 'sugar']]
model = Word2Vec(sentences, min_count=1, size = 5)
</code></pre>

<p>give <code>Word2Vec.trainables.syn1neg</code> matrix with zero elements only?</p>

<p>For 30MB dataset <code>Word2Vec.trainables.syn1neg</code> matrix also contain zero elements only, log is here:</p>

<p><a href=""https://pastebin.com/cKfxv2zz"" rel=""nofollow noreferrer"">gensim log</a></p>
","python, gensim, word2vec","<p>The <code>w2v_model.wv.vectors</code> is what was formerly called ""syn0"", and serves as the ""projection weights"" which essentially map a one-hot word-encoding into <em>N</em> dimensions. In your diagram, that's <em>WI</em>.</p>

<p>The <code>w2v_model.trainables.syn1neg</code> is the hidden-to-output weights for negative-sampling mode, what your diagram labels <em>WO</em>.</p>
",1,3,615,2018-07-30 12:38:19,https://stackoverflow.com/questions/51594165/how-i-can-extract-matrixes-wi-and-wo-from-gensim-word2vec
Adding additional words in word2vec or Glove (maybe using gensim),"<p>I have two pretrained word embeddings: <code>Glove.840b.300.txt</code> and <code>custom_glove.300.txt</code></p>

<p>One is pretrained by Stanford and the other is trained by me.
Both have different sets of vocabulary. To reduce oov, I'd like to add words that don't appear in file1 but do appear in file2 to file1.
How do I do that easily?</p>

<p>This is how I load and save the files in gensim 3.4.0.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

model = KeyedVectors.load_word2vec_format('path/to/thefile')
model.save_word2vec_format('path/to/GoogleNews-vectors-negative300.txt', binary=False)
</code></pre>
","nlp, gensim, word2vec, glove","<p>I don't know an <em>easy</em> way. </p>

<p>In particular, word-vectors that weren't co-trained together won't have compatible/comparable coordinate-spaces. (There's no one right place for a word – just a relatively-good place compared to the other words that are in the same model.)</p>

<p>So, you can't just append the missing words from another model: you'd need to transform them into compatible locations. Fortunately, it seems to work to use some set of shared anchor-words, present in both word-vector-sets, to learn a transformation – then apply that the words you want to move over.</p>

<p>There's a class, <code>[TranslationMatrix][1]</code>, and <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb"" rel=""nofollow noreferrer"">demo notebook</a> in gensim showing this process for language-translation (an application mentioned in the original word2vec papers). You could concievably use this, combined with the ability to <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.add"" rel=""nofollow noreferrer"">append extra vectors to a gensim <code>KeyedVectors</code></a> instance, to create a new set of vectors with a superset of the words in either of your source models.</p>
",4,2,2579,2018-07-30 20:52:57,https://stackoverflow.com/questions/51602111/adding-additional-words-in-word2vec-or-glove-maybe-using-gensim
Sharing memory for gensim&#39;s KeyedVectors objects between docker containers,"<p>Following <a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">related question solution</a> I created docker container which loads GoogleNews-vectors-negative300 KeyedVector inside docker container and load it all to memory</p>

<pre><code>KeyedVectors.load(model_path, mmap='r')
word_vectors.most_similar('stuff')
</code></pre>

<p>Also I have another Docker container which provides REST API which loads this model with </p>

<pre><code>KeyedVectors.load(model_path, mmap='r')
</code></pre>

<p>And I observe that fully loaded container takes more than 5GB of memory and each gunicorn worker takes 1.7 GB of memory.</p>

<pre><code>CONTAINER ID        NAME                        CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS
acbfd080ab50        vectorizer_model_loader_1   0.00%               5.141GiB / 15.55GiB   33.07%              24.9kB / 0B         32.9MB / 0B         15
1a9ad3dfdb8d        vectorizer_vectorizer_1     0.94%               1.771GiB / 15.55GiB   11.39%              26.6kB / 0B         277MB / 0B          17
</code></pre>

<p>However, I expect that all this processes share same memory for KeyedVector, so it only takes 5.4 GB shared between all containers.</p>

<p>Have someone tried to achieve that and succeed?</p>

<p>edit: 
I tried following code snippet and it indeed share same memory across different containers.</p>

<pre><code>import mmap
from threading import Semaphore

with open(""data/GoogleNews-vectors-negative300.bin"", ""rb"") as f:
    # memory-map the file, size 0 means whole file
    fileno = f.fileno()
    mm = mmap.mmap(fileno, 0, access=mmap.ACCESS_READ)
    # read whole content
    mm.read()
    Semaphore(0).acquire()
    # close the map
    mm.close()
</code></pre>

<p>So the problem that <code>KeyedVectors.load(model_path, mmap='r')</code> don't share memory</p>

<p>edit2:
Studying gensim's source code I see that <code>np.load(subname(fname, attrib), mmap_mode=mmap)</code> is called to open memmaped file. Following code sample shares memory across multiple container.</p>

<pre><code>from threading import Semaphore

import numpy as np

data = np.load('data/native_format.bin.vectors.npy', mmap_mode='r')
print(data.shape)
# load whole file to memory
print(data.mean())
Semaphore(0).acquire()
</code></pre>
","python, mmap, gensim, word2vec","<p>After extensive debugging I figured out that mmap works as expected for numpy arrays in <code>KeyedVectors</code> object.</p>

<p>However, KeyedVectors have other attributes like <code>self.vocab</code>, <code>self.index2word</code> and <code>self.index2entity</code> which are not shared and consumes ~1.7 GB of memory for each object.</p>
",3,1,1055,2018-07-31 14:41:18,https://stackoverflow.com/questions/51616074/sharing-memory-for-gensims-keyedvectors-objects-between-docker-containers
Gensim: calling docvecs.most_similar yields error,"<p>When I call <code>docvecs.most_similar</code> on a document, I am getting the error <code>AttributeError: 'list' object has no attribute 'shape'</code>:  </p>

<pre><code># load model from file
from gensim.models.doc2vec import Doc2Vec
model_doc2vec = Doc2Vec.load(""/path_to_file/doc2vec.bin"")

# attempt to get most similar documents from docvec
tokens = ""in space"".split()
new_vector = model_doc2vec.infer_vector(tokens)
sims = model_doc2vec.docvecs.most_similar( positive=[new_vector], topn=10 )
</code></pre>

<p>which yields <code>AttributeError: 'list' object has no attribute 'shape'</code>.</p>

<p>I have a hunch this may have to do with numpy and gensim version compatibility.  I am using Python 3.6, numpy 1.14, and gensim 1.0.1.</p>

<p>Full error:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-37-220db2331e84&gt; in &lt;module&gt;()
----&gt; 1 sims = model_doc2vec.docvecs.most_similar( positive=[new_vector], topn=10 )

~/doc2vec.py in most_similar(self, positive, negative, topn, clip_start, clip_end, indexer)
    436         there was chosen to be significant, such as more popular tag IDs in lower indexes.)
    437         """"""
--&gt; 438         self.init_sims()
    439         clip_end = clip_end or len(self.doctag_syn0norm)
    440 

~/doc2vec.py in init_sims(self, replace)
    419                         mode='w+', shape=self.doctag_syn0.shape)
    420                 else:
--&gt; 421                     self.doctag_syn0norm = empty(self.doctag_syn0.shape, dtype=REAL)
    422                 np_divide(self.doctag_syn0, sqrt((self.doctag_syn0 ** 2).sum(-1))[..., newaxis], self.doctag_syn0norm)
    423 

AttributeError: 'list' object has no attribute 'shape'
</code></pre>
","python, numpy, gensim","<p><a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">RTFD</a>:</p>

<blockquote>
  <p>delete_temporary_training_data(keep_doctags_vectors=True,
  keep_inference=True) <br>
  Discard parameters that are used in training and
  score. Use if you’re sure you’re done training a model.</p>
  
  <p>Parameters:    keep_doctags_vectors (bool, optional) – Set to False if
  you don’t want to save doctags vectors. <strong>In this case you will not be
  able to use most_similar()</strong>, similarity(), etc methods. keep_inference
  (bool, optional) – Set to False if you don’t want to store parameters
  that are used for infer_vector() method.</p>
</blockquote>
",0,0,499,2018-08-03 20:59:43,https://stackoverflow.com/questions/51680023/gensim-calling-docvecs-most-similar-yields-error
Word2vec - get rank of similarity,"<p>Given I got a word2vec model (by gensim), I want to get the rank similarity between to words.
For example, let's say I have the word ""desk"" and the most similar words to ""desk"" are:</p>

<blockquote>
  <ol>
  <li>table 0.64</li>
  <li>chair 0.61</li>
  <li>book 0.59</li>
  <li>pencil 0.52</li>
  </ol>
</blockquote>

<p>I want to create a function such that:</p>

<blockquote>
  <p>f(desk,book) = 3
  Since book is the 3rd most similar word to desk.
  Does it exists? what is the most efficient way to do this?</p>
</blockquote>
","python, python-3.x, nlp, gensim, word2vec","<p>You can use the <code>rank(entity1, entity2)</code> to get the distance - same as the index.</p>

<pre><code>model.wv.rank(sample_word, most_similar_word)
</code></pre>

<hr>

<p>A separate function as given below won't be necessary here. Keeping it for information sake.</p>

<p>Assuming you have the list of words and their vectors in a list of tuples, returned by <code>model.wv.most_similar(sample_word)</code> as shown</p>

<pre><code>[('table', 0.64), ('chair', 0.61), ('book', 0.59), ('pencil', 0.52)]
</code></pre>

<p>The following function accepts the sample word and the most similar word as params, and returns the index or rank (eg. [2]) if it's present in the output</p>

<pre><code>def rank_of_most_similar_word(sample_word, most_similar_word):
    l = model.wv.most_similar(sample_word)
    return [x+1 for x, y in enumerate(l) if y[0] == most_similar_word]

sample_word = 'desk'
most_similar_word = 'book'
rank_of_most_similar_word(sample_word, most_similar_word)
</code></pre>

<p>Note: use <code>topn=x</code> to get the top x most similar words while using <code>model.wv.most_similar()</code>, as suggested in the comments.</p>
",2,3,2021,2018-08-08 13:09:31,https://stackoverflow.com/questions/51747613/word2vec-get-rank-of-similarity
DeprecationWarning in Gensim `most_similar`?,"<p>While implementating Word2Vec in Python 3.7, I am facing an unexpected scenario related to depreciation. My question is what exactly is the depreciation warning with respect to 'most_similar' in word2vec gensim python?</p>
<p>Currently, I am getting the following issue.</p>
<p><strong>DeprecationWarning: Call to deprecated <code>most_similar</code> (Method will be removed in 4.0.0, use self.wv.most_similar() instead).
model.most_similar('hamlet')
FutureWarning: Conversion of the second argument of issubdtype from <code>int</code> to <code>np.signedinteger</code> is deprecated. In future, it will be treated as <code>np.int32 == np.dtype(int).type</code>.
if np.issubdtype(vec.dtype, np.int):</strong></p>
<p>Please help to curb this issue? Any help is appreciated.</p>
<p>The code what, I have tried is as follows.</p>
<pre><code>import re
from gensim.models import Word2Vec
from nltk.corpus import gutenberg

sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   
print('Type of corpus: ', type(sentences))
print('Length of corpus: ', len(sentences))

for i in range(len(sentences)):
    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]
print(sentences[0])    # title, author, and year
print(sentences[1])
print(sentences[10])
model = Word2Vec(sentences=sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
model.init_sims(replace = True)
model.save('word2vec_model')
model = Word2Vec.load('word2vec_model')
model.most_similar('hamlet')
</code></pre>
","python, python-3.x, gensim, word2vec","<p>It's a warning  which that it's about to become obsolete and non-functional.</p>

<blockquote>
  <p>Usually things are deprecated for a few versions giving anyone using them enough time to move to the new method before they are removed.</p>
</blockquote>

<p>They've moved <code>most_similar</code> to <a href=""https://radimrehurek.com/gensim/models/deprecated/word2vec.html"" rel=""noreferrer""><code>wv</code></a></p>

<p>So <code>most_simliar()</code> should look something like:</p>

<pre><code>model.wv.most_similar('hamlet')
</code></pre>

<p><a href=""http://pydoc.net/gensim/3.2.0/gensim.models.word2vec/"" rel=""noreferrer"">src ref</a></p>

<p>Hope this helps</p>

<p>Edit : using <code>wv.most_similar()</code></p>

<pre><code>import re
from gensim.models import Word2Vec
from nltk.corpus import gutenberg

sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   
print('Type of corpus: ', type(sentences))
print('Length of corpus: ', len(sentences))

for i in range(len(sentences)):
    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]
print(sentences[0])    # title, author, and year
print(sentences[1])
print(sentences[10])
model = Word2Vec(sentences=sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
model.init_sims(replace = True)
model.save('word2vec_model')
model = Word2Vec.load('word2vec_model')
similarities = model.wv.most_similar('hamlet')
for word , score in similarities:
    print(word , score)
</code></pre>
",7,4,9925,2018-08-10 18:11:40,https://stackoverflow.com/questions/51791964/deprecationwarning-in-gensim-most-similar
word2vec - find a word by a specific vector,"<p>I trained a gensim Word2Vec model.
Let's say I have a certain vector and I want the find the word it represents - what is the best way to do so?</p>

<p>Meaning, for a specific vector:</p>

<pre><code>vec = array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)
</code></pre>

<p>I want to get a word:</p>

<pre><code> 'computer' = model.vec2word(vec)
</code></pre>
","python-3.x, nlp, gensim, word2vec","<p>Word-vectors are generated through an iterative, approximative process – so shouldn't be thought of as precisely right (even though they do have exact coordinates), just ""useful within certain tolerances"".</p>

<p>So, there's no lookup of exact-word-for-exact-coordinates. Instead, in gensim <code>Word2Vec</code> and related classes there's <code>most_similar()</code>, which gives the known words <em>closest</em> to given known-words or vector coordinates, in ranked order, with the cosine-similarities. So if you've just trained (or loaded) a full <code>Word2Vec</code> model into the variable <code>model</code>, you can get the closest words to your vector with:</p>

<pre><code>vec = array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)
similars = model.wv.most_similar(positive=[vec])
print(similars)
</code></pre>

<p>If you just want the single closest word, it'd be in <code>similars[0][0]</code> (the first position of the top-ranked tuple).</p>
",2,3,2700,2018-08-15 06:56:45,https://stackoverflow.com/questions/51854220/word2vec-find-a-word-by-a-specific-vector
&#39;word&#39; not in Vocabulary in a corpus with words shown in a single list only in gensim library,"<p>Hello Community Members,</p>

<p>At present, I am implementing the Word2Vec algorithm.</p>

<p>Firstly, I have extracted the data (sentences), break and split the sentences into tokens (words), remove the punctuation marks and store the tokens in a single list. The list basically contain the words. Then I have calculated the frequency of words and then computed it occurrences in terms of frequency. It results a list. </p>

<p>Next, I am trying to load the model using gensim. However, I am facing a problem. The problem is about <code>the word is not in the vocabulary</code>. The code snippet, whatever I have tried is as follows.</p>

<pre><code>import nltk, re, gensim
import string
from collections import Counter
from string import punctuation
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from nltk.corpus import gutenberg, stopwords

def preprocessing():
    raw_data = (gutenberg.raw('shakespeare-hamlet.txt'))
    tokens = word_tokenize(raw_data)
    tokens = [w.lower() for w in tokens]
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    global words
    words = [word for word in stripped if word.isalpha()]
    sw = (stopwords.words('english'))
    sw1= (['.', ',', '""', '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])
    sw2= (['for', 'on', 'ed', 'es', 'ing', 'of', 'd', 'is', 'has', 'have', 'been', 'had', 'was', 'are', 'were', 'a', 'an', 'the', 't', 's', 'than', 'that', 'it', '&amp;', 'and', 'where', 'there', 'he', 'she', 'i', 'and', 'with', 'it', 'to', 'shall', 'why', 'ham'])
    stop=sw+sw1+sw2
    words = [w for w in words if not w in stop]
preprocessing()

def freq_count():
    fd = nltk.FreqDist(words)
    print(fd.most_common())
    freq_count()
def word_embedding():
    for i in range(len(words)):
        model = Word2Vec(words, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)
        model.init_sims(replace = True)
        model.save('word2vec_model')
        model = Word2Vec.load('word2vec_model')
        similarities = model.wv.most_similar('hamlet')
        for word, score in similarities:
            print(word , score)
word_embedding()
</code></pre>

<p>Note: I am using Python 3.7 in Windows OS. From the <code>syntax of gensim</code>, it is suggested to use sentences and split into tokens and apply the same to build and train the model. My question is that how to apply the same to a corpus with single list containing only words. I have specified the words also using list, i.e. [words], during the training of the model.       </p>
","python-3.x, nltk, gensim, word2vec, nltk-book","<p>The first parameter passed to <code>Word2Vec</code> expects an list of sentences. You're passing a list of words </p>

<pre><code>import nltk
import re
import gensim
import string
from collections import Counter
from string import punctuation
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from nltk.corpus import gutenberg, stopwords


def preprocessing():
    raw_data = (gutenberg.raw('shakespeare-hamlet.txt'))
    tokens = word_tokenize(raw_data)
    tokens = [w.lower() for w in tokens]
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    global words
    words = [word for word in stripped if word.isalpha()]
    sw = (stopwords.words('english'))
    sw1 = (['.', ',', '""', '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])
    sw2 = (['for', 'on', 'ed', 'es', 'ing', 'of', 'd', 'is', 'has', 'have', 'been', 'had', 'was', 'are', 'were', 'a', 'an', 'the', 't',
            's', 'than', 'that', 'it', '&amp;', 'and', 'where', 'there', 'he', 'she', 'i', 'and', 'with', 'it', 'to', 'shall', 'why', 'ham'])
    stop = sw + sw1 + sw2
    words = [w for w in words if not w in stop]


preprocessing()


def freq_count():
    fd = nltk.FreqDist(words)
    print(fd.most_common())
    freq_count()


def word_embedding():
    for i in range(len(words)):
        print(type(words))
        #pass words as a list.
        model = Word2Vec([words], size=100, sg=1, window=3,
                        min_count=1, iter=10, workers=4)
        model.init_sims(replace=True)
        model.save('word2vec_model')
        model = Word2Vec.load('word2vec_model')
        similarities = model.wv.most_similar('hamlet')
        for word, score in similarities:
            print(word, score)


word_embedding()
</code></pre>

<p>hope this helps :)</p>
",2,1,1343,2018-08-21 09:23:24,https://stackoverflow.com/questions/51945520/word-not-in-vocabulary-in-a-corpus-with-words-shown-in-a-single-list-only-in-g
gensim doc2vec - How to infer label,"<p>I am using gensim's doc2vec implementation and I have a few thousand documents tagged with four labels.</p>

<pre><code>yield TaggedDocument(text_tokens, [labels])
</code></pre>

<p>I'm training a Doc2Vec model with a list of these <strong>TaggedDocument</strong>s. However, I'm not sure how to infer the tag for a document that was not seen during training. I see that there is a infer_vector method which returns the embedding vector. But how can I get the most likely label from that?</p>

<p>An idea would be to infer the vectors for every label that I have and then calculate the cosine similarity between these vectors and the vector for the new document I want to classify. Is this the way to go? If so, how can I get the vectors for each of my four labels?</p>
","python, nlp, gensim, doc2vec","<p>I found the solution:</p>

<pre><code>model.docvecs['my_tag']
</code></pre>

<p>gives me the vector for a given tag. Easy</p>
",1,1,1920,2018-08-23 12:11:20,https://stackoverflow.com/questions/51985536/gensim-doc2vec-how-to-infer-label
Implementing Word to vector model using Gensim,"<p>We are trying to implement a word vector model for the set of words given below.</p>

<pre><code>stemmed = ['data', 'appli', 'scientist', 'mgr', 'microsoft', 'hire', 'develop', 'mentor', 'team', 'data', 'scientist', 'defin', 'data', 'scienc', 'prioriti', 'deep', 'understand', 'busi', 'goal', 'collabor', 'across', 'multipl', 'group', 'set', 'team', 'shortterm', 'longterm', 'goal', 'act', 'strateg', 'advisor', 'leadership', 'influenc', 'futur', 'direct', 'strategi', 'defin', 'partnership', 'align', 'efficaci', 'broad', 'analyt', 'effort', 'analyticsdata', 'team', 'drive', 'particip', 'data', 'scienc', 'bi', 'commun', 'disciplin', 'microsoftprior', 'experi', 'hire', 'manag', 'run', 'team', 'data', 'scientist', 'busi', 'domain', 'experi', 'use', 'analyt', 'must', 'experi', 'across', 'sever', 'relev', 'busi', 'domain', 'util', 'critic', 'think', 'skill', 'conceptu', 'complex', 'busi', 'problem', 'solut', 'use', 'advanc', 'analyt', 'larg', 'scale', 'realworld', 'busi', 'data', 'set', 'candid', 'must', 'abl', 'independ', 'execut', 'analyt', 'project', 'help', 'intern', 'client', 'understand']
</code></pre>

<p>We are using this code:</p>

<pre><code>import gensim
model = gensim.models.FastText(stemmed, size=100, window=5, min_count=1, workers=4, sg=1)
model.wv.most_similar(positive=['data'])
</code></pre>

<p>However, we are getting this error:</p>

<pre><code>KeyError: 'all ngrams for word data absent from model'
</code></pre>
","python-3.x, machine-learning, gensim, fasttext","<p>You need to provide your <strong>training data not as a list but rather as a generator</strong>. </p>

<p>Try: </p>

<pre><code>import gensim
from gensim.models.fasttext import FastText as FT_gensim

stemmed = ['data', 'appli', 'scientist', ... ]

def gen_words(stemmed):
    yield stemmed   

model = FT_gensim(size=100, window=5, min_count=1, workers=4, sg=1)
model.build_vocab(gen_words(stemmed))

model.train(gen_words(stemmed), total_examples=model.corpus_count, epochs=model.iter)
model.wv.most_similar(positive=['data'])
</code></pre>

<p>This prints out:</p>

<blockquote>
  <p>[('busi', -0.043828580528497696)]</p>
</blockquote>

<p>See also <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb"" rel=""nofollow noreferrer"">this notebook</a> from the gensim documentation. And <a href=""https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/"" rel=""nofollow noreferrer"">this excellent gensim tutorial</a> on all things iterable: </p>

<blockquote>
  <p>In gensim, it’s up to you how you create the corpus. <strong>Gensim algorithms
  only care that you supply them with an iterable</strong> of sparse vectors (and
  for some algorithms, even a generator = a single pass over the vectors
  is enough).</p>
</blockquote>
",2,3,920,2018-08-23 14:56:05,https://stackoverflow.com/questions/51988701/implementing-word-to-vector-model-using-gensim
Is it necessary to mix old corpus and new corpus in updating word2vec model?,"<p>I found it is not explicit in usage  </p>

<pre><code>from gensim.models import Word2Vec
sentences = [[""cat"", ""say"", ""meow""], [""dog"", ""say"", ""woof""]]

model = Word2Vec(min_count=1)
model.build_vocab(sentences)  # prepare the model vocabulary
model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors
(1, 30)
</code></pre>

<p>the sentences whether should contain the old corpus?</p>
","gensim, word2vec","<p>Your code doesn't show any incremental updating of an old model with new examples. </p>

<p>However, it's never guaranteed that incremental updating (as with <code>build_vocab(new_sentences, update=True)</code> and then <code>train(new_sentences, ...)</code>) necessarily improves the model overall. </p>

<p>The underlying algorithm gets its strength from a large dataset, of subtly-varied usage examples, being trained together in an interleaved fashion. The contrasting examples ""pull"" the model in various ways, sometimes reinforcing each other and sometimes cancelling out, resulting in final word-vector arrangements that are useful for other purposes. </p>

<p>Let's say you then do an incremental update with texts that are not the same as the original training data. (And after all, they must be meaningfully different, or else you wouldn't bother with more training.) During that new training, only the words affected by the new (possibly-smaller) dataset are changing. And they're changing just to be better at the new text examples. Any words (or senses-of-words) that only appeared in earlier data aren't being updated... and so the new training unavoidably pulls current words out of the balanced relationship with older words that existed after joint training. </p>

<p>In the extreme in some neural-network models, such new-data training can lead to ""<a href=""https://en.wikipedia.org/wiki/Catastrophic_interference"" rel=""nofollow noreferrer"">catastrophic interference</a>"", making the network much worse at things it once knew. </p>

<p>It might still work out ok, if there's a good overlap of vocabulary, or the right level of re-training and balance of learning-rates is chosen... but there are no hard-and-fast rules for picking the parameters/processes that make sure such 'tuning' works. You have to monitor the quality and optimize it yourself. </p>

<p>The safest, most robust course, when significant new data arrives, is to re-train the model from scratch using all available data – discarding the old model (because coordinates int he new model may not necessarily be comparable with older coordinates). It may be the case that starting this new model with vectors/weights from the old model may help it reach quality/stability sooner than starting from scratch – but still wouldn't guarantee coordinate-compatibility, or necessarily make it safe to leave out any older data. </p>
",1,0,194,2018-08-24 18:11:54,https://stackoverflow.com/questions/52009779/is-it-necessary-to-mix-old-corpus-and-new-corpus-in-updating-word2vec-model
Gensim Contruct the dictionary without loading all texts into memory gensim,"<p>Instead of having construct from one single document ('mycorpus.txt'), How can i frame dictionary from multiple documents (Each one going to be 25 MB in file size with 10,000 Files) and please be aware that i am trying ""to construct the dictionary without loading all texts into memory"" via gensim</p>

<pre><code>&gt;&gt;&gt; from gensim import corpora
&gt;&gt;&gt; from six import iteritems
&gt;&gt;&gt; dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))
&gt;&gt;&gt; stop_ids = [dictionary.token2id[stopword] for stopword in stoplist
&gt;&gt;&gt;             if stopword in dictionary.token2id]
&gt;&gt;&gt; once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]
&gt;&gt;&gt; dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once
&gt;&gt;&gt; dictionary.compactify()  # remove gaps in id sequence after words that were removed
&gt;&gt;&gt; print(dictionary)
</code></pre>
","python, bigdata, gensim","<p>You need an <code>iterator</code> for this.<br>
As taken from <a href=""https://rare-technologies.com/word2vec-tutorial/#preparing_the_input"" rel=""nofollow noreferrer"">the gensim webiste</a>:</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.lower().split()

sentences = MySentences('/some/directory') # a memory-friendly iterator
</code></pre>

<p><code>sentences</code> is an <code>iterator</code> which will open each file <strong>when needed</strong>, use it and then destroy the instance. So at any time, only one file is in memory.</p>

<p>From the website:  </p>

<blockquote>
  <p>if our input is strewn across several files on disk, with one sentence per line, then instead of loading everything into an in-memory list, we can process the input file by file, line by line</p>
</blockquote>

<p>To use it in your case, just replace your <code>dictionary</code> line with:  </p>

<pre><code>dictionary = corpora.Dictionary(line for line in sentences)
</code></pre>

<p>where <code>sentences</code> is the variable we defined earlier which was given the path to the folder with several <code>.txt</code> files.</p>

<p>To understand more about Iterators, Iterables and Generators, check out <a href=""https://nvie.com/posts/iterators-vs-generators/"" rel=""nofollow noreferrer"">this blog</a>.</p>
",3,1,644,2018-08-24 22:49:37,https://stackoverflow.com/questions/52012579/gensim-contruct-the-dictionary-without-loading-all-texts-into-memory-gensim
"Loss does not decrease during training (Word2Vec, Gensim)","<p>What can cause loss from <code>model.get_latest_training_loss()</code> increase on each epoch?  </p>

<p>Code, used for training: </p>

<pre><code>class EpochSaver(CallbackAny2Vec):
    '''Callback to save model after each epoch and show training parameters '''

    def __init__(self, savedir):
        self.savedir = savedir
        self.epoch = 0

        os.makedirs(self.savedir, exist_ok=True)

    def on_epoch_end(self, model):
        savepath = os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch))
        model.save(savepath)
        print(
            ""Epoch saved: {}"".format(self.epoch + 1),
            ""Start next epoch ... "", sep=""\n""
            )
        if os.path.isfile(os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch - 1))):
            print(""Previous model deleted "")
            os.remove(os.path.join(self.savedir, ""model_neg{}_epoch.gz"".format(self.epoch - 1))) 
        self.epoch += 1
        print(""Model loss:"", model.get_latest_training_loss())

    def train():

        ### Initialize model ###
        print(""Start training Word2Vec model"")

        workers = multiprocessing.cpu_count()/2

        model = Word2Vec(
            DocIter(),
            size=300, alpha=0.03, min_alpha=0.00025, iter=20,
            min_count=10, hs=0, negative=10, workers=workers,
            window=10, callbacks=[EpochSaver(""./checkpoints"")], 
            compute_loss=True
    )     
</code></pre>

<p>Output: </p>

<p>Losses from epochs (1 to 20): </p>

<pre><code>Model loss: 745896.8125
Model loss: 1403872.0
Model loss: 2022238.875
Model loss: 2552509.0
Model loss: 3065454.0
Model loss: 3549122.0
Model loss: 4096209.75
Model loss: 4615430.0
Model loss: 5103492.5
Model loss: 5570137.5
Model loss: 5955891.0
Model loss: 6395258.0
Model loss: 6845765.0
Model loss: 7260698.5
Model loss: 7712688.0
Model loss: 8144109.5
Model loss: 8542560.0
Model loss: 8903244.0
Model loss: 9280568.0
Model loss: 9676936.0
</code></pre>

<p>What am I doing wrong?</p>

<p>Language arabian. 
As input from DocIter - list with tokens. </p>
","python, gensim, word2vec, loss","<p>Up through gensim 3.6.0, the loss value reported may not be very sensible, only resetting the tally each call to <code>train()</code>, rather than each internal epoch. There are some fixes forthcoming in this issue:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/pull/2135"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/pull/2135</a></p>

<p>In the meantime, the <em>difference</em> between the previous value, and the latest, may be more meaningful. In that case, your data suggest the 1st epoch had a total loss of 745896, while the last had (9676936-9280568=) 396,368 – which may indicate the kind of progress hoped-for. </p>
",6,8,7575,2018-08-27 11:48:18,https://stackoverflow.com/questions/52038651/loss-does-not-decrease-during-training-word2vec-gensim
Wrong length for Gensim Word2Vec&#39;s vocabulary,"<p>I am trying to train the <code>Gensim Word2Vec</code> model by:</p>

<pre><code>X = train['text']    

model_word2vec = models.Word2Vec(X.values, size=150)
model_word2vec.train(X.values, total_examples=len(X.values), epochs=10)
</code></pre>

<p>after the training, I get a small vocabulary (<code>model_word2vec.wv.vocab</code>) of length <code>74</code> containing only the alphabet's letters.</p>

<p>How could I get the right vocabulary?</p>

<p><strong>Update</strong></p>

<p>I tried this before:</p>

<pre><code>tokenizer = Tokenizer(lower=True)
tokenized_text = tokenizer.fit_on_texts(X)
sequence = tokenizer.texts_to_sequences(X)

model_word2vec.train(sequence, total_examples=len(X.values), epochs=10
</code></pre>

<p>but I got the same wrong vocabulary size.</p>
","nlp, gensim, word2vec","<p>Supply the model with the kind of corpus it needs: a sequence of texts, where each text is a <em>list-of-string-tokens</em>. If you supply it with non-tokenized strings instead, it will think each single character is a token, giving the results you're seeing. </p>
",0,0,142,2018-08-28 15:33:05,https://stackoverflow.com/questions/52061585/wrong-length-for-gensim-word2vecs-vocabulary
Extracting Topic distribution from gensim LDA model,"<p>I created an LDA model for some text files using gensim package in python. I want to get topic's distributions for the learned model. Is there any method in gensim ldamodel class or a solution to get topic's distributions from the model?
For example, I use the coherence model to find a model with the best cohrence value subject to the number of topics in range 1 to 5. After getting the best model I use get_document_topics method (thanks <a href=""https://stackoverflow.com/users/6256482/kenhbs"">kenhbs</a>) to get topic distribution in the document that used for creating the model.</p>

<pre><code>id2word = corpora.Dictionary(doc_terms)

bow = id2word.doc2bow(doc_terms)

max_coherence = -1
best_lda_model = None

for num_topics in range(1, 6):

    lda_model = gensim.models.ldamodel.LdaModel(corpus=bow, num_topics=num_topics)

    coherence_model = gensim.models.CoherenceModel(model=lda_model, texts=doc_terms,dictionary=id2word)

    coherence_value = coherence_model.get_coherence()

    if coherence_value &gt; max_coherence:
        max_coherence = coherence_value
        best_lda_model = lda_model
</code></pre>

<p>The best has 4 topics</p>

<pre><code>print(best_lda_model.num_topics)

4
</code></pre>

<p>But when I use get_document_topics, I get less than 4 values for document distribution.</p>

<pre><code>topic_ditrs = best_lda_model.get_document_topics(bow)

print(len(topic_ditrs))

3
</code></pre>

<p>My question is: For best lda model with 4 topics (using coherence model) for a document, why get_document_topics returns fewer topics for the same document? why some topics have very small distribution (less than 1-e8)?</p>
","gensim, lda, topic-modeling","<p>From <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">the documentation</a>, you can use two methods for this.</p>
<p>If you are aiming to get the main terms in a specific topic, use <code>get_topic_terms</code>:</p>
<pre><code>from gensim.model.ldamodel import LdaModel

K = 10
lda = LdaModel(some_corpus, num_topics=K)

lda.get_topic_terms(5, topn=10)
# Or for all topics
for i in range(K):
    lda.get_topic_terms(i, topn=10)
</code></pre>
<p>You can also print the entire underlying <code>np.ndarray</code> (called either beta or phi in standard LDA papers, dimensions are (K, V) or (V, K)).</p>
<pre><code>phi = lda.get_topics()
</code></pre>
<p><strong>edit</strong>:
From the link i included in the original answer: if you are looking for a document's topic distribution, use</p>
<pre><code>res = lda.get_document_topics(bow)
</code></pre>
<p>As can be read from the documentation, the resulting object contains the following three lists:</p>
<blockquote>
<ul>
<li><p>list of (int, float) – Topic distribution for the whole document. Each element in the list is a pair of a topic’s id, and the probability that was assigned to it.</p>
</li>
<li><p>list of (int, list of (int, float), optional – Most probable topics per word. Each element in the list is a pair of a word’s id, and a list of topics sorted by their relevance to this word. Only returned if per_word_topics was set to True.</p>
</li>
<li><p>list of (int, list of float), optional – Phi relevance values, multipled by the feature length, for each word-topic combination. Each element in the list is a pair of a word’s id and a list of the phi values between this word and each topic. Only returned if per_word_topics was set to True.</p>
</li>
</ul>
</blockquote>
<p>Now,</p>
<pre><code>tops, probs = zip(*res[0])
</code></pre>
<p><code>probs</code> will contains K (for you 4) probabilities. Some may be zero, but they should sum up to 1</p>
",3,2,5795,2018-08-29 12:04:58,https://stackoverflow.com/questions/52077016/extracting-topic-distribution-from-gensim-lda-model
Continue training a FastText model,"<p>I have downloaded a <code>.bin</code> FastText model, and I use it with <code>gensim</code> as follows:</p>

<pre><code>model = FastText.load_fasttext_format(""cc.fr.300.bin"")
</code></pre>

<p>I would like to continue the training of the model to adapt it to my domain. After checking <a href=""https://github.com/facebookresearch/fastText/pull/423"" rel=""noreferrer"">FastText's Github</a> and the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/fasttext.py#L685"" rel=""noreferrer"">Gensim documentation</a> it seems like it is <strong>not</strong> currently feasible appart from using this person's proposed <a href=""https://github.com/ericxsun/fastText"" rel=""noreferrer"">modification</a> (not yet merged). </p>

<p>Am I missing something?</p>
","python, gensim, fasttext","<p>You can continue training in some versions of Gensim's <code>fastText</code> (for example, v.3.7.*). Here is an example of ""<a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""noreferrer"">Loading, inferring, continuing training</a>""</p>

<pre><code>from gensim.test.utils import datapath
model = load_facebook_model(datapath(""crime-and-punishment.bin""))
sent = [['lord', 'of', 'the', 'rings'], ['lord', 'of', 'the', 'semi-groups']]
model.build_vocab(sent, update=True)
model.train(sentences=sent, total_examples = len(sent), epochs=5)
</code></pre>

<p>For some reason, the <code>gensim.models.fasttext.load_facebook_model()</code> is missing on Windows, but exists on Mac's installation. Alternatively, one can use <code>gensim.models.FastText.load_fasttext_format()</code> to load a pre-trained model and continue training.</p>

<p>Here are various <a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""noreferrer"">pre-trained Wiki word models and vectors</a> (or <a href=""https://github.com/facebookresearch/fastText/blob/master/docs/pretrained-vectors.md"" rel=""noreferrer"">here</a>).</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb"" rel=""noreferrer"">Another example</a>. ""<em>Note: As in the case of Word2Vec, you can continue to train your model while using Gensim's native implementation of fastText.</em>""</p>
",5,10,7918,2018-08-29 14:47:33,https://stackoverflow.com/questions/52080365/continue-training-a-fasttext-model
Training LDA on Wikipedia corpus to tag arbitary article?,"<p>I followed the steps in gensim Python <a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/wiki.html</a> to train wikipedia on LDA model, now I want to compare arbitary article from cnn.com for example with the trained data, what do I need to do next? Suppose this article is in txt file? </p>
","python, nltk, gensim","<p>Taken <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">from here</a>: </p>

<pre><code># Create a new corpus, made of previously unseen documents.
cnn_article = [
    ['This', 'is', 'my', 'cnn', 'article'],
    ]
other_corpus = [common_dictionary.doc2bow(text) for text in cnn_article]
unseen_doc = other_corpus[0]
vector = lda[unseen_doc] # get topic probability distribution for a document
</code></pre>

<p>Then get the similarities by using <a href=""https://radimrehurek.com/gensim/similarities/docsim.html"" rel=""nofollow noreferrer"">gensims Similarity class</a>.</p>

<p><strong>Update:</strong></p>

<p>To more precisely refer to the tutorial and your text file:</p>

<pre><code># Create a corpus from a list of texts
common_dictionary = Dictionary(common_texts)
common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]

# Train the model on the corpus.
lda = LdaModel(common_corpus, id2word=common_dictionary, num_topics=10)

# optional: print topics of your model
for topic in lda.print_topics(10):
    print(topic)

# load your CNN article from file
with open(""cnn.txt"", ""r"") as file:
    cnn = file.read()

# split article into list of words and make this list an element of a list
cnn = [cnn.split("" "")]

cnn_corpus = [common_dictionary.doc2bow(text) for text in cnn]

unseen_doc = cnn_corpus[0]
vector = lda[unseen_doc] # get topic probability distribution for a document

# print out «similarity» of cnn article to each of the topics
# bigger number = more similar to topic 
print(vector)
</code></pre>
",0,0,353,2018-09-01 05:26:57,https://stackoverflow.com/questions/52125136/training-lda-on-wikipedia-corpus-to-tag-arbitary-article
Using pretrained gensim Word2vec embedding in keras,"<p>I have trained word2vec in gensim. In Keras, I want to use it to make matrix of sentence using that word embedding. As storing the matrix of all the sentences is very space and memory inefficient. So, I want to make embedding layer in Keras to achieve this so that It can be used in further layers(LSTM). Can you tell me in detail how to do this?</p>

<p>PS: It is different from other questions because I am using gensim for word2vec training instead of keras.</p>
","python, keras, gensim, word2vec, word-embedding","<p>Let's say you have following data that you need  to encode</p>

<pre><code>docs = ['Well done!',
        'Good work',
        'Great effort',
        'nice work',
        'Excellent!',
        'Weak',
        'Poor effort!',
        'not good',
        'poor work',
        'Could have done better.']
</code></pre>

<p>You must then tokenize it using the <code>Tokenizer</code> from Keras like this and find the <code>vocab_size</code></p>

<pre><code>t = Tokenizer()
t.fit_on_texts(docs)
vocab_size = len(t.word_index) + 1
</code></pre>

<p>You can then enocde it to sequences like this</p>

<pre><code>encoded_docs = t.texts_to_sequences(docs)
print(encoded_docs)
</code></pre>

<p>You can then pad the sequences so that all the sequences are of a fixed length</p>

<pre><code>max_length = 4
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
</code></pre>

<p>Then use the word2vec model to make embedding matrix </p>

<pre><code># load embedding as a dict
def load_embedding(filename):
    # load embedding into memory, skip first line
    file = open(filename,'r')
    lines = file.readlines()[1:]
    file.close()
    # create a map of words to vectors
    embedding = dict()
    for line in lines:
        parts = line.split()
        # key is string word, value is numpy array for vector
        embedding[parts[0]] = asarray(parts[1:], dtype='float32')
    return embedding

# create a weight matrix for the Embedding layer from a loaded embedding
def get_weight_matrix(embedding, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = zeros((vocab_size, 100))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
        weight_matrix[i] = embedding.get(word)
    return weight_matrix

# load embedding from file
raw_embedding = load_embedding('embedding_word2vec.txt')
# get vectors in the right order
embedding_vectors = get_weight_matrix(raw_embedding, t.word_index)
</code></pre>

<p>Once you have the embedding matrix you can use it in <code>Embedding</code> layer like this</p>

<pre><code>e = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=4, trainable=False)
</code></pre>

<p>This layer can be used in making a model like this</p>

<pre><code>model = Sequential()
e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)
model.add(e)
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
# summarize the model
print(model.summary())
# fit the model
model.fit(padded_docs, labels, epochs=50, verbose=0)
</code></pre>

<p>All the codes are adapted from <a href=""https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"" rel=""noreferrer"">this</a> awesome blog post. follow it to know more about Embeddings using Glove</p>

<p>For using word2vec see <a href=""https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"" rel=""noreferrer"">this</a> post</p>
",18,12,16281,2018-09-01 08:53:27,https://stackoverflow.com/questions/52126539/using-pretrained-gensim-word2vec-embedding-in-keras
Gensim: raise KeyError(&quot;word &#39;%s&#39; not in vocabulary&quot; % word),"<p>I have this code and I have list of article as dataset. Each raw has an article. </p>

<p>I run this code:</p>

<pre><code>import gensim    
docgen = TokenGenerator( raw_documents, custom_stop_words )    
# the model has 500 dimensions, the minimum document-term frequency is 20    
w2v_model = gensim.models.Word2Vec(docgen, size=500, min_count=20, sg=1)    
print( ""Model has %d terms"" % len(w2v_model.wv.vocab) )    
w2v_model.save(""w2v-model.bin"")    
# To re-load this model, run    
#w2v_model = gensim.models.Word2Vec.load(""w2v-model.bin"")    
    def calculate_coherence( w2v_model, term_rankings ):
        overall_coherence = 0.0
        for topic_index in range(len(term_rankings)):
            # check each pair of terms
            pair_scores = []
            for pair in combinations(term_rankings[topic_index], 2 ):
                pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )
            # get the mean for all pairs in this topic
            topic_score = sum(pair_scores) / len(pair_scores)
            overall_coherence += topic_score
        # get the mean score across all topics
        return overall_coherence / len(term_rankings)

import numpy as np    
def get_descriptor( all_terms, H, topic_index, top ):    
    # reverse sort the values to sort the indices    
    top_indices = np.argsort( H[topic_index,:] )[::-1]    
    # now get the terms corresponding to the top-ranked indices    
    top_terms = []    
    for term_index in top_indices[0:top]:    
        top_terms.append( all_terms[term_index] )    
    return top_terms    
from itertools import combinations    
k_values = []    
coherences = []    
for (k,W,H) in topic_models:    
    # Get all of the topic descriptors - the term_rankings, based on top 10 terms
    term_rankings = []    
    for topic_index in range(k):
        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )

    # Now calculate the coherence based on our Word2vec model
    k_values.append( k )
    coherences.append( calculate_coherence( w2v_model, term_rankings ) )
    print(""K=%02d: Coherence=%.4f"" % ( k, coherences[-1] ) )
</code></pre>

<p>I face with this error:</p>

<pre><code>raise KeyError(""word '%s' not in vocabulary"" % word)
</code></pre>

<p>KeyError: u""word 'business' not in vocabulary""</p>

<p>The original code works great with their data set. </p>

<p><a href=""https://github.com/derekgreene/topic-model-tutorial"" rel=""nofollow noreferrer"">https://github.com/derekgreene/topic-model-tutorial</a></p>

<p>Could you help what this error is?</p>
","python, nlp, gensim, word2vec, topic-modeling","<p>It could help answerers if you included more of the information around the error message, such as the multiple-lines of call-frames that will clearly indicate which line of your code triggered the error. </p>

<p>However, if you receive the error <code>KeyError: u""word 'business' not in vocabulary""</code>, you can trust that your <code>Word2Vec</code> instance, <code>w2v_model</code>, never learned the word <code>'business'</code>. </p>

<p>This might be because it didn't appear in the training data the model was presented, or perhaps appeared but fewer than <code>min_count</code> times. </p>

<p>As you don't show the type/contents of your <code>raw_documents</code> variable, or code for your <code>TokenGenerator</code> class, it's not clear why this would have gone wrong – but those are the places to look. Double-check that <code>raw_documents</code> has the right contents, and that individual items inside the <code>docgen</code> iterable-object look like the right sort of input for <code>Word2Vec</code>. </p>

<p>Each item in the <code>docgen</code> iterable object should be a list-of-string-tokens, not plain strings or anything else. And, the <code>docgen</code> iterable must be possible of being iterated-over multiple times. For example, if you execute the following two lines, you should see the same two lists-of-string tokens (looking something like <code>['hello', 'world']</code>:</p>

<pre><code>print(iter(docgen).next())
print(iter(docgen).next())
</code></pre>

<p>If you see plain strings, <code>docgen</code> isn't providing the right kind of items for <code>Word2Vec</code>. If you only see one item printed, <code>docgen</code> is likely a simple single-pass iterator, rather than an iterable object. </p>

<p>You could also enable logging at the <code>INFO</code> level and watch the output during the <code>Word2Vec</code> step carefully, and pay extra attention to any numbers/steps that seem incongruous. (For example, do any steps indicate nothing is happening, or do the counts of words/text-examples seem off?)</p>
",2,1,1603,2018-09-02 17:21:53,https://stackoverflow.com/questions/52139386/gensim-raise-keyerrorword-s-not-in-vocabulary-word
TypeError: ufunc &#39;add&#39; did not contain a loop with signature matching types dtype,"<p>I would like to pass the <code>X_train_word2vec</code> vector as input to <code>Gensim Word2Vec</code> model.
The vector type is <code>numpy.ndarray</code>, at example:</p>

<pre><code>X_train_word2vec[9] = array([   19,     7,     1, 20120,     2,     1,   856,   233,   671,
       1,  1208,  6016,     2,    32,     0,     0,     0,     0, ....)]
</code></pre>

<p>When I run this code:</p>

<pre><code>model_word2vec = models.Word2Vec(X_train_word2vec, size=150, window=9)
model_word2vec.train(X_train_word2vec,total_examples=X_train_word2vec.shape[0], epochs=10)
</code></pre>

<p>I get this error:</p>

<p><code>TypeError: ufunc 'add' did not contain a loop with signature matching types dtype('&lt;U11') dtype('&lt;U11') dtype('&lt;U11')</code></p>

<p>I have read <a href=""https://stackoverflow.com/questions/44527956/python-ufunc-add-did-not-contain-a-loop-with-signature-matching-types-dtype"">this</a> post, where the issue is due to different data types in the input array but, in my case, I have all the data of the same type: <code>int</code>.</p>

<p><strong>Update:</strong>
The code before <code>model_Word2Vec</code>:</p>

<pre><code>tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
sequence = tokenizer.texts_to_sequences(X)

seq_max_len = 50
X_seq = pad_sequences(sequenza, maxlen=seq_max_len,padding='post',truncating='post',dtype=int)

X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(X_seq, y_cat, test_size=0.2, random_state=123)
</code></pre>
","python, gensim, word2vec","<p>Gensim's <code>Word2Vec</code> requires a corpus of texts – such as in its intializer's 1st argument – that's an iterable sequence object of lists-of-string-tokens. It doesn't take a raw numpy array. </p>

<p>Further, if you do supply a corpus at instantiation, as in your line of code...</p>

<pre><code>model_word2vec = models.Word2Vec(X_train_word2vec, size=150, window=9)
</code></pre>

<p>...then it will automatically do its vocabulary-building and training steps. You don't need to then call <code>train()</code> explicitly. (And, while it's possible to call <code>train()</code> again, very few users doing very advanced things will need to do so. The usual, safe approach is a single training session on a complete corpus, after which the model is ""done"".)</p>

<p>Finally, <code>train()</code> also expects any corpus as an iterable sequence object of lists-of-string-tokens.</p>

<p>If you supply the right kind of corpus, it's doubtful you'll receive an error like you're getting. </p>
",1,1,874,2018-09-07 10:24:05,https://stackoverflow.com/questions/52220514/typeerror-ufunc-add-did-not-contain-a-loop-with-signature-matching-types-dtyp
What&#39;s the difference between Skip-gram word2vec and CBOW w2v during training with gensim library?,"<p>For Skip-gram word2vec training samples are obtained as follows:</p>

<pre><code>Sentence: The fox was running across the maple forest
</code></pre>

<p>The word <code>fox</code> give next pairs for training:</p>

<pre><code>fox-run, fox-across, fox-maple, fox-forest
</code></pre>

<p>and etc. for every word. CBOW w2v use reverse approach:</p>

<pre><code>run-fox, across-fox, maple-fox, forest-fox
</code></pre>

<p>or for <code>forest</code> word:</p>

<pre><code>fox-forest, run-forest, across-forest, maple-forest
</code></pre>

<p>So we get all the pairs. What's the difference between Skip-gram word2vec and CBOW w2v during training with gensim library, if we do not specify the target word when training in the CBOW-mode? In both cases all pairs of words are used, or not?</p>
","python, machine-learning, nlp, gensim, word2vec","<p>Only skip-gram uses training pairs of the form <code>(context_word)-&gt;(target_word)</code>.</p>

<p>In CBOW, the training examples are <code>(average_of_multiple_context_words)-&gt;(target_word)</code>. So, when the error from a single training example is backpropagated, multiple context-words get the same corrective nudge.</p>
",7,2,1913,2018-09-10 06:22:37,https://stackoverflow.com/questions/52252119/whats-the-difference-between-skip-gram-word2vec-and-cbow-w2v-during-training-wi
Calculation of Cosine Similarity of a single word in 2 different Word2Vec Models,"<p>I build two word embedding (word2vec models) using <code>gensim</code> and save it as (word2vec1 and word2vec2) by using the <code>model.save(model_name)</code> command for two different corpus (the two corpuses are somewhat similar, similar means they are related like part 1 and part 2 of a book). Suppose, the top words (in terms of frequency or occurrence) for the two corpuses is the same word (let's say it as <code>a</code>). </p>

<p>How to compute the degree of similarity (<code>cosine-similarity or similarity</code>) of the extracted top word (say 'a'), for the two word2vec models? Does <code>most_similar()</code> will work in this case efficiently? </p>

<p>I want to know by how much degree of similarity, does the same word (a), is related for two different generated models?</p>

<p>Any idea is deeply appreciated.</p>
","python-3.x, gensim, word2vec, word-embedding","<p>You seem to have the wrong idea about word2vec. It doesn't provide one absolute vector for one word. It manages to find a representation for a word relative to other words. So, for the same corpus, if you run word2vec twice, you will get 2 different vectors for the same word. The meaning comes in when you compare it relative to other word vectors. </p>

<p><code>king</code> - <code>man</code> will always be close(cosine similarity wise) to <code>queen</code> - <code>woman</code> no matter how many time you train it. But they will have different vectors after each train.</p>

<p>In your case, since the 2 models are trained differently, comparing vectors of the same word is the same as comparing two random vectors. You should rather compare the relative relations. Maybe something like: <code>model1.most_similar('dog')</code> vs <code>model2.most_similar('dog')</code></p>

<p>However, to answer your question, if you wanted to compare the 2 vectors, you could do it as below. But the results will be meaningless.</p>

<p>Just take the vectors from each model and manually calculate cosine similarity.</p>

<pre><code>vec1 = model1.wv['computer']
vec2 = model2.wv['computer']
print(np.sum(vec1*vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))
</code></pre>
",5,1,2958,2018-09-11 13:43:28,https://stackoverflow.com/questions/52277384/calculation-of-cosine-similarity-of-a-single-word-in-2-different-word2vec-models
Inaccurate similarities results by doc2vec using gensim library,"<p>I am working with Gensim library to train some data files using doc2vec,  while trying to test the similarity of one of the files using the method <code>model.docvecs.most_similar(&quot;file&quot;)</code> , I always get all the results above 91% with almost no difference between them (which is not logic), because the files do not have similarities between them. so the results are inaccurate.<br/><br/>
<strong>Here is the code for training the model</strong><br/></p>
<pre><code>model = gensim.models.Doc2Vec(vector_size=300, min_count=0, alpha=0.025, min_alpha=0.00025,dm=1)
model.build_vocab(it)
for epoch in range(100):
    model.train(it,epochs=model.iter, total_examples=model.corpus_count)
    model.alpha -= 0.0002
    model.min_alpha = model.alpha
model.save('doc2vecs.model')
model_d2v = gensim.models.doc2vec.Doc2Vec.load('doc2vecs.model')
sim = model_d2v.docvecs.most_similar('file1.txt')
print sim
</code></pre>
<br/> 
**this is the output result**
<blockquote>
<p>
[('file2.txt', 0.9279470443725586), ('file6.txt', 0.9258157014846802), ('file3.txt', 0.92499840259552), ('file5.txt', 0.9209873676300049), ('file4.txt', 0.9180108308792114), ('file7.txt', 0.9141069650650024)]
</p>
</blockquote>
<p>what am I doing wrong ? how could I improve the accuracy of results ?</p>
","python, nlp, gensim, doc2vec","<p>What is your <code>it</code> data, and how is it prepared? (For example, what does <code>print(iter(it).next())</code> do, especially if you call it twice in a row?)</p>

<p>By calling <code>train()</code> 100 times, and also retaining the default <code>model.iter</code> of 5, you're actually making 500 passes over the data. And the first 5 passes will use <code>train()</code>s internal, effective <code>alpha</code>-management to lower the learning rate gradually to your declared <code>min_alpha</code> value. Then your next 495 passes will be at your own clumsily-managed alpha rates, first back up near <code>0.025</code> and then lower each batch-of-5 until you reach <code>0.005</code>.</p>

<p>None of that is a good idea. You can just call <code>train()</code> once, passing it your desired number of <code>epochs</code>. A typical number of epochs in published work  is 10-20. (A bit more might help with a small dataset, but if you think you need hundreds, something else is probably wrong with the data or setup.)</p>

<p>If it's a small amount of data, you won't get very interesting <code>Word2Vec</code>/<code>Doc2Vec</code> results, as these algorithms depend on lots of varied examples. Published results tend to use training sets with tens-of-thousands to millions of documents, and each document at least dozens, but preferably hundreds, of words long. With tinier datasets, sometimes you can squeeze out adequate results by using more training passes, and smaller vectors. Also using the simpler PV-DBOW mode (<code>dm=0</code>) may help with smaller corpuses/documents. </p>

<p>The values reported by <code>most_similar()</code> are not similarity ""percentages"". They're cosine-similarity values, from -1.0 to 1.0, and their absolute values are less important than the relative ranks of different results. So it shouldn't matter if there are a lot of results with >0.9 similarities – as long as those documents are more like the query document than those lower in the rankings.</p>

<p>Looking at the individual documents suggested as most-similar is thus the real test. If they seem like nonsense, it's likely there are problems with your data  or its preparation, or training parameters. </p>

<p>For datasets with sufficient, real natural-language text, it's typical for higher <code>min_count</code> values to give better results. Real text tends to have lots of low-frequency words that don't imply strong things without many more examples, and thus keeping them during training serves as noise making the model less strong. </p>
",4,2,1326,2018-09-12 01:44:58,https://stackoverflow.com/questions/52286330/inaccurate-similarities-results-by-doc2vec-using-gensim-library
&#39;gensim&#39; is not recognized in pycharm,"<p>PyCharm can't find gensim that is listed in ""anaconda list"". in anaconda list I can see gensim but it does not exist in project interpreter!!!
I'm using paython version 3.7. I have no problem with other libraries, the problem is just in gensim installation </p>

<p><img src=""https://i.sstatic.net/PcrFs.png"" alt=""anaconda list""></p>

<p><img src=""https://i.sstatic.net/pHN8L.png"" alt=""project interpreter list""></p>

<p>If anyone has any advice, I'd appreciate it! Thanks.</p>
","python, python-3.x, pycharm, gensim","<p>The two lists seem to come from different interpreters. Most likely you have not selected in PyCharm correct interpreter and hence the error. Please check the <a href=""https://www.jetbrains.com/help/pycharm/configuring-python-interpreter.html"" rel=""nofollow noreferrer"">PyCharm docs</a> for information on how to configure interpreter.</p>
",4,1,380,2018-09-12 14:12:38,https://stackoverflow.com/questions/52297260/gensim-is-not-recognized-in-pycharm
What is the operation behind the word analogy in Word2vec?,"<p>According to <a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">https://code.google.com/archive/p/word2vec/</a>: </p>

<blockquote>
  <p>It was recently shown that the word vectors capture many linguistic
  regularities, for example vector operations vector('Paris') -
  vector('France') + vector('Italy') results in a vector that is very
  close to vector('Rome'), and vector('king') - vector('man') +
  vector('woman') is close to vector('queen') [3, 1]. You can try out a
  simple demo by running demo-analogy.sh.</p>
</blockquote>

<p>So we can try from the supplied demo script:</p>

<pre><code>+ ../bin/word-analogy ../data/text8-vector.bin
Enter three words (EXIT to break): paris france berlin

Word: paris  Position in vocabulary: 198365

Word: france  Position in vocabulary: 225534

Word: berlin  Position in vocabulary: 380477

                                              Word              Distance
------------------------------------------------------------------------
                                           germany      0.509434
                                          european      0.486505
</code></pre>

<p>Please note that <code>paris france berlin</code> is the input hint the demo suggest. The problem is that I'm unable to reproduce this behavior if I open the same word vectors in <code>Gensim</code> and try to compute the vectors myself. For example:</p>

<pre><code>&gt;&gt;&gt; word_vectors = KeyedVectors.load_word2vec_format(BIGDATA, binary=True)
&gt;&gt;&gt; v = word_vectors['paris'] - word_vectors['france'] + word_vectors['berlin']
&gt;&gt;&gt; word_vectors.most_similar(np.array([v]))
[('berlin', 0.7331711649894714), ('paris', 0.6669869422912598), ('kunst', 0.4056406617164612), ('inca', 0.4025722146034241), ('dubai', 0.3934606909751892), ('natalie_portman', 0.3909246325492859), ('joel', 0.3843030333518982), ('lil_kim', 0.3784593939781189), ('heidi', 0.3782389461994171), ('diy', 0.3767407238483429)]
</code></pre>

<p>So, what is the word analogy actually doing? How should I reproduce it?</p>
","python, gensim, word2vec, word-embedding","<p>You should be clear about exactly which word-vector set you're using: different sets will have a different ability to perform well on analogy tasks. (Those trained on the tiny <code>text8</code> dataset might be pretty weak; the big <code>GoogleNews</code> set Google released would probably do well, at least under certain conditions like discarding low-frequnecy words.)</p>

<p>You're doing the wrong arithmetic for the analogy you're trying to solve. For an analogy ""A is to B as C is to ?"" often written as:</p>

<pre><code>A : B :: C : _?_
</code></pre>

<p>You begin with 'B', subtract 'A', then add 'C'. So the example:</p>

<pre><code>France : Paris :: Italy : _?_
</code></pre>

<p>...gives the formula in your excerpted text:</p>

<pre><code>wv('Paris') - wv('France') + wv('Italy`) = target_coordinates  # close-to wv('Rome')
</code></pre>

<p>And to solve instead:</p>

<pre><code>Paris : France :: Berlin : _?_
</code></pre>

<p>You would try:</p>

<pre><code>wv('France') - wv('Paris') + wv('Berlin') = target_coordinates
</code></pre>

<p>...then see what's closest to <code>target_coordinates</code>. (Note the difference in operation-ordering to your attempt.)</p>

<p>You can think of it as:</p>

<ol>
<li>start at a country-vector ('France')</li>
<li>subtract the (country&amp;capital)-vector ('Paris'). This leaves you with an interim vector that's, sort-of, ""zero"" country-ness, and ""negative"" capital-ness.</li>
<li>add another (country&amp;capital)-vector ('Berlin'). This leaves you with a result vector that's, again sort-of, ""one"" country-ness, and ""zero"" capital-ness. </li>
</ol>

<p>Note also that <code>gensim</code>'s <code>most_similar()</code> takes multiple positive and negative word-examples, to do the arithmetic for you. So you can just do:</p>

<pre><code>sims = word_vectors.most_similar(positive=['France', 'Berlin'], negative=['Paris'])
</code></pre>
",5,5,4769,2018-09-17 09:30:43,https://stackoverflow.com/questions/52364632/what-is-the-operation-behind-the-word-analogy-in-word2vec
load a file with only its extension name,"<p>I would like to load a file for only it's extension name in gensim. </p>

<p>A normal code would be this:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec.load(""news.bin"")
</code></pre>

<p>But I would like it to auto open any file with "".bin"".</p>

<p>Example:</p>

<pre><code>model = gensim.models.word2vec.Word2Vec.load(***I would like to change this part to only load any .bin***)
</code></pre>

<p>.bin files:</p>

<p>It can be ""news.bin"", ""file.bin"" or ""guess.bin"". As long as it load only the extension. Thank you.</p>
","python, gensim, word2vec","<p>If you want to open <strong>ALL</strong> of them one by one, you can iterate over files in the target directory.    </p>

<p>This is the code example for Python 3:</p>

<pre><code>import os

directory_path = ""/path/to/directory""

for filename in os.listdir(directory_path):
    if filename.endswith("".bin""): 
        file_path = os.path.join(directory_path, filename)
        model = gensim.models.word2vec.Word2Vec.load(file_path)
        # Do whatever you want to do with model
</code></pre>

<p>If you only want to open <strong>ANY ONE</strong> of them, you can break out of the for loop after the first match:</p>

<pre><code>import os

directory_path = ""/path/to/directory""

for filename in os.listdir(directory_path):
    if filename.endswith("".bin""): 
        file_path = os.path.join(directory_path, filename)
        model = gensim.models.word2vec.Word2Vec.load(file_path)
        # Do whatever you want to do with model
        # Break out of the for loop afterwards so it stops iterating
        break
</code></pre>
",1,0,102,2018-09-19 02:14:15,https://stackoverflow.com/questions/52397065/load-a-file-with-only-its-extension-name
Non English Word Embedding from English Word Embedding,"<p>How can i generate non-english (french , spanish , italian ) word embedding from english word embedding ?</p>

<p>What are the best ways to generate high quality word embedding for non - english words .</p>

<p>Words may include (samsung-galaxy-s9)</p>
","tensorflow, nlp, gensim, word-embedding, chainer","<p>For non-english words, you can try to use a bilingual dictionary to translate English words with embedding vectors.</p>

<p>You need a large corpus to generate high-quality word embeddings. For non-english, you need to add the bilingual constraints into the original w2v loss with the input of bilingual corpora.</p>

<p>You can regard the compound word as a whole word or split it according to your applications.  </p>
",0,0,644,2018-09-19 09:34:49,https://stackoverflow.com/questions/52402693/non-english-word-embedding-from-english-word-embedding
Why Gensim most similar in doc2vec gives the same vector as the output?,"<p>I am using the following code to get the ordered list of user posts.</p>

<pre><code>model = doc2vec.Doc2Vec.load(doc2vec_model_name)
doc_vectors = model.docvecs.doctag_syn0
doc_tags = model.docvecs.offset2doctag

for w, sim in model.docvecs.most_similar(positive=[model.infer_vector('phone_comments')], topn=4000):
        print(w, sim)
        fw.write(w)
        fw.write("" ("")
        fw.write(str(sim))
        fw.write("")"")
        fw.write(""\n"")

fw.close()
</code></pre>

<p>However, I am also getting the vector <code>""phone comments""</code> (that I use to find nearest neighbours) in like 6th place of the list. Is there any mistake I do in the code? or is it a issue in Gensim (becuase the vector cannot be a neighbour of itself)?</p>

<p><strong>EDIT</strong></p>

<p>Doc2vec model training code</p>

<pre><code>######Preprocessing
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for key, value in my_d.items():
    value = re.sub(""[^1-9a-zA-Z]"","" "", value)
    words = value.lower().split()
    tags = key.replace(' ', '_')
    docs.append(analyzedDocument(words, tags.split(' ')))

sentences = []  # Initialize an empty list of sentences
######Get n-grams
#Get list of lists of tokenised words. 1 sentence = 1 list
for item in docs:
    sentences.append(item.words)

#identify bigrams and trigrams (trigram_sentences_project) 
trigram_sentences_project = []
bigram = Phrases(sentences, min_count=5, delimiter=b' ')
trigram = Phrases(bigram[sentences], min_count=5, delimiter=b' ')

for sent in sentences:
    bigrams_ = bigram[sent]
    trigrams_ = trigram[bigram[sent]]
    trigram_sentences_project.append(trigrams_)

paper_count = 0
for item in trigram_sentences_project:
    docs[paper_count] = docs[paper_count]._replace(words=item)
    paper_count = paper_count+1

# Train model
model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 5, workers = 4, iter = 20)

#Save the trained model for later use to take the similarity values
model_name = user_defined_doc2vec_model_name
model.save(model_name)
</code></pre>
","nlp, data-mining, gensim, word2vec, doc2vec","<p>The <code>infer_vector()</code> method expects a list-of-tokens, just like the <code>words</code> property of the text examples (<code>TaggedDocument</code> objects, usually) that were used to train the model. </p>

<p>You're supplying a simple string, <code>'phone_comments'</code>, which will look to <code>infer_vector()</code> like the list <code>['p', 'h', 'o', 'n', 'e', '_', 'c', 'o', 'm', 'm', 'e', 'n', 't', 's']</code>. Thus your origin vector for the <code>most_similar()</code> is probably garbage. </p>

<p>Further, you're not getting back the input <code>'phone_comments'</code>, you're getting back the different string <code>'phone comments'</code>. If that's a tag-name in the model, then that must have been a supplied <code>tag</code> during model training. Its superficial similarity to <code>phone_comments</code> may be meaningless - they're different strings. </p>

<p>(But it may also hints that your training had problems, too, and trained the text that should have been <code>words=['phone', 'comments']</code> as <code>words=['p', 'h', 'o', 'n', 'e', ' ', 'c', 'o', 'm', 'm', 'e', 'n', 't', 's']</code> instead.)</p>
",1,0,538,2018-09-24 19:26:58,https://stackoverflow.com/questions/52486070/why-gensim-most-similar-in-doc2vec-gives-the-same-vector-as-the-output
Finding the distance between &#39;Doctag&#39; and &#39;infer_vector&#39; with Gensim Doc2Vec?,"<p>Using Gensim's Doc2Vec how would I find the distance between a <code>Doctag</code> and an <code>infer_vector()</code>?</p>

<p>Many thanks</p>
","python, gensim, doc2vec","<p><code>Doctag</code> is the internal name for the keys to doc-vectors. The result of an <code>infer_vector()</code> operation is a vector. So as you've literally asked, these aren't comparable. </p>

<p>You could ask a model for a known doc-vector, by its doc-tag key that was supplied during training, via <code>model.docvecs[doctag]</code>. That would be comparable to the result of an <code>infer_vector()</code> call. </p>

<p>With two vectors in hand, you can use <code>scipy</code> routines to calculate various kinds of distance. For example:</p>

<pre><code>import scipy.spatial.distance.cosine as cosine_distance
vec_by_doctag = model.docvecs[""doc0007""]
vec_by_inference = model.infer_vector(['a', 'cat', 'was', 'in', 'a', 'hat'])
dist = cosine_distance(vec_by_doctag, vec_by_inference)
</code></pre>

<p>You can also look at how gensim's <code>Doc2VecKeyedVectors</code> does similarity/distance between vectors that are known (by their doctag key names) inside a model, in its <code>similarity()</code> and <code>distance()</code> functions, at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/ca0dcaa1eca8b1764f6456adac5719309e0d8e6d/gensim/models/keyedvectors.py#L1701"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/ca0dcaa1eca8b1764f6456adac5719309e0d8e6d/gensim/models/keyedvectors.py#L1701</a></p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/ca0dcaa1eca8b1764f6456adac5719309e0d8e6d/gensim/models/keyedvectors.py#L1743"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/ca0dcaa1eca8b1764f6456adac5719309e0d8e6d/gensim/models/keyedvectors.py#L1743</a></p>
",1,1,820,2018-09-25 00:10:37,https://stackoverflow.com/questions/52488877/finding-the-distance-between-doctag-and-infer-vector-with-gensim-doc2vec
compare documents using most similar method,"<p>I am able to build the model using the built-in lee_background corpus. But when I try to compare using most_similar method, I get an error.</p>

<pre><code>lee_train_file = '/opt/conda/lib/python3.6/site-packages/gensim/test/test_data/lee_background.cor'

train_corpus=list()
with open(lee_train_file) as f:
    for i, line in enumerate(f):
        train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]))

model = gensim.models.doc2vec.Doc2Vec(vector_size=48, min_count=2, epochs=40)
model.build_vocab(train_corpus)
model.wv.vocab['penalty'].count
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)

line=""""""
dummy text here...
""""""

inferred_vector=model.infer_vector(gensim.utils.simple_preprocess(line) )

model.docvecs.most_similar(inferred_vector, topn=3)
</code></pre>

<p>I tried this with list(inferred_vector) but still getting an error.</p>

<blockquote>
  <p>TypeError: 'numpy.float32' object is not iterable</p>
</blockquote>

<p>I am trying to compare the dummy text with the corpus and find if the entry already exist in the data file.</p>

<hr>

<p>Update:
Instead of list(inferred_vector) I need to use [inferred_vector]. This has solved my problem. But ever-time I run this code, I get different similar documents. How is this possible?</p>

<pre><code>line=""""""
The national executive of the strife-torn Democrats last night appointed little-known West Australian senator Brian Greig 
as interim leader--a shock move likely to provoke further conflict between the party's senators and its organisation. 
In a move to reassert control over the party's seven senators, the national executive last night rejected Aden Ridgeway's 
bid to become interim leader, in favour of Senator John, a supporter of deposed leader Natasha Stott Despoja and an outspoken 
gay rights activist.
""""""

inferred_vector=model.infer_vector(gensim.utils.simple_preprocess(line))

model.docvecs.most_similar([inferred_vector], topn=5)
</code></pre>

<p>Sometimes I get this list and the list keeps changing everytime I run the code even if there is no change in the model.</p>

<pre><code>[(151, 0.5980586409568787),
 (74, 0.5736572742462158),
 (106, 0.5714541077613831),
 (249, 0.5695925951004028),
 (209, 0.5642371773719788)]

[(249, 0.5727256536483765),
 (151, 0.5725511312484741),
 (74, 0.5711895823478699),
 (106, 0.5583171248435974),
 (292, 0.5491517782211304)]
</code></pre>

<p>As a matter of fact, the first line in training corpus is 99% similar to this line because only 1 word is changed. Surprisingly the document_id 1 is nowhere in the top 5 list.</p>
","nlp, gensim","<p>The dummy line should be selected from lee_background.cor and not from lee.cor
The model text will match with training corpus and not with test corpus.</p>
",0,0,53,2018-10-01 03:56:34,https://stackoverflow.com/questions/52584376/compare-documents-using-most-similar-method
Doc2Vec: Similarity Between Coded Documents and Unseen Documents,"<p>I have a sample of ~60,000 documents.  We've hand coded 700 of them as having a certain type of content.  Now we'd like to find the ""most similar"" documents to the 700 we already hand-coded.  We're using gensim doc2vec and I can't quite figure out the best way to do this.</p>

<p>Here's what my code looks like:</p>

<pre><code>cores = multiprocessing.cpu_count()

model = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, 
        epochs=10, workers=cores, dbow_words=1, train_lbls=False)

all_docs = load_all_files() # this function returns a named tuple
random.shuffle(all_docs)
print(""Docs loaded!"")
model.build_vocab(all_docs)
model.train(all_docs, total_examples=model.corpus_count, epochs=5)
</code></pre>

<p>I can't figure out the right way to go forward.  Is this something that doc2vec can do?  In the end, I'd like to have a ranked list of the 60,000 documents, where the first one is the ""most similar"" document.</p>

<p>Thanks for any help you might have!  I've spent a lot of time reading the gensim help documents and the various tutorials floating around and haven't been able to figure it out.</p>

<p>EDIT: I can use this code to get the documents most similar to a short sentence:</p>

<pre><code>token = ""words associated with my research questions"".split()
new_vector = model.infer_vector(token)
sims = model.docvecs.most_similar([new_vector])
for x in sims:
    print(' '.join(all_docs[x[0]][0]))
</code></pre>

<p>If there's a way to modify this to instead get the documents most similar to the 700 coded documents, I'd love to learn how to do it!</p>
","python, nlp, gensim, word2vec, doc2vec","<p>Your general approach is reasonable. A few notes about your setup:</p>

<ul>
<li>you'd have to specify <code>epochs=10</code> in your <code>train()</code> call to truly get 10 training passes – and 10 or more is most common in published work</li>
<li><code>sample</code>-controlled downsampling helps speed training and often improves vector quality as well, and the value can become more aggressive (smaller) with larger datasets</li>
<li><code>train_lbls</code> is not a parameter to <code>Doc2Vec</code> in any recent <code>gensim</code> version</li>
</ul>

<p>There are several possible ways to interpret and pursue your goal of ""find the 'most similar' documents to the 700 we already hand-coded"". For example, for a candidate document, how should its similarity to the set-of-700 be defined - as a similarity to one summary 'centroid' vector for the full set? Or as its similarity to any one of the documents? </p>

<p>There are a couple ways you could obtain a single summary vector for the set:</p>

<ul>
<li><p>average their 700 vectors together</p></li>
<li><p>combine all their words into one synthetic composite document, and <code>infer_vector()</code> on that document. (But note: texts fed to <code>gensim</code>'s optimized word2vec/doc2vec routines face an internal implementation limit of 10,000 tokens – excess words are silently ignored.)</p></li>
</ul>

<p>In fact, the <code>most_similar()</code> method can take a list of multiple vectors as its 'positive' target, and will automatically average them together before returning its results. So if, say, the 700 document IDs (tags used during training) are in the list <code>ref_docs</code>, you could try...</p>

<pre><code>sims = model.docvecs.most_similar(positive=ref_docs, topn=0)
</code></pre>

<p>...and get back a ranked list of all other in-model documents, by their similarity to the average of all those <code>positive</code> examples. </p>

<p>However, the alternate interpretation, that a document's similarity to the reference-set is its highest similarity to any one document inside the set, might be better for your purpose. This could especially be the case if the reference set itself is varied over many themes – and thus not well-summarized by a single average vector.</p>

<p>You'd have to compute these similarities with your own loops. For example, roughly:</p>

<pre><code>sim_to_ref_set = {}
for doc_id in all_doc_ids:
    sim_to_ref_set[doc_id] = max([model.docvecs.similarity(doc_id, ref_id) for ref_id in ref_docs])
sims_ranked = sorted(sim_to_ref_set.items(), key=lambda it:it[1], reverse=True)
</code></pre>

<p>The top items in <code>sims_ranked</code> would then be those most-similar to any item in the reference set. (Assuming the reference-set ids are also in <code>all_doc_ids</code>, the 1st 700 results will be the chosen docs again, all with a self-similarity of <code>1.0</code>.)</p>
",0,0,3418,2018-10-07 21:18:10,https://stackoverflow.com/questions/52693004/doc2vec-similarity-between-coded-documents-and-unseen-documents
Cannot align graph because multiple tag doc2vec returning more items in doctag_syn0 than there are in the training data,"<p>I am training a doc2vec model with multiple tags, so it includes the typical doc ""ID"" tag and then it also contains a label tag ""Category 1."" I'm trying to graph the results such that I get the doc distribution in a 2d (using LargeVis) but am able to color different tags. My problem is that the vectors the model returns exceed the number of training observations by 5 making difficult to align the original tags with the vectors: </p>

<pre><code>In[1]: data.shape 
Out[1]: (17717,5)
</code></pre>

<p>Training the model on 100 parameters  </p>

<pre><code>In[2]: model.docvecs.doctag_syn0.shape
Out[2]: (17722,100) 
</code></pre>

<p>I have no idea whether the 5 additional observations shift the order of the vectors or whether they're just appended to the end. I want to avoid using string tags for the doc IDs because I am preparing this code to use on a much larger dataset.
I found an explanation in a google group <a href=""https://groups.google.com/forum/#!topic/gensim/OdvQkwuADl0"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/gensim/OdvQkwuADl0</a>
which explained that using multiple tags per doc can result in this type of output. However, I haven't been able to find a way to avoid or correct it in any forum or documentation. </p>
","python, machine-learning, nlp, gensim, doc2vec","<p>The number of doc-vectors learned will be equal to the number of unique tags you've supplied. It looks like perhaps you've supplied 17,717 unique-IDs and then 5 extra repeating category-tags. Thus, there are 17,722 total known doc-tags (and thus corresponding learned doc-vectors). So, this is expected behavior. </p>

<p>If you need to pass just the 17,717 per-doc vectors to some other process (like a dimensionality-reduction to 2-d), you'll have to pull them out of the model. You could pull them out 1-by-1 – <code>model.docvecs[doc_id]</code> – and put them into whatever form the next step needs. </p>

<p>If your doc-IDs happen to have been plain ints, from 0 to 17,716, then they will in fact be the first 17,716 entries in the <code>model.docvecs.doctag_syn0</code> array, which might make things easier - you may just be able to use a view into that array. (The last five rows will be the string tags.)</p>

<p>I would suggest doing all your steps first <em>without</em> the extra complication of adding the secondary category string tags. Such extra tags may help or hurt vector-usefulness for downstream tasks in different situations, but definitely (as you've seen) make things a bit more complicated. So getting baseline results and outputs, without that complication, may be helpful. </p>
",1,0,112,2018-10-08 17:07:49,https://stackoverflow.com/questions/52707075/cannot-align-graph-because-multiple-tag-doc2vec-returning-more-items-in-doctag-s
Document similarity with doc2vec,"<p>With this Gensim example in github, <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a> it provides examples at the end to find simalarities with phrases or keywords, like 'lady gaga' or 'machine learning'. However am looking to find similarity with actual document in plain text file, could this be done? and how can I do it? suppose text file is located on my local laptop in txt format.</p>
","python, nlp, gensim, doc2vec","<p>Tokenize the query-document the same as the training data. Pass those tokens to the <code>Doc2Vec</code> model's <code>infer_vector()</code> method to get a vector for the query-document. Pass that vector to <code>most_similar()</code> to get a ranked list of known documents similar to that vector.</p>

<p>There are examples of using <code>infer_vector()</code> this way in cells 10 and forward in another demo notebook included with <code>gensim</code>:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb</a></p>
",2,1,4252,2018-10-15 12:44:07,https://stackoverflow.com/questions/52817087/document-similarity-with-doc2vec
Data set for Doc2Vec general sentiment analysis,"<p>I am trying to build doc2vec model, using gensim + sklearn to perform sentiment analysis on short sentences, like comments, tweets, reviews etc.</p>

<p>I downloaded <a href=""http://jmcauley.ucsd.edu/data/amazon/"" rel=""nofollow noreferrer"">amazon product review data set</a>, <a href=""https://www.kaggle.com/c/twitter-sentiment-analysis2"" rel=""nofollow noreferrer"">twitter sentiment analysis data set</a> and <a href=""https://www.kaggle.com/utathya/imdb-review-dataset"" rel=""nofollow noreferrer"">imbd movie review data set</a>.</p>

<p>Then combined these in 3 categories, positive, negative and neutral.</p>

<p>Next I trinaed gensim doc2vec model on the above data so I can obtain the input vectors for the classifying neural net.</p>

<p>And used sklearn LinearReggression model to predict on my test data, which is about 10% from each of the above three data sets.</p>

<p>Unfortunately the results were not good as I expected. Most of the tutorials out there seem to focus only on one specific task, 'classify amazon reviews only' or 'twitter sentiments only', I couldn't manage to find anything that is more general purpose.</p>

<p>Can some one share his/her thought on this? </p>
","dataset, artificial-intelligence, gensim, sentiment-analysis, doc2vec","<p>How good did you expect, and how good did you achieve? </p>

<p>Combining the three datasets may not improve overall sentiment-detection ability, if the signifiers of sentiment vary in those different domains. (Maybe, 'positive' tweets are very different in wording than product-reviews or movie-reviews. Tweets of just a few to a few dozen words are often quite different than reviews of hundreds of words.) Have you tried each separately to ensure the combination is helping? </p>

<p>Is your performance in line with other online reports of using roughly the same pipeline (Doc2Vec + LinearRegression) on roughly the same dataset(s), or wildly different? That will be a clue as to whether you're doing something wrong, or just have too-high expectations. </p>

<p>For example, the <code>doc2vec-IMDB.ipynb</code> notebook bundled with <code>gensim</code> tries to replicate an experiment from the original 'Paragraph Vector' paper, doing sentiment-detection on an IMDB dataset. (I'm not sure if that's the same dataset as you're using.) Are your results in the same general range as that notebook achieves? </p>

<p>Without seeing your code, and details of your corpus-handling &amp; parameter choices, there could be all sorts of things wrong. Many online examples have nonsense choices. But maybe your expectations are just off.</p>
",1,0,388,2018-10-16 19:11:06,https://stackoverflow.com/questions/52842474/data-set-for-doc2vec-general-sentiment-analysis
gensim Doc2Vec: Getting from txt files to TaggedDocuments,"<p>Beginner here. </p>

<p>I have a large body of .txt files that I want to train a Doc2Vec model on. However, I am having trouble importing the data into python in a usable way.</p>

<p>To import data, I have used:</p>

<pre><code>docLabels = []
docLabels = [f for f in listdir(“PATH TO YOU DOCUMENT FOLDER”) if 
f.endswith(‘.txt’)]
data = []
for doc in docLabels:
    data.append(open(‘PATH TO YOU DOCUMENT FOLDER’ + doc).read()) `
</code></pre>

<p>However, with this, I get a ""list"", which I can do no further work with. I cannot seem to find how to import text files in a way they can be used with NLTK / doc2vec anywhere on SO or in tutorials.</p>

<p>Help would be greatly appreciated. Thank you!</p>
","python, gensim, doc2vec","<p>I'm only addressing the portion of the question indicated by the title, about <code>Doc2Vec</code> and <code>TaggedDocument</code>. (NLTK is a separate matter.)</p>

<p>The <code>TaggedDocument</code> class requires you to specify <code>words</code> and <code>tags</code> for each object created. </p>

<p>So where you are currently just appending a big full-read of the file to your <code>data</code>, you will instead want to:</p>

<ul>
<li>break that data into words – one super-simple way is to just <code>.split()</code> it on whitespace, though most project do more</li>
<li>decide on a tag or tags, perhaps just the filename itself</li>
<li>instantiate a <code>TaggedDocument</code>, and append that to <code>data</code></li>
</ul>

<p>So, you could replace your existing loop with:</p>

<pre><code>for doc in docLabels:
    words = open(open(‘PATH TO YOU DOCUMENT FOLDER’ + doc).read()).split()
    tags = [doc]
    data.append(TaggedDocument(words=words, tags=tags)
</code></pre>
",1,0,1072,2018-10-19 13:08:32,https://stackoverflow.com/questions/52893017/gensim-doc2vec-getting-from-txt-files-to-taggeddocuments
"Gensim Keywords, how to load a german model?","<p>I'm try to get started with the gensim library. My goal is pretty simple. I want to use the keywords extraction provided by gensim on a german text. Unfortunately, i'm failing hard.</p>

<p>Gensim comes with a keywords extraction build in, it is build on TextRank. While the results look good on english text, it seems not to work on german. I simple installed gensim via pypi and used it out of the box. Well such AI Products are usually driven by a model. My guess is that gensim comes with a english model. A word2vec model for german is available on a <a href=""https://github.com/devmount/GermanWordEmbeddings"" rel=""nofollow noreferrer"">github page</a>.</p>

<p>But here i'm stuck, i can't find a way how the summarization module of gensim, which provides the <a href=""https://radimrehurek.com/gensim/summarization/keywords.html"" rel=""nofollow noreferrer"">keywords function</a> i'm looking for, can work with a external model.</p>

<p>So the basic question is, how do i load the german model and get keywords from german text?</p>

<p>Thanks</p>
","nlp, keyword, gensim, word2vec","<p>There's nothing in the <code>gensim</code> docs, or the <a href=""https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf"" rel=""nofollow noreferrer"">original TextRank paper</a> (from 2004), suggesting that algorithm requires a Word2Vec model as input. (Word2Vec was 1st published around 2013.) It just takes word-tokens. </p>

<p>See examples of its use in the tutorial notebook that's included with <code>gensim</code>:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/summarization_tutorial.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/summarization_tutorial.ipynb</a></p>

<p>I'm not sure the same algorithm would work as well on German text, given the differing importance of compound words. (To my eyes, TextRank isn't very impressive with English, either.) You'd have to check the literature to see if it still gives respected results. (Perhaps some sort of extra stemming/intraword-tokenizing/canonicalization would help.)</p>
",0,0,2328,2018-10-21 11:14:26,https://stackoverflow.com/questions/52914701/gensim-keywords-how-to-load-a-german-model
Tensorboard projector will compute PCA endlessly,"<p>I have just over 100k word embeddings which I created using gensim, originally each containing 200 dimensions. I've been trying to visualize them within tensorboard's projector but I have only failed so far.
My problem is that tensorboard seems to freeze while computing PCA. At first, I left the page open for 16 hours, imagining that it was just too much to be calculated, but nothing happened. At this point, I started to try and test different scenarios just in case all I needed was more time and I was trying to rush things. The following is a list of my testing so far, all of which failed at the same spot, computing PCA:</p>

<ul>
<li>I plotted only 10 points of 200 dimensions;</li>
<li>I retrained my gensim model so that I could reduce its dimensionality to 100;</li>
<li>Then I reduced it to 10;</li>
<li>Then to 2;</li>
<li>Then I tried plotting only 2 points, i.e. 2 two dimensional points;</li>
</ul>

<p>I am using Tensorflow 1.11;
You can find my last saved tensor flow session <a href=""https://drive.google.com/open?id=10Cnzc2RH9nDFUYrf51r6d8d0jsl0-H6Y"" rel=""nofollow noreferrer"">here</a>, would you mind trying it out?</p>

<p>I am still a beginner, therefore I used a couple tutorial to get me started; I used <a href=""https://github.com/sudharsan13296/visualise-word2vec/blob/master/Word2vec%20Embeddings.ipynb"" rel=""nofollow noreferrer"">Sud Harsan</a> work so far.</p>

<p>Any help is much appreciated. Thanks.</p>

<hr>

<p>Updates:</p>

<p>A) I've found someone else <a href=""https://stackoverflow.com/questions/44054907/tensorfboard-embeddings-hangs-with-computing-pca"">dealing with the same problem</a>; I tried the solution provided, but it didn't change anything. </p>

<p>B) I thought it could have something to do with my installation, therefore I tried uninstalling tensorflow and installing it back; no luck. I then proceeded to create a new environment dedicated to tensorflow and that also didn't work. </p>

<p>C) Assuming there was something wrong with my code, I ran <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"" rel=""nofollow noreferrer"">tensorflow's basic embedding tutorial</a> to check if I could open its projector's results. And guess what?! I still can't go past ""Calculating PCA""</p>

<p>Now, I did visit <a href=""https://projector.tensorflow.org"" rel=""nofollow noreferrer"">the online projector example</a> and that loads perfectly. </p>

<p>Again, Any help would be more than appreciated. Thanks!</p>
","tensorflow, pca, gensim, tensorboard","<p>As mentioned by Bluedrops, updating tensorboard and tensorflow seems to fix the problem.</p>

<p>I created a new environment with conda and installed the newest versions of Tensorflow, Tensorboard and their dependencies and that seems to fix the issue.</p>
",1,4,1364,2018-10-24 23:42:34,https://stackoverflow.com/questions/52979374/tensorboard-projector-will-compute-pca-endlessly
How to load files to Google-App-Engine in standard enviroment,"<p>I am using Google-App-Engine standard (Not flex) Enviroment with Python2.7, and I need to load some pre-trained models (Gensim's Word2vec and Keras's LSTM).</p>

<p>I need to load it once (since it very slow - takes around 1.5 seconds) and keep it in faster access for several hours.</p>

<p>What is the best &amp; fastest way to do so? </p>

<p>Thanks!</p>
","python, google-app-engine, keras, gensim, google-app-engine-python","<p>IMHO the best place for read-only data (including imported code!) needed to be accessed at any time by individual requests is the global application variables area.</p>

<p>Such variables would typically be loaded exactly once per GAE instance lifetime and available until the instance goes away. </p>

<p>Since loading of the data is expensive you need to be aware that it could impact the response time for requests coming in while the instance is starting up (i.e. while the loading request is still active). There are 2 ways to address this:</p>

<ul>
<li><p>one would be to use ""lazy"" loading of the data - effective if just a small percentage of the incoming requests actually need the data. But the requests which actually need the data when it's not available will still be affected, so it'll just reduce the impact of the problem. The method is described in detail in the <a href=""https://medium.com/google-cloud/app-engine-startup-time-and-the-global-variable-problem-7ab10de1f349"" rel=""nofollow noreferrer"">App Engine Startup time and the Global Variable problem</a> article:</p>

<pre><code>from google.appengine.ext import ndb
# a global variable
gCDNServer = None
def getCDN():
    global gCDNServer
    if gCDNServer==None:
        gCDNServer = Settings.query(Settings.name == ""gCDNServer"").value
    return gCDNServer
</code></pre></li>
<li><p>another approach, which would completely eliminate the problem, is to make your app support <a href=""https://cloud.google.com/appengine/docs/standard/python/configuring-warmup-requests"" rel=""nofollow noreferrer"">warmup requests</a> (available only if you're using automatic scaling). The data would be loaded by the warmup request handler and will always be available for ""live"" requests (because no ""live"" requests will be routed to the instance until the warmup request handling completes).</p></li>
</ul>

<p>It <em>might</em> be possible to add logic to drop the data from memory (to reduce the app's memory footprint) if/when you know it'll no longer be needed (i.e. after those several hours you mentioned expired), but that complicates the picture, especially if you configured your app as <code>threadsafe</code>. I'd simply separate the code which doesn't need the data from the one which does in different services and leave autoscaling shut down the instances with the global data when no longer needed.</p>
",2,3,230,2018-10-28 09:37:13,https://stackoverflow.com/questions/53030121/how-to-load-files-to-google-app-engine-in-standard-enviroment
Misunderstanding the use of filter_extreme in gensim,"<pre><code>import gensim
corpus = [[""a"",""b"",""c""],[""a"",""d"",""e""],[""a"",""f"",""g""]]
from gensim.corpora import Dictionary
dct = Dictionary(corpus)
print(dct)
dct.filter_extremes(no_below=1)
print(dct)
</code></pre>

<p>When I ran the code above, my output was -</p>

<pre><code>Dictionary(7 unique tokens: ['a', 'b', 'c', 'd', 'e']...)
Dictionary(6 unique tokens: ['b', 'c', 'd', 'e', 'f']...)
</code></pre>

<p>I supposed that since 'a' occurs in two documents, it should not be removed. However, this is not the case. Am I missing something?</p>
","python, python-2.7, gensim","<p>Looking at the <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes"" rel=""noreferrer"">documentation of <code>filter_extremes</code></a>:</p>

<pre><code>filter_extremes(no_below=5, no_above=0.5, keep_n=100000, keep_tokens=None)

Notes:    
This removes all tokens in the dictionary that are:

    1. Less frequent than no_below documents (absolute number, e.g. 5) or
    2. More frequent than no_above documents (fraction of the total corpus size, e.g. 0.3).
    3. After (1) and (2), keep only the first keep_n most frequent tokens (or keep all if keep_n=None).
</code></pre>

<p>You are only passing <code>no_below=1</code>. This means that tokens appearing in less than 1 document (out of 3) are removed. This means <code>a</code> stays, as well as any other token in your corpus.</p>

<p>But then <code>no_above=0.5</code> is checked according to its default value, since you didn't pass an explicit value for this keyword. This means that tokens appearing in more than 50% of documents (out of 3, i.e. the ones appearing in at least 2) will be removed. And <code>'a'</code> appears in all 3 documents, it's the only one that appears in at least 2 documents as a matter of fact. This is why this token and this token alone is removed from the result. (The default 10000 value for <code>keep_n</code> implies that step 3 is a no-op in your example case.)</p>

<p>If you <em>only</em> want to strip low-frequency extremal tokens, pass an explicit <code>no_above=1.0</code> to <code>filter_extremes</code>.</p>
",5,2,2206,2018-10-29 00:39:53,https://stackoverflow.com/questions/53037373/misunderstanding-the-use-of-filter-extreme-in-gensim
"Gensim example, TypeError:between str and int error","<p>When running the below code. this Python 3.6, latest Gensim library in Jupyter</p>

<pre><code>for model in models:
       print(str(model))
       pprint(model.docvecs.most_similar(positive=[""Machine learning""], topn=20))
</code></pre>

<p>[1]: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a><img src=""https://i.sstatic.net/wI25F.png"" alt=""enter image description here""></p>
","python, nlp, gensim","<pre><code>string= ""machine learning"".split()

doc_vector = model.infer_vector(string)
out= model.docvecs.most_similar([doc_vector])
</code></pre>

<p>I'm not sure 100% since I'm using a more recent release, but I think that the issue is connected to the fact that the most_similar function is expecting a string mapped in the feature space and not the raw string.</p>
",0,-2,369,2018-11-03 11:10:38,https://stackoverflow.com/questions/53130738/gensim-example-typeerrorbetween-str-and-int-error
nested loop over list and dynamically create variables,"<p>I have a list of sentences and I want to perform some action on two sentences each time, but not for al of the sentences.</p>

<p>for example:</p>

<pre><code>list= [""aaaaa"",""bbbbb"",""ccccc"",""ddddd"",""eeeee""]
similarity_a-d = sim(""aaaaa"",""ddddd"")
similarity_a-e = sim(""aaaaa"",""eeeee"")
similarity_b-d = sim(""bbbbb"",""ddddd"")
similarity_b-e = sim(""bbbbb"",""eeeee"")
similarity_c-d = sim(""ccccc"",""ddddd"")
similarity_c-e = sim(""ccccc"",""eeeee"")
</code></pre>

<p>That's what I tried:</p>

<pre><code>similarity={}
for i,vec_lda_topic in enumerate(vec_lda_topics)[:numOfUSs]:
    for j,vec_lda_topic in enumerate(vec_lda_topics)[numOfUSs:]:
        similarity[""sim{0}-{1}"".format(i,j)] = gensim.matutils.cossim(vec_lda_topics[i], vec_lda_topics[j])
        print('similarity between docs ', i, ' and ',j,': ', similarity[""sim{0}-{1}"".format(i,j)])
</code></pre>

<p>and receive the following error:</p>

<pre><code>TypeError: 'enumerate' object is not subscriptable
</code></pre>

<p>And besides the error, maybe there is a better way to do this?</p>
","python, python-3.x, for-loop, gensim","<p>enumerate the sliced list, don't slice the enumerated list (do this in both places)</p>

<pre><code>for i,vec_lda_topic in enumerate(vec_lda_topics[:numOfUSs]):
</code></pre>
",4,4,118,2018-11-05 17:38:13,https://stackoverflow.com/questions/53159443/nested-loop-over-list-and-dynamically-create-variables
How to use scraped data from website to Word2vec Gensim,"<p>I'm pretty new to MySQL, Gensim, and Word2Vec, and I'm still learning how to use by working on my personal project.</p>
<p>I have data that I got by doing web scraping so it's not hard coded.
(I used Instagram account to get hashtag data from several post, so my data is
Instagram hashtags)</p>
<p>I'm trying to use that data in this code below:</p>
<pre><code>import pymysql.cursors
import re
from gensim.models import Word2Vec

# Connect to the database
connection = pymysql.connect(host=secrets[0],
user=username,
password=password,
db='test',
charset='charsetExample',
cursorclass=pymysql.cursors.DictCursor)

try:
    # connection to database
    with connection.cursor() as cursor:
    # cursor is iterator / 'Select' - caption is column 
     # post is the table 
     cursor.execute(&quot;SELECT caption FROM posts LIMIT 1000&quot;)
     data = cursor.fetchall()
     # list of captions
      captions = [d['caption'].lower() for d in data]
     # hashtags = [re.findall(r&quot;#([A-Za-z_0-9]+)&quot;, caption) for caption in captions]
    # hashtags = [hashtag for hashtag in hashtags if hashtag != []]
    model = Word2Vec(captions, min_count=1)
    model = Word2Vec(hashtags) 
    res = model.wv.most_similar(&quot;fitness&quot;)

    print(captions)
    print(res)

finally:
    connection.close()
</code></pre>
<p>This is the part that I'm working on and not really sure how to do:</p>
<pre><code>res = model.wv.most_similar(&quot;fitness&quot;)
</code></pre>
<p>For now I was trying to use <code>most_similar()</code> method to see how it works.
What I'm trying to do is in the <code>most_similar(&quot;value&quot;)</code> I want to use my data
which will be each hashtags that I got by scraping the Instagram website as the value.</p>
<p>Thank you!</p>
","python, mysql, gensim, word2vec","<p>OK, so, You have to train the word2vec model yourself. What you have to do is make sure that your hashtags are actually without <code>#</code> sign and lowercase.</p>

<p>Now, group the hashtags by post. So, if some post had hashtags <code>#red</code>, <code>#Wine</code>, <code>#party</code>, you should make a list from it to look like: <code>[red, wine, party]</code>. Repeat that for every post and save a list from every post to a new list. So the output from this should be list of lists: <code>[[red, wine, party], [post_2_hashtags], ...]</code>. Now you can input that to the word2vec model and train it with this line:</p>

<pre><code>model = gensim.models.Word2Vec(
    documents,
    size=150,
    window=10,
    min_count=2,
    workers=10)
model.train(documents, total_examples=len(documents), epochs=10)
model.save(""word2vec.model"")
</code></pre>

<p><code>documents</code> is a list of lists created in the previous step. You can then load the model with <code>model = gensim.models.Word2Vec.load(""word2vec.model"")</code>. And the rest is the same. You still use <code>most_similar()</code> method to get the most similar word (in this case, hashtag). </p>

<p>The only thing you have to be aware of is the vector size (<code>size</code> parameter in <code>word2vec.model</code>). You define it before training. If you have a lot of data, you set it to be a bigger number, and if you have smaller amount of data, set it to smaller number. But this is something you have to figure out since you're the only one that can see the data you have. Try playing with the <code>size</code> parameter and evaluate the model with <code>most_similar()</code> method. </p>

<p>I hope this is clear enough :)</p>
",0,0,393,2018-11-07 10:05:24,https://stackoverflow.com/questions/53187257/how-to-use-scraped-data-from-website-to-word2vec-gensim
Python - Data Encoding Vector To Word,"<p>I have a code that converts word to vector. Below is my code:</p>

<pre><code># word_to_vec_demo.py

from gensim.models import word2vec
import logging

logging.basicConfig(format='%(asctime)s : \
%(levelname)s : %(message)s', level=logging.INFO)

sentences = [['In', 'the', 'beginning', 'Abba','Yahweh', 'created', 'the',
'heaven', 'and', 'the', 'earth.', 'And', 'the', 'earth', 'was',
'without', 'form,', 'and', 'void;', 'and', 'darkness', 'was',
'upon', 'the', 'face', 'of', 'the', 'deep.', 'And', 'the',
'Spirit', 'of', 'Yahweh', 'moved', 'upon', 'the', 'face',  'of',
'the', 'waters.']]

model = word2vec.Word2Vec(sentences, size=10, min_count=1)

print(""Vector for \'earth\' is: \n"")
print(model.wv['earth'])

print(""\nEnd demo"")
</code></pre>

<p>The output is </p>

<pre><code>Vector for 'earth' is: 

[-0.00402722  0.0034133   0.01583795  0.01997946  0.04112177  0.00291858
-0.03854967  0.01581967 -0.02399057  0.00539708]
</code></pre>

<p>Is it possible to encode from array of vector to words? If yes, how will I implement it in Python?</p>
","python, machine-learning, nlp, gensim, word2vec","<p>You can use the <a href=""https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.similar_by_vector.html"" rel=""nofollow noreferrer"">similar_by_vector()</a> method from your model to find the top-N most similar words by vector.
Hope this helps.</p>
",2,0,240,2018-11-12 15:12:41,https://stackoverflow.com/questions/53265028/python-data-encoding-vector-to-word
glove most similar to multiple words,"<p>I am supposed to do some exercises with python glove, most of it doesn't give me any problems but now i am supposed to find the 5 most similar words to ""norway - war + peace"" from the ""glove-wiki-gigaword-100"" package. But when i run my code it just says that the 'word' is not in the vocabulary. Now I'm guessing that this is some kind of formatting, but i don't know how to use it.</p>

<pre><code>import gensim.downloader as api
model = api.load(""glove-wiki-gigaword-100"")  # download the model and return as object ready for use

bests = model.most_similar(""norway - war + peace"", topn= 5)

print(""5 most similar words to 'norway - war + peace':"")

for best in bests:
    print(best)
</code></pre>
","python, nlp, gensim, glove","<p>Gensim's model word2vec only deals with previously seen words. Here you give an entire sentence... What you want to do is:</p>

<ol>
<li>get vectors v1, v2 and v3 for resp. words ""norway"", ""war"" and ""peace"".</li>
<li>Compute the math: v = v1 -v2 + v3.</li>
<li>get the most_similar words to v.  </li>
</ol>

<p>To do so, you will need these functions: <code>model.wv.most_similar()</code> and <code>model.wv.similar_by_vector()</code>. Note that <code>model.wv.most_similar()</code> does something similar to these three steps but in a more complicated way using a set of positive words and a set of negative words. See the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors"" rel=""nofollow noreferrer"">documentation</a> for details.</p>
",2,0,3684,2018-11-13 13:10:21,https://stackoverflow.com/questions/53281744/glove-most-similar-to-multiple-words
Python/Gensim - What is the meaning of syn0 and syn0norm?,"<p>I know that in <em>gensims</em> <em><code>KeyedVectors</code>-model</em>, one can access the embedding matrix by the attribute <code>model.syn0</code>. There is also a <code>syn0norm</code>, which doesn't seem to work for the <em>glove</em> model I recently loaded. I think I also have seen <code>syn1</code> somewhere previously. </p>

<p>I haven't found a doc-string for this and I'm just wondering what's the logic behind this?</p>

<p>So if <code>syn0</code> is the embedding matrix, what is <code>syn0norm</code>? What would then <code>syn1</code> be and generally, what does <code>syn</code> stand for?</p>
","python, deep-learning, nlp, gensim, word-embedding","<p>These names were inherited from the original Google <code>word2vec.c</code> implementation, upon which the <code>gensim</code> <code>Word2Vec</code> class was based. (I believe <code>syn0</code> only exists in recent versions for backward-compatbility.)</p>

<p>The <code>syn0</code> array essentially holds raw word-vectors. From the perspective of the neural-network used to train word-vectors, these vectors are a 'projection layer' that can convert a one-hot encoding of a word into a dense embedding-vector of the right dimensionality. </p>

<p>Similarity operations tend to be done on the <em>unit-normalized</em> versions of the word-vectors. That is, vectors that have all been scaled to have a magnitude of 1.0. (This makes the cosine-similarity calculation easier.) The <code>syn0norm</code> array is filled with these unit-normalized vectors, the first time they're needed. </p>

<p>This <code>syn0norm</code> will be empty until either you do an operation (like <code>most_similar()</code>) that requires it, or you explicitly do an <code>init_sims()</code> call. If you explicitly do an <code>init_sims(replace=True)</code> call, you'll actually clobber the raw vectors, in-place, with the unit-normed vectors. This saves the memory that storing both vectors for every word would otherwise require. (However, some word-vector uses may still be interested in the original raw vectors of varying magnitudes, so only do this when you're sure <code>most_similar()</code> cosine-similarity operations are all you'll need.)</p>

<p>The <code>syn1</code> (or <code>syn1neg</code> in the more common case of negative-sampling training) properties, when they exist on a full model (and not for a plain <code>KeyedVectors</code> object of only word-vectors), are the model neural network's internal 'hidden' weights leading to the output nodes. They're needed during model training, but not a part of the typical word-vectors collected after training. </p>

<p>I believe the <code>syn</code> prefix is just a convention from neural-network variable-naming, likely derived from 'synapse'. </p>
",10,9,8697,2018-11-14 13:56:33,https://stackoverflow.com/questions/53301916/python-gensim-what-is-the-meaning-of-syn0-and-syn0norm
Genisim doc2vec: how is short doc processed?,"<p>In each tiny step of doc2vec training process, it takes a word and its neighbors within certain length(called window size). The neighbors are summed up, averaged, or concated, and so on and so on.</p>

<p>My question is, what if the window exceed the boundary of a certain doc, like 
<a href=""https://i.sstatic.net/Iy5e0.png"" rel=""nofollow noreferrer"">this</a></p>

<p>Then how are the neighbors summed up, averaged, or concated? Or they are just simply discarded? </p>

<p>I am doing some nlp work and most doc in my dataset are quite short. Appeciate for any idea.</p>
","machine-learning, nlp, gensim, doc2vec","<p>The pure PV-DBOW mode (<code>dm=0</code>), which trains quickly and often performs very well (especially on short documents), makes use of no sliding <code>window</code> at all. Each per-document vector is just trained to be good at directly predicting the document's words - neighboring words don't make any difference. </p>

<p>Only when you either switch to PV-DM mode (<code>dm=1</code>), or add interleaved skip-gram word-vector training (<code>dm=0, dbow_words=1</code>) is the <code>window</code> relevant. And then, the window is handled the same as in Word2Vec training: if it would go past either end of the text, it's just truncated to not go over the end, perhaps leaving the effective window lop-sided. </p>

<p>So if you have a text ""A B C D E"", and a <code>window</code> of 2, when predicting the 1st word 'A', only the 'B' and 'C' to the right contribute (because there are zero words to the left). When predicting the 2nd word 'B', the 'A' to the left and the 'C' and 'D' to the right contribute. And so forth. </p>

<p>An added wrinkle is that to effect a stronger weighting of nearby words in a computationally-efficient manner, the actual window used for any one target prediction is actually of a random size from 1 up to the configured <code>window</code> value. So for <code>window=2</code>, half the time it's really only using a window of 1 on each side, and the other half the time using the full window of 2. (For <code>window=5</code>, it's using an effective value of 1 for 20% of the predictions, 2 for 20% of the predictions, 3 for 20% of the predictions, 4 for 20% of the predictions, and 5 for 20% of the predictions.) This effectively gives nearer words more influence, without the full computational cost of including all full-window words every time or any extra partial-weighting calculations.</p>
",3,1,73,2018-11-19 05:41:32,https://stackoverflow.com/questions/53368915/genisim-doc2vec-how-is-short-doc-processed
Gensim Doc2vec model: how to compute similarity on a corpus obtained using a pre-trained doc2vec model?,"<p>I have a model based on <code>doc2vec</code> trained on multiple documents. I would like to use that model to infer the vectors of another document, which I want to use as the corpus for comparison. So, when I look for the most similar sentence to one I introduce, it uses this new document vectors instead of the trained corpus.
Currently, I am using the <code>infer_vector()</code> to compute the vector for each one of the sentences of the new document, but I can't use the <code>most_similar()</code> function with the list of vectors I obtain, it has to be <code>KeyedVectors</code>.</p>

<p>I would like to know if there's any way that I can compute these vectors for the new document that will allow the use of the <code>most_similar()</code> function, or if I have to compute the similarity between each one of the sentences of the new document and the sentence I introduce individually (in this case, is there any implementation in Gensim that allows me to compute the cosine similarity between 2 vectors?).</p>

<p>I am new to Gensim and NLP, and I'm open to your suggestions.</p>

<p>I can not provide the complete code, since it is a project for the university, but here are the main parts in which I'm having problems.</p>

<p>After doing some pre-processing of the data, this is how I train my model:</p>

<pre><code>documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_data)]
assert gensim.models.doc2vec.FAST_VERSION &gt; -1

cores = multiprocessing.cpu_count()

doc2vec_model = Doc2Vec(vector_size=200, window=5, workers=cores)
doc2vec_model.build_vocab(documents)
doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=30)
</code></pre>

<p>I try to compute the vectors for the new document this way:</p>

<pre><code>questions = [doc2vec_model.infer_vector(line) for line in lines_4]
</code></pre>

<p>And then I try to compute the similarity between the new document vectors and an input phrase:</p>

<pre><code>text = str(input('Me: '))

tokens = text.split()

new_vector = doc2vec_model.infer_vector(tokens)

index = questions[i].most_similar([new_vector])
</code></pre>
","python, nlp, gensim, doc2vec","<p>A dirty solution I used about a month ago in gensim==3.2.0 (the syntax might have changed).</p>

<p>You can save your inferred vectors in KeyedVectors format.</p>

<pre><code>from gensim.models import KeyedVectors
from gensim.models.doc2vec import Doc2Vec
vectors = dict()
# y_names = doc2vec_model.docvecs.doctags.keys()
y_names = range(len(questions))

for name in y_names:
    # vectors[name] = doc2vec_model.docvecs[name]
    vectors[str(name)] = questions[name]
f = open(""question_vectors.txt"".format(filename), ""w"")
f.write("""")
f.flush()
f.close()
f = open(""question_vectors.txt"".format(filename), ""a"")
f.write(""{} {}\n"".format(len(questions), doc2vec_model.vector_size))
for v in vectors:
    line = ""{} {}\n"".format(v, "" "".join(questions[v].astype(str)))
    f.write(line)
f.close()
</code></pre>

<p>then you can load and use most_similar function</p>

<pre><code>keyed_model = KeyedVectors.load_word2vec_format(""question_vectors.txt"")
keyed_model.most_similar(str(list(y_names)[0]))
</code></pre>

<p>Another solution (esp. if the number of questions is not so high) would be just to convert questions to a np.array and get cosine distance), e.g.</p>

<pre><code>import numpy as np

questions = np.array(questions)
texts_norm = np.linalg.norm(questions, axis=1)[np.newaxis].T
norm = texts_norm * texts_norm.T

product = np.matmul(questions, questions.T)
product = product.T / norm

# Otherwise the item is the closest to itself
for j in range(len(questions)):
    product[j, j] = 0

# Gives the top 10 most similar items to the 0th question
np.argpartition(product[0], 10)
</code></pre>
",2,2,2112,2018-11-19 14:11:13,https://stackoverflow.com/questions/53376459/gensim-doc2vec-model-how-to-compute-similarity-on-a-corpus-obtained-using-a-pre
fixed-size topics vector in gensim LDA topic modelling for finding similar texts,"<p>I use gensim LDA topic modelling to find topics for each document and to check the similarity between documents by comparing the received topics vectors.
Each document is given a different number of matching topics, so the comparison of the vector (by cosine similarity) is incorrect because vectors of the same length are required.</p>

<p>This is the related code:</p>

<pre><code>lda_model_bow = models.LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=3, passes=1, random_state=47)

#---------------Calculating and Viewing the topics----------------------------
vec_bows = [dictionary.doc2bow(filtered_text.split()) for filtered_text in filtered_texts]

vec_lda_topics=[lda_model_bow[vec_bow] for vec_bow in vec_bows]

for id,vec_lda_topic in enumerate(vec_lda_topics):
    print ('document ' ,id, 'topics: ', vec_lda_topic)
</code></pre>

<p>The output vectors is:</p>

<pre><code>document  0 topics:  [(1, 0.25697246), (2, 0.08026043), (3, 0.65391296)]
document  1 topics:  [(2, 0.93666667)]
document  2 topics:  [(2, 0.07910537), (3, 0.20132676)]
.....
</code></pre>

<p>As you can see, each vector has a different length, so it is not possible to perform cosine similarity between them.</p>

<p>I would like the output to be:</p>

<pre><code>document  0 topics:  [(1, 0.25697246), (2, 0.08026043), (3, 0.65391296)]
document  1 topics:  [(1, 0.0), (2, 0.93666667), (3, 0.0)]
document  2 topics:  [(1, 0.0), (2, 0.07910537), (3, 0.20132676)]
.....
</code></pre>

<p>Any ideas how to do it? tnx</p>
","python, gensim, lda, topic-modeling, cosine-similarity","<p>So as <a href=""https://stackoverflow.com/users/10393194/panktijk"">panktijk</a> says in the comment and also <a href=""https://stackoverflow.com/questions/45310925/how-to-get-a-complete-topic-distribution-for-a-document-using-gensim-lda"">this topic</a> , the solution is to cange <code>minimum_probability</code> from the default value of <code>0.01</code> to <code>0.0</code>.</p>
",0,1,1193,2018-11-21 17:02:37,https://stackoverflow.com/questions/53417171/fixed-size-topics-vector-in-gensim-lda-topic-modelling-for-finding-similar-texts
How to sentence embed from gensim Word2Vec embedding vectors?,"<p>I have a <code>pandas</code> dataframe containing descriptions. I would like to cluster descriptions based on meanings usign <code>CBOW</code>. My challenge for now is to document embed each row into equal dimensions vectors. At first I am training the word vectors using <code>gensim</code> as so:</p>

<pre><code>from gensim.models import Word2Vec

vocab = pd.concat((df['description'], df['more_description']))
model = Word2Vec(sentences=vocab, size=100, window=10, min_count=3, workers=4, sg=0)
</code></pre>

<p>I am however a bit confused now on how to replace the full sentences from my <code>df</code> with document vectors of equal dimensions.</p>

<p>For now, my workaround is repacing each word in each row with a vector then applying PCA dimentinality reduction to bring each vector to similar dimensions. Is there a better way of doing this though <code>gensim</code>, so that I could say something like this:</p>

<pre><code>df['description'].apply(model.vectorize)
</code></pre>
","python-3.x, gensim, word2vec, word-embedding, doc2vec","<p>I think you are looking for sentence embedding. There are a lot ways of generating sentence embedding from word embeddings. You may find this useful: <a href=""https://stats.stackexchange.com/questions/286579/how-to-train-sentence-paragraph-document-embeddings"">https://stats.stackexchange.com/questions/286579/how-to-train-sentence-paragraph-document-embeddings</a></p>
",1,1,4581,2018-11-22 12:26:22,https://stackoverflow.com/questions/53430997/how-to-sentence-embed-from-gensim-word2vec-embedding-vectors
how to use cosssim in gensim,"<p>My questioon is about cossim usage.</p>

<p>I have this fragment of a very big fuction:</p>

<pre><code>for elem in lList:
    temp = []
    try:
        x = dict(np.ndenumerate(np.asarray(model[elem])))
    except:
        if x not in embedDict.keys():
            x = np.random.uniform(low=0.0, high=1.0, size=300)
            embedDict[elem] = x
        else:
            x  =  dict(np.ndenumerate(np.asarray(embedDict[elem])))

    for w in ListWords:
        try:
            y =  dict(np.ndenumerate(np.asarray(model[w])))
        except:
            if y not in embedDict.keys():
                y = np.random.uniform(low=0.0, high=1.0, size=300)
                embedDict[w] = y
            else:
                y =  dict(np.ndenumerate(np.asarray(embedDict[w])))

        temp.append(gensim.matutils.cossim(x,y))
</code></pre>

<p>I get the following exception:</p>

<pre class=""lang-none prettyprint-override""><code>File ""./match.py"", line 129, in getEmbedding
    test.append(gensim.matutils.cossim(x,y))
  File ""./Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/matutils.py"", line 746, in cossim
    vec1, vec2 = dict(vec1), dict(vec2)
TypeError: cannot convert dictionary update sequence element #0 to a sequence
</code></pre>

<p>Can you please help me and explain to me what this exception means? </p>
","python, python-2.7, gensim","<p>The arguments of <a href=""https://radimrehurek.com/gensim/matutils.html"" rel=""nofollow noreferrer"">gensim.matutils.cossim</a> are expected to be of type <code>list of (int, float)</code> but you are using dictionaries.</p>

<p>The exception happens in the <code>cossim</code> function with the following <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/matutils.py#L787"" rel=""nofollow noreferrer"">cossim implementation</a>:</p>

<pre><code>vec1, vec2 = dict(vec1), dict(vec2)
</code></pre>

<p>With the correct type, <code>dict(vec)</code> works:</p>

<pre><code>dict([(1, 2.), (3, 4.), (5, 6.)])
</code></pre>

<p>But if you do not provide the correct type, it throws the exception, for instance with:</p>

<pre><code>dict([1, 2, 3])
</code></pre>
",2,0,605,2018-11-23 15:06:20,https://stackoverflow.com/questions/53449019/how-to-use-cosssim-in-gensim
"Python Calculating similarity between two documents using word2vec, doc2vec","<p>I am trying to calculate similarity between two documents which are comprised of more than thousands sentences.</p>

<p>Baseline would be calculating cosine similarity using BOW.</p>

<p>However, I want to capture more of semantic difference between documents.</p>

<p>Hence, I built word embedding and calculated documents similarity by generating document vectors by simply averaging all the word vectors in each of documents and measure cosine similarity between these documents vectors. </p>

<p>However, since the size of each input document is rather big, the results I get from using the method above are very similar to simple BOW cosine similarity.</p>

<p>I have two questions, </p>

<p>Q1. I found gensim module offers soft cosine similarity. But I am having hard time understanding the difference from the methods I used above, and I think it may not be the mechanism to calculate similarity between million pairs of documents.</p>

<p>Q2. I found Doc2Vec by gensim would be more appropriate for my purpose. But I recognized that training Doc2Vec requires more RAM than I have (32GB) (the size of my entire documents is about 100GB). Would there be any way that I train the model with small part(like 20GB of them) of entire corpus, and use this model to calculate pairwise similarities of entire corpus?
If yes, then what would be the desirable train set size, and is there any tutorial that I can follow?</p>
","python, similarity, gensim, word2vec, doc2vec","<p>Ad Q1: If the similarity matrix contains the cosine similarities of the word embeddings (which it more or less does, see Equation 4 in <a href=""http://www.aclweb.org/anthology/S17-2051"" rel=""nofollow noreferrer"">SimBow at SemEval-2017 Task 3</a>) and if the word embeddings are L2-normalized, then the SCM (Soft Cosine Measure) is equivalent to averaging the word embeddings (i.e. your baseline). For a proof, see Lemma 3.3 in <a href=""https://arxiv.org/pdf/1808.09407.pdf"" rel=""nofollow noreferrer"">the Implementation Notes for the SCM</a>. My Gensim implementation of the SCM (<a href=""https://github.com/RaRe-Technologies/gensim/pull/1827"" rel=""nofollow noreferrer"">1</a>, <a href=""https://github.com/RaRe-Technologies/gensim/pull/2016"" rel=""nofollow noreferrer"">2</a>) additionally sparsifies the similarity matrix to keep the memory footprint small and to regularize the embeddings, so you will get slightly different results compared to vanilla SCM. If embedding averaging gives you similar results to simple BOW cosine similarity, I would question the quality of the embeddings.</p>

<p>Ad Q2: Training a Doc2Vec model on the entire dataset for one epoch is equivalent to training a Doc2Vec model on smaller segments of the entire dataset, one epoch for each segment. Just be aware that Doc2Vec uses document ids as a part of the training process, so you must ensure that the ids are still unique after the segmentation (i.e. the first document of the first segment must have a different id than the first document of the second segment).</p>
",1,2,1530,2018-11-25 12:25:40,https://stackoverflow.com/questions/53467414/python-calculating-similarity-between-two-documents-using-word2vec-doc2vec
Measure similarity between two documents using Doc2Vec,"<p>I have already trained gensim doc2Vec model, which is finding most similar documents to an unknown one.</p>

<p>Now I need to find the similarity value between two unknown documents (which were not in the training data, so they can not be referenced by doc id)</p>

<pre><code>d2v_model = doc2vec.Doc2Vec.load(model_file)

string1 = 'this is some random paragraph'
string2 = 'this is another random paragraph'

vec1 = d2v_model.infer_vector(string1.split())
vec2 = d2v_model.infer_vector(string2.split())
</code></pre>

<p>in the code above vec1 and vec2 are successfully initialized to some values and of size - 'vector_size'</p>

<p>now looking through the gensim api and examples I could not find method that works for me, all of them are expecting TaggedDocument</p>

<p>Can I compare the feature vectors value by value and if they are closer => the texts are more similar?</p>
","python, machine-learning, nlp, gensim, doc2vec","<p>Hello just In case someone is interested, to do this you just need the cosine distance between the two vectors.</p>
<p>I found that most people are using 'spatial' for this pourpose</p>
<p>Here is a small code sniped that should work pretty well if you already have trained doc2vec</p>
<pre><code>from gensim.models import doc2vec
from scipy import spatial

d2v_model = doc2vec.Doc2Vec.load(model_file)

fisrt_text = '..'
second_text = '..'

vec1 = d2v_model.infer_vector(fisrt_text.split())
vec2 = d2v_model.infer_vector(second_text.split())

cos_distance = spatial.distance.cosine(vec1, vec2)
# cos_distance indicates how much the two texts differ from each other:
# higher values mean more distant (i.e. different) texts
</code></pre>
",12,6,10710,2018-11-27 15:34:45,https://stackoverflow.com/questions/53503049/measure-similarity-between-two-documents-using-doc2vec
Why use TaggedBrownCorpus when training gensim doc2vec,"<p>I am currently using custom corpus that wields Tagged Documents</p>

<pre><code>class ClassifyCorpus(object):
    def __iter__(self):
        with open(train_data) as fp:
            for line in fp:
                splt = line.split(':')
                id = splt[0]
                text = splt[1].replace('\n', '')
                yield TaggedDocument(text.split(), [id])
</code></pre>

<p>Looking at the source code of Brown Corpus, is see that it just reads from directory and handles the tagging of the documents for me.</p>

<p>I tested it and didn't see improvements in the training speed.</p>
","python, gensim, corpus, doc2vec","<p>You shouldn't use <code>TaggedBrownCorpus</code>. It's just a demo class for reading a particular tiny demo dataset that's included with gensim for unit-tests and intro tutorials. </p>

<p>It does things in a reasonable way for that data-format-on-disk, but any other efficient way of getting your data into a repeat-iterable sequence of <code>TaggedDocument</code>-like objects is just as good. </p>

<p>So feel free to use it as a model if it helps, but don't view it as a requirement or ""best practice"". </p>
",1,0,53,2018-11-29 09:45:04,https://stackoverflow.com/questions/53536021/why-use-taggedbrowncorpus-when-training-gensim-doc2vec
What is the meaning of &quot;size&quot; of word2vec vectors [gensim library]?,"<p>Assume that we have 1000 words (A1, A2,..., A1000) in a dictionary. As fa as I understand, in words embedding or word2vec method, it aims to represent each word in the dictionary by a vector where each element represents the similarity of that word with the remaining words in the dictionary. Is it correct to say there should be 999 dimensions in each vector, or the size of each word2vec vector should be 999?</p>

<p>But with Gensim Python, we can modify the value of ""size"" parameter for Word2vec, let's say size = 100 in this case. So what does ""size=100"" mean? If we extract the output vector of A1, denoted (x1,x2,...,x100), what do x1,x2,...,x100 represent in this case?</p>
","python, gensim, word2vec, word-embedding","<p>It is <em>not</em> the case that ""[word2vec] aims to represent each word in the dictionary by a vector where each element represents the similarity of that word with the remaining words in the dictionary"". </p>

<p>Rather, given a certain target dimensionality, like say 100, the Word2Vec algorithm gradually trains word-vectors of 100-dimensions to be better and better at its training task, which is predicting nearby words. </p>

<p>This iterative process tends to force words that are related to be ""near"" each other, in rough proportion to their similarity - and even further the various ""directions"" in this 100-dimensional space often tend to match with human-perceivable semantic categories. So, the famous ""wv(king) - wv(man) + wv(woman) ~= wv(queen)"" example often works because ""maleness/femaleness"" and ""royalty"" are vaguely consistent regions/directions in the space. </p>

<p>The individual dimensions, alone, don't mean anything. The training process includes randomness, and over time just does ""whatever works"". The meaningful directions are not perfectly aligned with dimension axes, but angled through all the dimensions. (That is, you're not going to find that a <code>v[77]</code> is a gender-like dimension. Rather, if you took dozens of alternate male-like and female-like word pairs, and averaged all their differences, you might find some 100-dimensional vector-dimension that is suggestive of the gender direction.)</p>

<p>You can pick any 'size' you want, but 100-400 are common values when you have enough training data. </p>
",5,2,3213,2018-12-03 05:29:29,https://stackoverflow.com/questions/53587960/what-is-the-meaning-of-size-of-word2vec-vectors-gensim-library
Online updating Word2Vec,"<p>I've got a problem with online updating my Word2Vec model.</p>

<p>I have a document and build model by it. But this document can update with new words, and I need to update vocabulary and model in general.</p>

<p>I know that in gensim 0.13.4.1 we can do this</p>

<p>My code:</p>

<pre><code>model = gensim.models.Word2Vec(size=100, window=10, min_count=5, workers=11, alpha=0.025, min_alpha=0.025, iter=20)
model.build_vocab(sentences, update=False)

model.train(sentences, epochs=model.iter, total_examples=model.corpus_count)

model.save('model.bin')
</code></pre>

<p>And after this I have new words. For e.x.:</p>

<pre><code>sen2 = [['absd', 'jadoih', 'sdohf'], ['asdihf', 'oisdh', 'oiswhefo'], ['a', 'v', 'b', 'c'], ['q', 'q', 'q']]

model.build_vocab(sen2, update=True)
model.train(sen2, epochs=model.iter, total_examples=model.corpus_count)
</code></pre>

<p>What's wrong and how can I solve my problem?</p>
","python, nlp, gensim, word2vec","<p>Your model is set to ignore words with fewer than 5 occurrences: <code>min_count=5</code>. It will, in fact, require at least 5 occurrences in a single <code>build_vocab()</code> call. (It won't remember there were 3 before, then see 2 new occurrences, then train on all 5. It needs all 5 or more in one batch.) </p>

<p>If you're only calling your update with the tiny dataset shown, no new words will make the cut. </p>

<p>More generally, if at all possible, you should retrain the whole model with all old and new data. That will ensure equal influence is given to old and new words, and any words are treated properly according to their combined frequency. Making small incremental updates to a <code>Word2Vec</code> model risks pulling newer words, or old words that continue to reappear, out of meaningful arrangement with older words that were only trained in the original (or earlier) batches. (Only words that go through the same interleaved training cycles are fully positionally adjusted with respect to each other.)</p>
",2,0,901,2018-12-04 10:03:24,https://stackoverflow.com/questions/53610331/online-updating-word2vec
Doc2Vec online training,"<p>I train my doc2vec model:</p>

<pre><code>data = [""Sentence 1"",
        ""Sentence 2"",
        ""Sentence 3"",
        ""Sentence 4""]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags[str(i)]) 
                              for i, _d in enumerate(data)]
</code></pre>

<p>training part:</p>

<pre><code>model = Doc2Vec(size=100, window=10, min_count=1, workers=11, alpha=0.025, 
                min_alpha=0.025, iter=20)

model.build_vocab(tagged_data, update=False)

model.train(tagged_data,epochs=model.iter,total_examples=model.corpus_count)
</code></pre>

<p>Save model:</p>

<pre><code>model.save(""d2v.model"")
</code></pre>

<p>And it's work. Than I want to add some sentence to my vocabulary and model. E.x.:</p>

<pre><code>new_data = [""Sentence 5"",
            ""Sentence 6"",
            ""Sentence 7""]
new_tagged_data= 
[TaggedDocument(words=word_tokenize(_d.lower()),tags[str(i+len(data))]) 
                for i,_d in enumerate(new_data)]
</code></pre>

<p>And than update model:</p>

<pre><code>model.build_vocab(new_tagged_data, update=True)

model.train(new_tagged_data, 
            epochs=model.iter,total_examples=model.corpus_count)
</code></pre>

<p>But it doesn't work. Jupiter urgently shut down and no answer. I use the same way with word2vec model and it works!</p>

<p>What can be a problem with this?</p>
","python, python-3.x, nlp, gensim, doc2vec","<p>The <code>build_vocab(..., update-True)</code> functionality was only developed, experimentally, in gensim for <code>Word2Vec</code> and hasn't been tested/debugged for <code>Doc2Vec</code>. There's a long-open crashing bug when trying to use it with <code>Doc2Vec</code>:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/issues/1019"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/1019</a></p>

<p>So, it's not yet supported. </p>

<p>Separately, there are lots of murky &amp; difficult issues related to the <em>balance</em> and <em>vector-compatibility</em> of models that are incrementally trained in this way, and if at all possible, you should re-train the model with the full old &amp; new data, mixed together, rather than attempting small updates. </p>
",2,0,787,2018-12-04 15:15:05,https://stackoverflow.com/questions/53616003/doc2vec-online-training
Different results of Gensim Word2Vec Model in two editors for the same source code in same environment and platform?,"<p>I am trying to apply the word2vec model implemented in the library gensim 3.6 in python 3.7, Windows 10 machine. I have a list of sentences (each sentences is a list of words) as an input to the model after performing preprocessing.</p>

<p>I have computed the results (obtaining 10 most similar words of a given input word using <code>model.wv.most_similar</code>) in <code>Anaconda's Spyder</code> followed by <code>Sublime Text</code> editor. </p>

<p>But, I am getting different results for the same source code executed in two editors.</p>

<p>Which result should <strong>I need to choose and Why</strong>?</p>

<p>I am specifying the screenshot of the results obtained by running the same code in both spyder and sublime text. The input word for which I need to obtain 10 most similar word is <code>#universe#</code></p>

<p>I am really confused how to choose the results, on what basis? Also, I have started learning Word2Vec recently.</p>

<p>Any suggestion is appreciated.</p>

<p>Results Obtained in Spyder:</p>

<p><a href=""https://i.sstatic.net/v87AW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/v87AW.png"" alt=""enter image description here""></a></p>

<p>Results Obtained using Sublime Text:
<a href=""https://i.sstatic.net/JPN0X.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JPN0X.png"" alt=""enter image description here""></a></p>
","python, python-3.x, gensim, word2vec","<p>The Word2Vec algorithm makes use of randomization internally. Further, when (as is usual for efficiency) training is spread over multiple threads, some additional order-of-presentation randomization is introduced. These mean that two runs, even in the exact same environment, can have different results. </p>

<p>If the training is effective – sufficient data, appropriate parameters, enough training passes – all such models should be of similar quality when doing things like word-similarity, even though the actual words will be in different places. There'll be some jitter in the relative rankings of words, but the results should be broadly similar. </p>

<p>That your results are vaguely related to <code>'universe'</code> but not impressively so, and that they vary so much from one run to another, suggest there may be problems with your data, parameters, or quantity of training. (We'd expect the results to vary a little, but not that much.)</p>

<p>How much data do you have? (Word2Vec benefits from lots of varied word-usage examples.)</p>

<p>Are you retaining rare words, by making <code>min_count</code> lower than its default of 5? (Such words tend not to get good vectors, and also wind up interfering with the improvement of nearby words' vectors.)</p>

<p>Are you trying to make very-large vectors? (Smaller datasets and smaller vocabularies can only support smaller vectors. Too-large vectors allow 'overfitting', where idiosyncracies of the data are memorized rather than generalized patterns learned. Or, they allow the model to continue improving in many different non-competitive directions, so model end-task/similarity results can be very different from run-to-run, even though each model is doing about-as-well as the other on its <em>internal</em> word-prediction tasks.)</p>

<p>Have you stuck with the default <code>epochs=5</code> even with a small dataset? (A large, varied dataset requires fewer training passes - because all words appear many times, all throughout the dataset, anyway. If you're trying to squeeze results from thinner data, more <code>epochs</code> may help a little – but not as much as more varied data would.)</p>
",1,0,1457,2018-12-04 17:59:28,https://stackoverflow.com/questions/53618906/different-results-of-gensim-word2vec-model-in-two-editors-for-the-same-source-co
Gensim (word2vec) retrieve n most frequent words,"<p>How is it possible to retrieve the n most frequent words from a Gensim <code>word2vec</code> model? As I understand, the frequency and count are not the same, and I therefore can't use the <code>object.count()</code> method.  </p>

<p>I need to produce a list of the n most frequent words from my <code>word2vec</code> model. </p>

<p>Edit: </p>

<p>I've tried the following: </p>

<pre class=""lang-py prettyprint-override""><code>w2c = dict()
for item in model.wv.vocab:
   w2c[item]=model.wv.vocab[item].count
w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))
w2cSortedList = list(w2cSorted.keys())
</code></pre>

<p>My initial guess was to use code above, but this implements the count method. I'm not sure if this represents the most frequent words.</p>
",gensim,"<p>The <code>.count</code> property of each vocab-entries is the count of that word as seen during the initial vocabulary-survey. So sorting by that, and taking the highest-<code>count</code> words, will give you the most-frequent words.</p>
<p>But also, for efficiency, it's typical practice for the ordered-list of known-words to be ordered from most- to least-frequent. You can view this at the list <code>model.wv.index_to_key</code>, so can retrieve the 100 most frequent words by <code>model.wv.index_to_key[:100]</code>. (In Gensim before version 4.0, this same list was called either <code>index2entity</code> or <code>index2word</code>.)</p>
",15,8,8011,2018-12-04 21:31:56,https://stackoverflow.com/questions/53621737/gensim-word2vec-retrieve-n-most-frequent-words
Find similarity with doc2vec like word2vec,"<p>Is there a way to find similar docs like we do in word2vec</p>

<p>Like:</p>

<pre><code>  model2.most_similar(positive=['good','nice','best'],
    negative=['bad','poor'],
    topn=10)
</code></pre>

<p>I know we can use infer_vector,feed them to have similar ones, but I want to feed many positive and negative examples as we do in word2vec.</p>

<p>is there any way we can do that! thanks !</p>
","python, nlp, gensim, word2vec, doc2vec","<p>The doc-vectors part of a <code>Doc2Vec</code> model works just like word-vectors, with respect to a <code>most_similar()</code> call. You can supply multiple doc-tags or full vectors inside both the <code>positive</code> and <code>negative</code> parameters. </p>

<p>So you could call...</p>

<pre><code>sims = d2v_model.docvecs.most_similar(positive=['doc001', 'doc009'], negative=['doc102'])
</code></pre>

<p>...and it should work. The elements of the <code>positive</code> or <code>negative</code> lists could be doc-tags that were present during training, or raw vectors (like those returned by <code>infer_vector()</code>, or your own averages of multiple such vectors). </p>
",1,0,295,2018-12-05 08:51:11,https://stackoverflow.com/questions/53628382/find-similarity-with-doc2vec-like-word2vec
Extracting texts from pdf files for building a model with Gensim,"<p>I would like to train a model with Gensim using news texts from electronic newspapers (in pdf format). What is the best way to extract texts from pdf files and to process the texts ready for training? Any sample codes?</p>
","python-3.x, nlp, gensim","<p>You can extract text on a per-page basis with <a href=""https://pypi.org/project/PyPDF2/"" rel=""nofollow noreferrer"">PyPDF2</a>. The simplest code would look something like this:</p>

<pre><code>import PyPDF2

reader = PyPDF2.PdfFileReader(""your_file.pdf"")

for page in reader.pages:
    text = page.extractText()
    # do something with text
</code></pre>
",1,-2,555,2018-12-11 16:36:43,https://stackoverflow.com/questions/53728556/extracting-texts-from-pdf-files-for-building-a-model-with-gensim
How to run word2vec on Windows using gensim,"<p>A couple of years ago, a previous developer for my team wrote the following Python code calling word2vec, passing in a training file and the location of an output file. He worked on Linux. I have been asked to get this running on a Windows machine. Bearing in mind <em>I know next to no Python</em>, I have installed Gensim which I'm guessing implements word2vec now, but do not know how to rewrite the code to use the library rather than the executable which it doesnt seem possible to compile on a Windows box. Could someone help me update this code please?</p>

<pre><code>#!/usr/bin/env python3

import os
import csv
import subprocess
import shutil

from gensim.models import word2vec

def train_word2vec(trainFile, output):
    # run word2vec:
    subprocess.run([""word2vec"", ""-train"", trainFile, ""-output"", output,
                    ""-cbow"", ""0"", ""-window"", ""10"", ""-size"", ""100""],
                   shell=False)
    # Remove some invalid unicode:
    with open(output, 'rb') as input_,\
         open('%s.new' % output, 'w') as new_output:
        for line in input_:
            try:
                print(line.decode('utf-8'), file=new_output, end='')
            except UnicodeDecodeError:
                print(line)
                pass
    shutil.move('%s.new' % output, output)

def main():
    train_word2vec(""c:/temp/wc/test1_BigF.txt"", ""c:/temp/wc/test1_w2v_model.txt"")

if __name__ == '__main__':
    main()
</code></pre>
","python, python-3.x, gensim, word2vec","<p>I think the core of what you're after looks something like this:</p>

<pre><code>import sys

from gensim.models.word2vec import Word2Vec

def train_word2vec(trainFile, output):
    # compile word arrays for each sentence of input vocab
    sentences = list(line.split() for line in open(trainFile))

    # effective executable invocation of original code (included for reference)
    # word2vec -train {trainFile} -output {output} -cbow 0 -window 10 -size 100

    # invocation via word2vec module with (mostly) equivalent params
    model = Word2Vec(sentences, size=100, window=10, min_count=1, workers=4)

    # save generated model        
    model.save(output)

if __name__ == '__main__':
    train_word2vec(sys.argv[1], sys.argv[2])
</code></pre>

<p>Save as <code>train.py</code> and invoke as follows:</p>

<pre><code>python train.py input.txt output.txt
</code></pre>

<p>A few things to note:</p>

<ul>
<li>There's different capitalisation used for names of the module (<code>word2vec</code>) and the imported class (<code>Word2Vec</code>). It <em>will</em> break if you mix them up.</li>
<li>I've not found/included an equivalent for the command line <code>-cbow 0</code> argument. I'd guess this indicates a preference for the Skip-gram algorithm over CBOW, but would need someone with more <code>gensim</code> experience than me to advise on its ramifications - or indeed those of leaving it out.</li>
<li>Nor have I included (or attempted to reproduce) the Unicode removal logic of the original. The generated model output is largely binary data, so taken 'as is' it (a) falls over pretty much straight away and (b) leaves me rather in the dark as to what it's even trying to achieve.</li>
</ul>

<p>Hope this helps a little anyway.</p>
",1,0,822,2018-12-12 11:49:30,https://stackoverflow.com/questions/53742400/how-to-run-word2vec-on-windows-using-gensim
Gensim doc2vec file stream training worse performance,"<p>Recently I switched to gensim 3.6 and the main reason was the optimized training process, which streams the training data directly from file, thus avoiding the GIL performance penalties.</p>

<p>This is how I used to trin my doc2vec:</p>

<pre><code>training_iterations = 20
d2v = Doc2Vec(vector_size=200, workers=cpu_count(), alpha=0.025, min_alpha=0.00025, dm=0)
d2v.build_vocab(corpus)

for epoch in range(training_iterations):
    d2v.train(corpus, total_examples=d2v.corpus_count, epochs=d2v.iter)
    d2v.alpha -= 0.0002
    d2v.min_alpha = d2v.alpha
</code></pre>

<p>And it is classifying documents quite well, only draw back is that when it is trained CPUs are utilized at 70%</p>

<p>So the new way:</p>

<pre><code>corpus_fname = ""spped.data""
save_as_line_sentence(corpus, corpus_fname)

# Choose num of cores that you want to use (let's use all, models scale linearly now!)
num_cores = cpu_count()

# Train models using all cores
d2v_model = Doc2Vec(corpus_file=corpus_fname, workers=num_cores, dm=0, vector_size=200, epochs=50)
</code></pre>

<p><strong>Now all CPUs are utilized at 100%</strong></p>

<p>but the model is performing very poorly.
According to the documentation, I should not use the train method also, I should use only epoch count and not iterations, also the min_aplpha and aplha values should not be touched.</p>

<p>The configuration of both Doc2Vec looks the same to me so is there an issue with my new set up or configuration, or there is something wrong with the new version of gensim?</p>

<p>P.S I am using the same corpus in both cases, also I tried epoch count = 100, also with smaller numbers like 5-20, but I had no luck</p>

<p><strong>EDIT</strong>: First model was doing 20 iterations 5 epoch each, second was doing 50 epoch, so having the second model make 100 epochs made it perform even better, since I was no longer managing the alpha by myself.</p>

<p>About the second issue that popped up: when providing file with line documents, the doc ids were not always corresponding to the lines, I didn't manage to figure out what could be causing this, it seems to work fine for small corpus, If I find out what I am doing wrong I will update this answer.</p>

<p>The final configuration for corpus of size 4GB looks like this</p>

<pre><code>    d2v = Doc2Vec(vector_size=200, workers=cpu_count(), alpha=0.025, min_alpha=0.00025, dm=0)
    d2v.build_vocab(corpus)
    d2v.train(corpus, total_examples=d2v.corpus_count, epochs=100)
</code></pre>
","nlp, gensim, doc2vec","<p>Most users should not be calling <code>train()</code> more than once in their own loop, where they try to manage the <code>alpha</code> &amp; iterations themselves. It is too easy to do it wrong. </p>

<p>Specifically, your code where you call <code>train()</code> in a loop is doing it wrong. Whatever online source or tutorial you modeled this code on, you should stop consulting, as it's misleading or outdated. (The notebooks bundled with gensim are better examples on which to base any code.)</p>

<p>Even more specifically: your looping code is actually doing 100 passes over the data, 20 of your outer loops, then the default <code>d2v.iter</code> 5 times each call to <code>train()</code>. And your first <code>train()</code> call is smoothly decaying the effective <code>alpha</code> from 0.025 to 0.00025, a 100x reduction. But then your next <code>train()</code> call uses a fixed <code>alpha</code> of 0.0248 for 5 passes. Then 0.0246, etc, until your last loop does 5 passes at <code>alpha=0.0212</code> – not even 80% of the starting value. That is, the lowest alpha will have been reached early in your training.</p>

<p>Call the two options exactly the same except for the way the <code>corpus_file</code> is specified, instead of an iterable corpus. </p>

<p>You should get similar results from both corpus forms. (If you had a reproducible test case where the same corpus gets very different-quality results, and there wasn't some other error, that could be worth reporting to <code>gensim</code> as a bug.)</p>

<p>If the results for both aren't as good as when you were managing <code>train()</code> and <code>alpha</code> wrongly, it would likely be because you aren't doing a comparable amount of total training.</p>
",2,2,554,2018-12-13 17:17:43,https://stackoverflow.com/questions/53767024/gensim-doc2vec-file-stream-training-worse-performance
Similarity measure using vectors in gensim,"<p>I have a pair of word and semantic types of those words. I am trying to compute the relatedness measure between these two words using semantic types, for example: word1=king, type1=man, word2=queen, type2=woman
we can use gensim word_vectors.most_similar to get 'queen' from 'king-man+woman'. However, I am looking for similarity measure between vector represented by 'king-man+woman' and 'queen'.</p>

<p>I am looking for a solution to above (or)
way to calculate vector that is representative of 'king-man+woman' (and)
calculating similarity between two vectors using vector values in gensim (or)
 way to calculate simple mean of the projection weight vectors(i.e king-man+woman)</p>
","gensim, word2vec","<p>You should look at the source code for the gensim <code>most_similar()</code> method, which is used to propose answers to such analogy questions. Specifically, when you try...</p>

<pre><code>sims = wv_model.most_similar(positive=['king', 'woman'], negative=['man'])
</code></pre>

<p>...the top result will (in a sufficiently-trained model) often be 'queen' or similar. So, you can look at the source code to see exactly how it calculates the target combination of <code>wv('king') - wv('man') + wv('woman')</code>, before searching all known vectors for those closest vectors to that target. See...</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L486"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L486</a></p>

<p>...and note that the local variable <code>mean</code> is the combination of the <code>positive</code> and <code>negative</code> values provided.</p>

<p>You might also find other methods there useful, either directly or as models for your own code, such as <code>distances()</code>...</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L934"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L934</a></p>

<p>...or <code>n_similarity()</code>...</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L1005"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/5f6b28c538d7509138eb090c41917cb59e4709af/gensim/models/keyedvectors.py#L1005</a></p>
",1,0,1674,2018-12-15 11:34:03,https://stackoverflow.com/questions/53791972/similarity-measure-using-vectors-in-gensim
Value of alpha in gensim word-embedding (Word2Vec and FastText) models?,"<p>I just want to know the effect of the value of alpha in gensim <code>word2vec</code> and <code>fasttext</code> word-embedding models? I know that alpha is the <code>initial learning rate</code> and its default value is <code>0.075</code> form Radim blog.</p>

<p>What if I change this to a bit higher value i.e. 0.5 or 0.75? What will be its effect? Does it is allowed to change the same? However, I have changed this to 0.5 and experiment on a large-sized data with D = 200, window = 15, min_count = 5, iter = 10, workers = 4 and results are pretty much meaningful for the word2vec model. However, using the fasttext model, the results are bit scattered, means less related and unpredictable high-low similarity scores.</p>

<p>Why this imprecise result for same data with two popular models with different precision? Does the value of <code>alpha</code> plays such a crucial role during building of the model?</p>

<p>Any suggestion is appreciated.</p>
","python-3.x, gensim, word2vec, word-embedding, fasttext","<p>The default starting <code>alpha</code> is <code>0.025</code> in gensim's Word2Vec implementation. </p>

<p>In the stochastic gradient descent algorithm for adjusting the model, the effective <code>alpha</code> affects how strong of a correction to the model is made after each training example is evaluated, and will decay linearly from its starting value (<code>alpha</code>) to a tiny final value (<code>min_alpha</code>) over the course of all training. </p>

<p>Most users won't need to adjust these parameters, or might only adjust them a little, after they have a reliable repeatable way of assessing whether a change improves their model on their end tasks. (I've seen starting values of <code>0.05</code> or less commonly <code>0.1</code>, but never as high as your reported <code>0.5</code>.)</p>
",5,4,4505,2018-12-17 12:36:39,https://stackoverflow.com/questions/53815402/value-of-alpha-in-gensim-word-embedding-word2vec-and-fasttext-models
How to improve the reproducibility of Doc2vec cosine similarity,"<p>I am using Gensim's Doc2vec to train a model, and I use the infer_vector to infer the vector of a new document to compare the similarity document of the model. However, reusing the same document can have very different results. This way there is no way to accurately evaluate similar documents.<br>
The search network mentions that infer_vector has random characteristics, so each time a new text vector is produced, it will be different.<br>
Is there any way to solve this problem?</p>

<pre><code>model_dm =pickle.load(model_pickle)

inferred_vector_dm = model_dm.infer_vector(i)  

simsinput =model_dm.docvecs.most_similar([inferred_vector_dm],topn=10)
</code></pre>
","python-3.x, nlp, gensim, similarity, doc2vec","<p>If you supply an optional <code>epochs</code> argument to <code>infer_vector()</code> that's larger than the default, the resulting vectors, from run to run on a single text, should become more similar. (This will likely be especially helpful on small texts.) </p>

<p>That is, there should only be a small ""jitter"" between runs, and that shouldn't make a big difference in your later comparisons. (Your downstream comparisons should be tolerant of small changes.) With an algorithm like this, that uses randomization, there's no absolutely ""right"" result, just useful results. </p>

<p>If the variance between runs remains large – for example changing the <code>most_similar()</code> results significantly from run-to-run, then there might be other problems with your model or setup:</p>

<ul>
<li><p>Doc2Vec doesn't work well on toy-sized training sets – published work uses document sets of 10s-of-thousands to millions of documents, where documents are dozens to thousands of words each. If you're using just a handful of short sentences, you won't get good results.</p></li>
<li><p><code>infer_vector()</code> needs to get a list-of-string-tokens, <em>not</em> a string. And, those tokens should have been preprocessed in the same way as the training data. Any unknown words fed to <code>infer_vector()</code> will be ignored, making the input shorter (or zero-length), making results more (or totally) random.</p></li>
</ul>

<p>Separately, gensim's <code>Doc2Vec</code> has native <code>.save()</code> and <code>.load()</code> methods which should be used rather than raw <code>pickle</code> – especially on larger models, they'll do things more efficiently or without errors. (Though note: they may create multiple save files, which should be kept together so that loading the main file can find the subsidiary files.)</p>
",1,1,772,2018-12-19 14:03:18,https://stackoverflow.com/questions/53852871/how-to-improve-the-reproducibility-of-doc2vec-cosine-similarity
How to use a list in word2vec.similarity,"<p>I have a word2vec model using pre-trained GoogleNews-vectors-negative300.bin. The model works fine and I can get the similarities between the two words. For example:</p>

<pre><code>word2vec.similarity('culture','friendship')

0.2732939
</code></pre>

<p>Now, I want to use list elements instead of the words. For example, suppose that I have a list which its name is ""tag"". and the first two elements in the first row are culture and friendship. So, tag[0,0]= culture, and tag[0,1]=friendship.
I use the following code which gives me an error:</p>

<pre><code>word2vec.similarity(tag[0,0],tag[0,1])
</code></pre>

<p>the ""tag"" list is a <code>numpy.ndarray</code></p>

<p>the error is:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 992, in similarity
    return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2]))
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 337, in __getitem__
    return self.get_vector(entities)
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 455, in get_vector
    return self.word_vec(word)
  File ""C:\Users\s\AppData\Local\Programs\Python6436\Python36\lib\site-packages\gensim\models\keyedvectors.py"", line 452, in word_vec
    raise KeyError(""word '%s' not in vocabulary"" % word)
KeyError: ""word ' friendship' not in vocabulary""
</code></pre>
","python, gensim, word2vec","<p>I think there are some leading spaces in your word ' friendship'.</p>

<p>Could you try this:</p>

<pre><code>word2vec.similarity(tag[0,0].strip(),tag[0,1].strip())
</code></pre>
",0,0,401,2018-12-20 04:59:23,https://stackoverflow.com/questions/53862627/how-to-use-a-list-in-word2vec-similarity
How to fix &quot;Relative import error&quot; in python (gensim.summarization),"<p>I'm running this code</p>

<pre><code>from gensim.summarization import summarize
text = ""In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleones "" + \
       ""daughter Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando),""  + \
       ""the head of the Corleone Mafia family, is known to friends and associates as Godfather. ""  + \
       ""He and Tom Hagen (Robert Duvall), the Corleone family lawyer, are hearing requests for favors ""  + \
       ""because, according to Italian tradition, no Sicilian can refuse a request on his daughter's wedding "" + \
       "" day. One of the men who asks the Don for a favor is Amerigo Bonasera, a successful mortician ""  + \
       ""and acquaintance of the Don, whose daughter was brutally beaten by two young men because she""  + \
       ""refused their advances; the men received minimal punishment from the presiding judge. "" + \
       ""The Don is disappointed in Bonasera, who'd avoided most contact with the Don due to Corleone's"" + \
       ""nefarious business dealings. The Don's wife is godmother to Bonasera's shamed daughter, "" + \
       ""a relationship the Don uses to extract new loyalty from the undertaker. The Don agrees "" + \
       ""to have his men punish the young men responsible (in a non-lethal manner) in return for "" + \
        ""future service if necessary.""

print summarize(text)
</code></pre>

<p>It runs perfectly fine for the first time. But after that it shows me following error until I restart the kernel in spyder:</p>

<pre><code>File ""/home/taha/.local/lib/python2.7/site -packages/scipy/sparse/compressed.py"", line 50, in __init__ from .coo import coo_matrix
</code></pre>

<p><code>SystemError: Parent module 'scipy.sparse' not loaded, cannot perform relative import</code></p>

<p>I am using ubuntu 18.04</p>
","python, gensim, summarization","<p>The problem was with spyder IDE. You can run this code without any error using terminal (or cmd). Moreover, you need to restart kernal everytime you run this code in spyder IDE</p>
",0,0,178,2018-12-31 15:52:48,https://stackoverflow.com/questions/53989210/how-to-fix-relative-import-error-in-python-gensim-summarization
W2VTransformer: Only works with one word as input?,"<p>Following reproducible script is used to compute the accuracy of a Word2Vec classifier with the <code>W2VTransformer</code> wrapper in gensim:</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from gensim.sklearn_api import W2VTransformer
from gensim.utils import simple_preprocess

# Load synthetic data
data = pd.read_csv('https://pastebin.com/raw/EPCmabvN')
data = data.head(10)

# Set random seed
np.random.seed(0)

# Tokenize text
X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
# Get labels
y_train = data.label

train_input = [x[0] for x in X_train]

# Train W2V Model
model = W2VTransformer(size=10, min_count=1)
model.fit(X_train)

clf = LogisticRegression(penalty='l2', C=0.1)
clf.fit(model.transform(train_input), y_train)

text_w2v = Pipeline(
    [('features', model),
     ('classifier', clf)])

score = text_w2v.score(train_input, y_train)
score
</code></pre>

<blockquote>
  <p>0.80000000000000004</p>
</blockquote>

<p>The problem with this script is that it <strong>only</strong> works when <code>train_input = [x[0] for x in X_train]</code>, which essentially is always the first word only. 
Once change to <code>train_input = X_train</code> (or <code>train_input</code> simply substituted by <code>X_train</code>), the script returns:</p>

<blockquote>
  <p>ValueError: cannot reshape array of size 10 into shape (10,10)</p>
</blockquote>

<p>How can I solve this issue, i.e. how can the classifier work with more than one word of input?</p>

<p><strong>Edit:</strong></p>

<p>Apparently, the W2V wrapper can't work with the variable-length train input, as compared to D2V. Here is a working D2V version:</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import Pipeline
from gensim.utils import simple_preprocess, lemmatize
from gensim.sklearn_api import D2VTransformer

data = pd.read_csv('https://pastebin.com/raw/bSGWiBfs')

np.random.seed(0)

X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
y_train = data.label

model = D2VTransformer(dm=1, size=50, min_count=2, iter=10, seed=0)
model.fit(X_train)

clf = LogisticRegression(penalty='l2', C=0.1, random_state=0)
clf.fit(model.transform(X_train), y_train)

pipeline = Pipeline([
        ('vec', model),
        ('clf', clf)
    ])

y_pred = pipeline.predict(X_train)
score = accuracy_score(y_train,y_pred)
print(score)
</code></pre>
","scikit-learn, gensim, word2vec","<p>This is technically not an answer, but cannot be written in comments so here it is. There are multiple issues here:</p>

<ul>
<li><p><code>LogisticRegression</code> class (and most other scikit-learn models) work with 2-d data <code>(n_samples, n_features)</code>. </p>

<p>That means that it needs a collection of 1-d arrays (one for each row (sample), in which the elements of array contains the feature values).  </p>

<p>In your data, a single word will be a 1-d array, which means that the single sentence (sample) will be a 2-d array. Which means that the complete data (collection of sentences here) will be a collection of 2-d arrays. Even in that, since each sentence can have different number of words, it cannot be combined into a single 3-d array. </p></li>
<li><p>Secondly, the <code>W2VTransformer</code> in gensim looks like a scikit-learn compatible class, but its not. It tries to follows ""scikit-learn API conventions"" for defining the methods <code>fit()</code>, <code>fit_transform()</code> and <code>transform()</code>. They are <strong>not compatible</strong> with scikit-learn <code>Pipeline</code>. </p>

<p>You can see that the input param requirements of <code>fit()</code> and <code>fit_transform()</code> are different. </p>

<ul>
<li><p><a href=""https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html#gensim.sklearn_api.w2vmodel.W2VTransformer.fit"" rel=""nofollow noreferrer""><code>fit()</code></a>:</p>

<blockquote>
  <p><strong>X (iterable of iterables of str)</strong> – The input corpus. </p>
  
  <p>X can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from
  disk/network. See BrownCorpus, Text8Corpus or LineSentence in word2vec
  module for such examples.</p>
</blockquote></li>
<li><p><a href=""https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html#gensim.sklearn_api.w2vmodel.W2VTransformer.fit_transform"" rel=""nofollow noreferrer""><code>fit_transform()</code></a>:</p>

<blockquote>
  <p><strong>X (numpy array of shape [n_samples, n_features])</strong> – Training set.</p>
</blockquote></li>
</ul></li>
</ul>

<p>If you want to use scikit-learn, then you will need to have the 2-d shape. You will need to ""somehow merge"" word-vectors for a single sentence to form a 1-d array for that sentence. That means that you need to form a kind of sentence-vector, by doing:</p>

<ul>
<li>sum of individual words</li>
<li>average of individual words</li>
<li>weighted averaging of individual words based on frequency, tf-idf etc.</li>
<li>using other techniques like sent2vec, paragraph2vec, doc2vec etc.</li>
</ul>

<p><strong>Note</strong>:- I noticed now that <a href=""https://stackoverflow.com/questions/53997148/implementing-gensim-wrapper-into-sklearn"">you were doing this thing based on <code>D2VTransformer</code></a>. That should be the correct approach here if you want to use sklearn.</p>

<p>The issue in that question was this line (since that question is now deleted):</p>

<pre><code>X_train = vectorizer.fit_transform(X_train)
</code></pre>

<p>Here, you overwrite your original <code>X_train</code> (list of list of words) with already calculated word vectors and hence that error. </p>

<p>Or else, you can use other tools / libraries (keras, tensorflow) which allow sequential input of variable size. For example, LSTMs can be configured here to take a variable input and an ending token to mark the end of sentence (a sample).</p>

<p><strong>Update</strong>:</p>

<p>In the above given solution, you can replace the lines:</p>

<pre><code>model = D2VTransformer(dm=1, size=50, min_count=2, iter=10, seed=0)
model.fit(X_train)

clf = LogisticRegression(penalty='l2', C=0.1, random_state=0)
clf.fit(model.transform(X_train), y_train)

pipeline = Pipeline([
        ('vec', model),
        ('clf', clf)
    ])

y_pred = pipeline.predict(X_train)
</code></pre>

<p>with </p>

<pre><code>pipeline = Pipeline([
        ('vec', model),
        ('clf', clf)
    ])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_train)
</code></pre>

<p>No need to fit and transform separately, since <code>pipeline.fit()</code> will automatically do that.</p>
",2,-1,1907,2019-01-01 19:47:37,https://stackoverflow.com/questions/53998446/w2vtransformer-only-works-with-one-word-as-input
Accessing model in gensim wrapper,"<p>I use following gensim wrapper to train a word-vector model:</p>

<pre><code>import numpy as np
import pandas as pd
from gensim.sklearn_api import W2VTransformer
from gensim.utils import simple_preprocess

# Load synthetic data
data = pd.read_csv('https://pastebin.com/raw/EPCmabvN')
data = data.head(10)
# Set random seed
np.random.seed(0)

X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
y_train = data.label

model = W2VTransformer(size=10, min_count=1)
model.fit(X_train)

model.wv.vocab
</code></pre>

<p>However, once I try to access the trained model, i.e. <code>model.wv.vocab</code>, it outputs the error:</p>

<blockquote>
  <p>AttributeError: 'W2VTransformer' object has no attribute 'wv'</p>
</blockquote>

<p>Can I somehow access the vocabulary and other model parameters, or is this not possible with the wrapper?</p>

<pre><code>Current workaround: 

from gensim.models.doc2vec import TaggedDocument
from gensim.models.doc2vec import Doc2Vec

#Defining model without wrapper
documents = data.apply(lambda r: TaggedDocument(words=simple_preprocess(r['text'], min_len=2), tags=[r.label]), axis=1)
d2v = Doc2Vec(documents, window=2, vector_size=10, min_count=1, seed=0)
d2v.wv.vocab
</code></pre>
","model, wrapper, gensim","<p>What makes you think <code>W2VTransformer</code> has a <code>wv</code> property? It's not listed in the class docs:</p>

<p><a href=""https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/sklearn_api/w2vmodel.html</a></p>

<p>And, it's not quite idiomatic (within scikit-learn) to access a <code>Transformer</code>'s internal state like that. Instead, you would ask a model that's already been <code>fit()</code> to then <code>transform()</code> a list-of-words, to get back a list-of-word-vectors. </p>

<p>Indeed that's shown in the example at the top of those <code>gensim</code> docs, in a line which does both the <code>fit()</code> and `transform() in one line (even if you wouldn't want to do that):  </p>

<pre><code>wordvecs = model.fit(common_texts).transform(['graph', 'system'])
</code></pre>

<p>If you do want to access the native <code>gensim</code> <code>Word2Vec</code> model directly – a model which <strong>does</strong> have a <code>wv</code> property – you'd have to use a different approach. For example, you could review the <code>W2VTransformer</code> source code to see where that internal model is kept:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/sklearn_api/w2vmodel.py"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/sklearn_api/w2vmodel.py</a></p>

<p>There you would see that the <code>fit()</code> method stores the current <code>Word2Vec</code> instance in a property called <code>gensim_model</code>. </p>

<p>So, your line that is erroring, where <code>model</code> is an instance of <code>W2VTransformer</code>, could instead be:</p>

<pre><code>model.gensim_model.wv.vocab
</code></pre>
",1,0,668,2019-01-02 09:03:17,https://stackoverflow.com/questions/54003616/accessing-model-in-gensim-wrapper
Cross-validation for paragraph-vector model,"<p>I just came across an error when trying to apply a cross-validation for a paragraph vector model: </p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from gensim.sklearn_api import D2VTransformer

data = pd.read_csv('https://pastebin.com/raw/bSGWiBfs')
np.random.seed(0)

X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
y_train = data.label

model = D2VTransformer(size=10, min_count=1, iter=5, seed=1)
clf = LogisticRegression(random_state=0)

pipeline = Pipeline([
        ('vec', model),
        ('clf', clf)
    ])

pipeline.fit(X_train, y_train)

score = pipeline.score(X_train, y_train)
print(""Score:"", score) # This works
cval = cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=3)
print(""Cross-Validation:"", cval) # This doesn't work
</code></pre>

<blockquote>
  <p>KeyError: 0</p>
</blockquote>

<p>I experimented by replacing <code>X_train</code> in <code>cross_val_score</code> with <code>model.transform(X_train)</code> or <code>model.fit_transform(X_train)</code>. Also, I tried the same with raw input data (<code>data.text</code>), instead of pre-processed text. I suspect that something must be wrong with the format of <code>X_train</code> for the cross-validation, as compared to the <code>.score</code> function for Pipeline, which works just fine. I also noted that the <code>cross_val_score</code> worked with <code>CountVectorizer()</code>.</p>

<p>Does anyone spot the mistake?</p>
","scikit-learn, transform, cross-validation, gensim","<p>No, this has nothing to do with transformation from <code>model</code>. Its related to <code>cross_val_score</code>.</p>

<p><code>cross_val_score</code> will split the supplied data according the the <code>cv</code> param. For this, it will do something like this:</p>

<pre><code>for train, test in splitter.split(X_train, y_train):
    new_X_train, new_y_train = X_train[train], y_train[train]
</code></pre>

<p>But your <code>X_train</code> is a <code>pandas.Series</code> object in which the index based selection does not work like this. See this:<a href=""https://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-position"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-position</a></p>

<p>Change this line:</p>

<pre><code>X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1)
</code></pre>

<p>to:</p>

<pre><code># Access the internal numpy array
X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1).values

OR

# Convert series to list
X_train = data.apply(lambda r: simple_preprocess(r['text'], min_len=2), axis=1).tolist()
</code></pre>
",1,0,67,2019-01-02 10:54:48,https://stackoverflow.com/questions/54005055/cross-validation-for-paragraph-vector-model
Show progress in lemmatization,"<p>following script is used to lemmatize a given input column with text:</p>

<pre><code>%%time
import pandas as pd
from gensim.utils import lemmatize
from gensim.parsing.preprocessing import STOPWORDS
STOPWORDS = list(STOPWORDS)

data = pd.read_csv('https://pastebin.com/raw/0SEv1RMf')

def lemmatization(s):
    result = []
    # lowercase, tokenize, remove stopwords, len&gt;3, lemmatize
    for token in lemmatize(s, stopwords=STOPWORDS, min_length=3):
        result.append(token.decode('utf-8').split('/')[0])
    # print(len(result)) &lt;- This didn't work.
    return result

X_train = data.apply(lambda r: lemmatization(r['text']), axis=1)
print(X_train)
</code></pre>

<p><strong>Question:</strong></p>

<p><strong>How can I print the progress of the lemmatization progress?</strong></p>
","gensim, lemmatization","<p>You could pass a variable into the lemmatization function to keep track of the number of times it was called - and then print it every 1000 iterations or so. I have wrapped it in a list below so the int can be passed by reference rather than by value.</p>

<pre><code>%%time
import pandas as pd
from gensim.utils import lemmatize
from gensim.parsing.preprocessing import STOPWORDS
STOPWORDS = list(STOPWORDS)

data = pd.read_csv('https://pastebin.com/raw/0SEv1RMf')

iteration_count = [0]

def lemmatization(s, iteration_count):
    result = []
    # lowercase, tokenize, remove stopwords, len&gt;3, lemmatize
    for token in lemmatize(s, stopwords=STOPWORDS, min_length=3):
        result.append(token.decode('utf-8').split('/')[0])
    # print(len(result)) &lt;- This didn't work.

    iteration_count[0] += 1

    if iteration_count[0] % 1000 == 0:
        print(iteration_count[0])

    return result

X_train = data.apply(lambda r: lemmatization(r['text'], iteration_count), axis=1)
print(X_train)
</code></pre>
",1,0,229,2019-01-06 10:11:56,https://stackoverflow.com/questions/54060506/show-progress-in-lemmatization
FastText: Can&#39;t get cross_validation,"<p>I am struggling to implement FastText (<a href=""https://github.com/kataev/gensim/blob/969b7178a7c90690f18db33807b390048ef0c83e/gensim/sklearn_api/ftmodel.py"" rel=""nofollow noreferrer"">FTTransformer</a>) into a Pipeline that iterates over different vectorizers. More particular, I can't get cross-validation scores. Following code is used:</p>

<pre><code>%%time
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.pipeline import Pipeline
from gensim.utils import simple_preprocess
from gensim.sklearn_api.ftmodel import FTTransformer
np.random.seed(0)

data = pd.read_csv('https://pastebin.com/raw/dqKFZ12m')
X_train, X_test, y_train, y_test = train_test_split(data.text, data.label, random_state=0)
w2v_texts = [simple_preprocess(doc) for doc in X_train]

models = [FTTransformer(size=10, min_count=0, seed=42)]
classifiers = [LogisticRegression(random_state=0)]

for model in models:

    for classifier in classifiers:

        model.fit(w2v_texts)
        classifier.fit(model.transform(X_train), y_train)

        pipeline = Pipeline([
                ('vec', model),
                ('clf', classifier)
            ])

        print(pipeline.score(X_train, y_train))
        #print(model.gensim_model.wv.most_similar('kirk'))

        cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=5)
</code></pre>

<blockquote>
  <p>KeyError: 'all ngrams for word  ""Machine learning can be useful
  branding sometimes"" absent from model'</p>
</blockquote>

<p><strong>How can the problem be solved?</strong> </p>

<p>Sidenote: My other pipelines with <code>D2VTransformer</code> or <code>TfIdfVectorizer</code> work just fine. Here, I can simply apply <code>pipeline.fit(X_train, y_train)</code> after defining the pipeline, instead of the two fits as shown above. It seems like <a href=""https://github.com/kataev/gensim/blob/969b7178a7c90690f18db33807b390048ef0c83e/gensim/sklearn_api/ftmodel.py"" rel=""nofollow noreferrer"">FTTransformer</a> doesn't integrate so well with other given vectorizers?  </p>
","scikit-learn, cross-validation, gensim","<p>Yes, to be used in a pipeline, <code>FTTransformer</code> needs to be modified to split documents to words inside its <code>fit</code> method. One can do it as follows:</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.pipeline import Pipeline
from gensim.utils import simple_preprocess
from gensim.sklearn_api.ftmodel import FTTransformer
np.random.seed(0)


class FTTransformer2(FTTransformer):

    def fit(self, x, y):
        super().fit([simple_preprocess(doc) for doc in x])
        return self


data = pd.read_csv('https://pastebin.com/raw/dqKFZ12m')
X_train, X_test, y_train, y_test = train_test_split(data.text, data.label, random_state=0)

classifiers = [LogisticRegression(random_state=0)]

for classifier in classifiers:

    pipeline = Pipeline([
            ('ftt', FTTransformer2(size=10, min_count=0, seed=0)),
            ('clf', classifier)
        ])

    score = cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=5)
    print(score)
</code></pre>
",1,0,808,2019-01-10 15:11:13,https://stackoverflow.com/questions/54131612/fasttext-cant-get-cross-validation
What is the stochastic aspect of Word2Vec?,"<p>I'm vectorizing words on a few different corpora with Gensim and am getting results that are making me rethink how Word2Vec functions. My understanding was that Word2Vec was deterministic, and that the position of a word in a vector space would not change from training to training. If ""My cat is running"" and ""your dog can't be running"" are the two sentences in the corpus, then the value of ""running"" (or its stem) seems necessarily fixed.</p>

<p>However, I've found that that value indeed does vary across models, and words keep changing where they are on a vector space when I train the model. The differences are not always hugely meaningful, but they do indicate the existence of some random process. What am I missing here?</p>
","nlp, gensim, word2vec","<p>This is well-covered in the <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ#q11-ive-trained-my-word2vecdoc2vecetc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">Gensim FAQ</a>, which I quote here:</p>

<blockquote>
  <h3>Q11: I've trained my <code>Word2Vec</code>/<code>Doc2Vec</code>/etc model repeatedly using the exact same text corpus, but the vectors are different each time. Is there a bug or have I made a mistake? (*2vec training non-determinism)</h3>
  
  <p><strong>Answer:</strong> The *2vec models (word2vec, fasttext, doc2vec…) begin with random initialization, then most modes use additional randomization
  during training. (For example, the training windows are randomly
  truncated as an efficient way of weighting nearer words higher. The
  negative examples in the default negative-sampling mode are chosen
  randomly. And the downsampling of highly-frequent words, as controlled
  by the <code>sample</code> parameter, is driven by random choices. These
  behaviors were all defined in the original Word2Vec paper's algorithm
  description.)</p>
  
  <p>Even when all this randomness comes from a
  pseudorandom-number-generator that's been seeded to give a
  reproducible stream of random numbers (which gensim does by default),
  the usual case of multi-threaded training can further change the exact
  training-order of text examples, and thus the final model state.
  (Further, in Python 3.x, the hashing of strings is randomized each
  re-launch of the Python interpreter - changing the iteration ordering
  of vocabulary dicts from run to run, and thus making even the same
  string-of-random-number-draws pick different words in different
  launches.)</p>
  
  <p>So, it is to be expected that models vary from run to run, even
  trained on the same data. There's no single ""right place"" for any
  word-vector or doc-vector to wind up: just positions that are at
  progressively more-useful distances &amp; directions from other vectors
  co-trained inside the same model. (In general, only vectors that were
  trained together in an interleaved session of contrasting uses become
  comparable in their coordinates.) </p>
  
  <p>Suitable training parameters should yield models that are roughly as
  useful, from run-to-run, as each other. Testing and evaluation
  processes should be tolerant of any shifts in vector positions, and of
  small ""jitter"" in the overall utility of models, that arises from the
  inherent algorithm randomness. (If the observed quality from
  run-to-run varies a lot, there may be other problems: too little data,
  poorly-tuned parameters, or errors/weaknesses in the evaluation
  method.)</p>
  
  <p>You can try to force determinism, by using <code>workers=1</code> to limit
  training to a single thread – and, if in Python 3.x, using the
  <code>PYTHONHASHSEED</code> environment variable to disable its usual string hash
  randomization. But training will be much slower than with more
  threads. And, you'd be obscuring the inherent
  randomness/approximateness of the underlying algorithms, in a way that
  might make results more fragile and dependent on the luck of a
  particular setup. It's better to tolerate a little jitter, and use
  excessive jitter as an indicator of problems elsewhere in the data or
  model setup – rather than impose a superficial determinism.</p>
</blockquote>
",3,3,1164,2019-01-13 00:26:23,https://stackoverflow.com/questions/54165109/what-is-the-stochastic-aspect-of-word2vec
Doc2Vec: infer most similar vector from ConcatenatedDocvecs,"<p>I am generating a Doc2Vec embedding of a Pandas DataFrame by following the guidance provided <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
from gensim.test.test_doc2vec import ConcatenatedDoc2Vec
import gensim.models.doc2vec
from collections import OrderedDict
import pandas as pd
import numpy as np

cube_embedded =  # pandas cube
# convert the cube to documents
alldocs = [TaggedDocument(doc, [i]) for i, doc in enumerate(cube_embedded.values.tolist())]

# train models
simple_models = [
    # PV-DBOW plain
    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores),
    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes
    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),
    # PV-DM w/ concatenation - big, slow, experimental mode window=5 (both sides) approximates paper's apparent 10-word total window size
    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores),
]

for d2v_model in simple_models:
    d2v_model.build_vocab(alldocs)
    d2v_model.train(alldocs, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)

models_by_name = OrderedDict((str(d2v_model), d2v_model) for d2v_model in simple_models)
models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])
models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])
</code></pre>

<p>Given a document vector V, if I try to infer the most similar documents to the document vector V from a ConcatenatedDocvecs model, I get the following error:</p>

<pre><code>V = np.random.rand(200)
models_by_name['dbow+dmc'].docvecs.most_similar([V])

AttributeError: 'ConcatenatedDocvecs' object has no attribute 'most_similar'
</code></pre>

<p>Of course, I cannot use the simple models to infer similar documents as the produced vector embeddings have size 100 (and not 200 as the concatenated vectors do).</p>

<p>How can I get the list of most similar documents to a document vector from a ConcatenatedDocvecs model?</p>
","python, gensim, word2vec, doc2vec","<p>The <code>ConcatenatedDocvecs</code> is a simple utility wrapper class that lets you access the concatenation of a tag's vectors in multiple underlying <code>Doc2Vec</code> models. It exists to make it a little easier to reproduce some of the analysis in the original 'ParagraphVector' paper.</p>

<p>It doesn't reproduce all the functionality of a <code>Doc2Vec</code> model (or set of keyed-vectors), so can't directly hep you with the <code>most_similar()</code> you want to perform. </p>

<p>You could instead do a most-similar operation within each of the constituent models, then combine the two similarity measures (per neighbor) – such as by averaging them – to get a usable similarity-like value for the combined model (and then re-sort on that). I suspect, but am not sure, such a value from the two 100d models would behave very much like a a true cosine-similarity from the concatenated 200d model.</p>

<p>Alternatively, instead of using <code>ConcatenatedDoc2Vec</code> wrapper class (which only creates and returns the concatenated 200d vectors when requested), you could look at the various <code>KeyedVectors</code> class in gensim, and use (or adapt) one to be filled with all the concatenated 200d vectors from the two constituent models. Then, its <code>most_similar()</code> would work.</p>
",1,0,936,2019-01-14 17:15:22,https://stackoverflow.com/questions/54186233/doc2vec-infer-most-similar-vector-from-concatenateddocvecs
Streaming corpus to a vectorizer in a pipeline,"<p>I have a large language corpus and I use sklearn tfidf vectorizer and gensim Doc2Vec to compute language models. My total corpus has about 100,000 documents and I realized that my Jupyter notebook stops computing once I cross a certain threshold. I guess that the memory is full after applying the grid-search and cross-validation steps.</p>

<p>Even following example script already stops for Doc2Vec at some point:</p>

<pre><code>%%time
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.externals import joblib

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.sklearn_api import D2VTransformer

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from gensim.utils import simple_preprocess

np.random.seed(1)

data = pd.read_csv('https://pastebin.com/raw/dqKFZ12m')
X_train, X_test, y_train, y_test = train_test_split([simple_preprocess(doc) for doc in data.text],
                                                    data.label, random_state=1)

model_names = [
               'TfidfVectorizer',
               'Doc2Vec_PVDM',
              ]

models = [
    TfidfVectorizer(preprocessor=' '.join, tokenizer=None, min_df = 5),
    D2VTransformer(dm=0, hs=0, min_count=5, iter=5, seed=1, workers=1),
]

parameters = [
              {
              'model__smooth_idf': (True, False),
              'model__norm': ('l1', 'l2', None)
              },
              {
              'model__size': [200],
              'model__window': [4]
              }
              ]

for params, model, name in zip(parameters, models, model_names):

    pipeline = Pipeline([
      ('model', model),
      ('clf', LogisticRegression())
      ])

    grid = GridSearchCV(pipeline, params, verbose=1, cv=5, n_jobs=-1)
    grid.fit(X_train, y_train)
    print(grid.best_params_)

    cval = cross_val_score(grid.best_estimator_, X_train, y_train, scoring='accuracy', cv=5, n_jobs=-1)
    print(""Cross-Validation (Train):"", np.mean(cval))

print(""Finished."")
</code></pre>

<p><strong>Is there a way to ""stream"" each line in a document, instead of loading the full data into memory? Or another way to make it more memory efficient? I read a few articles on the topic but could not discover any that included a pipeline example.</strong></p>
","scikit-learn, streaming, gensim, corpus","<p>With just 100,000 documents, unless they're gigantic, it's not necessarily the loading-of-data into memory that's causing you problems. Note especially:</p>

<ul>
<li>loading &amp; tokenizing the docs has already succeeded before you even begin the scikit-learn pipelines/grid-search, and the further multiplication of memory usage is in the necessarily-repeated alternate models, <em>not</em> the original docs</li>
<li>scikit-learn APIs tend to assume the training data is fully in memory – so even though the innermost gensim classes (<code>Doc2Vec</code>) are happy with streamed data of arbitrary size, it's harder to adapt that into scikit-learn</li>
</ul>

<p>So you should look elsewhere, and there are other issues with your shown code. </p>

<p>I've often had memory or lockup issues with scikit-learn's attempts at parallelism (as enabled through <code>n_jobs</code>-like parameters), especially inside Jupyter notebooks. It forks full OS processes, which tend to blow up memory usage. (Each sub-process gets a full copy of the parent process's memory, which might be efficiently shared – until the subprocess starts moving/changing things.) Sometimes one process, or inter-process communication, fails and the main process is just left waiting for a response – which seems to especially confuse Jupyter notebooks. </p>

<p>So, unless you have tons of memory and absolutely need scikit-learn parallelism, I'd recommend trying to get things working with <code>n_jobs=1</code> first – and only later experimenting with more jobs. </p>

<p>In contrast, the <code>workers</code> of the <code>Doc2Vec</code> class (and <code>D2VTransformer</code>) uses lighter-weight threads, and you should use at least <code>workers=3</code>, and perhaps 8 (if you have at least that many cores, rather than the <code>workers=1</code> you're using now. </p>

<p>But also: you're doing a bunch of redundant actions of unclear value in your code. The test set from initial train-test split isn't ever used. (Perhaps you were thinking of keeping it aside as a final validation set? That's the most rigorous way to get a good estimate of your final result's performance on future unseen data, but in many contexts data is limited, and that estimate isn't as important as just doing the best possible with limited data.)</p>

<p>The <code>GridSearchCV</code> itself does a 5-way train/test split as part of its work, and its best results are remembered in its properties when it's done. </p>

<p>So you don't need to do the <code>cross_val_score()</code> again - you can read the results from <code>GridSearchCV</code>.  </p>
",3,1,474,2019-01-16 08:37:46,https://stackoverflow.com/questions/54213078/streaming-corpus-to-a-vectorizer-in-a-pipeline
Gensim pretrained model similarity,"<p>Problem :</p>

<p>Im using glove pre-trained model with vectors to retrain my model with a specific domain say #cars, after training I want to find similar words within my domain but I got words not in my domain corpus, I believe it's from glove's vectors. </p>

<pre><code>model_2.most_similar(positive=['spacious'],    topn=10)

[('bedrooms', 0.6275501251220703),
 ('roomy', 0.6149100065231323),
 ('luxurious', 0.6105825901031494),
 ('rooms', 0.5935696363449097),
 ('furnished', 0.5897485613822937),
 ('cramped', 0.5892841219902039),
 ('courtyard', 0.5721820592880249),
 ('bathrooms', 0.5618442893028259),
 ('opulent', 0.5592212677001953),
 ('expansive', 0.555268406867981)]
</code></pre>

<p>Here I expect something like leg-room, car's spacious features mentioned in the domain's corpus. How can we exclude the glove vectors while having similar vectors?</p>

<p>Thanks  </p>
","python, vector, nlp, gensim, word2vec","<p>There may not be enough info in a simple set of generic word-vectors to filter neighbors by domain-of-use. </p>

<p>You could try using a mixed-weighting: combine the similarities to <code>'spacious'</code>, and to <code>'cars'</code>, and return the top results in that combination – and it might help a little. </p>

<p>Supplying more than one <code>positive</code> word to the <code>most_similar()</code> method might approximate this. If you're sure of some major sources of interference/overlap, you might even be able to use <code>negative</code> word examples, similar to how word2vec finds candidate answers for analogies (though this might also suppress useful results that are legitimately related to both domains, like <code>'roomy'</code>). For example:</p>

<pre><code>candidates = vec_model.most_similar(positive=['spacious', 'car'], 
                                    negative=['house'])
</code></pre>

<p>(Instead of using single words like 'car' or 'house' you could also try using vectors combined from many words that define a domain.)</p>

<p>But a sharp distinction sounds like a research project, rather than something easily possible with off-the-shelf libraries/vectors – and may requires more sophisticated approaches and datasets. </p>

<p>You could also try using a set of vectors trained only on a dataset of text from the domain of interest – thus ensuring the vocabulary, and senses, of words are all in that domain. </p>
",1,0,440,2019-01-16 10:52:02,https://stackoverflow.com/questions/54215456/gensim-pretrained-model-similarity
Combining/adding vectors from different word2vec models,"<p>I am using gensim to create Word2Vec models trained on large text corpora. I have some models based on StackExchange data dumps. I also have a model trained on a corpus derived from English Wikipedia. </p>

<p>Assume that a vocabulary term is in both models, and that the models were created with the same parameters to Word2Vec. Is there any way to combine or add the vectors from the two separate models to create a single new model that has the same word vectors that would have resulted if I had combined both corpora initially and trained on this data?</p>

<p>The reason I want to do this is that I want to be able to generate a model with a specific corpus, and then if I process a new corpus later, I want to be able to add this information to an existing model rather than having to combine corpora and retrain everything from scratch (i.e. I want to avoid reprocessing every corpus each time I want to add information to the model). </p>

<p>Are there builtin functions in gensim or elsewhere that will allow me to combine models like this, adding information to existing models instead of retraining?</p>
","python, gensim, word2vec, training-data, corpus","<p>Generally, only word vectors that were trained together are meaningfully comparable. (It's the interleaved tug-of-war during training that moves them to relative orientations that are meaningful, and there's enough randomness in the process that even models trained on the same corpus will vary in where they place individual words.)</p>

<p>Using words from both corpuses as guideposts, it is possible to learn a transformation from one space A to the other B, that tries to move those known-shared-words to their corresponding positions in the other space. Then, applying that same transformation to the words in A that <em>aren't</em> in B, you can find B coordinates for those words, making them comparable to other native-B words.</p>

<p>This technique has been used with some success in word2vec-driven language translation (where the guidepost pairs are known translations), or as a means of growing a limited word-vector set with word-vectors from elsewhere. Whether it'd work well enough for your purposes, I don't know. I imagine it could go astray especially where the two training corpuses use shared tokens in wildly different senses. </p>

<p>There's a class, <code>TranslationMatrix</code>, that may be able to do this for you in the <code>gensim</code> library. See:</p>

<p><a href=""https://radimrehurek.com/gensim/models/translation_matrix.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/translation_matrix.html</a></p>

<p>There's a demo notebook of its use at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb</a></p>

<p>(Whenever practical, doing a full training on a mixed-together corpus, with all word examples, is likely to do better.)</p>
",2,1,3036,2019-01-17 20:27:57,https://stackoverflow.com/questions/54243797/combining-adding-vectors-from-different-word2vec-models
How to measure the accuracy of Word2vec model Trained on another language?,"<p>I've trained a word2vec model not for English but for an Asian language 'Sinhala'. in the later phase, I'm going to use this trained model to get the sentence similarities in order to detect plagiarism in Sinhala documents.
Please explain to me how to measure the accuracy of the trained model.I'm a university student. I have no previous knowledge of these things.</p>
","gensim, word2vec","<p>There is no universal measure of word2vec model quality or 'accuracy'. </p>

<p>The commonly-reported ""accuracy"" is typically based on a set of english-language analogy questions that were used by Google in their original word2vec paper (and included in their source code release). See for example:</p>

<p><a href=""https://github.com/tmikolov/word2vec/blob/master/questions-words.txt"" rel=""nofollow noreferrer"">https://github.com/tmikolov/word2vec/blob/master/questions-words.txt</a></p>

<p>To make a similar calculation for another language, you'd need to supply a similar set of evaluation questions for that language. I don't know of any collection of such questions for Sinhalese, or other languages, so you may have to find or create it yourself. (You could create an alternate file in the same format, and use the existing evaluation methods, specifying your alternate file.) </p>
",4,1,869,2019-01-20 14:18:14,https://stackoverflow.com/questions/54277363/how-to-measure-the-accuracy-of-word2vec-model-trained-on-another-language
How to train a Phrases model from a huge corpus of articles (wikipedia)?,"<p>I'd like to create a big gensim dictionary for french language to try getting better results in topic detection, similarities between texts and other things like that.
So I've planned to use a wikipedia dump and process it the following way:</p>

<ol>
<li>Extract each article from frwiki-YYYYMMDD-pages-articles.xml.bz2 (Done)</li>
<li>Tokenize each article (basically converting the text to lowercases, removing stop words and non-word characters) (Done)</li>
<li>Train a Phrases model on the articles to detect collocation.</li>
<li>Stem the resulting tokens in each article.</li>
<li>Feed the dictionary with the new corpus (one stemmed-collocated-tokenized article per line)</li>
</ol>

<p>Because of the very large size of the corpus, I don't store anything in memory and access the corpus via smart_open but it appears gensim Phrases model is consuming too much RAM to complete the third step.</p>

<p>Here is my sample code:</p>

<pre><code>corpus = smart_open(corpusFile, ""r"")
phrases = gensim.models.Phrases()
with smart_open(phrasesFile, ""wb"") as phrases_file:
    chunks_size = 10000
    texts, i = [], 0
    for text in corpus:
        texts.append(text.split())
        i += 1
        if i % chunks_size == 0:
            phrases.add_vocab(texts)
            texts = []
    phrases.save(phrases_file)
corpus.close()
</code></pre>

<p>Is there a way to complete the operation without freezing my computer or will I have to train the Phrases model only on a subset of my corpus?</p>
","python, nltk, gensim, collocation","<p>I'm answering myself because I realized I forgot to deal with some memory related parameters in the Phrases class.</p>

<p>So, first I've divided max_vocab_size by 2 so it should consume less memory, and also I've decided to save the Phrases object every 100 000 articles and then reload it from the saved file as these kind of tricks have shown they can be helpful with some other classes in the gensim lib...</p>

<p>Here is the new code, a little slower maybe but it has completed the task successfully:</p>

<pre><code>corpus = smart_open(corpusFile, ""r"")
max_vocab_size=20000000
phrases = Phrases(max_vocab_size=max_vocab_size)
chunks_size = 10000
save_every = 100000
texts, i = [], 0
for text in corpus:
    texts.append(text.split())
    i += 1
    if i % chunks_size == 0:
        phrases.add_vocab(texts)
        texts = []
    if i % save_every == 0:
        phrases.save(phrasesFile)
        phrases = Phrases.load(phrasesFile)
corpus.close()
phrases.save(phrasesFile)
</code></pre>

<p>Ending up with 412 816 phrasegrams in my case after putting all this in a Phraser object.</p>
",1,1,689,2019-01-23 01:15:31,https://stackoverflow.com/questions/54318701/how-to-train-a-phrases-model-from-a-huge-corpus-of-articles-wikipedia
How can I find semantically similar paragraph in two different text files (two documents),"<p>I have found so many similar questions none of them answer my problem can Someone help me . I have two legal documents I need to find which are contextually same or have same meaning what should be my approach. I thought of use something with LSTM wherever I see i get people having one or two sentences to compare . I want to do it for lot of docs and find out which of them are similar cannot get my head around how to begin my task  </p>
","nlp, gensim, recurrent-neural-network","<p>I think you are describing the purpose behind Doc2Vec. You can train this model to produce document vectors which can be used for measuring similarity. If you have heard of Word2Vec, this is related to that at the document level. You will likely need to tweak the model to get it to work how you want it to, but this is probably a good start at least.</p>

<p><a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Doc2Vec Official Documentation</a></p>

<p><a href=""https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e"" rel=""nofollow noreferrer"">Doc2Vec Tutorial</a></p>
",1,0,325,2019-01-28 11:10:47,https://stackoverflow.com/questions/54400712/how-can-i-find-semantically-similar-paragraph-in-two-different-text-files-two-d
Tracking loss and embeddings in Gensim word2vec model,"<p>I'm pretty new to Gensim and I'm trying to train my first model using word2vec model. I see that all the parameters are pretty straightforward and easy to understand, however I don't know how to track the loss of the model to see the progress. Also, I would like to be able to get the embeddings after each epoch so that I can also <em>show</em> that the predictions also get more <em>logical</em> with after each epoch. How can I do that?</p>

<p>OR, is it better to train for <em>iter=1</em> each time and save the loss and embeddings after each epoch? Sounds not too efficient.</p>

<p>Not much to show with the code but still posting it below:</p>

<pre><code>model = Word2Vec(sentences = trainset, 
             iter = 5, # epoch
             min_count = 10, 
             size = 150, 
             workers = 4, 
             sg = 1, 
             hs = 1, 
             negative = 0, 
             window = 9999)
</code></pre>
","gensim, word2vec","<p><code>gensim</code> allows us to use <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""noreferrer"">callbacks</a> for such purposes.</p>

<p>Example:</p>

<pre><code>from gensim.models.callbacks import CallbackAny2Vec

class MonitorCallback(CallbackAny2Vec):
    def __init__(self, test_words):
        self._test_words = test_words

    def on_epoch_end(self, model):
        print(""Model loss:"", model.get_latest_training_loss())  # print loss
        for word in self._test_words:  # show wv logic changes
            print(model.wv.most_similar(word))

""""""
prepare datasets etc.
... 
...
""""""

monitor = MonitorCallback([""word"", ""I"", ""less""])  # monitor with demo words
model = Word2Vec(sentences = trainset, 
             iter = 5, # epoch
             min_count = 10, 
             size = 150, 
             workers = 4, 
             sg = 1, 
             hs = 1, 
             negative = 0, 
             window = 9999, 
             callbacks=[monitor])
</code></pre>

<ul>
<li>now there's some <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=2ahUKEwjQg4zzm5PgAhUqpIsKHTcqBHUQFjAAegQIChAB&amp;url=https%3A%2F%2Fgithub.com%2FRaRe-Technologies%2Fgensim%2Fissues%2F1172&amp;usg=AOvVaw1f0_kOzCBG1EupM1GkAUPM"" rel=""noreferrer"">issues</a> with <code>get_latest_training_loss</code> - may be it's incorrect (bad luck, for now github is down, can't check). I've tested this code and loss increases - looks weird.</li>
<li>may be you prefer <code>logging</code> - gensim is <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""noreferrer"">fitted</a> for it.</li>
</ul>
",12,4,4387,2019-01-29 14:03:15,https://stackoverflow.com/questions/54422810/tracking-loss-and-embeddings-in-gensim-word2vec-model
Can&#39;t load saved gensim word2vec model,"<p>I tried saving a word2vec model that I had trained with gensim like so:</p>

<pre><code>from gensim.models import Word2Vec
model = Word2Vec(sentences, parameters)
model.save('modelfile.model')
</code></pre>

<p>Now when I try <code>Word2Vec.load('modelfile.model')</code>, I get:</p>

<pre><code>ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'
</code></pre>

<p>Can post the full traceback if it helps.</p>
","python, gensim","<p>That's odd. Are you using the exact same Python environment &amp; gensim version for the <code>load()</code> as the <code>save()</code>? How did you install gensim &amp; numpy? </p>

<p>When I search for that error, I find other discussions that suggest it may be a symptom of having pickled (saved) a numpy array from numpy 1.16. but trying to unpickle (load) it in an earlier numpy. See for example: </p>

<p><a href=""https://github.com/numpy/numpy/issues/12825#issuecomment-456561919"" rel=""nofollow noreferrer"">https://github.com/numpy/numpy/issues/12825#issuecomment-456561919</a></p>

<p>It looks like numpy has recently merged a fix – <a href=""https://github.com/numpy/numpy/issues/12837"" rel=""nofollow noreferrer"">https://github.com/numpy/numpy/issues/12837</a> – but in the meantime your best bet might be making sure the place where you're loading is using numpy 1.16.0+.</p>
",1,0,986,2019-02-01 03:14:39,https://stackoverflow.com/questions/54472367/cant-load-saved-gensim-word2vec-model
What are the defaults for gensim&#39;s fasttext?,"<p>I cannot find anything about the default values about the parameters for gensim fasttext <a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText"" rel=""nofollow noreferrer"">here</a></p>

<p>Or are they the same as for the original Facebook fasttext implementation?</p>
","gensim, fasttext","<p>The very link in your question, <a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText</a>, shows all the defaults right there. To excerpt it here:</p>

<pre><code>class gensim.models.fasttext.FastText(sentences=None, corpus_file=None, 
    sg=0, hs=0, size=100, alpha=0.025, window=5, min_count=5, 
    max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, 
    min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, 
    hashfxn=&lt;built-in function hash&gt;, iter=5, null_word=0, min_n=3, 
    max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, 
    batch_words=10000, callbacks=(), compatible_hash=True)
</code></pre>

<p>Those that are for the corresponding parameter to the Facebook native FastText probably <em>should</em> have the same defaults, but it's possible that some have varied slightly to match analogous parameters in other gensim classses. So, if you were counting on identical defaults for some analysis, you should check these values against the Facebook docs. </p>
",1,2,457,2019-02-02 11:00:22,https://stackoverflow.com/questions/54492390/what-are-the-defaults-for-gensims-fasttext
I get more vectors than my documents size - gensim doc2vec,"<p>I have protein sequences and want to do doc2vec. My goal is to have one vector for each sentence/sequence.</p>

<p>I have 1612 sentences/sequences and 30 classes so the label is not unique and many documents share the same labels.</p>

<p>So when I first tried doc2vec, it gave my just 30 vectors which is the number of unique labels. Then I decided to have multiple tags to get a vector for each sentence.</p>

<p>When I did this I ended up having more vectors than my sentences. Any explanations what might have gone wrong?</p>

<p><a href=""https://i.sstatic.net/0c734.jpg"" rel=""nofollow noreferrer"">Screenshot of my data</a></p>

<p><a href=""https://i.sstatic.net/MVLN3.jpg"" rel=""nofollow noreferrer"">Screenshot of corpus</a></p>

<p><code>tagged = data.apply(lambda r: TaggedDocument(words=(r[""A""]), tags=[r.label,r.id]), axis=1)</code></p>

<p><code>print(len(tagged))</code></p>

<p><code>1612</code></p>

<p><code>sents = tagged.values</code></p>

<p><code>model = Doc2Vec(sents, size=5, window=5, iter=20, min_count = 0)</code></p>

<p><code>sents.shape</code></p>

<p><code>(1612,)</code></p>

<p><code>model.docvecs.vectors_docs.shape</code></p>

<p><code>(1643,5)</code></p>

<p><a href=""https://i.sstatic.net/0c734.jpg"" rel=""nofollow noreferrer"">Screenshot of my data</a></p>
","python, tags, gensim, doc2vec","<p>The number of tags a <code>Doc2Vec</code> model will learn is equal to the number of unique tags you've provided. You provided 1612 different <code>r.id</code> values, and 30 different <code>r.label</code> values, hence a total number of tags larger than just your document count. </p>

<p>(I suspect your <code>r.id</code> values are plain integers, but start at 1. If you use plain integers, rather than strings, as tags, then <code>Doc2Vec</code> will use those ints as the indexes into its internal vector-array directly. And thus int indexes that are less than the numbers you use, like 0, will also be allocated. Hence, your count of 1612 + 30 + 1 total known tags, because it also allocated space for tag 0.)</p>

<p>So, that explains your tag count, and there's nothing necessarily wrong. Beware however:</p>

<ul>
<li><p>Your dataset is very small: most published work uses 10s-of-thousands to millions of documents. You can sometimes still eke out useful vectors by using smaller vectors or more training epochs, but mainly <code>Doc2Vec</code> and similar algorithms need more data to work best. (Still: a vector <code>size=5</code> is quite tiny!)</p></li>
<li><p>With small data, especially, the simple PV-DBOW mode (<code>dm=0</code>) is often a fast-training top-performer. (But note: it doesn't train word-vectors using context windows unless you add <code>dbow_words=1</code> option, which then again slows it down with that extra word-vector training.)</p></li>
<li><p>Whether you should be using the labels as document-tags at all is not certain - the classic use of <code>Doc2Vec</code> just gives each doc a unique ID – then lets downstream steps learn the relations to other things. Mixing in known other document-level labels can sometimes help, or hurt, depending on your data and ultimate goals. (More tags can, to an extent, ""dilute"" whatever is learned over a larger model.)</p></li>
<li><p>At least in natural language, retaining words that only appear once or a few times can often be harmful to overall vector-quality. There's too few occurrences to model them well, and since there will, by Zipf's law, be many such words, they can wind up interfering a lot wit the training of other entitites. So a default <code>min_count=5</code> (or even higher with larger datasets) often helps overall quality, and you shouldn't assume that simply retaining more data, with <code>min_count=0</code>, necessarily helps.</p></li>
</ul>
",2,2,858,2019-02-04 17:28:50,https://stackoverflow.com/questions/54521323/i-get-more-vectors-than-my-documents-size-gensim-doc2vec
Layer size in gensim&#39;s word2vec,"<p>When I start training my word2vec model, I am presented with the warning</p>

<blockquote>
  <p>consider setting layer size to a multiple of 4 for greater performance</p>
</blockquote>

<p>That sounds neat, but I can't find any reference to a <code>layer</code> argument or similar in <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">the documentation</a>. </p>

<p>So how can I increase the layer size, and how can I determine a good value?</p>
","python, python-3.x, nlp, gensim, word2vec","<p>The layer size simply means the size (dimension) of the word vectors which can be set with the <code>size</code> parameter. The default value is 100 but you can for example try with 128 to have a multiple of 4. The best size depends on your training data and has to be determined empirically. In general, more data means that you can go for a bigger size.</p>
",7,3,1887,2019-02-06 17:44:35,https://stackoverflow.com/questions/54559615/layer-size-in-gensims-word2vec
Understanding gensim word2vec&#39;s most_similar,"<p>I am unsure how I should use the most_similar method of gensim's Word2Vec. Let's say you want to test the tried-and-true example of: <em>man stands to king as woman stands to X</em>; find X. I thought that is what you could do with this method, but from the results I am getting I don't think that is true.</p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar"" rel=""nofollow noreferrer"">The documentation</a> reads:</p>

<blockquote>
  <p>Find the top-N most similar words. Positive words contribute
  positively towards the similarity, negative words negatively.</p>
  
  <p>This method computes cosine similarity between a simple mean of the
  projection weight vectors of the given words and the vectors for each
  word in the model. The method corresponds to the word-analogy and
  distance scripts in the original word2vec implementation.</p>
</blockquote>

<p>I assume, then, that <code>most_similar</code> takes the positive examples and negative examples, and tries to find points in the vector space that are as close as possible to the positive vectors and as far away as possible from the negative ones. Is that correct?</p>

<p>Additionally, is there a method that allows us to map the relation between two points to another point and get the result (cf. the man-king woman-X example)?</p>
","python, python-3.x, nlp, gensim, word2vec","<p>You can view exactly what <code>most_similar()</code> does in its source code:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#L485"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#L485</a></p>
<p>It's not quite &quot;find points in the vector space that are as close as possible to the positive vectors and as far away as possible from the negative ones&quot;. Rather, as described in the original word2vec papers, it performs vector arithmetic: adding the positive vectors, subtracting the negative, then from that resulting position, listing the known-vectors closest to that angle.</p>
<p>That is sufficient to solve <code>man : king :: woman :: ?</code>-style analogies, via a call like:</p>
<pre><code>sims = wordvecs.most_similar(positive=['king', 'woman'], 
                             negative=['man'])
</code></pre>
<p>(You can think of this as, &quot;start at 'king'-vector, add 'woman'-vector, subtract 'man'-vector, from where you wind up, report ranked word-vectors closest to that point (while leaving out any of the 3 query vectors).&quot;)</p>
",8,3,14132,2019-02-07 18:48:10,https://stackoverflow.com/questions/54580260/understanding-gensim-word2vecs-most-similar
What is the similarity score in the gensim similar_by_word function?,"<p>What is the similarity score in the genism similar_by_word function?</p>

<p>I was reading here about the genism similar_by_word function:
<a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html</a></p>

<p>The similar_by_word function returns a sequence of (word, similarity). What is the definition by similarity here and how is it calculated?</p>
",gensim,"<p>The similarity measure used here is the <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow noreferrer"">cosine similarity</a>, which takes values between -1 and 1. The cosine similarity measures the (cosine of) the angle between two vectors. If the angle is very small the vectors are considered similar since they are pointing in the same direction. This way of measuring similarity is common when working with high dimensional vector spaces such as word embeddings.</p>

<p>The formula for the cosine similarity of two vectors <code>A</code> and <code>B</code> is as follows:</p>

<p><a href=""https://i.sstatic.net/wzS83.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wzS83.png"" alt=""cosine similarity formula""></a></p>
",1,0,664,2019-02-11 04:08:38,https://stackoverflow.com/questions/54623849/what-is-the-similarity-score-in-the-gensim-similar-by-word-function
How to use word2vec2tensor in gensim?,"<p>I am following the following gensim tutorial to transform my word2vec model to tensor.
Link to the tutorial: <a href=""https://radimrehurek.com/gensim/scripts/word2vec2tensor.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/scripts/word2vec2tensor.html</a></p>

<p>More specifically, I ran the following command</p>

<pre><code>python -m gensim.scripts.word2vec2tensor -i C:\Users\Emi\Desktop\word2vec\model_name -o C:\Users\Emi\Desktop\word2vec
</code></pre>

<p>However, I get the following error for the above command.</p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>

<p>When I use <code>model.wv.save_word2vec_format(model_name)</code> to save my model (as mentioned in the following link: <a href=""https://github.com/RaRe-Technologies/gensim/issues/1847"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/1847</a>) and then use the above command I get the following error.</p>

<pre><code>ValueError: invalid vector on line 1 (is this really the text format?)
</code></pre>

<p>Just wondering if I have made any mistakes in the syntax of the commads. Please let me know how to resolve this issue.</p>

<p>I am happy to provide more details if needed.</p>
","python, gensim, word2vec","<p>I was able to solve the issue by using the following code:</p>

<pre><code>model = gensim.models.keyedvectors.KeyedVectors.load(file_name)

max_size = len(model.wv.vocab)-1
w2v = np.zeros((max_size,model.layer1_size))

if not os.path.exists('projections'):
    os.makedirs('projections')

with open(""projections/metadata.tsv"", 'w+') as file_metadata:

    for i, word in enumerate(model.wv.index2word[:max_size]):

        #store the embeddings of the word
        w2v[i] = model.wv[word]

        #write the word to a file 
        file_metadata.write(word + '\n')

sess = tf.InteractiveSession()
with tf.device(""/cpu:0""):
    embedding = tf.Variable(w2v, trainable=False, name='embedding')
tf.global_variables_initializer().run()
saver = tf.train.Saver()
writer = tf.summary.FileWriter('projections', sess.graph)
config = projector.ProjectorConfig()
embed= config.embeddings.add()
embed.tensor_name = 'embedding'
embed.metadata_path = 'metadata.tsv'
projector.visualize_embeddings(writer, config)
saver.save(sess, 'projections/model.ckpt', global_step=max_size)
</code></pre>
",2,5,1284,2019-02-11 04:31:22,https://stackoverflow.com/questions/54623993/how-to-use-word2vec2tensor-in-gensim
How can I find and print unmatched/dissimilar words from the documents?,"<p>I am trying to rewrite algorithm that basically takes a input text file and compares with different documents and results the similarities.</p>

<p>Now I want to print output of unmatched words and output a new textile with unmatched words.</p>

<p>From this code, ""hello force"" is the input and is checked against the raw_documents and prints out rank for matched document between 0-1(word ""force"" is matched with second document and ouput gives more rank to second document but ""hello"" is not in any raw_document i want to print unmatched word ""hello"" as not matched ), But what i want is to print unmatched input word that was not matched with any of the raw_document</p>

<pre><code>import gensim
import nltk

from nltk.tokenize import word_tokenize

raw_documents = [""I'm taking the show on the road"",
                 ""My socks are a force multiplier."",
             ""I am the barber who cuts everyone's hair who doesn't 
cut their own."",
             ""Legend has it that the mind is a mad monkey."",
            ""I make my own fun.""]

gen_docs = [[w.lower() for w in word_tokenize(text)]
            for text in raw_documents]

dictionary = gensim.corpora.Dictionary(gen_docs)

corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]

tf_idf = gensim.models.TfidfModel(corpus)
s = 0
for i in corpus:
    s += len(i)
sims =gensim.similarities.Similarity('/usr/workdir/',tf_idf[corpus],
                                  num_features=len(dictionary))
query_doc = [w.lower() for w in word_tokenize(""hello force"")]

query_doc_bow = dictionary.doc2bow(query_doc)

query_doc_tf_idf = tf_idf[query_doc_bow]
result = sims[query_doc_tf_idf] 
print result
</code></pre>
","python, scikit-learn, nltk, gensim","<p>If you just want to know that the word '<code>hello'</code> is not in a some other documents, that may not even require a natural-language helper library like <code>gensim</code>. You could insead just keep a record of all seen words – and for that a plain Python <code>dict</code> or <code>set</code> or <code>Counter</code> should be sufficient. (After loading it with all words, just check each word of your new text against it in turn.)</p>

<p>Gensim's <code>TfidfModel</code> and the <code>Similarity</code> comparisons actually. deals with more subtle relative-degrees-of-comparison (not the ""yes/no"" of a word's presence). </p>

<p>And, the <code>gensim.corpora.Dictionary.doc2bow()</code> method will usually <em>ignore</em> unknown-to-the-Dictionary words – because they don't have assigned slots, and are thus perhaps rare/unimportant – rather than include them in the returned data. So, the ""bag of words"" representations it returns by default, which are essentially lists of <code>(known_word_index, count)</code> can't help for simple not-yet-seen detection of not-yet-known words. </p>

<p>However, you could look at its optional <code>return_missing</code> parameter, and request <code>return_missing=True</code>. Then, it returns a tuple of (bag_of_words, dict_of_missing_words) - and by looking at that second returned value, see which words <em>weren't</em> already in the <code>gensim.corpora.Dictionary</code> object. See:</p>

<p><a href=""https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow</a></p>
",0,0,259,2019-02-11 09:05:05,https://stackoverflow.com/questions/54627037/how-can-i-find-and-print-unmatched-dissimilar-words-from-the-documents
Expected input to torch Embedding layer with pre_trained vectors from gensim,"<p>I would like to use pre-trained embeddings in my neural network architecture. The pre-trained embeddings are trained by gensim. I found <a href=""https://stackoverflow.com/a/49802495/1150683"">this informative answer</a> which indicates that we can load pre_trained models like so:</p>

<pre><code>import gensim
from torch import nn

model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')
weights = torch.FloatTensor(model.vectors)
emb = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors))
</code></pre>

<p>This seems to work correctly, also on 1.0.1. My question is, that I don't quite understand what I have to feed into such a layer to utilise it. Can I just feed the tokens (segmented sentence)? Do I need a mapping, for instance token-to-index? </p>

<p>I found that you can access a token's vector simply by something like</p>

<pre><code>print(weights['the'])
# [-1.1206588e+00  1.1578362e+00  2.8765252e-01 -1.1759659e+00 ... ]
</code></pre>

<p>What does that mean for an RNN architecture? Can we simply load in the tokens of the batch sequences? For instance:</p>

<pre><code>for seq_batch, y in batch_loader():
    # seq_batch is a batch of sequences (tokenized sentences)
    # e.g. [['i', 'like', 'cookies'],['it', 'is', 'raining'],['who', 'are', 'you']]
    output, hidden = model(seq_batch, hidden)
</code></pre>

<p>This does not seem to work so I am assuming you need to convert the tokens to its index in the final word2vec model. Is that true? I found that you can get the indices of words by using the word2vec model's <code>vocab</code>:</p>

<pre><code>weights.vocab['world'].index
# 147
</code></pre>

<p>So as an input to an Embedding layer, should I provide a tensor of <code>int</code> for a sequence of sentences that consist of a sequence of words? Example use with dummy dataloader (cf. example above) and dummy RNN welcome.</p>
","vector, pytorch, gensim, word2vec, recurrent-neural-network","<p>The <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding"" rel=""nofollow noreferrer"">documentation</a> says the following</p>

<blockquote>
  <p>This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.</p>
</blockquote>

<p>So if you want to feed in a sentence, you give a <code>LongTensor of</code> indices, each corresponding to a word in the vocabulary, which the <code>nn.Embedding</code> layer will map into word vectors going forward. </p>

<p>Here's an illustration</p>

<pre class=""lang-py prettyprint-override""><code>test_voc = [""ok"", ""great"", ""test""]
# The word vectors for ""ok"", ""great"" and ""test""
# are at indices, 0, 1 and 2, respectively.

my_embedding = torch.rand(3, 50)
e = nn.Embedding.from_pretrained(my_embedding)

# LongTensor of indicies corresponds to a sentence,
# reshaped to (1, 3) because batch size is 1
my_sentence = torch.tensor([0, 2, 1]).view(1, -1)

res = e(my_sentence)
print(res.shape)
# =&gt; torch.Size([1, 3, 50])
# 1 is the batch dimension, and there's three vectors of length 50 each
</code></pre>

<p>In terms of RNNs, next you can feed that tensor into your RNN module, e.g</p>

<pre class=""lang-py prettyprint-override""><code>lstm = nn.LSTM(input_size=50, hidden_size=5, batch_first=True)
output, h = lstm(res)
print(output.shape)
# =&gt; torch.Size([1, 3, 5])
</code></pre>

<p>I also recommend you look into <a href=""https://github.com/pytorch/text"" rel=""nofollow noreferrer"">torchtext</a>. It can automatate some of the stuff you will have to do manually otherwise.</p>
",1,4,2841,2019-02-12 17:28:57,https://stackoverflow.com/questions/54655604/expected-input-to-torch-embedding-layer-with-pre-trained-vectors-from-gensim
Is there a way to remove a word from a KeyedVectors vocab?,"<p>I need to remove an invalid word from the vocab of a ""gensim.models.keyedvectors.Word2VecKeyedVectors"". </p>

<p>I tried to remove it using <code>del model.vocab[word]</code>, if I print the <code>model.vocab</code> the word disappeared, but when I run <code>model.most_similar</code> using other words the word that I deleted is still appearing as similar. 
So how can I delete a word from <code>model.vocab</code> in a way that affect the <code>model.most_similar</code> to not bring it?</p>
","gensim, word2vec, embedding, glove","<p>There's no existing method supporting the removal of individual words. </p>

<p>A quick-and-dirty workaround might be to, at the same time as removing the <code>vocab</code> entry, noting the <code>index</code> of the existing vector (in the underlying large vector array), and also changing the string in the <code>kv_model.index2entity</code> list at that index to some plug value (like say, <code>'***DELETED***'</code>). </p>

<p>Then, after performing any <code>most_similar()</code>, discard any entries matching <code>'***DELETED***'</code>. </p>
",2,2,1205,2019-02-14 19:26:53,https://stackoverflow.com/questions/54697748/is-there-a-way-to-remove-a-word-from-a-keyedvectors-vocab
How to handle words that are not in word2vec&#39;s vocab optimally,"<p>I have a list of ~10 million sentences, where each of them contains up to 70 words.</p>

<p>I'm running gensim word2vec on every word, and then taking the simple average of each sentence. The problem is that I use min_count=1000, so a lot of words are not in the vocab. </p>

<p>To solve that, I intersect the vocab array (that contains about 10000 words) with every sentence, and if there's at least one element left in that intersection, it returns its the simple average, otherwise, it returns a vector of zeros.</p>

<p>The issue is that calculating every average takes a very long time when I run it on the whole dataset, even when splitting into multiple threads, and I would like to get a better solution that could run faster.</p>

<p>I'm running this on an EC2 r4.4xlarge instance.</p>

<p>I already tried switching to doc2vec, which was way faster, but the results were not as good as word2vec's simple average.</p>

<pre><code>word2vec_aug_32x = Word2Vec(sentences=sentences, 
                        min_count=1000, 
                        size=32, 
                        window=2,
                        workers=16, 
                        sg=0)

vocab_arr = np.array(list(word2vec_aug_32x.wv.vocab.keys()))

def get_embedded_average(sentence):
    sentence = np.intersect1d(sentence, vocab_arr)
    if sentence.shape[0] &gt; 0:
        return np.mean(word2vec_aug_32x[sentence], axis=0).tolist()
    else:
        return np.zeros(32).tolist()

pool = multiprocessing.Pool(processes=16)

w2v_averages = np.asarray(pool.map(get_embedded_average, np.asarray(sentences)))
pool.close()
</code></pre>

<p>If you have any suggestions of different algorithms or techniques that have the same purpose of sentence embedding and could solve my problem, I would love to read about it.</p>
","python, numpy, optimization, gensim, word2vec","<p>You could use <a href=""https://github.com/facebookresearch/fastText"" rel=""noreferrer"">FastText</a> instead of Word2Vec. FastText is able to embed out-of-vocabulary words by looking at subword information (character ngrams). Gensim also has a FastText implementation, which is very easy to use:</p>

<pre><code>from gensim.models import FastText

model = FastText(sentences=training_data, size=128, ...)

word = 'hello' # can be out of vocabulary
embedding = model[word] # fetches the word embedding
</code></pre>
",10,5,10102,2019-02-15 12:15:36,https://stackoverflow.com/questions/54709178/how-to-handle-words-that-are-not-in-word2vecs-vocab-optimally
Doc2Vec: get text of the label,"<p>I've trained <code>Doc2Vec</code> model I'm trying to get predictions.</p>

<p>I use</p>

<pre><code>test_data = word_tokenize(""Филип Моррис Продактс С.А."".lower())
model = Doc2Vec.load(model_path)
v1 = model.infer_vector(test_data)
sims = model.docvecs.most_similar([v1])
print(sims)
</code></pre>

<p>returns</p>

<pre><code>[('624319', 0.7534812092781067), ('566511', 0.7333904504776001), ('517382', 0.7264763116836548), ('523368', 0.7254455089569092), ('494248', 0.7212602496147156), ('382920', 0.7092794179916382), ('530910', 0.7086726427078247), ('513421', 0.6893941760063171), ('196931', 0.6776881814002991), ('196947', 0.6705600023269653)]
</code></pre>

<p>Next I've tried to know, what's text of this number</p>

<pre><code>model.docvecs['624319']
</code></pre>

<p>But it returns me only the vector representation</p>

<pre><code>array([ 0.36298314, -0.8048847 , -1.4890883 , -0.3737898 , -0.00292279,
   -0.6606688 , -0.12611026, -0.14547637,  0.78830665,  0.6172428 ,
   -0.04928801,  0.36754376, -0.54034036,  0.04631123,  0.24066721,
    0.22503968,  0.02870891,  0.28329515,  0.05591608,  0.00457001],
  dtype=float32)
</code></pre>

<p>So, is any way to get text of this label from the model?
Loading train dataset takes a lot of time, so I try to find out another way.</p>
","python, gensim, doc2vec","<p>There is no way to convert a doc vector directly back into the original text (the information about word ordering, etc is lost in the process of reduction of text --> vectors).</p>

<p>However, you <em>can</em> retrieve the original text by tagging each document with its index in your corpus list when you are creating your <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument"" rel=""noreferrer""><code>TaggedDocument</code></a>s for <code>Doc2Vec()</code>. Let's say you had a corpus of sentences/documents that are contained in a list called <code>texts</code>. Use <a href=""https://docs.python.org/3.7/library/functions.html#enumerate"" rel=""noreferrer""><code>enumerate()</code></a> like this to generate a unique index <code>i</code> for each sentence, and pass that as the <code>tags</code> argument for <code>TaggedDocument</code>:</p>

<pre><code>tagged_data = []
for i, t in enumerate(texts):
    tagged_data.append(TaggedDocument(words=word_tokenize(c.lower()), tags=[str(i)]))

model = Doc2Vec(vector_size=VEC_SIZE,
                window=WINDOW_SIZE,
                min_count=MIN_COUNT,
                workers=NUM_WORKERS)

model.build_vocab(tagged_data)
</code></pre>

<p>Then after training, when you get the results from <code>model.docvecs.most_similar()</code>, the first number in each tuple will be the index into your original list of corpus texts. So for example, if you run <code>model.docvecs.most_similar([some_vector])</code> and get:</p>

<p><code>[('624319', 0.7534812092781067), ('566511', 0.7333904504776001), ('517382', 0.7264763116836548), ('523368', 0.7254455089569092), ('494248', 0.7212602496147156), ('382920', 0.7092794179916382), ('530910', 0.7086726427078247), ('513421', 0.6893941760063171), ('196931', 0.6776881814002991), ('196947', 0.6705600023269653)]</code></p>

<p>... then you could retrieve the original document for the first result<code>('624319', 0.7534812092781067)</code> by indexing into your initial corpus list with: <code>texts[624319]</code>.</p>

<p>Or if you wanted to loop through and get all of the most similar texts, you could do something like:</p>

<pre><code>most_similar_docs = []
for d in model.docvecs.most_similar([some_vector]):
    most_similar_docs.append(texts[d[0]])
</code></pre>
",8,2,2149,2019-02-17 19:29:56,https://stackoverflow.com/questions/54736839/doc2vec-get-text-of-the-label
How to load numpy array to gensim Keyedvector format?,"<p>After I trained word embeddings, I saved it as npz format.
While I am trying to load it as KeyedVectors format, it makes errors.
How can I load numpy array as gensim.KeyedVectors format?
I really need it because I need to use functions like most_similar() not just vector values.</p>

<p>in model.py with tensorflow,</p>

<pre><code>self.verb_embeddings = tf.Variable(np.load(cfg.pretrained_target)[""embeddings""],
                                               name=""verb_embeddings"",
                                               dtype=tf.float32,
                                               trainable=cfg.tune_emb)
</code></pre>

<p>in saving.py</p>

<pre><code>target_emb = sess.run(model.verb_embeddings)
np.savez_compressed(""trained_target_emb.npz"", embeddings=target_emb)
</code></pre>

<p>in main.py</p>

<pre><code> model = KeyedVectors.load('trained_target_emb.npz')
</code></pre>

<p>I got</p>

<pre><code>_pickle.UnpicklingError: A load persistent id instruction was encountered, but no persistent_load function was specified.
</code></pre>

<p>also tried</p>

<pre><code> model = KeyedVectors.load_word2vec_format('trained_target_emb.npz')
</code></pre>

<p>but got</p>

<pre><code> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xde in position 14: invalid continuation byte
</code></pre>
","python, numpy, tensorflow, gensim, embedding","<p>Gensim <code>KeyedVectors</code> instances can't be loaded from a mere raw array: there's no information about which words are represented, and which indexes hold which words. </p>

<p>The plain <code>.load()</code> in gensim expects objects that were saved from gensim, using gensim's own <code>.save()</code> method.</p>

<p>Word vectors can be loaded from files that are in the same format as was used by the original Google/Mikolov <code>word2vec.c</code> tool. So perhaps your tensorflow code can save them that way?</p>

<p>Then, you'd use <code>.load_word2vec_format()</code>.</p>
",0,0,2265,2019-02-23 07:09:09,https://stackoverflow.com/questions/54839158/how-to-load-numpy-array-to-gensim-keyedvector-format
Exception during calling gensim?,"<p>I tried to load gensim in my code. Often it works fine. Today, I get the following exception:</p>

<pre><code>Traceback (most recent call last):
  File ""/project/6008168/tamouze/just.py"", line 2, in &lt;module&gt;
    import gensim
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/__init__.py"", line 5, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, summarization, utils  # noqa:F401
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/parsing/__init__.py"", line 4, in &lt;module&gt;
    from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/parsing/preprocessing.py"", line 40, in &lt;module&gt;
    from gensim import utils
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/gensim/utils.py"", line 44, in &lt;module&gt;
    from smart_open import smart_open
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/smart_open/__init__.py"", line 1, in &lt;module&gt;
    from .smart_open_lib import *
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/smart_open/smart_open_lib.py"", line 29, in &lt;module&gt;
    import requests
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/requests/__init__.py"", line 97, in &lt;module&gt;
    from . import utils
  File ""/project/6008168/tamouze/Python_directory/ENV2.7_new/lib/python2.7/site-packages/requests/utils.py"", line 26, in &lt;module&gt;
    from ._internal_utils import to_native_string
ImportError: cannot import name to_native_string
</code></pre>

<p>Im using python 2.7.14 and gensim 3.4.0.
How can I solve this problem?</p>
","python, gensim","<p>The error isn't really occurring in <code>gensim</code>, even though that's how you found it. If you look at the stack, it's only triggered because <code>gensim</code> uses <code>smart_open</code> which in turn uses <code>requests</code>. It is in <code>requests</code> that the error happens. </p>

<p>If this was working, but now stopped, something likely changed in your environment, or how you're launching this code, related to the relationship between Python and the <code>requests</code> package. </p>

<p>For such errors, you should try searching Google for the final-two lines of your error stack – those most connected to the problem. Those are:</p>

<pre><code>    from ._internal_utils import to_native_string
ImportError: cannot import name to_native_string
</code></pre>

<p>(These leave out the file path that's specific to you, but have a number of unique tokens likely to have also been reported by any others.)</p>

<p>A number of people have hit this, from a variety of other projects, but always triggered through <code>requests</code>. Some have reported re-installing <code>requests</code> (perhaps to ensure it's version 2.0.0 or later) helps. </p>

<p>If a simple re-install doesn't help, you could also try one or all of:</p>

<ul>
<li>uninstall, verify it's not present at all (that <code>requests</code> itself isn't found), then install – this could make sure you don't have overlapping redundant installations in different places that are both confusing the issue</li>
<li>start from a fresh Python environment, reinstalling all packages</li>
<li>double-check that all packages share the same Python2/Python3 compatibility</li>
</ul>
",1,0,687,2019-02-25 16:03:52,https://stackoverflow.com/questions/54870236/exception-during-calling-gensim
gensim word2vec print log loss,"<p>how to print to log (file or stout) the loss of each epoch in the training phase, when using gensim word2vec model. </p>

<p>I tried :</p>

<pre><code> logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
 logging.root.setLevel(level=logging.INFO)
</code></pre>

<p>But I didn't saw any loss printing.</p>
","python, gensim, word2vec","<p>You can get the latest training loss of a word2vec model with the method <code>get_latest_training_loss()</code>. If you want to print the loss after every epoch you can add a callback that does this. For example:</p>

<pre><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec
from gensim.models.callbacks import CallbackAny2Vec

class callback(CallbackAny2Vec):
    '''Callback to print loss after each epoch.'''

    def __init__(self):
        self.epoch = 0

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        print('Loss after epoch {}: {}'.format(self.epoch, loss))
        self.epoch += 1

model = Word2Vec(common_texts, size=100, window=5, min_count=1, 
                 compute_loss=True, callbacks=[callback()])
</code></pre>

<p>However, the loss is computed in a cumulative way (i.e. the loss that gets printed after each epoch is the total loss of all epochs so far). See <a href=""https://stackoverflow.com/a/52067942/10921263"">gojomo's answer here</a> for more explanation.</p>
",13,6,10112,2019-02-26 15:05:38,https://stackoverflow.com/questions/54888490/gensim-word2vec-print-log-loss
python gensim: AttributeError: &#39;list&#39; object has no attribute,"<p>I have a small python pipeline. One class cleans and lemmatizes the data.
It returns a List of Lists of Strings (i.e., <code>List[List[str]]</code>). I then pass the list on to another class which passes the data to gensim dictionary</p>

<p>The following code, however, throws this exception:</p>

<pre><code>dictionary = corpora.Dictionary(self.bowlist)
AttributeError: 'list' object has no attribute 'bowlist'
</code></pre>

<p>Code:</p>

<pre><code>from typing import List
import re
from gensim import corpora

class ListOfListsToGensimCorpora:
    def __init__(self, bow_list: List[List[str]]):
        self.bowlist = bow_list

    def perform(self):
        dictionary = corpora.Dictionary(self.bowlist)
        print(dictionary)
</code></pre>

<p>I am new to Python but I have checked through debug and other methods, bowlist is a List[List[str]].</p>
","python, gensim","<p>are you using like that?</p>

<pre><code>ListOfListsToGensimCorpora.perform(bow_list)
</code></pre>

<p>it is fine with this:</p>

<pre><code>l = ListOfListsToGensimCorpora(bow_list)
l.perform()
</code></pre>

<p>or change you code by this?</p>

<pre><code>from typing import List
import re
from gensim import corpora

class ListOfListsToGensimCorpora:
    def __init__(self, bow_list: List[List[str]]):
        self.bowlist = bow_list
        self.perform()  # run perform when instantiuation 

    def perform(self):
        dictionary = corpora.Dictionary(self.bowlist)
        print(dictionary)
</code></pre>
",2,0,2424,2019-02-28 01:53:58,https://stackoverflow.com/questions/54917218/python-gensim-attributeerror-list-object-has-no-attribute
Word2Vec vocab results in just letters and symbols,"<p>I'm new to Word2Vec and I am trying to cluster words based on their similarity.  To start I am using nltk to separate the sentences and then using the resulting list of sentences as the input into Word2Vec.  However, when I print the vocab, it is just a bunch of letters, numbers and symbols rather than words. To be specific, an example of one of the letters is ""&lt; gensim.models.keyedvectors.Vocab object at 0x00000238145AB438>, 'L':""</p>

<pre><code># imports needed and logging
import gensim
from gensim.models import word2vec
import logging

import nltk
#nltk.download('punkt')
#nltk.download('averaged_perceptron_tagger')
with open('C:\\Users\\Freddy\\Desktop\\Thesis\\Descriptions.txt','r') as f_open:
    text = f_open.read()
arr = []

sentences = nltk.sent_tokenize(text) # this gives a list of sentences

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)

model = word2vec.Word2Vec(sentences, size = 300)

print(model.wv.vocab)
</code></pre>
","python, python-3.x, tokenize, gensim, word2vec","<p>As the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">tutorial</a> and the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""noreferrer"">documentation</a> for <code>Word2Vec</code> class suggests the constructor of the class requires list of lists of words as the first parameter (or iterator of iterators of words in general):</p>

<blockquote>
  <p><strong>sentences</strong> (iterable of iterables, optional) – The sentences iterable can be simply a list of lists of tokens, but for larger
  corpora,...</p>
</blockquote>

<p>I believe before feeding in <code>sentences</code> into <code>Word2Vec</code> you need to use <code>words_tokenize</code> on each of the sentences changing the crucial line to:</p>

<pre><code>sentences = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text)]
</code></pre>

<p><strong>TL;DR</strong></p>

<p>You get letters as your ""words"" because <code>Word2Vec</code> treats strings corresponding to sentences as iterables containing words. Iterating over strings results in the sequence of letters. These letters are used as the basis for the model learning (instead of intended words).</p>

<p>As the ancient saying goes: <em>trash in - trash out</em>.</p>
",5,2,2751,2019-02-28 16:02:09,https://stackoverflow.com/questions/54929726/word2vec-vocab-results-in-just-letters-and-symbols
Word2Vec time complexity,"<p>I have googled this issue but I cannot find any reliable solution (some sources gives log(V) some log(V/2). But what is the time complexity of the word2vec model with the following parameters:</p>

<p><code>Word2Vec(corpus, size=4000, window=30, min_count=1, workers=50, iter=100, alpha=0.0001)</code></p>

<p>I have a vocabulary that equals to 10000 words (unique words). </p>
","python, time-complexity, big-o, gensim, word2vec","<p>Without a formal analysis/proof, in practice and in the default 'negative sampling' case, execution time is chiefly determined by the size of the corpus, and grows roughly linearly with the size of the corpus. The number of unique words (vocabulary size V) isn't a major factor.</p>
<p>Gensim's implementation uses a binary-search over a vocabulary-sized array to achieve the sampling of negative examples, so its time complexity might technically be:</p>
<pre><code>O(N * log(V))
</code></pre>
<ul>
<li>where N is the total corpus size and</li>
<li>V is the unique-words vocabulary count.</li>
</ul>
<p>But this particular <em>O(log(V))</em> operation is, in practice, often faster than the memory-hungry O(1) sampling lookup used by the original Google/Mikolov word2vec.c – probably due to improved CPU cache efficiency.</p>
<p>So with the defaults:</p>
<ul>
<li>If one corpus is twice as long, in words, than another, then it will take about twice as long to train the model on the larger corpus.</li>
<li>But if one corpus is the same size, in words, as another, but with a vocabulary twice as big, you probably won't notice much change in runtime.</li>
</ul>
<p>In the non-default <code>hierarchical softmax</code> case – <code>hs=1, negative=0</code> – words are encoded differently, and have longer encodings as the size of the vocabulary grows, and this increases the average number of training operations per corpus word – by a factor of <em>log(V)</em>, I believe, so we again technically have a *<em>O(N * log(V))</em> time-complexity.</p>
<p>But, this vocabulary-driven increase tends to be more significant, in practice, than the one inside negative-sampling's binary-search-based sampling.</p>
<p>So if you have two corpuses of the same length, but one has twice the number of unique words, you very well may notice a longer runtime, in hierarchical-softmax mode.</p>
",6,4,2413,2019-03-01 18:39:02,https://stackoverflow.com/questions/54950481/word2vec-time-complexity
Facebook fasttext bin model UnicodeDecodeError,"<p>I downloaded pretrained word vector file (.bin) from facebook (<a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a>)
However, when I tried to use this model it happens to make error.</p>

<pre><code>from gensim.models import FastText
fasttext_model = FastText.load_fasttext_format('cc.ko.300.bin', encoding='utf8')

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte
</code></pre>

<p>But weird thing is that it operates well when I use old version bin file (<a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/pretrained-vectors.html</a>)</p>

<p>What is wrong with these files?? And how can I fix it??</p>

<p>And I must use bin file because I need all n-grams to prevent OOV. So, solutions like 'use .vec file' couldn't be any help.</p>

<p>Thank you so much :)</p>
","python, facebook, utf-8, gensim, fasttext","<p>Make sure you're using the latest (3.7.1) version of gensim; there have been recent fixes &amp; improvements to <code>load_fasttext_model()</code>. </p>

<p>Also, double-check your download of <code>cc.ko.300.bin</code>, to be sure it hasn't bee corrupted or truncated. </p>

<p>If neither of these help, try enabling logging at the INFO level, try the load again, and share the full output and error stack inside your question to give more hints about where things are going wrong. </p>
",0,0,2007,2019-03-06 06:07:27,https://stackoverflow.com/questions/55016629/facebook-fasttext-bin-model-unicodedecodeerror
Why the FastText word embedding could generate the representation of a word from another language?,"<p>Recently, I trained a FastText word embedding from <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">sentiment140</a> to get the representation for English words. However, today just for a trial, I run the FastText module on a couple of Chinese words, for instance:</p>

<pre><code>import gensim.models as gs

path = r'\data\word2vec'

w2v = gs.FastText.load(os.path.join(path, 'fasttext_model'))

w2v.wv['哈哈哈哈']
</code></pre>

<p>It outputs:</p>

<pre><code>array([ 0.00303676,  0.02088235, -0.00815559,  0.00484574, -0.03576371,
       -0.02178247, -0.05090654,  0.03063928, -0.05999983,  0.04547168,
       -0.01778449, -0.02716631, -0.03326027, -0.00078981,  0.0168153 ,
        0.00773436,  0.01966593, -0.00756055,  0.02175765, -0.0050137 ,
        0.00241255, -0.03810823, -0.03386266,  0.01231019, -0.00621936,
       -0.00252419,  0.02280569,  0.00992453,  0.02770403,  0.00233192,
        0.0008545 , -0.01462698,  0.00454278,  0.0381292 , -0.02945416,
       -0.00305543, -0.00690968,  0.00144188,  0.00424266,  0.00391074,
        0.01969502,  0.02517333,  0.00875261,  0.02937791,  0.03234404,
       -0.01116276, -0.00362578,  0.00483239, -0.02257918,  0.00123061,
        0.00324584,  0.00432153,  0.01332884,  0.03186348, -0.04119627,
        0.01329033,  0.01382102, -0.01637722,  0.01464139,  0.02203292,
        0.0312229 ,  0.00636201, -0.00044287, -0.00489291,  0.0210293 ,
       -0.00379244, -0.01577058,  0.02185207,  0.02576622, -0.0054543 ,
       -0.03115215, -0.00337738, -0.01589811, -0.01608399, -0.0141606 ,
        0.0508234 ,  0.00775024,  0.00352813,  0.00573649, -0.02131752,
        0.01166397,  0.00940598,  0.04075769, -0.04704212,  0.0101376 ,
        0.01208556,  0.00402935,  0.0093914 ,  0.00136144,  0.03284211,
        0.01000613, -0.00563702,  0.00847146,  0.03236216, -0.01626745,
        0.04095127,  0.02858841,  0.0248084 ,  0.00455458,  0.01467448],
      dtype=float32)
</code></pre>

<p>Hence, I really want to know why the FastText module trained from <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">sentiment140</a> could do this. Thank you!</p>
","python, gensim, word-embedding, fasttext, nlp","<p>In fact, the proper behavior for a FastText model, based on the behavior of Facebook's original/reference implementation, is to <em>always</em> return a vector for an out-of-vocabulary word. </p>

<p>Essentially, if none of the supplied string's character n-grams are present, a vector will still be synthesized from whatever random vectors happen to be at the same lookup slots in the model's fixed-size collection of n-gram vectors. </p>

<p>In Gensim up through at least 3.7.1, the <code>FastText</code> class will throw a <code>KeyError: 'all ngrams for word _____ absent from model'</code> error if none of an out-of-vocabulary word's n-grams are present – but that's a buggy behavior that will be reversed, to match Facebook's FastText, in a future Gensim release. (The <a href=""https://github.com/RaRe-Technologies/gensim/pull/2370"" rel=""nofollow noreferrer"">PR to correct this behavior has been merged</a> to Gensim's develop branch and thus should take effect in the next release after 3.7.1.)</p>

<p>I'm not sure why you're not getting such an error with the specific model and dataset you've described. Perhaps your <code>fasttext_model</code> was actually trained with different text than you think? Or, trained with a very-small non-default <code>min_n</code> parameter, such that a single <code>哈</code> appearing inside the <code>sentiment140</code> data is enough to contribute to a synthesized vector for <code>哈哈哈哈</code>? </p>

<p>But given that the standard FastText behavior is to always report some synthesized vector, and Gensim will match that behavior in a future release, you shouldn't count on getting an error here. Expect to get back an essentially-random vector for completely unknown words with no resemblance to training data.</p>
",3,1,1352,2019-03-06 08:17:24,https://stackoverflow.com/questions/55018426/why-the-fasttext-word-embedding-could-generate-the-representation-of-a-word-from
"Fasttext representation for short phrase, but not for longer phrase containing the short one","<p>I'm using <code>Gensim</code> for loading the german <code>.bin</code> files from <code>Fasttext</code> in order to get vector representations for out-of-vocabulary words and phrases. So far it works fine and I achieve good results overall.<br>
I am familiar with the <code>KeyError :'all ngrams for word &lt;word&gt; absent from model'.</code> Clearly the model doesn't provide a vector representation for every possible ngram combination.<br>
But now I ran into a confusing (at least for me) issue.<br>
I'll just give a quick example:<br>
the model provides a representation for the phrase <code>AuM Wert</code>.<br>
But when I want to get a representation for <code>AuM Wert 50 Mio. Eur</code>, I'll get the <code>KeyError</code> mentioned above. So the model obviously has a representation for the shorter phrase but not for the extended one.<br>
It even returns a representation for <code>AuM Wert 50 Mio.Eur</code> (I just removed the space between 'Mio' and 'Eur')<br>
I mean, the statement in the Error is simply not true, because the first example shows that it knows some of the ngrams. Can someone explain that to me? What don't I understand here? Is my understanding of ngrams wrong?</p>

<p>Heres the code: </p>

<pre><code>from gensim.models.wrappers import FastText
model = FastText.load_fasttext_format('cc.de.300.bin')
model.wv['AuM Wert'] #returns a vector
model.wv['AuM Wert 50 Mio.EUR'] #returns a vector
model.wv['AuM Wert 50 Mio. EUR'] #triggers the error
</code></pre>

<p>Thanks in advance,<br>
Amos</p>
","python, nlp, gensim, fasttext","<p>I'm not certain what's causing the behavior you're seeing, though I have a theory below.</p>

<p>But, take note that the current gensim behavior (through 3.7.1), of sometimes returning <code>KeyError: all ngrams for word &lt;...&gt; absent</code> for an OOV word, does not conform to the behavior of Facebook's original FastText implementation, and is thus considered a bug. </p>

<p>It should be fixed in the next release. You can read a <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/CHANGELOG.md#out-of-vocab-word-handling"" rel=""nofollow noreferrer"">change note about the new compatible behavior</a>. </p>

<p>So, in the near future with an up-to-date version of gensim, you will never see this `KeyError'. </p>

<p>In the meantime, factors that <em>might</em> explain your observed behavior include:</p>

<ul>
<li>It's not typical to pass space-delimited phrases to <code>FastText</code>. Further, usual tokenizations of training texts will only pass word-tokens <em>without</em> any internal whitespace. So for a typical model, there's no chance such space-containing phrases will have full-word vectors. And, none of their character-n-grams that contains spaces will map to n-grams seen during training, either. To the extent you get a vector at all in gensim 3.7.1 and earlier, it will be because some of the n-grams <em>not</em> containing spaces were seen in training. (Post 3.7.1, you will always get a vector, though it may be composed from random collisions of the query-word's novel n-grams with n-grams learned in training, or simply with randomly-initialized-but-never-trained vectors inside the model's n-gram hashtable.)</li>
<li>N-grams are learned with a synthetic start-of-word prefix and end-of-word suffix – specifically the characters <code>&lt;</code> and <code>&gt;</code>. And the default n-gram size range is from 4 to 6 characters. So, your string <code>'AuM Wert'</code> will among its n-grams include <code>'&lt;AuM'</code>, <code>'Wert'</code>, and <code>'ert&gt;'</code>. (All of its other n-grams will include a space character, and thus couldn't possibly be in the set of n-grams learned during training on words without spaces.). But note that the longer phrase, on which you get the error, will <em>not</em> include the n-gram <code>'ert&gt;'</code>, because the prior end-of-token has been replaced with a space. So, the shorter phrase's n-grams is <em>not</em> a proper subset of the larger phrase's n-grams – and the larger phrase could error where the shorter does not. (And your longer phrase without a space, that does not error, also includes a number of extra 4-6 character n-grams that may have been in training data, that the erroring phrase does not have.)</li>
</ul>
",0,1,1148,2019-03-06 16:50:55,https://stackoverflow.com/questions/55028281/fasttext-representation-for-short-phrase-but-not-for-longer-phrase-containing-t
train Word2vec model using Gensim,"<p>this is my code.it reads reviews from an excel file (rev column) and make a list of list.</p>

<p>xp is like this</p>

<pre><code>[""['intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one'],['better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', 'investigator', 'thrust', 'murder', 'investigation', 'invisible'],[ 'man', 'alone', 'tell', 'fun', 'flow', 'decent', 'clip', 'need', 'say', 'sequence', 'comedy', 'gold', 'like', 'scene', 'restaurant', 'excellent', 'costello', 'pretending', 'work', 'ball', 'gym', 'final', 'reel']""]
</code></pre>

<p>but when use list for model, it gives me error""TypeError: 'float' object is not iterable"".i don't know where is my problem.
Thanks.</p>

<pre><code>xp=[]
import gensim 
import logging
import pandas as pd 
file = r'FileNamelast.xlsx'
df = pd.read_excel(file,sheet_name='FileNamex')
pages = [i for i in range(0,1000)]


for page in  pages:

 text =df.loc[page,[""rev""]]
 xp.append(text[0])


model = gensim.models.Word2Vec (xp, size=150, window=10, min_count=2, 
workers=10)
model.train(xp,total_examples=len(xp),epochs=10)
</code></pre>

<p>this is what i got.TypeError: 'float' object is not iterable</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-32-aa34c0e432bf&gt; in &lt;module&gt;()
     14 
     15 
---&gt; 16 model = gensim.models.Word2Vec (xp, size=150, window=10, min_count=2, workers=10)
     17 model.train(xp,total_examples=len(xp),epochs=10)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in __init__(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)
    765             callbacks=callbacks, batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window,
    766             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,
--&gt; 767             fast_version=FAST_VERSION)
    768 
    769     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py in __init__(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)
    757                 raise TypeError(""You can't pass a generator as the sentences argument. Try an iterator."")
    758 
--&gt; 759             self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)
    760             self.train(
    761                 sentences=sentences, corpus_file=corpus_file, total_examples=self.corpus_count,

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\base_any2vec.py in build_vocab(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    934         """"""
    935         total_words, corpus_count = self.vocabulary.scan_vocab(
--&gt; 936             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)
    937         self.corpus_count = corpus_count
    938         self.corpus_total_words = total_words

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in scan_vocab(self, sentences, corpus_file, progress_per, workers, trim_rule)
   1569             sentences = LineSentence(corpus_file)
   1570 
-&gt; 1571         total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)
   1572 
   1573         logger.info(

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\word2vec.py in _scan_vocab(self, sentences, progress_per, trim_rule)
   1552                     sentence_no, total_words, len(vocab)
   1553                 )
-&gt; 1554             for word in sentence:
   1555                 vocab[word] += 1
   1556             total_words += len(sentence)

TypeError: 'float' object is not iterable
</code></pre>
","python-3.x, gensim, word2vec","<p>The <code>sentences</code> corpus argument to <code>Word2Vec</code> should be an iterable sequence of lists-of-word-tokens.</p>

<p>Your reported value for <code>xp</code> is actually a list with one long string in it:</p>

<pre class=""lang-py prettyprint-override""><code>[
  ""['intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one'],['better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', 'investigator', 'thrust', 'murder', 'investigation', 'invisible'],[ 'man', 'alone', 'tell', 'fun', 'flow', 'decent', 'clip', 'need', 'say', 'sequence', 'comedy', 'gold', 'like', 'scene', 'restaurant', 'excellent', 'costello', 'pretending', 'work', 'ball', 'gym', 'final', 'reel']""
]
</code></pre>

<p>I don't see how this would give the error you've reported, but it's definitely wrong, so should be fixed. You should perhaps print <code>xp</code> just before you instantiate <code>Word2Vec</code> to be sure you know what it contains. </p>

<p>A true list, with each item being a list-of-string-tokens, would work. So if <code>xp</code> were the following that'd be correct:</p>

<pre class=""lang-py prettyprint-override""><code>    [
      ['intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one'],
      ['better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', 'investigator', 'thrust', 'murder', 'investigation', 'invisible'],
      [ 'man', 'alone', 'tell', 'fun', 'flow', 'decent', 'clip', 'need', 'say', 'sequence', 'comedy', 'gold', 'like', 'scene', 'restaurant', 'excellent', 'costello', 'pretending', 'work', 'ball', 'gym', 'final', 'reel']
    ]

</code></pre>

<p>Note, however:</p>

<ul>
<li><code>Word2Vec</code> doesn't do well with toy-sized datasets. So while this tiny setup may be helpful to check for basic syntax/format issues, don't expect realistic results until you're training with many hundreds-of-thousands of words.</li>
<li>You don't need to call <code>train()</code> if you already supplied your corpus at instantiation, as you have. The model will do all steps automatically. (If, on the other hand, you don't supply your corpus, you'd then have to call <em>both</em> <code>build_vocab()</code> and <code>train()</code>.) If you enable logging at the INFO level all the steps happening behind the scenes will be clearer.</li>
</ul>
",2,0,2731,2019-03-09 08:10:46,https://stackoverflow.com/questions/55075312/train-word2vec-model-using-gensim
gensim: KeyError: “word &#39;good&#39; not in vocabulary”,"<p>I am running the below code, but gensim word2vec is throwing a word not in vocabulary error. Can you let me know the solution?</p>

<p>this is my file(file.txt)</p>

<pre><code>'intrepid', 'bumbling', 'duo', 'deliver', 'good', 'one', 'better', 'offering', 'considerable', 'cv', 'freshly', 'qualified', 'private', ..
</code></pre>

<p>this is my code </p>

<pre><code> import gensim 
    with open('file.txt', 'r') as myfile:
      data = myfile.read()



    model = gensim.models.Word2Vec(data,min_count=1,size=32)
    w1 = ""good""
    model.wv.most_similar (positive=w1)
</code></pre>

<p>Output:</p>

<pre><code>KeyError: ""word 'good' not in vocabulary""


---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-34-22572d5a8082&gt; in &lt;module&gt;()
      7 model = gensim.models.Word2Vec(data,min_count=1,size=32)
      8 w1 = ""good""
----&gt; 9 model.wv.most_similar (positive=w1)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in most_similar(self, positive, negative, topn, restrict_vocab, indexer)
    529                 mean.append(weight * word)
    530             else:
--&gt; 531                 mean.append(weight * self.word_vec(word, use_norm=True))
    532                 if word in self.vocab:
    533                     all_words.add(self.vocab[word].index)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in word_vec(self, word, use_norm)
    450             return result
    451         else:
--&gt; 452             raise KeyError(""word '%s' not in vocabulary"" % word)
    453 
    454     def get_vector(self, word):

KeyError: ""word 'good' not in vocabulary""
</code></pre>

<p>​</p>
","python-3.x, gensim","<pre><code>import gensim
data=[]
with open('lastlast.txt', 'r') as myfile:
  raw_data = myfile.read()
  raw_data=raw_data.replace('\n',',')
  split_data=raw_data.split(',')
  data=[i.replace(""\'"",'').replace(' ','') for i in split_data if i!=""""]
</code></pre>

<p>The first parameter should be iterable. Since data is just iterable of sentences it takes every character, but [data] takes every word.
From the docs</p>

<pre><code>&gt;&gt;&gt; model = gensim.models.Word2Vec([data],min_count=1,size=32)
&gt;&gt;&gt; model = Word2Vec.load(""word2vec.model"")
&gt;&gt;&gt; model.train([[""hello"", ""world""]], total_examples=1, epochs=1)
</code></pre>

<p>Your solution:-
Now if you do this you will get the answer.</p>

<pre><code>&gt;&gt;&gt;model.most_similar(['good'])
</code></pre>
",0,0,4570,2019-03-11 04:47:00,https://stackoverflow.com/questions/55095368/gensim-keyerror-word-good-not-in-vocabulary
What is correct way to get doc vectors values?,"<p>How I can obtain specific doc vector values? By tag, like this:</p>

<pre><code>modelValues = model.docvecs['myDocTag']
</code></pre>

<p>or it is possible only by index, like this:</p>

<pre><code>modelValues = model.docvecs[12]
</code></pre>

<p>(in last case, I must know matching <code>tag</code>→<code>index</code>...)</p>
","python, gensim, doc2vec","<p>You can use either but should use the same sort of <code>tag</code> keys as were provided during training. </p>

<p>So if your tagged-documents during training had a string tag of <code>'myDocTag'</code>, you should use <code>model.docvecs['myDocTag']</code>.</p>

<p>If you explicitly provided plain int tags, you could use <code>model.docvecs[12]</code>. (But note in such a case, you should be careful to assign contiguous ints starting from 0.)</p>
",1,0,33,2019-03-11 13:47:51,https://stackoverflow.com/questions/55103288/what-is-correct-way-to-get-doc-vectors-values
Can I preserve the random state of a doc2vec mode for each document I want to infer by infering all documents at the same time?,"<p>is there a way to infer multiple documents at the same time to preserve the random state of the model using Gensim Doc2Vec?</p>

<p>The function infer_vector is defined as</p>

<pre><code>infer_vector(doc_words, alpha=None, min_alpha=None, epochs=None, steps=None)¶
</code></pre>

<p>where doc_words (list of str) – A document for which the vector representation will be inferred. And I could not find any opther option to infer multiple documents at the same time.</p>
","gensim, word2vec, doc2vec","<p>There's no current option to infer multiple documents at once. It's one of many wishlist improvements for <code>infer_vector()</code> (collected in an <a href=""https://github.com/RaRe-Technologies/gensim/issues/515"" rel=""nofollow noreferrer"">open issue</a>), but there's no work in progress or targeted release for that to arrive. </p>

<p>I'm not sure what you mean by ""preserve the random state of the model"". The main motivations for batching that I can see would be user convenience, or added performance via multithreading. </p>

<p>If what you really want is deterministic inference, see an <a href=""https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q12-ive-used-doc2vec-infer_vector-on-a-single-text-but-the-resulting-vector-is-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-doc2vec-inference-non-determinism"" rel=""nofollow noreferrer"">answer in the Gensim FAQ which explains why deterministic <code>Doc2Vec</code> inference isn't necessarily a good idea</a>. (It also includes a link to an issue with some ideas for how to force it, if you're determined to do that despite the good reasons not to.)</p>
",1,0,304,2019-03-14 18:30:43,https://stackoverflow.com/questions/55169721/can-i-preserve-the-random-state-of-a-doc2vec-mode-for-each-document-i-want-to-in
Add words per topic LDA,"<p>I'm building a LDA in python using Gensim and I'm struggling to increase the number of words printed per topic from the default of 10.  I'd like 20 topics with 30 words each.  Any advice would be greatly appreciated :)</p>

<pre><code># train the LDA model

lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)

# check out the topics

for idx, topic in lda_model.print_topics(-1):
   print('Topic: {} \nWords: {}'.format(idx, topic))
</code></pre>
","python, windows, gensim, lda, topic-modeling","<p>You have two options: <code>show_topics</code> and <code>print_topics</code>.</p>

<p>From the <a href=""https://radimrehurek.com/gensim/models/ldamulticore.html#module-gensim.models.ldamulticore"" rel=""nofollow noreferrer"">gensim ldamulticore documentation</a>: </p>

<p><code>show_topics</code> (a little bit more customizable alias for <code>print_topics</code> that gives prettier output in your case) has the parameter <code>num_words</code>  - the number of words you want to display, ranked by significance.</p>

<pre class=""lang-py prettyprint-override""><code>for idx, topic in lda_model.show_topics(idx, num_words=30):
   print('Topic: {} \nWords: {}'.format(idx, topic))
</code></pre>

<p>You can also leave out <code>idx</code> - output is unaffected by it. <code>print_topics</code> works similarly, but with a default 10 topics to display:</p>

<pre class=""lang-py prettyprint-override""><code>for idx, topic in lda_model2.show_topics(num_topics=20, num_words=30):
   print('Topic: {} \nWords: {}'.format(idx, topic))
</code></pre>
",0,0,232,2019-03-18 21:32:53,https://stackoverflow.com/questions/55230344/add-words-per-topic-lda
Add stop words in Gensim,"<p>Thanks for stopping by!  I had a quick question about appending stop words. I have a select few words that show up in my data set and I was hopping I could add them to gensims stop word list.  I've seen a lot of examples using nltk and I was hoping there would be a way to do the same in gensim.  I'll post my code below:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
            nltk.bigrams(token)
            result.append(lemmatize_stemming(token))
    return result</code></pre>
</div>
</div>
</p>
","python, windows, nlp, gensim, stop-words","<p>While <code>gensim.parsing.preprocessing.STOPWORDS</code> is pre-defined for your convenience, and happens to be a <code>frozenset</code> so it can't be directly added-to, you could easily make a larger set that includes both those words and your additions. For example:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.parsing.preprocessing import STOPWORDS
my_stop_words = STOPWORDS.union(set(['mystopword1', 'mystopword2']))
</code></pre>

<p>Then use the new, larger <code>my_stop_words</code> in your subsequent stop-word-removal code. (The <code>simple_preprocess()</code> function of <code>gensim</code> doesn't automatically remove stop-words.)</p>
",13,1,11548,2019-03-19 19:08:35,https://stackoverflow.com/questions/55248396/add-stop-words-in-gensim
How does LDA (Latent Dirichlet Allocation) inference from `gensim` work for a new data?,"<p>I am training my <code>ldamodel</code> using <code>gensim</code>, and predicting using a test corpus like this <code>ldamodel[doc_term_matrix_test]</code>, it works just fine but I don't understand how the prediction is actually done using the trained model (what <code>ldamodel[doc_term_matrix_test]</code> is doing).</p>

<p>Here is the code :</p>

<pre><code>dictionary2 = corpora.Dictionary(test)
dictionary = corpora.Dictionary(train)
dictionary.merge_with(dictionary2)
doc_term_matrix2 = [dictionary.doc2bow(doc) for doc in test]
doc_term_matrix = [dictionary.doc2bow(doc) for doc in train]
Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(doc_term_matrix, num_topics=2, id2word = 
dictionary,random_state=100, iterations=50, passes=1)
topics = sorted(ldamodel[doc_term_matrix2],
                key=lambda 
                x:x[1],
                reverse=True)
</code></pre>
","python, gensim, lda, topic-modeling, inference","<p>To quote from <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">gensim docs about ldamodel</a>:</p>

<blockquote>
  <p>This module allows both LDA model estimation from a training corpus and inference of topic distribution on new, unseen documents.</p>
</blockquote>

<p>So apparently, what your code does is not quite ""prediction"" but rather inference. That is, your trained LDA model yields for every test document <code>T</code> an estimation of the topic distribution of <code>T</code>.</p>
",2,2,2110,2019-03-20 14:58:16,https://stackoverflow.com/questions/55263867/how-does-lda-latent-dirichlet-allocation-inference-from-gensim-work-for-a-ne
Gensim mallet CalledProcessError: returned non-zero exit status,"<p>I'm getting an error while trying to access gensims mallet in jupyter notebooks.  I have the specified file 'mallet' in the same folder as my notebook, but cant seem to access it.  I tried routing to it from the C drive but I still get the same error.  Please help :)</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>import os
from gensim.models.wrappers import LdaMallet

#os.environ.update({'MALLET_HOME':r'C:/Users/new_mallet/mallet-2.0.8/'})

mallet_path = 'mallet' # update this path

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=bow_corpus, num_topics=20, id2word=dictionary)

result = (ldamallet.show_topics(num_topics=3, num_words=10,formatted=False))
for each in result:
    print (each)</code></pre>
</div>
</div>
</p>

<p><a href=""https://i.sstatic.net/xcjPF.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/xcjPF.png"" alt=""Mallet Error CalledProcessError""></a></p>

<p><a href=""https://i.sstatic.net/DZJFr.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/DZJFr.png"" alt=""enter image description here""></a></p>
","python, windows, jupyter-notebook, gensim, mallet","<p>Update the path to:</p>
<pre><code>mallet_path = 'C:/mallet/mallet-2.0.8/bin/mallet.bat'
</code></pre>
<p>and edit the notepad mallet.bat within the mallet 2.0.8 folder to:</p>
<pre><code>@echo off

rem This batch file serves as a wrapper for several
rem  MALLET command line tools.

if not &quot;%MALLET_HOME%&quot; == &quot;&quot; goto gotMalletHome

echo MALLET requires an environment variable MALLET_HOME.
goto :eof

:gotMalletHome

set MALLET_CLASSPATH=C:\mallet\mallet-2.0.8\class;C:\mallet\mallet-2.0.8\lib\mallet-deps.jar
set MALLET_MEMORY=1G
set MALLET_ENCODING=UTF-8

set CMD=%1
shift

set CLASS=
if &quot;%CMD%&quot;==&quot;import-dir&quot; set CLASS=cc.mallet.classify.tui.Text2Vectors
if &quot;%CMD%&quot;==&quot;import-file&quot; set CLASS=cc.mallet.classify.tui.Csv2Vectors
if &quot;%CMD%&quot;==&quot;import-svmlight&quot; set CLASS=cc.mallet.classify.tui.SvmLight2Vectors
if &quot;%CMD%&quot;==&quot;info&quot; set CLASS=cc.mallet.classify.tui.Vectors2Info
if &quot;%CMD%&quot;==&quot;train-classifier&quot; set CLASS=cc.mallet.classify.tui.Vectors2Classify
if &quot;%CMD%&quot;==&quot;classify-dir&quot; set CLASS=cc.mallet.classify.tui.Text2Classify
if &quot;%CMD%&quot;==&quot;classify-file&quot; set CLASS=cc.mallet.classify.tui.Csv2Classify
if &quot;%CMD%&quot;==&quot;classify-svmlight&quot; set CLASS=cc.mallet.classify.tui.SvmLight2Classify
if &quot;%CMD%&quot;==&quot;train-topics&quot; set CLASS=cc.mallet.topics.tui.TopicTrainer
if &quot;%CMD%&quot;==&quot;infer-topics&quot; set CLASS=cc.mallet.topics.tui.InferTopics
if &quot;%CMD%&quot;==&quot;evaluate-topics&quot; set CLASS=cc.mallet.topics.tui.EvaluateTopics
if &quot;%CMD%&quot;==&quot;prune&quot; set CLASS=cc.mallet.classify.tui.Vectors2Vectors
if &quot;%CMD%&quot;==&quot;split&quot; set CLASS=cc.mallet.classify.tui.Vectors2Vectors
if &quot;%CMD%&quot;==&quot;bulk-load&quot; set CLASS=cc.mallet.util.BulkLoader
if &quot;%CMD%&quot;==&quot;run&quot; set CLASS=%1 &amp; shift

if not &quot;%CLASS%&quot; == &quot;&quot; goto gotClass

echo Mallet 2.0 commands: 
echo   import-dir        load the contents of a directory into mallet instances (one per file)
echo   import-file       load a single file into mallet instances (one per line)
echo   import-svmlight   load a single SVMLight format data file into mallet instances (one per line)
echo   info              get information about Mallet instances
echo   train-classifier  train a classifier from Mallet data files
echo   classify-dir      classify data from a single file with a saved classifier
echo   classify-file     classify the contents of a directory with a saved classifier
echo   classify-svmlight classify data from a single file in SVMLight format
echo   train-topics      train a topic model from Mallet data files
echo   infer-topics      use a trained topic model to infer topics for new documents
echo   evaluate-topics   estimate the probability of new documents given a trained model
echo   prune             remove features based on frequency or information gain
echo   split             divide data into testing, training, and validation portions
echo   bulk-load         for big input files, efficiently prune vocabulary and import docs
echo Include --help with any option for more information


goto :eof

:gotClass

set MALLET_ARGS=

:getArg

if &quot;%1&quot;==&quot;&quot; goto run
set MALLET_ARGS=%MALLET_ARGS% %1
shift
goto getArg

:run

&quot;C:\Program Files\Java\jdk-12\bin\java&quot; -ea -Dfile.encoding=%MALLET_ENCODING% -classpath %MALLET_CLASSPATH% %CLASS% %MALLET_ARGS%

:eof
</code></pre>
<p>in command line these were helpful commands to figure out what was going on:</p>
<pre><code>notepad mallet.bat
java
C:\Program Files\Java\jdk-12\bin\java
dir /OD
cd %userdir%
cd %userpath%
cd\
cd users
cd your_username
cd appdata\local\temp\2
dir /OD
</code></pre>
<p>the problem is with java not being installed correctly or with the path not including java and the mallet classpath not being defined correctly.  More info here: <a href=""https://docs.oracle.com/javase/7/docs/technotes/tools/windows/classpath.html"" rel=""nofollow noreferrer"">https://docs.oracle.com/javase/7/docs/technotes/tools/windows/classpath.html</a> .  This solved my error hopefully it helps someone else :)</p>
",4,6,8115,2019-03-21 20:24:44,https://stackoverflow.com/questions/55288724/gensim-mallet-calledprocesserror-returned-non-zero-exit-status
How to compare the topical similarity between two documents in Python Gensim from their topic distributions?,"<p>I have trained a LDA model on a corpus using Gensim. Now that I have the topic distribution for each document, how can I compare how similar two documents are in topics? I would like to have a summary measure. For example, the following are the topic distributions of two documents. There are totally 75 topics. For brevity, I show only the first 10 topics with largest probabilities (so the topics are not in order). (40, 0.5523168) means that topic #40 has a probability of 0.5523168 for DOC #1. Should I calculate the Euclidean or Cosine distance between the two vectors? And using this summary measure, can I say that, for example, DOC 1 is more similar to DOC2 than to DOC3, or DOC1 and DOC 2 are more similar to each other than DOC 3 and DOC 4 topically? Thank you!</p>

<pre><code>DOC #1:
[(40, 0.5523168), (60, 0.12225048), (43, 0.07556598), (41, 0.065885976), 
(22, 0.05838573), (24, 0.044774733), (74, 0.019839266), (65, 0.019544959), 
(51, 0.015470431), (36, 0.013449047)]


DOC #2:
[(73, 0.58864516), (41, 0.16827711), (51, 0.09783472), (63, 0.06510383), 
(24, 0.04722658), (32, 0.014467965), (44, 0.012267662), (47, 0.0031533625), 
(18, 0.0022214972), (0, 1.2154361e-05)]
</code></pre>
","python, gensim, lda","<p><strong>Gensim Functionality</strong></p>

<p>Gensim provides the <code>similarities.docsim</code> functionality - to ""compute similarities across a collection of documents in the Vector Space Model."" You can see the <a href=""https://radimrehurek.com/gensim/similarities/docsim.html"" rel=""noreferrer"">documentation</a> here, there is also a <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Similarity_Queries.ipynb"" rel=""noreferrer"">tutorial</a> here for the similarity queries.</p>

<p><strong>Document Similarity Measures</strong></p>

<p>Using euclidian distances would be an uncommon choice - you could, but there are potential issues. You could use cosine similarity <a href=""https://www.machinelearningplus.com/nlp/cosine-similarity/"" rel=""noreferrer"">(link to python tutorial)</a> - this takes the cosine of the angle of two document vectors, which has the advantage of being easily understood (1= the documents are perfectly alike, to -1=the documents have no similarity at all) and yes, you can compare the cosine similarity of documents 1 &amp; 2 and compare it to that of documents 3 &amp; 4, or calculate the similarity values of doc1 to doc2 and doc1 and doc3 and compare them. There is a pretty <a href=""https://www.machinelearningplus.com/nlp/cosine-similarity/"" rel=""noreferrer"">good tutorial here.</a> </p>

<p>You might also find my answer to <a href=""https://stats.stackexchange.com/questions/402633/measuring-similarity-between-document-and-category/402653#402653"">this question over at CrossValidated</a> informative, even though your question is somewhat different.</p>

<p>Gensim also has other <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/distance_metrics.ipynb"" rel=""noreferrer"">distance metrics</a> available. These are pretty much all included in gensim's <code>matutils</code>.</p>

<p><strong>Topical distances</strong></p>

<p>You can also measure distances between <em>topics</em> using (some) of these distances in the above link, such as Hellinger distance.</p>
",6,4,4307,2019-03-22 01:38:09,https://stackoverflow.com/questions/55291657/how-to-compare-the-topical-similarity-between-two-documents-in-python-gensim-fro
Doc2Vec &amp; classification - very poor results,"<p>I have a dataset of 6000 observations; a sample of it is the following:</p>

<pre><code>job_id      job_title                                           job_sector
30018141    Secondary Teaching Assistant                        Education
30006499    Legal Sales Assistant / Executive                   Sales
28661197    Private Client Practitioner                         Legal
28585608    Senior hydropower mechanical project manager        Engineering
28583146    Warehouse Stock Checker - Temp / Immediate Start    Transport &amp; Logistics
28542478    Security Architect Contract                         IT &amp; Telecoms
</code></pre>

<p>The goal is to predict the job sector of each row based on the job title.</p>

<p>Firstly, I apply some preprocessing on the <code>job_title</code> column:</p>

<pre><code>def preprocess(document):
    lemmatizer = WordNetLemmatizer()
    stemmer_1 = PorterStemmer()
    stemmer_2 = LancasterStemmer()
    stemmer_3 = SnowballStemmer(language='english')

    # Remove all the special characters
    document = re.sub(r'\W', ' ', document)

    # remove all single characters
    document = re.sub(r'\b[a-zA-Z]\b', ' ', document)

    # Substituting multiple spaces with single space
    document = re.sub(r' +', ' ', document, flags=re.I)

    # Converting to lowercase
    document = document.lower()

    # Tokenisation
    document = document.split()

    # Stemming
    document = [stemmer_3.stem(word) for word in document]

    document = ' '.join(document)

    return document

df_first = pd.read_csv('../data.csv', keep_default_na=True)

for index, row in df_first.iterrows():

    df_first.loc[index, 'job_title'] = preprocess(row['job_title'])
</code></pre>

<p>Then I do the following with <code>Gensim</code> and <code>Doc2Vec</code>:</p>

<pre><code>X = df_first.loc[:, 'job_title'].values
y = df_first.loc[:, 'job_sector'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)

tagged_train = TaggedDocument(words=X_train.tolist(), tags=y_train.tolist())
tagged_train = list(tagged_train)

tagged_test = TaggedDocument(words=X_test.tolist(), tags=y_test.tolist())
tagged_test = list(tagged_test)

model = Doc2Vec(vector_size=5, min_count=2, epochs=30)

training_set = [TaggedDocument(sentence, tag) for sentence, tag in zip(X_train.tolist(), y_train.tolist())]

model.build_vocab(training_set)

model.train(training_set, total_examples=model.corpus_count, epochs=model.epochs)   

test_set = [TaggedDocument(sentence, tag) for sentence, tag in zip(X_test.tolist(), y_test.tolist())]

predictors_train = []
for sentence in X_train.tolist():

    sentence = sentence.split()
    predictor = model.infer_vector(doc_words=sentence, steps=20, alpha=0.01)

    predictors_train.append(predictor.tolist())

predictors_test = []
for sentence in X_test.tolist():

    sentence = sentence.split()
    predictor = model.infer_vector(doc_words=sentence, steps=20, alpha=0.025)

    predictors_test.append(predictor.tolist())

sv_classifier = SVC(kernel='linear', class_weight='balanced', decision_function_shape='ovr', random_state=0)
sv_classifier.fit(predictors_train, y_train)

score = sv_classifier.score(predictors_test, y_test)
print('accuracy: {}%'.format(round(score*100, 1)))
</code></pre>

<p>However, the result which I am getting is 22% accuracy.</p>

<p>This makes me a lot suspicious especially because by using the <code>TfidfVectorizer</code> instead of the <code>Doc2Vec</code> (both with the same classifier) then I am getting 88% accuracy (!).</p>

<p>Therefore, I guess that I must be doing something wrong in how I apply the <code>Doc2Vec</code> of <code>Gensim</code>.</p>

<p>What is it and how can I fix it?</p>

<p>Or it it simply that my dataset is relatively small while more advanced methods such as word embeddings etc require way more data?</p>
","python, classification, gensim, text-classification, doc2vec","<p>You don't mention the size of your dataset - in rows, total words, unique words, or unique classes. Doc2Vec works best with lots of data. Most published work trains on tens-of-thousands to millions of documents, of dozens to thousands of words each. (Your data appears to only have 3-5 words per document.)</p>

<p>Also, published work tends to train on data where every document has a unique-ID. It can sometimes make sense to use known-labels as tags instead of, or in addition to, unique-IDs. But it isn't necessarily a better approach. By using known-labels as the only tags, you're effectively only training one doc-vector per label. (It's essentially similar to concatenating all rows with the same tag into one document.)</p>

<p>You're inexplicably using fewer <code>steps</code> in inference than <code>epochs</code> in training - when in fact these are analogous values. In recent versions of <code>gensim</code>, inference will by default use the same number of inference epochs as the model was configured to use for training. And, it's more common to use <em>more</em> epochs during inference than training. (Also, you're inexplicably using different starting <code>alpha</code> values for inference for both classifier-training and classifier-testing.)</p>

<p>But the main problem is likely your choice of tiny <code>size=5</code> doc vectors. Instead of the <code>TfidfVectorizer</code>, which will summarize each row as a vector of width equal to the unique-word count – perhaps hundreds or thousands of dimensions – your <code>Doc2Vec</code> model summarizes each document as just 5 values. You've essentially lobotomized <code>Doc2Vec</code>. Usual values here are 100-1000 – though if the dataset is tiny smaller sizes may be required.</p>

<p>Finally, the lemmatization/stemming may not be strictly necessary and may even be destructive. Lots of <code>Word2Vec</code>/<code>Doc2Vec</code> work doesn't bother to lemmatize/stem - often because there's plentiful data, with many appearances of all word forms. </p>

<p>These steps are most likely to help with smaller data, by making sure rarer word forms are combined with related longer forms to still get value from words that would otherwise be too rare to be retained (or get useful vectors). </p>

<p>But I can see many ways they might hurt for your domain. <code>Manager</code> and <code>Management</code> won't have exactly the same implications in this context, but could both be stemmed to <code>manag</code>. Similar for <code>Security</code> and <code>Securities</code> both becoming <code>secur</code>, and other words. I'd only perform these steps if you can prove through evaluation that they're helping. (Are the words passed to the <code>TfidfVectorizer</code> being lemmatized/stemmed?)</p>
",5,3,5094,2019-03-22 23:51:46,https://stackoverflow.com/questions/55309197/doc2vec-classification-very-poor-results
How to tell if WikiCorpus from gensim is working?,"<p>I downloaded the full wikipedia archive 14.9gb and I am running thise line of code:</p>

<pre><code>wiki = WikiCorpus(""enwiki-latest-pages-articles.xml.bz2"")
</code></pre>

<p>My code doesn't seem to be getting past here and it has been running for an hour now, I understand that the target file is massive, but I was wondering how I could tell it is working, or what is the expected time for it to complete?</p>
","python, gensim","<p>You can often use an OS-specific monitoring tool, such as <code>top</code> on Linux/Unix/MacOS systems, to get an idea whether your Python process is intensely computing, using memory, or continuing with IO. </p>

<p>Even the simple vocabulary-scan done when 1st instantiating <code>WikiCorpus</code> may take a long time, to both decompress and tokenize/tally, so I wouldn't be surprised by a runtime longer than hour. (And if it's relying on any virtual-memory/swapping during this simple operation, as may be clear from the output of <code>top</code> or similar monitoring, that'd slow things down even more.)</p>

<p>As a comparative baseline, you could time how long decompression-only takes with a shell command like:</p>

<pre><code>% time bzcat enwiki-latest-pages-articles.xml.bz2 | wc
</code></pre>

<p>(A quick test on my MacBook Pro suggests 15GB of BZ2 data might take 30-minutes-plus just to decompress.)</p>

<p>In some cases, turning on Python logging at the <code>INFO</code> level will display progress information with <code>gensim</code> modules, though I'm not sure <code>WikiCorpus</code> shows anything until it finishes. Enabling <code>INFO</code>-level logging can be as simple as:</p>

<pre><code>import logging
logging.getLogger().setLevel(logging.INFO)
</code></pre>
",1,0,296,2019-04-02 15:12:48,https://stackoverflow.com/questions/55478104/how-to-tell-if-wikicorpus-from-gensim-is-working
Gensim Attribute Error when trying to use pre_scan on a doc2vec object,"<p>I am following the tutorial here:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>

<p>But when I get to this part:</p>

<pre><code>pre = Doc2Vec(min_count=0)
pre.scan_vocab(documents)
</code></pre>

<p>I get the following error on scan_vocab:</p>

<pre><code>    AttributeError: 'Doc2Vec' object has no attribute 'scan_vocab'
</code></pre>

<p>Does anyone know how to fix this? Thanks.</p>
","python, gensim","<p>That's a known problem after a 2018 refactoring of the <code>Doc2Vec</code> code:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/issues/2085"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/2085</a></p>

<p>You can just skip that cell to proceed with the rest of that demo notebook. (If you really needed to adjust the <code>min_count</code> using the info from a full-scan, you might be able to call some internal classes/methods mentioned in the above issue.)</p>
",1,0,282,2019-04-03 04:32:52,https://stackoverflow.com/questions/55487124/gensim-attribute-error-when-trying-to-use-pre-scan-on-a-doc2vec-object
How are word vectors co-trained with paragraph vectors in doc2vec DBOW?,"<p>I don't understand how word vectors are involved at all in the training process with gensim's <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec"" rel=""nofollow noreferrer"">doc2vec</a> in DBOW mode (<code>dm=0</code>). I know that it's disabled by default with <code>dbow_words=0</code>. But what happens when we set <code>dbow_words</code> to 1?</p>

<p>In my understanding of DBOW, the context words are predicted directly from the paragraph vectors. So the only parameters of the model are the <code>N</code> <code>p</code>-dimensional paragraph vectors plus the parameters of the classifier.</p>

<p>But multiple sources hint that it is possible in DBOW mode to co-train word and doc vectors. For instance:</p>

<ul>
<li>section 5 of <a href=""https://www.aclweb.org/anthology/W16-1609"" rel=""nofollow noreferrer"">An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation</a></li>
<li>this SO answer: <a href=""https://stackoverflow.com/questions/27470670/how-to-use-gensim-doc2vec-with-pre-trained-word-vectors/30337118#30337118"">How to use Gensim doc2vec with pre-trained word vectors?</a></li>
</ul>

<p>So, how is this done? <strong>Any clarification would be much appreciated!</strong></p>

<p>Note: for DM, the paragraph vectors are averaged/concatenated with the word vectors to predict the target words. In that case, it's clear that words vectors are trained simultaneously with document vectors. And there are <code>N*p + M*q + classifier</code> parameters (where <code>M</code> is vocab size and <code>q</code> word vector space dim).</p>
","gensim, word2vec, doc2vec","<p>If you set <code>dbow_words=1</code>, then skip-gram word-vector training is added the to training loop, interleaved with the normal PV-DBOW training. </p>

<p>So, for a given target word in a text, 1st the candidate doc-vector is used (alone) to try to predict that word, with backpropagation adjustments then occurring to the model &amp; doc-vector. Then, a bunch of the surrounding words are each used, one at a time in skip-gram fashion, to try to predict that same target word – with the followup adjustments made.</p>

<p>Then, the next target word in the text gets the same PV-DBOW plus skip-gram treatment, and so on, and so on. </p>

<p>As some logical consequences of this:</p>

<ul>
<li><p>training takes longer than plain PV-DBOW - by about a factor equal to the <code>window</code> parameter</p></li>
<li><p>word-vectors overall wind up getting more total training attention than doc-vectors, again by a factor equal to the <code>window</code> parameter</p></li>
</ul>
",2,1,641,2019-04-09 11:46:29,https://stackoverflow.com/questions/55592142/how-are-word-vectors-co-trained-with-paragraph-vectors-in-doc2vec-dbow
fasttext error TypeError: supervised() got an unexpected keyword argument &#39;pretrainedVectors&#39;,"<p>I am trying to add pretrained vectors to a training model using fasttext and getting the below error. Code is written in python with fasttext 0.8.3.</p>

<p>I thought with fasttext you could add pre trained vectors to a supervised training model?</p>

<p>TypeError: supervised() got an unexpected keyword argument 'pretrainedVectors'</p>

<pre><code>pretrainedVectors = 'vectorFile.vec'
classifier = ft.supervised(model_data, model_name, pretrainedVectors=pretrainedVectors, label_prefix=label_prefix, lr=lr, epoch=epoch, minn=minn, maxn=maxn, dim=dim, bucket=bucket)
</code></pre>
","python, gensim, fasttext","<p>According to the <a href=""https://pypi.org/project/fasttext/"" rel=""nofollow noreferrer"">documentation</a>, the named parameter to the function is called <code>pretrained_vectors</code> not <code>pretrainedVectors</code>. </p>

<p>This naming convention is in line with PEP-8 style and so is normal for a Python API.</p>
",1,0,827,2019-04-10 12:21:27,https://stackoverflow.com/questions/55612440/fasttext-error-typeerror-supervised-got-an-unexpected-keyword-argument-pretr
"How to only return actual tokens, rather than empty variables when tokenizing?","<p>I have a function:</p>

<pre><code>def remove_stopwords(text):
     return [[word for word in simple_preprocess(str(doc), min_len = 2) if word not in stop_words] for doc in texts] 
</code></pre>

<p>My input is a list with a tokenized sentence:</p>

<pre><code>input = ['This', 'is', 'an', 'example', 'of', 'my', 'input']
</code></pre>

<p>Assume that <code>stop_words</code> contains the words: 'this', 'is', 'an', 'of' and 'my', then the output I would like to get is:</p>

<pre><code>desired_output = ['example', 'input']
</code></pre>

<p>However, the actual output that I'm getting now is:</p>

<pre><code>actual_output = [[], [], [], ['example'], [], [], ['input']]
</code></pre>

<p>How can I adjust my code, to get this output?</p>
","python, apply, tokenize, gensim","<p>You can use the below code for removing stopwords, if there is no specific reason to use your code.</p>

<pre><code>wordsFiltered = []
def remove_stopwords(text):
    for w in text:
        if w not in stop_words:
            wordsFiltered.append(w)
    return wordsFiltered

input = ['This', 'is', 'an', 'example', 'of', 'my', 'input']

stop_words = ['This', 'is', 'an', 'of', 'my']

print remove_stopwords(input)
</code></pre>

<p>Output:</p>

<pre><code>['example', 'input']
</code></pre>
",1,0,54,2019-04-12 10:10:07,https://stackoverflow.com/questions/55649311/how-to-only-return-actual-tokens-rather-than-empty-variables-when-tokenizing
What text processing does WikiCorpus perform in gensim?,"<p>I have trained a doc2vec model on the Wikipedia corpus using gensim and I wish to retrieve vectors from different documents. </p>

<p>I was wondering what text processing the WikiCorpus function did when I used it to train my model e.g. removed punctuation, made all the text lower case, removed stop words etc. </p>

<p>This is important as I wish to perform the same text processing on the documents I am inferring vectors from for greater consistency/accuracy with my model. </p>
","python, gensim, doc2vec","<p>To know precisely what's done, your best reference is the source code for <code>WikiCorpus</code> itself, which you can view in your local installation, or online at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/wikicorpus.py"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/wikicorpus.py</a></p>

<p>Key functions in that file for dealing with the raw Wikipedia dump data include <code>process_article()</code>, <code>filter_wiki()</code> and <code>remove_markup()</code> – which ultimately also uses a local <code>tokenize()</code> function, that then relies on another <code>tokenize()</code> from the <code>gensim.utils</code> module. </p>

<p>And, <code>WikiCorpus</code> does in fact call that <code>utils.tokenize()</code> with a <code>lower=True</code> parameter to force lowercasing. </p>

<p>Further, that <code>utils.tokenize()</code> uses a <code>simple_tokenize()</code> function that, while it doesn't have a step that explicitly removes punctuation, looks for tokens via a <a href=""https://github.com/RaRe-Technologies/gensim/blob/b27d6ea11792bb658bfe4b605054a23bfb6c6c81/gensim/utils.py#L55"" rel=""nofollow noreferrer""><code>PAT_ALPHABETIC</code> regex</a> which selects tokens made of word-characters (<code>\w</code>) that don't start with digits (<code>\d</code>). </p>
",2,2,1537,2019-04-12 23:25:57,https://stackoverflow.com/questions/55660598/what-text-processing-does-wikicorpus-perform-in-gensim
How do I calculate the similarity of a word or couple of words compared to a document using a doc2vec model?,"<p>In gensim I have a trained doc2vec model, if I have a document and either a single word or two-three words, what would be the best way to calculate the similarity of the words to the document? </p>

<p>Do I just do the standard cosine similarity between them as if they were 2 documents? Or is there a better approach for comparing small strings to documents?</p>

<p>On first thought I could get the cosine similarity from each word in the 1-3 word string and every word in the document taking the averages, but I dont know how effective this would be.</p>
","python, gensim, doc2vec","<p>There's a number of possible approaches, and what's best will likely depend on the kind/quality of your training data and ultimate goals. </p>

<p>With any <code>Doc2Vec</code> model, you can infer a vector for a new text that contains known words – even a single-word text – via the <code>infer_vector()</code> method. However, like <code>Doc2Vec</code> in general, this tends to work better with documents of at least dozens, and preferably hundreds, of words. (Tiny 1-3 word documents seem especially likely to get somewhat peculiar/extreme inferred-vectors, especially if the model/training-data was underpowered to begin with.) </p>

<p>Beware that unknown words are ignored by <code>infer_vector()</code>, so if you feed it a 3-word documents for which two words are unknown, it's really just inferring based on the one known word. And if you feed it only unknown words, it will return a random, mild initialization vector that's undergone no inference tuning. (All inference/training always starts with such a random vector, and if there are no known words, you just get that back.)</p>

<p>Still, this may be worth trying, and you can directly compare via cosine-similarity the inferred vectors from tiny and giant documents alike. </p>

<p>Many <code>Doc2Vec</code> modes train both doc-vectors and compatible word-vectors. The default PV-DM mode (<code>dm=1</code>) does this, or PV-DBOW (<code>dm=0</code>) if you add the optional interleaved word-vector training (<code>dbow_words=1</code>). (If you use <code>dm=0, dbow_words=0</code>, you'll get fast training, and often quite-good doc-vectors, but the word-vectors won't have been trained at all - so you wouldn't want to look up such a model's word-vectors directly for any purposes.)</p>

<p>With such a <code>Doc2Vec</code> model that includes valid word-vectors, you could also analyze your short 1-3 word docs via their individual words' vectors. You might check each word individually against a full document's vector, or use the average of the short document's words against a full document's vector. </p>

<p>Again, which is best will likely depend on other particulars of your need. For example, if the short doc is a query, and you're listing multiple results, it may be the case that query result variety – via showing some hits that are really close to single words in the query, even when not close to the full query – is as valuable to users as documents close to the full query. </p>

<p>Another measure worth looking at is ""Word Mover's Distance"", which works just with the word-vectors for a text's words, as if they were ""piles of meaning"" for longer texts. It's a bit like the word-against-every-word approach you entertained – but working to match words with their nearest analogues in a comparison text. It can be quite expensive to calculate (especially on longer texts) – but can sometimes give impressive results in correlating alternate texts that use varied words to similar effect.</p>
",3,1,492,2019-04-13 11:57:31,https://stackoverflow.com/questions/55665180/how-do-i-calculate-the-similarity-of-a-word-or-couple-of-words-compared-to-a-doc
How to apply a sentence-level LDA model using Gensim?,"<p>Is it possible to apply a sentence-level LDA model using Gensim as proposed in Bao and Datta(2014)?  The paper <a href=""https://pubsonline.informs.org/doi/pdf/10.1287/mnsc.2014.1930"" rel=""nofollow noreferrer"">is here</a>.  </p>

<p>The distinct feature is that it makes the ""one topic per sentence assumption"" (p.1376). This is different from other sentence-level methods, which typically allow each sentence to include multiple topics. ""The most straightforward method is to treat each sentence as a document and apply the LDA model on the collection of sentences rather than documents."" (P.1376). But, I think it is more reasonable to assume that one sentence deals with one topic. </p>

<p>Thank you!</p>
","python, nlp, gensim, lda","<p>You can run what Brody &amp; Elhadad (2010) call <a href=""http://delivery.acm.org/10.1145/1860000/1858121/p804-brody.pdf?ip=212.184.196.254&amp;id=1858121&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;__acm__=1555353462_3ef0068d0053c86e7a68410f74f9a194"" rel=""nofollow noreferrer"">local-LDA</a> - just feeding your text data to LDA sentence by sentence - easily, if you split your documents into sentences. However, LDA will still give you more than one topic per sentence (by definition, you get values for all topics, although gensim has the <code>minimum_probabiliy</code> default of 0.01), which of course is not the same as the approach proposed by Bao &amp; Datta.</p>

<p>However, the <a href=""https://pubsonline.informs.org/doi/suppl/10.1287/mnsc.2014.1930"" rel=""nofollow noreferrer"">supplemental material</a> to the article by Bao &amp; Datta (2014) contains a C or C++ (I assume, it doesn't say in the readme) <code>.exe</code> plus usage instructions in the materials. You could just run that from the command line, or write a <a href=""https://intermediate-and-advanced-software-carpentry.readthedocs.io/en/latest/c++-wrapping.html"" rel=""nofollow noreferrer"">wrapper for Python</a> (to make the output in gensim format would be icing on the cake) - if you do, please share your code, it might be helpful to others.</p>
",3,0,1366,2019-04-15 15:36:47,https://stackoverflow.com/questions/55692700/how-to-apply-a-sentence-level-lda-model-using-gensim
Manage KeyError with gensim and pretrained word2vec model,"<p>I pretrained a word embedding using wang2vec (<a href=""https://github.com/wlin12/wang2vec"" rel=""nofollow noreferrer"">https://github.com/wlin12/wang2vec</a>), and i loaded it in python through gensim. When i tried to get the vector of some words not in vocabulary, i obviously get:</p>

<pre class=""lang-py prettyprint-override""><code>KeyError: ""word 'kjklk' not in vocabulary""
</code></pre>

<p>So, i thought about adding an item to the vocabulary to map oov (out of vocabulary) words, let's say <code>&lt;OOV&gt;</code>. Since the vocabulary is in <code>Dict</code> format, i would simply add the item <code>{""&lt;OOV&gt;"":0}</code>. </p>

<p>But, i searched an item of the vocabulary, with</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.KeyedVectors.load_word2vec_format(w2v_ext, binary=False, unicode_errors='ignore')
dict(list(model.vocab.items())[5:6])
</code></pre>

<p>The output was something like</p>

<pre class=""lang-py prettyprint-override""><code>{'word': &lt;gensim.models.keyedvectors.Vocab at 0x7fc5aa6007b8&gt;}
</code></pre>

<p>So, is there a way to add the <code>&lt;OOV&gt;</code> token to the vocabulary of a pretrained word embedding loaded through gensim, and avoid the KeyError? I looked at gensim doc and i found this: <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.build_vocab"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.build_vocab</a>
but it seems not work with the update parameter.</p>
","python, nlp, gensim","<p>Adding a synthetic <code>'&lt;OOV&gt;'</code> token would just let you look up that token, like <code>model['&lt;OOV&gt;']</code>.The model would still give key errors for absent keys like <code>'kjklk'</code>.</p>

<p>There's no built-in support for adding any such 'catch-all' mapping. Often, ignoring unknown tokens is better than using some plug value (such as a zero-vector or random-vector). </p>

<p>It's fairly idiomatic in Python to explicitly check if a key is present, via the <code>in</code> keyword, if you want to do something different for absent keys. For example: </p>

<pre><code>vector = model['kjklk'] if 'kjklk' in model else DEFAULT_VECTOR
</code></pre>

<p>(Notably, the <code>*expr1* if *expr2* else *expr3*</code> defers evaluation of the initial <em>expr1</em>, avoiding <code>KeyError</code>.)</p>

<p>Python also has the <code>defaultdict</code> variant dictionary, which can have a default value returned for any unknown key. See:</p>

<p><a href=""https://docs.python.org/3.7/library/collections.html#collections.defaultdict"" rel=""nofollow noreferrer"">https://docs.python.org/3.7/library/collections.html#collections.defaultdict</a></p>

<p>It'd be possible to try replacing the <code>KeyedVectors</code> <code>vocab</code> dictionary with one of those, if the behavior is really important, but there could be side effects on other code.</p>
",1,0,1477,2019-04-15 16:45:59,https://stackoverflow.com/questions/55693826/manage-keyerror-with-gensim-and-pretrained-word2vec-model
how to speed up gensim word2vec initialization with pre proccessed corpus?,"<p>i am training multiple word2vec models on the same corpus. (i am doing this to study the variation in learned word vectors)</p>

<p>i am using this tutorial as reference: <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">https://rare-technologies.com/word2vec-tutorial/</a></p>

<p>it is suggested that by default gensim.models.word2vec will iterate over the corpus at least twice. once for initialization and then again for training (iterating the number of epochs specified) </p>

<p>since i am always using the same corpus, i want to save time by initializing only once, and providing the same initialization as input to all successive models.</p>

<p>how can this be done?</p>

<p>this is my current setting:</p>

<pre class=""lang-py prettyprint-override""><code>subdirectory = 'corpus_directory'
for i in range(10):
    sentences = MySentences(subdirectory) # a memory-friendly iterator
    model = gensim.models.Word2Vec(sentences, min_count=20, size=100, workers=4)
    model.train(sentences, total_examples=model.corpus_count, epochs=1)
    word_vectors = model.wv
    fname = 'WV{}.kv'
    word_vectors.save(fname.format(i))
</code></pre>

<p>where MySentences is defined similarly to the tutorial:
(i made a slight change, so the order of corpus sentences would be shuffled with each initialization)</p>

<pre class=""lang-py prettyprint-override""><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname
        self.file_list = [fname for fname in os.listdir(dirname) if fname.endswith('.txt')]
        random.shuffle(self.file_list)

    def __iter__(self):
        for article in self.file_list:
            for line in open(os.path.join(self.dirname, article)):
                yield line.split()
</code></pre>
","gensim, word2vec","<p>If you supply a corpus of <code>sentences</code> to the class-instantiation, as your code has done, you don't need to call <code>train()</code>. It will already have done that automatically, and your second <code>train()</code> is redundant. (I recommend doing all such operations with logging enabled at the INFO level, and review the lgos after each run to understand what is happening – things like two full start-to-finish trainings should stick out in the logs.)</p>

<p>The case where you would call <code>train()</code> explicitly is if you want more control over the interim steps. You leave the <code>sentences</code> out of the class-instantiation, but then it is required for you to perform two explicit steps: both one call to <code>build_vocab()</code> (for initial vocabulary scan) and then one call to <code>train()</code> (for actual multi-epoch training). </p>

<p>In that case, you can use gensim's native <code>.save()</code> to save the model <em>after</em> the vocabulary-discovery, to have a model that's ready for re-training and doesn't need to report that step. </p>

<p>So, you could re-load that vocabulary-built model multiple times, to different variables, to train in different ways. For some of the model's meta-parameters – like <code>window</code> or even <code>dm</code> mode – you can even tamper directly with their values on a model after vocabulary-building to try different variants. </p>

<p>However, if there are any changes to the corpus's words/word-frequencies, or to other parameters that affect the initialization that happens during <code>build_vocab()</code> (like vector <code>size</code>), then the initialization will be out of sync with the configuration you're trying, and you could get strange errors. </p>

<p>In such a case, the best course is to repeat the <code>build_vocab()</code> step entirely. (You could also look into the source code to see the individual steps performed by <code>build_vocab()</code>, and just patch/repeat the initialization steps that are needed, but that requires strong familiarity with the code.)</p>
",1,0,461,2019-04-16 10:07:28,https://stackoverflow.com/questions/55705634/how-to-speed-up-gensim-word2vec-initialization-with-pre-proccessed-corpus
Dealing with new words in gensim not found in model,"<p>Lets say I am trying to compute the average distance between a word and a document using distances() or compute cosine similarity between two documents using n_similarity(). However, lets say these new documents contain words that the original model did not. How does gensim deal with that?</p>

<p>I have been reading through the documentation and cannot find what gensim does with unfound words.</p>

<p>I would prefer gensim to not count those in towards the average. So, in the case of distances(), it should simply not return anything or something I can easily delete later before I compute the mean using numpy. In the case of n_similarity, gensim of course has to do it by itself....</p>

<p>I am asking because the documents and words that my program will have to classify will in some instances contain unknown words, names, brands etc that I do not want to be taken into consideration during classification. So, I want to know if I'll have to preprocess every document that I am trying to classify. </p>
","python, nlp, gensim","<p>Depending on the context, Gensim will usually either <em>ignore</em> unknown words, or throw an error like <code>KeyError</code> when an exact-word lookup fails. (Also, some word-vector models, like <code>FastText</code>, can synthesize better-than-nothing guesswork vectors for unknown words based on word-fragments observed during training.)</p>

<p>You should try your desired operations with the specific models/method of interest to observe the results.</p>

<p>If operation-interrupting errors are thrown and a problem for your code, you could pre-filter your lists-of-words to remove those not also present in the model. </p>
",2,1,278,2019-04-16 14:48:12,https://stackoverflow.com/questions/55710967/dealing-with-new-words-in-gensim-not-found-in-model
How to load a word2vec txt file with vocabulary constraint,"<p>I have a word2vec file in the standard format, but it is huge with 2M items. I also have a vocabulary file where each row is a word, the file has about ~800K rows. Now I want to load the embeddings from the word2vec file, and I want only embeddings for words in the vocabulary file. Is there an efficient implementation in gensim?</p>
",gensim,"<p>There's no built-in support for filtering the words on load. But you could use the code for the <code>load_word2vec_format()</code> function as a model for your own alternate loading code that skips words not-of-interest. </p>

<p>You can view the code for that function in the <code>KeyedVectors</code> class...</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/ff107d6c5cb50d9ab99999cb898ff0aceb192592/gensim/models/keyedvectors.py#L1434"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/ff107d6c5cb50d9ab99999cb898ff0aceb192592/gensim/models/keyedvectors.py#L1434</a></p>

<p>...and some shared support functions...</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/ff107d6c5cb50d9ab99999cb898ff0aceb192592/gensim/models/utils_any2vec.py#L294"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/ff107d6c5cb50d9ab99999cb898ff0aceb192592/gensim/models/utils_any2vec.py#L294</a></p>
",0,0,792,2019-04-18 17:25:33,https://stackoverflow.com/questions/55751027/how-to-load-a-word2vec-txt-file-with-vocabulary-constraint
How to train a model that will result in the similarity score between two news titles?,"<p>I am trying to build a Fake news classifier and I am quite new in this field. I have a column ""title_1_en"" which has the title for fake news and another column called ""title_2_en"". There are 3 target labels; ""agreed"", ""disagreed"", and ""unrelated"" if the title of the news in column ""title_2_en"" agrees, disagrees or is unrelated to that in the first column. </p>

<p>I have tried calculating basic cosine similarity between the two titles after converting the words of the sentences into vectors. This has resulted in the the cosine similarity score but this needs a lot of improvement as synonyms and semantic relationship has not been considered at all. </p>

<pre><code>def L2(vector):
    norm_value = np.linalg.norm(vector)
    return norm_value

def Cosine(fr1, fr2):
    cos = np.dot(fr1, fr2)/(L2(fr1)*L2(fr2))
    return cos
</code></pre>
","nlp, classification, gensim, cosine-similarity, sentence-similarity","<p>The most important thing here is how you convert the two sentences into vectors. There are multiple ways to do that and the most naive way is:</p>

<ul>
<li>Convert each and every word into a vector - this can be done using standard pre-trained vectors such as word2vec or GloVe.</li>
<li>Now every sentence is just a bag of word vectors. This needs to be converted into a single vector, ie., mapping a full sentence text to a vector. There are many ways to do this too. For a start, just take the average of the bag of vectors in the sentence. </li>
<li>Compute cosine similarity between the two sentence vectors.</li>
</ul>

<p><a href=""https://explosion.ai/demos/similarity?text1=Here%27s%20a%20sentence!&amp;text2=A%20text%20that%27s%20kinda%20different..."" rel=""nofollow noreferrer"">Spacy's similarity</a>  is a good place to start which does the averaging technique. From the docs:</p>

<blockquote>
  <p>By default, spaCy uses an average-of-vectors algorithm, using
  pre-trained vectors if available (e.g. the en_core_web_lg model). If
  not, the doc.tensor attribute is used, which is produced by the
  tagger, parser and entity recognizer. This is how the en_core_web_sm
  model provides similarities. Usually the .tensor-based similarities
  will be more structural, while the word vector similarities will be
  more topical. You can also customize the .similarity() method, to
  provide your own similarity function, which can be trained using
  supervised techniques.</p>
</blockquote>
",1,1,1270,2019-04-19 02:46:54,https://stackoverflow.com/questions/55755962/how-to-train-a-model-that-will-result-in-the-similarity-score-between-two-news-t
Where to find a pretrained doc2vec model on Wikipedia or large article dataset like Google news?,"<p>Am struggling with training wikipedia dump on doc2vec model, not experienced in setting up a server as a local machine is out of question due to the ram it requires to do the training. I couldnt find a pre trained model except outdated copies for python 2.</p>
","python, nlp, gensim, word2vec, doc2vec","<p>I'm not aware of any publicly-available standard gensim <code>Doc2Vec</code> models trained on Wikipedia. </p>
",1,0,283,2019-04-19 05:02:37,https://stackoverflow.com/questions/55756841/where-to-find-a-pretrained-doc2vec-model-on-wikipedia-or-large-article-dataset-l
Load a part of Glove vectors with gensim,"<p>I have a word list like<code>['like','Python']</code>and I want to load pre-trained Glove word vectors of these words, but the Glove file is too large, is there any fast way to do it? </p>

<p><strong>What I tried</strong></p>

<p>I iterated through each line of the file to see if the word is in the list and add it to a dict if True. But this method is a little slow.</p>

<pre><code>def readWordEmbeddingVector(Wrd):
    f = open('glove.twitter.27B/glove.twitter.27B.200d.txt','r')
    words = []
    a = f.readline()
    while a!= '':
        vector = a.split()
        if vector[0] in Wrd:
            words.append(vector)
            Wrd.remove(vector[0])
        a = f.readline()
    f.close()
    words_vector = pd.DataFrame(words).set_index(0).astype('float')
    return words_vector
</code></pre>

<p>I also tried below, but it loaded the whole file instead of vectors I need</p>

<pre class=""lang-py prettyprint-override""><code>gensim.models.keyedvectors.KeyedVectors.load_word2vec_format('word2vec.twitter.27B.200d.txt')
</code></pre>

<p><strong>What I want</strong></p>

<p>Method like <code>gensim.models.keyedvectors.KeyedVectors.load_word2vec_format</code> but I can set a word list to load.</p>
","python, gensim, word-embedding, glove","<p>There's no existing <code>gensim</code> support for filtering the words loaded via <code>load_word2vec_format()</code>. The closest is an optional <code>limit</code> parameter, which can be used to limit how many word-vectors are read (ignoring all subsequent vectors). </p>

<p>You could conceivably create your own routine to perform such filtering, using the source code for <code>load_word2vec_format()</code> as a model. As a practical matter, you might have to read the file twice: 1st, to find out exactly how many words in the file intersect with your set-of-words-of-interest (so you can allocate the right-sized array without trusting the declared size at the front of the file), then a second time to actually read the words-of-interest.</p>
",0,0,955,2019-04-19 15:25:32,https://stackoverflow.com/questions/55764137/load-a-part-of-glove-vectors-with-gensim
Gensim&#39;s Word2Vec not training provided documents,"<p>I'm facing a Gensim training problem using Word2Vec. 
model.wv.vocab is not getting any further word from the trained corpus 
the only words in are from the ones from initialization instruction ! </p>

<p>In fact, after many times trying on my own code, even the official site's example didn't work !  </p>

<p>I tried saving model at many spots in my code 
I even tried saving and reloading the corpus alongside train instruction</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec

path = get_tmpfile(""word2vec.model"")

model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
model.save(""word2vec.model"")

print(len(model.wv.vocab))

model.train([[""hello"", ""world""]], total_examples=1, epochs=1)
model.save(""word2vec.model"")

print(len(model.wv.vocab))

</code></pre>

<p>first print statement gives 12 which is right </p>

<p>second 12 when it's supposed to give 14 (len(vocab + 'hello' + 'world'))</p>
","python-3.x, gensim, google-colaboratory","<p>Additional calls to <code>train()</code> don't expand the known vocabulary. So, there is no way that the value of <code>len(model.wv.vocab)</code> will change after another call to <code>train()</code>. (Either 'hello' and 'world' are already known to the model, in which case they were in the original count of 12, or they weren't known, in which case they were ignored.)</p>

<p>The vocabulary is only established during a specific <code>build_vocab()</code> phase, which happens automatically if, as your code shows, you supplied a training corpus (<code>common_texts</code>) in model instantiation.</p>

<p>You can use a call to <code>build_vocab()</code> with the optional added parameter <code>update=True</code> to incrementally update a model's vocabulary, but this is best considered an advanced/experimental technique that introduces added complexities. (Whether such vocab-expansion, and then followup incremental training, actually helps or hurts will depend on getting a lot of other murky choices about <code>alpha</code>, <code>epochs</code>, corpus-sizing, training modes, and corpus-contents correct.)</p>
",2,1,681,2019-04-20 13:54:20,https://stackoverflow.com/questions/55774197/gensims-word2vec-not-training-provided-documents
How to predict test data on Gensim Topic modelling,"<p>I have used Gensim LDAMallet for topic modelling but in what way we can predict sample paragraph and get their topic model using pretrained model.</p>

<pre><code># Build the bigram and trigram models
bigram = gensim.models.Phrases(t_preprocess(dataset.data), min_count=5, threshold=100)
bigram_mod = gensim.models.phrases.Phraser(bigram) 

def make_bigrams(texts):
   return [bigram_mod[doc] for doc in texts]

data_words_bigrams = make_bigrams(t_preprocess(dataset.data))

# Create Dictionary
id2word = corpora.Dictionary(data_words_bigrams)

# Create Corpus
texts = data_words_bigrams

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

mallet_path='/home/riteshjain/anaconda3/mallet/mallet2.0.8/bin/mallet' 
ldamallet = gensim.models.wrappers.LdaMallet(mallet_path,corpus=corpus, num_topics=12, id2word=id2word, random_seed = 0)

coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=texts, dictionary=id2word, coherence='c_v')

a = ""When Honda builds a hybrid, you've got to be sure it‚Äôs a marvel. And an Accord Hybrid is when technology surpasses the known and takes a leap of faith into tomorrow. This is the next generation Accord, the ninth generation to be precise.""
</code></pre>

<p>How to use this text (a) to get its topic from the pretrained model. Please help.</p>
","python, jupyter-notebook, gensim, topic-modeling, mallet","<p>You're going to want to process 'a' similarly to the trained set:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code># import a new data set to be passed through the pre-trained LDA

data_new = pd.read_csv('YourNew.csv', encoding = ""ISO-8859-1"");
data_new = data_new.dropna()
data_text_new = data_new[['Your Target Column']]
data_text_new['index'] = data_text_new.index

documents_new = data_text_new

# process the new data set through the lemmatization, and stopwork functions

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
            nltk.bigrams(token)
            result.append(lemmatize_stemming(token))
    return result

processed_docs_new = documents_new['Your Target Column'].map(preprocess)

# create a dictionary of individual words and filter the dictionary
dictionary_new = gensim.corpora.Dictionary(processed_docs_new[:])
dictionary_new.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)

# define the bow_corpus
bow_corpus_new = [dictionary_new.doc2bow(doc) for doc in processed_docs_new]</code></pre>
</div>
</div>
</p>

<p>Then you can just pass it through as a function:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>a = ldamallet[bow_corpus_new[:len(bow_corpus_new)]]
b = data_text_new

topic_0=[]
topic_1=[]
topic_2=[]

for i in a:
    topic_0.append(i[0][1])
    topic_1.append(i[1][1])
    topic_2.append(i[2][1])
    
d = {'Your Target Column': b['Your Target Column'].tolist(),
     'topic_0': topic_0,
     'topic_1': topic_1,
     'topic_2': topic_2}
     
df = pd.DataFrame(data=d)
df.to_csv(""YourAllocated.csv"", index=True, mode = 'a')</code></pre>
</div>
</div>
</p>

<p>I hope this helps :)</p>
",2,1,1452,2019-04-22 05:19:09,https://stackoverflow.com/questions/55789477/how-to-predict-test-data-on-gensim-topic-modelling
Making Gensim FAST_VERSION work on Windows 10 (Python 3.6),"<p>I am running Gensim with Python 3.6 on Windows 10. I have tried installing Visual Studio 2019 and MinGW (through TDM-GCC). I have uninstalled and reinstalled Gensim after both installations. I also did that after uninstalling and reinstalling Cython. </p>

<p>Regardless, it's not able to run the C extension, so I am stuck with the slower Numpy code.</p>

<p>I am not sure where the trouble lies, and I have run out of ideas on how to proceed. What can I do to make progress?</p>
","python-3.x, gensim","<p>If you have the option of using a Linux/UNIX-based OS, much of the Python/scientific stack is going to be easier to get working there, as the bulk of development, testing, and use happens on non-Windows OSes. </p>

<p>Using a package manager that's well-maintained for Windows OSes, like <code>conda</code>, may succeed in installing highly-optimized versions of <code>gensim</code> (&amp; other packages), where default instructions &amp; local tinkering does not. In some cases, this may be because it is able to use trusted, pre-compiled binaries/libraries. </p>

<p>In some cases, pre-compiled Python ""Wheel"" (<code>.whl</code>) packages may help. I can't vouch for these, but there appear to be a set of these at:</p>

<p><a href=""https://www.lfd.uci.edu/~gohlke/pythonlibs/#gensim"" rel=""nofollow noreferrer"">https://www.lfd.uci.edu/~gohlke/pythonlibs/#gensim</a></p>

<p>(This <a href=""https://stackoverflow.com/a/45613490/130288"">other SO answer</a>, which I also can't vouch for, may help guid a wheels-based installation.)</p>

<p>If you absolutely need to build locally, for example because you are customizing gensim's optimized code, debugging the issue would require paying close attention to the individual errors reported during installation and researching/addressing each one-by-one. </p>
",1,3,759,2019-04-23 13:38:57,https://stackoverflow.com/questions/55812580/making-gensim-fast-version-work-on-windows-10-python-3-6
How to input a series/list consisting of different tokens in a Gensim Dictionary?,"<p>I hava a pandas dataframe that has one column with conversational data. I preprocessed it in the following way:</p>

<pre><code>def preprocessing(text):
     return [word for word in simple_preprocess(str(text), min_len = 2, deacc = True) if word not in stop_words]

dataset['preprocessed'] = dataset.apply(lambda row: preprocessing(row['msgText']), axis = 1)
</code></pre>

<p>To make it one-dimensional I used (both):</p>

<pre><code>processed_docs = data['preprocessed']
</code></pre>

<p>as well as:</p>

<pre><code>processed_docs = data['preprocessed'].tolist()
</code></pre>

<p>Which now looks as follows:</p>

<pre><code>&gt;&gt;&gt; processed_docs[:2]
0    ['klinkt', 'alsof', 'zwaar', 'dingen', 'spelen...
1    ['waar', 'liefst', 'meedenk', 'betekenen', 'pe...
</code></pre>

<p>For both cases, I used: </p>

<pre><code>dictionary = gensim.corpora.Dictionary(processed_docs)     
</code></pre>

<p>However, in both cases I got the error:</p>

<pre><code>TypeError: doc2bow expects an array of unicode tokens on input, not a single string
</code></pre>

<p>How can I modify my data, so that I don't get this TypeError?</p>

<hr>

<hr>

<p>Given that similar questions have been asked before, I've considered:</p>

<p><a href=""https://stackoverflow.com/questions/33229360/gensim-typeerror-doc2bow-expects-an-array-of-unicode-tokens-on-input-not-a-si#33230682"">Gensim: TypeError: doc2bow expects an array of unicode tokens on input, not a single string</a></p>

<p>Based on the first answer, I tried the solution of:</p>

<pre><code>dictionary = gensim.corpora.Dictionary([processed_docs.split()])
</code></pre>

<p>And got the error(/s):</p>

<pre><code>AttributeError: 'Series'('List') object has no attribute 'split'
</code></pre>

<p>And in the second answer someone says that the input needs to be tokens, which already holds for me. </p>

<p>Furthermore, based on (<a href=""https://stackoverflow.com/questions/44352552/typeerror-doc2bow-expects-an-array-of-unicode-tokens-on-input-not-a-single-str?noredirect=1&amp;lq=1"">TypeError: doc2bow expects an array of unicode tokens on input, not a single string when using gensim.corpora.Dictionary()</a>), I used the <code>.tolist()</code> approach as I described above, which does not work either.</p>
","python, dictionary, nlp, typeerror, gensim","<p>I think you need:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>dictionary = gensim.corpora.Dictionary([processed_docs[:]])</code></pre>
</div>
</div>
</p>

<p>To iterate through the set.  You can write [2:] to start at two and iterate to the end or [:7] to start at 0 then go to 7 or [2:7].  You can also try [:len(processed_docs)]</p>

<p>I hope this helps :)</p>
",1,0,1013,2019-04-23 14:37:45,https://stackoverflow.com/questions/55813659/how-to-input-a-series-list-consisting-of-different-tokens-in-a-gensim-dictionary
Gensim most_similar() with Fasttext word vectors return useless/meaningless words,"<p>I'm using Gensim with <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">Fasttext Word vectors</a> for return similar words.</p>

<p>This is my code:</p>

<pre><code>import gensim

model = gensim.models.KeyedVectors.load_word2vec_format('cc.it.300.vec')

words = model.most_similar(positive=['sole'],topn=10)

print(words)
</code></pre>

<p>This will return:</p>

<pre><code>[('sole.', 0.6860659122467041), ('sole.Ma', 0.6750558614730835), ('sole.Il', 0.6727924942970276), ('sole.E', 0.6680260896682739), ('sole.A', 0.6419174075126648), ('sole.È', 0.6401025652885437), ('splende', 0.6336565613746643), ('sole.La', 0.6049465537071228), ('sole.I', 0.5922051668167114), ('sole.Un', 0.5904430150985718)]
</code></pre>

<p>The problem is that ""sole"" (""sun"", in english) return a series of words with a dot in it (like sole., sole.Ma, ecc...). Where is the problem? Why most_similar return this meaningless word?</p>

<p><strong>EDIT</strong></p>

<p>I tried with <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">english word vector</a> and the word ""sun"" return this:</p>

<pre><code>[('sunlight', 0.6970556974411011), ('sunshine', 0.6911839246749878), ('sun.', 0.6835992336273193), ('sun-', 0.6780728101730347), ('suns', 0.6730450391769409), ('moon', 0.6499731540679932), ('solar', 0.6437565088272095), ('rays', 0.6423950791358948), ('shade', 0.6366724371910095), ('sunrays', 0.6306195259094238)] 
</code></pre>

<p>Is it impossible to reproduce results like relatedwords.org?</p>
","gensim, fasttext","<p>Perhaps the bigger question is: why does the Facebook FastText <code>cc.it.300.vec</code> model include so many meaningless words? (I haven't noticed that before – is there any chance you've downloaded a peculiar model that has decorated words with extra analytical markup?)</p>

<p>To gain the unique benefits of FastText – including the ability to synthesize plausible (better-than-nothing) vectors for out-of-vocabulary words – you may not want to use the general <code>load_word2vec_format()</code> on the plain-text <code>.vec</code> file, but rather a Facebook-FastText specific load method on the <code>.bin</code> file. See:</p>

<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors</a></p>

<p>(I'm not sure that will help with these results, but if choosing to use FastText, you may be interesting it using it ""fully"".)</p>

<p>Finally, given the source of this training – common-crawl text from the open web, which may contain lots of typos/junk – these might be legimate word-like tokens, essentially typos of <code>sole</code>, that appear often enough in the training data to get word-vectors. (And because they really are typo-synonyms for 'sole', they're not necessarily bad results for all purposes, just for your desired purpose of only seeing ""real-ish"" words.) </p>

<p>You might find it helpful to try using the <code>restrict_vocab</code> argument of <code>most_similar()</code>, to only receive results from the leading (most-frequent) part of all known word-vectors. For example, to only get results from among the top 50000 words:</p>

<pre><code>words = model.most_similar(positive=['sole'], topn=10, restrict_vocab=50000)
</code></pre>

<p>Picking the right value for <code>restrict_vocab</code> might help in practice to leave out long-tail 'junk' words, while still providing the real/common similar words you seek. </p>
",5,2,3427,2019-04-26 18:02:10,https://stackoverflow.com/questions/55872853/gensim-most-similar-with-fasttext-word-vectors-return-useless-meaningless-word
python gensim word2vec gives typeerror TypeError: object of type &#39;generator&#39; has no len() on custom dataclass,"<p>I am trying to get word2vec to work in python3, however as my dataset is too large to easily fit in memory I am loading it via an iterator (from zip files). However when I run it I get the error </p>

<pre><code>Traceback (most recent call last):
  File ""WordModel.py"", line 85, in &lt;module&gt;
    main()
  File ""WordModel.py"", line 15, in main
    word2vec = gensim.models.Word2Vec(data,workers=cpu_count())
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/word2vec.py"", line 783, in __init__
    fast_version=FAST_VERSION)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/base_any2vec.py"", line 759, in __init__
    self.build_vocab(sentences=sentences, corpus_file=corpus_file, trim_rule=trim_rule)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/base_any2vec.py"", line 936, in build_vocab
    sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/word2vec.py"", line 1591, in scan_vocab
    total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)
  File ""/home/thijser/.local/lib/python3.7/site-packages/gensim/models/word2vec.py"", line 1576, in _scan_vocab
    total_words += len(sentence)
TypeError: object of type 'generator' has no len()
</code></pre>

<p>Here is the code:</p>

<pre><code>import zipfile
import os
from ast import literal_eval

from lxml import etree
import io
import gensim

from multiprocessing import cpu_count


def main():
    data = TrainingData(""/media/thijser/Data/DataSets/uit2"")
    print(len(data))
    word2vec = gensim.models.Word2Vec(data,workers=cpu_count())
    word2vec.save('word2vec.save')




class TrainingData:

    size=-1

    def __init__(self, dirname):
        self.data_location = dirname

    def __len__(self):
        if self.size&lt;0: 

            for zipfile in self.get_zips_in_folder(self.data_location): 
                for text_file in self.get_files_names_from_zip(zipfile):
                    self.size=self.size+1
        return self.size            

    def __iter__(self): #might not fit in memory otherwise
        yield self.get_data()

    def get_data(self):


        for zipfile in self.get_zips_in_folder(self.data_location): 
            for text_file in self.get_files_names_from_zip(zipfile):
                yield self.preproccess_text(text_file)


    def stripXMLtags(self,text):

        tree=etree.parse(text)
        notags=etree.tostring(tree, encoding='utf8', method='text')
        return notags.decode(""utf-8"") 

    def remove_newline(self,text):
        text.replace(""\\n"","" "")
        return text

    def preproccess_text(self,text):
        text=self.stripXMLtags(text)
        text=self.remove_newline(text)

        return text




    def get_files_names_from_zip(self,zip_location):
        files=[]
        archive = zipfile.ZipFile(zip_location, 'r')

        for info in archive.infolist():
            files.append(archive.open(info.filename))

        return files

    def get_zips_in_folder(self,location):
       zip_files = []
       for root, dirs, files in os.walk(location):
            for name in files:
                if name.endswith(("".zip"")): 
                    filepath=root+""/""+name
                    zip_files.append(filepath)

       return zip_files

main()


for d in data:
    for dd in d :
        print(type(dd))
</code></pre>

<p>Does show me that dd is of the type string and contains the correct preprocessed strings (with length somewhere between 50 and 5000 words each). </p>
","python, machine-learning, nlp, gensim, training-data","<p><strong>Update after discussion:</strong></p>

<p>Your <code>TrainingData</code> class <code>__iter__()</code> function isn't providing a generator which returns each text in turn, but rather a generator which returns a single <em>other</em> generator. (There's one too many levels of <code>yield</code>.) That's not what <code>Word2Vec</code> is expecting. </p>

<p>Changing the body of your <code>__iter__()</code> method to simply...</p>

<pre><code>return self.get_data()
</code></pre>

<p>...so that <code>__iter__()</code> is a synonym for your <code>get_data()</code>, and just returns the same text-by-text generator that <code>get_data()</code> does, should help. </p>

<p><strong>Original answer:</strong></p>

<p>You're not showing the <code>TrainingData.preproccess_text()</code> (sic) method, referenced inside <code>get_data()</code>, which is what is actually creating the data <code>Word2Vec</code> is processing. And, it's that data that's generating the error. </p>

<p><code>Word2Vec</code> requires its <code>sentences</code> corpus be an <em>iterable sequence</em> (for which a generator would be appropriate) where each individual item is a <em>list-of-string-tokens</em>. </p>

<p>From that error, it looks like the individual items in your <code>TrainingData</code> sequence may themselves be generators, rather than lists with a readable <code>len()</code>. </p>

<p>(Separately, if perchance you're choosing to using generators there because the individual texts may be very very long, be aware that gensim <code>Word2Vec</code> and related classes only train on individual texts with a length up to 10000 word-tokens. Any words past the 10000th will be silently ignored. If that's a concern, your source texts should be pre-broken into individual texts of 10000 tokens or fewer.)</p>
",1,0,1511,2019-04-26 19:59:51,https://stackoverflow.com/questions/55874253/python-gensim-word2vec-gives-typeerror-typeerror-object-of-type-generator-has
Understanding gensim model inference output,"<p>I'm a newbie to <code>gensim</code> and trying to understand the <code>Word2Vec</code> model it generates.</p>

<p>Here is a simple example:- </p>

<pre><code>sentences = [['first', 'sentence', 'for', 'word2vec']]
model = Word2Vec(sentences, min_count=1)
print(model)
print(model['first'])
</code></pre>

<p>Output:- </p>

<pre><code>Word2Vec(vocab=4, size=100, alpha=0.025)

[-3.2170122e-03 -2.9626938e-03 -4.0412871e-03 -5.9279817e-04
  2.5436375e-03  4.5433347e-03 -3.3862963e-03 -4.2654946e-03
  3.8285875e-03  4.3016393e-03  2.3948429e-03  8.1989179e-05
  3.6110645e-03  1.8498371e-03 -2.4455690e-04  4.1978257e-03
  2.9471173e-04  4.9666679e-03 -2.0676558e-03 -1.2046038e-03
 -4.3298928e-03  2.7839688e-03 -2.9434622e-03  4.0511941e-03
 -1.3770841e-03 -8.9504482e-04 -3.1494466e-03 -4.6084630e-03
 -3.3623597e-03  1.6870942e-04 -7.1172835e-04 -4.1482532e-03
  3.7355758e-03  2.3343530e-03 -6.3678029e-04 -1.9861995e-03
 -2.3025211e-03  1.5102652e-03 -2.8942723e-03 -3.0406206e-03
 -7.7123288e-04 -2.1534185e-03  4.0353332e-03 -2.0982060e-03
 -5.1215116e-04 -4.9524521e-03  3.9109741e-03  3.6507500e-03
  5.0717179e-04 -1.2909769e-03  1.7484331e-03  1.8906737e-03
 -2.5824555e-03 -3.3213641e-03  1.3024095e-03  4.8507750e-03
  3.5359471e-03  4.5252368e-03  2.1690773e-03  3.8934432e-03
  4.8941034e-03 -4.3265051e-03  1.2478753e-03  4.8012529e-03
  3.6689214e-04 -3.5324714e-03 -8.2519173e-04  4.6989080e-03
 -4.3403171e-03 -3.2295308e-03 -4.3292320e-03  1.4541810e-03
  2.6360361e-03  4.7351457e-03 -1.1666205e-03  4.0232311e-03
  2.3259546e-03 -4.5906431e-03 -2.3466926e-03 -1.4690498e-03
  4.9304329e-03  3.4869314e-04  1.7118681e-03 -3.9177295e-03
 -1.9519962e-03  4.0137409e-03  1.6459676e-03 -2.6613632e-03
 -3.4537977e-03  1.0973522e-03  1.9739978e-03  4.3450715e-03
  2.8814776e-03 -4.9455655e-03 -1.4207339e-03 -2.8513866e-03
 -3.7962969e-03 -2.7314643e-03 -6.0791872e-04 -5.9866998e-04]
</code></pre>

<p>The size of the model is defaulted to 100, what does each item in the size array represent?</p>

<p>For example:- first element is  <code>-3.2170122e-03</code></p>
","nlp, gensim","<p>A word2vec model learns a vector embedding for each word in the vocabulary, which is created from the corpus given for the model. </p>

<p>Embedding size is a hyper parameter. Hence it is user's choice. To know more about the word2vec or vector representation of words read <a href=""https://www.tensorflow.org/tutorials/representation/word2vec"" rel=""nofollow noreferrer"">here</a>.</p>

<p>when you execute <code>model['first']</code>,  it returns the embedding of the word <code>first</code>, which by default will have 100 dimensions. </p>

<p>Each values does not have any specific meaning into it but as a complete vector it holds the information about a particular word. </p>
",1,1,109,2019-04-30 14:37:51,https://stackoverflow.com/questions/55923298/understanding-gensim-model-inference-output
Doc2Vec - Finding document similarity in test data,"<p>I am trying to train a doc2vec model using training data, then finding the similarity of every document in the <strong>test data</strong> for a specific document in the <strong>test data</strong> using the trained doc2vec model. However, I am unable to determine how to do this.</p>

<p>I currently using <code>model.docvecs.most_similar(...)</code>. However, this function only finds the similarity of every document in the <strong>training data</strong> for a specific document in the <strong>test data</strong>. </p>

<p>I have tried manually comparing the inferred vector of a specific document in the test data with the inferred vectors of every other document in the test data using <code>model.docvecs.n_similarity(inferred_vector.tolist(), testvectors[i].tolist())</code> but this returns <code>KeyError: ""tag '-0.3502606451511383' not seen in training corpus/invalid""</code> as there are vectors not in the dictionary.</p>
","python, machine-learning, gensim, doc2vec","<p>The act of training-up a <code>Doc2Vec</code> model leaves it with a record of the doc-vectors learned from the training data, and yes, <code>most_similar()</code> just looks among those vectors. </p>

<p>Generally, doing any operations on new documents that weren't part of training will require the use of <code>infer_vector()</code>. Note that such inference:</p>

<ul>
<li>ignores any unknown words in the new document</li>
<li>may benefit from parameter tuning, especially for short documents</li>
<li>is currently done just one document at time in a single thread – so, acquiring inferred-vectors for a large batch of N-thousand docs can actually be slower than training a fresh model on the same N-thousand docs</li>
<li>isn't necessarily deterministic, unless you take extra steps, because the underlying algorithms use random initialization and randomized selection processes during training/inference</li>
<li>just gives you the vector, without loading it into any convenient storage-object for performing further <code>most_similar()</code>-like comparisons</li>
</ul>

<p>On the other hand, such inference from a ""frozen"" model can be parallelized across processes or machines. </p>

<p>The <code>n_similarity()</code> method you mention isn't really appropriate for your needs: it's expecting lists of lookup-keys ('tags') for existing doc-vectors, <em>not</em> raw vectors like you're supplying.</p>

<p>The <code>similarity_unseen_docs()</code> method you mention in your answer is somewhat appropriate, but just takes a pair of docs, re-calculating their vectors each time – somewhat wasteful if a single new document's doc-vector needs to be compared against many other new documents' doc-vectors.  </p>

<p>You may just want to train an all-new model, with both your ""training documents"" and your ""test documents"". Then all the ""test documents"" get their doc-vectors calculated, and stored inside the model, as part of the bulk training. This is an appropriate choice for many possible applications, and indeed could learn interesting relationships based on words that only appear in the ""test docs"" in a totally unsupervised way. And there's not yet any part of your question that gives reasons why it couldn't be considered here.</p>

<p>Alternatively, you'd want to <code>infer_vector()</code> all the new ""test docs"", and put them into a structure like the various <code>KeyedVectors</code> utility classes in <code>gensim</code> - remembering all the vectors in one array, remembering the mapping from doc-key to vector-index, and providing an efficient bulk <code>most_similar()</code> over the set of vectors. </p>
",2,0,2337,2019-04-30 15:36:33,https://stackoverflow.com/questions/55924378/doc2vec-finding-document-similarity-in-test-data
Word vectors from a whole doc2vec model vs. word vectors from a particular document,"<p>I trained a gensim's Doc2Vec model with default word2vec training (dm=1). I can get the word vectors from the global model in model.wv.vectors.
But the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">documentation</a> says that the same word (""leaves"" in the example) won't have the same vector depending of the document context where it appear.</p>

<p>So I'm a bit confused : in the model.wv.vectors, will the word ""leaves"" by example, have the same vector for all the documents used to train the model (that may be contradictory with what I understand from the documentation) ? If not, how to get the word vectors from a particular document ?</p>
","gensim, word2vec, doc2vec","<p>That documentation is misleading. The word-token <code>'leaves'</code> will have only one word-vector in that model. </p>

<p>I'm guessing the author of that comment <em>may</em> have meant that during model-training in PV-DM mode (<code>dm=1</code>), the training-predictions would be influenced by a combination of the word-vector and the 'floating' doc-vector for that text (and other neighboring word-vectors within the context-window). But still, the one word just has the one vector, and the description there is confused.</p>
",0,0,275,2019-05-01 16:37:26,https://stackoverflow.com/questions/55939511/word-vectors-from-a-whole-doc2vec-model-vs-word-vectors-from-a-particular-docum
gensim Word2Vec - how to apply stochastic gradient descent?,"<p>To my understanding, batch (vanilla) gradient descent makes one parameter update for all training data. Stochastic gradient descent (SGD) allows you to update parameter for each training sample, helping the model to converge faster, at the cost of high fluctuation in function loss. </p>

<p><a href=""https://i.sstatic.net/0LGA5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0LGA5.png"" alt=""enter image description here""></a></p>

<p>Batch (vanilla) gradient descent sets <code>batch_size=corpus_size</code>.</p>

<p>SGD sets <code>batch_size=1</code>.</p>

<p>And mini-batch gradient descent sets <code>batch_size=k</code>, in which <code>k</code> is usually 32, 64, 128...</p>

<p>How does gensim apply SGD or mini-batch gradient descent? It seems that <code>batch_words</code> is the equivalent of <code>batch_size</code>, but I want to be sure. </p>

<p>Is setting <code>batch_words=1</code> in gensim model equivalent to applying SGD?</p>
","nlp, gensim, word2vec, gradient-descent, stochastic","<p>No, <code>batch_words</code> in <code>gensim</code> refers to the size of work-chunks sent to worker threads. </p>

<p>The <code>gensim</code> <code>Word2Vec</code> class updates model parameters after each training micro-example of <code>(context)-&gt;(target-word)</code> (where <code>context</code> might be a single word, as in skip-gram, or the mean of several words, as in CBOW). </p>

<p>For example, you can review this optimized <code>w2v_fast_sentence_sg_neg()</code> cython function for skip-gram with negative-sampling, deep in the <code>Word2Vec</code> training loop:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/460dc1cb9921817f71b40b412e11a6d413926472/gensim/models/word2vec_inner.pyx#L159"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/460dc1cb9921817f71b40b412e11a6d413926472/gensim/models/word2vec_inner.pyx#L159</a></p>

<p>Observe that it is considering exactly one target-word (<code>word_index</code> parameter) and one context-word (<code>word2_index</code>), and updating both the word-vectors (aka 'projection layer' <code>syn0</code>) and the model's hidden-to-output weights (<code>syn1neg</code>) before it might be called again with a subsequent single <code>(context)-&gt;(target-word)</code> pair. </p>
",0,0,1866,2019-05-02 11:10:59,https://stackoverflow.com/questions/55951158/gensim-word2vec-how-to-apply-stochastic-gradient-descent
Get all similar documents with doc2vec,"<p>I am actually working with <code>doc2vec</code> from gensim library and I want to get all similarities with probabilites not only the top 10 similarities provided by <code>model.docvecs.most_similar()</code></p>

<p>Once my model is trained </p>

<pre><code>In [1]: print(model)
Out [1]: Doc2vec(...)
</code></pre>

<p>If I use <code>model.docvecs.most_similar()</code> I get only the Top 10 similar docs </p>

<pre><code>In [2]: model.docvecs.most_similar('1')
Out [2]: [('2007', 0.9171321988105774),
 ('606', 0.5638039708137512),
 ('2578', 0.530228853225708),
 ('4506', 0.5193327069282532),
 ('2550', 0.5178008675575256),
 ('4620', 0.5098666548728943),
 ('1296', 0.5071642994880676),
 ('3943', 0.5070815086364746),
 ('438', 0.5057751536369324),
 ('1922', 0.5048809051513672)]
</code></pre>

<p>And I am looking to get all probilities not only the top 10 for some analysis.</p>

<p>Thanks for your help :)</p>
","python, gensim, doc2vec","<p><code>most_similar()</code> takes an optional <code>topn</code> parameter, with a default value of <code>10</code>, meaning just the top 10 results will be returned.</p>

<p>If you supply another integer, such as the total number of doc-vectors known to the model, then that many sorted results will be provided. </p>

<p>(You can also supply Python <code>None</code>, which returns all similarities <em>unsorted</em>, in the same order as the vectors are stored in the model.)</p>

<p>Note these values are <em>cosine similarities</em>, with a range of values from -1.0 to 1.0, <em>not</em> 'probabilities'.</p>
",1,-1,46,2019-05-07 11:24:55,https://stackoverflow.com/questions/56021542/get-all-similar-documents-with-doc2vec
LDA detect new emerging topics,"<p>Thanks for stopping by.  I have a directional question - I've built a Latent Dirichlet Allocation using Gensims Mallet wrapper.  I trained the model once on OldDataSet.csv and measured coherence.  I have been using it to pass NewDataSet.csv through for topic allocation.  I need some guidance on how I might be able to predict how accurately my pre-trained model is allocating NewDataSet.csv.  That coherence score only checks the accuracy of the pre-trained model not the allocated data set.  I'd like a way to  track the occurrence of historical topics and detect the emergence of new topics without re-training the model.  Like say these are the topics in OldDataSet.csv:</p>

<ol>
<li>whiskey</li>
<li>Tango</li>
<li>Foxtrot</li>
</ol>

<p>It will assign NewDataSet.csv 1. whiskey 2. Tango or 3. Foxtrot but a more accurate allocation might be:</p>

<ol>
<li>whiskey</li>
<li>Tango</li>
<li>Alpha</li>
</ol>

<p>If I keep running the same model I might miss this new topic.  If there exists a numeric score that would measure how closely the topics adhere to NewDataSet.csv that would be a huge time saver.  Thanks Stack you always save me :)</p>
","python, windows, machine-learning, gensim, lda","<p>I found a solution it is called dynamic topic modeling.  I've linked an article documenting its' use.  It is still undergoing research, but it's basically an LDA that takes time into account and can print topics change over time. </p>

<p><a href=""https://github.com/rare-technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb"" rel=""nofollow noreferrer"">https://github.com/rare-technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb</a></p>

<p>Also check out Bleis' google talk on the matter: </p>

<p><a href=""https://www.youtube.com/watch?v=7BMsuyBPx90"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=7BMsuyBPx90</a></p>
",1,1,257,2019-05-07 17:34:08,https://stackoverflow.com/questions/56027806/lda-detect-new-emerging-topics
Words missing from trained word2vec model vocabulary,"<p>I am currently working with python where I train a Word2Vec model using sentences that I provide. Then, I save and load the model to get the word embedding of each and every word in the sentences that were used to train the model. However, I get the following error.</p>

<blockquote>
  <p>KeyError: ""word 'n1985_chicago_bears' not in vocabulary""</p>
</blockquote>

<p>whereas, one of the sentences provided during training is as follows.</p>

<pre><code>sportsteam n1985_chicago_bears teamplaysincity city chicago
</code></pre>

<p>Hence I would like to know why some words are missing from the vocabulary, despite being trained on those words from that sentence corpus. </p>

<p><strong>Training the word2vec model on own corpus</strong></p>

<pre><code>import nltk
import numpy as np
from termcolor import colored
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from sklearn.decomposition import PCA


#PREPARING DATA

fname = '../data/sentences.txt'

with open(fname) as f:
    content = f.readlines()

# remove whitespace characters like `\n` at the end of each line
content = [x.strip() for x in content]


#TOKENIZING SENTENCES

sentences = []

for x in content:
    nltk_tokens = nltk.word_tokenize(x)
    sentences.append(nltk_tokens)

#TRAINING THE WORD2VEC MODEL

model = Word2Vec(sentences)

words = list(model.wv.vocab)
model.wv.save_word2vec_format('model.bin')
</code></pre>

<p><strong>Sample sentences from sentences.txt</strong></p>

<pre><code>sportsteam hawks teamplaysincity city atlanta
stadiumoreventvenue honda_center stadiumlocatedincity city anaheim
sportsteam ducks teamplaysincity city anaheim
sportsteam n1985_chicago_bears teamplaysincity city chicago
stadiumoreventvenue philips_arena stadiumlocatedincity city atlanta
stadiumoreventvenue united_center stadiumlocatedincity city chicago
...
</code></pre>

<p>There are 1860 such lines in the <code>sentences.txt</code> file, each containing exactly 5 words and no stop words.</p>

<p>After saving the model, I tried to load it from a different python file within the same directory as the saved <code>model.bin</code> as shown below.</p>

<p><strong>Loading the saved model.bin</strong></p>

<pre><code>import nltk
import numpy as np
from gensim import models

w = models.KeyedVectors.load_word2vec_format('model.bin', binary=True)
print(w['n1985_chicago_bears'])
</code></pre>

<p>However, I end up with the following error</p>

<pre><code>KeyError: ""word 'n1985_chicago_bears' not in vocabulary""
</code></pre>

<p>Is there a way to get the word embedding for each and every word in the trained sentence corpus using the same method?</p>

<p>Any suggestions in this regard will be much appreciated.</p>
","python, tensorflow, nltk, gensim, word2vec","<p>The default <code>min_count=5</code> for Word2Vec implementation of gensim and looks like the token you are looking for <code>n1985_chicago_bears</code> occurs less then 5 times in your corpus. Change your min count appropriately. </p>

<p><a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">Method signature:</a></p>

<blockquote>
  <p>class gensim.models.word2vec.Word2Vec(sentences=None,
  corpus_file=None, size=100, alpha=0.025, window=5, min_count=5,
  max_vocab_size=None, sample=0.001, seed=1, workers=3,
  min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75,
  cbow_mean=1, hashfxn=, iter=5, null_word=0,
  trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False,
  callbacks=(), max_final_vocab=None)</p>
</blockquote>

<pre><code>content = [
    ""sportsteam hawks teamplaysincity city atlanta"",
    ""stadiumoreventvenue honda_center stadiumlocatedincity city anaheim"",
    ""sportsteam ducks teamplaysincity city anaheim"",
    ""sportsteam n1985_chicago_bears teamplaysincity city chicago"",
    ""stadiumoreventvenue philips_arena stadiumlocatedincity city atlanta"",
    ""stadiumoreventvenue united_center stadiumlocatedincity city chicago""
]

sentences = []

for x in content:
    nltk_tokens = nltk.word_tokenize(x)
    sentences.append(nltk_tokens)

model = Word2Vec(sentences, min_count=1)
print (model['n1985_chicago_bears'])
</code></pre>
",5,4,4869,2019-05-08 04:40:58,https://stackoverflow.com/questions/56033651/words-missing-from-trained-word2vec-model-vocabulary
Mapping doc2vec paragraph representation to its class tag post-training,"<p>I have trained Doc2Vec paragraph embeddings on text documents using the <code>Doc2Vec</code> module in Python's <code>gensim</code> package. Normally each document is tagged with a unique ID, yielding a unique output representation, as follows (see <a href=""https://fzr72725.github.io/2018/01/14/genism-guide.html"" rel=""nofollow noreferrer"">this link</a> for details):</p>

<pre><code>def tag_docs(docs, col):
    tagged = docs.apply(lambda r: TaggedDocument(words=simple_preprocess(r[col]), tags=[r.label]), axis=1)
    return tagged
</code></pre>

<p>However, you can also tag a group of documents with the same tag in order to train class representations, which is what I did here. You can query the number of output representations with the following command:</p>

<pre><code>print(model.docvecs.count)
</code></pre>

<p>My question is as follows: I trained the model of <code>n</code> classes of documents, yielding <code>n</code> document vectors in <code>model.docvecs</code>. Now I want to map each document vector to the corresponding class tag. How can I establish which vector is associated with which tag?</p>
","python, gensim, word2vec, text-classification, doc2vec","<p>If <code>classA</code> was one of the document-tags you provided during training, then <code>model.docvecs['classA']</code> will return the single doc-vector that was learned for that tag from training. </p>

<p>If you have another new vector – for example one inferred on new text via <code>model.infer_vector(words)</code>, then you can get a list of which learned doc-vectors in the model are closest via <code>model.docvecs.most_similar(positive=[new_vector])</code>.</p>

<p>If your true aim to classify new documents into one (or more) of these classes, then taking the top <code>most_similar()</code> result is one crude way to do that. </p>

<p>But  having reduced all classes to just a single summary vector (the one vector learned for that tag), then taking just the one nearest-neighbor of a new-document, may not perform well. It somewhat forces an assumption that that classes are very simple shapes in the n-dimensional space. </p>

<p>For classification, you may want to let all documents get individual vectors (not based on their known classes, or in addition to their known classes), then train a separate classifier on that set of (doc-vector, label) labeled-data. That could discover finer-grained, and oddly-shaped boundaries between the classes. </p>
",1,0,329,2019-05-10 10:59:45,https://stackoverflow.com/questions/56076298/mapping-doc2vec-paragraph-representation-to-its-class-tag-post-training
Python gensim create word2vec model from vectors (in ndarray),"<p>I have a ndarray with words and their corresponding vector (with the size of 100 per word).
For example:</p>

<pre><code>Computer 0.11 0.41 ... 0.56
Ball     0.31 0.87 ... 0.32
</code></pre>

<p>And so on.</p>

<p>I want to create a word2vec model from it:</p>

<pre><code>model = load_from_ndarray(arr)
</code></pre>

<p>How can it be done? I saw </p>

<blockquote>
  <p>KeyedVectors</p>
</blockquote>

<p>but it only takes file and not array</p>
","python, nlp, gensim, word2vec","<pre><code>from gensim.models import KeyedVectors
words = myarray[:,0]
vectors = myarray[:,1:]
model = KeyedVectors(vectors.shape[1])
model.add(words, vectors)
</code></pre>

<p>if you want you can then save it</p>

<pre><code>model.save('mymodel')
</code></pre>

<p>and later just load it</p>

<pre><code>model = KeyedVectors.load('mymodel')
</code></pre>
",3,3,1879,2019-05-14 10:47:33,https://stackoverflow.com/questions/56128701/python-gensim-create-word2vec-model-from-vectors-in-ndarray
How to perform efficient queries with Gensim doc2vec?,"<p>I’m working on a sentence similarity algorithm with the following use case: given a new sentence, I want to retrieve its n most similar sentences from a given set. I am using Gensim v.3.7.1, and I have trained both word2vec and doc2vec models. The results of the latter outperform word2vec’s, but I’m having trouble performing efficient queries with my Doc2Vec model. This model uses the distributed bag of words implementation (dm = 0).</p>

<p>I used to infer similarity using the built in method <code>model.most_similar()</code>, but this was not possible once I started training with more data that the one I want to query against. That's to say, <strong>I want to find the most similar sentence among a subset of my training dataset</strong>. My quick fix to this was comparing the vector of the new sentence with every vector on my set using cosine similarity, but obviously this does not scale as I have to compute loads of embeddings and make a lot of comparisons.</p>

<p>I successfully use <a href=""https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.WmdSimilarity"" rel=""nofollow noreferrer"">word-mover distance</a> for both of word2vec and doc2vec, but I get better results for doc2vec when using cosine similarity. How can I efficiently query a new document against my set using the PV-DBOW Doc2Vec model and a method from <a href=""https://radimrehurek.com/gensim/similarities/docsim.html#how-it-works"" rel=""nofollow noreferrer"">class Similarity</a>?</p>

<p>I'm looking for a similar approach to what I do with WMD, but for doc2vec cosine similarity:</p>

<pre class=""lang-py prettyprint-override""><code># set_to_query contains ~10% of the training data + some future updates
set_to_query_tokenized = [sentence.split() for sentence in set_to_query]
w2v_model = gensim.models.Word2Vec.load(""my_w2v_model"")
w2v_to_query = gensim.similarities.WmdSimilarity(
               corpus = set_to_query_tokenized,
               w2v_model = w2v_model,
               num_best=10
              )
new_query = ""I want to find the most similar sentence to this one"".split()
most_similar = w2v_to_query[new_query]
</code></pre>
","python, gensim, similarity, doc2vec, sentence-similarity","<p>Creating your own subset of vectors, as a <code>KeyedVectors</code> instance, isn't quite as easy as it could or should be. </p>

<p>But, you should be able to use a <code>WordEmbeddingsKeyedVectors</code> (even though you're working with doc-vectors) that you load with just the vectors of interest. I haven't tested this, but assuming <code>d2v_model</code> is your <code>Doc2Vec</code> model, and <code>list_of_tags</code> are the tags you want in your subset, try something like:</p>

<pre><code>subset_vectors = WordEmbeddingsKeyedVectors(vector_size)
subset_vectors.add(list_of_tags, d2v_model.docvecs[list_of_tags])
</code></pre>

<p>Then you can perform the usual operations, like <code>most_similar()</code> on <code>subset_vectors</code>.</p>
",0,3,1216,2019-05-14 12:06:29,https://stackoverflow.com/questions/56130065/how-to-perform-efficient-queries-with-gensim-doc2vec
CalledProcessError: Returned non-zero exit status 1,"<p>When I try to run:</p>

<pre><code>def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod1[doc] for doc in texts]

# Remove Stop Words
data_words_nostops1 = remove_stopwords(data_words1)

# Form Bigrams
data_words_bigrams1 = make_bigrams(data_words_nostops1)    
# Create Dictionary
    id2word1 = corpora.Dictionary(data_words_bigrams1)

# Create Corpus
texts1 = data_words_bigrams1

# Term Document Frequency
corpus1 = [id2word1.doc2bow(text) for text in texts1]

mallet_path = 'T:Python/Mallet/mallet-2.0.8/bin/mallet'

ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus1, num_topics=15, id2word=id2word1)
</code></pre>

<p>I get the following error:</p>

<pre><code>CalledProcessError: Command 'T:/Python/Mallet/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\E26E5~1.RIJ\AppData\Local\Temp\3\a66fc0_corpus.txt --output C:\Users\E26E5~1.RIJ\AppData\Local\Temp\3\a66fc0_corpus.mallet' returned non-zero exit status 1.
</code></pre>

<p>What can I do in my code specifically to make it work? </p>

<p>Furthermore, the question on this error has been asked a few times before. However, each answer seems so specific to a particular case, that I don't see what I can change on my code now so that it will work. Can someone elaborate on the meaning of this problem?</p>
","python, gensim, lda, mallet","<p>Make sure you have:</p>

<ul>
<li>Java Developers Kit downloaded <a href=""https://www.oracle.com/java/technologies/javase-downloads.html"" rel=""nofollow noreferrer"">JDK</a></li>
<li>Mallet unzipped <a href=""http://mallet.cs.umass.edu/download.php"" rel=""nofollow noreferrer"">Mallet</a></li>
</ul>

<p>And have your <strong>env</strong> in the correct folder, otherwise update it e.g.: </p>

<ul>
<li>import os</li>
<li>os.environ.update({'MALLET_PATH':r'Python/Mallet/mallet-2.0.8/bin'})</li>
</ul>
",0,1,3383,2019-05-15 11:44:23,https://stackoverflow.com/questions/56148576/calledprocesserror-returned-non-zero-exit-status-1
Use Spacy to find most similar sentences in doc,"<p>I'm looking for a solution to use something like <code>most_similar()</code> from <code>Gensim</code> but using <code>Spacy</code>.
I want to find the most similar sentence in a list of sentences using NLP.</p>

<p>I tried to use <code>similarity()</code> from <code>Spacy</code> (e.g. <a href=""https://spacy.io/api/doc#similarity"" rel=""nofollow noreferrer"">https://spacy.io/api/doc#similarity</a>) one by one in loop, but it takes a very long time.</p>

<p>To go deeper :</p>

<p>I would like to put all these sentences in a graph (like <a href=""https://cdn-images-1.medium.com/max/1600/1*vvtIsW1AblmgLkq1peKfOg.png"" rel=""nofollow noreferrer"">this</a>) to find sentence clusters.</p>

<p>Any idea ?</p>
","gensim, similarity, spacy, doc2vec, sentence-similarity","<p>This is a simple, built-in solution you could use:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy

nlp = spacy.load(&quot;en_core_web_lg&quot;)
text = (
    &quot;Semantic similarity is a metric defined over a set of documents or terms, where the idea of distance between items is based on the likeness of their meaning or semantic content as opposed to lexicographical similarity.&quot;
    &quot; These are mathematical tools used to estimate the strength of the semantic relationship between units of language, concepts or instances, through a numerical description obtained according to the comparison of information supporting their meaning or describing their nature.&quot;
    &quot; The term semantic similarity is often confused with semantic relatedness.&quot;
    &quot; Semantic relatedness includes any relation between two terms, while semantic similarity only includes 'is a' relations.&quot;
    &quot; My favorite fruit is apples.&quot;
)
doc = nlp(text)
max_similarity = 0.0
most_similar = None, None
for i, sent in enumerate(doc.sents):
    for j, other in enumerate(doc.sents):
        if j &lt;= i:
            continue
        similarity = sent.similarity(other)
        if similarity &gt; max_similarity:
            max_similarity = similarity
            most_similar = sent, other
print(&quot;Most similar sentences are:&quot;)
print(f&quot;-&gt; '{most_similar[0]}'&quot;)
print(&quot;and&quot;)
print(f&quot;-&gt; '{most_similar[1]}'&quot;)
print(f&quot;with a similarity of {max_similarity}&quot;)

</code></pre>
<p>(text from <a href=""https://en.wikipedia.org/wiki/Semantic_similarity"" rel=""nofollow noreferrer"">wikipedia</a>)</p>
<p>It will yield the following output:</p>
<pre><code>Most similar sentences are:
-&gt; 'Semantic similarity is a metric defined over a set of documents or terms, where the idea of distance between items is based on the likeness of their meaning or semantic content as opposed to lexicographical similarity.'
and
-&gt; 'These are mathematical tools used to estimate the strength of the semantic relationship between units of language, concepts or instances, through a numerical description obtained according to the comparison of information supporting their meaning or describing their nature.'
with a similarity of 0.9583859443664551
</code></pre>
<p>Note the following information from <a href=""https://spacy.io/usage/linguistic-features#vectors-similarity"" rel=""nofollow noreferrer"">spacy.io</a>:</p>
<blockquote>
<p>To make them compact and fast, spaCy’s small pipeline packages (all packages that end in sm) don’t ship with word vectors, and only include context-sensitive tensors. This means you can still use the similarity() methods to compare documents, spans and tokens – but the result won’t be as good, and individual tokens won’t have any vectors assigned. So in order to use real word vectors, you need to download a larger pipeline package:</p>
<pre><code>- python -m spacy download en_core_web_sm
+ python -m spacy download en_core_web_lg
</code></pre>
</blockquote>
<p>Also see <a href=""https://stackoverflow.com/questions/49767270/document-similarity-in-spacy-vs-word2vec"">Document similarity in Spacy vs Word2Vec</a> for advice on how to improve the similarity scores.</p>
",1,4,1171,2019-05-15 13:33:38,https://stackoverflow.com/questions/56150678/use-spacy-to-find-most-similar-sentences-in-doc
Use Word2Vec to build a sense embedding,"<p>I really accept every hint on the following problem, because all what i want is to obtain that embedding from that dataset, I will write my all solution because (hopefully) the problem is just in some parts that i didn't consider.</p>
<p>I'm working with an annotated corpus, such that i have disambiguate words in a given sentence thanks to WordNet synsets id, that i will call tags. For example:</p>
<h3>Dataset</h3>
<pre class=""lang-xml prettyprint-override""><code>&lt;sentence&gt;
  &lt;text&gt;word1 word2 word3&lt;/text&gt;
  &lt;annotations&gt;
    &lt;annotation anchor=word1 lemma=lemma1&gt;tag1&lt;/annotation&gt;
    &lt;annotation anchor=word2 lemma=lemma2&gt;tag2&lt;/annotation&gt;
    &lt;annotation anchor=word3 lemma=lemma3&gt;tag3&lt;/annotation&gt;
  &lt;annotations&gt;
&lt;/sentence&gt;
</code></pre>
<p>Starting from this, given an embedding dimension that i will call n, i would like to build an embedding like this:</p>
<h3>Embedding</h3>
<pre><code>lemma1_tag1 dim 1 dim 2 dim 3 ... dim n
lemma2_tag2 dim 1 dim 2 dim 3 ... dim n
lemma3_tag3 dim 1 dim 2 dim 3 ... dim n
</code></pre>
<p>I thought to generate a corpus for Word2Vec starting from each text of each sentence, and replace each <code>anchor</code> with the respective <code>lemma1_tag1</code> (some words can contain more underscore, because i replaced space in lemmas with underscores). Since not every single word is annotated, after a simple preprocessing performed to remove stopwords and other punctuation, in the end i have something like the following example:</p>
<h3>Corpus Example</h3>
<pre><code>let just list most_recent_01730444a headline_06344461n
</code></pre>
<p>Since I'm just interested in annotated words, I also generated a predefined vocabulary to use it as Word2Vec vocabulary. This file contains on each row entries like:</p>
<h3>Vocabulary Example</h3>
<pre><code>lemma1_tag1
lemma2_tag2
</code></pre>
<p>So, after having defined a corpus and a vocabulary, I used them in Word2Vec toolkit:</p>
<h3>Terminal emulation</h3>
<pre><code>./word2vec -train data/test.txt -output data/embeddings.vec -size 300 -window 7 -sample 1e-3 -hs 1 -negative 0 -iter 10 -min-count 1 -read-vocab data/dictionary.txt -cbow 1
</code></pre>
<h3>Output</h3>
<pre><code>Starting training using file data/test.txt
Vocab size: 80
Words in train file: 20811
</code></pre>
<p>The problem is that the number of words in the corpus is 32000000+ and the number of words in the predefined vocabulary file is about 80000. I even tried in Python with Gensim, but (of course) I had the very same output. I think that the problem is that Word2Vec doesn't consider words in the format <code>lemma1_tag1</code> because of the underscore, and i don't know how to solve this problem. Any hint is appreciated, thank you in advance!</p>
","python, gensim, word2vec, word-embedding","<p>Both the original <code>word2vec.c</code> from Google, and gensim's <code>Word2Vec</code>, handle words with underscores just fine. </p>

<p>If both are looking at your input file, and both reporting just 80 unique words where you're expecting 100,000-plus, there's probably something wrong with your input-file. </p>

<p>What does <code>wc data/test.txt</code> report?</p>
",1,1,268,2019-05-16 11:12:27,https://stackoverflow.com/questions/56167224/use-word2vec-to-build-a-sense-embedding
Why mmap flag reduces memory consumption for single Word2Vec instance,"<p>According to the docs and wikipedia:</p>

<p>mmap allows processes to share same chunk of ram</p>

<pre><code>word_vectors = KeyedVectors.load(config.get(wv_file))
</code></pre>

<p>This model loaded like this takes ~2.2 GB ram</p>

<pre><code>word_vectors = KeyedVectors.load(config.get(wv_file), mmap='r')
</code></pre>

<p>This model loaded like this takes ~1.2 GB ram</p>

<p>Why am I observing such drastic decrease in ram consumption?</p>

<p>Loading multiple models simultaneously, works as expected and models share the ~1 GM memory.  </p>
","ram, gensim, mmap, word2vec","<p>Memory-mapping re-uses the operating system's virtual-memory functionality to use the existing file as the backing-source for a range of addressable memory. </p>

<p>With a single process, it <em>won't</em> necessarily save any memory. Instead, it just:</p>

<ul>
<li><p><em>Delays</em> loading any range-of-the-addresses into RAM, leaving it on disk until requested. If it's never requested, then RAM is never used, so in that particular case it may ""save"" memory.</p></li>
<li><p><em>Allows</em> those loaded ranges to be cheaply discarded if they're not accessed for a while, and the RAM is required for other allocations – because those ranges can be reloaded on demand from disk if ever again needed. So it might ""save"" memory in that case, compared to exhausting RAM or activating other generic virtual-memory that's not aware of the 1:1 relationship with an existing disk file. (Without memory mapping, seldom-used ranges of material in RAM could get written-out to a separate swap file to free space for other allocations – which a wasteful operation, and redundant data, when the data already exists on disk somewhere.)</p></li>
</ul>

<p>Unfortunately, in the common-case of a single-process, and typical operations like a <code>most_similar()</code> which necessarily computes on every single vector, the whole structure will be brought into memory on each <code>most_similar()</code>. There's no net RAM ""savings"" there (though perhaps a slight CPU/IO benefit if other memory pressure would've forced paging-out the loaded ranges). (Whatever approach you're using the sample the ""~2.2 GB"" and ""~1.2 GB"" used-RAM values may not be properly measuring that.)</p>

<p>The main benefit is when using multiple processes that each need to consult the same file's data. If naively loaded into RAM, each process will have its own redundant copy of the same data. If using memory-mapping, you've let the OS know: these multiple arrays-in-address-space, in multiple separate processed, definitionally have the same data (as reflected in the file). No matter how many processes need the data, only one copy of each file-range will ever consume RAM. There, a large savings can be achieved. </p>
",1,0,155,2019-05-21 10:33:38,https://stackoverflow.com/questions/56236404/why-mmap-flag-reduces-memory-consumption-for-single-word2vec-instance
Is there a way to save and load the vocabulary of a Gensim Doc2Vec model,"<blockquote>
  <p>edit</p>
</blockquote>

<p>The train corpus is a Spark dataframe I built before this step. I load it from parquet format and created a ""Feed"" class that give to Gensim lib the iterator on the train corpus :</p>

<pre><code>class Feed():
    def __init__(self, train_data):
        self.train_data = train_data

    def __iter__(self):
        for row in self.train_data.rdd.toLocalIterator():
            yield \
                gensim.models.doc2vec.TaggedDocument(\
                words=[kw.lower() for kw in row[""keywords""]] + list(row[""tokens_filtered""]),\
                tags=[row[""id""]])


sdf = spark.read.parquet(save_dirname)
train_corpus = Feed(sdf)
</code></pre>

<blockquote>
  <p>end edit</p>
</blockquote>

<p>I wish to train a Gensim Doc2Vec model on ~9 millions news text documents. Here is my model definition :</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(
        workers=8,
        vector_size=300,
        min_count=50,
        epochs=10)
</code></pre>

<p>The first step is getting the vocabulary :</p>

<pre><code>model.build_vocab(train_corpus)
</code></pre>

<p>It ends up in 90 minutes. Here is the logging info at the end of this process :</p>

<pre><code>INFO:gensim.models.doc2vec:collected 4202859 word types and 8950263 unique tags from a corpus of 8950339 examples and 1565845381 words
INFO:gensim.models.word2vec:Loading a fresh vocabulary
INFO:gensim.models.word2vec:min_count=50 retains 325027 unique words (7% of original 4202859, drops 3877832)
INFO:gensim.models.word2vec:min_count=50 leaves 1546772183 word corpus (98% of original 1565845381, drops 19073198)
INFO:gensim.models.word2vec:deleting the raw counts dictionary of 4202859 items
INFO:gensim.models.word2vec:sample=0.001 downsamples 9 most-common words
INFO:gensim.models.word2vec:downsampling leaves estimated 1536820314 word corpus (99.4% of prior 1546772183)
INFO:gensim.models.base_any2vec:estimated required memory for 325027 words and 300 dimensions: 13472946500 bytes
</code></pre>

<p>Then I train the model with an iterator class on the train corpus :</p>

<pre><code>model.train(train_corpus, total_examples=nb_rows, epochs=model.epochs)
</code></pre>

<p>The last training logs are :</p>

<pre><code>INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 99.99% examples, 201921 words/s, in_qsize 16, out_qsize 0
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 7 more threads
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 6 more threads
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 5 more threads
INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads
</code></pre>

<p>But it never finish the remaining threads.
It's not the first time I encounter this problem, even with much smaller train corpus. Usually, I relaunch the entire process (vocabulary setting and model training) and it goes on.</p>

<p>By now, to save time, I wish to NOT calculate again the vocabulary, getting in place the previously succesfully calculated one, and only try to train again the model. Is there a way to save the vocab part only of the model, then load it to train the model directly on train corpus ?</p>
","python, pyspark, gensim, doc2vec","<p>As far as the reason for the hang, it looks like you're generally doing the right things, but something might be going wrong with your <code>train_corpus</code>, whose construction you haven't shown. </p>

<p>Double-check its implementation, and perhaps edit your question to show more details of its type/initialization. Review the logs to see if there's evidence any of the threads hit errors that left them in a state where they're not reporting back as required. </p>

<p>You can <code>.save()</code> various parts of the <code>Doc2Vec</code> model, such as <code>model.wv.save(wv_path)</code> – but there's no simple way to reconstruct a model from those sub-parts. (It's possible but requires careful attention to the object's required state, from reviewing the source, and is error prone.)</p>

<p>But also, and more relevant, you can <code>.save()</code> the full <code>Doc2Vec</code> model at any time – and that's probably a better approach for your needs. That is, you can <code>.save()</code> it after <code>build_vocab()</code>, or after a call to <code>.train()</code>, etc. </p>

<p>If in fact your current hang is in some notebook where you can interrupt the hung action, and execute new code in a new cell, you can probably just save the model from there, debug your <code>train_corpus</code>, then re-load &amp; re-train – as long as the corpus size/vocabulary matches what was learned in <code>build_vocab()</code>. </p>

<p><strong>However</strong>, such a partially-trained <code>.save()</code> will, at the end, have been influenced by the nearly-complete training that already occurred. So, the end results won't be a true reflection of just your configuration parameters, but also bunch of excess ad-hoc training. </p>

<p>If you want to ""clear"" the lingering effects of that partial training after re-loading a saved <code>Doc2Vec</code> model, I believe the following should do it:</p>

<pre><code>model.trainables.reset_weights(model.hs, model.negative, model.wv, model.docvecs)
</code></pre>
",0,0,2305,2019-05-22 07:51:27,https://stackoverflow.com/questions/56251839/is-there-a-way-to-save-and-load-the-vocabulary-of-a-gensim-doc2vec-model
How to specify additional tokens for tokenizator?,"<p>I want to tokenize text with <code>gensim.utils.tokenize()</code>. And I want to add some phrases that would be recognized as single tokens, for example: <code>'New York', 'Long Island'</code>. </p>

<p>Is it possible with gensim? If not, what other libraries is it possible to use?</p>
","python, nlp, token, tokenize, gensim","<p>I've found the solution with <code>nltk</code>:</p>

<pre><code>tokenizer = nltk.tokenize.mwe.MWETokenizer([('hors', ""d'oeuvre"")], separator=' ')
tokenizer.tokenize(""An hors d'oeuvre tonight, sir?"".split())

['An', ""hors d'oeuvre"", 'tonight,', 'sir?']
</code></pre>
",0,0,81,2019-05-24 12:30:44,https://stackoverflow.com/questions/56292749/how-to-specify-additional-tokens-for-tokenizator
Cosine similarity between 0 and 1,"<p>I am interested in calculating similarity between vectors, however this similarity has to be a number between 0 and 1. There are many questions concerning tf-idf and cosine similarity, all indicating that the value lies between 0 and 1. From <a href=""https://en.wikipedia.org/wiki/Cosine_similarity#Soft_cosine_measure"" rel=""noreferrer"">Wikipedia</a>:</p>

<blockquote>
  <p>In the case of information retrieval, the cosine similarity of two
  documents will range from 0 to 1, since the term frequencies (using
  tf–idf weights) cannot be negative. The angle between two term
  frequency vectors cannot be greater than 90°.</p>
</blockquote>

<p>The peculiarity is that I wish to calculate the similarity between two vectors from two different word2vec models. These models have been aligned, though, so they should in fact represent their words in the same vector space. I can calculate the similarity between a word in <code>model_a</code> and a word in <code>model_b</code> like so</p>

<pre class=""lang-py prettyprint-override""><code>import gensim as gs
from sklearn.metrics.pairwise import cosine_similarity

model_a = gs.models.KeyedVectors.load_word2vec_format(model_a_path, binary=False)
model_b = gs.models.KeyedVectors.load_word2vec_format(model_b_path, binary=False)

vector_a = model_a[word_a].reshape(1, -1)
vector_b = model_b[word_b].reshape(1, -1)

sim = cosine_similarity(vector_a, vector_b).item(0)
</code></pre>

<p>But <code>sim</code> is then a similarity metric in the [-1,1] range. Is there a scientifically sound way to map this to the [0,1] range? Intuitively I would think that something like</p>

<pre><code>norm_sim = (sim + 1) / 2
</code></pre>

<p>is okay, but I'm not sure whether that is good practice with respect to the actual meaning of cosine similarity. If not, are other similarity metrics advised? </p>

<p>The reason why I am trying to get the values to be between 0 and 1 is because the data will be transferred to a colleague who will use it as a feature for her machine learning system, which expects all values to be between 0 and 1. Her intuition was to take the absolute value, but that seems to me to be a worse alternative because then you map opposites to be identical. Considering the actual meaning of cosine similarity, though, I might be wrong. So if taking the absolute value is the good approach, we can do that as well.</p>
","python, scikit-learn, gensim, similarity, cosine-similarity","<p>You have a fair reason to prefer 0.0-1.0 (though many learning algorithms should do just fine with a -1.0 to 1.0 range). Your norm_sim rescaling of -1.0 to 1.0 to 0.0 to 1.0 is fine, if your only purpose is to get 0.0-1.0 ranges... but of course the resulting value isn't a true cosine-similarity anymore. </p>

<p>It won't necessarily matter that the values aren't real full-range angles any more. (If the algorithm needed real angles, it'd work with -1.0 to 1.0.) </p>

<p>Using the signless absolute value would be a bad idea, as it would change the rank order of similarities – moving some results that are ""natively"" most-dissimilar way up.</p>

<p>There's been work on constraining word-vectors to have only non-negative values in dimensions, &amp; the usual benefit is that the resulting dimensions are more likely to be individually interpretable. (See for example <a href=""https://cs.cmu.edu/~bmurphy/NNSE/"" rel=""noreferrer"">https://cs.cmu.edu/~bmurphy/NNSE/</a>.) However, gensim doesn't support this variant, &amp; only trying it could reveal whether it would be better for any particular project.</p>

<p>Also, there's other research that suggests usual word-vectors may not be 'balanced' around the origin (so you'll see fewer negative cosine-similiarities than would be expected from points in a random hypersphere), and that shifting them to be more balanced will usually improve them for other tasks. See: <a href=""https://arxiv.org/abs/1702.01417v2"" rel=""noreferrer"">https://arxiv.org/abs/1702.01417v2</a></p>
",6,10,26524,2019-05-26 19:53:31,https://stackoverflow.com/questions/56316903/cosine-similarity-between-0-and-1
Getting Segmentation fault: 11 while running gensim&#39;s Cosine Similarity function on a bunch of documents,"<p>I'm using gensim to perform cosine similarity on a bunch of documents getting the Segmentation fault: 11. Could you please help me to resolve this issue?</p>

<p><strong>Error Trace:</strong></p>

<pre><code>2019-05-28 15:11:22,779 : INFO : creating sparse index
2019-05-28 15:11:22,779 : INFO : creating sparse matrix from corpus
2019-05-28 15:11:22,780 : INFO : PROGRESS: at document #0/546
2019-05-28 15:11:22,790 : INFO : created &lt;546x430 sparse matrix of type '&lt;class 'numpy.float32'&gt;'
        with 2191 stored elements in Compressed Sparse Row format&gt;
2019-05-28 15:11:22,791 : INFO : creating sparse shard #0
2019-05-28 15:11:22,791 : INFO : saving index shard to /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,791 : INFO : saving SparseMatrixSimilarity object under /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0, separately None
2019-05-28 15:11:22,794 : INFO : saved /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,794 : INFO : loading SparseMatrixSimilarity object from /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
2019-05-28 15:11:22,794 : INFO : loaded /var/folders/s_/jrkppgc11h97hmtcs00cy6bc0000gn/T/simserver93714a.0
Segmentation fault: 11
</code></pre>

<p><strong>Code</strong></p>

<pre><code>    def cosine_similarity(self,documents, query_docs=None, task='pairwise_similarity', metric_threshold=0.85, num_best=20):
        self.log('computing cosine similarity started')
        # Compute cosine similarity between the query_docs and the documents.
        dictionary = Dictionary(documents)
        corpus = [dictionary.doc2bow(doc) for doc in documents]
        # index_tmpfile = get_tmpfile(""index"")
        index = Similarity(output_prefix=None,corpus=corpus, num_best=num_best, num_features=len(dictionary))
        similarities = []
        if task == 'pairwise_similarity':
            self.log('computing pairwise_similarity')
            for sim in index:
                similarities.append(sim)
        elif task == 'batch_query':
            self.log('computing similarity using batch query')

            query_docs = [self.tfidf[self.dictionary.doc2bow(doc)] for doc in query_docs]
            for sim in index[query_docs]:
                similarities.append(sim)
        # filter results based on metric threshold
        filtered_results = []
        for ind_sim in similarities:
            filtered_results.append([item[0] for item in ind_sim if item[1] &gt;= metric_threshold])
        self.log('computing cosine similarity completed')
        return filtered_results
</code></pre>
","python-3.x, gensim","<p>Resolved the issue, there was a problem with the feature representation I was using.</p>
",0,0,62,2019-05-28 09:52:26,https://stackoverflow.com/questions/56339518/getting-segmentation-fault-11-while-running-gensims-cosine-similarity-function
Create a new vector model in gensim,"<p>I already trained a word2vec model with gensim library. For example, my model contains vectors for 2 words: ""new"" and ""york"". However, I also want to train a vector for the word ""new york"", so I transform ""new york"" into ""new_york"" and train a new vector model. Finally, I want to combine 3 vectors: vector of the word ""new"", ""york"" and ""new_york"" into one vector representation for the word ""new york"".</p>

<p>How can I save the new vector value to the model?</p>

<p>I try to assign the new vector to the model but gensim did not allow to assign the new value for vector model.</p>
","python, vector, gensim, word2vec","<p>Word-vectors are generally only comparable to each other if they were trained together.</p>

<p>So, if you want to have vectors for all of 'new', 'york', and 'new_york', you should prepare a corpus which includes them all, in a variety of uses, and train a <code>Word2Vec</code> model from that. </p>
",1,1,162,2019-06-01 17:31:23,https://stackoverflow.com/questions/56408959/create-a-new-vector-model-in-gensim
Discrepancies in gensim doc2vec embedding vectors,"<p>I use gensim Doc2Vec package to train doc2vec embeddings. I would expect that two models trained with the identical parameters and data would have very close values of the doc2vec vectors. However, in my experience it is only true with doc2vec trained in the PV-DBOW without training word embedding (dbow_words = 0).
For PV-DM and for PV-DBOW with dbow_words = 1, i.e. every case the word embedding are trained along with doc2vec, the doc2vec embedding vectors for identically trained models are fairly different. </p>

<p>Here is my code</p>

<pre class=""lang-py prettyprint-override""><code>    from sklearn.datasets import fetch_20newsgroups
    from gensim import models
    import scipy.spatial.distance as distance
    import numpy as np
    from nltk.corpus import stopwords
    from string import punctuation
    def clean_text(texts,  min_length = 2):
        clean = []
        #don't remove apostrophes
        translator = str.maketrans(punctuation.replace('\'',' '), ' '*len(punctuation))
        for text in texts:
            text = text.translate(translator)
            tokens = text.split()
            # remove not alphabetic tokens
            tokens = [word.lower() for word in tokens if word.isalpha()]
            # filter out stop words
            stop_words = stopwords.words('english')
            tokens = [w for w in tokens if not w in stop_words]
            # filter out short tokens
            tokens = [word for word in tokens if len(word) &gt;= min_length]
            tokens = ' '.join(tokens)
            clean.append(tokens)
        return clean
    def tag_text(all_text, tag_type =''):
        tagged_text = []
        for i, text in enumerate(all_text):
            tag = tag_type + '_' + str(i)
            tagged_text.append(models.doc2vec.TaggedDocument(text.split(), [tag]))
        return tagged_text

    def train_docvec(dm, dbow_words, min_count, epochs, training_data):
        model = models.Doc2Vec(dm=dm, dbow_words = dbow_words, min_count = min_count)
        model.build_vocab(tagged_data)
        model.train(training_data, total_examples=len(training_data), epochs=epochs)    
        return model

    def compare_vectors(vector1, vector2):
        cos_distances = []
        for i in range(len(vector1)):
            d = distance.cosine(vector1[i], vector2[i])
            cos_distances.append(d)
        print (np.median(cos_distances))
        print (np.std(cos_distances))    

    dataset = fetch_20newsgroups(shuffle=True, random_state=1,remove=('headers', 'footers', 'quotes'))
    n_samples = len(dataset.data)
    data = clean_text(dataset.data)
    tagged_data = tag_text(data)
    data_labels = dataset.target
    data_label_names = dataset.target_names

    model_dbow1 = train_docvec(0, 0, 4, 30, tagged_data)
    model_dbow2 = train_docvec(0, 0, 4, 30, tagged_data)
    model_dbow3 = train_docvec(0, 1, 4, 30, tagged_data)
    model_dbow4 = train_docvec(0, 1, 4, 30, tagged_data)
    model_dm1 = train_docvec(1, 0, 4, 30, tagged_data)
    model_dm2 = train_docvec(1, 0, 4, 30, tagged_data)

    compare_vectors(model_dbow1.docvecs, model_dbow2.docvecs)
    &gt; 0.07795828580856323
    &gt; 0.02610614028793008

    compare_vectors(model_dbow1.docvecs, model_dbow3.docvecs)
    &gt; 0.6476179957389832
    &gt; 0.14797587172616306

    compare_vectors(model_dbow3.docvecs, model_dbow4.docvecs)
    &gt; 0.19878000020980835
    &gt; 0.06362519480831186

    compare_vectors(model_dm1.docvecs, model_dm2.docvecs)
    &gt; 0.13536489009857178
    &gt; 0.045365127475424386

    compare_vectors(model_dbow1.docvecs, model_dm1.docvecs)
    &gt; 0.6358324736356735
    &gt; 0.15150255674571805
</code></pre>

<blockquote>
  <p>UPDATE</p>
</blockquote>

<p>I tried, as suggested by gojomo, to compare the differences between the vectors, and, unfortunately, those are even worse:</p>

<pre class=""lang-py prettyprint-override""><code>def compare_vector_differences(vector1, vector2):
    diff1 = []
    diff2 = []
    for i in range(len(vector1)-1):
        diff1.append( vector1[i+1] - vector1[i])
    for i in range(len(vector2)-1):
        diff2[i].append(vector2[i+1] - vector2[i])
    cos_distances = []
    for i in range(len(diff1)):
        d = distance.cosine(diff1[i], diff2[i])
        cos_distances.append(d)
    print (np.median(cos_distances))
    print (np.std(cos_distances))    

compare_vector_differences(model_dbow1.docvecs, model_dbow2.docvecs)
&gt; 0.1134452223777771
&gt; 0.02676398444178949

compare_vector_differences(model_dbow1.docvecs, model_dbow3.docvecs)
&gt; 0.8464127033948898
&gt; 0.11423789350773429

compare_vector_differences(model_dbow4.docvecs, model_dbow3.docvecs)

&gt; 0.27400463819503784
&gt; 0.05984108730423529
</code></pre>

<blockquote>
  <p>SECOND UPDATE</p>
</blockquote>

<p>This time, after I finally understood gojomo, the things look fine.</p>

<pre class=""lang-py prettyprint-override""><code>def compare_distance_differences(vector1, vector2):
    diff1 = []
    diff2 = []
    for i in range(len(vector1)-1):
        diff1.append( distance.cosine(vector1[i+1], vector1[i]))
    for i in range(len(vector2)-1):
        diff2.append( distance.cosine(vector2[i+1], vector2[i]))
    diff_distances = []
    for i in range(len(diff1)):
        diff_distances.append(abs(diff1[i] - diff2[i]))
    print (np.median(diff_distances))
    print (np.std(diff_distances))    

compare_distance_differences(model_dbow1.docvecs, model_dbow2.docvecs)
&gt;0.017469733953475952
&gt;0.01659284710785352

compare_distance_differences(model_dbow1.docvecs, model_dbow3.docvecs)
&gt;0.0786697268486023
&gt;0.06092163158218411

compare_distance_differences(model_dbow3.docvecs, model_dbow4.docvecs)
&gt;0.02321992814540863
&gt;0.023095123172320778
</code></pre>
","gensim, word-embedding, doc2vec","<p>The doc-vectors (or word-vectors) of <code>Doc2Vec</code> &amp; <code>Word2Vec</code> models are only meaningfully comparable to other vectors that were co-trained, in the same interleaved training sessions. </p>

<p>Otherwise, randomness introduced by the algorithms (random-initialization &amp; random-sampling) and by slight differences in training ordering (from multithreading) will cause the trained positions of individual vectors to wander to arbitrarily different positions. Their <em>relative</em> distances/directions, to other vectors that shared interleaved training, should be about as equally-useful from one model to the next. </p>

<p>But there's no one right place for such a vector, and measuring the differences between the vector for document '1' (or word 'foo') in one model, and the corresponding vector in another model, isn't reflective of anything the models/algorithms are trained to provide.</p>

<p>There's more information in the Gensim FAQ:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q11-ive-trained-my-word2vecdoc2vecetc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">Q11: I've trained my Word2Vec/Doc2Vec/etc model repeatedly using the exact same text corpus, but the vectors are different each time. Is there a bug or have I made a mistake?</a></p>
",1,0,306,2019-06-02 04:51:47,https://stackoverflow.com/questions/56412272/discrepancies-in-gensim-doc2vec-embedding-vectors
word embedding of a lstm sequence,"<p>Suppose, I have a Seq2Seq model. I want to have the Embedding layer in this model.</p>

<p>Based on my research I can do it in three ways:</p>

<ol>
<li><p>train a word embedding separately on my data set or download a pre-trained word embedding, then use the weights of those embedding as the weight of the words in my data set. So here I do not need to have an embedding layer at all, I just load the weights of the already trained words into the words in my data set.</p></li>
<li><p>I create an embedding layer and set the trainable true, so not only I have an embedding, but also, that embedding will be trained based on my task</p></li>
<li><p>I create an Embedding layer, load already trained weights, and set trainable False. in this case, the weights will not get updated.</p></li>
</ol>

<p>(please correct me if Im wrong).</p>

<p>I have used the <code>first approach</code>. I want to know what will be the interpretation of the output of this code:</p>

<pre><code>model_wv = Word2Vec.load(""word2vec_50d_7w"")
embeddings = np.zeros((len(model_wv.wv.vocab), emb_dim))
for i in range(len(model_wv.wv.vocab)):
    # print(i)
    embedding_vector = model_wv.wv[model_wv.wv.index2word[i]]
    if embedding_vector is not None:
        embeddings[i] = embedding_vector

print(embeddings[[1,2,3],[3,4,1]])
</code></pre>

<p>this is the output:</p>

<pre><code>[-0.01566689 -1.36469996  0.59684211]
</code></pre>

<p>consider this <code>[1,2,3],[3,4,1]</code> as two sequence with <code>length=3</code>.</p>

<p>I was thinking we use word embedding in lstm to transform each word of the sequence into an embedding. I expected to see <code>two vectors</code> and <code>three items</code> in each vector.</p>

<p>The embedding is the word2vec in gensim,</p>

<p>Appreciate it if someone shed light on it where I am getting lost?</p>

<p>Thanks~</p>
","tensorflow, keras, lstm, gensim, word-embedding","<p>Your interpretations on the three variants of setting up the embedding layer is exactly correct as what I understand. </p>

<p>There are two major transfer learning techniques.</p>

<ol>
<li><p>Using the pre-learned model as a <strong>feature-vector</strong>. In your case, the word2vec model would be used as a lookup service to pre-process/convert tokens to ids and then to embedding vectors. These embedding vectors become the actual feature when you train your own model. (this is your #1)</p></li>
<li><p>Using a <strong>fine-tuning</strong> approach. Here you can choose to either continue training the pre-learned model (setting trainable=True) or fix the prelearned model (setting trainable=False). There could benefits in either approach. (This is your #2 and #3)</p></li>
</ol>

<p>(#1 and #3) produce similar result regarding quality from my experience. 
If you own a decent amount of training data, fine-tuning with trainable=True (#2) would be the best approach from my experience. </p>

<p>You problem here is a numpy issue. you probably should say, </p>

<pre><code>print(embeddings[[1,2,3]], embeddings[[3,4,1]])
</code></pre>

<p>Otherwise the <a href=""https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing"" rel=""nofollow noreferrer"">indexing</a> is not working as you expected. </p>

<pre><code>embeddings[[1,2,3],[3,4,1]]
</code></pre>

<p>This actually lookups the rows with indices 1, 2, 3 and get the column with indices 3, 4, 1 respectively. In other words, it picks up</p>

<pre><code>column 3 for row 1
column 4 for row 2
column 1 for row 3
</code></pre>
",1,0,561,2019-06-02 21:11:50,https://stackoverflow.com/questions/56418980/word-embedding-of-a-lstm-sequence
How to use document vectors in isolationforest in sklearn,"<p>In understanding what <code>isolation forest</code> really does, I did a sample project as follows using 8 features as follows.</p>

<pre><code>from sklearn.ensemble import IsolationForest    
#features
df_selected = df[[""feature1"", ""feature2"", ""feature3"", ""feature4"", ""feature5"", ""feature6"", ""feature7"", ""feature8""]]
X = np.array(df_selected)

#isolation forest
clf = IsolationForest(max_samples='auto', random_state=42, behaviour=""new"", contamination=.01)
clf.fit(X)
y_pred_train = clf.predict(X)

print(np.where(y_pred_train == -1)[0])
</code></pre>

<p>Now, I want to identify what are the <em>outlier documents</em> using <code>isolation forest</code>. For that I trained a <code>doc2vec</code> model using <code>gensim</code>. Now for each of my document in the dataset I have a <code>300-dimensional vector</code>.</p>

<p>My question is can I straight away use the document vectors in <code>isolation forest</code> as <code>X</code> in the above code to detect outliers? Or do I need to reduce the dimensionality of the vectors before applying them to <code>isolation forest</code>?</p>

<p>I am happy to provide more details if needed.</p>
","python, scikit-learn, gensim, outliers, doc2vec","<p>You can straight away use the <code>predict()</code> to detect outliers unless you plan on removing some variables that would not be considered in the training model.  </p>

<p>In general, I would say to do a correlation analysis and remove the variables that are highly correlated with each other (Logic basis being that if they are highly correlated, then they are the same and should not encourage the bias of the variables by doubling the consideration).  </p>

<p>Feel free to dispute otherwise or state your considerations as I think the above is really my opinion on how to approach the problem.</p>
",1,1,574,2019-06-03 05:05:14,https://stackoverflow.com/questions/56421404/how-to-use-document-vectors-in-isolationforest-in-sklearn
gensim save load model deprecation warning,"<p>I get the following deprecation warning when saving/loading a gensim word embedding:</p>

<pre><code>model.save(""mymodel.model"")

/home/.../lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: 
UserWarning: This function is deprecated, use smart_open.open instead. 
See the migration notes for details:
</code></pre>

<p><a href=""https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function</a></p>

<pre><code>  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL
</code></pre>

<p>I don't understand what to do following the notes on the page.
So, <strong>how should I save and open my models instead?</strong></p>

<p>I use python 3.7 , gensim 3.7.3. and smart_open 1.8.4. I think I did not get the warning when using gensim 3.7.1. and python 3.5. smart_open should have been 1.8.4. </p>
",gensim,"<p>You can ignore most ""deprecation warnings"", as they're just an advisory about underlying changes that for now still work, but there's a new preferred way to do things that may be required in the future. </p>

<p>In this case, the warning is about a function inside the <code>smart_open</code> package that the <code>gensim</code> package is using. That is, it's <strong>not</strong> the <code>.save()</code> you are calling that's deprecated, but something inside <code>.save()</code>. The <code>gensim</code> authors will eventually update <code>.save()</code> to use the newly-preferred variant of what <code>smart_open</code> offers. </p>

<p>You can just keep using <code>.save()</code>, ignoring the message as long as things still work for you – unless you'd like to contribute the fix to <code>.save()</code> to remove the warning to <code>gensim</code>. (It may, however, have already been fixed in the development code, to become available in the next <code>gensim</code> release.)</p>
",6,3,2934,2019-06-05 07:36:06,https://stackoverflow.com/questions/56456051/gensim-save-load-model-deprecation-warning
Sentence iterator to pass to Gensim language model,"<p>I am relatively new to NLP and I am trying to create my own words embeddings trained in my personal corpus of docs.</p>

<p>I am trying to implement the following code to create my own wordembedings:</p>

<pre><code>model = gensim.models.Word2Vec(sentences)
</code></pre>

<p>with sentences being a list of sentences.
Since I can not pass thousands and thousands of sentences I need an iterator</p>

<pre><code># with mini batch_dir a directory with the text files
# MySentences is a class iterating over sentences.
sentences = MySentences(minibatch_dir) # a memory-friendly iterator
</code></pre>

<p>I found this solution by the creator of gensim:</p>

<pre><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()
</code></pre>

<p>It does not work for me.
How can I create an iterator if I know how to get the list of sentences from every document?</p>

<p>And second very related question:
If I am aiming to compare documents similarity in a particular corpus, is always better to create from scratch word embeddings with all the documents of that particular corpus than using GloVec or word2vec? 
The amount of docs is around 40000.</p>

<p>cheers</p>

<p>More pre</p>
","python, nlp, gensim, word2vec, word-embedding","<p>Your illustrated class <code>MySentences</code> assumes one sentence per line. That might not be the case for your data.</p>
<p>One thing to note is - calling Word2Vec(sentences, iter=1) will run two passes over the sentences iterator (or, in general iter+1 passes; default iter=5). The first pass collects words and their frequencies to build an internal dictionary tree structure. The second and subsequent passes train the neural model. These two (or, iter+1) passes can also be initiated manually, in case your input stream is non-repeatable (you can only afford one pass), and you’re able to initialize the vocabulary some other way:</p>
<pre><code>model = gensim.models.Word2Vec(iter=1)  # an empty model, no training yet
model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator
model.train(other_sentences)  # can be a non-repeatable, 1-pass generator
</code></pre>
<p>For example, if you are trying to read dataset stored in a database, your generator function to stream text directly from a database, will throw TypeError:</p>
<pre><code>TypeError: You can't pass a generator as the sentences argument. Try an iterator.
</code></pre>
<p>A generator can be consumed only once and then it’s forgotten. So, you can write a wrapper which has an iterator interface but uses the generator under the hood.</p>
<pre><code>class SentencesIterator():
    def __init__(self, generator_function):
        self.generator_function = generator_function
        self.generator = self.generator_function()

    def __iter__(self):
        # reset the generator
        self.generator = self.generator_function()
        return self

    def __next__(self):
        result = next(self.generator)
        if result is None:
            raise StopIteration
        else:
            return result
</code></pre>
<p>The generator function is stored as well so it can reset and be used in Gensim like this:</p>
<pre><code>from gensim.models import FastText

sentences = SentencesIterator(tokens_generator)
model = FastText(sentences) 
</code></pre>
",5,2,2396,2019-06-05 22:29:49,https://stackoverflow.com/questions/56468865/sentence-iterator-to-pass-to-gensim-language-model
How to put maximum vocabulary frequency in doc2vec,"<p>Doc2vec while creating the vocabulary has possibility to put minimum occurence of the word in documents to be included in vocabulary as parameter <code>min_count</code>.</p>

<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=3, epochs=100,workers=8)
</code></pre>

<p>How is it possible to exclude words which appear far too often, with some parameter?</p>

<p>I know that one way is to do this in preprocessing step by manually deleting those words, and counting each, but would be nice to know if there is maybe some built in method to do so, as it gives more space for testing.
Many thanks for the answer.</p>
","python, gensim, word2vec, doc2vec","<p>There's no explicit <code>max_count</code> parameter in gensim's <code>Word2Vec</code>. </p>

<p>If you're sure some tokens are meaningless, you should preprocess your text to eliminate them. </p>

<p>There is also a <code>trim_rule</code> option that can be passed as model instantiation or <code>build_vocab()</code>, where your own function can discard some words; see the gensim docs at:</p>

<p><a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec</a></p>

<p>Similarly, you could possibly avoid calling <code>build_vocab()</code> directly, and instead call its substeps – but edit the discovered raw-counts dictionary before the vocabulary is finalized. You would want to consult the source code to do this, and could use the code that discards too-infrequent words as a model for your own new additional code. </p>

<p>The classic <code>sample</code> parameter of <code>Word2Vec</code> also controls a downsampling of high-frequency words, to prevent the model from spending too much relative effort on redundantly training abundant words. The more aggressive (smaller) this value is, the more instances of high-frequency words will be radomly skipped during training. The default of <code>1e-03</code> (<code>0.001</code>) is very conservative; in very-large natural language corpuses I've seen good results up to <code>1e-07</code> (<code>0.0000001</code>) or <code>1e-8</code> (<code>0.00000001</code>) – so in another domain where some lower-meaning tokens are very-frequent, similarly aggressive downsampling is worth trying. </p>

<p>The newer <code>ns_exponent</code> option changes negative sampling to adjust the relative favoring of less-frequent words. The original <code>word2vec</code> work used a fixed value of 0.75, but some research since has suggested other domains, like recommendation systems, might benefit from other values that are more or less sensitive to actual token frequencies. (The relevant paper is linked in the <code>gensim</code> docs for the <code>ns_exponent</code> parameter.)</p>
",3,2,1235,2019-06-06 13:14:00,https://stackoverflow.com/questions/56478384/how-to-put-maximum-vocabulary-frequency-in-doc2vec
Where is word2vec mapping coming from for DBOW doc2vec in gensim implementation?,"<p>I am trying to use gensim for doc2vec and word2vec.</p>

<p>Since PV-DM approach can generate word2vec and doc2vec at the same time,
I thought PV-DM is the right model to use.</p>

<p>So, I created a model using <code>gensim</code> by specifying <code>dm=1</code> for PV-DM</p>

<p>My questions are followings:</p>

<ol>
<li><p>Is it true that word2vec model gets trained along with doc2vec when I call <code>train</code> on Doc2vec object??</p></li>
<li><p>it seems like property <code>wv</code> contains word2vec and available even before training. Is this static version of word2vec?</p></li>
<li><p>I also created DBOW model and noticed that it also contains <code>wv</code>. Is this also the same static version of word2vec that I mentioned in the previous question?</p></li>
</ol>
","gensim, word2vec, doc2vec","<p>(1) Yes, word-vectors are trained simultaneously with doc-vectors in PV-DM mode.</p>

<p>(2) The contents of the <code>wv</code> property before training happens are the randomly-initialized, untrained word-vectors. (As in word2vec, all vectors get random, low-magnitude starting positions.)</p>

<p>(3) In plain PV-DBOW mode (<code>dm=0</code>), because of code-sharing, the <code>wv</code> vectors are still allocated &amp; initialized – but never trained. At the end of PV-DBOW training, the <code>wv</code> word-vectors will be unchanged, and thus random/useless. (They don't participate in training at all.)</p>

<p>If you enable the optional <code>dbow_words=1</code> parameter, then skip-gram word-vector training will be mixed-in with plain PV-DBOW training. This will be done in an interleaved fashion, so each target word (to be predicted) will be used to train a PV-DBOW doc-vector, then neighboring context word-vectors. As a result, the <code>wv</code> word-vectors will be trained, and in the ""same space"" for meaningful comparisons to doc-vectors. </p>

<p>With this option, training will take longer than in plain PV-DBOW (by a factor related to the <code>window</code> size). For any particular end-purpose, the doc-vectors in this mode might be better (if the word-to-word predictions effectively helped to extend the corpus in useful ways) or worse (if the model spending so much effort on word-to-word predictions effectively diluted/overwhelmed other patterns in the full-doc doc-to-word predictions).</p>
",2,2,124,2019-06-06 18:51:00,https://stackoverflow.com/questions/56483468/where-is-word2vec-mapping-coming-from-for-dbow-doc2vec-in-gensim-implementation
"Gensim: Manual generation of training tuples of (target, context, label)","<p>I am asking this question as a lazy researcher who just wants to try out random crazy ideas quickly, without spending a ton of time reinventing wheels. I completely understand these aren't the intended use cases.</p>

<p>To test a number of hypothesis, I would love to</p>

<ul>
<li>generate the (target, context, +1) tuples differently, instead of the default sliding window.</li>
<li>generate the negative samples (target, random_context, -1) tuples based on some rules, instead of from random NCE draws.</li>
</ul>

<p>For example, I can get the parse tree of a sentence and use parent-child relationship to generate tuples, which is a non-linear window(somebody already tried it in NLP research community, hand-coded ofc...). I can also get an antonyms dictionary to lookup and to generate more negative samples in addition to the random ones (not sure, may help with faster convergence).</p>

<p>Are there some private member functions (something that starts with <code>_XX</code>)I can override to achieve these?</p>
","python, nlp, gensim, word2vec, doc2vec","<p>Unfortunately, there are not easy extension-points for changing the (context->word) training-examples, or negative-example sampling. </p>

<p>Of course the full source code is available, and thus anything's possible as patches, or by using the existing code as a starting-point. Practically, however, the key loops/decisions about these steps are only efficiently run from inside the optimized Cython training routines – which are a bit harder to read/adapt/test/deploy.</p>

<p>(There's <a href=""https://github.com/RaRe-Technologies/gensim/issues/1623"" rel=""nofollow noreferrer"">an open issue #1623</a> to re-factor the code to make such related variants of Word2Vec easier to implement. But the project's prior effort to ostensibly meet this need, <a href=""https://github.com/RaRe-Technologies/gensim/pull/1777"" rel=""nofollow noreferrer"">PR #1777</a>, was somewhat of a disaster, adding more layers of indirection and scattering key operations across new classes, without offering the sorts of extension-points that were really needed.)</p>
",1,0,37,2019-06-11 21:01:09,https://stackoverflow.com/questions/56551612/gensim-manual-generation-of-training-tuples-of-target-context-label
How to get deterministic train results in Doc2Vec?,"<p>I am using Doc2Vec to analysis some paragraph and wish to get deterministic vector representation of the train data. Based on the <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">official documentation</a>, it seems that I need to set the parameters ""seed"" and ""workers"", as well as the PYTHONHASHSEED environment variable in Python 3. Therefore, I wrote the script as follows.</p>

<pre class=""lang-py prettyprint-override""><code>import os
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec


def main():
    # Check whether the environment variable has been set successfully
    print(os.environ.get('PYTHONHASHSEED'))

    docs = [TaggedDocument(['Apple', 'round', 'apple', 'red', 'Apple', 'juicy', 'apple', 'sweet'], ['A']),
            TaggedDocument(['I', 'have', 'a', 'little', 'frog', 'His', 'name', 'is', 'Tiny', 'Tim'], ['B']),
            TaggedDocument(['On', 'top', 'of', 'spaghetti', 'all', 'covered', 'with', 'cheese'], ['C'])]

    # Loop 3 times to check whether consistent results are produced within each run
    for i in range(3):
        model = Doc2Vec(min_count=1, seed=12345, workers=1)
        model.build_vocab(docs)
        model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)
        print(model.docvecs['B'])


if __name__ == '__main__':
    os.environ['PYTHONHASHSEED'] = '12345'
    main()
</code></pre>

<p>The problem is that within each run it does produce deterministic results, but when I run the whole script again it gives different results. Is there any problem with my environment variable setting, or am I missing out something else?</p>

<p>I am on Python 3.6.5.</p>
","python, gensim, doc2vec","<p>I believe setting <code>PYTHONHASHSEED</code> inside your code is too late: it needs to be set <strong>in the OS environment</strong>, before the Python interpreter runs at all. When Python launches, it checks for this to decide whether <strong>all</strong> dictionaries during this execution will use the specified randomization seed. (It isn't rechecked later, for each subsequent dictionary creation.)</p>

<p>But also, note that you generally <strong>shouldn't</strong> force determinism on these algorithms – but rather make your evaluations tolerant of small run-to-run jitter. Large jitter can be an indication of other problems with the sufficiency of your data or metaparameters – but forcing determinism hides this valuable indirect signal of model strength.</p>

<p>There's a bit more discussion in Q11 &amp; Q12 of the gensim project FAQ about these issues:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q11-ive-trained-my-word2vecdoc2vecetc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q11-ive-trained-my-word2vecdoc2vecetc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism</a></p>
",3,0,891,2019-06-12 02:07:03,https://stackoverflow.com/questions/56553753/how-to-get-deterministic-train-results-in-doc2vec
Python3 - Doc2Vec: Get document by vector/ID,"<p>I've already built my Doc2Vec model, using around 20.000 files. I'm looking for a way to find the string representation of a given vector/ID, which might be similar to Word2Vec's index2entity. I'm able to get the vector itself, using model['n'], but now I'm wondering whether there's a way to get some sort of string representation of it as well.</p>
","python, nlp, gensim, word2vec, doc2vec","<p>If you want to look up your actual training text, for a given text+tag that was part of training, you should retain that mapping outside the <code>Doc2Vec</code> model. (The model doesn't store training texts – only looking at them, repeatedly, during training.)</p>

<p>If you want to <em>generate</em> a text from a <code>Doc2Vec</code> doc-vector, that's not an existing feature, nor do I know any published work describing a reliable technique for doing so. </p>

<p>There's a speculative/experimental bit of work-in-progress for gensim <code>Doc2Vec</code> that will forward-propagate a doc-vector through the model's neural-network, and report back the most-highly-predicted target words. (This is somewhat the opposite of the way <code>infer_vector()</code> works.)</p>

<p>That <em>might</em>, plausibly, give a sort-of summary text. For more details see this open issue &amp; the attached PR-in-progress:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/issues/2459"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/2459</a></p>

<p>Whether this is truly useful or likely to become part of gensim is still unclear. </p>

<p>However, note that such a set-of-words <em>wouldn't</em> be grammatical. (It'll just be the ranked-list of most-predicted words. Perhaps some other subsystem could try to string those words together in a natural, grammatical way.) </p>

<p>Also, the subtleties of whether a concept has many potential associates words, or just one, could greatly affect the ""top N"" results of such a process. Contriving a possible example: there are many words for describing a 'cold' environment. As a result, a doc-vector for a text about something cold might have lots of near-synonyms for 'cold' in the 11th-20th ranked positions – such that the ""total likelihood"" of at least one cold-ish word is very high, maybe higher than any one other word. But just looking at the top-10 most-predicted words might instead list other ""purer"" words whose likelihood isn't so divided, and miss the (more-important-overall) sense of ""coldness"". So, this experimental pseudo-summarization method might benefit from a second-pass that somehow ""coalesces"" groups-of-related-words into their most-representative words, until some overall proportion (rather than fixed top-N) of the doc-vector's predicted-words are communicated. (This process might be vaguely like finding a set of M words whose ""Word Mover's Distance"" to the full set of predicted-words is minimized – though that could be a very expensive search.)</p>
",0,0,276,2019-06-13 14:25:26,https://stackoverflow.com/questions/56582711/python3-doc2vec-get-document-by-vector-id
Copying embeddings for gensim word2vec,"<p>I wanted to see if I can simply set new weights for gensim's Word2Vec without training. I get the 20 News Group data set from scikit-learn (from sklearn.datasets import fetch_20newsgroups) and trained an instance of Word2Vec on it:</p>

<pre><code>model_w2v = models.Word2Vec(sg = 1, size=300)
model_w2v.build_vocab(all_tokens)
model_w2v.train(all_tokens, total_examples=model_w2v.corpus_count, epochs = 30)
</code></pre>

<p>Here all_tokens is the tokenized data set. 
Then I created a new instance of Word2Vec without training </p>

<pre><code>model_w2v_new = models.Word2Vec(sg = 1, size=300)
model_w2v_new.build_vocab(all_tokens)
</code></pre>

<p>and set the embeddings of the new Word2Vec equal to the first one</p>

<pre><code>model_w2v_new.wv.vectors = model_w2v.wv.vectors
</code></pre>

<p>Most of the functions work as expected, e.g.</p>

<pre><code>model_w2v.wv.similarity( w1='religion', w2 = 'religions')
&gt; 0.4796233
model_w2v_new.wv.similarity( w1='religion', w2 = 'religions')
&gt; 0.4796233
</code></pre>

<p>and</p>

<pre><code>model_w2v.wv.words_closer_than(w1='religion', w2 = 'judaism')
&gt; ['religions']
model_w2v_new.wv.words_closer_than(w1='religion', w2 = 'judaism')
&gt; ['religions']
</code></pre>

<p>and</p>

<pre><code>entities_list = list(model_w2v.wv.vocab.keys()).remove('religion')

model_w2v.wv.most_similar_to_given(entity1='religion',entities_list = entities_list)
&gt; 'religions'
model_w2v_new.wv.most_similar_to_given(entity1='religion',entities_list = entities_list)
&gt; 'religions'
</code></pre>

<p>However, most_similar doesn't work:</p>

<pre><code>model_w2v.wv.most_similar(positive=['religion'], topn=3)
[('religions', 0.4796232581138611),
 ('judaism', 0.4426296651363373),
 ('theists', 0.43141329288482666)]

model_w2v_new.wv.most_similar(positive=['religion'], topn=3)
&gt;[('roderick', 0.22643062472343445),
&gt; ('nci', 0.21744996309280396),
&gt; ('soviet', 0.20012077689170837)]
</code></pre>

<p>What am I missing? </p>

<p>Disclaimer. I posted this question on <a href=""https://datascience.stackexchange.com/questions/53601/copying-embeddings-for-gensim-word2vec"">datascience.stackexchange</a> but got no response, hoping to have a better luck here. </p>
","gensim, word2vec, word-embedding","<p>Generally, your approach should work. </p>

<p>It's likely the specific problem you're encountering was caused by an extra probing step you took and is not shown in your code, because you had no reason to think it significant: some sort of <code>most_similar()</code>-like operation on <code>model_w2v_new</code> <strong>after</strong> its <code>build_vocab()</code> call but <strong>before</strong> the later, malfunctioning operations.</p>

<p>Traditionally, <code>most_similar()</code> calculations operate on a version of the vectors that has been normalized to unit-length. The 1st time these unit-normed vectors are needed, they're calculated – and then cached inside the model. So, if you then  replace the raw vectors with other values, but don't discard those cached values, you'll see results like you're reporting – essentially random, reflecting the randomly-initialized-but-never-trained starting vector values. </p>

<p>If this is what happened, just discarding the cached values should cause the next <code>most_similar()</code> to refresh them properly, and then you should get the results you expect:</p>

<pre><code>model_w2v_new.wv.vectors_norm = None
</code></pre>
",1,0,557,2019-06-14 03:52:24,https://stackoverflow.com/questions/56591149/copying-embeddings-for-gensim-word2vec
"word not in vocabulary after training gensim word2vec model, why?","<p>So I want to use word-embeddings in order to get some handy dandy cosine similarity values. After creating the model and checking for similarity of the word ""not"" (which is in the data I give the model) it tells me that the word is not in the vocabulary.</p>

<p>Why can't it find the similarity for the word 'not'?</p>

<p>the description data looks as follows:<br>
[['not', 'only', 'do', 'angles', 'make', 'joints', 'stronger', 'they', 'also', 'provide', 'more', 'consistent', 'straight', 'corners', 'simpson', 'strongtie', 'offers', 'a', 'wide', 'variety', 'of', 'angles', 'in', 'various', 'sizes', 'and', 'thicknesses', 'to', 'handle', 'lightduty', 'jobs', 'or', 'projects', 'where', 'a', 'structural', 'connection', 'is', 'needed', 'some', 'can', 'be', 'bent', 'skewed', 'to', 'match', 'the', 'project', 'for', 'outdoor', 'projects', 'or', 'those', 'where', 'moisture', 'is', 'present', 'use', 'our', 'zmax', 'zinccoated', 'connectors', 'which', 'provide', 'extra', 'resistance', 'against', 'corrosion', 'look', 'for', 'a', 'z', 'at', 'the', 'end', 'of', 'the', 'model', 'numberversatile', 'connector', 'for', 'various', 'connections', 'and', 'home', 'repair', 'projectsstronger', 'than', 'angled', 'nailing', 'or', 'screw', 'fastening', 'alonehelp', 'ensure', 'joints', 'are', 'consistently', 'straight', 'and', 'strongdimensions', 'in', 'x', 'in', 'x', 'inmade', 'from', 'gauge', 'steelgalvanized', 'for', 'extra', 'corrosion', 'resistanceinstall', 'with', 'd', 'common', 'nails', 'or', 'x', 'in', 'strongdrive', 'sd', 'screws']]</p>

<p>Note that I've already tried to give the data as separate sentences instead of separate words.</p>

<pre><code>def word_vec_sim_sum(row):
    description = row.product_description.split()
    description_embedding = gensim.models.Word2Vec([description], size=150,
        window=10,
        min_count=2,
        workers=10,
        iter=10)       
    print(description_embedding.wv.most_similar(positive=""not""))
</code></pre>
","python, gensim, word2vec","<p>You need to lower <code>min_count</code>.</p>

<p>From the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">documentation</a>: <em>min_count (int, optional) – Ignores all words with total frequency lower than this.</em> In the data you have provided <code>""not""</code> appears once, so it is ignored. By setting <code>min_count</code> to 1 it works. </p>

<pre><code>import gensim as gensim

data = [['not', 'only', 'do', 'angles', 'make', 'joints', 'stronger', 'they', 'also', 'provide', 'more', 'consistent',
         'straight', 'corners', 'simpson', 'strongtie', 'offers', 'a', 'wide', 'variety', 'of', 'angles', 'in',
         'various', 'sizes', 'and', 'thicknesses', 'to', 'handle', 'lightduty', 'jobs', 'or', 'projects', 'where', 'a',
         'structural', 'connection', 'is', 'needed', 'some', 'can', 'be', 'bent', 'skewed', 'to', 'match', 'the',
         'project', 'for', 'outdoor', 'projects', 'or', 'those', 'where', 'moisture', 'is', 'present', 'use', 'our',
         'zmax', 'zinccoated', 'connectors', 'which', 'provide', 'extra', 'resistance', 'against', 'corrosion', 'look',
         'for', 'a', 'z', 'at', 'the', 'end', 'of', 'the', 'model', 'numberversatile', 'connector', 'for', 'various',
         'connections', 'and', 'home', 'repair', 'projectsstronger', 'than', 'angled', 'nailing', 'or', 'screw',
         'fastening', 'alonehelp', 'ensure', 'joints', 'are', 'consistently', 'straight', 'and', 'strongdimensions',
         'in', 'x', 'in', 'x', 'inmade', 'from', 'gauge', 'steelgalvanized', 'for', 'extra', 'corrosion',
         'resistanceinstall', 'with', 'd', 'common', 'nails', 'or', 'x', 'in', 'strongdrive', 'sd', 'screws']]


def word_vec_sim_sum(row):
    description = row
    description_embedding = gensim.models.Word2Vec([description], size=150,
                                                   window=10,
                                                   min_count=1,
                                                   workers=10,
                                                   iter=10)
    print(description_embedding.wv.most_similar(positive=""not""))


word_vec_sim_sum(data[0])
</code></pre>

<p>And the output:</p>

<pre><code>[('do', 0.21456070244312286), ('our', 0.1713767945766449), ('can', 0.1561305820941925), ('repair', 0.14236785471439362), ('screw', 0.1322808712720871), ('offers', 0.13223429024219513), ('project', 0.11764446645975113), ('against', 0.08542445302009583), ('various', 0.08226475119590759), ('use', 0.08193354308605194)]
</code></pre>
",4,1,1241,2019-06-14 13:33:49,https://stackoverflow.com/questions/56599306/word-not-in-vocabulary-after-training-gensim-word2vec-model-why
How to dynamically assign the right &quot;size&quot; for Word2Vec?,"<p>The question is two-fold:
1.  How to select the ideal value for <code>size</code>?
2.  How to get the vocabulary size dynamically (per row as I intend) to set that ideal size? </p>

<p>My data looks like the following (example)—just one row and one column:</p>

<p>Row 1</p>

<pre><code>{kfhahf}    
Lfhslnf;
.
.
. 
</code></pre>

<p>Row 2</p>

<pre><code>(stdgff  ksshu, hsihf)
asgasf;
.
.
. 
</code></pre>

<p>Etc.</p>

<p>Based on this post: <a href=""https://stackoverflow.com/questions/45444964/python-what-is-the-size-parameter-in-gensim-word2vec-model-class"">Python: What is the &quot;size&quot; parameter in Gensim Word2vec model class</a> The <code>size</code> parameter should be less than (or equal to?) the vocabulary size. So, I am trying to dynamically assign the size as following:</p>

<pre><code>from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

# I do Word2Vec for each row
For item in dataset:
    Tokenized = word_tokenize(item)
    model = Word2Vec([Tokenized], min_count=1)
</code></pre>

<p>I get the vocabulary size here. So I create a second model:</p>

<pre><code>model1 = Word2Vec([Tokenized], min_count=1, size=len(model.wv.vocab))
</code></pre>

<p>This sets the <code>size</code> value to the current vocab value of the current row, as I intended. But is it the right way to do? What is the right size for a small vocabulary text?</p>
","python, python-3.x, nltk, gensim, word2vec","<p>There's no simple formula for the best <code>size</code> - it will depend on your data and purposes.</p>

<p>The best practice is to devise a robust, automatable way to score a set of word-vectors for your purposes – likely with some hand-constructed representative subset of the kinds of judgments, and preferred results, you need. Then, try many values of <code>size</code> (and other parameters) until you find the value(s) that score highest for your purposes. </p>

<p>In the domain of natural language modeling, where vocabularies are at least in the tens-of-thousands of unique words but possibly in the hundreds-of-thousands or millions, typical <code>size</code> values are usually in the 100-1000 range, but very often in the 200-400 range. So you might start a search of alternate values around there, if your task/vocabulary is similar. </p>

<p>But if your data or vocabulary is small, you may need to try smaller values. (Word2Vec really needs large, diverse training data to work best, though.)</p>

<p>Regarding your code-as-shown:</p>

<ul>
<li><p>there's unlikely any point to computing a new <code>model</code> for every <code>item</code> in your dataset (discarding the previous <code>model</code> on each loop iteration). If you want a count of the unique tokens in any one tokenized item, you could use idiomatic Python like <code>len(set(word_tokenize(item)))</code>. Any <code>Word2Vec</code> model of interest would likely need to be trained on the combined corpus of tokens from <strong>all</strong> items.</p></li>
<li><p>it's usually the case that <code>min_count=1</code> makes a model worse than larger values (like the default of <code>min_count=5</code>). Words that only appear once generally can't get good word-vectors, as the algorithm needs multiple subtly-contrasting examples to work its magic. But, trying-and-failing to make useful word-vectors from such singletons tends to take up training-effort and model-state that could be more helpful for other words with adequate examples – so retaining those rare words even makes <strong>other</strong> word-vectors worse. (It is most definitely <strong>not</strong> the case that ""retaining every raw word makes the model better"", though it is almost always the case that ""more real diverse data makes the model better"".)</p></li>
</ul>
",1,0,470,2019-06-14 21:32:01,https://stackoverflow.com/questions/56605373/how-to-dynamically-assign-the-right-size-for-word2vec
Converting string tokens into integers,"<p>I am trying to convert tokens of sentences into integers. But it is giving me floats. </p>

<pre><code>from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

sometext = ""hello how are you doing?""

tokens = word_tokenize(sometext)
model = Word2Vec([tokens], min_count=1, size=1)
</code></pre>

<p>when I do,</p>

<pre><code>print(model[""hello""])
</code></pre>

<p>it gives me,</p>

<pre><code>[-0.3843384]
</code></pre>

<p>I want this to be a positive integer. </p>
","python, python-3.x, nltk, gensim, word2vec","<p>There's no essential reason to use <code>Word2Vec</code> for this. The point of <code>Word2Vec</code> is to map words to multi-dimensional, ""dense"" vectors, with many floating-point coordinates. </p>

<p>Though <code>Word2Vec</code> happens to scan your training corpus for all unique words, and give each unique word an integer position in its internal data-structures, you wouldn't usually make a model of only one-dimension (<code>size=1</code>), or ask the model for the word's integer slot (an internal implementation detail). </p>

<p>If you just need a (string word)->(int id) mapping, the gensim class <code>Dictionary</code> can do that. See:</p>

<p><a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/corpora/dictionary.html</a></p>

<pre><code>from nltk.tokenize import word_tokenize
from gensim.corpora.dictionary import Dictionary

sometext = ""hello how are you doing?""

tokens = word_tokenize(sometext)
my_vocab = Dictionary([tokens])

print(my_vocab.token2id['hello'])
</code></pre>

<p>Now, if there's actually some valid reason to be using <code>Word2Vec</code> – such as needing the multidimensional vectors for a larger vocabulary, trained on a significant amount of varying text – and your real need is to know <strong>its</strong> internal integer slots for words, you can access those via the internal <code>wv</code> property's <code>vocab</code> dictionary:</p>

<pre><code>print(model.wv.vocab['hello'].index)
</code></pre>
",3,1,3792,2019-06-18 04:53:03,https://stackoverflow.com/questions/56641954/converting-string-tokens-into-integers
Gensim Import Error &quot;ImportError: DLL load failed: %1 is not a valid Win32 application.&quot;,"<p>Basically I have installed Gensim 3.7.3 from Python 3.7.1  , but while importing it in Pycharm i got an error: 
""ImportError: DLL load failed: %1 is not a valid Win32 application.""</p>

<p>I want to use Word2Vec model of Gensim but due to this error, I am stuck. I can't Change Python Version too. </p>

<p>Need Help! how I get Gensim Imported in this version of Python using Pycharm </p>

<p>import gensim
from gensim.models import Word2Vec</p>
","python, winapi, dll, pycharm, gensim","<p>Okay so my problem resolved when i tried to import gensim outside Pycharm. 
I Imported directly in python console and it worked. 
Thanks @James.</p>
",0,-1,284,2019-06-19 12:11:57,https://stackoverflow.com/questions/56667348/gensim-import-error-importerror-dll-load-failed-1-is-not-a-valid-win32-appli
Convert a column in a dask dataframe to a TaggedDocument for Doc2Vec,"<h1>Intro</h1>
<p>Currently I am trying to use dask in concert with gensim to do NLP document computation and I'm running into an issue when converting my corpus into a &quot;<a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument"" rel=""nofollow noreferrer"">TaggedDocument</a>&quot;.</p>
<p>Because I've tried so many different ways to wrangle this problem I'll list my attempts.</p>
<p>Each attempt at dealing with this problem is met with slightly different woes.</p>
<h1>First some initial givens.</h1>
<h2>The Data</h2>
<pre><code>df.info()
&lt;class 'dask.dataframe.core.DataFrame'&gt;
Columns: 5 entries, claim_no to litigation
dtypes: object(2), int64(3)
</code></pre>
<pre><code>  claim_no   claim_txt I                                    CL ICC lit
0 8697278-17 battery comprising interior battery active ele... 106 2 0
</code></pre>
<h2>Desired Output</h2>
<pre><code>&gt;&gt;tagged_document[0]
&gt;&gt;TaggedDocument(words=['battery', 'comprising', 'interior', 'battery', 'active', 'elements', 'battery', 'cell', 'casing', 'said', 'cell', 'casing', 'comprising', 'first', 'casing', 'element', 'first', 'contact', 'surface', 'second', 'casing', 'element', 'second', 'contact', 'surface', 'wherein', 'assembled', 'position', 'first', 'second', 'contact', 'surfaces', 'contact', 'first', 'second', 'casing', 'elements', 'encase', 'active', 'materials', 'battery', 'cell', 'interior', 'space', 'wherein', 'least', 'one', 'gas', 'tight', 'seal', 'layer', 'arranged', 'first', 'second', 'contact', 'surfaces', 'seal', 'interior', 'space', 'characterized', 'one', 'first', 'second', 'contact', 'surfaces', 'comprises', 'electrically', 'insulating', 'void', 'volume', 'layer', 'first', 'second', 'contact', 'surfaces', 'comprises', 'formable', 'material', 'layer', 'fills', 'voids', 'surface', 'void', 'volume', 'layer', 'hermetically', 'assembled', 'position', 'form', 'seal', 'layer'], tags=['8697278-17'])
&gt;&gt;len(tagged_document) == len(df['claim_txt'])
</code></pre>
<h1>Error Number 1 No Generators Allowed</h1>
<pre><code>def read_corpus_tag_sub(df,corp='claim_txt',tags=['claim_no']):
    for i, line in enumerate(df[corp]):
        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), (list(df.loc[i,tags].values)))

tagged_document = df.map_partitions(read_corpus_tag_sub,meta=TaggedDocument)
tagged_document = tagged_document.compute()
</code></pre>
<p>TypeError: Could not serialize object of type generator.</p>
<p>I found no way of getting around this while still using a generator. A fix for this would be great! As this works perfectly fine for regular pandas.</p>
<h1>Error Number 2 Only the first element of each partition</h1>
<pre><code>def read_corpus_tag_sub(df,corp='claim_txt',tags=['claim_no']):
    for i, line in enumerate(df[corp]):
        return gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), (list(df.loc[i,tags].values)))

tagged_document = df.map_partitions(read_corpus_tag_sub,meta=TaggedDocument)
tagged_document = tagged_document.compute()
</code></pre>
<p>This one is a bit dumb as the function won't iterate (I know) but gives the desired format, but only returns the first row in each partition.</p>
<h1>Error Number 3 function call hangs with 100% cpu</h1>
<pre><code>def read_corpus_tag_sub(df,corp='claim_txt',tags=['claim_no']):
    tagged_list = []
    for i, line in enumerate(df[corp]):
        tagged = gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), (list(df.loc[i,tags].values)))
        tagged_list.append(tagged)
    return tagged_list
</code></pre>
<p>Near as I can tell when refactoring the return outside the loop this function hangs builds memory in the dask client and my CPU utilization goes to 100% but no tasks are being computed. Keep in mind I'm calling the function the same way.</p>
<h1>Pandas Solution</h1>
<pre><code>def tag_corp(corp,tag):
    return gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(corp), ([tag]))

tagged_document = [tag_corp(x,y) for x,y in list(zip(df_smple['claim_txt'],df_smple['claim_no']))]
</code></pre>
<p>List comp I haven't time tested this solution</p>
<h1>Other Pandas Solution</h1>
<pre><code>tagged_document = list(read_corpus_tag_sub(df))
</code></pre>
<p>This solution will chug along pretty much for hours. However I don't have enough memory to juggle this thing when it's done.</p>
<h1>Conclusion(?)</h1>
<p>I feel Super lost right now. Here is a list of threads I've looked at. I admit to being really new to dask I've just spent so much time and I feel like I'm on a fools errand.</p>
<ol>
<li><a href=""https://stackoverflow.com/questions/50862165/creating-a-dask-bag-from-a-generator"">Dask Bag from generator</a></li>
<li><a href=""https://medium.com/mindorks/speeding-up-text-pre-processing-using-dask-45cc3ede1366"" rel=""nofollow noreferrer"">Processing Text With Dask</a></li>
<li><a href=""https://gdcoder.com/speed-up-pandas-apply-function-using-dask-or-swifter-tutorial/"" rel=""nofollow noreferrer"">Speed up Pandas apply using Dask</a></li>
<li><a href=""https://stackoverflow.com/questions/45545110/how-do-you-parallelize-apply-on-pandas-dataframes-making-use-of-all-cores-on-o"">How do you parallelize apply() on Pandas Dataframes making use of all cores on one machine?</a></li>
<li><a href=""https://stackoverflow.com/questions/31361721/python-dask-dataframe-support-for-trivially-parallelizable-row-apply"">python dask DataFrame, support for (trivially parallelizable) row apply?</a></li>
<li><a href=""https://stackoverflow.com/questions/39215617/what-is-map-partitions-doing"">What is map_partitions doing?</a></li>
<li><a href=""https://stackoverflow.com/questions/47125665/simple-dask-map-partitions-example"">simple dask map_partitions example</a></li>
<li><a href=""https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions"" rel=""nofollow noreferrer"">The Docs</a></li>
</ol>
","python, dask, gensim, doc2vec","<p>I'm not familiar with the Dask APIs/limitations, but generally:</p>

<ul>
<li><p>if you can iterate over your data as (words, tags) tuples – even ignoring the <code>Doc2Vec</code>/<code>TaggedDocument</code> steps – then the Dask side will have been handled, and converting those tuples to <code>TaggedDocument</code> instances should be trivial</p></li>
<li><p>in general for large datasets, you don't want to (and may not have enough RAM to) instantiate the full dataset as a <code>list</code> in memory – so your attempts that involve a <code>list()</code> or <code>.append()</code> may be working, up to a point, but exhausting local memory (causing severe swapping) and/or just not reaching the end of your data.</p></li>
</ul>

<p>The preferable approach to large datasets is to create an iterable object that, every time it is asked to iterate over the data (because <code>Doc2Vec</code> training will require multiple passes), can offer up each and every item in turn – but never reading the entire dataset into an in-memory object. </p>

<p>A good blogpost on this pattern is: <a href=""https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/"" rel=""nofollow noreferrer"">Data streaming in Python: generators, iterators, iterables</a> </p>

<p>Given the code you've shown, I suspect the right approach for you may be like:</p>

<pre><code>from gensim.utils import simple_preprocess

class MyDataframeCorpus(object):
    def __init__(self, source_df, text_col, tag_col):
        self.source_df = source_df
        self.text_col = text_col
        self.tag_col = tag_col

    def __iter__(self):
        for i, row in self.source_df.iterrows():
            yield TaggedDocument(words=simple_preprocess(row[self.text_col]), 
                                 tags=[row[self.tag_col]])

corpus_for_doc2vec = MyDataframeCorpus(df, 'claim_txt', 'claim_no')
</code></pre>
",5,2,1400,2019-06-20 07:38:02,https://stackoverflow.com/questions/56681210/convert-a-column-in-a-dask-dataframe-to-a-taggeddocument-for-doc2vec
How to add numbers with more than one digit to the word2vec-vocabulary,"<p>I am trying to get the embeddings for a list of 1043 nodes with word2vec. When I try to build the vocabulary I find that word2vec takes the list of lists with the nodes and treats them as single digits, eg that ""143"" becomes ""1"",""4"",""3"".</p>

<p>I already tried to have all the numbers as single entries and see wether it is an formatting problem and went with a buil_vocab_from_freq instead of build_vocab, but this also just produces errors (object of type 'int' has no len()).</p>

<p>My code is the following:</p>

<pre><code>from gensim.models import Word2Vec

def generateEmbeddings(all_walks,dimension,min_count):
    model = Word2Vec(min_count = min_count, size = dimension)
    mylist = list(range(1,1043))
    corpus = {}
    j=1
    for i in mylist:
      corpus[str(i)] = j
      j=j+1
    #mylist = [str(i) for i in mylist]
    print(corpus)
    model.build_vocab_from_freq(corpus)
    model.train(mylist, total_examples=model.corpus_count, epochs = 30)
    #if it reaches this point it throws the error ""14 not found in vocabulary""
    print(model.wv.most_similar(positive=['14']))
    return model

print(generateEmbeddings(all_walks,128,2))
</code></pre>

<p>I want to get the embedding for eg. the number ""14"" and not ""1"" as it is by now. Thanks for your help!</p>

<p>//Edit</p>

<p>I managed to fix this, if anybody else is having this specific problem:
you have to format the list as mentioned as [[""1"",""102"",""43""],[""54"",""43""]] etc.
You cant change the old list at runtime (or at least it didnt work the way I did it), so you could create a new list at runtime with</p>

<pre><code>new_list = []
    for i in all_walks:
      temp_list = []
      for j in i:
        temp_list.append(str(j))
      new_list.append(temp_list)
</code></pre>
","python, gensim, word2vec","<p>Per our discussion above, the working approach will feed <code>Word2Vec</code> the kind of corpus it expects – an iterable sequence, where each item is a list of string-tokens. </p>

<p>So, a list-of-lists-of-strings would work, something like...</p>

<pre><code>[
  ['1','2','3'],
  ['1','2','4'],
  ['10','11','12'],
  ['10','14','15','900']
]
</code></pre>

<p>...rather than anything with raw ints in it (like <code>list(range(1, 1043)</code>). </p>
",0,0,162,2019-06-20 13:16:11,https://stackoverflow.com/questions/56686830/how-to-add-numbers-with-more-than-one-digit-to-the-word2vec-vocabulary
How do I use the wikipedia dump as a Gensim model?,"<p>I am trying to use the English Wikipedia dump (<a href=""https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2</a>) as my pre-trained word2vec model using <code>Gensim</code>.</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors

model_path = 'enwiki-latest-pages-articles.xml.bz2'
w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)
</code></pre>

<p>when I do this, I get</p>

<pre><code>   342     with utils.smart_open(fname) as fin:
    343         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 344         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    345         if limit:
    346             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: '&lt;mediawiki'
</code></pre>

<p>Do I have to re-download or something?</p>
","python, gensim, word2vec","<p>That dump file includes the actual Wikipedia articles in an XML format – no vectors. The <code>load_word2vec_format()</code> methods only load sets-of-vectors that were trained earlier. </p>

<p>Your <code>gensim</code> installation's <code>docs/notebooks</code> directory includes a number of demo Jupyter notebooks you can run. One of those, <code>doc2vec-wikipedia.ipynb</code>, shows training document-vectors based on the Wikipedia articles dump. (It could be adapted fairly easily to train only word-vectors instead.)</p>

<p>You can also view this notebook online at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a> </p>

<p>Note that you'll learn more from these if you run them locally, and enable logging at the INFO level. Also, this particular training may take a full day or more to run, and require a machine with 16GB or more or RAM.</p>
",1,0,1496,2019-06-22 12:17:33,https://stackoverflow.com/questions/56715394/how-do-i-use-the-wikipedia-dump-as-a-gensim-model
How to save a list of Gensim LDA models?,"<p>I have different LDA models (on the same text, but all with different #topics) stored in one list. Now, I want to save this list with all the models in it to my disk. However, I am not sure how this works. Should I treat is as a list or as a LDA model?</p>

<p>On the <a href=""https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.save"" rel=""nofollow noreferrer"">gensim website</a> I found the following code:</p>

<pre><code>from gensim.test.utils import datapath
&gt;&gt;&gt;
&gt;&gt;&gt; # Save model to disk.
&gt;&gt;&gt; temp_file = datapath(""model"")
&gt;&gt;&gt; lda.save(temp_file)
</code></pre>

<p>However, this works for separate LDA models, not for lists with multiple models. What is the best way to save my list of models?</p>
","python, list, save, gensim","<p>Say <code>trained_models</code> is your list of LDA models: </p>

<pre><code>trained_models = 
[&lt;gensim.models.ldamodel.LdaModel at 0x1f321825668&gt;,
 &lt;gensim.models.ldamodel.LdaModel at 0x1f32181ffd0&gt;]
</code></pre>

<p>You can save multiple files at once with a for loop and counter:</p>

<pre><code>i = 1
for model in trained_models:
    model.save(""model{}.gensim"".format(i))
    i += 1
</code></pre>

<p>I don't know of a gensim function that does this, but if there's a more efficient way to do this, I'd love to know as well.</p>
",3,0,1219,2019-06-27 15:05:26,https://stackoverflow.com/questions/56793969/how-to-save-a-list-of-gensim-lda-models
Dynamic Topic Model Path,"<p>Thanks for stopping by!  I have a question about the dynamic topic model path: </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&gt;&gt;&gt; from gensim.test.utils import common_corpus, common_dictionary
&gt;&gt;&gt; from gensim.models.wrappers import DtmModel
&gt;&gt;&gt;
&gt;&gt;&gt; path_to_dtm_binary = ""/path/to/dtm/binary""
&gt;&gt;&gt; model = DtmModel(
...     path_to_dtm_binary, corpus=common_corpus, id2word=common_dictionary,
...     time_slices=[1] * len(common_corpus)</code></pre>
</div>
</div>
</p>

<p>what is the path the dynamic topic model binary?  Is that something I need to install or download?  Where can I install or download that?</p>

<p>Thanks!</p>
","python, nlp, gensim, lda, topic-modeling","<p>According to <a href=""https://radimrehurek.com/gensim/models/wrappers/dtmmodel.html"" rel=""nofollow noreferrer"">here</a>, you can go one of two ways:</p>

<ol>
<li><p>Use precompiled binaries for your OS version from <a href=""https://github.com/magsilva/dtm/tree/master/bin"" rel=""nofollow noreferrer"">https://github.com/magsilva/dtm/tree/master/bin</a></p></li>
<li><p>Compile binaries manually from /blei-lab/dtm (original instruction available in <a href=""https://github.com/blei-lab/dtm/blob/master/README.md"" rel=""nofollow noreferrer"">https://github.com/blei-lab/dtm/blob/master/README.md</a>), or use this</p></li>
</ol>

<pre><code>git clone https://github.com/blei-lab/dtm.git
sudo apt-get install libgsl0-dev
cd dtm/dtm
make
</code></pre>
",2,0,448,2019-06-27 18:22:26,https://stackoverflow.com/questions/56796761/dynamic-topic-model-path
Columnwise Summarize multiple sentences present in a list using the gensim summarizer,"<p>I am having a data-set consisting of faculty id and the feedback of students regarding the respective faculty. There are multiple comments for each faculty and therefore the comments regarding each faculty are present in the form of a list. I want to apply gensim summarization on the ""comments"" column of the data-set to generate the summary of faculty performance according to the student feedback.</p>

<p>Just for a trial I tried to summarize the feedbacks corresponding to the first faculty id. There are 8 distinct comments (sentences) in that particular feedback, still gensim throws an error ValueError: input must have more than one sentence. </p>

<pre><code>df_test.head()
    csf_id  comments
0   9   [' good subject knowledge.', ' he has good kn...
1   10  [' good knowledge of subject. ', ' good subjec...
2   11  [' good at clearing the concepts interactive w...
3   12  [' clears concepts very nicely interactive wit...
4   13  [' good teaching ability.', ' subject knowledg...
from gensim.summarization import summarize
text = df_test[""comments""][0]
print(""Text"")
print(text)
print(""Summary"")
print(summarize(text))
</code></pre>

<blockquote>
  <p>ValueError: input must have more than one sentence  </p>
</blockquote>

<p>what changes shold i make so that the summarizer reads all the sentenses and summarizes them.</p>
","python, nlp, gensim","<p>for gensim summarization, newline and full stop will divide the sentence. </p>

<pre><code> from gensim.summarization.summarizer import summarize


summarize(""punctual in time."") 
</code></pre>

<p>this will throw Same error ValueError: input must have more than one sentence</p>

<p>now when there is something after full stop it will interpret it as more than one sentence</p>

<pre><code>summarize(""punctual in time. good subject knowledge"")     
#o/p will be blank string since the text is very small, and now you won't receive any error
''
</code></pre>

<p>Now coming to ur problem, you need to join all the element into one string </p>

<pre><code> #example
 import pandas as pd
df = pd.DataFrame([[[""good subject."","" punctual in time."",""discipline person.""]]], columns = ['comment'])

print(df)
    comment
0   [good subject., punctual in time, discipline ...

df['comment'] = df['comment'].apply(''.join)


df['comment'].apply(summarize) #this will work for you but keep in mind you have long text to generate summary 
</code></pre>
",0,0,993,2019-07-04 08:45:50,https://stackoverflow.com/questions/56883959/columnwise-summarize-multiple-sentences-present-in-a-list-using-the-gensim-summa
Gensim Word2Vec Vocabulary: Unclear output,"<p>I'm starting to get familiar with Word2Vec, but I'm struggeling with a problem and coudln't find something similar...
I want to use gensims Word2Vec on an imported PDF document (a book). To import I used PyPDF2 and stored the whole book into a list. Furthermore, I used gensims simple_preprocess in order to preprocess the data. This worked so far, I got the following output:</p>

<pre class=""lang-py prettyprint-override""><code>text=['schottky','diode','semiconductors',...]
</code></pre>

<p>So then I tried to use the Word2Vec:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec
model=Word2Vec(text, size=100, window=5, min_count=5, workers=4)
words=list(model.wv.vocab)

</code></pre>

<p>but the output was like this:</p>

<pre class=""lang-py prettyprint-override""><code>print(words)
['c','h','t','k','d',...]
</code></pre>

<p>I expected also the same words as in the text list and not just some characters. When I tried to find relations between words (e.g. 'schottky' and 'diode') I got the error-message that none of these words is included in the vocabulary.</p>

<p>My first thought was that the import is wrong, but I got the same result with textract instead of PyPDF2.</p>

<p>Does someone know what's the problem? Thanks for your help!</p>

<p>Appendix:</p>

<p>Importing the book</p>

<p>content_text=[]
number_of_inputs=len(os.listdir(path))</p>

<pre><code>    file_to_open=path
open_file=open(file_to_open,'rb')
read_pdf=PyPDF2.PdfFileReader(open_file)
number_of_pages=read_pdf.getNumPages()
page_content=""""
for page_number in range(number_of_pages):
    page = read_pdf.getPage(page_number)
    page_content += page.extractText()
content_text.append(page_content)
</code></pre>
","python, python-3.x, text-mining, gensim, word2vec","<p><code>Word2Vec</code> requires as its <code>sentences</code> parameter a training corpus that is:</p>

<ul>
<li>an iterable sequence (such as a list)</li>
<li>where each item is itself a list of string-tokens</li>
</ul>

<p>If you supply just a list-of-strings, each string is seen as a list-of-one-character-strings, resulting in all the one-letter words you're seeing. </p>

<p>So, use a list-of-lists-of-words, more like:</p>

<pre><code>[
 ['schottky','diode','semiconductors'],
]
</code></pre>

<p>(Note also that you generally won't get interesting <code>Word2Vec</code> results on tiny toy-sized data sets of just a few texts and just dozens to hundreds of words. You need many thousands of unique words, across many dozens of contrasting examples of each word, to induce the useful word-vector arrangements that <code>Word2Vec</code> is known for.)</p>
",1,0,967,2019-07-04 13:49:34,https://stackoverflow.com/questions/56889408/gensim-word2vec-vocabulary-unclear-output
How to identify doc2vec instances seperately in gensim in python,"<p>I have a list of 1000 documents, where the first 500 belongs to documents in <code>movies</code> (i.e. list index from <code>0</code> to <code>499</code>) and the remaining 500 belings to documents in <code>tv series</code> (i.e. list index from <code>500</code> to <code>999</code>).</p>

<p>For movies the <code>document tag</code> starts with <code>movie_</code> (e.g., <code>movie_fast_and_furious</code>) and for tv series the document tag starts with <code>tv_series_</code> (e.g., <code>tv_series_the_office</code>)</p>

<p>I use these movies and tv series dataset to build a <code>doc2vec</code> model as follows.</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple

dataset = json.load(open(input_file))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')

for description in dataset:
    tags = [description[0]]
    words = description[1]
    docs.append(analyzedDocument(words, tags))

model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4, epochs = 20)
</code></pre>

<p>Now for each <code>movie</code>, I want to get its nearest 5 <code>tv series</code> along with their cosine similarity.</p>

<p>I know, the function gensim provides <code>model.docvecs.most_similar</code>. However, the results of this include movies as well (which is not my intension). Is it possible to do this in gensim (I assume that the document vectors are creating in the order of the <code>documents list</code> that we provide).</p>

<p>I am happy to provide more details if needed.</p>
","python, gensim","<p>All the <code>tags</code> are opaque identifiers to <code>Doc2Vec</code>. So, if there's internal distinctions to your data, you'll need to model and filter on that yourself. </p>

<p>So, my main recommendation would be to ask for a much larger <code>topn</code> than you need, then discard those results of the type you don't want, or in excess of the number you actually need.</p>

<p>(Note that every calculation of <code>most_similar()</code> requires a comparison against the whole known set of doc-tags, and using a smaller <code>topn</code> only saves some computation in sorting of those full results. So using a larger <code>topn</code>, even up to the full size of the known doc-tags, isn't as costly as you might fear.)</p>

<p>With just two categories, to get the 10 tv-shows closest to a query movie, you could make <code>topn</code> equal to the count of movies, minus 1 (the query), plus 10 – then in the absolute worst-case, where all movies are closer than than the 1st tv-show, you'd still get 10 valid tv-show results.</p>

<p>(The <code>most_similar()</code> function also includes a <code>restrict_vocab</code> parameter. It takes an int count, and limits results to only the 1st that-many items, in the internal storage order. So if in fact the 1st 500 documents were all tv-shows, <code>restrict_vocab=500</code> would give only results from that subset. However, I wouldn't recommend relying on this, as (a) it'd only work for one category that was front-loaded, not any others; (b) ideally for training, you <em>wouldn't</em> clump all similar documents together, but shuffle them to be interspersed with contrasting documents. Generally <code>Word2Vec</code> vector-sets are sorted to put the highest-frequency words 1st – no matter the order-of-appearance in the original data. That makes <code>restrict_vocab</code> more useful there, as often only results for the most-common words with the strongest vectors are most interesting.)</p>
",1,1,174,2019-07-05 05:48:32,https://stackoverflow.com/questions/56897173/how-to-identify-doc2vec-instances-seperately-in-gensim-in-python
Comparing NumPy Arrays for Similarity,"<p>I have a target NumPy array with shape (300,) and a set of candidate arrays also of shape (300,). These arrays are Word2Vec representations of words; I'm trying to find the candidate word that is most similar to the target word using their vector representations. What's the best way to find the candidate word that is most similar to the target word?</p>

<p>One way to do this is to sum up the absolute values of the element-wise differences between the target word and the candidate words, then select the candidate word with the lowest overall absolute difference. For example:</p>

<pre><code>candidate_1_difference = np.subtract(target_vector, candidate_vector)
candidate_1_abs_difference = np.absolute(candidate_1_difference)
candidate_1_total_difference = np.sum(candidate_1_abs_difference)
</code></pre>

<p>Yet, this seems clunky and potentially wrong. What's a better way to do this?</p>

<p>Edit to include example vectors:</p>

<pre><code>import numpy as np
import gensim

path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'


def func1(path):
    #Limited to 50K words to reduce load time
    model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True, limit=50000)
    context =  ['computer','mouse','keyboard']
    candidates = ['office','house','winter']
    vectors_to_sum = []
    for word in context:
        vectors_to_sum.append(model.wv[word])
    target_vector = np.sum(vectors_to_sum)

    candidate_vector = candidates[0]
    candidate_1_difference = np.subtract(target_vector, candidate_vector)
    candidate_1_abs_difference = np.absolute(candidate_1_difference)
    candidate_1_total_difference = np.sum(candidate_1_abs_difference)
    return candidate_1_total_difference
</code></pre>
","python, numpy, gensim","<p>What you have is basically correct. You are calculating the L1-norm, which is the sum of absolute differences. Another more common option is to calculate the euclidean norm, or the L2-norm, which is the familiar distance measure of square root of sum of squares. </p>

<p>You can use <code>numpy.linalg.norm</code> to calculate the different norms, which by default calculates the L-2 norm for vectors. </p>

<pre><code>distance = np.linalg.norm(target_vector - candidate_vector)
</code></pre>

<p>If you have one target vector and multiple candidate vectors stored in a list, the above still works, but you need to specify the axis for norm, and then you get a vector of norms, one for each candidate vector. </p>

<p>for list of candidate vectors: </p>

<pre><code>distance = np.linalg.norm(target_vector - np.array(candidate_vector), axis=1)
</code></pre>
",3,2,6427,2019-07-05 19:33:03,https://stackoverflow.com/questions/56908407/comparing-numpy-arrays-for-similarity
How to set time slices - Dynamic Topic Model,"<p><strong>Intro</strong></p>

<p>Currently I am using Gensim in combination with pandas and numpy to run document NLP computation.  I'd like to build a LDA seqential model to track how our topics change over time but am running into errors with the corpus format.</p>

<p>I am trying to figure out how to set time slices for dynamic topic models.  I am using <a href=""https://radimrehurek.com/gensim/models/ldaseqmodel.html"" rel=""nofollow noreferrer"">LdaSeqModel</a> which requires an integer time slice. </p>

<p><strong>The Data</strong></p>

<p>It's a csv:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>data = pd.read_csv('CGA Jan17 - Mar19 Time Slice.csv', encoding = ""ISO-8859-1"");
documents = data[['TextForTopics']]
documents['index'] = documents.index</code></pre>
</div>
</div>
</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>	       Month	Year	Begin Date	TextForTopics	                                      time_slice
0	march	2017	3/23/2017	request: the caller is requesting an appointme...	1</code></pre>
</div>
</div>
</p>

<p>This is then converted into an array of tuples called the bow_corpus:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[[(12, 2), (25, 1), (30, 1)], [(33, 1), (136, 1), (159, 1), (161, 1)], [(165, 1), (247, 2)], (326, 1), (354, 1), (755, 1), (821, 1)]]</code></pre>
</div>
</div>
</p>

<p><strong>Desired Output</strong></p>

<p>It should print one topic allocation for each time slice. If I entered 3 topics and two time slices I should get three topics printed twice showing how the topics evolved over time.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[(0,
  '0.165*""enrol"" + 0.108*""medicar"" + 0.051*""form""),
(1,
  '0.303*""caller"" + 0.290*""inform"" + 0.031*""abl""),
(2,
  '0.208*""date"" + 0.140*""effect"" + 0.060*""medicaid""')]
[(0,
  '0.165*""enrol"" + 0.108*""cats"" + 0.051*""form""),
(1,
  '0.303*""caller"" + 0.290*""puppies"" + 0.031*""abl""),
(2,
  '0.208*""date"" + 0.140*""elephants"" + 0.060*""medicaid""')]</code></pre>
</div>
</div>
</p>

<p><strong>What I've tried</strong></p>

<p>This is the function - the bow corpus is an array of tuples</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>ldaseq = LdaSeqModel(corpus=bow_corpus, time_slice=[], num_topics=15, chunksize=1)</code></pre>
</div>
</div>
</p>

<p>I've tried every version of integer inputs for those time_slices and they all produce errors.  The premise was that the time_slice would represent the number of indicies/rows/documents in each time slice.  For example my data has 1.8 million rows if I wanted two time slices I would order my data by time and enter an integer cutoff like time_slice = [489234, 1310766].  All inputs produce this error:</p>

<p><strong>The Error</strong>
<div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-5-e58059a7fb6f&gt; in &lt;module&gt;
----&gt; 1 ldaseq = LdaSeqModel(corpus=bow_corpus, time_slice=[], num_topics=15, chunksize=1)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in __init__(self, corpus, time_slice, id2word, alphas, num_topics, initialize, sstats, lda_model, obs_variance, chain_variance, passes, random_state, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    186 
    187             # fit DTM
--&gt; 188             self.fit_lda_seq(corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    189 
    190     def init_ldaseq_ss(self, topic_chain_variance, topic_obs_variance, alpha, init_suffstats):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in fit_lda_seq(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)
    275             # seq model and find the evidence lower bound. This is the E - Step
    276             bound, gammas = \
--&gt; 277                 self.lda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)
    278             self.gammas = gammas
    279 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in lda_seq_infer(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)
    351             bound, gammas = self.inferDTMseq(
    352                 corpus, topic_suffstats, gammas, lhoods, lda,
--&gt; 353                 ldapost, iter_, bound, lda_inference_max_iter, chunksize
    354             )
    355         elif model == ""DIM"":

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in inferDTMseq(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)
    401         time = 0  # current time-slice
    402         doc_num = 0  # doc-index in current time-slice
--&gt; 403         lda = self.make_lda_seq_slice(lda, time)  # create lda_seq slice
    404 
    405         time_slice = np.cumsum(np.array(self.time_slice))

~\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\ldaseqmodel.py in make_lda_seq_slice(self, lda, time)
    459         """"""
    460         for k in range(self.num_topics):
--&gt; 461             lda.topics[:, k] = self.topic_chains[k].e_log_prob[:, time]
    462 
    463         lda.alpha = np.copy(self.alphas)

IndexError: index 0 is out of bounds for axis 1 with size 0</code></pre>
</div>
</div>
</p>

<p><strong>Solutions</strong></p>

<p>I tried going back to the documentation and looking at the format of the common_corpus used as an example and the format of my bow_corpus is the same.  I also tried running the code in the documentation to see how it worked but it also produced the same error.  I'm not sure if the problem is my code anymore but I hope it is.</p>

<p>I've also tried messing with the file format by manually dividing my csv into 9 csvs containing my time_slices and creating an iterated corpus out of those, but that didn't work.  I've considered converting each row of my csv into txt files and then creating a corpus out of that like David Beil does, but that sounds pointlessly tedious as I already have an iterated corpus.</p>
","python-3.x, nlp, gensim, lda, topic-modeling","<p>I'm going to assume you are working in a single dataframe. Let's say you want to use years as your unit of time.</p>

<ol>
<li>For <code>time_slice</code> to work properly with <code>ldaseqmodel</code> you need to
first order your dataframe ascending, i.e. from oldest to newest.</li>
<li>Create a time_slice variable so you can later feed it back into the model</li>
</ol>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
uniqueyears, time_slices = np.unique(data.Year, return_counts=True) 
#takes all unique values in data.Year as well as how often they occur and returns them as an array.

print(np.asarray((uniqueyears, time_slices)).T) 
#see what youve made, technically you dont need this
</code></pre>

<p>returns (using example data)</p>

<pre class=""lang-py prettyprint-override""><code>[[1992   28]
 [1993   18]
 [1994   25]
 [1995   18]
 [1996   44]
 [1997   38]
 [1998   30]]
</code></pre>

<p>This works for years, if you want to go more fine-grained, you could adapt the same concept, as long as you have the ordering of the documents (which is how gensim connects them to time slices) right. So, for example if you want to take monthly slices, you could rewrite the dates as 20173 for March 2017 and 20174 for April 2014. Really, any grain will do as long as you can identify documents as belonging to the same slice. </p>
",2,2,1618,2019-07-05 21:14:14,https://stackoverflow.com/questions/56909294/how-to-set-time-slices-dynamic-topic-model
gensim installed in anaconda env but won&#39;t import in jupyter notebook,"<p>I'm trying to install gensim in a specific conda env on my Python 3 only, Windows 10 machine. I've tried 3 different ways based on suggestions in SO and elsewhere, summarized below. Each time it shows as successfully installed and present in the env, but when I try to import it in jupyter notebook I get the <code>ModuleNotFoundError: No module named 'gensim'</code> error. </p>

<p>Note: I closed and relaunched anaconda and jupyter after each install.</p>

<p>SUMMARY: 
3 attempts with 3 install commands:</p>

<pre><code>COMMAND                              CONDA LIST                              IMPORT IN JUPYTER NOTEBOOK
conda install -c anaconda gensim     gensim 3.4.0 py36hfa6e2cd_0 anaconda    ModuleNotFoundError: No module named 'gensim'
pip install -U gensim                gensim 3.7.3 pypi_0 pypi                ModuleNotFoundError: No module named 'gensim'
conda install -c conda-forge gensim  gensim 3.7.3 py36h6538335_0 conda-forge ModuleNotFoundError: No module named 'gensim'
</code></pre>

<pre><code>(base) C:\Users\kb&gt;conda activate SARC
(SARC) C:\Users\kb&gt;conda install -c anaconda gensim
(SARC) C:\Users\kb&gt;conda list
. . .
gensim                    3.4.0            py36hfa6e2cd_0    anaconda
. . .

. . .
</code></pre>

<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-1-e92e291fb8cb&gt; in &lt;module&gt;
      1 import loader
      2 import reader
----&gt; 3 import transformers
      4 import vectorization

~\OneDrive\Documents\ds\courses_books\Applied_Text_Analysis_Python_book_code\atap-master\snippets\ch04\transformers.py in &lt;module&gt;
      3 import os
      4 import nltk
----&gt; 5 import gensim
      6 import unicodedata
      7 

ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>Details of the install commands and output can be seen <a href=""https://github.com/nicolas-ivanov/debug_seq2seq/issues/23#issuecomment-508880177"" rel=""nofollow noreferrer"">here</a>.</p>
","python, jupyter-notebook, anaconda, conda, gensim","<p>Per our discussion in the comments, when you launch via <code>jupyter</code> directly, it seems you're using some other (perhaps system-wide) Python interpreter &amp; environment – one which doesn't have <code>gensim</code> installed – even though you've ""activated"" your <code>SARC</code> environment. (Essentially, ""activating"" an environment tries to alias <code>python</code> &amp; some other things to use the right enviroment, but other commands might still reach out to some other Python installation.)</p>

<p>Per @furas' suggestion of using <code>python -m jupyter notebook</code>, you'll be sure to invoke plain <code>python</code>, and thus the <code>SARC</code> environment – and thus test whether <code>gensim</code> and/or <code>jupyter</code> are even really installed inside the <code>SARC</code> environment. </p>

<p>If not, be sure to install them there, and make sure any interpreters/notebooks you launch truly use the intended environment. </p>
",1,1,3223,2019-07-06 01:11:46,https://stackoverflow.com/questions/56910538/gensim-installed-in-anaconda-env-but-wont-import-in-jupyter-notebook
"In Gensim Word2vec, how to reduce the vocab size of an existing model?","<p>In Gensims word2vec api, I trained a model where I initialized the model with max_final_vocab = 100000 and saved the model using model.save()
(This gives me one .model file, one .model.trainables.syn1neg.npy and one .model.wv.vectors.npy file).</p>

<p>I do not need to train model any further, so I'm fine with using just</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.Word2Vec.load(""train.fr.model"")
kv = model.wv
del model


</code></pre>

<p>the kv variable shown here. I now want to use only the <em>top</em> N (N=40000 in my case) vocabulary items instead of the entire vocabulary. The only way to even attempt cutting down the vocabulary I could find was</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
emb_matrix = np.load(""train.fr.model.wv.vectors.npy"")
emb_matrix.shape
# (100000, 300)
new_emb_matrix = emb_matrix[:40000]
np.save(""train.fr.model.wv.vectors.npy"", new_emb_matrix)
</code></pre>

<p>If I load this model again though, the vocabulary still has length 100000.</p>

<p>I want to reduce the vocabulary of the model or model.wv while retaining a working model. Retraining is not an option.</p>
","gensim, word2vec, vocabulary","<pre class=""lang-py prettyprint-override""><code>from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format('train.fr.model', limit=1000)
</code></pre>

<p>Use optional <code>limit</code>parameter to reduce number of vectors that will be loaded from <strong>Word2Vec</strong> model file.</p>
",4,1,2008,2019-07-10 10:37:59,https://stackoverflow.com/questions/56968915/in-gensim-word2vec-how-to-reduce-the-vocab-size-of-an-existing-model
How to train a word embedding representation with gensim fasttext wrapper?,"<p>I would like to train my own word embeddings with fastext. However, after following the tutorial I can not manage to do it properly. So far I tried:</p>

<p>In:</p>

<pre><code>from gensim.models.fasttext import FastText as FT_gensim

# Set file names for train and test data
corpus = df['sentences'].values.tolist()

model_gensim = FT_gensim(size=100)

# build the vocabulary
model_gensim.build_vocab(sentences=corpus)
model_gensim
</code></pre>

<p>Out:</p>

<pre><code>&lt;gensim.models.fasttext.FastText at 0x7f6087cc70f0&gt;
</code></pre>

<p>In:</p>

<pre><code># train the model
model_gensim.train(
    sentences = corpus, 
    epochs = model_gensim.epochs,
    total_examples = model_gensim.corpus_count, 
    total_words = model_gensim.corpus_total_words
)

print(model_gensim)
</code></pre>

<p>Out:</p>

<pre><code>FastText(vocab=107, size=100, alpha=0.025)
</code></pre>

<p>However, when I try to look in a vocabulary words:</p>

<pre><code>print('return' in model_gensim.wv.vocab)
</code></pre>

<p>I get <code>False</code>, even the word is present in the sentences I am passing to the fast text model. Also, when I check the most similar words to return I am getting characters:</p>

<pre><code>model_gensim.most_similar(""return"")

[('R', 0.15871645510196686),
 ('2', 0.08545402437448502),
 ('i', 0.08142799884080887),
 ('b', 0.07969795912504196),
 ('a', 0.05666942521929741),
 ('w', 0.03705815598368645),
 ('c', 0.032348938286304474),
 ('y', 0.0319858118891716),
 ('o', 0.027745068073272705),
 ('p', 0.026891689747571945)]
</code></pre>

<p>What is the correct way of using gensim's fasttext wrapper?</p>
","machine-learning, nlp, gensim, word-embedding, fasttext","<p>The gensim <code>FastText</code> class doesn't take plain strings as its training texts. It expects lists-of-words, instead. If you pass plain strings, they will look like lists-of-single-characters, and you'll get a stunted vocabulary like you're seeing.</p>

<p>Tokenize each item of your <code>corpus</code> into a list-of-word-tokens and you'll get closer-to-expected results. One super-simple way to do this might just be:</p>

<pre><code>corpus = [s.split() for s in corpus]
</code></pre>

<p>But, usually you'd want to do other things to properly tokenize plain-text as well – perhaps case-flatten, or do something else with punctuation, etc.</p>
",2,1,1335,2019-07-15 05:14:25,https://stackoverflow.com/questions/57033566/how-to-train-a-word-embedding-representation-with-gensim-fasttext-wrapper
"After training word embedding with gensim&#39;s fasttext&#39;s wrapper, how to embed new sentences?","<p>After reading the tutorial at gensim's <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb"" rel=""nofollow noreferrer"">docs</a>, I do not understand what is the correct way of generating new embeddings from a trained model. So far I have trained gensim's fast text embeddings like this:</p>

<pre><code>from gensim.models.fasttext import FastText as FT_gensim

model_gensim = FT_gensim(size=100)

# build the vocabulary
model_gensim.build_vocab(corpus_file=corpus_file)

# train the model
model_gensim.train(
    corpus_file=corpus_file, epochs=model_gensim.epochs,
    total_examples=model_gensim.corpus_count, total_words=model_gensim.corpus_total_words
)
</code></pre>

<p>Then, let's say I want to get the embeddings vectors associated with this sentences:</p>

<pre><code>sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()
sentence_president = 'The president greets the press in Chicago'.lower().split()
</code></pre>

<p>How can I get them with <code>model_gensim</code> that I trained previously?</p>
","machine-learning, nlp, gensim, embedding","<p>You can look up each word's vector in turn:</p>

<pre><code>wordvecs_obama = [model_gensim[word] for word in sentence_obama]
</code></pre>

<p>For your 7-word input sentence, you'll then have a list of 7 word-vectors in <code>wordvecs_obama</code>.</p>

<p>All FastText models do not, as a matter of their inherent functionality, convert longer texts into single vectors. (And specifically, the model you've trained doesn't have a default way of doing that.)</p>

<p>There is a ""classification mode"" in the original Facebook FastText code that involves a different style of training, where texts are associated with known labels at training time, and all the word-vectors of the sentence are combined during training, and when the model is later asked to classify new texts. But, the <code>gensim</code> implementation of FastText does not currently support this mode, as <code>gensim</code>'s goal has been to supply unsupervised rather than supervised algorithms.</p>

<p>You could approximate what that FastText mode does by averaging together those word-vectors:</p>

<pre><code>import numpy as np
meanvec_obama = np.array(wordvecs_obama).mean(axis=0)
</code></pre>

<p>Depending on your ultimate purposes, something like that might still be useful. (But, that average wouldn't be as useful for classification as if the word-vectors had originally ben trained for that goal, with known labels, in that FastText mode.)</p>
",5,1,2151,2019-07-17 15:39:01,https://stackoverflow.com/questions/57079642/after-training-word-embedding-with-gensims-fasttexts-wrapper-how-to-embed-new
How to get document vectors for a given topic in gensim,"<p>I have about 9000 documents and I am using Gensim's <code>doc2vec</code> to embed my documents. My code is as follows:</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple

dataset = json.load(open(input_file))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')

for description in dataset:
    tags = [description[0]]
    words = description[1]
    docs.append(analyzedDocument(words, tags))

model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4, epochs = 20)
</code></pre>

<p>I would like to get all the documents related to topic ""deep learning"". i.e. the documents that mainly have content related to deep learning. Is it possible to do this in doc2vec model in gensim?</p>

<p>I am happy to provide more details if needed.</p>
","python, gensim, word2vec, doc2vec","<p>If there was a document in your training set that was a great example of ""deep learning"" – say, <code>docs[17]</code> – then after successful training you could ask for documents similar to that example document, and that could be roughly what you'd need. For example:</p>

<pre><code>sims = model.docvecs.most_similar(docs[17].tags[0])
</code></pre>

<p>You'd then have in <code>sims</code> a ranked, scored list of the 10 most-similar documents to the <code>tag</code> for the target document. </p>
",1,0,607,2019-07-20 13:17:11,https://stackoverflow.com/questions/57125117/how-to-get-document-vectors-for-a-given-topic-in-gensim
Training word2vec model streaming data from file and tokenize to sentence,"<p>I need to process a large number of <code>txt</code> files for building a <code>word2vec</code> model.
Now, my txt-files are a bit messy and I need to remove all ´<code>\n</code>´ newlines, read all sentences from my loaded string (txt-file) and then tokenize each sentence for using the word2vec model.</p>

<p>The thing is: I cant read the files line-by-line, cause some sentences do not end after one line. Therefore, I use ´<code>nltk.tokenizer.tokenize()</code>´, which splits the file into sentences.</p>

<blockquote>
  <p>I cant figure out, how to convert a list of strings into a list of list, where each sub-list contains the sentences, while passing it thourgh a generator.</p>
</blockquote>

<p>Or do I actually need to save each sentences into a new file (one sentence per line) to pass it through a generator?</p>

<p>Well, my code looks like this:
´<code>tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')</code></p>

<pre><code># initialize tokenizer for processing sentences

class Raw_Sentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for file in file_loads: ## Note: file_loads includes directory name of files (e.g. 'C:/Users/text-file1.txt')
            with open(file,'r', encoding='utf-8') as t:     
               # print(tokenizer.tokenize(t.read().replace('\n', ' ')))           
                storage = tokenizer.tokenize(t.read().replace('\n', ' '))
# I tried to temporary store the list of sentences to a list for an iteration
                for sentence in storage:
                    print(nltk.word_tokenize(sentence))
                    yield nltk.word_tokenize(sentence)´
</code></pre>

<p>So the goal is: 
load file 1: ´<code>'some messy text here. And another sentence'</code>´
 tokenize into sentences ´<code>['some messy text here','And another sentence']</code>´
and then split each sentence into words ´<code>[['some','messy','text','here'],['And','another','sentence']]</code>´</p>

<p>load file 2: <code>'some other messy text. sentence1. sentence2.'</code>
etc.</p>

<p>and input sentences into word2vec model:
´<code>sentences = Raw_Sentences(directory)</code>´</p>

<p>´<code>model = gensim.models.Word2Vec(sentences)</code>´</p>
","python, streaming, nltk, gensim, word2vec","<p>Well... after writing it all down and reconsideration... I assume I solved my own question. <strong>Please correct me if Iam wrong:</strong></p>

<p>To iterate over every sentence created by the nltk punkt sentence tokenizer, one has to pass it directly to the for loop:</p>

<pre><code>def __iter__(self):
    for file in file_loads:
       with open(file,'r') as t:
           for sentence in tokenizer.tokenize(t.read().replace('\n',' ')):
                yield nltk.word_tokenize(sentence) 
</code></pre>

<p>as always, theres also the alternative to  <code>yield gensim.utils.simple_preprocess(sentence, deacc= True)</code> </p>

<p>Feeding that into <code>sentence = Raw_Sentences(directory)</code>  builds a proper working Word2Vec <code>gensim.models.Word2Vec(sentences)</code></p>
",0,0,1398,2019-07-20 14:44:18,https://stackoverflow.com/questions/57125757/training-word2vec-model-streaming-data-from-file-and-tokenize-to-sentence
How to get the nearest documents for a word in gensim in python,"<p>I am using the doc2vec model as follows to construct my document vectors.</p>

<pre><code>from gensim.models import doc2vec
from collections import namedtuple

dataset = json.load(open(input_file))

docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')

for description in dataset:
    tags = [description[0]]
    words = description[1]
    docs.append(analyzedDocument(words, tags))

model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4, epochs = 20)
</code></pre>

<p>I have seen that <strong>gensim doc2vec also includes word vectors</strong>. Suppose I have a word vector created for the word <code>deep learning</code>. My question is; is it possible to get the <code>documents</code> nearest to <code>deep learning</code> word vector in gensim in python?</p>

<p>I am happy to provide more details if needed.</p>
","python, gensim, word2vec, doc2vec","<p>Some <code>Doc2Vec</code> modes will co-train doc-vectors and word-vectors in the ""same space"". Then, if you have a word-vector for <code>'deep_learning'</code>, you can ask for documents near that vector, and the results may be useful for you. For example:</p>

<pre><code>similar_docs = d2v_model.docvecs.most_similar(
                   positive=[d2v_model.wv['deep_learning']]
               )
</code></pre>

<p>But:</p>

<ul>
<li><p>that's only going to be as good as your model learned <code>'deep_learning'</code> as a word to mean what you think of it as</p></li>
<li><p>a training set of known-good documents fitting the category <code>'deep_learning'</code> (and other categories) could be better - whether you hand-curate those, or try to bootstrap from other sources (like say the Wikipedia category '<a href=""https://en.wikipedia.org/wiki/Category:Deep_learning"" rel=""nofollow noreferrer"">Deep Learning</a>' or other curated/search-result sets that you trust).</p></li>
<li><p>reducing a category to a single summary point (one vector) may not be as good as having a variety of examples – many points - that all fit the category. (Relevant docs may not be a neat sphere around a summary point, but rather populate exotically-shaped regions of the doc-vector high-dimensional space.) If you have a lot of good examples of each category, you could train a classifier to then label, or rank-in-relation-to-trained-categories, any further uncategorized docs.</p></li>
</ul>
",2,1,424,2019-07-22 02:17:04,https://stackoverflow.com/questions/57138453/how-to-get-the-nearest-documents-for-a-word-in-gensim-in-python
AttributeError: module &#39;gensim.utils&#39; has no attribute &#39;smart_open&#39;,"<p>I am building the vocabulary table using Doc2vec, but there is an error ""AttributeError: module 'gensim.utils' has no attribute 'smart_open'"". How do I solve this?</p>

<p>This is for a notebook on Databricks platform, running in Python 3. In the past, I've tried on running the code on a local Jupyter Notebook but the same error occurred.</p>

<p>I've also searched <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a> but could not find anything related to smart_open.</p>

<pre class=""lang-py prettyprint-override""><code>model = Doc2Vec(window=5, min_count=1, size=50, sample=1e-5, negative=5, workers=1)

model.build_vocab(sentences.to_array())
</code></pre>

<p>I ran the above lines separately. The first line worked fine. The second says:
 AttributeError: module 'gensim.utils' has no attribute 'smart_open'</p>
","python, gensim, databricks, doc2vec","<p>I tried the first method given in the above answer, which unluckily doesn't apply to my case. And by looking into the file &quot;utils.py&quot;, I find that the statement about smart_open is &quot;from smart_open import open&quot;. Thus, I tried to use &quot;utils.open()&quot; instead, which exactly solves my problem ^-^</p>
<p>By the way, my gensim version is 3.8.3.</p>
",1,2,5874,2019-07-22 14:38:05,https://stackoverflow.com/questions/57148357/attributeerror-module-gensim-utils-has-no-attribute-smart-open
How to Implement a (statistical) Thematic Comparison of Texts via Text-Mining?,"<p>I try to compare texts in form of 'text-files' concerning their content.<br>
<strong>e.g.</strong>: I got 100 texts about animals and I want to analyze each text about what animals it discusses.<br>
I am looking for an analysis output like: <code>doc1: 60% cats, 10% rabbits, 10% dogs, 0% elephants, 20% else"", ""doc2: 0% cats, 10% rabbits, 40% dogs, ...</code></p>

<p>I have read a lot about Latent Dirichlet Allocation (and the word-probabilities for each topic) for Text Classification but a completely unsupervised approach seemed not to fit my set of documents.</p>

<p>Trying to implement the LDA-Stuff in Python I understood to prepare the data (tokenizing, lemmatizing/stemming) but I don't get the next steps. Do I have to generate training data for each topic (animal) and how could I implement this? </p>

<p>Also I've seen a tutorial manipulating the topics via the <code>eta-value</code> in <code>gensim</code> but I don't know how I could use this in my favor. </p>

<p>I am grateful for any advice that can lead me to the right direction. Thanks!</p>
","python, text-mining, gensim, text-classification, lda","<p>If you want to use LDA you're done with proprocessing (tokenizing, stemming/lemmatizing), the next step would be to create <code>gensim</code> dictionary and corpus. Assuming your set of documents is a list of lists like <code>[['my', 'first', 'doc'], ['the', 'second', 'doc']]</code> you could proceed like this: </p>

<pre><code>dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
</code></pre>

<p>Then create your model with appropriate number of topics (=animals):</p>

<pre><code>model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_animals)
</code></pre>

<p>You don't need to generate training data yourself at all. After a number of iterations, the LDA algorithm itself performs a quality check on a set of randomly chosen held out test documents which were not used for training. The corresponding measure is often called ""perplexity"" or ""log likelihood"" and will usually be displayed during iteration.</p>

<p>When your model is finally created you can have a look at the words in your topics:</p>

<pre><code>model.print_topics()
</code></pre>

<p>In many cases, you have a collection of documents and a rough idea of the number of topics contained. So the most relevant parameter to play around with is the topic number. 
Since you already know your topic number, you are left to tinker with other parameters. I could imagine that it's difficult to get topics that can be easily attributed to precisely one animal. Keep in mind though that every word appears in every topic, so even ""elephant"" is going to show up in the ""cat"" topic somewhere.</p>

<p>Things to try:</p>

<ul>
<li>Be more rigorous with your stemming/lemmatization to merge more tokens with the same meaning</li>
<li>Check out <code>filter_extremes</code> function of your dictionary to filter for very common or very rare tokens</li>
<li>Apply or expand your stopword filter to get rid of irrelevant terms</li>
<li>Play around with alpha (prevalence of topics per document) and eta (prevalence of tokens per topic) values</li>
</ul>
",0,0,277,2019-07-24 18:26:00,https://stackoverflow.com/questions/57189149/how-to-implement-a-statistical-thematic-comparison-of-texts-via-text-mining
How to turn a list of words into a list of vectors using a pre-trained word2vec model(Google)?,"<p>I am trying to learn word2vec.</p>

<p>I am using the code below to load the Google pre-trained word2vec model in Python 3. But I am unsure how to turn a list such as :[""I"", ""ate"", ""apple""] to a list of vectors (ie how to get vectors from this model?).</p>

<pre><code>import nltk
import gensim

# Load Google's pre-trained Word2Vec model.
model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>
","python-3.x, gensim, word2vec","<p>You get the vector via idiomatic Python keyed-index-access (brackets). For example:</p>

<pre><code>    wv_apple = model['apple']
</code></pre>

<p>You can create a new list based on some operation on every item of an existing list via an idiomatic Python 'list comprehension' (<code>[expression(x) for x in some_list]</code>), For example:</p>

<pre><code>    words = [""I"", ""ate"", ""apple""]
    vectors = [model[word] for word in words]
</code></pre>
",1,0,622,2019-07-28 20:25:02,https://stackoverflow.com/questions/57244699/how-to-turn-a-list-of-words-into-a-list-of-vectors-using-a-pre-trained-word2vec
How to combine 3D token embeddings into 2D vectors?,"<p>I have a set of strings that I am tokenizing. I am sending each string into the <code>word2vec</code> model in <code>gensim</code>. Say, if there are 100 tokens (e.g. 'I', 'ate', 'pizza', etc.), it is generating a 100 * 100 3D matrix (<code>list</code> of <code>list</code> in <code>python</code>). How is it possible to convert the generated 3D token embeddings in to a 2D vector?  </p>

<p>I am sending this 3D into a model in <code>Tensorflow</code> library. I am doing the following,</p>

<p><code>model.add(Embedding(max_features, 128, input_length=maxlen))</code></p>

<p>Here max_features is the size of the token vector i.e. 100 and input_length is also the same. </p>

<p>But I am not sure If this is getting the job done. Is it the right way to convert 3D token embeddings in to 2D vectors? Ideally, I want to covert the embeddings into 2D vectors before sending into the model.</p>
","python, tokenize, gensim, word2vec, word-embedding","<p>The final results of training aren't really ""3D"" in usual <code>Word2Vec</code>/<code>gensim</code> terminology. If you've used <code>Word2Vec</code> with its default <code>vector_size=100</code>, and you had 100 vocabulary words, then you'd have 100 vectors of 100-dimensions each. </p>

<p>(Note: you would never want to create such high-dimensional ""dense embedding"" vectors for such a tiny vocabulary. The essential benefits of such dense representations come from forcing a much-larger set of entities into many-fewer dimensions, so that they are ""compressed"" into subtle, continuous, meaningful relative positions against each other. Giving 100 words a full 100 continuous dimensions, before <code>Word2Vec</code> training, will leave the model prone to severe overfitting. It could in fact then trend towards a ""one-hot""-like encoding of each word, and become very good at the training task without really learning to pack related words near each other in a shared space – which is the usually-desired result of training. In my experience, for 100-dimension vectors, you probably want at least a 100^2 count of vocabulary words. If you really just care about 100 words, then you'd want to use much-smaller vectors – but also remember <code>Word2Vec</code> &amp; related techniques are really meant for ""large data"" problems, with many subtly-varied training examples, and just barely sometimes give meaningful results on toy-sized data.)</p>

<p>The 100 vectors of 100-dimensions each are internally stored inside the <code>Word2Vec</code> model (&amp; related components) as a raw <code>numpy</code> <code>ndarray</code>, which could be thought of as a ""2d array"" or ""2d matrix"". (It's not really a <code>list</code> of <code>list</code> unless you convert it to be that less-optimal form – though of course with Pythonic polymorphism you can generally pretend it was a <code>list</code> of <code>list</code>). If your <code>gensim</code> <code>Word2Vec</code> model is in <code>w2v_model</code>, then the raw <code>numpy</code> array of learned vectors is inside the <code>w2v_model.wv.vectors</code> property, though the interpretation of which row corresponds to which word-token depends on the <code>w2v_model.wv.vocab</code> dictionary entries.</p>

<p>As far as I can tell, the Tensorflow <code>Embedding</code> class is for training your own embeddings inside TF (though perhaps it can be initialized with vectors trained elsewhere). Its 1st initialization argument should the size-of-the-vocabulary (per your conjectured case 100), its second is the size-of-the-desired-embeddings (per your conjectured case, also 100 - but as noted above, this match of vocab-size and dense-embedding-size is inappropriate, and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"" rel=""nofollow noreferrer"">the example values in the TF docs of 1000 words and 64 dimensions</a> would be more appropriately balanced). </p>
",0,0,696,2019-07-30 03:56:34,https://stackoverflow.com/questions/57264086/how-to-combine-3d-token-embeddings-into-2d-vectors
gensim word2vec entry greater than 1,"<p>I'm new to NLP and gensim, currently trying to solve some NLP problems with gensim word2vec module. I my current understanding of word2vec, the result vectors/matrix should have all entries between -1 and 1. However, trying a simple one results into a vector which has entries greater than 1. I'm not sure which part is wrong, could anyone give some suggestions, please?</p>

<p>I've used gensim utils.simple_preprocess to generate a list of list of token. The list looks like: </p>

<pre><code>[['buffer', 'overflow', 'in', 'client', 'mysql', 'cc', 'in', 'oracle', 'mysql', 'and', 'mariadb', 'before', 'allows', 'remote', 'database', 'servers', 'to', 'cause', 'denial', 'of', 'service', 'crash', 'and', 'possibly', 'execute', 'arbitrary', 'code', 'via', 'long', 'server', 'version', 'string'], ['the', 'xslt', 'component', 'in', 'apache', 'camel', 'before', 'and', 'before', 'allows', 'remote', 'attackers', 'to', 'read', 'arbitrary', 'files', 'and', 'possibly', 'have', 'other', 'unspecified', 'impact', 'via', 'an', 'xml', 'document', 'containing', 'an', 'external', 'entity', 'declaration', 'in', 'conjunction', 'with', 'an', 'entity', 'reference', 'related', 'to', 'an', 'xml', 'external', 'entity', 'xxe', 'issue']]
</code></pre>

<p>I believe this is the correct input format for gensim word2vec.</p>

<pre><code>word2vec = models.word2vec.Word2Vec(sentences, size=50, window=5, min_count=1, workers=3, sg=1)
vector = word2vec['overflow']
print(vector)
</code></pre>

<p>I expect the output to be a vector containing probabilities (i.e., all between -1 and 1), but it actually turned out to be the following:</p>

<pre><code>[ 0.12800379 -0.7405527  -0.85575     0.25480416 -0.2535793   0.142656
 -0.6361196  -0.13117172  1.1251501   0.5350017   0.05962601 -0.58876884
  0.02858278  0.46106443 -0.22623934  1.6473309   0.5096218  -0.06609935
 -0.70007527  1.0663376  -0.5668168   0.96070313 -1.180383   -0.58649933
 -0.09380565 -0.22683378  0.71361005  0.01779896  0.19778453  0.74370056
 -0.62354785  0.11807996 -0.54997736  0.10106519  0.23364201 -0.11299669
 -0.28960565 -0.54400533  0.10737313  0.3354464  -0.5992898   0.57183135
 -0.67273194  0.6867607   0.2173506   0.15364875  0.7696457  -0.24330224
  0.46414775  0.98163396]
</code></pre>

<p>You can see there are <code>1.6473309</code> and <code>-1.180383</code> in the above vector.</p>
","python, nlp, gensim, word2vec","<p>It's <strong>not</strong> the case that individual word-vectors will have all their individual dimensions between <code>-1.0</code> and <code>1.0</code>. </p>

<p>Nor is it the case that the dimensions should be interpreted as ""probabilities"". </p>

<p>Rather, the word-vectors are learned such that the internal neural-network becomes as good as possible at predicting words from surrounding words. There's no constraint or normalization during that training forcing the individual dimensions into a restricted range, or making individual dimensions interpretable as nameable qualities. </p>

<p>It is sometimes the case that such vectors are converted, after training, into vectors of normalized unit-length, before comparison to each other. And further, when you request the cosine-similarity between two vectors, the result will always be in the range from <code>-1.0</code> to <code>1.0</code>. And, before doing the very-common <code>most_similar()</code> operation (or similar), the <code>Word2Vec</code> class with bulk-unit-normalize vectors &amp; cache the results internally. </p>

<p>But, directly asking for the raw word-vector, as per <code>model.wv['overflow']</code>, will return the raw vector with whatever original overall magnitude, and per-dimension values, as came from training. You can request the unit-normed vector instead with:</p>

<pre><code>model.wv.word_vec('overflow', use_norm=True)
</code></pre>

<p>(Separately be aware: testing <code>Word2Vec</code> on tiny toy-sized datasets will generally not get useful or realistic results: the algorithm really requires large, varied data to come up with balanced, useful word-vectors. For example, to train-up 50-dimensional vectors, I'd want at least 2,500 unique words in the vocabulary, with dozens of different uses of each word – so a corpus of many tens of thousands of words. And I might also use more than the default <code>epochs=5</code>, because that's still a very small corpus.)</p>
",0,0,970,2019-07-31 18:49:40,https://stackoverflow.com/questions/57297194/gensim-word2vec-entry-greater-than-1
How to combine vectors generated by PV-DM and PV-DBOW methods of doc2vec?,"<p>I have around 20k documents with 60 - 150 words. Out of these 20K documents, there are 400 documents for which the similar document are known. These 400 documents serve as my test data.</p>

<p>I am trying to find similar documents for these 400 datasets using gensim doc2vec. The paper ""Distributed Representations of Sentences and Documents"" says that ""The combination of PV-DM and PV-DBOW often work consistently better (7.42% in IMDB) and therefore recommended.""</p>

<p>So I would like to combine the vectors of these two methods and find cosine similarity with all the train documents and select the top 5 with the least cosine distance.</p>

<p>So what's the effective method to combine the vectors of these 2 methods: adding or averaging or any other method ???</p>

<p>After combining these 2 vectors I can normalise each vector and then find the cosine distance.</p>
","python, nlp, gensim, doc2vec, sentence-similarity","<p>The paper implies they've concatenated the vectors from the two methods. For example, given a 300d PV-DBOW vector, and a 300d PV-DM vector, you'd get a 600d vector for your text after concatenation. </p>

<p>However, note that their bottom-line results on IMDB have been hard for outsiders to reproduce. My test have only sometimes shown a small advantage for these concatenated vectors. (I especially wonder if 300d PV-DBOW + 300d PV-DM via separate-concatenated-models would be any better than just training a true 600d model of either, for the same amount of time, with fewer steps/complications.)</p>

<p>You can view my demonstration of repeating some of the experiments of the original 'Paragraph Vector' paper in one of the the example notebooks included with <code>gensim</code> in its <code>docs/notebooks</code> directory:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb</a></p>

<p>It includes, among other things, a few steps and helpful methods for treating pairs of models as a concatenated whole. </p>
",3,3,818,2019-08-06 10:05:48,https://stackoverflow.com/questions/57373626/how-to-combine-vectors-generated-by-pv-dm-and-pv-dbow-methods-of-doc2vec
How to use Sklearn linear regression with doc2vec input,"<p>I have 250k text documents (tweets and newspaper articles) represented as vectors obtained with a doc2vec model. Now, I want to use a regressor (multiple linear regression) to predict continuous value outputs - in my case the UK Consumer Confidence Index. 
My code runs, since forever. What am I doing wrong?</p>

<p>I imported my data from Excel and splitted it into x_train and x_dev. The data are composed of preprocessed text and CCI continuous values. </p>

<pre><code># Import doc2vec model
dbow = Doc2Vec.load('dbow_extended.d2v')
dmm = Doc2Vec.load('dmm_extended.d2v')
concat = ConcatenatedDoc2Vec([dbow, dmm]) # model uses vector_size 400

def get_vectors(model, input_docs):
    vectors = [model.infer_vector(doc.words) for doc in input_docs]
    return vectors

# Prepare X_train and y_train
train_text = x_train[""preprocessed_text""].tolist()
train_tagged = [TaggedDocument(words=str(_d).split(), tags=[str(i)]) for i, _d in list(enumerate(train_text))]
X_train = get_vectors(concat, train_tagged)
y_train=x_train['CCI_UK']

# Fit regressor 
from sklearn import linear_model
reg = linear_model.LinearRegression()
reg.fit(X_train, y_train)

# Predict and evaluate
prediction=reg.predict(X_dev)
print(classification_report(y_true=y_dev,y_pred=prediction),'\n')
</code></pre>

<p>Since the fitting never completed, I wonder whether I am using a wrong input. However, no error message is shown and the code simply runs forever. What am I doing wrong?</p>

<p>Thank you so much for your help!!</p>
","scikit-learn, linear-regression, gensim, doc2vec","<p>The variable X_train is a list or a list of lists (since the function get_vectors() return a list) whereas the input to sklearn's Linear Regression should be a 2-D array.</p>

<p>Try converting X_train to an array using this :</p>

<pre><code>X_train = np.array(X_train)
</code></pre>

<p>This should help !</p>
",1,2,540,2019-08-07 09:59:35,https://stackoverflow.com/questions/57391750/how-to-use-sklearn-linear-regression-with-doc2vec-input
Gensim built-in model.load function and Python Pickle.load file,"<p>I was trying to use Gensim to import GoogelNews-pretrained model on some English words (sampled 15 ones here only stored in a txt file with each per line, and there are no more context as corpus). Then I could use ""model.most_similar()"" to get their similar words/phrases for them. But actually the file loaded from Python-Pickle method couldn't be used for gensim-built-in <code>model.load()</code> and <code>model.most_similar()</code> function directly. </p>

<p>how should I do to cluster the 15 English words (and more in the future), since I couldn't train and save and load a model  from the beginning?</p>

<pre><code>import gensim
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

GOOGLE_WORD2VEC_MODEL = '../GoogleNews-vectors-negative300.bin'

GOOGLE_ENGLISH_WORD_PATH = '../testwords.txt'

GOOGLE_WORD_FEATURE = '../word.google.vector'

model = gensim.models.KeyedVectors.load_word2vec_format(GOOGLE_WORD2VEC_MODEL, binary=True) 

word_vectors = {}

#load 15 words as a test to word_vectors

with open(GOOGLE_ENGLISH_WORD_PATH) as f:
    lines = f.readlines()
    for line in lines:
        line = line.strip('\n')
        if line:                
            word = line
            print(line)
            word_vectors[word]=None
try:
    import cPickle
except :
    import _pickle as cPickle

def save_model(clf,modelpath): 
    with open(modelpath, 'wb') as f: 
        cPickle.dump(clf, f) 

def load_model(modelpath): 
    try: 
        with open(modelpath, 'rb') as f: 
            rf = cPickle.load(f) 
            return rf 
    except Exception as e:        
        return None 

for word in word_vectors:
    try:
        v= model[word]
        word_vectors[word] = v
    except:
        pass

save_model(word_vectors,GOOGLE_WORD_FEATURE)

words_set = load_model(GOOGLE_WORD_FEATURE)

words_set.most_similar(""knit"", topn=3)
</code></pre>

<blockquote>
<pre><code>---------------error message--------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-8-86c15e366696&gt; in &lt;module&gt;
----&gt; 1 words_set.most_similar(""knit"", topn=3)

AttributeError: 'dict' object has no attribute 'most_similar'
---------------error message--------
</code></pre>
</blockquote>
","gensim, word2vec","<p>You've defined <code>word_vectors</code> as a Python <code>dict</code>:</p>

<pre><code>word_vectors = {}
</code></pre>

<p>Then your <code>save_model()</code> function just saves that raw <code>dict</code>, and your <code>load_model()</code> loads that same raw <code>dict</code>. </p>

<p>Such dictionary objects <strong>don't</strong> implement the <code>most_similar()</code> method, which is specific to the <code>KeyedVectors</code> interface (&amp; related classes) of <code>gensim</code>. </p>

<p>So, you'll have to leave the data inside a <code>KeyedVectors</code>-like object to be able to use <code>most_similar()</code>. </p>

<p>Fortunately, you have a few options.</p>

<p>If you happened to need the just the <strong>first</strong> 15 words from inside the <code>GoogleNews</code> file (or first 15,000, etc), you could use the optional <code>limit</code> parameter to only read that many vectors:</p>

<pre><code>from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format(GOOGLE_WORD2VEC_MODEL, limit=15, binary=True)
</code></pre>

<p>Alternatively, if you really need to select an arbitrary subset of the words, and assemble them into a new <code>KeyedVectors</code> instance, you could re-use one of the classes inside <code>gensim</code> instead of a plain <code>dict</code>, then add your vectors in a slightly different way:</p>

<pre><code># instead of a {} dict
word_vectors = KeyedVectors(model.vector_size)  # re-use size from loaded model
</code></pre>

<p>...then later inside your loop of each <code>word</code> you want to add...</p>

<pre><code># instead of `word_vectors[word] = _SOMETHING_`
word_vectors.add(word, model[word])
</code></pre>

<p>Then you'll have a <code>word_vectors</code> that is an actual <code>KeyedVectors</code> object. While you <em>could</em> save that via plain Python-pickle, at that point you might as well use the <code>KeyedVectors</code> built-in <code>save()</code> and <code>load()</code> - they may be more efficient on large vector sets (by saving large sets of raw vectors as a separate file which should be kept alongside the main file). For example:</p>

<pre><code>word_vectors.save(GOOGLE_WORD_FEATURE)
</code></pre>

<p>...</p>

<pre><code>words_set = KeyedVectors.load(GOOGLE_WORD_FEATURE)

words_set.most_similar(""knit"", topn=3)  # should work
</code></pre>
",0,0,1846,2019-08-08 12:23:40,https://stackoverflow.com/questions/57412511/gensim-built-in-model-load-function-and-python-pickle-load-file
How to Cluster words and phrases with pre-trained model on Gensim,"<p>What I want exactly is to cluster words and phrases, e.g.
knitting/knit loom/loom knitting/weaving loom/rainbow loom/home decoration accessories/loom knit/knitting loom/...And I don'd have corpus while I have only the words/phrases. Could I use a pre-trained model like the one from GoogleNews/Wikipedia/... to realise it?</p>

<p>I am trying now to use Gensim to load GoogleNews pre-trained model to get phrases similarity. I've been told that The GoogleNews model includes vectors of phrases and words. But I find that I could only get word-similarity while phrase-similarity fails with an error message that the phrase is not in the vocabulary. Please advise me. Thank you.</p>

<pre><code>import gensim
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

GOOGLE_MODEL = '../GoogleNews-vectors-negative300.bin'

model = gensim.models.KeyedVectors.load_word2vec_format(GOOGLE_MODEL, binary=True) 


# done well
model.most_similar(""computer"", topn=3) 

# done with error message ""computer_software"" is not in the vocabulory.
model.most_similar(""computer_software"", topn=3) 
</code></pre>
","gensim, word2vec","<p>The <code>GoogleNews</code> set does include many multi-word phrases, as created via some statistical analysis, but might not include something specific you're hoping it does, like <code>'computer_software'</code>. </p>

<p>On the other hand, I see an online word-list suggesting that a phrase like <code>'composite_fillings'</code> <strong>is</strong> in the <code>GoogleNews</code> vocabulary, so this will likely work for you:</p>

<pre><code>model.most_similar(""composite_fillings"", topn=3) 
</code></pre>

<p>With that vector-set, you're limited to what they chose to model as phrases. If you need similarly-strong vectors for other phrases, you'd likely need to train your own model, on a corpus where the phrases important to you have been combined into single tokens. (If you just need something-better-than-nothing, averaging together the constituent words' word-vectors would give you something to work with... but that's a pretty-crude stand-in for truly modeling the bigram/multigram against its unique contexts.)</p>
",0,1,727,2019-08-09 09:01:22,https://stackoverflow.com/questions/57426745/how-to-cluster-words-and-phrases-with-pre-trained-model-on-gensim
New sentence from doc2vec model trained with wikicorpus,"<p>I'm training a Doc2Vec model from the french wikipedia.</p>

<p>My code is based on this notebook :
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>

<p>It's actually in the training phase, but, I don't know how to vectorize new sentences after that.</p>

<p>Should I just use : model.infer_vector[""Example sentence here""] ?
But in this case, how to make the same processing than the Wikicorpus method does ? (This is not explained here : <a href=""https://radimrehurek.com/gensim/corpora/wikicorpus.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/corpora/wikicorpus.html</a>)</p>

<p>Thanks!</p>
","python, gensim, doc2vec","<p>You're on the right track, but <code>infer_vector()</code> is a method to be called with arguments, rather than an object offering <code>[]</code>-indexing. And, it requires a list-of-word-tokens, not a raw string. So with your tiny example sentence, a better call would be:</p>

<pre><code>model.infer_vector(['Example', 'sentence', 'here'])
</code></pre>

<p>However, you do want to be sure to preprocess &amp; tokenize your later sentences the same way as was done with the training data - so that capitalization, punctuation, etc is treated the same way. (Otherwise, it's more likely many of the tokens you try to infer-from won't have exact equivalents in the model, and thus be ignored.)</p>

<p>You can review the source for what gensim's <code>WikiCorpus</code> does at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/f97d0e793faa57877a2bbedc15c287835463eaa9/gensim/corpora/wikicorpus.py#L340"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/f97d0e793faa57877a2bbedc15c287835463eaa9/gensim/corpora/wikicorpus.py#L340</a></p>

<p>Specifically, you could reuse the <code>gensim.utils.tokenize()</code> function on plain-text to match its tokenization. (If you needed to do the full ""wiki-text"" preprocessing, you'd want to match or re-use the other methods in that file). </p>
",1,0,131,2019-08-10 16:33:19,https://stackoverflow.com/questions/57443879/new-sentence-from-doc2vec-model-trained-with-wikicorpus
How can I print document wise topics in Gensim?,"<p>I'm using LDA with gensim for topic modeling. My data has 23 documents and I want separate topics/words for each document but gensim is giving topics for entire set of documents together. How to get it for  individual docs?</p>

<pre><code>dictionary = corpora.Dictionary(doc_clean)

# Converting list of documents (corpus) into Document Term Matrix using 
#dictionary prepared above.

corpus = [dictionary.doc2bow(doc) for doc in doc_clean]


# Creating the object for LDA model using gensim library
Lda = gensim.models.ldamodel.LdaModel

# Running and Trainign LDA model on the document term matrix.
ldamodel = Lda(corpus, num_topics=3, id2word = dictionary, passes=50)

result=ldamodel.print_topics(num_topics=3, num_words=3)
</code></pre>

<p>This is the output I'm getting:</p>

<pre><code>[(0, '0.011*""plex"" + 0.010*""game"" + 0.009*""racing""'),
(1, '0.008*""app"" + 0.008*""live"" + 0.007*""share""'),
(2, '0.015*""device"" + 0.009*""file"" + 0.008*""movie""')]
</code></pre>
","python, nltk, gensim, lda, topic-modeling","<p><code>print_topics()</code> returns a list of topics, the words loading onto that topic and those words.</p>

<p>If you want the topic loadings per document, instead, you need to use <code>get_document_topics()</code>.  </p>

<p>From the <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">gensim documentation</a>: </p>

<hr>

<p><code>get_document_topics(bow, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)</code></p>

<p>Get the topic distribution for the given document.</p>

<p><strong>Parameters:</strong> 
<code>bow (corpus : list of (int, float))</code> – The document in BOW format.
<code>minimum_probability (float)</code> – Topics with an assigned probability lower than this threshold will be discarded.
<code>minimum_phi_value (float)</code> – If <code>per_word_topics</code> is <code>True</code>, this represents a lower bound on the term probabilities that are included.
If set to <code>None</code>, a value of <code>1e-8</code> is used to prevent 0s.
<code>per_word_topics (bool)</code> – If <code>True</code>, this function will also return two extra lists as explained in the “Returns” section.</p>

<p><strong>Returns:</strong><br>
<code>list of (int, float)</code> – Topic distribution for the whole document. Each element in the list is a pair of a topic’s id, and the probability that was assigned to it.</p>

<p><code>list of (int, list of (int, float)</code>, optional – Most probable topics per word. Each element in the list is a pair of a word’s id, and a list of topics sorted by their relevance to this word. Only returned if <code>per_word_topics</code> was set to <code>True</code>.</p>

<p><code>list of (int, list of float)</code>, optional – <code>Phi</code> relevance values, multiplied by the feature length, for each word-topic combination. Each element in the list is a pair of a word’s id and a list of the phi values between this word and each topic. Only returned if <code>per_word_topics</code> was set to <code>True</code>.</p>

<hr>

<p><code>get_term_topics()</code> and <code>get_topic_terms()</code> may also be potentially interesting for you.</p>
",3,1,2807,2019-08-12 07:35:16,https://stackoverflow.com/questions/57457214/how-can-i-print-document-wise-topics-in-gensim
Could I use BERT to Cluster phrases with pre-trained model,"<p>I found it was a failure that I had used Gensim with GoogleNews pre-trained model to cluster phrases like:</p>

<ul>
<li>knitting</li>
<li>knit loom</li>
<li>loom knitting</li>
<li>weaving loom</li>
<li>rainbow loom</li>
<li>home decoration accessories</li>
<li>loom knit/knitting loom</li>
<li>...</li>
</ul>

<p>I am advised that <a href=""https://stackoverflow.com/questions/57426745/how-to-cluster-words-and-phrases-with-pre-trained-model-on-gensim"">GoogleNews model does't have the phrases in it</a>. The phrases I have are a little specific to GoogleNews model while I don't have corpus to train a new model. I have only the phrases. And now I am considering to turn to BERT. But could BERT do that as I expected as above? Thank you.</p>
","tensorflow, nlp, pytorch, gensim, word2vec","<p>You can feed a phrase into the pretrained BERT model and get an embedding, i.e. a fixed-dimension vector. So BERT can embed your phrases in a space. Then you can use a clustering algorithm (such as k-means) to cluster the phrases. The phrases do not need to occur in the training corpus of BERT, as long as the words they consist of are in the vocabulary. You will have to try to see if the embeddings give you relevant results.</p>
",1,1,2134,2019-08-13 10:30:07,https://stackoverflow.com/questions/57475889/could-i-use-bert-to-cluster-phrases-with-pre-trained-model
Not efficiently to use multi-Core CPU for training Doc2vec with gensim,"<p>I am using 24 cores virtual CPU and 100G memory to training Doc2Vec with Gensim, but the usage of CPU always is around 200% whatever to modify the number of cores.</p>
<pre><code>top
</code></pre>
<p><a href=""https://i.sstatic.net/1FgE9.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/1FgE9.png"" alt=""enter image description here"" /></a></p>
<pre><code>htop
</code></pre>
<p><a href=""https://i.sstatic.net/6jgCK.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/6jgCK.png"" alt=""enter image description here"" /></a></p>
<p>The above two pictures showed the percentage of cpu usage, this pointed out that cpu wasn't used efficiently.</p>
<pre><code>cores = multiprocessing.cpu_count()
assert gensim.models.doc2vec.FAST_VERSION &gt; -1, &quot;This will be painfully slow otherwise&quot;

simple_models = [
    # PV-DBOW plain
    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, 
            epochs=20, workers=cores),
    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes
    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, 
            epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),
    # PV-DM w/ concatenation - big, slow, experimental mode
    # window=5 (both sides) approximates paper's apparent 10-word total window size
    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, 
            epochs=20, workers=cores),
]

for model in simple_models:
    model.build_vocab(all_x_w2v)
    print(&quot;%s vocabulary scanned &amp; state initialized&quot; % model)

models_by_name = OrderedDict((str(model), model) for model in simple_models)
</code></pre>
<p>Edit:</p>
<p>I tried to use parameter corpus_file instead of documents, and resolved above problem. but, I need to adjust the code and convert all_x_w2v to file, and all_x_w2v didn't directly do this.</p>
",gensim,"<p>The Python Global Interpreter Lock (""GIL"") and other interthread-bottlenecks prevent its code from saturating all CPU cores with the classic gensim <code>Word2Vec</code>/<code>Doc2Vec</code>/etc flexible corpus-iterators – where you can supply any re-iterable sequence of the texts. </p>

<p>You can improve the throughput a bit with steps like:</p>

<ul>
<li><p>larger values of <code>negative</code>, <code>size</code>, &amp; <code>window</code></p></li>
<li><p>avoiding any complicated steps (like tokenization) in your iterator – ideally it will just be streaming from a simple on-disk format</p></li>
<li><p>experimenting with different <code>worker</code> counts – the optimal count will vary based on your other parameters &amp; system details, but is often in the 3-12 range (no matter how many more cores you have)</p></li>
</ul>

<p>Additionally, recent versions of <code>gensim</code> offer an alternative corpus-specification method: a <code>corpus_file</code> pointer to an already space-delimited, text-per-line file. If you supply your texts this way, multiple threads will each read the raw file in optimized code – and it's possible to achieve much higher CPU utilization. However, in this mode you lose the ability to specify your own document <code>tags</code>, or more than one <code>tag</code> per document. (The documents will just be given unique IDs based on their line-number in the file.)</p>

<p>See the docs for <code>Doc2Vec</code>, and its parameter <code>corpus_file</code>:</p>

<p><a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec</a></p>
",5,8,2081,2019-08-16 23:07:02,https://stackoverflow.com/questions/57532018/not-efficiently-to-use-multi-core-cpu-for-training-doc2vec-with-gensim
How to load pre-trained LDA model to Jupiter Notebook?,"<p>I trained the LDA model on my PC and saved it locally by using model.save() command. I can load this model and output topics in PyCharm, but when I try to load the same model in Jupiter Notebook I get an error.</p>

<p>Did anyone encounter the same problem and fix it?
Below is the full error output:</p>

<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-14-043e6d6083e2&gt; in &lt;module&gt;
      1 # Loading saved model
----&gt; 2 model = models.LdaModel.load('information_extraction/optimal_LDA3.model')
      3 # model_topics = model.show_topics(formatted=True)
      4 # pprint.pprint(model.print_topics(num_words=15))

~/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py in load(cls, fname, *args, **kwargs)
   1636         """"""
   1637         kwargs['mmap'] = kwargs.get('mmap', None)
-&gt; 1638         result = super(LdaModel, cls).load(fname, *args, **kwargs)
   1639 
   1640         # check if `random_state` attribute has been set after main pickle load

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in load(cls, fname, mmap)
    424         compress, subname = SaveLoad._adapt_by_suffix(fname)
    425 
--&gt; 426         obj = unpickle(fname)
    427         obj._load_specials(fname, mmap, compress, subname)
    428         logger.info(""loaded %s"", fname)

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in unpickle(fname)
   1382         # Because of loading from S3 load can't be used (missing readline in smart_open)
   1383         if sys.version_info &gt; (3, 0):
-&gt; 1384             return _pickle.load(f, encoding='latin1')
   1385         else:
   1386             return _pickle.loads(f.read())

ModuleNotFoundError: No module named 'numpy.random._pickle'
</code></pre>
","pycharm, jupyter-notebook, gensim, lda","<p>SOLVED:
The issue was a difference in numpy versions. I trained the LDA model on numpy==1.17.0, then I installed Anaconda and ran Jupiter with numpy==1.16.4. </p>
",0,0,615,2019-08-23 16:57:30,https://stackoverflow.com/questions/57630389/how-to-load-pre-trained-lda-model-to-jupiter-notebook
evaluating word2vec model using SimLex-999,"<p>i have trained my model with Gensim.now i wanna evaluate my model with simlexx-999 but it gives me error.
my code.</p>

<pre><code>model.wv.evaluate_word_analogies('SimLex-999.txt')
2019-08-25 13:43:22,766 : INFO : Evaluating word analogies for top 300000 words in the model on SimLex-999.txt
</code></pre>

<p>error</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-12-60cb96c45579&gt; in &lt;module&gt;()
----&gt; 1 model.wv.evaluate_word_analogies('SimLex-999.txt')

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in evaluate_word_analogies(self, analogies, restrict_vocab, case_insensitive, dummy4unknown)
   1088             else:
   1089                 if not section:
-&gt; 1090                     raise ValueError(""Missing section header before line #%i in %s"" % (line_no, analogies))
   1091                 try:
   1092                     if case_insensitive:

ValueError: Missing section header before line #0 in SimLex-999.txt
</code></pre>

<p>i have tried</p>

<pre><code>from gensim.test.utils import datapath

similarities = model.evaluate_word_pairs(datapath('SimLex-999.txt'))

print(similarities)
</code></pre>

<p>but it gives me keyError.Please help me to solve the problem.</p>

<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-29-caeb682cb7ff&gt; in &lt;module&gt;()
      1 from gensim.test.utils import datapath
      2 
----&gt; 3 similarities = model.wv.evaluate_word_pairs(datapath('SimLex-999.txt'),dummy4unknown=True)
      4 
      5 print(similarities)

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in evaluate_word_pairs(self, pairs, delimiter, restrict_vocab, case_insensitive, dummy4unknown)
   1287 
   1288         """"""
-&gt; 1289         ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
   1290         ok_vocab = {w.upper(): v for w, v in reversed(ok_vocab)} if case_insensitive else dict(ok_vocab)
   1291 

C:\ProgramData\Anaconda3\lib\site-packages\gensim\models\keyedvectors.py in &lt;listcomp&gt;(.0)
   1287 
   1288         """"""
-&gt; 1289         ok_vocab = [(w, self.vocab[w]) for w in self.index2word[:restrict_vocab]]
   1290         ok_vocab = {w.upper(): v for w, v in reversed(ok_vocab)} if case_insensitive else dict(ok_vocab)
   1291 

KeyError: 'movie'
</code></pre>
","python-3.x, gensim, word2vec","<p><code>SimLex-999.txt</code> does not appear to be a list of word analogies appropriate as an argument for the <code>evaluate_word_analogies()</code> function.</p>

<p>Have you tried the <code>evaluate_word_pairs()</code> function? Its description is at:</p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.evaluate_word_pairs"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.evaluate_word_pairs</a></p>
",0,1,1748,2019-08-26 06:31:05,https://stackoverflow.com/questions/57652804/evaluating-word2vec-model-using-simlex-999
How can I use a pretrained embedding to gensim skipgram model?,"<p>I want to train part of the corpus first and then based on the embeddings train on the whole corpus. Can I achieve this with gensim skipgram?</p>

<p>I haven't found an API that can pass initial embeddings.</p>

<p>what I want is some thing like</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec
sentences = [[""cat"", ""say"", ""meow""], [""dog"", ""say"", ""woof""],
             [""cat2"", ""say2"", ""meow""], [""dog2"", ""say"", ""woof""]]
model = Word2Vec(sentences[:2], min_count=1)
X = #construct a new one
model = Word2Vec(sentences, min_count=1, initial_embedding=X)
</code></pre>
","python, machine-learning, gensim, word2vec","<p>I'm not sure why you'd want to do this: if you have the whole corpus, and can train on the whole corpus, you're likely to get the best results from whole-corpus training. </p>

<p>And, to the extent there's anything missing from the 2nd-corpus, the 2nd-corpus training will tend to pull vectors for words still training away from words that are no longer in the corpus – causing comparability of vectors within the corpus to decay. (It's only the interleaved tug-of-war between examples including all words that nudges them into positions that are meaningfully related to each other.)</p>

<p>But, keeping that caveat in mind: you can continue to <code>train()</code> a model with new data. That is:</p>

<pre><code># initialize &amp; do all default training
model = Word2Vec(sentences[:2], min_count=1)
# now train again even more with a slightly different mix
model.train(sentences, total_examples = len(sentences), epochs=model.epochs)
</code></pre>

<p>Note in such a case the model's discovered vocabulary is only based on the original initialization. If there are words only in <code>sentences[0]</code>, when those sentences are presented to the model that didn't see those words during its initialization, they will be ignored – and never get vectors. (If using your tiny example corpus in this way, the word 'cat' won't get a vector. Again, you really want to train on the largest corpus – or at least use the largest corpus, with a superset of words, 1st.)</p>

<p>Also, a warning will be logged, because the 2nd training will again start the internal <code>alpha</code> learning-rate at its larger starting value, then gradually decrease it to the final <code>min_alpha</code> value. To be yo-yo'ing the value like this isn't standard SGD, and usually indicates a user error. But, it might be tolerable depending on your goals – you just need to be aware when you're doing unusual training sequences like this, you're off in experimental/advanced land and have to deal with possible side-effects via your own understanding.</p>
",0,1,252,2019-08-28 14:53:10,https://stackoverflow.com/questions/57695150/how-can-i-use-a-pretrained-embedding-to-gensim-skipgram-model
I get &#39;single&#39; characters as learned vocabulary on word2vec genism as an output,"<p>I am new for word2vec and I have trained a text file via word2vec for feature extraction than when I look at the words that are trained I found that it is single characters instead of words, what did I miss here? anyone help</p>

<p>I try to feed tokens instead of the raw text into the models</p>

<pre><code>import nltk

from pathlib import Path
data_folder = Path("""")
file_to_open = data_folder / ""test.txt""
#read the file
file = open(file_to_open , ""rt"")
raw_text = file.read()
file.close()

#tokenization
token_list = nltk.word_tokenize(raw_text)

#Remove Punctuation
from nltk.tokenize import punkt
token_list2 = list(filter(lambda token : punkt.PunktToken(token).is_non_punct,token_list))
#upper to lower case
token_list3 = [word.lower() for word in token_list2]
#remove stopwords
from nltk.corpus import stopwords
token_list4 = list(filter(lambda token: token not in stopwords.words(""english""),token_list3))

#lemmatization
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
token_list5 = [lemmatizer.lemmatize(word) for word in token_list4]
print(""Final Tokens are :"")
print(token_list5,""\n"")
print(""Total tokens : "", len(token_list5))

#word Embedding
from gensim.models import Word2Vec
# train model
model = Word2Vec(token_list5, min_count=2)
# summarize the loaded model

    print(""The model is :"")
    print(model,""\n"")`enter code here`

# summarize vocabulary

    words = list(model.wv`enter code here`.vocab)
    print(""The learned vocabulary words are : \n"",words)

Output- ['p', 'o', 't', 'e', 'n', 'i', 'a', 'l', 'r', 'b', 'u', 'm', 'h', 'd', 'c', 's', 'g', 'q', 'f', 'w', '-']
Expected -[ 'potenial', 'xyz','etc']
</code></pre>
","nlp, gensim, word2vec, feature-extraction, text-classification","<p><code>Word2Vec</code> needs its training corpus to be a sequence where each item (text/sentence) is a <strong>list-of-string-tokens</strong>. </p>

<p>If you instead pass texts that are raw strings, each will appear as a <strong>list-of-one-character-tokens</strong>, and that will result in the final vocabulary you're seeing, where all learned 'words' are just single-characters. </p>

<p>So, take a closer look at your <code>token_list5</code> variable. As it is a list, what is <code>token_list5[0]</code>? (Is it a list-of-strings?) What is <code>token_list5[0][0]</code>? (Is it a full word?)</p>
",1,0,348,2019-08-30 13:59:02,https://stackoverflow.com/questions/57728181/i-get-single-characters-as-learned-vocabulary-on-word2vec-genism-as-an-output
Can gensim Doc2Vec be used to compare a novel document to a trained model?,"<p>I have a set of documents that all fit a pre-defined category and have successfully trained a model off of those documents.</p>

<p>The question is, if I have a novel document, how can I calculate how closely this new document lines up with my trained model?</p>

<p>My current solution:</p>

<pre><code>novel_vector = model.infer_vector(novel_doc_words, steps = 20)
similarity_scores = model.docvecs.most_similar([novel_vector])
average = 0
for score in similarity_scores:
  average += score[1]
overall_similarity = average/len(similarity_scores)
</code></pre>

<p>I was unable to find any convenience methods in the documentation</p>
","python, python-3.x, nlp, gensim, doc2vec","<p>There's no built-in method to check this sort of ""lines up with"" value, with respect to the whole model. </p>

<p>A more typical approach, matching existing capabilities, would be to train a model on a diversity of documents – not just those in a specific category. Then, after inferring a new document's vector, calculate its average distance to documents of just the category of interest. </p>

<p>If you instead train a model on only documents of a certain self-similar category, the learned coordinate-space won't as well reflect the full range of possible documents <strong>outside</strong> that category.</p>

<p>That said, if your current code – which checks how similar a new document is to the top-N nearest neighbors - seems to give good results for your purposes, maybe it's acceptable. I'd just expect better results from a model that had trained on a wider variety of documents.</p>
",0,0,202,2019-08-30 15:59:38,https://stackoverflow.com/questions/57729961/can-gensim-doc2vec-be-used-to-compare-a-novel-document-to-a-trained-model
Does doc2vec work with multi-class problem with only 1 sample per class?,"<p>I have a test sentence (which define a skill, such as ""Perform equipment maintenance"") and a set of diplomas (10000 different diplomas) with description of the needed skills (=1 paragraph per diploma). My problem consists in finding the diploma closest to the test sentence in terms of semantic similarity.</p>

<p>I thought about creating a doc2vec model (multi-class, 1 class per diploma) in order to transform each diploma in feature vector, then infer vector for the test sentence and calculate cosine similarity with each feature vector. Yet, I only have one sample for each diploma. Will it still work? 
Or do I have to split the sentences of each diploma text in order to obtain several samples for a diploma ?</p>
","nlp, gensim, doc2vec","<p>Note that your dataset of ""one descriptive paragraph per diploma-type"" wouldn't normally be described as a ""multi-class"" problem. The term ""multi-class"" more usually describes situations where each item itself has more-than-one class/label applied.</p>

<p>But, your approaches as described might be productive, and the only way to know which is better, for your data &amp; goals, would be to evaluate them (and other variants) against each other.</p>

<p>Specifically:</p>

<p>As you describe, you could train a <code>Doc2Vec</code> model on your 10,000 short documents that each describe a different diploma. (This is on the smallish side for <code>Doc2Vec</code> training data, but perhaps enough. Using a smaller doc-vector size – say 100 dimensions or fewer – and/or more training <code>epochs</code> can sometimes help squeeze better results out of smaller datasets.) You'll now have 10,000 doc-vectors, one for each document/diploma.</p>

<p>Then for any new test document – your skill sentence – you'd calculate another doc-vector. You might do this by using the <code>gensim</code> <code>Doc2Vec</code> <code>infer_vector()</code> method. (Note that especially for short texts, you may want to use far more than default number of <code>epochs</code> for this calculation.) </p>

<p>Then, you'd search the model for which of the 1st 10,000 doc-vectors are closest to your new vector – and you might find that the closest match, or one of the top-N closest, is a good fit for what you need. </p>

<p>But, there are some other variants to consider:</p>

<ul>
<li><p>If in fact your diploma-descriptions are all multiple sentences – say dozens or even hundreds of words – while your skill-sentences are shorter – say a handful to dozen or two words – then perhaps the best matches to your test-sentence will be to just a <em>subset</em> of the longer-paragraphs. In such a case, it <em>might</em> be helpful to initially train the model on smaller fragments of text. For example, if the diploma-docs are each 5 sentences, maybe you train the model to know 50,000 separate doc-vectors. Testing a skill-sentence would then be both finding the top-N nearest matches, <em>then</em> looking up which diploma the nearby description-sentence was about. </p></li>
<li><p>If your skill-sentences are numerous, and also available at training time, you could plausibly include them in the model training as well. For example, perhaps in addition to your 10,000 diploma-descriptions, you have another 20,000 skill-sentences. You could let the initial model learn 30,000 doc-vectors. Then, you can look-up the diploma-doc-vectors closest to a skill-doc-vector without using <code>infer_vector()</code> at all – just using the skill-doc-vector learned during model-training – but you'd have to filter out other skill-doc-vector results from the <code>most_similar()</code> list. (You still have the option of using <code>infer_vector()</code> on any new texts that come along, but including more text in the initial training <strong>may</strong> make the overall model more expressive.)</p></li>
<li><p>Rather than simply mapping any probe skill-sentence to the single diploma-paragraph that's closest in <code>Doc2Vec</code>-coordinate space, if you already had many human-vetted examples of which-skill-sentences should be associates with which-diploma-paragraphs, you could use a second-step supervised classifier algorithm. That is, you'd only use <code>Doc2Vec</code> to create feature-vectors for texts, but learn the ""labels"" (diploma-types) for texts via another training process. There are many potential classifier algorithms, and they are data-hungry, but with enough data, some might have a better chance of learning irregular shapes in the underlying data, for example when a certain diploma-label should, based on historic examples, cover ""more"" of the skills-space than a simple text-vector-distance would indicate. </p></li>
</ul>
",0,0,51,2019-09-02 12:57:55,https://stackoverflow.com/questions/57757356/does-doc2vec-work-with-multi-class-problem-with-only-1-sample-per-class
Perform matrix multiplication with cosine similarity function,"<p>I have two lists:</p>

<pre><code>list_1 = [['flavor', 'flavors', 'fruity_flavor', 'taste'],
          ['scent', 'scents', 'aroma', 'smell', 'odor'],
          ['mental_illness', 'mental_disorders','bipolar_disorder']
          ['romance', 'romances', 'romantic', 'budding_romance']]

list_2 = [['love', 'eating', 'spicy', 'hand', 'pulled', 'noodles'],
          ['also', 'like', 'buy', 'perfumes'],
          ['suffer', 'from', 'clinical', 'depression'],
          ['really', 'love', 'my', 'wife']]
</code></pre>

<p>I would like to compute the cosine similarity between the two lists above in such a way where the cosine similarity between the first sub-list in list1 and all sublists of list 2 are measured against each other. Then the same thing but with the second sub-list in list 1 and all sub-lists in list 2, etc.</p>

<p>The goal is to create a <strong>len(list_2) by len(list_1) matrix</strong>, and each entry in that matrix is a cosine similarity score. Currently I've done this the following way:</p>

<pre><code>import gensim
import numpy as np
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True) 
similarity_mat = np.zeros([len(list_2), len(list_1)])

for i, L2 in enumerate(list_2):
    for j, L1 in enumerate(list_1):
        similarity_mat[i, j] = model.n_similarity(L2, L1)
</code></pre>

<p>However, I'd like to implement this with matrix multiplication and no for loops. </p>

<p>My two questions are:</p>

<ol>
<li>Is there a way to do some sort of element-wise matrix multiplication but with <code>gensim's n_similiarity() method</code> to generate the required matrix?</li>
<li>Would it be more efficient and faster using the current method or matrix multiplication?</li>
</ol>

<p>I hope my question was clear enough, please let me know if I can clarify even further.</p>
","python-3.x, numpy, gensim, word2vec, cosine-similarity","<p>There are two problems in code, the second last and last line.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>import gensim
import numpy as np
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format('/root/input/GoogleNews-vectors-negative300.bin.gz', binary=True) 
similarity_mat = np.zeros([len(list_2), len(list_1)])

for i, L2 in enumerate(list_2):
    for j, L1 in enumerate(list_1):
        similarity_mat[i, j] = model.n_similarity(L2, L1)</code></pre>
</div>
</div>
</p>

<p>Answers to you questions:<br>
1. You are already using a direct function to calculate the similarity between two sentences(L1 and L2) which are first converted to two vectors and then cosine similarity is calculated of those two vectors. Everything is already done inside the n_similarity() so you can't do any kind of matrix multiplication.<br>
If you want to do your own matrix multiplication then instead of directly using n_similarity() calculates the vectors of the sentences and then apply matrix multiplication while calculating cosine similarity.<br>
2. As I said in (1) that everything is done in n_similarity() and creators of gensim takes care of the efficiency when writing the libraries so any other multiplication method will most likely not make a difference.<br></p>
",-1,1,2965,2019-09-02 21:11:33,https://stackoverflow.com/questions/57762713/perform-matrix-multiplication-with-cosine-similarity-function
unable to install Gensim with Python 3.5 on ubuntu,"<p>I am running a dockerfile on a ubuntu base image as follows :</p>

<pre><code>FROM ubuntu:14.04

# Install dependencies
RUN apt-get update 
RUN apt-get install -y \
    software-properties-common
RUN add-apt-repository universe
RUN apt-get install -y python3.5 \
    python3-pip 

RUN apt-get install libav-tools -y

RUN apt-get update 

RUN apt-get upgrade

#RUN  apt-get install google-cloud-sdk

RUN pip3 install --upgrade pip 
RUN pip3 install pandas 
RUN pip3 install glob3

RUN     pip3 install --upgrade pip 
#RUN    pip3 install pandas 
RUN pip3 install glob3
#RUN    pip3 install json
RUN pip3 install numpy
RUN pip3 install fuzzywuzzy
RUN pip3 install gensim
</code></pre>

<p>I have python 3.5 installed on this machine, but still I am getting the error as follows :</p>

<pre><code>Collecting gensim
  Downloading https://files.pythonhosted.org/packages/3a/bc/1415be59292a23ff123298b4b46ec4be80b3bfe72c8d188b58ab2653dee4/gensim-3.8.0.tar.gz (23.4MB)
    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-klg_2vmh/gensim/setup.py'""'""'; __file__='""'""'/tmp/pip-install-klg_2vmh/gensim/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base pip-egg-info
         cwd: /tmp/pip-install-klg_2vmh/gensim/
    Complete output (5 lines):
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/pip-install-klg_2vmh/gensim/setup.py"", line 23, in &lt;module&gt;
        raise Exception('This version of gensim needs Python 2.7, 3.5 or later.')
    Exception: This version of gensim needs Python 2.7, 3.5 or later.
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
</code></pre>

<p>Is there some specific version of Gensim that i need to download or this is some different error.</p>
","python, docker, ubuntu, gensim","<p>use <code>python:3.5</code>:</p>

<pre><code>FROM python:3.5
RUN pip install gensim glob3 ....
</code></pre>

<p>that will save you a lot of space and steps ....</p>

<p>If you want to still using <code>ubuntu:14.04</code> then you need to use this:</p>

<pre><code>apt-get install -y python-dev &amp;&amp; python3.5 -m pip install gensim
</code></pre>

<p>since <code>python3.4</code> is the default on the <code>image</code></p>
",0,1,408,2019-09-05 11:32:05,https://stackoverflow.com/questions/57804554/unable-to-install-gensim-with-python-3-5-on-ubuntu
Does the &quot;iter&quot; parameter of gensim.models.Word2Vec method iterate over the whole corpus or the sentence passed to it at a time?,"<p>I am using gensim to train a word2Vec model. Here I am passing one sentence at a time to the gensim.models.Word2Vec() method from my corpus to gradually train the model on my whole corpus. But I am confused what should the value of iter parameter be as I'm not sure whether it iterates over the passed sentence n times or the whole corpus.    </p>

<p>I have tried checking the documentation of gensim. it states the definition as follows:<br></p>

<blockquote>
  <p>iter (int, optional) – Number of iterations (epochs) over the corpus.</p>
</blockquote>

<p>But I am confused as I am not passing the whole corpus but only a single sentence on each iteration.</p>

<p>My line in the code that trains the model looks like this:<br>
<code>model = gensim.models.Word2Vec(data, min_count=2, window=arg.window_size, size=arg.dim_size, workers=4, sg=0, hs=0, negative=10, ns_exponent=0.75, alpha=0.025, iter=1)</code>
<br>Here ""data"" represents a single sentence passed at a time from a generator. </p>

<p>Suppose I have a corpus of 2 sentences. ""X is a variable. Y is a variable too."". The model receives data = ""X is a variable."" first and data = ""Y is a variable too."" in 2nd iteration. 
Now to clarify, my question is,<b> whether iter = 50 will train my model iterating though ""X is a variable."" 50 times &amp; ""Y is a variable too."" 50 times or will it iterating though ""X is a variable. Y is a variable too."" (my whole corpus) 50 times. 
</b></p>
","python, gensim, word2vec","<p><code>Word2Vec</code> is a class. Calling it as <code>model = Word2Vec(...)</code> returns one new model instance. </p>

<p>If you supply data to that instantiation call, it expects a full training corpus, with all examples, as the data (<code>sentences</code> parameter). It will iterate over that data once to learn the vocabulary, then again the number of times specified in the <code>epochs</code> argument for training. (This argument was previously called <code>iter</code>, which still works.)</p>

<p>So:</p>

<ul>
<li>You shouldn't be calling <code>Word2Vec(...)</code> multiple times with single texts. You should call it once, with a re-iterable sequence of all your texts as the data. </li>
<li>That full supplied corpus will be iterated over <code>epochs</code> + 1 times as part of the model's initialization &amp; training, via the single call to <code>Word2Vec(...)</code>.</li>
</ul>

<p>You should enable logging at the INFO level to get a better idea of what's happening when you try different approaches. </p>

<p>You should also look at working examples, like the <code>word2vec.ipynb</code> notebook bundled with <code>gensim</code> inside its <code>docs/notebooks</code> directory, to understand usual usage patterns. (This is best viewed, and interactively run, from your local installation – but can also be browsed online at <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb</a>.)</p>

<p>Note that you <strong>can</strong> avoid supplying any data to the <code>Word2Vec(...)</code> instantiation call, but then you need to call <code>model.build_vocab(full_corpus)</code> and then <code>model.train(full_corpus, epochs=desired_iterations)</code> to complete the model initialization &amp; training. (While you can then continue calling <code>train()</code> with fragments of training data, that's an advanced &amp; highly error-prone approach. Only calling it just once, with one combined full training set, will easily and automatically do the right thing with the training learning-rate decay and number of training iterations.)</p>
",3,0,5355,2019-09-08 04:47:17,https://stackoverflow.com/questions/57839264/does-the-iter-parameter-of-gensim-models-word2vec-method-iterate-over-the-whol
How do I pass a file path containing spaces to the Gensim LDA Mallet wrapper?,"<p>I am attempting to use Gensim's Mallet wrapper. When I run the following code:</p>

<pre class=""lang-py prettyprint-override""><code>import os
import gensim

os.environ.update({
        'MALLET_HOME':
        r"":C\Users\me\OneDrive - My Company\Documents\Projects\Current\mallet-2.0.8""
    })
lda_mallet = gensim.models.wrappers.LdaMallet(
        r""C:\Users\me\OneDrive - My Company\Documents\Projects\Current\mallet-2.0.8\bin\mallet"",
        corpus=corpus,
        num_topics=10,
        id2word=id_dict)
</code></pre>

<p>I am thrown the following errors:</p>

<pre><code>'C:\Users\me\OneDrive' is not recognized as an internal or external command,
operable program or batch file.

subprocess.CalledProcessError: Command 'C:\Users\me\OneDrive - My Company\Documents\Projects\Current\mallet-2.0.8\bin\mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex ""\S+"" --input C:\Users\me\AppData\Local\Temp\17fe21_corpus.txt --output C:\Users\me\AppData\Local\Temp\17fe21_corpus.mallet' returned non-zero exit status 1.
</code></pre>

<p>After exhaustive online searches, I have found many proposed solutions that unfortunately do not resolve my issue.</p>

<p>Since the first error message does not print the entire path, I believe the spaces are the cause of the issue. </p>

<p>Unfortunately, my company requires that I use this directory and I cannot change the name. Is there a way to ""escape"" the spaces in order to run my code?</p>
","python, bash, gensim, lda, mallet","<p>Well, that's easy, <code>LdaMallet</code> class is a badly written piece of software, report this as a bug to its creators.</p>
",1,0,213,2019-09-11 19:49:13,https://stackoverflow.com/questions/57895899/how-do-i-pass-a-file-path-containing-spaces-to-the-gensim-lda-mallet-wrapper
Gensim&#39;s word2vec returning awkward vectors,"<p>Given heavily cleaned input in the format</p>

<pre class=""lang-py prettyprint-override""><code>model_input = [['TWO people admitted fraudulently using bank cards (...)'],
               ['All tyrants believe forever',
                'But history especially People Power (...) first Bulatlat']]
</code></pre>

<p>word2vec is returning alongside the more obvious results super-specific vectors such as</p>

<pre class=""lang-py prettyprint-override""><code>{'A pilot shot dogfight Pakistani aircraft returned India Friday freed Islamabad called peace gesture following biggest standoff two countries years':
     &lt;gensim.models.keyedvectors.Vocab at 0x12a93572828&gt;,
 'This story published content partnership POLITICO':
     &lt;gensim.models.keyedvectors.Vocab at 0x12a93572a58&gt;,
 'Facebook says none 200 people watched live video New Zealand mosque shooting flagged moderators underlining challenge tech companies face policing violent disturbing content real time': 
    &lt;gensim.models.keyedvectors.Vocab at 0x12a93572ba8&gt;}
</code></pre>

<p>It appears to be occurring to more documents than not, and I have a hard time believing they each appear more than five times.</p>

<p>I'm using the following code to create my model:</p>

<pre class=""lang-py prettyprint-override""><code>TRAIN_EPOCHS = 30
WINDOW = 5
MIN_COUNT = 5 
DIMS = 250

vocab_model = gensim.models.Word2Vec(model_input,
                                     size=DIMS,
                                     window=WINDOW,
                                     iter=TRAIN_EPOCHS,
                                     min_count=MIN_COUNT)
</code></pre>

<p>What am I doing wrong that I'm getting such useless vectors?</p>
","python, python-3.x, gensim, word2vec","<p><code>Word2Vec</code> expects its training corpus – its <code>sentences</code> argument – to be a re-iterable Python sequence where each item is itself a list-of-words. </p>

<p>Your <code>model_input</code> list appears to be a list, where each item is itself a list, but where each item in those lists is a full sentence of many words as a string. As a result, where it's expecting individual word-tokens (as strings), you're giving it full untokenized sentences (as strings). </p>

<p>If you break your texts into lists-of-words, and feed a sequence of those lists-of-words to the model as training data, then you'll get vectors for word-tokens, rather than sentence-strings.</p>
",1,0,77,2019-09-11 20:04:04,https://stackoverflow.com/questions/57896070/gensims-word2vec-returning-awkward-vectors
Gensim&#39;s FastText KeyedVector out of vocab,"<p>I want to use the read-only version of Gensim's FastText Embedding to save some RAM compared to the full model.</p>

<p>After loading the KeyVectors version, I get the following Error when fetching a vector:</p>

<p><code>IndexError: index 878080 is out of bounds for axis 0 with size 761210</code></p>

<p>The error occurs when using words that should be out-of-vocabulary e.g. ""lawyerxy"" instead of ""lawyer"". The full model returns a vector for both. </p>

<pre><code>from gensim.models import KeyedVectors
model = KeyedVectors.load(""model.kv"")
model .wv.__getitem__(""lawyerxy"")
</code></pre>

<p>So, my assumption is that the KeyedVectors do not offer FastText's out of vacabulary function - a key feature for my usecase. This limitation is not given in the documentation:
<a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a></p>

<p>Can anyone prove that assumption and/or name a fix to allow vectors for ""lawyerxy"" etc. ?</p>
","gensim, word-embedding, fasttext","<p>The <code>KeyedVectors</code> name is (as of <code>gensim-3.8.0</code>) just an <a href=""https://github.com/RaRe-Technologies/gensim/blob/a47eed80cf225181717cba09761922d4a54027d8/gensim/models/keyedvectors.py"" rel=""nofollow noreferrer"">alias</a> for class <code>Word2VecKeyedVectors</code>, which only maintains a simple word (as key) to vector (as value) mapping.</p>

<p>You shouldn't expect FastText's advanced ability to synthesize vectors for out-of-vocabulary words to appear in any model/representation that doesn't explicitly claim to offer that ability. </p>

<p>(I would expect a lookup of an out-of-vocabulary word to give a clearer <code>KeyError</code> rather than the <code>IndexError</code> you've reported. But, you'd need to show exactly what code created the file you're loading, and triggered the error, and the full error stack, to further guess what's going wrong in your case.)</p>

<p>Depending on how your <code>model.kv</code> file was saved, you might be able to load it, with retained OOV-vector functionality, by using the class <a href=""https://github.com/RaRe-Technologies/gensim/blob/a47eed80cf225181717cba09761922d4a54027d8/gensim/models/keyedvectors.py#L1943"" rel=""nofollow noreferrer""><code>FastTextKeyedVectors</code></a> instead of plain <code>KeyedVectors</code>. </p>
",2,0,1434,2019-09-12 09:28:22,https://stackoverflow.com/questions/57903695/gensims-fasttext-keyedvector-out-of-vocab
Gensim find vectors/words in ball of radius r,"<p>I would like take word ""book"" (for example) get its vector representation, call it v_1 and find all words whose vector representation is within ball of radius r of v_1 i.e. ||v_1 - v_i||&lt;=r, for some real number r.</p>

<p>I know gensim has <code>most_similar</code> function, which allows to state number of top vectors to return, but it is not quite what I need. I surely can use brute force search and get the answer, but it will be to slow. </p>
","python, gensim, word-embedding","<p>If you call <code>most_similar()</code> with a <code>topn=0</code>, it will return the raw <em>unsorted</em> cosine-similarities to <em>all</em> other words known to the model. (These similarities will not be in tuples with the words, but simply in the same order as the words in the <code>index2entity</code> property.) </p>

<p>You could then filter those similarities for those higher than your preferred threshold, and return just those indexes/words, using a function like <code>numpy</code>'s <a href=""https://stackoverflow.com/a/21632554/130288""><code>argwhere</code></a>.</p>

<p>For example:</p>

<pre><code>target_word = 'apple'
threshold = 0.9
all_sims = wv.most_similar(target_word, topn=0)
satisfactory_indexes = np.argwhere(all_sims &gt; threshold)
satisfactory_words = [wv.index2entity[i] for i in satisfactory_indexes]
</code></pre>
",1,0,39,2019-09-16 16:45:21,https://stackoverflow.com/questions/57961188/gensim-find-vectors-words-in-ball-of-radius-r
"Python3, word2vec, How can I get the list of similarity rank about &quot;price&quot; in my model","<p>In gensim's word2vec python, I want to get the list of cosine similarity for ""price"".</p>

<p>I read the document of gensim word2vec, but document it describes <code>most_similar</code> and <code>n_similarity</code> function)()</p>

<p>I want the whole list of similarity between price and all others.</p>
","python, gensim, word2vec, similarity, cosine-similarity","<p>If you call <code>wv.most_similar('price', topn=len(wv))</code>, with a <code>topn</code> argument of the full vocabulary count of your model, you'll get back a ranked list of every word's similarity to <code>'price'</code>. </p>

<p>If you call with <code>topn=0</code>, you'll get the raw similarities with all model words, unsorted (in the order the words appear inside <code>wv.index2entity</code>). </p>
",0,0,264,2019-09-17 07:53:15,https://stackoverflow.com/questions/57969707/python3-word2vec-how-can-i-get-the-list-of-similarity-rank-about-price-in-my
gensim word2vec extremely big and what are the methods to make file size smaller?,"<p>I have a pre-trained word2vec bin file by using skipgram. The file is pretty big (vector dimension of 200 ), over 2GB. I am thinking some methods to make the file size smaller. This bin file contains vectors for punctuation, some stop words. So, I want to know what are the options to decrease the file size for this word2vec. Is it safe to delete those punctuation and stop words rows and what would be the most effective way ?</p>
","python, gensim, word2vec","<p>The size of a full <code>Word2Vec</code> model is chiefly determined by the chosen vector-size, and the size of the vocabulary. </p>

<p>So your main options for big savings is to train smaller vectors, or a smaller vocabulary. </p>

<p>Discarding a few hundred stop-words or punctuation-tokens won't make a noticeable dent in the model size. </p>

<p>Discarding many of the least-frequent words can make a big difference in model size – and often those less-frequent words aren't as important as you might think. (While there are a lot of them in total, each only appears rarely. And because they're rare in the training data, they often tend not to have very good vectors, anyway – based on few examples, and their training influence is swamped by the influence of more-frequent words.)</p>

<p>The easiest way to limit the vocabulary size is to use a higher <code>min_count</code> value during training (ignoring all words with fewer occurrences), or a  fixed <code>max_final_vocab</code> cap (which will keep only that many of the most-frequent words). </p>

<p>Note also that if you've been saving/reloading full <code>Word2Vec</code> models (via the gensim-internal <code>.save()</code>/<code>.load()</code> methods), you're retaining model internal weights that are only needed for continued training, and will nearly double the model-size on disk or re-load. </p>

<p>You may want to save just the raw word-vectors in the <code>.wv</code> property instead (via either the gensim-internal <code>.save()</code> or the <code>.save_word2vec_format()</code> methods). </p>
",2,1,3507,2019-09-23 20:00:29,https://stackoverflow.com/questions/58069421/gensim-word2vec-extremely-big-and-what-are-the-methods-to-make-file-size-smaller
How to use doc2vec model in production?,"<p>I wonder how to deploy a doc2vec model in production to create word vectors as input features to a classifier. To be specific, let say, a doc2vec model is trained on a corpus as follows.</p>

<pre><code>dataset['tagged_descriptions'] = datasetf.apply(lambda x: doc2vec.TaggedDocument(
            words=x['text_columns'], tags=[str(x.ID)]), axis=1)

model = doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=150, workers=cores,
                                window=5, hs=0, negative=5, sample=1e-5, dm_concat=1)

corpus = dataset['tagged_descriptions'].tolist()

model.build_vocab(corpus)

model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)

</code></pre>

<p>and then it is dumped into a pickle file. The word vectors are used to train a classifier such as random forests to predict movies sentiment. </p>

<p>Now suppose that in production, there is a document entailing some totally new vocabularies. That being said, they were not among the ones present during the training of the doc2vec model. I wonder how to tackle such a case. </p>

<p>As a side note, I am aware of <a href=""https://stackoverflow.com/questions/47775557/updating-training-documents-for-gensim-doc2vec-model"">Updating training documents for gensim Doc2Vec model</a> and <a href=""https://stackoverflow.com/questions/39252207/gensim-how-to-retrain-doc2vec-model-using-previous-word2vec-model"">Gensim: how to retrain doc2vec model using previous word2vec model</a>. However, I would appreciate more lights to be shed on this matter. </p>
","python, nlp, gensim, doc2vec","<p>A <code>Doc2Vec</code> model will only be able to report trained-up vectors for documents that were present during training, and only be able to <code>infer_vector()</code> new doc-vectors for texts containing words that were present during training. (Unrecognized words passed to <code>.infer_vector()</code> will be ignored, similar to the way any words appearing fewer than <code>min_count</code> times are ignored during training.)</p>

<p>If over time you acquire many new texts with new vocabulary words, and those words are important, you'll have to occasionally re-train the <code>Doc2Vec</code> model. And, after re-training, the doc-vectors from the re-trained model will generally not be comparable to doc-vectors from the original model – so downstream classifiers and other applications using the doc-vectors will need updating, as well. </p>

<p>Your own production/deployment requirements will drive how often this re-training should happen, and old models replaced with newer ones. </p>

<p>(While a <code>Doc2Vec</code> model can be fed new training data at any time, doing so incrementally as a sort of 'fine-tuning' introduces hairy issues of balance between old and new data. And, there's no official gensim support for expanding existing the vocabulary of a <code>Doc2Vec</code> model. So, the most robust course is to retrain from scratch using all available data.)</p>

<p>A few side notes on your example training code:</p>

<ul>
<li><p>it's rare for <code>min_count=1</code> to be a good idea: rare words often serve as 'noise', without sufficient usage examples to model well, and thus 'noise' that just slows/interferes with patterns that can be learned from more common-words</p></li>
<li><p><code>dm_concat=1</code> is best considered an experimental/advanced mode, as it makes models significantly larger and slower to train, with unproven benefits. </p></li>
<li><p>much published work uses just 10-20 training epochs; smaller datasets or smaller docs will sometimes benefit from more, but 150 may be taking a lot of time with very little marginal benefit. </p></li>
</ul>
",5,2,1767,2019-09-23 20:24:45,https://stackoverflow.com/questions/58069724/how-to-use-doc2vec-model-in-production
Training a model from multiple corpus,"<p>Imagine I have a fasttext model that had been trained thanks to the Wikipedia articles (like explained on the official website).
Would it be possible to train it again with another corpus (scientific documents) that could add new / more pertinent links between words? especially for the scientific ones ?</p>

<p>To summarize, I would need the classic links that exist between all the English words coming from Wikipedia. But I would like to enhance this model with new documents about specific sectors. Is there a way to do that ? And if yes, is there a way to maybe 'ponderate' the trainings so relations coming from my custom documents would be 'more important'.</p>

<p>My final wish is to compute cosine similarity between documents that can be very scientific (that's why to have better results I thought about adding more scientific documents)</p>
","python, artificial-intelligence, gensim, training-data, fasttext","<p>Adjusting more-generic models with your specific domain training data is often called ""fine-tuning"". </p>

<p>The <code>gensim</code> implementation of <code>FastText</code> allows an existing model to expand its known-vocabulary via what's seen in new training data (via <code>build_vocab(..., update=True)</code>) and then for further training cycles including that new vocabulary to occur (through <code>train()</code>). </p>

<p>But, doing this particular form of updating introduces murky issues of balance between older and newer training data, with no clear best practices. </p>

<p>As just one example, to the extent there are tokens/ngrams in the original model that don't recur in the new data, new training is pulling those in the new data into new positions that are optimal for the new data... but potentially arbitrarily far from comparable compatibility with the older tokens/ngrams.)</p>

<p>Further, it's likely some model modes (like negative-sampling versus hierarchical-softmax), and some mixes of data, have a better chance of net-benefiting from this approach than others – but you pretty much have to hammer out the tradeoffs yourself, without general rules to rely upon. </p>

<p>(There may be better fine-tuning strategies for other kinds models; this is just speaking to the ability of the <code>gensim</code> <code>FastText</code> to update-vocabulary and repeat-train.)</p>

<p>But perhaps, your domain of interest is scientific texts. And maybe you also have a lot of representative texts – perhaps even, at training time, the complete universe of papers you'll want to compare. </p>

<p>In that case, are you sure you want to deal with the complexity of starting with a more-generic word-model? Why would you want to contaminate your analysis with any of the dominant word-senses in generic reference material, like Wikipedia, if in fact you already have sufficiently-varied and representative examples of <strong>your domain</strong> words in <strong>your domain</strong> contexts?</p>

<p>So I would recommend 1st trying to train your own model, from your own representative data. And only if you then fear you're missing important words/senses, try mixing in Wikipedia-derived senses. (At that point, another way to mix in that influence would be to mix Wikipedia texts with your other corpus. And  you should also be ready to test whether that really helps or hurts – because it could be either.)</p>

<p>Also, to the extent your real goal is comparing full papers, you might want to look into other document-modeling strategies, including bag-of-words representations, the <code>Doc2Vec</code> ('Paragraph Vector') implementation in <code>gensim</code>, or others. Those approaches will not necessarily require per-word vectors as an input, but might still work well for quantifying text-to-text similarities.</p>
",2,2,456,2019-09-25 13:28:56,https://stackoverflow.com/questions/58099559/training-a-model-from-multiple-corpus
Range for vector values in gensim model,"<p>I am extracting the word embeddings vector from a word2vec model using model.wv. What is the range of values for each element in this vector?</p>

<pre><code>import gensim

word2vec_model = gensim.models.Word2Vec.load(""testModel"")
word2vec_model.wv[""increase""] #What is range of values for each vector element?
</code></pre>

<p>Can't seem to find this information in the documentation.</p>
","gensim, word2vec","<p>Every dimension of the vector is 32-bit floating point value. </p>

<p>There's no essential or enforced limit other than that, though the training process is such that individual dimensions tend not to be ""very large"" – often staying in the range between -1.0 and 1.0.</p>

<p>It's common (but not required or beneficial for all applications) to normalize word-vectors to have a magnitude of 1.0 before comparing them to other similarly-normalized word-vectors. </p>

<p>You can request such a unit-normalized version of a word-vector with the <code>word_vec()</code> method's <code>use_norm</code> parameter:</p>

<pre><code>model.wv.word_vec(word, use_norm=True)
</code></pre>

<p>In such a unit-normed vector, no single dimension will be outside the range of -1.0 to 1.0.</p>
",4,1,1755,2019-09-26 19:01:59,https://stackoverflow.com/questions/58123189/range-for-vector-values-in-gensim-model
AWS Lambda Boto gensim model module initialization error: __exit__,"<p>Hosting a word2vec model with gensim on AWS lambda </p>

<p>using python 2.7
boto==2.48.0
gensim==3.4.0</p>

<p>and I have a few lines in my function.py file where I load the model directly from s3</p>

<pre><code>print('################### connecting to s3...')
s3_conn = boto.s3.connect_to_region(
        region,
        aws_access_key_id = Aws_access_key_id,
        aws_secret_access_key = Aws_secret_access_key,
        is_secure = True,
        calling_format = OrdinaryCallingFormat()
        )
print('################### connected to s3...')
bucket = s3_conn.get_bucket(S3_BUCKET)
print('################### got bucket...')
key = bucket.get_key(S3_KEY)
print('################### got key...')
model =  KeyedVectors.load_word2vec_format(key, binary=True)
print('################### loaded model...')
</code></pre>

<p>on the model loading line</p>

<pre><code>    model =  KeyedVectors.load_word2vec_format(key, binary=True)
</code></pre>

<p>getting a mysterious error without much details:</p>

<p>on the cloud watch can see all of my print messages til '################### got key...' inclusive, 
then I get: </p>

<pre><code>START RequestId: {req_id} Version: $LATEST 
</code></pre>

<p>then right after it [no time delays between these two messages]</p>

<pre><code>module initialization error: __exit__ 
</code></pre>

<p>please, is there a way to get a detailed error or more info?</p>

<p>More background details :
I was able to download the model from s3 to /tmp/ and it did authorize and retrieve the model file, but it went out of space [file is ~2GB, /tmp/ is 512MB]</p>

<p>so, switched to directly loading the model by gensim as above and now getting that mysterious error.</p>

<p>running the function with python-lambda-local works without issues </p>

<p>so, this probably narrows it down to an issue with gensim's smart open or aws lambda, would appreciate any hints, thanks! </p>
","python, amazon-web-services, aws-lambda, gensim, word2vec","<p>instead of connecting using boto,
simply:</p>

<pre><code>model = KeyedVectors.load_word2vec_format('s3://{}:{}@{}/{}'.format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, S3_BUCKET, S3_KEY), binary=True)
</code></pre>

<p>worked!</p>

<p>but of course, unfortunately, it doesn't answer the question on why the mysterious <strong>exit</strong> error came up and how to get more info :/</p>
",1,0,376,2019-10-01 09:53:47,https://stackoverflow.com/questions/58182293/aws-lambda-boto-gensim-model-module-initialization-error-exit
text classification model using doc2vec and gensim,"<p>I am doing text classification using gensim and doc2vec. I am using two data-sets for testing this, one being a stack exchange data-set and a Reddit data-set. I am trying to classify between posts from one subreddit/stackexchange site on a particular subject and then using posts from other unrelated subreddit/stackexchange sites as negative examples.</p>

<p>I am using a data-set of 10k posts to train the model and a testing set of 5k divided in to 50% positive examples and 50% negative. I then use the infer_vector and most_similar functions to classify the entry as positive or negative. Before training the model I pre-process the data to remove any words, symbols, links etc just leaving the most significant words to train the model. Below is the code used to train the model.</p>

<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv(""fulltrainingset.csv"")

df.columns.values[0] = ""A""

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df[""A""])]

epoch_list = [1,5,10,15,25,50,100,200,300,400]
size_list = [1,5,10,15,25,50,100,200,300]

for x in epoch_list:
    for y in size_list:

        vec_size = y
        max_epochs = x
        minimum_count = 1
        mode = 0
        window_ = 15
        negative_sampling = 5
        subsampling = 1e-5
        alpha = 0.025
        minalpha = 0.00025

        model = Doc2Vec(alpha=alpha, min_alpha=minalpha, vector_size=vec_size, dm=mode, min_count=minimum_count, window =window_, sample=subsampling ,hs =negative_sampling)
        model.build_vocab(tagged_data)

        for epoch in range(max_epochs):
            print('iteration {0}'.format(epoch))
            model.train(tagged_data,
                        total_examples=model.corpus_count,
                        epochs=model.epochs)#self.epochs
            model.alpha -= 0.0002
            model.min_alpha = model.alpha


        model.save(str(y)+""s_""+str(x)+""e.model"")
</code></pre>

<p>This method is working and I can get results from it, but I would like to know if there is a different way of training to achieve better results. Currently I am just training many models with different epochs and vector_sizes, then using the infer_vector and most_similar functions to see if the vector score returned from the most_similar entry is greater than a certain number, but is there a way to improve upon this in the aspect of training the model?</p>

<p>Also, aiming to get better results I trained another model in the same way with a larger data-set (100k+ entries). When I used this model on the same data-set it produced similar but worse results to the models trained on smaller data-sets. I thought that more training data would have improved the results not made them worse, does anyone know a reason for this ?</p>

<p>Also, to further test I created a new but bigger test-set (15k entries) which did even worse then the original test-set. The data in this test-set although being unique is the same type of data used in the original test-set yet produces worse results, what may be the reason for this ?</p>

<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv(""all_sec_tweets.csv"")

df.columns.values[0] = ""A""

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df[""A""])]

epoch_list = [1,5,10,15,25,50,100]
size_list = [1,5,10,15,25,50,100]

for x in epoch_list:
    for y in size_list:

        vec_size = y
        max_epochs = x
        mode = 0
        window_ = 5
        subsampling = 1e-5

        model = Doc2Vec(vector_size=vec_size, dm=mode, window =window_, sample=subsampling,epochs=max_epochs)
        model.build_vocab(tagged_data)

        model.train(tagged_data,total_examples=model.corpus_count,epochs=model.epochs)

        model.save(str(y)+""s_""+str(x)+""e.model"")
</code></pre>
","python, machine-learning, gensim, doc2vec","<p>It sounds as if you're training a separate <code>Doc2Vec</code> model for each forum's ""in""/""out"" decision, then using an improvised set of <code>infer_vector()</code>/<code>most_similar()</code> operations to make a decision. </p>

<p>That's a very rough, ad-hoc approach, and you should look into intros to more formal text-classification approaches, where there is a clear step of feature-discovery (which might include creating <code>Doc2Vec</code> vectors for your texts, or other techniques), then a clear step of classifier-training, then evaluation. </p>

<p>(You might also at that point then be training larger models which include labeled training examples from all forums, and classifiers which pick one-of-many possible classes.)</p>

<p>Separately, several things are wrong or non-optimal in your <code>Doc2Vec</code> training, including:</p>

<ul>
<li><p>It's almost always misguided to be calling <code>train()</code> more than once in your own loop, or to be changing the default <code>alpha</code>/<code>min_alpha</code> values. You current code is in fact making <code>model.epochs</code> (5) passes over the data for every call, and often decrementing <code>alpha</code> by <code>0.0002</code> hundreds of times (into nonsensensical negative values). Call <code>train()</code> just once, with the desired number of <code>epochs</code>, with default <code>alpha</code>/<code>min_alpha</code> values, and it will do the right thing. (And: don't trust whatever online tutorial/example suggested the above looping calls.)</p></li>
<li><p>Your <code>hs=5</code> will turn the strictly on/off hierarchical-softmax mode on, but leaves the default <code>negative=5</code> parameter in place - so your model will be using a (non-standard and probably unhelpful and slow) combination of both negative-sampling and hierarchical-softmax training. It's better to use either some <code>negative</code> value and <code>hs=0</code> (for pure negative-sampling), or <code>negative=0, hs=1</code> (for pure hierarchical-softmax). Or just stick with the default (<code>negative=5, hs=0</code>) unless/until everything is already working and you want to descend into deeper optimizations.</p></li>
<li><p><code>min_count=1</code> is rarely the best option: these models often benefit from discarding rare words.</p></li>
</ul>

<p>After correcting these issues, you may find that more data then tends to bring the usual expected improved results. (And if it doesn't at that time, double-check that all text preprocessing/tokenization is done right, at training and inference and evaluation – and if you're still having problems, perhaps post a new question then, with more specifics/numbers about where expected improvements have instead scored worse.) </p>
",0,0,424,2019-10-02 04:49:30,https://stackoverflow.com/questions/58195364/text-classification-model-using-doc2vec-and-gensim
Doc2Vec find the similar sentence,"<p>I am trying find similar sentence using doc2vec. What I am not able to find is actual sentence that is matching from the trained sentences.</p>
<p>Below is the code from <a href=""https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"" rel=""nofollow noreferrer"">this article</a>:</p>
<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
data = [&quot;I love machine learning. Its awesome.&quot;,
        &quot;I love coding in python&quot;,
        &quot;I love building chatbots&quot;,
        &quot;they chat amagingly well&quot;]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]
max_epochs = 100
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)
  
model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(&quot;d2v.model&quot;)
print(&quot;Model Saved&quot;)

model= Doc2Vec.load(&quot;d2v.model&quot;)
#to find the vector of a document which is not in training data
test_data = word_tokenize(&quot;I love building chatbots&quot;.lower())
v1 = model.infer_vector(test_data)
print(&quot;V1_infer&quot;, v1)

# to find most similar doc using tags
similar_doc = model.docvecs.most_similar('1')
print(similar_doc)


# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data
print(model.docvecs['1'])
</code></pre>
<p>But the above code only gives me vectors or numbers. But how can I get the actual sentence matched from training data. For Eg - In this case I am expecting the result as &quot;I love building chatbots&quot;.</p>
","python, nlp, gensim, doc2vec, sentence-similarity","<p>The output of <code>similar_doc</code> is: <code>[('2', 0.991769552230835), ('0', 0.989276111125946), ('3', 0.9854298830032349)]</code></p>
<p>This shows the similarity score of each document in the <code>data</code> with the requested document and it is sorted in descending order.</p>
<p>Based in this, <code>'2' index</code> in the <code>data</code> is the closest to the requested data i.e. <code>test_data</code>.</p>
<pre><code>print(data[int(similar_doc[0][0])])
// prints: I love building chatbots
</code></pre>
<p><strong>Note:</strong> this code is giving different results every time, maybe you need a better model or more training data.</p>
",3,1,6649,2019-10-02 17:39:01,https://stackoverflow.com/questions/58206571/doc2vec-find-the-similar-sentence
Loading Gensim FastText Model with Callbacks Fails,"<p>After creating a FastText model using Gensim, I want to load it but am running into errors seemingly related to callbacks. </p>

<p>The code used to create the model is</p>

<pre class=""lang-py prettyprint-override""><code>TRAIN_EPOCHS = 30
WINDOW = 5
MIN_COUNT = 50
DIMS = 256

vocab_model = gensim.models.FastText(sentences=model_input,
                                     size=DIMS,
                                     window=WINDOW,
                                     iter=TRAIN_EPOCHS,
                                     workers=6,
                                     min_count=MIN_COUNT,
                                     callbacks=[EpochSaver(""./ftchkpts/"")])

vocab_model.save('ft_256_min_50_model_30eps')
</code></pre>

<p>and the callback <code>EpochSaver</code> is defined as</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.callbacks import CallbackAny2Vec

class EpochSaver(CallbackAny2Vec):
    '''Callback to save model after each epoch and show training parameters '''

    def __init__(self, savedir):
        self.savedir = savedir
        self.epoch = 0
        os.makedirs(self.savedir, exist_ok=True)

    def on_epoch_end(self, model):
        savepath = os.path.join(self.savedir, f""ft256_{self.epoch}e"")
        model.save(savepath)
        print(f""Epoch saved: {self.epoch + 1}"")
        if os.path.isfile(os.path.join(self.savedir, f""ft256_{self.epoch-1}e"")):
            os.remove(os.path.join(self.savedir,  f""ft256_{self.epoch-1}e""))
            print(""Previous model deleted "")
        self.epoch += 1
</code></pre>

<p>Aside from the type of model, this is identical to my process for Word2Vec which worked without issue. However when I open another file and try to load the model with</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import FastText
vocab = FastText.load(r'vocab/ft_256_min_50_model_30eps')
</code></pre>

<p>I'm greeted with the error</p>

<blockquote>
  <p><code>AttributeError: Can't get attribute 'EpochSaver' on &lt;module '__main__'&gt;</code></p>
</blockquote>

<p>What can I do to get the vocabulary to load so I can create the embedding layer for my keras model? If it's relevant, this is happening in JupyterLab.</p>
","python, callback, gensim, jupyter-lab, fasttext","<p>This extra difficulty loading models with custom callbacks is a <a href=""https://github.com/RaRe-Technologies/gensim/issues/2136"" rel=""noreferrer"">known, open issue</a> (at least through <code>gensim-3.8.1</code> and October 2019). </p>

<p>You can see discussions of possible workarounds and fixes there – and the gensim team is considering simply disabling the auto-saving of callbacks at all, requiring them to be re-specified for each later <code>train()</code>/etc call that needs them. </p>

<p>You may be able to load existing models saved with your custom callbacks by importing those same callback classes, as the same names, into the code context where you're doing a <code>load()</code>. </p>

<p>You could save callback-free versions of your trained models by blanking the model's <code>callbacks</code> property to its empty default value, just before you <code>save()</code>, eg:</p>

<pre><code>model.callbacks = ()
model.save(save_path)
</code></pre>

<p>Then, you wouldn't need to do any special importing of custom classes before a <code>load()</code>. (Of course if you again needed callback functionality on the re-loaded model, they'd then have to be explicitly reestablished after <code>load()</code>). </p>
",6,1,1081,2019-10-04 14:08:54,https://stackoverflow.com/questions/58238043/loading-gensim-fasttext-model-with-callbacks-fails
Gensim word2vec model outputs 1000 dimension ndarray but the maximum number of ndarray dimensions is 32 - how?,"<p>I'm trying to use <a href=""https://github.com/idio/wiki2vec/"" rel=""nofollow noreferrer"">this</a> 1000 dimension wikipedia word2vec model to analyze some documents.</p>

<p>Using introspection I found out that the vector representation of a word is a 1000 dimension numpy.ndarray, however whenever I try to create an ndarray to find the nearest words I get a value error:</p>

<pre><code>ValueError: maximum supported dimension for an ndarray is 32, found 1000
</code></pre>

<p>and from what I can tell by looking around online 32 is indeed the maximum supported number of dimensions for an ndarray - so what gives? How is gensim able to output a 1000 dimension ndarray?</p>

<p>Here is some example code:</p>

<pre><code>doc = [model[word] for word in text if word in model.vocab]
out = []
n = len(doc[0])
print(n)
print(len(model[""hello""]))
print(type(doc[0]))
for i in range(n):
    sum = 0
    for d in doc:
        sum += d[i]
    out.append(sum/n)
out = np.ndarray(out)
</code></pre>

<p>which outputs:</p>

<pre><code>1000
1000
&lt;class 'numpy.ndarray'&gt;
ValueError: maximum supported dimension for an ndarray is 32, found 1000
</code></pre>

<p>The goal here would be to compute the average vector of all words in the corpus in a format that can be used to find nearby words in the model so any alternative suggestions to that effect are welcome.</p>
","python, gensim, word2vec","<p>You're calling <code>numpy</code>'s <code>ndarray()</code> constructor-function with a list that has 1000 numbers in it – your hand-calculated averages of each of the 1000 dimensions. </p>

<p>The <code>ndarray()</code> function expects its argument to be the <em>shape</em> of the matrix constructed, so it's trying to create a new matrix of shape <code>(d[0], d[1], ..., d[999])</code> – and then every individual value inside that matrix would be addressed with a 1000-int set of coordinates. And, indeed <code>numpy</code> arrays can only have 32 independent dimensions. </p>

<p>But even if you reduced the list you're supplying to <code>ndarray()</code> to just 32 numbers, you'd still have a problem, because your 32 numbers are floating-point values, and <code>ndarray()</code> is expecting integral counts. (You'd get a <code>TypeError</code>.)</p>

<p>Along the approach you're trying to take – which isn't quite optimal as we'll get to below – you really want to create a <em>single vector</em> of 1000 floating-point dimensions. That is, 1000 cell-like values – <strong>not</strong> <code>d[0] * d[1] * ... * d[999]</code> separate cell-like values. </p>

<p>So a crude fix along the lines of your initial approach could be replacing your last line with either:</p>

<pre><code>result = np.ndarray(len(d))
for i in range(len(d)):
    result[i] = d[i]
</code></pre>

<p>But there are many ways to incrementally make this more efficient, compact, and idiomatic – a number of which I'll mention below, even though the <em>best</em> approach, at bottom, makes most of these interim steps unnecessary.</p>

<p>For one, instead of that assignment-loop in my code just above, you could use Python's bracket-indexing assignment option:</p>

<pre><code>result = np.ndarray(len(d))
result[:] = d  # same result as previous 3-lines w/ loop
</code></pre>

<p>But in fact, <code>numpy</code>'s <code>array()</code> function can essentially create the necessary <code>numpy</code>-native <code>ndarray</code> from a given list, so instead of using <code>ndarray()</code> at all, you could just use <code>array()</code>:</p>

<pre><code>result = np.array(d)  # same result as previous 2-lines
</code></pre>

<p>But further, <code>numpy</code>'s many functions for natively working with arrays (and array-like lists) already include things to do averages-of-many-vectors in a single step (where even the looping is hidden inside very-efficient compiled code or CPU bulk-vector operations). For example, there's a <code>mean()</code> function that can average lists of numbers, or multi-dimensional arrays of numbers, or aligned sets of vectors, and so forth.</p>

<p>This allows faster, clearer, one-liner approaches that can replace your entire original code with something like:</p>

<pre><code># get a list of available word-vetors
doc = [model[word] for word in text if word in model.vocab]
# average all those vectors
out = np.mean(doc, axis=0)
</code></pre>

<p>(Without the <code>axis</code> argument, it'd average together all individual dimension-values , in all slots, into just one single final average number.)</p>
",2,1,1825,2019-10-06 00:41:33,https://stackoverflow.com/questions/58253405/gensim-word2vec-model-outputs-1000-dimension-ndarray-but-the-maximum-number-of-n
How to view word2vec model,"<p>I just want to be able to see the values in my word2vec model.</p>

<p>I have a  very small corpus. I just want to see exactly what happens in each step for this particular corpus.</p>

<p>A section of my code is below.</p>

<pre class=""lang-py prettyprint-override""><code>word2vec = Word2Vec(corpus, min_count=1)
word_vectors = word2vec.wv 

termsim_index = WordEmbeddingSimilarityIndex(word_vectors)


dictionary = corpora.Dictionary(food)
bow_corpus = [dictionary.doc2bow(doc) for doc in food]


similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)  
docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=10)

</code></pre>

<p>So I want to see what exactly is in <code>word_vectors</code>,<code>termsim_index</code>,<code>similarity_matrix</code> , <code>docsim_index</code></p>
","python, gensim, word2vec","<p>To see more of what's happening during each function, you should enable logging at the <code>INFO</code> level. </p>

<p>But then, each of your created objects have documented properties you can freely examine – either by looking at the gensim docs per class, or using generic Python operations – like those described in other SO questions, such as <a href=""https://stackoverflow.com/questions/192109/is-there-a-built-in-function-to-print-all-the-current-properties-and-values-of-a"">Is there a built-in function to print all the current properties and values of an object?</a>. </p>

<p>To give more specific suggestions, you'd have to explain more what exactly you ""want to see"".</p>
",1,0,259,2019-10-09 09:49:46,https://stackoverflow.com/questions/58301450/how-to-view-word2vec-model
How to save as a gensim word2vec file?,"<p>I have two lists, A is a list of words, for example [""hello"",""world"",......], Len(A) is 10000. List B contains the all pre-trained vectors corresponding to A, which is a [10000,512], 512 is the vector dimension. I want to convert two lists into gensim word2vec model format in order to load the model in later, such as <code>model = Word2Vec.load(""word2vec.model"")</code> how should I do this? </p>
","gensim, word2vec","<p>As you only have the words and their vectors, you don't quite have enough info for a full <code>Word2Vec</code> model (which includes other things like the internal neural network's hidden weights, and word frequencies). </p>

<p>But you can create a <code>gensim</code> <code>KeyedVectors</code> object, of the general kind that's in a <code>gensim</code> <code>Word2Vec</code> model <code>.wv</code> property. It has many of the helper methods (like <code>most_similar()</code>) you may be interested in using. </p>

<p>Let's assume your <em>A</em> list-of-words is in a more-helpfully named Python list called <code>words_list</code>, and your <em>B</em> list-of-vectors is in a more-helpfully named Python list called 'vectors_list`.</p>

<p>Try:</p>

<pre><code>from gensim.models import KeyedVectors
kv = new KeyedVectors(512)
kv.add(words_list, vectors_list)
kv.save(`mywordvecs.kvmodel`)
</code></pre>

<p>You could then later re-load these via:</p>

<pre><code>kv2 = KeyedVectors.load(`mywordvecs.kvmodel`)
</code></pre>

<p>(You could also use <code>save_word2vec_format()</code> and <code>load_word2vec_format()</code> instead of gensim's native <code>save()</code>/<code>load()</code>, if you wanted simpler plain-vectors formats that could also be loaded by other tools that use that format. But if you're staying within <code>gensim</code>, the plain <code>save()</code>/<code>load()</code> are just as good – and would be better if saving a more complex trained <code>Word2Vec</code> model, because they'd retain the extra info those objects contain.)</p>
",6,4,9525,2019-10-15 10:58:26,https://stackoverflow.com/questions/58393090/how-to-save-as-a-gensim-word2vec-file
"FastText .bin file cannot fit in memory, even though I have enough RAM","<p>I'm trying to load one of the FastText pre-trained models that has a form of a .bin file. The size of .bin file is 2.8GB and I have 8GB RAM and 8GB swap file. Unfortunately, the model starts loading and it occupies almost 15GB and then it breaks with the following error:</p>

<p><code>Process finished with exit code 137 (interrupted by signal 9: SIGKILL)</code></p>

<p>By observing the system monitor, I can see that RAM and swap are fully occupied, so I think it breaks because it is out of memory.</p>

<p>I'm trying to load the file using Gensim wrapper for FastText</p>

<p><code>from gensim.models.wrappers import FastText
 model = FastText.load_fasttext_format('../model/java_ftskip_dim100_ws5')</code></p>

<hr>

<p>My questions are the following:</p>

<p>1) Is there any way to fit this model in the current memory of my system?</p>

<p>2) Is it possible to reduce the size of this model? I tried the quantization using the following code</p>

<p><code>./fasttext quantize -output java_ftskip_dim100_ws5 -input unused_argument.txt</code></p>

<p>And I'm getting the following error:</p>

<p><code>terminate called after throwing an instance of 'std::invalid_argument'
  what():  For now we only support quantization of supervised models
Aborted (core dumped)</code></p>

<p>I would really appreciate your help!</p>
","python, gensim, fasttext","<p>Some expansion beyond the size-on-disk is expected – especially once you start performing operations like <code>most_similar()</code>. But, if you're truly getting that error from running a mere 2 lines to load the model, something else may be wrong. </p>

<p>You may want to try the non-<code>wrappers</code> gensim <code>FastText</code> implementation – <code>from gensim.models import FastText</code> – in the latest gensim, just in case there are extra memory issues with the version you're using. </p>

<p>(You may also want to check if using the original, compiled Facebook FastText implementation can load the file, and shows similar memory usage.)</p>

<p>I'm not aware of any straightforward ways to shrink a preexisting FastText model. (If you were training the model from your own data, there are a number of pre-training initialization options that could result in a smaller model. But those limits are not meaningful to apply to an already-trained model.)</p>

<p>As you've seen, Facebook has only implemented the 'quantize' trick for the supervised models – and even if that transformation could be applied to more modes, the supporting gensim code would also then need extra updates to understand the changed models.</p>

<p>If you could load it once, in the full (non-<code>wrappers</code>) gensim implementation, it might be practical to truncate all included vectors to be of a lower dimensionality for significant RAM savings, then re-save the model. But given that these are already just-100-dimension vectors, that might cost a lot in expressiveness.</p>
",2,2,2791,2019-10-16 07:10:25,https://stackoverflow.com/questions/58407649/fasttext-bin-file-cannot-fit-in-memory-even-though-i-have-enough-ram
Default values of doc2vec for alpha and min_alpha,"<p>can anybody tell me which default values are used in <code>Doc2Vec()</code> for <code>alpha</code> and <code>min_alpha</code>? </p>
","python, scikit-learn, gensim, doc2vec, hyperparameters","<p>The exact defaults for all parameters are listed in the documentation – but might, for parameters shared with a 'base' class, be shown in that superclass's docs.</p>
<p>So when you don't see <code>alpha</code> and <code>min_alpha</code> shown on the prototype-line of the <code>Doc2Vec</code> documentation....</p>
<p><a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec</a></p>
<p>...you can click the link just under it, where it says...</p>
<blockquote>
<p>Bases: <code>gensim.models.word2vec.Word2Vec</code></p>
</blockquote>
<p>...to reach its base class <code>Word2Vec</code> and find those &amp; many more defaults specified:</p>
<p><a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec</a></p>
<p>Specifically, per the text there...</p>
<blockquote>
<p>class <code>gensim.models.word2vec.Word2Vec</code>(sentences=None, corpus_file=None, vector_size=100, <strong>alpha=0.025</strong>, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, <strong>min_alpha=0.0001</strong>, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None)</p>
</blockquote>
<p>...the defaults are <code>alpha=0.025, min_alpha=0.0001</code>.</p>
<p>Most users shouldn't need to tinker with these at all: most metaparameter optimization effort should be directed elsewhere.</p>
<p>In some published work, in some modes of this and related algorithms, I've seen a higher starting <code>alpha</code> of <code>0.05</code> or <code>0.1</code> used.</p>
",3,2,1843,2019-10-16 11:58:44,https://stackoverflow.com/questions/58412763/default-values-of-doc2vec-for-alpha-and-min-alpha
best training methods for binary text classification using doc2vec gensim,"<p>I am trying to use doc2vec to do text classification based on document subject, for example, I want to classify all documents about sports as 1 and all other documents as 0. I want to do this by first training a doc2vec model with training data and then use a classification model such as logistic regression to classify the texts as positive or negative.</p>

<p>I have seen various examples online to do this [<a href=""https://fzr72725.github.io/2018/01/14/genism-guide.html"" rel=""nofollow noreferrer"">1</a>,<a href=""https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"" rel=""nofollow noreferrer"">2</a>] which employ different methods and I am unclear about some of the details as to why they are using certain methods, and which method is the best for text classification.</p>

<ol>
<li><p>Firstly Using the example above, is it better to train the model using just documents related to sports or documents on all subjects. My thinking was by training just on sports documents you could classify documents based on document similarity(although this wouldnt produce vectors for non sports documents to use to train the next model). Also, i feel like if training the model on all documents you would need a huge amount of documents to represent everything other than sports to get good classification. </p></li>
<li><p>Secondly, which features are actually used to train the logistic regression model. If training the model on all documents I assume you would track the documents using an index of some sort and then train the logistic regression model using the vectors with a class label, is this correct ? </p></li>
<li><p>Thirdly, I have seen various uses of TaggedDocument where a unique id is put for each document and also where a shared id is used to represent the same class, eg., 1 = sports 0 = non sports. From what I have read a shared id means the model has a single vector representing each class, while using a unique id provides unique vectors for each document, is this correct ?. If so, assuming that I need unique labeled vectors for training the logistic regression model what is the point of using a shared id ? Wouldnt this provide terrible classification results ?</p></li>
</ol>

<p>If anyone can help me with the questions above and generally what is the best way to do text classification using doc2vec vectors it would be greatly appreciated.</p>
","machine-learning, gensim, doc2vec","<p>There's an example included in <code>gensim</code> of using <code>Doc2Vec</code> for sentiment-classification, very close to your need. See:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/bcee414663bdcbdf6a58684531ee69c6949550bf/docs/src/gallery/howtos/run_doc2vec_imdb.py"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/bcee414663bdcbdf6a58684531ee69c6949550bf/docs/src/gallery/howtos/run_doc2vec_imdb.py</a></p>

<p>(It's likely a better model than the other tutorials you link. In particular, the <a href=""https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"" rel=""nofollow noreferrer"">second tutorial</a> you've linked currently has a very erroneous mismanagement of <code>alpha</code> in its misguided loop calling <code>train()</code> multiple times.)</p>

<p>Specifically with regard to your questions:</p>

<p>(1) Train with as much data, both inside and outside the desired class, as possible. A model that's only seen ""positive"" examples is unlikely to generate meaningful vectors from documents totally unlike those it's been trained on. (In particular, with a model like <code>Doc2Vec</code>, it only knows words it's seen during training, and if you later try to infer vectors for new documents with unknown words, those words are ignored entirely.)</p>

<p>(2) Yes, a classifier (of any algorithm) is fed features <em>and</em> known-labels. It then learns to deduce those labels from those features. </p>

<p>(3) Traditionally, <code>Doc2Vec</code> is trained with one unique ID 'tag' per document – and no known-label information. So, each document gets its own vector, and the process is totally ""unsupervised"". It's possible to instead give documents multiple tags, or use the same tag on more than one document. And, you could make those tags match known-labels – so all ""sports"" docs share <code>'sports'</code> tag (either in addition to their unique-ID, or instead of it). But, doing this adds a number of other complications over the simple, one-ID-tag-per-document, case. So I wouldn't recommend trying anything in that direction until you've got the simpler case working. (I have seen a few cases where mixing in known-labels as extra tags can help a little, especially in multi-class classification issues, where such extra labels each only apply to a small subset of all documents. But it's not assured – and thus only makes sense to tinker with that after you have a working straightforward baseline, and repeatable way to evaluate alternate models against each other.)</p>
",4,1,3006,2019-10-22 05:10:55,https://stackoverflow.com/questions/58497442/best-training-methods-for-binary-text-classification-using-doc2vec-gensim
"Calculate Cross-Lingual Phrase Similarity (using e.g., MUSE and Gensim)","<p>I am new to NLP and Word Embeddings and still need to learn many concepts within these topics, so any pointers would be appreciated. This question is related to <a href=""https://stackoverflow.com/questions/45571295/semantic-similarity-across-multiple-languages"">this</a> and <a href=""https://stackoverflow.com/questions/51233632/word2vec-gensim-multiple-languages?noredirect=1&amp;lq=1"">this</a>, and I think there may have been developments since these questions had been asked. Facebook <a href=""https://arxiv.org/pdf/1710.04087.pdf"" rel=""nofollow noreferrer"">MUSE</a> provides aligned, supervised <a href=""https://github.com/facebookresearch/MUSE#multilingual-word-embeddings"" rel=""nofollow noreferrer"">word embeddings for 30 languages</a>, and it can be used to calculate word similarity across different languages. As far as I understand, The embeddings provided by MUSE satisfy the requirement of <a href=""https://stackoverflow.com/questions/45571295/semantic-similarity-across-multiple-languages"">coordinate space compatibilty</a>. It seems that it is possible to <a href=""https://datascience.stackexchange.com/questions/20071/how-do-i-load-fasttext-pretrained-model-with-gensim"">load these embeddings into libraries such as Gensim</a>, but I wonder: </p>

<ol>
<li>Is it possible to load multiple-language word embeddings 
into Gensim (or other libraries), and if so:</li>
<li>What type of similarity measure
might fit in this use case?</li>
<li>How to use these loaded word embeddings
to calculate cross-lingual similarity score of phrases* instead of
words?</li>
</ol>

<p>*e.g., ""<em>ÖPNV</em>"" in German vs ""<em>Trasporto pubblico locale</em>"" in Italian for the English term ""<em>Public Transport</em>"". </p>

<p>I am open o any implementation (libraries/languages/embeddings) though I may need some time to learn this topic. Thank you in advance.</p>
","python, nlp, multilingual, gensim, word-embedding","<p>It is quite usual to average multiple word embeddings to get a phrase or sentence representation. After all, this is exactly what FastText does by default when it is used for sentence classification.</p>

<p>You can, of course, load as many word-embeddings sets in Gensim, but you would need to implement the cross-lingual comparison yourself. You can the vector just using the square bracket notation:</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.fasttext.load_facebook_model('your_path')
vector = model['computer']
</code></pre>

<p>Just use cosine similarity for comparing the vector. If you don't want to write it yourself, use <a href=""https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cosine.html"" rel=""nofollow noreferrer"">scipy</a>. </p>
",2,1,1033,2019-10-25 10:37:05,https://stackoverflow.com/questions/58556924/calculate-cross-lingual-phrase-similarity-using-e-g-muse-and-gensim
total_words must be provided alongside corpus_file argument,"<p>I am training doc2vec with corpus file, which is very huge.     </p>

<pre><code>model = Doc2Vec(dm=1, vector_size=200, workers=cores, comment='d2v_model_unigram_dbow_200_v1.0')
model.build_vocab(corpus_file=path)
model.train(corpus_file=path, total_examples=model.corpus_count, epochs=model.iter)
</code></pre>

<p>I want to know how to get value of total_words.</p>

<p>Edit:</p>

<pre><code>total_words=model.corpus_total_words
</code></pre>

<p>Is this right?</p>
",gensim,"<p>According to the current (gensim 3.8.1, October 2019) <a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec.train"" rel=""nofollow noreferrer""><code>Doc2Vec.train()</code> documentation</a>, you shouldn't need to supply both <code>total_examples</code> and <code>total_words</code>, only one or the other:</p>

<blockquote>
  <p>To support linear learning-rate decay from (initial) alpha to
  min_alpha, and accurate progress-percentage logging, either
  total_examples (count of documents) or total_words (count of raw words
  in documents) MUST be provided. If documents is the same corpus that
  was provided to build_vocab() earlier, you can simply use
  total_examples=self.corpus_count.</p>
</blockquote>

<p>But, it turns out the new <code>corpus_file</code> option does require both, and the doc-comment is wrong. I've filed <a href=""https://github.com/RaRe-Technologies/gensim/issues/2665"" rel=""nofollow noreferrer"">a bug</a> to fix this documentation oversight. </p>

<p>Yes, the model caches the number of words observed during the most-recent <code>build_vocab()</code> inside <code>model.corpus_total_words</code>, so <code>total_words=model.corpus_total_words</code> should do the right thing for you.  </p>

<p>When using the <code>corpus_file</code> space-delimited text input option, then the numbers given by <code>corpus_count</code> and <code>corpus_total_words</code> should match the line- and word- counts you'd also see by running <code>wc your_file_path</code> at a command-line. </p>

<p>(If you were using the classic, plain Python iterable corpus option (which can't use threads as effetively), then there would be no benefit to supplying both <code>total_examples</code> and <code>total_words</code> to <code>train()</code> – it would only use one or the other for estimating progress.)</p>
",1,0,992,2019-10-31 01:07:27,https://stackoverflow.com/questions/58635642/total-words-must-be-provided-alongside-corpus-file-argument
word2vec - KeyError: &quot;word X not in vocabulary&quot;,"<p>Using the <code>Word2Vec</code> implementation of the module <code>gensim</code> in order to construct word embeddings for the sentences I do have in a plain text file. Despite the word <code>happy</code> is defined in the vocabulary, getting the error <code>KeyError: ""word 'happy' not in vocabulary""</code>. Tried to apply the given the answers to <a href=""https://stackoverflow.com/questions/41133844/keyerror-word-word-not-in-vocabulary-in-word2vec"">a similar question</a>, but did not work. Hence, posted my own question.</p>

<p>Here is the code:</p>

<pre><code>try:
    data = []
    with open(TXT_PATH, 'r', encoding='utf-8') as txt_file:
        for line in txt_file:
            for part in line.split(' '):
                data.append(part.strip())

    # When I debug, both of the words 'happy' and 'birthday' exist in the variable 'data'
    word2vec = Word2Vec(data, min_count=5, size=10000, window=5, workers=4)

    # Print result
    word_1 = 'happy'
    word_2 = 'birthday'
    print(f'Similarity between {word_1} and {word_2} thru word2vec: {word2vec.similarity(word_1, word_2)}')
except Exception as err:
    print(f'An error happened! Detail: {str(err)}')
</code></pre>
","gensim, word2vec, word-embedding","<p>When you get a ""not in vocabulary"" error like this from <code>Word2Vec</code>, you can trust it: <code>'happy'</code> really isn't in the model. </p>

<p>Even if your visual check shows <code>'happy'</code> inside your file, a few reasons why it might not wind up inside the model include:</p>

<ul>
<li><p>it doesn't occur at least <code>min_count=5</code> times</p></li>
<li><p>the <code>data</code> format isn't correct for <code>Word2Vec</code>, so it's not seeing the words you expect it to see. </p></li>
</ul>

<p>Looking at how <code>data</code> is prepared by your code, it looks like a giant list of all words in your file. <code>Word2Vec</code> instead expects a sequence that has, as each item, a list-of-words for that one text. So: not a list-of-words, but a list where each item is a list-of-words. </p>

<p>If you've supplied...</p>

<pre class=""lang-py prettyprint-override""><code>[
  'happy',
  'birthday',
]
</code></pre>

<p>...instead of the expected...</p>

<pre class=""lang-py prettyprint-override""><code>[
  ['happy', 'birthday',],
]
</code></pre>

<p>...those single-word-strings will be seen a lists-of-characters, so <code>Word2Vec</code> will think you want to learn word-vectors for a bunch of one-character words. You can check if this has affected your model by seeing if the vocabulary size seems small (<code>len(model.wv)</code>) or if a sample of learned-words is only single-character words ('model.wv.index2entity[:10]`).</p>

<p>If you supply a word in the right format, at least <code>min_count</code> times, as part of the training-data, it will wind up with a vector in the model.</p>

<p>(Separately: <code>size=10000</code> is a choice way outside the usual range of 100-400. I've never seen a project using such high-dimensionality for word-vectors, and it would only be theoretically justifiable if you had a massively-large vocabulary and training-set. Oversized vectors with smaller vocabularies/data are likely to create uselessly overfit results.)</p>
",2,0,3011,2019-11-01 22:45:31,https://stackoverflow.com/questions/58666699/word2vec-keyerror-word-x-not-in-vocabulary
fastText - Throws exception without any reasons,"<p>I'm using <code>fastText</code> implementation of the module <code>gensim</code>. Despite getting no reasons, my program throws an exception.</p>

<p>Here is the code:</p>

<pre><code>try:
    data = []
    with open(TXT_PATH, 'r', encoding='utf-8') as txt_file:
        for line in txt_file:
            for part in line.split(' '):
                data.append(part.strip())

    fastText = FastText(data, min_count=1, size=10000, window=5, workers=4)

    # Print results
    word_1 = 'happy'
    word_2 = 'birthday'
    print(f'Similarity between {word_1} and {word_2} thru fastText: {fastText.similarity(word_1, word_2)}')
except Exception as err:
    print(f'\n!!!!! An error happened! Detail: {str(err)}')
</code></pre>

<p>The end of the output:</p>

<pre><code>!!!!! An error happened! Detail: 
</code></pre>
","python-3.x, gensim, word-embedding, fasttext","<p>Per my answer on <a href=""https://stackoverflow.com/questions/58666699/word2vec-keyerror-word-x-not-in-vocabulary"">your other question</a>, your <code>data</code> doesn't appear to be in the right format (where each item is a list-of-strings), and <code>size=10000</code> is far outside of the usual range of sensible vector-sizes. </p>

<p>But mainly, if you want more exception info, you shouldn't be catching <code>Exception</code> and printing your own minimal, cryptic error message. Remove the <code>try</code>/<code>except</code> handling from your code, run it again, and you should see a more helpful error message, including a call stack which shows exactly which line of your code (and lines of called library code) are involved in the error condition. </p>

<p>If that alone doesn't guide you to fix the issue, you could add the extra details of the full error &amp; call stack to your question to help others see what's happening. </p>
",0,0,198,2019-11-01 23:02:35,https://stackoverflow.com/questions/58666807/fasttext-throws-exception-without-any-reasons
Is there a way to load spacy trained model into gensim?,"<p>I want to get the list of similar words. Since Spacy doesn't have a built-in support for this I want to convert the spacy model to gensim word2vec and get the list of similar words.</p>

<p>I have tried to use the below method. But it is time consuming.</p>

<pre class=""lang-py prettyprint-override""><code>def most_similar(word):
    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)
    return [w.orth_ for w in by_similarity[:10]]
</code></pre>

<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load('en_core_web_md')
nlp.to_disk(filename)
nlp.vocab.vectors.to_disk(filename)
</code></pre>

<p>This does not save the model to a text file. Hence, I am not able to use the following method.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec

glove_file = datapath('test_glove.txt')
tmp_file = get_tmpfile(""test_word2vec.txt"")

_ = glove2word2vec(glove_file, tmp_file)
</code></pre>
","python-3.x, nlp, gensim, spacy, similarity","<p><strong>step 1</strong>: Extract the words and their vectors for the Spacy model   (see relevant documentation <a href=""https://spacy.io/api/vectors#items"" rel=""noreferrer"">here</a>).<br>
<strong>step 2</strong> : Create an instance of the class  gensim.models.keyedvectors.WordEmbeddingsKeyedVectors
(see relevant documentation <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors"" rel=""noreferrer"">here</a>).<br>
<strong>step 3</strong>: add add the words and vectors to the WordEmbeddingsKeyedVectors instance.</p>

<pre><code>import spacy
from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors

nlp = spacy.load('en_core_web_lg')

wordList =[]
vectorList = []
for key, vector in nlp.vocab.vectors.items():
    wordList.append(nlp.vocab.strings[key] )
    vectorList.append(vector)

kv = WordEmbeddingsKeyedVectors(nlp.vocab.vectors_length)

kv.add(wordList, vectorList)

print(kv.most_similar('software'))
# [('Software', 0.9999999403953552), ('SOFTWARE', 0.9999999403953552), ('Softwares', 0.738474428653717), ('softwares', 0.738474428653717), ('Freeware', 0.6730758547782898), ('freeware', 0.6730758547782898), ('computer', 0.67071533203125), ('Computer', 0.67071533203125), ('COMPUTER', 0.67071533203125), ('shareware', 0.6497008800506592)]



</code></pre>
",5,1,763,2019-11-05 11:04:03,https://stackoverflow.com/questions/58710000/is-there-a-way-to-load-spacy-trained-model-into-gensim
How to add words and vectors manually to Word2vec gensim?,"<p>Let's say, <strong>word2vec.model</strong> is my trained word2vec model. When a out-of-vocabulary word (<strong>oov_word</strong>) occurs, I compute a vector <strong>vec</strong> using <em>compute_vec(oov_word)</em> method. Now, I want to add/append <strong>oov_word</strong> and its corresponding vector <strong>vec</strong> to my already trained model <strong>word2vec.model</strong>.</p>

<p>I have already checked the below links. But they do not answer my question.</p>

<p><a href=""https://stackoverflow.com/questions/54243797/combining-adding-vectors-from-different-word2vec-models"">Combining/adding vectors from different word2vec models</a></p>

<p><a href=""https://datascience.stackexchange.com/questions/49431/how-to-train-an-existing-word2vec-gensim-model-on-new-words"">https://datascience.stackexchange.com/questions/49431/how-to-train-an-existing-word2vec-gensim-model-on-new-words</a></p>

<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.add"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.add</a></p>
","gensim, word2vec","<pre><code>from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors
vector_length = 100
kv = WordEmbeddingsKeyedVectors(vector_length)

# wordList - list of words
# vectorList - list of the vector corresponding to the words

kv.add(wordList, vectorList)

kv.most_similar(word1) # gives the list of words similar to word1
</code></pre>
",3,3,4785,2019-11-05 13:55:28,https://stackoverflow.com/questions/58712856/how-to-add-words-and-vectors-manually-to-word2vec-gensim
Gensim -- [Errno 2] No such file or directory: &#39;model.wv&#39;,"<p>I've got a question during following the simple gensim tutorial on <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim website</a>,</p>

<pre><code>&gt;&gt;&gt; from gensim.test.utils import common_texts, get_tmpfile
&gt;&gt;&gt; from gensim.models import Word2Vec
&gt;&gt;&gt;
&gt;&gt;&gt; path = get_tmpfile(""word2vec.model"")
&gt;&gt;&gt;
&gt;&gt;&gt; model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
&gt;&gt;&gt; model.save(""word2vec.model"")
&gt;&gt;&gt; model = Word2Vec.load(""word2vec.model"")
&gt;&gt;&gt; model.train([[""hello"", ""world""]], total_examples=1, epochs=1)

&gt;&gt;&gt; from gensim.models import KeyedVectors
&gt;&gt;&gt;
&gt;&gt;&gt; path = get_tmpfile(""wordvectors.kv"")
&gt;&gt;&gt;
</code></pre>

<p>And when I tried below,</p>

<pre><code>&gt;&gt;&gt; model.wv.save(path)
&gt;&gt;&gt; wv = KeyedVectors.load(""model.wv"", mmap='r')
</code></pre>

<p>I've got a following error :</p>

<pre><code>---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
&lt;ipython-input-81-eee6865b677b&gt; in &lt;module&gt;
      1 path = get_tmpfile('wordvectors.kv')
      2 model.wv.save(path)
----&gt; 3 KeyedVectors.load(""model.wv"",mmap='r')

/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load(cls, fname_or_handle, **kwargs)
    210     @classmethod
    211     def load(cls, fname_or_handle, **kwargs):
--&gt; 212         return super(BaseKeyedVectors, cls).load(fname_or_handle, **kwargs)
    213 
    214     def similarity(self, entity1, entity2):

/anaconda3/lib/python3.7/site-packages/gensim/utils.py in load(cls, fname, mmap)
    420         compress, subname = SaveLoad._adapt_by_suffix(fname)
    421 
--&gt; 422         obj = unpickle(fname)
    423         obj._load_specials(fname, mmap, compress, subname)
    424         logger.info(""loaded %s"", fname)

/anaconda3/lib/python3.7/site-packages/gensim/utils.py in unpickle(fname)
   1356 
   1357     """"""
-&gt; 1358     with smart_open(fname, 'rb') as f:
   1359         # Because of loading from S3 load can't be used (missing readline in smart_open)
   1360         if sys.version_info &gt; (3, 0):

/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py in smart_open(uri, mode, **kw)
    179         raise TypeError('mode should be a string')
    180 
--&gt; 181     fobj = _shortcut_open(uri, mode, **kw)
    182     if fobj is not None:
    183         return fobj

/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py in _shortcut_open(uri, mode, **kw)
    299     #
    300     if six.PY3:
--&gt; 301         return open(parsed_uri.uri_path, mode, buffering=buffering, **open_kwargs)
    302     elif not open_kwargs:
    303         return open(parsed_uri.uri_path, mode, buffering=buffering)

FileNotFoundError: [Errno 2] No such file or directory: 'model.wv'
</code></pre>

<p>Does anyone know the reason for this message? How can I know that I do have 'model.wv' file?</p>

<p>Thank you in advance!</p>
","python, model, gensim","<p>Change it from:  <code>wv = KeyedVectors.load(""model.wv"", mmap='r')</code></p>

<p>to: <code>wv = KeyedVectors.load(path, mmap='r')</code></p>

<p>You should be loading the file <code>'wordvectors.kv'</code></p>
",2,0,4522,2019-11-05 15:40:56,https://stackoverflow.com/questions/58714746/gensim-errno-2-no-such-file-or-directory-model-wv
What does the score indicate in topic modelling,"<p>I used gimsm for LSA as per this tutorial
<a href=""https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"" rel=""nofollow noreferrer"">https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python</a></p>

<p>and I got the following output after running it for a list of text</p>

<pre><code>
[(1, '-0.708*""London"" + 0.296*""like"" + 0.294*""go"" + 0.287*""dislike"" + 0.268*""great"" + 0.200*""romantic"" + 0.174*""stress"" + 0.099*""lovely"" + 0.082*""good"" + -0.075*""Tower"" + 0.072*""see"" + 0.063*""nice"" + 0.061*""amazing"" + -0.053*""Palace"" + 0.053*""walk"" + -0.050*""Eye"" + 0.046*""eat"" + -0.042*""Bridge"" + 0.041*""Garden"" + 0.040*""Covent"" + -0.040*""old"" + -0.039*""visit"" + 0.039*""really"" + 0.035*""spend"" + 0.034*""watch"" + 0.034*""get"" + -0.032*""Buckingham"" + 0.032*""Weather"" + -0.032*""Museum"" + -0.032*""Westminster""')]

</code></pre>

<p>What does -0.708 London indicate?</p>
","python, nlp, gensim, topic-modeling","<p>Those are the words mostly contributing to your topic, both positively and negatively. One of the characteristics of your topic seems to be, that it does not have anything to do with London. You can see that other ""London-related"" words also contribute negatively to your topic: Westminster, Tower and Eye are also negative for this topic.</p>

<p>So if a text lacks the word London, it is highly plausible that the text is about this topic, according to your model.</p>
",1,0,36,2019-11-06 12:30:19,https://stackoverflow.com/questions/58730230/what-does-the-score-indicate-in-topic-modelling
Gensim: Any chance to get word frequency in Word2Vec format?,"<p>I am doing my research with fasttext pre-trained model and I need word frequency to do further analysis. Does the .vec or .bin files provided on fasttext website contain the info of word frequency? if yes, how do I get?</p>

<p>I am using load_word2vec_format to load the model tried using model.wv.vocab[word].count, which only gives you the word frequency rank not the original word frequency.</p>
","python-3.6, gensim, fasttext","<p>I don't believe those formats include any word frequency information. </p>

<p>To the extent any pre-trained word-vectors declare what they were trained on – like, say, Wikipedia text – you could go back to the training corpus (or some reasonable approximation) to perform your own frequency-count. Even if you've only got a ""similar"" corpus, the frequencies might be ""close enough"" for your analytical need.</p>

<p>Similarly, you could potentially use the frequency-rank to synthesize a dummy frequency table, using <a href=""https://en.wikipedia.org/wiki/Zipf&#39;s_law"" rel=""nofollow noreferrer"">Zipf's Law</a>, which roughly holds for normal natural-language corpora. Again, the relative proportions between words might be roughly close enough to the real proportions for your need, even with real/precise frequencies as were used during word-vector training. </p>

<p>Synthesizing the version of the Zipf's law formula on the Wikipedia page that makes use of the Harmonic number (H) in the denominator, with the efficient approximation of H given in <a href=""https://stackoverflow.com/a/27683292/130288"">this answer</a>, we can create a function that, given a word's (starting at 1) rank and the total number of unique words, gives the proportionate frequency predicted by Zipf's law:</p>

<pre class=""lang-py prettyprint-override""><code>from numpy import euler_gamma
from scipy.special import digamma

def digamma_H(s):
    """""" If s is complex the result becomes complex. """"""
    return digamma(s + 1) + euler_gamma

def zipf_at(k_rank, N_total):
    return 1.0 / (k_rank * digamma_H(N_total))
</code></pre>

<p>Then, if you had a pretrained set of 1 million word-vectors, you could estimate the first word's frequency as:</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; zipf_at(1, 1000000)
0.06947953777315177
</code></pre>
",3,2,1001,2019-11-06 17:28:04,https://stackoverflow.com/questions/58735585/gensim-any-chance-to-get-word-frequency-in-word2vec-format
&#39;similar_by_word&#39; did not improve over iterations,"<p>I'm using Gensim to train a skip-gram word2vec model. The dataset has 1 million sentences, but the vocabulary is of size 200. I would like to see the model accuracy over iterations, so I used <code>model.wv.similar_by_word</code> in the callback function to see the scores. But the returned values were not updated over iterations.</p>

<p>The <code>iter</code> was set to be <code>100</code>.
I tried to change the values of <code>window</code> and <code>size</code>, but it has no effect.</p>

<p>The model was initialized with callbacks:</p>

<pre class=""lang-py prettyprint-override""><code>Word2Vec(self.train_corpus, workers=multiprocessing.cpu_count(), compute_loss=True, callbacks=[A_CallBack], **word2vec_params)
</code></pre>

<p>In the class <code>A_CallBack</code>, I have something like this:</p>

<pre class=""lang-py prettyprint-override""><code>def on_epoch_end(self, model):
    word, score = model.wv.similar_by_word(word='target_word', topn=1)[0]
    print(word, score)
</code></pre>

<p>The <code>word</code> and <code>score</code> were printed out for every epoch, but the values have never changed.</p>

<p>I was expecting the values of them to be updated over iterations, which should make sense?</p>

<p>I'm new to machine learning and word2vec. Thanks a lot for the help.</p>
","python, gensim, word2vec","<p>The various <code>gensim</code> similarity functions are optimized via the pre-calculation of unit-length normed vectors, and that pre-calculation is cached in a way that doesn't expect further training to happen. </p>

<p>As a result, when you first check similarities mid-training, as you've done with your callback code, the cache gets filled with the model's early state – and not refreshed after later training. There's a <a href=""https://github.com/RaRe-Technologies/gensim/issues/2260"" rel=""nofollow noreferrer"">pending bug</a> (as of <code>gensim-3.8.1</code> in November 2019) to fix this behavior, in the meantime, you can either:</p>

<ul>
<li>refrain from checking similarity-operations until after training is done, or</li>
<li>manually clear some of the caches after you've done more training. For a plain <code>Word2Vec</code> model, it should be enough to do: <code>model.wv.vectors_norm = None</code>. (Some other models require extra steps, see the bug discussion for more details.)</li>
</ul>
",0,0,349,2019-11-07 17:50:21,https://stackoverflow.com/questions/58754450/similar-by-word-did-not-improve-over-iterations
How to store gensim&#39;s KeyedVectors object in a global variable inside a Redis Queue worker,"<p>I'm trying to store data in a global variable inside a Redis Queue (RQ) worker so that this data remains pre-loaded, i.e. it doesn't need to be loaded for every RQ job.</p>

<p>Specifically, I'm working with Word2Vec vectors and loading them using gensim's KeyedVectors.</p>

<p>My app is in Python Flask, running on a Linux server, containerized using Docker.</p>

<p>My goal is to reduce processing time by keeping a handful of large vectors files loaded in memory at all times. </p>

<p>I first tried storing them in global variables in Flask, but then <em>each</em> of my 8 gunicorn workers loads the vectors, which eats up a lot of RAM.</p>

<p>I only need <em>one</em> worker to store a particular vectors file.</p>

<p>I've been told that one solution is to have a set number of RQ workers holding the vectors in a global variable, so that I can control which workers get which vectors files loaded in.</p>

<p>Here is what I have so far:</p>

<p><em>RQ_worker.py</em></p>

<pre><code>from rq import Worker, Connection
from gensim.models.keyedvectors import KeyedVectors
from my_common_methods import get_redis

W2V = KeyedVectors.load_word2vec_format('some_path/vectors.bin', binary=True)

def rq_task(some_args):
    # use some_args and W2V to do some processing, e.g.:
    with open(some_args_filename, 'w') as f_out:
        f_out.write(str(W2V['word']))

if __name__ == '__main__':
    with Connection(get_redis()):
        worker = Worker(['default'])
        worker.work()
</code></pre>

<p><em>app.py</em></p>

<pre><code>from rq import Queue, Connection
from RQ_worker import rq_task

@app.route(""/someroute"", methods=['POST'])
def some_route():
    # test Redis Queue
    with Connection(get_redis()):
        q = Queue()
        task = q.enqueue(rq_task, some_args)
</code></pre>

<p><em>docker-stack.yml</em></p>

<pre><code>version: '3.7'

services:
  nginx:
    image: nginx:mainline-alpine
    deploy: ...
    configs: ...
    networks: ...

  flask:
    image: ...
    deploy: ...
    environment: ...
    networks: ...
    volumes: ...

  worker:
    image: ...
    command: python2.7 RQ_worker.py
    deploy:
      replicas: 1
    networks: ...
    volumes:
      - /some_path/data:/some_path/data

configs:
  nginx.conf:
    external: true
    name: nginx.conf

networks:
  external:
    external: true
  database:
    external: true
</code></pre>

<p>(I redacted a bunch of stuff from Docker, but can provide more details, if relevant.)</p>

<p>The above generally works, <strong>except</strong> that the RQ worker seems to load W2V <strong>from scratch</strong> each time it gets a new job, which defeats the whole purpose. It should keep the vectors stored in W2V as a global variable, so they don't need to be reloaded each time.</p>

<p>Am I missing something? Should I set it up differently? </p>

<p>I've been told that it might be possible to use mmap to load the vectors file into a global variable that the RQ worker sits on, but I'm not sure how that would work with KeyedVectors.</p>

<p>Any advice would be much appreciated!</p>
","docker, flask, redis, gensim, word2vec","<p>If you use <code>load_word2vec_format()</code>, the code will always be parsing the (not-native-to-gensim-or-Python) word-vectors format, and allocating new objects/memory to store the results. </p>

<p>You can instead use gensim's native <code>.save()</code> to store in a friendlier format for later native <code>.load()</code> operations. Large arrays of vectors will be stored in separate, memory-map ready files. Then, when you <code>.load(..., mmap='r')</code> those files, even multiple times from different threads or processes within the same container, they'll share the same RAM. </p>

<p>(Note that this doesn't even require any shared globals. The OS will notice that each process is requesting the same read-only memory-mapped file, and automatically share those RAM pages. The only duplication will be redundant Python <code>dict</code>s helping each separate <code>.load()</code> know indexes into the shared-array.)</p>

<p>There are some extra wrinkles to consider when doing similarity-operations on vectors that the model will want to repeatedly unit-norm - see this older answer for more details on how to work-around that:</p>

<p><a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">How to speed up Gensim Word2vec model load time?</a></p>

<p>(Note that <code>syn0</code> and <code>syn0_norm</code> have been renamed <code>vectors</code> and <code>vectors_norm</code> in more-recent <code>gensim</code> versions, but the old names might still work with deprecation warnings for a while still.)</p>
",0,2,681,2019-11-11 07:24:27,https://stackoverflow.com/questions/58797101/how-to-store-gensims-keyedvectors-object-in-a-global-variable-inside-a-redis-qu
Why does gensim ignore underscores during preprocessing?,"<p>Going through the gensim source, I noticed the <code>simple_preprocess</code> utility function clears all punctuations except those with words starting with an underscore, <code>_</code>. Is there a reason for this?</p>

<pre class=""lang-py prettyprint-override""><code>def simple_preprocess(doc, deacc=False, min_len=2, max_len=15):
    tokens = [
        token for token in tokenize(doc, lower=True, deacc=deacc, errors='ignore')
        if min_len &lt;= len(token) &lt;= max_len and not token.startswith('_')
    ]
    return tokens

</code></pre>
","nltk, gensim","<p>The underscore (<code>'_'</code>) isn't typically meaningful punctuation, but is often considered a ""word"" character in programming and text-processing. </p>

<p>For example, common regular-expression syntax uses <code>\w</code> to indicate a ""word character"". Per <a href=""https://www.regular-expressions.info/shorthand.html"" rel=""nofollow noreferrer"">https://www.regular-expressions.info/shorthand.html</a> : </p>

<blockquote>
  <p><code>\w</code> stands for ""word character"". It always matches the ASCII characters
  <code>[A-Za-z0-9_]</code>. Notice the inclusion of the underscore and digits. In
  most flavors that support Unicode, <code>\w</code> includes many characters from
  other scripts. There is a lot of inconsistency about which characters
  are actually included. Letters and digits from alphabetic scripts and
  ideographs are generally included. Connector punctuation other than
  the underscore and numeric symbols that aren't digits may or may not
  be included. XML Schema and XPath even include all symbols in <code>\w</code>.
  Again, Java, JavaScript, and PCRE match only ASCII characters with <code>\w</code>.</p>
</blockquote>

<p>As such, it's often used in authoring, or in other text-preprocessing steps, to connect other groups of letters/numbers that <em>should</em> be kept together as a unit. Thus it's not often cleared with other true punctuation. </p>

<p>The code you've referenced also does something else, different than your question about clearing punctuation: it drops word-tokens beginning with <code>_</code>. </p>

<p>I'm not sure why it does that; at some point that code may have be designed with some specific text-format in mind where leading-underscore tokens were semantically-unimportant formatting directives. </p>

<p>The <code>simple_preprocess()</code> function in gensim is just a quick-and-dirty baseline helpful for internal tests and compact beginner tutorials. It shouldn't be considered a ""best practice"". </p>

<p>Real projects should give more consideration to the kind of word-tokenization that makes sense for their data and purposes – and either look to libraries with more options, or custom approaches (which still need not be more than a few lines of Python), to implement tokenization that best suits their needs.</p>
",0,0,260,2019-11-11 15:18:43,https://stackoverflow.com/questions/58804099/why-does-gensim-ignore-underscores-during-preprocessing
Type error when trying to create a doc2vec model in gensim,"<p>I am trying to train a Doc2Vec model using gensim.</p>

<p>The dataset i am using is the 20 newsgroups dataset [1] which is included in sklearn's datasets module.</p>

<p>I have used the example in the gensim documentation to create the model.</p>

<pre class=""lang-py prettyprint-override""><code>docs = newsgroups_train['data']
enumerated_docs = enumerate(docs)
documnets= [TaggedDocument(doc.split(),i) for i, doc in enumerated_docs]
model = Doc2Vec(documnets, vector_size=20, window=2, min_count=30, workers=4)
</code></pre>

<p>I checked every line of code, all seems to be working up to the line which initializes the model.</p>

<p>I  get a type error:
<code>TypeError: 'int' object is not iterable</code></p>

<p>[1] <a href=""https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html</a></p>
","python, gensim, doc2vec","<p><code>Enumerate</code> returns an integer counter and the value in the list. So, in your third line of code, <code>i</code> is an integer. However, the second parameter of <code>TaggedDocument</code> function should be an iterable.</p>
",1,0,256,2019-11-12 10:38:53,https://stackoverflow.com/questions/58816895/type-error-when-trying-to-create-a-doc2vec-model-in-gensim
Batch-train word2vec in gensim with support of multiple workers,"<p><strong>Context</strong></p>

<p>There exists severals questions about how to train <code>Word2Vec</code> using <code>gensim</code> with streamed data. Anyhow, these questions don't deal with the issue that streaming cannot use multiple workers since there is no array to split between threads.</p>

<p>Hence I wanted to create a generator providing such functionality for gensim. My results look like:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec as w2v

#The data is stored in a python-list and unsplitted.
#It's too much data to store it splitted, so I have to do the split while streaming.
data = ['this is document one', 'this is document two', ...]

#Now the generator-class
import threading

class dataGenerator:
    """"""
    Generator for batch-tokenization.
    """"""

    def __init__(self, data: list, batch_size:int = 40):
        """"""Initialize generator and pass data.""""""

        self.data = data
        self.batch_size = batch_size
        self.lock = threading.Lock()


    def __len__(self):
        """"""Get total number of batches.""""""
        return int(np.ceil(len(self.data) / float(self.batch_size)))


    def __iter__(self) -&gt; list([]):
        """"""
        Iterator-wrapper for generator-functionality (since generators cannot be used directly).
        Allows for data-streaming.
        """"""
        for idx in range(len(self)):
            yield self[idx]


    def __getitem__(self, idx):

        #Make multithreading thread-safe
        with self.lock:

            # Returns current batch by slicing data.
            return [arr.split("" "") for arr in self.data[idx * self.batch_size : (idx + 1) * self.batch_size]]


#And now do the training
model = w2v(
             sentences=dataGenerator(data),
             size=300,
             window=5,
             min_count=1,
             workers=4
            )
</code></pre>

<p>This results in the error </p>

<blockquote>
  <p>TypeError: unhashable type: 'list'</p>
</blockquote>

<p>Since <code>dataGenerator(data)</code> would work if I'd just yield a single splitted document, I assume that gensims <code>word2vec</code> wraps the generator within an extra list. In this case the <code>__iter__</code> would look like:</p>

<pre class=""lang-py prettyprint-override""><code>def __iter__(self) -&gt; list:
    """"""
    Iterator-wrapper for generator-functionality (since generators cannot be used directly.
    Allows for data-streaming.
    """"""
    for text in self.data:
        yield text.split("" "")
</code></pre>

<p>Hence, my batch would also be wrapped resulting in something like <code>[[['this', '...'], ['this', '...']], [[...], [...]]]</code> (=> list of list of list) which cannot be processed by gensim.</p>

<p><br>
<br>
<br>
<strong>My question:</strong></p>

<p><em>Can I ""stream""-pass batches in order to use multiple workers?
How can I change my code accordingly?</em></p>
","python, nlp, batch-processing, gensim, word2vec","<p>It seems I was too impatient. I ran the streaming-function written above which processes only one document instead of a batch:</p>

<pre class=""lang-py prettyprint-override""><code>def __iter__(self) -&gt; list:
    """"""
    Iterator-wrapper for generator-functionality (since generators cannot be used directly.
    Allows for data-streaming.
    """"""
    for text in self.data:
        yield text.split("" "")
</code></pre>

<p>After starting the <code>w2v</code>-function it took about ten minutes until all cores were working correctly. </p>

<p>It seems that building the vocabulary does not support multiple cores and, hence, only one was used for this task. Presumably, it took so long because auf the corpus-size. After gensim built the vocab, all cores were used for the training.</p>

<p>So if you are running in this issue as well, maybe some patience will already help :)</p>
",1,1,1485,2019-11-12 15:56:58,https://stackoverflow.com/questions/58822292/batch-train-word2vec-in-gensim-with-support-of-multiple-workers
How to do language representation on huge documents of 3000-4000 word for query-based retrieval?,"<p>I am trying to implement a semantic search to retrieve similar documents from a dataset of unstructured French documents.</p>

<ul>
<li>These documents are not categorized and are templates with 300 - 3000 words per document.</li>
<li>I am using doc2vec using gensim to find the paragraph embeddings with 300 dimensions and a window of 5 of the dataset.</li>
<li>I am then converting the search query which is a maximum of 5 words to the vector with 300 dimensions and comparing the cosine distance to find the document close to the search queries.</li>
</ul>

<p>I am not getting good results. Please suggest some strategies to do the semantic search. I was trying to reduce the number of words in my dataset by doing rake keyword extraction.</p>
","search, nlp, gensim, cosine-similarity, doc2vec","<p>The reason for your poor result if the queries are just too short to be embedded by  doc2vec. If you only care about performance, I would recommend using some off-the-shelf information retrieval tools like Lucene.</p>

<p>If you want to play with neural nets and embeddings, you can do the following:</p>

<ul>
<li><p>Just use word embedding, e.g., from <a href=""https://fasttext.cc/"" rel=""nofollow noreferrer"">FastText</a>. Remove stop words both in the query and the documents and represent them with the average word embedding and do the comparison by cosine distance.</p></li>
<li><p>If you don't care about efficiency a lot, you can also try multilingual BERT (available in the <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">Transformers</a> library) or brand new French model called <a href=""https://camembert-model.fr/"" rel=""nofollow noreferrer"">CamemBERT</a>. In this case, you would just take the <code>[cls]</code> vectors and do the cosine distance on them.</p></li>
</ul>
",0,0,438,2019-11-13 11:40:57,https://stackoverflow.com/questions/58836322/how-to-do-language-representation-on-huge-documents-of-3000-4000-word-for-query
Python connect composed keywords in texts,"<p>So, I have a keyword list lowercase. Let's say </p>

<pre><code>keywords = ['machine learning', 'data science', 'artificial intelligence']
</code></pre>

<p>and a list of texts in lowercase. Let's say</p>

<pre><code>texts = [
  'the new machine learning model built by google is revolutionary for the current state of artificial intelligence. it may change the way we are thinking', 
  'data science and artificial intelligence are two different fields, although they are interconnected. scientists from harvard are explaining it in a detailed presentation that could be found on our page.'
]
</code></pre>

<p>I need to transform the texts into:</p>

<pre><code>[[['the', 'new',
   'machine_learning',
   'model',
   'built',
   'by',
   'google',
   'is',
   'revolutionary',
   'for',
   'the',
   'current',
   'state',
   'of',
   'artificial_intelligence'],
  ['it', 'may', 'change', 'the', 'way', 'we', 'are', 'thinking']],
 [['data_science',
   'and',
   'artificial_intelligence',
   'are',
   'two',
   'different',
   'fields',
   'although',
   'they',
   'are',
   'interconnected'],
  ['scientists',
   'from',
   'harvard',
   'are',
   'explaining',
   'it',
   'in',
   'a',
   'detailed',
   'presentation',
   'that',
   'could',
   'be',
   'found',
   'on',
   'our',
   'page']]]
</code></pre>

<p>What I do right now is checking if the keywords are in a text and replace them with the keywords with _. But this is of complexity m*n and it is really slow when you have 700 long texts and 2M keywords as in my case.</p>

<p>I was trying to use Phraser, but I can't manage to build one with only my keywords.</p>

<p>Could someone suggest me a more optimized way of doing it?</p>
","python, data-science, gensim","<p>The <code>Phrases</code>/<code>Phraser</code> classes of <code>gensim</code> are designed to use their internal, statistically-derived records of what word pairs should be promoted to phrases – not user-supplied pairings. (You could probably poke &amp; prod a <code>Phraser</code> to do what you want, by synthesizing scores/thresholds, but that would be somewhat awkward &amp; kludgey.)</p>

<p>You could, mimic their general approach: (1) operate on lists-of-tokens rather than raw strings; (2) learn &amp; remember token-pairs that should be combined; &amp; (3) perform combination in a single pass. That should work far more efficiently than anything based on doing repeated search-and-replace on a string – which it sounds like you've already tried and found wanting. </p>

<p>For example, let's first create a dictionary, where the keys are tuples of word-pairs that should be combined, and the values are tuples that include both their designated combination-token, and a 2nd item that's just an empty-tuple. (The reason for this will become clear later.)</p>

<pre class=""lang-py prettyprint-override""><code>keywords = ['machine learning', 'data science', 'artificial intelligence']
texts = [
    'the new machine learning model built by google is revolutionary for the current state of artificial intelligence. it may change the way we are thinking', 
    'data science and artificial intelligence are two different fields, although they are interconnected. scientists from harvard are explaining it in a detailed presentation that could be found on our page.'
]

combinations_dict = {tuple(kwsplit):('_'.join(kwsplit), ()) 
                     for kwsplit in [kwstr.split() for kwstr in keywords]}
combinations_dict
</code></pre>

<p>After this step, <code>combinations_dict</code> is:</p>

<pre class=""lang-py prettyprint-override""><code>{('machine', 'learning'): ('machine_learning', ()),
 ('data', 'science'): ('data_science', ()),
 ('artificial', 'intelligence'): ('artificial_intelligence', ())}
</code></pre>

<p>Now, we can use a Python generator function to create an iterable transformation of any other sequence-of-tokens, that takes original tokens one-by-one – but before emitting any, adds the next to a buffered candidate pair-of-tokens. If that pair is one that should be combined, a single combined token is <code>yield</code>ed – but if not, just the 1st token is emitted, leaving the 2nd to be combined with the next token in a new candidate pair. </p>

<p>For example:</p>

<pre class=""lang-py prettyprint-override""><code>def combining_generator(tokens, comb_dict):
    buff = ()  # start with empty buffer
    for in_tok in tokens:
        buff += (in_tok,)  # add latest to buffer
        if len(buff) &lt; 2:  # grow buffer to 2 tokens if possible
            continue
        # lookup what to do for current pair... 
        # ...defaulting to emit-[0]-item, keep-[1]-item in new buff
        out_tok, buff = comb_dict.get(buff, (buff[0], (buff[1],)))
        yield out_tok 
    if buff:
        yield buff[0]  # last solo token if any
</code></pre>

<p>Here we see the reason for the earlier <code>()</code> empty-tuples: that's the preferred state of the <code>buff</code> after a successful replacement. And driving the result &amp; next-state this way helps us use the form of <code>dict.get(key, default)</code> that supplies a specific value to be used if the key isn't found. </p>

<p>Now designated combinations can be applied via:</p>

<pre><code>tokenized_texts = [text.split() for text in texts]
retokenized_texts = [list(combining_generator(tokens, combinations_dict)) for tokens in tokenized_texts]
retokenized_texts
</code></pre>

<p>...which reports <code>tokenized_texts</code> as:</p>

<pre class=""lang-py prettyprint-override""><code>[
  ['the', 'new', 'machine_learning', 'model', 'built', 'by', 'google', 'is', 'revolutionary', 'for', 'the', 'current', 'state', 'of', 'artificial', 'intelligence.', 'it', 'may', 'change', 'the', 'way', 'we', 'are', 'thinking'], 
  ['data_science', 'and', 'artificial_intelligence', 'are', 'two', 'different', 'fields,', 'although', 'they', 'are', 'interconnected.', 'scientists', 'from', 'harvard', 'are', 'explaining', 'it', 'in', 'a', 'detailed', 'presentation', 'that', 'could', 'be', 'found', 'on', 'our', 'page.']
]
</code></pre>

<p>Note that the tokens <code>('artificial', 'intelligence.')</code> <strong>aren't</strong> combined here, as the dirt-simple <code>.split()</code> tokenization used has left the punctuation attached, preventing an exact match to the rule. </p>

<p>Real projects will want to use a more-sophisticated tokenization, that might either strip the punctuation, or retain punctuation as tokens, or do other preprocessing - and as a result would properly pass <code>'artificial'</code> as a token without the attached <code>'.'</code>. For example a simple tokenization that just retains runs-of-word-characters discarding punctuation would be:</p>

<pre class=""lang-py prettyprint-override""><code>import re
tokenized_texts = [re.findall('\w+', text) for text in texts]
tokenized_texts
</code></pre>

<p>Another that also keeps any stray non-word/non-space characters (punctuation) as standalone tokens would be:</p>

<pre class=""lang-py prettyprint-override""><code>tokenized_texts = [re.findall(r'\w+|(?:[^\w\s])', text) for text in texts]
tokenized_texts
</code></pre>

<p>Either of these alternatives to a simple <code>.split()</code> would ensure your 1st text presents the necessary <code>('artificial', 'intelligence')</code> pair for combination.</p>
",4,0,283,2019-11-13 14:12:14,https://stackoverflow.com/questions/58839049/python-connect-composed-keywords-in-texts
"How to export a fasttext model created by gensim, to a binary file?","<p>I'm trying to export the fasttext model created by gensim to a binary file. But the docs are unclear about how to achieve this. 
What I've done so far: </p>

<pre><code>model.wv.save_word2vec_format('model.bin')
</code></pre>

<p>But this does not seems like the best solution. Since later when I want to load the model using the :</p>

<pre><code>fasttext.load_facebook_model('model.bin')
</code></pre>

<p>I get into an infinite loop. While loading the <code>fasttext.model</code> created by <code>model.save('fasttext.model)</code> function gets completed in around 30 seconds.</p>
","python, nlp, gensim, fasttext","<p>Using <code>.save_word2vec_format()</code> saves just the full-word vectors, to a simple format that was used by Google's original <code>word2vec.c</code> release. It doesn't save unique things about a full FastText model. Such files would be reloaded with the matched <code>.load_word2vec_format()</code>.</p>

<p>The <code>.load_facebook_format()</code> method loads files in the format saved by Facebook's original (non-Python) FastText code release. (The name of this method is pretty misguided, since 'facebook' could mean so many different things other than a specific data format.) Gensim doesn't have a matched method for saving to this same format – though it probably wouldn't be very hard to implement, and would make symmetric sense to support this export option. </p>

<p>Gensim's models typically implement gensim-native  <code>.save()</code> and <code>.load()</code> options, which make use of a mix of Python 'pickle' serialization and raw large-array files. These are your best options if you want to save the full model state, for later reloading back into Gensim. </p>

<p>(Such files can't be loaded by other FastText implementations.) </p>

<p>Be sure to keep the multiple related files written by this <code>.save()</code> (all with the same user-supplied prefix) together when moving the saved model to a new location. </p>

<p><strong>Update (May 2020):</strong> Recent versions of <code>gensim</code> such as 3.8.3 and later include a new contributed <code>FastText.save_facebook_model()</code> method which saves to the original Facebook FastTExt binary format.</p>
",6,5,5530,2019-11-15 12:01:11,https://stackoverflow.com/questions/58876630/how-to-export-a-fasttext-model-created-by-gensim-to-a-binary-file
How to find the most similar words from a set of input words by CBOW (GenSim)?,"<p>I use GenSim and CBOW for training the corpus. How can I get the most similar words from a set of input words?</p>

<p>For example:
Given a set of input words: [""David"", ""Mary"", ""married""]. Can I infer some output words like: ""wedding"", ""husband"", ""wife"", ""couple"", etc?</p>
","gensim, word2vec","<p>You can use the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar"" rel=""nofollow noreferrer"">wv.most_similar</a> method of your model.</p>
",2,0,1668,2019-11-16 01:04:05,https://stackoverflow.com/questions/58886579/how-to-find-the-most-similar-words-from-a-set-of-input-words-by-cbow-gensim
"How do you save a model, dictionary and corpus to disk in Gensim, and then load them again?","<p>In Gensim's <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html"" rel=""noreferrer"">documentation</a>, it says:</p>

<blockquote>
  <p>You can save trained models to disk and later load them back, either to continue training on new training documents or to transform new documents.</p>
</blockquote>

<p>I would like to do this with a dictionary, corpus and tf.idf model. However, the documentation seems to say that it is possible, without explaining how to save these things and load them back up again.</p>

<p>How do you do this?</p>

<hr>

<p>I've been using Pickle, but don't know if this is right...</p>

<pre><code>import pickle
pickle.dump(tfidf, open(""tfidf.p"", ""wb""))
tfidf_reloaded = pickle.load(open(""tfidf.p"", ""rb""))
</code></pre>
","python, nlp, gensim","<p>In general, you can save things with generic Python <code>pickle</code>, but most <code>gensim</code> models support their own native <code>.save()</code> method. </p>

<p>It takes a target filesystem path, and will save the model more efficiently than <code>pickle()</code> – often by placing large component arrays in separate files, alongside the main file. (When you later move the saved model, keep all these files with the same root name together.)</p>

<p>In particular, some models which have multi-gigabyte subcomponents may not save at all with <code>pickle()</code> – but <code>gensim</code>'s native <code>.save()</code> will work. </p>

<p>Models saved with <code>.save()</code> can typically be loaded by using the appropriate class's <code>.load()</code> method. (For example if you've saved a instance of <code>gensim.corpora.dictionary.Dictionary</code>, you'd load it with <code>gensim.corpora.dictionary.Dictionary.load(filepath)</code>. </p>
",6,8,7953,2019-11-20 19:30:20,https://stackoverflow.com/questions/58961983/how-do-you-save-a-model-dictionary-and-corpus-to-disk-in-gensim-and-then-load
Text representations : How to differentiate between strings of similar topic but opposite polarities?,"<p>I have been doing clustering of a certain corpus, and obtaining results that group sentences together by obtaining their <em>tf-idf</em>, checking similarity weights > a certain threshold value from the gensim model. </p>

<pre class=""lang-py prettyprint-override""><code>tfidf_dic = DocSim.get_tf_idf()
ds = DocSim(model,stopwords=stopwords, tfidf_dict=tfidf_dic)
sim_scores = ds.calculate_similarity(source_doc, target_docs)
</code></pre>

<p>The problem is that despite putting high threshold values, sentences of similar topics but <strong>opposite polarities</strong> get clustered together as such:</p>

<p><img src=""https://i.sstatic.net/K1uIn.png"" alt=""cluster results.""></p>

<blockquote>
  <p>Here is an example of the similarity weights obtained between ""don't like it"" &amp; ""i like it""</p>
</blockquote>

<p><img src=""https://i.sstatic.net/8sFvK.png"" alt=""similarity results."">
Are there any other methods, libraries or alternative models that can differentiate the polarities effectively by assigning them very low similarities or opposite vectors?</p>

<p>This is so that the outputs ""i like it"" and ""dont like it"" are in separate clusters.</p>

<p>PS: Pardon me if there are any conceptual errors as I am rather new to NLP. Thank you in advance!</p>
","nlp, cluster-analysis, gensim, similarity","<p>The problem is in how you represent your documents. Tf-idf is good for representing long documents where keywords play a more important role. Here, it is probably the <em>idf</em> part of tf-idf that disregards the polarity because negative particles like ""no"" or ""not"" will appear in most documents and they will always receive a low weight.</p>

<p>I would recommend trying some neural embeddings that might capture the polarity. If you want to keep using Gensim, you can try <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">doc2vec</a> but you would need quite a lot of training data for that. If you don't have much data to estimate the representation, I would use some pre-trained embeddings.</p>

<p>Even averaging word embeddings (you can load <a href=""https://fasttext.cc/"" rel=""nofollow noreferrer"">FastText</a> embeddings <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">in Gensim</a>). Alternatively, if you want a stronger model, you can try BERT or another large pre-trained model from the <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">Transformers package</a>.</p>
",1,0,136,2019-11-22 02:38:46,https://stackoverflow.com/questions/58986684/text-representations-how-to-differentiate-between-strings-of-similar-topic-but
How to map topic to a document after topic modeling is done with LDA?,"<p>Is there any way I can map generated topic from LDA to the list of documents and identify to which topic it belongs to ? I am interested in clustering documents using unsupervised learning and segregating it into appropriate cluster. </p>

<p>Example, I have 10 topics after running LDA model with the best hyperparameter. So, it should return a number of Topic is already defined withe pre-trained LDA model with new sentence or document that user input. </p>

<p>I am waiting you guys good solution. :)</p>

<p>Ps. I am using Gensim for NLP.</p>
","nlp, gensim, lda","<p>Using Quanteda You can achieve this as follows</p>

<pre><code>dtm &lt;- convert(dfmat_news, to = ""topicmodels"")
lda &lt;- LDA(dtm, k = 10). #10 topics in this case
</code></pre>

<p>Then you can obtain the most likely topics using the command topics() and save them as a document-level variable.</p>

<pre><code>docvars(dfmat_news, 'topic') &lt;- topics(lda)
head(topics(lda), 20)    
</code></pre>

<p>here the tutorial : <a href=""https://tutorials.quanteda.io/machine-learning/topicmodel/"" rel=""nofollow noreferrer"">https://tutorials.quanteda.io/machine-learning/topicmodel/</a></p>

<p>hope it is clear and useful :)</p>
",-1,0,875,2019-11-22 18:01:00,https://stackoverflow.com/questions/58999509/how-to-map-topic-to-a-document-after-topic-modeling-is-done-with-lda
How does Gensim implement subsampling in Word2Vec?,"<p>I am trying to reimplement wor2vec in pytorch. I implemented subsamping according to the <a href=""https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L407"" rel=""nofollow noreferrer"">code</a> of the original paper. However, I am trying to understand how subsampling is implemented in Gensim. I looked at the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"" rel=""nofollow noreferrer"">source code</a>, but I did not manage to grasp how it reconnects to the original paper.</p>

<p>Thanks a lot in advance.</p>
","gensim, word2vec, subsampling","<p>The key line is:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/e391f0c25599c751e127dde925e062c7132e4737/gensim/models/word2vec_inner.pyx#L543"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/e391f0c25599c751e127dde925e062c7132e4737/gensim/models/word2vec_inner.pyx#L543</a></p>

<pre class=""lang-py prettyprint-override""><code>    if c.sample and word.sample_int &lt; random_int32(&amp;c.next_random):
        continue
</code></pre>

<p>If <code>c.sample</code> tests if frequent-word downsampling is enabled at all (any non-zero value).</p>

<p>The <code>word.sample_int</code> is a value, per vocabulary word, that was precalculated during the vocabulary-discovery phase. It's essentially the 0.0-to-1.0 probability that a word should be kept, but scaled to the range 0-to-(2^32-1). </p>

<p>Most words, that are never down-sampled, simply have the value (2^32-1) there - so no matter what random int was just generated, that random int is less than the threshold, and the word is retained.</p>

<p>The few most-frequent words have other scaled values there, and thus sometimes the random int generated is larger than their <code>sample_int</code>. Thus, that word is, in that one training-cycle, skipped via the <code>continue</code> to the next word in the sentence. (That one word doesn't get made part of <code>effective_words</code>, this one time.)</p>

<p>You can see the original assignment &amp; precalculation of the <code>.sample_int</code> values, per unique vocabulary word, at and around:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/e391f0c25599c751e127dde925e062c7132e4737/gensim/models/word2vec.py#L1544"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/e391f0c25599c751e127dde925e062c7132e4737/gensim/models/word2vec.py#L1544</a></p>
",2,0,640,2019-11-23 16:19:38,https://stackoverflow.com/questions/59009670/how-does-gensim-implement-subsampling-in-word2vec
Error in Computing the Coherence Score – AttributeError: &#39;dict&#39; object has no attribute &#39;id2token&#39;,"<p>I am a beginner in NLP and it's my first time to do Topic Modeling. I was able to generate my model however I cannot produce the coherence metric.</p>

<p>Converting the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus</p>

<pre><code>sparse_counts = scipy.sparse.csr_matrix(data_dtm)
corpus = matutils.Sparse2Corpus(sparse_counts)
corpus
</code></pre>

<p><a href=""https://i.sstatic.net/EU6kl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EU6kl.png"" alt=""enter image description here""></a></p>

<pre><code>df_lemmatized.head()
</code></pre>

<p><a href=""https://i.sstatic.net/NUk6h.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NUk6h.png"" alt=""enter image description here""></a></p>

<pre><code># Gensim also requires dictionary of the all terms and their respective location in the term-document matrix
tfidfv = pickle.load(open(""tfidf.pkl"", ""rb""))
id2word = dict((v, k) for k, v in tfidfv.vocabulary_.items())
id2word
</code></pre>

<p><a href=""https://i.sstatic.net/vN9Dm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vN9Dm.png"" alt=""enter image description here""></a></p>

<p>This is my model:</p>

<pre><code>lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=15, passes=10, random_state=43)
lda.print_topics()
</code></pre>

<p><a href=""https://i.sstatic.net/VXdpA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VXdpA.png"" alt=""enter image description here""></a></p>

<p>And finally, here is where I attempted to get Coherence Score Using Coherence Model:</p>

<pre><code># Compute Perplexity
print('\nPerplexity: ', lda.log_perplexity(corpus))  

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda, texts=df_lemmatized.long_title, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
</code></pre>

<p>This is the error:</p>

<p><em>---> 57     if not dictionary.id2token:  # may not be initialized in the standard gensim.corpora.Dictionary
     58         setattr(dictionary, 'id2token', {v: k for k, v in dictionary.token2id.items()})
     59 
AttributeError: 'dict' object has no attribute 'id2token'</em></p>
","python, scipy, nlp, gensim, topic-modeling","<p>I don't have your data, so I can't reproduce the error. So, I will take a guess! The problem is within your <code>id2word</code>, it should be a <code>corpora.dictionary.Dictionary</code> not just <code>dict</code>. So, you need to do the following:</p>

<pre><code>&gt;&gt;&gt; from gensim import corpora
&gt;&gt;&gt;
&gt;&gt;&gt; word2id = dict((k, v) for k, v in tfidfv.vocabulary_.items())
&gt;&gt;&gt; d = corpora.Dictionary()
&gt;&gt;&gt; d.id2token = id2word
&gt;&gt;&gt; d.token2id = word2id
&gt;&gt;&gt; #...
&gt;&gt;&gt; # change `id2word` to `d`
&gt;&gt;&gt; coherence_model_lda = CoherenceModel(model=lda, texts=df_lemmatized.long_title, dictionary=d, coherence='c_v')
</code></pre>

<p>And I think it should work just fine now!</p>
",4,2,2717,2019-11-25 01:48:29,https://stackoverflow.com/questions/59024220/error-in-computing-the-coherence-score-attributeerror-dict-object-has-no-at
MemoryError: unable to allocate array with shape and data type float32 while using word2vec in python,"<p>I am trying to train the word2vec model from Wikipedia text data, for that I am using following code.</p>

<pre><code>import logging
import os.path
import sys
import multiprocessing

from gensim.corpora import  WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence


if __name__ == '__main__':
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info(""running %s"" % ' '.join(sys.argv))

    # check and process input arguments

    if len(sys.argv) &lt; 3:
        print (globals()['__doc__'])
        sys.exit(1)
    inp, outp = sys.argv[1:3]

    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count())

    # trim unneeded model memory = use (much) less RAM
    model.init_sims(replace=True)

    model.save(outp)
</code></pre>

<p>But after 20 minutes of program running, I am getting following error</p>

<p><a href=""https://i.sstatic.net/6BjOz.png"" rel=""nofollow noreferrer"">Error message</a></p>
","python, multiprocessing, python-multiprocessing, gensim, word2vec","<p>Ideally, you should paste the <em>text</em> of your error into your question, rather than a screenshot. However, I see the two key lines:</p>

<pre><code>&lt;TIMESTAMP&gt; : INFO : estimated required memory for 2372206 words and 400 dimensions: 8777162200 bytes
...
MemoryError: unable to allocate array with shape (2372206, 400) and data type float32
</code></pre>

<p>After making one pass over your corpus, the model has learned how many unique words will survive, which reports how large of a model must be allocated: one taking about <code>8777162200 bytes</code> (about 8.8GB). But, when trying to allocate the required vector array, you're getting a <code>MemoryError</code>, which indicates not enough computer addressable-memory (RAM) is available. </p>

<p>You can either:</p>

<ol>
<li>run where there's more memory, perhaps by adding RAM to your existing system; or</li>
<li>reduce the amount of memory required, chiefly by reducing either the number of unique word-vectors you'd like to train, or their dimensional size.</li>
</ol>

<p>You could reduce the number of words by increasing the default <code>min_count=5</code> parameter to something like <code>min_count=10</code> or <code>min_count=20</code> or <code>min_count=50</code>. (You probably don't need over 2 million word-vectors – many interesting results are possible with just a vocabulary of a few tens-of-thousands of words.) </p>

<p>You could also set a <code>max_final_vocab</code> value, to specify an exact number of unique words to keep. For example, <code>max_final_vocab=500000</code> would keep just the 500000 most-frequent words, ignoring the rest. </p>

<p>Reducing the <code>size</code> will also save memory. A setting of <code>size=300</code> is popular for word-vectors, and would reduce the memory requirements by a quarter.</p>

<p>Together, using <code>size=300, max_final_vocab=500000</code> should trim the required memory to under 2GB.</p>
",6,4,47193,2019-11-26 12:07:13,https://stackoverflow.com/questions/59050644/memoryerror-unable-to-allocate-array-with-shape-and-data-type-float32-while-usi
Can I optimize this Word Mover&#39;s Distance look-up function?,"<p>I am trying to measure the Word Mover's Distance between a lot of texts using Gensim's Word2Vec tools in Python. I am comparing each text with all other texts, so I first use itertools to create pairwise combinations like <code>[1,2,3] -&gt; [(1,2), (1,3), (2,3)]</code>. For memory's sake, I don't do the combinations by having all texts repeated in a big dataframe, but instead make a reference dataframe <code>combinations</code> with indices of the texts, which looks like:</p>

<pre><code>    0   1
0   0   1
1   0   2
2   0   3
</code></pre>

<p>And then in the comparison function I use these indices to look up the text in the original dataframe. The solution works fine, but I am wondering whether I would be able to it with big datasets. For instance I have a 300.000 row dataset of texts, which gives me about a 100 year's worth of computation on my laptop:</p>

<pre><code>C2​(300000) = 300000​! / (2!(300000−2))!
           = 300000⋅299999​ / 2 * 1
           = 44999850000 combinations
</code></pre>

<p>Is there any way this could be optimized better?</p>

<p>My code right now:</p>

<pre><code>import multiprocessing
import itertools
import numpy as np
import pandas as pd
import dask.dataframe as dd
from dask.diagnostics import ProgressBar
from gensim.models.word2vec import Word2Vec
from gensim.corpora.wikicorpus import WikiCorpus

def get_distance(row):
    try: 
        sent1 = df.loc[row[0], 'text'].split()
        sent2 = df.loc[row[1], 'text'].split()
        return model.wv.wmdistance(sent1, sent2)  # Compute WMD
    except Exception as e:
        return np.nan

df = pd.read_csv('data.csv')

# I then set up the gensim model, let me know if you need that bit of code too.

# Make pairwise combination of all indices
combinations = pd.DataFrame(itertools.combinations(df.index, 2))

# To dask df and apply function
dcombinations = dd.from_pandas(combinations, npartitions= 2 * multiprocessing.cpu_count())
dcombinations['distance'] = dcombinations.apply(get_distance, axis=1)
with ProgressBar():
    combinations = dcombinations.compute()
</code></pre>
","python, python-multiprocessing, dask, gensim, wmd","<p>You might use <a href=""https://github.com/src-d/wmd-relax"" rel=""nofollow noreferrer"">wmd-relax</a> for performance improvement. However, you'll first have to convert your model to spaCy and use the SimilarityHook as described on their webpage:</p>
<pre><code>import spacy
import wmd

nlp = spacy.load('en_core_web_md')
nlp.add_pipe(wmd.WMD.SpacySimilarityHook(nlp), last=True)
doc1 = nlp(&quot;Politician speaks to the media in Illinois.&quot;)
doc2 = nlp(&quot;The president greets the press in Chicago.&quot;)
print(doc1.similarity(doc2))
</code></pre>
",4,1,203,2019-12-10 13:12:54,https://stackoverflow.com/questions/59268044/can-i-optimize-this-word-movers-distance-look-up-function
gensim lemmatize error generator raised StopIteration,"<p>I'm trying to execute simple code to lemmatize string, but there's an error about iteration.
I have found some solutions which are about reinstalling web.py, but this not worked for me.</p>

<p>python code</p>

<pre><code>from gensim.utils import lemmatize
lemmatize(""gone"")
</code></pre>

<p>error is</p>

<pre><code>---------------------------------------------------------------------------
StopIteration                             Traceback (most recent call last)
I:\Anaconda\lib\site-packages\pattern\text\__init__.py in _read(path, encoding, comment)
    608             yield line
--&gt; 609     raise StopIteration
    610 

StopIteration: 

The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-4-9daceee1900f&gt; in &lt;module&gt;
      1 from gensim.utils import lemmatize
----&gt; 2 lemmatize(""gone"")

-------------------------------------------------------------------------------------

I:\Anaconda\lib\site-packages\pattern\text\__init__.py in &lt;genexpr&gt;(.0)
    623     def load(self):
    624         # Arnold NNP x
--&gt; 625         dict.update(self, (x.split("" "")[:2] for x in _read(self._path) if len(x.split("" "")) &gt; 1))
    626 
    627 #--- FREQUENCY -------------------------------------------------------------------------------------

RuntimeError: generator raised StopIteration
</code></pre>
","python, nlp, gensim, lemmatization","<p>The error message is misleading – it occurs when there's nothing to properly lemmatize.</p>

<p>By default, <code>lemmatize()</code> only accepts word tags <code>NN|VB|JJ|RB</code>. Pass in a regexp that matches any string to change this:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; lemmatize(""gone"", allowed_tags=re.compile('.*'))
[b'go/VB']
</code></pre>
",0,0,848,2019-12-11 08:17:32,https://stackoverflow.com/questions/59281409/gensim-lemmatize-error-generator-raised-stopiteration
Memory efficiently loading of pretrained word embeddings from fasttext library with gensim,"<p>I would like to load pretrained multilingual word embeddings from the fasttext library with gensim; here the link to the embeddings:</p>

<p><a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>

<p>In particular, I would like to load the following word embeddings: </p>

<ul>
<li>cc.de.300.vec (4.4 GB) </li>
<li>cc.de.300.bin (7 GB)</li>
</ul>

<p>Gensim offers the following two options for loading fasttext files:</p>

<ol>
<li><p><code>gensim.models.fasttext.load_facebook_model(path, encoding='utf-8')</code>    </p>

<blockquote>
  <ul>
  <li><em>Load the input-hidden weight matrix from Facebook’s native fasttext
  .bin output file.</em></li>
  <li><em>load_facebook_model() loads the full model, not just
  word embeddings, and enables you to continue model training.</em></li>
  </ul>
</blockquote></li>
<li><p><code>gensim.models.fasttext.load_facebook_vectors(path, encoding='utf-8')</code></p>

<blockquote>
  <ul>
  <li><em>Load word embeddings from a model saved in Facebook’s native fasttext .bin format.</em></li>
  <li><em>load_facebook_vectors() loads the word embeddings only. Its faster, but does not enable you to continue training.</em></li>
  </ul>
</blockquote></li>
</ol>

<p>Source Gensim documentation: 
<a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model</a></p>

<p>Since my laptop has only 8 GB RAM, I am continuing to get MemoryErrors or the loading takes a very long time (up to several minutes).</p>

<p>Is there an option to load these large models from disk more memory efficient?</p>
","python, nlp, gensim, word-embedding, fasttext","<p>As vectors will typically take at least as much addressable-memory as their on-disk storage, it will be challenging to load fully-functional versions of those vectors into a machine with only 8GB RAM. In particular:</p>

<ul>
<li><p>once you start doing the most common operation on such vectors – finding lists of the <code>most_similar()</code> words to a target word/vector – the gensim implementation will also want to cache a set of the word-vectors that's been normalized to unit-length – which nearly doubles the required memory</p></li>
<li><p>current versions of gensim's FastText support (through at least 3.8.1) also waste a bit of memory on some unnecessary allocations (especially in the full-model case)</p></li>
</ul>

<p>If you'll only be using the vectors, not doing further training, you'll definitely want to use only the <code>load_facebook_vectors()</code> option. </p>

<p>If you're willing to give up the model's ability to synthesize new vectors for out-of-vocabulary words, not seen during training, then you could choose to load just a subset of the full-word vectors from the plain-text <code>.vec</code> file. For example, to load just the 1st 500K vectors:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
KeyedVectors.load_word2vec_format('cc.de.300.vec', limit=500000)
</code></pre>

<p>Because such vectors are typically sorted to put the more-frequently-occurring words first, often discarding the long tail of low-frequency words isn't a big loss. </p>
",6,3,5905,2019-12-11 09:29:46,https://stackoverflow.com/questions/59282572/memory-efficiently-loading-of-pretrained-word-embeddings-from-fasttext-library-w
"Word2Vec - How to rid of &quot;TypeError: unhashable type: &#39;list&#39;&quot; and &quot;AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found&quot;?","<p>Getting <code>TypeError: unhashable type: 'list'</code> and <code>AttributeError: dlsym(0x7fa8c57be020, AttachDebuggerTracing): symbol not found</code> errors when I create my model based on <code>Word2Vec</code> implementation of the <code>gensim</code> module.</p>

<p><strong>Each entry has three parts</strong> which are presented within a list. And, <strong>the model contains three entries</strong> for the sake of demonstration.</p>

<p>Here is what I have tried:</p>

<pre><code>model = Word2Vec(sentences=features, size=100, sg=1, window=3, min_count=1, iter=10, workers=Pool()._processes)

model.build_vocab(features)

model.train(features)
</code></pre>

<p>The value of the <code>features</code> is: </p>

<pre><code>  [
    [
      ['permission.ACCESS_WIFI_STATE', 'permission.ACCESS_NETWORK_STATE', 'permission.READ_PHONE_STATE', 'permission.INTERNET', 'permission.CHANGE_WIFI_STATE'],
       ['intent.action.MAIN', 'intent.action.BATTERY_CHANGED_ACTION', 'intent.action.SIG_STR', 'intent.action.BOOT_COMPLETED'],
       []
    ],
    [
      ['permission.WRITE_EXTERNAL_STORAGE', 'permission.ACCESS_NETWORK_STATE', 'permission.READ_PHONE_STATE', 'permission.INTERNET', 'permission.INSTALL_PACKAGES', 'permission.SEND_SMS', 'permission.DELETE_PACKAGES'],
      ['intent.action.BOOT_COMPLETED', 'intent.action.USER_PRESENT', 'intent.action.PHONE_STATE', 'intent.action.MAIN'],
      []
    ], 
    [
      ['permission.WRITE_EXTERNAL_STORAGE', 'permission.ACCESS_FINE_LOCATION', 'permission.INTERNET', 'permission.READ_PHONE_STATE', 'permission.ACCESS_COARSE_LOCATION', 'permission.CALL_PHONE', 'permission.READ_CONTACTS', 'permission.READ_SMS'], 
      ['intent.action.PHONE_STATE', 'intent.action.MAIN'], 
      []
    ]
  ]
</code></pre>

<p><strong>Edit:</strong> The error stack trace after correcting the form of the feature vector according to the comment of @gojomo.</p>

<pre><code>Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 3 more threads
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 2 more threads
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 1 more threads
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
2019-12-13 12:24:34,519:gensim.models.base_any2vec:INFO - worker thread finished; awaiting finish of 0 more threads
2019-12-13 12:24:34,520:gensim.models.base_any2vec:INFO - EPOCH - 10 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,520:gensim.models.base_any2vec:INFO - training on a 60 raw words (2 effective words) took 0.1s, 21 effective words/s
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,520:gensim.models.base_any2vec:WARNING - under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
2019-12-13 12:24:34,521:gensim.utils:INFO - saving Word2Vec object under model/word2vec_model, separately None
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
    debugger.enable_tracing(apply_to_all_threads=True)  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads

  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
      File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
2019-12-13 12:24:34,521:gensim.utils:INFO - not storing attribute vectors_norm
        func = self.__getitem__(name)func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__

2019-12-13 12:24:34,522:gensim.utils:INFO - not storing attribute cum_table
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
    func = self._FuncPtr((name_or_ordinal, self))AttributeError: dlsym(0x7fed18f247e0, AttachDebuggerTracing): symbol not found

AttributeError: dlsym(0x7fed18f247e0, AttachDebuggerTracing): symbol not found
func = self._FuncPtr((name_or_ordinal, self))
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
AttributeError: dlsym(0x7fed18d2ff70, AttachDebuggerTracing): symbol not found
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
2019-12-13 12:24:34,524:gensim.utils:INFO - saved model/word2vec_model
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18a163b0, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
        pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
result = lib.AttachDebuggerTracing(  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads

  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed15fd5850, AttachDebuggerTracing): symbol not found
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18a163b0, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed18b09320, AttachDebuggerTracing): symbol not found
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1631, in settrace
    stop_at_frame,
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1711, in _locked_settrace
    debugger.enable_tracing(apply_to_all_threads=True)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 482, in enable_tracing
    pydevd_tracing.set_trace_to_threads(self.dummy_trace_dispatch)
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd_tracing.py"", line 241, in set_trace_to_threads
    result = lib.AttachDebuggerTracing(
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 361, in __getattr__
    func = self.__getitem__(name)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py"", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: dlsym(0x7fed15fd5850, AttachDebuggerTracing): symbol not found
</code></pre>
","gensim, word2vec, word-embedding","<p>Gensim's <code>Word2Vec</code> expects its corpus <code>sentences</code> to be a <em>sequence</em> where each individual item is a <em>list of string tokens</em>. (That is, those string tokens are words.)</p>

<p>Instead, you have a list (which is acceptable as a sequence), where each of its items is a list (which is also acceptable), but then each of those lists instead has as each item yet another <em>list</em> – when for <code>Word2Vec</code> training, each of those items should be a string token (word). </p>

<p>(I've edited your example data to be be structurally-indented, to make the levels of nesting clearer.)</p>

<p>If those innermost lists-of-strings are your real individual ""sentences"", you need to be sure they're the items in your outermost list. </p>

<p>(If, on the other hand, you really want a cluster like <code>['intent.action.PHONE_STATE', 'intent.action.MAIN']</code> to be a single ""word"" in your model, you'll want to change that list into a single string token, so it can look like a word – and thus hashable key – to <code>Word2Vec</code> and Python.)</p>
",2,0,1405,2019-12-12 20:08:37,https://stackoverflow.com/questions/59312001/word2vec-how-to-rid-of-typeerror-unhashable-type-list-and-attributeerro
How does gensim.models.FatText.wv.wmdistance calculate between two documents?,"<p>I already have a training model for fastText with gensim, and<br>
I can get the distance between two sentence as described below,  </p>

<pre><code>sentence_1 = ""Today is very cold.""  
sentence_2 = ""I'd like something to drink.""    

print(model.wv.wmdistance(sentence_1.split("" ""), sentence_2.split("" "")))
# 0.8446287678977793  # for example
</code></pre>

<p>but how does <code>vmdistance</code> calculate this value?<br>
I'd like to know the formula.  </p>

<p>API documents: <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Doc2VecKeyedVectors.distance"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Doc2VecKeyedVectors.distance</a></p>
","python-3.x, gensim, fasttext","<p>The <code>wmdistance()</code> function calculates the ""Word Mover's Distance"" between two sets-of-words. </p>

<p>You can view the academic paper which coined the ""Word Mover's Distance"" (WMD) measure, via the application of an older idea from operations research called ""Earth Mover's Distance"" to text, at:</p>

<p><a href=""http://proceedings.mlr.press/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">From Word Embeddings To Document Distances</a>, by Matt Kusner et al</p>

<p>You can view the exact code used by gensim's <code>wmdistance()</code> function at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/de0dcc39fee0ae4eaf45d79bd5418d32780f9aa5/gensim/models/keyedvectors.py#L677"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/de0dcc39fee0ae4eaf45d79bd5418d32780f9aa5/gensim/models/keyedvectors.py#L677</a></p>

<p>WMD is fairly time-consuming to calculate, as it involves a search through many possible ""shifts"" of the ""piles of meaning"" for a minimal-expenditure approach. It becomes especially time-consuming as the texts become longer. (It's more practical for short sentences than full paragraphs or documents.)</p>

<p>Often texts are instead summarized into a single vectors – via either an averaging of their word-vectors, or a shallow text-to-vector algorithm like <code>Doc2Vec</code>, or a deep-learning model (BERT, ELMo, etc). Then those single vectors can be far more quickly compared via simple cosine-similarity. (That's what the plain <code>similarity()</code> or <code>distance()</code> methods of gensim's vector models do.)</p>
",3,1,2318,2019-12-13 08:45:43,https://stackoverflow.com/questions/59318935/how-does-gensim-models-fattext-wv-wmdistance-calculate-between-two-documents
PyLDAvis visualisation does not align with generated topics,"<p>I am using PyLDAvis to visualise the results of the LDA from Mallet. </p>

<p>Before I can do that, I need the wrapper of the gensim library:</p>

<pre><code>model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(model_list[8])
</code></pre>

<p>When I print the found topics, they are ordered from 0-10.</p>

<p>However when I am using the pyLDAvis to visualise the Topics, the Topic order (0-10), does not align with printed topics.</p>

<p>Example:  </p>

<pre><code>(5,
  '0.042*""euro"" + 0.030*""smartpho"" + 0.022*""camera"" + 0.020*""display"" + '
  '0.018*""model"" + 0.016*""picture"" + 0.012*""price"" + 0.010*""android""')
</code></pre>

<p>As you can see this topic is about smartphones.</p>

<p>However when I visualise the model with pyLDAvis, Topic 5 is not about smartphones, but about another Topic (cars for example). The smartphone topic is not 5 anymore but topic 1.</p>

<p>Example1: </p>

<p><a href=""https://i.sstatic.net/XeS6s.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XeS6s.png"" alt=""enter image description here""></a></p>

<p>Example2:
<a href=""https://i.sstatic.net/p1zUU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/p1zUU.png"" alt=""enter image description here""></a></p>

<p>Is this a known error or is this the normal? 
Somebody can help?</p>
","python, gensim, lda, topic-modeling, mallet","<p>By default, pyLDAvis sorts the topics by topic proportion -- To keep the original sort order, pass <code>sort_topics=False</code> to <code>pyLDAvis.prepare()</code>. Note that the pyLDAvis topics will still be off by one (i.e., Topic 1 in pyLDAvis will be Topic 0 from gensim).</p>

<p>There is a similar question here: <a href=""https://stackoverflow.com/questions/43260074/is-there-any-way-to-match-gensim-lda-output-with-topics-in-pyldavis-graph"">Is there any way to match Gensim LDA output with topics in pyLDAvis graph?</a></p>

<p>And an associated issue on the pyLDAvis repo: <a href=""https://github.com/bmabey/pyLDAvis/issues/127"" rel=""noreferrer"">https://github.com/bmabey/pyLDAvis/issues/127</a></p>
",5,3,1871,2019-12-13 12:14:22,https://stackoverflow.com/questions/59322409/pyldavis-visualisation-does-not-align-with-generated-topics
Loading Wikipedia XML files into Gensim,"<p>I'm a complete novice to NLP and would like to load a zipped XLM file of the Hungarian Wikipedia corpus (807 MB). I downloaded the dumpfile and started parsing it in Python with Gensim, but after 4 hours my laptop crashed, complaining that I had run out of RAM. I have a fairly old laptop (4GB RAM) and was wondering whether there is any way I could solve this problem by </p>

<ul>
<li>(1) either  tinkering with my code, e.g, by reducing the corpus by taking, say, a 1/10th random sample of it; </li>
<li>(2) or using some cloud platform to enhance my CPU power. I read in <a href=""https://stackoverflow.com/questions/32543235/python-gensim-memory-error"">this SO post</a> that AWS can be used for such puposes, but I am unsure which service I should select (Amazon EC2?). I also checked Google Colab, but got confused that it lists hardware acceleration options (GPU and CPU) in the context of Tensorflow, and I am not sure if that is suitable for NLP. I didn't find any posts about that. </li>
</ul>

<p>Here's my Jupyter Notebook code that I've tried after downloading the wikipedia dumps from <a href=""https://dumps.wikimedia.org/huwiki/latest/huwiki-latest-pages-articles.xml.bz2"" rel=""nofollow noreferrer"">here</a>: </p>

<pre><code>! pip install gensim 
from nltk.stem import SnowballStemmer
from gensim.corpora import WikiCorpus
from gensim.models.word2vec import Word2Vec

hun_stem = SnowballStemmer(language='hungarian')

%%time
hun_wiki = WikiCorpus(r'huwiki-latest-pages-articles.xml.bz2')
hun_articles = list(hun_wiki.get_texts())
len(hun_articles)
</code></pre>

<p>Any guidance would be much appreciated. </p>
","python, nlp, gensim","<p>807MB compressed will likely expand to more than 4GB uncompressed, so you're not going to have luck loading the whole data into memory on your machine. </p>

<p>But, lots of NLP tasks <strong>don't</strong> require the full dataset in memory: they can just stream the data repeatedly from the disk as necessary. </p>

<p>For example, whatever your ultimate goal is, you will often be able to just iterate over the <code>hun_wiki.get_texts()</code> sequence, article by article. <strong>Don't</strong> try to load it into a single in-memory list with a <code>list()</code> operation.</p>

<p>(If you really wanted to just load a subset as a list, you could just take the first <em>n</em> from that iterator, or take a random subset via one of the ideas at an answer like <a href=""https://stackoverflow.com/questions/12581437/python-random-"">this one</a>.)</p>

<p>Or, you could rent a cloud machine with more memory. Almost anythin you choose with more memory will be suitable for running Python-based text-processing code, so just follow each service's respective tutorials to learn how to set up &amp; log-into a new rented instance. </p>

<p>(4GB is quite small for modern serious work, but if you're just tinkering/learning, you can work with smaller datasets and be efficient about not loading everything into memory when not necessary.)</p>
",0,0,676,2019-12-15 21:27:14,https://stackoverflow.com/questions/59348206/loading-wikipedia-xml-files-into-gensim
Gensim Word2Vec or FastText build vocab from frequency,"<p>I wonder what does <code>.build_vocab_from_freq()</code> function from gensim actually do? What is the difference when I'm not using it? Thank you!</p>
","python, gensim, word2vec, fasttext","<p>It ""builds a vocabulary from a dictionary of word frequencies"". You need a vocabulary for your gensim models. Usually you build it from your corpus. This is basically an alternative option to build your vocabulary from a word frequencies dictionary. Word frequencies for example are usually used to filter low or high frequent words which are meaningless for your model.</p>
",0,0,611,2019-12-17 05:48:21,https://stackoverflow.com/questions/59368232/gensim-word2vec-or-fasttext-build-vocab-from-frequency
How to get document-topics using models.hdpmodel – Hierarchical Dirichlet Process in gensim,"<p>I just study gensim for topic modeling. when I use </p>

<pre><code>lda_model = gensim.models.ldamodel.LdaModel(...)
</code></pre>

<p>the result lda_model has two functions: get_topics() and get_document_topics(). I can find the topic-word and document-topics by them. But, I want to try:</p>

<pre><code>hdp_lda_model = gensim.models.hdpmodel.HdpModel(...)
</code></pre>

<p>I can only find there is get_topics() in its result, no something like get_document_topics(). So I cannot find the relation of document and topics. But it should be somewhere. I read some instruction from <a href=""https://radimrehurek.com/gensim/models/hdpmodel.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/hdpmodel.html</a>. But I did not find any (maybe I miss something?). So is there a function in hdp model, which is like get_document_topics() in lda model?</p>
","document, gensim, cpu-word, lda, hdp","<p>Both models have a <code>__getitem__</code> method that does what you want.</p>

<p>For LDA it's actually a wrapper of <code>get_document_topics</code>
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/ldamodel.py#L1503"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/ldamodel.py#L1503</a></p>

<p>And for HDP it's wrapping the <code>inference</code> method but doing additionally more than just calling it:
<a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/hdpmodel.py#L427"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/hdpmodel.py#L427</a></p>

<p>So, to answer your question. You can do for both models:</p>

<pre><code>lda_model[bow_doc]
</code></pre>

<p>or </p>

<pre><code>hdp_lda_model[bow_doc]
</code></pre>

<p>and then get a topic distribution for <code>bow_doc</code></p>

<p>Results in something like:</p>

<pre><code>[(5, 0.05342164806543596),
 (7, 0.04307238446604077),
 (11, 0.5281130394662548),
 (31, 0.28899472194287035),
 (60, 0.07985460856925444)]
</code></pre>
",2,2,1145,2019-12-20 00:14:31,https://stackoverflow.com/questions/59418433/how-to-get-document-topics-using-models-hdpmodel-hierarchical-dirichlet-proces
How to use gensim topic modeling to predict new document?,"<p>I am new to gensim topic modeling. Here is my sample code:</p>

<pre><code>import nltk
nltk.download('stopwords')
import re
from pprint import pprint
# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
# spacy for lemmatization
import spacy
# Plotting tools
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
#%matplotlib inline
# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)
import warnings
warnings.filterwarnings(""ignore"",category=DeprecationWarning)
# NLTK Stop words
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
train=pd.DataFrame({'text':['find the most representative document for each topic',
                            'topic distribution across documents',
                            'to help with understanding the topic',
                            'one of the practical application of topic modeling is to determine']})
text=pd.DataFrame({'text':['how to find the optimal number of topics for topic modeling']})

data =  train.loc[:,'text'].values.tolist()

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

data_words = list(sent_to_words(data))
id2word = corpora.Dictionary(data_words)
corpus = [id2word.doc2bow(text) for text in data_words]
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                            id2word=id2word,
                                            num_topics=3)
</code></pre>

<p>So far so good. But I want to use lda_model to predict text. I at least need to know topic distribution over text and all the topic-word relation. </p>

<p>I think prediction is very common and important function for lda. But I do not know where I can find such function in gensim. Some answers says doc_lda = model[doc_bow] is prediction (<a href=""https://stackoverflow.com/questions/40924185/calculating-topic-distribution-of-an-unseen-document-on-gensim"">Calculating topic distribution of an unseen document on GenSim</a>). But I am not sure about it.</p>
","document, gensim, predict, lda","<pre><code>import pandas as pd 
train=pd.DataFrame({'text':['find the most representative document for each topic',
                        'topic distribution across documents',
                        'to help with understanding the topic',
                        'one of the practical application of topic modeling is to determine']})
text=pd.DataFrame({'text':['how to find the optimal number of topics for topic modeling']})


def sent_to_words(sentences):
for sentence in sentences:
    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

#using your train data to train the model with 4 topics

data_words = list(sent_to_words(train['text']))
id2word = corpora.Dictionary(data_words)
corpus = [id2word.doc2bow(text) for text in data_words]

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                        id2word=id2word,
                                        num_topics=4)

#  predicting new text which is in text dataframe  
new_text_corpus =  id2word.doc2bow(text['text'][0].split())
lda[new_text_corpus]

#op

Out[75]:
[(0, 0.5517368), (1, 0.38150477), (2, 0.032756805), (3, 0.03400166)]
</code></pre>
",2,0,7107,2019-12-20 02:16:48,https://stackoverflow.com/questions/59419123/how-to-use-gensim-topic-modeling-to-predict-new-document
How to perform doc2vec.infer_vector() on millions of documents?,"<p>I trained a doc2vec model using <code>python gensim</code> on a corpus of 40,000,000 documents. This model is used for infering docvec on millions of documents everyday. To ensure stability, I set <code>alpha</code> to a small value and a large <code>steps</code> instead of setting a constant random seed:</p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
model = Doc2Vec.load('doc2vec_dm.model')
doc_demo = ['a','b']
# model.random.seed(0)
model.infer_vector(doc_demo, alpha=0.1, min_alpha=0.0001, steps=100)
</code></pre>

<p><code>doc2vec.infer_vector()</code> accepts only one documents each time and it takes almost 0.1 second to infer each docvec. Is there any <code>API</code> that can handle a series of documents in each infering step?</p>
","gensim, doc2vec","<p>Currently, there's no gensim API which does large batches of inference at once, which could help by using multiple threads. It is a wishlist item, among other improvements: <a href=""https://github.com/RaRe-Technologies/gensim/issues/515"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/515</a></p>

<p>You might get some speedup, up to the number of cores in your CPU, by spreading your own inference jobs over multiple threads. </p>

<p>To eliminate all multithreaded contention due to the Python GIL, you could spread your inference over separate Python processes. If each process loads the model using some of the tricks described at <a href=""https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time/43067907#43067907"">another answer</a>n (see below), the OS will help them share the large model backing arrays (only paying the cost in RAM once), while they each could completely independently due one unblocking thread of inference. </p>

<p>(Specifically, <code>Doc2Vec.load()</code> can also use the <code>mmap='r'</code> mode to load an existing on-disk model with memory-mapping of the backing files. Inference alone, with no <code>most_similar()</code>-like operations, will only read the shared raw backing arrays, so no fussing with the <code>_norm</code> variants should be necessary if you're launching single-purpose processes that just do inference then save their results and exit.)</p>
",0,0,1187,2019-12-25 13:45:50,https://stackoverflow.com/questions/59478986/how-to-perform-doc2vec-infer-vector-on-millions-of-documents
Gensim Doc2Vec infer_vector on unseen words differs based on characters in these words,"<p>Gensim Doc2Vec infer_vector on paragraphs with unseen words generates vectors that differ based on the characters in the unsween words.</p>

<pre><code>for i in range(0, 2):
    print(model.infer_vector([""zz""])[0:2])
    print(model.infer_vector([""zzz""])[0:2])
    print(model.infer_vector([""zzzz""])[0:2])
    print(""\n"")

[ 0.00152548 -0.00055992]
[-0.00165872 -0.00047997]
[0.00125548 0.00053445]


[ 0.00152548 -0.00055992] # same as in previous iteration
[-0.00165872 -0.00047997]
[0.00125548 0.00053445]
</code></pre>

<p>I am trying understand how unseen words affect initialization of the infer_vector. It looks like different characters will produce different vectors. Trying to understand why.</p>
","gensim, word2vec, doc2vec","<p>Unseen words are ignored for the actual process of iterative inference: tuning a vector to better-predict a text's words, according to a frozen <code>Doc2Vec</code> model. </p>

<p>However, inference starts with a pseudorandomly-initialized vector. And, the full set of tokens passed-in (including unknown words) are used as the seed for that random-initialization. </p>

<p>This seeded initialization is done as a potential small aid to those seeking fully-reproducible inference – but in practice, seeking such exact-reproduction, rather than just run-to-run similarity, is usually a bad idea. See the gensim FAQs  <a href=""https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q11-ive-trained-my-word2vecdoc2vecetc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">Q11</a> &amp; <a href=""https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q12-ive-used-doc2vec-infer_vector-on-a-single-text-but-the-resulting-vector-is-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-doc2vec-inference-non-determinism"" rel=""nofollow noreferrer"">Q12</a> about varying results from run-to-run for more details.</p>

<p>So what you're seeing is:</p>

<ul>
<li>your different tokenized texts each cause a pseudorandom, but deterministic with respect to the source text, vector initialization</li>
<li>since no words are known, inference afterwards is a no-op: there are no words to predict</li>
<li>the pseudorandom initialized vector is returned</li>
</ul>

<p>The <code>infer_vector()</code> method should probably log a warning, or return a flag value (like perhaps the origin vector), as a better hint that nothing meaningful is actually happening. </p>

<p>But you may wish to check any text before you supply it to <code>infer_vector()</code> – if none of its words are in the <code>d2v_model.wv</code>, then inference will simply be returning a small random initialization vector. </p>
",4,2,750,2019-12-25 22:01:34,https://stackoverflow.com/questions/59482140/gensim-doc2vec-infer-vector-on-unseen-words-differs-based-on-characters-in-these
Sentiment Classification using Doc2Vec,"<p>I am confused as to how I can use <strong>Doc2Vec(using Gensim)</strong> for IMDB sentiment classification dataset. I have got the Doc2Vec embeddings after training on my corpus and built my Logistic Regression model using it. How do I use it to make predictions for new reviews? sklearn TF-IDF has a <em>transform</em> method that can be used on test data after training on training data, what is its equivalent in Gensim Doc2Vec?</p>
","python, nlp, gensim, doc2vec","<p>Have you seen the demo notebook, included with the gensim source code through gensim-3.8.1, which applies <code>Doc2Vec</code> to the IMDB dataset?</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/3.8.1/docs/notebooks/doc2vec-IMDB.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/3.8.1/docs/notebooks/doc2vec-IMDB.ipynb</a></p>
",0,0,418,2019-12-27 12:55:42,https://stackoverflow.com/questions/59501121/sentiment-classification-using-doc2vec
Unable to load a file in flask application,"<p>We have a <code>flask</code> application where we need to load a pretrained model located at the path <code>'/root/apps/mlapi/resources/emoji2vec.bin'</code> using <code>gensim</code>. While running the code I am getting below error</p>

<pre><code>File ""mlapi.py"", line 26, in &lt;module&gt;
    e2v_model = ModelEmoji2Vec()
  File ""/home/atinesh/Downloads/Current/vnc_chat/apps2/mlapi/models/susheels/text2emoji/vector_model/modelE2V.py"", line 19, in __init__
    self.e2v = gsm.KeyedVectors.load_word2vec_format(emoji2vec_path, binary=True)
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/gensim/models/keyedvectors.py"", line 1498, in load_word2vec_format
    limit=limit, datatype=datatype)
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/gensim/models/utils_any2vec.py"", line 342, in _load_word2vec_format
    with utils.open(fname, 'rb') as fin:
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/smart_open/smart_open_lib.py"", line 308, in open
    errors=errors,
  File ""/home/atinesh/Downloads/Current/vnc_chat/vnc_env/lib/python3.6/site-packages/smart_open/smart_open_lib.py"", line 517, in _shortcut_open
    return _builtin_open(parsed_uri.uri_path, mode, buffering=buffering, **open_kwargs)
PermissionError: [Errno 13] Permission denied: '/root/apps/mlapi/resources/emoji2vec.bin'
</code></pre>

<p>Basically, error is occurring at </p>

<pre><code>#modelE2V.py, line 19
self.e2v = gsm.KeyedVectors.load_word2vec_format(emoji2vec_path, binary=True)
</code></pre>

<p>It says that permission denied. But if I try to load the model using simple python script outside flask app it works fine, why this error is occurring inside flask application</p>
","python, python-3.x, flask, gensim","<p>You are running into a File Permission error. This means the unix user that is attempting to access your file does not have the required permissions to do so.</p>

<p>It appears you are storing your pretrained model file at
<code>/root/apps/mlapi/resources/emoji2vec.bin</code></p>

<p>This looks like a location that requires root or sudo privileges to access.
To verify this run</p>

<pre><code>ls -l /root/apps/mlapi/resources/emoji2vec.bin
</code></pre>

<p>The output will probably be something like:</p>

<pre><code>-rw-rw---- 1 root root 6 Dec 29 XX:XX /root/apps/mlapi/resources/emoji2vec.bin
</code></pre>

<p>This indicates only the users in the root group can read and write (rw) the file, and that only the root user can read and write the file.</p>

<p>When you preprend <code>sudo</code> to a command you are changing to the root user and then executing the rest of the command as this root user. So running the python script with sudo means that the root user will execute the script, because the root user has access to /root/apps/mlapi/resources/emoji2vec.bin the script will run without a problem.</p>

<p>However when you run flask, you are running it with a different user. On yourt development box (your computer) most likely the unix user running flask is <code>atinesh</code>.</p>

<p>So the basic solution is to modify the permissions of /root/apps/mlapi/resources/emoji2vec.bin to be owned or at least read, by atinesh.</p>

<p>@furas pointed out in the comments that installing resources in /root/, as you have done here, is usually a bad idea, and I agree.</p>

<p>The optimal solution is to:
Relocated your file and change the permissions.</p>

<pre><code>mkdir /home/atinesh/Downloads/Current/vnc_chat/apps2/mlapi/resources
sudo cp /root/apps/mlapi/resources/emoji2vec.bin /home/atinesh/Downloads/Current/vnc_chat/apps2/mlapi/resources/emoji2vec.bin
sudo chown atinesh:atinesh /home/atinesh/Downloads/Current/vnc_chat/apps2/mlapi/resources/emoji2vec.bin
</code></pre>

<p>If you are set on not relocating the file you could just change the permissions where it is now:</p>

<pre><code>sudo chown atinesh:atinesh /root/apps/mlapi/resources/emoji2vec.bin
</code></pre>

<p>As an aside, in production environments, it may be wise to create a new unix user called ""app"" (or something like that) and then only give the user ""app"" permissions to a single folder that contains your flask code. This prevents your app code from touching files you don't want it to. Also if your flask application was somehow compromised the attacker would only have the permissions of ""app"" user.</p>
",1,0,194,2019-12-28 10:32:18,https://stackoverflow.com/questions/59510075/unable-to-load-a-file-in-flask-application
Understanding of the parameter model.infer_vector for doc2vec gensim,"<p><a href=""https://i.sstatic.net/SZxpG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SZxpG.png"" alt=""enter image description here""></a></p>

<p>Does it mean that I must provide tokenized words of a document as list of strings or simply a document as a list of string for the input doc_words. Please clarify</p>
","python, gensim, doc2vec","<p>The <code>doc_words</code> should be a list of individual word-tokens as strings, equivalent to the <code>words</code> of each training document during training. That is: it should have been preprocessed and tokenized the same as your training data was. </p>

<p>(When you ask in your question, ""tokenized words of a document as list of strings or simply a document as a list of string"", as far as I understand those words, those two alternatives are the same thing: a Python <code>list</code>, where each item is a string word.)</p>

<p>Other important things to note about <code>infer_vector()</code>:</p>

<ul>
<li><p>inference always starts with a low-magnitude random vector, then iteratively improves that vector</p></li>
<li><p>words not known to the model will be silently ignored; at the extreme, if you supply a text with all unknown words, no inference will happen – but because of the random initialization above, you'll still get a vector back</p></li>
<li><p>if you don't specify an <code>epochs</code> value, it will reuse the value cached in the model (left over from model initialization or your last <code>train()</code> call). You will generally want it to use a number of epochs at least as large as was used in training – which is most commonly 10-20, but sometimes larger. (And, larger values may be especially helpful with shorter texts.)</p></li>
</ul>
",0,0,292,2020-01-07 15:20:33,https://stackoverflow.com/questions/59631259/understanding-of-the-parameter-model-infer-vector-for-doc2vec-gensim
LDA Topic Modelling : Topics predicted from huge corpus make no sense,"<p>I am using LDA for a Topic Modelling task. As suggested in various forums online, I have trained my model on a fairly large corpus : NYTimes news dataset (~ 200 MB csv file) which has reports regarding a wide variety of news topics.
Surprisingly the topics predicted out of it are mostly related to US politics and when I test it on a new document regarding 'how to educate children and parenting stuff' it predicts the most likely topic as this :</p>

<p>['two', 'may', 'make', 'company', 'house', 'things', 'case', 'use']</p>

<p>Kindly have a look at my model :</p>

<pre><code>def ldamodel_english(filepath, data):
  data_words = simple_preprocess(str(data), deacc=True)

  # Building the bigram model and removing stopwords
  bigram = Phrases(data_words, min_count=5, threshold=100)
  bigram_mod = Phraser(bigram)
  stop_words_english = stopwords.words('english')
  data_nostops = [[word for word in simple_preprocess(str(doc)) if word not in stop_words_english] 
for doc in data_words]
  data_bigrams = [bigram_mod[doc] for doc in data_nostops]
  data_bigrams = [x for x in data_bigrams if x != []]

  # Mapping indices to words for computation purpose
  id2word = corpora.Dictionary(data_bigrams)
  corpus = [id2word.doc2bow(text) for text in data_bigrams]

  # Building the LDA model. The parameters 'alpha' and 'eta' handle the number of topics per document and words per topic respectively
  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=20, random_state=10, iterations=100,
                                            update_every=1, chunksize=1000, passes=8, alpha=0.09, per_word_topics=True, eta=0.8)
  print('\nPerplexity Score: ' + str(lda_model.log_perplexity(corpus)) + '\n')
  for i, topic in lda_model.show_topics(formatted=True, num_topics=20, num_words=10):
      print('TOPIC #' + str(i) + ': ' + topic + '\n')
  coherence_model_lda = CoherenceModel(model=lda_model, texts=data_bigrams, dictionary=id2word, coherence='c_v')
  print('\nCoherence Score: ', coherence_model_lda.get_coherence())
  saved_model_path = os.path.join(filepath, 'ldamodel_english')
  lda_model.save(saved_model_path)

return saved_model_path, corpus, id2word
</code></pre>

<p>The 'data' part comes from the 'Content' section of the NYTimes News dataset and I used GENSIM library for LDA.</p>

<p>My question is if a well trained LDA model predicts so badly why there is such a hype and what is an effective alternative method?</p>
","python, data-science, gensim, lda","<p>It can be a perfectly valid output of the model. Given the source texts which are not necessary related to ""children education and parenting"" the topic that was found to be the most similar might just be very rudimentarily similar to the article. It is likely that there is not much of the vocabulary in common between NY Times articles and your article. So the words that made the topic distinctive among the topics typical for NY Times might have very little in common with your article. In fact, the only words that are shared may be really rather typical of anything as in your case.</p>

<p>This is happening frequently when the corpus used for training the LDA model has little to do with the documents it is applied to later. So there is really not much surprise here. <strong>The size of the corpus does not help as what matters is the vocabulary/topical overlap.</strong></p>

<p>I suggest that you either change the number of topics and the corpus or find a suitable corpus to train LDA on (that contains texts related to the documents you intend to classify).</p>
",1,1,709,2020-01-16 08:53:24,https://stackoverflow.com/questions/59765941/lda-topic-modelling-topics-predicted-from-huge-corpus-make-no-sense
Error while implementing Word2Vec model with embedding_vector,"<p>I'm getting an <strong>AttributeError</strong> while trying to implement with embedding_vector:</p>

<pre><code>from gensim.models import KeyedVectors
embeddings_dictionary = KeyedVectors.load_word2vec_format('model', binary=True)

embedding_matrix = np.zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
</code></pre>

<blockquote>
  <p>AttributeError: 'Word2VecKeyedVectors' object has no attribute 'get'</p>
</blockquote>
","python, machine-learning, keras, gensim, word2vec","<p>Yes, <code>gensim</code>'s <code>KeyedVectors</code> abstraction does not offer a <code>get()</code> method. (What docs or example are you following that suggests it does?)</p>

<p>You can use standard Python <code>[]</code>-indexing, eg:</p>

<pre><code>embedding_dictionary[word]
</code></pre>

<p>Though, there isn't really a reason for your loop copying each vector into your own <code>embedding_matrix</code>. The <code>KeyedVectors</code> instance already has a raw array, with each vector in a row, in the order of the <code>KeyedVectors</code> <code>.index2entity</code> list – in its <code>vectors</code> property:</p>

<pre><code>embedding_dictionary.vectors
</code></pre>
",2,0,11363,2020-01-19 19:33:28,https://stackoverflow.com/questions/59813664/error-while-implementing-word2vec-model-with-embedding-vector
save/reuse doc2vec based model for further predictions,"<p>I have been following the following example for using doc2vec for text classification:</p>

<p><a href=""https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb"" rel=""nofollow noreferrer"">https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb</a></p>

<p>I ran this notebook on my datasets and want to apply one of the doc2vec models to a 3rd dataset (eg, the overall dataset the test/train model was built on).  I tried:</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)
X_train = label_sentences(X_train, 'Train')
X_test = label_sentences(X_test, 'Test')

#added
big_text = label_sentences(big_text, 'Test') #big_text = larger dataframe

#old
#all_data = X_train + X_test

#new
all_data = X_train + X_test + big_text 
</code></pre>

<p>1 - this is not really practical for applied purposes.  The data that one wants to predict might not be available at the time of train/testing.</p>

<p>2 - the model performance decreased as a result</p>

<p>So how can I save once of the models and applying to a completely different dataset?  It would seems that I would need to update the doc2vec model with docs of the other dataset as well.</p>
","machine-learning, scikit-learn, gensim","<p>A gensim <code>Doc2Vec</code> model may be saved and loaded using the <code>.save(filepath)</code> &amp; <code>.load(filepath)</code> methods. (Using these native-to-gensim methods will work on larger models than plain Python pickling can support, and more-efficiently store some of the larger internal arrays as separate files. (If moving the saved model, be sure to keep this subsidiary files alongside the main file that's at exactly the <code>filepath</code> location.)</p>

<p>A previously-trained <code>Doc2Vec</code> model can generate doc-vectors for new texts via the <code>.infer_vector(list_of_words)</code> method. </p>

<p>Note that the <code>list_of_words</code> provided to this method should have been preprocessed/tokenized exactly the same as the training data – and any words that weren't present (or sufficiently <code>min_count</code> frequent) in the training data will be ignored. (At the extreme, this means if you pass in a <code>list_of_words</code> with no recognized words, all words will be ignored, and you'll get back a randomly-initialized but completely-unimproved-by-inference vector.)</p>

<p>Still, if you're re-evaulating or re-training the downstream predictive models on new data from some new domain, you'd often want to re-train the <code>Doc2Vec</code> stage as well, with all available data, so that it has a chance to learn new words from new usage contexts. (It's mainly when your training data was extensive &amp; representative, and your new data comes in incrementally and without major shifts in vocabulary/usage/domain, that you'd want to rely on <code>.infer_vector()</code>.)</p>
",1,0,1171,2020-01-20 16:48:53,https://stackoverflow.com/questions/59827730/save-reuse-doc2vec-based-model-for-further-predictions
"API calls from NLTK, Gensim, Scikit Learn","<p>I plan to use NLTK, Gensim and Scikit Learn for some NLP/text mining. But i will be using these libraries to work with my org data. The question is while using these libraries 'do they make API calls to process the data' or is the data taken out of the python shell to be processed. It is a security question, so was wondering if someone has any documentation for reference.</p>

<p>Appreciate any help on this.</p>
","python, api, nlp, nltk, gensim","<p>Generally with NLTK, gensim, and scikit-learn, algorithms are implemented in their source code, and run locally on your data, without sending data elsehwere for processing. </p>

<p>I've never noticed any documentation/functionality of these packages mentioning a reliance on an remote/cloud service, nor seen users discussing the same. </p>

<p>However, they're each large libraries, with many functions I've never reviewed, and with many contributors adding new options. And I don't know if the project leads have stated an explicit commitment to never rely on external services. </p>

<p>So a definitive, permanent answer may not be possible. To the extent such security is a concern for your project, you should carefully review the documentation, and even source code, for those functions/classes/methods you're using. (None of these projects would intentionally hide a reliance on outside services.)</p>

<p>You could also develop, test, and deploy the code on systems whose ability to contact outside services is limited by firewalls – so that you could detect and block any undisclosed or inadvertent communication with outside machines. </p>

<p>Note also that each of these libraries in turn relies on other public libraries. If your concern also extends to the potential for either careless or intentionally, maliciously-inserted methods of private data exfiltration, you would want to do a deeper analysis of these libraries &amp; all other libraries they bring-in. (Simply trusting the top-level documentation could be insufficient.) </p>

<p>Also, each of these libraries have utility functions which, on explicit user demand, download example datasets or shared non-code resources (like lists of stopwords or lexicons). Using such functions doesn't upload any of your data elsewhere, but may leak that you're using specific functionality. The firewall-based approach mentioned above could interfere with such download steps. Under a situation of maximum vigilance/paranoia, you might want pay special attention to the use &amp; behavior of such extra-download methods, to be sure they're not doing any more than they should to change the local environment or execute/replace other library code. </p>

<p>Finally, by sticking to widely-used packages/functions, and somewhat older versions that have remained continuously available, you may benefit from a bit of ""community assurance"" that a package's behavior is well-understood, without surprising dependencies or vulnerabilities. That is, many other users will have already given those code-paths some attention, analysis, &amp; real-usage – so any problems may have already been discovered, disclosed, and fixed.</p>
",1,1,204,2020-01-21 16:17:13,https://stackoverflow.com/questions/59845191/api-calls-from-nltk-gensim-scikit-learn
word2vec window size at sentence boundaries,"<p>I am using word2vec (and doc2vec) to get embeddings for sentences, but i want to completely ignore word order.
I am currently using gensim, but can use other packages if necessary.</p>

<p>As an example, my text looks like this:</p>

<pre><code>[
['apple', 'banana','carrot','dates', 'elderberry', ..., 'zucchini'],
['aluminium', 'brass','copper', ..., 'zinc'],
...
]
</code></pre>

<p>I intentionally want 'apple' to be considered as close to 'zucchini' as it is to 'banana' so I have set the window size to a very large number, say 1000.
I am aware of 2 problems that may arise with this.</p>

<p>Problem 1:
The window might <em>roll</em> in at the start of a sentence creating the following training pairs:
<code>('apple', ('banana')), ('apple', ('banana', 'carrot')), ('apple', ('banana', 'carrot', 'date'))</code> before it eventually gets to the correct <code>('apple', ('banana','carrot', ..., 'zucchini'))</code>.
This would seem to have the effect of making 'apple' closer to 'banana' than 'zucchini',
since their are so many more pairs containing 'apple' and 'banana' than there are pairs containing 'apple' and 'zucchini'.</p>

<p>Problem 2:
I heard that pairs are sampled with inverse proportion to the distance from the target word to the context word- This also causes an issue making nearby words more seem more connected than I want them to be.</p>

<p>Is there a way around problems 1 and 2?
Should I be using cbow as opposed to sgns? Are there any other hyperparameters that I should be aware of?
What is the best way to go about removing/ignoring the order in this case?</p>

<p>Thank you</p>
","gensim, word2vec","<p>I'm not sure what you mean by ""Problem 1"" - there's no ""roll"" or ""wraparound"" in the usual interpretation of a word2vec-style algorithm's <code>window</code> parameter. So I wouldn't worry about this. </p>

<p>Regarding ""Problem 2"", this factor can be essentially made negligible by the choice of a giant <code>window</code> value – say for example, a value one million times larger than your largest sentence. Then, any difference in how the algorithm treats the nearest-word and the 2nd-nearest-word is vanishingly tiny. </p>

<p>(More specifically, the way the gensim implementation – which copies the original Google <code>word2vec.c</code> in this respect – achieves a sort of distance-based weighting is actually via random dynamic shrinking of the actual <code>window</code> used. That is, for each visit during training to each target word, the effective <code>window</code> truly used is some random number from 1 to the user-specified <code>window</code>. By effectively using smaller windows much of the time, the nearer words have more influence – just without the cost of performing other scaling on the whole window's words every time. But in your case, with a giant <code>window</code> value, it will be incredibly rare for the effective-window to ever be smaller than your actual sentences. Thus every word will be included, equally, almost every time.)</p>

<p>All these considerations would be the same using SG or CBOW mode. </p>

<p>I believe a million-times-larger <code>window</code> will be adequate for your needs, for if for some reason it wasn't, another way to essentially cancel-out any nearness effects could be to ensure your corpus's items individual word-orders are re-shuffled between each time they're accessed as training data. That ensures any nearness advantages will be mixed evenly across all words – especially if each sentence is trained on many times. (In a large-enough corpus, perhaps even just a 1-time shuffle of each sentence would be enough. Then, over all examples of co-occurring words, the word co-occurrences would be sampled in the right proportions even with small windows.)</p>

<p>Other tips:</p>

<p>If your training data starts in some arranged order that clumps words/topics together, it can be beneficial to shuffle them into a random order instead. (It's better if the full variety of the data is interleaved, rather than presented in runs of many similar examples.) </p>

<p>When your data isn't true natural-language data (with its usual distributions &amp; ordering significance), it may be worth it to search further from the usual defaults to find optimal metaparameters. This goes for <code>negative</code>, <code>sample</code>, &amp; especially <code>ns_exponent</code>. (One paper has suggested the optimal <code>ns_exponent</code> for training vectors for recommendation-systems is far different from the usual 0.75 default for natural-language modeling.)</p>
",3,2,1778,2020-01-23 05:25:42,https://stackoverflow.com/questions/59872029/word2vec-window-size-at-sentence-boundaries
Correct way to represent documents containing multiple sentences in gensim file-based training,"<p>I am trying to use gensim's file-based training (example from documentation below):</p>

<pre><code>from multiprocessing import cpu_count
from gensim.utils import save_as_line_sentence
from gensim.test.utils import get_tmpfile
from gensim.models import Word2Vec, Doc2Vec, FastText
 # Convert any corpus to the needed format: 1 document per line, words delimited by "" ""
corpus = api.load(""text8"")
corpus_fname = get_tmpfile(""text8-file-sentence.txt"")
save_as_line_sentence(corpus, corpus_fname)
 # Choose num of cores that you want to use (let's use all, models scale linearly now!)
num_cores = cpu_count()
 # Train models using all cores
w2v_model = Word2Vec(corpus_file=corpus_fname, workers=num_cores)
d2v_model = Doc2Vec(corpus_file=corpus_fname, workers=num_cores)
ft_model = FastText(corpus_file=corpus_fname, workers=num_cores)
</code></pre>

<p>However, my actual corpus contains many documents, each containing many sentences.
For example, let's assume my corpus is the plays of Shakespeare - Each play is a document, each document has many many sentences, and I would like to learn embeddings for each play, but the word embeddings only from within the same sentence.
Since the file-based training is meant to be one document per line, I assume that I should put one play per line. However, the documentation for file-based-training doesn't have an example of any documents with multiple sentences.
Is there a way to peek inside the model to see the documents and word context pairs that have been found before they are trained?</p>

<p>What is the correct way to build this file, maintaining sentence boundaries?</p>

<p>Thank you</p>
","gensim, corpus, doc2vec, sentence","<p>These algorithm implementations don't have any real understanding of, or dependence on, actual sentences. They just take texts – runs of word-tokens. </p>

<p>Often the texts provided to <code>Word2Vec</code> will be multiple sentences. Sometimes punctuation like sentence-ending periods are even retained as pseudo-words. (And when the sentences were really consecutive with each other in the source data, the overlapping word-context windows, between sentences, may even be a benefit.)</p>

<p>So you don't have to worry about ""maintaining sentence boundaries"". Any texts you provide that are sensible units of words that really co-occur will work about as well. (Especially in <code>Word2Vec</code> and <code>FastText</code>, even changing your breaks between texts to be sentences, or paragraphs, or sections, or documents is unlikely to have very much effect on the final word-vectors – it's just changing a subset of the training contexts, and probably not in any way that significantly changes which words influence which other words.)</p>

<p>There is, however, another implementation limit in <code>gensim</code> that you should watch out for: each training text can only be 10,000 tokens long, and if you supply larger texts, the extra tokens will be silently ignored. </p>

<p>So, be sure to use texts that are 10k tokens or shorter – even if you have to arbitrarily split longer ones. (Per above, any such arbitrary extra break in the token grouping is unlikely to have a noticeable effect on results.)</p>

<p>However, this presents a special problem using <code>Doc2Vec</code> in <code>corpus_file</code> mode, because in that mode, you don't get to specify your preferred <code>tags</code> for a text. (A text's tag, in this mode, is essentially just the line-number.)</p>

<p>In the original sequence corpus mode, the workaround for this 10k token limit was just to break up larger docs into multiple docs - but use the same repeated <code>tags</code> for all sub-documents from an original document. (This very closely approximates how a doc of any size would affect training.)</p>

<p>If you have documents with more than 10k tokens, I'd recommend either not using <code>corpus_file</code> mode, or figuring some way to use logical sub-documents of less than 10k tokens, then perhaps modeling your larger docs as the set of their sub-documents, or otherwise adjusting your downstream tasks to work on the same sub-document units. </p>
",1,0,942,2020-01-24 02:33:11,https://stackoverflow.com/questions/59889710/correct-way-to-represent-documents-containing-multiple-sentences-in-gensim-file
Word2Vec - How can I store and retrieve extra information regarding each instance of corpus?,"<p>I need to combine Word2Vec with my <code>CNN</code> model. To this end, I need to persist a flag (a binary one is enough) for each sentence as my corpus has two types (<em>a.k.a.</em> target classes) of sentences. So, I need to retrieve this flag of each vector after creation. How can I store and retrieve this information inside the input sentences of <code>Word2Vec</code> as I need both of them in order to train my deep neural network?</p>

<p>p.s. I'm using <code>Gensim</code> implementation of <code>Word2Vec</code>.</p>

<p>p.s. My corpus has <strong>6,925</strong> sentences, and <code>Word2Vec</code> produces <strong>5,260</strong> vectors.</p>

<p><strong>Edit:</strong> More detail regarding my corpus (as requested):</p>

<p>The structure of the corpus is as follows:</p>

<ol>
<li><p>sentences (label: <code>positive</code>) -- A <strong>Python list</strong></p>

<ul>
<li><code>Feature-A</code>: <strong>String</strong></li>
<li><code>Feature-B</code>: <strong>String</strong></li>
<li><code>Feature-C</code>: <strong>String</strong></li>
</ul></li>
<li><p>sentences (label: <code>negative</code>) -- A <strong>Python list</strong></p>

<ul>
<li><code>Feature-A</code>: <strong>String</strong></li>
<li><code>Feature-B</code>: <strong>String</strong></li>
<li><code>Feature-C</code>: <strong>String</strong></li>
</ul></li>
</ol>

<p>Then all the sentences were given as the input to <code>Word2Vec</code>.</p>

<pre><code>word2vec = Word2Vec(all_sentences, min_count=1)
</code></pre>

<p>I'll feed my CNN with the extracted features (which is the <code>vocabulary</code> in this case) and the <code>targets</code> of sentences. So, I need these labels of the sentences as well.</p>
","deep-learning, gensim, word2vec, one-hot-encoding, word-embedding","<p>Because the <code>Word2Vec</code> model doesn't retain any representation of the individual training texts, this is entirely a matter for you in your own Python code. </p>

<p>That doesn't seem like very much data. (It's rather tiny for typical <code>Word2Vec</code> purposes to have just a 5,260-word final vocabulary.) </p>

<p>Unless each text (aka 'sentence') is very long, you could even just use a Python dict where each key is the full string of a sentence, and the value is your flag. </p>

<p>But if, as is likely, your source data has some other unique identifier per text – like a unique database key, or even a line/row number in the canonical representation – you should use that identifier as a key instead. </p>

<p>In fact, if there's a canonical source ordering of your 6,925 texts, you could just have a list <code>flags</code> with 6,925 elements, in order, where each element is your flag. When you need to know the status of a text from position <code>n</code>, you just look at <code>flags[n]</code>. </p>

<p>(To make more specific suggestions, you'd need to add more details about the original source of the data, and exactly when/why you'd need to be checking this extra property later.)</p>
",0,2,443,2020-01-27 00:56:17,https://stackoverflow.com/questions/59924168/word2vec-how-can-i-store-and-retrieve-extra-information-regarding-each-instanc
How to find number of tokens in gensim model,"<p>This is the code for my model using <strong>Gensim.i</strong> run it and it returned a tuple. I wanna know that which one is the number of tokens?</p>

<pre><code>model = gensim.models.Word2Vec(mylist5,size=100, sg=0, window=5, alpha=0.05, min_count=5, workers=12, iter=20, cbow_mean=1, hs=0, negative=15)

model.train(mylist5, total_examples=len(mylist5), epochs=10)
</code></pre>

<p>The value that was returned by my model is: I need to know what is this?</p>

<pre><code> (167131589, 208757070)
</code></pre>

<p>I wanna know what is the number of tokens?</p>
","python-3.x, gensim","<p>Since you already passed in your <code>mylist5</code> corpus` when you instantiated the model,  it will have automatically done all steps to train the model with that data.</p>
<p>(You don't need to, and almost certainly should not, be calling <code>.train()</code> again. Typically <code>.train()</code> should only be called if you didn't provide any corpus at instnatiation. And in such a case, you'd then call both <code>.build_vocab()</code> and <code>.train()</code>.)</p>
<p>As noted by other answerers, the numbers reported by <code>.train()</code> are two tallies of the total tokens seen by the training process. (Most users won't actually need this info.)</p>
<p>If you want to know the number of <strong>unique tokens</strong> for which the model learned word-vectors, <code>len(model.wv)</code> is one way. (Before Gensim 4.0, <code>len(model.wv.vocab)</code> would have worked.)</p>
",3,0,926,2020-01-27 07:22:16,https://stackoverflow.com/questions/59926638/how-to-find-number-of-tokens-in-gensim-model
Why aren&#39;t all bigrams created in gensim&#39;s `Phrases` tool?,"<p>I have created a bigram model using gensim and the try to get the bigram sentences but it's not picking all bigram sentences why?</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models.phrases import Phrases, Phraser
phrases = Phrases(sentences, min_count=1, threshold=1)
bigram_model = Phraser(phrases)
sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
print(bigram_model[sent])
[u'the', u'mayor', u'of', u'new_york', u'was', u'there']
</code></pre>
<p>Can anyone explain how to get all bigrams.</p>
<h1>Why only 'new_york' not 'the_mayor' and others?</h1>
","python, nlp, gensim, n-gram, word-embedding","<p>The <code>Phrases</code> algorithm decides which word-pairs to promote to bigrams by a statistical analysis, which compares the base frequencies of each word individually with their frequency together. </p>

<p>So, some word-pairs will pass this test, and be combined, and others won't. If you're not getting the pairings you expect, then you can tune the algorithm somewhat using the <code>Phrases</code> class options, including <code>threshold</code>, <code>min_count</code>, and at least one alternate scoring-mechanism. </p>

<p>But, even maximally tuned, it won't typically create all the phrases that we, as natural-language speakers, would perceive – as it knows nothing of grammar, or the actually logically-related entities of the world. It only knows frequency statistics in the training text. </p>

<p>So there will be pairings it misses we'd see as natural &amp; desirable, and pairings it creates we'd see as illogical. Still, even with these unaesthetic pairings – creating text that doesn't look right to people – the transformed text can often work better in certain downstream classification or information-retrieval tasks. </p>

<p>If you really just wanted all possible bigrams, that'd be a much more simple text transformation, not requiring the multiple-passes &amp; internal statistics-collection of gensim's <code>Phrases</code>. </p>

<p>But also, if you do want to use gensim's <code>Phrases</code> technique, it will only perform well when it has a lot of training data.  Toy-sized texts of just a few dozen words –or even many tens-of-thousands of words – won't give good results. You'd want millions to tens-of-millions of training words to have some chance of it really detecting statistically-valid word-pairings.</p>
",2,1,1289,2020-02-07 07:30:54,https://stackoverflow.com/questions/60108919/why-arent-all-bigrams-created-in-gensims-phrases-tool
Transforming a gensim.interfaces.TransformedCorpus to a readable result,"<p>I am using the the Mallet LDA with gensims implemented wrapper.</p>

<p>Now I want to get the Topic distribution of several unseen documents, store it in a nested list and then print it out.</p>

<p>This is my code:</p>

<pre><code>other_texts = [
        ['wlan', 'usb', 'router'],
        ['auto', 'auto', 'auto'],
        ['human', 'system', 'computer']
 ]

corpus1 = [id2word.doc2bow(text) for text in other_texts]

to_pro = []
for t in corpus1:
    unseen_doc = corpus1
    vector = lda[unseen_doc] # get topic probability distribution for a document
    to_pro.append(vector)
</code></pre>

<p>If I try to print the list <code>vector</code> it yields this result:</p>

<pre><code>[&lt;gensim.interfaces.TransformedCorpus object at 0x0000024CC1DFC940&gt;, &lt;gensim.interfaces.TransformedCorpus object at 0x0000024CC1DFC320&gt;, &lt;gensim.interfaces.TransformedCorpus object at 0x0000024CC1DFC6A0&gt;]

</code></pre>

<p>I tried this code to print them out properly, but the probabilities of the topic distributuion are wrong:</p>

<pre><code>topic_dist = []
for line in to_pro:
    topic_dist += lda.get_document_topics(line)
td=[]
for topic in topic_dist:
    td.append(topic)
</code></pre>

<p>And I get this result:</p>

<pre><code>[[(0, 0.05458162849743133), (1, 0.05510823556400538), (2, 0.05603786367505091), (3, 0.05472256432318962), (4, 0.05471966342417517), (5, 0.05454446883678316), (6, 0.060267211268385176), (7, 0.05590590303517797), (8, 0.054558298009463865), (9, 0.0570497751708577), (10, 0.05586054626708894), (11, 0.05611284070096096), (12, 0.05483861615903838), (13, 0.054548627713420714), (14, 0.0548708631793431), (15, 0.055097199555668705), (16, 0.05572779508710042), (17, 0.05544789953285848)], [(0, 0.05457482739088479), (1, 0.05509130205455064), (2, 0.05599364448566309), (3, 0.05479472333893934), (4, 0.05489998490024729), (5, 0.054542940465732534), (6, 0.06014649090195501), (7, 0.0558787316629024), (8, 0.05455634249554292), (9, 0.05651159582517287), (10, 0.0558343047708517), (11, 0.05605027364084813), (12, 0.05483134591787102), (13, 0.054546952683828316), (14, 0.05488058477867337), (15, 0.0550725066190555), (16, 0.055951974201133244), (17, 0.055841473866147906)], [(0, 0.05457665942453363), (1, 0.055255130626316235), (2, 0.05616834056392741), (3, 0.05472749675259328), (4, 0.0547199851837743), (5, 0.054544546873748226), (6, 0.06037007389117332), (7, 0.05593838115178327), (8, 0.05456190582329174), (9, 0.056409168851414615), (10, 0.0559404965748031), (11, 0.05614914322415512), (12, 0.054842094317369555), (13, 0.054550171326841215), (14, 0.054870520851845996), (15, 0.05511732934346291), (16, 0.05579100118297473), (17, 0.05546755403599123)], [(0, 0.054581620307290336), (1, 0.05510823907508528), (2, 0.056037876384335425), (3, 0.05472256410518629), (4, 0.05471967034475046), (5, 0.05454446871605657), (6, 0.06026693118061518), (7, 0.05590622478877356), (8, 0.054558295773128575), (9, 0.05704995161755483), (10, 0.05586057502348091), (11, 0.056112803329985396), (12, 0.05483861481767718), (13, 0.05454862663175604), (14, 0.054870865577993026), (15, 0.055097113943380405), (16, 0.05572773919917307), (17, 0.055447819183777246)], [(0, 0.05457482815837349), (1, 0.05509132071436994), (2, 0.05599364089981504), (3, 0.05479471920764724), (4, 0.05489999995707833), (5, 0.05454293700828862), (6, 0.06014645177706313), (7, 0.05587868116251209), (8, 0.05455634846240247), (9, 0.056511585085478364), (10, 0.055834295810939794), (11, 0.056050296895854265), (12, 0.054831353686471636), (13, 0.05454695325610574), (14, 0.05488059866846103), (15, 0.055072528844072065), (16, 0.05595218064057245), (17, 0.05584127976449436)], [(0, 0.054576657976703774), (1, 0.05525504608539575), (2, 0.05616829811928526), (3, 0.05472749878845379), (4, 0.05471997497183866), (5, 0.054544547686709126), (6, 0.06037016659013718), (7, 0.05593821008515276), (8, 0.05456190840675052), (9, 0.05640917964821885), (10, 0.05594054039873076), (11, 0.05614912143569156), (12, 0.0548420823035294), (13, 0.054550172872614225), (14, 0.054870521717331436), (15, 0.055117319561282), (16, 0.05579110737872705), (17, 0.055467645973447846)], [(0, 0.054581639915369816), (1, 0.055108252268374285), (2, 0.056037916094392765), (3, 0.05472256597071497), (4, 0.05471966744573819), (5, 0.0545444687939403), (6, 0.06026693966026536), (7, 0.055906213964449725), (8, 0.05455829555351338), (9, 0.05704968653857304), (10, 0.0558606261827436), (11, 0.05611290790292455), (12, 0.05483860593828801), (13, 0.05454862649308445), (14, 0.05487085805236639), (15, 0.05509715099521129), (16, 0.05572773695595529), (17, 0.05544784127409454)], [(0, 0.05457482754746605), (1, 0.05509132328696252), (2, 0.055993666140583764), (3, 0.05479472184721206), (4, 0.05489996963702654), (5, 0.05454294168997213), (6, 0.060146365105445465), (7, 0.05587886571230439), (8, 0.05455633757025994), (9, 0.056511632004648656), (10, 0.055834239764847755), (11, 0.05605028881626678), (12, 0.054831347261978546), (13, 0.05454695137813789), (14, 0.05488060185684171), (15, 0.05507250450434276), (16, 0.055951827151308337), (17, 0.05584158872439472)], [(0, 0.05457665857245025), (1, 0.05525503335748317), (2, 0.05616811411295409), (3, 0.054727501563580076), (4, 0.054719978109952404), (5, 0.05454454660618627), (6, 0.060370135879343034), (7, 0.05593823717454384), (8, 0.05456190762146366), (9, 0.056409205316000424), (10, 0.05594060935464846), (11, 0.056149148701409454), (12, 0.05484207733245972), (13, 0.054550172010398135), (14, 0.05487051175914863), (15, 0.05511731933953272), (16, 0.055791267296383236), (17, 0.055467575892062346)]]
</code></pre>

<p>However printing one element from the list yields the correcr results:</p>

<pre><code>to_pro = []
for t in corpus1:
    unseen_doc = corpus1
    vector = lda[unseen_doc[1]] # specifying document at index 1
    to_pro.append(vector)
</code></pre>

<pre><code>[[(0, 0.052410901467505704), (1, 0.052410901467505704), (2, 0.052410901467505704), (3, 0.052410901467505704), (4, 0.052410901467505704), (5, 0.052410901467505704), (6, 0.052410901467505704), (7, 0.052410901467505704), (8, 0.052410901467505704), (9, 0.052410901467505704), (10, 0.052410901467505704), (11, 0.052410901467505704), (12, 0.052410901467505704), (13, 0.052410901467505704), (14, 0.10901467505240292), (15, 0.052410901467505704), (16, 0.052410901467505704), (17, 0.052410901467505704)], [(0, 0.052410901467505704), (1, 0.052410901467505704), (2, 0.052410901467505704), (3, 0.052410901467505704), (4, 0.052410901467505704), (5, 0.052410901467505704), (6, 0.052410901467505704), (7, 0.052410901467505704), (8, 0.052410901467505704), (9, 0.052410901467505704), (10, 0.052410901467505704), (11, 0.052410901467505704), (12, 0.052410901467505704), (13, 0.052410901467505704), (14, 0.10901467505240292), (15, 0.052410901467505704), (16, 0.052410901467505704), (17, 0.052410901467505704)], [(0, 0.052410901467505704), (1, 0.052410901467505704), (2, 0.052410901467505704), (3, 0.052410901467505704), (4, 0.052410901467505704), (5, 0.052410901467505704), (6, 0.052410901467505704), (7, 0.052410901467505704), (8, 0.052410901467505704), (9, 0.052410901467505704), (10, 0.052410901467505704), (11, 0.052410901467505704), (12, 0.052410901467505704), (13, 0.052410901467505704), (14, 0.10901467505240289), (15, 0.052410901467505704), (16, 0.052410901467505704), (17, 0.052410901467505704)]]
</code></pre>

<p>Another problem is, that for one document, the same distribution is printed 3 times. </p>

<p>I also looked into this answer: <a href=""https://stackoverflow.com/questions/45317151/gensim-interfaces-transformedcorpus-how-use"">gensim.interfaces.TransformedCorpus - How use?</a>, but it didnt help.</p>

<p>What am I doin wrong here? </p>
","python, gensim, lda","<p>I made a simple mistake:</p>

<p>The part which calculates the topic probabilities must be moved out of the loop:</p>

<pre><code>to_pro = []
unseen_doc = corpus1
vector = lda[unseen_doc]
for t in vector:
    print(t)
</code></pre>
",0,0,1099,2020-02-12 10:17:04,https://stackoverflow.com/questions/60185968/transforming-a-gensim-interfaces-transformedcorpus-to-a-readable-result
Gensim LDA Coherence Score Nan,"<p>I created a Gensim LDA Model as shown in this tutorial: <a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""noreferrer"">https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/</a></p>

<pre><code>lda_model = gensim.models.LdaMulticore(data_df['bow_corpus'], num_topics=10, id2word=dictionary, random_state=100, chunksize=100, passes=10, per_word_topics=True)
</code></pre>

<p>And it generates 10 topics with a log_perplexity of: </p>

<blockquote>
  <p>lda_model.log_perplexity(data_df['bow_corpus']) = -5.325966117835991</p>
</blockquote>

<p>But when I run the coherence model on it to calculate coherence score, like so:</p>

<pre><code>coherence_model_lda = CoherenceModel(model=lda_model, texts=data_df['bow_corpus'].tolist(), dictionary=dictionary, coherence='c_v')
with np.errstate(invalid='ignore'):
    lda_score = coherence_model_lda.get_coherence()
</code></pre>

<p>My LDA-Score is nan. What am I doing wrong here?</p>
","python, machine-learning, gensim, lda, topic-modeling","<p>Solved!
Coherence Model requires the original text, instead of the training corpus fed to LDA_Model - so when i ran this:</p>

<pre><code>coherence_model_lda = CoherenceModel(model=lda_model, texts=data_df['corpus'].tolist(), dictionary=dictionary, coherence='c_v')
with np.errstate(invalid='ignore'):
    lda_score = coherence_model_lda.get_coherence()
</code></pre>

<blockquote>
  <p>I got a coherence score of: 0.462</p>
</blockquote>

<p>Hope this helps someone else making the same mistake. Thanks!</p>
",12,8,6436,2020-02-16 08:03:17,https://stackoverflow.com/questions/60246570/gensim-lda-coherence-score-nan
Gensim&#39;s Doc2Vec - How to use pre-trained word2vec (word similarities),"<p>I don't have large corpus of data to train word similarities e.g. 'hot' is more similar to 'warm' than to 'cold'. However, I like to train doc2vec on a relatively small corpus ~100 docs so that it can classify my domain specific documents. </p>

<p><strong>To elaborate let me use this toy example.</strong> Assume I've only 4 training docs given  by 4 sentences - ""I love hot chocolate."", ""I hate hot chocolate."", ""I love hot tea."", and ""I love hot cake."".
Given a test document ""I adore hot chocolate"", I would expect, doc2vec will invariably return ""I love hot chocolate."" as the closest document. This expectation will be true if word2vec already supplies the knowledge that ""adore"" is very similar to ""love"". However, I'm getting most similar document as ""I hate hot chocolate"" -- which is a bizarre!!</p>

<p>Any suggestion on how to circumvent this, i.e. be able to use pre-trained word embeddings so that I don't need to venture into training ""adore"" is close to ""love"", ""hate"" is close to ""detest"", and so on.</p>

<p><strong>Code (Jupyter Nodebook. Python 3.7. Jensim 3.8.1)</strong></p>

<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
data = [""I love hot chocolate."",
        ""I hate hot chocolate"",
       ""I love hot tea."",
       ""I love hot cake.""]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]
print(tagged_data)
#Train and save
max_epochs = 10
vec_size = 5
alpha = 0.025


model = Doc2Vec(vector_size=vec_size, #it was size earlier
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    if epoch % 10 == 0:
        print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.epochs) #It was model.iter earlier
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

print(""Model Ready"")

test_sentence=""I adore hot chocolate""
test_data = word_tokenize(test_sentence.lower())
v1 = model.infer_vector(test_data)
#print(""V1_infer"", v1)

# to find most similar doc using tags
sims = model.docvecs.most_similar([v1])
print(""\nTest: %s\n"" %(test_sentence))
for indx, score in sims:
    print(""\t(score: %.4f) %s"" %(score, data[int(indx)]))
</code></pre>
","python, nlp, gensim, doc2vec","<p>Just ~100 documents is way too small to meaningfully train a <code>Doc2Vec</code> (or <code>Word2Vec</code>) model. Published <code>Doc2Vec</code> work tends to use tens-of-thousands to millions of documents.</p>

<p>To the extent you may be able to get slightly meaningful results from smaller datasets, you'll usually need to reduce the vector-sizes a lot – to far smaller than the number of words/examples – and increase the training epochs. (Your toy data has 4 texts &amp; 6 unique words. Even to get 5-dimensional vectors, you probably want something like 5^2 constrasting documents.)</p>

<p>Also, gensim's <code>Doc2Vec</code> doesn't offer any official option to import word-vectors from elsewhere. The internal <code>Doc2Vec</code> training is not a process where word-vectors are trained 1st, then doc-vectors calculated. Rather, doc-vectors &amp; word-vectors are trained in a simultaneous process, gradually improving together. (Some modes, like the fast &amp; often highly effective <code>DBOW</code> that can be enabled with <code>dm=0</code>, don't create or use word-vectors at all.)</p>

<p>There's not really anything bizarre about your 4-sentence results, when looking at the data as if we were the <code>Doc2Vec</code> or <code>Word2Vec</code> algorithms, which have no prior knowledge about words, only what's in the training data. In your training data, the token <code>'love'</code> and the token <code>'hate'</code> are used in nearly exactly the same way, with the same surrounding words. Only by seeing many subtly varied alternative uses of words, alongside many contrasting surrounding words, can these ""dense embedding"" models move the word-vectors to useful relative positions, where they are closer to related words &amp; farther from other words. (And, since you've provided no training data with the token <code>'adore'</code>, the model knows nothing about that word – and if it's provided inside a test document, as if to the model's <code>infer_vector()</code> method, it will be ignored. So the test document it 'sees' is only the known words <code>['i', 'hot', 'chocolate']</code>.)</p>

<p>But also, even if you did manage to train on a larger dataset, or somehow inject the knowledge from other word-vectors that <code>'love'</code> and <code>'adore'</code> are somewhat similar,  it's important to note that antonyms are typically quite similar in sets of word-vectors, too – as they are used in the same contexts, and often syntactically interchangeable, and of the same general category. These models often <em>aren't</em> very good at detecting the flip-in-human-perceived meaning from the swapping of a word for its antonym (or insertion of a single 'not' or other reversing-intent words). </p>

<p>Ultimately if you want to use gensim's <code>Doc2Vec</code>, you should train it with far more data. (If you were willing to grab some other pre-trainined word-vectors, why not grab some other source of somewhat-similar bulk sentences? The effect of using data that isn't exactly like your actual problem will be similar whether you leverage that outside data via bulk text or a pre-trained model.)</p>

<p>Finally: it's a bad, error-prone pattern to be calling <code>train()</code> more than once in your own loop, with your own <code>alpha</code> adjustments. You can just call it once, with the right number of <code>epochs</code>, and the model will perform the multiple training passes &amp; manage the internal <code>alpha</code> smoothly over the right number of epochs.</p>
",1,2,2043,2020-02-18 17:47:38,https://stackoverflow.com/questions/60286735/gensims-doc2vec-how-to-use-pre-trained-word2vec-word-similarities
TypeError during extracting bigrams with Gensim(Python),"<p>I want to extract and print bigrams using Gensim. For this purpose I used that code in GoogleColab:</p>

<pre><code>import gensim.downloader as api
from gensim.models import Word2Vec
from gensim.corpora import WikiCorpus, Dictionary
from gensim.models import Phrases
from gensim.models.phrases import Phraser
from collections import Counter

data = api.load(""text8"") # wikipedia corpus
bigram = Phrases(data, min_count=3, threshold=10)


cntr = Counter()
for key in bigram.vocab.keys():
  if len(key.split('_')) &gt; 1:
    cntr[key] += bigram.vocab[key]

for key, counts in cntr.most_common(50):
  print(key, "" - "", counts)
</code></pre>

<p>But there's an error: </p>

<p><a href=""https://i.sstatic.net/eIjwF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eIjwF.png"" alt=""TypeError""></a></p>

<p>Then I tried this:</p>

<pre><code>cntr = Counter()
for key in bigram.vocab.keys():
  if len(key.split(b'_')) &gt; 1:
    cntr[key] += bigram.vocab[key]

for key, counts in cntr.most_common(50):
  print(key, "" - "", counts)
</code></pre>

<p>And then:</p>

<p><a href=""https://i.sstatic.net/ir6eB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ir6eB.png"" alt=""again""></a></p>

<p>What is wrong?</p>
","python, machine-learning, nlp, gensim","<pre><code> bigram_token  = list(bigram.vocab.keys())
 type(bigram_token[0])

 #op
 bytes
</code></pre>

<p>convert this into string and it will solve problem, in your code just while splitting do</p>

<pre><code>cntr = Counter()
for key in bigram.vocab.keys():
    if len(key.decode('utf-8').split(b'_')) &gt; 1: # here added .decode('utf-8')
       cntr[key] += bigram.vocab[key]
</code></pre>
",0,0,92,2020-02-19 03:51:08,https://stackoverflow.com/questions/60292744/typeerror-during-extracting-bigrams-with-gensimpython
No module named &#39;gensim&#39; but already installed it,"<p>i'm having this error problem, i have ran this script in jupyter notebook in base (root) environment, the log said that gensim library has been installed and i have run the command <strong>!pip install gensim</strong> before i import it, but it still can not be imported, and the error said <strong>ModuleNotFoundError: No module named 'gensim'</strong></p>

<pre><code>!pip install gensim
import gensim
from gensim.models import KeyedVectors
model = KeyedVectors.load('model_fasttext2.vec')
model.vector_size
------------------------------------------------------------------------
Requirement already satisfied: gensim in c:\users\ip-03\anaconda3\lib\site-packages (3.8.1)
Requirement already satisfied: scipy&gt;=0.18.1 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.4.1)
Requirement already satisfied: six&gt;=1.5.0 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.14.0)
Requirement already satisfied: smart-open&gt;=1.8.1 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.9.0)
Requirement already satisfied: numpy&gt;=1.11.3 in c:\users\ip-03\anaconda3\lib\site-packages (from gensim) (1.18.1)
Requirement already satisfied: boto&gt;=2.32 in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (2.49.0)
Requirement already satisfied: boto3 in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (1.12.3)
Requirement already satisfied: bz2file in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (0.98)
Requirement already satisfied: requests in c:\users\ip-03\anaconda3\lib\site-packages (from smart-open&gt;=1.8.1-&gt;gensim) (2.22.0)
Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.3.3)
Requirement already satisfied: botocore&lt;1.16.0,&gt;=1.15.3 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (1.15.3)
Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in c:\users\ip-03\anaconda3\lib\site-packages (from boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.9.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2019.11.28)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (3.0.4)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (1.25.8)
Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in c:\users\ip-03\anaconda3\lib\site-packages (from requests-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2.8)
Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in c:\users\ip-03\anaconda3\lib\site-packages (from botocore&lt;1.16.0,&gt;=1.15.3-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (2.8.1)
Requirement already satisfied: docutils&lt;0.16,&gt;=0.10 in c:\users\ip-03\anaconda3\lib\site-packages (from botocore&lt;1.16.0,&gt;=1.15.3-&gt;boto3-&gt;smart-open&gt;=1.8.1-&gt;gensim) (0.15.2)
</code></pre>

<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-10-ee4a48d372cd&gt; in &lt;module&gt;
      1 get_ipython().system('pip install gensim')
----&gt; 2 import gensim
      3 from gensim.models import KeyedVectors
      4 model = KeyedVectors.load('model_fasttext2.vec')
      5 model.vector_size

ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>Is there anyone who can help this problem? i will really appreciate your help, it will help my thesis work, thank you for your attention </p>
","python, machine-learning, jupyter-notebook, gensim, word-embedding","<p>It may be that your jupyter lab maybe running the base kernel and not the kernel of the virtual environment.</p>

<p>Check by doing the following:</p>

<pre class=""lang-py prettyprint-override""><code>import sys
sys.executable
</code></pre>

<p>into my notebook and got the result</p>

<pre><code>'/anaconda3/bin/python'
</code></pre>

<p>If you get the above instead of the below then that means you're using the wrong kernel.</p>

<pre><code>'/anaconda3/envs/myenv/bin/python'
</code></pre>

<p>You can solve it by creating a new iPython kernel for your new environment. Read more <a href=""https://ipython.readthedocs.io/en/stable/install/kernel_install.html#kernels-for-different-environments"" rel=""nofollow noreferrer"">here</a>.</p>

<pre class=""lang-sh prettyprint-override""><code>conda install -n myenv ipython
conda activate myenv
python -m ipykernel install --user --name myenv --display-name ""Python (myenv)""
```Then, to run Jupyter Lab in the new environment:
</code></pre>
",1,0,4431,2020-02-20 11:05:14,https://stackoverflow.com/questions/60318511/no-module-named-gensim-but-already-installed-it
Cannot load model with gensim FastText,"<p>I`ve faced with trouble to load model using gensim.model.FastText.load().</p>

<p>Here is some code and error which I get:</p>

<pre><code>from gensim.models import FastText

class FastTextModel:
    def __init__(self, model_path, dim=300):
        self.dim = dim
        self.model = FastText.load(model_path).wv

...

class GeneralModel:
    def __init__(self, config):
        if config[""type""] == ""fasttext"":
            # path - path to model
            # dim -  dimension, here 300
            self.model = FastTextModel(config[""path""], config[""dim""])
</code></pre>

<pre><code>  File ""/project/preprocessing/pipeline.py"", line 15, in __init__
    self.model_ru = GeneralModel(config[""models""][""ru""])
  File ""/project/models/nlp_models.py"", line 101, in __init__
    self.model = FastTextModel(config[""path""], config[""dim""])
  File ""/project/models/nlp_models.py"", line 16, in __init__
    self.model = FastText.load(model_path).wv
  File ""/usr/local/lib64/python3.6/site-packages/gensim/models/fasttext.py"", line 936, in load
    model = super(FastText, cls).load(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/models/base_any2vec.py"", line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/models/base_any2vec.py"", line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/utils.py"", line 423, in load
    obj._load_specials(fname, mmap, compress, subname)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/utils.py"", line 453, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)
  File ""/usr/local/lib64/python3.6/site-packages/gensim/utils.py"", line 464, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)
  File ""/usr/local/lib64/python3.6/site-packages/numpy/lib/npyio.py"", line 447, in load
    pickle_kwargs=pickle_kwargs)
  File ""/usr/local/lib64/python3.6/site-packages/numpy/lib/format.py"", line 738, in read_array
    array.shape = shape
ValueError: cannot reshape array of size 67239904 into shape (445446,300)
</code></pre>

<p>I've downloaded models from Google Drive folder, and though that it can somehow damage .npy files (as they are quite big), so I've downloaded each file (there 7 files for that model) separately, but this didn`t help me.</p>

<p>Also, I read that sometimes it can be caused because of bad unzipping in the 'load' method, but I'm passing already unzipped files into it, so this also don`t work for me.</p>

<p>Will be grateful for the help!</p>
","python, numpy, gensim","<p>Where did the model(s) originate? The gensim <code>FastText.load()</code> method is only for FastText models created &amp; saved from gensim (via its <code>.save()</code> method). Such models use a combination of Python-pickling &amp; sibling <code>.npy</code> raw-array files (to store large arrays) which must be kept together.</p>

<p>Models saved from Facebook's original FastText implementation are a different format, for which you'd use the <code>load_facebook_model()</code> utility function:</p>

<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model</a></p>

<p>If you only need the vectors – as seems to be the case from your immediate use of only the <code>.wv</code> property – you can also use the <code>load_facebook_vectors()</code> function: </p>

<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors</a></p>

<p>(Also, not sure why you've wrapped the loaded model in your own <code>FastTextModel</code> class which allows the caller to specify a dimensionality. You can't change the dimensionality of a loaded model, so it'd make more sense to just read the existing <code>vector_size</code> from the model, rather than specify it outside.)</p>
",0,0,1195,2020-02-21 18:17:58,https://stackoverflow.com/questions/60344269/cannot-load-model-with-gensim-fasttext
Python Gensim LDA Model show_topics funciton,"<p>I am training a LDA Model using Gensim:</p>

<pre><code>dictionary = corpora.Dictionary(section_2_sentence_df['Tokenized_Sentence'].tolist())
dictionary.filter_extremes(no_below=20, no_above=0.7)
corpus = [dictionary.doc2bow(text) for text in (section_2_sentence_df['Tokenized_Sentence'].tolist())]

num_topics = 15
passes = 200
chunksize = 100
lda_sentence_model = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=num_topics, 
                                                              id2word=dictionary, 
                                                              passes=passes, 
                                                              chunksize=chunksize,
                                                              random_state=100,
                                                              workers = 3)
</code></pre>

<p>After training i need the topics for further analysis. Unfortunately the show_topics function only returns <strong>10 topics</strong>. I expected the defined number of <strong>15 topics</strong>. Does anyone know if that is on purpose or an error that can be solved?</p>

<pre><code>print(len(lda_sentence_model.show_topics(formatted=False)))
</code></pre>
","python, gensim, lda","<p>According to the gensim documentation for the <code>.show_topics()</code> method, its default <code>num_topics</code> parameter value (""Number of topics to be returned"") is 10:</p>

<p><a href=""https://radimrehurek.com/gensim/models/ldamulticore.html#gensim.models.ldamulticore.LdaMulticore.show_topics"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/ldamulticore.html#gensim.models.ldamulticore.LdaMulticore.show_topics</a></p>

<p>If you want it to return more than 10, supply your preferred non-default value for that method's <code>num_topics</code> parameter. For example:</p>

<pre class=""lang-py prettyprint-override""><code>len(lda_sentence_model.show_topics(formatted=False, num_topics=15))
</code></pre>
",1,0,2576,2020-02-26 18:57:43,https://stackoverflow.com/questions/60420718/python-gensim-lda-model-show-topics-funciton
Does WikiCorpus from gensim library works on Arabic Wikipedia dump?,"<p>I see a code which uses Wikicorpus on an Arabic Wikipedia dump, and I know that the process will take a long time to execute, I also searched around about the warning that I get when executing it which says: </p>

<blockquote>
  <p>(UserWarning: detected Windows; aliasing chunkize to chunkize_serial<br>
  warnings.warn(""detected Windows; aliasing chunkize to
  chunkize_serial""))</p>
</blockquote>

<p>and answers said that it's ok, nothing serious, it's just a warning. 
But after waiting about 3 days without any response! I start wondering whether is it truly work on the Arabic dump file, or I have to do certain kind of pre-processing before passing the Arabic dump file to the Wikicorpus object?
the data size is about 989.6 MB.
and I surround the WikiCorpus code line with two print commands, to know when it started and when it finished executing, like this:</p>

<pre><code>print('start WikiCorpus')
wiki = WikiCorpus(self.in_f)
print('finish WikiCorpus')
</code></pre>

<p>where the self.in_f is the Arabic Wikipedia dump like this: (/the path where the file located/arwiki-20200201-pages-articles.xml.bz2), but never reached the second print command during the runtime.</p>
","python, gensim","<p>It should work, especially if Arabic has clear word-delimiters (like spaces between words). </p>

<p>However, lots of things are harder on Windows, given that <code>gensim</code> &amp; most related Python data-science libraries get more development/testing/use elsewhere, &amp; there are some Windows-specific oddities with multiprocessing. If you have the option of working on another OS, that can make things easier. </p>

<p>There was another recent question describing a similar problem with an <code>en</code> dump &amp; <code>WikiCorpus</code> – there are ideas of things to check in <a href=""https://stackoverflow.com/a/60289555/130288"">my answer there</a>, though it's unclear if the asker ever resolved the problem.</p>

<p>Also, when using code that relies on Python <code>multiprocessing</code> in Windows, it may be especially necessary to set your code off in a 'main' block that's won't be re-run if your file is re-imported by other processes, and call a Windows-specific <code>freeze_support()</code> function. See <a href=""https://groups.google.com/d/msg/gensim/-gMNdkujR48/i4Dn1_bjBQAJ"" rel=""nofollow noreferrer"">some recent discussion of a related matter on the gensim project list</a>. </p>
",0,0,1203,2020-02-28 12:21:32,https://stackoverflow.com/questions/60451614/does-wikicorpus-from-gensim-library-works-on-arabic-wikipedia-dump
Draw 3D Plot for Gensim model,"<p>I have trained my model using Gensim. I draw a 2D plot using PCA but it is not clear too much. I wanna change it to 3D  with capable of zooming .my result is so dense.</p>

<pre><code>from sklearn.decomposition import PCA
from matplotlib import pyplot
X=model[model.wv.vocab]
pca=PCA(n_components=2)
result=pca.fit_transform(X)
pyplot.scatter(result[:,0],result[:,1])
word=list(model.wv.most_similar('eden_lake'))
for i, word in enumerate(words):
  pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
pyplot.show()
</code></pre>

<p>And the result:
<a href=""https://i.sstatic.net/eUFwI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eUFwI.jpg"" alt=""enter image description here""></a></p>

<p>it possible to do that?</p>
","python-3.x, pca, gensim","<p>The following function uses t-SNE instead of PCA for dimension reduction, but will generate a plot in two, three or both two and three dimensions (using subplots). Furthermore, it will color the topics for you so it's easier to distinguish them. Adding <code>%matplotlib notebook</code> to the start of a Jupyter notebook environment from <a href=""https://www.anaconda.com/"" rel=""nofollow noreferrer"">anaconda</a> will allow a 3d plot to be rotated and a 2d plot to be zoomed (don't do both versions at the same time with <code>%matplotlib notebook</code>).</p>
<p>The function is very long, with most of the code being for plot formatting, but produces a professional output.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
import seaborn as sns

from gensim.models import LdaModel
from gensim import corpora
from sklearn.manifold import TSNE
# %matplotlib notebook # if in Jupyter for rotating and zooming
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def LDA_tSNE_topics_vis(dimension='both',
                        corpus=None, 
                        num_topics=10,
                        remove_3d_outliers=False,
                        save_png=False):
    &quot;&quot;&quot;
    Returns the outputs of an LDA model plotted using t-SNE (t-distributed Stochastic Neighbor Embedding)

    Note: t-SNE reduces the dimensionality of a space such that similar points will be closer and dissimilar points farther

    Parameters
    ----------
        dimension : str (default=both)
            The dimension that t-SNE should reduce the data to for visualization
            Options: 2d, 3d, and both (a plot with two subplots)

        corpus : list, list of lists
            The tokenized and cleaned text corpus over which analysis should be done

        num_topics : int (default=10)
            The number of categories for LDA based approaches

        remove_3d_outliers : bool (default=False)
            Whether to remove outliers from a 3d plot

        save_png : bool (default=False)
            Whether to save the figure as a png

    Returns
    -------
        A t-SNE lower dimensional representation of an LDA model's topics and their constituent members
    &quot;&quot;&quot;
    dirichlet_dict = corpora.Dictionary(corpus)
    bow_corpus = [dirichlet_dict.doc2bow(text) for text in corpus]

    dirichlet_model = LdaModel(corpus=bow_corpus,
                               id2word=dirichlet_dict,
                               num_topics=num_topics,
                               update_every=1,
                               chunksize=len(bow_corpus),
                               passes=10,
                               alpha='auto',
                               random_state=42) # set for testing

    df_topic_coherences = pd.DataFrame(columns = ['topic_{}'.format(i) for i in range(num_topics)])

    for i in range(len(bow_corpus)):
        df_topic_coherences.loc[i] = [0] * num_topics
        
        output = dirichlet_model.__getitem__(bow=bow_corpus[i], eps=0)
    
        for j in range(len(output)):
            topic_num = output[j][0]
            coherence = output[j][1]
            df_topic_coherences.iloc[i, topic_num] = coherence

    for i in range(num_topics):
        df_topic_coherences.iloc[:, i] = df_topic_coherences.iloc[:, i].astype('float64', copy=False)

    df_topic_coherences['main_topic'] = df_topic_coherences.iloc[:, :num_topics].idxmax(axis=1)

    if num_topics &gt; 10:
        # cubehelix better for more than 10 colors
        colors = sns.color_palette(&quot;cubehelix&quot;, num_topics)
    else:
        # The default sns color palette
        colors = sns.color_palette('deep', num_topics)

    tsne_2 = None
    tsne_3 = None
    if dimension == 'both':
        tsne_2 = TSNE(n_components=2, perplexity=40, n_iter=300)
        tsne_3 = TSNE(n_components=3, perplexity=40, n_iter=300)
    elif dimension == '2d':
        tsne_2 = TSNE(n_components=2, perplexity=40, n_iter=300)
    elif dimension == '3d':
        tsne_3 = TSNE(n_components=3, perplexity=40, n_iter=300)
    else:
        ValueError(&quot;An invalid value has been passed to the 'dimension' argument - choose from 2d, 3d, or both.&quot;)

    if tsne_2 is not None:
        tsne_results_2 = tsne_2.fit_transform(df_topic_coherences.iloc[:, :num_topics])
        
        df_tsne_2 = pd.DataFrame()
        df_tsne_2['tsne-2d-d1'] = tsne_results_2[:,0]
        df_tsne_2['tsne-2d-d2'] = tsne_results_2[:,1]
        df_tsne_2['main_topic'] = df_topic_coherences.iloc[:, num_topics]
        df_tsne_2['color'] = [colors[int(t.split('_')[1])] for t in df_tsne_2['main_topic']]

        df_tsne_2['topic_num'] = [int(i.split('_')[1]) for i in df_tsne_2['main_topic']]
        df_tsne_2 = df_tsne_2.sort_values(['topic_num'], ascending = True).drop('topic_num', axis=1)
    
    if tsne_3 is not None:
        colors = [c for c in sns.color_palette()]

        tsne_results_3 = tsne_3.fit_transform(df_topic_coherences.iloc[:, :num_topics])
        
        df_tsne_3 = pd.DataFrame()
        df_tsne_3['tsne-3d-d1'] = tsne_results_3[:,0]
        df_tsne_3['tsne-3d-d2'] = tsne_results_3[:,1]
        df_tsne_3['tsne-3d-d3'] = tsne_results_3[:,2]
        df_tsne_3['main_topic'] = df_topic_coherences.iloc[:, num_topics]
        df_tsne_3['color'] = [colors[int(t.split('_')[1])] for t in df_tsne_3['main_topic']]

        df_tsne_3['topic_num'] = [int(i.split('_')[1]) for i in df_tsne_3['main_topic']]
        df_tsne_3 = df_tsne_3.sort_values(['topic_num'], ascending = True).drop('topic_num', axis=1)

        if remove_3d_outliers:
            # Remove those rows with values that are more than three standard deviations from the column mean
            for col in ['tsne-3d-d1', 'tsne-3d-d2', 'tsne-3d-d3']:
                df_tsne_3 = df_tsne_3[np.abs(df_tsne_3[col] - df_tsne_3[col].mean()) &lt;= (3 * df_tsne_3[col].std())]

    if tsne_2 is not None and tsne_3 is not None:
        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, # pylint: disable=unused-variable
                                       figsize=(20,10))
        ax1.axis('off')

    else:
        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,10))

    if tsne_2 is not None and tsne_3 is not None:
        # Plot tsne_2, with tsne_3 being added later
        ax1 = sns.scatterplot(data=df_tsne_2, x=&quot;tsne-2d-d1&quot;, y=&quot;tsne-2d-d2&quot;,
                              hue=df_topic_coherences.iloc[:, num_topics], alpha=0.3)
        
        light_grey_tup = (242/256, 242/256, 242/256)
        ax1.set_facecolor(light_grey_tup)
        ax1.axes.set_title('t-SNE 2-Dimensional Representation', fontsize=25)
        ax1.set_xlabel('tsne-d1', fontsize=20)
        ax1.set_ylabel('tsne-d2', fontsize=20)

        handles, labels = ax1.get_legend_handles_labels()
        legend_order = list(np.argsort([i.split('_')[1] for i in labels]))
        ax1.legend([handles[i] for i in legend_order], [labels[i] for i in legend_order], 
                   facecolor=light_grey_tup)

    elif tsne_2 is not None:
        # Plot just tsne_2
        ax = sns.scatterplot(data=df_tsne_2, x=&quot;tsne-2d-d1&quot;, y=&quot;tsne-2d-d2&quot;,
                             hue=df_topic_coherences.iloc[:, num_topics], alpha=0.3)

        ax.set_facecolor(light_grey_tup)
        ax.axes.set_title('t-SNE 2-Dimensional Representation', fontsize=25)
        ax.set_xlabel('tsne-d1', fontsize=20)
        ax.set_ylabel('tsne-d2', fontsize=20)

        handles, labels = ax.get_legend_handles_labels()
        legend_order = list(np.argsort([i.split('_')[1] for i in labels]))
        ax.legend([handles[i] for i in legend_order], [labels[i] for i in legend_order], 
                  facecolor=light_grey_tup)

    if tsne_2 is not None and tsne_3 is not None:
        # tsne_2 has been plotted, so add tsne_3
        ax2 = fig.add_subplot(121, projection='3d')
        ax2.scatter(xs=df_tsne_3['tsne-3d-d1'], 
                    ys=df_tsne_3['tsne-3d-d2'], 
                    zs=df_tsne_3['tsne-3d-d3'],  
                    c=df_tsne_3['color'],
                    alpha=0.3)

        ax2.set_facecolor('white')
        ax2.axes.set_title('t-SNE 3-Dimensional Representation', fontsize=25)
        ax2.set_xlabel('tsne-d1', fontsize=20)
        ax2.set_ylabel('tsne-d2', fontsize=20)
        ax2.set_zlabel('tsne-d3', fontsize=20)

        with plt.rc_context({&quot;lines.markeredgewidth&quot; : 0}):
            # Add handles via blank lines and order their colors to match tsne_2
            proxy_handles = [Line2D([0], [0], linestyle=&quot;none&quot;, marker='o', markersize=8,
                                    markerfacecolor=colors[i]) for i in legend_order]
            ax2.legend(proxy_handles, ['topic_{}'.format(i) for i in range(num_topics)], 
                       loc='upper left', facecolor=(light_grey_tup))

    elif tsne_3 is not None:
        # Plot just tsne_3
        ax.axis('off')
        ax.set_facecolor('white')
        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(xs=df_tsne_3['tsne-3d-d1'], 
                   ys=df_tsne_3['tsne-3d-d2'], 
                   zs=df_tsne_3['tsne-3d-d3'],  
                   c=df_tsne_3['color'],
                   alpha=0.3)

        ax.set_facecolor('white')
        ax.axes.set_title('t-SNE 3-Dimensional Representation', fontsize=25)
        ax.set_xlabel('tsne-d1', fontsize=20)
        ax.set_ylabel('tsne-d2', fontsize=20)
        ax.set_zlabel('tsne-d3', fontsize=20)

        with plt.rc_context({&quot;lines.markeredgewidth&quot; : 0}):
            # Add handles via blank lines
            proxy_handles = [Line2D([0], [0], linestyle=&quot;none&quot;, marker='o', markersize=8,
                                    markerfacecolor=colors[i]) for i in range(len(colors))]
            ax.legend(proxy_handles, ['topic_{}'.format(i) for i in range(num_topics)], 
                      loc='upper left', facecolor=light_grey_tup)

    if save_png:
        plt.savefig('LDA_tSNE_{}.png'.format(time.strftime(&quot;%Y%m%d-%H%M%S&quot;)), bbox_inches='tight', dpi=500)

    plt.show()
</code></pre>
<p>An example plot for both 2d and 3d (with outliers removed) representations of a 10 topic <a href=""https://github.com/RaRe-Technologies/gensim"" rel=""nofollow noreferrer"">gensim</a> LDA model on subplots would be:</p>
<p><a href=""https://i.sstatic.net/A18ih.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A18ih.jpg"" alt=""LDA t-SNE Dimensional Reduction Plots"" /></a></p>
",1,1,1041,2020-02-28 15:05:28,https://stackoverflow.com/questions/60454355/draw-3d-plot-for-gensim-model
Unable to Load Model Trained in Gensim- pickle-related error,"<p>When attempting to load a word2vec model trained by Gensim on a Windows machine, I receive the following error:</p>

<p><code>AttributeError: Can't get attribute 'EpochProgress' on &lt;module '__main__'&gt;</code></p>

<p>I've successfully trained numerous models with Gensim in the past on this system. The only variation being this time I split the <code>model.build_vocab()</code> and <code>model.train()</code> phases, adding in saves &amp; time hacks for each epoch. I also used a different iterator for the vocab build and the training phrases, but on the same dataset with the same tokenization pipeline.</p>

<p>Here is how I did the epoch progress tracking/saving:</p>

<pre class=""lang-py prettyprint-override""><code>class EpochProgress(CallbackAny2Vec):
    '''saves the model after each epoch'''

    def __init__(self, path_prefix):
        self.path_prefix = path_prefix
        self.epoch = 0
        self.start_time = time.time()

    def on_epoch_begin(self, model):
        print(""epoch #{} started"".format(self.epoch))

    def on_epoch_end(self, model):
        print(""epoch #{} completed"".format(self.epoch))
        passed = (time.time() - self.start_time)/60/60 # elapsed time since start in HOURS
        print(""{} hours have passed"".format(str(passed)))
        output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))
        model.save(output_path)
        print(""model saved at: {}"".format(output_path))
        self.epoch +=1
</code></pre>

<p><code>epoch_progress = EpochProgress('E:/jade_prism/embeddings/phrase-embed-over- time/mega_WOS_word2vec/w2v_models/in_progress/')</code></p>

<p>I then load the baseline model with the vocab build and set a few parameters:</p>

<pre class=""lang-py prettyprint-override""><code>model = gensim.models.Word2Vec.load(baseline_models_directory+chosen_name)
model.window = window
model.size = size
model.workers = workers 
model.callbacks = [epoch_progress]
</code></pre>

<p>Then I do the training like this:</p>

<p><code>model.train(corpus, total_examples=model.corpus_count, epochs=epochs)</code></p>

<p>And finally, save the end product like this: </p>

<p><code>model.save('E:/w2v_models/trained/{}'.format(new_model_filename))</code></p>

<p>Training appeared to work properly, and model saved as expected- unfortunately now I can't load it.</p>

<p>Here is the full Debug readout:</p>

<pre><code>&gt; AttributeError                            Traceback (most recent call
&gt; last)
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\word2vec.py
&gt; in load(cls, *args, **kwargs)    1329         try:
&gt; -&gt; 1330             model = super(Word2Vec, cls).load(*args, **kwargs)    1331 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\base_any2vec.py
&gt; in load(cls, *args, **kwargs)    1243         """"""
&gt; -&gt; 1244         model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)    1245         if not hasattr(model,
&gt; 'ns_exponent'):
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\base_any2vec.py
&gt; in load(cls, fname_or_handle, **kwargs)
&gt;     602         """"""
&gt; --&gt; 603         return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
&gt;     604 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\utils.py in
&gt; load(cls, fname, mmap)
&gt;     425 
&gt; --&gt; 426         obj = unpickle(fname)
&gt;     427         obj._load_specials(fname, mmap, compress, subname)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\utils.py in
&gt; unpickle(fname)    1383         if sys.version_info &gt; (3, 0):
&gt; -&gt; 1384             return _pickle.load(f, encoding='latin1')    1385         else:
&gt; 
&gt; AttributeError: Can't get attribute 'EpochProgress' on &lt;module
&gt; '__main__'&gt;
&gt; 
&gt; During handling of the above exception, another exception occurred:
&gt; 
&gt; AttributeError                            Traceback (most recent call
&gt; last) &lt;ipython-input-4-0206f9f8f3ad&gt; in &lt;module&gt;
&gt;       3 
&gt;       4 # Load the model based onthe model name
&gt; ----&gt; 5 model = gensim.models.Word2Vec.load(model_name)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\word2vec.py
&gt; in load(cls, *args, **kwargs)    1339             logger.info('Model
&gt; saved using code from earlier Gensim Version. Re-loading old model in
&gt; a compatible way.')    1340             from
&gt; gensim.models.deprecated.word2vec import load_old_word2vec
&gt; -&gt; 1341             return load_old_word2vec(*args, **kwargs)    1342     1343 
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\word2vec.py
&gt; in load_old_word2vec(*args, **kwargs)
&gt;     170 
&gt;     171 def load_old_word2vec(*args, **kwargs):
&gt; --&gt; 172     old_model = Word2Vec.load(*args, **kwargs)
&gt;     173     vector_size = getattr(old_model, 'vector_size', old_model.layer1_size)
&gt;     174     params = {
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\word2vec.py
&gt; in load(cls, *args, **kwargs)    1639     @classmethod    1640     def
&gt; load(cls, *args, **kwargs):
&gt; -&gt; 1641         model = super(Word2Vec, cls).load(*args, **kwargs)    1642         # update older models    1643         if hasattr(model,
&gt; 'table'):
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\old_saveload.py
&gt; in load(cls, fname, mmap)
&gt;      85         compress, subname = SaveLoad._adapt_by_suffix(fname)
&gt;      86 
&gt; ---&gt; 87         obj = unpickle(fname)
&gt;      88         obj._load_specials(fname, mmap, compress, subname)
&gt;      89         logger.info(""loaded %s"", fname)
&gt; 
&gt; C:\anaconda\envs\mega_WOS\lib\site-packages\gensim\models\deprecated\old_saveload.py
&gt; in unpickle(fname)
&gt;     377             b'gensim.models.wrappers.fasttext', b'gensim.models.deprecated.fasttext_wrapper')
&gt;     378         if sys.version_info &gt; (3, 0):
&gt; --&gt; 379             return _pickle.loads(file_bytes, encoding='latin1')
&gt;     380         else:
&gt;     381             return _pickle.loads(file_bytes)
&gt; 
&gt; AttributeError: Can't get attribute 'EpochProgress' on module '__main__'\&gt;
</code></pre>
","python-3.x, nlp, gensim","<p>Python pickling/unpickling can run into problems when saving code blocks, or classes/instances-of-classes that you defined before saving, but may not be available at load-time. (Especially, anonymous or global-scope types not imported from explicit paths.)</p>

<p>It's a known hiccup with gensim model-saving, and future versions will likely avoid storing such callback code inside models at all. (Instead, you'll have to specify callbacks each time you execute a method using them, and they'll only remain effective for that one call.)</p>

<p>See <a href=""https://github.com/RaRe-Technologies/gensim/issues/2136"" rel=""nofollow noreferrer"">gensim project issue #2136</a> for more details, including a workaround that seems to have helped others re-load their models: ensuring the same <code>EpochProgress</code> class is defined/imported where the load is attempted. </p>
",0,0,1076,2020-03-02 14:35:01,https://stackoverflow.com/questions/60491035/unable-to-load-model-trained-in-gensim-pickle-related-error
Cannot reproduce pre-trained word vectors from its vector_ngrams,"<p>Just curiosity, but I was debugging gensim's FastText code for replicating the implementation of Out-of-Vocabulary (OOV) words, and I'm not being able to accomplish it.
So, the process i'm following is training a tiny model with a toy corpus, and then comparing the resulting vectors of a word in the vocabulary. That means if the whole process is OK, the output arrays should be the same.</p>

<p>Here is the code I've used for the test:</p>

<pre><code>from gensim.models import FastText
import numpy as np
# Default gensim's function for hashing ngrams
from gensim.models._utils_any2vec import ft_hash_bytes

# Toy corpus
sentences = [['hello', 'test', 'hello', 'greeting'],
             ['hey', 'hello', 'another', 'test']]

# Instatiate FastText gensim's class
ft = FastText(sg=1, size=5, min_count=1, \
window=2, hs=0, negative=20, \
seed=0, workers=1, bucket=100, \
min_n=3, max_n=4)

# Build vocab
ft.build_vocab(sentences)

# Fit model weights (vectors_ngram)
ft.train(sentences=sentences, total_examples=ft.corpus_count, epochs=5)

# Save model
ft.save('./ft.model')
del ft

# Load model
ft = FastText.load('./ft.model')

# Generate ngrams for test-word given min_n=3 and max_n=4
encoded_ngrams = [b""&lt;he"", b""&lt;hel"", b""hel"", b""hell"", b""ell"", b""ello"", b""llo"", b""llo&gt;"", b""lo&gt;""]
# Hash ngrams to its corresponding index, just as Gensim does
ngram_hashes = [ft_hash_bytes(n) % 100 for n in encoded_ngrams]
word_vec = np.zeros(5, dtype=np.float32)
for nh in ngram_hashes:
    word_vec += ft.wv.vectors_ngrams[nh]

# Compare both arrays
print(np.isclose(ft.wv['hello'], word_vec))

</code></pre>

<p>The output of this script is False for every dimension of the compared arrays.</p>

<p>It would be nice if someone could point me out if i'm missing something or doing something wrong. Thanks in advance!</p>
","python-3.x, gensim, fasttext, oov","<p>The calculation of a full word's FastText word-vector is not <em>just</em> the sum of its character n-gram vectors, but also a raw full-word vector that's also trained for in-vocabulary words. </p>

<p>The full-word vectors you get back from <code>ft.wv[word]</code> for known-words have already had this combination pre-calculated. See the <code>adjust_vectors()</code> method for an example of this full calculation:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/68ec5b8ed7f18e75e0b13689f4da53405ef3ed96/gensim/models/keyedvectors.py#L2282"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/68ec5b8ed7f18e75e0b13689f4da53405ef3ed96/gensim/models/keyedvectors.py#L2282</a></p>

<p>The <em>raw</em> full-word vectors are in a <code>.vectors_vocab</code> array on the <code>model.wv</code> object.</p>

<p>(If this isn't enough to reconcile matters: ensure you're using the latest <code>gensim</code>, as there have been many recent FT fixes. And, ensure your list of ngram-hashes matches the output of the <code>ft_ngram_hashes()</code> method of the library – if not, your manual ngram-list-creation and subsequent hashing may be doing something different.)</p>
",0,0,251,2020-03-04 11:02:45,https://stackoverflow.com/questions/60524589/cannot-reproduce-pre-trained-word-vectors-from-its-vector-ngrams
Gensim row wise dataframe summary,"<p>I am using 'Gensim' to generate summary of different rows I have. Here is what the original dataframe looks like:</p>

<pre><code>df.head()

                                   Example Content
0   Not happy they have just reduced rates for Und...
1   One of the worst banks. I had a very bad exper...
2   Some one in lloyds has signed a form in My nam...
3   Card blocked due to ordering a takeaway from m...
4   There are plenty of better banks than Lloyds.\...
</code></pre>

<p>I am able to apply summarization to every row using gensim. Problem is, I want every rows summary to appear against its original, and this is not happening. Here is what my code looks like:</p>

<pre><code>a = []

for i in df['Example Content']:

    i= i + str("". This is second sentence. This is third"")             # this is to add two more sentences so that gensim summarizes it. These sentence add no value to summary.
    a = summarize(i, ratio=0.4, split = True)

df['Summary'] = a
</code></pre>

<p>And here is the ouput to the above code:</p>

<pre><code>                                     Example Content                                 Summary
0   Not happy they have just reduced rates for Und...       Today I got a new phone and switched my sim an...
1   One of the worst banks. I had a very bad exper...       Today I got a new phone and switched my sim an...
2   Some one in lloyds has signed a form in My nam...       Today I got a new phone and switched my sim an...
3   Card blocked due to ordering a takeaway from m...       Today I got a new phone and switched my sim an...
4   There are plenty of better banks than Lloyds.\...       Today I got a new phone and switched my sim an...
</code></pre>

<p>Below shown are all the individual summaries, generated by gensim, of each row:</p>

<pre><code>The 2nd address was a shopping centre and they didnt even give me the name of the business.
I wasn't to know as I through Gallarias Novas was the shop name but that was just the place.
They said that they had issued a new card that I hadn't received and even though they new I was abroad using my card they stopped it anyway.
When my new card did arrive after getting home I now know the reason was that they were making me have a con tactless card whcih I did nto request.

 Today I got a new phone and switched my sim and set up my banking apps inc Halifax and LloydÕs.
Halifax worked fine, usual 4 digit code and confirmation call came through and all set up in mins.
</code></pre>

<p>How should i grab individual summaries corresponding to the original content and place them in the dataframe? </p>
","python, pandas, for-loop, gensim, summarization","<p>You keep overwriting your list. Replace</p>

<pre><code>a = summarize(i, ratio=0.4, split = True)
</code></pre>

<p>with </p>

<pre><code>a.append(summarize(i, ratio=0.4, split = True))
</code></pre>
",0,0,248,2020-03-08 14:35:36,https://stackoverflow.com/questions/60588557/gensim-row-wise-dataframe-summary
Scikit-Learn GridSearchCV failing on on a gensim LDA model,"<p>This is the code for creating the model :</p>

<pre><code>import gensim
NUM_TOPICS = 4
ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics = 
NUM_TOPICS,id2word=dictionary,passes=100)
ldamodel.save('model5.gensim')
topics = ldamodel.print_topics(num_words=4)
print(topics)
</code></pre>

<p>This is the code for GridSearchCV :</p>

<pre><code>search_params = {'n_components': [4, 6, 8, 10, 20], 'learning_decay': [.5, .7, .9]}


# Init Grid Search Class
model = GridSearchCV(ldamodel, param_grid=search_params)

# Do the Grid Search
model.fit(data_vectorized)
</code></pre>

<p>This is the output : </p>

<pre><code>*---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-108-1a35c49ac19e&gt; in &lt;module&gt;
      9 
     10 # Do the Grid Search
---&gt; 11 model.fit(data_vectorized)
~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\model_selection\_search.py in fit(self, X, y, groups, **fit_params)
    627 
    628         scorers, self.multimetric_ = _check_multimetric_scoring(
--&gt; 629             self.estimator, scoring=self.scoring)
    630 
    631         if self.multimetric_:
~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py in _check_multimetric_scoring(estimator, scoring)
    471     if callable(scoring) or scoring is None or isinstance(scoring,
    472                                                           str):
--&gt; 473         scorers = {""score"": check_scoring(estimator, scoring=scoring)}
    474         return scorers, False
    475     else:
~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py in check_scoring(estimator, scoring, allow_none)
    399     if not hasattr(estimator, 'fit'):
    400         raise TypeError(""estimator should be an estimator implementing ""
--&gt; 401                         ""'fit' method, %r was passed"" % estimator)
    402     if isinstance(scoring, str):
    403         return get_scorer(scoring)
TypeError: estimator should be an estimator implementing 'fit' method, &lt;gensim.models.ldamodel.LdaModel object at 0x000002121E55D3C8&gt; was passed*
</code></pre>
","python, scikit-learn, gensim, lda, gridsearchcv","<p>You are trying to use <code>GridSearchCV</code> object from a <code>scikit-learn</code> package which requires the model object on which it is run to implement certain methods (as in the error message:<code>fit</code> method in particular). Since <code>scikit-learn</code> is not related in any way to <code>gensim</code> you need to ensure that they are compatible by <a href=""https://scikit-learn.org/stable/developers/develop.html"" rel=""nofollow noreferrer"">subclassing an <code>Estimator</code> class in <code>scikit-learn</code></a> and encapsulating <code>gensim</code> training in the <code>fit</code> method.</p>

<p>Also, it does not seem to me in <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">the <code>LdaModel</code> documentation</a> that it uses the parameters (<code>n_components</code>, <code>learning_decay</code>) that you are attempting to search for. You can only search for the values of the parameters that the model uses.</p>
",1,1,2076,2020-03-09 14:36:19,https://stackoverflow.com/questions/60602768/scikit-learn-gridsearchcv-failing-on-on-a-gensim-lda-model
"Trying to make use of a library to conduct some topic modeling, but it&#39;s not going well","<p>I have a .csv term-document matrix, and I wanna perform some latent dirichlet allocation using gensim in python. However, I'm not particularly familiar with Python <em>or</em> LDA.</p>

<p>I posted in the gensim...forum? I dunno if that's what it's called. The guy that wrote the package responded and had this to say:</p>

<blockquote>
  <p>how big is your term-document CSV matrix?</p>
  
  <p>If it's small enough = fits in RAM, you could: </p>
  
  <p>1) use numpy.loadtxt()
  to load your CSV into an in-memory matrix </p>
  
  <p>2) convert the matrix to a corpus with gensim.matutils.Dense2Corpus() . Check out its documents_columns flag, it lets you switch between document-term and term-document transposition easily. </p>
  
  <p>3) use that corpus to train your LDA model.</p>
</blockquote>

<p>So that leads me to believe that the answer to <a href=""https://stackoverflow.com/questions/27220927/passing-term-document-matrix-to-gensim-lda-model"">this question</a> isn't correct. </p>

<p>It seems like a dictionary is a necessary input to a LDA model; is this not correct? Here's what I have that I think successfully sticks the .csv into a corpus. </p>

<pre><code>file = np.genfromtxt(fname=fPathName, dtype=""int"", delimiter="","", skip_header=True, missing_values="""", filling_values=0)


corpus = gensim.matutils.Dense2Corpus(file, documents_columns=False)
</code></pre>

<p>Any help would be appreciated.</p>

<p>Edit: turns out that a Gensim dictionary and a Python dictionary are not exactly the same things.</p>
","python, gensim, lda, corpus","<p>So, from Gensim documentation I took this snip of code:</p>

<pre><code>from gensim.models import LdaModel
from gensim.test.utils import common_texts
from gensim.corpora.dictionary import Dictionary

# Create a corpus from a list of texts
common_dictionary = Dictionary(common_texts)
common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]

# Train the model on the corpus.
lda = LdaModel(common_corpus, num_topics=10)
</code></pre>

<p>The file you want to analyse is a csv so to open it you can use pandas</p>

<pre><code>import pandas as pd
df = pd.read_csv(filename) # add header=None if the file has no column names
</code></pre>

<p>Once you import the file you have everything loaded into a data frame, you need to combine all text into a unique list (see first comment of gensim code snip) that should look like this </p>

<pre><code>[""text one.."", ""text 2.."", ""text 3...""]
</code></pre>

<p>You can do that by iterating through the data frame and iteratevely adding text to an empty list. Before to do that you also need to check which column of your csv file contain the text to analyse. </p>

<pre><code>common_texts = [] # initialise empty list
for ind, row in df.iteritem():
    text = row[name_column_with_text]
    common_texts.append(text)
</code></pre>

<p>Once you get your list of text you can simply apply the code from gensim documentation.
Of course you might get memory problems, it depends on the size of your csv file.</p>
",0,0,137,2020-03-11 05:19:59,https://stackoverflow.com/questions/60629671/trying-to-make-use-of-a-library-to-conduct-some-topic-modeling-but-its-not-goi
gensim corpus from sparse matrix,"<p>I have a data frame like this</p>

<pre><code>import pandas as pd
from gensim.corpora import Dictionary

tmp = pd.DataFrame({""word"":  [1, 0, 0, 0, 0, 0],
                    ""house"": [0, 1, 0, 0, 0, 0],
                    ""tree"":  [0, 0, 1, 0, 0, 1], # occurred twice
                    ""car"":   [0, 0, 0, 1, 0, 0],
                    ""food"":  [0, 0, 0, 0, 1, 0],
                    ""train"": [0, 0, 0, 0, 0, 1]})
mydict = gensim.corpora.Dictionary()
</code></pre>

<p>from this, I want to create a <code>gensim</code> corpus.</p>

<p>I have tried <code>mycorp = [mydict.doc2bow(col, allow_update=True) for col in tmp.columns]</code> but the resulting corpus seems to not have been properly created:</p>

<blockquote>
  <p>TypeError: doc2bow expects an array of unicode tokens on input, not a single string</p>
</blockquote>

<p>Can someone help me with this? I would like the resulting dictionary to represent the fact that word ""tree"" occurred twice in this data frame (i.e. the sum of the column).</p>
","python, python-3.x, gensim","<p>The input to <code>mydict.doc2bow</code> doesn't seem to be correct. It takes a list of strings, not a single string. The list of strings being the document.</p>

<h2>Scenario 1</h2>

<p>If you consider each column name to be a document (i.e. document 1 is <code>[""word""]</code>), then you could do:</p>

<pre><code>[mydict.doc2bow([col], allow_update=True) for col in tmp.columns]
# [[(0, 1)], [(1, 1)], [(2, 1)], [(3, 1)], [(4, 1)], [(5, 1)]]
</code></pre>

<p>These are six documents (each sublist) with only a single word. The tuples in the sublist indicate the <code>(word_id, frequency)</code>. So the first document contains <code>word0</code> once. The second document contains <code>word1</code> once, etc.</p>

<h2>Scenario 2</h2>

<p>If you consider your column names to be a single document, then you could do:</p>

<pre><code>mydict.doc2bow(tmp.columns, allow_update=True) 
# [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]
</code></pre>

<p>Where your corpus consists of a single document, which contains <code>word0</code> to <code>word5</code> all once</p>

<h2>Little bit of background</h2>

<p>Instead of working with strings (""tokens"") directly, like ""word"", ""house"", etc, <code>gensim</code> uses integers that represent a string. These integers are word ids. To see which word corresponds to which id, you can use:</p>

<pre><code>mydict.token2id['word']
# 0
</code></pre>

<p>The bag of words is represented as a tuple with <code>(word_id, frequency)</code>, because any given word may occur multiple times in a document. Especially in longer documents, a single word may appear 100 times.</p>

<p>Instead of saving a reference to that word a 100 times, gensim is clever and saves <code>(word_id, 100)</code> instead. This then represents that some word occurs 100 times in a document. </p>
",1,0,549,2020-03-11 14:42:06,https://stackoverflow.com/questions/60638629/gensim-corpus-from-sparse-matrix
Gensim phrase handling sentence with a lot of punctuation,"<p>Now I am trying to use <code>gensim Phrases</code> in order to learn the phrase/special meaning base on my own corpus.</p>

<p>Suppose I have the corpus related to the car brand, by removing the <strong>punctuation</strong> and <strong>stopwords</strong>, <strong>tokenizing the sentence</strong>, eg:</p>

<pre><code>sent1 = 'aston martin is a car brand'
sent2 = 'audi is a car brand'
sent3 = 'bmw is a car brand'
...
</code></pre>

<p>In this way, I would like to use <code>gensim Phrases</code> to learn so that output looks like:</p>

<pre><code>from gensim.models import Phrases
sents = [sent1, sent2, sent3, ...]
sents_stream = [sent.split() for sent in sents]
bigram = Phrases(sents_stream)

for sent in sents:
    print(bigram [sent])

# Ouput should be like:
['aston_martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
...
</code></pre>

<p>However, if a lot of sentences that have a lot of punctuation:</p>

<pre><code>sent1 = 'aston martin is a car brand'
sent2 = 'audi is a car brand'
sent3 = 'bmw is a car brand'
sent4 = 'jaguar, aston martin, mini cooper are british car brand'
sent5 = 'In all brand, I love jaguar, aston martin and mini cooper'
...

</code></pre>

<p>Then the output looks like:</p>

<pre><code>from gensim.models import Phrases
sents = [sent1, sent2, sent3, sent4, sent5, ...]
sents_stream = [sent.split() for sent in sents]
bigram = Phrases(sents_stream)

for sent in sents:
    print(bigram [sent])

# Ouput should be like:
['aston', 'martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
['jaguar', 'aston', 'martin_mini', 'cooper', 'british', 'car', 'brand']
['all', 'brand', 'love', 'jaguar', 'aston', 'martin_mini', 'cooper']
...
</code></pre>

<p>In this case, how should I handle the sentence with lot of punctuation to prevent <code>martin_mini</code> case and make the output looks like:</p>

<pre><code>['aston', 'martin', 'car', 'brand']
['audi', 'car', 'brand']
['bmw', 'car', 'brand']
['jaguar', 'aston_martin', 'mini_cooper', 'british', 'car', 'brand'] # Change
['all', 'brand', 'love', 'jaguar', 'aston_martin', 'mini_cooper'] # Change
...
</code></pre>

<p>Thanks so much for helping!</p>
","python, nlp, gensim, phrase","<p>The punctuation may not be the major contributor to your unsatisfactory results. </p>

<p>The <code>Phrases</code> class needs lots of natural usage examples to apply its purely-statistics-based combination of plausible bigrams. (It won't work well on smal/toy-sized/contrived datasets.) </p>

<p>And even with lots of data, that <code>Phrases</code> class won't consistently match the ""phrases"" or ""entities"" that humans naturally perceive, using their understanding of parts-of-speech and the underlying concepts in the world. Even with lots of tuning of its various meta-parameters, it will miss pairings you might prefer, and make pairings you may consider unnatural. Text with its pairings added may still be useful for many purposes – especially classification &amp; info-retrieval tasks – but is unlikely to appear aesthetically correct to human reviewers.</p>

<p>In your tiny contrived example, it appears that <code>martin_mini</code> becomes a bigram because the words <code>martin</code> and <code>mini</code> appear alongside each other enough, compared to their individual frequencies, to trigger the <code>Phrases</code> algorithmic-combination. </p>

<p>To prevent that particular outcome, you could consider (1) giving <code>Phrases</code> more/better data; (2) tuning <code>Phrases</code> parameters like <code>min_count</code>, <code>threshold</code>, or <code>scorer</code>; or (3) changing your preprocessing/tokenization. </p>

<p>I'm not sure what would work best, for your full dataset &amp; project goals, and as noted above, the results of this technique may never closely match your ideas of mutli-word car terms. </p>

<p>You might also consider leaving in punctuation as tokens, and leaving in stop words, so that your preprocessing doesn't create false pairings like ""martin mini"". For example, your <code>sent5</code> tokenization could become: </p>

<pre class=""lang-py prettyprint-override""><code>['in', 'all', 'brand', ',', 'i', 'love', 'jaguar', ',', 'aston', 'martin', 'and', 'mini', 'cooper']
</code></pre>

<p>The data's natural splitting of <code>martin</code> and <code>mini</code> would then be restored in the version that reaches <code>Phrases</code> – so you'd be unlikely to see the same failure you're seeing. (You might very well see other failures instead, where undesired punctuation or stop-words become part of identified bigrams, when statistics imply those tokens co-occur often enough to be considered a single unit. But that's the essence and limitation of the <code>Phrases</code> algorithm.)</p>
",1,0,224,2020-03-13 11:17:40,https://stackoverflow.com/questions/60669506/gensim-phrase-handling-sentence-with-a-lot-of-punctuation
How to plot the output of k-means clustering of word embedding using python?,"<p>I have used gensims word embeddings to find vectors of each word. Then I used K-means to find clusters of word. There are close to <code>10,000</code> tokens/words and I want to plot them.</p>

<p>I want to plot the result in the following way:</p>

<ul>
<li>Annotate points with name of <code>words</code></li>
<li>Different color for clusters</li>
</ul>

<p>Here is what I have done. </p>

<pre><code>tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=500)#, random_state=13)


def tsne_plot(data):
    ""Creates and TSNE model and plots it""

    data=data.sample(n = 500).reset_index()
    word=data[""word""]
    cluster=data[""clusters""]
    data=data.drop([""clusters"",""word""],axis=1)

    X = tsne.fit_transform(data)

    plt.figure(figsize=(48, 48)) 
    for i in range(len(X)):
        plt.scatter(X[:,0][i],X[:,1][i],c=cluster[i])
        plt.annotate(word[i],
                     xy=(X[:,0][i],X[:,1][i]),
                     xytext=(3, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.show()

tsne_plot(data)
</code></pre>

<p>Though it's annotating the <code>words</code> but failing to color different groups/clusters?</p>

<p>Anyother other approach which annoates with word anmes and colors different clusters?</p>
","python-3.x, matplotlib, gensim","<p>This is how it's typically done; with annotations and rainbow colors.</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.cluster import KMeans
import seaborn as sns
import matplotlib.pyplot as plt


X = np.array([[5,3],
     [10,15],
     [15,12],
     [24,10],
     [30,45],
     [85,70],
     [71,80],
     [60,78],
     [55,52],
     [80,91],])

kmeans = KMeans(n_clusters=2)
kmeans.fit(X)

print(kmeans.cluster_centers_)

print(kmeans.labels_)

#plt.scatter(X[:,0],X[:,1], c=kmeans.labels_, cmap='rainbow')

data = X
labels = kmeans.labels_


#######################################################################


plt.subplots_adjust(bottom = 0.1)
plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_, cmap='rainbow') 

for label, x, y in zip(labels, data[:, 0], data[:, 1]):
    plt.annotate(
        label,
        xy=(x, y), xytext=(-20, 20),
        textcoords='offset points', ha='right', va='bottom',
        bbox=dict(boxstyle='round,pad=0.5', fc='red', alpha=0.5),
        arrowprops=dict(arrowstyle = '-&gt;', connectionstyle='arc3,rad=0'))

plt.show()


#######################################################################
</code></pre>

<p><a href=""https://i.sstatic.net/zyZn7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zyZn7.png"" alt=""enter image description here""></a></p>

<p>See the link below for all details.</p>

<p><a href=""https://stackabuse.com/k-means-clustering-with-scikit-learn/"" rel=""nofollow noreferrer"">https://stackabuse.com/k-means-clustering-with-scikit-learn/</a></p>

<p>See the link below for some samples of how to do annotations with characters, rather tan numbers.</p>

<p><a href=""https://i.sstatic.net/dGwop.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dGwop.png"" alt=""enter image description here""></a></p>

<p><a href=""https://nikkimarinsek.com/blog/7-ways-to-label-a-cluster-plot-python"" rel=""nofollow noreferrer"">https://nikkimarinsek.com/blog/7-ways-to-label-a-cluster-plot-python</a></p>
",2,3,3313,2020-03-13 14:34:59,https://stackoverflow.com/questions/60672361/how-to-plot-the-output-of-k-means-clustering-of-word-embedding-using-python
Issues while loading a trained fasttext model using gensim,"<p>I am trying to load a trained fasttext model using gensim. The model has been trained on some data. Earlier, I have used <code>model.save()</code> with a extension of <code>.bin</code> to use it later. After the training process and saving the model using <code>model.save</code> in <code>.bin</code> format, generates 3 files respectively. They are:</p>

<p>1) .bin  </p>

<p>2) bin.trainable vectors_ngrams_lockf</p>

<p>3) bin.wv.vectors_ngrams </p>

<p>Now I am unable to load the trained binary file (.bin).  </p>

<p>But I don't understand why I am getting a error named:</p>

<blockquote>
  <p>raise NotImplementedError(""Supervised fastText models are not supported"")
  NotImplementedError: Supervised fastText models are not supported</p>
</blockquote>

<p>After going through many blogs, peoples have suggested that <code>gensim</code> does not supports supervised training. It's fine. My question is how can I be able to load the trained binary model. Shall I need to train the model differently.</p>

<p>Any help is appreciated.</p>

<p>What I have tried after the training process: </p>

<pre><code>import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from gensim.models import FastText, fasttext
model = FastText.load_fasttext_format('m1.bin')
print(model)
</code></pre>
","python, python-3.x, gensim, word-embedding, fasttext","<p>If the model was saved with <code>gensim</code>'s native <code>.save()</code> method, you'd load it with <code>.load()</code> - <strong>not</strong> <code>load_fasttext_format()</code>, which is only for models saved in the raw format used by Facebook's original FastText C++ code. </p>
",3,3,1328,2020-03-14 12:18:28,https://stackoverflow.com/questions/60682634/issues-while-loading-a-trained-fasttext-model-using-gensim
What kind of model/technique should I use to compare supermarket product names,"<p>I have a database with supermarket product items(it contains name, descriptions, price, stock, etc). </p>

<p>I want to make a price comparison between those supermarkets, but, for that i need to know if supermarket A and B refers to the same product. </p>

<p>For example I found out that supermarket <strong>A</strong> has a product called <code>Leche Evaporada GLORIA Azul Paquete 6un Lata 400g</code> and supermarket <strong>B</strong> has a product named <code>Leche Evaporada Gloria Azul Pack 6 Unid x 400 g</code> and those refers to the same product.</p>

<p>I pointed out that I will need to have semantic comparison for those cases. I'm new in this problems so I don't really know what is the best solution to not underestimate the problem or overkill it.</p>

<p>What I'm doing right now with not so great results:</p>

<ol>
<li>I'm only using product names.</li>
<li>Remove stop words from those product names.</li>
<li>Convert the sentence in an array of words.</li>
<li>Get frequency for every word.</li>
<li>If a word has frequency &lt;= 1, then delete it.</li>
<li>With that words I create a dictionary(bag of words) that i will use to map an array of words(a sentence converted) to a feature vector.</li>
<li>Then I ""train"" a TFIDF model with all feature vectors.</li>
<li>Make comparisons(with no great results).</li>
</ol>

<p>I'm using python as LP and <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow noreferrer"">gensim</a> to create models, dictionaries(bag of word) and to make comparisons.</p>

<p>EDIT:
Another examples:</p>

<pre><code>Leche Fresca UHT GLORIA Entera Bolsa 946ml == Leche Entera UHT Gloria Bolsa 946 ml
Yogurt Griego Gloria con Miel y Granola Vaso 115 g == Yogurt Griego GLORIA Batido con Miel Vaso 115g
Leche sin Lactosa GLORIA Mocaccino Botella 330ml == Shake Mocaccino UHT Gloria Frasco 330 ml.
</code></pre>
","python, machine-learning, nlp, artificial-intelligence, gensim","<p>I think a good solution for this problem would be that you compare the products based on a similarity score. For instance, I would use the <a href=""https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance"" rel=""nofollow noreferrer"">Jaro-Winkler distance</a> to compare two product descriptions and if the descriptions match to a defined threshold, I would compare the prices.</p>
",1,-1,99,2020-03-17 13:17:19,https://stackoverflow.com/questions/60723228/what-kind-of-model-technique-should-i-use-to-compare-supermarket-product-names
[Word2Vec][gensim] Handling missing words in vocabulary with the parameter min_count,"<p>Some similar questions have been asked regarding this topic, but I am not really satisfied with the replies so far; please excuse me for that first.</p>

<p>I'm using the function <code>Word2Vec</code> from the python library <code>gensim</code>.</p>

<p>My problem is that I <strong>can't run my model on every word of my corpus as long as I set the parameter <code>min_count</code> greater than one</strong>. Some would say it's logic cause I choose to ignore the words appearing only once. But the function is behaving weird cause it gives an <strong>error saying <em>word 'blabla' is not in the vocabulary</em></strong>, whereas this is exactly what I want ( I want this word to be out of the vocabulary).</p>

<p>I can imagine this is not very clear, then find below a reproducible example:</p>

<pre><code>import gensim
from gensim.models import Word2Vec

# My corpus
corpus=[[""paris"",""not"",""great"",""city""],
       [""praha"",""better"",""great"",""than"",""paris""],
       [""praha"",""not"",""country""]]

# Load a pre-trained model - The orignal one based on google news 
model_google = gensim.models.KeyedVectors.load_word2vec_format(r'GoogleNews-vectors-negative300.bin', binary=True)

# Initializing our model and upgrading it with Google's 
my_model = Word2Vec(size=300, min_count=2)#with min_count=1, everything works fine
my_model.build_vocab(corpus)
total_examples = my_model.corpus_count
my_model.build_vocab([list(model_google.vocab.keys())], update=True)
my_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, lockf=1.0)
my_model.train(corpus, total_examples=total_examples, epochs=my_model.iter)

# Show examples
print(my_model['paris'][0:10])#works cause 'paris' is present twice
print(my_model['country'][0:10])#does not work cause 'country' appears only once
</code></pre>

<p>You can find Google's model <a href=""https://github.com/mmihaltz/word2vec-GoogleNews-vectors"" rel=""nofollow noreferrer"">there</a> for example, but feel free to use any model or just do without, this is not the point of my post.</p>

<p>As notified in the commentaries of the code: running the model on 'paris' works but not on 'country'. And of course, if I set the parameter <code>min_count</code> to 1, everything works fine.</p>

<p>I hope it is clear enough.</p>

<p>Thanks.</p>
","python, nlp, gensim, word2vec, word-embedding","<p>It is supposed to throw an error if you ask for a word that's not present because you chose not to learn vectors for rare words, like <code>'country'</code> in your example. (And: such words with few examples usually don't get good vectors, and retaining them can worsen the vectors for remaining words, so a <code>min_count</code> as large as you can manage, and perhaps much larger than <code>1</code>, is usually a good idea.)</p>

<p>The fix is to do one of the following:</p>

<ol>
<li>Don't ask for words that aren't present. Check first, via something like Python's <code>in</code> operator. For example:</li>
</ol>

<pre><code>if 'country' in my_model:
    print(my_model['country'][0:10])
else: 
    pass  # do nothing, since `min_count=2` means there's no 'country' vector
</code></pre>

<ol start=""2"">
<li>Catch the error, falling back to whatever you want to happen for absent words:</li>
</ol>

<pre><code>try:
    print(my_model['country'][0:10])
except:
    pass  # do nothing, or perhaps print an error, whatever
</code></pre>

<ol start=""3"">
<li>Change to using a model that always returns something for any word, like <code>FastText</code> – which will try to synthesize a vector for unknown words, using subwords learned during training. (It might be garbage, it might be pretty good if the unknown word is highly similar to known words in characters &amp; meaning, but for some uses it's better than nothing.) </li>
</ol>
",2,2,4752,2020-03-17 17:05:46,https://stackoverflow.com/questions/60727025/word2vecgensim-handling-missing-words-in-vocabulary-with-the-parameter-min-c
How can I load chinese fasttext model with gensim?,"<p>While trying to load chines fasttext model(cc.zh.300.bin) with gensim, I stucked with following error</p>

<blockquote>
  <p>UnicodeDecodeError:'utf-8' codec can't decode byte 0xba in position 0:
  invalid start byte</p>
</blockquote>

<p>Anyone can help me, please? Detailed error below :</p>

<p><a href=""https://i.sstatic.net/73L4H.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/73L4H.png"" alt=""enter image description here""></a></p>
","gensim, fasttext","<p>The <code>KeyedVectors.load_word2vec_format()</code> method only loads files in the plain words-and-vectors format used by Google's original <code>word2vec.c</code> code. It would not be expected to work on a FastText-format file.</p>

<p>You should try instead the method <code>load_facebook_vectors()</code> that's specifically for FastText format files:</p>

<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors</a></p>

<p>For some uses, the alternate <code>load_facebook_model()</code> might also be appropriate:</p>

<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model</a></p>
",1,0,464,2020-03-20 17:31:17,https://stackoverflow.com/questions/60778921/how-can-i-load-chinese-fasttext-model-with-gensim
"In gensim with pretrained model, wmdistance is working well, but n_similarity is not","<p>I have calculated distances between two sentences using wmdistance() funtion of gensim with pre-trained model</p>
<p>Now, I want to similarity between them and tried with  n_similarity() funnction, but keyerror occured</p>
<p>keyerror : word not in vacabulary</p>
<p>This shows  screenshoot of error example
<a href=""https://i.sstatic.net/r1h9F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/r1h9F.png"" alt=""screenshoot of error example"" /></a></p>
<p>Anyone have got idea on this, please?</p>
",gensim,"<p>When you get an error that a word is not in the vocabulary, it means the word is not in that model.</p>

<p>Any attempt to look it up will generate a <code>KeyError</code>, to let you know you are trying to get a word-vector that isn't there. </p>

<p>You should filter your lists-of-tokens, before passing them to <code>n_similarity()</code>, to only include valid words. </p>

<p>Of course, that means you can't get a meaningful result about the word <code>'selfie'</code>. It's unknown nonsense to the model, as if you asked for the word <code>'asruhfglaiwurfliuawiufsdfsdfs'</code>.</p>
",1,0,165,2020-03-21 07:42:28,https://stackoverflow.com/questions/60785538/in-gensim-with-pretrained-model-wmdistance-is-working-well-but-n-similarity-is
Gensim: How to load corpus from saved lda model?,"<p>When I saved my LdaModel <code>lda_model.save('model')</code>, it saved 4 files:</p>

<ol>
<li><code>model</code> </li>
<li><code>model.expElogbeta.npy</code> </li>
<li><code>model.id2word</code> </li>
<li><code>model.state</code></li>
</ol>

<p>I want to use <code>pyLDAvis.gensim</code> to visualize the topics, which seems to need the model, corpus and dictionary. I was able to load the model and dictionary with:</p>

<pre><code>lda_model = LdaModel.load('model')
dict = corpora.Dictionary.load('model.id2word')
</code></pre>

<p>Is it possible to load the corpus? How?</p>
","gensim, lda, corpus","<p>Sharing this here because it took me awhile to find out the answer to this as well. Note that <code>dict</code> is not a valid name for a dictionary and we use <code>lda_dict</code> instead.</p>
<pre><code># text array is a list of lists containing text you are analysing
# eg. text_array = [['volume', 'eventually', 'metric', 'rally'], ...]
# lda_dict is a gensim.corpora.Dictionary object

bow_corpus = [lda_dict.doc2bow(doc) for doc in text_array]
</code></pre>
",1,1,2740,2020-03-24 23:18:27,https://stackoverflow.com/questions/60840809/gensim-how-to-load-corpus-from-saved-lda-model
Topic Coherence with Dictionary from Glove (gensim),"<p>I'm trying to evaluate a home-made topic model. For this, I'm using the list of topics (represented by keywords), and want to use a <code>gensim.models.coherencemodel.CoherenceModel</code>, and call it on a corpus, which is a list of strings (each one being a document).
The <code>CoherenceModel</code> requires a <code>Dictionary</code>, but I don't understand what this corresponds to, and how I can get it.
I'm using the <code>TfidfVectorizer</code> from <code>sklearn</code> to vectorize the text, and <code>glove</code> embeddings from <code>gensim</code> to compute similarities within my model. </p>
","python, gensim","<p>From the docs, a <code>Dictionary</code> can be created from a corpus where the corpus is a <code>list of lists of str</code>. This same corpus should be passed in the <code>text</code> argument of the <code>CoherenceModel</code>.</p>
",0,0,179,2020-03-25 14:31:31,https://stackoverflow.com/questions/60850956/topic-coherence-with-dictionary-from-glove-gensim
Training time of gensim word2vec,"<p>I'm training word2vec from scratch on 34 GB pre-processed MS_MARCO corpus(of 22 GB). (Preprocessed corpus is sentnecepiece tokenized and so its size is more) I'm training my word2vec model using following code : </p>

<pre><code>from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec

class Corpus():
    """"""Iterate over sentences from the corpus.""""""
    def __init__(self):
        self.files = [
            ""sp_cor1.txt"",
            ""sp_cor2.txt"",
            ""sp_cor3.txt"",
            ""sp_cor4.txt"",
            ""sp_cor5.txt"",
            ""sp_cor6.txt"",
            ""sp_cor7.txt"",
            ""sp_cor8.txt""
        ]

    def __iter__(self):
        for fname in self.files:
            for line in open(fname):
                words = line.split()
                yield words

sentences = Corpus()

model = Word2Vec(sentences, size=300, window=5, min_count=1, workers=8, sg=1, hs=1, negative=10)
model.save(""word2vec.model"")

</code></pre>

<p>My model is running now for about more than 30 hours now. This is doubtful since on my i5 laptop with 8 cores, I'm using all the 8 cores at 100% for every moment of time. Plus, my program seems to have read more than 100 GB of data from the disk now. I don't know if there is anything wrong here, but the main reason after my doubt on the training is because of this 100 GB of read from the disk. The whole corpus is of 34 GB, then why my code has read 100 GB of data from the disk? Does anyone know how much time should it take to train word2vec on 34 GB of text, with 8 cores of i5 CPU running all in parallel? Thank you. For more information, I'm also attaching the photo of my process from system monitor. </p>

<p><a href=""https://i.sstatic.net/QrJRM.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/QrJRM.png"" alt=""enter image description here""></a></p>

<p>I want to know why my model has read 112 GB from memory, even when my corpus is of 34 GB in total? Will my training ever get finished? Also I'm bit worried about health of my laptop, since it is running constantly at its peak capacity since last 30 hours. It is really hot now. 
Should I add any additional parameter in <code>Word2Vec</code> for quicker training without much performance loss?</p>
","python, nlp, gensim, word2vec","<p>Completing a model requires one pass over all the data to discover the vocabulary, then multiple passes, with a default of 5, to perform vector training. So, you should expect to see about 6x your data size in disk-reads, just from the model training.</p>

<p>(If your machine winds up needing to use virtual-memory swapping during the process, there could be more disk activity – but you absolutely do not want that to happen, as the random-access pattern of word2vec training is nearly a worst-case for virtual memory usage, which will slow training immensely.)</p>

<p>If you'd like to understand the code's progress, and be able to estimate its completion time, you should enable Python logging to at least the <code>INFO</code> level. Various steps of the process will report interim results (such as the discovered and surviving vocabulary size) and estimated progress. You can often tell if something is going wrong before the end of a run by studying the logging outputs for sensible values, and once the 'training' phase has begun the completion time will be a simple projection from the training completed so far. </p>

<p>I believe most laptops should throttle their own CPU if it's becoming so hot as to become unsafe or risk extreme wear on the CPU/components, but whether yours does, I can't say, and definitely make sure its fans work &amp; vents are unobstructed. </p>

<p>I'd suggest you choose some small random subset of your data – maybe 1GB? – to be able to run all your steps to completion, becoming familiar with the <code>Word2Vec</code> logging output, resource usage, and results, and tinkering with settings to observe changes, before trying to run on your full dataset, which might require days of training time. </p>

<p>Some of your shown parameters aren't optimal for speedy training. In particular:</p>

<ul>
<li><p><code>min_count=1</code> retains every word seen in the corpus-survey, including those with only a single occurrence. This results in a much, much larger model - potentially risking a model that doesn't fit into RAM, forcing disastrous swapping. But also, words with just a few usage examples can't possibly get good word vectors, as the process requires seeing many subtly-varied alternate uses. Still, via typical 'Zipfian' word-frequencies, the number of such words with just a few uses may be very large in total, so retaining all those words takes a lot of training time/effort, and even serves a bit like 'noise' making the training of other words, with plenty of usage examples, less effective. So for model size, training speed, <strong>and</strong> quality of remaining vectors, a larger <code>min_count</code> is desirable. The default of <code>min_count=5</code> is better for more projects than <code>min_count=1</code> – this is a parameter that should only really be changed if you're sure you know the effects. And, when you have plentiful data – as with your 34GB – the <code>min_count</code> can go much higher to keep the model size manageable. </p></li>
<li><p><code>hs=1</code> should only be enabled if you want to use the 'hierarchical-softmax' training mode instead of 'negative-sampling' – and in that case, <code>negative=0</code> should also be set to disable 'negative-sampling'. You probably don't want to use hierarchical-softmax: it's not the default for a reason, and it doesn't scale as well to larger datasets. But here you've enabled in in addition to negative-sampling, likely more-than-doubling the required training time. </p></li>
<li><p>Did you choose <code>negative=10</code> because you had problems with the default <code>negative=5</code>? Because this non-default choice, again, would slow training noticeably. (But also, again, a non-default choice here would be more common with smaller datasets, while larger datasets like yours are more likely to experiment with a smaller <code>negative</code> value.)</p></li>
</ul>

<p>The theme of the above observations is: ""only change the defaults if you've already got something working, and you have a good theory (or way of testing) how that change might help"". </p>

<p>With a large-enough dataset, there's another default parameter to consider changing to speed up training (&amp; often improve word-vector quality, as well): <code>sample</code>, which controls how-aggressively highly-frequent words (with many redundant usage-examples) may be downsampled (randomly skipped). </p>

<p>The default value, <code>sample=0.001</code> (aka <code>1e-03</code>), is very conservative. A smaller value, such as <code>sample=1e-05</code>, will discard many-more of the most-frequent-words' redundant usage examples, speeding overall training considerably. (And, for a corpus of your size, you could eventually experiment with even smaller, more-aggressive values.)</p>

<p>Finally, to the extent all your data (for either a full run, or a subset run) can be in an already-space-delimited text file, you can use the <code>corpus_file</code> alternate method of specifying the corpus. Then, the <code>Word2Vec</code> class will use an optimized multithreaded IO approach to assign sections of the file to alternate worker threads – which, if you weren't previously seeing full saturation of all threads/CPU-cores, could increase our throughput. (I'd put this off until after trying other things, then check if your best setup still leaves some of your 8 threads often idle.)</p>
",14,7,5907,2020-03-25 16:22:38,https://stackoverflow.com/questions/60852962/training-time-of-gensim-word2vec
"Having trouble loading custom trained word vectors created in Gensim, into Spacy","<p>I've trained a model:</p>

<pre><code>from gensim.models import Word2Vec    

model = Word2Vec(master_sent_list,
                     min_count=5,   
                     size=300,      
                     workers=5,    
                     window=5,      
                     iter=30)  
</code></pre>

<p>Saved it according to <a href=""https://stackoverflow.com/questions/50466643/in-spacy-how-to-use-your-own-word2vec-model-created-in-gensim"">this</a> post:</p>

<pre><code>model.wv.save_word2vec_format(""../moj_word2vec.txt"")
!gzip ../moj_word2vec.txt
!python -m spacy init-model en ../moj_word2vec.model --vectors-loc ../moj_word2vec.txt.gz
</code></pre>

<p>Everything looks fine:</p>

<pre><code>✔ Successfully created model
22470it [00:02, 8397.55it/s]j_word2vec.txt.gz
✔ Loaded vectors from ../moj_word2vec.txt.gz
✔ Sucessfully compiled vocab
22835 entries, 22470 vectors
</code></pre>

<p>I then load the model <strong>under a different name</strong>:</p>

<pre><code>nlp = spacy.load('../moj_word2vec.model/')
</code></pre>

<p>Something goes wrong however, because I can't use common commands on <code>nlp</code>; that I can on <code>model</code>.</p>

<p>For example, these work:</p>

<pre><code>model.wv.most_similar('police')
model.vector_size
</code></pre>

<p>But these don't:</p>

<pre><code>nlp.wv.most_similar('police')
AttributeError: 'English' object has no attribute 'wv'

nlp.most_similar('police')
AttributeError: 'English' object has no attribute 'most_similar'

nlp.vector_size
AttributeError: 'English' object has no attribute 'vector_size'
</code></pre>

<p>So something seems to have broken in the loading, or perhaps the saving, could someone help please?</p>
","python-3.x, spacy, gensim","<p>Nothing's broken - you just have the wrong expectations.</p>

<p>The models from <code>spacy</code>, as loaded into your <code>nlp</code> variable, won't support methods from <code>gensim</code> model classes. </p>

<p>It's a different library, code, classes, and API – which does not itself make use of <code>gensim</code> code under-the-hood – even if it can import the plain set-of-vectors from the plain <code>word2vec_format</code>. </p>

<p>(Compare, for example, the results of <code>type(model)</code> or <code>type(model.wv)</code> on your working <code>gensim</code> model, then <code>type(nlp)</code> of the <code>spacy</code> object that's created later: totally different types, with different methods/properties.)</p>

<p>You'll have to use some combination of:</p>

<ul>
<li><p>checking the <code>spacy</code> docs for equivalent operations</p></li>
<li><p>if you need the <code>gensim</code> operations, load the vectors into a <code>gensim</code> model class. For example:</p></li>
</ul>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.keyedvectors import KeyedVectors
wv = KeyedVectors.load_word2vec_format(filename)
# then do gensim ops on the `wv` object
</code></pre>

<p>(You could also save the entire <code>gensim</code> <code>Word2Vec</code> model, using the <code>.save()</code> method, which will store it in one or more files using Python pickling. It could then be reloaded into a <code>gensim</code> <code>Word2Vec</code> model using <code>Word2Vec.load()</code> – though if you're only needing to look at individual word-vector by word-key, you don't need the full model.)</p>
",2,0,361,2020-03-26 17:54:33,https://stackoverflow.com/questions/60873334/having-trouble-loading-custom-trained-word-vectors-created-in-gensim-into-spacy
Gensim word2vec downsampling sample=0,"<p>Does <code>sample= 0</code> in Gensim word2vec mean that no downsampling is being used during my training? The documentation says just that </p>

<blockquote>
  <p>""useful range is (0, 1e-5)""</p>
</blockquote>

<p>However putting the threshold to 0 would cause P(wi) to be equal to 1, meaning that no word would be discarded, am I understanding it right or not? </p>

<p>I'm working on a relatively small dataset of 7597 Facebook posts (18945 words) and my embeddings perform far better using <code>sample= 0</code>rather than anything else within the recommended range. Is there any particular reason? Text size? </p>
","python, math, gensim, word-embedding, subsampling","<p>That seems an incredibly tiny dataset for <code>Word2Vec</code> training. (Is that only 18945 unique words, or 18945 words total, so hardly more than 2 words per post?) </p>

<p>Sampling is most useful on larger datasets - where there are <em>so many</em> examples of common words, more training examples of them aren't adding much – but they are stealing time from, and overwieghting those words' examples compared to, other less-frequent words. </p>

<p>Yes, <code>sample=0</code> means no down-sampling.</p>
",2,1,1137,2020-03-30 19:42:38,https://stackoverflow.com/questions/60938299/gensim-word2vec-downsampling-sample-0
How to measure the accuracy of a Doc2vec model?,"<p>I have a dataset of reviews for different Hotels. 
I'm trying to find out similar hotels using the reviews of hotels. So, I'm using a <code>Doc2vec</code> algorithm to achieve this.</p>

<p>Is there any way to measure the accuracy of a <code>Doc2Vec</code> model using <code>Gensim</code>, rather than evaluating the results using <code>most_similar()</code> function of <code>Gensim</code>?</p>
","gensim, unsupervised-learning, doc2vec","<p>As <code>Doc2Vec</code> (aka the 'Paragraph Vector' algorithm) is an unsupervised method, there are no strictly right or wrong results – just trained models that are better or worse for some downstream task.</p>

<p>How do you, personally, in your own mind, determine if the results are valuable to your project? </p>

<p>You have to capture some of that judgement into a repeatable process – for example, one way might be hand-crafting a list of pairs of hotels that, in your expert human-level judgement, ""ought to be more similar"" to each other than others, or perhaps in each others' ""top N"" closest results. Then score the <code>Doc2Vec</code> model against that ideal, compared to other methods (or multiple alternatively-parameterized runs of <code>Doc2Vec</code>).</p>

<p>You might be able to bootstrap some ""ought to be more similar"" pairs from existing sources of data. For example, maybe two hotels that are in the same chain ""ought to be more similar"" to each other than some random third hotel. (So, the outside data of their brand-name would guide your evaluation, ideally if you were sure that the brand name didn't leak into the document texts used to train the model.) Or maybe, two hotels that are both geographically &amp; price-wise near each other ""ought to be more similar"" than some random third.</p>

<p>But there's no standard/automatic idea of ""accuracy"" for such fuzzy representations on the domain of all possible documents and project goals. You need to develop your own custom evaluations to be able to choose between algorithms, or tune the algorithms. </p>
",5,1,1744,2020-04-04 14:23:43,https://stackoverflow.com/questions/61029524/how-to-measure-the-accuracy-of-a-doc2vec-model
significance of periods in sentences while training documents with Doc2Vec,"<p>Doubt - 1</p>
<p>I am training Doc2Vec with 150000 documents. Since these documents are from legal domain they are really hard to clean and get it ready for further training. Hence I decided to remove all the periods from a document. Having said that, I am confused on how the parameter of <code>Window_size</code> in doc2vec recognize the sentences now. There are two views presented in the question :<a href=""https://stackoverflow.com/questions/42242521/doc2vec-differentiate-sentence-and-document"">Doc2Vec: Differentiate Sentence and Document</a></p>
<ol>
<li>The algorithm just works on chunks of text, without any idea of what a sentence/paragraph/document etc might be.</li>
<li>It's even common for the tokenization to retain punctuation, such as the periods between sentences, as standalone tokens.</li>
</ol>
<p>Therefore I am in confusion if my adopted approach of eliminating the punctuation (periods) is right. Kindly provide me with some supportive answers.</p>
<p>Doubt-2</p>
<p>The documents that I <strong>scraped</strong> range from 500 - 5500 tokens hence my approach to have a pretty even sized documents for training doc2vec and even to reduce the vocabulary is :
Consider a document of size greater than 1500 tokens in this case I make use of First 50 to 400 tokens + 600 to 1000 tokens + last 250 tokens. The motivation for this kind of approach is from a paper related to Classification of documents using BERT where the sequence of 512 tokens were generated like this.</p>
<p>So I want to know if this idea is somewhat good to proceed or it's not recommended to do this?</p>
<p><strong>Update</strong> - I just saw the common_text corpus used by gensim in the tutorial link <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a> and found that the documents in that corpus are simply tokens of words and do not contain any punctuation.
eg:</p>
<p><code>from gensim.test.utils import common_texts, common_dictionary, common_corpus</code></p>
<p><code>print(common_texts[0:10])</code></p>
<p>Output:</p>
<p><code>[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]</code></p>
<p>Same has been followed in the tutorial <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html</a>.
So is my approach of removing periods in the document valid, if so then how will the window parameter work because in the documentation it is defined as follows:
window (int, optional) – The maximum distance between the current and predicted word within a sentence.</p>
","python, gensim, word2vec, doc2vec","<p>Some people keep periods and other punctuation as standalone tokens, some eliminate them. </p>

<p>There's no definitively 'right' approach, and depending on your end goals, one or the other might make a slight difference in the doc-vector quality. So for now just do what's easiest for you, and then later if you have time, you can evaluate the alternate approach to see if it helps. </p>

<p>Despite any reference to 'sentences' in the docs, the <code>Word2Vec</code>/<code>Doc2Vec</code>/etc classes in <code>gensim</code> don't have any understanding of sentences, or special sensitivity to punctuation. They just see the lists-of-tokens you pass in as individual items in the corpus. So if you were to leave periods in, as in a short text like...</p>

<pre><code>['the', 'cat', 'was', 'orange', '.', 'it', 'meowed', '.']
</code></pre>

<p>...then the <code>'.'</code> string is just another pseudo-word, which will get a vector, and the training windows will reach through it just like any other word. (And, <code>'meowed'</code> will be 5 tokens away from <code>'cat'</code>, and thus have some influence if <code>window=5</code>.)</p>

<p>I don't quite understand what you mean about ""make use of First 50 to 400 tokens + 600 to 1000 tokens + last 250 tokens"". <code>Doc2Vec</code> works fine up to texts of 10000 tokens. (More tokens than that will be silently ignored, due to an internal implementation limit of <code>gensim</code>.) It's not necessary or typical to break docs into smaller chunks, unless you have some other need to model smaller chunks of text. </p>

<p>The tiny <code>common_texts</code> set of word-lists is a contrived, toy-sized bit of data to demonstrate some basic code usage - it's not an example of recommended practices. The demos based on the 'Lee' corpus are similarly a quick intro to a tiny and simple approach that's just barely sufficient to show basic usage and results. It's text tokenization – via the <code>simple_preprocess()</code> utility method – is an OK thing to try but not 'right' or 'best' compared to all the other possibilities.  </p>
",2,0,431,2020-04-05 10:19:12,https://stackoverflow.com/questions/61041080/significance-of-periods-in-sentences-while-training-documents-with-doc2vec
Gensim train word2vec and Fasttext,"<p>I need to train my own model with word2vec and fasttext. By readind different sourcs I found different information. 
So I did the model and trained it like this:</p>

<pre><code>model = FastText(all_words, size=300, min_count= 3,sg=1)
model = Word2Vec(all_words, min_count=3, sg = 1, size = 300 )
</code></pre>

<p>So I read that that should be enough to creat and train the model. But then I saw, that some people do it seperatly:</p>

<pre><code>model = FastText(size=4, window=3, min_count=1)  # instantiate
model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)  # train
</code></pre>

<p>Now I am confused and dont know if what I did is correct. Can sombody help me to make it clear? 
Thank you</p>
","python, machine-learning, gensim","<p>It's perfectly acceptable to supply your training corpus – <code>all_words</code> – when you instantiate the model object. In that case, the model will automatically perform all steps needed to train the model, using that data. So you can do this:</p>

<pre class=""lang-py prettyprint-override""><code>model = Word2Vec(all_words, ...)  # where '...' is your non-default params
</code></pre>

<p>It's also acceptable to not provide the corpus when instantiating the model - but then the model is extremely minimal, with just your initial parameters. It still needs to discover the relevant vocabulary (which requires a single pass over the training data), then allocate some vary-large internal structures to accommodate those words, then do the actual training (which requires multiple additional passes over the training data). </p>

<p>So if you don't provide the corpus when the model is instantiated, you should do <em>two</em> extra method calls:</p>

<pre class=""lang-py prettyprint-override""><code>model = Word2Vec(...)  # where '...' is your non-default params
model.build_vocab(all_words)  # discover vocabulary &amp; allocate model
# now train, with #-of-passes &amp; #-of-texts set by earlier steps
model.train(all_words, epochs=model.iter, total_examples=model.corpus_count)
</code></pre>

<p>These two code blocks I've shown are equivalent. The top does the usual steps for you; the bottom breaks the steps out into your explicit control. </p>

<p>(The code you'd excerpted in your question, showing <strong>only</strong> a <code>.train()</code> call, would error for a number of reasons. The <code>.build_vocab()</code> is a necessary step to have a fully-allocated model, and the call to <code>.train()</code> must explicitly state the desired <code>epochs</code> and an accurate count <code>total_examples</code> of the number-of-items in the corpus. But, you can and typically should re-use values that were already cached into the <code>model</code> by the two previous steps.)</p>

<p>It's your choice which approach to use. Generally people only use the 3-separate-steps process if they want to do other output/logging between the steps, or something advanced between the steps that might tamper with the model state. </p>
",2,0,84,2020-04-06 14:35:23,https://stackoverflow.com/questions/61062237/gensim-train-word2vec-and-fasttext
Improve performance of large document text tokenization through Python + RegEx,"<p>I'm currently trying to process a large amount of very big (>10k words) text files. In my data pipeline, I identified the gensim tokenize function as my bottleneck, the relevant part is provided in my MWE below:</p>

<pre><code>import re
import urllib.request

url='https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/genesis/english-web.txt'
doc=urllib.request.urlopen(url).read().decode('utf-8')

PAT_ALPHABETIC = re.compile('(((?![\d])\w)+)', re.UNICODE)

def tokenize(text):
    text.strip()
    for match in PAT_ALPHABETIC.finditer(text):
        yield match.group()

def preprocessing(doc):
    tokens = [token for token in tokenize(doc)]
    return tokens

foo=preprocessing(doc)
</code></pre>

<p>Calling the <code>preprocessing</code> function for the given example takes roughly <code>66ms</code> and I would like to improve this number. Is there anything I can still optimize in my code? Or is my hardware (Mid 2010s Consumer Notebook) the issue? I would be interested in the runtimes from people with some more recent hardware as well.</p>

<p>Thank you in advance</p>
","regex, python-3.x, nltk, gensim","<p>You may use</p>

<pre><code>PAT_ALPHABETIC = re.compile(r'[^\W\d]+')

def tokenize(text):
    for match in PAT_ALPHABETIC.finditer(text):
        yield match.group()
</code></pre>

<p>Note:</p>

<ul>
<li><code>\w</code> matches letters, digits, underscores, some other connector punctuation and diacritics in Python 3.x by default, you do not need to use <code>re.UNICODE</code> or <code>re.U</code> options</li>
<li>To ""exclude"" (or ""subtract"") digit matching from <code>\w</code>, the <code>((?!\d)\w)+</code> looks an overkill, all you need to do is to ""convert"" the <code>\w</code> into an equivalent negated character class, <code>[^\W]</code>, and add a <code>\d</code> there: <code>[^\W\d]+</code>.</li>
<li>Note the extraneous <code>text.strip()</code>: Python strings are immutable, if you do not assign a value to a variable, there is no use in <code>text.strip()</code>. Since whitespace in the input string do not interfere with the regex, <code>[^\W\d]+</code>, you may simply strip this <code>text.strip()</code> from your code.</li>
</ul>
",0,1,205,2020-04-07 13:14:53,https://stackoverflow.com/questions/61080872/improve-performance-of-large-document-text-tokenization-through-python-regex
saving word2vec in text format,"<p>I tried to save word2vec vector as text, but it didnt work out, I got an error, that I dont really understand, what duplicates appear here and what is this ""wv"", that is proposed. Maybe somone can explain is to me. Thank you in advance </p>

<pre><code>model = Word2Vec(all_words, min_count=3, sg = 1, size = 300 )
model.save_word2vec_format('test_w2v.txt', binary=False)
</code></pre>

<pre><code>WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
Word2Vec(vocab=20, size=300, alpha=0.025)
Traceback (most recent call last):
  File ""/word2vec.py"", line 26, in &lt;module&gt;
    model.save_word2vec_format('test_w2v.txt', binary=False)
  File ""/word2vec.py"", line 1307, in save_word2vec_format
    raise DeprecationWarning(""Deprecated. Use model.wv.save_word2vec_format instead."")
DeprecationWarning: Deprecated. Use model.wv.save_word2vec_format instead.

</code></pre>
","python, gensim, word2vec","<p>Because <code>.save_word2vec_format()</code> only saves the vectors – not the full model – it should only be used on the sub-property <code>.wv</code> of the model. (That's an object that just contains the vectors.)</p>

<p>So, if you run <code>model.wv.save_word2vec_format('test_w2v.txt', binary=False)</code>, as recommended by the error message, you'll save the vectors in text format.</p>

<p>(If you need to save the full model, use <code>model.save()</code> - it will save more information, and possibly use multiple additional files, but it will be in a Python- and gensim-specific format, unlike the plain text format that other tools can read.)</p>
",3,0,4528,2020-04-07 18:58:56,https://stackoverflow.com/questions/61087427/saving-word2vec-in-text-format
How to map detailed text to a unigram or a bigram,"<p>I am trying to figure out solution for requirement where in I am required to map long text to unigrams or bigrams. 
For example
""Ability to motivate and manage team. You should be able to track the progress of the team and intervene to improve the progress"". This long text should be mapped to ""Team management"". Basically I am trying to figure out communication/analytical skills from the long text seen in document like Job descriptions. I am struggling to figure out a solution for this. I do not want to hard code as the long text keep changing. Thanks for any help.</p>
","python-3.x, nlp, cluster-analysis, gensim, topic-modeling","<p>This seems like a multi-class multi-label classification problem. There is a lexical way of solving this problem. The article <a href=""https://www.listendata.com/2018/05/sentiment-analysis-using-python.html"" rel=""nofollow noreferrer"">here</a> will help you in solving the problem. One could skip the sentiment analysis part as the focus of this question is topic mapping.</p>
",0,0,159,2020-04-15 17:43:48,https://stackoverflow.com/questions/61235170/how-to-map-detailed-text-to-a-unigram-or-a-bigram
Plotting DBSCAN Clustering of Doc2Vec model,"<p>I have a Doc2Vec model created with Gensim and want to use <code>scikit-learn</code> DBSCAN to look for clustering of sentences within the model.</p>

<p>I'm struggling to work out how to best transform the model vectors to work with DBSCAN and plot clusters and am not finding many directly applicable examples on the web.</p>

<p>Here is what I have so far:</p>

<pre class=""lang-python prettyprint-override""><code>import gensim
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

fnIn = 'NLPModels/doc2VecModel_vector_size{0}_epochs{1}.bin'

def doCluster(vector_size, epochs):
    model = gensim.models.doc2vec.Doc2Vec.load(fnIn.format(vector_size, epochs))

    Y = model.docvecs.index2entity # tags

    X = [] # Document vectors
    for tag in Y:
        X.append(model.docvecs[tag])

    db = DBSCAN(eps=.1, min_samples=5, metric='cosine').fit_predict(X)
    labels = set(db)
    print(labels)


doCluster(100, 10)
</code></pre>

<p>Output: <code>{0, 1, -1}</code></p>

<p>Which I believe to be two clusters (0 and 1) and outliers (-1).</p>

<p>Am I going about this in the right way?</p>

<p>How would I plot this on a chart to visualise the clusters?</p>

<p>Thanks.</p>
","python, machine-learning, scikit-learn, gensim, dbscan","<p>There are two questions here:</p>

<ol>
<li><p>Visualization: I suggest you refine the <a href=""https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html"" rel=""nofollow noreferrer"">DBSCAN clustering example code</a></p></li>
<li><p>If you are doing the clustering correctly.
On the first glance - yes.</p></li>
</ol>
",0,1,1054,2020-04-16 15:13:45,https://stackoverflow.com/questions/61253758/plotting-dbscan-clustering-of-doc2vec-model
meaning of in_qsize and out_qsize in gensim word2vec log files,"<p>I am running word2vec models in gensim. I don't understand 2 metrics (in_qsize/out_qsize) reported by the log file. I've spent a bit of time searching and can't find an explanation. Here is a sample from my log files:</p>

<pre><code>2020-04-17 21:04:09,032 : INFO : EPOCH 5 - PROGRESS: at 68.67% examples, 657466 words/s, in_qsize 18, out_qsize 1
2020-04-17 21:04:10,038 : INFO : EPOCH 5 - PROGRESS: at 68.92% examples, 657527 words/s, in_qsize 20, out_qsize 0
2020-04-17 21:04:11,078 : INFO : EPOCH 5 - PROGRESS: at 69.14% examples, 657513 words/s, in_qsize 20, out_qsize 1
2020-04-17 21:04:12,136 : INFO : EPOCH 5 - PROGRESS: at 69.39% examples, 657458 words/s, in_qsize 18, out_qsize 1
2020-04-17 21:04:13,139 : INFO : EPOCH 5 - PROGRESS: at 69.68% examples, 657687 words/s, in_qsize 17, out_qsize 4
</code></pre>
","python, nlp, gensim","<p><code>in_qsize</code> and <code>out_qsize</code> are the lengths of two internal queues used by the code to send work to the worker-threads, and receive results.</p>

<p>Their names in the source code are <code>job_queue</code> and <code>progress_queue</code>, but you can also find that, and more about them, by searching through the source code for the lines that print <code>in_qsize</code> and <code>out_qsize</code>.</p>

<p>In general they are a sufficiently internal detail that most users won't need to care about their values – unless debugging some atypical performance issues. In some cases it could add a little more understanding to how different choices of corpus-preparation, <code>workers</code> value, or other parameters are affecting througput. But in general such optimization can just involve trying lots of different values to see which in practice achieves the best throughput, without caring about those internal queue sizes.</p>
",1,1,390,2020-04-17 21:10:55,https://stackoverflow.com/questions/61280748/meaning-of-in-qsize-and-out-qsize-in-gensim-word2vec-log-files
Is the Gensim word2vec model same as the standard model by Mikolov?,"<p>I am implementing a paper to compare our performance. In the paper, the uathor says </p>

<blockquote>
  <p>300-dimensional pre-trained word2vec vectors (Mikolov et al., 2013)</p>
</blockquote>

<p>I am wondering whether the pretrained word2vec Gensim model <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"" rel=""nofollow noreferrer"">here</a> is same as the pretrained embeddings on the official <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google site</a> (the GoogleNews-vectors-negative300.bin.gz file)</p>

<p><br>
My source of doubt arises from this line in Gensim documentation (in Word2Vec Demo section) </p>

<blockquote>
  <p>We will fetch the Word2Vec model trained on part of the Google News dataset, covering approximately 3 million words and phrases</p>
</blockquote>

<p>Does this mean the model on gensim is not fully trained? Is it different from the official embeddings by Mikolov? </p>
","python, nlp, gensim, word2vec","<p>That demo code for reading word-vectors is downloading the exact same Google-trained <code>GoogleNews-vectors-negative300</code> set of vectors. (No one else can try re-training that dataset, because the original corpus of news articles user, over 100B words of training data from around 2013 if I recall correctly, is internal to Google.)</p>

<p>Algorithmically, the <code>gensim</code> <code>Word2Vec</code> implementation was closely modeled after the <code>word2vec.c</code> code released by Google/Mikolov, so should match its results in measurable respects with regard to any newly-trained vectors. (Slight differences in threading approaches may have a slight difference.)</p>
",1,0,259,2020-04-19 11:28:03,https://stackoverflow.com/questions/61303579/is-the-gensim-word2vec-model-same-as-the-standard-model-by-mikolov
Is this a bug on gensim hdp model for python 3.8?,"<p>I want to use the HDP model from <code>gensim</code> to get the number of topics for my corpus, I already used this corpus and dictionary to train a regular LDA model from <code>gensim</code> and it works fine. But now when I do</p>

<pre><code>hdp = models.HdpModel(bow_corpus, dictionary)
</code></pre>

<p>I get</p>

<pre><code>Traceback (most recent call last):
  File ""models.py"", line 185, in &lt;module&gt;
    hdp = models.HdpModel(bow_corpus, dictionary)
  File ""/usr/lib/python3.8/site-packages/gensim/models/hdpmodel.py"", line 391, in __init__
    self.update(corpus)
  File ""/usr/lib/python3.8/site-packages/gensim/models/hdpmodel.py"", line 467, in update
    start_time = time.clock()
AttributeError: module 'time' has no attribute 'clock'
</code></pre>

<p>Is this a bug?</p>

<pre><code>$ python --version
Python 3.8.2 (default, Feb 26 2020, 22:21:03) 
</code></pre>

<p>Edit to add more system information</p>

<pre><code>&gt;&gt;&gt; print(gensim.__version__)
3.8.1

uname -a
Linux ** 5.5.9-arch1-2 #1 SMP PREEMPT Thu, 12 Mar 2020 23:01:33 +0000 x86_64 GNU/Linux
</code></pre>
","python-3.x, time, gensim, python-3.8","<p>You are coming across an issue caused by <a href=""https://docs.python.org/3.3/library/time.html#time.clock"" rel=""nofollow noreferrer"">deprecation of <code>clock</code> function of <code>time</code> module</a>. It has been <a href=""https://stackoverflow.com/questions/58569361/attributeerror-module-time-has-no-attribute-clock-in-python-3-8"">deprecated since Python v. 3.3 and removed in v. 3.8</a>.</p>

<p>To resolve it you have 2 options:</p>

<ol>
<li>Try to upgrade <code>gensim</code> if you do not have the newest version.</li>
<li>Try to downgrade Python.</li>
</ol>
",1,2,297,2020-04-19 19:17:45,https://stackoverflow.com/questions/61310229/is-this-a-bug-on-gensim-hdp-model-for-python-3-8
How to embed user names in word2vec model in gensim,"<p>I have some volunteer essay writings in the format of:</p>

<pre><code>volunteer_names, essay
[""emi"", ""jenne"", ""john""], [[""lets"", ""protect"", ""nature""], [""what"", ""is"", ""nature""], [""nature"", ""humans"", ""earth""]]
[""jenne"", ""li""], [[""lets"", ""manage"", ""waste""]]
[""emi"", ""li"", ""jim""], [[""python"", ""is"", ""cool""]]
...
...
...
</code></pre>

<p>I want to identify the similar users based on their essay writings. I feel like word2vec is more suitable in problems like this. However, since I want to embed user names too in the model I am not sure how to do it. The examples I found in the internet only uses the words (See example code).</p>

<pre><code>import gensim 
sentences = [['first', 'sentence'], ['second', 'sentence']]
# train word2vec on the two sentences
model = gensim.models.Word2Vec(sentences, min_count=1)
</code></pre>

<p>In that case, I am wondering if there is special way of doing this in word2vec or can I simply consider user names as just words to input to the model. please let me know your thoughts on this.</p>

<p>I am happy to provide more details if needed.</p>
","python, gensim, word2vec","<p>Word2vec infers the word representation from surrounding words: words similarly often appear in a similar company end up with similar vectors. Usually, a window of 5 words is considered. So, if you want to hack Word2vec, you would need to make sure that the student names will appear frequently enough (perhaps at a beginning and at the end of a sentence or something like that).</p>

<p>Alternatively, you can have a look at Doc2vec. During training, each document gets an ID and learns an embedding for the ID, they are in a lookup table as if they were word embeddings. If you use student names as document IDs, you would get student embeddings. If you have multiple essays from one student, I suppose you would need to hack Gensim a little bit not to have a unique ID for each essay.</p>
",2,0,997,2020-04-21 07:03:49,https://stackoverflow.com/questions/61337725/how-to-embed-user-names-in-word2vec-model-in-gensim
Load Doc2Vec without the docs vectors only for infer_vector,"<p>I have big gensim Doc2vec model, I only need to infer vectors while i am loading the training documents vectors from other source.
Is it possible to load it as is without the big npy file</p>

<p>I did </p>

<p><strong>Edit:</strong></p>

<pre><code>from gensim.models.doc2vec import Doc2Vec
model_path = r'C:\model/model'
model = Doc2Vec.load(model_path)
model.delete_temporary_training_data(keep_doctags_vectors=False, keep_inference=True)
model.save(model_path)
</code></pre>

<p>remove the files <code>(model.trainables.syn1neg.npy,model.wv.vectors.npy)</code> <strong>manually</strong></p>

<pre><code>model = Doc2Vec.load(model_path)
</code></pre>

<p>but it ask for </p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-5-7f868a7dbe0c&gt;"", line 1, in &lt;module&gt;
    model = Doc2Vec.load(model_path)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\models\doc2vec.py"", line 1113, in load
    return super(Doc2Vec, cls).load(*args, **kwargs)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\models\base_any2vec.py"", line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\models\base_any2vec.py"", line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\utils.py"", line 427, in load
    obj._load_specials(fname, mmap, compress, subname)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\utils.py"", line 458, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\gensim\utils.py"", line 469, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)

  File ""C:\ProgramData\Anaconda3\envs\py\lib\site-packages\numpy\lib\npyio.py"", line 428, in load
    fid = open(os_fspath(file), ""rb"")

FileNotFoundError: [Errno 2] No such file or directory: 'C:\\model/model.trainables.syn1neg.npy'
</code></pre>

<p>Note:
Those files not exists in the directory,
The model run on a server and download the model file from the storage
My question is, Do the model must have those files for inference?
I want to run it as low memory consumption as possible.
Thanks.</p>

<p><strong>Edit:</strong>
Is the file model.trainables.syn1neg.npy is the model weights?
Is the file model.wv.vectors.npy is necessary for running an inference? </p>
","gensim, doc2vec","<p>I'm not a fan of the <code>delete_temporary_training_data()</code> method. It implies there's a clearer separation between training-state and that needed for later uses. (Inference is very similar to training, though it doesn't need the cached doc-vectors for training texts.)</p>

<p>That said, if you've used that method, you shouldn't then be deleting any of the side-files that were still part of the save. If they were written by <code>.save()</code>, they'll be expected, by name, by the <code>.load()</code>. They must be kept with the main model file. (There might be fewer such files, or smaller such files, after the <code>delete_temporary_training_data()</code> call - but any written must be kept for reading.)</p>

<p>The <code>syn1neg</code> file is absolutely required for inference: it's the model's hidden-to-output weights, needed to perform new forward-predictions (and thus also backpropagated inference-adjustments). The <code>wv.vectors</code> file is definitely needed in default <code>dm=1</code> mode, where word-vectors are part of the doc-vector calculation. (It might be optional in <code>dm=0</code> mode, but I'm not sure the code is armored against  them being absent - not via in-memory trimming, and definitely not against the expected file being deleted out-of-band.)</p>
",2,1,1506,2020-04-22 14:37:26,https://stackoverflow.com/questions/61367839/load-doc2vec-without-the-docs-vectors-only-for-infer-vector
Supervised training and testing in GenSims FastText implementation,"<p>I am currently training a Gensim FastText model with a document from a certain domain with the unsupervised training method from Gensim. </p>

<p>After this training of the word representations i would like to train a set of sentence+label lines and ultimately test the model and return a precision and recall value like it is possible in facebooks fastText implementation via train_supervised + test. Does GenSims implementation support the supervised training and testing? I couldnt get it to work / find the required methods.</p>

<p>Any help is much appreciated.</p>
","machine-learning, text, classification, gensim, fasttext","<p>Gensim's <code>FastText</code> implementation has so far chosen not to support the same <code>supervised</code> mode of Facebook's original FastText, where known-labels can be used to drive the training of word-vectors – because <code>gensim</code> sees it focus as being unsupervised topic-modeling techniques.</p>
",2,1,613,2020-04-24 09:24:06,https://stackoverflow.com/questions/61405111/supervised-training-and-testing-in-gensims-fasttext-implementation
How to extract sentences which has similar meaning/intent compared against a example list of sentences,"<p>I have chat interaction [Utterances] between Customer and Advisor and would want to know if the advisor interactions contains particular sentences or similar sentences in the below list:</p>

<p>Example sentences i am looking for in the Advisor interactions  </p>

<pre><code>[""I would be more than happy to help you with this"",
""I would be happy to look over the account to see how I can help get this sorted out for you"",
""I’d be more than happy to look into this for you!"",
""Oh, I see, let me assist you with this concern."",
""I am more than happy to do everything I can to resolve this matter for you."",
""I would be happy to look over the account to see how I can help get this sorted out for you."",
""I am happy to have a look.""]


I have a dataset which contains the list of interaction_id and Utterances(Sample below)

```Example Chat interaction between Advisor and CLient : 
Client : Hello I would like to place an order for replacement battery
Agent: Hi Welcome to Battery service department. I would be happy to help you with your battery replacement Order.
</code></pre>

<p>How do get/Extract the sentences with similar intent or meaning.
I am newbie to NLP and i believe I have a sentences classification/Extraction problem in hand and would like to know is there any way i can achieve what i need</p>

<p>Basically I am trying to achieve the below:  </p>

<pre><code>ID    Utt                                               Help_Stmt_Present

IRJST   Hi Welcome to Battery service department. 
        I would be happy to help you with your battery
        replacement Order.                                     Yes 


</code></pre>
","python-3.x, nlp, gensim, doc2vec, sentence-similarity","<p>There could be multiple ways for doing this in multiple steps: <br>
<strong>1. Calculating sentence vectors</strong> <br></p>

<blockquote>
  <p>a. Using pretrained word embeddings(glove, word2vec, fasttext, etc) and calculating word embeddings for each word and then average it across words of the sentence to calculate the sentence embedding. <br><br>
  b. Use <a href=""https://colab.research.google.com/github/tensorflow/hub/blob/50bbebaa248cff13e82ddf0268ed1b149ef478f2/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb"" rel=""nofollow noreferrer"">Universal Sentence Encoder</a> to get the sentence embeddings. </p>
</blockquote>

<p><strong>2. Calculate similarity match</strong> <br></p>

<blockquote>
  <p>a. Calculate the distance between between the target and all other N sentences using euclidean or cosine or any other distance metric that works best for your problem. <br> <br>
  b. Train a KNN model with N sentence vectors you have and apply K-NN prediction with the target sentence to get K most similar sentences. </p>
</blockquote>

<p>To get even better results you can use deep learning based techniques and SOTA architectures such as transformers and the architectures built over it. You can checkout <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">this repository</a> which solves your task using transformers. Also to play with different architectures and other NLP tasks you can checkout the <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">Hugging Face Repository</a></p>
",0,2,1280,2020-04-26 22:25:59,https://stackoverflow.com/questions/61448908/how-to-extract-sentences-which-has-similar-meaning-intent-compared-against-a-exa
"word2vec, using document body or keywords as training corpus","<p>I would like to train a <code>word2vec</code> model using what is an unordered list of keywords and categories for each document. Therefore my vocabulary is quite small around 2.5k tokens.</p>

<p>Would the performance be improved if at the training step, I used actual sentences from the document?</p>

<p>From example:</p>

<pre><code>doc_keywords = ['beach', 'holiday', 'warm']
doc_body = 'Going on a beach holiday it can be very warm'
</code></pre>

<p>If there is a benefit to using the full documents, could someone also explain why this is the case?</p>

<p>Since the model predicts the next word in a document, what would be the benefit to it learning <code>very -&gt; warm</code> as two words which often come together, given that <code>very</code> is not in my vocabulary.</p>
","machine-learning, nlp, gensim, word2vec, doc2vec","<p>Your dataset seems quite small – perhaps too small to expect good word2vec vectors. But, a small dataset at least means it shouldn't take too much time to try things in many different ways. </p>

<p>So, the best answer (and the only one that truly takes into account whatever uniqueness might be in your data &amp; project goals): do you get better final word-vectors, for your project-specific needs, when training on just the keywords, or the longer documents?</p>

<p>Two <em>potential</em> sources of advantage from using the full texts:</p>

<ul>
<li><p>Those less-interesting words might still help tease-out subtleties of meaning in the full vector space. For example, a contrast between <code>'warm'</code> and <code>'hot'</code> might become clearer when those words are forced to predict other related words that co-occur with each in different proportions. (But, such qualities of word2vec vectors require lots of subtly-varied real usage examples – so such a benefit might not be possible in a small dataset.)</p></li>
<li><p>Using the real texts preserves the original proximity-influences – words nearer each other have more influence. The keywords-only approach might be scrambling those original proximities, depending on how you're turning raw full texts into your reduced keywords. (In particular, you definitely do <em>not</em> want to always report keywords in some database-sort order – as that would tend to create a spurious influence between keywords that happen to sort next-to each other, as opposed to appear next-to each other in natural language.)</p></li>
</ul>

<p>On the other hand, including more words makes the model larger &amp; the training slower, which might limit the amount of training or experiments you can run. And, keeping very-rare words – that don't have enough varied usage examples to get good word-vectors themselves – tends to act like 'noise' that dilutes the quality of other word-vectors. (That's why dropping rare words, with a <code>min_count</code> similiar to its default of <code>5</code> – or larger in larger corpuses – is almost always a good idea.)</p>

<p>So, there's no sure answer for which will be better: different factors, and other data/parameter/goals choices, will pull different ways. You'll want to try it in multiple ways. </p>
",1,0,385,2020-04-27 14:03:20,https://stackoverflow.com/questions/61460683/word2vec-using-document-body-or-keywords-as-training-corpus
Segmentation Fault with Gensim,"<p>I am currently getting a segmentation fault when I am loading a model with gensim. In order to create the model and save it, I do:</p>

<pre><code>glove_file = 'QGModels/embeddings/glove.6B.300d.txt'
tmp_file = 'QGModels/embeddings/word2vec-glove.6B.300d.txt'
glove2word2vec(glove_file, tmp_file)
model = KeyedVectors.load_word2vec_format(tmp_file)
model.save('QGModels/embeddings/model.model')
</code></pre>

<p>However the problem starts when I load the model and use the most_similar method using:</p>

<pre><code>model = KeyedVectors.load('QGModels/embeddings/model.model')
closestWords = model.most_similar(positive=[answer], topn=count)
</code></pre>

<p>And then get a segmentation fault:</p>

<blockquote>
  <p>Process finished with exit code 137 (interrupted by signal 9: SIGKILL)</p>
</blockquote>

<p>Any and all help is appreciated! Thank You.</p>
","python, segmentation-fault, gensim","<p>Looks like I had too many Firefox tabs open. Closing most of them helped fix the Segmentation Fault.</p>
",0,0,226,2020-04-29 18:48:56,https://stackoverflow.com/questions/61509408/segmentation-fault-with-gensim
How does gensim manage to find the most similar words so fast?,"<p>Let's say we train a model with more than 1 million words. In order to find the most similar words we need to calculate the distance between the embedding of the test word and embeddings of all the 1 million words words, and then find the nearest words. It seems that Gensim calculate the results very fast. Although when I want to calculate the most similar, my function is extremely slow:</p>

<pre><code>def euclidean_most_similars (model, word, topn = 10):
  distances = {}
  vec1 = model[word]
  for item in model.wv.vocab:
    if item!= node:
      vec2 = model[item]
      dist = np.linalg.norm(vec1 - vec2)
      distances[(node, item)] = dist
  sorted_distances = sorted(distances.items(), key=operator.itemgetter(1))
</code></pre>

<p>I would like to know how Gensim manages to calculate the most nearest words so fast and what is an efficient way to calculate the most similares.</p>
","python, time-complexity, gensim, word2vec, similarity","<p>As @g-anderson commented, the <code>gensim</code> source can be reviewed to see exactly what it does. However, <code>gensim</code> is not actually using any of its own optimized Cython or compiled-C code as part of its <code>most_similar()</code> method – which can be reviewed at:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/b287fd841c31d0dfa899d784da0bd5b3669e104d/gensim/models/keyedvectors.py#L689"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/b287fd841c31d0dfa899d784da0bd5b3669e104d/gensim/models/keyedvectors.py#L689</a></p>
<p>Instead, by using <code>numpy</code>/<code>scipy</code> bulk array operations, those libraries' highly optimized code will take advantage of both CPU primitives and multithreading to calculate <em>all</em> the relevant similarities far faster than an interpreted Python loop.</p>
<p>(The key workhorse is the <code>numpy</code> <code>dot</code> operation: one call which creates an ordered array of all the similarities – skipping the loop &amp; your interim-results <code>dict</code> entirely. But the <code>argsort</code>, passing through to <code>numpy</code> implementations as well, likely also outperforms the idiomatic <code>sorted()</code>.)</p>
",1,3,1523,2020-04-29 20:29:46,https://stackoverflow.com/questions/61511101/how-does-gensim-manage-to-find-the-most-similar-words-so-fast
Build the corpus by Wikipedia: ModuleNotFoundError: No module named &#39;gensim&#39;,"<p>I copy a simple Python script by <a href=""https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html"" rel=""nofollow noreferrer"">Building a Wikipedia Text Corpus for Natural Language Processing</a> to build the corpus by stripping all Wikipedia markup from the articles, using gensim. This is the cose:</p>

<pre><code>""""""
Creates a corpus from Wikipedia dump file.
Inspired by:
https://github.com/panyang/Wikipedia_Word2vec/blob/master/v1/process_wiki.py
""""""

import sys
from gensim.corpora import WikiCorpus

    def make_corpus(in_f, out_f):

    """"""Convert Wikipedia xml dump file to text corpus""""""

    output = open(out_f, 'w')
    wiki = WikiCorpus(in_f)

    i = 0
    for text in wiki.get_texts():
        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
        i = i + 1
        if (i % 10000 == 0):
            print('Processed ' + str(i) + ' articles')
    output.close()
    print('Processing complete!')


if __name__ == '__main__':

    if len(sys.argv) != 3:
        print('Usage: python make_wiki_corpus.py &lt;wikipedia_dump_file&gt; &lt;processed_text_file&gt;')
        sys.exit(1)
    in_f = sys.argv[1]
    out_f = sys.argv[2]
    make_corpus(in_f, out_f)
</code></pre>

<p>Anyway, I obtained the error:</p>

<pre><code>ModuleNotFoundError: No module named 'gensim'
</code></pre>

<p>although I have installed the <code>gensim</code> package:</p>

<pre><code>python3 -m pip install gensim
</code></pre>

<p><strong>EDIT</strong>. If I try with</p>

<pre><code>pip install -U gensim
</code></pre>

<p>I obtain the error</p>

<pre><code> ImportError: cannot import name 'SourceDistribution' from 
 'pip._internal.distributions.source' (C:\Users\Standard\Anaconda3\lib\site- 
 packages\pip\_internal\distributions\source\__init__.py)
</code></pre>
","python, gensim","<p>You do not have the <code>gensim</code> module installed in your system.</p>

<pre><code>pip install -U gensim
</code></pre>

<p>Or download it from <a href=""https://pypi.python.org/pypi/gensim"" rel=""nofollow noreferrer"">https://pypi.python.org/pypi/gensim</a>.</p>

<p><code>gensim</code> depends on <code>scipy</code> and <code>numpy</code>. You must have them installed prior to installing <code>gensim</code>. </p>

<p>There is a bug in <code>pip 20.0.0</code>. Either upgrade to 20.0.1 using:</p>

<pre><code>python get-pip.py
</code></pre>

<p>Or downgrade to 19.3.1.</p>

<pre><code>python get-pip.py pip==19.3.1
</code></pre>
",1,0,385,2020-05-03 09:41:08,https://stackoverflow.com/questions/61572397/build-the-corpus-by-wikipedia-modulenotfounderror-no-module-named-gensim
Calculating cosine similarity from a Gensim model,"<p>I'm trying to calculate a between-topic cosine similarity score from a <code>Gensim</code> LDA topic model, but this proves more complicated than I first expected.</p>

<p><code>Gensim</code> has a method to calculate distances between topics <code>model.diff(model)</code>, but unfortunately cosine distance is not implemented; it has jaccard distance, but it is a bit too vector-length dependent (i.e., when comparing top 100 most important words per topic the distance is lower than comparing top 500, and the distance is 0 when full-length vectors are compared, as each topic includes all terms, but with different probabilities).</p>

<p>My problem is that the output from the model looks like this (only shown 4 top words):</p>

<pre><code>(30, '0.008*""tax"" + 0.004*""cut"" + 0.004*""bill"" + 0.004*""spending""')
(18, '0.009*""candidate"" + 0.009*""voter"" + 0.009*""vote"" + 0.009*""election""')
(42, '0.047*""shuttle"" + 0.034*""astronaut"" + 0.026*""launch"" + 0.025*""orbit""')
(22, '0.023*""boat"" + 0.020*""ship"" + 0.015*""migrant"" + 0.013*""vessel""')
</code></pre>

<p>So, in order to calculate the cosine sim/distance, I would have to parse the second element of the tuple (i.e., the  <code>'0.008*""tax"" +...'</code> part, which indicates term probabilities. </p>

<p>I was wondering whether there is an easier way to get cosine similarity out of the model? Or parsing each individual string of term/probabilities is really the only way to go?</p>

<p>Thanks for the help.</p>
","python, gensim, topic-modeling, cosine-similarity","<p>The <code>get_topics()</code> method gives you a full (sparse) array where each row is a topic, and each column a vocabulary word. So you may be able to calculate topic-to-topic cosine-similarities something roughly like:</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.metrics.pairwise import cosine_similarity

topics = lda_model.get_topics()
sim_18_to_30 = cosine_similarity(topics[18], topics[30])   # topic 18 to topic 30
all_sims = cosine_similarity(topics)  # all pairwise similarities
</code></pre>

<p>(I haven't checked this code on a live model; exact required shapes/etc may be off.)</p>
",2,2,5912,2020-05-04 15:42:09,https://stackoverflow.com/questions/61596101/calculating-cosine-similarity-from-a-gensim-model
Save a gensim LDA model to s3,"<p>I've got an LDA model through using gensim. I can save it locally:</p>

<pre><code>ldamodel.save('models/lda/lda.model')
</code></pre>

<p>This results in four files in the specified place:</p>

<pre><code>lda.model
lda.model.expElogbeta.npy
lda.model.id2word
lda.model.state
</code></pre>

<p>Loading them back is as simple as </p>

<pre><code>ldamodel =  models.LdaModel.load('models/lda/lda.model')
</code></pre>

<p>However, I want this model to be saved on s3. I can work out how to save individual bits, for example:</p>

<pre><code>s3.meta.client.upload_file('models/lda/lda.model', 'bucket-name', 'lda.model')
</code></pre>

<p>But I can't work out how to actually meaningfully read them back in so they will function as expected as a coherent model. So the idea being that somebody other than me could take the files from s3 and use them as a model in Python. </p>

<p>Can anybody help?</p>
","python, amazon-s3, gensim","<p>Why won't you implement a class that takes into account the logical relationship of the separate files comprising the model and treat it as one?</p>

<p>e.g.</p>

<pre><code>class LdaModel:
   def __init__(self, lda_local_path, s3bucket_name, s3bucket_obj_prefix):
       self.local_path = lda_local_path
       self.s3bucket = s3bucket_name
       self.s3bucket_obj_pre = s3bucket_obj_prefix

    def upload(self):
        for fl in os.listdir(self.local_path):
            fl_local = os.path.join(self.local_path, fl)
            s3.meta.client.upload_file(fl_local, self.s3bucket, self.s3bucket_obj_pre + '__' + fl_local)

    def download(self): 
        ...
</code></pre>

<p>(I left out <code>download</code> method for you to implemented by analogy to the <code>upload</code> one).</p>

<p>Another approach is to implement a zip-wrapper that will compress the 4 files and store them as one in S3.</p>
",1,0,612,2020-05-06 15:22:45,https://stackoverflow.com/questions/61638940/save-a-gensim-lda-model-to-s3
How to compare cosine similarities across three pretrained models?,"<p>I have two corpora - one with all women leader speeches and the other with men leader speeches. I would like to test the hypothesis that cosine similarity between two words in the one corpus is significantly different than cosine similarity between the same two words in another corpus. Is such a t-test (or equivalent) logical and possible?</p>

<p>Further, if the cosine similarities are different across the two corpora, how could I examine if cosine similarity between the same two words in a third corpus is more similar to the first or the second corpus?</p>
","nlp, gensim, word2vec, word-embedding, glove","<p>It's certainly <em>possible</em>. Whether it's meaningful, given a certain amount of data, is harder to answer. </p>

<p>Note that in separate training sessions, a given word <em>A</em> won't necessarily wind up in the same coordinates, due to inherent randomness used by the algorithm. That's even the case when training on the <em>exact same data</em>. </p>

<p>It's just the case that in general, the distances/directions to other words <em>B</em>, <em>C</em>, etc should be of similar overall usefulness, when there's sufficient data/training and well-chosen parameters. So <em>A</em>, <em>B</em>, <em>C</em>, etc may be in different places, with slightly-different distances/directions – but the relative relationships are still similar, in terms of neighborhoods-of-words, or the <em>(A-B)</em> direction still be predictive of certain human-perceptible meaning-differences if applied to other words <em>C</em> etc. </p>

<p>So, you should avoid making direct cosine-similarity comparisons between words from different training-runs or corpuses, but you may find meaning in differences in similarities ( <em>A-B</em> vs <em>A' - B'</em> ) or top-N lists or relative-rankings. (This could also be how to compare against 3rd corpora: to what extent is there variance or correlation in certain pairwise-similarities, or top-N lists, or ordinal ranks of relevant words in each other words' 'most similar' results.)</p>

<p>You might want to perform a sanity check on your measures, by seeing to what extent they imply meaningful differences in comparisons where they logically ""shouldn't"". For example, multiple runs against the exact same corpus that's just bee reshuffled, or against random subsets of the exact same corpus. (I'm not aware of anything as formal as a 't-test' in checking the significance of differences between word2vec models, but checking whether some differences are enough to distinguish a truly-different corpus, from just a 1/Nth random subset of the same corpus, to a certain confidence level might be a grounded way to assert meaningful differences.)</p>

<p>To the extent such ""oughtta be very similar"" runs instead show end vector results that are tangibly different, it could be suggestive that either:</p>

<ul>
<li><p>the corpus is too small, with too few varied usage examples per word - word2vec benefits from lots of data, and political speech collections may be quite small compared to the sometimes hundreds-of-billions training words used for large word2vec models</p></li>
<li><p>the model is mis-parameterized - an oversized (and thus prone to overfitting) model, or insufficient training passes, or other suboptimal parameters may yield models that vary more for the same training data</p></li>
</ul>

<p>You'd also want to watch out for mismatches in training-corpus size. A corpus that's 10x as large means many more words would pass a fixed <code>min_count</code> threshold, and any chosen <em>N</em> <code>epochs</code> of training will involve 10x as many examples of common-words, and support stable results in a larger (vector-size) model - whereas the same model parameters with a smaller corpus would give more volatile results. </p>

<p>Another technique you could consider would be combining corpuses into one training set, but munging the tokens of key words-of-interest to be different depending on the relevant speaker. For example, you'd replace the word <code>'family'</code> with <code>'f-family'</code> or <code>'m-family'</code>, depending on the gender of the speaker. (You might do this for every occurrence, or some fraction of the occurrences. You might also enter each speech into your corpus more than once, sometimes with the actual words and sometimes with some-or-all replaced with the context-labeled alternates.)</p>

<p>In that case, you'd wind up with one final model, and all words/context-tokens in the 'same' coordinate space for direct comparison. But, the pseudowords <code>'f-family'</code> and <code>'m-family'</code> would have been more influenced by their context-specific usages - and thus their vectors might vary from each other, and from the original <code>'family'</code> (if you've also retained unmunged instances of its use) in interestingly suggestive ways.</p>

<p>Also note: if using the 'analogy-solving' methods of the original Google word2vec code release, or other libraries that have followed its example (like <code>gensim</code>), note that it specifically <em>won't</em> return as an answer any of the words supplied as input. So when solving the gender-fraught analogy <code>'man' : 'doctor' :: 'woman' : _?_</code>, via the call <code>model.most_similar(positive=['doctor', 'woman'], negative=['man'])</code>, even if the underlying model <em>still</em> has <code>'doctor'</code> as the closest word to the target-coordinates, it is automatically skipped as one of the input words, yielding the second-closest word instead. </p>

<p>Some early ""bias-in-word-vectors"" write-ups ignored this detail and thus tended to imply larger biases, due to this implementation artifact, even where such biases small-to-nonexistent. (You can supply raw vectors, instead of string-tokens, to <code>most_similar()</code> - and then get full results, without any filtering of input-tokens.)</p>
",4,1,1401,2020-05-11 18:38:20,https://stackoverflow.com/questions/61736874/how-to-compare-cosine-similarities-across-three-pretrained-models
Python Gensim FastText Saving and Loading Model,"<p>I am working with Gensim FASTText modeling and have the following questions.</p>

<ul>
<li>The output of ""ft_model.save(BASE_PATH + MODEL_PATH + fname)"" saves the following 3 files. Is this correct? is there a way to combine all three files? </li>
</ul>

<blockquote>
<pre><code>ft_gensim-v3
ft_gensim-v3.trainables.vectors_ngrams_lockf.npy
ft_gensim-v3.wv.vectors_ngrams.npy
</code></pre>
</blockquote>

<p>When I attempt to load the training file and then use it, I get the following error from <code>if model.wv.similarity(real_data, labelled['QueryText'][i]) &gt; maxSimilaity:</code>  </p>

<blockquote>
  <p>'function' object has no attribute 'wv'</p>
</blockquote>

<p>Finally, both models, is there a way not to have to store the output of <code>def read_train(path,label_path)</code> and <code>def lemmetize(df_col)</code>so I do not have to run this part of the code every time I want to train the model or compare? </p>

<p>Thanks for the assistance. </p>

<p><strong>Here is my FastText Train Model</strong></p>

<pre><code>import os
import logging
from config import BASE_PATH, DATA_PATH, MODEL_PATH
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from pprint import pprint as print
from gensim.models.fasttext import FastText as FT_gensim
from gensim.test.utils import datapath

#Read Training data
import pandas as pd
def read_train(path,label_path):
    d = []
    #e = []
    df = pd.read_excel(path)
    labelled = pd.read_csv(label_path)
    updated_col1 = lemmetize(df['query_text'])
    updated_col2 = lemmetize(labelled['QueryText'])
    for i in range(len(updated_col1)):
        d.append(updated_col1[i])
        #print(d)
    for i in range(len(updated_col2)):
        d.append(updated_col2[i])
    return d


from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import string
from nltk.stem import PorterStemmer

def lemmetize(df_col):
    df_updated_col = pd.Series(0, index = df_col.index)
    stop_words = set(stopwords.words('english'))
    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()
    ps = PorterStemmer()
    for i, j in zip(df_col, range(len(df_col))):
        lem = []
        t = str(i).lower()
        t = t.replace(""'s"","""")
        t = t.replace(""'"","""")
        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))
        t = t.translate(translator)
        word_tokens = word_tokenize(t)
        for i in range(len(word_tokens)):
            l1 = lemmatizer.lemmatize(word_tokens[i])
            s1 = ps.stem(word_tokens[i])
            if list(l1) != [''] and list(l1) != [' '] and l1 != '' and l1 != ' ':
                lem.append(l1)
        filtered_sentence = [w for w in lem if not w in stop_words]
        df_updated_col[j] = filtered_sentence
    return df_updated_col

#read test data
def read_test(path):
    return pd.read_excel(path)


#Read labelled data
def read_labelled(path):
    return pd.read_csv(path)


word_tokenized_corpus = read_train('Train Data.xlsx','SMEQueryText.csv')


#Train fasttext model
import tempfile
import os

from gensim.models import FastText
from gensim.test.utils import get_tmpfile
fname = get_tmpfile(""ft_gensime-v3"")

def train_fastText(data, embedding_size = 60, window_size = 40, min_word = 5, down_sampling = 1e-2, iter=100):
    ft_model = FastText(word_tokenized_corpus,
                      size=embedding_size,
                      window=window_size,
                      min_count=min_word,
                      sample=down_sampling,
                      sg=1,
                      iter=100)

    #with tempfile.NamedTemporaryFile(prefix=BASE_PATH + MODEL_PATH + 'ft_gensim_v2-', delete=False) as tmp:
    #    ft_model.save(tmp.name, separately=[])
    ft_model.save(BASE_PATH + MODEL_PATH + fname)
    return ft_model


# main function to output
def main(test_path, train_path, labelled):
    test_data = read_test(test_path)
    train_data = read_train(train_path,labelled)
    labelled = read_labelled(labelled)
    output_df = pd.DataFrame(index = range(len(test_data)))
    output_df['test_query'] = str()
    output_df['Similar word'] = str()
    output_df['category'] = str()
    output_df['similarity'] = float()
    model = train_fastText(train_data)

# run main
if __name__ == ""__main__"":
    output = main('Test Data.xlsx','Train Data.xlsx','QueryText.csv')
</code></pre>

<p><strong>Here is my Usage Model</strong></p>

<pre><code>import pandas as pd
from gensim.models import FastText
import gensim
from config import BASE_PATH, DATA_PATH, MODEL_PATH

#Read Training data
def read_train(path,label_path):
    d = []
    #e = []
    df = pd.read_excel(path)
    labelled = pd.read_csv(label_path)
    updated_col1 = lemmetize(df['query_text'])
    updated_col2 = lemmetize(labelled['QueryText'])
    for i in range(len(updated_col1)):
        d.append(updated_col1[i])
    for i in range(len(updated_col2)):
        d.append(updated_col2[i])
    return d

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import string
from nltk.stem import PorterStemmer

def lemmetize(df_col):
    df_updated_col = pd.Series(0, index = df_col.index)
    stop_words = set(stopwords.words('english'))
    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()
    ps = PorterStemmer()
    for i, j in zip(df_col, range(len(df_col))):
        lem = []
        t = str(i).lower()
        t = t.replace(""'s"","""")
        t = t.replace(""'"","""")
        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))
        t = t.translate(translator)
        word_tokens = word_tokenize(t)
        for i in range(len(word_tokens)):
            l1 = lemmatizer.lemmatize(word_tokens[i])
            s1 = ps.stem(word_tokens[i])
            if list(l1) != [''] and list(l1) != [' '] and l1 != '' and l1 != ' ':
                lem.append(l1)
        filtered_sentence = [w for w in lem if not w in stop_words]
        df_updated_col[j] = filtered_sentence
    return df_updated_col

#read test data
def read_test(path):
    return pd.read_excel(path)

#Read labelled data
def read_labelled(path):
    return pd.read_csv(path)

def load_training():
    return FT_gensim.load(BASE_PATH + MODEL_PATH +'ft_gensim-v3')

#compare similarity
def compare_similarity(model, real_data, labelled):
    maxWord = ''
    category = ''
    maxSimilaity = 0
    #print(""train data"",labelled[1])
    for i in range(len(labelled)):
        if model.similarity(real_data, labelled['QueryText'][i]) &gt; maxSimilaity:
            #print('labelled',labelled['QueryText'][i], 'i', i)
            maxWord = labelled['QueryText'][i]
            category = labelled['Subjectmatter'][i]
            maxSimilaity = model.similarity(real_data, labelled['QueryText'][i])

    return maxWord, category, maxSimilaity

# Output from Main to excel
from pandas import ExcelWriter
def export_Excel(data, aFile = 'FASTTEXTOutput.xlsx'):
    df = pd.DataFrame(data)
    writer = ExcelWriter(aFile)
    df.to_excel(writer,'Sheet1')
    writer.save()

# main function to output
def main(test_path, train_path, labelled):
    test_data = read_test(test_path)
    train_data = read_train(train_path,labelled)
    labelled = read_labelled(labelled)
    output_df = pd.DataFrame(index = range(len(test_data)))
    output_df['test_query'] = str()
    output_df['Similar word'] = str()
    output_df['category'] = str()
    output_df['similarity'] = float()
    model = load_training
    for i in range(len(test_data)):
        output_df['test_query'][i] = test_data['query_text'][i]
        #&lt;first change&gt;
        maxWord, category, maxSimilaity = compare_similarity(model, str(test_data['query_text'][i]), labelled)
        output_df['Similar word'][i] = maxWord
        output_df['category'][i] = category
        output_df['similarity'][i] = maxSimilaity
    #&lt;second change&gt;    
    return output_df

# run main
if __name__ == ""__main__"":
    output = main('Test Data.xlsx','Train Data.xlsx','SMEQueryText.csv')
    export_Excel(output)
</code></pre>

<p><strong>Here is the full tracible error message</strong></p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-22-57803b59c0b9&gt; in &lt;module&gt;
      1 # run main
      2 if __name__ == ""__main__"":
----&gt; 3     output = main('Test Data.xlsx','Train Data.xlsx','SMEQueryText.csv')
      4     export_Excel(output)

&lt;ipython-input-21-17cb88ee0f79&gt; in main(test_path, train_path, labelled)
     13         output_df['test_query'][i] = test_data['query_text'][i]
     14         #&lt;first change&gt;
---&gt; 15         maxWord, category, maxSimilaity = compare_similarity(model, str(test_data['query_text'][i]), labelled)
     16         output_df['Similar word'][i] = maxWord
     17         output_df['category'][i] = category

&lt;ipython-input-19-84d7f268d669&gt; in compare_similarity(model, real_data, labelled)
      6     #print(""train data"",labelled[1])
      7     for i in range(len(labelled)):
----&gt; 8         if model.wv.similarity(real_data, labelled['QueryText'][i]) &gt; maxSimilaity:
      9             #print('labelled',labelled['QueryText'][i], 'i', i)
     10             maxWord = labelled['QueryText'][i]

AttributeError: 'function' object has no attribute 'wv'
</code></pre>
","python, gensim, fasttext","<p>You've got three separate, only-vaguely-related questions here. Taking each in order:</p>

<ul>
<li>Why are there 3 files, and can they be combined?</li>
</ul>

<p>It's more efficient to store the big raw arrays separately from the main 'pickled' model – and for models above a few gigabytes in size, necessary to work-around 'pickle' implementation limits. So I'd recommend just keeping the default behavior, and keeping the habit of managing/moving/copying the sets of files together.</p>

<p>If your model is small enough, there is something you can try, though. The <code>.save()</code> method has an optional parameter <code>sep_limit</code> which controls the threshold array size, over which arrays are stored as separate files. By setting that much larger, say <code>sep_limit=2*1024*1024*1024</code> (2GiB), smaller models should save a single file. (But, loading will be slower, you won't have the sometimes-useful option of memory-map loading, and saving may break on oversized models.)</p>

<ul>
<li>Why is there a <code>AttributeError: 'function' object has no attribute 'wv'</code> error?</li>
</ul>

<p>Your line of code <code>model = load_training</code> assigns an actual function to the <code>model</code> variable, rather than what you probably intended, the return-value of calling that function with some arguments. That function has no <code>.wv</code> attribute, hence the error. If <code>model</code> were an actual instance of <code>FastText</code>, you'd not get that error.</p>

<ul>
<li>Can the corpus text be stored to avoid repeat preprocessing and conversion from pandas formats? </li>
</ul>

<p>Sure, you can just write the text to a file. Roughly:</p>

<pre class=""lang-py prettyprint-override""><code>with open('mycorpus.txt', mode='w') as corpusfile:
    for text in word_tokenized_corpus:
        corpusfile.write(' '.join(text))
        corpusfile.write('\n')
</code></pre>

<p>Though in fact, <code>gensim</code> offers a utility function, <code>utils.save_as_line_sentence()</code>, that can do this (&amp; explicitly handles some extra encoding concerns). See:</p>

<p><a href=""https://radimrehurek.com/gensim/utils.html#gensim.utils.save_as_line_sentence"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/utils.html#gensim.utils.save_as_line_sentence</a></p>

<p>The <code>LineSentence</code> utility class in <code>gensim.models.word2vec</code> can stream texts from such a file back for future re-use:</p>

<p><a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence</a></p>
",0,0,1541,2020-05-12 07:56:42,https://stackoverflow.com/questions/61746512/python-gensim-fasttext-saving-and-loading-model
Gensim saving word vectors in txt format error,"<p>My issue is the following. I have some pretrained vectors saved in txt format, I load them in a dict. But when I try to save them after training them again in gensim it gives me an error, like the following:</p>

<pre><code>UnicodeDecodeError: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)
</code></pre>

<p>I'm using this code to create the gensim word2vec:</p>

<pre><code>w2vObject = gensim.models.Word2Vec(min_count=1, sample=threshold, sg=1,size=dimension, negative=15, iter=epochsNum, window=3) # create only the shell

print('Starting vocab build')
# t = time()
w2vObject.build_vocab(sentences, progress_per=10000) #here is the vocab being built as told in google groups gensim

print(w2vObject.wv['the'], 'before train')
</code></pre>

<p>Then I'm replacing the current untrained vectors with:</p>

<pre><code>f = codecs.open(f'../../../WordNetGraphHD/StorageEmbeddings/EmbeddingFormat{dimension}.txt')##os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))
embeddings_index = {}
for num, line in enumerate(f):
    values = my_split(line) # line.split('\t')
    word = values[0].rstrip()
    # vector = ''.join(num for num in values[1:])
    vector = values[1]
    if len(vector) != 300:
        print(line, 'here not 300')

    else:
        coefs = np.asarray(vector)

f.close()
</code></pre>

<p>This code replaces the untrained random vectors wit my own pretrained:</p>

<pre><code>i = 0
for elem in w2vObject.wv.vocab:
    if elem in embeddings_index.keys():
        w2vObject.wv[elem] = embeddings_index[elem]
        i += 1
        print('Found one', i)

print(i)
</code></pre>

<p>Next I train them again with gensim:</p>

<pre><code>w2vObject.train(sentences, total_examples=w2vObject.corpus_count, epochs=epochsNum)#w2vObject.iter)
</code></pre>

<p>Finally save them:</p>

<pre><code>print(w2vObject.wv, 'after train')
w2vObject.wv.save_word2vec_format('./GensimOneWNet.txt', binary=False)
print('saved')
</code></pre>

<p>If I don't replace the vectors with my own saving works, but I need to replace them and save them as txt, any help?</p>

<p>EDIT:</p>

<p>Here is my_split() function:</p>

<pre><code>def my_split(s):
    return list(re.split(""-?\d+.?\d*(?:[Ee]-\d+)?"", s))[0] ,list(re.findall(""-?\d+.?\d*(?:[Ee]-\d+)?"", s))
</code></pre>

<p>And here is a bit of data 300 dimensions for the embedding_index:</p>

<pre><code>'hood -0.013093032778433955 -0.004199660490964164 -0.013285915004532987 0.004154925177649314 -0.004331536946207293 -0.013220217973950956 -0.004774150107654365 0.004774714449991327 0.0040749706101727646...
's gravenhage 0.01400977963089465 -0.0047073654478706935 -0.004326147699308312 0.01323622314514233 -0.004702524319745591 0.004695915697719624 0.00497792763673179 -0.004391661500805715 0.0046651111592470...
'tween 0.008467020793348493 -0.008027116343722267 0.007882368315816719 0.00754852526967863 0.008563484027417608 0.00812782576892597 0.008192394872536986 0.0075759585496093206...
</code></pre>

<p>Added code here:
<a href=""https://pastebin.com/GKPnENxv"" rel=""nofollow noreferrer"">Python code runs fine without my vectors, crashes with them</a></p>

<p>Populate embedding_index I go through all the words and vectors in the txt and if for some reason a vector is not 300 dim, skip it:</p>

<pre><code>f = codecs.open(f'../../../WordNetGraphHD/StorageEmbeddings/EmbeddingFormat{dimension}.txt', encoding='utf-8')##os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))
embeddings_index = {}
for num, line in enumerate(f):
    values = my_split(line) # line.split('\t')
    word = values[0].rstrip()
    vector = values[1]
    if len(vector) != 300:
        print(line, 'here not 300')
    else:
        coefs = np.asarray(vector)
        embeddings_index[word] = coefs

f.close()
</code></pre>

<p>EDIT2:
Here is the stack trace of the full error:
Traceback (most recent call last):</p>

<pre><code>  File ""GensimTestSave.py"", line 136, in &lt;module&gt;
    w2vObject.wv.save_word2vec_format('./GensimOneWNet.txt', binary=False) #encoding='utf-8' )
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/keyedvectors.py"", line 1453, in save_word2vec_format
    fname, self.vocab, self.vectors, fvocab=fvocab, binary=binary, total_vec=total_vec)
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/utils_any2vec.py"", line 291, in _save_word2vec_format
    fout.write(utils.to_utf8(""%s %s\n"" % (word, ' '.join(repr(val) for val in row))))
  File ""/home/pedalo/anaconda3/envs/ltu/lib/python3.7/site-packages/gensim/models/utils_any2vec.py"", line 291, in &lt;genexpr&gt;
    fout.write(utils.to_utf8(""%s %s\n"" % (word, ' '.join(repr(val) for val in row))))
UnicodeDecodeError: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)
</code></pre>
","python, gensim, word2vec","<p>I fixed it now. Apparently I was trying to use a nd.array() but of strings as coefficients and gensim uses an nd.array(floats) that was the problem where my own vectors when switching  to the .wv[] were of type [str]. So It ended up being empty.</p>

<p>The switching of vectors now is done like:</p>

<pre><code>for elem in setIntersection:
    if len(embeddings_index[elem]) != 300:
        print('here', elem) #cast it to the fire
    w2vObject.wv[elem] = np.asarray(embeddings_index[elem], dtype=np.float32)
print('Done!!!')
</code></pre>

<p>Thanks for your comments they helped me figure it out.</p>
",0,0,195,2020-05-18 16:01:41,https://stackoverflow.com/questions/61873864/gensim-saving-word-vectors-in-txt-format-error
Cannot load Doc2vec object using gensim,"<p>I am trying to load a pre-trained Doc2vec model using gensim and use it to map a paragraph to a vector. I am referring to <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">https://github.com/jhlau/doc2vec</a> and the pre-trained model I downloaded is the English Wikipedia DBOW, which is also in the same link. However, when I load the Doc2vec model on wikipedia and infer vectors using the following code:</p>

<pre><code>import gensim.models as g
import codecs

model=""wiki_sg/word2vec.bin""
test_docs=""test_docs.txt""
output_file=""test_vectors.txt""

#inference hyper-parameters
start_alpha=0.01
infer_epoch=1000

#load model
test_docs = [x.strip().split() for x in codecs.open(test_docs, ""r"", ""utf-8"").readlines()]
m = g.Doc2Vec.load(model)

#infer test vectors
output = open(output_file, ""w"")
for d in test_docs:
    output.write("" "".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + ""\n"")
output.flush()
output.close()
</code></pre>

<p>I get an error:</p>

<pre><code>/Users/zhangji/Desktop/CSE547/Project/NLP/venv/lib/python2.7/site-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function
  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL
Traceback (most recent call last):
  File ""/Users/zhangji/Desktop/CSE547/Project/NLP/AbstractMapping.py"", line 19, in &lt;module&gt;
    output.write("" "".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + ""\n"")
AttributeError: 'Word2Vec' object has no attribute 'infer_vector'
</code></pre>

<p>I know there are couple of threads regarding the infer_vector issue on stack overflow, but none of them resolved my problem. I downloaded the gensim package using</p>

<pre><code>pip install git+https://github.com/jhlau/gensim
</code></pre>

<p>In addition, after I looked at the source code in gensim package, I found that when I use Doc2vec.load(), the Doc2vec class doesn't really have a load() function by itself, but since it is a subclass of Word2vec, it calls the super method of load() in Word2vec and then make the model m a Word2vec object. However, the infer_vector() function is unique to Doc2vec and does not exist in Word2vec, and that's why it is causing the error. I also tried casting the model m to a Doc2vec, but I got this error:</p>

<pre><code>&gt;&gt;&gt; g.Doc2Vec(m)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/doc2vec.py"", line 599, in __init__
    self.build_vocab(documents, trim_rule=trim_rule)
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/word2vec.py"", line 513, in build_vocab
    self.scan_vocab(sentences, trim_rule=trim_rule)  # initial survey
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/doc2vec.py"", line 635, in scan_vocab
    for document_no, document in enumerate(documents):
  File ""/Users/zhangji/Library/Python/2.7/lib/python/site-packages/gensim/models/word2vec.py"", line 1367, in __getitem__
    return vstack([self.syn0[self.vocab[word].index] for word in words])
TypeError: 'int' object is not iterable
</code></pre>

<p>In fact, all I want with gensim for now is to convert a paragraph to a vector using a pre-trained model that works well on academic articles. For some reasons I don't want to train the models on my own. I would be really grateful if someone can help me resolve the issue.</p>

<p>Btw, I am using python2.7, and the current gensim version is 0.12.4.</p>

<p>Thanks!</p>
","python, gensim, word2vec, doc2vec","<p>I would avoid using either the 4-year-old nonstandard gensim fork at <a href=""https://github.com/jhlau/doc2vec"" rel=""nofollow noreferrer"">https://github.com/jhlau/doc2vec</a>, or any 4-year-old saved models that only load with such code.</p>
<p>The Wikipedia DBOW model there is also suspiciously small at 1.4GB. Wikipedia had well over 4 million articles even 4 years ago, and a 300-dimensional <code>Doc2Vec</code> model trained to have doc-vectors for the 4 million articles would be at least <code>4000000 articles * 300 dimensions * 4 bytes/dimension</code> = 4.8GB in size, not even counting other parts of the model. (So, that download is clearly <em>not</em> the 4.3M doc, 300-dimensional model mentioned in the associated paper – but something that's been truncated in other unclear ways.)</p>
<p>The current gensim version is 3.8.3, released a few weeks ago.</p>
<p>It'd likely take a bit of tinkering, and an overnight or more runtime, to build your own <code>Doc2Vec</code> model using current code and a current Wikipedia dump - but then you'd be on modern supported code, with a modern model that better understands words coming into use in the last 4 years. (And, if you trained a model on a corpus of the exact kind of documents of interest to you – such as academic articles – the vocabulary, word-senses, and match to your own text-preprocessing to be used on later inferred documents will all be better.)</p>
<p>There's a Jupyter notebook example of building a <code>Doc2Vec</code> model from Wikipedia that either functional or very-close-to-functional inside the <code>gensim</code> source tree at:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>
",0,2,954,2020-05-20 19:43:49,https://stackoverflow.com/questions/61921588/cannot-load-doc2vec-object-using-gensim
Why does a Gensim Doc2vec object return empty doctags?,"<p>My question is <strong>how I should interpret my situation?</strong></p>

<p>I trained a Doc2Vec model following this tutorial <a href=""https://blog.griddynamics.com/customer2vec-representation-learning-and-automl-for-customer-analytics-and-personalization/"" rel=""nofollow noreferrer"">https://blog.griddynamics.com/customer2vec-representation-learning-and-automl-for-customer-analytics-and-personalization/</a>. </p>

<p>For some reason, <code>doc_model.docvecs.doctags</code> returns <code>{}</code>. But <code>doc_model.docvecs.vectors_docs</code> seems to return a proper value.</p>

<p>Why the doc2vec object doesn't return any doctags but vectors_docs?</p>

<p>Thank you for any comments and answers in advance.</p>

<p>This is the code I used to train a Doc2Vec model.</p>

<pre><code>from gensim.models.doc2vec import LabeledSentence, TaggedDocument, Doc2Vec
import timeit
import gensim

embeddings_dim = 200    # dimensionality of user representation

filename = f'models/customer2vec.{embeddings_dim}d.model'
if TRAIN_USER_MODEL:

    class TaggedDocumentIterator(object):
        def __init__(self, df):
           self.df = df
        def __iter__(self):
            for row in self.df.itertuples():
                yield TaggedDocument(words=dict(row._asdict())['all_orders'].split(),tags=[dict(row._asdict())['user_id']])

    it = TaggedDocumentIterator(combined_orders_by_user_id)

    doc_model = gensim.models.Doc2Vec(vector_size=embeddings_dim, 
                                      window=5, 
                                      min_count=10, 
                                      workers=mp.cpu_count()-1,
                                      alpha=0.055, 
                                      min_alpha=0.055,
                                      epochs=20)   # use fixed learning rate

    train_corpus = list(it)

    doc_model.build_vocab(train_corpus)

    for epoch in tqdm(range(10)):
        doc_model.alpha -= 0.005                    # decrease the learning rate
        doc_model.min_alpha = doc_model.alpha       # fix the learning rate, no decay
        doc_model.train(train_corpus, total_examples=doc_model.corpus_count, epochs=doc_model.iter)
        print('Iteration:', epoch)

    doc_model.save(filename)
    print(f'Model saved to [{filename}]')

else:
    doc_model = Doc2Vec.load(filename)
    print(f'Model loaded from [{filename}]')
</code></pre>

<p><code>doc_model.docvecs.vectors_docs</code> returns<a href=""https://i.sstatic.net/bN6r4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bN6r4.jpg"" alt=""enter image description here""></a></p>
","gensim, doc2vec","<p>If all of the <code>tags</code> you supply are plain Python ints, those ints are used as the direct-indexes into the vectors-array. </p>

<p>This saves the overhead of maintaining a mapping from arbitrary tags to indexes.</p>

<p>But, it may also cause an over-allocation of the vectors array, to be large enough for the largest int tag you provided, even if other lower ints are never used. (That is: if you provided a single document, with a <code>tags=[1000000]</code>, it will allocate an array sufficient for tags 0 to 1000000, even if most of those never appear in your training data.)</p>

<p>If you want <code>model.docvecs.doctags</code> to collect a list of all your tags, use string tags rather than plain ints. </p>

<p>Separately: don't call <code>train()</code> multiple times in your own loop, or manage the <code>alpha</code> learning-rate in your own code, unless you have an overwhelmingly good reason to do so. It's inefficient &amp; error-prone. (Your code, for example, is actually performing 200 training-epochs, and if you were to increase the loop count without carefully adjusting your <code>alpha</code> increment, you could wind up with nonsensical negative <code>alpha</code> values – a very common error in code following this bad practice. Call <code>.train()</code> once with your desired number of epochs. Set the <code>alpha</code> and <code>min_alpha</code> at reasonable starting and nearly-zero values – probably just the defaults unless you're sure your change is helping – and then leave them alone.</p>
",2,2,552,2020-05-25 16:54:27,https://stackoverflow.com/questions/62007088/why-does-a-gensim-doc2vec-object-return-empty-doctags
gensim - fasttext - Why `load_facebook_vectors` doesn&#39;t work?,"<p>I've tried to load pre-trained FastText vectors from <a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""noreferrer"">fastext - wiki word vectors</a>.</p>

<p>My code is below, and it works well. </p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import FastText
model = FastText.load_fasttext_format('./wiki.en/wiki.en.bin')
</code></pre>

<p>but, the warning message is a little annoying. </p>

<pre><code>gensim_fasttext_pretrained_vector.py:13: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings)
</code></pre>

<p>The message said, <code>load_fasttext_format</code> will be deprecated so, it will be better to use <code>load_facebook_vectors</code>. </p>

<p>So I decided to changed the code. and My changed code is like below.</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import FastText
model = FastText.load_facebook_vectors('./wiki.en/wiki.en.bin')
</code></pre>

<p><strong>But</strong>, the error occurred, the error message is like this. </p>

<pre><code>Traceback (most recent call last):
  File ""gensim_fasttext_pretrained_vector.py"", line 13, in &lt;module&gt;
    model = FastText.load_facebook_vectors('./wiki.en/wiki.en.bin')
AttributeError: type object 'FastText' has no attribute 'load_facebook_vectors'
</code></pre>

<p>I couldn't understand why these thing happen.
I just change what the messages said, but it doesn't work. 
If you know anything about this, please let me know. </p>

<p>Always, thanks for you guys help. </p>
","python, gensim, fasttext","<p>You're almost there, you need to change two things:</p>

<ul>
<li>First of all, it's <code>fasttext</code> all lowercase letters, not <code>Fasttext</code>.</li>
<li>Second of all, to use <code>load_facebook_vectors</code>, you need first to create a <code>datapath</code> object before using it.</li>
</ul>

<p>So, you should do like so:</p>

<pre><code>from gensim.models import fasttext
from gensim.test.utils import datapath

wv = fasttext.load_facebook_vectors(datapath(""./wiki.en/wiki.en.bin""))
</code></pre>
",8,8,5239,2020-05-28 07:27:12,https://stackoverflow.com/questions/62059196/gensim-fasttext-why-load-facebook-vectors-doesnt-work
I want to Train 4 more Word2vec models and average the resulting embedding matrices,"<p>I wrote the code below, I used Used spacy to restrict the words in the tweets to content words, i.e., nouns, verbs, and adjectives. Transform the words to lower case and add the POS with an underderscore. E.g.:</p>

<p>love_VERB old-fashioneds_NOUN</p>

<p>now I want to Train 4 more Word2vec models and average the resulting embedding matrices.
but I dont have any idea for it, can you help me please ?</p>

<pre><code># Tokenization of each document
from gensim.models.word2vec import FAST_VERSION
from gensim.models import Word2Vec
import spacy
import pandas as pd
from zipfile import ZipFile
import wget

url = 'https://raw.githubusercontent.com/dirkhovy/NLPclass/master/data/reviews.full.tsv.zip'
wget.download(url, 'reviews.full.tsv.zip')

with ZipFile('reviews.full.tsv.zip', 'r') as zf:
    zf.extractall()

# nrows , max amount of rows
df = pd.read_csv('reviews.full.tsv', sep='\t', nrows=100000)
documents = df.text.values.tolist()

nlp = spacy.load('en_core_web_sm')  # you can use other methods
# excluded tags
included_tags = {""NOUN"", ""VERB"", ""ADJ""}


vocab = [s for s in new_sentences]

sentences = documents[:103]  # first 10 sentences
new_sentences = []
for sentence in sentences:
    new_sentence = []
    for token in nlp(sentence):
        if token.pos_ in included_tags:
            new_sentence.append(token.text.lower()+'_'+token.pos_)
    new_sentences.append(new_sentence)


# initialize model
w2v_model = Word2Vec(
                     size=100,
                     window=15,
                     sample=0.0001,
                     iter=200,
                     negative=5,
                     min_count=1,  # &lt;-- it seems your min_count was too high
                     workers=-1,
                     hs=0
                     )


new_sentences


w2v_model.build_vocab(vocab)

w2v_model.train(vocab, 
                total_examples=w2v_model.corpus_count, 
                epochs=w2v_model.epochs)
w2v_model.wv['car_NOUN']
</code></pre>
","python, pandas, nlp, spacy, gensim","<p>There's no reason to average together vectors from multiple training runs; it is more likely to destroy any value from the individual runs than provide any benefit. </p>

<p>No one run creates the 'right' final positions, nor do they all approach some idealized positions. Rather, each just creates a set-of-vectors that is internally comparable to others in that same co-trained set. Comparisons or combinations with vectors from other, non-interleaved training runs are usually going to be nonsense. </p>

<p>Instead, aim for one adequate run. If vectors move around a lot, in repeated runs, that's normal. But each reconfiguration should be about as useful, if used for word-to-word comparisons, or analysis of word neighborhoods/directions, or as input to downstream algorithms. If they vary wildly in usefulness, there are likely other inadequacies in the data or model parameters. (For example: too little data – word2vec requires lots to give meaningful results – or a model that's too large for the data – making it prone to overfitting.)</p>

<p>Other observations about your setup:</p>

<ul>
<li><p>Just 103 sentences/texts is tiny for word2vec; you shouldn't expect the vectors from such a run to have any of the value that the algorithm would usually provide. (Running such a tiny dataset might be helpful for verifying no halting-errors in the process, or familiarize yourself with the steps/APIs, but the results will tell you nothing.)</p></li>
<li><p><code>min_count=1</code> is almost always a bad idea in word2vec and similar algorithms. Words that only appear once (or a few times) don't have the variety of subtly-different uses that are needed to train it into a balanced position against other words – so they wind up with weak/strange final positions, and the sheer number of such words dilutes the training effectiveness for other more-frequent words. The common practice of discarding rare words usually gets better results. </p></li>
<li><p><code>iter=200</code> is an extreme choice which is typically only valuable to try to squeeze results out of inadequate data. (In such a case, you might also have to reduce the vector-<code>size</code> from normal 100-plus dimensions.) So if you seem to need that, getting more data should be a top priority. (Using 20x more data is far, far better than using 20x more iterations on smaller data – but involves the same amount of training time.)</p></li>
</ul>
",1,0,187,2020-05-29 11:39:24,https://stackoverflow.com/questions/62085134/i-want-to-train-4-more-word2vec-models-and-average-the-resulting-embedding-matri
Least Similar with Gensim Doc2Vec,"<p>The <code>most_similar</code> method finds the top-N most similar words.</p>

<p>Is there a method or a way to find the N least similar words?</p>
","gensim, doc2vec","<p>You could get the full ranked list of all vectors by similarity, using a <code>topn</code> parameter as large as the full set of vectors. Then look at just the last N. For example:</p>

<pre class=""lang-py prettyprint-override""><code>import sys
all_sims = vec_model.most_similar(target_value, topn=sys.maxsize)
last_10 = list(reversed(all_sims[-10:]))
</code></pre>

<p>However, note:</p>

<ul>
<li><p>This will require a bit more sorting, &amp; momentarily need a lot more memory, to return the full list before trimming it to the last few</p></li>
<li><p>These are unlikely to be especially meaningful, as either words or documents, to human perception. That is, it's unlikely to be a word's or document's 'opposite' in the senses we perceive. Such opposites, or indeed any words/docs that are interestingly contrasted with an origin point, are usually going to be quite close to the origin in the high-dimensional space, just shifted in some meaningful way. (For example, a word's antonyms are far closer to the word than the most-dissimilar words this will find.)</p></li>
</ul>
",1,1,1287,2020-06-01 20:01:45,https://stackoverflow.com/questions/62140106/least-similar-with-gensim-doc2vec
Is there a way to infer topic distributions on unseen document from gensim LDA pre-trained model using matrix multiplication?,"<p>Is there a way to get the topic distribution of an unseen document using a pretrained LDA model without using the LDA_Model[unseenDoc] syntax? I am trying to implement my LDA model into a web application, and if there was a way to use matrix multiplication to get a similar result then I could use the model in javascript.</p>

<p>For example, I tried the following:</p>

<pre><code>import numpy as np
import gensim
from gensim.corpora import Dictionary
from gensim import models
import nltk
from nltk.stem import WordNetLemmatizer, SnowballStemmer
nltk.download('wordnet')


def Preprocesser(text_list):

    smallestWordSize = 3
    processedList = []

    for token in gensim.utils.simple_preprocess(text_list):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; smallestWordSize:
            processedList.append(StemmAndLemmatize(token))

    return processedList

lda_model = models.LdaModel.load('LDAModel\GoldModel')  #Load pretrained LDA model
dictionary = Dictionary.load(""ModelTrain\ManDict"")      #Load dictionary model was trained on

#Sample Unseen Doc to Analyze
doc = ""I am going to write a string about how I can't get my task executor \
to travel properly. I am trying to use the \
AGV navigator, but it doesn't seem to be working network. I have been trying\
to use the AGV Process flow but that isn't working either speed\
trailer offset I am now going to change this so I can see how fast it runs""

termTopicMatrix = lda_model.get_topics()    #Get Term-topic Matrix from pretrained LDA model
cleanDoc = Preprocesser(doc)                #Tokenize, lemmatize, clean and stem words
bowDoc = dictionary.doc2bow(cleanDoc)       #Create bow using dictionary
dictSize = len(termTopicMatrix[0])          #Get length of terms in dictionary
fullDict = np.zeros(dictSize)               #Initialize array which is length of dictionary size
First = [first[0] for first in bowDoc]      #Get index of terms in bag of words
Second = [second[1] for second in bowDoc]   #Get frequency of term in bag of words
fullDict[First] = Second                    #Add word frequency to full dictionary


print('Matrix Multiplication: \n', np.dot(termTopicMatrix,fullDict))
print('Conventional Syntax: \n', lda_model[bowDoc])

Output:
Matrix Multiplication: 
 [0.0283254  0.01574513 0.03669142 0.01671816 0.03742738 0.01989461
 0.01558603 0.0370233  0.04648389 0.02887623 0.00776652 0.02147539
 0.10045133 0.01084273 0.01229849 0.00743788 0.03747379 0.00345913
 0.03086953 0.00628912 0.29406082 0.10656977 0.00618827 0.00406316
 0.08775404 0.00785408 0.02722744 0.09957815 0.01669402 0.00744392
 0.31177135 0.03063149 0.07211428 0.01192056 0.03228589]
Conventional Syntax: 
 [(0, 0.070313625), (2, 0.056414187), (18, 0.2016589), (20, 0.46500313), (24, 0.1589748)]
</code></pre>

<p>In the pretrained model there are 35 topics and 1155 words.</p>

<p>In the ""Conventional Syntax"" output, the first element of each tuple is the index of the topic and the second element is the probability of the topic. In the ""Matrix Multiplication"" version, the probability is the index and the value is the probability. Clearly the two don't match up.</p>

<p>For example, the lda_model[unseenDoc] shows that topic 0 has a 0.07 probability, but the matrix multiplication method says that topic has a 0.028 probability. Am I missing a step here?</p>
","gensim, lda, topic-modeling","<p>You can review the full source code used by <code>LDAModel</code>'s <code>get_document_topics()</code> method in your installation, or online at:</p>

<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/e75f6c8e8d1dee0786b1b2cd5ef60da2e290f489/gensim/models/ldamodel.py#L1283"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/e75f6c8e8d1dee0786b1b2cd5ef60da2e290f489/gensim/models/ldamodel.py#L1283</a></p>

<p>(It also makes use of the <code>inference()</code> method in the same file.)</p>

<p>It's doing a lot more scaling/normalization/clipping than your code, which is likely the cause of the discrepancy. But you should be able to examine, line-by-line, where your process &amp; its differ to get the steps to match up.</p>

<p>It also shouldn't be hard to use the gensim code's steps as guidance for creating parallel Javascript code that, given the right parts of the model's state, can reproduce its results. </p>
",1,0,866,2020-06-04 17:00:29,https://stackoverflow.com/questions/62200198/is-there-a-way-to-infer-topic-distributions-on-unseen-document-from-gensim-lda-p
How does gensim word2vec word embedding extract training word pair for 1 word sentence?,"<p>Refer to below image (the process of how word2vec skipgram extract training datasets-the word pair from the input sentences). </p>

<p>E.G. ""I love you."" ==> [(I,love), (I, you)]</p>

<p>May I ask what is the word pair when the sentence contains only one word? </p>

<p>Is it  ""Happy!"" ==> [(happy,happy)] ?</p>

<p>I tested the word2vec algorithm in genism, when there is just one word in the training set sentences, (and this word is not included in other sentences), the word2vec algorithm can still construct an embedding vector for this specific word. I am not sure how the algorithm is able to do so.</p>

<p><a href=""https://i.sstatic.net/zQPX6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zQPX6.png"" alt=""enter image description here""></a></p>

<p>===============UPDATE===============================</p>

<p>As the answer posted below, I think the word embedding vector created for the word in the 1-word-sentence is just the random initialization of neural network weights.</p>
","nlp, text-mining, gensim, word2vec, word-embedding","<p>No word2vec training is possible from a 1-word sentence, because there's no neighbor words to use as input to predict a center/target word. Essentially, that sentence is skipped.</p>

<p>If that was the only appearance of the word in the corpus, and you're seeing a vector for that word, it's just the starting random-initialization of the word, with no further training. (And, you should probably use a higher <code>min_count</code>, as keeping such rare words is usually a mistake in word2vec: they won't get good vectors, and other nearby words' vectors will improve if the 'noise' from all such insufficiently model-able rare words is removed.)</p>

<p>If that 1-word sentence actually appeared next-to other real sentences in your corpus, it could make sense to combine it with surrounding texts. There's nothing magic about actual sentences for this kind word-from-surroundings modeling - the algorithm is just working on 'neighbors', and it's common to use multi-sentence chunks as the texts for training, and sometimes even punctuation (like sentence-ending periods) is also retained as 'words'. Then words from an actually-separate sentence – but still related by having appeared in the same document – will appear in each other's contexts.</p>
",1,0,728,2020-06-05 08:42:07,https://stackoverflow.com/questions/62211396/how-does-gensim-word2vec-word-embedding-extract-training-word-pair-for-1-word-se
Models generate different results when moving to Azure Machine Learning Studio,"<p>We developed a Jupyter Notebook in a local machine to train models with the Python (V3) libraries <code>sklearn</code> and <code>gensim</code>.
As we set the <code>random_state</code> variable to a fixed integer, the results were always the same.</p>

<p>After this, we tried moving the notebook to a workspace in Azure Machine Learning Studio (classic), but the results differ even if we leave the <code>random_state</code> the same.</p>

<p>As suggested in the following links, we installed the same libraries versions and checked the <code>MKL</code> version was the same and the <code>MKL_CBWR</code> variable was set to <code>AUTO</code>.</p>

<p><a href=""https://stackoverflow.com/questions/46766714/t-sne-generates-different-results-on-different-machines"">t-SNE generates different results on different machines</a></p>

<p><a href=""https://stackoverflow.com/questions/38228088/same-python-code-same-data-different-results-on-different-machines"">Same Python code, same data, different results on different machines</a></p>

<p>Still, we are not able to get the same results.</p>

<p>What else should we check or why is this happening?</p>

<p><strong>Update</strong></p>

<p>If we generate a <code>pkl</code> file in the local machine and import it in AML, the results are the same (as the intention of the pkl file is).</p>

<p>Still, we are looking to get the same results (if possible) without importing the pkl file.</p>

<p><strong>Library versions</strong></p>

<pre><code>gensim 3.8.3.
sklearn 0.19.2.
matplotlib 2.2.3.
numpy 1.17.2.
scipy 1.1.0.
</code></pre>

<p><strong>Code</strong></p>

<p>Full code can be found <a href=""https://t.ly/YlCi"" rel=""nofollow noreferrer"">here</a>, sample data link inside.</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib
from matplotlib import pyplot as plt

from gensim.models import KeyedVectors
%matplotlib inline

import time

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import seaborn as sns

wordvectors_file_vec = '../libraries/embeddings-new_large-general_3B_fasttext.vec'
wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)

math_quests = # some transformations using wordvectors

df_subset = pd.DataFrame()

pca = PCA(n_components=3, random_state = 42)
pca_result = pca.fit_transform(mat_quests)
df_subset['pca-one'] = pca_result[:,0]
df_subset['pca-two'] = pca_result[:,1] 

time_start = time.time()
tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state = 42)
tsne_results = tsne.fit_transform(mat_quests)

df_subset['tsne-2d-one'] = tsne_results[:,0]
df_subset['tsne-2d-two'] = tsne_results[:,1]

pca_50 = PCA(n_components=50, random_state = 42)
pca_result_50 = pca_50.fit_transform(mat_quests)
print('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))

time_start = time.time()
tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300, random_state = 42)
tsne_pca_results = tsne.fit_transform(pca_result_50)
print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))
</code></pre>
","python, scikit-learn, gensim, azure-machine-learning-service","<p>Definitely empathize with the issue you're having. Every data scientist has struggled with this at some point.</p>

<p>The hard truth I have for you is that Azure ML Studio (classic) isn't really capable of  solving this ""works on my machine"" problem. However, the good news is that Azure ML Service is incredible at it. Studio classic doesn't let you define custom environments deterministically, only add and remove packages (and not so well even at that) </p>

<p>Because ML Service's execution is built on top of <code>Docker</code> containers and <code>conda</code> environments, you can feel more confident in repeated results. I highly recommend you take the time to learn it (and I'm also happy to debug any issues that come up). Azure's <a href=""https://github.com/Azure/MachineLearningNotebooks"" rel=""nofollow noreferrer"">MachineLearningNotebooks repo</a> has a lot of great tutorials for getting started.</p>

<p>I spent two hours making <a href=""https://github.com/swanderz/MachineLearningNotebooks/blob/SO_CPR/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"" rel=""nofollow noreferrer"">a proof of concept</a> that demonstrate how ML Service solves the problem you're having by synthesizing:</p>

<ul>
<li>your code sample (before you shared your notebook),</li>
<li><a href=""https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py"" rel=""nofollow noreferrer"">Jake Vanderplas's sklearn example</a>, and</li>
<li><a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"" rel=""nofollow noreferrer"">this Azure ML tutorial</a> on remote training.</li>
</ul>

<p>I'm no T-SNE expert, but from the screenshot below, you can see that the t-sne outputs are the same when I run the script locally and remotely. This might be possible with Studio classic, but it would be hard to guarantee that it will always work.</p>

<p><a href=""https://i.sstatic.net/mhlg6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mhlg6.png"" alt=""Azure ML Experiment Results Page""></a></p>
",1,1,382,2020-06-06 17:23:19,https://stackoverflow.com/questions/62235365/models-generate-different-results-when-moving-to-azure-machine-learning-studio
"Word embedding with gensim and FastText, training on pretrained vectors","<p>I am trying to load the pretrained vec file of Facebook fasttext crawl-300d-2M.vec with the next code:</p>

<pre><code>from gensim.models.fasttext import load_facebook_model, load_facebook_vectors

model_facebook = load_facebook_vectors('fasttext/crawl-300d-2M.vec')
</code></pre>

<p>But it fails with the next error:</p>

<pre><code>NotImplementedError: Supervised fastText models are not supported
</code></pre>

<p>It is not possible to load this vector?</p>

<p>If it is possible, afterwards can I train it with my own sentences?</p>

<p>Thanks in advance.</p>

<p>Whole error trace:</p>

<pre><code>---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-181-f8262e0857b8&gt; in &lt;module&gt;
----&gt; 1 model_facebook = load_facebook_vectors('fasttext/crawl-300d-2M.vec')

/opt/conda/lib/python3.7/site-packages/gensim/models/fasttext.py in load_facebook_vectors(path, encoding)
   1196 
   1197     """"""
-&gt; 1198     model_wrapper = _load_fasttext_format(path, encoding=encoding, full_model=False)
   1199     return model_wrapper.wv
   1200 

/opt/conda/lib/python3.7/site-packages/gensim/models/fasttext.py in _load_fasttext_format(model_file, encoding, full_model)
   1220     """"""
   1221     with gensim.utils.open(model_file, 'rb') as fin:
-&gt; 1222         m = gensim.models._fasttext_bin.load(fin, encoding=encoding, full_model=full_model)
   1223 
   1224     model = FastText(

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in load(fin, encoding, full_model)
    339         model.update(dim=magic, ws=version)
    340 
--&gt; 341     raw_vocab, vocab_size, nwords, ntokens = _load_vocab(fin, new_format, encoding=encoding)
    342     model.update(raw_vocab=raw_vocab, vocab_size=vocab_size, nwords=nwords, ntokens=ntokens)
    343 

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in _load_vocab(fin, new_format, encoding)
    192     # Vocab stored by [Dictionary::save](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc)
    193     if nlabels &gt; 0:
--&gt; 194         raise NotImplementedError(""Supervised fastText models are not supported"")
    195     logger.info(""loading %s words for fastText model from %s"", vocab_size, fin.name)
    196 

NotImplementedError: Supervised fastText models are not supported
</code></pre>
","python, gensim, word-embedding, fasttext","<p>I believe, but am not certain, that in this particular case you're getting this error because you're trying to load a set of just-plain vectors (which FastText projects tend to name as files ending <code>.vec</code>) with a method that's designed for use on the FastText-specific format that includes subword/model info.</p>

<p>As a result, it's misinterpreting the file's leading bytes as declaring the model as one using FastText's '-supervised' mode. (Gensim truly doesn't support such full models, in that less-common mode. But it could load the end-vectors from such a model, and in any case your file isn't truly from that mode.)</p>

<p>Released files that will work with <code>load_facebook_vectors()</code> typically end with <code>.bin</code>. See the docs for this method for more details:</p>

<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors</a></p>

<p>So, you could either:</p>

<ul>
<li><p>Supply an alternate <code>.bin</code>-named, Facebook-FastText-formatted set of vectors (with subword info) to this method. (From a quick look at their download options, I believe their file analogous to your 1st try would  be named <code>crawl-300d-2M-subword.bin</code> &amp; be about 7.24GB in size.)</p></li>
<li><p>Load the file you have, with just its full-word vectors, via:</p></li>
</ul>

<pre class=""lang-py prettyprint-override""><code>    from gensim.models import KeyedVectors
    model = KeyedVectors.load_word2vec_format('fasttext/crawl-300d-2M.vec', binary=False)
</code></pre>

<p>In this latter case, no FastText-specific features (like the synthesis of guess-vectors for out-of-vocabulary words using subword vectors) will be available - but that info isn't in the 'crawl-300d-2M.vec' file, anyway. (Those features would be available if you used the larger <code>.bin</code> file &amp; <code>.load_facebook_vectors()</code> method above.)</p>
",4,2,2021,2020-06-10 16:24:41,https://stackoverflow.com/questions/62308418/word-embedding-with-gensim-and-fasttext-training-on-pretrained-vectors
Saving FastText custom model binary with Gensim,"<p>I am trying to save a custom FastText model trained with gensim. I want to save the binary files to have the possibility of training again the model, if it may.</p>

<p>The code to save the binary file is the next one:</p>

<pre><code>from gensim.models.fasttext import save_facebook_model

save_facebook_model(model,'own_fasttext_model.bin')
</code></pre>

<p>But I am obtaining the next error in that same line:</p>

<pre><code>---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-192-c9c2c41985af&gt; in &lt;module&gt;
      2 from gensim.models.fasttext import save_facebook_model
      3 
----&gt; 4 save_facebook_model(model,'own_fasttext_model.bin')

/opt/conda/lib/python3.7/site-packages/gensim/models/fasttext.py in save_facebook_model(model, path, encoding, lr_update_rate, word_ngrams)
   1334     """"""
   1335     fb_fasttext_parameters = {""lr_update_rate"": lr_update_rate, ""word_ngrams"": word_ngrams}
-&gt; 1336     gensim.models._fasttext_bin.save(model, path, fb_fasttext_parameters, encoding)

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in save(model, fout, fb_fasttext_parameters, encoding)
    666     if isinstance(fout, str):
    667         with open(fout, ""wb"") as fout_stream:
--&gt; 668             _save_to_stream(model, fout_stream, fb_fasttext_parameters, encoding)
    669     else:
    670         _save_to_stream(model, fout, fb_fasttext_parameters, encoding)

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in _save_to_stream(model, fout, fb_fasttext_parameters, encoding)
    629 
    630     # Save words and ngrams vectors
--&gt; 631     _input_save(fout, model)
    632     fout.write(struct.pack('@?', False))  # Save 'quot_', which is False for unsupervised models
    633 

/opt/conda/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py in _input_save(fout, model)
    573 
    574     assert vocab_dim == ngrams_dim
--&gt; 575     assert vocab_n == len(model.wv.vocab)
    576     assert ngrams_n == model.wv.bucket
    577 

AssertionError: 
</code></pre>

<p>Any clue on what could be happening?</p>

<p>Thanks in advance.</p>
","save, gensim, fasttext","<p>Opened the next <a href=""https://github.com/RaRe-Technologies/gensim/issues/2853"" rel=""nofollow noreferrer"">issue</a> in github and this will fix the problem.</p>
",0,0,937,2020-06-10 18:01:13,https://stackoverflow.com/questions/62310124/saving-fasttext-custom-model-binary-with-gensim
word2vec recommendation system KeyError: &quot;word &#39;21883&#39; not in vocabulary&quot;,"<p>The code works absolutely fine for the data set containing 500000+ instances but whenever I reduce the data set to 5000/10000/15000 it throws a key error : word ""***"" not in vocabulary.Not for every data point but for most them it throws the error.The data set is in excel format.  [1]: <a href=""https://i.sstatic.net/YCBiQ.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/YCBiQ.png</a>
I don't know how to fix this problem since i have very little knowledge about it,,I am still learning.Please help me fix this problem! </p>

<pre><code>    purchases_train = []
    for i in tqdm(customers_train):
        temp = train_df[train_df[""CustomerID""] == i][""StockCode""].tolist()
        purchases_train.append(temp)

    purchases_val = []
    for i in tqdm(validation_df['CustomerID'].unique()):
        temp = validation_df[validation_df[""CustomerID""] == i][""StockCode""].tolist()
        purchases_val.append(temp)


    model = Word2Vec(window = 10, sg = 1, hs = 0,
                     negative = 10, # for negative sampling
                     alpha=0.03, min_alpha=0.0007,
                     seed = 14)

    model.build_vocab(purchases_train, progress_per=200)

    model.train(purchases_train, total_examples = model.corpus_count, 
                epochs=10, report_delay=1)


    model.save(""word2vec_2.model"")
    model.init_sims(replace=True)

    # extract all vectors
    X = model[model.wv.vocab]

    X.shape

    products = train_df[[""StockCode"", ""Description""]]

    products.drop_duplicates(inplace=True, subset='StockCode', keep=""last"")


 products_dict=products.groupby('StockCode'['Description'].apply(list).to_dict()

    def similar_products(v, n = 6):
        ms = model.similar_by_vector(v, topn= n+1)[1:]
        new_ms = []
        for j in ms:
            pair = (products_dict[j[0]][0], j[1])
            new_ms.append(pair)

        return new_ms

        similar_products(model['21883'])
</code></pre>
","gensim, word2vec","<p>If you get a <code>KeyError</code> saying a word is not in the vocabulary, that's a reliable indicator that the word you're looking-up was not in the training data fed to <code>Word2Vec</code>, or did not appear enough (default <code>min_count=5</code>) times. </p>

<p>So, your error indicates the word-token <code>'21883'</code> did not appear at least 5 times in the texts (<code>purchases_train</code>) supplied to <code>Word2Vec</code>. You should do either or both of:</p>

<ul>
<li><p>Ensure all words you're going to look-up appear enough times, either with more training data or a lower <code>min_count</code>. (However, words with only one or a few occurrences tend <strong>not</strong> to get good vectors &amp; instead just drag the quaality of surrounding-words' vectors down - so keeping this value above <code>1</code>, or even raising it above the default of <code>5</code> to discard <strong>more</strong> rare words, is a better path whenever you have sufficient data.)</p></li>
<li><p>If your later code will be looking up words that might not be present, either check for their presence first (<code>word in model.wv.vocab</code>) or set up a <code>try: ... except: ...</code> to catch &amp; handle the case where they're not present.</p></li>
</ul>
",0,0,319,2020-06-12 16:58:33,https://stackoverflow.com/questions/62348981/word2vec-recommendation-system-keyerror-word-21883-not-in-vocabulary
Improving DOC2VEC Gensim efficiency,"<p>I am trying to train Gensim Doc2Vec model on tagged documents. I have around 4000000 documents. Following is my code:</p>

<pre><code>import pandas as pd
import multiprocessing
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
import logging
from tqdm import tqdm
from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import os
import re



def text_process(text):
    logging.basicConfig(format=""%(levelname)s - %(asctime)s: %(message)s"", datefmt='%H:%M:%S', level=logging.INFO)
    stop_words_lst = ['mm', 'machine', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'first', 'second', 'third', 'plurality', 'one', 'more', 'least', 'at', 'example', 'memory', 'exemplary', 'fourth', 'fifth', 'sixth','a', 'A', 'an', 'the', 'system', 'method', 'apparatus', 'computer', 'program', 'product', 'instruction', 'code', 'configure', 'operable', 'couple', 'comprise', 'comprising', 'includes', 'cm', 'processor', 'hardware']
    stop_words = set(stopwords.words('english'))

    temp_corpus =[]
    text = re.sub(r'\d+', '', text)
    for w in stop_words_lst:
        stop_words.add(w)
    tokenizer = RegexpTokenizer(r'\w+')
    word_tokens = tokenizer.tokenize(text)
    lemmatizer= WordNetLemmatizer()
    for w in word_tokens:
        w = lemmatizer.lemmatize(w)
        if w not in stop_words:
            temp_corpus.append(str(w))
    return temp_corpus

chunk_patent = pd.DataFrame()
chunksize = 10 ** 5
cores = multiprocessing.cpu_count()
directory = os.getcwd()
for root,dirs,files in os.walk(directory):
    for file in files:
       if file.startswith(""patent_cpc -""):
           print(file)
           #f=open(file, 'r')
           #f.close()
           for chunk_patent_temp in pd.read_csv(file, chunksize=chunksize):
                #chunk_patent.sort_values(by=['cpc'], inplace=True)
                #chunk_patent_temp = chunk_patent_temp[chunk_patent_temp['cpc'] == ""G06K7""]
                if chunk_patent.empty:
                    chunk_patent = chunk_patent_temp
                else:
                    chunk_patent = chunk_patent.append(chunk_patent_temp)
train_tagged = chunk_patent.apply(lambda r: TaggedDocument(words=text_process(r['text']), tags=[r.cpc]), axis=1)
print(train_tagged.values)

if os.path.exists(""cpcpredict_doc2vec.model""):
    doc2vec_model = Doc2Vec.load(""cpcpredict_doc2vec.model"")
    doc2vec_model.build_vocab((x for x in tqdm(train_tagged.values)), update=True)
    doc2vec_model.train(train_tagged, total_examples=doc2vec_model.corpus_count, epochs=50)
    doc2vec_model.save(""cpcpredict_doc2vec.model"")
else:
    doc2vec_model = Doc2Vec(dm=0, vector_size=300, min_count=100, workers=cores-1)
    doc2vec_model.build_vocab((x for x in tqdm(train_tagged.values)))
    doc2vec_model.train(train_tagged, total_examples=doc2vec_model.corpus_count, epochs=50)
    doc2vec_model.save(""cpcpredict_doc2vec.model"")
</code></pre>

<p>I have tried modifying the Doc2vec parameters but without any luck.</p>

<p>On the same data I have trained Word2vec model, which is much accurate in comparison to the doc2vec model. Further, ""most_similar"" results for word2vec model is very different from the doc2vec model. </p>

<p>Following is the code for searching most similar results:</p>

<pre><code>from gensim.models import Word2Vec
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
import logging
from gensim.models import Doc2Vec
import re

def text_process(text):
    logging.basicConfig(format=""%(levelname)s - %(asctime)s: %(message)s"", datefmt='%H:%M:%S', level=logging.INFO)
    stop_words_lst = ['mm', 'machine', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'first', 'second', 'third', 'example', 'memory', 'exemplary', 'fourth', 'fifth', 'sixth','a', 'A', 'an', 'the', 'system', 'method', 'apparatus', 'computer', 'program', 'product', 'instruction', 'code', 'configure', 'operable', 'couple', 'comprise', 'comprising', 'includes', 'cm', 'processor', 'hardware']
    stop_words = set(stopwords.words('english'))
    #for index, row in df.iterrows():
    temp_corpus =[]
    text = re.sub(r'\d+', '', text)
    for w in stop_words_lst:
        stop_words.add(w)
    tokenizer = RegexpTokenizer(r'\w+')
    word_tokens = tokenizer.tokenize(text)
    lemmatizer= WordNetLemmatizer()
    for w in word_tokens:
        w = lemmatizer.lemmatize(w)
        if w not in stop_words:
            temp_corpus.append(str(w))
    return temp_corpus

model = Word2Vec.load(""cpc.model"")
print(model.most_similar(positive=['barcode'], topn=30))

model1 = Doc2Vec.load(""cpcpredict_doc2vec.model"")

pred_tags = model1.most_similar('barcode',topn=10)
print(pred_tags)
</code></pre>

<p>Further, the output of the aforementioned is cited below:</p>

<pre><code>[('indicium', 0.36468246579170227), ('symbology', 0.31725651025772095), ('G06K17', 0.29797130823135376), ('dataform', 0.29535001516342163), ('rogue', 0.29372256994247437), ('certification', 0.29178398847579956), ('reading', 0.27675414085388184), ('indicia', 0.27346929907798767), ('Contra', 0.2700084149837494), ('redemption', 0.26682156324386597)]

[('searched', 0.4693435728549957), ('automated', 0.4469209909439087), ('production', 0.4364866018295288), ('hardcopy', 0.42193126678466797), ('UWB', 0.4197841286659241), ('technique', 0.4149003326892853), ('authorized', 0.4134449362754822), ('issued', 0.4129987359046936), ('installing', 0.4093806743621826), ('thin', 0.4016669690608978)]
</code></pre>
","python, nltk, gensim, word2vec, doc2vec","<p>The <code>Doc2Vec</code> mode you've chosen, <code>dm=0</code> (aka plain ""PV-DBOW""), does not train word-vectors at all. Word vectors will still be randomly-initialized, due to shared code-paths of the different models, but never trained and thus meaingless.</p>

<p>So the results of your <code>most_similar()</code>, using a word as the query, will be essentially random. (Using <code>most_similar()</code> on the model itself, rather than its <code>.wv</code> word-vectors or <code>.docvecs</code> doc-vectors, should also be generating a deprecation warning.)</p>

<p>If you need your <code>Doc2Vec</code> model to train word-vectors in addition to the doc-vectors, use either the <code>dm=1</code> mode (""PV-DM"") or <code>dm=0, dbow_words=1</code> (adding optional interleaved skip-gram word training to plain DBOW training). In both cases, words will be trained very similarly to a <code>Word2Vec</code> model (of the 'CBOW' or 'skip-gram' modes, respectively) – so your word-based <code>most_similar()</code> results should then be very comparable. </p>

<p>Separately:</p>

<ul>
<li>if you have enough data to train 300-dimensional vectors, &amp; discard all words with fewer than 100 occurrences, then 50 training epochs <em>may</em> be more than needed. </li>
<li>those <code>most_similar()</code> results don't particularly look like they're result of any lemmatization, as seems intended by your <code>text_process()</code> method, but maybe that's not an issue, or some other issue entirely. Note, though, that with sufficient data, lemmatization may be a superfluous step - all variants of the same word tend to wind up usefully near each other, when there are plenty of varied examples of al the word variants in real contexts. </li>
</ul>
",2,1,836,2020-06-13 10:38:34,https://stackoverflow.com/questions/62358583/improving-doc2vec-gensim-efficiency
Select texts by topic (LDA),"<p>Would it be possible to look for texts that are within a certain topic (determined by LDA)? </p>

<p>I have a list of 5 topics with 10 words each, found by using lda.</p>

<p>I have analysed the texts in a dataframe’s column. 
I would like to select/filter rows/texts that are in one specific topic. </p>

<p>If you need more information, I will provide you. </p>

<p>What I am referring to is the step that returns this output:</p>

<pre><code>[(0,
  '0.207*""house"" + 0.137*""apartment"" + 0.118*""sold"" + 0.092*""beach"" + '
  '0.057*""kitchen"" + 0.049*""rent"" + 0.033*""landlord"" + 0.026*""year"" + '
  '0.024*""bedroom"" + 0.023*""home""'),
 (1,
  '0.270*""school"" + 0.138*""homeworks"" + 0.117*""students"" + 0.084*""teacher"" + '
  '0.065*""pen"" + 0.038*""books"" + 0.022*""maths"" + 0.020*""exercise"" + '
  '0.020*""friends"" + 0.020*""college""'),
 ... ]
</code></pre>

<p>created by </p>

<pre><code># LDA Model

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=num_topics, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto', 
                                           # alpha=[0.01]*num_topics,
                                           per_word_topics=True,
                                           eta=[0.01]*len(id2word.keys()))
</code></pre>

<h1>Print the Keyword in the 10 topics</h1>

<pre><code>from pprint import pprint
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]
</code></pre>

<p>The original column with texts that have been analysed is called <code>Texts</code> and it looks like: </p>

<pre><code>Texts 

""Children are happy to go to school...""
""The average price for buying a house is ... ""
""Our children love parks so we should consider to buy an apartment nearby""

etc etc...
</code></pre>

<p>My expected output would be </p>

<pre><code>Texts                                            Topic 
    ""Children are happy to go to school...""         2
    ""The average price for buying a house is ... ""  1
    ""Our children love parks so we should consider to buy an apartment nearby""                                   

      2
</code></pre>

<p>Thanks </p>
","python, gensim, text-classification, lda","<p><code>doc_lda</code> contains a list of (topic, score) tuple for each sentence. Hence you can flexibly assign a topic to the sentence using any heuristics, for example a simple heuristic would by assigning the topic which has the maximum score.</p>
<p>We can extract the topic scores of each sentence by doing this:</p>
<pre class=""lang-py prettyprint-override""><code>topic_scores = [[topic_score[1] for topic_score in sent] for sent in doc_lda]
</code></pre>
<p>You can also convert the above into a pandas dataframe where each row is a sentence and each column is the topic id. The dataframe data structure usually allows for a flexible and more complex operation on the topic-score sentence relationships</p>
<pre class=""lang-py prettyprint-override""><code>df_topics = pd.DataFrame(topic_scores)
</code></pre>
<p>If you just want to assign a single topic which has the maximum score on a sentence, you can do this:</p>
<pre class=""lang-py prettyprint-override""><code>max_topics = [max(sent, key=lambda x: x[1])[0] for sent in doc_lda]
</code></pre>
",2,3,886,2020-06-16 23:57:19,https://stackoverflow.com/questions/62419353/select-texts-by-topic-lda
Inconsistent results when training gensim model with gensim.downloader vs manual loading,"<p>I am trying to understand what is going wrong in the following example.</p>
<p>To train on the 'text8' dataset as described in the docs, one only has to do the following:</p>
<pre><code>import gensim.downloader as api
from gensim.models import Word2Vec

dataset = api.load('text8')
model = Word2Vec(dataset)
</code></pre>
<p>doing this gives very good embedding vectors, as verified by evaluating on a word-similarity task.</p>
<p>However, when loading the same textfile which is used above manually, as in</p>
<pre><code>text_path = '~/gensim-data/text8/text'
text = []
with open(text_path) as file:
    for line in file:
        text.extend(line.split())
text = [text]

model = Word2Vec(test)
</code></pre>
<p>The model still says it's training for the same number of epochs as above (5), but training is much faster, and the resulting vectors have a very, very bad performance on the similarity task.</p>
<p>What is happening here? I suppose it could have to do with the number of 'sentences', but the text8 file seems to have only a single line, so does gensim.downloader split the text8 file into sentences? If yes, of which length?</p>
","python, gensim, word2vec","<p>In your second example, you've created a training dataset with just a single text with the entire contents of the file. That's about 1.1 million word tokens, in a single list.</p>
<p><code>Word2Vec</code> (&amp; other related algorithms) in gensim have an internal implementation limitation, in their optimized paths, of 10,000 tokens per text item. All additional tokens are ignored.</p>
<p>So, in your 2nd case, 99% of your data is being discarded. Training may seem instant, but very little actual training will have occurred. (Word-vectors for words that only appear past the 1st 10,000 tokens won't have been trained at all, having only their initial randomly-set values.) If you enable logging at the INFO level, you'll see more details about each step of the process, and discrepancies like this may be easier to identify.</p>
<p>Yes, the <code>api.load()</code> variant takes extra steps to break the single-line-file into 10,000-token chunks. I believe it's using the <code>LineSentence</code> utility class for this purpose, whose source can be examined here:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/e859c11f6f57bf3c883a718a9ab7067ac0c2d4cf/gensim/models/word2vec.py#L1209"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/e859c11f6f57bf3c883a718a9ab7067ac0c2d4cf/gensim/models/word2vec.py#L1209</a></p>
<p>However, I recommend avoiding the <code>api.load()</code> functionality entirely. It doesn't just download data; it also downloads a shim of additional outside-of-version-control Python code for prepping that data for extra operations. Such code is harder to browse &amp; less well-reviewed than official gensim release code as packaged for PyPI/etc, which also presents a security risk. Each load target (by name like 'text8') might do something different, leaving you with a different object type as the return value.</p>
<p>It's much better for understanding to directly download precisely the data files you need, to known local paths, and do the IO/prep yourself, from those paths, so you know what steps have been applied, and the only code you're running is the officially versioned &amp; released code.</p>
",1,0,353,2020-06-23 20:47:35,https://stackoverflow.com/questions/62543491/inconsistent-results-when-training-gensim-model-with-gensim-downloader-vs-manual
Grouping words with same meaning. in LDA,"<p>I would like to know if it is possible to group together same words included in the LDA's output, i.e. words generated by</p>
<pre><code>doc_lda = lda_model[corpus]
</code></pre>
<p>for example</p>
<pre><code>[(0,
  '0.084*&quot;tourism&quot; + 0.013*&quot;touristic&quot; + 0.013*&quot;Madrid&quot; + '
  '0.010*&quot;travel&quot; + 0.008*&quot;half&quot; + 0.007*&quot;piare&quot; + '
  '0.007*&quot;turism&quot;')]
</code></pre>
<p>I would like to group <code>tourism, touristic</code> and <code>turism</code> (mispelled) together.
Would it be possible?</p>
<p>This is some relevant previous code:</p>
<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=num_topics, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha=[0.01]*num_topics,
                                           per_word_topics=True,
                                           eta=[0.01]*len(id2word.keys())) 
</code></pre>
<p>Thank you</p>
","python, gensim, lda","<p>The key thing to understand is LDA requires a great deal of tuning and iteration to work properly unlike say linear regression. But it can be useful for a certain set of problems.</p>
<p>Your intuition is right in that 'tourism', 'touristic' and 'turism' should all be one word. The fix, however, is not at the end where you are presented with their respective loadings but rather, early on with stemming and lemmatization (aka, stemming and lemming), adding unwanted words to your stopwords list, and some preprocessing to some degree or another. I'll address them separately but not as a group as I think that's fairly obvious. Also, because you only gave the one set of words and loadings, it's not really fruitful to go into providing the number of topics as you may be doing that just fine.</p>
<p><strong>Stemming/Lemming (Pick One)</strong></p>
<p>This is where the science and experience part starts, as well as the frustration. But, this is where you'll make the biggest and easiest gains. It seems like 'tourism' and 'touristic' might be best combined by stemming (as tour). The truth is a lot less clear cut as there are cases where one beats the other. In the below example, PortaStemer suffers from making sensible stems but lemmatizing fails to catch how 'studies' and studying are the same though it accurately catches 'cry'.</p>
<pre><code>Using PorterStemer
studies is studi
studying is studi
cries is cri
cry is cri

Lemmatize
studies is study
studying is studying
cries is cry
cry is cry
</code></pre>
<p>There are multiple stemmers such as Porter2, Snowball, Hunspell, and Paice-Husk. So, the obvious first step would be to see if any of these is more useful out of the box.</p>
<p>As mentioned above, lemmatization will get you a similar -- but somewhat different -- set of results.</p>
<p>There is no substitute for the work here. This is what separates a data scientist from a hobbiest or data analyst with plussed up title. The best time to do this was in the past so you would have an intuition of what works best for this sort of corpus; the second-best time is now.</p>
<p><strong>Iterate But Satisfice</strong></p>
<p>I presume you don't have infinite resources; you have to <a href=""https://en.wikipedia.org/wiki/Satisficing"" rel=""nofollow noreferrer"">satisfice</a>. For the above, you might consider preprocessing your text to correct or remove misspelled words. What to do with non-English words is trickier. The easiest solution is to remove them or add them to your stopwords list but that may not be the best solution. Customizing your dictionary is an option too.</p>
<p><strong>Know The Current Limits</strong></p>
<p>As of 2020, <em>no one</em> is doing a good job with <a href=""https://en.wikipedia.org/wiki/Code-switching"" rel=""nofollow noreferrer"">codeswitching</a>; certainly not a free and opensource resource. Gridspace is about the best I know of, and while their <a href=""https://www.youtube.com/watch?v=Xu0OKD3UL_k"" rel=""nofollow noreferrer"">demo is pretty amazing</a>, they can't handle codeswitching well.  Now, I'm doing some induction here because I'm assuming 'piare' is Spanish for 'I will', at least that what google translate says. If that's the case, your results will be confounded. But when you look at the loading (.007) that seems to be more work than would be worth it.</p>
",2,1,252,2020-06-24 23:52:38,https://stackoverflow.com/questions/62565772/grouping-words-with-same-meaning-in-lda
(gensim) LdaMallet vs LdaModel?,"<p>What is the difference between using <code>gensim.models.LdaMallet</code> and <code>gensim.models.LdaModel</code>? I noticed that the parameters are not all the same and would like to know when one should be used over the other?</p>
","gensim, lda, topic-modeling, mallet","<p>TL;DR: Both are two completely independent implementations of Latent Dirichlet Allocation.
Use gensim if you simply want to try out LDA and you are not interested in special features of Mallet.</p>
<p><a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""noreferrer""><code>gensim.models.LdaModel</code></a> is the single-core version of LDA implemented in gensim.
There is also parallelized LDA version available in gensim (<a href=""https://radimrehurek.com/gensim/models/ldamulticore.html"" rel=""noreferrer""><code>gensim.models.ldamulticore</code></a>).
Both Gensim implementations use an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation as described in <a href=""https://proceedings.neurips.cc/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf"" rel=""noreferrer"">Hoffman et al.</a> [1].</p>
<p>Gensim algorithms (not limited to LDA) are memory-independent w.r.t. the corpus size (can process input larger than RAM, streamed, out-of-core).</p>
<p>Gensim also offers wrappers for the popular tools <a href=""https://github.com/mimno/Mallet"" rel=""noreferrer"">Mallet</a> (Java) and <a href=""https://github.com/VowpalWabbit/vowpal_wabbit"" rel=""noreferrer"">Vowpal Wabbit</a> (C++).</p>
<p><a href=""https://radimrehurek.com/gensim/models/wrappers/ldavowpalwabbit.html"" rel=""noreferrer""><code>gensim.models.wrappers.LdaVowpalWabbit</code></a> uses the same online variational Bayes (VB) algorithm that Gensim’s LdaModel is based on [1].</p>
<p><a href=""https://radimrehurek.com/gensim/models/wrappers/ldamallet.html"" rel=""noreferrer""><code>gensim.models.wrappers.LdaMallet</code></a> uses an <a href=""https://mimno.infosci.cornell.edu/papers/fast-topic-model.pdf"" rel=""noreferrer"">optimized Gibbs sampling algorithm</a> for Latent Dirichlet Allocation [2].
This is the reason for different parameters.
However, most of the parameters, e.g., the number of topics, alpha and (b)eta) are shared between both algorithms because both implement LDA.</p>
<p>Both wrappers (<a href=""https://radimrehurek.com/gensim/models/wrappers/ldavowpalwabbit.html"" rel=""noreferrer""><code>gensim.models.wrappers.LdaVowpalWabbit</code></a> and
<a href=""https://radimrehurek.com/gensim/models/wrappers/ldamallet.html"" rel=""noreferrer""><code>gensim.models.wrappers.LdaMallet</code></a>) need to have the respective tool installed (independent of gensim). Therefore, gensim is easier to use.</p>
<p>Besides that, try out the different implementations and see what works for you.</p>
<h2>References</h2>
<p>[1] Hoffman, Matthew, Francis R. Bach, and David M. Blei. &quot;Online learning for latent dirichlet allocation.&quot; advances in neural information processing systems. 2010.</p>
<p>[2] Yao, Limin, David Mimno, and Andrew McCallum. &quot;Efficient methods for topic model inference on streaming document collections.&quot; Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009.</p>
",7,4,5911,2020-06-25 18:19:05,https://stackoverflow.com/questions/62581874/gensim-ldamallet-vs-ldamodel
Using Gensim Fasttext model with LSTM nn in keras,"<p>I have trained fasttext model with Gensim over the corpus of very short sentences (up to 10 words). I know that my test set includes words that are not in my train corpus, i.e some of the words in my corpus are like &quot;Oxytocin&quot; &quot;Lexitocin&quot;, &quot;Ematrophin&quot;,'Betaxitocin&quot;</p>
<p>given a new word in the test set, fasttext knows pretty well to generate a vector with high cosine-similarity to the other similar words in the train set by using the characters level n-gram</p>
<p>How do i incorporate the fasttext model inside a LSTM keras network without losing the fasttext model to just a list of vectors in the vocab? because then I won't handle any OOV even when fasttext do it well.</p>
<p>Any idea?</p>
","tensorflow, keras, nlp, gensim, word-embedding","<p>here the procedure to incorporate the fasttext model inside an LSTM Keras network</p>
<pre><code># define dummy data and precproces them

docs = ['Well done',
        'Good work',
        'Great effort',
        'nice work',
        'Excellent',
        'Weak',
        'Poor effort',
        'not good',
        'poor work',
        'Could have done better']

docs = [d.lower().split() for d in docs]

# train fasttext from gensim api

ft = FastText(size=10, window=2, min_count=1, seed=33)
ft.build_vocab(docs)
ft.train(docs, total_examples=ft.corpus_count, epochs=10)

# prepare text for keras neural network

max_len = 8

tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True)
tokenizer.fit_on_texts(docs)

sequence_docs = tokenizer.texts_to_sequences(docs)
sequence_docs = tf.keras.preprocessing.sequence.pad_sequences(sequence_docs, maxlen=max_len)

# extract fasttext learned embedding and put them in a numpy array

embedding_matrix_ft = np.random.random((len(tokenizer.word_index) + 1, ft.vector_size))

pas = 0
for word,i in tokenizer.word_index.items():
    
    try:
        embedding_matrix_ft[i] = ft.wv[word]
    except:
        pas+=1

# define a keras model and load the pretrained fasttext weights matrix

inp = Input(shape=(max_len,))
emb = Embedding(len(tokenizer.word_index) + 1, ft.vector_size, 
                weights=[embedding_matrix_ft], trainable=False)(inp)
x = LSTM(32)(emb)
out = Dense(1)(x)

model = Model(inp, out)

model.predict(sequence_docs)
</code></pre>
<p>how to deal unseen text</p>
<pre><code>unseen_docs = ['asdcs work','good nxsqa zajxa']
unseen_docs = [d.lower().split() for d in unseen_docs]

sequence_unseen_docs = tokenizer.texts_to_sequences(unseen_docs)
sequence_unseen_docs = tf.keras.preprocessing.sequence.pad_sequences(sequence_unseen_docs, maxlen=max_len)

model.predict(sequence_unseen_docs)
</code></pre>
",7,6,4095,2020-07-05 16:39:07,https://stackoverflow.com/questions/62743531/using-gensim-fasttext-model-with-lstm-nn-in-keras
"My Doc2Vec code, after many loops/epochs of training, isn&#39;t giving good results. What might be wrong?","<p>I'm training a <code>Doc2Vec</code> model using the below code, where <code>tagged_data</code> is a list of <code>TaggedDocument</code> instances I set up before:</p>
<pre class=""lang-py prettyprint-override""><code>max_epochs = 40

model = Doc2Vec(alpha=0.025, 
                min_alpha=0.001)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.001
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save(&quot;d2v.model&quot;)
print(&quot;Model Saved&quot;)
</code></pre>
<p>When I later check the model results, they're not good. What might have gone wrong?</p>
","gensim, word2vec, doc2vec","<p><strong>Do not call <code>.train()</code> multiple times in your own loop that tries to do <code>alpha</code> arithmetic.</strong></p>
<p>It's unnecessary, and it's error-prone.</p>
<p>Specifically, in the above code, decrementing the original <code>0.025</code> alpha by <code>0.001</code> forty times results in (<code>0.025 - 40*0.001</code>) <code>-0.015</code> final <code>alpha</code>, which would also have been negative for many of the training epochs. But a negative <code>alpha</code> <em>learning-rate</em> is nonsensical: it essentially asks the model to nudge its predictions a little bit in the <em>wrong</em> direction, rather than a little bit in the <em>right</em> direction, on every bulk training update. (Further, since <code>model.iter</code> is by default 5, the above code actually performs <code>40 * 5</code> training passes – <code>200</code> – which probably isn't the conscious intent. But that will just confuse readers of the code &amp; slow training, not totally sabotage results, like the <code>alpha</code> mishandling.)</p>
<p>There are other variants of error that are common here, as well. If the <code>alpha</code> were instead decremented by <code>0.0001</code>, the 40 decrements would only reduce the final <code>alpha</code> to <code>0.021</code> – whereas the proper practice for this style of SGD (Stochastic Gradient Descent) with linear learning-rate decay is for the value to end &quot;very close to <code>0.000</code>&quot;). If users start tinkering with <code>max_epochs</code> – it is, after all, a parameter pulled out on top! – but don't also adjust the decrement every time, they are likely to far-undershoot or far-overshoot <code>0.000</code>.</p>
<p>So don't use this pattern.</p>
<p>Unfortunately, many bad online examples have copied this anti-pattern from each other, <em>and</em> make serious errors in their own <code>epochs</code> and <code>alpha</code> handling. Please don't copy their error, and please let their authors know they're misleading people wherever this problem appears.</p>
<p>The above code can be improved with the much-simpler replacement:</p>
<pre class=""lang-py prettyprint-override""><code>max_epochs = 40
model = Doc2Vec()  # of course, if non-default parameters needed, use them here
                   # most users won't need to change alpha/min_alpha at all
                   # but many will want to use more than default `epochs=5`

model.build_vocab(tagged_data)
model.train(tagged_data, total_examples=model.corpus_count, epochs=max_epochs)

model.save(&quot;d2v.model&quot;)
</code></pre>
<p>Here, the <code>.train()</code> method will do exactly the requested number of <code>epochs</code>, smoothly reducing the internal effective <code>alpha</code> from its default starting value to near-zero. (It's rare to need to change the starting <code>alpha</code>, but even if you wanted to, just setting a new non-default value at initial model-creation is enough.)</p>
<p>Also: note that later calls to <code>infer_vector()</code> will reuse the <code>epochs</code> specified at the time of model-creation. If nothing is specified, the default <code>epochs=5</code> will be used - which is often smaller than is best for training or inference. So if you find a larger number of <code>epochs</code> (such as 10, 20 or more) is better for training, remember to also use at least the same number of <code>epochs</code> for inference. (<code>.infer_vector()</code> takes an optional <code>epochs</code> parameter whihc can override any value set at model-contruction.</p>
",9,3,1963,2020-07-08 18:10:33,https://stackoverflow.com/questions/62801052/my-doc2vec-code-after-many-loops-epochs-of-training-isnt-giving-good-results
Remove custom stopwords,"<p>I am trying to remove stopwords during an NLP pre-processing step. I use the <code>remove_stopwords()</code> function from <code>gensim</code> but would also like to add my own stopwords</p>
<pre><code># under this method, these custom stopwords still show up after processing
custom_stops = [&quot;stopword1&quot;, &quot;stopword2&quot;]
data_text['text'].apply(lambda x: [item for item in x if item not in custom_stops])
# remove stopwords with gensim
data_text['filtered_text'] = data_text['text'].apply(lambda x: remove_stopwords(x.lower()))
# split the sentences into a list
data_text['filtered_text'] = data_text['filtered_text'].apply(lambda x: str.split(x))
</code></pre>
","python, nlp, gensim","<p>I was able to get it to work with the following:</p>
<pre><code>custom_stops = [&quot;stopword1&quot;, &quot;stopword2&quot;]
# remove stopwords with gensim
data_text['filtered_text'] = data_text['text'].apply(lambda x: remove_stopwords(x.lower()))
# split the sentence
data_text['filtered_text'] = data_text['filtered_text'].apply(lambda x: str.split(x))
# remove the custom stopwords
data_text['filtered_text'] = data_text['filtered_text'].apply(lambda x: [item for item in x if item.lower() not in custom_stops])
</code></pre>
",0,1,607,2020-07-08 20:05:57,https://stackoverflow.com/questions/62802812/remove-custom-stopwords
How to find most similar to an array in gensim,"<p>I know the <code>most_similar</code> method works when entering a previously added string, but how do you <em>reverse</em> search a numpy array of some word?</p>
<pre class=""lang-py prettyprint-override""><code>modelw2v = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True)
differenceArr = modelw2v[&quot;King&quot;] - modelw2v[&quot;Queen&quot;]


# This line does not work
modelw2v.most_similar(differenceArr) 
</code></pre>
","python-3.x, nlp, gensim","<p>The <code>most_similar()</code> method can take vectors as the origin of a search, but you should explicitly specify them as one member of a list provided to the method's <code>positive</code> parameter, so that its logic for handling more simple origins (like a string or list of strings) isn't confused.</p>
<p>Specifically, this should work with your other code:</p>
<pre><code>model23v.most_similar(positive=[differenceArr,])
</code></pre>
<p>More generally, you can supply lists of vectors (or word-keys for looking up vectors) to both the <code>positive</code> and <code>negative</code> parameters of this method, and the method will combine them (according to the exact logic you can see in the source code). So for example the prominent word2vec example...</p>
<pre><code>wv('king') - wv('man') + wv('woman') = ?
</code></pre>
<p>...can be effected with the <code>most_similar()</code> method without doing your own other vector-arithmetic:</p>
<pre><code>sims = modelw2v.most_similar(positive=['king', 'woman'], negative=['man'])
</code></pre>
",0,0,441,2020-07-10 05:29:48,https://stackoverflow.com/questions/62827849/how-to-find-most-similar-to-an-array-in-gensim
How to get the score of filtered bi-grams in gensim?,"<p>Given a list of document words e.g. <code>[['cow','boy','hat','mat],['village','boy','water','cow']....]</code>, gensim can be used to get bi-grams as follows:</p>
<pre><code>bigrams = gensim.models.Phrases(data_words, min_count=1,threshold=1) 
bigram_model = gensim.models.phrases.Phraser(bigrams)
</code></pre>
<p>I was wondering as to how to get the score of each bi-gram detected in the bigram_model?</p>
","python, gensim, lda","<p>It turns out that it is as simple as using:</p>
<pre><code>bigram_model.phrasegrams
</code></pre>
<p>that yields something like below:</p>
<pre><code>{(b'cow', b'boy'): 23.3228613654742079,
 (b'village', b'water'): 1.3228613654742079}
</code></pre>
",1,0,217,2020-07-21 18:42:48,https://stackoverflow.com/questions/63021096/how-to-get-the-score-of-filtered-bi-grams-in-gensim
How to split a text file into sentences for word2vec/gensim,"<p>I have extracted about 40MB of the English wikipedia into plain text. I would to use it to build a word2vec model with gensim. To do this I need to split it into sentences first. How can I do this?  I tried:</p>
<pre><code>from __future__ import unicode_literals, print_function
import spacy
from spacy.lang.en import English 
nlp = spacy.load('en_core_web_sm')
nlp.max_length = 47084146
ftest = open(&quot;test_02&quot;, &quot;r&quot;)
raw_test = ftest.read().replace(&quot;\n&quot;, &quot; &quot;)
sentences = [i for i in nlp(raw_test).sents] 

f = open(&quot;sentences.txt&quot;, &quot;w&quot;)

for sent in sentences:
    f.write(str(sent)+&quot;\n&quot;)
f.write(&quot;\n&quot;)
f.close()
</code></pre>
<p>But this fails with: <code>MemoryError: Unable to allocate 34.8 GiB for an array with shape (9112793, 8, 64, 2) and data type float32</code></p>
<p>I have no idea why it wants to use so much RAM!</p>
<p>How can I do this?</p>
<hr />
<pre><code>Traceback (most recent call last):
  File &quot;../../processwiki.py&quot;, line 8, in &lt;module&gt;
    sentences = [i for i in nlp(raw_test).sents] 
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/spacy/language.py&quot;, line 449, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))
  File &quot;nn_parser.pyx&quot;, line 233, in spacy.syntax.nn_parser.Parser.__call__
  File &quot;nn_parser.pyx&quot;, line 274, in spacy.syntax.nn_parser.Parser.predict
  File &quot;nn_parser.pyx&quot;, line 287, in spacy.syntax.nn_parser.Parser.greedy_parse
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/thinc/neural/_classes/model.py&quot;, line 167, in __call__
    return self.predict(x)
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/thinc/neural/_classes/model.py&quot;, line 131, in predict
    y, _ = self.begin_update(X, drop=None)
  File &quot;_parser_model.pyx&quot;, line 243, in spacy.syntax._parser_model.ParserModel.begin_update
  File &quot;_parser_model.pyx&quot;, line 300, in spacy.syntax._parser_model.ParserStepModel.__init__
  File &quot;_parser_model.pyx&quot;, line 425, in spacy.syntax._parser_model.precompute_hiddens.__init__
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/spacy/_ml.py&quot;, line 183, in begin_update
    Yf = self._add_padding(Yf)
  File &quot;/mnt/storage/home/user/.local/lib/python3.7/site-packages/spacy/_ml.py&quot;, line 214, in _add_padding
    Yf_padded = self.ops.xp.vstack((self.pad, Yf))
  File &quot;&lt;__array_function__ internals&gt;&quot;, line 6, in vstack
  File &quot;/mnt/storage/software/languages/anaconda/Anaconda3-2020.02-tflow-2.2.0/lib/python3.7/site-packages/numpy/core/shape_base.py&quot;, line 283, in vstack
    return _nx.concatenate(arrs, 0)
  File &quot;&lt;__array_function__ internals&gt;&quot;, line 6, in concatenate
MemoryError: Unable to allocate 34.8 GiB for an array with shape (9112793, 8, 64, 2) and data type float32
</code></pre>
","python, spacy, gensim","<p>The problem is, the content of <code>test_02</code> is processed at once, and the intermediate data structures don't fit in memory. Processing it in chunks should solve the problem. For example, if sentences are never split between lines, gradual processing would look like:</p>
<pre><code>with  open(&quot;test_02&quot;, &quot;r&quot;) as ftest, open(&quot;sentences.txt&quot;, &quot;w&quot;) as f:
    for line in ftest:
        for sent in nlp(line).sents:
            f.write(str(sent)+&quot;\n&quot;)
</code></pre>
<p>Since sentences can stretch over multiple lines, you might want to use a different strategy of splitting <code>test_02</code>, e.g. by splitting on double newline instead <code>for line in ftest.read().split('\n\n')</code> but most likely even this naive approach will work just fine.</p>
",1,0,2159,2020-07-24 18:55:28,https://stackoverflow.com/questions/63079854/how-to-split-a-text-file-into-sentences-for-word2vec-gensim
Assessing doc2vec accuracy,"<p>I am trying to assess a doc2vec model based on the code from <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#assessing-the-model"" rel=""nofollow noreferrer"">here</a>. Basically, I want to know the percentual of  inferred documents are found to be most similar to itself. This is my current code an:</p>
<pre><code>    for doc_id, doc in enumerate(cur.execute('SELECT Text FROM Patents')):
        docs += 1
        doc = clean_text(doc)
        inferred_vector = model.infer_vector(doc)
        sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
        rank = [docid for docid, sim in sims].index(doc_id)
        ranks.append(rank) 

    counter = collections.Counter(ranks)
    accuracy = counter[0] / docs
</code></pre>
<p>This code works perfectly with smaller datasets. However, since I have a huge file with millions of documents, this code becomes too slow, it would take months to compute. I profiled my code and most of the time is consumed by the following line: <code>sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))</code>.</p>
<p>If I am not mistaken, this is having to measure each document to every other document. I think computation time might be massively reduced if I change this to <code>topn=1</code> instead since the only thing I want to know is if the most similar document is itself or not. Doing this will basically take each doc (i.e., <code>inferred_vector</code>), measure its most similar document (i.e., <code>topn=1</code>), and then I just see if it is itself or not. How could I implement this? Any help or idea is welcome.</p>
","gensim, doc2vec","<p>To have <code>most_similar()</code> return only the single most-similar document, that is as simple as specifying <code>topn=1</code>.</p>
<p>However, to know which one document of the millions is the most-similar to a single target vector, the similarities to <em>all</em> the candidates must be calculated &amp; sorted. (If even one document was left out, it might have been the top-ranked one!)</p>
<p>Making sure absolutely no virtual-memory swapping is happening will help ensure that brute-force comparison happens as fast as possible, all in RAM – but with millions of docs, it will still be time-consuming.</p>
<p>What you're attempting is a fairly simple &quot;self-check&quot; as to whether training led to self-consistent model: whether the re-inference of a document creates a vector &quot;very close to&quot; the same doc-vector left over from bulk training. Failing that will indicate some big problems in doc-prep or training, but it's not a true measure of the model's &quot;accuracy&quot; for any real task, and the model's value is best evaluated against your intended use.</p>
<p>Also, because this &quot;re-inference self-check&quot; is just a crude sanity check, there's no real need to do it for <em>every</em> document. Picking a thousand (or ten thousand, or whatever) random documents will give you a representative idea of whether most of the re-inferred vectors have this quality, or not.</p>
<p>Similarly, you could simply check the similarity of the re-inferred vector against the single in-model vector for that same document-ID, and check whether they are &quot;similar enough&quot;. (This will be much faster, but could also be done on just a random sample of docs.) There's no magic proper threshold for &quot;similar enough&quot;; you'd have to pick one that seems to match your other goals. For example, using <code>scikit-learn</code>'s <code>cosine_similarity()</code> to compare the two vectors:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.metrics.pairwise import cosine_similarity

# ...

    inferred_vector = model.infer_vector(doc_words)
    looked_up_vector = model.dv[doc_id]
    self_similarity = cosine_similarity([inferred_vector], [looked_up_vector])[0]
    # then check that value against some threshold
</code></pre>
<p>(You have to wrap the single vectors in lists as arguments to <code>cosine_similarity()</code>, then access the 0th element of the return value, because it is designed to usually work on larger lists of vectors.)</p>
<p>With this calculation, you wouldn't know if, for example, some of the other stored-doc-vectors are a little closer to your inferred target - but that may not be that important, anyway. The docs might be really similar! And while the original &quot;closest to itself&quot; self-check will fail miserably if there were major defects in training, even a well-trained model will likely have some cases where natural model jitter prevents a &quot;closest to itself&quot; for every document. (With more documents inside the same number of dimensions, or certain corpuses with lots of very-similar documents, this would become more common... but not be a concerning indicator of any model problems.)</p>
",2,1,1284,2020-07-26 02:30:44,https://stackoverflow.com/questions/63095512/assessing-doc2vec-accuracy
How to interpret output from gensim&#39;s Word2vec most similar method and understand how it&#39;s coming up with the output values,"<p>I am trying to implement word2vec on a problem. I will briefly explain my problem statement:</p>
<p>I am dealing with clinical data. I want to predict the top N diseases given a set of symptoms.</p>
<pre><code>Patient1: ['fever', 'loss of appetite', 'cold', '#flu#']
Patient2: ['hair loss', 'blood pressure', '#thyroid']
Patient3: ['hair loss', 'blood pressure', '#flu]
..
..
Patient30000: ['vomiting', 'nausea', '#diarrohea']
</code></pre>
<p>Note:
1.words with #prefix are diagnosis and the rest are symptoms</p>
<ol start=""2"">
<li>My corpus doesn't have any sentences or paragraphs. It just contains symptom names and diagnosis for a patient</li>
</ol>
<p>Applying word2vec on this corpus, I am able to generate the top 10 diagnosis given a set of input symptoms. Now, I want to understand how that output is generated. I know it's cosine similarity by adding the input vectors but I am unable to validate this output. Or understand how to improve this.  Really want to understand what exactly is going on in the background which leads to these output.</p>
<p>Can anyone help me answer these questions or highlight what are the drawbacks/advantages of this approach</p>
","python, nlp, gensim, word2vec, word-embedding","<p>Word2vec is going to give you n-dimensional vectors that represent each of the diseases based on their co-occurrence. This means that you are representing each of the symptoms as a vector.</p>
<p>One row -</p>
<pre><code>X = ['fever', 'loss of appetite']

X_onehot= [[0,0,0,1,0,0,0,0,0,0,0],
           [0,0,0,0,0,0,0,0,1,0,0]]

X_word2vec= [[0.002,0.25,-0.1,0.335,0.7264],
             [0.746,0.6463,0.0032,0.6301,0.223]]

Y = #flu
</code></pre>
<p>Now, you can represent each row in the data by taking the average of the word2vec such as -</p>
<pre><code>X_avg = [[0.374 ,0.44815, -0.0484, 0.48255, 0.4747]]
</code></pre>
<p>Now you have a 5 length feature vector and a class for each row in your dataset. Next, you can treat it like any other machine learning problem.</p>
<p>If you want to predict the disease then just use a classification model after train-test split. That way you can validate the data.</p>
<p><strong>Using cosine similarity to the word2vec vectors only yields similar symptoms. It will not let you build a disease recommendation model, because then you will be recommending a symptom based on other similar symptoms.</strong></p>
",1,0,217,2020-07-26 06:36:54,https://stackoverflow.com/questions/63096909/how-to-interpret-output-from-gensims-word2vec-most-similar-method-and-understan
LDA Gensim Mallet setting alpha as &#39;auto&#39;,"<p>I am using LDA for Topic Modelling in Python.Gensim implementation of LDA allows us to set alpha as 'auto' as below:</p>
<pre><code>alpha ({numpy.ndarray, str}, optional) –
    
            ’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.
    
            ’auto’: Learns an asymmetric prior from the corpus (not available if distributed==True).
</code></pre>
<p>For LDA Mallet wrapper provided in Gensim there is no option of setting alpha as auto.</p>
<p>Is there way to learn alpha from the corpus in LDA Mallet?</p>
","python, gensim, lda","<p>This is in the <code>optimize_interval</code> argument. From the <a href=""https://radimrehurek.com/gensim/models/wrappers/ldamallet.html"" rel=""nofollow noreferrer"">wrapper documentation</a>:</p>
<blockquote>
<p>optimize_interval (int, optional) – Optimize hyperparameters every optimize_interval iterations</p>
</blockquote>
<p>So although alpha is originally set (or left as the default), if you set <code>optimize_interval</code> then every n iterations, the alpha and beta will be optimised automatically.</p>
",2,1,1023,2020-07-29 06:44:37,https://stackoverflow.com/questions/63147796/lda-gensim-mallet-setting-alpha-as-auto
word2vec logging missing values,"<p>Im using gensim version '3.8.3' <br></p>
<p>when im running for model Word2Vec and FastText <code>build_vocab</code> and <code>train</code> <br>
the logs from those functions are missing the values</p>
<p>for example part of the logs of  <code>build_vocab</code> of FastText</p>
<pre><code>08/09/2020 08:19:18 AM [INFO] collecting all words and their counts
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
08/09/2020 08:19:18 AM [INFO] PROGRESS: at sentence #%i, processed %i words, keeping %i word types
</code></pre>
<p>the index is missing and printed as <code>i</code></p>
<p>is there a way to solve it? is it a version bug?</p>
","python-3.x, gensim, word2vec","<p>As <a href=""https://github.com/RaRe-Technologies/gensim/issues/2914"" rel=""nofollow noreferrer"">per the discussion on the <code>gensim</code> project issue you opened for the same problem</a>, this appears to be some problem with your Python installation's logging functionality that is unrelated to <code>gensim</code> or the word2vec algorithm. And in some respects, the problem is more foundational &amp; concerning, as it indicates some replacement of core functionality with a sloppy alternative.</p>
<p>For example, if you see a similar problem with the test code...</p>
<pre><code>import logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s')

logging.info(
    &quot;TEST A %i B %.2f C %.0f D %i F %i&quot;,
    1, 2, 3, 4, 5
)
</code></pre>
<p>...then the problem is in the core <code>logging</code> module.</p>
<p>I would suggest starting from a fresh development environment – at the very least, a fresh separate Python environment (using either the core <code>venv</code> functionality or an environment-manager like <code>conda</code>), and if practical even a fresh machine/OS install.</p>
<p>If the problem with the above simple test code goes away in a fresh environment, then you can incrementally reproduce the original environment by adding libraries/tools, checking for working logging after each major step, and if the problem recurs you'll have a better idea of which step introduced it.</p>
",1,1,202,2020-08-09 08:24:40,https://stackoverflow.com/questions/63324116/word2vec-logging-missing-values
Error when building image from requirement.txt in docker,"<p>I am trying to build a Docker application that uses Python's gensim library, version 3.8.3, which is being installed via pip from a requirements.txt file.</p>
<p>However, Docker seems to have trouble while trying to do
RUN pip install -r requirements.txt</p>
<p>My Requirement.txt for reference -</p>
<pre><code>boto==2.49.0
boto3==1.14.33
botocore==1.17.33
certifi==2020.6.20
chardet==3.0.4
click==7.1.2
Cython==0.29.14
docutils==0.15.2
Flask==1.1.2
gensim==3.8.3
idna==2.10
itsdangerous==1.1.0
Jinja2==2.11.2
jmespath==0.10.0
MarkupSafe==1.1.1
numpy==1.19.1
python-dateutil==2.8.1
requests==2.24.0
s3transfer==0.3.3
scipy==1.5.2
six==1.15.0
smart-open==2.1.0
urllib3==1.25.10
Werkzeug==1.0.1
</code></pre>
<p>dockerFile</p>
<pre><code>FROM python:3.8.2-alpine
WORKDIR /project
ADD . /project
RUN set -x &amp;&amp; apk add --no-cache build-base &amp;&amp; apk add --no-cache libexecinfo-dev
RUN pip install --upgrade pip
RUN pip install -r requirements.txt
CMD [&quot;python&quot;,&quot;similarity.py&quot;] 
</code></pre>
<p>error:</p>
<pre><code>(venv) C:\Users\verma\PycharmProjects\flaskTest&gt;docker image build -t similarity-flask-api  .
Sending build context to Docker daemon  302.7MB
Step 1/7 : FROM python:3.8.2-alpine
 ---&gt; 6c32e2504283
Step 2/7 : WORKDIR /project
 ---&gt; Using cache
 ---&gt; 554b6bda89ad
Step 3/7 : ADD . /project
 ---&gt; d085a645ecb1
Step 4/7 : RUN set -x &amp;&amp; apk add --no-cache build-base &amp;&amp; apk add --no-cache libexecinfo-dev
 ---&gt; Running in e7117c1e18ff
+ apk add --no-cache build-base
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/community/x86_64/APKINDEX.tar.gz
(1/18) Installing libgcc (9.2.0-r4)
(2/18) Installing libstdc++ (9.2.0-r4)
(3/18) Installing binutils (2.33.1-r0)
(4/18) Installing libmagic (5.37-r1)
(5/18) Installing file (5.37-r1)
(6/18) Installing gmp (6.1.2-r1)
(7/18) Installing isl (0.18-r0)
(8/18) Installing libgomp (9.2.0-r4)
(9/18) Installing libatomic (9.2.0-r4)
(10/18) Installing mpfr4 (4.0.2-r1)
(11/18) Installing mpc1 (1.1.0-r1)
(12/18) Installing gcc (9.2.0-r4)
(13/18) Installing musl-dev (1.1.24-r2)
(14/18) Installing libc-dev (0.7.2-r0)
(15/18) Installing g++ (9.2.0-r4)
(16/18) Installing make (4.2.1-r2)
(17/18) Installing fortify-headers (1.1-r0)
(18/18) Installing build-base (0.5-r1)
Executing busybox-1.31.1-r9.trigger
OK: 182 MiB in 52 packages
+ apk add --no-cache libexecinfo-dev
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/community/x86_64/APKINDEX.tar.gz
(1/2) Installing libexecinfo (1.1-r1)
(2/2) Installing libexecinfo-dev (1.1-r1)
OK: 183 MiB in 54 packages
Removing intermediate container e7117c1e18ff
 ---&gt; 9e7a97f8bddc
Step 5/7 : RUN pip install --upgrade pip
 ---&gt; Running in 0286591e9e70
Collecting pip
  Downloading pip-20.2.1-py2.py3-none-any.whl (1.5 MB)
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 20.1
    Uninstalling pip-20.1:
      Successfully uninstalled pip-20.1
Successfully installed pip-20.2.1
Removing intermediate container 0286591e9e70
 ---&gt; ca837786d695
Step 6/7 : RUN pip install -r requirements.txt
 ---&gt; Running in 7f124c100c0b
Collecting boto==2.49.0
  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)
Collecting boto3==1.14.33
  Downloading boto3-1.14.33-py2.py3-none-any.whl (129 kB)
Collecting botocore==1.17.33
  Downloading botocore-1.17.33-py2.py3-none-any.whl (6.5 MB)
Collecting certifi==2020.6.20
  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)
Collecting chardet==3.0.4
  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)
Collecting click==7.1.2
  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)
Collecting Cython==0.29.14
  Downloading Cython-0.29.14.tar.gz (2.1 MB)
Collecting docutils==0.15.2
  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)
Collecting Flask==1.1.2
  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)
Collecting gensim==3.8.3
  Downloading gensim-3.8.3.tar.gz (23.4 MB)
Collecting idna==2.10
  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)
Collecting itsdangerous==1.1.0
  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)
Collecting Jinja2==2.11.2
  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)
Collecting jmespath==0.10.0
  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)
Collecting MarkupSafe==1.1.1
  Downloading MarkupSafe-1.1.1.tar.gz (19 kB)
Processing /root/.cache/pip/wheels/df/b2/64/111c431ca7f7d49afb42126b7351fe1a4894803d75026360de/numpy-1.19.1-cp38-cp38-linux_x86_64.whl
Collecting python-dateutil==2.8.1
  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)
Collecting requests==2.24.0
  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)
Collecting s3transfer==0.3.3
  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)
Collecting scipy==1.5.2
  Downloading scipy-1.5.2.tar.gz (25.4 MB)
  Installing build dependencies: started
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: still running...
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
    Preparing wheel metadata: started
    Preparing wheel metadata: finished with status 'error'
    ERROR: Command errored out with exit status 1:
     command: /usr/local/bin/python /usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpoyjzx5wb
         cwd: /tmp/pip-install-r078skp_/scipy
    Complete output (139 lines):
    lapack_opt_info:
    lapack_mkl_info:
    customize UnixCCompiler
      libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    openblas_lapack_info:
    customize UnixCCompiler
    customize UnixCCompiler
      libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    openblas_clapack_info:
    customize UnixCCompiler
    customize UnixCCompiler
      libraries openblas,lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    flame_info:
    customize UnixCCompiler
      libraries flame not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    atlas_3_10_threads_info:
    Setting PTATLAS=ATLAS
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries tatlas,tatlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries tatlas,tatlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries tatlas,tatlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&gt;
      NOT AVAILABLE

    atlas_3_10_info:
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries satlas,satlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries satlas,satlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries satlas,satlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_3_10_info'&gt;
      NOT AVAILABLE

    atlas_threads_info:
    Setting PTATLAS=ATLAS
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries ptf77blas,ptcblas,atlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries ptf77blas,ptcblas,atlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_threads_info'&gt;
      NOT AVAILABLE

    atlas_info:
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries f77blas,cblas,atlas not found in /usr/local/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib
    customize UnixCCompiler
      libraries f77blas,cblas,atlas not found in /usr/lib
    customize UnixCCompiler
      libraries lapack_atlas not found in /usr/lib/
    customize UnixCCompiler
      libraries f77blas,cblas,atlas not found in /usr/lib/
    &lt;class 'numpy.distutils.system_info.atlas_info'&gt;
      NOT AVAILABLE

    accelerate_info:
      NOT AVAILABLE

    lapack_info:
    customize UnixCCompiler
      libraries lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
      NOT AVAILABLE

    lapack_src_info:
      NOT AVAILABLE

      NOT AVAILABLE

    setup.py:460: UserWarning: Unrecognized setuptools command ('dist_info --egg-base /tmp/pip-modern-metadata-ujofw06w'), proceeding with generating Cython sources
and expanding templates
      warnings.warn(&quot;Unrecognized setuptools command ('{}'), proceeding with &quot;
    Running from SciPy source directory.
    /tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/numpy/distutils/system_info.py:1712: UserWarning:
        Lapack (http://www.netlib.org/lapack/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [lapack]) or by setting
        the LAPACK environment variable.
      if getattr(self, '_calc_info_{}'.format(lapack))():
    /tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/numpy/distutils/system_info.py:1712: UserWarning:
        Lapack (http://www.netlib.org/lapack/) sources not found.
        Directories to search for the sources can be specified in the
        numpy/distutils/site.cfg file (section [lapack_src]) or by setting
        the LAPACK_SRC environment variable.
      if getattr(self, '_calc_info_{}'.format(lapack))():
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py&quot;, line 280, in &lt;module&gt;
        main()
      File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py&quot;, line 263, in main
        json_out['return_val'] = hook(**hook_input['kwargs'])
      File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py&quot;, line 133, in prepare_metadata_for_build_wheel
        return hook(metadata_directory, config_settings)
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 157, in prepare_metadata_for_build_wheel
        self.run_setup()
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 248, in run_setup
        super(_BuildMetaLegacyBackend,
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 142, in run_setup
        exec(compile(code, __file__, 'exec'), locals())
      File &quot;setup.py&quot;, line 583, in &lt;module&gt;
        setup_package()
      File &quot;setup.py&quot;, line 579, in setup_package
        setup(**metadata)
      File &quot;/tmp/pip-build-env-mw61mr08/overlay/lib/python3.8/site-packages/numpy/distutils/core.py&quot;, line 137, in setup
        config = configuration()
      File &quot;setup.py&quot;, line 477, in configuration
        raise NotFoundError(msg)
    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.
    ----------------------------------------
ERROR: Command errored out with exit status 1: /usr/local/bin/python /usr/local/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_bu
ild_wheel /tmp/tmpoyjzx5wb Check the logs for full command output.
The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1
</code></pre>
<p><strong>I tried this thread - <a href=""https://stackoverflow.com/questions/44732303/docker-unable-to-install-numpy-scipy-or-gensim"">Docker unable to install numpy, scipy, or gensim</a>
As suggested I added line 4 and 5 in my dockerFile but it is still not working.</strong></p>
","python, docker, gensim","<p>in the post you mension, thye install <code>libc-dev</code> to compile packs ...<br />
you dont.</p>
<pre><code>RUN apt-get -y install libc-dev
RUN apt-get -y install build-essential
</code></pre>
<p>I have problems trying to use &quot;alpine&quot;  with Python...
so we choose <a href=""https://hub.docker.com/_/python"" rel=""nofollow noreferrer"">&quot;slim-buster&quot;</a>  as docker image for Python.</p>
<p>so if you can...</p>
<p>1 - I would try slim-buster if you can<br />
2 - Try a numpy ready docker image and install your python packages.</p>
",1,-1,3319,2020-08-10 17:55:23,https://stackoverflow.com/questions/63345527/error-when-building-image-from-requirement-txt-in-docker
How to fix unpickling key error when loading word2vec (gensim)?,"<p>I am trying to load a pre-trained word2vec model in pkl format taken from <a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">here</a></p>
<p>The line of code I use to load it:</p>
<pre><code>model = gensim.models.KeyedVectors.load('enwiki_20180420_500d.pkl') 
</code></pre>
<p>However, i keep getting the following error (full traceback):</p>
<pre><code>UnpicklingError                           Traceback (most recent call last)
&lt;ipython-input-15-ebd5780b6636&gt; in &lt;module&gt;
     55 
     56 #Load pretrained word2vec
---&gt; 57 model = gensim.models.KeyedVectors.load('enwiki_20180420_500d.pkl',mmap='r')
     58 

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load(cls, fname_or_handle, **kwargs)
   1551     @classmethod
   1552     def load(cls, fname_or_handle, **kwargs):
-&gt; 1553         model = super(WordEmbeddingsKeyedVectors, cls).load(fname_or_handle, **kwargs)
   1554         if isinstance(model, FastTextKeyedVectors):
   1555             if not hasattr(model, 'compatible_hash'):

~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py in load(cls, fname_or_handle, **kwargs)
    226     @classmethod
    227     def load(cls, fname_or_handle, **kwargs):
--&gt; 228         return super(BaseKeyedVectors, cls).load(fname_or_handle, **kwargs)
    229 
    230     def similarity(self, entity1, entity2):

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in load(cls, fname, mmap)
    433         compress, subname = SaveLoad._adapt_by_suffix(fname)
    434 
--&gt; 435         obj = unpickle(fname)
    436         obj._load_specials(fname, mmap, compress, subname)
    437         logger.info(&quot;loaded %s&quot;, fname)

~/anaconda3/lib/python3.7/site-packages/gensim/utils.py in unpickle(fname)
   1396         # Because of loading from S3 load can't be used (missing readline in smart_open)
   1397         if sys.version_info &gt; (3, 0):
-&gt; 1398             return _pickle.load(f, encoding='latin1')
   1399         else:
   1400             return _pickle.loads(f.read())

UnpicklingError: invalid load key, ':'.
</code></pre>
<p>I tried loading it with load_word2vec_format, but no luck. Any ideas what might be wrong with it?</p>
","python, gensim, word2vec","<p>Per your link <a href=""https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"" rel=""nofollow noreferrer"">https://wikipedia2vec.github.io/wikipedia2vec/pretrained/</a> these are to be loaded using that library's <code>Wikipedia2Vec.load()</code> method.</p>
<p>Gensim's <code>.load()</code> methods should only be used with files saved directly from Gensim model objects.</p>
<p>The Wikipedia2Vec project does say that their <code>.txt</code> file formats would load with <code>.load_word2vec_format()</code>, so you could also try that - but with one of their <code>.txt</code> format files.</p>
<p>Their full model <code>.pkl</code> files are only going to work with their class's own loading function.</p>
",1,0,1520,2020-08-12 22:01:03,https://stackoverflow.com/questions/63385272/how-to-fix-unpickling-key-error-when-loading-word2vec-gensim
Array reshape error when loading word2vec model,"<p>I have the following piece of code:</p>
<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load('model2')
X = model[model.wv.vocab]
</code></pre>
<p>This piece of code works on one of my machines but not another. The model file is the same. What's going on? The error message I get is the following:</p>
<pre><code>  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/word2vec.py&quot;, line 1330, in load
    model = super(Word2Vec, cls).load(*args, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 1244, in load
    model = super(BaseWordEmbeddingsModel, cls).load(*args, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/models/base_any2vec.py&quot;, line 603, in load
    return super(BaseAny2VecModel, cls).load(fname_or_handle, **kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 427, in load
    obj._load_specials(fname, mmap, compress, subname)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 458, in _load_specials
    getattr(self, attrib)._load_specials(cfname, mmap, compress, subname)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/gensim/utils.py&quot;, line 469, in _load_specials
    val = np.load(subname(fname, attrib), mmap_mode=mmap)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/numpy/lib/npyio.py&quot;, line 440, in load
    pickle_kwargs=pickle_kwargs)
  File &quot;/home/ec2-user/miniconda3/envs/word2vec/lib/python3.7/site-packages/numpy/lib/format.py&quot;, line 771, in read_array
    array.shape = shape
ValueError: cannot reshape array of size 16777184 into shape (134441,128)
</code></pre>
<p>To install gensim, I used <code>conda install -c anaconda gensim</code></p>
","python, amazon-ec2, gensim, word2vec","<p>I checked what @gojomo referred to in the comments and he was correct, my file sizes were wrong. Something must have happened during upload. For large models, word2vec saves the model in 3 files. Assuming your model name is &quot;model2&quot; you will have:</p>
<ol>
<li>model2</li>
<li>model2.trainables.syn1neg.npy</li>
<li>model2.wv.vectors.npy</li>
</ol>
<p>My <code>.wv.vectors.npy</code> was a few kilo bytes too small than the version in my other machine.</p>
",0,0,404,2020-08-14 19:59:19,https://stackoverflow.com/questions/63419318/array-reshape-error-when-loading-word2vec-model
Best practice for deploying machine learning web app with Flask,"<p>Hoping you can help. I’m creating a web app with python and Flask. One of the the things that my web app will do is provide a smart document search. You can enter text and it will fetch results of documents similar to the portion of text you entered.</p>
<p>I’ve used Flask for the front end to serve the HTML, manage any DB interactions required and display results. It will pass the query through to a Gensim similarity model and query it.</p>
<p>My question here is what is the best way to host these? I’ve explored loading the model as part of loading flask but it slows things down quite a lot (it’s c. 6gb in memory) but it works. I can then query the model quite easily as it’s all within the same program scope.</p>
<p>My concern is that this would then not be scalable and possibly not best practice and that I may be better to host the model separately and make API calls to it from my Flask web app.</p>
<p>Thoughts and views would be much appreciated.</p>
<p>Thanks,
Pete</p>
","python, flask, gensim","<p>Your thoughts are definitely on the right track.</p>
<p>Yes, you should separate the hosting of the model from your web app. Your suggestion of an API is a good one. Even if in the beginning it is all hosted on one machine, it is still worth doing this separation.</p>
<p>Once you are hosting this separately via an API, then as your web app has more users, it becomes easy to scale the model API.</p>
<p>Whether by launching more instances and balancing requests. Or, depending on requirements, you could add scalability and robustness via messaging, like Rabbitmq, or a mix of the two.</p>
<p>For example, some systems that access extremely large datasets, return a response via email to let you know your answer is ready to download or view. In this case, you may host one instance of the model, and put requests in q queue to answer one by one.</p>
<p>If you need very fast responses from your model, then you are likely to scale via more instances and balancing.</p>
<p>Both options above can be rolled out yourself, using open source solutions, or you can go straight to managed services in the cloud that will auto scale via either of these methods.</p>
<p>If you are just producing this project yourself with no funding, then you most likely do not want to start by using managed services in the cloud, as these will auto scale your bank account in the wrong direction.</p>
<p>The above solutions allow you to make changes, update the model, even use a different one, and release it on its own as long as it still conforms to the API.</p>
<p>Separation of boundaries in data, and responsibilities in behaviour are important in having a scalable and maintainable architecture.</p>
",1,0,543,2020-08-20 05:56:51,https://stackoverflow.com/questions/63499110/best-practice-for-deploying-machine-learning-web-app-with-flask
Gensim&#39;s word2vec has a loss of 0 from epoch 1?,"<p>I am using the Word2vec module of Gensim library to train a word embedding, the dataset is 400k sentences with 100k unique words (its not english)</p>
<p>I'm using this code to monitor and calculate the loss :</p>
<pre class=""lang-py prettyprint-override""><code>class MonitorCallback(CallbackAny2Vec):
    def __init__(self, test_words):
        self._test_words = test_words

    def on_epoch_end(self, model):
        print(&quot;Model loss:&quot;, model.get_latest_training_loss())  # print loss
        for word in self._test_words:  # show wv logic changes
            print(model.wv.most_similar(word))


monitor = MonitorCallback([&quot;MyWord&quot;])  # monitor with demo words

w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT  , callbacks=[monitor])

w2v_model.build_vocab(tokenized_corpus)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print(&quot;Vocab size&quot;, vocab_size)

print(&quot;[*] Training...&quot;)

# Train Word Embeddings
w2v_model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=W2V_EPOCH)
</code></pre>
<p>The problem is from epoch 1 the loss is 0 and the vector of the monitored words dont change at all!</p>
<pre><code>[*] Training...
Model loss: 0.0
Model loss: 0.0
Model loss: 0.0
Model loss: 0.0
</code></pre>
<p>so what is the problem here? is this normal?  the tokenized corpus is a list of lists that are something like tokenized_corpus[0] = [ &quot;word1&quot; , &quot;word2&quot; , ...]</p>
<p>I googled and seems like some of the old versions of gensim had problem with calculating loss function, but they are from almost a year ago and it seems like it should be fixed right now?</p>
<p>I tried the code provided in the answer of this question as well but still the loss is 0 :</p>
<p><a href=""https://stackoverflow.com/questions/52038651/loss-does-not-decrease-during-training-word2vec-gensim"">Loss does not decrease during training (Word2Vec, Gensim)</a></p>
<p>EDIT1 : after adding compute_loss=True, the loss shows up, but it keeps going higher and higher, and the top similar words and their similarity doesn't change at all :</p>
<pre><code>Model loss: 2187903.5
Model loss: 3245492.0
Model loss: 4103624.5
Model loss: 4798541.0
Model loss: 5413940.0
Model loss: 5993822.5
Model loss: 6532631.0
Model loss: 7048384.5
Model loss: 7547147.0
</code></pre>
","nlp, pytorch, gensim, word2vec","<p>The top issue with your code is that you haven't used the <code>Word2Vec</code> initialization parameter necessary to toggle loss-tracking on: <code>compute_loss=True</code></p>
<p>(See 'parameters' section of <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec</a> )</p>
<p>Even with that fix, the loss-reporting is still quite buggy (as of <code>gensim-3.8.3</code> &amp; this writing in August 2020):</p>
<ul>
<li>it's not the per-epoch total, or per-example average, one might expect. (So if you need that, as a workaround, your callback should be remembering the last value and computing the delta, or resetting the internal counter to <code>0.0</code>, each epoch's end.)</li>
<li>it definitely loses precision in larger training runs, eventually becoming useless. (This may not be an issue for you.)</li>
<li>it might lose some tallies due to multithreaded value-overwriting. (This may not be a practical issue for you, depending on why you're consulting the loss value.)</li>
</ul>
",2,2,1748,2020-08-20 16:58:37,https://stackoverflow.com/questions/63509864/gensims-word2vec-has-a-loss-of-0-from-epoch-1
Find most similar words to randomy initialized array,"<p>Using the Gensim package, I have trained a word2vec model on the corpus that I am working with as follows:</p>
<pre><code>word2vec = Word2Vec(all_words, min_count = 3, size = 512, sg = 1)
</code></pre>
<p>Using Numpy, I have initialized a random array with the same dimensions:</p>
<pre><code>vector = (rand(512)-0.5) *20
</code></pre>
<p>Now, I would like to find the words from the word2vec that are most similar to the random vector that I initialized.</p>
<p>For words in the word2vec, you can run:</p>
<pre><code>word2vec.most_similar('word')
</code></pre>
<p>And the output is a list with most similar words and their according distance.</p>
<p>I would like to get a similar output for my initialized array.</p>
<p>However, when I run:</p>
<pre><code>word2vec.most_similar(vector)
</code></pre>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-297-3815cf183d05&gt;&quot;, line 1, in &lt;module&gt;
    word2vec.most_similar(vector)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\utils.py&quot;, line 1461, in new_func1
    return func(*args, **kwargs)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\base_any2vec.py&quot;, line 1383, in most_similar
    return self.wv.most_similar(positive, negative, topn, restrict_vocab, indexer)

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 549, in most_similar
    for word, weight in positive + negative:

TypeError: cannot unpack non-iterable numpy.float64 object
</code></pre>
<p>What can I do to overcome this error and find the most similar words to my arrays?</p>
<p>I've checked <a href=""https://stackoverflow.com/questions/54273077/cannot-unpack-non-iterable-numpy-float64-object-python3-opencv"">this</a> and <a href=""https://stackoverflow.com/questions/59357940/typeerror-cannot-unpack-non-iterable-numpy-float64-object"">this</a> page. However, it is unclear to me how I could solve my problem with these suggestions.</p>
","python, numpy, typeerror, gensim, word2vec","<p>Gensim's <code>KeyedVectors</code> interface <code>.most_similar()</code> method <strong>can</strong> take raw vectors as its target, but in order for its current (at least through <code>gensim-3.8.3</code>) argument-type-detection to not mistake a single vector for a list-of-keys, you would need to provide it explicitly as one member of a list of items for the named <code>positive</code> parameter.</p>
<p>Specifically, this should work:</p>
<pre class=""lang-py prettyprint-override""><code>similars = word2vec.wv.most_similar(positive=[vector,])
</code></pre>
",1,1,233,2020-08-21 14:11:48,https://stackoverflow.com/questions/63524493/find-most-similar-words-to-randomy-initialized-array
Is a gensim vocab index the index in the corresponding 1-hot-vector?,"<p>I am doing research that requires direct manipulation &amp; embedding of one-hot vectors and I am trying to use gensim to load a pretrained word2vec model for this.</p>
<p>The problem is they don't seem to have a direct api for working with 1-hot-vectors. And I am looking for work arounds.</p>
<p>So I wanted to know if anyone knows of a way to do this? Or more specifically if these vocab indices (which are defined quite ambiguously). Could be indices into corresponding 1-hot-vectors?</p>
<p>Context I have found:</p>
<ul>
<li>Seems <a href=""https://stackoverflow.com/questions/40458742/gensim-word2vec-accessing-in-out-vectors"">this question</a> is related but I tried accessing the 'input embeddings' (assuming they were one-hot representations), via model.syn0 (from link in answer), but I got a non-sparse matrix...</li>
<li>Also appears <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">they refer to word indices as 'doctags'</a> (search for Doctag/index).</li>
<li><a href=""https://stackoverflow.com/questions/47117569/how-to-get-word2index-from-gensim"">Here</a> is another question giving some context to the indices (although not quite answering my question).</li>
<li><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">Here</a> is the official documentation:</li>
</ul>
<p>################################################</p>
<p>class gensim.models.keyedvectors.Vocab(**kwargs)
Bases: object</p>
<p>A single vocabulary item, used internally for collecting per-word frequency/sampling info, and for constructing binary trees (incl. both word leaves and inner nodes).</p>
<p>################################################</p>
","gensim, word2vec, one-hot-encoding","<p>Yes, you can think of the <code>index</code> (position) of gensim's <code>Word2Vec</code> word-vectors as being the one dimension that would be <code>1.0</code> – with all other V dimensions, where V is the count of unique words, being <code>0.0</code>.</p>
<p>The implementation doesn't actually ever create one-hot vectors, as a sparse or explicit representation. It's just using the word's index as a look-up for its dense vector – following in the path of the <code>word2vec.c</code> code from Google on which the gensim implementation was originally based.</p>
<p>(The term 'doctags' is only relevant in the <code>Doc2Vec</code> – aka 'Paragraph Vector' – implementation. There it is the name for the distinct tokens/ints that are used for looking up document-vectors, using a different namespace from in-document words. That is, in <code>Doc2Vec</code> you could use <code>'doc_007'</code> as a doc-vector name, aka a 'doctag', and even if the string-token <code>'doc_007'</code> also appears as a word inside documents, the doc-vector referenced by doctag-key <code>'doc_007'</code> and the word-vector referenced by word-key <code>'doc_007'</code> wouldn't be the same internal vector.)</p>
",1,0,414,2020-08-23 17:24:27,https://stackoverflow.com/questions/63549977/is-a-gensim-vocab-index-the-index-in-the-corresponding-1-hot-vector
Does the gensim `Word2Vec()` constructor make a completely independent model?,"<p>I'm testing feeding gensim's Word2Vec different sentences with the same overall vocabulary to see if some sentences carry &quot;better&quot; information than others. My method to train Word2Vec looks like this</p>
<pre><code>def encode_sentences(self, w2v_params, sentences):
    model = Word2Vec(sentences, **w2v_params)
    
    idx_order = torch.tensor([int(i) for i in model.wv.index2entity], dtype=torch.long)
    X = torch.zeros((idx_order.max()+1, w2v_params['size']), dtype=torch.float)
    
    # Put embeddings back in order
    X[idx_order] = torch.tensor(model.wv.vectors)    
    return X, y
</code></pre>
<p>What I'm hoping for here, is each time w2v runs, it starts with a fresh model and trains from scratch. However, I'm testing 3 kinds of sentences, so my test code looks like this:</p>
<pre><code>def test(sentence):
    w2v = {'size': 128, 'sg': 1}
    X = encode_sentences(w2v, sentence)
    evaluate(X) # Basic cluster analysis stuff here

# s1, s2 and s3 are the 3 sets of sentences with the same vocabulary in different order/frequency
[print(test(s) for s in [s1, s2, s3]]
</code></pre>
<p>However, I noticed if I remove one of the test sets, and only test <code>s1</code> and <code>s2</code> (or any combination of 2 sets of the three), the overall quality of the clusterings decreases. If I go back into <code>encode_sentences</code> and add <code>del model</code> before the <code>return</code> call, the overall cluster quality also goes down but remains consistent no matter how many datasets are tested.</p>
<p>What gives? Is the constructor not actually building a fresh model each time with new weights? The docs and source code give no indication of this. I'm quite sure it isn't my evaluation method, as everything was fixed after the <code>del model</code> was added. I'm at a loss here... Are these runs actually independent, or is each call to <code>Word2Vec(foo, ...)</code> equivalent to retraining the previous model with <code>foo</code> as new data?</p>
<p>And before you ask, no <code>model</code> is nowhere outside of the scope of the <code>encode_sentence</code> variable; that's the only time that variable name is used in the whole program. Very odd.</p>
<h3>Edit with more details</h3>
<hr />
<p>If it's important, I'm using Word2Vec to build node embeddings on a graph the way Node2Vec does with different walk strategies. These embeddings are then fed to a Logistic Regression model (<code>evaluate(X)</code>) and which calculates area under the roc.</p>
<p>Here is some sample output of the model before adding the <code>del model</code> call to the <code>encode_sentences</code> method averaged over 5 trials:</p>
<pre><code>Random walks:   0.9153 (+/-) 0.002
Policy walks:   0.9125 (+/-) 0.005
E-greedy walks: 0.8489 (+/-) 0.011
</code></pre>
<p>Here is the same output with the only difference being <code>del model</code> in the encoding method:</p>
<pre><code>Random walks:   0.8627 (+/-) 0.005
Policy walks:   0.8527 (+/-) 0.002
E-greedy walks: 0.8385 (+/-) 0.009
</code></pre>
<p>As you can see, in each case, the variance is very low (the +/- value is the standard error) but the difference between the two runs is almost a whole standard deviation. It seems odd that if each call to <code>Word2Vec</code> was truly independent that manually freeing the data structure would have such a large effect.</p>
","python, gensim, word2vec","<p>Each call to the <code>Word2Vec()</code> constructor creates an all-new model.</p>
<p>However, runs are <em>not</em> completely deterministic under normal conditions, for a <a href=""https://github.com/RaRe-Technologies/gensim/wiki/recipes-&amp;-faq#q11-ive-trained-my-word2vecdoc2vecetc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">variety of reasons</a>, so results quality for downstream evaluations (like your unshown clustering) will jitter from run-to-run.</p>
<p>If the variance in repeated runs with the same data is very large, there are probably other problems, such an oversized model prone to overfitting. (Stability from run-to-run can be one indicator that your process is sufficiently specified that the data and model choices are driving results, not the randomness used by the algorithm.)</p>
<p>If this explanation isn't satisfying, try adding more info to your question - such as the actual magnitude of your evaluation scores, in repeated runs, both with and without the changes that you conjecture are affecting results. (I suspect the variations from the steps you think are having effect will be no larger than variations from re-runs or different <code>seed</code> values.)</p>
<p>(More generally, <code>Word2Vec</code> is generally hungry for as much varies training data as possible; only if texts are <em>non-representative of the relevant domain</em> are they likely to result in a worse model. So I generally wouldn't expect being choosier about which subset of sentences is best to be an important technique, unless some of the sentences are total junk/noise, but of course there's always a change you'll find some effects in your particular data/goals.)</p>
",1,0,188,2020-08-23 20:03:59,https://stackoverflow.com/questions/63551484/does-the-gensim-word2vec-constructor-make-a-completely-independent-model
IndexError: index is out of bounds - word2vec,"<p>I have trained a word2vec model called <code>word_vectors</code>, using the Gensim package with size = 512.</p>
<pre><code>fname = get_tmpfile('word2vec.model')
word_vectors = KeyedVectors.load(fname, mmap='r')
</code></pre>
<p>Now, I have created a new Numpy array (also of size 512) which I have added to the word2vec as follows:</p>
<pre><code>vector = (rand(512)-0.5) *20
word_vectors.add('koffie', vector)
</code></pre>
<p>Doing this seems to go fine and even when I call</p>
<pre><code>word_vectors['koffie']
</code></pre>
<p>I get the array as output, as expected.</p>
<p>However, when I want to look for the most similar words in my model and run the following code:</p>
<pre><code>word_vectors.most_similar('koffie')
</code></pre>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-283-ce992786ce89&gt;&quot;, line 1, in &lt;module&gt;
    word_vectors.most_similar('koffie')

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\envs\ldaword2vec\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 553, in most_similar
    mean.append(weight * self.word_vec(word, use_norm=True))

  File &quot;C:\Users\20200016\AppData\Local\Continuum\anaconda3\envs\ldaword2vec\lib\site-packages\gensim\models\keyedvectors.py&quot;, line 461, in word_vec
    result = self.vectors_norm[self.vocab[word].index]

IndexError: index 146139 is out of bounds for axis 0 with size 146138


word_vector.size()
Traceback (most recent call last):

  File &quot;&lt;ipython-input-284-2606aca38446&gt;&quot;, line 1, in &lt;module&gt;
    word_vector.size()

NameError: name 'word_vector' is not defined
</code></pre>
<p>The error seems to indicate that my indexing isn't correct here. But since I am only indexing indirectly (with a key rather than an actual numeric index), I don't see what I need to change here.</p>
<p>Who knows what goes wrong here? And what can I do to overcome this error?</p>
","python-3.x, numpy, gensim, word2vec, index-error","<p>The 1st time you do a <code>.most_similar()</code>, a <code>KeyedVectors</code> instance (in gensim versions through 3.8.3) will create a cache of unit-normalized vectors to assist in all subsequent bulk-similarity operations, and place it in <code>.vectors_norm</code>.</p>
<p>It looks like your addition of a new vector didn't flush/recalculate/expand that cached <code>.vectors_norm</code> - originally the <code>KeyedVectors</code> class and <code>.most_similar()</code> operation were not designed with constantly-growing or constantly-changing sets-of-vectors in mind, but rather as utilities for a post-training, frozen set of vectors.</p>
<p>So that's the cause of your <code>IndexError</code>.</p>
<p>You should be able to work-around this by explicitly clearing the <code>.vectors_norm</code> any time you perform modifications/additions to the <code>KeyedVectors</code>, eg:</p>
<pre class=""lang-py prettyprint-override""><code>word_vectors.vectors_norm = None
</code></pre>
<p>(This shouldn't be necessary in the next 4.0.0 release of gensim, but I'll double-check there's not a similar problem there.)</p>
<p>Separately:</p>
<ul>
<li><p>Your <code>'word_vector' is not defined</code> error is simply because you seem to have left the 's' off your chosen variable name <code>word_vectors</code></p>
</li>
<li><p>You probably don't need to be using the gensim-testing-utility-method <code>get_tmpfile()</code> - just use your own explicit, intentional filesystem paths for saving and loading</p>
</li>
<li><p>Whether it's proper to use <code>KeyedVectors.load()</code> depends on what was saved. If you are in fact saving a full <code>Word2Vec</code> class instance (more than just the vectors), using <code>Word2Vec.load()</code> would be more appropriate.</p>
</li>
</ul>
",1,0,1111,2020-08-24 19:40:43,https://stackoverflow.com/questions/63567713/indexerror-index-is-out-of-bounds-word2vec
How to load pre-trained fastText model in gensim with .npy extension,"<p>I am new to deep learning and I am trying to play with a pretrained word embedding model from a <a href=""https://www.cse.iitb.ac.in/%7Epb/papers/sltu-ccurl20-il-we.pdf"" rel=""nofollow noreferrer"">paper</a>. I  downloaded the following files:</p>
<p>1)sa-d300-m2-fasttext.model</p>
<p>2)sa-d300-m2-fasttext.model.trainables.syn1neg.npy</p>
<p>3)sa-d300-m2-fasttext.model.trainables.vectors_ngrams_lockf.npy</p>
<p>4)sa-d300-m2-fasttext.model.wv.vectors.npy</p>
<p>5)sa-d300-m2-fasttext.model.wv.vectors_ngrams.npy</p>
<p>6)sa-d300-m2-fasttext.model.wv.vectors_vocab.npy</p>
<p>If in  case these details are needed
sa - sanskrit
d300 - embedding dimension
fastText - fastText</p>
<p>I dont have a prior experience with gensim, how can load the model into gensim or into tensorflow.</p>
<p>I tried</p>
<pre><code>from gensim.models.wrappers import FastText
FastText.load_fasttext_format('/content/sa/300/fasttext/sa-d300-m2-fasttext.model.wv.vectors_ngrams.npy')
</code></pre>
<blockquote>
<p>FileNotFoundError: [Errno 2] No such file or directory: '/content/sa/300/fasttext/sa-d300-m2-fasttext.model.wv.vectors_ngrams.npy.bin'</p>
</blockquote>
","gensim, pre-trained-model, fasttext","<p>That set of multiple files looks like it was saved from Gensim's FastText implementation, using Gensim's <code>save()</code> method - and thus is <strong>not</strong> in Facebook's original 'fasttext_format'.</p>
<p>So, try loading them with the following instead:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models.fasttext import FastText
model = FastText.load('/content/sa/300/fasttext/sa-d300-m2-fasttext.model')
</code></pre>
<p>(Upon loading that main/root file, it will find the subsidiary related files in the same directory, as long as they're all present.)</p>
<p>The source where you downloaded these files should have included clear instructions for loading them nearby!</p>
",2,1,3156,2020-08-28 15:59:06,https://stackoverflow.com/questions/63637245/how-to-load-pre-trained-fasttext-model-in-gensim-with-npy-extension
Gensim Word2vec model is not updating the previous word&#39;s embedding weights during increased training,"<p>I want to train a previous-trained word2vec model in a increased way that is update the word's weights if the word has been seen in the previous training process and create and update the weights of the new words that has not been seen in the previous training process. For example:</p>
<pre><code>from gensim.models import Word2Vec
# old corpus
corpus = [[&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;2&quot;, &quot;3&quot;, &quot;1&quot;]]
# first train on old corpus
model = Word2Vec(sentences=corpus, size=2, min_count=0, window=2)
# checkout the embedding weights for word &quot;1&quot;
print(model[&quot;1&quot;])

# here comes a new corpus with new word &quot;4&quot; and &quot;5&quot;
newCorpus = [[&quot;4&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;1&quot;, &quot;5&quot;, &quot;2&quot;]]

# update the previous trained model
model.build_vocab(newCorpus, update=True)
model.train(newCorpus, total_examples=model.corpus_count, epochs=1)

# check if new word has embedding weights:
print(model[&quot;4&quot;])  # yes

# check if previous word's embedding weights are updated
print(model[&quot;1&quot;])  # output the same as before
</code></pre>
<p>It seems that the previous word's embedding is not updated even though the previous word's context has benn changed in the new corpus. Could someone tell me how to make the previous embedding weights updated?</p>
","python, nlp, gensim, word2vec","<p><strong>Answer for original question</strong></p>
<p>Try printing them out (or even just a few leading dimensions, eg <code>print(model['1'][:5])</code>) before &amp; after to see if they've changed.</p>
<p>Or, at the beginning, make <code>preEmbed</code> a proper <em>copy</em> of the values (eg: <code>preEmbed = model['1'].copy()</code>).</p>
<p>I think you'll see the values have really changed.</p>
<p>Your current <code>preEmbed</code> variable will only be a <em>view</em> into the array that changes along with the underlying array, so will always return <code>True</code>s for your later check.</p>
<p>Reviewing a writeup on <a href=""https://www.tutorialspoint.com/numpy/numpy_copies_and_views.htm"" rel=""nofollow noreferrer"">Numpy Copies &amp; Views</a> will help explain what's happening with further examples.</p>
<p><strong>Answer for updated code</strong></p>
<p>It's likely that in your subsequent single-epoch training, all examples of <code>'1'</code> are being skipped via the <code>sample</code> downsampling feature, because <code>'1'</code> is a very-frequent word in your tiny corpus: 28.6% of all words. (In realistic natural-language corpora, the most-frequent word won't be more than a few percent of all words.)</p>
<p>I suspect if you disable this downsampling feature with <code>sample=0</code>, you'll see the changes you expect.</p>
<p>(Note that this feature is really helpful with adequate training data, and more generally, lots of things about <code>Word2Vec</code> &amp; related algorithms, and especially their core benefits, require lots of diverse data – and won't work well, or behave in expected ways, with toy-sized datasets.)</p>
<p>Also note: your second <code>.train()</code> should use an explicitly accurate count for the <code>newCorpus</code>. (Using <code>total_examples=model.corpus_count</code> to re-use the cached corpus count may not always be appropriate when you're supplying extra data, even if it works OK here.)</p>
<p>Another thing to watch out for: once you start using a model for more-sophisticated operations like <code>.most_similar()</code>, it will have cached some calculated data for vector-to-vector comparisons, and this data won't always (at least through <code>gensim-3.8.3</code>) be refreshed with more training. So, you may have to discard that data (in <code>gensim-3.8.3</code> by <code>model.wv.vectors_norm = None</code>) to be sure to have fresh unit-normed vectors, or fresh <code>most_similar()</code> (&amp; related method) results.</p>
",2,0,429,2020-09-05 08:40:41,https://stackoverflow.com/questions/63752033/gensim-word2vec-model-is-not-updating-the-previous-words-embedding-weights-duri
Topic wise document distribution in Gensim LDA,"<p>Is there a way in python to map documents belonging to a certain topic. For example a list of documents that are primarily &quot;Topic 0&quot;. I know there are ways to list topics for each document but how do I do it the other way around?</p>
<p>Edit:</p>
<p>I am using the following script for LDA:</p>
<pre class=""lang-py prettyprint-override""><code>    doc_set = []
    for file in files:
        newpath = (os.path.join(my_path, file)) 
        newpath1 = textract.process(newpath)
        newpath2 = newpath1.decode(&quot;utf-8&quot;)
        doc_set.append(newpath2)

    texts = []
    for i in doc_set:
        raw = i.lower()
        tokens = tokenizer.tokenize(raw)
        stopped_tokens = [i for i in tokens if not i in stopwords.words()]
        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
        texts.append(stemmed_tokens)

    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, random_state=0, id2word = dictionary, passes=1)
</code></pre>
","python, gensim, lda","<p>You've got a tool/API (Gensim LDA) that, when given a document, gives you a list of topics.</p>
<p>But you want the reverse: a list of documents, for a topic.</p>
<p>Essentially, you'll want to build the reverse-mapping yourself.</p>
<p>Fortunately Python's native dicts &amp; idioms for working with mapping make this pretty simple - just a few lines of code - as long as you're working with data that fully fits in memory.</p>
<p>Very roughly the approach would be:</p>
<ul>
<li>create a new structure (<code>dict</code> or <code>list</code>) for mapping topics to lists-of-documents</li>
<li>iterate over all docs, adding them (perhaps with scores) to that topic-to-docs mapping</li>
<li>finally, look up (&amp; perhaps sort) those lists-of-docs, for each topic of interest</li>
</ul>
<p>If your question could be edited to include more information about the format/IDs of your documents/topics, and how you've trained your LDA model, this answer could be expanded with more specific example code to build the kind of reverse-mapping you'd need.</p>
<p><strong>Update for your code update:</strong></p>
<p>OK, if your model is in <code>ldamodel</code> and your BOW-formatted docs in <code>corpus</code>, you'd do something like:</p>
<pre class=""lang-py prettyprint-override""><code># setup: get the model's topics in their native ordering...
all_topics = ldamodel.print_topics()
# ...then create a empty list per topic to collect the docs:
docs_per_topic = [[] for _ in all_topics]

# now, for every doc...
for doc_id, doc_bow in enumerate(corpus):
    # ...get its topics...
    doc_topics = ldamodel.get_document_topics(doc_bow)
    # ...&amp; for each of its topics...
    for topic_id, score in doc_topics:
        # ...add the doc_id &amp; its score to the topic's doc list
        docs_per_topic[topic_id].append((doc_id, score))
</code></pre>
<p>After this, you can see the list of all <code>(doc_id, score)</code> values for a certain topic like this (for topic 0):</p>
<pre class=""lang-py prettyprint-override""><code>print(docs_per_topic[0])
</code></pre>
<p>If you're interested in the top docs per topic, you can further sort each list's pairs by their score:</p>
<pre class=""lang-py prettyprint-override""><code>for doc_list in docs_per_topic:
    doc_list.sort(key=lambda id_and_score: id_and_score[1], reverse=True)
</code></pre>
<p>Then, you could get the top-10 docs for topic 0 like:</p>
<pre class=""lang-py prettyprint-override""><code>print(docs_per_topic[0][:10])
</code></pre>
<p>Note that this does everything using all-in-memory lists, which might become impractical for very-large corpuses. In some cases, you might need to compile the per-topic listings into disk-backed structures, like files or a database.</p>
",5,3,2171,2020-09-07 11:52:42,https://stackoverflow.com/questions/63777101/topic-wise-document-distribution-in-gensim-lda
Sentences embedding using word2vec,"<p>I'd like to compare the difference among the same word mentioned in different sentences, for example &quot;travel&quot;.
What I would like to do is:</p>
<ul>
<li>Take the sentences mentioning the term &quot;travel&quot; as plain text;</li>
<li>In each sentence, replace 'travel' with travel_sent_x.</li>
<li>Train a word2vec model on these sentences.</li>
<li>Calculate the distance between travel_sent1, travel_sent2, and other relabelled mentions of &quot;travel&quot;
So each sentence's &quot;travel&quot; gets its own vector, which is used for comparison.</li>
</ul>
<p>I know that word2vec requires much more than several sentences to train reliable vectors. The official page recommends datasets including billions of words, but I have not a such number in my dataset(I have thousands of words).</p>
<p>I was trying to test the model with the following few sentences:</p>
<pre><code>    Sentences
    Hawaii makes a move to boost domestic travel and support local tourism
    Honolulu makes a move to boost travel and support local tourism
    Hawaii wants tourists to return so much it's offering to pay for half of their travel expenses
</code></pre>
<p>My approach to build the vectors has been:</p>
<pre><code>from gensim.models import Word2Vec

vocab = df['Sentences']))
model = Word2Vec(sentences=vocab, size=100, window=10, min_count=3, workers=4, sg=0)
df['Sentences'].apply(model.vectorize)
</code></pre>
<p>However I do not know how to visualise the results to see their similarity and get some useful insight.
Any help and advice will be welcome.</p>
<p>Update: I would use Principal Component Analysis algorithm to visualise embeddings in 3-dimensional space. I know how to do for each individual word, but I do not know how to do it in case of sentences.</p>
","python, gensim, word2vec, embedding","<p>If you are interested in comparing sentences, Word2Vec is not the best choice. It was shown that using it to create sentence embedding produces inferior results than a dedicated sentence embedding algorithm. If your dataset is not huge, you can't create (train a new) embedding space using your own data. This forces you to use a pre trained embedding for the sentences. Luckily, there are enough of those nowadays. I believe that Universal Sentence Encoder (by Google) will suit your needs best.</p>
<p>Once you get vector representation for you sentences you can go 2 ways:</p>
<ol>
<li>create a matrix of pairwise comparisons and visualize it as a heatmap. This representation is useful when you have some prior knowledge about how close are the sentences and you want to check you hypothesis. You can even <a href=""https://jinglescode.github.io/textual-similarity-universal-sentence-encoder/"" rel=""nofollow noreferrer"">try it online</a>.
<a href=""https://i.sstatic.net/z0CqK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/z0CqK.png"" alt=""enter image description here"" /></a></li>
<li>run t-SNE on the vector representations. This will create a 2D projection of the sentences that will preserve relative distances between them. It presents data much better than PCA. Than you can easily find neighbors of the certain sentence:
<a href=""https://i.sstatic.net/zRieX.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zRieX.gif"" alt=""enter image description here"" /></a></li>
</ol>
<p>You can learn more from <a href=""https://jinglescode.github.io/2020/02/10/build-textual-similarity-analysis-web-app/"" rel=""nofollow noreferrer"">this</a> and <a href=""https://medium.com/oneassist-tech-blog/visualizing-context-with-googles-universal-sentence-encoder-and-graphdb-c5f92b2f3db3"" rel=""nofollow noreferrer"">this</a></p>
",3,3,7172,2020-09-07 14:48:02,https://stackoverflow.com/questions/63779875/sentences-embedding-using-word2vec
Looking for an effective NLP Phrase Embedding model,"<p>The goal I want to achieve is to find a good word_and_phrase embedding model that can do:
(1) For the words and phrases that I am interested in, they have embeddings.
(2) I can use embeddings to compare similarity between two things(could be word or phrase)</p>
<p>So far I have tried two paths:</p>
<p>1: Some Gensim-loaded pre-trained models, for instance:</p>
<pre><code>from gensim.models.word2vec import Word2Vec
import gensim.downloader as api
# download the model and return as object ready for use
model_glove_twitter = api.load(&quot;fasttext-wiki-news-subwords-300&quot;)
model_glove_twitter.similarity('computer-science', 'machine-learning')
</code></pre>
<p>The problem with this path is that I do not know if a phrase has embedding. For this example, I got this error:</p>
<pre><code>KeyError: &quot;word 'computer-science' not in vocabulary&quot;
</code></pre>
<p>I will have to try different pre-trained models, such as word2vec-google-news-300, glove-wiki-gigaword-300, glove-twitter-200, etc. Results are similar, there are always phrases of interests not having embeddings.</p>
<ol start=""2"">
<li>Then I tried to use some BERT-based sentence embedding method: <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a>.</li>
</ol>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('distilbert-base-nli-mean-tokens')

from scipy.spatial.distance import cosine

def cosine_similarity(embedding_1, embedding_2):
    # Calculate the cosine similarity of the two embeddings.
    sim = 1 - cosine(embedding_1, embedding_2)
    print('Cosine similarity: {:.2}'.format(sim))

phrase_1 = 'baby girl'
phrase_2 = 'annual report'
embedding_1 = model.encode(phrase_1)
embedding_2 = model.encode(phrase_2)
cosine_similarity(embedding_1[0], embedding_2[0])
</code></pre>
<p>Using this method I was able to get embeddings for my phrases, but the similarity score was 0.93, which did not seem to be reasonable.</p>
<p>So what can I try else to achieve the two goals mentioned above?</p>
","nlp, gensim, word2vec, fasttext","<p>The problem with the first path is that <strong>you are loading fastText embeddings like word2vec embeddings and word2vec can't cope with Out Of Vocabulary words</strong>.</p>
<p>The good thing is that <strong>fastText can manage OOV words</strong>.
You can use Facebook original implementation (<code>pip install fasttext</code>) or Gensim implementation.</p>
<p>For example, using Facebook implementation, you can do:</p>
<pre><code>import fasttext
import fasttext.util

# download an english model
fasttext.util.download_model('en', if_exists='ignore')  # English
model = fasttext.load_model('cc.en.300.bin')

# get word embeddings
# (if instead you want sentence embeddings, use get_sentence_vector method)
word_1='computer-science'
word_2='machine-learning'
embedding_1=model.get_word_vector(word_1)
embedding_2=model.get_word_vector(word_2)

# compare the embeddings
cosine_similarity(embedding_1, embedding_2)
</code></pre>
",3,3,1655,2020-09-11 08:53:05,https://stackoverflow.com/questions/63843793/looking-for-an-effective-nlp-phrase-embedding-model
What is the best way to drop old &quot;words&quot; from gensim word2vec model?,"<p>I have a &quot;corpus&quot; built from an item-item graph, which means each sentence is a graph walk path and each word is an item. I want to train a word2vec model upon the corpus to obtain items' embedding vectors. The graph is updated everyday so the word2vec model is trained in an increased way (using <code>Word2Vec.save()</code> and <code>Word2Vec.load()</code>) to keep updating the items' vectors.</p>
<p>Unlike words, the items in my corpus have their lifetime and there will be new items added in everyday. In order to prevent the constant growth of the model size, I need to drop items that reached their lifetime while keep the model trainable. I've read the similar question
<a href=""https://stackoverflow.com/questions/48941648/how-to-remove-a-word-completely-from-a-word2vec-model-in-gensim"">here</a>, but this question's answer doesn't related to increased-training and is based on <code>KeyedVectors</code>. I come up with the below code, but I'm not sure if it is correct and proper:</p>
<pre><code>from gensim.models import Word2Vec
import numpy as np

texts = [[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], [&quot;a&quot;, &quot;h&quot;, &quot;b&quot;]]
m = Word2Vec(texts, size=5, window=5, min_count=1, workers=1)

print(m.wv.index2word)
print(m.wv.vectors)

# drop old words
wordsToDrop = [&quot;b&quot;, &quot;c&quot;]
for w in wordsToDrop:
    i = m.wv.index2word.index(w)
    m.wv.index2word.pop(i)
    m.wv.vectors = np.delete(m.wv.vectors, i, axis=0)
    del m.wv.vocab[w]

print(m.wv.index2word)
print(m.wv.vectors)
m.save(&quot;m.model&quot;)
del m

# increased training
new = [[&quot;a&quot;, &quot;e&quot;, &quot;n&quot;], [&quot;r&quot;, &quot;s&quot;]]
m = Word2Vec.load(&quot;m.model&quot;)
m.build_vocab(new, update=True)
m.train(new, total_examples=m.corpus_count, epochs=2)
print(m.wv.index2word)
print(m.wv.vectors)
</code></pre>
<p>After deleting and increased training, is the <code>m.wv.index2word</code> and <code>m.wv.vectors</code> still element-wise corresponding? Is there any side-effect of above code? If my way is not good, could someone give me an example to show how to drop the old &quot;words&quot; properly and keep the model trainable?</p>
","python, gensim, word2vec","<p>There's no official support for removing words from a Gensim <code>Word2Vec</code> model, once they've ever &quot;made the cut&quot; for inclusion.</p>
<p>Even the ability to <em>add</em> words isn't on a great footing, as the feature isn't based on any proven/published method of updating a <code>Word2Vec</code> model, and glosses over difficult tradeoffs in how update-batches affect the model, via choice of learning-rate or whether the batches fully represent the existing vocabulary. The safest course is to regularly re-train the model from scratch, with a full corpus with sufficient examples of all relevant words.</p>
<p>So, my main suggestion would be to regularly replace your model with a new one trained with all still-relevant data. That would ensure it's no longer wasting model state on obsolete terms, and that all still-live terms have received coequal, interleaved training.</p>
<p>After such a reset, word-vectors won't be comparable to word-vectors from a prior 'model era'. (The same word, even if its tangible meaning hasn't changed, could be an arbitrarily different place - but the relative relationships with other vectors should remain as good or better.) But, that same sort of drift-out-of-comparison is <em>also</em> happening with any set of small-batch updates that don't 'touch' every existing word equally, just at some unquantifiable rate.</p>
<p>OTOH, if you think you need to stay with such incremental updates, even knowing the caveats, it's plausible that you could patch-up the model structures to retain as much as is sensible from the old model &amp; continue training.</p>
<p>Your code so far is a reasonable start, missing a few important considerations for proper functionality:</p>
<ul>
<li>because deleting earlier-words changes the index location of later-words, you'd need to update the <code>vocab[word].index</code> values for every surviving word, to match the new <code>index2word</code> ordering. For example, after doing all deletions, you might do:</li>
</ul>
<pre><code>for i, word in enumerate(m.wv.index2word):
    m.wv.vocab[word].index = i
</code></pre>
<ul>
<li><p>because in your (default negative-sampling) <code>Word2Vec</code> model, there is <em>also</em> another array of per-word weights related to the model's output layer, that should also be updated in sync, so that the right output-values are being checked per word. Roughly, wheneever you delete a row from <code>m.wv.vectors</code>, you should delete the same row from <code>m.traininables.syn1neg</code>.</p>
</li>
<li><p>because the surviving vocabulary has different relative word-frequencies, both the negative-sampling and downsampling (controlled by the <code>sample</code> parameter) functions should work off different pre-calculated structures to assist their choices. For the cumulative-distribution table used by negative-sampling, this is pretty easy:</p>
</li>
</ul>
<pre><code>m.make_cum_table(m.wv)
</code></pre>
<p>For the downsampling, you'd want to update the <code>.sample_int</code> values similar to the logic you can view around the code at <a href=""https://github.com/RaRe-Technologies/gensim/blob/3.8.3/gensim/models/word2vec.py#L1534"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/3.8.3/gensim/models/word2vec.py#L1534</a>. (But, looking at that code now, I think it may be <a href=""https://github.com/RaRe-Technologies/gensim/issues/2951"" rel=""nofollow noreferrer"">buggy</a> in that it's updating all words with just the frequency info in the new dict, so probably fouling the usual downsampling of truly-frequent words, and possibly erroneously downsampling words that are only frequent in the new update.)</p>
<p>If those internal structures are updated properly in sync with your existing actions, the model is probably in a consistent state for further training. (But note: these structures change a lot in the forthcoming <code>gensim-4.0.0</code> release, so any custom tampering will need to be updated when upgrading then.)</p>
<p>One other efficiency note: the <code>np.delete()</code> operation will create a new array, the full size of the surviving array, and copy the old values over, each time it is called. So using it to remove many rows, one at a time, from a very-large original array is likely to require a lot of redundant allocation/copying/garbage-collection. You may be able to call it once, at the end, with a list of all indexes to remove.</p>
<p>But really: the simpler &amp; better-grounded approach, which may also yield significantly better continually-comparable vectors, would be to retrain with all current data whenever possible or a large amount of change has occurred.</p>
",1,1,772,2020-09-16 08:43:17,https://stackoverflow.com/questions/63916338/what-is-the-best-way-to-drop-old-words-from-gensim-word2vec-model
Unable to install &#39;gensim&#39;,"<p>When I  try to install <code>gensim</code> through cmd prompt, it gives me following error:</p>
<blockquote>
<p>&quot;ERROR: Could not install packages due to an EnvironmentError:
[WinError 5] Access is denied:
'c:\programdata\anaconda3\lib\site-packages\<strong>pycache</strong>\cython.cpython-38.pyc'
Consider using the <code>--user</code> option or check the permissions. &quot;</p>
</blockquote>
<p>I'm unable to sort this issue, please help me out!</p>
","python-3.x, gensim","<p>Open windows command prompt with administrative permissions (Right-click on <code>cmd</code> and choose Run as administrator option).</p>
<hr />
<p><em><strong>or</strong></em></p>
<p>Try using the <code>--user</code> flag</p>
<p>Example :
<code>pip install gensim --user</code></p>
",0,0,609,2020-09-20 13:16:39,https://stackoverflow.com/questions/63979393/unable-to-install-gensim
Why are the signs of my topic weights changing from run to run?,"<p>I'm running the LSI program from Gensim's <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html"" rel=""nofollow noreferrer"">Topics and Transformations tutorial</a> and for some reason, the signs of the topic weights keep switching from positive to negative and vice versa. For example, this is what I get when I print using the line</p>
<pre><code>for doc, as_text in zip(corpus_lsi, documents):
    print(doc, as_text)

Run 1
[(0, 0.066007833960900791), (1, 0.52007033063618491), (2, -0.37649581219168904)]
[(0, 0.196675928591421), (1, 0.7609563167700063), (2, 0.5080674581001664)]
[(0, 0.089926399724459982), (1, 0.72418606267525132), (2, -0.408989731553764)]
[(0, 0.075858476521777865), (1, 0.63205515860034334), (2, -0.53935336057339001)]
[(0, 0.10150299184979866), (1, 0.57373084830029653), (2, 0.67093385852959075)]
[(0, 0.70321089393783254), (1, -0.1611518021402539), (2, -0.18266089635241448)]
[(0, 0.87747876731198449), (1, -0.16758906864658912), (2, -0.10880822642632856)]
[(0, 0.90986246868185872), (1, -0.14086553628718496), (2, 0.00087117874886860625)]
[(0, 0.61658253505692762), (1, 0.053929075663897361), (2, 0.25568697959599318)]

Run 2
[(0, 0.066007833960908563), (1, -0.52007033063618446), (2, -0.37649581219168959)]
[(0, 0.19667592859143226), (1, -0.76095631677000253), (2, 0.50806745810016629)]
[(0, 0.089926399724470751), (1, -0.72418606267525032), (2, -0.40898973155376284)]
[(0, 0.075858476521787177), (1, -0.63205515860034223), (2, -0.5393533605733889)]
[(0, 0.10150299184980684), (1, -0.57373084830029419), (2, 0.67093385852959098)]
[(0, 0.70321089393782976), (1, 0.16115180214026417), (2, -0.18266089635241456)]
[(0, 0.87747876731198149), (1, 0.16758906864660211), (2, -0.10880822642632891)]
[(0, 0.90986246868185627), (1, 0.14086553628719861), (2, 0.00087117874886795399)]
[(0, 0.61658253505692828), (1, -0.053929075663887563), (2, 0.25568697959599251)]

Run 3
[(0, 0.066007833960902929), (1, -0.52007033063618535), (2, 0.37649581219168821)]
[(0, 0.19667592859142491), (1, -0.76095631677000497), (2, -0.50806745810016662)]
[(0, 0.089926399724463771), (1, -0.7241860626752511), (2, 0.40898973155376317)]
[(0, 0.075858476521781085), (1, -0.63205515860034334), (2, 0.5393533605733889)]
[(0, 0.10150299184980124), (1, -0.57373084830029542), (2, -0.67093385852959064)]
[(0, 0.70321089393783143), (1, 0.16115180214025732), (2, 0.18266089635241564)]
[(0, 0.87747876731198304), (1, 0.16758906864659326), (2, 0.10880822642632952)]
[(0, 0.90986246868185761), (1, 0.1408655362871892), (2, -0.00087117874886778746)]
[(0, 0.61658253505692784), (1, -0.053929075663894419), (2, -0.25568697959599318)]
</code></pre>
<p>I am running Python 3.5.2 on a PC, coding in IntelliJ.</p>
<p>Anyone encountered this problem, using the Gensim library or elsewhere?</p>
","python, python-3.x, gensim, topic-modeling, latent-semantic-indexing","<p>LSI model is nothing but an implementation of fast truncated SVD underneath it. SVD calculates eigen vectors and these vectors correspond to the topics. However, eigenvectors remain eigenvectors even after multiplying by -1. So the sign might keep flipping based on the how the algorithm is implemented. In fact it is the case with the SVD implementation of the popular library LAPACK and even the numpy implementation.</p>
<p>The sign really does not matter here, as multiplication by -1 is also an eigen vector.</p>
",1,0,129,2020-09-22 22:43:07,https://stackoverflow.com/questions/64018628/why-are-the-signs-of-my-topic-weights-changing-from-run-to-run
How is the output of glove2word2vec() different from keyed_vectors.save(),"<p>I am new to NLP and I am running into this issue that I do not understand at all:</p>
<p>I have a text file with gloVe vectors.
I converted it to Word2Vec using</p>
<pre><code>glove2word2vec(TXT_FILE_PATH, KV_FILE_PATH)
</code></pre>
<p>this creates a KV file in my path which can then be loaded using</p>
<pre><code>word_vectors = KeyedVectors.load_word2vec_format(KV_FILE_PATH, binary=False)
</code></pre>
<p>I then save it using</p>
<pre><code>word_vectors.save(KV_FILE_PATH)
</code></pre>
<p>But when I now try to use the new KV file in intersect_word2vec_format it gives me an encoding error</p>
<pre><code>---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-11-d975bb14af37&gt; in &lt;module&gt;
      6 
      7 print(&quot;Intersect with pre-trained model...&quot;)
----&gt; 8 model.intersect_word2vec_format(KV_FILE_PATH, binary=False)
      9 
     10 print(&quot;Train custom word2vec model...&quot;)

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/word2vec.py in intersect_word2vec_format(self, fname, lockf, binary, encoding, unicode_errors)
    890         logger.info(&quot;loading projection weights from %s&quot;, fname)
    891         with utils.open(fname, 'rb') as fin:
--&gt; 892             header = utils.to_unicode(fin.readline(), encoding=encoding)
    893             vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    894             if not vector_size == self.wv.vector_size:

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/utils.py in any2unicode(text, encoding, errors)
    366     if isinstance(text, unicode):
    367         return text
--&gt; 368     return unicode(text, encoding, errors=errors)
    369 
    370 

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
","python, nlp, stanford-nlp, gensim, word2vec","<p>The <code>.save()</code> method saves a model in Gensim's native format - which is primarily Python pickling, with large arrays as separate files (which must be kept alongside the main save file).</p>
<p>That format is not the same as the <code>word2vec_format</code> that can be loaded by <code>load_word2vec_format()</code> or <code>intersect_word2vec_format()</code>.</p>
<p>If you want to save a set of vectors into the <code>word2vec_format</code>, use the method <code>.save_word2vec_format()</code>, not plain <code>.save()</code>.</p>
",1,0,290,2020-09-24 04:15:04,https://stackoverflow.com/questions/64039454/how-is-the-output-of-glove2word2vec-different-from-keyed-vectors-save
How to deal with large amount of sentences with gensim word2vec?,"<p>I have a very large amount of sentences, the problem is i cannot load them all at once in memory, specially when i tokenize the sentences and split them into list of words my RAM goes full really fast.</p>
<p>but i couldn't find any example of how can i train the gensim word2vec with batches, meaning in each epoch i guess i have to somehow load batches of data from disk, tokenize them and give it to the model then unload it and load the next batch.</p>
<p>how can i overcome this problem and train a word2vec model when i don't have enough ram to load all the sentences (not even 20% of them).</p>
<p>my sentences are basically in a text file, each line representing a sentence.</p>
","nlp, gensim, word2vec","<p>You can define your own corpus as suggested in <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#training-your-own-model"" rel=""nofollow noreferrer"">docs</a> and basically size of corpus doesn't matter in this case:</p>
<pre><code>from gensim.test.utils import datapath
from gensim import utils

class MyCorpus(object):
    &quot;&quot;&quot;An interator that yields sentences (lists of str).&quot;&quot;&quot;

    def __iter__(self):
        corpus_path = datapath('lee_background.cor')
        for line in open(corpus_path):
            # assume there's one document per line, tokens separated by whitespace
            yield utils.simple_preprocess(line)
</code></pre>
<p>Then train it as follow:</p>
<pre><code>import gensim.models

sentences = MyCorpus()
model = gensim.models.Word2Vec(sentences=sentences)
</code></pre>
",2,0,908,2020-09-24 07:14:27,https://stackoverflow.com/questions/64041275/how-to-deal-with-large-amount-of-sentences-with-gensim-word2vec
"Load fasttext quantized model (.ftz), and look up words","<p>I have a pretrained embeddings file, which was quantized, in .ftz format. I need it to look up words, find the nearest neighbours. But I fail to find any toolkits that can do that. FastText can load the embeddings file, yet not able to look up the nearest neighbour, Gensim can lookup the nearest neighbour, but not be able to load the model...</p>
<p>Or it's me not finding the right function?</p>
<p>Thank you!</p>
","python, gensim, word-embedding, fasttext","<p>FastText models come in two flavours:</p>
<ul>
<li><strong>unsupervised models</strong> that produce word embeddings and can find similar words. The native Facebook package does not support quantization for them.</li>
<li><strong>supervised models</strong> that are used for text classification and can be quantized natively, but generally do not produce meaningful word embeddings.</li>
</ul>
<p>To compress unsupervised models, I have created a package <a href=""https://github.com/avidale/compress-fasttext"" rel=""nofollow noreferrer"">compress-fasttext</a> which is a wrapper around Gensim that can reduce the size of unsupervised models by pruning and quantization. <a href=""https://towardsdatascience.com/compressing-unsupervised-fasttext-models-eb212e9919ca"" rel=""nofollow noreferrer"">This post</a> describes it in more details.</p>
<p>With this package, you can lookup similar words in small models as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import compress_fasttext
small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load(
    'https://github.com/avidale/compress-fasttext/releases/download/v0.0.4/cc.en.300.compressed.bin'
)
print(small_model.most_similar('Python'))
# [('PHP', 0.5253), ('.NET', 0.5027), ('Java', 0.4897),  ... ]
</code></pre>
<p>Of course, it works only if the model has been compressed using the same package. You can compress your own unsupervised model this way:</p>
<pre><code>import compress_fasttext
from gensim.models.fasttext import load_facebook_model
big_model = load_facebook_model('path-to-original-model').wv
small_model = compress_fasttext.prune_ft_freq(big_model, pq=True)
small_model.save('path-to-new-model')
</code></pre>
",0,1,976,2020-09-25 15:39:22,https://stackoverflow.com/questions/64067272/load-fasttext-quantized-model-ftz-and-look-up-words
Can doc2vec work on an artificial &quot;text&quot;?,"<p>I've created an artificial corpus (with 52624 documents). Each document is a list of objects (there are 461 of them).</p>
<p>So one possibility could be: <code>['chair', 'chair', 'chair', 'chair', 'chair', 'table', 'table']</code></p>
<p>Here's a bar plot (log-scale) of the vocabulary.</p>
<p><a href=""https://i.sstatic.net/B1CP3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B1CP3.png"" alt=""enter image description here"" /></a></p>
<p>And this is how I defined the model:</p>
<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=8, workers=4, min_count=1,epochs=40, dm=0)
</code></pre>
<p>Observing at:
<code>model.wv.most_similar_cosmul(positive = [&quot;chair&quot;])</code></p>
<p>I see non related words</p>
<p>And it seems to me that the following works poorly as well:</p>
<pre><code>inferred_vector = model.infer_vector([&quot;chair&quot;])
model.docvecs.most_similar([inferred_vector])
</code></pre>
<p>Where has my model failed?</p>
<p><strong>UPDATE</strong></p>
<p>There is the data (JSON file):</p>
<p><a href=""https://gofile.io/d/bZDcPX"" rel=""nofollow noreferrer"">https://gofile.io/d/bZDcPX</a></p>
","machine-learning, nlp, gensim, word2vec, doc2vec","<p>Yes, <code>Doc2Vec</code> &amp; <code>Word2Vec</code> are often tried, and useful, on synthetic data. But whether they work may require a lot more tinkering, and atypical parameters, when the data doesn't reflect the same sort of correlations/distributions as the natural-language on which these algorithms were 1st developed.</p>
<p>First and foremost with your setup, you're using the <code>dm=0</code> mode. That's the <code>PV-DBOW</code> mode of the original &quot;Paragraph Vector&quot; paper, which specifically <strong>does not</strong> train word-vectors <strong>at all</strong>, only the doc-vectors. So if you're testing such a model by looking at word-vectors, your results will only reflect the random, untrained initialization values of any word-vectors.</p>
<p>Check the <code>model.docvecs</code> instead, for similarities between any doc-tags you specified in your data, and their may be more useful relationships.</p>
<p>(If you want your <code>Doc2Vec</code> model to learn words too – which isn't necessarily important, especially with a small dataset or where the doc-vectors are your main interest – you'd have to use either <code>dm=1</code> mode, or add <code>dbow_words=1</code> to <code>dm=0</code> so that the model adds interleaved skip-gram training. But note word-vector training may be weak/meaningless/harmful with data that's looks like it's just sorted runs of repeating tokens, as with your <code>['chair', 'chair', 'chair', 'chair', 'chair', 'table', 'table']</code> example item.)</p>
<p>Separately, using a very-low <code>min_count=1</code> is often a bad idea in such models - as such tokens with arbitrary idiosyncractic non-representative appearances do more damage to the coherence of surrounding more-common tokens than they help.</p>
",2,1,56,2020-09-28 12:15:48,https://stackoverflow.com/questions/64102023/can-doc2vec-work-on-an-artificial-text
MemoryError in pd.DataFrame(),"<p>I have a df with 2 columns and 5 million rows, all text (customer reviews of businesses).
<code>df.head()</code> produces:
<a href=""https://i.sstatic.net/JXtXR.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JXtXR.jpg"" alt=""enter image description here"" /></a></p>
<p><code>df.info()</code> shows that memory usage is only 120.3+ MB</p>
<p>I am trying to do topic modelling of <code>df['text']</code> using the gensim library. I attempt to create a document-term matrix (dtm) first and then perform latent Dirichlet allocation (LDA) as follow:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from gensim import matutils, models
import scipy.sparse

cv = CountVectorizer(stop_words='english')
data_cv = cv.fit_transform(df.text)
data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names()) #LINE THROWING MemoryError

data_dtm.index = df.index

tdm = data_dtm.transpose()

sparse_counts = scipy.sparse.csr_matrix(tdm)
corpus = matutils.Sparse2Corpus(sparse_counts)

id2word = dict((v, k) for k, v in cv.vocabulary_.items())
lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)
lda.print_topics()
</code></pre>
<p><strong>Issue</strong>: But the 7th line (pd.DataFrame()) throws the MemoryError  while I still have 60% of the machine memory free. Even when I repeat the operation on the first 100,000 rows of df, I get the same MemoryError.</p>
<p>Since this is topic modeling, I would rather analyze all the rows together, or at least analyze them in a few batches.</p>
<p><strong>Question</strong> What is making Python run out of memory when converting <code>data_cv</code> to dataframe? How can I get past it?</p>
","pandas, gensim, lda, topic-modeling","<p>It's quite possibly the <code>data_cv.toarray()</code> that's most-responsible for the memory expansion, by turning an efficient sparse representation into a full/dense array.</p>
<p>Try doing that step on a separate preceding line into a temporary variable, to check.</p>
<p>But if your end-goal is doing a Gensim LDA analysis, those steps  work well with tokenized text as input, so other actions (involving <code>CountVectorizer</code> and storing interim results or giant term-document arrays in Pandas data structures) may be superfluous.</p>
<p>For example (ignoring any stopword filtering):</p>
<pre><code>from gensim.corpora.dictionary import Dictionary
from gensim.models import LdaModel

tokenized_texts = [text_string.split() for text_string in df.text]
texts_dictionary = Dictionary(tokenized_texts)
texts_bows = [texts_dictionary.doc2bow(text_tokens) for text_tokens in tokenized_texts]

lda = LdaModel(corpus=texts_bows, id2word=texts_dictionary, num_topics=2)
</code></pre>
<p><strong>This still creates two giant copies of the <code>df.text</code> column in memory</strong>, a list of <code>tokenized_texts</code> and a list of <code>texts_bows</code>, so it uses more memory than optimal. (The best practice for very-large corpora is to leave them on disk, and stream them item-by-item into the processing steps.)</p>
<p>But, it may use less than the giant dense <code>.toarray()</code> step, and never even creates not-strictly-necessary <code>CountVectorizer</code> or interim array and <code>DataFrame</code> objects.</p>
",0,0,270,2020-09-29 17:54:57,https://stackoverflow.com/questions/64125039/memoryerror-in-pd-dataframe
Extract token frequencies from gensim model,"<p>Questions like <a href=""https://stackoverflow.com/questions/37190989/how-to-get-vocabulary-word-count-from-gensim-word2vec"">1</a> and <a href=""https://stackoverflow.com/questions/55657062/how-can-i-count-word-frequencies-in-word2vecs-training-model"">2</a> give answers for retrieving vocabulary frequencies from gensim word2vec models.</p>
<p>For some reason, they actually just give a deprecating counter from n (size of vocab) to 0, alongside the most frequent tokens, ordered.</p>
<p>For example:</p>
<pre><code>for idx, w in enumerate(model.vocab):
    print(idx, w, model.vocab[w].count)
</code></pre>
<p>Gives:</p>
<pre><code>0 &lt;/s&gt; 111051
1 . 111050
2 , 111049
3 the 111048
4 of 111047
...
111050 tokiwa 2
111051 muzorewa 1
</code></pre>
<p>Why is it doing this? How can I extract term frequencies from the model, given a word?</p>
","python, gensim","<p>Those answers are correct for reading the declared token-counts out of a model which has them.</p>
<p>But in some cases, your model may only have been initialized with a fake, descending-by-1 count for each word. This is most likely, in using Gensim, if it was loaded from a source where either the counts weren't available, or weren't used.</p>
<p>In particular, if you created the model using <code>load_word2vec_format()</code>, that simple vectors-only format (whether <code>binary</code> or plain-text) inherently contains no word counts. But such words are almost always, by convention, sorted in most-frequent to least-frequent order.</p>
<p>So, Gensim has chosen, when frequencies are not present, to synthesize fake counts, with linearly descending int values, where the (first) most-frequent word begins with the count of all unique words, and the (last) least-frequent word has a count of 1.</p>
<p>(I'm not sure this is a good idea, but Gensim's been doing it for a while, and it ensures code relying on the per-token <code>count</code> won't break, and will preserve the original order, though obviously not the unknowable original true-proportions.)</p>
<p>In some cases, the original source of the file <em>may</em> have saved a separate <code>.vocab</code> file with the word-frequencies alongside the <code>word2vec_format</code> vectors. (In Google's original <code>word2vec.c</code> code release, this is the file generated by the optional <code>-save-vocab</code> flag. In Gensim's <code>.save_word2vec_format()</code> method, the optional <code>fvocab</code> parameter can be used to generate this side file.)</p>
<p>If so, that 'vocab' frequencies filename may be supplied, when you call <code>.load_word2vec_format()</code>, as the <code>fvocab</code> parameter - and then your vector-set will have true counts.</p>
<p>If you word-vectors were originally created in Gensim from a corpus giving actual frequencies, and were always saved/loaded using the Gensim native functions <code>.save()</code>/<code>.load()</code> which use an extended form of Python-pickling, then the original true <code>count</code> info will never have been lost.</p>
<p>If you've lost the original frequency data, but you know the data was from a real natural-language source, and you want a more realistic (but still faked) set of frequencies, an option could be to use the Zipfian distribution. (Real natural-language usage frequencies tend to roughly fit this 'tall head, long tail' distribution.) A formula for creating such more-realistic dummy counts is available in the answer:</p>
<p><a href=""https://stackoverflow.com/questions/58735585/gensim-any-chance-to-get-word-frequency-in-word2vec-format/58737377#58737377"">Gensim: Any chance to get word frequency in Word2Vec format?</a></p>
",1,0,672,2020-10-01 08:52:41,https://stackoverflow.com/questions/64151977/extract-token-frequencies-from-gensim-model
Doc2Vec most similar vectors don&#39;t match an input vector,"<p>I've got a dataset of job postings with about 40 000 records. I extracted skills from descriptions using NER with about 30 000 skills in the dictionary. Every skill is represented as an unique identificator.</p>
<p>The distribution of skills number for a posting looks like that:</p>
<p>mean        15.12 |
std         11.22 |
min          1.00 |
25%          7.00 |
50%         13.00 |
75%         20.00 |</p>
<p>I've trained a word2vec model using only skill ids and it works more or less fine. I can find most similar skills to a given one and the result looks okay.</p>
<p>But when it comes to a doc2vec model I'm not satisfied with the result.</p>
<p>I have about 3200 unique job titles, most of them have only few entries and there are quite a few of them being from the same field ('front end developer', 'senior javascript developer', 'front end engineer'). I delibirately went for a variety of job titles which I use as tags in doc2vec.TaggedDocument(). My goal is to see a number of relevant job titles when I input a vector of skills into docvecs.most_similar().</p>
<p>After training a model (I've tried different number of epochs (100,500,1000) and vector sizes (40 and 100)) sometimes it works correctly, but most of the time it doens't. For example for a skills set like [numpy, postgresql, pandas, xgboost, python, pytorch] I get the most similar job title with a skill set like [family court, acting, advising, social work].</p>
<p>Can it be a problem with the size of my dataset? Or the size of docs (I consider that I have short texts)? I also think that I misunderstand something about doc2vec mechanism and just ignore it. I'd also like to ask if you know any other, maybe more advanced, ideas how I can get relevant job titles from a skill set and compare two skill set vectors if they are close or far.</p>
<p>UPD:</p>
<p>Job titles from my data are 'tags' and skills are 'words'. Each text has a single tag. There are 40 000 documents with 3200 repeating tags. 7881 unique skill ids appear in the documents. The average number of skill words per document is 15.</p>
<p>My data example:</p>
<pre><code>         job_titles                                             skills
1  business manager                 12 13 873 4811 482 2384 48 293 48
2    java developer      48 2838 291 37 484 192 92 485 17 23 299 23...
3    data scientist      383 48 587 475 2394 5716 293 585 1923 494 3
</code></pre>
<p>The example of my code:</p>
<pre><code>def tagged_document(df):
    #tagging documents
    for index, row in df.iterrows():
        yield gensim.models.doc2vec.TaggedDocument(row['skills'].split(), [row['job_title']])


data_for_training = list(tagged_document(job_data[['job_titles', 'skills']])

model_d2v = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)

model_d2v.train(data_for_training, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)

#the skill set contains close skills which represent a front end developer
skillset_ids = '12 34 556 453 1934'.split()                                                  
new_vector = model_d2v.infer_vector(skillset_ids, epochs=100)
model_d2v.docvecs.most_similar(positive=[new_vector], topn=30)
</code></pre>
<p>I've been experimenting recently and noticed that it performs a little better if I filter out documents with less than 10 skills. Still, there are some irrelevant job titles coming out.</p>
","python, nlp, gensim, word2vec, doc2vec","<p>Without seeing your code (or at least a sketch of its major choices), it's hard to tell if you might be making shooting-self-in-foot mistakes, like perhaps the common &quot;managing <code>alpha</code> myself by following crummy online examples&quot; issue: <a href=""https://stackoverflow.com/questions/62801052/my-doc2vec-code-after-many-loops-of-training-isnt-giving-good-results-what-m"">My Doc2Vec code, after many loops of training, isn&#39;t giving good results. What might be wrong?</a></p>
<p>(That your smallest number of tested <code>epochs</code> is 100 seems suspicious; 10-20 epochs are common values in published work, when both the size of the dataset and size of each doc are plentiful, though more passes can sometimes help with thinner data.)</p>
<p>Similarly, it's not completely clear from your description what your training docs are like. For example:</p>
<ul>
<li>Are the <code>tags</code> titles and the <code>words</code> skills?</li>
<li>Does each text have a single <code>tag</code>?</li>
<li>If there are 3,200 unique <code>tags</code> and 30,000 unique <code>words</code>, is that just 3,200 <code>TaggedDocuments</code>, or more with repeating titles?</li>
<li>What's the average number of skill-words per <code>TaggedDocument</code>?</li>
</ul>
<p>Also, if you are using word-vectors (for skills) as query vectors, you have to be sure to use a training mode that actually trains those. Some <code>Doc2Vec</code> modes, such as plain PV-DBOW (<code>dm=0</code>) don't train word-vectors at all, but they will exist as randomly-initialized junk. (Either adding non-default <code>dbow_words=1</code> to add skip-gram word-training, or switching to PV-DM <code>dm=1</code> mode, will ensure word-vectors are co-trained and in a comparable coordinate space.)</p>
",1,1,524,2020-10-02 15:15:43,https://stackoverflow.com/questions/64174071/doc2vec-most-similar-vectors-dont-match-an-input-vector
Is there a pretrained Gensim phrase model?,"<p>Is there a pretrained <code>Gensim</code>'s <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">Phrases</a> model? If not, would it be possible to reverse engineer and create a phrase model using a pretrained word embedding?</p>
<p>I am trying to use <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">GoogleNews-vectors-negative300.bin</a> with Gensim's <code>Word2Vec</code>. First, I need to map my words into phrases so that I can look up their vectors from the Google's pretrained embedding.</p>
<p>I search on the official Gensim's documentation but could not find any info. Thanks!</p>
","python, machine-learning, gensim, word-embedding, phrase","<p>I'm not aware of anyone sharing a <code>Phrases</code> model. Any such model would be very sensitive to the preprocessing/tokenization step, and the specific parameters, the creator used.</p>
<p>Other than the high-level algorithm description, I haven't seen Google's exact choices for tokenization/canonicalization/phrase-combination done to the data that fed into the <code>GoogleNews</code> 2013 word-vectors have been documented anywhere. Some guesses about preprocessing can be made by reviewing the tokens present, but I'm unaware of any code to apply similar choices to other text.</p>
<p>You could try to mimic their unigram tokenization, then speculatively combine strings of unigrams into ever-longer multigrams up to some maximum, check if those combinations are present, and when not present, revert to the unigrams (or largest combination present). This might be expensive if done naively, but be amenable to optimizations if really important - especially for some subset of the more-frequent words – as the <code>GoogleNews</code> set appears to obey the convention of listing words in descending frequency.</p>
<p>(In general, though it's a quick &amp; easy starting set of word-vectors, I think <code>GoogleNews</code> is a bit over-relied upon. It will lack words/phrases and new senses that have developed since 2013, and any meanings it does capture are determined by news articles in the years leading up to 2013... which may not match the dominant senses of words in other domains. If your domain isn't specifically news, and you have sufficient data, deciding your own domain-specific tokenization/combination will likely perform better.)</p>
",1,1,407,2020-10-15 18:43:57,https://stackoverflow.com/questions/64377890/is-there-a-pretrained-gensim-phrase-model
Syntax for return after function,"<p>I try to reproduce this lines of code from gensim</p>
<pre><code>import gensim
def coherence_values_computation(dictionary, corpus, texts, limit, start=2, step=3):
   coherence_values = []
   model_list = []
   for num_topics in range(start, limit, step):
      model = gensim.models.wrappers.LdaMallet(
         mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word
      )
      model_list.append(model)
   coherencemodel = CoherenceModel(
      model=model, texts=texts, dictionary=dictionary, coherence='c_v'
   )
   coherence_values.append(coherencemodel.get_coherence())
return model_list, coherence_values
</code></pre>
<p>However in the return function I receive this error:</p>
<blockquote>
<pre><code>File &quot;&lt;ipython-input-10-65490721eef3&gt;&quot;, line 13
    return model_list, coherence_values)
    ^
SyntaxError: invalid syntax
</code></pre>
</blockquote>
<p>Any idea with this happens?</p>
","python, python-3.x, gensim","<p><code>return</code> should be inside the function. Indentation should be:</p>
<pre><code>def coherence_values_computation(dictionary, corpus, texts, limit, start=2, step=3):
   coherence_values = []
   model_list = []
   for num_topics in range(start, limit, step):
      model = gensim.models.wrappers.LdaMallet(
         mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word
      )
      model_list.append(model)
   coherencemodel = CoherenceModel(
      model=model, texts=texts, dictionary=dictionary, coherence='c_v'
   )
   coherence_values.append(coherencemodel.get_coherence())
   return model_list, coherence_values
</code></pre>
",2,0,63,2020-10-25 12:34:04,https://stackoverflow.com/questions/64523731/syntax-for-return-after-function
UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0x80 in position 0: invalid start byte while reading a text file,"<p>I am training a word2vec model, using about 700 text files as my corpus. But, when I start reading the files after the preprocessing step, I get the mentioned error. The code is as follows</p>
<pre><code>class MyCorpus(object):
    def __iter__(self):
        for i in ceo_path:                              /// ceo_path contains abs path of all text files
            file = open(i, 'r', encoding='utf-8')
            text = file.read()

            ###########                                        
            ###########                                 /// text preprocessing steps
            ###########
            
            yield final_text                            /// returns preprocessed text


sentences = MyCorpus()
logging.basicConfig(format=&quot;%(levelname)s - %(asctime)s: %(message)s&quot;, datefmt= '%H:%M:%S', level=logging.INFO)

# training the model
cores = multiprocessing.cpu_count()
w2v_model = Word2Vec(min_count=5,
                     iter=30,
                     window=3,
                     size=200,
                     sample=6e-5,
                     alpha=0.025,
                     min_alpha=0.0001,
                     negative=20,
                     workers=cores-1,
                     sg=1)
w2v_model.build_vocab(sentences)
w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)
w2v_model.save('ceo1.model')
</code></pre>
<p>The error that I am getting is:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:/Users/name/PycharmProjects/prac2/hbs_word2vec.py&quot;, line 131, in &lt;module&gt;
    w2v_model.build_vocab(sentences)
  File &quot;C:\Users\name\PycharmProjects\prac1\venv\lib\site-packages\gensim\models\base_any2vec.py&quot;, line 921, in build_vocab
    total_words, corpus_count = self.vocabulary.scan_vocab(
  File &quot;C:\Users\name\PycharmProjects\prac1\venv\lib\site-packages\gensim\models\word2vec.py&quot;, line 1403, in scan_vocab
    total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)
  File &quot;C:\Users\name\PycharmProjects\prac1\venv\lib\site-packages\gensim\models\word2vec.py&quot;, line 1372, in _scan_vocab
    for sentence_no, sentence in enumerate(sentences):
  File &quot;C:/Users/name/PycharmProjects/prac2/hbs_word2vec.py&quot;, line 65, in __iter__
    text = file.read()
  File &quot;C:\Users\name\AppData\Local\Programs\Python\Python38-32\lib\codecs.py&quot;, line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
<p>I am not able to understand the error as I am new to this. I was not getting the error in reading the text files when I wasn't using the <strong>iter</strong> function and sending the data in chunks as I am doing currently.</p>
","python-3.x, gensim, word2vec","<p>It looks like one of your files doesn't have proper <code>utf-8</code>-encoded text.</p>
<p>(Your <code>Word2Vec</code>-related code probably isn't necessary for hitting the error, at all. You could probably trigger the same error with just: <code>sentences_list = list(MyCorpus())</code>.)</p>
<p>To find which file, two different possibilities might be:</p>
<ol>
<li>Change your <code>MyCorpus</code> class so that it prints the path of each file before it tries to read it.</li>
<li>Add a Python <code>try: ... except UnicodeDecodeError: ...</code> statement around the <code>read</code>, and when the exception is caught, print the offending filename.</li>
</ol>
<p>Once you know the file involved, you may want to fix the file, or change the code to be able to handle the files you have.</p>
<p>Maybe they're not really in <code>utf-8</code> encoding, in which case you'd specify a different <code>encoding</code>.</p>
<p>Maybe just one or a few have problems, and it's be OK to just print their names for later investigation, and skip them. (You could use the exception-handling approach above to do that.)</p>
<p>Maybe, those that aren't <code>utf-8</code> are always in some other platform-specific encoding, so when <code>utf-8</code> fails, you could try a 2nd encoding.</p>
<p>Separately, when you solve the encoding issue, your iterable <code>MyCorpus</code> is not yet returning whet the <code>Word2Vec</code> class expects.</p>
<p>It doesn't want full text plain strings. It needs those texts to already be broken up into individual word-tokens.</p>
<p>(Often, simply performing a <code>.split()</code> on a string is close-enough-to-real-tokenization to try as a starting point, but usually, projects use some more-sophisticated punctuation-aware tokenization.)</p>
",0,1,1177,2020-10-26 15:48:02,https://stackoverflow.com/questions/64540488/unicodedecodeerror-utf-8-codec-cant-decode-byte-0x80-in-position-0-invalid
Text similarity using WMD within the same time period,"<p>I have a dataset</p>
<pre><code>       Title                                                Year
0   Sport, there will be a match between United and Tottenham ...   2020
1   Forecasting says that it will be cold next week                 2019
2   Sport, Mourinho is approaching the anniversary at Tottenham     2020
3   Sport, Tottenham are sixth favourites for the title behind Arsenal. 2020
4   Pochettino says clear-out of fringe players at Tottenham is inevitable.     2018
... ... ...
</code></pre>
<p>I would like to study the text similarity within the same year, rather in the whole dataset. To find most similar texts, I am using the WM distance similarity.
For two text would be:</p>
<pre><code>word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
distance = word2vec_model.wmdistance(&quot;string 1&quot;.split(), &quot;string 2&quot;.split())
</code></pre>
<p>However I would need to iterate the distance through sentences in the same year to get the similarity of each text with others, creating a list of similar text per row in the dataframe.
Could you please tell me how to iterate the wmdistance function across text published in the same year, in order to get for each text the most similar ones within the same period?</p>
","python, pandas, gensim, word2vec, similarity","<p>Generating a distance matrix for every group and then picking min value should work. This will get you a single nearest document index in a given year. You should be able to modify this code if you want n documents, or something else like that, quite easily.</p>
<pre><code>from scipy.spatial.distance import pdist, squareform

def nearest_doc(group):
    sq = squareform(pdist(group.to_numpy()[:,None], metric=lambda x, y:word2vec_model.wmdistance(x[0], y[0])))

    return group.index.to_numpy()[np.argmin(np.where(sq==0, np.inf, sq), axis=1)]

df['nearest_doc'] = df.groupby('Year')['Title'].transform(nearest_doc)
</code></pre>
<p>result:</p>
<pre><code>Title   Year    nearest_doc
0   Sport, there will be a match between United an...   2020    3
1   Forecasting says that it will be cold next week     2019    1
2   Sport, Mourinho is approaching the anniversary...   2020    3
3   Sport, Tottenham are sixth favourites for the ...   2020    2
4   Pochettino says clear-out of fringe players at...   2018    4
</code></pre>
",0,1,86,2020-11-01 02:25:08,https://stackoverflow.com/questions/64628163/text-similarity-using-wmd-within-the-same-time-period
Extract top N words that are most similar to an input word from a text file,"<p>I have a text file that contains the content of a web page that I have extracted using BeautifulSoup. I need to find N similar words from the text file based on a given word. The process is as follows:</p>
<ol>
<li>The website from which text was extracted: <a href=""https://en.wikipedia.org/wiki/Football"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Football</a></li>
<li>The extracted text is saved to a text file.</li>
<li>The User inputs a word, ex: &quot;goal&quot; and I have to display the top N most similar words from the text file.</li>
</ol>
<p>I have only worked in Computer Vision and completely new to NLP. I'm currently stuck in step 3. I have tried Spacy and Gensim, but my approach is not at all efficient. I currently do this:</p>
<pre class=""lang-py prettyprint-override""><code>for word in ['goal', 'soccer']:
    # 1. compute similarity using spacy for each word in the text file with the given word.
    # 2. sort them based on the scores and choose the top N-words.
</code></pre>
<p>Is there any other approach or a simple solution to solve this problem? Any help is appreciated. Thanks!</p>
","python, deep-learning, nlp, spacy, gensim","<p>You can make use of spacy <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer""><code>similarity</code></a> method, that will calculate cosine similarity between tokens for you. In order to use vectors, load a model with vectors:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_md&quot;)

text = &quot;I have a text file that contains the content of a web page that I have extracted using BeautifulSoup. I need to find N similar words from the text file based on a given word. The process is as follows&quot;
doc = nlp(text)
words = ['goal', 'soccer']

# compute similarity    
similarities = {}   
for word in words:
    tok = nlp(word)
    similarities[tok.text] ={}
    for tok_ in doc:
        similarities[tok.text].update({tok_.text:tok.similarity(tok_)})

# sort
top10 = lambda x: {k: v for k, v in sorted(similarities[x].items(), key=lambda item: item[1], reverse=True)[:10]}

# desired output
top10(&quot;goal&quot;)
{'need': 0.41729581641359625,
 'that': 0.4156277030017712,
 'to': 0.40102258054859163,
 'is': 0.3742535591719576,
 'the': 0.3735002888862756,
 'The': 0.3735002888862756,
 'given': 0.3595024941701789,
 'process': 0.35218102758578645,
 'have': 0.34597281472837316,
 'as': 0.34433650293640194}
</code></pre>
<p>Note, (1) if you're comfortable with <code>gensim</code>, and/or (2) have a <code>word2vec</code> model trained on your text, you can do directly:</p>
<pre class=""lang-py prettyprint-override""><code>word2Vec.most_similar(positive=['goal'], topn=10)
</code></pre>
",4,3,2564,2020-11-01 09:01:06,https://stackoverflow.com/questions/64630194/extract-top-n-words-that-are-most-similar-to-an-input-word-from-a-text-file
Capture bigram topics instead of unigrams using latent dirichlet allocat,"<p>I try to make an attempt like <a href=""https://stackoverflow.com/questions/32476336/how-to-abstract-bigram-topics-instead-of-unigrams-using-latent-dirichlet-allocat"">this</a> question</p>
<p>LDA Original Output</p>
<pre><code>Uni-grams

    topic1 -scuba,water,vapor,diving

    topic2 -dioxide,plants,green,carbon
</code></pre>
<p>Required Output</p>
<pre><code>Bi-gram topics

    topic1 -scuba diving,water vapor

    topic2 -green plants,carbon dioxide
</code></pre>
<p>And there is this answer</p>
<pre><code>from nltk.util import ngrams

for doc in docs:
    docs[doc] = docs[doc] + [&quot;_&quot;.join(w) for w in ngrams(docs[doc], 2)]
</code></pre>
<p>Any help what update should I make in order to have only bigrams?</p>
","python, nltk, gensim, n-gram","<p>Create only documents with bigrams:</p>
<pre><code>from nltk.util import ngrams

for doc in docs:
    docs[doc] = [&quot;_&quot;.join(w) for w in ngrams(docs[doc], 2)]
</code></pre>
<p>Or specific method for bigrams:</p>
<pre><code>from nltk.util import bigrams

for doc in docs:
    docs[doc] = [&quot;_&quot;.join(w) for w in bigrams(docs[doc])]
</code></pre>
<p>Then use lists of these bigrams in <code>texts</code> for future operations.</p>
",2,1,395,2020-11-02 19:19:35,https://stackoverflow.com/questions/64651943/capture-bigram-topics-instead-of-unigrams-using-latent-dirichlet-allocat
How to define the optimal number of topics (k)?,"<p>I want to know that is the best topic number (k) to feed to gensim for LDA, I've found an answer on StackOverflow. However, I got an error mentioned below.</p>
<p>Here is the link to the suggested way to feed the number of the optimal topics that I've found.</p>
<p><a href=""https://stackoverflow.com/questions/32313062/what-is-the-best-way-to-obtain-the-optimal-number-of-topics-for-a-lda-model-usin"">What is the best way to obtain the optimal number of topics for a LDA-Model using Gensim?</a></p>
<pre><code># import modules 

import seaborn as sns
import matplotlib.pyplot as plt
from gensim.models import LdaModel, CoherenceModel
from gensim import corpora

# make models with n k

dirichlet_dict = corpora.Dictionary(corpus)
bow_corpus = [dirichlet_dict.doc2bow(text) for text in corpus]

# Considering 1-15 topics, as the last is cut off
num_topics = list(range(16)[1:])
num_keywords = 15

LDA_models = {}
LDA_topics = {}
for i in num_topics:
    LDA_models[i] = LdaModel(corpus=bow_corpus,
                             id2word=dirichlet_dict,
                             num_topics=i,
                             update_every=1,
                             chunksize=len(bow_corpus),
                             passes=20,
                             alpha='auto',
                             random_state=42)

    shown_topics = LDA_models[i].show_topics(num_topics=num_topics, 
                                             num_words=num_keywords,
                                             formatted=False)
    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]
</code></pre>
<p>When I try to implent the code i got this error:</p>
<pre><code>-&gt; 1145         if num_topics &lt; 0 or num_topics &gt;= self.num_topics:
   1146             num_topics = self.num_topics
   1147             chosen_topics = range(num_topics)

TypeError: '&lt;' not supported between instances of 'list' and 'int'
</code></pre>
","python, python-3.x, gensim","<p>This line:</p>
<pre><code>shown_topics = LDA_models[i].show_topics(num_topics=num_topics
</code></pre>
<p>should be:</p>
<pre><code>shown_topics = LDA_models[i].show_topics(num_topics=i
</code></pre>
<p>Arguably, this happened because of a bad variable naming. It could be avoided by replacing <code>num_topics = list(range(16)[1:])</code> and the subsequent loop with:</p>
<pre><code>max_topics = 15
for num_topics in range(1, max_topics+1):
    # use num_topics instead of i in the loop
</code></pre>
<p>This would eliminate the possible confusion</p>
",2,1,452,2020-11-08 17:14:15,https://stackoverflow.com/questions/64740901/how-to-define-the-optimal-number-of-topics-k
How can i optimize my Embedding transformation on a huge dataset?,"<p>I use FastText from the <code>gensim</code> package, and I use the code below to transform my text into a dense a representation but it takes many times when I have a huge dataset.
Could you help me to accelerate it?</p>
<pre class=""lang-py prettyprint-override""><code>def word2vec_features(self, templates, model):
    if self.method == 'mean':
        feats = np.vstack([sum_vectors(p, model) / len(p) for p in templates])
    else:
        feats = np.vstack([sum_vectors(p, model) for p in templates])
    return feats

def get_vect(word, model):
    try:
        return model.wv[word]
    except KeyError:
        return np.zeros((model.size,))


def sum_vectors(phrase, model):
    return sum(get_vect(w, model) for w in phrase)
</code></pre>
","python, python-3.x, numpy, gensim, fasttext","<p>Note that this sort of summary-vector for a text – the average (or sum) of all its word-vectors – is fairly crude. It can work OK as a baseline in some contexts – such fuzzy info-retrieval among short texts, or as a classifier input.</p>
<p>In some cases, if the <code>KeyError</code> is hit often, that exception-handling can be expensive - and it may make sense to instead check for whether a key is <code>in</code> the collection. But also, you may not want to be using an origin-vector (all zeros) for any missing word - it likely offers no benefit over just skipping those words.</p>
<p>So you might get some speedup by changing your code to ignore missing words, rather than adding an all-zeros vector in an exception handlers.</p>
<p>But also: if you're truly using a <code>FastText</code> model (rather than say <code>Word2Vec</code>), it will <em>never</em> <code>KeyError</code> for an unknown word, because it will always synthesize a vector out of the character n-grams (word fragments) it learned during training. You should probably just drop your <code>get_vect()</code> function entirely - relying just on normal <code>[]</code>-access.</p>
<p>Further, Gensim's <code>KeyedVector</code> models already support returning multiple results when indexed by a list of multiple keys. And, the <code>numpy</code> <code>np.sum()</code> might work a slight bit faster on these arrays than the pure-Python <code>sum()</code>. So you might get a small speedup if you replace your <code>sum_vectors()</code> with:</p>
<pre class=""lang-py prettyprint-override""><code>def sum_vectors(phrase, model):
    return np.sum(model.wv[phrase], axis=0)
</code></pre>
<p>To optimize further, you might need to profile the code in a heavy-usage loop, or even reconsider whether this is the form of text-vectorization you want to pursue. (Though, better methods typically require more calculation than this simple sum/average.)</p>
",2,1,334,2020-11-13 08:35:56,https://stackoverflow.com/questions/64817706/how-can-i-optimize-my-embedding-transformation-on-a-huge-dataset
Understanding the role of the function build_vocab in Doc2Vec,"<p>I have recently started studying Doc2Vec model.
I have understood its mechanism and how it works.
I'm trying to implement it using gensim framework.
I have transormed my training data into TaggedDocument.
But i have one question :
What is the role of this line <code>model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])</code> ?
is it to create random vectors that represent text ?
Thank you for your help</p>
","nlp, data-science, gensim, text-classification, doc2vec","<p>The <code>Doc2Vec</code> model needs to know several things about the training corpus before it is fully allocated &amp; initialized.</p>
<p>First &amp; foremost, the model needs to know the words present &amp; their frequencies – a working vocabulary – so that it can determine the words that will remain after the <code>min_count</code> floor is applied, and allocate/initialize word-vectors &amp; internal model structures for the relevant words. The word-frequencies will also be used to influence the random sampling of negative-word-examples (for the default negative-sampling mode) and the downsampling of very-frequent words (per the <code>sample</code> parameter).</p>
<p>Additionally, the model needs to know the rough size of the overall training set in order to gradually decrement the internal <code>alpha</code> learning-rate over the course of each epoch, and give meaningful progress-estimates in logging output.</p>
<p>At the end of <code>build_vocab()</code>, all memory/objects needed for the model have been created. Per the needs of the underlying algorithm, all vectors will have been initialized to low-magnitude random vectors to ready the model for training. (It essentially won't use any more memory, internally, through training.)</p>
<p>Also, after <code>build_vocab()</code>, the vocabulary is frozen: any words presented during training (or later inference) that aren't already in the model will be ignored.</p>
",0,0,578,2020-11-16 21:37:08,https://stackoverflow.com/questions/64866067/understanding-the-role-of-the-function-build-vocab-in-doc2vec
Low accuracy rate after training Doc2Vec model,"<p>I'm trying to train a Doc2Vec model in order to create a multi-label text classifier.<br />
In order to do that, i have chosen a data set that contains approximately 70000 article, and every article contains between 1500 and 2000 words.<br />
These articles are divided into 5 classes.<br />
while setting up my input, i chosen as tag for my document their corresponding label.
I have done it as follow :
<code>tagged_article = data.apply(lambda r: TaggedDocument(words=r['article'].split(), tags=[r.labels]), axis=1)</code><br />
then i have trained my model with the following line codes:</p>
<pre><code>model_dbow = Doc2Vec(dm=1, vector_size=300, negative=5, min_count=10, workers=cores)
model_dbow.build_vocab([x for x in tqdm(tagged_article.values)])

print(&quot;Training the Doc2Vec model for &quot;, no_epochs, &quot;number of epochs&quot; )
for epoch in range(no_epochs):
     model_dbow.train(utils.shuffle([x for x in tqdm(tagged_article.values)]),total_examples=len(tagged_article.values), epochs=1)
     model_dbow.alpha -= 0.002
     model_dbow.min_alpha = model_dbow.alpha   
</code></pre>
<p>After that I have created a logistic regression model in order to predict tags for every article.</p>
<p>To do that I have created the following functions:\</p>
<pre><code>def vec_for_learning(model, tagged_docs):
sents = tagged_docs.values
targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=inference_steps)) for doc in tqdm(sents)])
return targets, regressors

y_train, X_train = vec_for_learning(model_dbow, tagged_article)

logreg = LogisticRegression(solver='lbfgs',max_iter=1000)
logreg.fit(X_train, y_train)
</code></pre>
<p>Unfortunately i am getting a very bad result. In fact I'm getting 22% as accuracy rate and 21 % as an F1 score</p>
<p>Can you please explain me why i am getting these bad results.</p>
","python, nlp, data-science, gensim, doc2vec","<p>First &amp; foremost, you almost certainly don't want to use your own loop to call <code>train()</code> multiple times while managing <code>alpha</code> yourself. See: <a href=""https://stackoverflow.com/questions/62801052/my-doc2vec-code-after-many-loops-of-training-isnt-giving-good-results-what-m/62801053#62801053"">My Doc2Vec code, after many loops of training, isn&#39;t giving good results. What might be wrong?</a></p>
<p>As you don't show your <code>no_epochs</code> value, I can't be sure you're doing the absolute worst thing - eventually decrementing <code>alpha</code> to a negative value – but you might be. Still, there's no need for that error-prone loop. (And, you may want to contact whatever source suggested this code template to you and let them know they are promoting an anti-pattern.)</p>
<p>It is probably also a mistake to use your just 5 known-labels as the document-tags. That means the model is essentially only learning 5 doc-vectors, as if all articles were just fragments of 5 giant texts. While it's sometimes helpful to use (or add) known-labels as tags, the more classic manner of training <code>Doc2Vec</code> gives each document a unique ID, so the model is learning (in your case) about 70,000 distinct doc-vectors, and may more richly model the document-possibility spaces spanned, in various irregular shapes, by all your documents and labels.</p>
<p>While your data is certainly of a size comparable to published work that shows the value of the <code>Doc2Vec</code> algorithm, your corpus isn't gigantic (and it's unclear how large &amp; diverse your vocabulary might be). So it's possible that 300 dimensions is oversized for the quanitity/variety of data you have, or <code>min_count=10</code> too aggressive (or not aggressive enough) in trimming less-important &amp; less-well-sampled words.</p>
<p>Finally, note that the <code>Doc2Vec</code> class will inherit a default <code>epochs</code> value of 5, but most published work uses 10-20 training epochs, and often with smaller datasets even more can be helpful. Additionally, inference will reuse the same <code>epochs</code> set (or defaulted) at model creation, and works best with (at least) the same number of <code>epochs</code> as training - while it's unclear what <code>inference_steps</code> you're using.</p>
<p>(As a separate matter of code legibility: you've named your model <code>model_dbow</code>, but by using <code>dm=1</code> you're actually using PV-DM mode, <strong>not</strong> PV-DBOW mode.)</p>
",0,0,508,2020-11-17 13:52:56,https://stackoverflow.com/questions/64876611/low-accuracy-rate-after-training-doc2vec-model
How to download glove-wiki-gigaword-100 or other word vector package using gensim.downloader behind a proxy?,"<p>Usually, I can use the following code to download the word vector package in jupyter lab:</p>
<pre><code>import gensim.downloader as api
word_vectors = api.load(&quot;glove-wiki-gigaword-50&quot;)
</code></pre>
<p>But now, i am using a windows server, which has a firewall. So this way does not work anymore. I also tried the way on <a href=""https://github.com/RaRe-Technologies/gensim-data"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim-data</a>:</p>
<pre><code>python -m gensim.downloader --download glove-twitter-25 
</code></pre>
<p>But I do not know how to set up the proxy in this line. If my proxy is <a href=""http://my-proxy.com:80"" rel=""nofollow noreferrer"">http://my-proxy.com:80</a>. Then how can I download the word vector behind a firewall?</p>
","api, vector, download, gensim, cpu-word","<p>I would not use the <code>gensim.downloader</code> facility at all, given the extra complexity/hidden-steps it introduces (which include <a href=""https://github.com/RaRe-Technologies/gensim/issues/2283"" rel=""nofollow noreferrer"">what I consider an unnecessary security risk</a> of downloading &amp; running extra 'shim' Python code that's not in the normal Gensim release).</p>
<p>Instead, find the plain dataset you want, download it to somewhere you can, then use whatever other method you have for transferring files to your firewalled Windows Server.</p>
<p>Specifically, the 50d GLoVe vectors appear to be included as part of the <code>glove.6B.zip</code> download available on the canonical GLoVe home page:</p>
<p><a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/glove/</a></p>
",3,1,8808,2020-11-18 06:13:27,https://stackoverflow.com/questions/64887979/how-to-download-glove-wiki-gigaword-100-or-other-word-vector-package-using-gensi
Python LDA Gensim model with over 20 topics does not print properly,"<p>Using the Gensim package (both LDA and Mallet), I noticed that when I create a model with more than 20 topics, and I use the print_topics function, it will print a maximum of 20 topics (note, not the first 20 topics, rather any 20 topics), and they will be out of order.</p>
<p>And so my question is, how do i get all of the topics to print? I am unsure if this is a bug or an issue on my end. I have looked back at my library of LDA models (over 5000, different data sources), and have noted this happens in all of them where topics are above 20.</p>
<p>Below is sample code with output. In the output, you will see the topics are not ordered (they should be) and topics are missing such as topic 3.</p>
<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus=jr_dict_corpus,
                                           id2word=jr_dict,
                                           num_topics=25, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)

pprint(lda_model.print_topics())
#note, if the model contained 20 topics, the topics would be listed in order 0-19
[(21,
  '0.001*&quot;commitment&quot; + 0.001*&quot;study&quot; + 0.001*&quot;evolve&quot; + 0.001*&quot;outlook&quot; + '
  '0.001*&quot;value&quot; + 0.001*&quot;people&quot; + 0.001*&quot;individual&quot; + 0.001*&quot;client&quot; + '
  '0.001*&quot;structure&quot; + 0.001*&quot;proposal&quot;'),
 (18,
  '0.001*&quot;self&quot; + 0.001*&quot;insurance&quot; + 0.001*&quot;need&quot; + 0.001*&quot;trend&quot; + '
  '0.001*&quot;statistic&quot; + 0.001*&quot;propose&quot; + 0.001*&quot;analysis&quot; + 0.001*&quot;perform&quot; + '
  '0.001*&quot;impact&quot; + 0.001*&quot;awareness&quot;'),
 (2,
  '0.001*&quot;link&quot; + 0.001*&quot;task&quot; + 0.001*&quot;collegiate&quot; + 0.001*&quot;universitie&quot; + '
  '0.001*&quot;banking&quot; + 0.001*&quot;origination&quot; + 0.001*&quot;security&quot; + 0.001*&quot;standard&quot; '
  '+ 0.001*&quot;qualifications_bachelor&quot; + 0.001*&quot;greenfield&quot;'),
 (11,
  '0.024*&quot;collegiate&quot; + 0.016*&quot;interpersonal&quot; + 0.016*&quot;prepare&quot; + '
  '0.016*&quot;invite&quot; + 0.016*&quot;aspect&quot; + 0.016*&quot;college&quot; + 0.016*&quot;statistic&quot; + '
  '0.016*&quot;continent&quot; + 0.016*&quot;structure&quot; + 0.016*&quot;project&quot;'),
 (10,
  '0.049*&quot;enjoy&quot; + 0.049*&quot;ambiguity&quot; + 0.017*&quot;accordance&quot; + 0.017*&quot;liberalize&quot; '
  '+ 0.017*&quot;developing&quot; + 0.017*&quot;application&quot; + 0.017*&quot;vacancie&quot; + '
  '0.017*&quot;service&quot; + 0.017*&quot;initiative&quot; + 0.017*&quot;discontinuing&quot;'),
 (20,
  '0.028*&quot;negotiation&quot; + 0.028*&quot;desk&quot; + 0.018*&quot;enhance&quot; + 0.018*&quot;engage&quot; + '
  '0.018*&quot;discussion&quot; + 0.018*&quot;ability&quot; + 0.018*&quot;depth&quot; + 0.018*&quot;derive&quot; + '
  '0.018*&quot;enjoy&quot; + 0.018*&quot;balance&quot;'),
 (12,
  '0.036*&quot;individual&quot; + 0.024*&quot;validate&quot; + 0.018*&quot;greenfield&quot; + '
  '0.018*&quot;capability&quot; + 0.018*&quot;coordinate&quot; + 0.018*&quot;create&quot; + '
  '0.018*&quot;programming&quot; + 0.018*&quot;safety&quot; + 0.010*&quot;evaluation&quot; + '
  '0.002*&quot;reliability&quot;'),
 (1,
  '0.028*&quot;negotiation&quot; + 0.021*&quot;responsibility&quot; + 0.014*&quot;master&quot; + '
  '0.014*&quot;mind&quot; + 0.014*&quot;experience&quot; + 0.014*&quot;worker&quot; + 0.014*&quot;ability&quot; + '
  '0.007*&quot;summary&quot; + 0.007*&quot;proposal&quot; + 0.007*&quot;alert&quot;'),
 (23,
  '0.043*&quot;banking&quot; + 0.026*&quot;origination&quot; + 0.026*&quot;round&quot; + 0.026*&quot;credibility&quot; '
  '+ 0.026*&quot;entity&quot; + 0.018*&quot;standard&quot; + 0.017*&quot;range&quot; + 0.017*&quot;pension&quot; + '
  '0.017*&quot;adapt&quot; + 0.017*&quot;information&quot;'),
 (13,
  '0.034*&quot;priority&quot; + 0.034*&quot;reconciliation&quot; + 0.034*&quot;purchaser&quot; + '
  '0.023*&quot;reporting&quot; + 0.023*&quot;offer&quot; + 0.023*&quot;investor&quot; + 0.023*&quot;share&quot; + '
  '0.023*&quot;region&quot; + 0.023*&quot;service&quot; + 0.023*&quot;manipulate&quot;'),
 (22,
  '0.017*&quot;analyst&quot; + 0.017*&quot;modelling&quot; + 0.016*&quot;producer&quot; + 0.016*&quot;return&quot; + '
  '0.016*&quot;self&quot; + 0.009*&quot;scope&quot; + 0.008*&quot;mind&quot; + 0.008*&quot;need&quot; + 0.008*&quot;detail&quot; '
  '+ 0.008*&quot;statistic&quot;'),
 (9,
  '0.021*&quot;decision&quot; + 0.014*&quot;invite&quot; + 0.014*&quot;balance&quot; + 0.014*&quot;commercialize&quot; '
  '+ 0.014*&quot;transform&quot; + 0.014*&quot;manage&quot; + 0.014*&quot;optionality&quot; + '
  '0.014*&quot;problem_solving&quot; + 0.014*&quot;fuel&quot; + 0.014*&quot;stay&quot;'),
 (7,
  '0.032*&quot;commitment&quot; + 0.032*&quot;study&quot; + 0.016*&quot;impact&quot; + 0.016*&quot;outlook&quot; + '
  '0.011*&quot;operation&quot; + 0.011*&quot;expand&quot; + 0.011*&quot;exchange&quot; + 0.011*&quot;management&quot; '
  '+ 0.011*&quot;conde&quot; + 0.011*&quot;evolve&quot;'),
 (15,
  '0.032*&quot;agility&quot; + 0.019*&quot;feasibility&quot; + 0.019*&quot;self&quot; + 0.014*&quot;deploy&quot; + '
  '0.014*&quot;define&quot; + 0.013*&quot;investment&quot; + 0.013*&quot;option&quot; + 0.013*&quot;control&quot; + '
  '0.013*&quot;action&quot; + 0.013*&quot;incubation&quot;'),
 (5,
  '0.020*&quot;desk&quot; + 0.018*&quot;agility&quot; + 0.016*&quot;vender&quot; + 0.016*&quot;coordinate&quot; + '
  '0.016*&quot;committee&quot; + 0.012*&quot;acquisition&quot; + 0.012*&quot;target&quot; + '
  '0.012*&quot;counterparty&quot; + 0.012*&quot;approval&quot; + 0.012*&quot;trend&quot;'),
 (17,
  '0.022*&quot;option&quot; + 0.017*&quot;working&quot; + 0.017*&quot;niche&quot; + 0.011*&quot;business&quot; + '
  '0.011*&quot;constrain&quot; + 0.011*&quot;meeting&quot; + 0.011*&quot;correspond&quot; + 0.011*&quot;exposure&quot; '
  '+ 0.011*&quot;element&quot; + 0.011*&quot;face&quot;'),
 (0,
  '0.025*&quot;expertise&quot; + 0.025*&quot;banking&quot; + 0.021*&quot;universitie&quot; + '
  '0.017*&quot;spreadsheet&quot; + 0.013*&quot;negotiation&quot; + 0.013*&quot;shipment&quot; + '
  '0.013*&quot;arise&quot; + 0.013*&quot;billing&quot; + 0.013*&quot;assistance&quot; + 0.013*&quot;sector&quot;'),
 (4,
  '0.024*&quot;provide&quot; + 0.017*&quot;consider&quot; + 0.017*&quot;allow&quot; + 0.015*&quot;outlook&quot; + '
  '0.015*&quot;value&quot; + 0.015*&quot;contract&quot; + 0.012*&quot;study&quot; + 0.012*&quot;technology&quot; + '
  '0.012*&quot;scenario&quot; + 0.012*&quot;indicator&quot;'),
 (6,
  '0.058*&quot;impulse&quot; + 0.027*&quot;shall&quot; + 0.027*&quot;shape&quot; + 0.024*&quot;marketer&quot; + '
  '0.017*&quot;availability&quot; + 0.014*&quot;determine&quot; + 0.014*&quot;load&quot; + '
  '0.014*&quot;constantly_change&quot; + 0.014*&quot;instrument&quot; + 0.014*&quot;interface&quot;'),
 (19,
  '0.042*&quot;task&quot; + 0.038*&quot;tariff&quot; + 0.038*&quot;recommend&quot; + 0.024*&quot;example&quot; + '
  '0.023*&quot;future&quot; + 0.021*&quot;people&quot; + 0.021*&quot;math&quot; + 0.021*&quot;capacity&quot; + '
  '0.021*&quot;spirit&quot; + 0.020*&quot;price&quot;')]
</code></pre>
<p>Same model as above, but using 20 topics. As you can see, the output is in order by topic # and it contains all the topics.</p>
<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus=jr_dict_corpus,
                                           id2word=jr_dict,
                                           num_topics=20, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)

pprint(lda_model.print_topics())

[(0,
  '0.031*&quot;enjoy&quot; + 0.031*&quot;ambiguity&quot; + 0.028*&quot;accordance&quot; + 0.016*&quot;statistic&quot; '
  '+ 0.016*&quot;initiative&quot; + 0.016*&quot;service&quot; + 0.016*&quot;liberalize&quot; + '
  '0.016*&quot;application&quot; + 0.011*&quot;community&quot; + 0.011*&quot;identifie&quot;'),
 (1,
  '0.016*&quot;transformation&quot; + 0.016*&quot;negotiation&quot; + 0.016*&quot;community&quot; + '
  '0.016*&quot;clock&quot; + 0.011*&quot;marketer&quot; + 0.011*&quot;desk&quot; + 0.011*&quot;mandate&quot; + '
  '0.011*&quot;closing&quot; + 0.011*&quot;initiative&quot; + 0.011*&quot;experience&quot;'),
 (2,
  '0.026*&quot;priority&quot; + 0.026*&quot;reconciliation&quot; + 0.026*&quot;purchaser&quot; + '
  '0.020*&quot;safety&quot; + 0.020*&quot;region&quot; + 0.020*&quot;query&quot; + 0.020*&quot;share&quot; + '
  '0.020*&quot;manipulate&quot; + 0.020*&quot;ibex&quot; + 0.020*&quot;investor&quot;'),
 (3,
  '0.022*&quot;improve&quot; + 0.021*&quot;committee&quot; + 0.021*&quot;affect&quot; + 0.012*&quot;target&quot; + '
  '0.012*&quot;acquisition&quot; + 0.011*&quot;basis&quot; + 0.011*&quot;profitability&quot; + '
  '0.011*&quot;economic&quot; + 0.011*&quot;natural&quot; + 0.011*&quot;profit&quot;'),
 (4,
  '0.024*&quot;provide&quot; + 0.019*&quot;value&quot; + 0.017*&quot;consider&quot; + 0.017*&quot;allow&quot; + '
  '0.015*&quot;scenario&quot; + 0.015*&quot;outlook&quot; + 0.015*&quot;contract&quot; + 0.014*&quot;forecast&quot; + '
  '0.014*&quot;decision&quot; + 0.012*&quot;indicator&quot;'),
 (5,
  '0.037*&quot;desk&quot; + 0.030*&quot;coordinate&quot; + 0.030*&quot;agility&quot; + 0.030*&quot;vender&quot; + '
  '0.023*&quot;counterparty&quot; + 0.023*&quot;immature_emerge&quot; + 0.023*&quot;metric&quot; + '
  '0.022*&quot;approval&quot; + 0.015*&quot;maximization&quot; + 0.015*&quot;undergraduate&quot;'),
 (6,
  '0.053*&quot;impulse&quot; + 0.025*&quot;shall&quot; + 0.025*&quot;shape&quot; + 0.018*&quot;availability&quot; + '
  '0.018*&quot;marketer&quot; + 0.012*&quot;determine&quot; + 0.012*&quot;language&quot; + '
  '0.012*&quot;monitoring&quot; + 0.012*&quot;integration&quot; + 0.012*&quot;month&quot;'),
 (7,
  '0.026*&quot;commitment&quot; + 0.026*&quot;study&quot; + 0.013*&quot;impact&quot; + 0.013*&quot;outlook&quot; + '
  '0.009*&quot;operation&quot; + 0.009*&quot;management&quot; + 0.009*&quot;expand&quot; + 0.009*&quot;exchange&quot; '
  '+ 0.009*&quot;conde&quot; + 0.009*&quot;balance&quot;'),
 (8,
  '0.057*&quot;insurance&quot; + 0.029*&quot;propose&quot; + 0.028*&quot;rule&quot; + 0.026*&quot;self&quot; + '
  '0.023*&quot;product&quot; + 0.023*&quot;asset&quot; + 0.023*&quot;pricing&quot; + 0.023*&quot;amount&quot; + '
  '0.023*&quot;result&quot; + 0.020*&quot;liquidity&quot;'),
 (9,
  '0.012*&quot;universitie&quot; + 0.012*&quot;need&quot; + 0.012*&quot;statistic&quot; + 0.012*&quot;trend&quot; + '
  '0.008*&quot;invite&quot; + 0.008*&quot;commercialize&quot; + 0.008*&quot;transform&quot; + 0.008*&quot;manage&quot; '
  '+ 0.008*&quot;problem_solving&quot; + 0.008*&quot;optionality&quot;'),
 (10,
  '0.024*&quot;background&quot; + 0.024*&quot;curve&quot; + 0.020*&quot;allow&quot; + 0.019*&quot;collect&quot; + '
  '0.019*&quot;basis&quot; + 0.017*&quot;accordance&quot; + 0.013*&quot;improve&quot; + 0.013*&quot;datum&quot; + '
  '0.013*&quot;component&quot; + 0.013*&quot;reliability&quot;'),
 (11,
  '0.054*&quot;task&quot; + 0.049*&quot;tariff&quot; + 0.049*&quot;recommend&quot; + 0.031*&quot;future&quot; + '
  '0.027*&quot;spirit&quot; + 0.027*&quot;capacity&quot; + 0.027*&quot;math&quot; + 0.022*&quot;ensure&quot; + '
  '0.022*&quot;profit&quot; + 0.022*&quot;variable_margin&quot;'),
 (12,
  '0.001*&quot;impulse&quot; + 0.001*&quot;availability&quot; + 0.001*&quot;reliability&quot; + '
  '0.001*&quot;shall&quot; + 0.001*&quot;component&quot; + 0.001*&quot;agent&quot; + 0.001*&quot;marketer&quot; + '
  '0.001*&quot;shape&quot; + 0.001*&quot;assisting&quot; + 0.001*&quot;supply&quot;'),
 (13,
  '0.021*&quot;region&quot; + 0.016*&quot;greenfield&quot; + 0.016*&quot;collegiate&quot; + 0.011*&quot;transfer&quot; '
  '+ 0.011*&quot;remuneration&quot; + 0.011*&quot;organization&quot; + 0.011*&quot;structure&quot; + '
  '0.011*&quot;continent&quot; + 0.011*&quot;project&quot; + 0.011*&quot;prepare&quot;'),
 (14,
  '0.033*&quot;originator&quot; + 0.025*&quot;vender&quot; + 0.025*&quot;expertise&quot; + 0.025*&quot;banking&quot; + '
  '0.019*&quot;evolve&quot; + 0.017*&quot;management&quot; + 0.017*&quot;market&quot; + 0.017*&quot;site&quot; + '
  '0.012*&quot;component&quot; + 0.012*&quot;discontinuing&quot;'),
 (15,
  '0.027*&quot;agility&quot; + 0.022*&quot;mind&quot; + 0.022*&quot;negotiation&quot; + 0.011*&quot;deploy&quot; + '
  '0.011*&quot;define&quot; + 0.011*&quot;ecosystem&quot; + 0.011*&quot;control&quot; + 0.011*&quot;lead&quot; + '
  '0.011*&quot;industry&quot; + 0.011*&quot;option&quot;'),
 (16,
  '0.001*&quot;region&quot; + 0.001*&quot;master&quot; + 0.001*&quot;orginiation&quot; + 0.001*&quot;greenfield&quot; '
  '+ 0.001*&quot;agent&quot; + 0.001*&quot;identifie&quot; + 0.001*&quot;remuneration&quot; + 0.001*&quot;mark&quot; + '
  '0.001*&quot;reviewing&quot; + 0.001*&quot;closing&quot;'),
 (17,
  '0.030*&quot;banking&quot; + 0.018*&quot;option&quot; + 0.018*&quot;round&quot; + 0.018*&quot;credibility&quot; + '
  '0.018*&quot;origination&quot; + 0.018*&quot;entity&quot; + 0.016*&quot;working&quot; + 0.015*&quot;niche&quot; + '
  '0.015*&quot;standard&quot; + 0.012*&quot;coordinate&quot;'),
 (18,
  '0.027*&quot;negotiation&quot; + 0.018*&quot;reporting&quot; + 0.018*&quot;perform&quot; + 0.018*&quot;world&quot; + '
  '0.015*&quot;offer&quot; + 0.015*&quot;manipulate&quot; + 0.011*&quot;query&quot; + 0.010*&quot;control&quot; + '
  '0.010*&quot;working&quot; + 0.009*&quot;self&quot;'),
 (19,
  '0.047*&quot;example&quot; + 0.039*&quot;people&quot; + 0.039*&quot;price&quot; + 0.039*&quot;excel&quot; + '
  '0.039*&quot;excellent&quot; + 0.038*&quot;base&quot; + 0.031*&quot;office&quot; + 0.031*&quot;optimizing&quot; + '
  '0.031*&quot;participate&quot; + 0.031*&quot;package&quot;')]
</code></pre>
","python, gensim, lda","<p>The default number of topics for print_topics is 20. You must use the num_topics argument to include topics above 20...</p>
",2,2,1414,2020-11-18 22:13:28,https://stackoverflow.com/questions/64902215/python-lda-gensim-model-with-over-20-topics-does-not-print-properly
Difference between Text Embedding and Word Embedding,"<p>I am working on a dataset of amazon alexa reviews and wish to cluster them in positive and negative clusters. I am using Word2Vec for vectorization so wanted to know the difference between <strong>Text Embedding</strong> and <strong>Word Embedding</strong>. Also, which one of them will be useful for my clustering of reviews (Please consider that I want to predict the cluster of any reviews that I enter.)
Thanks in advance!</p>
","python-3.x, nlp, k-means, gensim, word2vec","<p>Text Embeddings are typically a way to aggregate a number of Word Embeddings for a sentence or a paragraph of text. There are various ways this can be done. The easiest way is to average word embeddings but not necessarily yielding best results.</p>
<p>Application-wise:</p>
<ul>
<li><a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">Doc2vec</a> from <code>gensim</code></li>
<li><a href=""https://datascience.stackexchange.com/questions/17140/difference-between-paragraph2vec-and-doc2vec"">par2vec vs. doc2vec</a></li>
</ul>
",0,1,319,2020-11-19 20:19:47,https://stackoverflow.com/questions/64919359/difference-between-text-embedding-and-word-embedding
Can the gensim pretrained models be used for doc2vec models?,"<p>I am trying to load a pretrained model <a href=""https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html#sphx-glr-auto-examples-howtos-run-downloader-api-py"" rel=""nofollow noreferrer"">listed here</a> to test the similarity of a handful of paragraphs.</p>
<p>Can gensim's pretrained models only be used with word-level vectors, or can the models also be used for document-length vectors?</p>
","python, gensim, word-embedding, doc2vec","<p>Most of the models currently listed there (as of 2020-11-21) are just sets of word-vectors - allowing lookup of vectors, by individual word, but not the full algorithmic model that would allow for followup training. (The only exception I see is the FastText model, which *might8 be a full FastText model, I'm not sure. But even there, the model only reports word-vectors for known words, or synthesizes a vector for out-of-vocabulary words - with no native method of creating vectors for larger texts.)</p>
<p>From any set of word-vectors, there are some crude ways to either create a simple vector for larger texts (such as averaging all the word-vectors for the words of the text together), or do other comparisons between sets of words using the word-vectors to influence the similarity (such as the &quot;<a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html"" rel=""nofollow noreferrer"">Word Mover's Distance</a>&quot; algorithm, available on Gensim word-vector sets as <code>wmdistance()</code>.)</p>
<p>But none of those models availabe via the <code>gensim.downloader</code> utility are for algorithms that inherently create vectors for larger texts (such as <code>Doc2Vec</code>).</p>
<p>(Separately: I would strongly recommend downloading models explicitly, as data, from their original locations, rather than using the <code>gensim.downloader</code> utility. It obscures key aspects of the process, including running extra 'shim' code for each dataset that is downloaded outside of normal code-versioning &amp; package-installation processes, a practice that I consider <a href=""https://github.com/RaRe-Technologies/gensim/issues/2283"" rel=""nofollow noreferrer"">recklessly insecure</a>.)</p>
",2,0,280,2020-11-22 01:23:40,https://stackoverflow.com/questions/64949799/can-the-gensim-pretrained-models-be-used-for-doc2vec-models
Force gensim&#39;s word2vec vectors to be positive?,"<p>Is there any way in gensim that i can force the learned vectors in word2vec to be all positive? (all the elements of vector be positive). i am working on a different task that needs these vectors to be positive ( the reason is really complicated so don't ask why )</p>
<p>so what is the easiest way for me to force gensim to learn positive vectors?</p>
","gensim, word2vec","<p>There is no built-in feature of Gensim that would allow this extra constraint/regularization to be applied during training.</p>
<p>You should probably try to explain your 'really complicated' reason for this idosyncratic request. There might be a better way to achieve the real end-goal, rather than shoehorning vectors that are typically bushy-and-balanced around the origin into a non-negative representation.</p>
<p>Notably, a paper called '<a href=""https://arxiv.org/abs/1702.01417"" rel=""nofollow noreferrer"">All-but-the-Top: Simple and Effective Postprocessing for Word Representations</a>' has suggested word-vectors can be improved by postprocessing to ensure they are <em>more</em> balanced around the origin, rather than less (as seems a reliable side-effect of typical negative-sampling configurations).</p>
<p>If you're still interested to experiment in the opposite direction – transforming usual word2vec word-vectors into a representation where all dimensions are positive – I can think of a number of trivial, superficial ways to achieve that. I have no idea whether they'd actually preserve, or ruin, beneficial properties in the vectors – but you could try them, and see. For example:</p>
<ul>
<li>You could try simply setting all negative dimensions to 0.0 - truncation. (Loses lots of info but might give a quick indication if a dirt-simple experiment gives you any of the benefits you seek.)</li>
<li>You could find the largest negative dimension that appears anywhere in any of the vectors, then add its absolute value to all other dimensions. Voila! No vector dimension is now lower than 0.0. (You could also try this in a per-dimension manner - only correct dimension #0 with the lowest dimension #0 value. Or, try other re-scalings of each dimension such that the previously-highly-negative values are 0.0, and the previous-highly-positive values stay where they are or only shift a little.)</li>
<li>You could try turning every dimension in the original word-vectors into two dimensions in a transformed set: one that's the original positive value, or 0.0 if it was negative, and a 2nd dimension that's the absolute value of the original negative value, or 0.0 if it was positive. (Or similarly: one dimension that's the absolute-value of the original value, and one dimension that's 0.0 or 1.0 depending on whether original value was negative or positive.)</li>
</ul>
<p>There are probably other more-sophisticated factorization/decompositions for re-representing the full set of word-vectors in a transformed array with only non-negative individual values, but I don't know them offhand, other than to think it might be worth searching for them.</p>
<p>And, whether any of these transformations work for your next steps, who knows? But it might be worth trying. (And if any of these offer surprisingly good results, it'd be great to hear in a followup comment!)</p>
",3,0,570,2020-11-22 14:28:03,https://stackoverflow.com/questions/64955331/force-gensims-word2vec-vectors-to-be-positive
Gensim most similar word to vector,"<p>I am using the pretrained word vectors from Wikipedia, <code>&quot;glove-wiki-gigaword-100&quot;</code>, in Gensim. As <a href=""https://github.com/kavgan/nlp-in-practice/blob/master/pre-trained-embeddings/Pre-trained%20embeddings.ipynb"" rel=""nofollow noreferrer"">this example documentation</a> shows, you can query the most similar words for a given word or set of words using</p>
<pre><code>model_gigaword.wv.most_similar(positive=['dirty','grimy'],topn=10)
</code></pre>
<p>However, I would like to query the most similar words to a <strong>given vector</strong>, specified as an array (of the same format as a word-vector from the pretrained model). For example, the result from adding or subtracting two word-vectors in the pretrained model, like</p>
<pre><code>vec = model_gigaword['king']-model_gigaword['man']
</code></pre>
<p>Output: (for <code>vec</code>)</p>
<pre><code>array([-0.696     , -1.26119   , -0.49109   ,  0.91179   ,  0.23077281,
       -0.18835002, -0.65568995, -0.29686698, -0.60074997, -1.35762   ,
       -0.11816999,  0.01779997, -0.74096   ,  0.21192   , -0.407071  ,
       -1.04871   , -0.480674  , -0.95541   , -0.06046999,  0.20678002,
       -1.1516    , -0.98955095,  0.44508   ,  0.32682198, -0.03306001,
       -0.31138003,  0.87721   ,  0.34279   ,  0.78621   , -0.297459  ,
        0.529243  , -0.07398   ,  0.551844  ,  0.54218   , -0.39394   ,
        0.96368   ,  0.22518003,  0.05197001, -0.912573  , -0.718755  ,
        0.08056   ,  0.421177  , -0.34256   , -0.71294   , -0.25391   ,
       -0.65362   , -0.31369498,  0.216278  ,  0.41873002, -0.21784998,
        0.21340999,  0.480393  ,  0.47077006, -1.00272   ,  0.16624999,
       -0.07340002,  0.09219003, -0.02021003, -0.58403   , -0.47306   ,
        0.05066001, -0.64416003,  0.80061007,  0.224344  , -0.20483994,
       -0.33785298, -1.24589   ,  0.08900005, -0.08385998, -0.195515  ,
        0.08500999, -0.55749   ,  0.19473001, -0.0751    , -0.61184   ,
       -0.08018   , -0.34303   ,  1.03759   , -0.36085004,  0.93508005,
       -0.00997001, -0.57282   ,  0.33101702,  0.271261  ,  0.47389007,
        1.1219599 , -0.00199997, -1.609     ,  0.57377803, -0.17023998,
       -0.22913098, -0.33818996, -0.367797  ,  0.367965  , -1.08955   ,
       -0.664806  ,  0.05213001,  0.40829998,  0.125692  , -0.44967002],
      dtype=float32)
</code></pre>
<p>How do I get the most similar words to <code>vec</code>?</p>
","python, nlp, gensim, word2vec","<p>You can directly use this with <code>model_gigaword.wv.most_similar</code></p>
<pre><code>your_word_vector = np.array([-0.696, -1.26119, -0.49109, 0.91179, 0.23077281,
       -0.18835002, -0.65568995, -0.29686698, -0.60074997, -1.35762   ,
       -0.11816999,  0.01779997, -0.74096   ,  0.21192   , -0.407071  ,
       -1.04871   , -0.480674  , -0.95541   , -0.06046999,  0.20678002,
       -1.1516    , -0.98955095,  0.44508   ,  0.32682198, -0.03306001,
       -0.31138003,  0.87721   ,  0.34279   ,  0.78621   , -0.297459  ,
        0.529243  , -0.07398   ,  0.551844  ,  0.54218   , -0.39394   ,
        0.96368   ,  0.22518003,  0.05197001, -0.912573  , -0.718755  ,
        0.08056   ,  0.421177  , -0.34256   , -0.71294   , -0.25391   ,
       -0.65362   , -0.31369498,  0.216278  ,  0.41873002, -0.21784998,
        0.21340999,  0.480393  ,  0.47077006, -1.00272   ,  0.16624999,
       -0.07340002,  0.09219003, -0.02021003, -0.58403   , -0.47306   ,
        0.05066001, -0.64416003,  0.80061007,  0.224344  , -0.20483994,
       -0.33785298, -1.24589   ,  0.08900005, -0.08385998, -0.195515  ,
        0.08500999, -0.55749   ,  0.19473001, -0.0751    , -0.61184   ,
       -0.08018   , -0.34303   ,  1.03759   , -0.36085004,  0.93508005,
       -0.00997001, -0.57282   ,  0.33101702,  0.271261  ,  0.47389007,
        1.1219599 , -0.00199997, -1.609     ,  0.57377803, -0.17023998,
       -0.22913098, -0.33818996, -0.367797  ,  0.367965  , -1.08955   ,
       -0.664806  ,  0.05213001,  0.40829998,  0.125692  , -0.44967002])

model_gigaword.wv.most_similar(positive=[your_word_vector], topn=10)
</code></pre>
<pre><code>[('vajiravudh', 0.7130449414253235),
 ('prajadhipok', 0.6764554381370544),
 ('andrianampoinimerina', 0.6474215984344482),
 ('jeongjo', 0.6449092626571655),
 ('taejong', 0.6352322697639465),
 ('rehoboam', 0.6319528818130493),
 ('injo', 0.6317901611328125),
 ('gojong', 0.6302404999732971),
 ('seonjo', 0.6272163391113281),
 ('elessar', 0.6250109672546387)]

</code></pre>
<p><strong>These results will be almost garbage, as expected. Read the reason below.</strong></p>
<hr />
<p>One important point though. I see you are trying to find the words that are similar to the difference vector in the euclidean space of the word vectors. The difference between <code>king</code> and <code>man</code> results in a vector that is similar to the difference between <code>queen</code> and <code>woman</code> means that the length and direction of the difference vector encode the contextual difference between the 2 respective pairs of words.</p>
<p><strong>The literal position of that vector maybe garbage because by checking it in the euclidean space, you will anchor it on the origin. Both the difference vectors (King-&gt;Man and Queen-&gt;Woman) above are anchored on 'King' and 'Queen' respectively.</strong></p>
<p>The intuition you should have is that A-&gt;B and C-&gt;D may have similar vectors connecting them even though A, B and C, D may line in completely separate parts of the euclidean space, IF they have a similar contextual difference between them. This is what the vector space in a properly trained word2vec is encoding.</p>
<p><a href=""https://i.sstatic.net/XgKwq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XgKwq.png"" alt=""enter image description here"" /></a></p>
",4,2,3522,2020-11-23 18:48:18,https://stackoverflow.com/questions/64974507/gensim-most-similar-word-to-vector
How to tune the parameters for gensim `LdaMulticore` in Python,"<p>I was running <code>gensim</code> <code>LdaMulticore</code> package for the topic modelling using Python.
I tried to understand the meaning of the parameters within <code>LdaMulticore</code> and found the website that provides some explanations on the usage of the parameters. As a non-expert, I have some difficulty understanding these intuitively. I also referred some other materials from the website but I guess this page gives relatively full explanations of every parameters.
<br>
<a href=""https://radimrehurek.com/gensim/models/ldamulticore.html"" rel=""nofollow noreferrer"">This page</a></p>
<ol>
<li><code>chunksize</code>
Number of documents to be used in each training chunk.
<br>-&gt;Does it mean that it determines how many documents to be analyzed (trained) at once?
<br>Does changing the <code>chunksize</code> number generate significantly different outcomes? or does it just matter to the running time?</li>
</ol>
<p>2.<code>alpha</code>, <code>eta</code>, <code>decay</code>
<br>-&gt;I kept reading the explanations but couldn't understand these at all.
<br>Could someone give me some intuitive explanations on what these are about/when do I need to adjust these?</p>
<p>3.iteration
<br>Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.
<br>-&gt;It seems that Python goes over n times of the entire corpus when I set it to n. So the higher the number, the more data is analyzed but takes longer time.</p>
<p>4.random state
<br>Either a <code>randomState</code> object or a seed to generate one. Useful for reproducibility.
<br>-&gt;I've seen people setting up this by putting a random number. But what is random state about?</p>
","python, python-3.x, nlp, gensim, lda","<p>I am wondering if you saw <a href=""https://stackoverflow.com/a/50811902/6573902"">this answer</a>? There I provide some explanation regarding <code>chunksize</code> and <code>alpha</code>. <a href=""https://miningthedetails.com/blog/python/lda/GensimLDA/"" rel=""nofollow noreferrer"">This blog post</a> has practical tips and can be of help too.</p>
<p>In short:
<code>chunksize</code> - how many documents are loaded into memory while calculating &quot;expectation&quot; step before updating the model. Each &quot;expectation&quot; step of <a href=""https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"" rel=""nofollow noreferrer"">Expectation Maximization</a> algorithm takes into account this number of documents at once and updates the matrix only after it finishes the calculation on the &quot;chunk&quot;. Size of the chunk determines the performance of the process - the more documents in memory at once - the better. Overly small chunks also impact numerical accuracy, particularly for a very large number of documents.</p>
<p><code>alpha</code>, <code>eta</code>, <code>decay</code> - these are strictly linked to the LDA algorithm and there are no &quot;intuitive explanations&quot; unless you have a grasp of the algorithm which requires some understanding of Bayesian methods, <a href=""https://machinelearningmastery.com/expectation-maximization-em-algorithm/"" rel=""nofollow noreferrer"">Expectation Maximization</a> in particular.</p>
<p><code>iteration</code> - you are not correct. The higher the number the more times the algorithm goes through <strong>the whole set of documents</strong>. So there is no &quot;more data&quot;. It is only the corpus you provide, only iterated over more times.</p>
<p><code>random_state</code> - this serves as a seed (in case you wanted to repeat exactly the training process it is enough that you set the seed to the same value and you are going to receive the same model on the same data + other parameters). This is useful when you care about <a href=""https://determined.ai/blog/reproducibility-in-ml/"" rel=""nofollow noreferrer"">reproducibility</a>.</p>
",3,2,2382,2020-11-26 00:29:10,https://stackoverflow.com/questions/65014553/how-to-tune-the-parameters-for-gensim-ldamulticore-in-python
Importing a gensim doc2vec model in deeplearning4j,"<p>I have trained a <code>doc2vec</code> model with <code>gensim</code> and like to import it into <code>Deeplearning4j</code> in order to deploy that model.</p>
<p>For <code>word2vec</code> models, I know that this is possible by saving the model with</p>
<pre><code>model.wv.save_word2vec_format(&quot;word2vec.bin&quot;, binary=True)
</code></pre>
<p>and importing if in Java with</p>
<pre><code>Word2Vec w2vModel = WordVectorSerializer.readWord2VecModel(&quot;word2vec.bin&quot;);
</code></pre>
<p>Is there a similar way to import a <code>doc2vec</code> model?</p>
","java, gensim, word2vec, doc2vec, deeplearning4j","<p>The <code>save_word2vec_format()</code> method saves just the word-vectors, not the full model.</p>
<p>If you were to use Gensim's <code>.save()</code> to save the full model, it'd use Python's native serialization - so any Java code to read it would have to understand that format before rearranging relevant properties into the DL4J objects.</p>
<p>I don't see anything in the docs for <a href=""https://deeplearning4j.org/api/latest/org/deeplearning4j/models/paragraphvectors/ParagraphVectors.html"" rel=""nofollow noreferrer"">DL4J's <code>ParagraphVectors</code> class docs</a> suggesting it can read Gensim-formatted models, so I doubt there's any built-in support.</p>
<p>It's theoretically possible that some Python code could be written to dump all the relevant subparts of the model in forms amenable to reading in Java, then patching into a Dl4J model, or for Java code to be written to understand the Python serialized objects – but that'd require some familiarity with both the Gensim &amp; DL4J source code.</p>
<p>(If the <code>toJson()</code> &amp; <code>fromJson()</code> methods in DL4J work with full model representations – which isn't clear from the docs, and would be an extremely bloated format for the bulk of the model state – that'd likely make the model-translation a little easier, as it'd provide a straightforward template for what some new Python code would need to write-out.)</p>
",1,0,446,2020-11-26 09:39:11,https://stackoverflow.com/questions/65019412/importing-a-gensim-doc2vec-model-in-deeplearning4j
Which document embedding model for document similarity,"<p>First, I want to explain my task. I have a dataset of 300k documents with an average of 560 words (no stop word removal yet) 75% in German, 15% in English and the rest in different languages. The goal is to recommend similar documents based on an existing one. At the beginning I want to focus on the German and English documents.  </p>
<p>To achieve this goal I looked into several methods on feature extraction for document similarity, especially the word embedding methods have impressed me because they are context aware in contrast to simple TF-IDF feature extraction and the calculation of cosine similarity. </p>
<p>I'm overwhelmed by the amount of methods I could use and I haven't found a proper evaluation of those methods yet. I know for sure that the size of my documents are too big for BERT, but there is FastText, Sent2Vec, Doc2Vec and the Universal Sentence Encoder from Google. My favorite method based on my research is Doc2Vec even though there aren't any or old pre-trained models which means I have to do the training on my own.</p>
<p>Now that you know my task and goal, I have the following questions:</p>
<ul>
<li>Which method should I use for feature extraction based on the rough overview of my data?</li>
<li>My dataset is too small to train Doc2Vec on it. Do I achieve good results if I train the model on English / German Wikipedia? </li>
</ul>
","python, gensim, word-embedding, doc2vec, fasttext","<p>You really have to try the different methods on your data, with your specific user tasks, with your time/resources budget to know which makes sense.</p>
<p>You 225K German documents and 45k English documents are each plausibly large enough to use <code>Doc2Vec</code> - as they match or exceed some published results. So you wouldn't necessarily need to add training on something else (like Wikipedia) instead, and whether adding that to your data would help or hurt is another thing you'd need to determine experimentally.</p>
<p>(There might be special challenges in German given compound words using common-enough roots but being individually rare, I'm not sure. FastText-based approaches that use word-fragments might be helpful, but I don't know a <code>Doc2Vec</code>-like algorithm that necessarily uses that same char-ngrams trick. The closest that might be possible is to use Facebook FastText's supervised mode, with a rich set of meaningful known-labels to bootstrap better text vectors - but that's highly speculative and that mode isn't supported in Gensim.)</p>
",1,-1,601,2020-11-26 18:45:04,https://stackoverflow.com/questions/65027694/which-document-embedding-model-for-document-similarity
"gensim most_similar with positive and negative, how does it work?","<p>I was reading <a href=""https://stackoverflow.com/a/54581599/7339624"">this answer</a> That says about Gensim <code>most_similar</code>:</p>
<blockquote>
<p>it performs vector arithmetic: adding the positive vectors,
subtracting the negative, then from that resulting position, listing
the known-vectors closest to that angle.</p>
</blockquote>
<p>But when I tested it, that is not the case. I trained a Word2Vec with Gensim <code>&quot;text8&quot;</code> dataset and tested these two:</p>
<pre><code>model.most_similar(positive=['woman', 'king'], negative=['man'])

&gt;&gt;&gt; [('queen', 0.7131118178367615), ('prince', 0.6359186768531799),...]
</code></pre>
<hr />
<pre><code>model.wv.most_similar([model[&quot;king&quot;] + model[&quot;woman&quot;] - model[&quot;man&quot;]])

&gt;&gt;&gt; [('king', 0.84305739402771), ('queen', 0.7326322793960571),...]
</code></pre>
<p>They are clearly not the same. even the queen score in the first is <code>0.713</code> and on the second <code>0.732</code> which are not the same.</p>
<p><strong>So</strong> I ask the question again, How does Gensim <code>most_similar</code> work? why the result of the two above are different?</p>
","python, nlp, gensim, word2vec","<p>The adding and subtracting isn't <em>all</em> that it does; for an exact description, you should look at the source code:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#LC690:%7E:text=def%20most_similar,self%2C"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#LC690:~:text=def%20most_similar,self%2C</a></p>
<p>You'll see there that the addition and subtraction is on the <em>unit-normed</em> version of each vector, via the <code>get_vector(key, use_norm=True)</code> accessor.</p>
<p>If you change your use of <code>model[key]</code> to <code>model.get_vector(key, use_norm=True)</code>, you should see your outside-the-method calculation of the target vector give the same results as letting the method combine the <code>positive</code> and <code>negative</code> vectors.</p>
",6,4,3434,2020-11-29 12:16:18,https://stackoverflow.com/questions/65059959/gensim-most-similar-with-positive-and-negative-how-does-it-work
loop over pandas column for wmd similarity,"<p>I have two dataframe. both have two columns. I want to use wmd to find closest match for each entity in column <code>source_label</code> to entities in column <code>target_label</code> However, at the end I would like to have a DataFrame with all the 4 columns with respect to the entities.</p>
<h3>df1</h3>
<pre><code>,source_Label,source_uri
'neuronal ceroid lipofuscinosis 8',&quot;http://purl.obolibrary.org/obo/DOID_0110723&quot;
'autosomal dominant distal hereditary motor neuronopathy',&quot;http://purl.obolibrary.org/obo/DOID_0111198&quot;
</code></pre>
<h3>df2</h3>
<pre><code>,target_label,target_uri
'neuronal ceroid ',&quot;http://purl.obolibrary.org/obo/DOID_0110748&quot;
'autosomal dominanthereditary',&quot;http://purl.obolibrary.org/obo/DOID_0111110&quot;
</code></pre>
<h3>Expected result</h3>
<pre><code>,source_label, target_label, source_uri, target_uri, wmd score
'neuronal ceroid lipofuscinosis 8', 'neuronal ceroid ', &quot;http://purl.obolibrary.org/obo/DOID_0110723&quot;, &quot;http://purl.obolibrary.org/obo/DOID_0110748&quot;, 0.98
'autosomal dominant distal hereditary motor neuronopathy', 'autosomal dominanthereditary', &quot;http://purl.obolibrary.org/obo/DOID_0111198&quot;, &quot;http://purl.obolibrary.org/obo/DOID_0111110&quot;, 0.65
</code></pre>
<p>The dataframe is so big that I am looking for some faster way to iterate over both label columns. So far I tried this:</p>
<pre><code>list_distances = []
temp = []

def preprocess(sentence):
    return [w for w in sentence.lower().split()]

entity = df1['source_label']
target = df2['target_label']

 for i in tqdm(entity):
    for j in target:
        wmd_distance = model.wmdistance(preprocess(i), preprocess(j))
        temp.append(wmd_distance)
    list_distances.append(min(temp))
# print(&quot;list_distances&quot;, list_distances)
WMD_Dataframe = pd.DataFrame({'source_label': pd.Series(entity),
                              'target_label': pd.Series(target),
                              'source_uri': df1['source_uri'],
                              'target_uri': df2['target_uri'],
                              'wmd_Score': pd.Series(list_distances)}).sort_values(by=['wmd_Score'])
WMD_Dataframe = WMD_Dataframe.reset_index()

</code></pre>
<p>First of all this code is not working well as the other two columns are coming directly from the dfs' and do not take entities relation with the uri into consideration.
How one can make it faster as the entities are in millions. Thanks in advance.</p>
","python, pandas, numpy, gensim, word2vec","<p>A quick fix :</p>
<pre><code>closest_neighbour_index_df2 = []


def preprocess(sentence):
    return [w for w in sentence.lower().split()]



 
for i in tqdm(entity):
    temp = []
    for j in target:
        wmd_distance = model.wmdistance(preprocess(i), preprocess(j))
        temp.append(wmd_distance)
    # maybe assert to make sure its always right
    closest_neighbour_index_df2.append(np.argmin(np.array(temp))) 
    # return argmin to return index rather than the value. 
    
# Add the indices from df2 to df1

df1['closest_neighbour'] = closest_neighbour_index_df2 
# add information to respective row from df2 using the closest_neighbour column
</code></pre>
",1,0,111,2020-11-30 08:57:09,https://stackoverflow.com/questions/65070534/loop-over-pandas-column-for-wmd-similarity
Using Word2Vec in scikit-learn pipeline,"<p>I am trying to run the w2v on this sample of data</p>
<pre><code>Statement              Label
Says the Annies List political group supports third-trimester abortions on demand.       FALSE
When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.         TRUE
&quot;Hillary Clinton agrees with John McCain &quot;&quot;by voting to give George Bush the benefit of the doubt on Iran.&quot;&quot;&quot;     TRUE
Health care reform legislation is likely to mandate free sex change surgeries.    FALSE
The economic turnaround started at the end of my term.     TRUE
The Chicago Bears have had more starting quarterbacks in the last 10 years than the total number of tenured (UW) faculty fired during the last two decades.    TRUE
Jim Dunnam has not lived in the district he represents for years now.    FALSE
</code></pre>
<p>using the code provided in this GitHub folder (FeatureSelection.py):</p>
<p><a href=""https://github.com/nishitpatel01/Fake_News_Detection"" rel=""nofollow noreferrer"">https://github.com/nishitpatel01/Fake_News_Detection</a></p>
<p>I would like to include word2vec features in my Naive Bayes model.
First I considered X and y and used train_test_split:</p>
<pre><code>X = df['Statement']
y = df['Label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)

dataset = pd.concat([X_train, y_train], axis=1)
</code></pre>
<p>This is the code I am currently using:</p>
<pre><code>#Using Word2Vec 
with open(&quot;glove.6B.50d.txt&quot;, &quot;rb&quot;) as lines:
    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))
           for line in lines}

training_sentences = DataPrep.train_news['Statement']

model = gensim.models.Word2Vec(training_sentences, size=100) # x be tokenized text
w2v = dict(zip(model.wv.index2word, model.wv.syn0))


class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(word2vec.itervalues().next())

    def fit(self, X, y): # what are X and y?
        return self

    def transform(self, X): # should it be training_sentences?
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])


&quot;&quot;&quot;
class TfidfEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.word2weight = None
        self.dim = len(word2vec.itervalues().next())
    def fit(self, X, y):
        tfidf = TfidfVectorizer(analyzer=lambda x: x)
        tfidf.fit(X)
        # if a word was never seen - it must be at least as infrequent
        # as any of the known words - so the default idf is the max of 
        # known idf's
        max_idf = max(tfidf.idf_)
        self.word2weight = defaultdict(
            lambda: max_idf,
            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])
        return self
    def transform(self, X):
        return np.array([
                np.mean([self.word2vec[w] * self.word2weight[w]
                         for w in words if w in self.word2vec] or
                        [np.zeros(self.dim)], axis=0)
                for words in X
            ])
&quot;&quot;&quot;
</code></pre>
<p>and in classifier.py, I am running</p>
<pre><code>nb_pipeline = Pipeline([
        ('NBCV',FeaturesSelection.w2v),
        ('nb_clf',MultinomialNB())])
</code></pre>
<p>However this is not working and I am getting this error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-14-07045943a69c&gt; in &lt;module&gt;
      2 nb_pipeline = Pipeline([
      3         ('NBCV',FeaturesSelection.w2v),
----&gt; 4         ('nb_clf',MultinomialNB())])

/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)
     71                           FutureWarning)
     72         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
---&gt; 73         return f(**kwargs)
     74     return inner_f
     75 

/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py in __init__(self, steps, memory, verbose)
    112         self.memory = memory
    113         self.verbose = verbose
--&gt; 114         self._validate_steps()
    115 
    116     def get_params(self, deep=True):

/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py in _validate_steps(self)
    160                                 &quot;transformers and implement fit and transform &quot;
    161                                 &quot;or be the string 'passthrough' &quot;
--&gt; 162                                 &quot;'%s' (type %s) doesn't&quot; % (t, type(t)))
    163 
    164         # We allow last estimator to be None as an identity transformation

TypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '{' ': array([-0.17019527,  0.32363772, -0.0770281 , -0.0278154 , -0.05182227, ....
</code></pre>
<p>I am using all the programs in that folder, so the code can be reproducible if you use them.</p>
<p>If you could explain me how to fix it and what other changes in the code would be necessary, it would be great. My goal is to compare models (naive bayes, random forest,...) with BoW, TF-IDF and Word2Vec.</p>
<p>Update:</p>
<p>After the answer below (from Ismail), I updated the code as follows:</p>
<pre><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec, size=100):
        self.word2vec = word2vec
        self.dim = size
</code></pre>
<p>and</p>
<pre><code>#building Linear SVM classfier
svm_pipeline = Pipeline([
        ('svmCV',FeaturesSelection_W2V.MeanEmbeddingVectorizer(FeaturesSelection_W2V.w2v)),
        ('svm_clf',svm.LinearSVC())
        ])

svm_pipeline.fit(DataPrep.train_news['Statement'], DataPrep.train_news['Label'])
predicted_svm = svm_pipeline.predict(DataPrep.test_news['Statement'])
np.mean(predicted_svm == DataPrep.test_news['Label'])
</code></pre>
<p>However, I am still getting errors.</p>
","python, scikit-learn, gensim, word2vec","<p>Step 1. MultinomialNB <code>FeaturesSelection.w2v</code> is a <code>dict</code> and it does not have <code>fit</code> or <code>fit_transform</code> functions. Also <code>MultinomialNB</code> needs non-negative values, so it doesn't work. So I decided to add a pre-processing stage to normalize negative values.</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.preprocessing import MinMaxScaler

nb_pipeline = Pipeline([
        ('NBCV',MeanEmbeddingVectorizer(FeatureSelection.w2v)),
        ('nb_norm', MinMaxScaler()),
        ('nb_clf',MultinomialNB())
    ])
</code></pre>
<p>... instead of</p>
<pre class=""lang-py prettyprint-override""><code>nb_pipeline = Pipeline([
        ('NBCV',FeatureSelection.w2v),
        ('nb_clf',MultinomialNB())
    ])
</code></pre>
<p>Step 2. I have got an error on <code>word2vec.itervalues().next()</code>. So I have decided to change dimension shape with predefined that is the same value of <code>Word2Vec</code>'s size.</p>
<pre class=""lang-py prettyprint-override""><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec, size=100):
        self.word2vec = word2vec
        self.dim = size
</code></pre>
<p>... instead of</p>
<pre class=""lang-py prettyprint-override""><code>class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.dim = len(word2vec.itervalues().next())
</code></pre>
",1,3,5732,2020-12-06 01:45:44,https://stackoverflow.com/questions/65163881/using-word2vec-in-scikit-learn-pipeline
SciSpacy equivalent of Gensim&#39;s functions/parameters,"<p>With Gensim, there are three functions I use regularly, for example this one:</p>
<pre><code>model = gensim.models.Word2Vec(corpus,size=100,min_count=5)
</code></pre>
<p>The output from gensim, but I cannot understand how to set the size and min_count parameters in the equivalent SciSpacy command of:</p>
<pre><code>model = spacy.load('en_core_web_md')
</code></pre>
<p>(The output is a model of embeddings (too big to add here))).</p>
<p>This is another command I regularly use:</p>
<pre><code>model.most_similar(positive=['car'])
</code></pre>
<p>and this is the output from gensim/Expected output from SciSpacy:</p>
<pre><code>[('vehicle', 0.7857330441474915),
 ('motorbike', 0.7572781443595886),
 ('train', 0.7457204461097717),
 ('honda', 0.7383008003234863),
 ('volkswagen', 0.7298516035079956),
 ('mini', 0.7158907651901245),
 ('drive', 0.7093928456306458),
 ('driving', 0.7084407806396484),
 ('road', 0.7001082897186279),
 ('traffic', 0.6991947889328003)]
</code></pre>
<p>This is the third command I regularly use:</p>
<pre><code>print(model.wv['car'])
</code></pre>
<p>Output from Gensim/Expected output from SciSpacy (in reality this vector is length 100):</p>
<pre><code>    [ 1.0942473   2.5680697  -0.43163642 -1.171171    1.8553845  -0.3164575
  1.3645878  -0.5003705   2.912658    3.099512    2.0184739  -1.2413547
  0.9156444  -0.08406237 -2.2248871   2.0038593   0.8751471   0.8953876
  0.2207374  -0.157277   -1.4984075   0.49289042 -0.01171476 -0.57937795...]
</code></pre>
<p>Could someone show me the equivalent commands for SciSpacy? For example, for 'gensim.models.Word2Vec' I can't find how to specify the length of the vectors (size parameter), or the minimum number of times the word should be in the corpus (min_count) in SciSpacy (e.g. I looked <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/allenai/scispacy"" rel=""nofollow noreferrer"">here</a>), but I'm not sure if I'm missing them?</p>
","python, nlp, spacy, gensim","<p>A possible way to achieve your goal would be to:</p>
<blockquote>
<ol>
<li>parse you documents via <code>nlp.pipe</code></li>
<li>collect all the words and pairwise similarities</li>
<li>process similarities to get the desired results</li>
</ol>
</blockquote>
<p>Let's prepare some data:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_md&quot;, disable = ['ner', 'tagger', 'parser'])
</code></pre>
<p>Then, to get a vector, like in <code>model.wv['car']</code> one would do:</p>
<pre><code>nlp(&quot;car&quot;).vector
</code></pre>
<p>To get most similar words like <code>model.most_similar(positive=['car'])</code> let's process the corpus:</p>
<pre><code>corpus = [&quot;This is a sentence about cars. This a sentence aboout train&quot;
          , &quot;And this is a sentence about a bike&quot;]
docs = nlp.pipe(corpus)

tokens = []
tokens_orth = []

for doc in docs:
    for tok in doc:
        if tok.orth_ not in tokens_orth:
            tokens.append(tok)
            tokens_orth.append(tok.orth_)
            
sims = np.zeros((len(tokens),len(tokens)))

for i, tok in enumerate(tokens):
    sims[i] = [tok.similarity(tok_) for tok_ in tokens]
</code></pre>
<p>Then to retrieve <code>top=3</code> most similar words:</p>
<pre><code>def most_similar(word, tokens_orth = tokens_orth, sims=sims, top=3):
    tokens_orth = np.array(tokens_orth)
    id_word = np.where(tokens_orth == word)[0][0]
    sim = sims[id_word]
    id_ms = np.argsort(sim)[:-top-1:-1]
    return list(zip(tokens_orth[id_ms], sim[id_ms]))


most_similar(&quot;This&quot;)
</code></pre>
<hr />
<pre><code>[('this', 1.0000001192092896), ('This', 1.0), ('is', 0.5970357656478882)]
</code></pre>
<hr />
<p>PS</p>
<p>I have also noticed you asked for specification of dimension and frequency. Embedding length is fixed at the time the model is initialized, so it can't be changed after that. You can start from a blank model if you wish so, and feed embeddings you're comfortable with. As for the frequency, it's doable, via counting all the words and throwing away anything that is below desired threshold. But again, underlying embeddings will be from a not filtered text. SpaCy is different from Gensim in that it uses readily available embeddings whereas Gensim trains them.</p>
",1,1,195,2020-12-08 11:52:07,https://stackoverflow.com/questions/65198394/scispacy-equivalent-of-gensims-functions-parameters
How to get a dump of all vectors from a gensim W2V model?,"<p>Using a <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">KeyedVectors</a> object, I can get the W2V vector, given a word, like so.</p>
<pre><code>from gensim.models import KeyedVectors

model = KeyedVectors.load('vectors.kv')
model.get_vector('example')  # output =&gt; [0.12, 0.41, ..., 0.92]
</code></pre>
<p>How can I do the same, for <em>every</em> term (key) contained in the model?</p>
<p><em>Note that this doesn't <em>have</em> to be a KeyedVectors object, it could alternatively be a <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Word2Vec</a> object.</em></p>
<p><strong>EDIT</strong> - thanks to gojomo:</p>
<pre><code>vector_dct = {}
for word in kv_model.index2word: 
    vector_dct[word] = kv_model.get_vector(word)

df = pd.DataFrame(vector_dct).T
</code></pre>
","python, vector, gensim, word2vec","<pre class=""lang-py prettyprint-override""><code>for word in kv_model.index_to_key:  # was kv_model.index2word pre-gensim-4.0.0, when Q 1st asked
    kv_model.get_vector(word)
</code></pre>
",2,1,1475,2020-12-12 14:56:30,https://stackoverflow.com/questions/65266342/how-to-get-a-dump-of-all-vectors-from-a-gensim-w2v-model
How to load and use word2vec model properly in a web-application via Flask RESTful APIs?,"<p>I built a small code to find analogies using word2vec and it runs fine as stand alone application. Here is the working code</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>import numpy as np

# Get the interactive Tools for Matplotlib
%matplotlib notebook


from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
import os
glove_file = os.path.abspath('glove.6B/glove.6B.100d.txt')
word2vec_glove_file = get_tmpfile(""glove.6B.100d.word2vec.txt"")
glove2word2vec(glove_file, word2vec_glove_file)
model = KeyedVectors.load_word2vec_format(word2vec_glove_file)
def analogy(x1, x2, y1):
    result = model.most_similar(positive=[y1, x2], negative=[x1])
    return result[0][0]
analogy('woman', 'queen', 'man')    </code></pre>
</div>
</div>
</p>
<p>Now, I plan to use flask to create a small web application, so that users can find analogies via the webpage. For this I have a basic question</p>
<ol>
<li>I assume I need to save the model and then load it when I start the server. Please correct me  I am I am wrong.</li>
</ol>
<p>Here is the code that using Flask, it is working, but can you please suggest if saving model is required here?
2. Any suggestions to improve this code are welcome!</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>import numpy as np



from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
import os 

from flask import Flask, request


app = Flask(__name__)
@app.route(""/"", methods=['GET'])
def welcome():
    return ""Welcome to our Machine Learning REST API!""
@app.route(""/analogy"", methods=['GET'])
def analogy_route():
    word1 = request.args.get(""word1"")
    word2 = request.args.get(""word2"")
    word3 = request.args.get(""word3"")
    result = model.most_similar(positive=[word3, word2], negative=[word1])
    return str(result[0][0])
if __name__ == ""__main__"":
    glove_file = os.path.abspath('glove.6B/glove.6B.100d.txt')
    word2vec_glove_file = get_tmpfile(""glove.6B.100d.word2vec.txt"")
    glove2word2vec(glove_file, word2vec_glove_file)

    model = KeyedVectors.load_word2vec_format(word2vec_glove_file)
    app.run(host='0.0.0.0', port=5000, debug=True)</code></pre>
</div>
</div>
</p>
<p><a href=""https://i.sstatic.net/H5g4j.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/H5g4j.png"" alt=""enter image description here"" /></a></p>
","flask, gensim, word2vec, flask-restful","<p>You probably don't want to be doing the GLoVe-to-word2vec format conversion, into a temporary file, every time you start your service. (It probably takes a noticeable amount of time, and may be filling a temp directory with redundant copies of the same data.)</p>
<p>Instead, perform the conversion only once, into a non-temporary location. Then, ignore the original <code>glove.6B.100d.txt</code> file entirely – it's no longer needed. Instead, just ensure the converted file is available to your web service in a stable location.</p>
<p>Very roughly, that means:</p>
<ol>
<li>Run once, anywhere:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>glove2word2vec('glove.6B/glove.6B.100d.txt', `glove.6B.100d.word2vec.txt`)
</code></pre>
<p>(Note that neither the use of <code>absfile()</code> for <code>get_tmpfile()</code> are strictly necessary – you can supply string paths directly to the <code>glove2word2vec()</code> function.)</p>
<ol start=""2"">
<li><p>Ensure that the new file <code>glove.6B.100d.word2vec.txt</code> is available in the working directory of your web service.</p>
</li>
<li><p>Have your web service's <code>__main__</code> branch just load the already-converted file, avoiding redundant repeated conversion work:</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>if __name__ == &quot;__main__&quot;:
    model = KeyedVectors.load_word2vec_format('glove.6B.100d.word2vec.txt')
    app.run(host='0.0.0.0', port=5000, debug=True)
</code></pre>
<p>(The exact path <code>'glove.6B.100d.word2vec.txt'</code> might be slightly different depending on where you choose to place the full file.)</p>
",2,0,607,2020-12-13 16:53:10,https://stackoverflow.com/questions/65278189/how-to-load-and-use-word2vec-model-properly-in-a-web-application-via-flask-restf
"Gensim error with .most_similar(), jupyter kernel restarting","<p>I cannot get the .most_similar() function to work. I have tried both Gensim 3.8.3 version and now am on the beta version 4.0 . I am working right off of the Word2Vec Model tutorial on each documentation version.</p>
<p>The code giving me error and restarting my kernel:</p>
<pre><code>print(wv.most_similar(positive=['car', 'minivan'], topn=5))
</code></pre>
<p>The above code is verbatim in both 3.8.3 documentation and 4.0. Following tutorials verbatim.</p>
<p>As stated in other stack overflow answers I have tried model.wv.most_similar()</p>
<p>I don't think .most_similar() is depreciated.</p>
<p>Additionally the .doesnt_match() function is not working.</p>
<p>EDIT in regards to gojomo:</p>
<p>Right now I am on Genism 3.8.3. I am using the GloVe Model and Word2Vec models, actually just tried it and it worked with the GloVe model, maybe the Word2Vec model is having a memory problem like gojomo suggested my code below:</p>
<p>I am using linx laptop, I-7 core 1065 cpu, memory 7.4 GiB, 64 bit ubuntu</p>
<pre><code>%matplotlib inline

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

import gensim.downloader as api
wv = api.load('word2vec-google-news-300')

for i, word in enumerate(wv.vocab):
    if i == 10:
        break
    print(word)

pairs = [
    ('programming', 'linux'),   
    ('programming', 'bicycle'), 
    ('programming', 'apple'),  
    ('programming', 'cereal'),    
    ('programming', 'capitalism'),
    ('programming', 'computers'), 
    ('programming', 'python'),  
    ('programming', 'algebra'),  
    ('programming', 'logic'),    
    ('programming', 'math'),
]
for w1, w2 in pairs:
    print('%r\t%r\t%.2f' % (w1, w2, wv.similarity(w1, w2)))

print(wv.most_similar(positive=['math'], topn=5))
</code></pre>
","python-3.x, machine-learning, nlp, gensim, word2vec","<p>If the Jupyter kernel is dying without a clear error message, you are likely running out of memory.</p>
<p>There may be more information logged to the console where you started the Jupyter server. If you expand you question to include any info there, as well as details about the model you've loaded (size on disk) and system you're running on (especially, RAM available), it may be possible to make other suggestions.</p>
<p>Also:</p>
<p>Whereas <code>gensim-3.8.3</code> requires a big new increment of RAM when the first <code>.most_similar()</code> call is made, the <code>gensim-4.0.0beta</code> pre-release only needs a much-smaller increment at that time - so it is far more likely that if a model succeeds in loading, you should also be able to get <code>.most_similar()</code> results. So it would also be useful to know:</p>
<ul>
<li>How did you install the <code>gensim-4.0.0beta</code>, and did you confirm that's the version actually used by your notebook kernel's environment?</li>
<li>Are you certain that the prior steps (such as loading) have succeeded, and that it's only &amp;  exactly the <code>most_similar()</code> that's triggering the failure? (Is it in a separate cell, and before attempting the <code>most_similar()</code> can you query other aspects of the model, such as its length or whether it contains certain words, successfully?)</li>
</ul>
",1,0,503,2020-12-17 02:21:44,https://stackoverflow.com/questions/65333831/gensim-error-with-most-similar-jupyter-kernel-restarting
What does build_vocab() do exactly?,"<p>I am trying to build a Doc2Vec model. I have a list of sentences with their labels, labeled using Gensim’s LabeledSentence() function. After building the model, I see that they used build_vocab() on the labeled sentences before training the model.</p>
<p>Can someone explain what does build_vocab() do and what happens if I don't use it !?</p>
<p>Please check out the following pictures:</p>
<p><a href=""https://i.sstatic.net/1OPNV.png"" rel=""nofollow noreferrer"">labeled sentences</a></p>
<p><a href=""https://i.sstatic.net/SsedC.png"" rel=""nofollow noreferrer"">model</a></p>
","python-3.x, nlp, gensim, doc2vec","<p>The <code>build_vocab()</code> step is how the model discovers the set of all possible words/doc-tags – and in the case of words, finds which words occur more than <code>min_count</code> times.</p>
<p>You have to use it: an attempt to train a model that hasn't gone through that discovery step will error.</p>
<p>(If you use the form of model-instantiation where you supply your corpus when creating the object, both <code>build_vocab()</code> and <code>train()</code> will be called automatically for you.)</p>
<p>Separately regarding your mention of <code>LabeledSentence</code>:</p>
<p>To stay up to date with preferred terminology/types, you should be using the <code>TaggedDocument</code> class. The individual training items are better described as documents than sentences, and within the model their whole-text doc-vector keys are called tags, not labels. (In some cases, they might also be the sort of labels used by classifiers, but not always, and most typically the tags are unique per-document IDs. So the term 'tag' is preferred in the code to discourage conflating these keys-for-doc-vectors with other things that might be 'labels.)</p>
<p>(The <code>LabeledSentence</code> class was an older name, and is now simply an alias to <code>TaggedDocument</code>. So using it as a name will work, but is mismatched with the rest of the code.)</p>
",2,2,2993,2020-12-18 00:58:05,https://stackoverflow.com/questions/65350423/what-does-build-vocab-do-exactly
training a Fasttext model,"<p>I want to train a Fasttext model in Python using the &quot;gensim&quot; library. First, I should tokenize each sentences to its words, hence converting each sentence to a list of words. Then, this list should be appended to a final list. Therefore, at the end, I will have a  nested list containing all tokenized sentences:</p>
<pre><code>word_punctuation_tokenizer = nltk.WordPunctTokenizer()
word_tokenized_corpus = []
for line in open('sentences.txt'):
   new = line.strip()
   new = word_punctuation_tokenizer.tokenize(new)
   if len(new) != 0:
       word_tokenized_corpus.append(new)
</code></pre>
<p>Then, the model should be built as the following:</p>
<pre><code>embedding_size = 60
window_size = 40
min_word = 5
down_sampling = 1e-2
ft_model = FastText(word_tokenized_corpus,
                  size=embedding_size,
                  window=window_size,
                  min_count=min_word,
                  sample=down_sampling,
                  sg=1,
                  iter=100)
</code></pre>
<p>However, the number of sentences in &quot;word_tokenized_corpus&quot; is very large and the program can't handle it. Is it possible that I train the model by giving each tokenized sentence to it one by one, such as the following:?</p>
<pre><code> for line in open('sentences.txt'):
  new = line.strip()
  new = word_punctuation_tokenizer.tokenize(new)
  if len(new) != 0:
   ft_model = FastText(new,
              size=embedding_size,
              window=window_size,
              min_count=min_word,
              sample=down_sampling,
              sg=1,
              iter=100)
</code></pre>
<p>Does this make any difference to the final results? Is it possible to train the model without having to build such a large list and keeping it in the memory?</p>
","python, gensim, fasttext","<p>Since the volume of the data is very high, it is better to convert the text file into a COR file. Then, read it in the following way:</p>
<pre><code>from gensim.test.utils import datapath
corpus_file = datapath('sentences.cor')
</code></pre>
<p>As for the next step:</p>
<pre><code>model = FastText(size=embedding_size,
                  window=window_size,
                  min_count=min_word,
                  sample=down_sampling,
                  sg=1,
                  iter=100)
model.build_vocab(corpus_file=corpus_file)
total_words = model.corpus_total_words
model.train(corpus_file=corpus_file, total_words=total_words, epochs=5)
</code></pre>
",2,0,2740,2020-12-19 11:22:01,https://stackoverflow.com/questions/65369269/training-a-fasttext-model
Deal with Out of vocabulary word with Gensim pretrained GloVe,"<p>I am working on an NLP assignment and loaded the GloVe vectors provided by Gensim:</p>
<pre><code>import gensim.downloader
glove_vectors = gensim.downloader.load('glove-twitter-25')
</code></pre>
<p>I am trying to get the word embedding for each word in a sentence, but some of them are not in the vocabulary.</p>
<p>What is the best way to deal with it working with the Gensim API?</p>
<p>Thanks!</p>
","nlp, stanford-nlp, gensim, word-embedding","<p>Load the <a href=""https://radimrehurek.com/gensim/downloader.html"" rel=""nofollow noreferrer"">model</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import gensim.downloader as api
model = api.load(&quot;glove-twitter-25&quot;)  # load glove vectors
# model.most_similar(&quot;cat&quot;)  # show words that similar to word 'cat'
</code></pre>
<p>There is a very simple way to find out if the words exist in the model's vocabulary.</p>
<pre class=""lang-py prettyprint-override""><code>result = print('Word exists') if word in model.wv.vocab else print('Word does not exist&quot;)
</code></pre>
<p>Apart from that, I had used the following logic to create sentence embedding (25 dim) with <strong>N</strong> tokens:</p>
<pre class=""lang-py prettyprint-override""><code>from __future__ import print_function, division
import os
import re
import sys
import regex
import numpy as np
from functools import partial

from fuzzywuzzy import process
from Levenshtein import ratio as lev_ratio

import gensim
import tempfile


def vocab_check(model, word):
    similar_words = model.most_similar(word)
    match_ratio = 0.
    match_word = ''
    for sim_word, sim_score in similar_words:
        ratio = lev_ratio(word, sim_word)
        if ratio &gt; match_ratio:
            match_word = sim_word
    if match_word == '':
        return similar_words[0][1]
    return model.similarity(word, match_word)


def sentence2vector(model, sent, dim=25):
    words = sent.split(' ')
    emb = [model[w.strip()] for w in words]
    weights = [1. if w in model.wv.vocab else vocab_check(model, w) for w in words]
    
    if len(emb) == 0:
        sent_vec = np.zeros(dim, dtype=np.float16)
    else:
        sent_vec = np.dot(weights, emb)

    sent_vec = sent_vec.astype(&quot;float16&quot;)
    return sent_vec   
</code></pre>
",2,2,3003,2020-12-19 16:35:40,https://stackoverflow.com/questions/65372032/deal-with-out-of-vocabulary-word-with-gensim-pretrained-glove
ValueError: invalid vector on line 440902 | while loading wiki.ar.vec using gensim.models.keyedvectors.word2vec() function,"<p>I'm trying to load wiki.ar.vec arabic word embedding file using word2vec function from gensim.</p>
<p>Below is the code use to load embedding file.</p>
<pre><code>import gensim.models.keyedvectors as word2vec 
print( &quot;Word Embedding is loading&quot;)
embedding = word2vec.KeyedVectors.load_word2vec_format('/home/user/Documents/wiki.ar.vec', binary=False)
print( &quot;Word Embedding is loaded&quot;)
</code></pre>
<p>Facing the Error describe in below screenshot:</p>
<p><a href=""https://i.sstatic.net/wZbVP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wZbVP.jpg"" alt=""enter image description here"" /></a></p>
<p>or any other way to load wiki.ar.vec embedding file?</p>
<p>Any suggestion and answers are highly appriciated.</p>
","python, arabic, gensim, word2vec, word-embedding","<p>This error indicates the file is not in the proper format, at that specified line/vector.</p>
<p>Where did the file come from? Are you sure it's a binary-format file of the right format?</p>
<p>Have you tried re-downloading the file to ensure it hasn't been corrupted or truncated?</p>
",0,0,277,2020-12-22 09:52:23,https://stackoverflow.com/questions/65406526/valueerror-invalid-vector-on-line-440902-while-loading-wiki-ar-vec-using-gens
Which trained embeddings vectors from Gensim (word2vec model) should be used for Tensorflow? Unnormalised or normalised ones?,"<p>I want to use the Gensim (word2vec model) trained vectors inside a neural network (Tensorflow). There are two kinds of weights I can use for this purpose. The first group is <code>model.syn0</code> and the second group is <code>model.vectors_norm</code> (after calling <code>model.init_sims(replace=True)</code>). The second one is the group of vectors we use for calculating similarity. Which one has the correct order (match with <code>model.wv.index2word</code> and <code>model.wv.vocab[X].index</code>) and weights for the embedding layer of a neural network?</p>
","tensorflow, keras, gensim, word2vec, word-embedding","<p>If you are using Google's<code>GoogleNews-vectors</code> as pretrained model you can use <code>model.syn0</code>. If you are using Facebook's <code>fastText</code> word embeddings you can directly load the binary file.<br />
Below are the example to load both the instances.</p>
<p><strong>Load GoogleNews pretrained embedding:</strong></p>
<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True,limit=500000) # To load the model first time.
model.wv.save_word2vec_format(model_path) #You can save the loaded model to binary file to load the model faster
model = gensim.models.KeyedVectors.load(model_path,mmap='r')
model.syn0norm = model.syn0
index2word_set = set(model.index2word)

model[word] gives the vector representation of the word which can be used to find similarity. 
</code></pre>
<p><strong>Load fastText pretrained embeddings:</strong></p>
<pre><code>import gensim
from gensim.models import FastText
model = FastText.load_fasttext_format('cc.en.300') # to load the model for first time.
model.save(&quot;fasttext_en_bin&quot;) # Save the model to binary file to load faster.
model = gensim.models.KeyedVectors.load(&quot;fasttext_en_bin&quot;,mmap=&quot;r&quot;)
index2word_set = set(model.index2word)

model[word] gives the vector representation of the word which can be used to find similarity. 
</code></pre>
<p>General example:</p>
<pre><code>if word in index2word:
   feature_vec = model[word]
</code></pre>
",2,1,966,2020-12-29 16:55:46,https://stackoverflow.com/questions/65495775/which-trained-embeddings-vectors-from-gensim-word2vec-model-should-be-used-for
Difference between &quot;alpha&quot; and &quot;start_alpha&quot;,"<p>I want to train a Word2Vec model using &quot;gensim&quot;. I want to determine the initial rating rate. However, it is written that both &quot;alpha&quot; and &quot;start_alpha&quot; parameters can be used to do so. What is the difference between them? Are they the same?</p>
","python, gensim, word2vec","<p>The parameter <code>alpha</code> is used in <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">the constructor</a>:</p>
<blockquote>
<p>classgensim.models.word2vec.Word2Vec(sentences=None, corpus_file=None,
vector_size=100, alpha=0.025, window=5, min_count=5,
max_vocab_size=None, sample=0.001, seed=1, workers=3,
min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75,
cbow_mean=1, hashfxn=, epochs=5, null_word=0,
trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False,
callbacks=(), comment=None, max_final_vocab=None)</p>
</blockquote>
<p>whereas the parameter <code>start_alpha</code> is used in the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.train"" rel=""nofollow noreferrer"">train</a> method:</p>
<blockquote>
<p>train(corpus_iterable=None, corpus_file=None, total_examples=None,
total_words=None, epochs=None, start_alpha=None, end_alpha=None,
word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False,
callbacks=(), **kwargs)</p>
</blockquote>
<p>As for which to use:</p>
<blockquote>
<p><code>start_alpha</code> (float, optional) – Initial learning rate. If supplied,
replaces the starting <code>alpha</code> from the constructor, for this one call
to<code>train()</code>. Use only if making multiple calls to train(), when you
want to manage the alpha learning-rate yourself (not recommended).</p>
</blockquote>
",0,0,172,2021-01-03 12:10:01,https://stackoverflow.com/questions/65549685/difference-between-alpha-and-start-alpha
Embedding multiword ngram phrases with PathLineSentences in gensim word2vec,"<p>I have around 82 gzipped files (around 180MB each and 14GB total) where each file contains new line separated sentences. I am thinking of using <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.PathLineSentences"" rel=""nofollow noreferrer"">PathLineSentences</a> from gensim Word2Vec to train word2vec model on the vocabularies. In that way <a href=""https://stackoverflow.com/questions/58925659/how-to-incrementally-train-a-word2vec-model-with-new-vocabularies"">I do not have to worry about taking all the sentences</a> list into the RAM.</p>
<p>Now I also wanted to get the embedding to include multiword phrases. But from the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#embeddings-with-multiword-ngrams"" rel=""nofollow noreferrer"">documentation</a>, it seems that I need to have an already trained phrase detector an all the sentences I have e.g.</p>
<pre><code>from gensim.models import Phrases
# Train a bigram detector.
bigram_transformer = Phrases(all_sentences)
# Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.
model = Word2Vec(bigram_transformer[all_sentences], min_count=1)
</code></pre>
<p>Now, I have two questions:</p>
<ol>
<li>Is there any way I can do the Phrase Detection while running the Word2Vec on top of each of the individual files in a streaming manner?</li>
<li>If not, is there any way I can do the initial phrase detection in the similar fashion of PathLineSentences, as in doing the phrase detection in a streaming manner?</li>
</ol>
","python, gensim, word2vec","<p>The Gensim <code>Phrases</code> class will accept data in the exact same form as <code>Word2Vec</code>: an iterable of all the tokenized texts.</p>
<p>You can provide that both as the initial training corpus, then as the corpus to be transformed into paired bigrams.</p>
<p>However, I would highly suggest that you <em>not</em> try to do the phrase-combinations in a simultaneous stream as feeding to <code>Word2Vec</code>, for both clarity and efficiency reasons.</p>
<p>Instead, do the transformation once, writing the results to a new, single corpus file. Then:</p>
<ul>
<li>you can easily review the results of the bigram-combinations</li>
<li>the pair-by-pair calculations that decide which words will be combined will be done only once, creating a simple corpus of space-delimited tokens. (Otherwise, each of the <code>epochs + 1</code> passes done by `Word2Vec will need to repeat the same calculations.)</li>
</ul>
<p>Roughly that'd look like:</p>
<pre class=""lang-py prettyprint-override""><code>with open('corpus.txt', 'w') as of:
    for phrased_sentence in bigram_transformer[all_sentences]:
        of.write(' '.join(phrased_sentence)
        of.write('\n')
</code></pre>
<p>(You could instead write to a gzipped file like <code>corpus.txt.gz</code> instead, using <code>GzipFile</code> or <code>smart_open</code>'s gzip functionality, if you'd like.)</p>
<p>Then the new file shows you exact data <code>Word2Vec</code> is operating on, and can be fed as a simple corpus - wrapped as an iterable with <code>LineSentence</code> or even passed using the <code>corpus_file</code> option that can better use more <code>workers</code> threads.</p>
",1,0,755,2021-01-05 04:14:57,https://stackoverflow.com/questions/65573173/embedding-multiword-ngram-phrases-with-pathlinesentences-in-gensim-word2vec
How do I subtract and add vectors with gensim KeyedVectors?,"<p>I need to <strong>add and subtract word vectors</strong>, for a project in which I use <strong><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">gensim.models.KeyedVectors</a></strong> (from the <code>word2vec-google-news-300</code> model)</p>
<p>Unfortunately, I've tried but can't manage to do it correctly.</p>
<p>Let's look at the poular example <em>queen ~= king - man + woman</em>.<br />
When I want to subtract <em>man</em> from <em>king</em> and add <em>woman</em>,<br />
I can do this with gensim by</p>
<pre class=""lang-py prettyprint-override""><code># model is loaded using gensim.models.KeyedVectors.load()
model.wv.most_similar(positive=[&quot;king&quot;, &quot;woman&quot;], negative=[&quot;man&quot;])[0]
</code></pre>
<p>which, as expected, returns <code>('queen', 0.7118192911148071)</code> for the model I use.</p>
<p>Now, to achieve the same with adding and subtracting vectors (all of them are unit-normed), I've tried the following code:</p>
<pre class=""lang-py prettyprint-override""><code> vec_king, vec_man, vec_woman = model.wv[&quot;king&quot;], model.wv[&quot;man&quot;], model.wv[&quot;woman&quot;]
 result = model.similar_by_vector(vec_king - vec_man + vec_woman)[0]
</code></pre>
<p><code>result</code> in the code above is <code>('king', 0.7992597222328186)</code> which is not what I'd expect.</p>
<p><em>What is my mistake?</em></p>
","python, nlp, gensim, word2vec, vector-space","<p>You're generally doing the right thing, but note:</p>
<ul>
<li><p>the <code>most_similar()</code> method also disqualifies from its results any of the named words provided - so even if <code>'king'</code> is (still) the closest word to the result, it will be ignored. Your formulation might very well have <code>'queen'</code> as the next-closest word, after ignoring the input words - which is all that the 'analogy' tests need.</p>
</li>
<li><p>the <code>most_similar()</code> method also does its vector-arithmetic on versions of the vectors that are <em>normalized to unit length</em>, which can result in slightly different answers. If you change your uses of <code>model.wv['king']</code> to <code>model.get_vector('king', norm=True)</code>, you'll get the unit-normed vectors instead.</p>
</li>
</ul>
<p>See also similar earlier answer: <a href=""https://stackoverflow.com/a/65065084/130288"">https://stackoverflow.com/a/65065084/130288</a></p>
",0,2,1496,2021-01-07 12:03:11,https://stackoverflow.com/questions/65612062/how-do-i-subtract-and-add-vectors-with-gensim-keyedvectors
How to prevent certain words from being included when building bigrams using Gensim&#39;s Phrases?,"<p>I am using Gensim's Phraser model to find bigrams in some reviews, to be later used in an LDA topic modelling scenario. My issue is that the reviews mention the word &quot;service&quot; quite often and so Phraser finds lots of different bigrams with &quot;service&quot; as one of the pairs (e.g &quot;helpful_service&quot;, &quot;good_service&quot;, &quot;service_price&quot;).</p>
<p>These are then present across multiple topics in the final result*. I'm thinking that I could prevent this from occurring if I was able to tell Phraser <em>not</em> to include &quot;service&quot; when making bigrams. Is this possible?</p>
<p>(*) I am aware that &quot;service&quot;-related bigrams being present across multiple topics might indeed be the optimal result, but I just want to experiment with leaving them out.</p>
<p>Sample code:</p>
<pre><code># import gensim models
from gensim.models import Phrases
from gensim.models.phrases import Phraser

# sample data
data = [
    &quot;Very quick service left a big tip&quot;,
    &quot;Very bad service left a complaint to the manager&quot;
]
data_words = [doc.split(&quot; &quot;) for doc in data]

# build the bigram model
bigram_phrases = Phrases(data_words, min_count=2, threshold=0, scoring='npmi') 
# note I used the arguments above to force &quot;service&quot; based bigrams to be created for this example
bigram_phraser = Phraser(bigram_phrases)

# print the result
for word in data_words:
    tokens_ = bigram_phraser[word]
    print(tokens_)
</code></pre>
<p>The above prints:</p>
<pre><code>['Very', 'quick', 'service_left', 'a', 'big', 'tip']
['Very', 'bad', 'service_left', 'complaint', 'to', 'the', 'manager']
</code></pre>
","python, nlp, gensim","<p><strong>Caution:</strong> The following behavior seems to change with version 4.0.0!</p>
<p>If you are indeed only working with bigrams, you can utilize the <code>common_terms={}</code> parameter of the function, which is (according to the <a href=""https://radimrehurek.com/gensim_3.8.3/models/phrases.html#gensim.models.phrases.Phrases"" rel=""nofollow noreferrer"">docs</a></p>
<blockquote>
<p>[a] list of “stop words” that won’t affect frequency count of expressions containing them. Allow to detect expressions like “bank_of_america” or “eye_of_the_beholder”.</p>
</blockquote>
<p>If I add a simple <code>common_terms={&quot;service&quot;}</code> to your sample code, I am left with tge following result:</p>
<pre><code>['Very', 'quick', 'service', 'left_a', 'big', 'tip']
['Very', 'bad', 'service', 'left_a', 'complaint', 'to', 'the', 'manager']
</code></pre>
<p>Starting with version 4.0.0, gensim seemingly dropped this parameter, but replaced it with <code>connector_words</code>), see <a href=""https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases"" rel=""nofollow noreferrer"">here</a>. The results should largely be the same, though!</p>
",0,1,320,2021-01-13 15:22:59,https://stackoverflow.com/questions/65704851/how-to-prevent-certain-words-from-being-included-when-building-bigrams-using-gen
Gensim word2vec training doesn&#39;t callback on batch end,"<p>I am interested in placing a callback on the Gensim word2vec model to trigger some function after each batch. Per <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">documentation</a>, it is possible to place a callback on batch end or epoch end. However, as shown in the MVE below, only the epoch callback actually triggers.</p>
<p>To run the sample, let <code>corpus_filepath</code> direct to a line separated file of unpunctuated sentences (words in a sentence on given a line should be space separated). You may also need to change <code>workers</code> in the <code>Word2Vec</code> instantiation.</p>
<pre><code>from gensim.models import Word2Vec
from gensim.models.callbacks import CallbackAny2Vec

corpus_filepath = 'train.txt'
out_filepath = 'out.txt'

class MyCallback(CallbackAny2Vec):
    def __init__(self):
        pass

    def on_batch_end(self, model):
        print('batch end')

    def on_epoch_end(self, model):
        print('epoch end')


callback = MyCallback()
model = Word2Vec(size=300, window=5, min_count=0, workers=64)
print('Making vocabulary...')
model.build_vocab(corpus_file=corpus_filepath)
print('Beginning training...')
model.train(corpus_file=corpus_filepath, epochs=5, total_words=model.corpus_total_words, callbacks=[callback])
</code></pre>
<p>Incorrect output (missing batch printouts):</p>
<pre><code>Making vocabulary...
Beginning training...
epoch end
epoch end
epoch end
epoch end
epoch end
</code></pre>
<p>What am I doing wrong?</p>
","python, machine-learning, gensim, word2vec","<p>Looking at the code, it appears the <code>on_batch_begin</code> and <code>on_batch_end</code> callbacks have not been implemented by Gensim in the <code>corpus_file</code> mode you're using.</p>
<p>Thus, you could try changing to the traditional corpus-iterable mode to see the callbacks fire. (Overall training throughput in that mode tends to max out with around 8-12 workers, no matter how many CPU cores are available.)</p>
<p>However, note also that even there, the per-batch callbacks are run at arbitrary times in multiple threads - so many things are unwise/unsafe to attempt in those callbacks. Attempted saves of the model, for example, could result in errors or other file corruption, and even purely informational output might be mixed from multiple threads or reflect inconsistent changing state. See <a href=""https://github.com/RaRe-Technologies/gensim/issues/2182"" rel=""nofollow noreferrer"">Gensim's open bug report #2181 for more details</a>. It's possible the <code>on_batch</code> callbacks are removed entirely due to this risk in an upcoming release.</p>
<p>So I'd recommend adapting your code to use some other approach – perhaps the <code>on_epoch</code> callbacks? – instead. What operation did you want to do in such frequent/simultaneous worker-thread callbacks?</p>
",1,0,751,2021-01-18 00:24:18,https://stackoverflow.com/questions/65767390/gensim-word2vec-training-doesnt-callback-on-batch-end
Add progress bar (verbose) when creating gensim dictionary,"<p>I want to create a <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html"" rel=""nofollow noreferrer"">gensim dictionary</a> from lines of a dataframe. The <code>df.preprocessed_text</code> is a list of words.</p>
<pre><code>from gensim.models.phrases import Phrases, Phraser
from gensim.corpora.dictionary import Dictionary


def create_dict(df, bigram=True, min_occ_token=3):

    token_ = df.preprocessed_text.values
    if not bigram:
        return Dictionary(token_)
    
    bigram = Phrases(token_,
                     min_count=3,
                     threshold=1,
                     delimiter=b' ')

    bigram_phraser = Phraser(bigram)

    bigram_token = []
    for sent in token_:
        bigram_token.append(bigram_phraser[sent])
    
    dictionary = Dictionary(bigram_token)
    dictionary.filter_extremes(no_above=0.8, no_below=min_occ_token)
    dictionary.compactify() 
    
    return dictionary
</code></pre>
<p>I couldn't find a progress bar option for it and the <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">callbacks</a> doesn't seem to work for it too. Since my corpus is huge, I really appreciate a way to show the progress. Is there any?</p>
","python, dictionary, text, progress-bar, gensim","<p>I'd recommend against changing <code>prune_at</code> for monitoring purposes, as it changes the behavior around which bigrams/words are remembered, possibly discarding many more than is strictly required for capping memory usage.</p>
<p>Wrapping <code>tqdm</code> around the iterables used (including the <code>token_</code> use in the <code>Phrases</code> constructor and the <code>bigram_token</code> use in the <code>Dictionary</code> constructor) should work.</p>
<p>Alternatively, enabling <code>INFO</code> or greater logging should display logging that, while not as pretty/accurate as a progress-bar, will give some indication of progress.</p>
<p>Further, if as shown in the code, the use of <code>bigram_token</code> is only to support the next <code>Dictionary</code>, it need not be created as a full in-memory <code>list</code>. You should be able to just use layered iterators to transform the text, &amp; tally the <code>Dictionary</code>, item-by-item. EG:</p>
<pre class=""lang-py prettyprint-override""><code>    # ...
    dictionary = Dictionary(tqdm(bigram_phraser[token_]))
    # ...
</code></pre>
<p>(Also, if you're only using the <code>Phraser</code> once, you may not be getting any benefit from creating it at all - it's an optional memory optimization for when you want to keep applying the same phrase-creation operation without the full overhead of the original <code>Phrases</code> survey object. But if the <code>Phrases</code> is still in-scope, and all of it will be discarded immediately after this step, it <em>might</em> be just as fast to use the <code>Phrases</code> object directly without ever taking a detour to create the <code>Phraser</code> - so give that a try.)</p>
",2,1,1854,2021-01-19 09:46:55,https://stackoverflow.com/questions/65788950/add-progress-bar-verbose-when-creating-gensim-dictionary
Did we update an existing gemsim model with our own data correctly?,"<p>Purpose: We are exploring the use of word2vec models in clustering our data. We are looking for the ideal model to fit our needs and have been playing with using (1) existing models offered via Spacy and Gensim (trained on internet data only), (2) creating our own custom models with Gensim (trained on our technical data only) and (3) now looking into creating hybrid models that add our technical data to existing models (trained on internet + our data).</p>
<p>Here is how we created our hybrid model of adding our data to an existing Gensim model:</p>
<pre><code>model = api.load(&quot;word2vec-google-news-300&quot;)
model = Word2Vec(size=300, min_count =1)
model.build_vocab(our_data)
model.train(our_data, total_examples=2, epochs =1)
model.wv.vocab
</code></pre>
<p>Question: Did we do this correctly in terms of our intentions of having a model that is trained on the internet and layered with our data?</p>
<p>Concerns: We are wondering if our data was really added to the model. When using the most similar function, we see really high correlations with more general words with this model. Our custom model has much lower correlations with more technical words. See output below.</p>
<pre><code>Most Similar results for 'Python'

This model (internet + our data):
'technicians' = .99
'system'      = .99
'working'     = .99

Custom model (just our data):
'scripting'   = .65
'perl'        = .63
'julia'       = .58
</code></pre>
","python, nlp, spacy, gensim, word2vec","<p>No: your code won't work for your intents.</p>
<p>When you execute the line...</p>
<pre><code>model = Word2Vec(size=300, min_count=1)
</code></pre>
<p>...you've created an all-new, empty <code>Word2Vec</code> object, assigning it into the <code>model</code> variable, which discards anything that's already there. So the prior-loaded data will have no effect. You're just training a new model on your (tiny) data.</p>
<p>Further, the object you had loaded isn't a full <code>Word2Vec</code> model. The 'GoogleNews' vectors that Google released back in 2013 are <em>only the vectors</em>, not a full model. There's no straightforward &amp; reliable way to keep training that object, as it is missing lots of information a real full model would have (including word-frequencies and the model's internal weights).</p>
<p>There are some advanced ways you could try to seed your own model with those values - but they involve lots of murky tradeoffs &amp; poorly-documented steps, in order for the end-results to have any value, compared to just training your own model on your own sufficient data. There's no officially-documented/supported way to do it in Gensim.</p>
",0,1,59,2021-01-19 17:55:53,https://stackoverflow.com/questions/65796905/did-we-update-an-existing-gemsim-model-with-our-own-data-correctly
LDA: topic model gensim gives same set of topics,"<p>Why am I getting same set of topics # words in gensim lda model? I used these parameters. I checked there are no duplicate documents in my corpus.</p>
<pre><code>lda_model = gensim.models.ldamodel.LdaModel(corpus=MY_CORPUS,
                                           id2word=WORD_AND_ID,
                                           num_topics=4, 
                                           minimum_probability=minimum_probability,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto', # symmetric, asymmetric
                                           per_word_topics=True)
</code></pre>
<h1>Results</h1>
<pre><code>[
(0, '0.004*lily + 0.01*rose + 0.00*jasmine'),
(1, '0.005*geometry + 0.07*algebra + 0.01*calculation'),
(2, '0.003*painting + 0.001*brush + 0.01*colors'),
(3, '0.005*geometry + 0.07*algebra + 0.01*calculation')
]
</code></pre>
<p>Notice: Topic #1 and #3 are identical.</p>
","python, nlp, gensim, lda, topic-modeling","<p>Each of the topics likely contains a large number of words weighted differently. When a topic is being displayed (e.g. using <code>lda_model.show_topics()</code>) you are going to get only a few words with the largest weights. This does not mean that there are no differences between topics among the remaining vocabulary.</p>
<p>You can steer the number of displayed words to inspect the remaining weights:</p>
<pre><code> show_topics(num_topics=4, num_words=10, log=False, formatted=True)
</code></pre>
<p>and change <code>num_words</code> parameter to include even more words.</p>
<p>Now, there is also a possibility that:</p>
<ul>
<li>the number of topics should be different (e.g. 3),</li>
<li>or <code>minimum_probability</code> smaller (what is the value you use?),</li>
<li>or number of <code>passes</code> larger,</li>
<li><code>chunksize</code> smaller,</li>
<li>corpus larger (what is the size?) or stripped off of stop words (did you do that?).</li>
</ul>
<p>I encourage you to experiment with different values of these parameters to check if any of the combination works better.</p>
",1,1,2091,2021-01-20 20:49:55,https://stackoverflow.com/questions/65817456/lda-topic-model-gensim-gives-same-set-of-topics
Does doc2vec model give accuracy on non-dictionary words?,"<p>I have sentences in corpus with mixed words (dictionary and non-dictionary words). Non-dictionary words are as important as they are domain specific. I'm not performing any nlp on non-dictionary words. Does doc2vec model compare non-dictionary words to same words in matching criteria?</p>
<p>Ex. I'm giving input ['AMDML','release']. Here AMDML is domain specific word. Will it match to same words if I've sentences in training model like ['AMDML','release','process'] or ['DML','release']. or only words like 'release' and 'process' is matched in most similar method?</p>
","python, gensim, doc2vec","<p><strong>I guess not;</strong></p>
<p>According to <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#introducing-paragraph-vector"" rel=""nofollow noreferrer"">radimrehurek-gensim</a> page which mentioned <a href=""https://cs.stanford.edu/%7Equocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">Le and Mikolov paper</a> (introducers of Doc2Vec algorithm), They refer to the Paragraph Vector model as Doc2Vec;</p>
<blockquote>
<p>In Gensim, we refer to the Paragraph Vector model as Doc2Vec. Which usually outperforms such simple-averaging of Word2Vec vectors.
The basic idea is: act as if a document has another floating word-like
vector, which contributes to all training predictions, and is updated
like other word-vectors, but we will call it a doc-vector. Gensim’s
Doc2Vec class implements this algorithm.</p>
</blockquote>
<p>So i guess Doc2Vec just follow Word2Vec model/algorithm; As i know if Word2Vec model for example has <strong><code>AMDML</code></strong> word on its training corpus it can generate a vector for it; Otherwise it has know idea about that and show something like <code>error: missing word</code> to you or at least returns padding/empty vector.</p>
<p>I think you need something like <a href=""https://fasttext.cc/"" rel=""nofollow noreferrer"">fasttext</a>; fasttext model always has vector for any words even if they don't exist on its traianing corpus; Unlike word2vec, fasttext can learn from n-gram character of words so you can find similar words by measuring their similarity values. After that for each sentence/doc averaging these similarities and find similar sentences/docs.</p>
",0,1,79,2021-01-23 07:02:50,https://stackoverflow.com/questions/65856634/does-doc2vec-model-give-accuracy-on-non-dictionary-words
gensim installation error on macs10.15.7 with 3.9.0,"<p>I tried to install gensim by <code>pip install gensim</code> but it fails with following pile of error on <strong>macs10.15.7 Catalina</strong> in <strong>python 3.9</strong></p>
<pre><code>      /Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/cmath:649:25: error: no template named 'numeric_limits'
      return _FloatBigger ? numeric_limits&lt;_IntT&gt;::max() :  (numeric_limits&lt;_IntT&gt;::max() &gt;&gt; _Bits &lt;&lt; _Bits);
                            ^
    fatal error: too many errors emitted, stopping now [-ferror-limit=]
    220 warnings and 20 errors generated.
    error: command '/usr/bin/clang' failed with exit code 1
    ----------------------------------------
ERROR: Command errored out with exit status 1: ~/.pyenv/versions/3.9.0/envs/jupyter/bin/python3.9 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/private/var/folders/pg/1drqvjn54tbczc1pl5qd8qwh0000gp/T/pip-install-ndvn1a9y/gensim_2b2eae30f7e140c0af90d98d9e598905/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/private/var/folders/pg/1drqvjn54tbczc1pl5qd8qwh0000gp/T/pip-install-ndvn1a9y/gensim_2b2eae30f7e140c0af90d98d9e598905/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /private/var/folders/pg/1drqvjn54tbczc1pl5qd8qwh0000gp/T/pip-record-_xbpvhdn/install-record.txt --single-version-externally-managed --compile --install-headers ~/.pyenv/versions/3.9.0/envs/jupyter/include/site/python3.9/gensim
</code></pre>
<p>I googled and could not find any solution for this. Could someone share some pointer</p>
","python, gensim","<p>According to of of the <a href=""https://pypi.org/project/gensim/"" rel=""nofollow noreferrer"">main gensim pages</a> it might not be ready for Python 3.9 yet.</p>
<p>&quot;Gensim is being continuously tested under Python 3.5, 3.6, 3.7 and 3.8. Support for Python 2.7 was dropped in gensim 4.0.0 – install gensim 3.8.3 if you must use Python 2.7&quot;</p>
<p>You can &quot;downgrade&quot; your Python version to 3.5/3.6/3.7 or 3.8 and it should work.</p>
",3,1,1758,2021-01-24 22:37:11,https://stackoverflow.com/questions/65876755/gensim-installation-error-on-macs10-15-7-with-3-9-0
Access dictionary in Python gensim topic model,"<p>I would like to see how to access dictionary from gensim lda topic model. This is particularly important when you train lda model, save and load it later on. In the other words, suppose lda_model is the model trained on a collection of documents. To get document-topic matrix one can do something like below or something like the one explained in <a href=""https://www.kdnuggets.com/2019/09/overview-topics-extraction-python-latent-dirichlet-allocation.html"" rel=""nofollow noreferrer"">https://www.kdnuggets.com/2019/09/overview-topics-extraction-python-latent-dirichlet-allocation.html</a>:</p>
<pre><code>def regTokenize(text):
    # tokenize the text into words
    import re
    WORD = re.compile(r'\w+')
    words = WORD.findall(text)
    return words

from gensim.corpora.dictionary import Dictionary
ttext = [regTokenize(d) for d in text]  
dic = Dictionary(ttext)
ttext = [dic.doc2bow(text) for text in ttext]
ttext = lda_model.get_document_topics(ttext)
</code></pre>
<p>However, dictionary in trained <code>lda_model</code> might be different from new data and gives error for the last line, like:</p>
<pre><code>&quot;IndexError: index 41021 is out of bounds for axis 1 with size 41021&quot;
</code></pre>
<p>Is there any way (or parameter) to obtain dictionary from trained <code>lda_model</code>, to use it instead of <code>dic = Dictionary(ttext)</code>? Your help and answer much appreciated!</p>
","python, dictionary, gensim, lda, topic-modeling","<p>The general approach should be to store the dictionary created while training the model to a file using <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.save"" rel=""nofollow noreferrer""><code>Dictionary.save</code></a> method and read it back for reuse using <a href=""https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.load"" rel=""nofollow noreferrer""><code>Dictionary.load</code></a>.</p>
<p>Only then <code>Dictionary.token2id</code> remain the same and can be used to map ids to words and vice-versa for a pretrained model.</p>
",1,2,772,2021-01-25 12:03:24,https://stackoverflow.com/questions/65884395/access-dictionary-in-python-gensim-topic-model
Does Gensim handling pad index and UNK index in W2V models?,"<p>I'm using Gensim for building W2V models and, I didn't find a way for adding a vector for Unkown words or padding parts in Gensim and, I have to do it manually.
I also check the index of 0 in the created embedding and, it is also used for a specific word. This matter could cause a problem for padding words because they have the same index.</p>
<p>Am I missing something in here? Is Gensim handle this problem?</p>
<p>P.S: For handling this issue, I always append two vectors in the model weights after I train the model.</p>
","python, gensim, word2vec","<p>A Gensim <code>Word2Vec</code> model only learns, and reports, vectors for words that it learned during training.</p>
<p>If you want it to learn some vector for any synthetic 'unknown' or 'padding' symbols, you need to include them in the training data. (They may not be very interesting/useful vector-values, though, and having such synthetic token vectors may not outperform simply ignoring unknown-tokens or avoiding artificial padding entirely.)</p>
",2,1,1264,2021-01-31 10:49:35,https://stackoverflow.com/questions/65978214/does-gensim-handling-pad-index-and-unk-index-in-w2v-models
How to load pre-trained glove model with gensim load_word2vec_format?,"<p>I am trying to load a pre-trained glove as a word2vec model in gensim. I have downloaded the glove file from <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">here</a>. I am using the following script:</p>
<pre><code>from gensim import models
model = models.KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=True)
</code></pre>
<p>but get the following error</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-38-e0b48b51f433&gt; in &lt;module&gt;()
      1 from gensim import models
----&gt; 2 model = models.KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=True)

2 frames
/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py in &lt;genexpr&gt;(.0)
    171     with utils.smart_open(fname) as fin:
    172         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 173         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    174         if limit:
    175             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: 'the'
</code></pre>
<p>What is the underlying problem? Does gensim need a specific format to be able to load it?</p>
","stanford-nlp, gensim, word2vec, word-embedding","<p>The GLoVe format is slightly different – missing a 1st-line declaration of vector-count &amp; dimensions – than the format that <code>load_word2vec_format()</code> supports.</p>
<p>There's a <code>glove2word2vec</code> utility script included you can run once to convert the file:</p>
<p><a href=""https://radimrehurek.com/gensim/scripts/glove2word2vec.html"" rel=""noreferrer"">https://radimrehurek.com/gensim/scripts/glove2word2vec.html</a></p>
<p>Also, starting in Gensim 4.0.0 (currentlyu in prerelease testing), the <code>load_word2vec_format()</code> method gets a new optional <code>no_header</code> parameter:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=load_word2vec_format#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=load_word2vec_format#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format</a></p>
<p>If set as <code>no_header=True</code>, the method will deduce the count/dimensions from a preliminary scan of the file - so it can read a GLoVe file with that option – but at the cost of two full-file reads instead of one. (So, you may still want to re-save the object with <code>.save_word2vec_format()</code>, or use the <code>glove2word2vec</code> script, to make future loads faster.)</p>
",7,4,8931,2021-02-03 04:08:58,https://stackoverflow.com/questions/66021131/how-to-load-pre-trained-glove-model-with-gensim-load-word2vec-format
Gensim returns &quot;ValueError: input must have more than one sentence&quot; in for loop through list of paragraphs,"<p>I'm trying to use gensim <code>summarize()</code> to simplify paragraphs in job descriptions.
I webscraped a bunch of job descriptions using the selenium package and stored them in a list.</p>
<pre><code>descriptions=[]
for link in job_urls:
    driver.get(link)
    jd = driver.find_element_by_xpath('//div[@id=&quot;jobDescriptionText&quot;]').text
    #The form element with attribute id set to jobDescriptionText
    descriptions.append(jd)
</code></pre>
<p><a href=""https://i.sstatic.net/aXRFc.jpg"" rel=""nofollow noreferrer"">The output is a list of text; each item is multiple paragraphs.  EX:</a></p>
<p>If I summarize item one at a time with an index, the code works.:</p>
<pre><code>    text = descriptions[2] # Change index to desired job description.
    summarize(str(text), ratio=0.5)
'The core function of this opening is to conduct regional studies and mapping.\nAs the successful candidate you would be expected to conduct regional exploration studies and evaluations of the petroleum system elements, and possess the experience to integrate geological and geophysical data to create regional maps.\nYou should have the aptitude for, and tireless energy around data mining and analysis, with high level computer mapping skills.\nMinimum Requirements\nYou will be required to perform the following:\nConduct regional exploration studies and evaluations of the petroleum system elements, and integrate available geological and geophysical data to create regional maps.\nDevelop gross depositional environment maps, effectiveness maps, common risk segment maps of all petroleum system elements (source, reservoir seal), and composite common risk segment maps of different plays, to develop new play concepts and exploration opportunities.\nAnalyze data mining with high level of computer mapping skills, using major Exploration software packages, preferably Petrel.'
</code></pre>
<p>But if I loop through the list, the function throws the ValueError:</p>
<pre><code>for text in descriptions:
    text = str(text)
    summarize(text, ratio=0.5)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-40-b3969fcb2610&gt; in &lt;module&gt;
      4 for text in descriptions:
      5     text = str(text)
----&gt; 6     summarize(text, ratio=0.5)

~\Anaconda3\lib\site-packages\gensim\summarization\summarizer.py in summarize(text, ratio, word_count, split)
    426     # If only one sentence is present, the function raises an error (Avoids ZeroDivisionError).
    427     if len(sentences) == 1:
--&gt; 428         raise ValueError(&quot;input must have more than one sentence&quot;)
    429 
    430     # Warns if the text is too short.
ValueError: input must have more than one sentence
</code></pre>
<p>And with a list comprehension:</p>
<pre><code>summary = [summarize(str(text),ratio=0.5) for text in descriptions]

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-31-8d79c7c19d53&gt; in &lt;module&gt;
      1 #text = descriptions[2] # Change index to desired job description.
      2 #summarize(str(text), ratio=0.5)
----&gt; 3 summary = [summarize(str(text),ratio=0.5) for text in descriptions]
      4 #for text in descriptions:
      5    # print(str(text)+&quot;\n&quot;)

&lt;ipython-input-31-8d79c7c19d53&gt; in &lt;listcomp&gt;(.0)
      1 #text = descriptions[2] # Change index to desired job description.
      2 #summarize(str(text), ratio=0.5)
----&gt; 3 summary = [summarize(str(text),ratio=0.5) for text in descriptions]
      4 #for text in descriptions:
      5    # print(str(text)+&quot;\n&quot;)

~\Anaconda3\lib\site-packages\gensim\summarization\summarizer.py in summarize(text, ratio, word_count, split)
    426     # If only one sentence is present, the function raises an error (Avoids ZeroDivisionError).
    427     if len(sentences) == 1:
--&gt; 428         raise ValueError(&quot;input must have more than one sentence&quot;)
    429 
    430     # Warns if the text is too short.

ValueError: input must have more than one sentence
</code></pre>
<p>The items are more than one sentence and <code>summarize()</code> works individually.  Why would <code>summarize()</code> throw this error in a loop or list comprehension?</p>
","python, python-3.x, selenium, for-loop, gensim","<p>As usual, a day later I figured it out.  As gojomo mentioned, there was a job description with only one sentence (who does that?).  I was able to catch it with some exploration of character length and watching the web driver webscrape.</p>
<p>To debug use:</p>
<pre><code>for i, text in enumerate(descriptions):
        try:
            summarize(text, ratio=0.5)
        except:
            print(&quot;Job description {} could not be summarized&quot;.format(i))
            continue
</code></pre>
<p>Lesson learned: don't assume all job descriptions have more than one sentence.</p>
",0,0,567,2021-02-03 16:21:47,https://stackoverflow.com/questions/66031545/gensim-returns-valueerror-input-must-have-more-than-one-sentence-in-for-loop
What does documents_columns paramter in Sparse2Corpus do?,"<p>I searched gensim.matutils.Dense2Corpus documentation but I do not find what does True/False value for documents_columns do.
Ex: gensim.matutils.Dense2Corpus(input, documents_columns=True)</p>
","python, nlp, gensim","<p>It represents your documents as columns <code>documents_columns=True</code> or rows <code>documents_columns=False</code>. From the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/matutils.py#L537"" rel=""nofollow noreferrer"">source code</a></p>
<pre><code>if documents_columns:
    self.dense = dense.T
else:
    self.dense = dense
</code></pre>
",0,0,187,2021-02-05 03:20:37,https://stackoverflow.com/questions/66057115/what-does-documents-columns-paramter-in-sparse2corpus-do
gensim - word2vec: AttributeError: &#39;Word2Vec&#39; object has no attribute &#39;most_common&#39;,"<pre><code>PAD = 0
UNK = 1
START = 2
END = 3
def make_vocab(wc, vocab_size):
    word2id, id2word = {}, {}
    word2id['&lt;pad&gt;'] = PAD
    word2id['&lt;unk&gt;'] = UNK
    word2id['&lt;start&gt;'] = START
    word2id['&lt;end&gt;'] = END
    for i, (w, _) in enumerate(wc.most_common(vocab_size), 4):
        word2id[w] = i
    return word2id
</code></pre>
<p>I got this error &quot;AttributeError: 'Word2Vec' object has no attribute 'most_common'&quot; when calling this function. I tried with different version of gensim. Could you give me some hints to  solve this.</p>
","python, nlp, gensim, word2vec","<p>Gensim's Word2Vec doesn't contain a <code>most_common</code> method.</p>
<p>If, for whatever reason you must extract <code>word,frequency</code> pairs from your model you can use</p>
<pre><code>[(word, wc.w2v.vocab[word]) for word in wc.wv.vocab]
</code></pre>
<p>and sort the resulting list. This is a decidedly strange use case, however.</p>
",1,0,1626,2021-02-14 16:56:32,https://stackoverflow.com/questions/66197779/gensim-word2vec-attributeerror-word2vec-object-has-no-attribute-most-comm
Minimum number of words in the vocabulary for Word2Vec models?,"<p>I have a corpus of short text(~5000 sentences) which forms a vocabulary of ~2000 words. I used Gensim to build a Word2Vec model, but the output from most_similar doesn't look reasonable. Is this because I don't have enough words in the vocabulary? If so, is there any rule of thumbs for the vocabulary size?</p>
","gensim, word2vec","<p>Generally word2vec needs a lot of data, with many varied examples of each word, for good word-vectors. You can sometimes squeeze some usefulness out of smaller datasets with:</p>
<ul>
<li>smaller vector-dimensionality; and/or</li>
<li>more training epochs</li>
</ul>
<p>(While I've not formally tested this, my hunch/rule-of-thumb is vector-dimensionality should be no more than the square-root of the count of unique words. So with only 2000 unique words, even a dimensionality of 50 is pushing it.)</p>
<p>You may be tempted to use a lower-than-default <code>min_count</code> so those words that only appear one or two times train vectors. But such vectors without varied usage examples will themselves be poor - dominated by those one or two not-broadly-representative contexts. Also, in aggregate all such &quot;noise&quot; words, interspersed with the words that <strong>do</strong> have enough examples, tend to make those other word-vectors worse. (Discarding words with too few examples usually <em>improves</em> the surviving words' vectors.)</p>
<p>If at all possible, get more training data, from a similar usage domain, to mix in with your data of primary interest.</p>
",1,0,1690,2021-02-18 20:39:04,https://stackoverflow.com/questions/66267818/minimum-number-of-words-in-the-vocabulary-for-word2vec-models
Can you provide additional tags for documents using TaggedLineDocument?,"<p>When training a doc2vec model using a corpus in the <code>TaggedDocument</code> class, you can provide a list of tags. When the doc2vec model is trained it learns a vector representation for the tags. For example you could have one tag representing the document, and another representing some classification that can be shared between documents.</p>
<p>How would one provide additional tags when streaming a corpus using <code>TaggedLineDocument</code>?</p>
","gensim, doc2vec","<p>The <code>TaggedLineDocument</code> class only considers documents to be one per line, with a single tag that is their line-number.</p>
<p>If you want more tags, you'll have to provide your own iterable which does that. It should only be a few lines of code, depending on where your other tags come from. You can use the source for <code>TaggedLineDocument</code> – which is itself only 9 lines of Python code –as a model to build on:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/e4199cb4e9a90df44ca59c1d0505b138caa21951/gensim/models/doc2vec.py#L1126"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/e4199cb4e9a90df44ca59c1d0505b138caa21951/gensim/models/doc2vec.py#L1126</a></p>
<p>Note: while supplying ore than one tag per document is a natural extension of the original 'Paragraph Vectors' approach, and often can provide benefits, sometimes it also 'dilutes' the salience of each tag's vector – which will be a special concern as the average number of tags per document grows, or the model acquires many more tags than unique documents. So be sure to comparatively evaluate whether any multiple-tag strategy is helping or hurting, in different modes, and whether things like pre-known categories work better as extra tags or known-labels for some later steps.</p>
",1,0,188,2021-02-24 17:24:33,https://stackoverflow.com/questions/66355819/can-you-provide-additional-tags-for-documents-using-taggedlinedocument
How to change Topic list (from gensim lda get_document_topics()) to a DataFrame format,"<p>I have performed some topic modelling using <em>gensim.models.ldamodel.LdaModel()</em> and I want to label my data, to visualize my findings.</p>
<p><strong>This is what I have so far:</strong></p>
<p>My current dataframe has the following columns:</p>
<pre><code>['text']['date']['gender']['tokens']['topics']['main_topic']
    
</code></pre>
<p>Text is just the pure textdata, date has the form (yyyy-mm-dd), gender is binary with female being 1, tokens is the text after preprocessing, topics is derived from:</p>
<pre><code>df['topics'] = LDA_model.get_document_topics(corpus)
</code></pre>
<p>and main_topic is a little change from the second answer from this <a href=""https://stackoverflow.com/questions/39969919/gensim-lda-topic-assignment"">post</a> and is populated like this:</p>
<pre><code>df['main_topic'] = [int(str(sorted(LDA_model[i],reverse=True,key=lambda x: x[1])[0][0]).zfill(3)) for i in corpus]
</code></pre>
<p>Finally, the first 10 rows of topics and main_topics look like this (notice that num_topics=30):</p>
<pre><code>    topics  main_topic
[(0, 0.051341455), (1, 0.21204428), (2, 0.1145254), (4, 0.055585753), (11, 0.20260869), (29, 0.25616828)]   29
[(0, 0.052005265), (1, 0.21128647), (2, 0.08015486), (3, 0.11465485), (29, 0.4478401)]  29
[(0, 0.05355798), (1, 0.1394092), (2, 0.10734849), (4, 0.32699445), (29, 0.273105)] 4
[(0, 0.053568278), (1, 0.22299954), (2, 0.22616898), (11, 0.0959242), (29, 0.2897638)]  29
[(0, 0.05404401), (1, 0.4482777), (4, 0.141311), (29, 0.24849494)]  1
[(0, 0.054245334), (1, 0.18933308), (2, 0.14567153), (4, 0.11169399), (23, 0.05768766), (29, 0.35825193)]   29
[(0, 0.05449035), (2, 0.114870586), (4, 0.13284092), (11, 0.075592585), (23, 0.13247918), (24, 0.06598773), (29, 0.32016253)]   29
[(0, 0.055871632), (1, 0.23100668), (4, 0.06832383), (29, 0.4730603)]   29
[(0, 0.057746172), (1, 0.057121024), (2, 0.07247137), (3, 0.26388222), (13, 0.07291462), (29, 0.34331965)]  29
[(0, 0.057841185), (1, 0.19891246), (2, 0.09586754), (29, 0.5344914)]   29
</code></pre>
<p><strong>Now what I want is:</strong></p>
<p>I want 30 new columns: &quot;topic 0, topic 1, topic 2,..., topic 29&quot;. And for the first row I want to use df['topics'] and save the values in the new columns so that:</p>
<p>topic 0 in row 1 = 0.0513414, topic 1 in row 1 = 0.21204, topic 2 in row 1 = 0.11452 and topic 3 in row 1 = 0, and so on.</p>
<p>But I dont know how. Can someone help?</p>
","python, pandas, gensim, lda","<p>I figured it out. If someone is looking to achieve the same thing:</p>
<pre><code>LDA_model = gensim.models.ldamodel.LdaModel()
dir(gensim.models.ldamodel.LdaModel)

df['topics'] = LDA_model.get_document_topics(corpus)

sf = pd.DataFrame(data=df['topics'])
af = pd.DataFrame()

for i in range(30):
    af[str(i)]=[]

frames = [sf,af]
af = pd.concat(frames).fillna(0)

for i in range(6301):
    for j in range(len(df['topics'][i])):
        af[str(df['topics'][i][j][0])].loc[i] = df['topics'][i][j][1]
</code></pre>
<p>(<em>notice that <strong>30</strong> is my <strong>num_topics</strong> and <strong>6301</strong> is my <strong>number of rows in df['topics']</strong></em>)</p>
<p>Now the dataframe <strong>af</strong> is looking like this [<em>restrained to 5 rows &amp; 5 columns</em>]:</p>
<pre><code>    topics  0   1   2   3
0   [(1, 0.055395175), (5, 0.0647138), (7, 0.13507782), (9, 0.055264555), (13, 0.19258575), (21, 0.05181323), (27, 0.07139948)] 0.0 0.05539517477154732 0.0 0.0
1   [(0, 0.052290276), (6, 0.064590134), (13, 0.24019116), (16, 0.07827738), (27, 0.0994899)]   0.05229027569293976 0.0 0.0 0.0
2   [(6, 0.054943837), (7, 0.07324204), (10, 0.052613333), (12, 0.12482096), (27, 0.19818054), (29, 0.06280263)]    0.0 0.0 0.0 0.0
3   [(4, 0.12759669), (8, 0.06937062), (10, 0.2261674), (16, 0.066699274), (24, 0.06150386), (27, 0.096883684)] 0.0 0.0 0.0 0.0
4   [(2, 0.09043305), (8, 0.15643781), (10, 0.13145259), (16, 0.064689845), (17, 0.05019963), (24, 0.09253424), (28, 0.10176642)]   0.0 0.0 0.09043305367231369 0.0
</code></pre>
",1,1,1248,2021-02-27 21:26:18,https://stackoverflow.com/questions/66403628/how-to-change-topic-list-from-gensim-lda-get-document-topics-to-a-dataframe
Memory Error in Python using gensim.utils.simple_preprocess,"<p>I am a beginner with gensim word2vec, and I am encountering a memory error when preparing text for training the model. I am using Python 3.8.8. I have about 900,000 text files in 12 different folders. I was thinking I should send all text documents through gensim.utils.simple_preprocess, and then I'd have a list of lists for the model. After going through about 150,000 documents, I received a memory error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;word2vec_part1.py&quot;, line 58, in &lt;module&gt;
    documents = list(read_input(paths))
  File &quot;word2vec_part1.py&quot;, line 39, in read_input
    myfile = infile.read()
MemoryError
</code></pre>
<p>Is there a way to fix this memory issue? I included the code I am using below. I am new to Python, word2vec, and stackoverflow, so I apologize if my question is poorly worded or if this is a dumb question! Thank you for your time!</p>
<pre><code># imports and logging

import gensim 
import logging
import os
import os.path
import glob


logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# define function

def read_input(inputs):

    # logging info
    logging.info(&quot;reading files&quot;)

    # set working directories and load files into a list
    for path in inputs:
        os.chdir(path)
        read_files=glob.glob(&quot;*.txt&quot;) 
        # preprocess and counting
        for i, file in enumerate(read_files):
            if(i%10000==0):
                logging.info(&quot;read {0} reviews&quot;.format(i))
            # preprocessing and return a list of words
            with open(file, &quot;rb&quot;) as infile:
                myfile = infile.read()
                yield gensim.utils.simple_preprocess(myfile)


# create a list of all file paths
paths = [#here is a list of file paths]

# call function
documents = list(read_input(paths))
logging.info(&quot;done reading files!!&quot;)
print(len(documents))
print(documents[1])

# training word2vec model
model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2)
model.train(documents,total_examples=len(documents),epochs=10)
model.save(&quot;word2vec.model&quot;)

# look up top 6 words similar to 'law'
w1 = [&quot;law&quot;]
model.wv.most_similar (positive=w1,topn=6)

logging.info(&quot;done!!!&quot;)
</code></pre>
","python, memory, gensim, word2vec","<p>It appears that attempting to hold all the documents in a <code>list</code> in memory requires more RAM than your system has. (Perhaps also: one of the files is gigantic. What's the largest single file?)</p>
<p>It's not necessary to hold all docs in memory. Gensim's <code>Word2Vec</code> (&amp; other algorithms) can almost always accept any Python <em>iterable</em> object - one that can iterate over its contents one-by-one, repeatedly, even if they're coming from some other back-end. This typically uses far less RAM.</p>
<p>The leader of the Gensim project has a useful post about iterables that could help you adapt your <code>read_input()</code> function into a wrapper class that can re-iterate over the files repeatedly:</p>
<p><a href=""https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/"" rel=""nofollow noreferrer"">https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/</a></p>
<p>Two other notes:</p>
<p>(1) <code>simple_preprocess()</code> isn't especially sophisticated, and you may want to do your own tokenization instead; but:</p>
<p>(2) If you re-tokenize on every iteration, especially if your tokenization does anything sophisticated or uses regular-expressions, you're doing a lot of redundant re-tokenizing of the same texts, which is likely to be a bottleneck in your training. So in fact you might want to just use your <code>read_input()</code> not to stuff all rokenized docs into a list, but write them to a new file, post-tokenization, one-document to a line and all tokens separated by single spaces. Then, a utility class like Gensim's <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence"" rel=""nofollow noreferrer""><code>LineSentence</code></a> can provide the iterable-wrapper for feeding that almost-fully-ready file to <code>Word2Vec</code>.</p>
",0,0,375,2021-03-02 23:21:20,https://stackoverflow.com/questions/66448514/memory-error-in-python-using-gensim-utils-simple-preprocess
what does &#39;corpus_count&#39; in gensim word2vec?,"<p>I want to train my word Embedding from scratch and I use gensim.models.word2vec as my model.
My corpus is so large that I can not read it at once , so I divide my corpus file into <strong>many parts</strong> and train my model iteratively。I find this is helpful:</p>
<pre><code>train(corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)
</code></pre>
<h2>I confused about the parameter &quot;total_words&quot; .
Is it means total words of all my corpus or the part corpus trained now?</h2>
<p>UPDATE:</p>
<p>my code is like this:</p>
<pre><code>model =  gensim.models.word2vec.Word2Vec.load(init_model)  
for i in range(parts):
    model.build_vocab(corpus_file=this_part_file_name, update=True)
    model.train(corpus_file = this_part_file_name, 
                   total_words=word_count(this_part_file_name) )

</code></pre>
<p>Should the parameter total_words be <code>word_count(this_part_file_name)</code> or <code>word_count(ALL_my_corpus_file)</code> ?</p>
","nlp, gensim, word2vec","<p><code>total_words</code> is the count of all raw words in the sentences in the corpus. You only have to provide one of the two: <code>total_examples</code> or <code>total_words</code>. If you ran <code>build_vocab()</code>, you may get the value for total words from <code>model.corpus_total_words</code>.
There is another count - <code>word_count</code> that refers to the count of words that are already trained. You can set this to 0 if you want to train on all the words, but this is optional.</p>
<p>More info: <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html</a></p>
",1,0,1464,2021-03-07 15:09:42,https://stackoverflow.com/questions/66517974/what-does-corpus-count-in-gensim-word2vec
pass from a model of type gensim.models.keyedvectors.Word2VecKeyedVectors to a model of type gensim.models.word2vec.Word2Vec,"<p>I downloaded a word embedding already train in &quot;glove.txt&quot; format
I imported it in as a model of type gensim.models.keyedvectors.Word2VecKeyedVectors thanks to this documentation :</p>
<p><a href=""https://radimrehurek.com/gensim/scripts/glove2word2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/scripts/glove2word2vec.html</a></p>
<p>But I would like a model of type gensim.models.word2vec.Word2Vec</p>
<p>Will there be a way to convert it or import it directly into the desired format?</p>
","python, deep-learning, nlp, gensim, word-embedding","<p>A set of word-vectors isn't enough to create a full <code>Word2Vec</code> algorithm model, which includes a lot more information from training, including extra internal model weights &amp; word-frequencies. (The word-vectors alone are less than half the state of the model.)</p>
<p>Why do you want a full model rather than just the vectors?</p>
<p>Can you train your own model from text data that's the same as, or similar is size/value to, the text used for creating the <code>glove.txt</code> word-vectors?</p>
",1,0,583,2021-03-09 20:58:10,https://stackoverflow.com/questions/66554689/pass-from-a-model-of-type-gensim-models-keyedvectors-word2veckeyedvectors-to-a-m
gensim LDA training,"<p>I am working with gensim LDA model for a project. I cant seem to find a proper number of topics. My question is, just to be sure, every time I train the model it re-starts, right?
For example, I try it out with 47 topics, terrible results; so then I go back to the cell and change 47 to 80 topics and run it again. It completely starts a new training and erases what it has learned with the 47 topics, right?</p>
<p>I am having terrible results with LDA, similarity comes to 100% or 0% and I am having trouble parameter tuning. LSI has given me excellent results.
Thanks!</p>
","python, nlp, gensim, lda","<p>Yes, every time you train LDA, it forgets what it has learned so far.</p>
<p>Some suggestions and comments that may help you to get better results:</p>
<ul>
<li>Make sure that you've preprocessed the text appropriately. This usually includes removing punctuation and numbers, removing stopwords and words that are too frequent or rare, (optionally) lemmatizing the text. Preprocessing is dependent on the language and the domain of the texts.</li>
<li>About the hyperparameters, you can use the &quot;auto&quot; mode for alpha and beta, letting the model learn the best values of alpha and beta. If you want to fix them, usually values lower than 1 are suggested. <a href=""https://datascience.stackexchange.com/questions/199/what-does-the-alpha-and-beta-hyperparameters-contribute-to-in-latent-dirichlet-a"">Check this</a></li>
<li>LDA is a probabilistic model, which means that if you re-train it with the same hyperparameters, you will get different results each time.</li>
</ul>
",1,0,213,2021-03-11 22:25:55,https://stackoverflow.com/questions/66591464/gensim-lda-training
Should I split sentences in a document for Doc2Vec?,"<p>I am building a Doc2Vec model with 1000 documents using Gensim.
Each document has consisted of several sentences which include multiple words.</p>
<p>Example)</p>
<p>Doc1: [[word1, word2, word3], [word4, word5, word6, word7],[word8, word9, word10]]</p>
<p>Doc2: [[word7, word3, word1, word2], [word1, word5, word6, word10]]</p>
<p>Initially, to train the Doc2Vec, I first split sentences and tag each sentence with the same document tag using  &quot;TaggedDocument&quot;. As a result, I got the final training input for Doc2Vec as follows:</p>
<p>TaggedDocument(words=[word1, word2, word3], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word4, word5, word6, word7], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word8, word9, word10], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word7, word3, word1, word2], tags=['Doc2'])</p>
<p>TaggedDocument(words=[word1, word5, word6, word10], tags=['Doc2'])</p>
<p>However, would it be okay to train the model with the document as a whole without splitting sentences?</p>
<p>TaggedDocument(words=[word1, word2, word3,word4, word5, word6, word7,word8, word9, word10], tags=['Doc1'])</p>
<p>TaggedDocument(words=[word4, word5, word6, word7,word1, word5, word6, word10], tags=['Doc2'])</p>
<p>Thank you in advance :)</p>
","gensim, word2vec, doc2vec","<p>Both approaches are going to be very similar in their effect.</p>
<p>The slight difference is that in PV-DM modes (<code>dm=1</code>), or PV-DBOW with added skip-gram training (<code>dm=0, dbow_words=1</code>), if you split by sentence, words in different sentences will never be within the same context-window.</p>
<p>For example, your <code>'Doc1'</code> words <code>'word3'</code> and <code>'word4'</code> would never be averaged-together in the same PV-DM context-window-average, nor be used to PV-DBOW skip-gram predict-each-other, if you split by sentences. If you just run the whole doc's words together into a single <code>TaggedDocument</code> example, they would interact more, via appearing in shared context-windows.</p>
<p>Whether one or the other is better for your purposes is something you'd have to evaluate in your own analysis - it could depend a lot on the nature of the data &amp; desired similarity results.</p>
<p>But, I can say that your <em>second</em> option, all the words in one <code>TaggedDocument</code>, is the more common/traditional approach.</p>
<p>(That is, as long as the document is still no more than 10,000 tokens long. If longer, splitting the doc's words into multiple <code>TaggedDocument</code> instances, each with the same <code>tags</code>, is a common workaround for an internal 10,000-token implementation limit.)</p>
",2,2,384,2021-03-17 02:04:41,https://stackoverflow.com/questions/66665981/should-i-split-sentences-in-a-document-for-doc2vec
Can&#39;t upgrade Gensim library to version 4 in anaconda,"<p>I need to upgrade to the newest Gensim version. I ran <code>pip install --upgrade gensim</code> and I get <code>Requirement already satisfied.</code></p>
<p>Then:</p>
<pre><code>Python 3.8.1 (default, Jan  8 2020, 22:29:32) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import gensim
&gt;&gt;&gt; print(gensim.__version__)
3.8.3
</code></pre>
<p>I am running the pip install inside my conda environment. Any ideas?</p>
","python, python-3.x, anaconda, gensim","<p>if you take a look at the release history <a href=""https://pypi.org/project/gensim/#history"" rel=""nofollow noreferrer"">here</a>, you will see that gensim version 4 is not yet relreased. However, you can upgrade to the pre release version using the <code>--pre</code> flag
so you can upgrade to gensim v4 using</p>
<pre><code>pip install  --upgrade gensim --pre
</code></pre>
",2,0,989,2021-03-24 11:26:44,https://stackoverflow.com/questions/66780051/cant-upgrade-gensim-library-to-version-4-in-anaconda
Which service to run doc2vec on AWS?,"<p>I would like to find the best hyperparameters for my model, but tuning 6 metaparameters over a total of 486 permutations and 200k documents takes a while. That's why I'm thinking about using the free credits on AWS. Ideally I want to run my script and get a .csv file as ouput.</p>
<pre><code>vector_size = [100, 200, 300]
window = [2, 5, 10]
epochs = [10, 20, 30]
count =[2, 5, 10] 
dm = [0,1]
sample = [10e-4, 10e-5, 10e-6 ]
</code></pre>
<p>The problem is that I've never used AWS and the amount of different services is overwhelming. Can you guys give me a hint which service is suitable for my problem?</p>
","python, amazon-web-services, gensim, doc2vec","<p>EC2 is one of the original core services that gives you a virtual system in the cloud, with a variety of CPU/RAM options, to run anything you want. You could, with effort, fire up 468 nodes to train &amp; evaluate each model in parallel, saving aside the results, shutting down each node as soon as its run finishes.</p>
<p>(There might be a newer higher-level service which offers some other sort of assistance with job-management, but EC2 is the original generic node-in-the-cloud.)</p>
<p>Another thought for your meta-optimization:</p>
<p>Overdoing <code>epochs</code> shouldn't ever hurt - it'll just be wasteful. So you could just do the big test with your largest value, <code>epochs=30</code>, and be fairly confident that the other parameters that are best, with that maxed value, won't improve much with fewer <code>epochs</code>.</p>
<p>(But, especially if you need to re-run the job often, 30 might only be marginally better than some smaller epochs count - so you could then separately run a test to balance time/cost and evaluation quality.)</p>
",1,0,153,2021-03-25 22:56:33,https://stackoverflow.com/questions/66808720/which-service-to-run-doc2vec-on-aws
How to get back hyperparameters from a trained world2vec model gensim?,"<p>I have a trained word2vec model which I need to train further with more data. I want to use the same hyperparameters that is used while training the model for the new model as well. But I don't want to hardcode it. Is there a method which I can use to get the hyperparameters used while training the existing model.
I am using Gensim word2vec.</p>
","python, gensim, word2vec, hyperparameters","<p>Any full <code>Word2Vec</code> model has every metaparameter that was supplied at its initial creation somewhere in its object properties.</p>
<p>It's almost always on the model itself, using the exact same name as was used for the constructor parameters. So, <code>model.window</code> will return the <code>window</code>, etc - and thus you can just create a new model pulling each value from the old model.</p>
<p>Note that continuing training on an already-trained model involves a lot of thorny tradeoffs.</p>
<p>For example, the <code>.build_vocab(..., update=True)</code> on an existing model won't be applying <code>min_count</code> consistently against all word totals from all prior calls, but only those in the latest 'batch'.</p>
<p>The proper learning-rate (<code>alpha</code> to <code>min_alpha</code> values) for incremental updates isn't well-defined by theory or rules-of-thumb. And if the vocabulary &amp; word-balance in the new texts mainly train some words, not all, those recently-updated words can be pulled arbitrarily out of strong-comparability-alignment with earlier words that didn't get more training. (The underlying method of mdoel optimization, stochastic gradient descent, is best-grounded when all training texts are given equal training attention, without any subset begin intensely trained later.)</p>
",1,0,586,2021-03-26 09:05:13,https://stackoverflow.com/questions/66813857/how-to-get-back-hyperparameters-from-a-trained-world2vec-model-gensim
Scale cosine distance to 0-1 using Gensim,"<p>I've built a Doc2Vec model with around 3M documents, now I want to compare it to another model I've previously built. The second model has been scaled to 0-1 so I now also want to scale the gensim model to the same range so that they are comparable.
This is my first time using gensim so I'm not sure how this is done. It's nothing fancy but this is the code I have so far (model generation code ommited). I thought about scaling (minmax scaling with max/min in the union of  vectors) the inferred vectors (v1 and v2) but I don't think this would be correct approach.
The idea here is to compare two documents (with tokens likely to be in the corpus) and output a similarity score between them. I've seen a few Gensim's tutorials and they often compare a single string to the corpus' documents, which is not really the idea here.</p>
<blockquote>
<pre><code> def get_similarity_score(self,string_1, string_2):
    split_tokens1 = string_1.split()
    split_tokens2 = string_2.split()
    v1 = self.model.infer_vector(split_tokens1)
    v2 = self.model.infer_vector(split_tokens2)
    text_score = nltk.cluster.util.cosine_distance(v1, v2)
    return text_score
</code></pre>
</blockquote>
<p>Any recommendations?</p>
","python, math, nlp, text-mining, gensim","<p>Note that 'cosine similarity' &amp; 'cosine distance' are different things.</p>
<p>A cosine-similarity can range from <code>-1.0</code> to <code>1.0</code> – but in some models, such as those based only on positive word counts, you might only practically see values from <code>0.0</code> to <code>1.0</code>. But in both cases, items with similarities close to <code>1.0</code> are <em>most-similar</em>.</p>
<p>On the other hand, a cosine-distance can range from <code>0.0</code> to <code>2.0</code>, and items with a distance of <code>0.0</code> are <em>least-distant</em> (or <em>nearest</em>). A cosine-distance can be larger than <code>1.0</code> - but you might only see such distances in models which use the dense coordinate space (like <code>Doc2Vec</code>), not in word-count models which leave half the coordinate space empty (all negative coordinates).</p>
<p>So: you shouldn't really be calling your function <code>similarity</code> if it's returning a distance, and if it's now returning surprise numbers over <code>1.0</code>, there's nothing wrong: that's possible in some models, but not others.</p>
<p>You could naively rescale the <code>0.0</code> to <code>2.0</code> distances that your calculation will get with <code>Doc2Vec</code> vectors, with some crude hammer like:</p>
<pre><code>new_distance = old_distance / 2
</code></pre>
<p>However, note that in general, the <em>absolute</em> similarities from different models are still not necessarily meaningfully comparable. This is even true between two diferent <code>Doc2Vec</code> models. Their magnitudes are highly influenced by things like the model-metaparameters.</p>
<p>For example, if you used the exact same sufficiently-large set of texts to train a 100-dimensional <code>Doc2Vec</code> model, and a 300-dimensional <code>Doc2Vec</code> model, both models might wind up very similarly-useful. And for a doc A, its nearest-neighbor might consistently be doc B. Indeed, its top-10 neighbors might be very similar or identical.</p>
<p>But the cosine-similarities may have far different maxes/ranges, like the same neighbor B having the similarity <code>0.9</code> in one but <code>0.6</code> in another. Thy're the same docs, and correctly identified as 'most-similar', and neither <code>0.9</code> or <code>0.6</code> is truly a worse number to report, because in both cases the proper most-similar doc is at the top of the rankings. The models have just wound up using the available coordinate space differently. So, you shouldn't compare that <code>0.6</code> or <code>0.9</code> similarity (or in your case other distance numbers) against some other model – <em>especially</em> if the models use different algorithms, as seems to be the case for you. (If looks like you may be comparing absolute cosine-distances from a word-counting model against a dense learned <code>Doc2Vec</code> model.)</p>
<p>It <em>may</em> make more sense to compare result-<em>rankings</em> between the models. That is, ignore the raw similarity numbers, but care whether desirable documents appear in the top-N for other documents.  Alternatively, it might be possible to learn some scaling-rule for making the distances more comparable, but it's hard to make a more specific recommendation, or even know if that's a good step to take, without knowing your ultimate goal in comparing the models.</p>
",2,1,1892,2021-03-30 08:33:10,https://stackoverflow.com/questions/66867375/scale-cosine-distance-to-0-1-using-gensim
ModuleNotFoundError: No module named &#39;gensim.models.wrappers&#39;,"<p>I am trying to use LDA MAllet model. but I am facing with &quot;No module named 'gensim.models.wrappers'&quot; error.</p>
<ul>
<li><p>I have gensim installed and  ' gensim.models.LdaMulticore' works properly.</p>
</li>
<li><p>Java developer’s kit is installed</p>
</li>
<li><p>I have already downloaded mallet-2.0.8.zip and unzipped it on c:\ drive.</p>
</li>
<li><p>This is the code I am trying to use:</p>
<pre><code>import os
from gensim.models.wrappers import LdaMallet
os.environ.update({'MALLET_HOME':r'C:/mallet-2.0.8/'}) 
mallet_path = r'C:/mallet-2.0.8/bin/mallet' 

</code></pre>
</li>
</ul>
<p>Does anyone know what is wrong here? Many thanks!</p>
","python, gensim, lda, mallet, modulenotfounderror","<p>If you've installed the latest Gensim, 4.0.0 (as of late March, 2021), the <code>LdaMallet</code> model has been removed, along with a number of other tools which simply wrapped external tools/APIs.</p>
<p>You can see the note in the Gensim migration guide at:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#15-removed-third-party-wrappers"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#15-removed-third-party-wrappers</a></p>
<p>If the use of that tool is essential to your project, you may be able to:</p>
<ul>
<li><p>install an older version of Gensim, such as 3.8.3 - though of course you'd then be missing the latest fixes &amp; optimizations on any other Gensim models you're using</p>
</li>
<li><p>extract the <a href=""https://github.com/RaRe-Technologies/gensim/blob/release-3.8.3/gensim/models/wrappers/ldamallet.py"" rel=""noreferrer""><code>ldamallet.py</code> source code from that older version</a> &amp; update/move it to your own code for private use - dealing with whatever issues arise</p>
</li>
</ul>
",5,6,12313,2021-03-31 08:41:10,https://stackoverflow.com/questions/66884353/modulenotfounderror-no-module-named-gensim-models-wrappers
Phrase extraction with Spacy,"<p>Does <code>spacy</code> have some APIs to do phrase* extraction as one would do when using <code>word2phrase</code> or the <code>Phrases</code> class from <code>gensim</code>? Thank you.</p>
<p>PS. Phrases meant as collocations in Linguistics.</p>
","nlp, spacy, gensim, phrase","<p>I am wondering if you have you seen <a href=""https://spacy.io/universe/project/spacy-pytextrank"" rel=""noreferrer"">PyTextRank</a> or <a href=""https://pypi.org/project/spacycake/"" rel=""noreferrer"">spacycaKE</a> extension to SpaCy?</p>
<p>Both can help with phrase extraction which is <a href=""https://stackoverflow.com/a/40949414/6573902"">not possible directly</a> with SpaCy.</p>
",5,5,3627,2021-03-31 17:01:34,https://stackoverflow.com/questions/66892154/phrase-extraction-with-spacy
Is there a way to iterate through the vectors of Gensim&#39;s Word2Vec?,"<p>I'm trying to perform a simple task which requires iterations and interactions with specific vectors after loading it into gensim's Word2Vec.</p>
<p>Basically, given a txt file of the form:</p>
<pre><code>t1 -0.11307 -0.63909 -0.35103 -0.17906 -0.12349
t2 0.54553 0.18002 -0.21666 -0.090257 -0.13754
t3 0.22159 -0.13781 -0.37934 0.39926 -0.25967 
</code></pre>
<p>where t1 is the name of the vector and what follows are the vectors themselves. I load it in using the function <code>vecs = KeyedVectors.load_word2vec_format(datapath(f), binary=False)</code>.</p>
<p>Now, I want to iterate through the vectors I have and make a calculation, take summing up all of the vectors as an example. If this was read in using <code>with open(f)</code>, I know I can just use <code>.split(' ')</code> on it, but since this is now a KeyedVector object, I'm not sure what to do.</p>
<p>I've looked through the word2vec documentation, as well as used <code>dir(KeyedVectors)</code> but I'm still not sure if there is an attribute like <code>KeyedVectors.vectors</code> or something that allows me to perform this task.</p>
<p>Any tips/help/advice would be much appreciated!</p>
","python, gensim, word2vec","<p>There's a list of all words in the <code>KeyedVectors</code> object in its <code>.index_to_key</code> property. So one way to sum all the vectors would be to retrieve each by name in a list comprehension:</p>
<pre class=""lang-py prettyprint-override""><code>np.sum([vecs[key] for key in vecs.index_to_key], axis=0)
</code></pre>
<p>But, if all you really wanted to do is sum the vectors – and the keys (word tokens) aren't an important part of your calculation, the set of all the raw word-vectors is available in the <code>.vectors</code> property, as a numpy array with one vector per row. So you could also do:</p>
<pre class=""lang-py prettyprint-override""><code>np.sum(vecs.vectors, axis=0)
</code></pre>
",-1,1,1926,2021-04-05 20:45:10,https://stackoverflow.com/questions/66959571/is-there-a-way-to-iterate-through-the-vectors-of-gensims-word2vec
How to interpret doc2vec results on previously seen data?,"<p>I use gensim 4.0.1 and train doc2vec:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
    
sentences = [['hello', 'world'], ['james', 'bond'], ['adam', 'smith']]
documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)]
model = Doc2Vec(documents, vector_size=5, window=5, min_count=0, workers=4) 
</code></pre>
<pre><code>documents
    [TaggedDocument(words=['hello', 'world'], tags=[0]),
    TaggedDocument(words=['james', 'bond'], tags=[1]),
    TaggedDocument(words=['adam', 'smith'], tags=[2])]
</code></pre>
<pre><code>model.dv[0],model.dv[1],model.dv[2]
        (array([-0.10461631, -0.11958256, -0.1976151 ,  0.1710569 ,  0.0713223 ],
               dtype=float32),
         array([ 0.00526548, -0.19761242, -0.10334401, -0.19437183,  0.04021204],
               dtype=float32),
         array([ 0.05662392,  0.09290017, -0.08597242, -0.06293383, -0.06159503],
               dtype=float32))
</code></pre>
<p>I expect to get a match on TaggedDocument #1</p>
<pre><code>seen = ['james','bond']
</code></pre>
<p>Surprisingly, that known text (james bond) produces a completely &quot;unseen&quot; vector:</p>
<pre><code>new_vector = model.infer_vector(seen)
new_vector
        
        array([-0.07762126,  0.03976333, -0.02985927,  0.07899596, -0.03556045],
              dtype=float32)
</code></pre>
<p>The <em>most_similar()</em> does not point to the expected Tag=1. Moreover, all 3 scores are quite weak implying completely unseen data.</p>
<pre><code>model.dv.most_similar_cosmul(positive=[new_vector]) 
[(0, 0.5322251915931702), (2, 0.4972134530544281), (1, 0.46321794390678406)]
</code></pre>
<p>What is wrong here, any ideas?</p>
","gensim, doc2vec","<p>Five dimensions is still too many for a toy-sized dataset of just 6 words, 6 unique words, and 3 2-word texts.</p>
<p>None of the <code>Word2Vec</code>/<code>Doc2Vec</code>/<code>FastText</code>-type algorithms works well on tiny amounts of contrived data. They only learn their patterns from many, subtly-contrasting usages of words in varied contexts.</p>
<p>Their real strengths only emerge with vectors that are 50, 100, or hundreds-of-dimensions wide - and training that many dimensions requires a unique vocabulary of (at least) many thousands of words – ideally tens or hundreds of thousands of words – with many usage examples of each. (For a variant like <code>Doc2Vec</code>, you'd similarly want many thousands of varied documents.)</p>
<p>You'll see improved correlations with expected results when using sufficient training data.</p>
",0,0,182,2021-04-06 19:27:36,https://stackoverflow.com/questions/66975292/how-to-interpret-doc2vec-results-on-previously-seen-data
&#39;word not in the vocabulary&#39; when evaluating similarity using Gensim Word2Vec.most_similar method,"<p>Through the <a href=""https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html"" rel=""nofollow noreferrer"">method</a></p>
<p><code>gensim.models.Word2Vec.most_similar</code></p>
<p>I get the top-N most similar words.</p>
<p>I trained a model with a list of sentences like</p>
<pre><code>list_of_list = [[&quot;i like going to the beach&quot;],
                [&quot;the war is over&quot;], 
                [&quot;we are all made of stars&quot;],  
                         ...
                [&quot;i don't know what to do&quot;]] 
model = gensim.models.Word2Vec(list_of_list, size=100, window=longest_list, min_count=2)

suggestions = model.most_similar(&quot;I don't know what to do&quot;, topn=10)       
</code></pre>
<p>and I wanted to evaluate phrases similarity.</p>
<p>If for example I run</p>
<pre><code>suggestions = model.most_similar(&quot;I don't know what to do&quot;, topn=10)       
</code></pre>
<p>It works correctly.</p>
<p>But if I give a subquery like <code>&quot;to the beach&quot;</code> or <code>&quot;what to do&quot;</code>, it returns an error message because the sub-phrase is not in the vocabulary.</p>
<pre><code> &quot;word 'to the beach' not in vocabulary&quot;
</code></pre>
<p>How can I solve this issue without training again the model?
How can the model identify the most similar phrases based on a new phrase, not necessary a subphrase?</p>
","python, nlp, gensim, similarity","<p>It seems that you are not training the <code>Word2Vec</code> model correctly. Sentences should be lists of words not list of single strings. So, if you change it to:</p>
<pre><code>list_of_list = [[&quot;i like going to the beach&quot;],
                [&quot;the war is over&quot;], 
                [&quot;we are all made of stars&quot;],  
                         ...
                [&quot;i don't know what to do&quot;]]

list_for_training = [sent[0].split() for sent in list_of_list]
</code></pre>
<p>and use <code>list_for_training</code> as the first parameter of the constructor of <code>Word2Vec</code>.</p>
<p>Similarly, when calling <code>most_similar</code> method, provide a list of strings instead of a string:</p>
<pre><code>suggestions = model.most_similar(&quot;I don't know what to do&quot;.split(), topn=10)  
</code></pre>
<p>or</p>
<pre><code>suggestions = model.most_similar(&quot;to the beach&quot;.split(), topn=10) 
</code></pre>
",2,1,419,2021-04-08 11:41:54,https://stackoverflow.com/questions/67003269/word-not-in-the-vocabulary-when-evaluating-similarity-using-gensim-word2vec-mo
Genism Module attribute error for wrappers,"<p>I am going to find the optimal number of topics for LDA. To do this, I used GENSIM as follows :</p>
<pre><code>def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values

    
</code></pre>
<p>But I have an attribute error: I used spyder.</p>
<pre><code>AttributeError: module 'gensim.models' has no attribute 'wrappers'
</code></pre>
","python, gensim, lda, topic-modeling","<p>The latest major Gensim release, 4.0, removed the <code>wrappers</code> of other library algorithms. Per the <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#15-removed-third-party-wrappers"" rel=""noreferrer"">&quot;Migrating from Gensim 3.x to 4&quot; wiki page</a>:</p>
<blockquote>
<p><strong>15. Removed third party wrappers</strong></p>
<p>These wrappers of 3rd party libraries required too much effort. There were no volunteers to maintain and
support them properly in Gensim.</p>
<p>If your work depends on any of the modules below, feel free to copy it
out of Gensim 3.8.3 (the last release where they appear), and extend &amp;
maintain the wrapper yourself.</p>
<p>The removed submodules are:</p>
<pre><code>- gensim.models.wrappers.dtmmodel
- gensim.models.wrappers.ldamallet
- gensim.models.wrappers.ldavowpalwabbit
- gensim.models.wrappers.varembed
- gensim.models.wrappers.wordrank
- gensim.sklearn_api.atmodel
- gensim.sklearn_api.d2vmodel
- gensim.sklearn_api.ftmodel
- gensim.sklearn_api.hdp
- gensim.sklearn_api.ldamodel
- gensim.sklearn_api.ldaseqmodel
- gensim.sklearn_api.lsimodel
- gensim.sklearn_api.phrases
- gensim.sklearn_api.rpmodel
- gensim.sklearn_api.text2bow
- gensim.sklearn_api.tfidf
- gensim.sklearn_api.w2vmodel
- gensim.viz
</code></pre>
</blockquote>
<p>If you desperately needed the old support, you could also consider installing &amp; using the older Gensim. (For example, via <code>pip</code>, <code>pip install gensim==3.8.3</code>.) But in general, the latest version will be best-supported.</p>
",12,5,10017,2021-04-14 16:35:15,https://stackoverflow.com/questions/67095698/genism-module-attribute-error-for-wrappers
How to explain gensim word2vec output?,"<p>I run the following code and just wonder why the top 3 most similar words for &quot;exposure&quot; don't include &quot;charge&quot; and &quot;lend&quot;?</p>
<pre><code>from gensim.models import Word2Vec
corpus = [['total', 'exposure', 'charge', 'lend'],
          ['customer', 'paydown', 'rate', 'months', 'month']]
gens_mod = Word2Vec(corpus, min_count=1, vector_size=300, window=2, sg=1, workers=1, seed=1)
keyword=&quot;exposure&quot;
gens_mod.wv.most_similar(keyword)

Output:
[('customer', 0.12233059108257294),
 ('month', 0.008674687705934048),
 ('total', -0.011738087050616741),
 ('rate', -0.03600010275840759),
 ('months', -0.04291829466819763),
 ('paydown', -0.044823747128248215),
 ('lend', -0.05356598272919655),
 ('charge', -0.07367636263370514)]
</code></pre>
","python, nlp, gensim, word2vec, word-embedding","<p>The word2vec algorithm is only useful &amp; valuable with large amounts of training data, where every word of interest has a variety of realistic, subtly-contrasting usage examples.</p>
<p>A toy-sized dataset won't show its value. It's always a bad idea to set <code>min_count=1</code>. And, it's nonsensical to try to train 300-dimensional word-vectors from a corpus of only 9 words, 9 unique words, and most of the words having the exact same neighbors.</p>
<p>Try it on a more realistic dataset - tens-of-thousands of unique words, all with multiple usage examples – and you'll see more intuitively-correct similarity results.</p>
",2,0,455,2021-04-14 17:37:56,https://stackoverflow.com/questions/67096547/how-to-explain-gensim-word2vec-output
When should I consider to use pretrain-model word2vec model weights?,"<p>Suppose my corpus is reasonably large - having tens-of-thousands of unique words. I can either use it to build a word2vec model directly(Approach #1 in the code below) or initialize a new word2vec model with pre-trained model weights and fine tune it with my own corpus(Approach #2). Is the approach #2 worth consideration? If so, is there a rule of thumb on when I should consider a pre-trained model?</p>
<pre><code># Approach #1
from gensim.models import Word2Vec
model = Word2Vec(my_corpus, vector_size=300, min_count=1)

# Approach #2
model = Word2Vec(vector_size=300, min_count=1)
model.build_vocab(my_corpus)
model.intersect_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True, lockf=1.0)
model.train(my_corpus, total_examples=len(my_corpus))
</code></pre>
","python, gensim, word2vec, word-embedding, pre-trained-model","<p>The general answer to this type of question is: you should try them both, and see which works better for your purposes.</p>
<p>No one without your exact data &amp; project goals can be sure which will work better in your situation, and you'll need to exact same kind of ability-to-evaluate alterante choices to do all sorts of very basic, necessary tuning of your work.</p>
<p>Separately:</p>
<ul>
<li>&quot;fine-tuning&quot; word2vec-vectors can mean many things, and can introduce a number of expert-leve thorny tradeoff-decisions - the sorts of tradeoffs that can only be navigated if you've got a robust way to test different choices against each other.</li>
<li>The specific simple tuning approach your code shows - which relies on an experimental method (<code>intersect_word2vec_format()</code>) that might not work in the latest Gensim – is pretty limited, and since it discards all the words in the outside vectors that aren't already in your own corpus, also discards one of the major reasons people often want to mix older vectors in - to cover more words not in their training data. (I doubt that approach will be useful in many cases, but as per above, to be sure you'd want to try it with respect to your data/goals.</li>
<li>It's almost always a bad idea to use <code>min_count=1</code> with word2vec &amp; similar algorithms. If such rare words are truly important, find more training examples so good vectors can be trained for them. But without enough training examples, they're usually better to ignore - keeping them even makes the vectors for surrounding words worse.</li>
</ul>
",1,0,1219,2021-04-14 22:06:59,https://stackoverflow.com/questions/67099706/when-should-i-consider-to-use-pretrain-model-word2vec-model-weights
Understanding Gensim Doc2vec ranking,"<p>I use gensim 4.0.1 and follow tutorial <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">1</a> and <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html"" rel=""nofollow noreferrer"">2</a>:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

texts = [
    &quot;Human machine interface for lab abc computer applications&quot;,
    &quot;A survey of user opinion of computer system response time&quot;,
    &quot;The EPS user interface management system&quot;,
    &quot;System and human system engineering testing of EPS&quot;,
    &quot;Relation of user perceived response time to error measurement&quot;,
    &quot;The generation of random binary unordered trees&quot;,
    &quot;The intersection graph of paths in trees&quot;,
    &quot;Graph minors IV Widths of trees and well quasi ordering&quot;,
    &quot;Graph minors A survey&quot;,
]

texts = [t.lower().split() for t in texts]

documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(texts)]
model = Doc2Vec(documents, epochs=50, vector_size=5, window=2, min_count=2, workers=4)

new_vector = model.infer_vector(&quot;human machine interface&quot;.split())


for rank,(doc_id,score) in enumerate(model.dv.most_similar_cosmul(positive=[new_vector])):
        print('{}. {:.5f} [{}] {}'.format(rank, score, doc_id, ' '.join(documents[doc_id].words)))


1. 0.56613 [7] graph minors iv widths of trees and well quasi ordering
2. 0.55941 [6] the intersection graph of paths in trees
3. 0.55061 [2] the eps user interface management system
4. 0.54981 [1] a survey of user opinion of computer system response time
5. 0.52249 [4] relation of user perceived response time to error measurement
6. 0.52240 [8] graph minors a survey
7. 0.49214 [0] human machine interface for lab abc computer applications
8. 0.49016 [3] system and human system engineering testing of eps
9. 0.47899 [5] the generation of random binary unordered trees
​
</code></pre>
<p>Why the document[0] containing &quot;human machine interface&quot; has such a low (position 7) ranking? Is it a result of semantic generalization or the model needs to be tuned? Is larger corpus tutorial available to get repeatable results?</p>
","gensim, doc2vec","<p>The problem is the same as in my prior anwer to a similar question:</p>
<p><a href=""https://stackoverflow.com/a/66976706/130288"">https://stackoverflow.com/a/66976706/130288</a></p>
<p>Doc2Vec needs far more data to start working. 9 texts, with maybe 55 total words and perhaps around half that unique words is far too small to show any interesting results with this algorithm.</p>
<p>A few of Gensim's Doc2Vec-specific test cases &amp; tutorials manage to squeeze some vaguely understandable similarities out of a test dataset (from a file  <code>lee_background.cor</code>) that has 300 documents, each of a few hundred words - so tens of thousands of words, several thousand of which are unique. But it still needs to reduce the dimensionality &amp; up the epochs, and the results are still very weak.</p>
<p><strong>If you want to see meaningful results from Doc2Vec, you should be aiming for tens-of-thousands of documents, ideally with each document having dozens or hundreds or words.</strong></p>
<p>Everything short of that is going to be disappointing and not-representative of what sort of tasks the algorithm was designed to work with.</p>
<p>There's a tutorial using a larger movie-review dataset (100K documents) that was also used in the original 'Paragraph Vector' paper at:</p>
<p><a href=""https://radimrehurek.com/gensim/auto_examples/howtos/run_doc2vec_imdb.html#sphx-glr-auto-examples-howtos-run-doc2vec-imdb-py"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/howtos/run_doc2vec_imdb.html#sphx-glr-auto-examples-howtos-run-doc2vec-imdb-py</a></p>
<p>There's a tutorial based on Wikipedia (millions of documents) that might need some fixup to work nowadays at:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb</a></p>
",1,0,122,2021-04-15 21:40:48,https://stackoverflow.com/questions/67116370/understanding-gensim-doc2vec-ranking
How to change parameters of saved model without training docs in Gensim Doc2Vec?,"<p>I preprocess my docs, trained my model, and saved it by following the guidelines given here: <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html</a></p>
<p>After a period of time, I want to re-train my model with different parameters. However, I don't want to preprocess docs and create &quot;train corpus&quot; again because it takes nearly 3 days. Is there a solution to easily load saved model, change parameters and train the model with these new parameters for the following codes:</p>
<pre><code>model = Doc2Vec.load(myPath/myModel.doc2vec)
model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=40, dm=1, window=8)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)
</code></pre>
<p>Best.</p>
","parameters, model, gensim, doc2vec","<p>First, note that this section of your current code does nothing with the loaded model, because it's immediately replaced by the <em>new</em> model created by the 2nd line's instantiation of a model from scratch:</p>
<pre class=""lang-py prettyprint-override""><code>model = Doc2Vec.load(myPath/myModel.doc2vec)
model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=40, dm=1, window=8)
</code></pre>
<p>Second, in general, you can <code>.save()</code> a model <strong>after</strong> the <code>.build_vocab()</code> step, to then re-load that model for multiple later training sessions. And, a bunch of the model's parameters can be direcly changed, by simply assigning them new values (like <code>d2v_model.window = 10</code>) before that training, to affect it. For example:</p>
<pre class=""lang-py prettyprint-override""><code>d2v_model = Doc2Vec(vector_size=300, min_count=2, epochs=40, dm=1, window=8)
d2v_model.build_vocab(training_texts)
d2v_model.save(base_model_path)
</code></pre>
<p>Then, later:</p>
<pre class=""lang-py prettyprint-override""><code>d2v_model_w10 = Doc2Vec.load(base_model_path)
d2v_model_w10.window = 10
d2v_model_w10.train(training_texts, total_examples=d2v_model_w10.corpus_count, epochs=d2v_model_w10.epochs)
</code></pre>
<p>Some of the model parameters for which this modification <em>after</em> <code>.build_vocab()</code> modification should work well include:</p>
<ul>
<li><code>window</code></li>
<li><code>sg</code> (in <code>Word2Vec</code>) or <code>dm</code>/<code>dbow_words</code> (in <code>Doc2Vec</code>)</li>
<li><code>negative</code> (if changing between positive values)</li>
<li><code>workers</code></li>
<li><code>alpha</code>, <code>min_alpha</code></li>
</ul>
<p><em>However</em>, parameters which chiefly affect the <code>.build_vocab()</code> step, which includes discovering the working vocabulary &amp; allocating the starting vectors, or pre-calculating vocabulary-based values, <strong>won't</strong> change model behavior if modified after <code>.build_vocab()</code> was run. Some of these parameters include:</p>
<ul>
<li><code>vector_size</code> (because <code>.build_vocab()</code>'s last step allocates vectors)</li>
<li><code>min_count</code>, <code>max_final_vocab</code>, <code>trim_rule</code></li>
<li><code>hs</code> (or changing <code>negative</code> between zero &amp; positive numbers)</li>
<li><code>sample</code>, <code>ns_exponent</code> (threshold tables calculated in <code>.build_vocab</code>)</li>
</ul>
<p>(At a more-expert level, it <em>is</em> possible to save a model after the costly 1st vocabulary-scan, but before the final-steps. That would allow a load-then-change for most of these - but you should look at the <a href=""https://github.com/RaRe-Technologies/gensim/blob/f8964f4411964b0175589784ffa4a9b8f540e49c/gensim/models/doc2vec.py#L827"" rel=""nofollow noreferrer"">source for <code>Doc2Vec.build_vocab()</code></a> for hints. You'd have to never call <code>.build_vocab()</code>, but call the scan steps yourself, then <code>.save()</code>. Then after <code>.load()</code> &amp; changing parameters, call the <code>.prepare_vocab()</code> &amp; <code>.prepare_weights()</code> steps.)</p>
<p>Finally, &amp; separately from your main question, if your corpus is large enough for the training step to take 3 days, some things to consider:</p>
<ul>
<li><code>epochs=40</code> may be excessive - a lot of published work uses just 10-20 epochs, and more epochs are more often needed on smaller training sets.</li>
<li>with a large corpus, using a larger <code>min_count</code> often shrinks model size &amp; training time, with either no hit to quality, or even improving quality (by eliminating rare words which never become more than noise)</li>
<li>with a large corpus, more aggressive (smaller) values of <code>sample</code> can save a lot of time, especially with true natural language text in with Zipfian word frequencies. (Highly-frequent words don't need so mcuh redundant training, so more-aggressively skipping them both saves time &amp; improves the relative-attention-given-to, and quality of, rarer words.)</li>
</ul>
",1,0,322,2021-04-17 23:38:52,https://stackoverflow.com/questions/67143926/how-to-change-parameters-of-saved-model-without-training-docs-in-gensim-doc2vec
Can I get topics distribution of a word in LDA?,"<p>I'm new to LDA and I want to calculate the topic similarity between words. Can I get the topic distribution of a word? If so, how can I do this in gensim.ldamodel?</p>
","python, gensim, lda, topic-modeling","<p>Gensim's LDA mallet wrapper has a <code>load_word_topics()</code> function (I would assume this is true for its python LDA implementation as well).  It returns a matrix that is words X topics.  From that, you can get a vector of frequencies each word in each topic.  That would be the topic-word distribution for a given word.</p>
",0,0,502,2021-04-18 12:40:07,https://stackoverflow.com/questions/67148633/can-i-get-topics-distribution-of-a-word-in-lda
Why is my Doc2Vec model in gensim not reproducible?,"<p>I have noticed that my gensim Doc2Vec (DBOW) model is sensitive to document tags. My understanding was that these tags are cosmetic and so they should not influence the learned embeddings. Am I misunderstanding something? Here is a minimal example:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import numpy as np
import os
    
os.environ['PYTHONHASHSEED'] = '0'
    
reps = []
for a in [0,500]:
    documents = [TaggedDocument(doc, [i + a]) 
                 for i, doc in enumerate(common_texts)]
    model = Doc2Vec(documents, vector_size=100, window=2, min_count=0,
                    workers=1, epochs=10, dm=0, seed=0)
    reps.append(np.array([model.docvecs[k] for k in range(len(common_texts))])
    
reps[0].sum() == reps[1].sum()
</code></pre>
<p>This last line returns <code>False</code>. I am working with gensim 3.8.3 and Python 3.5.2. More generally, is there <em>any</em> role that the values of the tags play (assuming they are unique)? I ask because I have found that using different tags for documents in a classification task leads to widely varying performance.</p>
<p>Thanks in advance.</p>
","gensim, word2vec, random-seed, doc2vec","<p>Have you checked the magnitude of the differences?</p>
<p>Just running:</p>
<pre><code>delta = reps[0].sum() - reps[1].sum()
</code></pre>
<p>for the aggregate differences results with <code>-1.2598932e-05</code> when I run it.</p>
<p>Comparison dimension-wise:</p>
<pre><code> eps = 10**-4
 over = (np.abs(diff) &lt;= eps).all()
</code></pre>
<p>Returns <code>True</code> on a vast majority of the runs which means that you are getting quite reproducible results given the complexity of the calculations.</p>
<p>I would blame <a href=""https://nhigham.com/2020/08/04/what-is-numerical-stability/#:%7E:text=Numerical%20stability%20concerns%20how%20errors,can%20be%20from%20any%20source."" rel=""nofollow noreferrer"">numerical stability</a> of the calculations or uncontrolled randomness. Even though you do try to control the random seed, there is a different random seed in NumPy and different in <code>random</code> standard library so you are not controlling for all of the sources of randomness. This can also have an influence on the results but I did not check the actual implementation in <code>gensim</code> and it's dependencies.</p>
",1,0,614,2021-04-20 13:00:41,https://stackoverflow.com/questions/67179473/why-is-my-doc2vec-model-in-gensim-not-reproducible
Word2vec on documents each one containing one sentence,"<p>I have some unsupervised data (100.000 files) and each file has a paragraph containing one sentence. The preprocessing went wrong and deleted all stop points (.).
I used word2vec on a small sample (2000 files) and it treated each document as one sentence.
Should I continue the process on all remaining files? Or this would result to a bad model ?</p>
<p>Thank you</p>
","python, nlp, gensim, word2vec","<p>Did you try it, and get bad results?</p>
<p>I'm not sure what you mean by &quot;deleted all stop points&quot;. But, Gensim's <code>Word2Vec</code> is oblivious to what your tokens are, and doesn't really have any idea of 'sentences'.</p>
<p>All that matters is the lists-of-tokens you provide. (Sometimes people include puntuation like <code>'.'</code> as tokens, and sometimes it's stripped - and it doesn't make a very big different either way, and to the extent it does, whether it's good or bad may depend on your data &amp; goals.)</p>
<p>Any lists-of-tokens that include neighboring related tokens, for the sort of context-window training that's central to the word2vec algorithm, should work well.</p>
<p>For example, it can't learn anything from one-word texts, where there are no neighboring words. But running togther sentences, paragraphs, and even full documents into long texts works fine.</p>
<p>Even concatenating wholly-unrelated texts doesn't hurt much: the bit of random noise from unrelated words now in-each-others' windows is outweighed, with enough training, by the meaningful relationships in the much-longer runs of truly-related text.</p>
<p>The main limit to consider is that each training text (list of tokens) shouldn't be more than 10,000 tokens long, as internal implementation limits up through Gensim 4.0 mean tokens past the 10,000th position will be ignored. (This limit might eventually be fixed - but until then, just splitting overlong texts into 10,000-token chunks is a fine workaround with negligible effects via the lost contexts at the break points.)</p>
",1,0,238,2021-04-20 20:32:21,https://stackoverflow.com/questions/67185941/word2vec-on-documents-each-one-containing-one-sentence
Gensim LDA : error cannot compute LDA over an empty collection (no terms),"<p>I have te same error as this thread : <a href=""https://stackoverflow.com/questions/40840731/valueerror-cannot-compute-lda-over-an-empty-collection-no-terms"">ValueError: cannot compute LDA over an empty collection (no terms)</a> but the solution needed isn't the same.</p>
<p>I'm working on a notebook with Sklearn, and I've done an LDA and a NMF.</p>
<p>I'm now trying to do the same using Gensim: <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.htm"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.htm</a></p>
<p>Here is a piece of code (in Python) from my notebook of what I'm trying to do :</p>
<pre><code>dic = gensim.corpora.Dictionary(texts_lem)
dic.filter_extremes(no_below=10, no_above=0.8)
corpus = [dic.doc2bow(doc) for doc in texts_lem]

model = gensim.models.LdaModel(
    corpus=corpus,
    id2word=dic.id2token,
    num_topics=10,
)
</code></pre>
<p>I'm using the existing texts_lem list from another section of my notebook to do the Gensim LDA.
I'm following the guide : Creating a dictionary, filtering extremes, creating a corpus and sending it to LdaModel().</p>
<p>Unfortunately, it doesn't work, and commenting filter_extremes's row doesn't help (This is the answer of the other thread with same error).</p>
<p>texts_lem is the list of list of words like the following :</p>
<pre><code>[
 ['word', 'word', 'word', 'word'],
 ['word', 'word', 'word', 'word'],
 ['word', 'word', 'word', 'word'],
]
</code></pre>
<p>My error is :</p>
<pre><code>ValueError: cannot compute LDA over an empty collection (no terms)
</code></pre>
<p>Many thanks for your help.</p>
","python, nlp, gensim, lda","<p>Just don't use id2token.</p>
<p>Your model should be :</p>
<pre><code>model = gensim.models.LdaModel(
corpus=corpus,
id2word=dic.id2token,
num_topics=10,
)
</code></pre>
<p>Works fine. Who knows what's going on ?</p>
",0,0,1313,2021-04-23 11:44:02,https://stackoverflow.com/questions/67229373/gensim-lda-error-cannot-compute-lda-over-an-empty-collection-no-terms
Preprocessing a list of list removing stopwords for doc2vec using map without losing words order,"<p>I am implementing a simple <code>doc2vec</code> with <code>gensim</code>, <strong>not</strong> a <code>word2vec</code></p>
<p>I need to remove stopwords without losing the correct order to a list of list.</p>
<p>Each list is a document and, as I understood for doc2vec, the model will have as input a list of TaggedDocuments</p>
<p><code>model = Doc2Vec(lst_tag_documents, vector_size=5, window=2, min_count=1, workers=4)</code></p>
<pre><code>dataset = [['We should remove the stopwords from this example'],
     ['Otherwise the algo'],
     [&quot;will not work correctly&quot;],
     ['dont forget Gensim doc2vec takes list_of_list' ]]

STOPWORDS = ['we','i','will','the','this','from']


def word_filter(lst):
  lower=[word.lower() for word in lst]
  lst_ftred = [word for word in lower if not word in STOPWORDS]
  return lst_ftred

lst_lst_filtered= list(map(word_filter,dataset))
print(lst_lst_filtered)
</code></pre>
<p>Output:</p>
<pre><code>[['we should remove the stopwords from this example'], ['otherwise the algo'], ['will not work correctly'], ['dont forget gensim doc2vec takes list_of_list']]
</code></pre>
<p>Expected Output:</p>
<pre><code>[[' should remove the stopwords   example'], ['otherwise the algo'], [' not work correctly'], ['dont forget gensim doc2vec takes list_of_list']]
</code></pre>
<hr />
<ul>
<li><p><strong>What was my mistake and how to fix?</strong></p>
</li>
<li><p><strong>There are other efficient ways to solve this issue without losing the
proper order?</strong></p>
</li>
</ul>
<hr />
<p>List of questions I examined before asking:</p>
<p><a href=""https://stackoverflow.com/questions/30060970/how-to-apply-a-function-to-each-sublist-of-a-list-in-python"">How to apply a function to each sublist of a list in python?</a></p>
<ul>
<li>I studied this and tried to apply on my specific case</li>
</ul>
<p><a href=""https://stackoverflow.com/questions/65413876/removing-stopwords-from-list-of-lists"">Removing stopwords from list of lists</a></p>
<ul>
<li>The order is important I can't use set</li>
</ul>
<p><a href=""https://stackoverflow.com/questions/42042163/removing-stopwords-from-a-list-of-text-files"">Removing stopwords from a list of text files</a></p>
<ul>
<li>This could be a possible solution is similar to what I have implemented.</li>
<li>I undestood that the difference, but I don't know how to deal with it.
In my case the document is not tokenized (and should not be tokenized because is a doc2vec not a word2vec)</li>
</ul>
<p><a href=""https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python"">How to remove stop words using nltk or python</a></p>
<ul>
<li>In this question the SO is dealing with a list not a list of list</li>
</ul>
","python, list, gensim, stop-words","<p><code>lower</code> is a list of one element, <code>word not in STOPWORDS</code> will return <code>False</code>. Take the first item in the list with index and split by blank space</p>
<pre><code>lst_ftred = ' '.join([word for word in lower[0].split() if word not in STOPWORDS])
# output: ['should remove stopwords example', 'otherwise algo', 'not work correctly', 'dont forget gensim doc2vec takes list_of_list']
# 'the' is also in STOPWORDS
</code></pre>
",1,0,640,2021-04-25 12:06:15,https://stackoverflow.com/questions/67253213/preprocessing-a-list-of-list-removing-stopwords-for-doc2vec-using-map-without-lo
"I had a problem using word2vec. Maybe it&#39;s a version problem, but I don&#39;t know how to solve it ？","<p>This is my code</p>
<pre><code>w2v = Word2Vec(vector_size=150,min_count = 10)
w2v.build_vocab(x_train)
w2v.train(x_train)

def average_vec(text):
    vec = np.zeros(300).reshape((1,300))
    for word in text:
        try:
            vec += w2v[word].reshape((1,300))
        except KeyError:
            continue
        return vec
</code></pre>
<p><em><strong>And this throws the following error:</strong></em></p>
<pre><code>Traceback (most recent call last):   File
&quot;C:/Users/machao/Desktop/svm-master/word2vec.py&quot;, line 27, in &lt;module&gt;
    train_vec = np.concatenate([average_vec(z) for z in x_train])   File &quot;C:/Users/machao/Desktop/svm-master/word2vec.py&quot;, line 27, in
&lt;listcomp&gt;
    train_vec = np.concatenate([average_vec(z) for z in x_train])   File &quot;C:/Users/machao/Desktop/svm-master/word2vec.py&quot;, line 21, in
average_vec
    vec += w2v[word] TypeError: 'Word2Vec' object is not subscriptable

Process finished with exit code 1
</code></pre>
","python, gensim, word2vec","<p>The <code>Word2Vec</code> model object itself – <code>w2v</code> in your code – no longer supports direct access to individual vectors by lookup word key, in Gensim 4.0 and above.</p>
<p>Instead, you should use the subsidiary object in its <code>.wv</code> property - an object of type <code>KeyedVectors</code> which can be used to work with the set of word-vectors separately. (Separating functionality like this helps in cases where you only want the word-vectors, or only have the word-vectors from someone else, but not the full model's overhead.)</p>
<p>So, everywhere you might use <code>w2v[word]</code>, try <code>w2v.wv[word]</code> instead.</p>
<p>Or perhaps, name things more like the following, and hold a different variable reference to the word-vectors:</p>
<pre class=""lang-py prettyprint-override""><code>w2v_model = Word2Vec(...)
word_vectors = w2v_model.wv
print(word_vectors[word])
</code></pre>
<p>For other tips in adapting your own older code, or examples online, to Gensim 4.0, the following project wiki page may be helpful:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"" rel=""nofollow noreferrer"">Migrating from Gensim 3.x to 4</a></p>
",0,0,570,2021-04-26 06:47:10,https://stackoverflow.com/questions/67261993/i-had-a-problem-using-word2vec-maybe-its-a-version-problem-but-i-dont-know-h
No module named &#39;gensim.sklearn_api&#39; how to resolve,"<p>Im little confuse i would like use texthero library for some pca analysis. But when i trying run my code :</p>
<pre><code>import texthero as hero
import pandas as pd


df['pca']=(df['clean_tweet'].pipe(hero.clean).pipe(hero.do_tfidf).pipe(hero.do_pca))
hero.scatterplot(df, col='pca', color='topic', title=&quot;PCA BBC Sport news&quot;)
</code></pre>
<p>I get error:</p>
<pre><code>ModuleNotFoundError: No module named 'gensim.sklearn_api
</code></pre>
<p>But when i put !pip show gensim. i got</p>
<pre><code>Name: gensim
Version: 4.0.1
Summary: Python framework for fast Vector Space Modelling
Home-page: http://radimrehurek.com/gensim
</code></pre>
","python, pandas, machine-learning, gensim","<p>It seems that the module &quot;gensim.sklearn_api&quot; has been removed with version 4 of Gensim. Try downgrading Gensim's version.</p>
<pre><code>python -m pip install gensim==3.8.3
</code></pre>
<p>Reference: <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4</a></p>
",6,4,1890,2021-04-27 07:10:28,https://stackoverflow.com/questions/67278439/no-module-named-gensim-sklearn-api-how-to-resolve
Difference between VectorSize in word2Vec and numFeatures in TF-IDF,"<p>What is the difference between <strong>vectorSize</strong> in <strong>Word2Vec</strong> and <strong>numFeatures</strong> in <strong>HashingTF</strong>? I refer to class <em>Word2Vec</em> and <em>HashingTF</em> in pyspark:</p>
<p><strong>WORD2VEC</strong>: class pyspark.ml.feature.Word2Vec(*, <strong>vectorSize=100</strong>, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000)</p>
<p><strong>HashingTF</strong>:  class pyspark.ml.feature.HashingTF(*, <strong>numFeatures=262144</strong>, binary=False, inputCol=None, outputCol=None)</p>
","python, gensim, word2vec, tf-idf","<p>They're both the dimensionality of the representation, but the values will be in different ranges and useful in different ways.</p>
<p>In <code>Word2Vec</code>, each word gets a vector of <code>vectorSize</code> dimensions - where each dimension is a floating-point number (rather than a whole number). The values will be both positive and negative, and essentially never zero. Thus all words have coordinates in a fuzzy 'cloud' of space around the origin point.</p>
<p>Thus a word2vec vector is considered a 'dense embedding' of the word: it represents the word into a smaller vector space ('embeds' it) in a way where every dimension varies and holds some of the info ('dense'). As a result, all (100 in your example) dimensions will be used to represent any one item (word).</p>
<p>In <code>HashingTF</code> (which probably stands for 'hashing term frequency' or 'hashing trick frequency'), a text document of many words gets a vector of <code>numFeatures</code> dimensions - where each dimension is a non-negative integer count of how many times certain words appear in the document.</p>
<p>By using a technique called the 'hashing trick', it ensures any word, whether seen before or not, is assigned (by a hash value) to one of a fixed-set of counting buckets. The value of each dimension in the vector is the count of the words assigned to one bucket. In typical cases, many if not nearly-all of the buckets will be empty – and thus have zero values in the corresponding dimensions.</p>
<p>Thus a <code>HashingTF</code> vector is considered a 'sparse embedding' of a document: it represents the document into a smaller vector sapce ('embeds' it) in a way where most dimensions often stay zero, but a small relevant subset of dimensions become nonzero ('sparse'). As a result, the (262,144 in your example) dimensions might only be represented by a short list of which dimensions are non-zero and their value.</p>
",1,0,179,2021-04-28 09:09:09,https://stackoverflow.com/questions/67297183/difference-between-vectorsize-in-word2vec-and-numfeatures-in-tf-idf
invalid literal for int() with base 10: &#39;&lt;!DOCTYPE,"<p>I'm trying to use pre-trained word2vec in Google Colab. Previously I downloaded the model onto my C:/, and then uploaded it to my Google Drive. However, I get this error I can't seem to find anywhere.</p>
<p>My code is:</p>
<pre><code>from gensim.models import word2vec
import urllib.request

urllib.request.urlretrieve(&quot;https://drive.google.com/file/d/1lgCddPxJC__QA-qGtYTdNNoHRiYWyOpQ/view?usp=sharing/GoogleNews-vectors-negative300.bin&quot;, &quot;GoogleNews-vectors-negative300.bin&quot;)

word2vec_path = 'GoogleNews-vectors-negative300.bin'
word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)
</code></pre>
<p>Error Message:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-354-492ef9dcbbcc&gt; in &lt;module&gt;()
      1 word2vec_path = 'GoogleNews-vectors-negative300.bin'
----&gt; 2 word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)

2 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py in &lt;genexpr&gt;(.0)
    171     with utils.smart_open(fname) as fin:
    172         header = utils.to_unicode(fin.readline(), encoding=encoding)
--&gt; 173         vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
    174         if limit:
    175             vocab_size = min(vocab_size, limit)

ValueError: invalid literal for int() with base 10: '&lt;!DOCTYPE'
</code></pre>
","python, gensim, word2vec","<p>As use ~deceze notes, that error hints that the file has some typical HTML boilerplate (<code>&lt;~DOCTYPE</code>) where the code is expecting 2 <code>int</code>s declaring the forthcoming count-of-vectors (<code>vocab_size</code>) &amp; their dimensionality (<code>vector_size</code>).</p>
<p>It's likely your <code>urlrequest()</code> action didn't receive the file you expected, and perhaps got a 'file not found' or other error instead. So:</p>
<ul>
<li>Check its size &amp; contents to see if it's what you expect.</li>
<li>Check your request code, to ensure it can even get what you need from a random cloud notebook. (Maybe the Google Drive URL requires a logged-in user, and your Colab notebook isn't able to make web requests as a logged-in version of you?)</li>
<li>If you have the valid file elsewhere, see if you can send that valid copy directly to the scratch storage space of the notebook.</li>
</ul>
",0,-1,765,2021-05-06 14:13:42,https://stackoverflow.com/questions/67419932/invalid-literal-for-int-with-base-10-doctype
Gensim update to W2vec:AttributeError: &#39;int&#39; object has no attribute &#39;index&#39;,"<p><strong>This is my code below and the error I have is beneath it but I cant figure out why this is happening. Please share your thoughts: I checked here <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4</a>  but I wasn't able to figure out</strong></p>
<pre><code>word_vec_unpack_idx = [(word, idx.index) for word, idx in \
                   word_vec.wv.key_to_index.items()]
# unpacking vecs tpo create singulrized dataframe 
tokens, indexes = zip(*word_vec_unpack)

word_vec_df = pd.DataFrame(word_vec.wv.syn0[indexes, :], index=tokens)

tokenized_array = np.array(tokenized)
model_array = np.array([word_vec_df.loc[doc].mean(axis=0) for doc in tokenized_array])
model_df = pd.DataFrame(model_array)
# manually adding the label 
model_df[&quot;label&quot;] = df_final[&quot;Classification&quot;]

display(model_df.head())

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-64-8de619ecbc5b&gt; in &lt;module&gt;
----&gt; 1 word_vec_unpack = [(word, idx.index) for word, idx in \
      2                    word_vec.wv.key_to_index.items()]
      3 # unpacking vecs tpo create singulrized dataframe
      4 tokens, indexes = zip(*word_vec_unpack)
      5 

&lt;ipython-input-64-8de619ecbc5b&gt; in &lt;listcomp&gt;(.0)
----&gt; 1 word_vec_unpack = [(word, idx.index) for word, idx in \
      2                    word_vec.wv.key_to_index.items()]
      3 # unpacking vecs tpo create singulrized dataframe
      4 tokens, indexes = zip(*word_vec_unpack)
      5 

AttributeError: 'int' object has no attribute 'index'
</code></pre>
<p><strong>I broke the code down and removed idx so the first part of the code is:</strong>
now the error is gone.</p>
<pre><code>word_vec_unpack = [(word, index) for word, index in \
                   word_vec.wv.key_to_index.items()]
# unpacking vecs tpo create singulrized dataframe 
tokens, indexes = zip(*word_vec_unpack)
</code></pre>
<p><strong>Now I get another error for the second part of the code</strong></p>
<pre><code>word_vec_df = pd.DataFrame(word_vec.wv.syn0[indexes, :], index=tokens)

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-80-a185de0b1b16&gt; in &lt;module&gt;
----&gt; 1 word_vec_df = pd.DataFrame(word_vec.wv.syn0[indexes, :], index=tokens)

AttributeError: 'KeyedVectors' object has no attribute 'syn0'
</code></pre>
","python, gensim","<p>Try using <code>word_vec.wv.vectors</code> instead of <code>word_vec.wv.syn0</code>. That's the array holding the raw vectors.</p>
<p>(<code>KeyedVectors</code> hasn't had a true <code>syn0</code> for a while but if it might have had a backward-compatibility alias at the time your code was 1st crafted.)</p>
",1,0,601,2021-05-06 21:14:54,https://stackoverflow.com/questions/67426039/gensim-update-to-w2vecattributeerror-int-object-has-no-attribute-index
How do i retain numbers while preprocessing data using gensim in python?,"<p>I have used gensim.utils.simple_preprocess(str(sentence) to create a dictionary of words that I want to use for topic modelling. However, this is also filtering important numbers (house resolutions, bill no, etc) that I really need. How did I overcome this? Possibly by replacing digits with their word form. How do i go about it, though?</p>
","nlp, gensim, preprocessor, lda, latent-semantic-analysis","<p>You don't have to use <code>simple_preprocess()</code> - it's not doing much, it's not that configurable or sophisticated, and typically the other Gensim algorithms just need lists-of-tokens.</p>
<p>So, choose your own tokenization - which in some cases, depnding on your source data, could be as simple as a <code>.split()</code> on whitespace.</p>
<p>If you want to look at what <code>simple_preprocess()</code> does, as a model, you can view its Python source at:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/351456b4f7d597e5a4522e71acedf785b2128ca1/gensim/utils.py#L288"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/351456b4f7d597e5a4522e71acedf785b2128ca1/gensim/utils.py#L288</a></p>
",2,1,732,2021-05-09 13:21:30,https://stackoverflow.com/questions/67458203/how-do-i-retain-numbers-while-preprocessing-data-using-gensim-in-python
How to run Fasttext get_nearest_neighbors() faster?,"<p>I'm trying to extract morphs/similar words in Sinhala language using Fasttext.
But FastText takes a 1 second for 2.64 words. How can I increase the speed without changing the model size?</p>
<p>My code looks like this:</p>
<pre><code>import fasttext
fasttext.util.download_model('si', if_exists='ignore')  # Sinhala
ft = fasttext.load_model('cc.si.300.bin')
words_file = open(r'/Datasets/si_words_filtered.txt')
words = words_file.readlines()
words = words[0:300]
synon_dict = dict()
from tqdm import tqdm_notebook
for i in tqdm_notebook(range(len(words))):
    word = words[i].strip()
    synon = ft.get_nearest_neighbors(word)[0][1] ### takes a lot of time
    if is_strictly_sinhala_word(synon):
        synon_dict[word] = synon
import json
with open(&quot;out.json&quot;, &quot;w&quot;, encoding='utf8') as f:
    json.dump(synon_dict, f, ensure_ascii=False)

</code></pre>
","python, machine-learning, nlp, gensim, fasttext","<p>To do a fully accurate <code>get_nearest_neighbors()</code>-type of calculation is inherently fairly expensive, requiring a lookup &amp; calculation against <em>every</em> word in the set, for each new word.</p>
<p>As it looks like that set of vectors is near or beyond 2GB in size, when just the word-vectors are loaded, that means a scan of 2GB of addressable memory may be the dominant factor in the runtime.</p>
<p>Some things to try that might help:</p>
<ul>
<li>Ensure that you have plenty of RAM - if there's any use of 'swap'/virtual-memory, that will make things far slower.</li>
<li>Avoid all unnecessary comparisons - for example, perform your <code>is_strictly_sinhala_word()</code> check <em>before</em> the expensive step, so you can skip the costly step if not interested in the results. Also, you could consider shrinking the full set of word-vectors to eliminate those that you are unlikely to want as responses. This might involve throwing out words you know are not of the language-of-interest, or all lower-frequency words. (If you can throw out half the words as possible nearest-neighbors before even trying the <code>get_nearest_neighbors()</code>, it will go roughly twice as fast.) More on these options below.</li>
<li>Try other word-vector libraries, to see if they offer any improvement. For example, the Python Gensim project can load either plain sets of full-word vectors (eg, the <code>cc.si.300.vec</code> words-only file) or FastText models (the <code>.bin</code> file), and offers a <code>.most_similar()</code> function that has some extra options &amp; <em>might</em>, in some cases, offer different performance. (Though, the official Facebook Fasttext <code>.get_nearest_neighbors()</code> is probably pretty good.)</li>
<li>Use an &quot;approximate nearest neighbors&quot; library to pre-build an index of the word-vector space that can then offer extra-fast nearest-neighbor lookups - although at some risk of not finding the exact right top-N neighbors. There are many such libraries – see this <a href=""https://github.com/erikbern/ann-benchmarks"" rel=""nofollow noreferrer"">benchmarking project</a> that compares over 20 of them. But, adding this step complicates things &amp; the tradeoff of that complexity &amp; the imperfect result may not be worth the effort &amp; time-savings. So, just remember that it's a possibility if your need s large enough &amp; nothing else helps.</li>
</ul>
<p>With regard to slimming the set of vectors searched:</p>
<ul>
<li>The Gensim <code>KeyedVectors.load_word2vec_format()</code> function, which can load the <code>.vec</code> words-only file, has an option <code>limit</code> that will only read the specified number of words from the file. It looks like the <code>.vec</code> file for your dataset has over 800k words - but if you chose to load only 400k, your <code>.most_similar()</code> calculations would go about twice as fast. (And, since such files typically front-load the files with the most-common words, the loss of the far-rarer words may not be a concern.)</li>
<li>Siilarly, even if you load all the vectors, the Gensim <code>.most_similar()</code> function has a <code>restrict_vocab</code> option that can limit <em>searches</em> to just the 1st words of that count, which could also speed things or helpfully drop obscure words that may be of less interest.</li>
<li>The <code>.vec</code> file may be easier to work with if you wanted to pre-filter the words to, for example, eliminate non-Sinhala words. (Note: the usual <code>.load_word2vec_format()</code> text format needs a 1st line that declares the count of words &amp; word-dimensionality, but you may leave that off, then load using the <code>no_header=True</code> option, which instead uses 2 full passes over the file to get the count.)</li>
</ul>
",1,2,1082,2021-05-13 11:07:55,https://stackoverflow.com/questions/67518203/how-to-run-fasttext-get-nearest-neighbors-faster
Minimum Number of Words for Each Sentence for Training Gensim Word2vec Model,"<p>Suppose I have a corpus of short sentences of which the number of words ranges from 1 to around 500 and the average number of words is around 9. If I train a Gensim Word2vec model using window=5(which is the default), should I use all of the sentences? or I should remove sentences with low word count? If so, is there a rule of thumb for the minimum number of words?</p>
","nlp, gensim, word2vec, hyperparameters","<p>Texts with only 1 word are essentially 'empty' to the word2vec algorithm: there are no neighboring words, which are necessary for all training modes. You could drop them, but there's little harm in leaving them in, either. They're essentially just no-ops.</p>
<p>Any text with 2 or more words can contribute to the training.</p>
",1,0,271,2021-05-13 17:56:41,https://stackoverflow.com/questions/67523963/minimum-number-of-words-for-each-sentence-for-training-gensim-word2vec-model
Unable to recreate Gensim docs for training FastText. TypeError: Either one of corpus_file or corpus_iterable value must be provided,"<p>I am trying to make my own Fasttext embeddings so I went to official Gensim documentation and <a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""noreferrer"">implemented this exact code below</a> with exact <code>4.0</code> version.</p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import common_texts

model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
model.build_vocab(sentences=common_texts)
model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)
</code></pre>
<p>And to my surprise it is giving me errors as:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-4-6b2d1de02d90&gt; in &lt;module&gt;
      1 model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
----&gt; 2 model.build_vocab(sentences=common_texts)
      3 model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)

~/anaconda3/lib/python3.8/site-packages/gensim/models/word2vec.py in build_vocab(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)
    477 
    478         &quot;&quot;&quot;
--&gt; 479         self._check_corpus_sanity(corpus_iterable=corpus_iterable, corpus_file=corpus_file, passes=1)
    480         total_words, corpus_count = self.scan_vocab(
    481             corpus_iterable=corpus_iterable, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)

~/anaconda3/lib/python3.8/site-packages/gensim/models/word2vec.py in _check_corpus_sanity(self, corpus_iterable, corpus_file, passes)
   1484         &quot;&quot;&quot;Checks whether the corpus parameters make sense.&quot;&quot;&quot;
   1485         if corpus_file is None and corpus_iterable is None:
-&gt; 1486             raise TypeError(&quot;Either one of corpus_file or corpus_iterable value must be provided&quot;)
   1487         if corpus_file is not None and corpus_iterable is not None:
   1488             raise TypeError(&quot;Both corpus_file and corpus_iterable must not be provided at the same time&quot;)

TypeError: Either one of corpus_file or corpus_iterable value must be provided
</code></pre>
<p>Can someone please help what is happening here?</p>
","python, nlp, gensim, fasttext","<p>So I found the answer to this. They have a problem with the argument <code>sentence</code> in both:</p>
<pre><code>model.build_vocab(sentences=common_texts)
model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)
</code></pre>
<p>All you have to do is to <strong>remove the argument name</strong> or simply pass the first argument which is <code>corpus_iterable</code></p>
<pre><code>model.build_vocab(common_texts)
model.train(common_texts, total_examples=len(common_texts), epochs=10)
</code></pre>
<p>OR</p>
<pre><code>model.build_vocab(corpus_iterable=common_texts)
model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=10)
</code></pre>
",15,6,2877,2021-05-17 16:13:30,https://stackoverflow.com/questions/67573416/unable-to-recreate-gensim-docs-for-training-fasttext-typeerror-either-one-of-c
What would the output of skip-gram model look like?,"<p>To my understanding, the output of the skip-gram model must be compared with many training labels (depending on the window size)</p>
<p>My question is: Does the final output of the skip-gram model look like the description in this picture?
<img src=""https://i.sstatic.net/njSDy.jpg"" alt=""skip-gram"" /></p>
<p>Ps. the most similar question I can find:[1]<a href=""https://stackoverflow.com/questions/45431179/what-does-the-multiple-outputs-in-skip-gram-mean"">What does the multiple outputs in skip-gram mean?</a></p>
","python, tensorflow, nlp, nltk, gensim","<p>It's hard to answer about what &quot;should&quot; happen in degenerate/toy/artificial cases, especially given how much randomness is used in the actual initialization/training.</p>
<p>Both the model's internal weights <strong>and</strong> the 'projection layer' (aka 'input vectors' or just 'word vectors') are changed by backpropagation. So it can't be answered what the internal-weights should be without also considering the intialization &amp; updates to projection-weights. And nothing is meaningful with only two training examples, as opposed to &quot;many many more examples than  coudl be approximated by the model's state&quot;.</p>
<p>If you think you've constructed a tiny case that's informative when run, I'd suggest trying it against actual implementations to see what happens.</p>
<p>But beware: tiny models &amp; training sets are likely to be weird, or allow for multiple/overfit/idiosyncratic end-states, in ways that don't reveal much about how the algorithm behaves when used in its intended fashion – on large varied amounts of training data.</p>
",2,1,342,2021-05-17 17:58:15,https://stackoverflow.com/questions/67574809/what-would-the-output-of-skip-gram-model-look-like
How to interpret doc2vec classifier in terms of words?,"<p>I have trained a doc2vec (PV-DM) model in <code>gensim</code> on documents which fall into a few classes. I am working in a non-linguistic setting where both the number of documents and the number of unique words are small (~100 documents, ~100 words) for practical reasons. Each document has perhaps 10k tokens. My goal is to show that the doc2vec embeddings are more predictive of document class than simpler statistics and to explain which <em>words</em> (or perhaps word sequences, etc.) in each document are indicative of class.</p>
<p>I have good performance of a (cross-validated) classifier trained on the embeddings compared to one compared on the other statistic, but I am still unsure of how to connect the results of the classifier to any features of a given document. Is there a standard way to do this? My first inclination was to simply pass the co-learned word embeddings through the document classifier in order to see which words inhabited which classifier-partitioned regions of the embedding space. The document classes output on word embeddings are very consistent across cross validation splits, which is encouraging, although I don't know how to turn these effective labels into a statement to the effect of &quot;Document X got label Y because of such and such properties of words A, B and C in the document&quot;.</p>
<p>Another idea is to look at similarities between word vectors and document vectors. The ordering of similar word vectors is pretty stable across random seeds and hyperparameters, but the output of this sort of labeling does not correspond at all to the output from the previous method.</p>
<p>Thanks for help in advance.</p>
<p><strong>Edit</strong>: Here are some clarifying points. The tokens in the &quot;documents&quot; are ordered, and they are measured from a discrete-valued process whose states, I suspect, get their &quot;meaning&quot; from context in the sequence, much like words. There are only a handful of classes, usually between 3 and 5. The documents are given unique tags and the classes are not used for learning the embedding. The embeddings have rather dimension, always &lt; 100, which are learned over many epochs, since I am only worried about overfitting when the classifier is learned, not the embeddings. For now, I'm using a multinomial logistic regressor for classification, but I'm not married to it. On that note, I've also tried using the normalized regressor coefficients as vector in the embedding space to which I can compare words, documents, etc.</p>
","gensim, word2vec, word-embedding, doc2vec","<p>That's a very small dataset (100 docs) and vocabulary (100 words) compared to much published work of <code>Doc2Vec</code>, which has usually used tens-of-thousands or millions of distinct documents.</p>
<p>That each doc is thousands of words and you're using PV-DM mode that mixes both doc-to-word and word-to-word contexts for training helps a bit. I'd still expect you might need to use a smaller-than-defualt dimensionaity (vector_size&lt;&lt;100), &amp; more training epochs - but if it does seem to be working for you, great.</p>
<p>You don't mention how many classes you have, nor what classifier algorithm you're using, nor whether known classes are being mixed into the (often unsupervised) <code>Doc2Vec</code> training mode.</p>
<p>If you're only using known classes as the doc-tags, and your &quot;a few&quot; classes is, say, only 3, then to some extent you only have 3 unique &quot;documents&quot;, which you're training on in fragments. Using only &quot;a few&quot; unique doctags might be prematurely hiding variety on the data that could be useful to a downstream classifier.</p>
<p>On the other hand, if you're giving each doc a unique ID - the original 'Paragraph Vectors' paper approach, and then you're feeding those to a downstream classifier, that can be OK alone, but may also benefit from adding the known-classes as extra tags, in addition to the per-doc IDs. (And perhaps if you have many classes, those may be OK as the only doc-tags. It can be worth comparing each approach.)</p>
<p>I haven't seen specific work on making <code>Doc2Vec</code> models explainable, other than the observation that when you are using a mode which co-trains both doc- and word- vectors, the doc-vectors &amp; word-vectors have the same sort of useful similarities/neighborhoods/orientations as word-vectors alone tend to have.</p>
<p>You could simply try creating synthetic documents, or tampering with real documents' words via targeted removal/addition of candidate words, or blended mixes of documents with strong/correct classifier predictions, to see how much that changes either (a) their doc-vector, &amp; the nearest other doc-vectors or class-vectors; or (b) the predictions/relative-confidences of any downstream classifier.</p>
<p>(A wishlist feature for <code>Doc2Vec</code> for a while has been to synthesize a pseudo-document from a doc-vector. See <a href=""https://github.com/RaRe-Technologies/gensim/issues/2459"" rel=""nofollow noreferrer"">this issue</a> for details, including a link to one partial implementation. While the mere ranked list of such words would be nonsense in natural language, it might give doc-vectors a certain &quot;vividness&quot;.)</p>
<p>Whn you're not using real natural language, some useful things to keep in mind:</p>
<ul>
<li>if your 'texts' are really unordered bags-of-tokens, then <code>window</code> may not really be an interesting parameter. Setting it to a very-large number can make sense (to essentially put all words in each others' windows), but may not be practical/appropriate given your large docs. Or, trying PV-DBOW instead - potentially even mixing known-classes &amp; word-tokens in either <code>tags</code> or <code>words</code>.</li>
<li>the default <code>ns_exponent=0.75</code> is inherited from word2vec &amp; natural-language corpora, &amp; at least one research paper (linked from the class documentation) suggests that for other applications, especially recommender systems, very different values may help.</li>
</ul>
",2,0,652,2021-05-18 05:24:35,https://stackoverflow.com/questions/67580388/how-to-interpret-doc2vec-classifier-in-terms-of-words
How to get similarity score for unseen documents using Gensim Doc2Vec model?,"<p>I have trained a gensim doc2vec model for an English news recommender system. the model was trained with 40K news data. I am using the code below to recommend the top 5 most similar news for e.g. news_1:</p>
<pre><code>inferred_vector = model.infer_vector(news_1)
sims = model.dv.most_similar([inferred_vector], topn=5)
</code></pre>
<p>The problem is that if I add another 100 news data to the database(so our database will have 40K + 100 news data now) and re-run the same code, the code will only be able to recommend news based on the original 40K(instead of 40K + 100) to me, in another word, the recommended articles will never come from the 100 articles.</p>
<p>how can I address this issue without the need to retrain the model? Thank you in advanced!</p>
<p>Ps: As our APP is for news, so everyday we'll have lots of news data coming into our database, so we won't consider to retrain the model everyday(doing so may crash our backend server).</p>
","python, gensim, doc2vec, recommendation-engine","<p>There's a bulk contiguous vector structure initially created by training, for the initial known set of vectors. It's amenable to the every-candidate bulk vector calculation at the heart of <code>most_similar()</code> - so that operation goes about as fast as it can, with the right vector libraries for your OS/processor.</p>
<p>But, that structure wasn't originally designed with incremental expansion in mind. Indeed, if you have 1 million vectors in a dense array, then want to add 1 to the end, the straightforward approach requires you to allocate a new 1-million-and-1 long array, bulk copy over the 1 million, then add the last 1. That works, but what seems like a &quot;tiny&quot; operation then takes a while, and ever-longer as the structure grows. And, each add more-than-doubles the temporary memory usage, for the bulk copy. So, the naive pattern of adding a whole bunch of new items individuall in a loop can be really slow &amp; memory-intensive.</p>
<p>So, Gensim hasn't yet focused on providing a set-of-vectors that's easy &amp; efficient to incrementally <em>grow</em> with new vectors. But, it's still indirectly <em>possible</em>, if you understand the caveats.</p>
<p>Especially in <code>gensim-4.0.0</code> &amp; above, the <code>.dv</code> set of doc-vectors is an instance of <code>KeyedVectors</code> with all that class's standard functions. Thos include the <code>add_vector()</code> and <code>add_vectors()</code> methods:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.add_vector"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.add_vector</a></p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.add_vectors"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.add_vectors</a></p>
<p>You can try these methods to add your new inferred vectors to the <code>model.dv</code> object - and then they'll also be ncluded in folloup <code>most_similar()</code> results.</p>
<p>But keep in mind:</p>
<ol>
<li><p>The above caveats about performance &amp; memory-usage - which may be minor concerns as long as your dataset isn't too large, or manageable if you do additions in occasional larger batches.</p>
</li>
<li><p>The containing <code>Doc2Vec</code> model generally isn't expecting its internal <code>.dv</code> to be arbitrarily modified or expanded by other code. So, once you start doing that, parts of the <code>model</code> may not behave as expected. If you have problems with this, you could consider saving-aside the full <code>Doc2Vec</code> <code>model</code> before any direct-tampering with its <code>.dv</code>, and/or only expanding a completely separate instance of the doc-vectors, for example by saving them aside (eg: <code>model.dv.save(DOC_VECS_FILENAME)</code>) &amp; reloading them into a separate <code>KeyedVectors</code> (eg: <code>growing_docvecs = KeyedVectors.load(DOC_VECS_FILENAME)</code>).</p>
</li>
</ol>
",0,1,618,2021-05-19 04:28:24,https://stackoverflow.com/questions/67596945/how-to-get-similarity-score-for-unseen-documents-using-gensim-doc2vec-model
Inner workings of Gensim Word2Vec,"<p>I have a couple of issues regarding Gensim in its Word2Vec model.</p>
<p>The first is what is happening if I set it to train for 0 epochs? Does it just create the random vectors and calls it done. So they have to be random every time, correct?</p>
<p>The second is concerning the WV object in the doc page says:</p>
<pre><code>This object essentially contains the mapping between words and embeddings.
After training, it can be used directly to query those embeddings in various ways.  
See the module level docstring for examples.
</code></pre>
<p>But that is not clear to me, allow me to explain I have my own created word vectors which I have substitute in the</p>
<pre><code>   word2vecObject.wv['word'] = my_own
</code></pre>
<p>Then call the train method with those replacement word vectors. But I would like to know which part am I replacing, is it the input to hidden weight layer or the hidden to input? This is to check if it can be called pre-training or not. Any help? Thank you.</p>
","python, gensim, word2vec","<p>I've not tried the nonsense parameter <code>epochs=0</code>, but it might behave as you expect. (Have you tried it and seen otherwise?)</p>
<p>However, if your real goal is to be able to tamper with the model after initialization, but before training, the usual way to do that is to not supply any corpus when constructing the model instance, and instead manually do the two followup steps, <code>.build_vocab()</code> &amp; <code>.train()</code>, in your own code - inserting extra steps between the two. (For even finer-grained control, you can examine the source of <code>.build_vocab()</code> &amp; its helper methods, and simply ensure you do all those necessary things, with your own extra steps interleaved.)</p>
<p>The &quot;word vectors&quot; in the <code>.wv</code> property of type <code>KeyedVectors</code> are essentially the &quot;input projection layer&quot; of the model: the data which converts a single word into a <code>vector_size</code>-dimensional dense embedding. (You can think of the keys – word token strings – as being somewhat like a one-hot word-encoding.)</p>
<p>So, assigning into that structure only changes that &quot;input projection vector&quot;, which is the &quot;word vector&quot; usually collected from the model. If you need to tamper with the hidden-to-output weights, you need to look at the model's <code>.syn1neg</code> (or <code>.syn1</code> for HS mode) property.</p>
",2,0,215,2021-05-19 19:21:58,https://stackoverflow.com/questions/67609635/inner-workings-of-gensim-word2vec
why does gensim summarize() return blank sometimes?,"<p>I'm beginner at nlp and I'm using gensim for the first time.
I noticed that some text it returns a blank summary. For example:</p>
<pre><code>from gensim.summarization import summarize
text =&quot;The continued digitization of most every sector of society and industry means that an ever-growing volume of data will continue to be generated. The ability to gain insights from these vast datasets is one key to addressing an enormous array of issues — from identifying and treating diseases more effectively, to fighting cyber criminals, to helping organizations operate more effectively to boost the bottom line.&quot;
summarize(text, 0.6)
</code></pre>
<p>returns:
<code>''</code></p>
<p>When I have equivalent sized paragraphs in other instances it returns a summary, so I know it's not that my ratio is too small. Any insights appreciated!</p>
","python, nlp, gensim","<p>For the sake of the answer I'll assume Gensim version 3.8.3 - this is the latest version that (currently) supports summarization, since there are no API stubs in version 4 anymore.</p>
<p>Specifically, when looking at the reference for <a href=""https://radimrehurek.com/gensim_3.8.3/summarization/summariser.html#gensim.summarization.summarizer.summarize"" rel=""nofollow noreferrer""><code>summarize()</code></a>, we can read the following:</p>
<blockquote>
<p>Get a summarized version of the given text.<br />
The output summary will consist of the most <strong>representative sentences</strong> and will be returned as a string, divided by newlines.</p>
</blockquote>
<p>The highlighted part also explains why your output is empty: Gensim employs an extractive summarizer, which can only choose different <em>sentences</em>, not <em>sentence parts</em>. Therefore, either the entire sentence is selected (resulting in no &quot;summarization&quot;), or return the empty answer. Fixing this problem is also not trivial, and I think you have only one of two (sub-optimal) choices:</p>
<ul>
<li>Employ an abstractive summarizer. Compared to extractive summarization, abstractive models can actually do what humans usually &quot;expect&quot; from a system, namely re-wording and selection of phrases from a sentence to form a shorter output, without relying on the selection of sentences. However, such models are usually quite compute-intensive, and there is no such model available through Gensim (AFAIK).</li>
<li>Pre-chunk your text. If you can achieve a reasonable segmentation of your input sentence into several chunks of text, these can be a stand-in for &quot;multiple sentences&quot;, and therefore would allow you to have an approximate summary, even though it probably isn't very good.</li>
</ul>
",2,0,756,2021-05-21 00:25:43,https://stackoverflow.com/questions/67629458/why-does-gensim-summarize-return-blank-sometimes
TypeError: &#39;Word2Vec&#39; object is not subscriptable,"<p>I am trying to build a Word2vec model but when I try to reshape the vector for tokens, I am getting this error. Any idea ?</p>
<pre><code>wordvec_arrays = np.zeros((len(tokenized_tweet), 100)) 
for i in range(len(tokenized_tweet)):
    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 100)
wordvec_df = pd.DataFrame(wordvec_arrays) 
wordvec_df.shape

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-101-71156bf1c4a3&gt; in &lt;module&gt;
      1 wordvec_arrays = np.zeros((len(tokenized_tweet), 100))
      2 for i in range(len(tokenized_tweet)):
----&gt; 3     wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 100)
      4 wordvec_df = pd.DataFrame(wordvec_arrays)
      5 wordvec_df.shape

&lt;ipython-input-100-e3a82e60af93&gt; in word_vector(tokens, size)
      4     for word in tokens:
      5         try:
----&gt; 6             vec += model_w2v[word].reshape((1, size))
      7             count += 1.
      8         except KeyError: # handling the case where the token is not in vocabulary

TypeError: 'Word2Vec' object is not subscriptable
</code></pre>
","python-3.x, jupyter-notebook, gensim, word2vec","<p>As of Gensim 4.0 &amp; higher, the <code>Word2Vec</code> model doesn't support subscripted-indexed access (the <code>['...']') to individual words. (Previous versions would display a deprecation warning, </code>Method will be removed in 4.0.0, use self.wv.<strong>getitem</strong>() instead`, for such uses.)</p>
<p>So, when you want to access a specific word, do it via the <code>Word2Vec</code> model's <code>.wv</code> property, which holds just the word-vectors, instead. So, your (unshown) <code>word_vector()</code> function should have its line highlighted in the error stack changed to:</p>
<pre><code>            vec += model_w2v.wv[word].reshape((1, size))
</code></pre>
",13,8,21257,2021-05-25 12:30:41,https://stackoverflow.com/questions/67687962/typeerror-word2vec-object-is-not-subscriptable
AttributeError: Can&#39;t get attribute on &lt;module &#39;gensim.models.word2vec&#39;,"<p>I use python 3.9.1 on macOS Big Sur with an M1 chip.
And, gensim is 4.0.1</p>
<p>I tried to use the pre-trained Word2Vec model and I ran the code below:</p>
<pre><code>from gensim.models.word2vec import Word2Vec

model_path = '/path/to/word2vec.gensim.model'

model = Word2Vec.load(model_path)
</code></pre>
<p>However, I got an error below:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-11-4c1c2f93fadb&gt; in &lt;module&gt;
      1 from gensim.models.word2vec import Word2Vec
      2 
----&gt; 3 model = Word2Vec.load(model_path)

~/opt/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py in load(cls, rethrow, *args, **kwargs)
   1932                 &quot;Try loading older model using gensim-3.8.3, then re-saving, to restore &quot;
   1933                 &quot;compatibility with current code.&quot;)
-&gt; 1934             raise ae
   1935 
   1936     def _load_specials(self, *args, **kwargs):

~/opt/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py in load(cls, rethrow, *args, **kwargs)
   1920         &quot;&quot;&quot;
   1921         try:
-&gt; 1922             model = super(Word2Vec, cls).load(*args, **kwargs)
   1923             if not isinstance(model, Word2Vec):
   1924                 rethrow = True

~/opt/miniconda3/lib/python3.9/site-packages/gensim/utils.py in load(cls, fname, mmap)
    484         compress, subname = SaveLoad._adapt_by_suffix(fname)
    485 
--&gt; 486         obj = unpickle(fname)
    487         obj._load_specials(fname, mmap, compress, subname)
    488         obj.add_lifecycle_event(&quot;loaded&quot;, fname=fname)

~/opt/miniconda3/lib/python3.9/site-packages/gensim/utils.py in unpickle(fname)
   1456     &quot;&quot;&quot;
   1457     with open(fname, 'rb') as f:
-&gt; 1458         return _pickle.load(f, encoding='latin1')  # needed because loading from S3 doesn't support readline()
   1459 
   1460 

AttributeError: Can't get attribute 'Vocab' on &lt;module 'gensim.models.word2vec' from '/Users//opt/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py'&gt;

</code></pre>
<p>here is the link where I got the this model
<a href=""https://github.com/shiroyagicorp/japanese-word2vec-model-builder"" rel=""nofollow noreferrer"">https://github.com/shiroyagicorp/japanese-word2vec-model-builder</a></p>
<p>Thank you in advance.</p>
","python, nlp, gensim, word2vec","<p>The problem is that the referenced repository trained a model on an <a href=""https://github.com/shiroyagicorp/japanese-word2vec-model-builder/blob/master/requirements.txt"" rel=""nofollow noreferrer"">incredibly old version of GenSim</a>, which makes it incompatible with current versions.</p>
<p>You can potentially check whether the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2VecTrainables.add_lifecycle_event"" rel=""nofollow noreferrer"">lifecycle meta data</a> gives you any indication on the actual version, and then try to update your model from there.
The documentation also gives some tips for <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Gensim-And-Compatibility#upgrading-trained-models"" rel=""nofollow noreferrer"">upgrading your older trained models</a>, but even those are relatively weak and point mostly to re-training. Similarly, even <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"" rel=""nofollow noreferrer"">migrating from GenSim 3.X to 4.X</a> is not referencing direct upgrade methods, but could give you ideas on what parameters to look out for specifically.</p>
<p>My suggestion would be to try loading it with any of the previous 3.X versions, and see if you have more success loading it there.</p>
",2,0,1425,2021-06-07 03:56:42,https://stackoverflow.com/questions/67865773/attributeerror-cant-get-attribute-on-module-gensim-models-word2vec
Ploting function word2vec Error &#39;Word2Vec&#39; object is not subscriptable,"<p>I have my word2vec model.
I can use it in order to see the most similars words.
Now i create a function in order to plot the word as vector.
Here my function :</p>
<pre><code>def tsne_plot(model):

    labels = []
    tokens = []

    for word in model.wv.key_to_index:
        tokens.append(model[word])
        labels.append(word)
    
    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
    new_values = tsne_model.fit_transform(tokens)

    x = []
    y = []
for value in new_values:
    x.append(value[0])
    y.append(value[1])
    
plt.figure(figsize=(16, 16)) 
for i in range(len(x)):
    plt.scatter(x[i],y[i])
    plt.annotate(labels[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')
plt.show()
</code></pre>
<p>When i call the function, I have the following error :</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-47-d0f4ea6902bf&gt; in &lt;module&gt;
----&gt; 1 tsne_plot(model)

&lt;ipython-input-46-b4714ffe935b&gt; in tsne_plot(model)
      5 
      6     for word in model.wv.key_to_index:
----&gt; 7         tokens.append(model[word])
      8         labels.append(word)
      9 

TypeError: 'Word2Vec' object is not subscriptable
</code></pre>
<p>I really don't how to remove this error. I think it's maybe because the newest version of Gensim do not use array [...].
Thanks for advance !</p>
","python, gensim, word2vec","<p>In Gensim 4.0, the <code>Word2Vec</code> object itself is no longer directly-subscriptable to access each word. Instead, you should access words via its subsidiary <code>.wv</code> attribute, which holds an object of type <code>KeyedVectors</code>.</p>
<p>So, replace <code>model[word]</code> with <code>model.wv[word]</code>, and you should be good to go.</p>
",9,1,3451,2021-06-10 14:04:46,https://stackoverflow.com/questions/67922777/ploting-function-word2vec-error-word2vec-object-is-not-subscriptable
How can you pass a pretrainend LDA Model to ldaseq in Gensim for DTM?,"<p>I have a tuned and pretrainend LDA Model that I want to pass on to the ldaseq model in gensim, but don't understand how to do it. I've tried lda_model and sstats but it doesn'T seem to work, I still get this from the logging:</p>
<blockquote>
<p>running online (multi-pass) LDA training, 10 topics, 10 passes over
the supplied corpus of 1699 documents, updating model once every 1699
documents, evaluating perplexity every 1699 documents, iterating 50x
with a convergence threshold of 0.001000</p>
</blockquote>
","gensim, lda, topic-modeling","<p>In case anyone ever wonders this:
initialize='own' you need to supply sstats of the previously trained model in the shape (vocab_len, num_topics), and initialize='lda_model' you need to supply the previously trained lda model.</p>
<p>I found the answer <a href=""https://github.com/NLPatVCU/Sentiment-Classification/blob/master/keras_test/keras_test/lib/python3.6/site-packages/gensim/models/ldaseqmodel.py"" rel=""nofollow noreferrer"">here</a></p>
",0,0,195,2021-06-16 13:31:21,https://stackoverflow.com/questions/68003709/how-can-you-pass-a-pretrainend-lda-model-to-ldaseq-in-gensim-for-dtm
Not able to import from `gensim.summarization` module in Django,"<p>I have included the 2 import statements in my views.py</p>
<pre><code>from gensim.summarization.summarizer import summarizer
from gensim.summarization import keywords
</code></pre>
<p>However, even after I installed gensim using pip, I am getting the error:</p>
<pre><code>ModuleNotFoundError: No module named 'gensim.summarization'
</code></pre>
","python, django, nlp, gensim","<p>The <code>summarization</code> code was removed from Gensim 4.0. See:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#12-removed-gensimsummarization"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#12-removed-gensimsummarization</a></p>
<blockquote>
<h4>12. Removed <code>gensim.summarization</code></h4>
<p>Despite its general-sounding name, the module will not satisfy the
majority of use cases in production and is likely to waste people's
time. See <a href=""https://github.com/RaRe-Technologies/gensim/issues/2592"" rel=""noreferrer"">this Github
ticket</a> for
more motivation behind this.</p>
</blockquote>
<p>If you need it, you could try:</p>
<ul>
<li>installing the older gensim version; or…</li>
<li>copy the source code out to your own local module</li>
</ul>
<p>However, I expect you'd likely be disappointed by its inflexibility and how little it can do.</p>
<p>It was <em>only</em> extractive summarization - choosing a few key sentences from those that already exist. That only gives impressive results when the source text was already well-written in an expository style mixing high-level overview sentences with separate detail sentences. And, its method of analyzing/ranking words was very crude &amp; hard-to-customize – totally unconnected to the more generic/configurable/swappable approaches used elsewhere in Gensim or in other text libraries.</p>
",13,9,23592,2021-06-17 11:49:39,https://stackoverflow.com/questions/68018745/not-able-to-import-from-gensim-summarization-module-in-django
What Metrics Are Used in the Output of Gensim&#39;s evaluate_word_pairs?,"<p>Gensim offers evaluate_word_pairs function for <a href=""https://radimrehurek.com/gensim_4.0.0/auto_examples/tutorials/run_word2vec.html#evaluating"" rel=""nofollow noreferrer"">evaluating semantic similarity</a>.</p>
<p>Here is an example from its page:</p>
<pre><code>model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))

Out:
((0.1014236962315867, 0.44065378924434523), SpearmanrResult(correlation=0.07441989763914543, pvalue=0.5719973648460552), 83.0028328611898)
</code></pre>
<p>I would like to know what metrics are used to generate each value(0.1014236962315867, 0.44065378924434523,...) in the output?</p>
","gensim, word2vec, similarity","<p>Per the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs"" rel=""nofollow noreferrer"">documentation for <code>evaluate_word_pairs()</code></a>:</p>
<blockquote>
<p><strong>Returns</strong></p>
<ul>
<li><strong>pearson</strong> <em>(tuple of (float, float))</em> – Pearson correlation coefficient with 2-tailed p-value.</li>
<li><strong>spearman</strong> <em>(tuple of (float, float))</em> – Spearman rank-order correlation coefficient between the similarities from the dataset and the similarities produced by the model itself, with 2-tailed p-value.</li>
<li><strong>oov_ratio</strong> <em>(float)</em> – The ratio of pairs with unknown words.</li>
</ul>
</blockquote>
<p>Per your output, it looks like the Pearson result is still just a plain tuple, while the Spearman result has been reported as a named tuple. But in each case, it appears the correlation-coefficient is 1st, then the p-value.</p>
<p>Note that the <code>oov_ratio</code> is reported as the <em>percentage</em> of test words that weren't known to the model.</p>
<p>Consult other references for definitions/explanations of the <a href=""https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"" rel=""nofollow noreferrer"">Pearson</a> and <a href=""https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"" rel=""nofollow noreferrer"">Spearman</a> coefficients/p-values.</p>
",1,0,391,2021-06-17 17:16:19,https://stackoverflow.com/questions/68023855/what-metrics-are-used-in-the-output-of-gensims-evaluate-word-pairs
Gensim: Is doc2vec a model or operation? Differences from R implementation,"<p>I have been tasked with putting a document vector model into production.
I am an R user, and so my original model is in R. One of the avenues we have is to recreate the code and the models in Python.</p>
<p><strong>I am confused by the Gensim implementation of Doc2vec</strong>.</p>
<p>The process that works in R goes like this:</p>
<p><strong>Offline</strong></p>
<hr />
<ul>
<li><p>Word vectors are trained using the functions in the <code>text2vec</code> package, namely GloVe or GlobalVectors, on a large corpus This gives me a large Word Vector text file.</p>
</li>
<li><p>Before the ML step takes place, the <code>Doc2Vec</code> function from the <code>TextTinyR</code> library is used to turn each piece of text from a smaller, more specific training corpus into a vector. <em>This is not a machine learning step. No model is trained</em>. The Doc2Vec function effectively aggregates the word vectors in the sentence, in the same sense that finding the sum or mean of vectors does, but in a way that preserves information about word order.</p>
</li>
<li><p>Various models are then trained on these smaller text corpuses.</p>
</li>
</ul>
<hr />
<p><strong>Online</strong></p>
<hr />
<ul>
<li>The new text is converted to Document Vectors using the pretrained word vectors.</li>
<li>The Document Vectors are fed into the pretrained model to obtain the output classification.</li>
</ul>
<hr />
<p><strong>The example code I have found for Gensim appears to be a radical departure from this.</strong></p>
<p>It appears in <code>gensim</code> that Doc vectors are a separate class of model from word vectors that you can train. It seems in some cases, the word vectors and doc vectors are all trained at once. Here are some examples from tutorials and stackoverflow answers:</p>
<p><a href=""https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"" rel=""nofollow noreferrer"">https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5</a></p>
<p><a href=""https://stackoverflow.com/questions/27470670/how-to-use-gensim-doc2vec-with-pre-trained-word-vectors"">How to use Gensim doc2vec with pre-trained word vectors?</a></p>
<p><a href=""https://stackoverflow.com/questions/36815038/how-to-load-pre-trained-model-with-in-gensim-and-train-doc2vec-with-it?rq=1"">How to load pre-trained model with in gensim and train doc2vec with it?</a></p>
<p><a href=""https://stackoverflow.com/questions/45037860/gensim1-0-1-doc2vec-with-google-pretrained-vectors?noredirect=1&amp;lq=1"">gensim(1.0.1) Doc2Vec with google pretrained vectors</a></p>
<p>So my questions are these:</p>
<p><strong>Is the gensim implementation of Doc2Vec fundamentally different from the TextTinyR implementation?</strong></p>
<p><strong>Or is the gensim doc2vec model basically just encapsulating the word2vec model and the doc2vec process into a single object?</strong></p>
<p><strong>Is there anything else I'm missing about the process?</strong></p>
","python, r, gensim, word2vec, doc2vec","<p>In R, you can use text2vec (<a href=""https://cran.r-project.org/package=text2vec"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=text2vec</a>) to train Glove embeddings, word2vec (<a href=""https://cran.r-project.org/package=word2vec"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=word2vec</a>) to train word2vec embeddings or train fasttext embeddings (<a href=""https://cran.r-project.org/package=fastText"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=fastText</a> / <a href=""https://cran.r-project.org/package=fastTextR"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=fastTextR</a>). You can aggregate these embeddings to the document level by just taking e.g. the average of the words or relevant nouns/adjectives (if you tag the text using udpipe (<a href=""https://cran.r-project.org/package=udpipe"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=udpipe</a>) or use the approach from R package TextTinyR  (<a href=""https://cran.r-project.org/package=TextTinyR"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=TextTinyR</a>) which provides 3 other agrregation options: sum_sqrt / min_max_norm / idf</p>
<p>R package doc2vec (<a href=""https://cran.r-project.org/package=doc2vec"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=doc2vec</a>) allows one to train paragraph vector embeddings (PV-DBOW / PV-DM in Gensim terminology) which is not just averaging of word vectors but trains a specific model (e.g. see <a href=""https://www.bnosac.be/index.php/blog/103-doc2vec-in-r"" rel=""nofollow noreferrer"">https://www.bnosac.be/index.php/blog/103-doc2vec-in-r</a>).
ruimtehol (<a href=""https://cran.r-project.org/package=ruimtehol"" rel=""nofollow noreferrer"">https://cran.r-project.org/package=ruimtehol</a>) allows to train Starspace embeddings which has the option of training sentence embeddings as well</p>
",2,1,697,2021-06-17 20:09:41,https://stackoverflow.com/questions/68025964/gensim-is-doc2vec-a-model-or-operation-differences-from-r-implementation
Dutch pre-trained model not working in gensim,"<p>When trying to upload the fasttext model (cc.nl.300.bin) in gensim I get the following error:</p>
<pre><code>!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.bin.gz
!gunzip cc.nl.300.bin.gz
model = FastText_gensim.load_fasttext_format('cc.nl.300.bin')
model.build_vocab(cleaned_text, update=True)

AttributeError: 'FastTextTrainables' object has no attribute 'syn1neg'
</code></pre>
<p>The code goes wrong when building the vocab with my own dataset. The format of that dataset is all right, as I already used it to build and train other (not pre-trained) Word2Vec and FastText models.</p>
<p>I saw other had the same error on this blog, however their solution did not work for me: <a href=""https://github.com/RaRe-Technologies/gensim/issues/2588"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/2588</a></p>
<p>Also, I read somewhere that I should use 'load_facebook_model'? However I was not able to import load_facebook_model at all? Is this even a good way to solve this problem?</p>
<p>Any other suggestions?</p>
","gensim, fasttext","<p>Are you sure you're using the latest version of Gensim, <code>4.0.1</code>, with many improvements to the FastText implementation?</p>
<p>And, there you will definitely want to use <code>.load_facebook_model()</code> to load a full <code>.bin</code> Facebook-format model:</p>
<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model</a></p>
<p>But also note: the post-training expansion of the vocabulary is best considered an advanced &amp; experimental function. It may not offer any improvement on typical tasks - indeed, without careful consideration of tradeoffs &amp; balancing influence of later traiing against earlier, it can make things worse.</p>
<p>A <code>FastText</code> model trained on a large, diverse corpus may already be able to synthesize better-than-nothing guess vectors for out-of-vocabulary words, via its subword vectors.</p>
<p>If there's some data with very-different words &amp; word-senses you need to integrate, it will often be better to re-train from scratch, using an equal combination of all desired text influences. Then you'll be doing things in a standard and balanced way, without harder-to-tune and harder-to-evaluate improvised changes to usual practice.</p>
",1,0,432,2021-06-19 15:01:48,https://stackoverflow.com/questions/68048018/dutch-pre-trained-model-not-working-in-gensim
Cannot import name &#39;lemmatize&#39; from &#39;gensim.utils&#39; although I have installed Pattern,"<p>I try to use <code>lemmatize()</code> function from <code>gensim</code>. They said I have to install <code>pattern</code> also to use this function. I have already install both <code>gensim</code> and pattern but every time I try to import lemmatize from <code>gensim</code>, it keeps showing this error</p>
<p>I use <code>pip install gensim</code> and <code>pip install pattern</code> to install the libraries. My gensim version is 4.0.1 and pattern is 3.6</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\huynh\Desktop\machine_learning\test.py&quot;, line 1, in &lt;module&gt;
    from gensim.utils import lemmatize
ImportError: cannot import name 'lemmatize' from 'gensim.utils' (C:\Users\huynh\AppData\Local\Programs\Python\Python39\lib\site-packages\gensim\utils.py)
</code></pre>
<p>I tried to look up documentation about this but all I could find is that I have to install <code>pattern</code> to be able to use it. Does anyone have an idea why I still don't have <code>lemmatize()</code>? Thank you</p>
","python, gensim","<p>Gensim only ever previously wrapped the lemmatization routines of another library (<code>Pattern</code>) – which was not a particularly modern or well-maintained option, so it was removed from Gensim-4.0.</p>
<p>Users can choose &amp; apply their own lemmatization operations, if they think that's necessary, as a preprocessing step before applying Gensim's algorithms.</p>
<p>Some Python libraries offering lemmatization include:</p>
<ul>
<li>Pattern (Gensim's previously-included option, which can be used directly rather than through Gensim's old support): <a href=""https://github.com/clips/pattern"" rel=""nofollow noreferrer"">https://github.com/clips/pattern</a></li>
<li>NLTK: <a href=""https://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer"" rel=""nofollow noreferrer"">https://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer</a></li>
<li>UDPipe: <a href=""https://ufal.mff.cuni.cz/udpipe"" rel=""nofollow noreferrer"">https://ufal.mff.cuni.cz/udpipe</a></li>
<li>Spacy: <a href=""https://spacy.io/api/lemmatizer"" rel=""nofollow noreferrer"">https://spacy.io/api/lemmatizer</a></li>
<li>Stanza: <a href=""https://stanfordnlp.github.io/stanza/"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/</a></li>
</ul>
",2,4,4780,2021-06-19 19:34:52,https://stackoverflow.com/questions/68050134/cannot-import-name-lemmatize-from-gensim-utils-although-i-have-installed-pat
Why similar words will be close to each other under a word2vec model?,"<p>One of the tasks can be done with a word2vec model is to find most similar words for a give word using cosine similarity. What is the intuitive explanation for why similar words under a good word2vec model will be close to each other in the space?</p>
","gensim, word2vec, similarity, cosine-similarity","<p>The model is essentially performing compression.</p>
<p>It has to force a large number of distinct words (such as 100,000 or millions) into vectors of a much smaller number of dimensions (such as 100 or 300).</p>
<p>Necessarily, then, a simple &quot;one-hot&quot; encoding, where every word lights up one dimension entirely (<code>1.0</code>), and others not at all (<code>0.0</code>) is not possible. The words <em>must</em> use mixes of values across the small number of dimensions.</p>
<p>In order for such a model to become better on its training task – predicting nearby words – words that often have similar neighbors will grow to have similar coordinates. That allows a 'neighborhood' of the coordinate space to generate similar predictions.</p>
<p>But also, the remaining ways in which those words should predict <em>different</em> neighbors will also tend to be reflected in gradual coordinate changes (directions or neighborhoods).</p>
<p>For example, consider the case of polysemy. The words 'vault' &amp; 'bank' will both appear around 'money' (&amp; related concepts) - but 'bank' in another sense will also appear around 'river' &amp; 'water' (&amp; related concepts). So training in many contexts nudges 'bank' &amp; 'vault' to be more alike (to generate shared 'money'-related predictions). But then other interleaved example usage contexts then nudge 'bank' towards 'river'/'water'.</p>
<p>The net effect of these various tug-of-war backpropagation updates, by the end of sufficient &amp; well-parameterized training, tends to arrange the words in interesting ways - where similar words are nearer, and even certain directions/neighborhoods are vaguely associated with human-describable concepts. (For example, &quot;that direction&quot; might vaguely shift concepts towards money, &quot;this other direction&quot; in terms of gendered concepts, &quot;this 3rd direction&quot; in terms of colors, etc. - though such directions are necessarily fuzzy, imbued by patterns in the training data, can vary from run-to-run, and are not neatly aligned with each dimensions' perpendicula axes.)</p>
<p>Most generally: words that must, for model-optimization purposes, predict the same neighbor words tend to acqire similar coordinates. Their remaining differences are correlated with which neighboring words they need to predict diferently.</p>
",2,1,647,2021-06-22 19:21:57,https://stackoverflow.com/questions/68089626/why-similar-words-will-be-close-to-each-other-under-a-word2vec-model
How to retrofit a fasttext model?,"<p>I have read various research papers that one can retrofitting a fasttext model to improve its accuracy (<a href=""https://github.com/mfaruqui/retrofitting"" rel=""nofollow noreferrer"">https://github.com/mfaruqui/retrofitting</a>). However I am having trouble on how to implement it.</p>
<p>The github link above, will take one vector file and retrofitting it, output another vector file. I can load it using gensim library. However, since it is a vector file, it is no longer a model and it will not predict OOV (out-of-vocabulary) words. This makes it pointless. Is there a way to retrain the model somehow so it has better accuracy?</p>
","python, gensim, fasttext","<p>As far as I understand by reading the <a href=""https://arxiv.org/pdf/1411.4166.pdf"" rel=""nofollow noreferrer"">paper</a> and browsing the <a href=""https://github.com/mfaruqui/retrofitting"" rel=""nofollow noreferrer"">repository</a>, <strong>the proposed methodology only allows to improve the quality of the vectors</strong> (.vec) given in input.</p>
<p>As you can read <a href=""https://stackoverflow.com/questions/47118678/difference-between-fasttext-vec-and-bin-file"">here</a>, fastText's ability to represent out-of-vocabulary words is inherent in the .bin model (which contains the vectors for all the n-grams).</p>
<p><strong>As you too may have understood, there is no out-of-the-box way to retrofit a fastText model, using the proposed methodology.</strong></p>
",1,0,214,2021-06-23 13:01:01,https://stackoverflow.com/questions/68100358/how-to-retrofit-a-fasttext-model
Python gensim (TfidfModel): How is the Tf-Idf computed?,"<p><strong>1. For the below test text,</strong></p>
<pre><code>test=['test test', 'test toy']
</code></pre>
<p>the tf-idf score [without normalisation (smartirs: 'ntn')] is</p>
<pre><code>[['test', 1.17]]  
[['test', 0.58], ['toy', 1.58]]
</code></pre>
<p>This doesn't seem to tally with what I get via direct computation of</p>
<pre><code>tfidf (w, d) = tf x idf  
where idf(term)=log (total number of documents / number of documents containing term)   
tf = number of instances of word in d document / total number of words of d document  
</code></pre>
<p><strong>Eg</strong></p>
<pre><code>doc 1: 'test test'  
for &quot;test&quot; word  
tf= 1  
idf= log(2/2) = 0  
tf-idf = 0  
</code></pre>
<p>Can someone show me the computation using my above test text?</p>
<p><strong>2) When I change to cosine normalisation (smartirs:'ntc'), I get</strong></p>
<pre><code>[['test', 1.0]]  
[['test', 0.35], ['toy', 0.94]]
</code></pre>
<p>Can someone show me the computation too?</p>
<p>Thank you</p>
<pre><code>import gensim
from gensim import corpora
from gensim import models
import numpy as np
from gensim.utils import simple_preprocess

test=['test test', 'test toy']
 
texts = [simple_preprocess(doc) for doc in test]
 
mydict= corpora.Dictionary(texts)
mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in texts]
tfidf = models.TfidfModel(mycorpus, smartirs='ntn')
 
for doc in tfidf[mycorpus]:
    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])  
</code></pre>
","python, gensim, tf-idf","<p>If you care to know the details of the implementation of the <code>model.TfidfModel</code> you can check them directly in the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/tfidfmodel.py"" rel=""nofollow noreferrer"">GitHub repository for gensim</a>. The particular calculation scheme corresponding to <code>smartirs='ntn'</code> is described on the Wikipedia page for <a href=""https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System"" rel=""nofollow noreferrer"">SMART Information Retrieval System</a> and the exact calculations are different than the ones you use hence the difference in the results.</p>
<p>E.g. the particular discrepancy you are referring to:</p>
<pre><code>idf= log(2/2) = 0  
</code></pre>
<p>should actually be log2(N+1/n_k):</p>
<pre><code>idf= log(2/1) = 1  
</code></pre>
<p>I suggest that you review both the implementation and the mentioned page so that to ensure your manual checks follow the implementation for the chosen <code>smartirs</code> flags.</p>
",0,1,706,2021-06-28 06:47:21,https://stackoverflow.com/questions/68158729/python-gensim-tfidfmodel-how-is-the-tf-idf-computed
Gensim word2vec and large amount of texts,"<p>I need to put the texts contained in a column of a MySQL database (about 3 million rows) into a list of lists of tokens. These texts (which are tweets, therefore they are generally short) must be preprocessed before being included in the list (stop words, hashtags, tags etc. must be removed). This list should be passed later as a <code>Word2Vec</code> parameter. This is the part of the code involved</p>
<pre><code>import mysql.connector
import re
from gensim.models import Word2Vec
import preprocessor as p
p.set_options(
    p.OPT.URL,
    p.OPT.MENTION,
    p.OPT.HASHTAG,
    p.OPT.NUMBER
)

conn = mysql.connector.connect(...)
cursor = conn.cursor()
query = &quot;SELECT text FROM tweet&quot;
cursor.execute(query)
table = cursor.fetchall()

stopwords = open('stopwords.txt', encoding='utf-8').read().split('\n')
sentences = []
for row in table:
    sentences = sentences + [[w for w in re.sub(r'[^\w\s-]', ' ', p.clean(row[0])).lower().split() if w not in stopwords and len(w) &gt; 2]]

cursor.close()
conn.close()

model = Word2Vec(sentences)
...
</code></pre>
<p>Obviously it takes a lot of time and I know that my method is probably inefficient. Can anyone recommend a better one? I know it is not a question directly related to <code>gensim</code> and <code>Word2Vec</code> but perhaps those who use them have already faced the problem of working with a large amount of texts.</p>
","python, nlp, gensim, word2vec","<p>You haven't mentioned how long your code takes to run, but some potential sources of slowdown in your current technique might include:</p>
<ul>
<li>the overhead of regex-based preprocessing, especially if a large number of independent regexes are each applied, separately, to the same texts</li>
<li>the inefficiency of expanding a Python list by appending one new item at a time - which as the list grows larger can sometimes be a factor</li>
<li>virtual-memory swapping, if the size of your data exceeds physical RAM</li>
</ul>
<p>You can check the swapping issue by monitoring memory use using a platform-specific tool (like <code>top</code> on Linux systems) to view memory usage during the operation. If that's a contributor, using a machine with more RAM, or making other code changes to reduce RAM usage (see below), will help.</p>
<p>Your full <code>prprocessing</code> code isn't shown, but a common  approach is a lot of independent steps, each of which involves one or more regular-expressions, but then returns a plain modified string (for future steps).</p>
<p>As appealingly simple &amp; pluggable as that is, it often becomes a source of avoidable slowness in preprocessing large amounts of text. For example, each regex/step itself might have to repeat detecting token-boundaries, or splitting then re-concatenating a string. Or, the regexes might use complex match patterns, or techniques (like backtracking) that can be expensive on worst-case inputs.</p>
<p>Often this sort of preprocessing can be greatly improved by one or more of:</p>
<ul>
<li>coalescing multiple regexes into a single step, so a string faces one front-to-back pass, rather than N</li>
<li>breaking into short tokens early, then leaving the text as a list-of-tokens for later steps - thus never redundantly splitting/joining, and letting later token-oriented steps to work on smaller strings and perhaps even simpler (non-regex) string-tests</li>
</ul>
<p>Also, even if the preprocessing is still a bit time-consuming, a big process improvement is usually to be sure to <em>only repeat it when the data changes</em>. That is, if you're going to try a bunch of different downstream steps, like different <code>Word2Vec</code> parameters, make sure you're not doing the expensive preprocessing every time. Do it once, write the results aside to a file, then reuse the results file until it needs to be regenerated (because the data or preprocessing rules have changed).</p>
<p>Finally, if the append-one-more pattern is contributing to your slowness, you could pre-allocate your <code>sentences</code> (<code>sentences = [Null,] * desired_length</code>), then replace each row in your loop rather than append (<code>sentences[row_num] = preprocessed_text</code>). But that might not be a major factor, and in fact the suggestion above, about &quot;reuse the results file&quot;, is a better way to minimize list-ops/RAM-usage, as well as enable reuse across alternate runs.</p>
<p>That is, open a new working file before your loop. Append each preprocessed text – with spaces between the tokens, and a newline at the end – as one new line to this file. Then, have your <code>Word2Vec</code> step work directly from that file. (In Gensim, you can do this by wrapping the file with a <code>LineSentence</code> utility object, which reads a file of that format back as a re-iterable sequence, with each item being a list-of-tokens, <em>or</em> by using the <code>corpus_file</code> parameter to feed the filename directly to <code>Word2Vec</code>.)</p>
<p>From that list of possible tactics, I'd try:</p>
<ul>
<li>First, time your existing code for preprocessing (creating your <code>sentences</code></li>
<li>Then, <em>eliminate all fancy preprocessing</em>, doing nothing more complicated than <code>.split()</code>, and re-time. If there's a big change, then yes, the preprocessing is the major slowdown, and concentrate on improving that.</li>
<li>If even that minimal preprocessing still seems slower-than-desired, then maybe the RAM/concatenation issues are a concern, and try writing to an interim file.</li>
</ul>
<p>Separately: it's not strictly necessary to worry about removing stop-words in word2vec training - much published work doesn't bother with that step, and the algorithm already includes a <code>sample</code> parameter which causes it to skip a lot of the very-overrepresented words during training as less-interesting. Similarly, 2- and even 1- character tokens may still be interesting, especially in the domain of tweets, so you might not want to always discard them. (For example, lone emoji can be significant 'words'.)</p>
",2,0,434,2021-07-01 13:02:47,https://stackoverflow.com/questions/68210732/gensim-word2vec-and-large-amount-of-texts
Gensim Doc2Vec model returns different cosine similarity depending on the dataset,"<p>I trained two versions of doc2vec models with two datasets.</p>
<p>The first dataset was made with 2400 documents and the second one was made with 3000 documents including the documents which were used in the first dataset.</p>
<p>For an example,</p>
<p>dataset 1 = doc1, doc2, ... doc2400</p>
<p>dataset 2 = doc1, doc2, ... doc2400, doc2401, ... doc3000</p>
<p>I thought that both doc2vec models should return the same similarity score between doc1 and doc2, however, they returned different scores.</p>
<p>Does doc2vec model's result change upon the datasets even they include the same documents?</p>
","gensim, word2vec, doc2vec","<p>Yes, any addition to the training set will change the relative results.</p>
<p>Further, as explained in the Gensim FAQ, even re-training with the exact same data will typically result in different end coordinates for each training doc, though each run should be about equivalently useful:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ#q11-ive-trained-my-word2vec--doc2vec--etc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&amp;-FAQ#q11-ive-trained-my-word2vec--doc2vec--etc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism</a></p>
<p>What should remain roughly the same between runs is the <em>neighborhoods</em> around each document. That is, adding some extra training docs shouldn't change the general result that some candidate doc is &quot;very close&quot; or &quot;closer than other docs&quot; to some target doc - except to the extent that (1) the new docs might include some even-closer docs; and (2) a small amount of 'jitter' between runs, per the FAQ answer above.</p>
<p>If in fact you see <em>lots</em> of change in the relative neighborhoods and top-N neighbors of a document, either in repeated runs or runs with small increments of extra data, there's possibly something else wrong in the training.</p>
<p>In particular, 2400 docs is a pretty small dataset for <code>Doc2Vec</code> - smaller datasets might need smaller <code>vector_size</code> and/or more <code>epochs</code> and/or other tweaks to get more reliable results, and even then, might not show off the strengths of this algorithm on larger (tens-of-thousands to millions of docs) datasets.</p>
",0,0,258,2021-07-02 01:17:29,https://stackoverflow.com/questions/68218550/gensim-doc2vec-model-returns-different-cosine-similarity-depending-on-the-datase
Set the parameters of Word2Vec for a practical example,"<p>I have a database containing about 2.8 million texts (more precisely tweets, so they are short texts). I put clean tweets (removing hashtags, tags, stop words...) in a list of lists of tokens called <code>sentences</code> (so it contains a list of tokens for each tweet).</p>
<p>After these steps, if I write</p>
<p><code>model = Word2Vec(sentences, min_count=1)</code></p>
<p>I obtain a vocabulary of about 400,000 words.</p>
<p>This was just an attempt, I would need some help to set the parameters (<code>size</code>, <code>window</code>, <code>min_count</code>, <code>workers</code>, <code>sg</code>) of <code>Word2Vec</code> in the most appropriate and consistent way.</p>
<p>Consider that my goal is to use</p>
<p><code>model.most_similar(terms)</code> (where <code>terms</code> is a list of words)</p>
<p>to find, within the list of lists of tokens <code>sentences</code>, the words most similar to those contained in <code>terms</code>.</p>
<p>The words in <code>terms</code> belong to the same topic and I would like to see if there are other words within the texts that could have to do with the topic.</p>
","python, nlp, gensim, word2vec, word-embedding","<p>Generally, the usual approach is:</p>
<ul>
<li>Start with the defaults, to get things initially working at a baseline level, perhaps only on a faster-to-work-with subset of the data.</li>
<li>Develop an objective way to determine whether one model is better than another, for your purposes. This might start as a bunch of ad hoc, manual comparisons of results for some representative probes - but <em>should</em> become a process that can automatically score each variant model, giving a higher score to the 'better' model according to some qualitative, repeatable process.</li>
<li>Either tinker with parameters one-by-one, or run a large search over many permutations, to find which model does best on your scoring.</li>
</ul>
<p>Separately: the quality of word2vec results is almost always improved by discarding the very rarest words, such as those appearing only once. (The default value of <code>min_count</code> is <code>5</code> for good reason.)</p>
<p>The algorithm can't make good word-vectors from words that only appear once, or a few times. It <em>needs</em> multiple, contrasting examples of its usage. But, given the typical Zipfian distribution of word usages in a corpus, there are a lot of such rare words. Discarding them speeds training, shrinks the model, &amp; eliminates what's essentially 'noise' from the training of other words - leaving those remaining word-vectors much better. (If you really need vectors for such words – gather more data.)</p>
",2,1,1343,2021-07-02 12:56:58,https://stackoverflow.com/questions/68225624/set-the-parameters-of-word2vec-for-a-practical-example
Accessing MALLET&#39;s diagnostics file via Gensim,"<p>Is there a way to access MALLET's <a href=""http://mallet.cs.umass.edu/diagnostics.php"" rel=""nofollow noreferrer"">diagnostics file</a> or its content by using the provided API via <a href=""https://radimrehurek.com/gensim/index.html"" rel=""nofollow noreferrer"">Gensim</a> in Python?</p>
","python, nlp, gensim, evaluation, mallet","<p>Seems like there is no possibility.
I solved this issue by running MALLET in the command line via Python's subprocess module:</p>
<pre class=""lang-py prettyprint-override""><code>import subprocess
from pathlib import Path

MALLET_PATH = r&quot;C:\mallet&quot;  # set to where your &quot;bin/mallet&quot; path is

seglen = 500
topic_count = 20
start = 0
iterations = 20
num_threads = 10  # determines threads used for parallel training

# remember to change backslashes if needed
wdir = Path(&quot;../..&quot;)

corpusdir = wdir.joinpath(&quot;5_corpus&quot;, f&quot;seglen-{seglen}&quot;)
corpusdir.mkdir(exist_ok=True, parents=True)
mallet_dir = wdir.joinpath(&quot;6_evaluation/models/mallet&quot;, f&quot;seglen-{seglen}&quot;)

topic_dir = mallet_dir.joinpath(f&quot;topics-{topic_count}&quot;)


def create_input_files():
    # create MALLETs input files
    for file in corpusdir.glob(&quot;*.txt&quot;):
        output = mallet_dir.joinpath(f&quot;{file.stem}.mallet&quot;)
        # doesn't need to happen more than once -- usually.
        if output.is_file(): continue
        print(f&quot;--{file.stem}&quot;)
        cmd = f&quot;bin\\mallet import-file &quot; \
              f&quot;--input {file.absolute()} &quot; \
              f&quot;--output {output.absolute()} &quot; \
              f&quot;--keep-sequence&quot;
        subprocess.call(cmd, cwd=MALLET_PATH, shell=True)
    print(&quot;import finished&quot;)


def modeling():
    # start modeling
    for file in mallet_dir.glob(&quot;*.mallet&quot;):
        for i in range(start, iterations):
            print(&quot;iteration &quot;, str(i))
            print(f&quot;--{file.stem}&quot;)
            # output directory
            formatdir = topic_dir.joinpath(f&quot;{file.stem.split('-')[0]}&quot;)
            outputdir = formatdir.joinpath(f&quot;iteration-{i}&quot;)
            outputdir.mkdir(parents=True, exist_ok=True)
            outputdir = str(outputdir.absolute())
            # output files
            statefile = outputdir + r&quot;\topic-state.gz&quot;
            keysfile = outputdir + r&quot;\keys.txt&quot;
            compfile = outputdir + r&quot;\composition.txt&quot;
            diagnostics_xml = outputdir + r&quot;\diagnostics.xml&quot;
            # building cmd string
            cmd = f&quot;bin\\mallet train-topics &quot; \
                  f&quot;--input {file.absolute()} &quot; \
                  f&quot;--num-topics {topic_count} &quot; \
                  f&quot;--output-state {statefile} &quot; \
                  f&quot;--output-topic-keys {keysfile} &quot; \
                  f&quot;--output-doc-topics {compfile} &quot; \
                  f&quot;--diagnostics-file {diagnostics_xml} &quot; \
                  f&quot;--num-threads {num_threads}&quot;
            # call mallet
            subprocess.call(cmd, cwd=MALLET_PATH, shell=True)
    print(&quot;models trained&quot;)

#create_input_files()
modeling()
</code></pre>
",1,1,80,2021-07-05 16:15:03,https://stackoverflow.com/questions/68259216/accessing-mallets-diagnostics-file-via-gensim
Fine-tuning pre-trained Word2Vec model with Gensim 4.0,"<p>With Gensim &lt; 4.0, we can retrain a word2vec model using the following code:</p>
<pre><code>model = Word2Vec.load_word2vec_format(&quot;GoogleNews-vectors-negative300.bin&quot;, binary=True)
model.train(my_corpus, total_examples=len(my_corpus), epochs=model.epochs)
</code></pre>
<p>However, what I understand is that Gensim 4.0 is no longer supporting <code>Word2Vec.load_word2vec_format</code>. Instead, I can only load the keyedVectors.</p>
<p>How to fine-tune a pre-trained word2vec model (such as the model trained on GoogleNews) with my domain-specific corpus using Gensim 4.0?</p>
","gensim, word2vec, transfer-learning, pre-trained-model","<p>I don't think that code would've ever have worked in Gensim versions before 4.0. A plain list-of-word-vectors, like <code>GoogleNews-vectors-negative300.bin</code>, does not (&amp; never has) had enough info to continue training.</p>
<p>It's missing the hidden-to-output layer weights &amp; word-frequency info essential for training.</p>
<p>Looking at past source code, as of release 1.0.0 (February 2017), that code wouldn've already given a <a href=""https://github.com/RaRe-Technologies/gensim/blob/1.0.0/gensim/models/word2vec.py#L1304"" rel=""nofollow noreferrer"">deprecation-error with a pointer to the method for loading a plain set-of-word-vectors</a> - to address people with the mistaken notion that could work – and <a href=""https://github.com/RaRe-Technologies/gensim/blob/1.0.0/gensim/models/word2vec.py#L792"" rel=""nofollow noreferrer"">raised other errors on any attempts to <code>train()</code> such a model</a>. (Pre-1.0.0, docs also <a href=""https://github.com/RaRe-Technologies/gensim/blob/0.13.4.1/gensim/models/word2vec.py#L1137"" rel=""nofollow noreferrer"">warned that this would not work</a>, &amp; would have failed with a less-helpful error.)</p>
<p>As one of those errors mentioned, there has at times been experimental support for loading <em>some</em> of a prior set-of-word-vectors to clobber any words in an existing model's already-initialized vocabulary, via <code>.intersect_word2vec_format()</code>. But by default that both (1) locks the imported vectors against further change; (2) brings in no new words. That's unlike what people most often want from &quot;fine-tuning&quot;, so it's not a ready-made help for that goal.</p>
<p>I believe some people have cobbled together custom code to achieve various kinds of fine-tuning in their projects – but I don't know of anyone who's published a reliable recipe or strong results. (And I suspect some of the people who <em>think</em> they're doing this well just haven't rigorously evaluated the steps they are taking.)</p>
<p>If you have any recipe you know worked pre-Gensim-4.0.0, it should be adaptable - 4.0 changes to the <code>Word2Vec</code>-related classes were mainly refactorings, optimizations, &amp; new options (with little-to-none removal of functionality). But a reliable description of what used-to-work, or which particular fine-tuning strategy is being pursued for what specific benefits, to make more specific recommendations.</p>
",-1,0,1811,2021-07-08 08:42:56,https://stackoverflow.com/questions/68298289/fine-tuning-pre-trained-word2vec-model-with-gensim-4-0
Gensim W2V - UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0x80 in position 0: invalid start byte,"<p>I trained and saved a word2Vec model 'myWord2Vec.model' to pass it to a logistic regression model for training, but the vector size is bigger than my training dataset so, I needed to reduce the vector size. I tried the code below:</p>
<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format('myWord2Vec.model', limit=2021)
</code></pre>
<p>It gave me this error:</p>
<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
<p>I have no clue how to fix it nor how to reduce the vector size.
I would appreciate the help!</p>
","python-3.x, utf-8, gensim, word2vec","<p>How did you save <code>myWord2Vec.model</code>?</p>
<p>If you saved it with <code>.save()</code>, you need to load it with <code>.load()</code>, of the same model class. (And note: <code>.load()</code> is all-or-nothing, without any <code>limit=</code> option to just load part of the the set-of-vectors.)</p>
<p>Only if you saved with <code>.save_word2vec_format()</code> would it then be appropriate to load with <code>.load_word2vec_format()</code>. (And, trying to load the wrong-format file with <code>.load_word2vec_format()</code> could generate the kind of error you're seeing.)</p>
<p>Separately: <code>limit=2021</code> is a very strange option. Do you really want just 2,021 vectors loaded? (Usually people want at least tens-of-thousands to load.)</p>
<p>Also, your observation &quot;the vector size is bigger than my training dataset&quot; doesn't really make sense. Vector sizes in word2vec are most often 100-400 dimensions, if you have enough training data to support that size. You would more likely reduce the vector-size if you had <em>less</em> data, which was insufficient to train a higher-dimensional model. Reducing vector size <em>does</em> save a bit of memory. But, the model size is more a function of the vocabulary size (number of unique words) than training dataset site. And, if you truly have too large of a vocabulary to fit in memory, usually discarding lower-frequency words, by choosing a higher <code>min_count</code> option, is better than shrinking the <code>vector_size</code>.</p>
",1,0,1153,2021-07-12 15:44:30,https://stackoverflow.com/questions/68350237/gensim-w2v-unicodedecodeerror-utf-8-codec-cant-decode-byte-0x80-in-positio
Inconsistencies between bigrams found by TfidfVectorizer and Word2Vec model,"<p>I am building a topic model from scratch, one step of which uses the TfidfVectorizer method to get unigrams and bigrams from my corpus of texts:</p>
<pre><code>    tfidf_vectorizer = TfidfVectorizer(min_df=0.1, max_df=0.9, ngram_range = (1,2))
</code></pre>
<p>After topics are created, I use the similarity scores provided by gensim's Word2Vec to determine coherence of topics. I do this by training on the same corpus:</p>
<pre><code>    bigram_transformer = Phrases(corpus)
    model = Word2Vec(bigram_transformer[corpus], min_count=1)
</code></pre>
<p>For many of the bigrams in my topics however, I get a KeyError because that bigram was not picked up in the training of Word2Vec, despite them being trained on the same corpus. I think this is because Word2Vec decides on which bigrams to choose based on statistical analysis (<a href=""https://stackoverflow.com/questions/60108919/why-arent-all-bigrams-created-in-gensims-phrases-tool"">Why aren&#39;t all bigrams created in gensim&#39;s `Phrases` tool?</a>)</p>
<p>Is there a way to get the Word2Vec to include all those bigrams identified by TfidfVectorizer? I see trimming capabilities such as 'trim_rule' but not anything in the other direction.</p>
","python, nlp, gensim, word2vec, tfidfvectorizer","<p>The point of the <code>Phrases</code> model in Gensim is to pick <em>some</em> bigrams, which are calculated to be statistically-significant.</p>
<p>If you then apply that model's determinations as a preprocessing step on your corpus, certain pairs of unigrams will be outright replaced in your text with the combined bigram. (As such, it's possible some unigrams that were there originally will no longer appear even once.)</p>
<p>Thus the concepts of bigrams as used by Gensim's <code>Phrases</code> and the <code>TfidfVectorizer</code>'s <code>ngram_range</code> facility are different. <code>Phrases</code> is meant for destructive replacements where specific bigrams are inferred to be more interesting than the unigrams. <code>TfidfVectorizer</code> will add extra bigrams as additional dimensional features.</p>
<p>I suppose the right tuning of <code>Phrases</code> could cause it to consider every bigram as significant. Without checking, it looks like a super-tiny value, like <code>0.0000000001</code>, might have essentially that effect. (The <code>Phrases</code> class will reject a value of <code>0</code> as nonsensical given its usual use.)</p>
<p>But at that point, your later transformation (via <code>bigram_transformer[corpus]</code>) will combine every possible pair of words before <code>Word2Vec</code> training. For example, the sentence:</p>
<pre><code>['the', 'skittish', 'cat', 'jumped', 'over', 'the', 'gap',]
</code></pre>
<p>...would indiscriminately become...</p>
<pre><code>['the_skittish', 'cat_jumped', 'over_the', 'gap',]
</code></pre>
<p>It seems unlikely that you want that, for a number of reasons:</p>
<ul>
<li>There might then be no training texts with the <code>'cat'</code> unigram alone, leaving you with no word-vector for that word at all.</li>
<li>Bigrams that are rare or of little grammatical value (like  <code>'the_skittish'</code>) will receive trained word-vectors, &amp; take up space in the model.</li>
<li>The kinds of text corpus that are large enough for good <code>Word2Vec</code> results might have far more bigrams than are manageable. (A corpus small enought that you can afford to track every bigram may be on the thin side for good <code>Word2Vec</code> results.)</li>
</ul>
<p>Further, to perform that greedy-combination of <em>all</em> bigrams, the <code>Phrases</code> frequency-survey &amp; calculations aren't even necessary. (It can be done automatically with no preparation/analysis.)</p>
<p>So, you shouldn't expect every bigram of <code>TfidfVectorizer</code> to be get a word-vector, unless you take some extra steps, outside the normal behavior of <code>Phrases</code>, to ensure every such bigram was in the training texts.</p>
<p>To try to do so wouldn't necessarily need <code>Phrases</code> at all, and might be unmanageable, and involve other tradeoffs. (For example, I could imagine repeating the corpus many times, only combining a fraction of the bigrams each time – so that each is sometimes surrounded by other unigrams, and sometimes by other bigrams – to create a synthetic corpus with enough meaningful texts to create all your desired vectors. But the logic &amp; storage space for that model would be larger &amp; complicated, and without prominent precedent, so it'd be a novel experiment.)</p>
",2,1,206,2021-07-13 18:15:39,https://stackoverflow.com/questions/68367520/inconsistencies-between-bigrams-found-by-tfidfvectorizer-and-word2vec-model
Want to call words from a list but there is always a /n before every entry,"<p>Hello Community Members,</p>
<p>I would like to output the 1000 most frequently used words with frequency from a Gensim Word2Vec model. However, I am not interested in certain words, which I therefore filter using numpy (np.stdiff1d).After that I create a new list using '/n'.join, but now I have the problem that every time I call an entry from the list '/n'.join is entered in front of the word (e.g. instead of house /nhouse), so I get a key error.</p>
<p>I tried to work around it by saving the list (corpus_words) as .txt and “open with“, but even then, there is a /n in front of each entry, when I try to get the frequency of the word.</p>
<p>to use a print statement beforer &quot;/n&quot;.join(new_list) did not help either.</p>
<p>is there any way to fix this?</p>
<pre><code>Model_Pfad = r'D:\OneDrive\Phyton\modelC.model'
ausgabe= open('D:\OneDrive\Phyton\wigbelsZahlen.txt', 'w')

model = Word2Vec.load(Model_Pfad)


x = list(model.wv.index_to_key[:1000])

stop_words = set ([&quot;an&quot;,
              'as',
              'art',
              'ab',
              'al',
            &quot;aber&quot;,
            &quot;abk.&quot;,
            &quot;alle&quot;,
            &quot;allem&quot;,
            &quot;allen&quot;,
            &quot;aller&quot;,
            &quot;alles&quot;,
            &quot;allg.&quot;
            ])

new_list = [item for item in x if item not in stop_words]

for i in new_list:
    result = model.wv.get_vecattr(i, &quot;count&quot;)
    ausgabe.write(i + '\t' + str(result))
    ausgabe.write('\n')
ausgabe.close

</code></pre>
","python, list, nltk, gensim","<p>First, <code>np.setdiff1d()</code> is a somewhat odd way to remove items from a list. More typical would be to use a list comprehension:</p>
<pre><code>stop_words = set(['an',v'as', 'art', 'ab', 'al'])
new_list = [item for item in x if item not in stop_words
</code></pre>
<p>Second, your code as currently shown then uses <code>.join</code> to re-composes all the words into one big string, with <code>'\n'</code> between them, and appends that one big string to a file.</p>
<p>So of course that's all that'll be in the file.</p>
<p>Also, that one big <code>corpus_words</code> string is <strong>not</strong> going to be a good argument for <code>.get_vecattr()</code>, which wants a single word key. (I'd expect your line <code>model.wv.get_vecattr(corpus_words, &quot;count&quot;)</code> to <code>KeyError</code> before any printing-to-file is even attempted.)</p>
<p>There's nothing in your code as shown which would remove the <code>'\n'</code> characters, nor anything that would add the frequency numbers, nor re-read the file in any way or look up frquencies in any way. Is some of the code still missing?</p>
<p>Is your ultimate goal simply to have a text-file report of the 1,000 most common words, or to be able to look up individual frequencies in later code?</p>
",0,0,62,2021-07-18 13:41:59,https://stackoverflow.com/questions/68429677/want-to-call-words-from-a-list-but-there-is-always-a-n-before-every-entry
Genesis most_similar find synonym only (not antonyms),"<p>Is there a way to let <code>model.wv.most_similar</code> in gensim return positive-meaning words only (i.e. that shows synonyms but not antonyms)?</p>
<p>For example, if I do:</p>
<pre><code>import fasttext.util
from gensim.models.fasttext import load_facebook_model
from gensim.models.fasttext import FastTextKeyedVectors
fasttext.util.download_model('en', if_exists='ignore')  # English
model = load_facebook_model('cc.en.300.bin')
model.wv.most_similar(positive=['honest'], topn=2000)
</code></pre>
<p>Then the mode is also going to return words such as &quot;dishonest&quot;.</p>
<pre><code>('dishonest', 0.5542981028556824),
</code></pre>
<p>However, what if I want words with the positive-meaning only?</p>
<p>I have tried the following - subtracting &quot;not&quot; from &quot;honest&quot; in the vector space:</p>
<pre><code>import fasttext.util
from gensim.models.fasttext import load_facebook_model
from gensim.models.fasttext import FastTextKeyedVectors
fasttext.util.download_model('en', if_exists='ignore')  # English
model = load_facebook_model('cc.en.300.bin')
model.wv.most_similar(positive=['honest'], negative=['not'], topn=2000)
</code></pre>
<p>But somehow it is still returning &quot;dishonest&quot; somehow.</p>
<pre><code>('dishonest', 0.23721608519554138)
('dishonesties', 0.16536088287830353)
</code></pre>
<p>Any idea how to do this in a better way?</p>
","python, nlp, gensim, word2vec, fasttext","<p>Unfortunately, the vector-space created by word2vec algorithm training <em>doesn't</em> neatly match our human, intuitive understandin of pure-synonymity.</p>
<p>Rather, word2vec's sense of 'similarity' is more general - and overall, antonyms tend to be quite similar to each other: they're used in similar contexts (the driving force of word2vec training), about the same topics.</p>
<p>And further even though many understandable contrasts do vaguely correlate with various directions, there is no universal &quot;opposite&quot; (or &quot;positive&quot;) direction. So composing <code>'not'</code> with a word <em>doesn't</em> neatly invert the dominant sense of a word, and <code>'honest' + 'not'</code> won't reliably help find the direction of <code>'dishonest'</code>.</p>
<p>Barring finding some extra technique for this task beyond basic word2vec (in other research literature or via your own experimentation), the best you may be able to do is using already-known unwanted answers to further refine the results. That is, something like the following <em>might</em> offer marginally-improved results:</p>
<pre><code>word_vecs.most_similar(positive=['honest'], negative=['dishonest'])
</code></pre>
<p>(Further expanding the examples with more related words, either of the kind you want or not, might also help.)</p>
<p>See also some of the comments &amp; links in a previous answer for more ideas: <a href=""https://stackoverflow.com/a/44491124/130288"">https://stackoverflow.com/a/44491124/130288</a></p>
",1,2,385,2021-07-19 03:32:57,https://stackoverflow.com/questions/68434829/genesis-most-similar-find-synonym-only-not-antonyms
Gensim sort_by_descending_frequency changes most_similar results,"<p>It seems that when retrieving the most similar word vectors, sorting by word frequency will change the results in <code>Gensim</code>.</p>
<p>Before sorting:</p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import common_texts  # some example sentences
print(len(common_texts))
model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
model.build_vocab(corpus_iterable=common_texts)
model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=1)  

model.wv.most_similar(positive=[&quot;human&quot;])
</code></pre>
<blockquote>
<pre><code>[('interface', 0.7432922720909119),
 ('minors', 0.6719315052032471),
 ('time', 0.3513716757297516),
 ('computer', 0.05815044790506363),
 ('response', -0.11714297533035278),
 ('graph', -0.15643596649169922),
 ('eps', -0.2679084539413452),
 ('survey', -0.34035828709602356),
 ('trees', -0.63677978515625),
 ('user', -0.6500451564788818)]
</code></pre>
</blockquote>
<p>However, if I sort the vectors by descending frequency:</p>
<pre><code>model.wv.sort_by_descending_frequency()

model.wv.most_similar(positive=[&quot;human&quot;])
</code></pre>
<blockquote>
<pre><code>[('minors', 0.9638221263885498),
 ('time', 0.6335864067077637),
 ('interface', 0.40014874935150146),
 ('computer', 0.03224882856011391),
 ('response', -0.14850640296936035),
 ('graph', -0.2249641716480255),
 ('survey', -0.26847705245018005),
 ('user', -0.45202943682670593),
 ('eps', -0.497650682926178),
 ('trees', -0.6367797255516052)]
</code></pre>
</blockquote>
<p>The most similar word ranking as well as the word similarities change. Any idea why?</p>
<p><strong>Update:</strong></p>
<p>Before calling sort:</p>
<pre><code>model.wv.index_to_key
</code></pre>
<blockquote>
<pre><code>['system',
 'graph',
 'trees',
 'user',
 'minors',
 'eps',
 'time',
 'response',
 'survey',
 'computer',
 'interface',
 'human']
</code></pre>
</blockquote>
<pre><code>model.wv.expandos['count']
</code></pre>
<blockquote>
<p>array([4, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2])</p>
</blockquote>
<p>After calling sort:</p>
<pre><code>model.wv.index_to_key
</code></pre>
<blockquote>
<pre><code>['system',
 'user',
 'trees',
 'graph',
 'human',
 'interface',
 'computer',
 'survey',
 'response',
 'time',
 'eps',
 'minors']
</code></pre>
</blockquote>
<pre><code>model.wv.expandos['count']
</code></pre>
<blockquote>
<p>array([4, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2])</p>
</blockquote>
","python, nlp, gensim, word2vec, fasttext","<p>That change-of-reported similarities definitely <strong>shouldn't</strong> happen, so something is surely going wrong here. (Maybe, cached-subword info isn't re-sorting.)</p>
<p>But also note:</p>
<ul>
<li>That method wasn't particularly meant for use <em>after</em> training - indeed, you should be seeing a warning message if using it that way.</li>
<li>Such a sort <em>should already happen by default</em> in all 2Vec algorithms at the end of vocab-discovery phase - it's the usual behavior, only rarely turned off. So requesting it again should at most be a no-op.</li>
</ul>
<p>To dig into what may be gowing wrong, can you edit your question to show the values of both…</p>
<ul>
<li><code>model.wv.index_to_key</code></li>
<li><code>model.wv.expandos['count']</code></li>
</ul>
<p>…before <em>and</em> after the <code>. sort_by_descending_frequency()</code> call?</p>
",1,1,382,2021-07-20 08:40:25,https://stackoverflow.com/questions/68451937/gensim-sort-by-descending-frequency-changes-most-similar-results
Does Spacy support multiple GPUs?,"<p>I was wondering if Spacy supports multi-GPU via <a href=""https://mpi4py.readthedocs.io/en/stable/tutorial.html#running-python-scripts-with-mpi"" rel=""nofollow noreferrer"">mpi4py</a>?</p>
<p>I am currently using Spacy's nlp.pipe for Named Entity Recognition on a high-performance-computing cluster that supports the MPI protocol and has many GPUs. It says <a href=""https://github.com/explosion/spaCy/issues/3394"" rel=""nofollow noreferrer"">here</a> that I would need to specify the GPU to use with cupy, but with PyMPI, I am not sure if the following will work (should I import spacy after calling cupy device?):</p>
<pre><code>
from mpi4py import MPI
import cupy

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    data = [&quot;His friend Nicolas J. Smith is here with Bart Simpon and Fred.&quot;*100]
else:
    data = None

unit = comm.scatter(data, root=0)

with cupy.cuda.Device(rank):
    import spacy
    from thinc.api import set_gpu_allocator, require_gpu
    set_gpu_allocator(&quot;pytorch&quot;)
    require_gpu(rank)
    nlp = spacy.load('en_core_web_lg')
    nlp.add_pipe(&quot;merge_entities&quot;)
    tmp_list = []
    for doc in nlp.pipe(unit):
        res = &quot; &quot;.join([t.text if not t.ent_type_ else t.ent_type_ for t in doc])
        tmp_list.append(res)

result = comm.gather(tmp_list, root=0)

if comm.rank == 0:
    print (result)
else:
    result = None

</code></pre>
<p>Or if i have 4 GPUs on the same machine and I do not want to use MPI, can I do the following:</p>
<pre><code>from joblib import Parallel, delayed
import cupy

rank = 0

def chunker(iterable, total_length, chunksize):
    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))

def flatten(list_of_lists):
    &quot;Flatten a list of lists to a combined list&quot;
    return [item for sublist in list_of_lists for item in sublist]

def process_chunk(texts):
    with cupy.cuda.Device(rank):
        import spacy
        from thinc.api import set_gpu_allocator, require_gpu
        set_gpu_allocator(&quot;pytorch&quot;)
        require_gpu(rank)
        preproc_pipe = []
        for doc in nlp.pipe(texts, batch_size=20):
            preproc_pipe.append(lemmatize_pipe(doc))
        rank+=1
        return preproc_pipe

def preprocess_parallel(texts, chunksize=100):
    executor = Parallel(n_jobs=4, backend='multiprocessing', prefer=&quot;processes&quot;)
    do = delayed(process_chunk)
    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))
    result = executor(tasks)
    return flatten(result)

preprocess_parallel(texts = [&quot;His friend Nicolas J. Smith is here with Bart Simpon and Fred.&quot;*100], chunksize=1000)
</code></pre>
","python-3.x, nlp, mpi, spacy, gensim","<p>I think I have figured out how to do this:</p>
<p>The key is to instruct cupy to use a new GPU.</p>
<pre><code>import multiprocessing as mp
mp.set_start_method('spawn', force=True)
from joblib import Parallel, delayed
from itertools import cycle
import cupy
import spacy
from thinc.api import set_gpu_allocator, require_gpu


def chunker(iterable, total_length, chunksize):
    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))

def flatten(list_of_lists):
    &quot;Flatten a list of lists to a combined list&quot;
    return [item for sublist in list_of_lists for item in sublist]

def process_entity(doc):
    super_word_ls = []
    for s in doc.sents:
        word_ls = []
        for t in s:
            if not t.ent_type_:
                if (t.text.strip()!=&quot;&quot;):
                    word_ls.append(t.text)
            else:
                word_ls.append(t.ent_type_)
        if len(word_ls)&gt;0:
            super_word_ls.append(&quot; &quot;.join(word_ls))
    return &quot; &quot;.join(super_word_ls)

def process_chunk(texts, rank):
    print(rank)
    with cupy.cuda.Device(rank):
        set_gpu_allocator(&quot;pytorch&quot;)
        require_gpu(rank)
        nlp = spacy.load(&quot;en_core_web_trf&quot;)
        preproc_pipe = []
        for doc in nlp.pipe(texts, batch_size=20):
            preproc_pipe.append(process_entity(doc))
        rank+=1
        return preproc_pipe


def preprocess_parallel(texts, chunksize=100):
    executor = Parallel(n_jobs=2, backend='multiprocessing', prefer=&quot;processes&quot;)
    do = delayed(process_chunk)
    tasks = []
    gpus = list(range(0, cupy.cuda.runtime.getDeviceCount()))
    rank = 0
    for chunk in chunker(texts, len(texts), chunksize=chunksize):
        tasks.append(do(chunk, rank))
        rank = (rank+1)%len(gpus)
    result = executor(tasks)
    return flatten(result)

if __name__ == '__main__':
    print(preprocess_parallel(texts = [&quot;His friend Nicolas J. Smith is here with Bart Simpon and Fred.&quot;]*100, chunksize=50))
</code></pre>
",1,2,1036,2021-07-21 10:50:14,https://stackoverflow.com/questions/68468195/does-spacy-support-multiple-gpus
word-embedding: Convert supervised model into unsupervised model,"<p>I want to load an pre-trained embedding to initialize my own unsupervise FastText model and retrain with my dataset.</p>
<p>The trained embedding file I have loads fine with <code>gensim.models.KeyedVectors.load_word2vec_format('model.txt')</code>. But when I try:
<code>FastText.load_fasttext_format('model.txt')</code> I get: <code>NotImplementedError: Supervised fastText models are not supported</code>.</p>
<p>Is there any way to convert supervised KeyedVectors to unsupervised FastText? And if possible, is it a bad idea?</p>
<p>I know that has an great difference between supervised and unsupervised models. But I really wanna try use/convert this and retrain it. I'm not finding a trained unsupervised model to load for my case (it's a portuguese dataset), and the best model I find <a href=""http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc"" rel=""nofollow noreferrer"">is that</a></p>
","python, nlp, gensim, unsupervised-learning, fasttext","<p>If your <code>model.txt</code> file loads OK with <code>KeyedVectors.load_word2vec_format('model.txt')</code>, then that's just a simple set of word-vectors. (That is, not a 'supervised' model.)</p>
<p>However, Gensim's <code>FastText</code> doesn't support preloading a simple set of vectors for further training - for continued training, it needs a full <code>FastText</code> model, either from Facebook's binary format, or a prior Gensim <code>FastText</code> model <code>.save()</code>.</p>
<p>(That trying to load a plain-vectors file generates that error suggests the <code>load_fasttext_format()</code> method is momentarily mis-interpreting it as some other kind of binary FastText model it doesn't support.)</p>
<p><em>Update after comment below:</em></p>
<p>Of course you <em>can</em> mutate a model however you like, including ways not officially supported by Gensim. Whether that's helpful is another matter.</p>
<p>You can create an FT model with a compatible/overlapping vocabulary, load old word-vectors separately, then copy each prior vector over to replace the corresponding (randomly-initialized) vectors in the new model. (Note that the property to affect further training is actually <code>ftModel.wv.vectors_vocab</code> trained-up full-word vectors, <em>not</em> the <code>.vectors</code> which is composited from full-words &amp; ngrams,)</p>
<p>But the tradeoffs of such an ad-hoc strategy are many. The ngrams would still start random. Taking some prior model's just-word vectors isn't quite the same as a FastText model's full-words-to-be-later-mixed-with-ngrams.</p>
<p>You'd want to make sure your new model's sense of word-frequencies is meaningful, as those affect further training - but that data <em>isn't</em> usually available with a plain-text prior word-vector set. (You could plausibly synthesize a good-enough set of frequencies by assuming a Zipf distribution.)</p>
<p>Your further training might get a &quot;running start&quot; from such initialization - but that wouldn't necessarily mean the end-vectors remain comparable to the starting ones. (All positions may be arbitrarily changed by the volume of newer training, progressively diluting away most of the prior influence.)</p>
<p>So: you'd be in an improvised/experimental setup, somewhat far from usual FastText practices and thus where you'd want to re-verify lots of assumptions, and rigorously evaluate if those extra steps/approximations are actually improving things.</p>
",1,0,160,2021-07-21 15:18:42,https://stackoverflow.com/questions/68472183/word-embedding-convert-supervised-model-into-unsupervised-model
Doing gensim text summarization in each row python,"<p>I have a dataset that looks like this (not the actual values, but just to get the idea of it):</p>
<pre class=""lang-none prettyprint-override""><code>id  text                                      group 
1   what is the difference and why is it ...  2
2   let me introduce myself, first.           1 
</code></pre>
<p>The length of the &quot;text&quot; column can be from one sentence to many sentences. What I'm trying to do is to summarize each text from the row and save the summarized text in a new column. I'm using gensim for summarization.</p>
<p>My desired output is as follows, and please disregard the content.</p>
<pre class=""lang-none prettyprint-override""><code>id  text                                     group  text_summary 
1   what is the difference and why is it ...  2     the difference between object a and b 
2   let me introduce myself, first.           1     let me introduce myself, first.
</code></pre>
<p>Below is the code I used, but I'm getting the following error.</p>
<pre class=""lang-py prettyprint-override""><code>import gensim 
from gensim.summarization import summarize 
from gensim.summarization import keywords 

for i in range(0, df.shape[0]):
    text = df.iloc[i]['Answers']
    if len(text) &gt; 1:
        df.loc[i, 'summary_answer'] = summarize(text)
    else: 
        df.loc[i, 'summary_answer'] = text
</code></pre>
<p><a href=""https://i.sstatic.net/ytMQw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ytMQw.png"" alt=""enter image description here"" /></a></p>
<p>I understand the problem, but my <code>if/else</code> statement seems to not work in this case.</p>
","python, gensim","<p>Your code should probably be more like this:</p>
<pre><code>def summary_answer(text):
    try:
        return summarize(text)
    except ValueError:
        return text
df['summary_answer'] = df['Answers'].apply(summary_answer)
</code></pre>
<p>Edit:
The above code was quick code to solve the original error, it returns the original text if the <code>summarize</code> call raises an exception. You can of course add more complicated logic to the function if this one doesn't cut it. Some simple examples:</p>
<pre><code>def summary_answer(text):
    try:
        if not isinstance(text,str):#data of wrong type
            return 'not text'
        ans = summarize(text)
        if len(ans.split())&gt;3:#summary must be longer than 3 words
            return ans
    except ValueError:
        pass
    return text
</code></pre>
",0,1,576,2021-07-23 00:38:59,https://stackoverflow.com/questions/68492838/doing-gensim-text-summarization-in-each-row-python
Structure of Gensim Word Embedding corpus,"<p>I want to train a word2vec model using Gensim. I preprocessed my corpus, which is made of hundreds of thousands of articles from a specific newspaper. I preprocessed them (lower casing, lemmatizing, removing stop words and punctuations, etc.) and then make a list of lists, in which each element is a list of words.</p>
<pre><code>corpus = [['first', 'sentence', 'second', 'dictum', 'third', 'saying', 'last', 'claim'],
          ['first', 'adage', 'second', 'sentence', 'third', 'judgment', 'last', 'pronouncement']]
</code></pre>
<p>I wanted to know if it is the right way, or it should be like the following:</p>
<pre><code>corpus = [['first', 'sentence'], ['second', 'dictum'], ['third', 'saying'], ['last', 'claim'], ['first', 'adage'], ['second', 'sentence'], ['third', 'judgment'], ['last', 'pronouncement']]
</code></pre>
","gensim, word2vec, word-embedding, corpus","<p>Both would minimally work.</p>
<p>But in the second, no matter how big your <code>window</code> parameter, the fact all texts are no more than 2 tokens long means words will only affect their immediate neighbors. That's probably not what you want.</p>
<p>There's no real harm in longer texts, except to note that:</p>
<ul>
<li>Tokens all in the same list will appear in each other's <code>window</code>-sized neighborhood - so don't run words together that shouldn't imply any realistic use alongside each other. (But, in large-enough corpuses, even the noise of some run-together unrelated texts won't make much difference, swamped by the real relationships in the bulk of the texts.)</li>
<li>Each text shouldn't be more than 10,000 tokens long, as an internal implementation limit will cause any tokens beyond that limit to be ignored.</li>
</ul>
",2,0,73,2021-07-27 21:45:48,https://stackoverflow.com/questions/68552107/structure-of-gensim-word-embedding-corpus
LDA Mallet Gensim CalledProcessError,"<p>Seems like many people are having issues with Mallet.</p>
<pre><code>import os
from gensim.models.wrappers import LdaMallet

os.environ.update({'MALLET_HOME':r'C:/Users/myusername/Desktop/Topic_Modelling/mallet-2.0.8'})

mallet_path = r'C:/Users/myusername/Desktop/Topic_Modelling/mallet-2.0.8/bin/mallet' 

model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus,num_topics=num_topics, id2word=id2word)
</code></pre>
<p>Getting the following errors:</p>
<pre><code>/bin/sh: C:/Users/myusername/Desktop/Topic_Modelling/mallet-2.0.8/bin/mallet.bat: No such file or directory

CalledProcessError: Command 'C:/Users/myusername/Desktop/Topic_Modelling/mallet-2.0.8/bin/mallet.bat import-file --preserve-case --keep-sequence --remove-stopwords --token-regex &quot;\S+&quot; --input /var/folders/ml/lxzrtxwn02vbvq65c80g1b640000gn/T/c52cdc_corpus.txt --output /var/folders/ml/lxzrtxwn02vbvq65c80g1b640000gn/T/c52cdc_corpus.mallet' returned non-zero exit status 127.
</code></pre>
<p>I downloaded mallet from <a href=""http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip"" rel=""nofollow noreferrer"">http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip</a> and unzipped it in my directory. I've tried running the command in the error in the terminal and I'm getting the same 'no such file found' error, but it's there in my directory?</p>
<p>I've also followed this: <a href=""https://ps.au.dk/fileadmin/ingen_mappe_valgt/installing_mallet.pdf"" rel=""nofollow noreferrer"">https://ps.au.dk/fileadmin/ingen_mappe_valgt/installing_mallet.pdf</a></p>
<p>When I go to the directory via command line and type <code>./bin/mallet</code> I get a whole bunch of commands, which according to the instructions, is what I'm looking for to know that it's been installed ok.</p>
<p>I'm running the following on MacOS</p>
<ul>
<li>Python==3.9.6</li>
<li>gensim==3.8.3</li>
</ul>
<p>Anyone have any ideas?</p>
","python, gensim","<p>As silly as this sounds, I resolved this by changing the path to:</p>
<pre><code>os.environ.update({'MALLET_HOME':r'mallet-2.0.8'})

mallet_path = r'mallet-2.0.8/bin/mallet' 
</code></pre>
<p>So if you have the mallet directory in the same one as where your code is, this will work!</p>
",0,0,399,2021-07-29 13:50:21,https://stackoverflow.com/questions/68577177/lda-mallet-gensim-calledprocesserror
"Gensim doc2vec produce more vectors than given documents, when I pass unique integer id as tags","<p>I'm trying to make documents vectors of gensim example using doc2vec.
I passed TaggedDocument which contains 9 docs and 9 tags.</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
idx = [0,1,2,3,4,5,6,7,100]
documents = [TaggedDocument(doc, [i]) for doc, i in zip(common_texts, idx)]
model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)
</code></pre>
<p>and it produces 101 vectors like this image.
<a href=""https://i.sstatic.net/b1ciY.png"" rel=""nofollow noreferrer"">gensim doc2vec produced 101 vectors</a></p>
<p>and what I want to know is</p>
<ol>
<li>How can I be sure that the tag I passed is attached to the right vector?</li>
<li>How did the vectors with the tags which I didn't pass (8~99 in my case) come out? Were they computed as a blank?</li>
</ol>
","machine-learning, gensim, word-embedding, doc2vec","<p>If you use plain ints as your document-tags, then the <code>Doc2Vec</code> model will allocate enough doc-vectors for every int up to the highest int you provide - even if you don't use some of those ints.</p>
<p>This assumption, that all ints up to the highest declared are used, allows the code to avoid creating a redundant {tag -&gt; slot} dictionary, saving a little memory. That specific potential savings is the main reason for supporting plain ints (rather than unique strings) as tag names.</p>
<p>Any such doc-vectors allocated but never subject to any traiing will be randomly-initialized the same as others - but never adjusted by training.</p>
<p>If you want to use plain int tag names, you should either be comfortable with this over-allocation, or make sure you only use all contiguous int IDs from <code>0</code> to your max ID, with none ununused. But unless your training data is very large, using unique string tags, and allowing the {tag -&gt; slot} dictionary to be created, is straightforward and not too expensive in memory.</p>
<p>(Separately: <code>min_count=1</code> is almost always a bad idea in these algorithms, as discarding rare tokens tends to give better results than letting their thin example usages interfere with other training.)</p>
",1,0,201,2021-07-31 16:39:33,https://stackoverflow.com/questions/68604006/gensim-doc2vec-produce-more-vectors-than-given-documents-when-i-pass-unique-int
How to find similar Sentences using FastText ( Sentences with Out of Vocabulary words),"<p>I am trying to create an NLP model which can find similar sentences. For example, It should be able to say that &quot;Software Engineer&quot;, &quot;Software Developer&quot;, &quot;Software Dev&quot;, &quot;Soft Engineer&quot; are similar sentences.</p>
<p>I have a dataset with a list of roles such as Cheif Executives, Software Engineer and the variation of these terms will be unknown ( out of vocabulary).</p>
<p>I am trying to use fastText with Gensim but struggling.
Does anyone have suggested readings/ tutorials that might help me?</p>
","machine-learning, nlp, gensim, fasttext","<p>A mere list-of-roles may not be enough data for FastText (and similar word2vec-like algorithms), which need to see words (or tokens) <strong>in natural ussage contexts</strong>, alongside other related words, to gradually nudge them into interesing relative-similarity alignments.</p>
<p>Do you just have the titles, or other descriptions of the roles?</p>
<p>To the extent that the titles are composed of individual words, which in their title-context mostly mean the same as in normal contexts, and they are very short (2-3 words each), one potential approach is to try the &quot;word mover's distance&quot; (WMD) metric.</p>
<p>You'd want good word-vectors trained from elsewhere with good contexts and compatible word senses, so that the vectors for <code>'software'</code>, '<code>engineer'</code>, etc individually are all reasonably good. Then you could use the <code>.wmdistance()</code> method in Gensim's word-vector classes to calculate a measure of how much, across all of a texts words, one run-of-words differs from another run-of-words.</p>
<p><strong>Update</strong>: Note that for the values from WMD (and those from cosine-similarity), you generally <em>shouldn't</em> obsess over their absolute values, only how they affect relative rankings. That is, no matter what raw value <code>wmd(['software', 'engineer'], ['electric', 'engineer'])</code> returns, be it <code>0.01</code> or <code>100</code>, the important measure is how that number compares to other pairwise comparisons, like say <code>wmd(['software', 'engineer'], ['software', 'developer'])</code>.</p>
",3,2,965,2021-08-03 10:35:03,https://stackoverflow.com/questions/68634515/how-to-find-similar-sentences-using-fasttext-sentences-with-out-of-vocabulary
AttributeError: &#39;Word2Vec&#39; object has no attribute &#39;most_similar&#39; (Word2Vec),"<p>I am using Word2Vec and using a wiki trained model that gives out the most similar words. I ran this before and it worked but now it gives me this error even after rerunning the whole program. I tried to take off  <code>return_path=True</code> but im still getting the same error</p>
<pre><code>print(api.load('glove-wiki-gigaword-50', return_path=True))
model.most_similar(&quot;glass&quot;)
</code></pre>
<p>#ERROR:</p>
<pre><code>/Users/me/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-153-3bf32168d154&gt; in &lt;module&gt;
      1 print(api.load('glove-wiki-gigaword-50', return_path=True))
----&gt; 2 model.most_similar(&quot;glass&quot;) 

AttributeError: 'Word2Vec' object has no attribute 'most_similar'
</code></pre>
<p>#MODEL
this is the model I used</p>
<pre><code>    print(
        '%s (%d records): %s' % (
            model_name,
            model_data.get('num_records', -1),
            model_data['description'][:40] + '...',
        )
    )
</code></pre>
<p>Edit: here is my gensim download &amp; output</p>
<pre><code>!python -m pip install -U gensim
</code></pre>
<p>OUTPUT:</p>
<p>Requirement already satisfied: gensim in ./opt/anaconda3/lib/python3.8/site-packages (4.0.1)</p>
<p>Requirement already satisfied: numpy&gt;=1.11.3 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)</p>
<p>Requirement already satisfied: smart-open&gt;=1.8.1 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (5.1.0)</p>
<p>Requirement already satisfied: scipy&gt;=0.18.1 in ./opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)</p>
","python, nlp, gensim, word2vec, doc2vec","<p>You are probably looking for <code>&lt;MODEL&gt;.wv.most_similar</code>, so please try:</p>
<pre><code>model.wv.most_similar(&quot;glass&quot;) 
</code></pre>
",18,5,17563,2021-08-06 05:41:06,https://stackoverflow.com/questions/68676637/attributeerror-word2vec-object-has-no-attribute-most-similar-word2vec
Creating Corpus from wiki dump file using Jupyter notebook,"<p>I'm trying to follow this page to create a wiki corpus, but I'm using Jupiter notebook <a href=""https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html"" rel=""nofollow noreferrer"">https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html</a></p>
<p>this is my code:</p>
<pre><code>import sys
from gensim.test.utils import datapath
from gensim.corpora import WikiCorpus

path_to_wiki_dump = datapath(&quot;enwiki-latest-pages-articles.xml.bz2&quot;)

wiki = WikiCorpus(path_to_wiki_dump)

output = open('wiki_en.txt', 'w',  encoding='utf-8')

i = 0
for text in wiki.get_texts():
    output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
    i = i + 1
    if (i % 10000 == 0):
        print('Processed ' + str(i) + ' articles')
output.close()
print('Processing complete!')

</code></pre>
<p>The Error I got was</p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/enwiki-latest-pages-articles.xml.bz2'

</code></pre>
<p>All the files are in one place so I'm not sure what's wrong</p>
","python, jupyter-notebook, gensim, wiki, corpus","<p>Did you ever download the file <code>enwiki-latest-pages-articles.xml.bz2</code> somehow, somewhere?</p>
<p>Did you specifically place it at the path <code>/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/enwiki-latest-pages-articles.xml.bz2</code>?</p>
<p>If not the <code>datapath()</code> function you're using won't construct the right path. (That particular function is meant to find a directory of test data bundled with Gensim, and shouldn't really be used to construct paths to your own dowloaded/created files!)</p>
<p>Instead of using that function, you should just specify the <em>actual</em> path, local to the Jupyter notebook server, where you put the file, as a string argument to <code>WikiCorpus</code>.</p>
",0,0,410,2021-08-12 13:07:28,https://stackoverflow.com/questions/68758090/creating-corpus-from-wiki-dump-file-using-jupyter-notebook
Does the input of Skip gram model have multiple labels?,"<p>Assuming the source text is:</p>
<blockquote>
<p>The quick brown fox jumps over the lazy dog</p>
</blockquote>
<p>And the window size is 2, like the following picture:</p>
<p><a href=""https://i.sstatic.net/vJQ42.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vJQ42.png"" alt=""window size is 2"" /></a></p>
<p>So we have a lots of training samples, and the format of training samples is (input, label).</p>
<p>For example:</p>
<blockquote>
<p>(the,quick), (the,brown), (quick,the), (quick,brown), (quick,fox).....</p>
</blockquote>
<p>Does that mean the input (<code>quick</code>) has three output(<code>the</code>, <code>brown</code>, <code>fox</code>)?</p>
","python, tensorflow, nlp, nltk, gensim","<p>Each of the 3 pairs – <code>(quick,the)</code>, <code>(quick,brown)</code>, <code>(quick,fox)</code> – essentially a separate training example.</p>
<p>Each pair is essentially presented to the shallow neural network independently, and the outputs of that network evaluated.</p>
<p>(That's either via the default negative-sampling sparse approach, where the output node for what you're calling the 'label' is checked, plus N other random laternatives, or via the alternative hierarcical-softmax, where just the nodes involved in the encoding of the 'label' word are checked. In either mode, only a tiny subset of the neural network's outputs are checked, for efficiency.)</p>
<p>To the extent those handful of checked outputs are not what are ideal for that one training example – a single pair – some corrective nudges are backpropagated. Then the next (<em>input</em>, <em>label</em>) is handled spearately.</p>
<p>It's only via the accumulated effect of all those contrasting examples, interleaved over many epochs, that the final network weights will reflect, in varying intensities that sometimes <code>quick</code> should activate <code>brown</code> more than other nodes, and other times <code>quick</code> should activate <code>fox</code> over other nodes.</p>
<p>So yes, in a sense, <code>quick</code> is associated in the skip-gram training micro-examples with 3 different desirable outputs, from just your one text. (In a good corpus, with many other subtly-varied usages of <code>quick</code>, it will also be associated, a varying number of times, with other potential outputs.)</p>
",1,0,124,2021-08-15 15:34:29,https://stackoverflow.com/questions/68792982/does-the-input-of-skip-gram-model-have-multiple-labels
Modifying .trainables.syn1neg[i] with previously trained vectors in Gensim word2vec,"<p>My issue is the following.</p>
<p>In my code I'm modifying the .wv[word] before training but after .build_vocab(), which is fairly straight forward. Just instead of the vectors in there add mine for every word.</p>
<pre><code>for elem in setIntersection:
    if len(word_space[elem]) != 300:
        print('here', elem) #cast it to the fire
        sys.exit()
    w2vObjectRI.wv[elem] = np.asarray(word_space[elem], dtype=np.float32)
</code></pre>
<p>Where setIntersection is just a set of common words between gensim word2vec and RandomIndexing trained. Same size of 300 in both.</p>
<p>Now I want to also modify the hidden-to-output layer weights, which I was told that they are in .trainables.syn1neg[i], but here is my issue this matrix is not word addressable, is just a normal matrix with out names. How could I know which letter I will be modifying in this matrix? Also I see that they are initialised with 0s, I was just thinking if these weights are not reset before training? More clearly if I change those weights and then call train will it use the ones I provided? Thanks.</p>
<pre><code>for i in range(len(setIntersection)):
if len(word_space[setIntersection[i]]) != 300:
    print('here', setIntersection[i]) #cast it to the fire
    sys.exit()
w2vObjectRI.trainables.syn1neg[i] = np.asarray(word_space[setIntersection[i]], dtype=np.float32)
</code></pre>
<p>Cheers,</p>
<p>Pedro.</p>
","python, gensim, word2vec","<p>In Gensim 4.0+, that &quot;hidden to output layer&quot; is just in <code>w2v_model.syn1neg</code>, instead of a (now-removed) subcomponent <code>.trainables</code>.</p>
<p>Following the original <code>word2vec.c</code> on which Gensim's implementation is based, those weights begin training as uninitialized zeros.</p>
<p>As the output (predicted-word) nodes are exactly the same vocabulary as are considered in the input/projection layer, the correspondence of rows-to-words is exactly the same as in the input layer, aka the word-vectors being trained. (That was previously in an array called <code>.syn0</code>, more recently called just <code>.vectors</code>.)</p>
<p>So the word that's in slot 0 in <code>w2v_model.wv.vectors</code> is also the word represented by the output-node fed by <code>w2v_model.syn1neg[0]</code>.</p>
<p>In Gensim 4.0+, these word-to-slot values can be read from <code>w2v_model.wv.key_to_index[word]</code>. (Pre-4.0, I think it was <code>w2v_model.wv.vocab[word].index</code>.)</p>
",2,0,218,2021-08-18 13:49:23,https://stackoverflow.com/questions/68833707/modifying-trainables-syn1negi-with-previously-trained-vectors-in-gensim-word2
Difference between Gensim&#39;s FastText and Facebook&#39;s FastText,"<p>I came upon the realization that there exists the original implementation of FastText <a href=""https://fasttext.cc/"" rel=""nofollow noreferrer"">here</a> by which you can use <code>fasttext.train_unsupervised</code> in order to generate word vectors (see <a href=""https://fasttext.cc/docs/en/unsupervised-tutorial.html"" rel=""nofollow noreferrer"">this link</a> as an example). However, turns out that gensim also supports fasttext and its API is similar to that of word2vec. <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py"" rel=""nofollow noreferrer"">See example here</a>.</p>
<p>I am wondering if there is a difference between the 2 implementations? The documentation was not clear <strong>but do they both mimic the paper <a href=""https://arxiv.org/pdf/1607.04606.pdf"" rel=""nofollow noreferrer"">Enriching Word Vectors with Subword Information</a>? And if yes then why would one use gensim's fasttext over fasttext ?</strong></p>
","gensim, fasttext","<p>Gensim intends to match the Facebook implementation, but with a few known or intentional differences. Specifically, Gensim doesn't implement:</p>
<ul>
<li>the <code>-supervised</code> option, &amp; specific-to-that-mode autotuning/quantization/pretrained-vectors options</li>
<li>word-multigrams (as controlled by the <code>-wordNgrams</code> paramerter to <code>fasttext</code>)</li>
<li>the plain <code>softmax</code> option for loss-optimization</li>
</ul>
<p>Regarding options to <code>-loss</code>, I'm relatively sure that despite <a href=""https://fasttext.cc/docs/en/options.html"" rel=""nofollow noreferrer"">Facebook's command-line options docs</a> indicating that the <code>fasttext</code> default is <code>softmax</code>, it is actually <code>ns</code> except when in <code>-supervised</code> mode, just like <code>word2vec.c</code> &amp; Gensim. See for example <a href=""https://github.com/facebookresearch/fastText/blob/a20c0d27cd0ee88a25ea0433b7f03038cd728459/src/args.cc#L29"" rel=""nofollow noreferrer"">this source code</a>.</p>
<p>I suspect a future contribution to Gensim that adds <code>wordNgrams</code> support would be welcome, if that mode is useful to some users, and to match the reference implementation.</p>
<p>So far the choice of Gensim has been to avoid any supervised algorithms, so the <code>-supervised</code> mode is less-likely to appear in any future Gensim. (I'd argue for it, though, if a working implementation was contributed.)</p>
<p>The plain <code>softmax</code> mode is so much slower on typical large output vocabularies that few non-academic projects would want to use it over <code>hs</code> or <code>ns</code>. (It may still be practical with a smaller-number of output-labels, as in <code>-supervised</code> mode, though.)</p>
",2,3,1549,2021-08-18 14:21:57,https://stackoverflow.com/questions/68834211/difference-between-gensims-fasttext-and-facebooks-fasttext
the repetitions in Gensim Word2Vec training corpus,"<p>I am using <code>Gensim</code> to train a <code>Word2Vec</code> embedding on different corpora, pertaining to different years, to compare the embedding vectors.
My question is: if I repeat the documents of a specific year twice and documents of another year just once, do the resulting embeddings give more weight to the repeated documents?
I have in my mind to make a corpus that gives more weight to recent documents and less weight to documents from far past.
I simply train the model on my <code>Line Sentence</code> corpus file.</p>
<pre><code>Word2Vec(corpus_file=corpus, vector_size=100, window=5, min_count=5, workers=4)
</code></pre>
","python, gensim, word2vec, word-embedding, corpus","<p>Sure, repeating some texts (even more than the re-iterations controlled by the <code>epochs</code> count) means they'll have more influence on the final model.</p>
<p>In general, repeating identical texts isn't as good as truly varied alternative examples of the same words. For example, if you only have one text using a certain word, repeating it 5 times might make the word survive the <code>min_count=5</code> cutoff, but the lack of many subtly-contrasting appearances means its final vector will only reflect that one peculiar, repeated use. The kind of good relative word-vector positions that people are usually seeking need a training tug-of-war between all the ways a word is used.</p>
<p>But, in this case, you should still have many examples, you're just overtraining on some of them.</p>
<p>Do note that it might be a <em>little</em> bit better to ensure the repeats are shuffled throughout the whole corpus, at least once before training begins, rather than clustered all together. (Repeating a text 10 times in a row will overtrain those words/contexts – but not in as balanced of a way as if interleaved with all the other differently-weighted training.)</p>
<p>And, that you might not want to turn all the 1-occurrence words in a subset that you repeat 5 times to automatically survive the <code>min_count</code> cut - because they still just have that one true weak context example. So you might want to learn the vocabulary from a non-reweighted corpus, but then train on the new corpus with artificial repeats (being sure to provide your <code>.train()</code> call with the right new <code>total_examples</code> count for it to report progress &amp; adjust the learning-rate properly).</p>
",1,0,395,2021-08-21 11:03:35,https://stackoverflow.com/questions/68872427/the-repetitions-in-gensim-word2vec-training-corpus
Using weight from a Gensim Word2Vec model as a starting point of another model,"<p>I have two corpora that are from the same field, but with a temporal shift, say one decade. I want to train Word2vec models on them, and then investigate the different factors affecting the semantic shift.</p>
<p>I wonder how should I initialize the second model with the first model's embeddings to avoid as much as possible the effect of variance in co-occurrence estimates.</p>
","python, gensim, word2vec, word-embedding, fine-tuning","<p>At a naive &amp; easy level, you can just load one existing model, and <code>.train()</code> on new data. But note if doing that:</p>
<ul>
<li>Any words not already known by the model will be ignored, and the word-frequencies that feed algorithmic steps will only be from the initial survey</li>
<li>While all words in the current corpus will get as many training-updates as their appearances (&amp; your <code>epochs</code> setting) dictate, and thus be nudged arbitrarily-far from their original-model locations, other words from the seed model will stay exactly where they were. But, it's only the interleaved tug-of-war between words in the same training session that makes them usefully comparable. So doing this sequential training – updating only some words in a new training session – is likely to degrade the meaningfulness of word-to-word comparisons, in hard-to-measure ways.</li>
</ul>
<p>Another approach that might be woth trying could be to train single model over the combined corpus - but transform/repeat the era-specific texts/words in certain ways to be able to distinguish earlier-usages from later-usages. There are more details about this suggestion in the context of word-vectors varying over usage-eras in a couple previous answers:</p>
<p><a href=""https://stackoverflow.com/a/57400356/130288"">https://stackoverflow.com/a/57400356/130288</a></p>
<p><a href=""https://stackoverflow.com/a/59095246/130288"">https://stackoverflow.com/a/59095246/130288</a></p>
",1,0,403,2021-08-28 10:15:35,https://stackoverflow.com/questions/68963361/using-weight-from-a-gensim-word2vec-model-as-a-starting-point-of-another-model
Using Gensin Word2Vec to improve search,"<p>I have a dataset off millions of arrays like follows:</p>
<pre><code>  sentences=[
    [
     'query_foo bar',
     'split_query_foo',
     'split_query_bar',
     'sku_qwre',
     'brand_A B C',
     'split_brand_A',
     'split_brand_B',
     'split_brand_C',
     'color_black',
     'category_C1',
     'product_group_clothing',
     'silhouette_t_shirt_top',
  ],
  [...]
  ]
</code></pre>
<p>where you find a query, a sku that was acquired by the user doing the query and a few attributes of the SKU. My idea was to do a very basic model based on word2vec where I could find similar things together.</p>
<p>In a simple way, if I search for <code>t-shirt</code> on the model I would expect to have t-shirt SKUs near the query.</p>
<p>I try to use gensim (I'm new to this library) with different attributes to build a model:</p>
<pre><code>from gensim.models.callbacks import CallbackAny2Vec

class callback(CallbackAny2Vec):
    '''Callback to print loss after each epoch.'''

    def __init__(self):
        self.epoch = 0
        self.loss_to_be_subed = 0

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        loss_now = loss - self.loss_to_be_subed
        self.loss_to_be_subed = loss
        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))
        self.epoch += 1

model = Word2Vec(
  sentences=sentences, 
  vector_size=100, 
  window=1000, 
  min_count=2, 
  workers=-1,
  epochs=10,
#   negative=5,
  compute_loss=True,
  callbacks=[callback()]
)
</code></pre>
<p>I got this output:</p>
<pre><code>Loss after epoch 0: 0.0
Loss after epoch 1: 0.0
Loss after epoch 2: 0.0
Loss after epoch 3: 0.0
Loss after epoch 4: 0.0
Loss after epoch 5: 0.0
Loss after epoch 6: 0.0
Loss after epoch 7: 0.0
Loss after epoch 8: 0.0
Loss after epoch 9: 0.0
</code></pre>
<p>All losses of 0!!!
I start to get very suspicious at this point.</p>
<p>Note: Each element of <code>sentences</code> are independent, I hop the library don't try to mix different terms in different arrays.</p>
<p>For trying to test the model, I tried a very frequent query like <code>model.wv.most_similar('query_t-shirt', topn=100)</code> and the results are completely absurd.</p>
<p>Is my idea crazy or am I using incorrectly the library?</p>
","python, nlp, gensim, information-retrieval","<p><code>workers=-1</code> is not a valid parameter value. If there's an example suggesting that negative-count somewhere, it's a bad example. If you got the impression that would work from something in Gensim's official docs, please report that documentation as a bug to be fixed.</p>
<p>More generally: enabling logging at the <code>INFO</code> level will show a lot more detail about what's happening, and something like &quot;misguided parameter that prevents any training from happening&quot; may become more obvious when using such logging.</p>
<p>Separately:</p>
<ul>
<li><p>The <code>Word2Vec</code> loss-tracking of Gensim has a lot of open issues (including the failure to tally by epoch, which your <code>Callback</code> tries to correct). I'd suggest not futzing with loss-display unless/until you've already achieved some success without it.</p>
</li>
<li><p>Such a low <code>min_count=2</code> is usually a bad idea with the word2vec algorithm, at least in normal natural-language settings. Words with so few occurrences lack the variety of contrasting usage examples to achieve a generalizable word-vector, or, individually, to influence the model much compared to the far-more-numerous other words. But such rare words are, altogether, quite numerous - essentially serving as 'noise' worsening other words. Discarding more such rare words often improves the remaining words, and overall model, noticeably. So, if you have enough raw training data to make word2vec worth applying, it should be more common to <em>increase</em> this cutoff higher than the default <code>min_count=5</code> than reduce it.</p>
</li>
<li><p>For recommendation-like systems being fed by pseudotexts that aren't exactly natural-language-like, it may be especially worthwhile to experiment with the <code>ns_exponent</code> parameter. As per <a href=""https://arxiv.org/abs/1804.04212"" rel=""nofollow noreferrer"">the research paper</a> linked in <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">the class docs</a>, the original <code>ns_exponent=0.75</code> value, which was an unchangeable constant in early word2vec implementations, might not be ideal for other applications like recommender systems.</p>
</li>
</ul>
",0,0,76,2021-08-29 09:58:03,https://stackoverflow.com/questions/68971791/using-gensin-word2vec-to-improve-search
Gensim fast text get vocab or word index,"<p>Trying to use <code>gensim's fasttext</code>, testing the sample code from <code>gensim</code> with a small change of replacing the arguement to <code>corpus_iterable</code></p>
<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html</a></p>
<p><code>gensim_version == 4.0.1</code></p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import common_texts  # some example sentences

print(common_texts[0])
['human', 'interface', 'computer']
print(len(common_texts))
9
model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
model.build_vocab(corpus_iterable=common_texts)
model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=10)
</code></pre>
<p>It works, but is there any way to <code>get the vocab</code> for the model. For example, in <code>Tensorflow Tokenizer</code> there is a <code>word_index</code> which will return <code>all the words</code>. Is there something similar here?</p>
","machine-learning, nlp, gensim, word-embedding, fasttext","<p>The model stores word vectors in <code>.wv</code> object. I don't know which gensim version you're using, but for Gensim 4 you can get keyed vectors by calling <code>model.wv.key_to_index</code>. You'll get a dict with words and their indices</p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import common_texts  # some example sentences
print(common_texts[0])
# ['human', 'interface', 'computer']
print(len(common_texts))
# 9
model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
model.build_vocab(corpus_iterable=common_texts)
model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=10)
# get vocab keys with indices
vocab = model.wv.key_to_index
print(vocab)
# output
# {'system': 0, 'graph': 1, 'trees': 2, 'user': 3, 'minors': 4, 'eps': 5, 'time': 6, 
# 'response': 7, 'survey': 8, 'computer': 9, 'interface': 10, 'human': 11}
</code></pre>
",2,0,2208,2021-09-02 02:42:48,https://stackoverflow.com/questions/69023485/gensim-fast-text-get-vocab-or-word-index
How to import gensim summarize,"<p>I got gensim to work in Google Collab by following this process:</p>
<pre><code>!pip install gensim
from gensim.summarization import summarize
</code></pre>
<p>Then I was able to call <code>summarize(some_text)</code></p>
<p>Now I'm trying to run the same thing in VS code:</p>
<p>I've installed gensim:
<code>pip3 install gensim</code></p>
<p>but when I run</p>
<pre><code>from gensim.summarization import summarize
</code></pre>
<p>I get the error</p>
<pre><code>Import &quot;gensim.summarization&quot; could not be resolvedPylancereportMissingImports
</code></pre>
<p>I've also tried <code>from gensim.summarization.summarizer import summarize</code> with same error. Regardless I haven't been able to call the function <code>summarize(some_text)</code> outside of Google Collab.</p>
","python, visual-studio-code, nlp, gensim","<p>The <code>summarization</code> code was removed from Gensim 4.0. See:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#12-removed-gensimsummarization"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#12-removed-gensimsummarization</a></p>
<blockquote>
<h4>12. Removed <code>gensim.summarization</code></h4>
<p>Despite its general-sounding name, the module will not satisfy the
majority of use cases in production and is likely to waste people's
time. See <a href=""https://github.com/RaRe-Technologies/gensim/issues/2592"" rel=""noreferrer"">this Github
ticket</a> for
more motivation behind this.</p>
</blockquote>
<p>If you need it, you could try:</p>
<ul>
<li>installing an older gensim version (such as <code>3.8.3</code>, the last official release in which it remained); or…</li>
<li>copy the source code out to your own local module</li>
</ul>
<p>However, I expect you'd likely be disappointed by its inflexibility and how little it can do.</p>
<p>It was <em>only</em> extractive summarization - choosing a few key sentences from those that already exist. That only gives impressive results when the source text was already well-written in an expository style mixing high-level overview sentences with separate detail sentences. And, its method of analyzing/ranking words was very crude &amp; hard-to-customize – totally unconnected to the more generic/configurable/swappable approaches used elsewhere in Gensim or in other text libraries.</p>
",11,9,26095,2021-09-05 15:54:22,https://stackoverflow.com/questions/69064948/how-to-import-gensim-summarize
Can a gensim word2vec model be trained in a federated way?,"<p>I am trying to find out how I could train a word2vec model in a federated way.</p>
<p>The data would be split into multiple parts, e.g. 4 &quot;institutions&quot;, and I would like to train the word2vec model on the data from each institution separately. They key restraint here is that the data from the institutions can not be moved to another location, so it can never be trained in a centralized way.</p>
<p>I know that it is possible to train the word2vec model iteratively, such that the data from the first institution is read and used to train &amp; update the word2vec model, but I wonder if its possible to do it simultaneously on all four institutions and then, for example, to merge all four word2vec models into one model.</p>
<p>Any ideas or suggestions are appreciated</p>
","python, gensim, word2vec, text-processing, federated","<p>There's no official support in Gensim, so any approach would involve a lot of custom research-like innovation.</p>
<p>Neural models like the word2vec algorithm (but not Gensim) have been trained in a very-distributed/parallel fashion – see for example 'Hogwild' &amp; related followup work, for asynchronous SGD. Very roughly, many separate simultaneous processes train separately &amp; asynchronously, but keep updating each other intermittently, even without locking – &amp; it works OK. (See more llinks in prior answer: <a href=""https://stackoverflow.com/a/66283392/130288."">https://stackoverflow.com/a/66283392/130288.</a>)</p>
<p>But:</p>
<ul>
<li>still this is usually done for performance, &amp; within a highly-connected datacenter – <em>not</em> for the sake of keeping separate data sources private, between institutions that may be less connected/trusting, or where the shards of data might in fact be very different in vocabulary/word-senses</li>
<li>there's never been support in Gensim for this - though many years ago, in an older version of Gensim, someone whipped up a kinda sorta demo that purported to do such scatter/merge training via Spark – see <a href=""https://github.com/dirkneumann/deepdist"" rel=""nofollow noreferrer"">https://github.com/dirkneumann/deepdist</a>.</li>
</ul>
<p>So: it's something a project could try to simulate, or test in practice, though the extra lags/etc of cross-&quot;institution&quot; updates might make it unpractical or ineffective. (And, they'd still have to initially consense on a shared vocabulary, which without due care would leak aspects of each's data.)</p>
<p>As you note, you could consider an approach where each trains one shared model in serial turns, which coudl very closely simulate a single training, albeit with the overhead of passing the interim model around, and no parallelism. Roughly:</p>
<ul>
<li>share word counts to reach a single consensus vocabulary</li>
<li>for each intended training epoch, each institution would train one pass on its whole dataset, then pass the model to the next institution</li>
<li>the calls to <code>.train()</code> would manually manage item counts &amp; <code>alpha</code>-related values to simulate one single SGD run</li>
</ul>
<p>Note that there'd still be some hints of each instititions relative co-occurrences of terms, which would leak some info about their private datasets – perhaps most clearly on rare terms.</p>
<p>Still, if you weren't in a rush, that'd best simulate a single integrated model training.</p>
<p>I'd be tempted to try to fix the sharing concerns with some other trust-creating process or intermediary. (Is there an 3rd party that each could trust with their data, temporarily? Could a single shared training system be created which could <em>only</em> stream the individual datasets in for training, with no chance of saving/summarizing the full data? Might 4 cloud hosts, each under the separate institution's sole management but physically in a shared facility effect the above 'serial turns' approach with hardly any overhead?)</p>
<p>There's also the potential to map one model into another: taking a number of shared words as reference anchor points, learning a projection from one model to the other, which allows other non-reference-point words to be moved from one coordinate space to the other. This is has been mentioned as a tool for either extending a vocabulary with vectors from elsewhere (eg section 2.2 of the Kiros et al 'Skip-Thought Vectors' paper) or doing language translation (Mikolov et al 'Exploiting Similarities among Languages for Machine Translation' paper).</p>
<p>Gensim includes a <code>TranslationMatrix</code> class for learning such projections. Conceivably the institutions could pick one common dataset, or one institution with the largest dataset, as the creator of some canonical starting model. Then each institution creates their own models based on private data. Then, based on some set of 'anchor words' (that are assumed to have <em>stable</em> meaning across all models, perhaps because they are very common) each of these followup models are projected into the canonical space - allowing words that are either unique to each model to be moved into the shared model, or words that vary a lot across models to be projected to contrasting points in the same space (that it might then make sense to average together).</p>
",1,1,315,2021-09-06 11:05:05,https://stackoverflow.com/questions/69073499/can-a-gensim-word2vec-model-be-trained-in-a-federated-way
Why is gensim FastText model smaller in size than the native Fasttext model by Facebook?,"<p>It seems that the <a href=""https://github.com/RaRe-Technologies/gensim"" rel=""nofollow noreferrer"">Gensim's</a> implementation in FastText leads to a smaller model size than <a href=""https://github.com/facebookresearch/fastText"" rel=""nofollow noreferrer"">Facebook's</a> native implementation. With a corpus of 1 million words, the fasttext native model is is 6GB, while the gensim fasttext model size is only 68MB.</p>
<p>Is there any information stored in Facebook's implementation not present in Gensim's implementation?</p>
","python, machine-learning, nlp, gensim, fasttext","<p>Please show which models generated this comparison, or what process was used. It probably has bugs/misunderstandings.</p>
<p>The size of a model is more influenced by the number of unique words (and character n-gram buckets) than the 'corpus' size.</p>
<p>The saved sizes of a Gensim-trained <code>FastText</code> model, or a native Facebook FastText-trained model, should be roughly in the same ballpark. Be sure to include all subsidiary raw <code>numpy</code> files (ending <code>.npy</code>, alongside the main save-file) created by Gensim's <code>.save()</code> - as all such files are required to re-<code>.load()</code> the model!</p>
<p>Similarly, if you were to load a Facebook FastText model into Gensim, then use Gensim's <code>.save()</code>, the total disk space taken in both alternate formats should be quite close.</p>
",2,1,628,2021-09-09 03:04:01,https://stackoverflow.com/questions/69111745/why-is-gensim-fasttext-model-smaller-in-size-than-the-native-fasttext-model-by-f
Gensim fasttext cannot get latest training loss,"
<h4>Problem description</h4>
<p>It seems that the <code>get_latest_training_loss</code> function in <code>fasttext</code> returns only 0. Both gensim <strong>4.1.0</strong> and <strong>4.0.0</strong> do not work.</p>
<pre><code>from gensim.models.callbacks import CallbackAny2Vec
from pprint import pprint as print
from gensim.models.fasttext import FastText
from gensim.test.utils import datapath

class callback(CallbackAny2Vec):
    '''Callback to print loss after each epoch.'''

    def __init__(self):
        self.epoch = 0

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        print('Loss after epoch {}: {}'.format(self.epoch, loss))
        self.epoch += 1

# Set file names for train and test data
corpus_file = datapath('lee_background.cor')

model = FastText(vector_size=100, callbacks=[callback()])

# build the vocabulary
model.build_vocab(corpus_file=corpus_file)

# train the model
model.train(
    corpus_file=corpus_file, epochs=model.epochs,
    total_examples=model.corpus_count, total_words=model.corpus_total_words,
    callbacks=model.callbacks, compute_loss=True,
)

print(model)
</code></pre>
<pre><code>'Loss after epoch 0: 0.0'
'Loss after epoch 1: 0.0'
'Loss after epoch 2: 0.0'
'Loss after epoch 3: 0.0'
'Loss after epoch 4: 0.0'
</code></pre>
<p><strong>If currently FastText does not support <code>get_latest_training_loss</code>, the documentation here needs to be removed:</strong></p>
<p><a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.get_latest_training_loss"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.get_latest_training_loss</a></p>
<h4>Versions</h4>
<p>I have tried this in three different environments and neither of them works.</p>
<p><strong>First environment:</strong></p>
<pre><code>[GCC 9.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
Linux-3.10.0-1160.36.2.el7.x86_64-x86_64-with-glibc2.17
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:39:48)
[GCC 9.3.0]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.21.2
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
gensim 4.1.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
<p><strong>Second environment:</strong></p>
<pre><code>Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
macOS-10.16-x86_64-i386-64bit
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.20.3
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
gensim 4.1.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
<p><strong>Third environment:</strong></p>
<pre><code>Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import platform; print(platform.platform())
macOS-10.16-x86_64-i386-64bit
&gt;&gt;&gt; import sys; print(&quot;Python&quot;, sys.version)
Python 3.9.5 (default, May 18 2021, 12:31:01)
[Clang 10.0.0 ]
&gt;&gt;&gt; import struct; print(&quot;Bits&quot;, 8 * struct.calcsize(&quot;P&quot;))
Bits 64
&gt;&gt;&gt; import numpy; print(&quot;NumPy&quot;, numpy.__version__)
NumPy 1.20.3
&gt;&gt;&gt; import scipy; print(&quot;SciPy&quot;, scipy.__version__)
SciPy 1.7.1
&gt;&gt;&gt; import gensim; print(&quot;gensim&quot;, gensim.__version__)
/Users/jinhuawang/miniconda3/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package &lt;https://pypi.org/project/python-Levenshtein/&gt; is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  warnings.warn(msg)
gensim 4.0.0
&gt;&gt;&gt; from gensim.models import word2vec;print(&quot;FAST_VERSION&quot;, word2vec.FAST_VERSION)
FAST_VERSION 0
</code></pre>
","python, nlp, gensim, word2vec, fasttext","<p>Indeed, loss-tracking hasn't ever been implemented in Gensim's <code>FastText</code> model, at least through release 4.1.0 (August 2021).</p>
<p>The docs for that method appear in error, due to the inherited method from the <code>Word2Vec</code> superclass not being overriden to prevent the default assumption that superclass methods work.</p>
<p>There is a <a href=""https://github.com/RaRe-Technologies/gensim/issues/2617"" rel=""noreferrer"">long-open issue</a> to fill the gaps &amp; fix the problems in Gensim's loss-tracking (which is also somewhat buggy &amp; incomplete for <code>Word2Vec</code>). But, at the moment I don't think any contributor is working on it, &amp; it hasn't been prioritized for any upcoming release. It may require someone to volunteer to step forward &amp; fix things.</p>
",5,5,681,2021-09-10 04:02:38,https://stackoverflow.com/questions/69127120/gensim-fasttext-cannot-get-latest-training-loss
Should bi-gram and tri-gram be used in LDA topic modeling?,"<p>I read several posts(<a href=""https://towardsdatascience.com/topic-modeling-quora-questions-with-lda-nmf-aff8dce5e1dd"" rel=""nofollow noreferrer"">here</a> and <a href=""http://www.cse.chalmers.se/%7Erichajo/dit862/L13/LDA%20with%20gensim%20(small%20example).html"" rel=""nofollow noreferrer"">here</a>) online about LDA topic modeling. All of them only use uni-grams. I would like to know why bi-grams and tri-grams are not used for LDA topic modeling?</p>
","nlp, gensim, topic-modeling, n-gram","<p>It's a matter of scale. If you have 1000 types (ie &quot;dictionary words&quot;), you might end up (in the worst case, which is not going to happen) with 1,000,000 bigrams, and 1,000,000,000 trigrams. These numbers are hard to manage, especially as you will have a lot more types in a realistic text.</p>
<p>The gains in accuracy/performance don't outweigh the computational cost here.</p>
",1,1,1502,2021-09-13 05:50:31,https://stackoverflow.com/questions/69157848/should-bi-gram-and-tri-gram-be-used-in-lda-topic-modeling
How to visualize Gensim Word2vec Embeddings in Tensorboard Projector,"<p>Following <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">gensim word2vec embedding tutorial</a>, I have trained a simple word2vec model:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models import Word2Vec
model = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)
model.save(&quot;/content/word2vec.model&quot;)
</code></pre>
<p>I would like to visualize it <a href=""https://projector.tensorflow.org/"" rel=""noreferrer"">using the Embedding Projector in TensorBoard</a>. <a href=""https://radimrehurek.com/gensim/scripts/word2vec2tensor.html"" rel=""noreferrer"">There is another straightforward tutorial in gensim documentation</a>. I did the following in Colab:</p>
<pre><code>!python3 -m gensim.scripts.word2vec2tensor -i /content/word2vec.model -o /content/my_model

Traceback (most recent call last):
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/scripts/word2vec2tensor.py&quot;, line 94, in &lt;module&gt;
    word2vec2tensor(args.input, args.output, args.binary)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/scripts/word2vec2tensor.py&quot;, line 68, in word2vec2tensor
    model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=binary)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py&quot;, line 1438, in load_word2vec_format
    limit=limit, datatype=datatype)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py&quot;, line 172, in _load_word2vec_format
    header = utils.to_unicode(fin.readline(), encoding=encoding)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/utils.py&quot;, line 355, in any2unicode
    return unicode(text, encoding, errors=errors)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
<p>Please note that I did check first this <a href=""https://stackoverflow.com/questions/50492676/visualize-gensim-word2vec-embeddings-in-tensorboard-projector"">exact same question from 2018</a> - but the accepted answer no longer works as both in gensim and tensorflow have been updated so I considered it was worth asking again in Q4 2021.</p>
","python, tensorflow, gensim, word2vec, tensorboard","<p>Saving the model in the original C word2vec implementation format resolves the issue:
<code>model.wv.save_word2vec_format(&quot;/content/word2vec.model&quot;)</code>:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models import Word2Vec
model = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)
model.wv.save_word2vec_format(&quot;/content/word2vec.model&quot;)
</code></pre>
<p>There are two formats of storing word2vec models in <code>gensim</code>: keyed vector format from the original word2vec implementation and format that additionally stores hidden weights, vocabulary frequencies, and more. Examples and details can be found in the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#usage-examples"" rel=""nofollow noreferrer"">documentation</a>. The script <code>word2vec2tensor.py</code> uses the original format and loads the model with <code>load_word2vec_format</code>: <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/scripts/word2vec2tensor.py#L68"" rel=""nofollow noreferrer"">code</a>.</p>
",1,5,1041,2021-09-18 13:11:18,https://stackoverflow.com/questions/69234978/how-to-visualize-gensim-word2vec-embeddings-in-tensorboard-projector
gensim word2vec vocabulary size fluctuates up &amp; down as corpus grows despite `max_vocab_size` setting,"<p>I am training word embeddings using <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim Word2Vec</a> model with a multi-million sentence corpus that is made of 3 million unique tokens with <code>max_vocab_size = 32_000</code>.</p>
<p>Even though I set <code>min_count = 1</code>, model creates a vocabulary of far less than 32_000. When I use a subset of the corpus, vocabulary size increases!</p>
<p>In order to troubleshoot, I set up an experiment where I control the size of vocabulary with different sized subcorpus. The size of the vocabulary flactuates!</p>
<p>You can re-produce with the code below:</p>
<pre><code>import string
import numpy as np
from gensim.models import Word2Vec

letters = list(string.ascii_lowercase)

# creating toy sentences
sentences = []
number_of_sentences = 100_000

for _ in range(number_of_sentences):
    number_of_tokens = np.random.randint(1, 15, 1)[0]
    sentence = []
    for i in range(number_of_tokens):
        token = &quot;&quot;
        len_of_token = np.random.randint(1, 5, 1)[0]
        for j in range(len_of_token):
            token += np.random.choice(letters)
        sentence.append(token)
    sentences.append(sentence)

# Sanity check to ensure that input data is a list of list of strings(tokens)
for _ in range(4):
    print(np.random.choice(sentences))

# collecting some statistics about tokens
flattened = []
for sublist in sentences:
    for item in sublist:
        flattened.append(item)
        
unique_tokens = {}
for token in flattened:
    if token not in unique_tokens:
        unique_tokens[token] = len(unique_tokens)

print('Number of tokens:', f'{len(flattened):,}')
print('Number of unique tokens:', f'{len(unique_tokens):,}')


# gensim model
vocab_size = 32_000
min_count = 1
collected_data = []
for num_sentence in range(5_000, number_of_sentences + 5_000, 5_000):
    model = Word2Vec(min_count=min_count, max_vocab_size= vocab_size)
    model.build_vocab(sentences[:num_sentence])

    collected_data.append((num_sentence, len(model.wv.key_to_index)))

for duo in collected_data:
    print('Vocab size of', duo[1], 'for', duo[0], 'number of sentences!')
</code></pre>
<p>Output:</p>
<pre><code>['cpi', 'bog', 'df', 'tgi', 'xck', 'kkh', 'ktw', 'ay']
['z', 'h', 'w', 'jek', 'w', 'dqm', 'wfb', 'agq', 'egrg']
['kgwb', 'lahf', 'kzx', 'd', 'qdok', 'xka', 'hbiz', 'bjo', 'fvk', 'j', 'hx']
['old', 'c', 'ik', 'n', 'e', 'n', 'o', 'r', 'ehx', 'dlud', 'd']

Number of tokens: 748,383
Number of unique tokens: 171,485

Vocab size of 16929 for 5000 number of sentences!
Vocab size of 30314 for 10000 number of sentences!
Vocab size of 19017 for 15000 number of sentences!
Vocab size of 31394 for 20000 number of sentences!
Vocab size of 19564 for 25000 number of sentences!
Vocab size of 31831 for 30000 number of sentences!
Vocab size of 19543 for 35000 number of sentences!
Vocab size of 31744 for 40000 number of sentences!
Vocab size of 19536 for 45000 number of sentences!
Vocab size of 31642 for 50000 number of sentences!
Vocab size of 18806 for 55000 number of sentences!
Vocab size of 31255 for 60000 number of sentences!
Vocab size of 18497 for 65000 number of sentences!
Vocab size of 31166 for 70000 number of sentences!
Vocab size of 18142 for 75000 number of sentences!
Vocab size of 30886 for 80000 number of sentences!
Vocab size of 17693 for 85000 number of sentences!
Vocab size of 30390 for 90000 number of sentences!
Vocab size of 17007 for 95000 number of sentences!
Vocab size of 30196 for 100000 number of sentences!
</code></pre>
<p>I tried increasing <code>min_count</code> but it did not help this flactuation of vocabulary size. What am I missing?</p>
","python, nlp, gensim, word2vec, word-embedding","<p>In Gensim, the <code>max_vocab_size</code> parameter is a <em>very</em> crude mechanism to limit RAM usage during the initial scan of the training corpus to discover the vocabulary. You should only use this parameter if it's the only way to work around RAM problems.</p>
<p>Essentially: try without using <code>max_vocab_size</code>. If you want control over which words are retained, use alternate parameters like <code>min_count</code> (to discard words less-frequent than a certain threshold) or <code>max_final_vocab</code> (to take no more than a set number of the most-frequent words).</p>
<p><em>If and only if</em> you hit out-of-memory errors (or massive virtual-memory swapping), then consider using <code>max_vocab_size</code>.</p>
<p>But even then, because of the way it works, you still wouldn't want to set <code>max_vocab_size</code> to the actual final size you want. Instead, you should set it to some value much, much larger - but just small enough to not exhaust your RAM.</p>
<p>This allows the most accurate possible word-counts <em>before</em> other parameters (like <code>min_count</code> &amp; <code>max_final_vocab</code>) are applied.</p>
<p>If you instead use a low <code>max_vocab_size</code>, the running survey will prematurely trim the counts any time the number of known words reaches that value. That is, as soon as the interim count reaches that many entries, say <code>max_vocab_size=32000</code>, many of the least-frequent counts are <em>forgotten</em> to cap memory usage (and more each time the threshold is reached).</p>
<p>That makes all final counts approximate (based on how often a term missed the cutoff), and means the final number of unique tokens in the full survey will be some value even less than <code>max_vocab_size</code>, somewhat arbitrarily based on how recently a forgetting-trim was triggered. (Hence, the somewhat random, but always lower than <code>max_vocab_size</code>, counts seen in your experiment output.)</p>
<p>So: <code>max_vocab_size</code> is unlikely to do what most people want, or in a predictable way. Still, it can help a fuzzy survey complete for extreme corpora where unique terms would otherwise overflow RAM.</p>
<p>Separately: <code>min_count=1</code> is usually a bad idea in word2vec, as words that lack sufficient varied usage examples won't themselves get good word-vectors, but leaving all such poorly-represented words in the training data tends to serve as noise that dilutes (&amp; delays) what can be learned about adequately-frequent words.</p>
",2,0,682,2021-09-19 21:01:40,https://stackoverflow.com/questions/69247049/gensim-word2vec-vocabulary-size-fluctuates-up-down-as-corpus-grows-despite-ma
How to reduce RAM consumption of gensim fasttext model through training parameters?,"<p>What <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#training-hyperparameters"" rel=""nofollow noreferrer"">parameters</a> when training a gensim fasttext model have the biggest effect on the resulting models' size in memory?</p>
<p>gojomos answer to <a href=""https://stackoverflow.com/questions/58407649/fasttext-bin-file-cannot-fit-in-memory-even-though-i-have-enough-ram"">this question</a> mentions ways to reduce a model's size during training, apart from reducing embedding dimensionality.</p>
<p>There seem a few parameters that might have an effect: thresholds for including words in the vocabulary especially. Do the other parameters also influence model size, for example ngram range, and which parameters have the largest effect?</p>
<p>I hope this is not too lazy of a question :-)</p>
","python, gensim, fasttext","<p>The main parameters affecting <code>FastText</code> model size are:</p>
<ul>
<li><code>vector_size</code> (dimensionality) - the size of the model is overwhelmingly a series of vectors (both whole-word and n-gram) of this length. Thus, reducing <code>vector_size</code> has a direct, large effect on total model size.</li>
<li><code>min_count</code> and/or <code>max_final_vocab</code> - by affecting how many whole words are considered 'known' (in-vocabulary) for the model, these directly influence how many bulk vectors are in the model. Especially if you have large enough training data that model size is an issue – &amp; are using <code>FastText</code> – you should be considering <em>higher</em> values than the default <code>min_count=5</code>. Very-rare words with just a handful of usage examples typically <em>don't</em> learn good generalizable representations in word2vec-like models. (Good vectors come from many subtly-contrasting usage examples.) But because by Zipfian distributions, there are typically a lot of such words in natural language data, they <em>do</em> wind up taking a lot of the training time, &amp; tug <em>against</em> other words' training, &amp; push more-frequent words out of each-other's context windows. Hence this is a case where, counter to many peoples' intuition, throwing away some data (the rarest words) can often <em>improve</em> the final model.</li>
<li><code>bucket</code> – which specifies exactly how may n-gram vectors will be learned by the model, because they all share a collision-oblivious hashmap. That is, no matter how many unique n-grams there really are in the training data, they'll all be forced into exactly this many vectors. (Essentially, rarer n-grams will often collide with more-frequent ones, and be just background noise.)</li>
</ul>
<p>Notably, because of the collisions tolerated by the <code>bucket</code>-sized hashmap, the parameters <code>min_n</code> &amp; <code>max_n</code> actually <em>don't</em> affect the model size at all. Whether they allow for lots of n-grams of many sizes, or much fewer of a single/smaller range of sizes, they'll be shoehorned into the same number of <code>bucket</code>s. (If more n-grams are used, a larger <code>bucket</code> value may help reduce collisions, and with more n-grams, training time will be longer. But the model will only grow with a larger <code>bucket</code>, not different <code>min_n</code> &amp; <code>max_n</code> values.)</p>
<p>You can get a sense of a model's RAM size by using <code>.save()</code> to save it to disk - the size of the multiple related files created (without compression) will roughly be of a similar magnitude as the RAM needed by the model. So, you can improve your intuition for how varying parameters changes the model size, by running varied-parameter experiments with smaller models, and watching their different <code>.save()</code>-sizes. (Note that you don't actually have to <code>.train()</code> these models - they'll take up their full allocated size once the <code>.build_vocab()</code> step has completed.)</p>
",3,3,698,2021-09-20 16:04:01,https://stackoverflow.com/questions/69257594/how-to-reduce-ram-consumption-of-gensim-fasttext-model-through-training-paramete
What does Similarity Score mean in gensim?,"<p>I have used <strong>Gensim</strong> library to find the similarity between a sentence against a collection of paragraphs, a dataset of texts. I have used Cosine similarity, Soft cosine similarity and Mover measures separately. Gensim returns a list of items including <em><strong>docid</strong></em> and <em><strong>similarity score</strong></em>. For Cosine similarity and Soft cosine similarity, I guess the similarity score is the cosine between the vectors. Am I right?</p>
<p>In Gensim documents, they wrote it is the semantic relatedness, and no extra explanation. I have search a lot, but did not find any answer. Any help please</p>
","python, text, gensim, sentence-similarity","<p>Usually by 'similarity', people are seeking to find a measure <em>semantic relatedness</em> - but whether the particular values calculated achieve that will depend on lots of other factors, such as the sufficiency of training data &amp; choice of other appropriate parameters.</p>
<p>Within each code context, 'similarity' has no more and no less meaning than how it's calculated right there - usually, that's 'cosine similarity between vector representations'. (When there's no other hints it means something different, 'cosine similarity' is typically a safe starting assumption.)</p>
<p>But really: the meaning of 'similarity' at each use is no more and no less than whatever that one code path's docs/source-code dictate.</p>
<p>(I realize that may seem an indirect &amp; unsatisfying answer. If there are specific uses in context in Gensim source/docs/example where the meaning is unclear, you could point those out &amp; I might be able to clarify those more.)</p>
",0,0,235,2021-09-20 16:37:46,https://stackoverflow.com/questions/69258013/what-does-similarity-score-mean-in-gensim
Find most similar sentence in a large dataset of sentences,"<p>I currently have a text file with around a million sentences, each on a new line.
I am trying to build a solution where I can take a new sentence outside of this text file and have the program return the most similar sentence present in the file.</p>
<p>I have found some solutions which return the pair of sentences with the highest similarity INSIDE the existing dataset.For example <a href=""https://stackoverflow.com/questions/63718559/finding-most-similar-sentences-among-all-in-python"">this</a> one. But that is not what I am going for. I want to be able to compare a new sentence with all of those in the text file.</p>
<p>Also, I am not sure if I should be focusing on semantic similarity or cosine similarity.</p>
","python, scikit-learn, nlp, gensim, word2vec","<p>I advise you to read about <a href=""https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance"" rel=""nofollow noreferrer"">Damerau–Levenshtein distance</a>.
I was also looking for a similar solution and settled on this algorithm.</p>
<p>There are implementations for Python:</p>
<ul>
<li><a href=""https://pypi.org/project/fastDamerauLevenshtein/"" rel=""nofollow noreferrer"">fastDamerauLevenshtein</a></li>
<li><a href=""https://pypi.org/project/pyxDamerauLevenshtein/"" rel=""nofollow noreferrer"">pyxDamerauLevenshtein</a></li>
</ul>
",1,0,1471,2021-09-21 18:53:09,https://stackoverflow.com/questions/69274178/find-most-similar-sentence-in-a-large-dataset-of-sentences
How to preprocess a text to remove stopwords?,"<p>I would like to remove a list of stopwords, namely the ones in</p>
<pre><code>from gensim.parsing.preprocessing import STOPWORDS
print(STOPWORDS)
</code></pre>
<p>In gensim, this should be pretty straightforward with <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/parsing/preprocessing.py"" rel=""nofollow noreferrer""><code>remove_stopwords </code>function</a>.</p>
<p>My code to read the text and remove the stopwords is the following:</p>
<pre><code>def read_text(text_path):
  text = []
  with open(text_path) as file:
    lines = file.readlines()
    for index, line in enumerate(lines):
      text.append(simple_preprocess(remove_stopwords(line)))
  return text

text = read_text('/content/text.txt')
text =  [x for x in text if x]
text[:3]
</code></pre>
<p>This is the output I get that contains words such as &quot;we&quot; or &quot;however&quot; which should have been removed from the <a href=""https://drive.google.com/file/d/1JKIPRb9y6XK7dnVybfwm5Yoz_lBV7mLP/view?usp=sharing"" rel=""nofollow noreferrer"">original text</a> though for instance &quot;the&quot; has been correctly removed from the first setence. I am very confused... what am I missing here?</p>
<pre><code>[['clinical', 'guidelines', 'management', 'ibd'],
 ['polygenetic',
  'risk',
  'scores',
  'add',
  'predictive',
  'power',
  'clinical',
  'models',
  'response',
  'anti',
  'tnfα',
  'therapy',
  'inflammatory',
  'bowel',
  'disease'],
 ['anti',
  'tumour',
  'necrosis',
  'factor',
  'alpha',
  'tnfα',
  'therapy',
  'widely',
  'management',
  'crohn',
  'disease',
  'cd',
  'ulcerative',
  'colitis',
  'uc',
  'however',
  'patients',
  'respond',
  'induction',
  'therapy',
  'patients',
  'lose',
  'response',
  'time',
  'to',
  'aid',
  'patient',
  'stratification',
  'polygenetic',
  'risk',
  'scores',
  'identified',
  'predictors',
  'response',
  'anti',
  'tnfα',
  'therapy',
  'we',
  'aimed',
  'replicate',
  'association',
  'polygenetic',
  'risk',
  'scores',
  'response',
  'anti',
  'tnfα',
  'therapy',
  'independent',
  'cohort',
  'patients',
  'establish',
  'clinical',
  'validity']]
</code></pre>
<p><strong>Text</strong> (complete file available <a href=""https://drive.google.com/file/d/1JKIPRb9y6XK7dnVybfwm5Yoz_lBV7mLP/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>)</p>
<p>Clinical Guidelines for the Management of IBD.</p>
<p>Polygenetic risk scores do not add predictive power to clinical models for response to anti-TNFα therapy in inflammatory bowel disease.
Anti-tumour necrosis factor alpha (TNFα) therapy is widely used in the management of Crohn's disease (CD) and ulcerative colitis (UC). However, up to a third of patients do not respond to induction therapy and another third of patients lose response over time. To aid patient stratification, polygenetic risk scores have been identified as predictors of response to anti-TNFα therapy. We aimed to replicate the association between polygenetic risk scores and response to anti-TNFα therapy in an independent cohort of patients, to establish its clinical validity.</p>
","python, nlp, gensim, word2vec, stop-words","<p>Your remove_stopwords() function is case-sensitive and it doesn't ignore punctuation. For example, 'However' is not in STOPWORDS, but 'however' is in. You should call the simple_preprocess() function first. This should work:</p>
<pre><code>from gensim.parsing.preprocessing import STOPWORDS
from gensim.parsing.preprocessing import remove_stopword_tokens

def read_text(text_path):
  text = []
  with open(text_path) as file:
    lines = file.readlines()
    for index, line in enumerate(lines):
      tokens = simple_preprocess(line)
      text.append(remove_stopword_tokens(tokens,stopwords=STOPWORDS))
  return text
</code></pre>
",3,0,627,2021-09-28 11:22:10,https://stackoverflow.com/questions/69360816/how-to-preprocess-a-text-to-remove-stopwords
cosine similarity doc vectors and word vectors for topical prevalence using doc2vec,"<p>I have a corpus of 250k Dutch news articles 2010-2020 to which I've applied word2vec models to uncover relationships between sets of neutral words and dimensions (e.g. good-bad). Since my aim is also to analyze the prevalence of certain topics over time, I was thinking of using doc2vec instead so as to simultaneously learn word and document embeddings. The 'prevalence' of topics in a document could then be calculated as the cosine similarities between doc vectors and word embeddings (or combinations of word vectors). In this way, I can calculate the annual topical prevalence in the corpus and see whether there's any changes over time. An example of such an approach can be found <a href=""https://export.arxiv.org/pdf/1707.03490"" rel=""nofollow noreferrer"">here</a>.</p>
<p>My issue is that the avg. yearly cosine similarities yield really strange results. As an example, the cosine similarities between document vectors and a mixture of keywords related to covid-19/coronavirus show a decrease in topical prevalence since 2016 (which obviously cannot be the case).</p>
<p>My question is whether the approach that I'm following is actually valid. Or that maybe there's something that I'm missing. A 250k documents and 100k + vocabulary should be sufficient enough?</p>
<p>Below is the code that I've written:</p>
<pre><code># Doc2Vec model 
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_docs)]
d2vmodel = Doc2Vec(docs, min_count = 5, vector_size = 200, window = 10, dm = 1)
docvecs = d2vmodel.docvecs
wordvecs = d2vmodel.wv
    
# normalize vector 
from numpy.linalg import norm
def nrm(x):
  return x/norm(x)

# topical prevalence per doc
def topicalprevalence(topic, docvecs, wordvecs):
  proj_lst = []
  for i in range(0, len(docvecs)):
    topic_lst = []
    for j in topic: 
      cossim =  nrm(docvecs[i]) @ nrm(wordvecs[j])
      topic_lst.append(cossim)
    topic_avg = sum(topic_lst) / len(topic_lst)
    proj_lst.append(topic_avg)
  topicsyrs = { 
      'topic': proj_lst,
      'year': df['datetime'].dt.year
  }
  return pd.DataFrame(topicsyrs)

# avg topic prevalence per year
def avgtopicyear(topic, docvecs, wordvecs):
  docs = topicalprevalence(topic, docvecs, wordvecs)
  return pd.DataFrame(docs.groupby(&quot;year&quot;)[&quot;topic&quot;].mean())

# run 
covid = ['corona', 'coronapandemie', 'coronacrisis', 'covid', 'pandemie']
covid_scores = topicalprevalence(covid, docvecs, wordvecs)
</code></pre>
","python, gensim, word2vec, doc2vec","<p>The word-vec-to-doc-vec relatioships in modes that train both are interesting, but a bit hard to characterize as to what they really mean. In a sense the CBOW-like mode of <code>dm=1</code> (PV-DM) mixes doc-vectors in as one equal word among the whole <code>window</code>, when training to predict the 'target' word. But in the skip-gram-mixed mode <code>dm=0, dbow_words=1</code>, there'll be <code>window</code> count context-word-vec-to-target-word pair cycles to every 1 doc-vec-to-target-word pair cycle, changing the relative weight.</p>
<p>So if you saw a big improvement in <code>dm=0, dbow_words=1</code>, it might also be because that made the model relatively more word-to-word trained. Varying <code>window</code> is another way to change that balance, or increase <code>epochs</code>, in plain <code>dm=1</code> mode – which should also result in doc/word compatible training, though perhaps not at the same rate/balance.</p>
<p>Whether a single <code>topicalprevalence()</code> mean vector for a full year would actually be reflective of individual word occurrences for a major topic may or may not be a valid conjecture, depending on possible other changes in the training data. Something like a difference in the relative mix of other major categories in the corpus might swamp even a giant new news topic. (EG: what if in y2020 some new section or subsidiary with a different focus, like entertainment, launched? It <em>might</em> swamp the effects of other words, especially when compressing down to a <em>single</em> vector of some particular dimensionality.)</p>
<p>Someting like a clustering of the year's articles, and identification of the closest 1 or N clusters to the target-words, with their similarities, might be more reflective even if the population of articles in changing. Or, a plot of each year's full set of articles as a histogram-of-similarities to the target-words - which might show a 'lump' of individual articles (not losing their distinctiveness to a full-year average) developing, over time, closer to the new phenomenon.</p>
",1,0,199,2021-09-28 12:21:04,https://stackoverflow.com/questions/69361669/cosine-similarity-doc-vectors-and-word-vectors-for-topical-prevalence-using-doc2
Process to intersect with pre-trained word vectors with gensim 4.0.0,"<p>I'm trying to learn from an example which uses an older version of gensim. In particular, I have a section of code like:</p>
<pre><code>word_vectors = Word2Vec(vector_size=word_vector_dim, min_count=1)
word_vectors.build_vocab(corpus_iterable)
word_vectors.intersect_word2vec_format(pretrained_dir + 'GoogleNews-vectors-negative300.bin.gz', binary=True)
</code></pre>
<p>My understanding is that this fills the word vector vocabulary with pre-trained word vectors when available. When the words in my vocabulary are not in the pretrained vectors, they are initialized to random values. However, the method <code>intersect_word2vec_format</code> doesn't exist in the latest version of gensim. What is the cleanest way to replicate this process in gensim 4.0.0?</p>
","python, gensim","<p>The <code>.intersect_word2vec_format()</code> method still exists, but as an operation on a set of word-vectors, has moved to <code>KeyedVectors</code>. So in some cases, older code that had called the method on a <code>Word2Vec</code> model itself will need to call it on the model's <code>.wv</code> property, holding a <code>KeyedVectors</code> object, instead. EG:</p>
<pre class=""lang-py prettyprint-override""><code>w2v_model = Word2Vec(vector_size=word_vector_dim, min_count=1)
w2v_model.build_vocab(corpus_iterable)
# (you'll likely need another workaround here, see below)
w2v_model.wv.intersect_word2vec_format(pretrained_dir + 'GoogleNews-vectors-negative300.bin.gz', binary=True)
</code></pre>
<p>However, you'll still hit some problems:</p>
<ul>
<li>It's always been at best an experimental, advanced feature – and not a part of any well-documented processes. So it's best used if you're able to <a href=""https://github.com/RaRe-Technologies/gensim/blob/5bec27767ad40712e8912d53a896cb2282c33880/gensim/models/keyedvectors.py#L1634"" rel=""nofollow noreferrer"">review its source code</a>, &amp; understand what limits &amp; tradeoffs will come with using such (partially)-pre-initialized word-vectors, maybe-further-trained or maybe-frozen (depending on the <code>vectors_lockf</code> values chosen).</li>
<li>The equally experimental <code>vectors_lockf</code> functionality will now, in Gensim 4+, require manual initialization by the knowledgeable - &amp; because <code>.intersect_word2vec_format()</code> assumes a particular pre-allocation, that method will break in Gensim 4.1 without an explicit workaround. See <a href=""https://github.com/RaRe-Technologies/gensim/issues/3094#issuecomment-809763444"" rel=""nofollow noreferrer"">this open issue for more details</a>.</li>
</ul>
<p>Most generally: pre-initializing with other word-vectors is at best a fussy, advanced technique, so be sure to study the code, consider the potential tradeoffs, &amp; carefully evaluate its effects on your end-results, before embracing it. It's not an easy, automatic, or well-characterized shortcut.</p>
",2,0,1346,2021-10-01 21:28:40,https://stackoverflow.com/questions/69412142/process-to-intersect-with-pre-trained-word-vectors-with-gensim-4-0-0
Default estimation method of Gensim&#39;s Word2vec Skip-gram?,"<p>I am now trying to use word2vec by estimating skipgram embeddings via NCE (noise contrastive estimation) rather than conventional negative sampling method, as a recent paper did (<a href=""https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24421?casa_token=uCHp2XQZVV8AAAAA%3Ac7ETNVxnpqe7u9nhLzX7pIDjw5Fuq560ihU3K5tYVDcgQEOJGgXEakRudGwEQaomXnQPVRulw8gF9XeO"" rel=""nofollow noreferrer"">https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24421?casa_token=uCHp2XQZVV8AAAAA%3Ac7ETNVxnpqe7u9nhLzX7pIDjw5Fuq560ihU3K5tYVDcgQEOJGgXEakRudGwEQaomXnQPVRulw8gF9XeO</a>). The paper has a replication GitHub repository (<a href=""https://github.com/sandeepsoni/semantic-progressiveness"" rel=""nofollow noreferrer"">https://github.com/sandeepsoni/semantic-progressiveness</a>), and it mainly relied on gensim for implementing word2vec, but the repository is not well organized and in a mess, so I have no clue about how the authors implemented NCE estimation via gensim's word2vec.</p>
<p>The authors just used gensim's word2vec as a default status without including any options, so my question is what is the default estimation method for gensim's word2vec under Skip-gram embeddings. NCE? According to your manual,  it just says there is an option for negative sampling, and if set to 0, then no negative sampling is used. But then what estimation method is used?
negative (int, optional) – If &gt; 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.</p>
<p>Thanks you in advance, and look forward to hearing from you soon!</p>
","python, nlp, gensim, word2vec","<p>You can view the default parameters for the Gensim <code>Word2Vec</code> model, in an unmodified Gensim library, in the Gensim docs. Here's a link to the current version (4.1) docs for the <code>Word2Vec</code> constructor method, showing all default parameter values:</p>
<p><a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec</a></p>
<blockquote>
<p><em>class</em> gensim.models.word2vec.Word2Vec(<em>sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True</em>)</p>
</blockquote>
<p>Two of those parameters – <code>hs=0, negative=5</code> – mean the default mode has hierarchical-softmax disabled, and negative-sampling enabled with 5 negative words. These have been the default of Gensim's <code>Word2Vec</code> for many versions, so even other code is using an older version, this is likely the mode used (unless parameters or modified/overriden code changed them).</p>
",2,0,387,2021-10-05 05:49:03,https://stackoverflow.com/questions/69445437/default-estimation-method-of-gensims-word2vec-skip-gram
How to get list of words for each topic for a specific relevance metric value (lambda) in pyLDAvis?,"<p>I am using pyLDAvis along with gensim.models.LdaMulticore for topic modeling. I have totally 10 topics. When I visualize the results using pyLDAvis, there is a bar called lambda with this explanation: &quot;Slide to adjust relevance metric&quot;. I am interested to extract the list of words for each topic separately for lambda = 0.1. I cannot find a way to adjust lambda in the document for extracting keywords.</p>
<p>I am using these lines:</p>
<pre><code>if 1 == 1:
    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, lambda_step=0.1)
LDAvis_prepared.topic_info
</code></pre>
<p>And these are the results:</p>
<pre><code>   Term     Freq        Total       Category logprob loglift
321 ra      2336.000000 2336.000000 Default 30.0000 30.0000
146 may     1741.000000 1741.000000 Default 29.0000 29.0000
66  doctor  1310.000000 1310.000000 Default 28.0000 28.0000
</code></pre>
<p>First of all these results are not related to what I observe with lambda of 0.1 in visualization. Secondly I cannot see the results separated by the topics.</p>
","nlp, gensim, lda, topic-modeling, pyldavis","<p>You may want to read this github page:
<a href=""https://nicharuc.github.io/topic_modeling/"" rel=""nofollow noreferrer"">https://nicharuc.github.io/topic_modeling/</a></p>
<p>According to this example, your code could go like this:</p>
<pre><code>lambd = 0.6 # a specific relevance metric value

all_topics = {}
num_topics = lda_model.num_topics 
num_terms = 10 

for i in range(1,num_topics+1): ## Indecies are 1-based, not 0-based
    topic = LDAvis_prepared.topic_info[LDAvis_prepared.topic_info.Category == 'Topic'+str(i)].copy()
    topic['relevance'] = topic['loglift']*(1-lambd)+topic['logprob']*lambd
    all_topics['Topic '+str(i)] = topic.sort_values(by='relevance', ascending=False).Term[:num_terms].values
pd.DataFrame(all_topics).T
</code></pre>
",4,2,1159,2021-10-08 07:27:53,https://stackoverflow.com/questions/69492078/how-to-get-list-of-words-for-each-topic-for-a-specific-relevance-metric-value-l
training a Word2Vec model with a lot of data,"<p>I am using <code>gensim</code> to train a <code>word2vec</code> model. The problem is that my data is very large (about 10 million documents) so my session is crashing when I try to estimate the model.</p>
<p>Note that I am able to load all the data at once in the RAM in a Pandas dataframe <code>df</code>, which looks like:</p>
<pre><code>text               id
long long text      1
another long one    2
...                 ...
</code></pre>
<p>My simple approach is to do the following:</p>
<pre><code>tokens = df['text'].str.split(r'[\s]+')
model = Word2Vec(tokens, min_count = 50)
</code></pre>
<p>However, my session crashed when it tries to create the tokens all at once. Is there a better way to proceed in <code>gensim</code>? Like feeding the data line by line?</p>
<p>Thanks!</p>
","python, pandas, gensim","<p>Iterate over your dataframe row by row, tokenizing just one row at a time. Write each tokenized text to a file in turn, with spaces between the tokens, and a line-end at the end of each text.</p>
<p>You can then use the <code>LineSentence</code> utility class in Gensim to provide a read-from-disk iterable corpus to the <code>Word2Vec</code> model.</p>
",1,0,556,2021-10-14 02:08:10,https://stackoverflow.com/questions/69564296/training-a-word2vec-model-with-a-lot-of-data
Training fasttext word embedding on your own corpus,"<p>I want to train fasttext on my own corpus. However, I have a small question before continuing. Do I need each sentences as a different item in corpus or can I have many sentences as one item?</p>
<p>For example, I have this DataFrame:</p>
<pre><code> text                                               |     summary
 ------------------------------------------------------------------
 this is sentence one this is sentence two continue | one two other
 other similar sentences some other                 | word word sent
</code></pre>
<p>Basically, the column <code>text</code> is an article so it has many sentences. Because of the preprocessing, I no longer have full stop <code>.</code>. So the question is can I do something like this directly or do I need to split each sentences.</p>
<pre><code>docs = df['text']
vectorizer = TfidfVectorizer()
vectorizer.fit_transform(docs)
</code></pre>
<p>From the tutorials I read, I need list of words for each sentences but what if I have list of words from an article? What are the differences? Is this the right way of training fasttext in your own corpus?</p>
<p>Thank you!</p>
","python, tensorflow, gensim, word-embedding, fasttext","<p>FastText requires <em>text</em> as its training data - not anything that's pre-vectorized, as if by <code>TfidfVectorizer</code>. (If that's part of your FastText process, it's misplaced.)</p>
<p>The Gensim FastText support requires the training corpus as a <em>Python iterable</em>, where each item is a <em>list of string word-tokens</em>.</p>
<p>Each list-of-tokens is typically some cohesive text, where the neighboring words have the relationship of usage together in usual natural-language. It might be a sentence, a paragraph, a post, an article/chapter, or whatever. Gensim's only limitation is that each text shouldn't be more than 10,000 tokens long. (If your texts are longer than that, they should be fragmented into separate 10,000-or-fewer parts. But don't worry too much about the loss of association around the split points - in training sets sufficiently large for an algorithm like FastText, any such loss-of-contexts is negligible.)</p>
",1,2,1626,2021-10-15 11:21:42,https://stackoverflow.com/questions/69583960/training-fasttext-word-embedding-on-your-own-corpus
Word2Vec for network embedding ignores words (nodes) in corpus (walks),"<p>I am trying to run word2vec (Skipgram) to a set of walks for training a network embedding model, in my graph I have 169343 nodes, i.e; word in the context of Word2vec, and for each node I run a random walk with length 80. Therefore, I have (169343,80) walks, i.e; sentences in Word2vec. after running SkipGram for 3 epochs I only get 28015 vectors instead of 169343. and here is the code for my Network Embedding.</p>
<pre class=""lang-py prettyprint-override""><code>def run_skipgram(walk_path):

    walks = np.load(walk_path).tolist()

    skipgram = Word2Vec(sentences=walks, vector_size=128, negative=5, window=8, sg=1, workers=6, epochs=3)
    
    keys = list(map(int, skipgram.wv.index_to_key))
    keys.sort()

    vectors = [skipgram.wv[key] for key in keys]

    return np.array(vectors)
</code></pre>
","python, machine-learning, graph, gensim, word2vec","<p>Are you sure your <code>walks</code> corpus is what you expect, and what Gensim <code>Word2Vec</code> expects?</p>
<p>For example, is <code>len(walks)</code> equal to 169343? Is <code>len(walks[0])</code> equal to 80? Is <code>walks[0]</code> a list of 80 string-tokens?</p>
<p>Note also: by default <code>Word2Vec</code> uses a <code>min_count=5</code> - so any token that appears fewer than 5 times is ignored during training. In most cases, this minimum – or an even higher one! – makes sense, because tokens with only 1, or a few, usage examples in usual natural-language training data <em>can't</em> get good word-vectors (but can, in aggregate, function as dilutive noise that worsens other vectors).</p>
<p>Depending on your graph, one walk from each node might not ensure that node appears at least 5 times in all the walks. So you could try <code>min_count=1</code>.</p>
<p>But it'd probably be better to do <em>5</em> walks from every starting point, or enough walks to ensure all nodes appear at least 5 times. <code>169,343 * 80 * 5</code> is still only 67,737,200 training words, with a manageable 169,343 count vocabulary. (If there's an issue expanding the whole training set as one list, you could make an iterable that generates the walks as needed, one by one, rather than all up-front.)</p>
<p>Alternatively, something like 5 walks per starting-node, but of only 20 steps each, would keep the corpus about the same size bu guarantee each node appears at least 5 times.</p>
<p>Or even: adaptively keep adding walks <em>until</em> you're sure every node is represented enough times. For example, pick a random node, do a walk, keep a running tally of each node's appearances so far – &amp; keep growing the net total of every node's
You could also try an adaptive corpus that keeps adding walks <em>until</em> every node is represented a minimum number of times.</p>
<p>Conceivably, for some remote nodes, that might take quite long to happen upon them, so another refinement might be: do some initial walk or walks, then tally how many visits each node got, &amp; while the least-frequent node is below the target <code>min_count</code>, start another walk from it – guaranteeing it at least one more visit.</p>
<p>This could help oversample less-connected regions, which might be good or bad. Notably, with natural language text, the <code>Word2Vec</code> <code>sample</code> parameter is quite helpful to <em>discard</em> certain overrepresented words, preventing them from monopolizing training time redundantly, ensuring less-frequent words also get good representations. (It's a parameter which can sometimes provide the double-whammy of less training time <em>and</em> better results!) Ensuring your walks spend more time in less-connected areas might provide a similar advantage, especially if your downstream use for the vectors is just as interested in the vectors for the less-visited regions.</p>
",0,0,210,2021-10-17 10:20:40,https://stackoverflow.com/questions/69603325/word2vec-for-network-embedding-ignores-words-nodes-in-corpus-walks
FastText: AttributeError: type object &#39;FastText&#39; has no attribute &#39;reduce_model&#39;,"<p>I use FastText to generate the word embedding. I download the pre-trained model from <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a>
The model has 300 dimensions but I want 100 dimensions so I use reduce model command but I got an error</p>
<p><code>import gensim</code><br />
<code>model = gensim.models.fasttext.FastText.load_fasttext_format('cc.th.300.bin')</code><br />
<code>gensim.models.fasttext.utils.reduce_model(model, 100)</code><br />
I got <code>AttributeError: module 'gensim.utils' has no attribute 'reduce_model'</code><br /></p>
<p>Heres are the code from FastText docs</p>
<p><code>import fasttext</code><br />
<code>import fasttext.util</code><br />
<code>ft = fasttext.load_model('cc.en.300.bin')</code><br />
<code>fasttext.util.reduce_model(ft, 100)</code></p>
<p>How to fix this error, I cannot find any docs for the new command.</p>
<p>Thank you</p>
","python, nlp, gensim, fasttext","<p>The module <code>gensim.fasttext.utils</code> does not have a function <code>reduce_model()</code>, as the error message describes.</p>
<p>That's not a common/standard operation - it's just something the Facebook wrapper decided to implement. (It looks like it's using standard PCA on a tiny subsample of the vectors, per <a href=""https://github.com/facebookresearch/fastText/blob/a20c0d27cd0ee88a25ea0433b7f03038cd728459/python/fasttext_module/fasttext/util/util.py#L114"" rel=""nofollow noreferrer"">source code here</a>.)</p>
<p>Why do you want to reduce the dimensionality?</p>
<p>Note that you'll lose some of the model's expressiveness, and if you were able to load the model at all to do the reduction, it's not too big for your RAM. If your main goal is to save model size, there might be better ways, such as discarding more rare words, depending on your reasons.</p>
<p>If you absolutely need to perform such a reduction, some options could be:</p>
<ul>
<li>Do it using the Facebook wrapper, then save the results in a form Gensim can load.</li>
<li>Reimplement the same operation for a Gensim model, perhaps using the FB code as a guide. (You'd have to make sure the Gensim model is updated in all ways that it considered the original dimensionality, which might be tricky – it's never been a goal or function of Gensim to enable after-the-fact model-shrinking.)</li>
</ul>
",0,0,854,2021-10-20 03:36:19,https://stackoverflow.com/questions/69640025/fasttext-attributeerror-type-object-fasttext-has-no-attribute-reduce-model
Preparing large txt file for gensim FastText unsupervised model,"<p>When I attempt to run FastText using gensim in Python, the best I can get is a result that gives me the most similar but each result is a single character.  (I'm on a windows machine, which I've heard affects the result.)</p>
<p>I have all of my data stored in <em>either</em> a csv file in which I've already tokenized each sentence or in the original txt file I started with.  When I try to use the csv file, I end up with the single character result.</p>
<p>Here's the code I'm using to process my csv file (I'm looking at analyzing how sports articles discuss white vs. nonwhite NFL quarterbacks differently, this is the code for my NonWhite results csv file):</p>
<pre><code>from gensim.models import FastText
from gensim.test.utils import get_tmpfile, datapath
import os

embedding_size = 200
window_size = 10
min_word = 5
down_sampling = 1e-2

if os.path.isfile(modelpath):
    model1 = FastText.load(modelpath)
else:
    class NWIter():
        def __iter__(self):
            path = datapath(csvpath)
            with utils.open(path, 'r') as fin:
                for line in fin:
                    yield line

    model1 = FastText(vector_size=embedding_size, window=window_size, min_count=min_word,sample=down_sampling,workers=4)
    model1.build_vocab(corpus_iterable=NWIter())
    exs1=model1.corpus_count
    model1.train(corpus_iterable=NWIter(), total_examples=exs1, epochs=50)  
    model1.save(modelpath)

</code></pre>
<p>The cleaned CSV data looked like this, with each row representing a sentence that had been cleaned (stopwords removed, tokenized, and lemmatized).
<a href=""https://i.sstatic.net/5HlPv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5HlPv.png"" alt=""Cleaned CSV File"" /></a></p>
<p>When that didn't work, I attempted to bring in the raw text but got lots of UTF-8 encoding errors with unrecognizable characters.  I attempted to work around this issue, finally getting to a point where it tried to read in the raw text file - only for the single character returns to come back.</p>
<p>So it seems the issue persists regardless of if I use my csv file or if I use the txt file.  So I'd prefer to stick with the csv as I've already processed the information; how can I bring that data in without Python (or gensim) seeing the individual characters as the unit of analysis?</p>
<p>Edit:
Here are the results I get when I run:</p>
<pre><code>print('NonWhite: ',model1.wv.most_similar('smart', topn=10))
</code></pre>
<p>NonWhite:  [('d', 0.36853086948394775), ('q', 0.326141357421875), ('s', 0.3181183338165283), ('M', 0.27458563446998596), ('g', 0.2703150510787964), ('o', 0.215525820851326), ('x', 0.2153075635433197), ('j', 0.21472081542015076), ('f', 0.20139966905117035), ('a', 0.18369245529174805)]</p>
","python-3.x, gensim, fasttext","<p>The Gensim <code>FastText</code> model (like its other models in the <code>Word2Vec</code> family) needs each individual text as a <em>list-of-string-tokens</em>, <em>not</em> a plain string.</p>
<p>If you pass texts as plain strings, they appear to be lists-of-single-characters – because of the way Python treats strings. Hence, the only 'words' the model sees are single-characters – including the individual spaces.</p>
<p>If the format of your file is such that each line is already a space-delimited text, you could simply change your <code>yield</code> line to:</p>
<pre><code>yield line.split()
</code></pre>
<p>If instead it's truly a CSV, and your desired training texts are in only one column of the CSV, you should pick out that field and properly break it into a list-of-string-tokens.</p>
",1,0,584,2021-10-20 04:18:25,https://stackoverflow.com/questions/69640267/preparing-large-txt-file-for-gensim-fasttext-unsupervised-model
tokenizing the data properly in gensim,"<p>I am a bit confused as how to tokenize the data correctly in <code>gensim</code>.
I have a text file  <code>myfile.txt</code> that contains the following text</p>
<pre><code>&quot;&quot;&quot; 
this is a very long string with a title


and some white space. Multiple sentences, too. This is nuts!
Yay! :):):) 
&quot;&quot;&quot;
</code></pre>
<p>I load this file in <code>gensim</code> using <code>LineReader('myfile.txt')</code> to train a <code>word2vec</code> model (of course my data is much bigger than the example above)</p>
<p>But is this text tokenized propertly? I am asking this because <code>LineReader</code> seems to be very specific :</p>
<blockquote>
<p>The format of files (either text, or compressed text files) in the
path is one sentence = one line, with words already preprocessed and
separated by whitespace.
see <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence</a></p>
</blockquote>
<p>I am confused. Am I doing things right? How should I tokenize my text for <code>LineReader</code>?</p>
<p>Thanks!</p>
","python, gensim","<p>That will work, but because Gensim's <code>LineSentence</code> class (what I assume you mean) breaks tokens on whitespace, your line...</p>
<pre><code>and some white space. Multiple sentences, too. This is nuts!
</code></pre>
<p>...will become the list of word-tokens:</p>
<pre><code>['and', 'some', 'white', 'space.', 'Multiple', 
'sentences,', 'too.', 'This', 'is', 'nuts!']
</code></pre>
<p>That means tokens like <code>'space.'</code>, <code>'sentences,'</code>, &amp; <code>'nuts!'</code> will be treated as words – potentially even receiving trained word-vectors, too (if they appear at least <code>min_count</code> times).</p>
<p>That's probably not what you want – but also not necessarily a big problem. In a sufficiently-large corpus, all the words you care about will appear so many times without this connected-punctuation issue, you'll probably still get good vectors for them.</p>
<p>But more typically, you'd preprocess your text to either strip that punctuation, or split it off from words with extra space delimiter characters. (When you do that, the punctuation marks themselves become 'words' of a sort.)</p>
",1,0,1230,2021-10-21 01:14:12,https://stackoverflow.com/questions/69654710/tokenizing-the-data-properly-in-gensim
Find the most common sentences/phrases among millions of documents using Python,"<p>I have about 5 million documents.  A document is composed of many sentences, and may be about one to five pages long.  Each document is a text file.</p>
<p>I have to find the most common sentences / phrases (at least 5 words long) among all the documents. How should I achieve this?</p>
","python, pandas, scikit-learn, nlp, gensim","<p>For exactly 5-word-long phrases, this is relatively simple Python (which may require lots of memory). For variably-longer phrases, it's a bit more complicated – &amp; may need additional clarification about what kinds of longer phrases you'd want to find.</p>
<p>For the 5-word (aka '5-gram') case:</p>
<p>In one pass over the corpus, you generate <em>all 5-grams</em>, &amp; tally their occurrences (say into a <code>Counter</code>), then report the top-N.</p>
<p>For example, let's assume <code>docs</code> is a Python sequence of all your tokenized texts, where each individual item is a list-of-string-words. Then roughly:</p>
<pre><code>from collections import Counter

ngram_size = 5
tallies = Counter()

for doc in docs:
    for i in range(0, len(doc)-4):
        ngram = tuple(doc[i:i+5])
        tallies[ngram] += 1

# show the 10 most-common n-grams
print(tallies.most_common(10))
</code></pre>
<p>If you then wanted to also consider variably longer phrases, it's a little trickier – but note any such phrase would have to start with some of the 5-grams you'd already found.</p>
<p>So you could consider gradually repeating the above, for 6-grams, 7-grams, etc.</p>
<p>But to optimize for memory/effort, you could add a step to <em>ignore</em> all n-grams that don't already start with one of the top-N candidates you chose from an earlier run. (For example, in a 6-gram run, the <code>+=</code> line above would be conditional on the 6-gram starting-with one of the few 5-grams you've already considered to be of interest.)</p>
<p>And further, you'd stop looking for ever-longer n-grams when (for example) the count of top 8-grams is already below the relevant top-N counts of shorter n-grams. (That is, when any further longer n-grams are assured of being less-frequent than your top-N of interest.)</p>
",-1,0,805,2021-10-26 08:15:13,https://stackoverflow.com/questions/69719659/find-the-most-common-sentences-phrases-among-millions-of-documents-using-python
does gensim.corpora wikiCorpus work only with bz2 file?,"<p>I'm trying to load a wiki dump (.gz) and use it in gensim word2vec. I convert it into bz2 using bzip2 in terminal but Wikicorpus class seems to refuse the file. Can someone please explain me how to get the text from a wiki dump in a easy way?
thanks</p>
","python, nlp, gensim, wikipedia, dump","<p>The <code>WikiCorpus</code> utility class in Gensim expects the <code>pages-articles</code> dumps, not different dumps containing only abstracts.</p>
<p>To read another format, you'll need to write your own code.</p>
<p>Some things you could try:</p>
<ul>
<li>Study the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/wikicorpus.py"" rel=""nofollow noreferrer"">source for the WikiCorpus class</a> &amp; use it as a model for your own code, adapting it to read the different elements out of your other dump.</li>
<li>Use some other utility, for example the <a href=""https://stedolan.github.io/jq/"" rel=""nofollow noreferrer"">command-line tool <code>jq</code></a> or similar, to just dump the relevant text from the XML element(s) of interest, into a plain-text file, which you could then read line-by-line in Python (to either further preprocess/tokenize or even just feed directly to Gensim's <code>LineSentence</code> helper class).</li>
</ul>
",0,0,254,2021-10-27 00:00:40,https://stackoverflow.com/questions/69731202/does-gensim-corpora-wikicorpus-work-only-with-bz2-file
is there a way to stop creation of vocabulary in gensim.WikiCorpus when reach 2000000 tokens?,"<p>I downloaded the latest wiki dump multi-stream bz2. I call the WikiCorpus class from gensim corpora and after 90000 document the vocabulary reaches the highest value (2000000 tokens).
I got this in terminal:</p>
<p>keeping 2000000 tokens which were in no less than 0 and no more than 580000 (=100.0%) documents
resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'able', 'abolish', 'abolition', 'about']...)
adding document #580000 to Dictionary(2000000 unique tokens: ['ability', 'able', 'abolish', 'abolition', 'about']...)</p>
<p>The WikiCorpus class continues to work until the end of the documents in my bz2.
Is there a way to stop it? or to split the bz2 file in a sample?
thanks for help!</p>
","python, nlp, gensim, wikipedia, dump","<p>There's no specific parameter to cap the number of tokens. But when you use <code>WikiCorpus.get_texts()</code>, you don't have to read them all: you can stop at any time.</p>
<p>If, as suggested by another question of yours, you plan to use the article texts for Gensim <code>Word2Vec</code> (or a similar model), you don't need the constructor to do its own expensive full-scan vocabulary-discovery. If you supply any dummy object (such as an empty <code>dict</code>) as the optional <code>dictionary</code> parameter, it'll skip this unnecessary step. EG:</p>
<pre class=""lang-py prettyprint-override""><code>wiki_corpus = WikiCorpus(filename, dictionary={})
</code></pre>
<p>If you also want to use some truncated version of the full set of articles, I'd suggest manually iterating over just a subset of the articles. For example if the subset will easily fit as a <code>list</code> in RAM,  say 50000 articles, that's as simple as:</p>
<pre class=""lang-py prettyprint-override""><code>import itertools
subset_corpus = list(itertools.islice(wiki_corpus, 50000))
</code></pre>
<p>If you want to create a subset larger than RAM, iterate over the set number of articles, writing their tokenized texts to a scratch text file, one per line. Then use that file as your later input. (By spending the <code>WikiCorpus</code> extraction/tokenization effort only once, then reusing the file from disk, this can sometimes offer a performance boost even if you don't need to do it.)</p>
",1,0,124,2021-10-28 11:10:18,https://stackoverflow.com/questions/69753004/is-there-a-way-to-stop-creation-of-vocabulary-in-gensim-wikicorpus-when-reach-20
What am I doing wrong using Gensim.Phrases to extract repeating multiple-word terms from within a single sentence?,"<p>I would like to first extract repeating n-grams from within a single sentence using Gensim's <a href=""https://radimrehurek.com/gensim/models/phrases.html"" rel=""nofollow noreferrer"">Phrases</a>, then use those to get rid of duplicates within sentences. Like so:</p>
<blockquote>
<p>Input: &quot;Testing test this test this testing again here testing again here&quot;</p>
<p>Desired output: &quot;Testing test this testing again here&quot;</p>
</blockquote>
<p>My code <strong>seemed</strong> to have worked for generating up to 5-grams using multiple sentences but whenever I pass it a single sentence (even a list full of the same sentence) it doesn't work. If I pass a single sentence, it splits the words into characters. If I pass the list full of the same sentence, it detects nonsense like non-repeating words while not detecting repeating words.</p>
<p>I thought my code was working because I used like 30MB of text and produced very intelligible n-grams up to n=5 that seemed to correspond to what I expected. I have no idea how to tell its precision and recall, though. Here is the full function, which recursively generates all n-grams from 2 to n::</p>
<pre><code>def extract_n_grams(documents, maximum_number_of_words_per_group=2, threshold=10, minimum_count=6, should_print=False, should_use_keywords=False):
    from gensim.models import Phrases
    from gensim.models.phrases import Phraser

    tokens = [doc.split(&quot; &quot;) for doc in documents] if type(documents) == list else [documents.split(&quot; &quot;) for _ in range(100)] # this is what I tried

    final_n_grams = []
    for current_n in range(maximum_number_of_words_per_group - 1):
        n_gram = Phrases(tokens, min_count=minimum_count, threshold=threshold, connector_words=connecting_words)

        n_gram_phraser = Phraser(n_gram)

        resulting_tokens = []
        for token in tokens:
            resulting_tokens.append(n_gram_phraser[token])

        current_n_gram_final = []
        for token in resulting_tokens:
            for word in token:
                if '_' in word:
                    # no n_gram should have a comma between words
                    if ',' not in word:
                        word = word.replace('_', ' ')

                        if word not in current_n_gram_final and all([word not in gram for gram in final_n_grams]):
                            current_n_gram_final.append(word)

        tokens = n_gram[tokens]

        final_n_grams.append(current_n_gram_final)
</code></pre>
<p>In addition to trying repeating the sentence in the list, I also tried using NLKT's word_tokenize as suggested <a href=""https://stackoverflow.com/questions/45159693/word2vec-models-consist-of-characters-instead-of-words"">here</a>. What am I doing wrong? Is there an easier approach?</p>
","python, gensim","<p>The Gensim <code>Phrases</code> class is designed to statistically detect when certain pairs of words appear so often together, compared to independently, that it might be useful to combine them into a single token.</p>
<p>As such, it's unlikely to be helpful for your example task, of eliminating the duplicate 3-word <code>['testing', 'again', 'here']</code> run-of-tokens.</p>
<p>First, it never eliminates tokens – only combines them. So, <em>if</em> it saw the couplet <code>['again', 'here']</code> appearing ver often together, rather than as separate <code>'again'</code> and <code>'here'</code>, it'd turn it into <code>'again_here'</code> – not eliminate it.</p>
<p>But second, it does these combinations not for every repeated n-token grouping, but only <em>if</em> the large amount of training data implies, based on the <code>threshold</code> configured, that certain pairs stick out. (And it only goes beyond pairs if run repeatedly.) Your example 3-word grouping, <code>['testing', 'again', 'here']</code>, does not seem likely to stick out as a composition of extra-likely pairings.</p>
<p>If you have a more rigorous definition of which tokens/runs-of-tokens need to be eliminated, you'd probably want to run other Python code on the lists-of-tokens to enforce that de-duplication. Can you describe in more detail, perhaps with more examples, the kinds of n-grams you want removed? (Will they only be at the beginning or end of a text, or also the middle? Do they have to be next-to each other, or can they be spread throughout the text? Why are such duplicates present in the data, &amp; why is it thought important to remove them?)</p>
<p><strong>Update:</strong> Based on the comments about the real goal, a few lines of Python that check, at each position in a token-list, whether the next N tokens match the previous N tokens (and thus can be ignored) should do the trick. For example:</p>
<pre class=""lang-py prettyprint-override""><code>def elide_repeated_ngrams(list_of_tokens):
    return_tokens = [] 
    i = 0
    while i &lt; len(list_of_tokens):
        for candidate_len in range(len(return_tokens)):
            if list_of_tokens[i:i+candidate_len] == return_tokens[-candidate_len:]:
                i = i + candidate_len  # skip the repeat
                break  # begin fresh forward repeat-check
        else:
            # this token not part of any repeat; include &amp; proceed
            return_tokens.append(list_of_tokens[i])
            i += 1
    return return_tokens 
</code></pre>
<p>On your test case:</p>
<pre><code>&gt;&gt;&gt; elide_repeated_ngrams(&quot;Testing test this test this testing again here testing again here&quot;.split())
['Testing', 'test', 'this', 'testing', 'again', 'here']
</code></pre>
",1,1,477,2021-10-30 02:49:10,https://stackoverflow.com/questions/69776426/what-am-i-doing-wrong-using-gensim-phrases-to-extract-repeating-multiple-word-te
Pythonic way to obtain a distance matrix from word vectors in gensim 4.0,"<p>I am currently using gensim version 4.0.1 to generate word vectors. My ultimate goal is to compute cosine distances between all pairwise combinations word vectors and to use the obtained distance matrix for clustering the word vectors. So far I have been been generating the distance matrix with the following code:</p>
<pre><code>    print('Setting up Word2Vec model')
    model = gensim.models.Word2Vec (genome_tokens, vector_size=100, window=args.window_size, min_count=args.min_cluster_size, workers=args.threads, sg=1)

    print('Training Word2Vec model')
    model.train(genome_tokens,total_examples=len(genome_tokens),epochs=10)

    words = sorted(model.wv.index_to_key)
    scaled_data = [model.wv[w] for w in words]
    print('Calculating distribution distance among clusters')
    cluster_distrib_distance = pairwise_distances(scaled_data, metric=args.metric)
</code></pre>
<p>I was wondering if there is a specific function to obtain the distance matrix directly from the model object, without having to create the words and scaled data object.</p>
<p>Going through the gensim documentation I have mostly found information regarding ways to calculate similarities, rather than distances and often between documents rather than individual words. There does seem to be some discussion on this topic on the <a href=""https://github.com/RaRe-Technologies/gensim/issues/140"" rel=""nofollow noreferrer"">github repository</a>, but the methods described there seem to be specific to the older versions as is the case for the solution presented <a href=""https://stackoverflow.com/questions/45280020/getting-distance-matrix-and-features-matrix-from-word2vec-model"">here</a></p>
","python, nlp, gensim, word2vec","<p>There's no built-in utility method for that.</p>
<p>But, you can get the raw backing array, with all the vectors in it, in the <code>model.wv.vectors</code> property. Each row is the word-vector for the corresponding word in the same position in <code>index_to_key</code>.</p>
<p>You can feed this into <code>sklearn.metrics.pairwise_distances</code> (or similar) directly, without the need for the separate (&amp; differently-sorted) <code>scaled_data</code> outside.</p>
<p>Note that if using something like Euclidean distance, you <em>might</em> want the word-vectors to be unit-length-normalized before calculating distances. Then all distances will be in the range <code>[0.0, 2.0]</code>, and ranked distances will be the exact reverse of ranked cosine-similarities.</p>
<p>In that case you'd again want to work from an external set of vectors – either by using <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_vector"" rel=""nofollow noreferrer""><code>get_vector(key, norm=True)</code></a> to get them 1-by-1, or <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_normed_vectors"" rel=""nofollow noreferrer""><code>get_normed_vectors()</code></a> to get a fully unit-normed version of the <code>.vectors</code> array.</p>
",1,0,498,2021-11-02 16:02:51,https://stackoverflow.com/questions/69813465/pythonic-way-to-obtain-a-distance-matrix-from-word-vectors-in-gensim-4-0
Word2Vec error: TypeError: unhashable type: &#39;list&#39;,"<p>I'm experimenting with peptide sequences and NLP right now and am trying to embed the peptide sequences using word2vec.
The peptides come in a long string format (ex: 'KCNTATCATQRLANFLVRSSNNLGPVLPPTNVGSNTY'), so I've split the peptide sequences into trigrams. But as I'm trying to embed them, I keep getting this error: <code>TypeError: unhashable type:'list.'</code></p>
<p>Not sure how to fix this error as I don't quite understand why it's coming up. My code is linked <a href=""https://colab.research.google.com/drive/1HWny5SfklWnhUcfO-y-QdOgRe8tmJcaZ?usp=sharing"" rel=""nofollow noreferrer"">here</a>, and here is the full error output:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-17-966c68819734&gt; in &lt;module&gt;()
      6 
      7 # embeddings pos
----&gt; 8 w2vpos = Word2Vec(kmersdatapos, size=EMB_DIM,window=5,min_count=5,negative=15,iter=10,workers=multiprocessing.cpu_count())

4 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py in _scan_vocab(self, sentences, progress_per, trim_rule)
   1553                 )
   1554             for word in sentence:
-&gt; 1555                 vocab[word] += 1
   1556             total_words += len(sentence)
   1557 

TypeError: unhashable type: 'list'
</code></pre>
<p>Any suggestions are appreciated!</p>
","python, nlp, bioinformatics, gensim, word2vec","<p>You need to pass a list of list of strings to gensim's Word2Vec. In your code you are passing kmersdatapos to Word2Vec, which is list of list of list of strings.
For example:</p>
<pre><code>corpus = [[&quot;lorem&quot;, &quot;ipsum&quot;], [&quot;dolor&quot;], [&quot;sit&quot;, &quot;amet&quot;]]
</code></pre>
<p>is a valid parameter for the Word2Vec function. Whereas,</p>
<pre><code>corpus = [[[&quot;lorem&quot;, &quot;ipsum&quot;], [&quot;dolor&quot;]], [[&quot;sit&quot;, &quot;amet&quot;]]] 
</code></pre>
<p>is invalid.</p>
",2,0,485,2021-11-03 08:38:54,https://stackoverflow.com/questions/69821842/word2vec-error-typeerror-unhashable-type-list
How to seek for bigram similarity in gensim word2vec model,"<p>Here I have a word2vec model, suppose I use the google-news-300 model</p>
<pre><code>import gensim.downloader as api
word2vec_model300 = api.load('word2vec-google-news-300')
</code></pre>
<p>I want to find the similar words for &quot;AI&quot; or &quot;artifical intelligence&quot;, so I want to write</p>
<pre><code>word2vec_model300.most_similar(&quot;artifical intelligence&quot;)
</code></pre>
<p>and I got errors</p>
<pre><code>KeyError: &quot;word 'artifical intelligence' not in vocabulary&quot;
</code></pre>
<p>So what is the right way to extract similar words for bigram words?</p>
<p>Thanks in advance!</p>
","machine-learning, nlp, gensim, word2vec","<p>At one level, when a word-token isn't in a fixed set of word-vectors, the creators of that set of word-vectors chose not to train/model that word. So, anything you do will only be a crude workaround for its absence.</p>
<p>Note, though, that when Google prepared those vectors – based on a dataset of news articles from before 2012 – they also ran some statistical multigram-combinations on it, creating multigrams with connecting <code>_</code> characters. So, first check if a vector for <code>'artificial_intelligence'</code> might be present.</p>
<p>If it isn't, you could try other rough workarounds like averaging together the vectors for <code>'artificial'</code> and <code>'intelligence'</code> – though of course that won't really be what people mean by the distinct combination of those words, just meanings suggested by the independent words.</p>
<p>The Gensim <code>.most_similar()</code> method can take either a raw vectors you've created by operations such as averaging, or even a list of multiple words which it will average for you, as arguments via its explicit keyword <code>positive</code> parameter. For example:</p>
<pre class=""lang-py prettyprint-override""><code>word2vec_model300.most_similar(positive=[average_vector])
</code></pre>
<p>...or...</p>
<pre class=""lang-py prettyprint-override""><code>word2vec_model300.most_similar(positive=['artificial', 'intelligence'])
</code></pre>
<p>Finally, though Google's old vectors are handy, they're a bit old now, &amp; from a particular domain (popular news articles) where senses may not match tose used in other domains (or more recently). So you may want to seek alternate vectors, or train your own if you have sufficient data from your area of interest, to have apprpriate meanings – including vectors for any particular multigrams you choose to tokenize in your data.</p>
",3,3,826,2021-11-10 08:22:12,https://stackoverflow.com/questions/69909863/how-to-seek-for-bigram-similarity-in-gensim-word2vec-model
disable logging for specific lines of code,"<p>I am tuning the word2vec model hyper-parameters. Word2Vec has to many log in console that I cannot read Optuna or my custom log. Is there any trick to suppress logs generated by Word2Vec?</p>
","python, stdout, gensim","<p>I used following code in python 3.7 in python 3.6 we have send <code>logging.ERROR</code> to disable function.</p>
<pre><code>import logging

logging.disable()
#your code
logging.disable(logging.DEBUG)
</code></pre>
",1,3,1356,2021-11-19 18:41:33,https://stackoverflow.com/questions/70039495/disable-logging-for-specific-lines-of-code
How to get Doc2Vec to run faster with a CPU count of 40?,"<p>I am building my own vocabulary to measure document similarity. I also attached the log of the run.</p>
<pre><code>tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]

max_epochs = 1000
vec_size =50
alpha = 0.025

tic = time.perf_counter()
#Building a model from the tokenized data

model = Doc2Vec(vector_size=vec_size,
                alpha=alpha, 
                min_alpha=0.0025,
                min_count=5,
                workers =5,
                dm =1)
  
model.build_vocab(tagged_data)


model.train(tagged_data,total_examples=model.corpus_count,epochs=max_epochs)



model.save(&quot;d2v.model&quot;)
print(&quot;Model Saved&quot;)

toc = time.perf_counter()
print(f&quot;Time {toc - tic:0.4f} seconds&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/k5jFF.png"" rel=""nofollow noreferrer"">Log of Doc2Vec</a></p>
","python, gensim, doc2vec","<p>Generally due to threading-contention inherent to both the Python 'Global Interpreter Lock' ('GIL'), and the default Gensim master-reader-thread, many-worker-thread approach, the training can't keep all cores mostly-busy with separate threads, once you get past about 8-16 cores.</p>
<p>If you can accept that the only <code>tag</code> for each text will be its ordinal number in the corpus, the alternate <code>corpus_file</code> method of specifying the training-data allow arbitrarily many threads to each open their own readers into the (whitespace-token-delimited) plain-text corpus file, achieving much higher core utilization when you have 16+ cores/workers.</p>
<p>See the Gensim docs for the <code>corpus_file</code> parameter:</p>
<p><a href=""https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec</a></p>
<p>Note though there are some <a href=""https://github.com/RaRe-Technologies/gensim/issues/2757"" rel=""nofollow noreferrer"">unsolved bugs</a> that hint this mode might mishandle or miss training data at some segmentation boundaries. (This may not be significant in large training data.)</p>
<p>Otherwise, other tweaks to parameters that help <code>Word2Vec</code>/<code>Doc2Vec</code> training run faster may be worth trying, such as altered <code>window</code>, <code>vector_size</code>, <code>negative</code> values. (Though note that counterintuitively, when the bottleneck is thread-contention in Gensim's default corpus-iterable mode, some values in these parameters that normally require more computation and thus imply slower training manage to mainly soak up previously-idle contention time, and thus are comparitively 'free'. So when suffering contention, trying more-expensive values for <code>window</code>/<code>negative</code>/<code>vector_size</code> may become more practical.)</p>
<p>Generally, a higher <code>min_count</code> (discarding more rare words), or a more-aggressive (smaller) <code>sample</code> value (discarding more of the overrepresented high-frequency words), can also reduce the amount of raw training happening and thus finish training faster, with minimal effect on quality. (Sometimes, more-aggressive <code>sample</code> values manage to <em>both</em> speed training &amp; improve results on downstream evaluations, by letting the model spend relatively more time on rarer words that are still important downstream.)</p>
",0,0,425,2021-12-01 20:54:49,https://stackoverflow.com/questions/70191077/how-to-get-doc2vec-to-run-faster-with-a-cpu-count-of-40
How to plot tsne on word2vec (created from gensim) for the most_similar 20 cases?,"<p>I am using TSNE to plot a trained word2vec model (created from gensim):</p>
<pre><code>labels = []
tokens = []

for word in model.wv.vocab:
    tokens.append(model[word])
    labels.append(word)

tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
new_values = tsne_model.fit_transform(tokens)

x = []
y = []
for value in new_values:
    x.append(value[0])
    y.append(value[1])
    
plt.figure(figsize=(50, 50)) 
for i in range(len(x)):
    plt.scatter(x[i],y[i])
    plt.annotate(labels[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')
plt.show()
</code></pre>
<p>Like as the inbuilt gensim method 'most_similar', per ex.</p>
<pre><code>w2v_model.wv.most_similar(postive=['word'], topn=20)
</code></pre>
<p>will output 20 of the most similar words to 'word', I will like to plot only the most similar words (n=20) of a given word. Any advice on how to modify the plot to do that?</p>
","python, gensim, word2vec, tsne","<p>Using an example from the package:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models import Word2Vec
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

model = Word2Vec(sentences=common_texts, window=5, min_count=1)

labels = [i for i in model.wv.vocab.keys()]
tokens = model[labels]

tsne_model = TSNE(init='pca',learning_rate='auto')
new_values = tsne_model.fit_transform(tokens)
</code></pre>
<p>tsne will look something like this:</p>
<pre><code>plt.figure(figsize=(7, 5)) 
for i in range(new_values.shape[0]):
    plt.scatter(x[i],y[i])
    plt.annotate(labels[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')
</code></pre>
<p><a href=""https://i.sstatic.net/bSfPS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bSfPS.png"" alt=""enter image description here"" /></a></p>
<p>Extract most similar for 'trees' (5 in my case) :</p>
<pre><code>most_sim_words = [i[0] for i in model.wv.most_similar(positive='trees', topn=5)]
most_sim_words
['human', 'graph', 'time', 'interface', 'system']
</code></pre>
<p>You can use code you have, just iterating through the most common words, and using <code>index()</code> to get their index in <code>tokens</code> :</p>
<pre><code>for word in most_sim_words:
    i = labels.index(word)
    plt.scatter(x[i],y[i])
    plt.annotate(labels[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')
</code></pre>
<p><a href=""https://i.sstatic.net/KycV9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KycV9.png"" alt=""enter image description here"" /></a></p>
",1,1,851,2021-12-07 23:41:52,https://stackoverflow.com/questions/70268270/how-to-plot-tsne-on-word2vec-created-from-gensim-for-the-most-similar-20-cases
Extract Topic Scores for Documents LDA Gensim Python,"<p>I am trying to extract topic scores for documents in my dataset after using and LDA model. Specifically, I have followed most of the code from here: <a href=""https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"" rel=""nofollow noreferrer"">https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/</a></p>
<p>I have completed the topic model and have the results I want, but the provided code only gives the most dominant topic for each document. Is there a simple way to modify the following code to give me the scores for say the 5 most dominant topics?</p>
<pre><code>##dominant topic for each document
def format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data):
    # Init output
    sent_topics_df = pd.DataFrame()

# Get main topic in each document
for i, row in enumerate(ldamodel[corpus]):
    row = sorted(row, key=lambda x: (x[1]), reverse=True)
    # Get the Dominant topic, Perc Contribution and Keywords for each document
    for j, (topic_num, prop_topic) in enumerate(row):
        if j == 0:  # =&gt; dominant topic
            wp = ldamodel.show_topic(topic_num)
            topic_keywords = &quot;, &quot;.join([word for word, prop in wp])
            sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
        else:
            break
sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

# Add original text to the end of the output
contents = pd.Series(texts)
sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
return(sent_topics_df)


df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']

# Show
df_dominant_topic.head(10)
</code></pre>
","python, gensim, lda, topic-modeling","<p>Right this is a crusty example because you haven't provided data to reproduce but using some gensim testing corpus, texts and dictionary we can do:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.test.utils import common_texts, common_corpus, common_dictionary
from gensim.models import LdaModel

# train a quick lda model using the common _corpus, _dictionary and _texts from gensim
optimal_model = LdaModel(common_corpus, id2word=common_dictionary, num_topics=10)
</code></pre>
<p>We can then rewrite the function slightly to become:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

##dominant topic for each document
def format_topics_sentences(ldamodel=optimal_model, 
                            corpus=common_corpus, 
                            texts=common_texts, 
                            n=1):
    &quot;&quot;&quot;
    A function for extracting a number of dominant topics for a given document
    using an existing LDA model
    &quot;&quot;&quot;
    # Init output
    sent_topics_df = pd.DataFrame()


    # Get main topic in each document
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            # we use range here to iterate over the n parameter
            if j in range(n):  # =&gt; dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = &quot;, &quot;.join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(
                    # and also use the i value here to get the document label
                    pd.Series([int(i), int(topic_num), round(prop_topic, 4), topic_keywords]),
                    ignore_index=True,
                )
            else:
                break
    sent_topics_df.columns = [&quot;Document&quot;, &quot;Dominant_Topic&quot;, &quot;Perc_Contribution&quot;, &quot;Topic_Keywords&quot;]

    # Add original text to the end of the output
    text_col = [texts[int(i)] for i in sent_topics_df.Document.tolist()]
    contents = pd.Series(text_col, name='original_texts')
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return sent_topics_df

</code></pre>
<p>Then we can use the function like this:</p>
<pre class=""lang-py prettyprint-override""><code>format_topics_sentences(ldamodel=optimal_model, corpus=common_corpus, texts=common_texts, n=2)
</code></pre>
<p>Where the <code>n</code> parameter specifies the number of dominant topics you want to extract.</p>
",2,0,1800,2021-12-09 19:38:19,https://stackoverflow.com/questions/70295773/extract-topic-scores-for-documents-lda-gensim-python
Upload a pre-trained spanish language word vectors and then retrain it with custom sentences? (GENSIM -FASTTEXT),"<p>I am trying to upload a pre-trained spanish language word vectors and then retrain it with custom sentences:</p>
<pre><code>!pip install fasttext
import fasttext
import fasttext.util
#download pre-trained spanish language word vectors c
fasttext.util.download_model('es', if_exists='ignore')  # Spanish
ft = fasttext.load_model('cc.es.300.bin')
</code></pre>
<p>but once I try to update the vocabulary it gives me this AttributeError:</p>
<pre><code>ft.build_vocab(sentences, update=True)
AttributeError: '_FastText' object has no attribute 'build_vocab'
</code></pre>
<p>Any advices?</p>
","python, nlp, gensim, fasttext","<p>The <code>build_vocab()</code> method supports a step in the <a href=""https://github.com/RaRe-Technologies/gensim"" rel=""nofollow noreferrer""><em>Gensim</em> library</a> implementation of the FastText algorithm - not the <a href=""https://github.com/facebookresearch/fastText"" rel=""nofollow noreferrer"">original <code>fastttext</code> package from Facebook</a> that you seem to be loading. (You're mixing code meant for two different libraries.)</p>
<p>If you switch to using Gensim code, rather than Facebook's implementation, you won't get that same error when trying to use <code>build_vocab()</code>.</p>
<p>Note, though, that what you're attempting, incremental retraining of an existing model, is an advanced/experimental technique that can easily backfire. So it's usually a bad idea to attempt without expertise &amp; rigorous checks as to whether the extra complications are helping.</p>
",0,0,440,2021-12-13 06:54:19,https://stackoverflow.com/questions/70330903/upload-a-pre-trained-spanish-language-word-vectors-and-then-retrain-it-with-cust
Gensim word2vec score function when out-of-vocabulary,"<p>Word2Vec cannot handle out-of-vocabulary words (returns an error). However, when I try the score function <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.score"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.score</a>
with sentences including OOV words, surprisingly, I do not get an error. Why is this the case?
Thank you!</p>
",gensim,"<p>The <code>score()</code> function is a training-like function, and like <code>train()</code> itself, simply ignores unknown words as if they weren't there. (Pondering whether this is the right decision or not for the goals of such 'scoring' is the subject of a nearby <a href=""https://github.com/RaRe-Technologies/gensim/blob/7d7bb84598e2e02e839b38c6b662d4357cbdce0a/gensim/models/word2vec_inner.pyx#L741"" rel=""nofollow noreferrer"">source-code-comment</a>.)</p>
<p>Note that these <code>score()</code> functions are a non-standard extension of <code>Word2Vec</code> contributed a while ago as part of the research paper mentioned in the related docs. Whether they work for any purpose, or still work for as originally intended in the latest versions of Gensim, isn't clear or certain. They might not be maintained in the future (and even now don't work in for usual default negative-sampling <code>Word2Vec</code> models).</p>
<p>So, you may not want to rely on them, and should study their raw source for info on their functionality.</p>
",0,0,229,2021-12-13 19:10:14,https://stackoverflow.com/questions/70339743/gensim-word2vec-score-function-when-out-of-vocabulary
Gensim doc2vec&#39;s d2v.wv.most_similar() gives not relevant words with high similarity scores,"<p>I've got a dataset of job listings with about 150 000 records. I extracted skills from descriptions using NER using a dictionary of 30 000 skills. Every skill is represented as an unique identificator.</p>
<p>My data example:</p>
<pre><code>          job_title    job_id                                         skills
1  business manager         4               12 13 873 4811 482 2384 48 293 48
2    java developer        55    48 2838 291 37 484 192 92 485 17 23 299 23...
3    data scientist        21    383 48 587 475 2394 5716 293 585 1923 494 3
</code></pre>
<p>Then, I train a doc2vec model using these data where job titles (their ids to be precise) are used as tags and skills vectors as word vectors.</p>
<pre><code>def tagged_document(df):
    for index, row in df.iterrows():
        yield gensim.models.doc2vec.TaggedDocument(row['skills'].split(), [str(row['job_id'])])
        
        
data_for_training = list(tagged_document(data[['job_id', 'skills']]))

model_d2v = gensim.models.doc2vec.Doc2Vec(dm=0, dbow_words=1, vector_size=80, min_count=3, epochs=100, window=100000)

model_d2v.build_vocab(data_for_training)

model_d2v.train(data_for_training, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)
</code></pre>
<p>It works mostly okay, but I have issues with some job titles. I tried to collect more data from them, but I still have an unpredictable behavior with them.</p>
<p>For example, I have a job title &quot;Director Of Commercial Operations&quot; which is represented as 41 data records having from 11 to 96 skills (mean 32). When I get most similar words for it (skills in my case) I get the following:</p>
<pre><code>docvec = model_d2v.docvecs[id_]
model_d2v.wv.most_similar(positive=[docvec], topn=5)
</code></pre>
<pre><code>capacity utilization 0.5729076266288757
process optimization 0.5405482649803162
goal setting 0.5288119316101074
aeration 0.5124399662017822
supplier relationship management 0.5117508172988892
</code></pre>
<p>These are top 5 skills and 3 of them look relevant. However the top one doesn't look too valid together with &quot;aeration&quot;. The problem is that none of the job title records have these skills at all. It seems like a noise in the output, but why it gets one of the highest similarity scores (although generally not high)?
Does it mean that the model can't outline very specific skills for this kind of job titles?
Can the number of &quot;noisy&quot; skills be reduced? Sometimes I see much more relevant skills with lower similarity score, but it's often lower than 0.5.</p>
<p>One more example of correct behavior with similar amount of data:
BI Analyst, 29 records, number of skills from 4 to 48 (mean 21). The top skills look alright.</p>
<pre><code>business intelligence 0.6986587047576904
business intelligence development 0.6861011981964111
power bi 0.6589289903640747
tableau 0.6500121355056763
qlikview (data analytics software) 0.6307920217514038
business intelligence tools 0.6143202781677246
dimensional modeling 0.6032138466835022
exploratory data analysis 0.6005223989486694
marketing analytics 0.5737696886062622
data mining 0.5734485387802124
data quality 0.5729933977127075
data visualization 0.5691111087799072
microstrategy 0.5566076636314392
business analytics 0.5535123348236084
etl 0.5516749620437622
data modeling 0.5512707233428955
data profiling 0.5495884418487549
</code></pre>
","nlp, gensim, word2vec, word-embedding, doc2vec","<p>If the your gold standard of what the model should report is skills that appeared in the training data, are you sure you don't want a simple count-based solution? For example, just provide a ranked list of the skills that appear most often in <code>Director Of Commercial Operations</code> listings?</p>
<p>On the other hand, the essence of compressing N job titles, and 30,000 skills, into a smaller (in this case <code>vector_size=80</code>) coordinate-space model is to force some non-intuitive (but perhaps real) relationships to be reflected in the model.</p>
<p>Might there be some real pattern in the model – even if, perhaps, just some idiosyncracies in the appearance of less-common skills – that makes <code>aeration</code> necessarily slot near those other skills? (Maybe it's a rare skill whose few contextual appearances co-occur with other skills very much near 'capacity utilization' -meaning with the tiny amount of data available, &amp; tiny amount of overall attention given to this skill, there's no better place for it.)</p>
<p>Taking note of whether your 'anomalies' are often in low-frequency skills, or lower-freqeuncy job-ids, might enable a closer look at the data causes, or some disclaimering/filtering of <code>most_similar()</code> results. (The <code>most_similar()</code> method can limit its returned rankings to the more frequent range of the known vocabulary, for cases when the long-tail or rare words are, in with their rougher vectors, intruding in higher-quality results from better-reqpresented words. See the <code>restrict_vocab</code> parameter.)</p>
<p>That said, tinkering with training parameters may result in rankings that better reflect your intent. A larger <code>min_count</code> might remove more tokens that, lacking sufficient varied examples, mostly just inject noise into the rest of training. A different <code>vector_size</code>, smaller or larger, might better capture the relationships you're looking for. A more-aggressive (smaller) <code>sample</code> could discard more high-frequency words that might be starving more-interesting less-frequent words of a chance to influence the model.</p>
<p>Note that with <code>dbow_words=1</code> &amp; a large window, and records with (perhaps?) dozens of skills each, the words are having a much-more <em>neighborly</em> effect on each other, in the model, than the <code>tag</code>&lt;-&gt;<code>word</code> correlations. That might be good or bad.</p>
",0,1,459,2021-12-14 14:53:20,https://stackoverflow.com/questions/70350954/gensim-doc2vecs-d2v-wv-most-similar-gives-not-relevant-words-with-high-simila
Is there a way to use only the word result of most_similar function of gensim?,"<p>I am trying to use the most similar function of gensim but the results came in as a list that is let's say a=(word, cosine similarity). However I can't retrieve the word by a[0].Is there a way to access the word itself? I need to use it as an input.</p>
","trigonometry, gensim, word2vec, similarity","<p>The ranked list of results returned by <code>most_similar()</code> is a <em>list</em> where each item is a tuple of a word and its similarity value.</p>
<p>After...</p>
<pre><code>sims = model.wv.most_similar(my_word)
top_word = sims[0][0]
</code></pre>
<p>...<code>top_word</code> would have the word most-similar to <code>my_word</code>.</p>
",0,0,305,2021-12-16 06:24:45,https://stackoverflow.com/questions/70374705/is-there-a-way-to-use-only-the-word-result-of-most-similar-function-of-gensim
rare misspelled words messes my fastText/Word-Embedding Classfiers,"<p>I'm currently trying to make a sentiment analysis on the IMDB review dataset as a part of homework assignment for my college, I'm required to firstly do some preprocessing e.g. : tokenization, stop words removal, stemming, lemmatization. then use different ways to convert this data to vectors to be classfied by different classfiers, Gensim FastText library was one of the required models to obtain word embeddings on the data I got from text pre-processing step.</p>
<p>the problem I faced with Gensim is that I firstly tried to train on my data using vectors of feature size (100,200,300) but yet they always fail at some point, I tried later to use many pre-trained Gensim data vectors, but none of them worked to find word embeddings for all of the words, they'd rather fail at some point with error</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-28-644253c70aa3&gt; in &lt;module&gt;()
----&gt; 1 model.most_similar(some-rare-word)

1 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py in word_vec(self, word, use_norm)
    450             return result
    451         else:
--&gt; 452             raise KeyError(&quot;word '%s' not in vocabulary&quot; % word)
    453 
    454     def get_vector(self, word):

KeyError: &quot;word some-rare-word not in vocabulary&quot;
</code></pre>
<p>the ones I've tried so far are :<br />
conceptnet-numberbatch-17-06-300 : doesn't contain &quot;glass&quot;<br />
word2vec-google-news-300 : ram insufficient in Google Colab<br />
glove-twitter-200 : doesn't contain &quot;5&quot;<br />
crawl-300d-2M : doesn't contain &quot;waltons&quot;<br />
wiki-news-300d-1M : doesn't contain &quot;waltons&quot;<br />
glove-wiki-gigaword-300 : doesn't contain &quot;riget&quot;</p>
<p>got their names from these sources, <a href=""https://github.com/RaRe-Technologies/gensim-data#readme"" rel=""nofollow noreferrer"">here</a> and <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">here</a></p>
<p>by inspecting the failing words, I found that even the largest libraries would usually fail because of the misspelled words that has no meaning like 'riget', 'waltons',...etc</p>
<p>Is their a way to classify and neglect this strange words before trying to inject them to Gensim and receiving this error ? or am I using Gensim very wrong and there's another way to use it ?</p>
<p>any snippet of code or some sort of lead on what to do would be appreciated</p>
<p>my code so far :</p>
<pre><code>import gensim.downloader as api
model = api.load(&quot;glove-wiki-gigaword-300&quot;) # this can be any vector-library of the previously mentioned ones
train_word_embeddings = []
# train_lemm is a vector of size (number of examples, number of words remaining in example sentence i after removal of stop words and lemmatization to nouns)
# they're mainly 25000 example review sentence while the second dimension is of random value depending on number of words
for i in range (len(train_lemm)): 
  train_word_embeddings.append(model.wv[train_lemm[i]])
</code></pre>
","python, gensim, sentiment-analysis, word-embedding","<p>If you train your own word-vector model, then it will contain vectors for all the words you told it to learn. If a word that was in your training data doesn't appear to have a vector, it likely did not appear the required <code>min_count</code> number of times. (These models tend to <em>improve</em> if you discard rare words who few example usages may not be suitably-informative, so the default <code>min_words=5</code> is a good idea.)</p>
<p>It's often reasonable for downstream tasks, like feature engineering using the text &amp; set of word-vectors, to simply ignore words with no vector. That is, if <code>some_rare_word in model.wv</code> is <code>False</code>, just don't try to use that word – &amp; its missing vector – for anything. So you don't necessarily need to find, or train, a set of word-vectors with <em>every</em> word you need. Just elide, rather than worry-about, the rare missing words.</p>
<p>Separate observations:</p>
<ul>
<li>Stemming/lemmatization &amp; stop-word removal aren't always worth the trouble, with all corpora/algorithms/goals. (And, stemming/lemmatization may wind up creating pseudowords that limit the model's interpretability &amp; easy application to any texts that don't go through identical preprocessing.) So if those are required parts of laerning exercise, sure, get some experience using them. But don't assume they're necessarily helping, or worth the extra time/complexity, unless you verify that rigrously.</li>
<li>FastText models will also be able to supply synthetic vectors for words that <em>aren't</em> known to the model, based on substrings. These are often pretty weak, but may better than nothing - especially when they give vectors for typos, or rare infelcted forms, similar to morphologically-related known words. (Since this deduced similarity, from many similarly-written tokens, provides some of the same value as stemming/lemmatization via a different path that <em>required</em> the original variations to all be present during initial training, you'd especially want to pay attention to whether FastText &amp; stemming/lemmatization mix well for your goals.) Beware, though: for very-short unknown words – for which the model learned no reusable substring vectors – FastText may still return an error or all-zeros vector.</li>
<li>FastText has a <code>supervised</code> classification mode, but it's not supported by Gensim. If you want to experiment with that, you'd need to use the Facebook FastText implementation. (You could still use a traditional, non-<code>supervised</code> FastText word vector model as a contributor of features for other possible representations.)</li>
</ul>
",1,0,756,2021-12-16 19:53:52,https://stackoverflow.com/questions/70384870/rare-misspelled-words-messes-my-fasttext-word-embedding-classfiers
How to avoid Gensim Simple Preprocess to remove digits?,"<p>I am having some problems in preprocessing some data with <code>gensim.utils.simple_preprocess</code>.
In a few words, I noticed that the <code>simple_preprocess</code> function removes the digits from my text, but I don't want to!
For instance, I have this code:</p>
<pre><code>import gensim
from gensim.utils import simple_preprocess

my_text = [&quot;I am doing activity number 1&quot;, &quot;Instead, I am doing the number 2&quot;]

def gen_words(texts):
    final = []
    for text in texts:
        new = gensim.utils.simple_preprocess(text, deacc=True, min_len=1)
        final.append(new)
    return (final)

solution = gen_words(my_text)

print (solution)
</code></pre>
<p>The output is the following:</p>
<pre><code>[['i', 'am', 'doing', 'activity', 'number'], ['instead', 'i', 'am', 'doing', 'the', 'number']]
</code></pre>
<p>I would like instead to have this as a solution:</p>
<pre><code>[['i', 'am', 'doing', 'activity', 'number', '1'], ['instead', 'i', 'am', 'doing', 'the', 'number', '2']]
</code></pre>
<p>How to avoid seeing the digits erased from my code? I have also tried setting the <code>min_len=0</code> but still is not working.</p>
","python, gensim","<p>The <code>simple_preprocess()</code> function is just one rather simple convenience option for tokenizing text from a string, into a list-of-tokens.</p>
<p>It's not especially well-tuned for any particular need – and it has no configurable option to retain tokens that don't match its particular hardcoded pattern (<code>PAT_ALPHABETIC</code>) which rules-out tokens with leading digits.</p>
<p>Many projects will want to apply their own tokenization/preprocessing instead, better suited to their data &amp; problem domain. If you need ideas for how to start, youc can consult the actual source code for <code>simple_preprocess()</code> (and other functions it relies upon like <code>tokenize()</code> &amp; <code>simple_tokenize()</code>) that Gensim uses:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/utils.py"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/utils.py</a></p>
",2,1,1054,2021-12-21 11:06:33,https://stackoverflow.com/questions/70434454/how-to-avoid-gensim-simple-preprocess-to-remove-digits
Can&#39;t load the pre-trained word2vec of korean language,"<p>I would like to download and load the pre-trained word2vec for analyzing Korean text.</p>
<p>I download the pre-trained word2vec here: <a href=""https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view?resourcekey=0-Dq9yyzwZxAqT3J02qvnFwg"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view?resourcekey=0-Dq9yyzwZxAqT3J02qvnFwg</a>
from the Github Pre-trained word vectors of 30+ languages: <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">https://github.com/Kyubyong/wordvectors</a></p>
<p>My gensim version is 4.1.0, thus I used:
<code>KeyedVectors.load_word2vec_format('./ko.bin', binary=False)</code> to load the model. But there was an error that :</p>
<blockquote>
<p>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte</p>
</blockquote>
<p>I already tried many options including in stackoverflow and Github, but it still not work well.
Would you mind letting me the suitable solution?</p>
<p>Thanks,</p>
","gensim, word2vec","<p>While the page at <a href=""https://github.com/Kyubyong/wordvectors"" rel=""nofollow noreferrer"">https://github.com/Kyubyong/wordvectors</a> isn't clear about the formats this author has chosen, by looking at their source code at...</p>
<p><a href=""https://github.com/Kyubyong/wordvectors/blob/master/make_wordvectors.py#L61"" rel=""nofollow noreferrer"">https://github.com/Kyubyong/wordvectors/blob/master/make_wordvectors.py#L61</a></p>
<p>...shows it using the Gensim model <code>.save()</code> method.</p>
<p>Such saved models should be reloaded using the <code>.load()</code> class method of the same model class. For example, if a <code>Word2Vec</code> model was saved with...</p>
<pre><code>model.save('language.bin')
</code></pre>
<p>...then it could be reloaded with...</p>
<pre><code>loaded_model = Word2Vec.load('language.bin')
</code></pre>
<p>Note, through, that:</p>
<ul>
<li>Models saved this way are often split over multiple files that should be kept together (and all start with the same root name) - but I don't see those here.</li>
<li>This work appears to be ~5 years old, based on a pre-1.0 version of Gensim – so there might be issues loading the models directly into the latest Gensim. If you do run into such issues, &amp; absolutely need to make these vectors work, you might need to temporarily use a prior version of Gensim to <code>.load()</code> the model. Then, you could save the plain vectors out with <code>.save_word2vec_format()</code> for later reloading across any version. (Or, using the latest interim version that can load the model, re-save the model as <code>.save()</code>, then repeat the process with the latest version that can read <em>that</em> model, until you reach the current Gensim.)</li>
</ul>
<p>But, you also might want to find a more recent &amp; better-documented set of pretrained word-vectors.</p>
<p>For example, Facebook makes FastText pretrained vectors available in both a 'text' format and a 'bin' format for many languages at <a href=""https://fasttext.cc/docs/en/pretrained-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/pretrained-vectors.html</a> (trained on Wikipedia only) or <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">https://fasttext.cc/docs/en/crawl-vectors.html</a> (trained on Wikipedia plus web crawl data).</p>
<p>The 'text' format should in fact be loadable with <code>KeyedVectors.load_word2vec_format(filename, binary=False)</code>, but will only include full-word vectors. (It will also be relatively easy to view as text, or write simply code to massage into other formats.)</p>
<p>The 'bin' format is Facebook's own native FastText model format, and should be loadable with either the <a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model"" rel=""nofollow noreferrer""><code>load_facebook_model()</code></a> or <a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_vectors"" rel=""nofollow noreferrer""><code>load_facebook_vectors()</code></a> utility methods. Then, the loaded model (or vectors) will be able to create the FastText algorithm's substring-based guesstimate vectors even for many words that weren't in the model or training data.</p>
",1,1,494,2021-12-23 07:03:09,https://stackoverflow.com/questions/70458726/cant-load-the-pre-trained-word2vec-of-korean-language
gensim/ Training a LDA Model: &#39;int&#39; object is not subscriptable,"<p>I create a new word list in which stop words from 'text8' have been removed, in order to train a LDA Model. However, I received <code>TypeError: 'int' object is not subscriptable</code>, guessing problems from corpus, and cannot find the solutions.</p>
<p>Here is my code:</p>
<pre><code>import gensim.downloader as api
corpus=api.load('text8')
dictionary=gensim.corpora.Dictionary(corpus) # generate a dictionary from the text corpus

# removing stop words
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('stopwords')
nltk.download('punkt')

stop_words = set(stopwords.words('english'))
word_tokens = dictionary

filtered_sentence = []
for w in word_tokens:
    if word_tokens[w] not in stop_words:
        filtered_sentence.append(word_tokens[w])

#print(filtered_sentence)

# generate a new dictionary from &quot;filtered_sentence&quot;

dct=gensim.corpora.Dictionary([filtered_sentence])
corpus2=dct.doc2bow(filtered_sentence)
</code></pre>
<p>The following line is not working-- TypeError: 'int' object is not subscriptable</p>
<pre><code>model=gensim.models.ldamodel.LdaModel(corpus2, num_topics=5, id2word=dct) #TypeError

model.print_topics(num_words=5)
</code></pre>
<p>Detailed error message:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-64-75e1fe1a727b&gt; in &lt;module&gt;()
----&gt; 1 model=gensim.models.ldamodel.LdaModel(corpus2, num_topics=5, id2word=dct) #TypeError: 'int' object is not subscriptable
      2 model.print_topics(num_words=5)

3 frames
/usr/local/lib/python3.7/dist-packages/gensim/models/ldamodel.py in inference(self, chunk, collect_sstats)
    651         # to Blei's original LDA-C code, cool!).
    652         for d, doc in enumerate(chunk):
--&gt; 653             if len(doc) &gt; 0 and not isinstance(doc[0][0], six.integer_types + (np.integer,)):
    654                 # make sure the term IDs are ints, otherwise np will get upset
    655                 ids = [int(idx) for idx, _ in doc]

TypeError: 'int' object is not subscriptable
</code></pre>
<p>Really appreciate your help. Thank you so much!</p>
","python, nlp, gensim, lda","<p>The error is likely related to <code>filtered_sentence</code> being used as <code>corpus2</code>. For the code to work <code>corpus2</code> must be a list of lists of tuples. So, this trick should help:</p>
<pre><code>corpus2 = [dct.doc2bow(filtered_sentence),]
</code></pre>
",1,0,274,2021-12-24 16:25:40,https://stackoverflow.com/questions/70474885/gensim-training-a-lda-model-int-object-is-not-subscriptable
How to load pre trained FastText Word Embeddings using Gensim?,"<p>I downloaded word embedding <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">from this link</a>. I want to load it in <code>Gensim</code> to do some work but I am not able to load it. I have found many resources and none of it is working. I am using <code>Gensim</code> version <code>4.1</code>.</p>
<p>I have tried</p>
<pre><code>gensim.models.fasttext.load_facebook_model('/home/admin1/embeddings/crawl-300d-2M.vec')
gensim.models.fasttext.load_facebook_vectors('/home/admin1/embeddings/crawl-300d-2M.vec')
</code></pre>
<p>and it is showing me</p>
<pre><code>NotImplementedError: Supervised fastText models are not supported
</code></pre>
<p>I went to try to load it using using <code>FastText.load('/home/admin1/embeddings/crawl-300d-2M.vec',)</code> but then it showed <code>UnpicklingError: could not find MARK</code>.</p>
<p>Also, using</p>
","nlp, stanford-nlp, gensim, fasttext","<p>Per the <code>NotImplementedError</code>, those are the one kind of full Facebook FastText model, <code>-supervised</code> mode, that Gensim does not support.</p>
<p>So sadly, the answer to &quot;How do you load these?&quot; is &quot;you don't&quot;.</p>
<p>The <code>.vec</code> files contain just the full-word vectors in a plain-text format – no subword info for synthesizing OOV vectors, or supervised-classification output features. Those can be loaded into a <code>KeyedVectors</code> model:</p>
<pre class=""lang-py prettyprint-override""><code>kv_model = KeyedVectors.load_word2vec_format('crawl-300d-2M.vec')
</code></pre>
",3,1,1130,2021-12-29 16:20:14,https://stackoverflow.com/questions/70522109/how-to-load-pre-trained-fasttext-word-embeddings-using-gensim
Gensim train not updating weights,"<p>I have a domain specific corpus for which I am trying to train embeddings. Since I want to be comprehensive in vocabulary, I am adding word vectors from <code>glove.6B.50d.txt</code>. Post adding vectors from here, I am training the model using the corpus I have.</p>
<p>I am trying the solutions from <a href=""https://datascience.stackexchange.com/questions/10695/how-to-initialize-a-new-word2vec-model-with-pre-trained-model-weights"">here</a> but the word embeddings don't seem to update.</p>
<p>This is the solution I have so far.</p>
<pre><code>#read glove embeddings
glove_wv = KeyedVectors.load_word2vec_format(GLOVE_PATH, binary=False)

#initialize w2v model
model =  Word2Vec(vector_size=50, min_count=0, window=20, epochs=10, sg=1, workers=10, 
                      hs=1, ns_exponent=0.5, seed=42, sample=10**-2, shrink_windows=True)
model.build_vocab(sentences_tokenized)
training_examples_count = model.corpus_count

# add vocab from glove
model.build_vocab([list(glove_wv.key_to_index.keys())], update=True)
model.wv.vectors_lockf = np.zeros(len(model.wv)) # ALLOW UPDATE OF WEIGHTS FROM BACK PROP; 0 WILL SUPPRESS

# add glove embeddings
model.wv.intersect_word2vec_format(GLOVE_PATH,binary=False, lockf=1.0)
</code></pre>
<p>Below I am training the model and checking word embedding of a particular word explicitly present in training</p>
<pre><code># train model
model.train(sentences_tokenized,total_examples=training_examples_count, epochs=model.epochs)

#CHECK IF EMBEDDING CHANGES FOR 'oyo'
print(model.wv.get_vector('oyo'))
print(glove_wv.get_vector('oyo'))
</code></pre>
<p>The word embeddings of the word <code>oyo</code> comes out to be same before and after the training. Where am I going wrong?</p>
<p>The input corpus- <code>sentences_tokenized</code> contains few sentences that contains the word <code>oyo</code>. One of such sentences-</p>
<pre><code>'oyo global platform empowers entrepreneur small business hotel home providing full stack technology increase earnings eas operation bringing affordable trusted accommodation guest book instantly india largest budget hotel chain oyo room one preferred hotel booking destination vast majority student country hotel chain offer many benefit include early check in couple room id card flexibility oyo basically network budget hotel completely different famous hotel aggregator like goibibo yatra makemytrip partner zero two star hotel give makeover room bring customer hotel website mobile app'
</code></pre>
","python, stanford-nlp, gensim, word2vec, word-embedding","<p>You're improvising a lot here with a bunch of potential errors or suboptimalities. Note especially that:</p>
<ul>
<li>While (because it's Python) you can always mutate the models however you want for interesting effects, seeding a model with outside word-vectors then continuing training isn't formally- or well-supported by Gensim. As far as I can tell – &amp; I wrote a bunch of this code! – there aren't any good docs/examples of doing it well, or doing the necessary tuning/validation of results, or demonstrating a reliable advantage of this technique. Most examples online are of eager people plowing ahead unaware of the tradeoffs, seeing a trivial indicator of completion or a tiny bit of encouraging results, and then overconfidently showing their work as if this were a well-grounded technique or best-practice. It isn't. Without a deep understanding of the model, &amp; review of the source code, &amp; regular re-checking of your results for sanity/improvement, there will be hidden gotchas. It is especially the case that fresh training on just a subset of all words could pull those words out of compatible coordinate alignment with other words not receiving training.</li>
<li>The <code>intersect_word2vec_format()</code> feature, and especially the <code>lockf</code> function, are also experimental - one stab at maybe offering a way to mix in other word-vectors, but without any theoretical support. (I also believe <code>intersect_word2vec_format()</code> remains slightly broken in recent (circa 4.1.2) Gensim versions, though there may be a <a href=""https://github.com/RaRe-Technologies/gensim/issues/3094"" rel=""nofollow noreferrer"">simple workaround</a>.) Still, the <code>lockf</code> functionality may require tricky manual initialization &amp; adaptation to other non-standard steps. To use it, it'd be best to read &amp; understand the Gensim source code where related variables appear.</li>
</ul>
<p>So, if you really need a larger vocabulary than your initial domain-specific corpus, the safest approach is probably to extend your training corpus with more texts that feature the desired words, as used in similar language contexts. (For example, if you rdomain is scientific discourse, you'd want to extend your corpus with more similar scientific text to learn compatible words – not, say, classic fiction.) Then all words go through the well-characterized simultaneous training process.</p>
<p>That said, if you really want to continue experimenting with this potentially complicated and error-prone improvised approach, your main problems might be:</p>
<ul>
<li>using strings as your sentences instead of lists-of-tokens (so the training 'words' wind up actually just being single-characters)</li>
<li>something related to the <code>intersect_word2vec_format</code> bug; check if <code>.vectors_lockf</code> is the right length, with <code>1.0</code> in the all the right slots for word-updates, before training</li>
</ul>
<p>Separately, other observations:</p>
<ul>
<li><code>min_count=0</code> is usually a bad idea: these models improve when you discard rare words entirely. (Though, when doing a <code>.build_vocab(…, update=True)</code> vocab-expansion, a bunch of things with the usual neat handling of low-frequency words and frequency-sorted vocabularies become screwy.)</li>
<li><code>hs=1</code> should generally not be set without also disabling the usually-preferred default negative-sampling with <code>negative=0</code>. (Otherwise, you're creating a hybrid franken-model, using both modes on one side of the internal neural network, that share the same input word-vectors: a much slower approach not especially likely to be better than either alone.)</li>
<li><code>ns_exponent=0.5</code> is non-standard, and using non-standard values for the parameter is most-likely to offer benefit in peculiar situations (like training texts that aren't true natural language sentences), and should only be tweaked within a harness for comparing results with alternate values.</li>
<li><code>sample=10**-2</code> is also non-standard, and such a large value might be nearly the same as turning off <code>sample</code> (say with a <code>0</code> value) entirely. It's more common to want to make this parameter more-aggressive (smaller than the default), if you have plentiful training data.</li>
</ul>
<p>In general, while the defaults aren't sacred, you generally should avoid tinkering with them until you have both (a) a good idea of why your corpus/goals might benefit from a different value; &amp; (b) a system for verifying which alterations are helping or hurting, such as a grid-search over many parameter combinations that scores the fitness of resulting models on (some proxy for) your true end task.</p>
",1,1,392,2021-12-30 14:48:04,https://stackoverflow.com/questions/70533179/gensim-train-not-updating-weights
gensim w2k - additional file,"<p>I trained w2v on rather big (&gt; 200 million sentences) corpus, and got, in addition to file w2v_model.model, files: w2v_model.model.trainables.syn1neg.npy and w2v.model_model.wv.vectors.npy. Model file was successfully loaded and read all npy files without any exceptions. The obtained model performed OK.</p>
<p>Now I retrained the model on much bigger corpus (&gt; 1 billion sentences). The same 3 files were automatically saved, as expected.</p>
<p>When I try to load my new retrained model:</p>
<pre><code>w2v_model = Word2Vec.load(path_filename)
</code></pre>
<p>I get:</p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/Users/...../w2v_US.model.trainables.vectors_lockf.npy'
</code></pre>
<p>But no .npy file with such extension was saved by gensim at the end of the training
(I save all output files in the same library, as required).</p>
<p>What should I do to obtain such file as a part of output .npy files (may be some option in  gensim w2v when training)? May be there are other ways to overcome this issue?</p>
","gensim, word2vec","<p>If a <code>.save()</code> is creating any files with the word <code>trainables</code> in it, you're using a older version fo Gensim. Any new training should definitely prefer using a current version. As of now (January 2022), that's <code>gensim-4.1.2</code>, released 2021-09.</p>
<p>If an attempt at a <code>.load()</code> generated that particular error, then there should've been that file, alongside the others you mention, created when the <code>.save()</code> had been done. (In fact, the only way that the <em>main</em> file you named with <code>path_filename</code> should be able to know that <em>other</em> filename is if that other file was written successfully, allowing the main file to complete writing.)</p>
<p>Are you sure that file wasn't written, but then somehow left behind, perhaps getting deleted or not moving alongside the other few files to some new filesystem path?</p>
<p>In general, I would suggest:</p>
<ul>
<li>using latest Gensim for any new training</li>
<li>always enable Python logging at the INFO level, &amp; watch the logging/console output of training/saving processes closely to see confirmation of expected activity/steps</li>
<li>keep all files from a <code>.save()</code> that begin with the same main filename (in your examples above, <code>w2v_US.model</code>) together - &amp; keep in mind that for larger models it may be a larger roster of files than for a small test model</li>
</ul>
<p>You will probably have to re-train the model, but you <em>might</em> be able to re-generate a compatible <code>lockf</code> file via steps like the following:</p>
<ul>
<li>save aside all files of any potential use</li>
<li>from the exact same configuration as your original <code>.save()</code> – including the same outdated Gensim version, exact same model parameters, &amp; exact same training corpus – repeat all the model-building steps you did before up through the <code>.build_vocab()</code> step. (That is: no extra need to <code>.train()</code>.) This will create an <em>untrained</em> dummy model that should exactly match the vocabulary 'shape' of your broken model.</li>
<li>use <code>.save()</code> to save <em>that</em> dummy model again - watching the logs/output for errors. There should be, alongside the other files, a file with a name like <code>dummy.model.trainables.vectors_lockf.npy</code>. If so, you <em>might</em> be able to copy that away, rename it to tbe the file expected by the original model whose load failed, then leave it alongside that original model - and the <code>.load()</code> might then succeed, or fail in a different way.</li>
</ul>
<p>(If there were other problems/corruption at the time of the original model creation, this might not work. In particular, I wonder if when you talk about retraining the model, you didn't start with a fresh <code>Word2Vec</code> instance, but somehow expanded the older one, which might've added other problems/complications. In that case, a full retraining, ideally in the latest Gensim, would be necessary, and also a better basis for going forward.)</p>
",1,1,142,2022-01-13 08:20:41,https://stackoverflow.com/questions/70693372/gensim-w2k-additional-file
Unable to load pre-trained gensim Doc2Vec from publication data,"<p>I want to use an already trained Doc2Vec from a published paper.</p>
<p><strong>Paper</strong></p>
<blockquote>
<p>Whalen, R., Lungeanu, A., DeChurch, L., &amp; Contractor, N.
(2020). Patent Similarity Data and Innovation Metrics. Journal of
Empirical Legal Studies, 17(3), 615–639.
<a href=""https://doi.org/10.1111/jels.12261"" rel=""nofollow noreferrer"">https://doi.org/10.1111/jels.12261</a></p>
</blockquote>
<p><strong>Code</strong></p>
<blockquote>
<p><a href=""https://github.com/ryanwhalen/patent_similarity_data"" rel=""nofollow noreferrer"">https://github.com/ryanwhalen/patent_similarity_data</a></p>
</blockquote>
<p><strong>Data</strong></p>
<blockquote>
<p><a href=""https://zenodo.org/record/3552078#.YeWkFvgxmUk"" rel=""nofollow noreferrer"">https://zenodo.org/record/3552078#.YeWkFvgxmUk</a></p>
</blockquote>
<p>However, when trying to load the model (patent_doc2v_10e.model) an error is raised. <strong>Edit</strong>: The file can be downloaded from the data repository (link above). I am not the author of the paper nor the creator of the model.</p>
<pre><code>from gensim.models.doc2vec import Doc2Vec
model = Doc2Vec.load(&quot;patent_doc2v_10e.model&quot;)


FileNotFoundError: [Errno 2] No such file or directory: 'patent_doc2v_10e.model.trainables.syn1neg.npy'
</code></pre>
<p>Am I missing files or do I have to load the model in other ways?</p>
","python, numpy, gensim, doc2vec, pre-trained-model","<p>Where did the file <code>patent_doc2v_10e.model</code> come from?</p>
<p>If trying to load that file, it generates such an  error about another file with the name <code>patent_doc2v_10e.model.trainables.syn1neg.npy</code>, then that other file is a necessary part of the full model that should have been created alongside <code>patent_doc2v_10e.model</code> when that <code>patent_doc2v_10e.model</code> file was first <code>.save()</code>-persisted to disk.</p>
<p>You'll need to go back to where <code>patent_doc2v_10e.model</code> was created, &amp; find the extra missing  <code>patent_doc2v_10e.model.trainables.syn1neg.npy</code> file (&amp; possibly others also starting <code>patent_doc2v_10e.model…</code>). All such files created at the same <code>.save()</code> must be kept/moved together, at the same filesystem path, for any future <code>.load()</code> to succeed.</p>
<p>(Additionally, if you are training these yourself from original data, I'd suggest being sure to use a current version of Gensim. Only older pre-4.0 versions will create any save files with <code>trainables</code> in the name.)</p>
",1,1,331,2022-01-17 17:21:38,https://stackoverflow.com/questions/70745209/unable-to-load-pre-trained-gensim-doc2vec-from-publication-data
Problem with creating dictionary with gensim for LDA,"<p>I have a problem running gensim to create a Dictionary and the Doc Term Matrix.</p>
<p>When I run:</p>
<pre><code>from gensim import corpora, models
import gensim
clean = ['door', 'cat', 'mom']
dictionary = corpora.Dictionary(clean)
</code></pre>
<p>I get:</p>
<pre><code>doc2bow expects an array of unicode tokens on input, not a single string
</code></pre>
<p>In the real problem, Clean is still a list-type variable. It's all the words in a large corpus after applying a tokenizer, tagger, removing punctuation, etc.</p>
<p>Why am I getting this error?</p>
","python, machine-learning, nlp, gensim, corpus","<p>Each item in the corpus should be a sequence of unicode tokens (words), not a string.</p>
<p>If you want the strings <code>'door'</code>, <code>'cat'</code>, &amp; <code>'mom'</code> to be the words in the dictionary, you could do:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim import corpora
corpus = [
    ['door', 'cat', 'mom'],
]
dictionary = corpora.Dictionary(corpus)
</code></pre>
",1,0,256,2022-01-23 05:00:05,https://stackoverflow.com/questions/70819255/problem-with-creating-dictionary-with-gensim-for-lda
Understanding results of word2vec gensim for finding substitutes,"<p>I have implemented the word2vec model on transaction data <a href=""https://docs.google.com/spreadsheets/d/1mcFnagqajPm9XqJhBjgXW2waGraJ4mVq/edit?usp=sharing&amp;ouid=105487340749910932665&amp;rtpof=true&amp;sd=true"" rel=""nofollow noreferrer"">(link)</a> of a single category. <br>
My goal is to find substitutable items from the data.<br>
The model is giving results but I want to make sure that my model is giving results based on customers historical data (considering context) and not just based on content (semantic data). Idea is similar to the recommendation system. <br>
I have implemented this using the gensim library, where I passed the data (products) in form of a list of lists.</p>
<p>Eg.</p>
<pre><code>[['BLUE BELL ICE CREAM GOLD RIM', 'TILLAMK CHOC CHIP CK DOUGH  IC'],
 ['TALENTI SICILIAN PISTACHIO GEL', 'TALENTI BLK RASP CHOC CHIP GEL'],
 ['BREYERS HOME MADE VAN ICE CREAM',
  'BREYERS HOME MADE VAN ICE CREAM',
  'BREYERS COFF ICE CREAM']]
</code></pre>
<p>Here, each of the sub lists is the past one year purchase history of a single customer. <br></p>
<pre><code># train word2vec model
model = Word2Vec(window = 5, sg = 0,
                 alpha=0.03, min_alpha=0.0007,
                 seed = 14)

model.build_vocab(purchases_train, progress_per=200)

model.train(purchases_train, total_examples = model.corpus_count, 
            epochs=10, report_delay=1)

# extract all vectors
X = []
words = list(model.wv.index_to_key)
for word in words:
    x = model.wv.get_vector(word)
    X.append(x)
Y = np.array(X)
Y.shape

def similar_products(v, n = 3):
    
    # extract most similar products for the input vector
    ms = model.wv.similar_by_vector(v, topn= n+1)[1:]
    
    # extract name and similarity score of the similar products
    new_ms = []
    for j in ms:
        pair = (products_dict[j[0]][0], j[1]) 
        new_ms.append(pair)
        
    return new_ms 

similar_products(model.wv['BLUE BELL ICE CREAM GOLD RIM'])
</code></pre>
<p>Results:</p>
<pre><code> [('BLUE BELL ICE CREAM BROWN RIM', 0.7322707772254944),
     ('BLUE BELL ICE CREAM LIGHT', 0.4575043022632599),
     ('BLUE BELL ICE CREAM NSA', 0.3731085956096649)]
</code></pre>
<p>To get intuitive understanding of word2vec and its working on how results are obtained, I created a dummy <a href=""https://docs.google.com/spreadsheets/d/16n2CbXiiZSgFy3wAroC4uV4QUdiS3T8-/edit?usp=sharing&amp;ouid=105487340749910932665&amp;rtpof=true&amp;sd=true"" rel=""nofollow noreferrer"">dataset</a> where I wanted to find subtitutes of <code>'FOODCLUB VAN IC PAIL'</code>. <br>
If two products are in the same basket multiple times then they are substitutes. <br>
Looking at the data first substitute should be <code>'FOODCLUB CHOC CHIP IC PAIL'</code> but the results I obtained are:</p>
<pre><code>[('FOODCLUB NEAPOLITAN IC PAIL', 0.042492810636758804),
 ('FOODCLUB COOKIES CREAM ICE CREAM', -0.04012278839945793),
 ('FOODCLUB NEW YORK VAN IC PAIL', -0.040678512305021286)]
</code></pre>
<ol>
<li>Can anyone help me understand the intuitive working of word2vec model in gensim? Will each product be treated as word and customer list as sentence?</li>
<li>Why are my results so absurd in dummy dataset? How can I improve?</li>
<li>What hyperparameters play a significant role w.r.t to this model? Is negative sampling required?</li>
</ol>
","python, gensim, word2vec, word-embedding, recommendation-engine","<p>You may not get a very good intuitive understanding of usual word2vec behavior using these sorts of product-baskets as training data. The algorithm was originally developed for natural-language texts, where texts are runs of tokens whose frequencies, &amp; co-occurrences, follow certain indicative patterns.</p>
<p>People certainly do use word2vec on runs-of-tokens that aren't natural language - like product baskets, or logs-of-actions, etc – but to the extent such tokens have very-different patterns, it's possible extra preprocessing or tuning will be necessary, or useful results will be harder to get.</p>
<p>As just a few ways customer-purchases might be different from real language, depending on what your &quot;pseudo-texts&quot; actually represent:</p>
<ul>
<li>the ordering within a text might be an artifact of how you created the data-dump rather than anything meaningful</li>
<li>the nearest-neighbors to each token within the <code>window</code> may or may not be significant, compared to more distant tokens</li>
<li>customer ordering patterns might in general not be as reflective of shades-of-relationships as words-in-natural-language text</li>
</ul>
<p>So it's not automatic that word2vec will give interesting results here, for recommendatinos.</p>
<p>That's especially the case with small datasets, or tiny dummy datasets. Word2vec requires <em>lots</em> of varied data to pack elements into interesting relative positions in a high-dimensional space. Even small demos usually have a vocabulary (count of unique tokens) of tens-of-thousands, with training texts that provide varied usage examples of every token dozens of times.</p>
<p>Without that, the model never learns anything interesing/generalizable. That's especially the case if trying to create a many-dimensions model (say the default <code>vector_size=100</code>) with a tiny vocabulary (just dozens of unique tokens) with few usage examples per example. And it only gets worse if tokens appear fewer than the default <code>min_count=5</code> times – when they're ignored entirely. So don't expect anything interesting to come from your dummy data, at all.</p>
<p>If you want to develop an intuition, I'd try some tutorials &amp; other goals with real natural language text 1st, with a variety of datasets &amp; parameters, to get a sense of what has what kind of effects on result usefulness – &amp; only after that try to adapt word2vec to other data.</p>
<p>Negative-sampling is the default, &amp; works well with typical datasets, especially as they grow large (where negative-sampling suffes less of a performance hit than hierarchical-softmax with large vocabularies). But a toggle between those two modes is unlike to cause giant changes in quality unless there are other problems.</p>
<p>Sufficient data, of the right kind, is the key – &amp; then tweaking parameters may nudge end-result usefulness in a better direction, or shift it to be better for certain purposes.</p>
<p>But more specific parameter tips are only possible with clearer goals, once some baseline is working.</p>
",2,0,449,2022-01-25 05:32:55,https://stackoverflow.com/questions/70843845/understanding-results-of-word2vec-gensim-for-finding-substitutes
Loading fasttext binary model from s3 fails,"<p>I am hosting a pretrained fasttext model on s3 (uncompressed) and I am trying to load it in a lambda function. I am using the <code>gensim.models.fasttext</code> module to load the model:</p>
<pre><code>from gensim.models.fasttext import load_facebook_vectors

def load_model(obj):
    model = load_facebook_vectors(obj[&quot;path&quot;])
</code></pre>
<p>with <code>obj[&quot;path&quot;]</code> being the s3 path, but I keep getting the following error:</p>
<pre><code>&quot;errorMessage&quot;: &quot;fileno&quot;
&quot;errorType&quot;: &quot;UnsupportedOperation&quot;
&quot;stackTrace&quot;: [
...
&quot;  File \&quot;/var/task/gensim/models/fasttext.py\&quot;, line 784, in load_facebook_vectors\n    full_model = _load_fasttext_format(path, encoding=encoding, full_model=False)\n&quot;
&quot;  File \&quot;/var/task/gensim/models/fasttext.py\&quot;, line 808, in _load_fasttext_format\n    m = gensim.models._fasttext_bin.load(fin, encoding=encoding, full_model=full_model)\n&quot;
&quot;  File \&quot;/var/task/gensim/models/_fasttext_bin.py\&quot;, line 348, in load\n    vectors_ngrams = _load_matrix(fin, new_format=new_format)\n&quot;
&quot;  File \&quot;/var/task/gensim/models/_fasttext_bin.py\&quot;, line 282, in _load_matrix\n    matrix = np.fromfile(fin, _FLOAT_DTYPE, count)\n&quot;
]
</code></pre>
","python, amazon-s3, aws-lambda, gensim, fasttext","<p>Unfortunately, the <code>np.fromfile()</code> method on which this load depends doesn't work on a streamed-from-S3 file.</p>
<p>Some alternate options include:</p>
<ul>
<li>download the S3 file to a local path first, then use <code>load_facebook_vectors()</code> from there; or…</li>
<li>while having the FastText file local, load it locally, then use Python's <code>pickle</code> functionality to save it to a single file (now of Python's format), then put that file on S3, and in the future re-load it using Python's unpickling</li>
</ul>
<p>The utility functions in <code>gensim.utils</code> <code>pickle()</code> and <code>unpickle()</code> (which take a file path, including S3 URLs) may be helpful for the 2nd option, eg:</p>
<p><a href=""https://radimrehurek.com/gensim/utils.html#gensim.utils.unpickle"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/utils.html#gensim.utils.unpickle</a></p>
<p>Since your prior code only shows using the vectors (via <code>.load_facebook_vector</code>), not the whole model, you could just pickle &amp; upload the <code>model.wv</code> subcomponent of the loaded model, rather than the whole model, to save some storage/bandwidth.</p>
<p>If perhaps in future Gensim versions, the <code>FastText</code>-model related classes change in shape/operation, an old pickled-model might not cleanly load. In such an eventuality, you could potentially either:</p>
<ul>
<li>go back to the original Facebook-format model file (which could then be loaded, &amp; then re-saved in a modern format, again); OR...</li>
<li>load your pickled model into the older Gensim where it works, save it locally using Gensim's native <code>.save()</code> (which may split it over multiple local files), then in the newer Gensim use Gensim's native <code>FastText.load()</code> to load those older files (which will usually handle older formats), then re-pickle that loaded model, for future re-unpickles into the matching latest Gensim.</li>
</ul>
",1,0,607,2022-01-27 15:30:49,https://stackoverflow.com/questions/70881262/loading-fasttext-binary-model-from-s3-fails
How to obtain a parameter &#39;total_words&#39; for model.train() of gensim&#39;s doc2vec,"<p>As you might know, when you make a doc2vec model, you might do <code>model.build_vocab(corpus_file='...')</code> first, then do <code>model.train(corpus_file='...', total_examples=..., total_words=..., epochs=10)</code>.</p>
<p>I am making the model w/ huge wikipedia data file. So, I have to designate the 'total_examples' and the 'total_words' for parameters of train(). Gensim's <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py"" rel=""nofollow noreferrer"">Tutorial</a> says that I can get the first one as <code>total_examples=model.corpus_count</code>. This is fine. But I don't know how to get second one, <code>total_words</code>. I can see the # of total words in the last log from model.build_vocab() as below. So, I directory put the number, like <code>total_words=1304592715</code>, but I'd like to designate it like model.corpus_count manner.
Can someone tell me how to obtain the number?
Thank you,</p>
<pre><code>:
2022-01-29 15:03:22,377 : INFO : PROGRESS: at example #1290000, processed 1253078267 words (6147969/s), 7881288 word types, 0 tags
2022-01-29 15:03:26,434 : INFO : PROGRESS: at example #1300000, processed 1277357579 words (5984975/s), 7959581 word types, 0 tags
2022-01-29 15:03:30,955 : INFO : collected 8039609 word types and 1309452 unique tags from a corpus of 1309452 examples and 1304592715 words
:
</code></pre>
","python, gensim, doc2vec","<p>Similar to <code>model.corpus_count</code>, the tally of words from the last corpus provided to <code>.build_vocab()</code> should be cached in the model as <code>model.corpus_total_words</code>.</p>
",1,0,122,2022-01-29 13:53:27,https://stackoverflow.com/questions/70906111/how-to-obtain-a-parameter-total-words-for-model-train-of-gensims-doc2vec
Use word2vec to expand a glossary in order to classify texts,"<p>I have a database containing about 3 million texts (tweets). I put clean texts (removing stop words, tags...) in a list of lists of tokens called <code>sentences</code> (so it contains a list of tokens for each text).</p>
<p>After these steps, if I write</p>
<p><code>model = Word2Vec(sentences, min_count=1)</code></p>
<p>I obtain a vocabulary of about 400,000 words.</p>
<p>I have also a list of words (belonging to the same topic, in this case: economics) called <code>terms</code>. I found that 7% of the texts contain at least one of these words (so we can say that 7% of total tweets talk about economics).</p>
<p>My goal is to expand the list <code>terms</code> in order to retrieve more texts belonging to the economic topic.</p>
<p>Then I use</p>
<p><code>results = model.most_similar(terms, topn=5000)</code></p>
<p>to find, within the list of lists of tokens <code>sentences</code>, the words most similar to those contained in <code>terms</code>.</p>
<p>Finally if I create the data frame</p>
<p><code>df = pd.DataFrame(results, columns=['key', 'similarity'])</code></p>
<p>I get something like that:</p>
<pre><code>key       similarity
word1     0.795432
word2     0.787954
word3     0.778942
...       ...
</code></pre>
<p>Now I think I have two possibilities to define the expanded glossary:</p>
<ul>
<li>I take the first N words (what should be the value of N?);</li>
<li>I look at the suggested words one by one and decide which one to include in the expanded glossary based on my knowledge (does this word really belong to the economic glossary?)</li>
</ul>
<p>How should I proceed in a case like this?</p>
","python, nlp, gensim, word2vec, word-embedding","<p>There's no general answer for what the cutoff should be, or how much you should use your own manual judgement versus cruder (but fast/automatic) processes. Those are inherently decisions which will be heavily influenced by your data, model quality, &amp; goals – so you have to try different approaches &amp; see what works there.</p>
<p>If you had a goal for what percentage of the original corpus you want to take – say, 14% instead of 7% – you could go as deeply into the ranked candidate list of 'similar words' as necessary to hit that 14% target.</p>
<p>Note that when you retrieve <code>model.most_similar(terms)</code>, you are asking the model to 1st average all words in <code>terms</code> together, then return words close to that one average point. To the extent your seed set of terms is tightly around the idea of <code>economics</code>, that might find words close to that generic average idea – but might not find other interesting words, such as close sysnonyms of your seed words that you just hadn't thought of. For that, you might want to get not 5000 neighbors for one generic average point, but (say) 3 neighbors for every individual term. To the extent the 'shape' of the topic isn't a perfect sphere around someplace in the word-vector-space, but rather some lumpy complex volume, that might better reflect your intent.</p>
<p>Instead of using your judgement of the candidate words standing alone to decide whether a word is <code>economics</code>-related, you could instead look at the texts that a word uniquely brings in. That is, for new word X, look at the N texts that contain that word. How many, when applying your full judgement to their full text, deserve to be in your 'economics' subset? Only if it's above some threshold T would you want to move X into your glossary.</p>
<p>But such an exercise may just highlight: using a simple glossary – &quot;for any of these hand-picked N words, every text mentioning at least 1 word is in&quot; – is a fairly crude way of assessing a text's topic. There are other ways to approach the goal of &quot;pick a relevant subset&quot; in an automated way.</p>
<p>For example, you could view your task as that of training a text binary classifier to classify texts as 'economics' or 'not-economics'.</p>
<p>In such a case, you'd start with some training data - a set of example documents that are already labeled 'economics' or 'not-economics', perhaps via individual manual review, or perhaps via some crude bootstrapping (like labeling all texts with some set of glossary words as 'economics', &amp; all others 'not-economics'). Then you'd draw from the full range of potential text-preprocessing, text-feature-extracton, &amp; classification options to train &amp; evaluate classifiers that make that judgement for you. Then you'd evaluate/tune those – a process wich might also improve your training data, as you add new definitively 'economics' or 'not-economics' texts – &amp; eventually settle on one that works well.</p>
<p>Alternatively, you could use some other richer topic-modeling methods (LDA, word2vec-derived <code>Doc2Vec</code>, deeper neural models etc) for modeling the whole dataset, then from some seed-set of definite-'economics' texts, expand outward from them – finding nearest-examples to known-good documents, either auto-including them or hand-reviewing them.</p>
<p>Separately: <code>min_count=1</code> is almost always a mistake in word2vec &amp; related algorihtms, which do better if you <em>discard</em> words so rare they lack the variety of multiple usage examples the algorithm needs to generate good word-vectors.</p>
",1,0,290,2022-01-30 14:57:09,https://stackoverflow.com/questions/70915829/use-word2vec-to-expand-a-glossary-in-order-to-classify-texts
No such file or directory: &#39;GoogleNews-vectors-negative300.bin&#39;,"<p>I have this code :</p>
<pre><code>import gensim
filename = 'GoogleNews-vectors-negative300.bin'
model = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)
</code></pre>
<p>and this is my folder organization thing :
<a href=""https://i.sstatic.net/KGd27.png"" rel=""nofollow noreferrer"">image of my folder tree that shows that the .bin file is in the same directory as the file calling it, the file being ai_functions</a></p>
<p>But sadly I'm not sure why I'm having an error saying that it can't find it. Btw I checked, I am sure the file is not corrupted. Any thoughts?</p>
<p>Full traceback :</p>
<pre><code>  File &quot;/Users/Ile-Maurice/Desktop/Flask/flaskapp/run.py&quot;, line 1, in &lt;module&gt;
    from serv import app
  File &quot;/Users/Ile-Maurice/Desktop/Flask/flaskapp/serv/__init__.py&quot;, line 13, in &lt;module&gt;
    from serv import routes
  File &quot;/Users/Ile-Maurice/Desktop/Flask/flaskapp/serv/routes.py&quot;, line 7, in &lt;module&gt;
    from serv.ai_functions import checkplagiarism
  File &quot;/Users/Ile-Maurice/Desktop/Flask/flaskapp/serv/ai_functions.py&quot;, line 31, in &lt;module&gt;
    model = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/models/keyedvectors.py&quot;, line 1629, in load_word2vec_format
    return _load_word2vec_format(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/models/keyedvectors.py&quot;, line 1955, in _load_word2vec_format
    with utils.open(fname, 'rb') as fin:
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/smart_open/smart_open_lib.py&quot;, line 188, in open
    fobj = _shortcut_open(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/smart_open/smart_open_lib.py&quot;, line 361, in _shortcut_open
    return _builtin_open(local_path, mode, buffering=buffering, **open_kwargs)
FileNotFoundError: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'
</code></pre>
","python, gensim","<p>The 'current working directory' that the Python process will consider active, and thus will use as the expected location for your plain relative filename <code>GoogleNews-vectors-negative300.bin</code>, will depend on how you launched Flask.</p>
<p>You could print out the directory to be sure – see some ways at <a href=""https://stackoverflow.com/questions/3718657/how-do-you-properly-determine-the-current-script-directory"">How do you properly determine the current script directory?</a> – but I suspect it may just be the <code>/Users/Ile-Maurice/Desktop/Flask/flaskapp/</code> directory.</p>
<p>If so, you could relatively-reference your file with the path relative to the above directory...</p>
<pre><code>serv/GoogleNews-vectors-negative300.bin
</code></pre>
<p>...or you could use a full 'absolute' path...</p>
<pre><code>/Users/Ile-Maurice/Desktop/Flask/flaskapp/serv/GoogleNews-vectors-negative300.bin
</code></pre>
<p>...or you could move the file up to its parent directory, so that it is alonside your Flask <code>run.py</code>.</p>
",1,3,1653,2022-02-03 15:07:27,https://stackoverflow.com/questions/70973660/no-such-file-or-directory-googlenews-vectors-negative300-bin
How to store the Phrase trigrams gensim model after training,"<p>I would like to know can I store the gensim Phrase model after training it on the sentences</p>
<pre><code>documents = [&quot;the mayor of new york was there&quot;, &quot;human computer interaction and 
machine learning has now become a trending research area&quot;,&quot;human computer interaction 
is interesting&quot;,&quot;human computer interaction is a pretty interesting subject&quot;, &quot;human 
computer interaction is a great and new subject&quot;, &quot;machine learning can be useful 
sometimes&quot;,&quot;new york mayor was present&quot;, &quot;I love machine learning because it is a new 
subject area&quot;, &quot;human computer interaction helps people to get user friendly 
applications&quot;]

sentences = [doc.split(&quot; &quot;) for doc in documents]

bigram_transformer = Phrases(sentences)
bigram_sentences = bigram_transformer[sentences]
print(&quot;Bigrams - done&quot;)
# Here we use a phrase model that detects the collocation of 3 words (trigrams).
trigram_transformer = Phrases(bigram_sentences)
trigram_sentences = trigram_transformer[bigram_sentences]
print(&quot;Trigrams - done&quot;)
</code></pre>
<p>How to store trigram_transformer physically  to reuse it again using pickle maybe?</p>
<p>Thank you in advance for your help.</p>
","python, nlp, gensim","<p>You can use Gensim's native <code>.save()</code> method:</p>
<pre class=""lang-py prettyprint-override""><code>trigram_transformer.save(TRIPHRASER_PATH)
</code></pre>
<p>...then reload similarly:</p>
<pre class=""lang-py prettyprint-override""><code>reloads_trigram_transformer = Phrases.load(TRIPHRASER_PATH)
</code></pre>
<p>(The Gensim save/load methods generally use Python pickling, but may for some models &amp; version-transitions handle some properties specially.)</p>
<p>You could also use Python's own pickle, which should work OK unless/until you try to load a too-old model into a newer version of Gensim that might have changed things about the <code>Phrases</code> model.</p>
",1,1,183,2022-02-03 18:35:52,https://stackoverflow.com/questions/70976566/how-to-store-the-phrase-trigrams-gensim-model-after-training
Using gensim most_similar function on a subset of total vocab,"<p>I am trying to use the gensim word2vec <code>most_similar</code> function in the following way:</p>
<pre><code>wv_from_bin.most_similar(positive=[&quot;word_a&quot;, &quot;word_b&quot;])
</code></pre>
<p>So basically, I multiple query words and I want to return the most similar outputs, but from a finite set. i.e. if vocab is 2000 words, then I want to return the most similar from a set of say 100 words, and not all 2000.</p>
<p>e.g.</p>
<pre><code>Vocab:
word_a, word_b, word_c, word_d, word_e ... words_z

Finite set:
word_d, word_e, word_f

</code></pre>
<p><em><strong>most_similar on whole vocab</strong></em></p>
<pre><code>wv_from_bin.most_similar(positive=[&quot;word_a&quot;, &quot;word_b&quot;])
output = ['word_d', 'word_f', 'word_g', 'word_x'...]
</code></pre>
<p><em><strong>desired output</strong></em></p>
<pre><code>finite_set = ['word_d', 'word_e', 'word_f']
wv_from_bin.most_similar(positive=[&quot;word_a&quot;, &quot;word_b&quot;], finite_set) &lt;-- some way of passing the finite set

output = ['word_d', 'word_f']
</code></pre>
","python, gensim, word2vec","<p>Depending on your specific patterns of use, you have a few options.</p>
<p>If you want to confine your results to a <em>contiguous range</em> of words in the <code>KeyedVectors</code> instance, a few optional parameters can help.</p>
<p>Most often, people want to confine results to the <em>most frequent</em> words. Those are generally those with the best-trained word-vectors. (When you get deep into less-frequent words, the few training examples tend to make their vectors somewhat more idiosyncratic – both from randomization that's part of the algorithm, and from any ways the limited number of examples don't reflect the word's &quot;true&quot; generalizable sense in the wider world.)</p>
<p>Using the optional parameter <code>restrict_vocab</code>, with an integer value N, will limit the results to just the first N words in the <code>KeyedVectors</code> (which by usual conventions are those that were most-frequent in the training data). So for example, adding <code>restrict_vocab=10000</code> to a call against a set-of-vectors with 50000 words will only retun the most-similar words from the 1st 10000 known words. Due to the effect mentioned above, these will often be the most reliable &amp; sensible results - while nearby words from the longer-tail of low-frequency words are more likely to seem a little out of place.</p>
<p>Similarly, instead of <code>restrict_vocab</code>, you can use the optional <code>clip_start</code> &amp; <code>clip_end</code> parameters to limit results to any other contiguous range. For example, adding <code>clip_start=100, clip_end=1000</code> to your <code>most_similar()</code> call will only return results from the 900 words in that range (leaving out the 100 most-common words in the usual case). I suppose that might be useful if you're finding the most-frequent words to be too generic – though I haven't noticed that being a typical problem.</p>
<p>Based on the way the underlying bulk-vector libraries work, both of the above options efficiently calculate <em>only</em> the needed similarities before sorting out the top-N, using native routines that might achieve nice parallelism without any extra effort.</p>
<p>If your words are a discontiguous mix throughout the whole <code>KeyedVectors</code>, there's no built-in support for limiting the results.</p>
<p>Two options you could consider include:</p>
<ul>
<li><p>Especially if you repeatedly search against the exact same subset of words, you could try creating a new <code>KeyedVectors</code> object with just those words - then every <code>most_similar()</code> against that separate set is just what you need. See the constructor &amp; <code>add_vector()</code> or <code>add_vectors()</code> methods in the <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer""><code>KeyedVectors</code> docs</a> for how that could be done.</p>
</li>
<li><p>Requesting a larger set of results, then filtering your desired subset. For example, if you supply <code>topn=len(wv_from_bin)</code>, you'll get back <em>every</em> word, ranked. You could then filter those down to only your desired subset. This does extra work, but that might not be a concern depending on your model size &amp; required throughput. For example:</p>
</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>finite_set = set(['word_d', 'word_e', 'word_f'])  # set for efficient 'in'
all_candidates = wv_from_bin.most_similar(positive=[&quot;word_a&quot;, &quot;word_b&quot;],
                                          topn=len(vw_from_bin))
filtered_results = [word_sim for word_sim in all_candidates if word_sim[0] in finite_set]
</code></pre>
<ul>
<li>You could save a <em>little</em> of the cost of the above by getting <em>all</em> the similarities, unsorted, using the <code>topn=None</code> option - but then you'd still have to subset those down to your words-of-interest, then sort yourself. But you'd still be paying the cost of all the vector-similarity calculations for all words, which in typical large-vocabularies is more of the runtime than the sort.</li>
</ul>
<p>If you were tempted to iterate over your subset &amp; calculate the similarities 1-by-1, be aware that can't take advantage of the math library's bulk vector operations – which use vector CPU operations on large ranges of the underlying data – so will usually be a lot slower.</p>
<p>Finally, as an aside: if your vocabulary is truly only ~2000 words, youre far from the bulk of data/words for which word2vec (and dense embedding word-vectors in general) usually shine. You may be disappointed in results unless you get a lot more data. (And in the meantime, such small vocabs may have problems effectively training typical word2vec dimensionalities (<code>vector_size</code>) of 100, 300, or more. (Using smaller <code>vector_size</code>, when you have a smaller vocab &amp; less training data, can help a bit.)</p>
<p>On the other hand, if you're in some domain other than real-language texts with an inherently limited unique vocabulary – like say category-tags or product-names or similar – and you have the chance to train your own word-vectors, you may want to try a wider range of training parameters than the usual defaults. Some recommendation-type apps may benefit from values very different from the <code>ns_exponent</code> default, &amp; if the source data's token-order is arbitrary, rather than meaningful, using a giant <code>window</code> or setting <code>shrink_windows=False</code> will deemphasize immediate-neighbors.</p>
",0,0,1161,2022-02-16 00:04:05,https://stackoverflow.com/questions/71134911/using-gensim-most-similar-function-on-a-subset-of-total-vocab
How to properly recover a Word2Vec model created using a SKLearn wrapper?,"<p>I am trying to create and store a gensin Word2Vec model using the <em>fit</em> function, then turn it into a SKLearn pipeline, pickle it, to later use it with <em>transform</em> on new data.</p>
<p>I created the wrapper, but the <em>self.w2v</em> object seems not to have been fitted and does not recognize any word. It is as if <em>self.w2v</em> had never seen <em>any</em> word.</p>
<p>Any ideas about how to address this?</p>
<pre><code>from sklearn.base import TransformerMixin, BaseEstimator
from gensim.models import Word2Vec

class SentenceVectorizer(TransformerMixin, BaseEstimator):

    def __init__(self, vector_size=50):
        self.vector_size = vector_size
   
    def sent_vectorizer(self, sentence, vectorizer):
        '''
        Applies the fitted W2V model for each token of each sentence and returns their vector representation.
        '''

        sent_vec =[]
        numw = 0

        for word in sentence:
            try:
                if numw == 0:
                    sent_vec = vectorizer.wv[word]       
                else:
                    sent_vec = np.add(sent_vec, vectorizer.wv[word])
                numw += 1

            except: # if word not present
                if numw == 0:
                    sent_vec = np.zeros(self.vector_size)
                else:
                    sent_vec = np.add(sent_vec, np.zeros(self.vector_size))

        if numw &gt; 0:
            return np.asarray(sent_vec) / numw
        else:
            return np.zeros(self.vector_size)

    def fit(self, X):
        self.w2v = Word2Vec(X, vector_size=self.vector_size)
        return self

    def transform(self, X):
        X_vec=[]
        for sentence in X:
            X_vec.append(self.sent_vectorizer(sentence, self.w2v))
        return X_vec
</code></pre>
<p>This code currently does well in training but returns zeroed vectors on inference (because no word has been recognized).</p>
<p>Most likely problem: fit method is not properly storing self.w2v, although when transform is called it seems to exist.</p>
","python, scikit-learn, gensim, word2vec","<p>Turns out I had an outdated gensim version which required vectorizer[word] instead of vectorizer.wv[word]. I'll leave the question here as it might be usefull to someone.</p>
",1,0,272,2022-02-17 18:50:05,https://stackoverflow.com/questions/71163820/how-to-properly-recover-a-word2vec-model-created-using-a-sklearn-wrapper
Replace objetive function in Word2Vec Gensim,"<p>I'm doing my final degree project. I need to create an extended version of the word2vec algorithm, changing the default objective function of the original paper. This has already been done (check this <a href=""https://www.frontiersin.org/articles/10.3389/fict.2018.00014/full"" rel=""nofollow noreferrer"">paper</a>). In that paper, they only say the new objective function, but they do not say how they have run the model.</p>
<p>Now, I need to extend that model too, with another function, but I'm not sure if I have to implement word2vec myself with the new function, or there is a way to replace it in the Gensim word2vec implementation.</p>
<p>I have checked the <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Word2Vec Gensim documentation</a> but I have not seen any parameter to do this. Do you have any idea how to do it? It is even possible?</p>
<p>I was unsure if this StackExchange site was the correct one, maybe <a href=""https://ai.stackexchange.com/"">https://ai.stackexchange.com/</a> is more appropriate.</p>
","python, gensim, word2vec","<p>There's no official support in Gensim for simply dropping in your own objective function.</p>
<p>However, the full source code is available – <a href=""https://github.com/RaRe-Technologies/gensim"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim</a> – so by editing it, or using it as a model for your own implementation, you could theoretically do anything.</p>
<p>Beware, though:</p>
<ul>
<li>the code has gone through a lot of optimization &amp; customization for new options that may not be relevant to your needs, so may not be the most clean &amp; simple starting point</li>
<li>for performance, the core routines are written in Cython – see the <code>.pyx</code> files – which can be especially hard to debug, and rely on library bulk array functions that may obscure how to implement your alternate function instead</li>
</ul>
",1,0,77,2022-02-19 07:55:38,https://stackoverflow.com/questions/71183157/replace-objetive-function-in-word2vec-gensim
Display document to topic mapping after LSI using Gensim,"<p>I am new to using LSI with Python and Gensim + Scikit-learn tools. I was able to achieve topic modeling on a corpus using LSI from both the Scikit-learn and Gensim libraries, however, when using the Gensim approach I was not able to display a list of documents to topic mapping.</p>
<p><strong>Here is my work using Scikit-learn LSI where I successfully displayed document to topic mapping:</strong></p>
<pre><code>tfidf_transformer = TfidfTransformer()
transformed_vector = tfidf_transformer.fit_transform(transformed_vector)
NUM_TOPICS = 14
lsi_model = TruncatedSVD(n_components=NUM_TOPICS)
lsi= nmf_model.fit_transform(transformed_vector)

topic_to_doc_mapping = {}
topic_list = []
topic_names = []

for i in range(len(dbpedia_df.index)):
    most_likely_topic =  nmf[i].argmax()

    if most_likely_topic not in topic_to_doc_mapping:
        topic_to_doc_mapping[most_likely_topic] = []

    topic_to_doc_mapping[most_likely_topic].append(i)

    topic_list.append(most_likely_topic)
    topic_names.append(topic_id_topic_mapping[most_likely_topic])

dbpedia_df['Most_Likely_Topic'] = topic_list
dbpedia_df['Most_Likely_Topic_Names'] = topic_names

print(topic_to_doc_mapping[0][:100])

topic_of_interest = 1
doc_ids = topic_to_doc_mapping[topic_of_interest][:4]
for doc_index in doc_ids:
    print(X.iloc[doc_index])
</code></pre>
<p><a href=""https://i.sstatic.net/XH703.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XH703.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>Using Gensim I was unable to proceed to display the document to topic mapping:</strong></p>
<pre><code>processed_list = []
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

for doc in documents_list:
    tokens = word_tokenize(doc.lower())
    stopped_tokens = [token for token in tokens if token not in stop_words]
    lemmatized_tokens = [lemmatizer.lemmatize(i, pos=&quot;n&quot;) for i in stopped_tokens]
    processed_list.append(lemmatized_tokens)
    
term_dictionary = Dictionary(processed_list)
document_term_matrix = [term_dictionary.doc2bow(document) for document in processed_list]

NUM_TOPICS = 14
model = LsiModel(corpus=document_term_matrix, num_topics=NUM_TOPICS, id2word=term_dictionary)
lsi_topics = model.show_topics(num_topics=NUM_TOPICS, formatted=False)
lsi_topics
</code></pre>
<p><strong>How can I display the document to topic mapping here?</strong></p>
","machine-learning, scikit-learn, gensim, topic-modeling, lsa","<p>In order to get the representation of a document (represented as a bag-of-words) from a trained <code>LsiModel</code> as a vector of topics, you use Python dict-style bracket-accessing (<code>model[bow]</code>).</p>
<p>For example, to get the topics for the 1st item in your training data, you can use:</p>
<pre><code>first_doc = document_term_matrix[0]
first_doc_lsi_topics = model[first_doc]
</code></pre>
<p>You can also supply a list of docs, as in training, to get the LSI topics for an entire batch at once. EG:</p>
<pre><code>all_doc_lsi_topics = model[document_term_matrix]
</code></pre>
",1,0,650,2022-02-22 08:29:33,https://stackoverflow.com/questions/71218086/display-document-to-topic-mapping-after-lsi-using-gensim
What is right way to sum up word2vec vectors generated by Gensim?,"<p>I got four 300-dimention word2vec vectors like:</p>
<pre><code>v1=model.wv.get_vector('A')
v2=model.wv.get_vector('B')
v3=model.wv.get_vector('C')
v4=model.wv.get_vector('D')
</code></pre>
<p>I want to compare cosine similarity of <code>v1+v2</code> and <code>v3+v4</code>.</p>
<p>Should I reduce them two 2-dimention vectors first or not?</p>
<p>What <code>numpy</code> function should I use?</p>
","python, gensim, word2vec","<p>You can add the vectors with simple Python math operators:</p>
<pre><code>va = v1 + v2
vb = v3 + v4
</code></pre>
<p><code>numpy</code> actually doesn't have a cosine-similarity (or cosine-distance) function, so you'd have to use the formula for calculating from the dot-product &amp; unit-norm (both of which <code>numpy</code> has:</p>
<pre><code>cossim = np.dot(va, vb) / (np.linalg.norm(va) * np.linalg.norm(vb))
</code></pre>
<p>Or, you could leverage the cosine-distance function in <code>scipy</code>, and convert it to cosine-similarity by subtracting it from 1:</p>
<pre><code>cosdist = scipy.spatial.distance.cosine(va, vb)
cossim = 1 - cosdist
</code></pre>
",0,0,384,2022-02-23 15:57:42,https://stackoverflow.com/questions/71240225/what-is-right-way-to-sum-up-word2vec-vectors-generated-by-gensim
Why Word2Vec function returns me a lot of 0.99 values,"<p>I'm trying to apply a word2vec model on a review dataset. First of all I apply the preprocessing to the dataset:</p>
<pre><code>df=df.text.apply(gensim.utils.simple_preprocess)
</code></pre>
<p>and this is the dataset that I get:</p>
<pre><code>0       [understand, location, low, score, look, mcdon...
3       [listen, it, morning, tired, maybe, hangry, ma...
6       [super, cool, bathroom, door, open, foot, nugg...
19      [cant, find, better, mcdonalds, know, getting,...
27      [night, went, mcdonalds, best, mcdonalds, expe...
                              ...
1677    [mcdonalds, app, order, arrived, line, drive, ...
1693    [correct, order, filled, promptly, expecting, ...
1694    [wow, fantastic, eatery, high, quality, ive, e...
1704    [let, tell, eat, lot, mcchickens, best, ive, m...
1716    [entertaining, staff, ive, come, mcdees, servi...
Name: text, Length: 283, dtype: object
</code></pre>
<p>Now I create the Word2Vec model and train it:</p>
<pre><code>model = gensim.models.Word2Vec(sentences=df, vector_size=200, window=10, min_count=1, workers=6)
model.train(df,total_examples=model.corpus_count,epochs=model.epochs)
print(model.wv.most_similar(&quot;service&quot;,topn=10))
</code></pre>
<p>What I dont understand is that the function most_similar() returns to me a lot of 0.99 of similarity.</p>
<pre><code>[('like', 0.9999310970306396), ('mcdonalds', 0.9999251961708069), ('food', 0.9999234080314636), ('order', 0.999918520450592), ('fries', 0.9999175667762756), ('got', 0.999911367893219), ('window', 0.9999082088470459), ('way', 0.9999075531959534), ('it', 0.9999069571495056), ('meal', 0.9999067783355713)]
</code></pre>
<p>What am I doing wrong?</p>
","python, gensim, word2vec","<p>You're right that's not normal.</p>
<p>It is unlikely that your <code>df</code> is the proper format <code>Word2Vec</code> expects. It needs a re-iterable Python sequence, where each item is a <em>list</em> of <em>string tokens</em>.</p>
<p>Try displaying <code>next(iter(df))</code>, to see the 1st item in <code>df</code>, if iterated over as <code>Word2Vec</code> does. Does it look like a good piece of training data?</p>
<p>Separately regarding your code:</p>
<ul>
<li><code>min_count=1</code> is always a bad idea with <code>Word2Vec</code> - rare words can't get good vectors but do, in aggregate, serve a lot like random noise making nearby words harder to train. Generally, the default <code>min_count=5</code> shouldn't be lowered unless you're sure that will help your results, because you can compare that value's effects versus lower values. And if it seems like too much of your vocabulary disappears because words don't appear even a measly 5 times, you likely have too little data for this data-hungry algorithm.</li>
<li>Only 283 texts are unlikely to be enough training data unless each text has tens of thousands of tokens. (And even if it were possible to squeeze some results from this far-smaller-than-ideal corpus, you might need to shrink the <code>vector_size</code> and/or increase the <code>epochs</code> to get the most out of minimal data.</li>
<li>If you supply a corpus to <code>sentences</code> in the <code>Word2Vec()</code> construction, you <em>don't</em> need to call <code>.train()</code>. It will have already automatically used that corpus fully as part of the constructor. (You only need to call the indepdendent, internal <code>.build_vocab()</code> &amp; <code>.train()</code> steps if you <em>didn't</em> supply a corpus at construction-time.)</li>
</ul>
<p>I highly recommend you enable logging to at least the <code>INFO</code> level for the relevant classes (either all Gensim or just <code>Word2Vec</code>). Then you'll see useful logging/progress info which, if you read over, will tend to reveal problems like the redundant second training here. (That redundant training <em>isn't</em> the cause of your main problem, though.)</p>
",1,0,536,2022-02-25 16:24:18,https://stackoverflow.com/questions/71268790/why-word2vec-function-returns-me-a-lot-of-0-99-values
Can I use a different corpus for fasttext build_vocab than train in Gensim Fasttext?,"<p>I am curious to know if there are any implications of using a different source while calling the <code>build_vocab</code> and <code>train</code> of Gensim <code>FastText</code> model. Will this impact the contextual representation of the word embedding?</p>
<p>My intention for doing this is that there is a specific set of words I am interested to get the vector representation for and when calling <code>model.wv.most_similar</code>. I only want words defined in this vocab list to get returned rather than all possible words in the training corpus. I would use the result of this to decide if I want to group those words to be relevant to each other based on similarity threshold.</p>
<p>Following is the code snippet that I am using, appreciate your thoughts if there are any concerns or implication with this approach.</p>
<ul>
<li>vocab.txt contains a list of unique words of interest</li>
<li>corpus.txt contains full conversation text (i.e. chat messages) where each line represents a paragraph/sentence per chat</li>
</ul>
<p>A follow up question to this is what values should I set for <code>total_examples</code> &amp; <code>total_words</code> during training in this case?</p>
<pre><code>from gensim.models.fasttext import FastText

model = FastText(min_count=1, vector_size=300,)

corpus_path = f'data/{client}-corpus.txt'
vocab_path = f'data/{client}-vocab.txt'
# Unsure if below counts should be based on the training corpus or vocab
corpus_count = get_lines_count(corpus_path)
total_words = get_words_count(corpus_path)

# build the vocabulary
model.build_vocab(corpus_file=vocab_path)

# train the model
model.train(corpus_file=corpus.corpus_path, epochs=100, 
    total_examples=corpus_count, total_words=total_words,
)

# save the model
model.save(f'models/gensim-fastext-model-{client}')
</code></pre>
","python, nlp, gensim, word-embedding, fasttext","<p>Incase someone has similar question, I'll paste the reply I got when asking this question in the <a href=""https://groups.google.com/d/msgid/gensim/e1330c28-e5df-4494-9bdb-bc1eaccd18dfn%40googlegroups.com?utm_medium=email&amp;utm_source=footer"" rel=""nofollow noreferrer"">Gensim Disussion Group</a> for reference:</p>
<blockquote>
<p>You can try it, but I wouldn't expect it to work well for most
purposes.</p>
<p>The <code>build_vocab()</code> call establishes the known vocabulary of the
model, &amp; caches some stats about the corpus.</p>
<p>If you then supply another corpus – &amp; especially one with <em>more</em> words
– then:</p>
<ul>
<li>You'll want your <code>train()</code> parameters to reflect the actual size of your training corpus. You'll want to provide a true <code>total_examples</code> and <code>total_words</code> count that are accurate for the training-corpus.</li>
<li>Every word in the training corpus that's not in the know vocabulary is ignored completely, as if it wasn't even there. So you might as
well filter your corpus down to just the words-of-interest first, then
use that same filtered corpus for both steps. Will the example texts
still make sense? Will that be enough data to train meaningful,
generalizable word-vectors for just the words-of-interest, alongside
other words-of-interest, without the full texts? (You could look at
your pref-filtered corpus to get a sense of that.) I'm not sure - it
could depend on how severely trimming to just the words-of-interest
changed the corpus. In particular, to train high-dimensional dense
vectors – as with <code>vector_size=300</code> – you need a lot of varied data.
Such pre-trimming might thin the corpus so much as to make the
word-vectors for the words-of-interest far less useful.</li>
</ul>
<p>You could certainly try it both ways – pre-filtered to just your
words-of-interest, or with the full original corpus – and see which
works better on downstream evaluations.</p>
<p>More generally, if the concern is training time with the full corpus,
there are likely other ways to get an adequate model in an acceptable
amount of time.</p>
<p>If using <code>corpus_file</code> mode, you can increase <code>workers</code> to equal the
local CPU core count for a nearly-linear speedup from number of cores.
(In traditional <code>corpus_iterable</code> mode, max throughput is usually
somewhere in the 6-12 <code>workers</code> threads, as long as you ahve that many
cores.)</p>
<p><code>min_count=1</code> is usually a bad idea for these algorithms: they tend to
train faster, in less memory, leaving better vectors for the remaining
words when you discard the lowest-frequency words, as the default
<code>min_count=5</code> does. (It's possible <code>FastText</code> can eke a little bit of
benefit out of lower-frequency words via their contribution to
character-n-gram-training, but I'd only ever lower the default
<code>min_count</code> if I could confirm it was actually improving relevant
results.</p>
<p>If your corpus is so large that training time is a concern, often a
more-aggressive (smaller) <code>sample</code> parameter value not only speeds
training (by dropping many redundant high-frequency words), but ofthen
improves final word-vector quality for downstream purposes as well (by
letting the rarer words have relatively more influence on the model in
the absense of the downsampled words).</p>
<p>And again if the corpus is so large that training time is a concern,
than <code>epochs=100</code> is likely overkill. I believe the <code>GoogleNews</code>
vectors were trained using only 3 passes – over a gigantic corpus. A
sufficiently large &amp; varied corpus, with plenty of examples of all
words all throughout, could potentially train in 1 pass – because each
word-vector can then get more total training-updates than many epochs
with a small corpus. (In general larger <code>epochs</code> values are more often
used when the corpus is thin, to eke out something – not on a corpus
so large you're considering non-standard shortcuts to speed the
steps.)</p>
<p>-- Gordon</p>
</blockquote>
",1,2,425,2022-02-28 01:06:02,https://stackoverflow.com/questions/71289683/can-i-use-a-different-corpus-for-fasttext-build-vocab-than-train-in-gensim-fastt
python pip: &quot;error: legacy-install-failure&quot;,"<p>I want to install <code>gensim</code> python package via <code>pip install gensim</code></p>
<p>But this error occurs and I have no idea what should I do to solve it.</p>
<pre><code>      running build_ext
      building 'gensim.models.word2vec_inner' extension
      error: Microsoft Visual C++ 14.0 or greater is required. Get it with &quot;Microsoft C++ Build Tools&quot;: https://visualstudio.microsoft.com/visual-cpp-build-tools/
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: legacy-install-failure

× Encountered error while trying to install package.
╰─&gt; gensim

note: This is an issue with the package mentioned above, not pip.
hint: See above for output from the failure.
</code></pre>
","python, pip, gensim","<p>If you fail to install plugins,<br />
you can download it from other repositories like this one:
<a href=""https://www.lfd.uci.edu/%7Egohlke/pythonlibs/#gensim"" rel=""noreferrer"">repository</a> depends on the version of python and the system.</p>
<p>for example: for  windows 11(x64) and python 3.10 you should take this file: <em><strong>gensim‑4.1.2‑cp310‑cp310‑win_amd64.whl</strong></em></p>
",11,31,157768,2022-02-28 13:37:56,https://stackoverflow.com/questions/71295840/python-pip-error-legacy-install-failure
Gensim unable to load word2vec models,"<p>I trained a word2vec model using gensim on my local machine and uploaded all files to AWS. I am able to load the model on my local machine but loading on AWS gives</p>
<blockquote>
<p>FileNotFoundError: [Errno 2] No such file or directory:
's3://saltsagemaker/models/bilstm_models/word2vec/word2vec_model.wv.vectors.npy'</p>
</blockquote>
<p>This works 👇🏻</p>
<pre><code># LOCAL MACHINE
from gensim.models import Phrases, Word2Vec
WV_MODEL = 'model_train_script/models/bilstm_models/word2vec/word2vec_model'
wv_model = Word2Vec.load(WV_MODEL)
</code></pre>
<p>This doesnt works 👇🏻</p>
<pre><code># AWS CODE
from gensim.models import Phrases, Word2Vec
WV_MODEL = 's3://saltsagemaker/models/bilstm_models/word2vec/word2vec_model'
wv_model = Word2Vec.load(WV_MODEL)
</code></pre>
<p>above code gives the following</p>
<blockquote>
<p>FileNotFoundError: [Errno 2] No such file or directory:
's3://saltsagemaker/models/bilstm_models/word2vec/word2vec_model.wv.vectors.npy'</p>
</blockquote>
<p>Files uploaded on local machine
<a href=""https://i.sstatic.net/Jnmr5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Jnmr5.png"" alt=""enter image description here"" /></a></p>
<p>Files uploaded on AWS
<a href=""https://i.sstatic.net/xfUtl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xfUtl.png"" alt=""enter image description here"" /></a></p>
","python, gensim","<p>The <code>numpy</code> code on which Gensim relies to load subsidiary files (like your <code>word2vec_model.wv.vectors.npy</code>) doesn't support remote S3 paths like <code>'s3://saltsagemaker/models/bilstm_models/word2vec/word2vec_model.wv.vectors.npy'</code>. So, it's interpreting it as if it were a local path – &amp; finding nothing.</p>
<p>You could:</p>
<ul>
<li>download the files manually, to a local temporary space, before use; or</li>
<li>avoid using the Gensim object's custom <code>.save()</code> &amp; <code>.load()</code> methods at all, &amp; instead use Python's <code>pickle</code> functionality to write the model object to a single file, then use <code>unpickle</code> on that single file to read it back in from a single S3 path</li>
</ul>
",1,0,1837,2022-03-03 15:20:58,https://stackoverflow.com/questions/71339571/gensim-unable-to-load-word2vec-models
Retrieve n-grams with word2vec,"<p>I have a list of texts. I turn each text into a token list. For example if one of the texts is <code>'I am studying word2vec'</code> the respective token list will be (assuming I consider n-grams with n = 1, 2, 3) <code>['I', 'am', 'studying ', 'word2vec, 'I am', 'am studying', 'studying word2vec', 'I am studying', 'am studying word2vec']</code>.</p>
<ol>
<li>Is this the right way to transform any text in order to apply <code>most_similar()</code>?</li>
</ol>
<p>(I could also delete n-grams that contain at least one stopword, but that's not the point of my question.)</p>
<p>I call this list of lists of tokens <code>texts</code>. Now I build the model:</p>
<p><code>model = Word2Vec(texts)</code></p>
<p>then, if I use</p>
<p><code>words = model.most_similar('term', topn=5)</code></p>
<ol start=""2"">
<li>Is there a way to determine what kind of results i will get? For example, if <code>term</code> is a 1-gram then will I get a list of five 1-gram? If <code>term</code> is a 2-gram then will I get a list of five 2-gram?</li>
</ol>
","python, gensim, word2vec","<p>Generally, the very best way to determine &quot;what kinds of results&quot; you will get if you were to try certain things is to try those things, and observe the results you actually get.</p>
<p>In preparing text for word2vec training, it is not typical to convert an input text to the form you've shown, with a bunch of space-delimited word n-grams added. Rather, the string <code>'I am studying word2vec'</code> would typically just be preprocessed/tokenized to a list of (unigram) tokens like <code>['I', 'am', 'studying', 'word2vec']</code>.</p>
<p>The model will then learn one vector per single word – with no vectors for multigrams. And since it only knows such 1-word vectors, all the results its reports from <code>.most_similar()</code> will also be single words.</p>
<p>You can preprocess your text to combine some words into multiword entities, based on some sort of statistical or semantic understanding of the text. Very often, this process converts the runs-of-related-words to underscore-connected single tokens. For example, <code>'I visited New York City'</code> might become <code>['I', 'visited', 'New_York_City']</code>.</p>
<p>But any such preprocessing decisions are separate from the word2vec algorithm itself, which just considers whatever 'words' you feed it as 1:1 keys for looking-up vectors-in-training. It only knows tokens, not n-grams.</p>
",1,0,1254,2022-03-07 17:02:33,https://stackoverflow.com/questions/71384680/retrieve-n-grams-with-word2vec
Gensim phrases model vocabulary length does not correspond to amount of iteratively added documents,"<p>I iteratively apply the...</p>
<pre><code>bigram.add_vocab(&lt;List of List with Tokens&gt;)
</code></pre>
<p>method in order to update a...</p>
<pre><code>bigram = gensim.models.phrases.Phrases(min_count=bigramMinFreq, threshold=10.0)
</code></pre>
<p>Gensim phrases model. With each iteration up to ~10'000 documents are added. Therefore my intuition is that the Phrases model grows with each added document set. I check this intuition by checking the length of the bigram vocabulary with...</p>
<pre><code>len(bigram.vocab))
</code></pre>
<p>Furthermore I also check the amount of phrasegrams in the freezed Phrase model with...</p>
<pre><code>bigram_freezed = bigram.freeze()
len(bigram_freezed.phrasegrams)
</code></pre>
<p>A resulting output looks as follows:</p>
<pre><code>Data of directory:  000  is loaded
Num of Docs: 97802
Updated Bigram Vocab is:  31819758
Amount of phrasegrams in freezed bigram model:  397554
-------------------------------------------------------
Data of directory:  001  
Num of Docs: 93368
Updated Bigram Vocab is:  17940420
Amount of phrasegrams in freezed bigram model:  429162
-------------------------------------------------------
Data of directory:  002  
Num of Docs: 87265
Updated Bigram Vocab is:  36120292
Amount of phrasegrams in freezed bigram model:  661023
-------------------------------------------------------
Data of directory:  003
Num of Docs: 55852
Updated Bigram Vocab is:  20330876
Amount of phrasegrams in freezed bigram model:  604504
-------------------------------------------------------
Data of directory:  004
Num of Docs: 49390
Updated Bigram Vocab is:  31101880
Amount of phrasegrams in freezed bigram model:  745827
-------------------------------------------------------
Data of directory:  005
Num of Docs: 56258
Updated Bigram Vocab is:  19236483
Amount of phrasegrams in freezed bigram model:  675705
-------------------------------------------------------
...
</code></pre>
<p>As can be seen neither the bigram vocab count nor the phrasegram count of the freezed bigram model is continuously increasing. I expected both counts to increase with added documents.</p>
<p>Do I not understand what <strong>phrase.vocab</strong> and <strong>phraser.phrasegrams</strong> are referring to? (if needed I can add the whole corrsponding Jupyter Notebook cell)</p>
","python, nlp, gensim, phrase","<p>By default, to avoid using an unbounded amount of RAM, the Gensim <code>Phrases</code> class uses a default parameter <code>max_vocab_size=40000000</code>, per the source code &amp; docs at:</p>
<p><a href=""https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases</a></p>
<p>Unfortunately, the mechanism behind this cap is very crude &amp; non-intuitive. Whenever the tally of all known keys in they survey-dict (which includes both unigrams &amp; bigrams) hits this threshold (default 40,000,000), a <code>prune</code> operation is performed that discards <em>all</em> token counts (unigrams &amp; bigrams) at low-frequencies until the total unique-keys is under the threshold. And, it sets the low-frequency floor for future prunes to be at least as high as was necessary for this prune.</p>
<p>For example, the 1st time this is hit, it might need to discard all the 1-count tokens. And due to the typical Zipfian distribution of word-frequencies, that step along might not just get the total count of known tokens slightly under the threshold, but <em>massively</em> under the threshold. And, any subsequent prune will start by eliminated <em>at least</em> everything with fewer than <em>2</em> occurrences.</p>
<p>This results in the sawtooth counts you're seeing. When the model can't fit in <code>max_vocab_size</code>, it overshrinks. It may do this many times in the course of processing a very-large corpus. As a result, final counts of lower-frequency words/bigrams can also be serious undercounts - depending somewhat arbitrarily on whether a key's counts survived the various prune-thresholds. (That's also influenced by where in the corpus a token appears. A token that only appears in the corpus <em>after</em> the last prune will still have a precise count, even if it only appears once! Although rare tokens that appeared any number of times could be severely undercounted, if they were always below the cutoff at each prior prune.)</p>
<p>The best solution would be to use a precise count that uses/correlates some spillover storage on-disk, to only prune (if at all) at the very end, ensuring only the truly-least-frequent keys are discarded. Unfortunately, Gensim's never implemented that option.</p>
<p>The next-best, for many cases, could be to use a memory-efficient approximate counting algorithm, that vaguely maintains the right magnitudes of counts for a much-larger number of keys. There's been a litte work in Gensim on this in the past, but not yet integrated with the <code>Phrases</code> functionality.</p>
<p>That leaves you with the only practical workaround in the short term: change the <code>max_vocab_size</code> parameter to be larger.</p>
<p>You could try setting it to <code>math.inf</code> (might risk lower performance due to int-vs-float comparisons) or <code>sys.maxsize</code> – essentially turning off the pruning entirely, to see if your survey can complete without exhausting your RAM. But, you might run out of memory anyway.</p>
<p>You could also try a larger-but-not-essentially-infinite cap – whatever fits in your RAM – so that far less pruning is done. But you'll still see the non-intuitive decreases in total counts, sometimes, if in fact the threshold is ever enforced. Per the docs, a <em>very rough</em> (perhaps outdated) estimate is that the default <code>max_vocab_size=40000000</code> consumes about 3.6GB at peak saturation. So if you've got a 64GB machine, you could possibly try a <code>max_vocab_size</code> thats 10-14x larger than the default, etc.</p>
",1,1,290,2022-03-13 13:17:53,https://stackoverflow.com/questions/71457117/gensim-phrases-model-vocabulary-length-does-not-correspond-to-amount-of-iterativ
KeyedVectors\&#39; object has no attribute \&#39;wv for gensim 4.1.2,"<p>i have migrated from gensim 3.8.3 to 4.1.2 and i am using this</p>
<p><code>claim = [token for token in claim_text if token in w2v_model.wv.vocab]</code></p>
<p><code>reference = [token for token in ref_text if token in w2v_model.wv.vocab]</code></p>
<p>i am not sure how to replace w2v_model.wv.vocab to newer attribute and i am getting this error</p>
<p><strong>KeyedVectors' object has no attribute 'wv'</strong> can anyone please help.</p>
","python, machine-learning, artificial-intelligence, gensim","<p>You only use the <code>.wv</code> property to fetch the <code>KeyedVectors</code> object from another more complete algorithmic model, like a full <code>Word2Vec</code> model (which contains a <code>KeyedVectors</code> in its <code>.wv</code> attribute).</p>
<p>If you're already working with just-the-vectors, there's no need to request the word-vectors subcomponent. Whatever you were going to do, you just do to the <code>KeyedVectors</code> directly.</p>
<p>However, you're also using the <code>.vocab</code> attribute, which has been replaced. See the migration FAQ for more details:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes"" rel=""noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes</a></p>
<p>(Mainly: instead of doing an <code>in w2v_model.wv.vocab</code>, you may only need to do <code>in kv_model</code> or <code>in kv_model.key_to_index</code>.)</p>
",6,4,12207,2022-03-20 07:09:50,https://stackoverflow.com/questions/71544767/keyedvectors-object-has-no-attribute-wv-for-gensim-4-1-2
word not in vocabulary error in gensim model,"<pre><code>from bs4 import BeautifulSoup
import requests
cont = requests.get(&quot;https://ichi.pro/tr/veri-biliminde-uzaklik-olculeri-159983401462266&quot;).content
soup = BeautifulSoup(cont,&quot;html.parser&quot;)
metin = soup.text

import re
sonuç1 = re.search(&quot;1. Öklid Mesafesi&quot;,metin)
sonuç2 = re.search('Okuduğunuz için teşekkürler!',metin)
metin = metin[sonuç1.start():sonuç2.start()].split(&quot;\n&quot;)

from gensim.models import Word2Vec
model = Word2Vec(metin,size=200,window=15,min_count=5,sg=5)

model.wv[&quot;Jaccard mesafesi&quot;]
</code></pre>
<p>metin is:</p>
<pre><code>....'     Jaccard mesafesi',
 '    ',
 '',
 'Dezavantajları',
 'Jaccard endeksinin önemli bir dezavantajı, verilerin büyüklüğünden oldukça etkilenmesidir. Büyük veri kümelerinin endeks üzerinde büyük bir etkisi olabilir, çünkü kesişme noktasını benzer tutarken birleşmeyi önemli ölçüde artırabilir.',
 'Kullanım Durumları',
 'Jaccard indeksi, genellikle ikili veya ikili verilerin kullanıldığı uygulamalarda kullanılır. Bir görüntünün segmentlerini, örneğin bir arabayı tahmin eden bir derin öğrenme modeliniz olduğunda, Jaccard indeksi daha sonra, doğru etiketler verilen tahmin edilen segmentin ne kadar doğru olduğunu hesaplamak için kullanılabilir.',
 'Benzer şekilde, belgeler arasında ne kadar kelime seçiminin örtüştüğünü ölçmek için metin benzerlik analizinde kullanılabilir. Böylece, desen setlerini karşılaştırmak için kullanılabilir.',
 '8. Haversine',
 '',
 '',
 '',
 '     Haversine mesafesi. Yazar tarafından görüntü.',
 '    ',
....
</code></pre>
<p>note: ı am turkish so my content is turkish but this not important ı think if you are stranger this is not problem
second note:ı try another words but ı can not train the model?
what should ı do?</p>
","web-scraping, nlp, nltk, gensim, word2vec","<p>There are multiple problems:</p>
<ol>
<li><p>If you want a Turkish model, you can try to find a pretrained Word2Vec model for Turkish (e.g. check out <a href=""https://github.com/akoksal/Turkish-Word2Vec"" rel=""nofollow noreferrer"">this repository</a>) or train a model for Turkish yourself. The way you use it now you seem to train a model but only from a single website, which will barely do anything because the model needs a large amount of sentences to learn anything (like at least 10.000, better much more). Also you set <code>min_count=5</code> anyway, so any word appearing less than 5 times is ignored generally. Try something like training it on the Turkish Wikipedia, see the linked repository.</p>
</li>
<li><p>Word2Vec by default is a unigram model, so the input is a <em>single</em> word. If you hand it a bigram consisting of two words like <code>&quot;Jaccard mesafesi&quot;</code> it will not find anything. Also you should catch the case that the word is not in vocabulary, otherwise each unknown word will cause an error and your program to cancel. Search for the unigram representation of each token, then combine the two, e.g. by using the statistical mean of the vectors:</p>
<pre><code>import numpy

ngram = &quot;Jaccard mesafesi&quot;
split_ngram = ngram.split()
try:
    ngram_vector = []
    for w in split_ngram:
        ngram_vector.append(model.wv[w])
    ngram_vector = numpy.mean(unigram_vectors, axis=0)
except:
    ngram_vector = None
    print(f&quot;Word {word} is not in vocabulary&quot;)
</code></pre>
</li>
<li><p>The Word2Vec class for training takes as argument a list of <code>tokenized</code> sentences, so a list word lists. You handed it entire, untokenized sentences instead.</p>
</li>
</ol>
",1,0,157,2022-03-21 09:05:07,https://stackoverflow.com/questions/71555062/word-not-in-vocabulary-error-in-gensim-model
&#39;utf-8&#39; codec can&#39;t decode byte 0x93 in position 0: invalid start byte,"<p>I want to use Word2Vec, and i have download a Word2Vec's corpus in indonesian language, but when i call it, it was give me an error, this is what i try :</p>
<pre><code>Model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Feature Extraction Lexicon Based/Word2Vec/idwiki_word2vec_100_new_lower.model.wv.vectors.npy', binary=True,)
</code></pre>
<p>and it was give me an error, like this :</p>
<pre><code>---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-73-219e152ee7d9&gt; in &lt;module&gt;()
----&gt; 1 Model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Feature Extraction Lexicon Based/Word2Vec/idwiki_word2vec_100_new_lower.model.wv.vectors.npy', binary=True,)

2 frames
/usr/local/lib/python3.7/dist-packages/gensim/utils.py in any2unicode(text, encoding, errors)
    353     if isinstance(text, unicode):
    354         return text
--&gt; 355     return unicode(text, encoding, errors=errors)
    356 
    357 

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x93 in position 0: invalid start byte
</code></pre>
","utf-8, gensim, word2vec","<p>A file named <code>idwiki_word2vec_100_new_lower.model.wv.vectors.npy</code> is unlikely to be in the format needed by <code>load_word2vec_format()</code>.</p>
<p>The <code>.npy</code> suggests it is a raw <code>numpy</code> array, which is not the format expected.</p>
<p>Also, the <code>.wv.vectors.</code> section suggests this could be part of a full, multi-file Gensim <code>.save()</code> of a complete <code>Word2Vec</code> model. That's more than just the vectors, &amp; requires all associated files to re-load.</p>
<p>You should double-check the source of the vectors and what their claims are about its format and the proper ways to load. (If you're still having problems &amp; need more guidance, you should specify more details about the origin of the file – for example a link to the website where it was obtained – to support other suggestions.)</p>
",0,0,2499,2022-03-23 01:50:19,https://stackoverflow.com/questions/71580934/utf-8-codec-cant-decode-byte-0x93-in-position-0-invalid-start-byte
What is gensim&#39;s simple_preprocess alternative in Scikit learn,"<p>Using Gensim to perform LDA, I was able to do initial text preprocessing and cleanup using:</p>
<pre><code>gensim.utils.simple_preprocess(str(sentence),deacc=True)
</code></pre>
<p>It was very efficient and almost does all required forms of text cleanup in one command. Now, I am trying to learn LDA using Scikit learn and I was wondering if there is a similar way to achieve the same preprocessing using Sci-kit learn, instead of having to load both libraries.</p>
","scikit-learn, nlp, gensim","<p>I don't think <code>scikit-learn</code> provides a similar utility function, but the whole logic of what Gensim's <code>simple_preprocess()</code> is doing is only about 20 lines of source code, spread across 5 functions:</p>
<ol>
<li><a href=""https://github.com/RaRe-Technologies/gensim/blob/0bd1d0651f7a52e1b6bb38529161e395986a803e/gensim/utils.py#L288"" rel=""nofollow noreferrer""><code>simple_preprocess()</code></a>, which relies on...</li>
<li><a href=""https://github.com/RaRe-Technologies/gensim/blob/0bd1d0651f7a52e1b6bb38529161e395986a803e/gensim/utils.py#L227"" rel=""nofollow noreferrer""><code>tokenize()</code></a>, which relies on...</li>
<li><a href=""https://github.com/RaRe-Technologies/gensim/blob/0bd1d0651f7a52e1b6bb38529161e395986a803e/gensim/utils.py#L345"" rel=""nofollow noreferrer""><code>to_unicode()</code></a>, <a href=""https://github.com/RaRe-Technologies/gensim/blob/0bd1d0651f7a52e1b6bb38529161e395986a803e/gensim/utils.py#L174"" rel=""nofollow noreferrer""><code>deaccent()</code></a>, &amp; <a href=""https://github.com/RaRe-Technologies/gensim/blob/0bd1d0651f7a52e1b6bb38529161e395986a803e/gensim/utils.py#L270"" rel=""nofollow noreferrer""><code>simple_tokenize()</code></a></li>
</ol>
<p>So if you wanted the same behavior, without installing Gensim, you have the option to just copy &amp; paste (or otherwise lightly adapt) that source code.</p>
",0,0,383,2022-03-27 08:09:40,https://stackoverflow.com/questions/71634668/what-is-gensims-simple-preprocess-alternative-in-scikit-learn
How to get the dimensions of a word2vec vector?,"<p>I have run a word2vec model on my data <code>list_of_sentence</code>:</p>
<pre><code>from gensim.models import Word2Vec

w2v_model=Word2Vec(list_of_sentence,min_count=5, workers=4)

print(type(w2v_model))

&lt;class 'gensim.models.word2vec.Word2Vec'&gt;
</code></pre>
<p>I would like to know the dimensionality of <code>w2v_model</code> vectors. How can I check it?</p>
","python, machine-learning, nlp, gensim, word2vec","<p>The vector dimensionality is included as an argument in <code>Word2Vec</code>:</p>
<ul>
<li>In gensim versions up to 3.8.3, the argument was called <code>size</code> (<a href=""https://radimrehurek.com/gensim_3.8.3/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">docs</a>)</li>
<li>In the latest gensim versions (4.0 onwards), the relevant argument is renamed to <code>vector_size</code> (<a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">docs</a>)</li>
</ul>
<p>In both cases, the argument has a default value of <strong>100</strong>; this means that, if you do not specify it explicitly (as you do here), the dimensionality will be 100.</p>
<p>Here is a reproducible example using gensim 3.6:</p>
<pre><code>import gensim
gensim.__version__
# 3.6.0

from gensim.test.utils import common_texts
from gensim.models import Word2Vec

model = Word2Vec(sentences=common_texts, window=5, min_count=1, workers=4) # do not specify size, leave the default 100

wv = model.wv['computer']  # get numpy vector of a word in the corpus
wv.shape # verify the dimension of a single vector is 100
# (100,)
</code></pre>
<p>If you want to change this dimensionality to, say, 256, you should call <code>Word2Vec</code> with the argument <code>size=256</code> (for gensim versions up to 3.8.3) or <code>vector_size=256</code> (for gensim versions 4.0 or later).</p>
",1,1,2010,2022-04-08 06:53:43,https://stackoverflow.com/questions/71792841/how-to-get-the-dimensions-of-a-word2vec-vector
Extract meaningful words from spaceless texts,"<p>I have not done much NLP, but have a need. For example for the string 'australiafreedomrally', I need to automatically extract meaningful words i.e., 'australia', 'freedom' and 'rally'.</p>
<p>Is there any python package that can do it? Thanks</p>
","nlp, nltk, stanford-nlp, spacy, gensim","<p>Check out <a href=""https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words"">this thread</a>, where among other things a package is mentioned which does this. Generally an approach with a predefined list of common words can get you far. Your question has an overlap with the task of <a href=""https://sites.google.com/view/icdar2019-postcorrectionocr"" rel=""nofollow noreferrer"">Optical Character Recognition (OCR) Post Correction</a> which you can find some pretrained models for, although the problem being that strongly shifted towards one issue (missing whitespace character) probably leads to it not performing too great.</p>
<p>If you want to really get into this topic you could try to train a new model on this task, I can imagine that recent popular transformer models which use subtoken-level embeddings for unknown words could be trained to bring a decent performance on this task since there are models which go into a similar direction as <a href=""https://huggingface.co/vennify/t5-base-grammar-correction"" rel=""nofollow noreferrer"">grammar correction</a> and <a href=""https://huggingface.co/flexudy/t5-base-multi-sentence-doctor"" rel=""nofollow noreferrer"">sentence boundary correction</a>. There are also some older, rule-based approach papers which call this problem &quot;word boundary detection&quot; or more specifcally &quot;agglutination&quot;, check out e.g. <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351975/"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351975/</a>, but generally the amount of off-the-shelf solutions you find for that problems is quite low.</p>
",2,0,676,2022-04-08 18:24:55,https://stackoverflow.com/questions/71801656/extract-meaningful-words-from-spaceless-texts
How do I use gensim to vectorize these words in my dataframe so I can perform clustering on them?,"<p>I am trying to do a clustering analysis (preferably k-means) of poetry words on a pandas dataframe. I am firstly trying to vectorize the words by using the word-to-vector feature in the gensim package. However, the vectors just come out with 0s, so my code is failing to translate the words into vectors. As a result, the clustering doesn't work. Here is my code:</p>
<pre><code># create a gensim model 
model = gensim.models.Word2Vec(vector_size=100) 
# copy original pandas dataframe with poems
data = poems.copy(deep=True)
# get data ready for kmeans clustering
final_data = [] # empty list 
for i, row in data.iterrows(): 
    poem_vectorized = [] 
    poem = row['Main_text']
    poem_all_words = poem.split(sep=&quot; &quot;)
    for poem_w in poem_all_words: #iterate through list of words 
        try:
            poem_vectorized.append(list(model.wv[poem_w]))
        except Exception as e:
            pass
    try:
        poem_vectorized = np.asarray(poem_vectorized)
        poem_vectorized_mean = list(np.mean(poem_vectorized, axis=0))
    except Exception as e:
        poem_vectorized_mean = list(np.zeros(100))
        pass
    try:
        len(poem_vectorized_mean)
    except:
        poem_vectorized_mean = list(np.zeros(100))
    temp_row = np.asarray(poem_vectorized_mean)
    final_data.append(temp_row)
X = np.asarray(final_data)
print(X)
</code></pre>
<p><a href=""https://i.sstatic.net/jUSKd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jUSKd.png"" alt=""Output"" /></a></p>
<p>At closer inspection of:</p>
<pre><code>poem_vectorized.append(list(model.wv[poem_w]))
</code></pre>
<p>the problem seems to be this:
<a href=""https://i.sstatic.net/0cgkP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0cgkP.png"" alt=""syntax error"" /></a></p>
","python, nlp, cluster-analysis, gensim","<p>If I understand it correctly you want to use an <strong>existing</strong> model to get the semantic embeddings of the tokens and then cluster the words, right?</p>
<p>Because the way you set the model up you are preparing a new model for training, but then don't feed any training data to it and train it, so your model doesn't know any words and just always throws a KeyError when calling <code>model.wv[poem_w]</code>.</p>
<p>Use <code>gensim.downloader</code> to load an existing model (check out <a href=""https://github.com/RaRe-Technologies/gensim-data#models"" rel=""nofollow noreferrer"">their repository</a> for a list of all available models):</p>
<pre><code>import gensim.downloader as api
import numpy as np
import pandas

poems = pandas.DataFrame({&quot;Main_text&quot;: [&quot;This is a sample poem.&quot;, &quot;This is another sample poem.&quot;]})
model = api.load(&quot;glove-wiki-gigaword-100&quot;)
</code></pre>
<p>Then use it to retrieve the vectors for all words the models knows:</p>
<pre><code>final_data = []
for poem in poems['Main_text']:
    poem_all_words = poem.split()
    poem_vectorized = []
    for poem_w in poem_all_words:
        if poem_w in model:
            poem_vectorized.append(model[poem_w])
    poem_vectorized_mean = np.mean(poem_vectorized, axis=0)
    final_data.append(poem_vectorized_mean)
</code></pre>
<p>Or as list comprehension:</p>
<pre><code>final_data = []
for poem in poems['Main_text']:
    poem_vectorized_mean = np.mean([model[poem_w] for poem_w in poem.split() if poem_w in model], axis=0)
    final_data.append(poem_vectorized_mean)
</code></pre>
<p>Which both will give you:</p>
<pre><code>X = np.asarray(final_data)
print(X)
&gt; [[-3.74696642e-01  3.73661995e-01  4.09943342e-01 -2.07784668e-01
    ...
    -1.85739681e-01 -7.07386672e-01  3.31366658e-01  3.31600010e-01]
   [-3.29973340e-01  4.13213342e-01  5.26199996e-01 -2.29261339e-01
    ...
    -1.25366330e-01 -5.87253332e-01  2.80240029e-01  2.56700337e-01]]
</code></pre>
<p>Note that attempting to get <code>np.mean()</code> on an empty list will throw an error so you might want to catch that in case there are poems which are empty or where all words are unknown to the model.</p>
",0,0,745,2022-04-10 10:49:50,https://stackoverflow.com/questions/71815866/how-do-i-use-gensim-to-vectorize-these-words-in-my-dataframe-so-i-can-perform-cl
word2vec/gensim — RuntimeError: you must first build vocabulary before training the model,"<p>I am having trouble training my own <code>word2vec</code> model on the <code>.txt</code> files.</p>
<p>The code:</p>
<pre><code>import gensim
import json
import pandas as pd
import glob
import gensim.downloader as api
import matplotlib.pyplot as plt
from gensim.models import KeyedVectors


# loading the .txt files

sentences = []
sentence = []
for doc in glob.glob('./data/*.txt'): 
     with(open(doc, 'r')) as f:
        for line in f:
            line = line.rstrip()
            if line == &quot;&quot;:
                if len(sentence) &gt; 0:
                    sentences.append(sentence)
                    sentence = []
            else:
                cols = line.split(&quot;\t&quot;)
                if len(cols) &gt; 4:
                    form = cols[1]
                    lemma = cols[2]
                    pos = cols[3]
                    if pos != &quot;PONCT&quot;:
                        sentence.append(form.lower())


# trying to train the model

from gensim.models import Word2Vec
model_hugo = Word2Vec(sentences, vector_size=200, window=5, epochs=10, sg=1, workers=4)
</code></pre>
<p>Message error:</p>
<pre><code>RuntimeError: you must first build vocabulary before training the model
</code></pre>
<p>How do I build the vocabulary?</p>
<p>The code works with the sample <code>.conll</code> files, but I want to train the model on my own data.</p>
","python, gensim, word2vec, word-embedding","<p>Thanks to the @gojomo's suggestion and to <a href=""https://stackoverflow.com/a/55091252/9737944"">this answer</a>, I resolved the empty <code>sentences</code> issue. I needed the following block of code:</p>
<pre><code># make an iterator that reads your file one line at a time instead of reading everything in memory at once
# reads all the sentences

class SentenceIterator: 
    def __init__(self, filepath): 
        self.filepath = filepath 

    def __iter__(self): 
        for line in open(self.filepath): 
            yield line.split() 

</code></pre>
<p>before training the model:</p>
<pre><code># training the model

sentences = SentenceIterator('/content/drive/MyDrive/rousseau/rousseau_corpus.txt') 
model = gensim.models.Word2Vec(sentences, min_count=2) # min_count is for pruning 
                                                       # the internal dictionary. 
                                                       # Words that appear only once 
                                                       # in the corpus are probably 
                                                       # uninteresting typos and garbage. 
                                                       # In addition, there’s not enough 
                                                       # data to make any meaningful 
                                                       # training on those words, so it’s
                                                       # best to ignore them
</code></pre>
",1,0,1325,2022-04-13 21:11:20,https://stackoverflow.com/questions/71863897/word2vec-gensim-runtimeerror-you-must-first-build-vocabulary-before-training
Generating Trigrams with Gensim&#39;s Phraser Package in Python,"<p>I have the following code snippet which I created with the help of <a href=""https://towardsdatascience.com/unsupervised-sentiment-analysis-a38bf1906483"" rel=""nofollow noreferrer"">this</a> tutorial for unsupervised sentiment analysis purposes:</p>
<pre><code>sent = [row for row in file_model.message]
phrases = Phrases(sent, min_count=1, progress_per=50000)
bigram = Phraser(phrases)
sentences = bigram[sent]
sentences[1]

file_export = file_model.copy()
file_export['old_message'] = file_export.message
file_export.old_message = file_export.old_message.str.join(' ')
file_export.message = file_export.message.apply(lambda x: ' '.join(bigram[x]))

file_export.to_csv('cleaned_dataset.csv', index=False)
</code></pre>
<p>Since now I want to have bigrams as well as trigrams, I tried it by adjusting it to:</p>
<pre><code>sent = [row for row in file_model.message]
phrases = Phrases(sent, min_count=1, progress_per=50000)
bigram = Phraser(phrases)
trigram = Phraser(bigram[phrases])
sentences = trigram[sent]
sentences[1]

file_export = file_model.copy()
file_export['old_message'] = file_export.message
file_export.old_message = file_export.old_message.str.join(' ')
file_export.message = file_export.message.apply(lambda x: ' '.join(trigram[x]))

file_export.to_csv('cleaned_dataset.csv', index=False)
</code></pre>
<p>But when I run this, I get <strong><code>TypeError: 'int' object is not iterable</code></strong> which I assume refers to my adjustment to <code>trigram = Phraser(bigram[phrases])</code>. I am using <code>gensim 4.1.2</code>.
Unfortunately, I have no computer science background and solutions I find online don't help out.</p>
","python, nlp, gensim, phrase","<p>As a general matter, it's best if you include in your question (by later editing if necessary) <em>the entire multiline error message you received</em>, including any 'traceback' showing involved filenames, line-numbers, &amp; lines-of-source-code. That helps potential answerers focus on exactly where things are going wrong.</p>
<p>Also, beware that many of the tutorials at 'towardsdatascience.com' are of very poor quality. I can't see the exact one you've linked without registering (which I'd rather not do), but from your code excerpts, I already see a few issues of varying severity for what you're trying to do:</p>
<ul>
<li>(fatal) If you want to apply the <code>Phrases</code> algorithm more than once, to compose up phrases longes than bigrams, you can't reuse the model trained for bigrams. You need to train a new model for each new level-of-combination, on the <em>output</em> of the prior model. That is, the input to the trigrams <code>Phrases</code> model (which must be trained) for trigrams must be the results of applying the bigram model, so it sees a mixture of the original unigrams &amp; now-combined bigrams.</li>
<li>(unwise) Generally, using a low <code>min_count=1</code> on these sorts of data-hungry models can easily backfire. They need many examples for their statistical-methods to do anything sensible; discarding the rarest words usually helps to speed processing, shrink the models, &amp; work mainly on tokens where there's enough examples to do something possibly sensible. (With very few, or only 1, usage examples, results may seem somewhat random/arbitrary.)</li>
<li>(a bit oudated but not a big problem) In Gensim 4+, the <code>Phraser</code> utiity class – which just exists to optimized the <code>Phrases</code> model a bit, when you're sure you're done training/tuning – has been renamed <code>FrozenPhrases</code>. (The old name still works, but this is an indication the tutorial hasn't been recently refreshed.)</li>
</ul>
<p>And in general, beware: without a lot of data, the output of any number of <code>Phrases</code> applications may not be strong. And in all cases, it may not 'look right' to human sensibilities – because it's pure statistical, co-occurrence driven. (Though, even if its output looks weird, it will sometimes help on certain info-retrieval/classification tasks, as it manages to create useful new features that are different than the unigrams alone.)</p>
<p>My suggestions would be:</p>
<ul>
<li>only add any <code>Phrases</code> combinations after things are working without, so you can compare results &amp; see if it's helping.</li>
<li>start with bigrams only, and be sure via careful review or rigorous scoring that's working/helping</li>
<li>if you need another level of combination, add that later, &amp; ensure the trigram <code>Phrases</code> is initialized with the already-bigram-combined texts.</li>
</ul>
<p>(Unfortunately, I can't find an example of two-level <code>Phrases</code> use in the current Gensim docs – I think some old examples were edited-out in doc simplification work. But there are a couple examples of it not being used all-wrong in the project's testing source code – search the file <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_phrases.py"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_phrases.py</a> for <code>trigram</code>. But remember those aren't best practices, either, as focused minimal tests.)</p>
",0,1,545,2022-04-18 16:22:33,https://stackoverflow.com/questions/71914383/generating-trigrams-with-gensims-phraser-package-in-python
Cannot see DEBUG logs for “number of documents converged“ info when running Gensim&#39;s LDA suggested for choosing iterations and passes,"<p>In official <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"" rel=""nofollow noreferrer"">Gensim tutorial</a> there is a mention about how to set number of iterations and passes:</p>
<blockquote>
<p>I suggest the following way to choose iterations and passes. First, enable logging (as described in many Gensim tutorials), and set eval_every = 1 in LdaModel. When training the model look for a line in the log that looks something like this:</p>
</blockquote>
<pre><code>2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations
</code></pre>
<p>I've never saw anything like this line in my LDA logs though. <a href=""https://pastebin.com/eyKsEuZK"" rel=""nofollow noreferrer"">Those are my logs on Pastebin</a>. I've folowed the <a href=""https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"" rel=""nofollow noreferrer"">official tutorial</a>.</p>
<p>I'm alllowing debugging like this:</p>
<pre><code>logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO,
                            filename='content_based_algorithms/training_logs/lda/logs.log')
</code></pre>
<p>I even tried to explicitly define callbacks::</p>
<pre><code>perplexity_logger = PerplexityMetric(corpus=corpus, logger='shell')
convergence_logger = ConvergenceMetric(logger='shell')

lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes, alpha=alpha, eta=eta, update_every=1, eval_every=1, callbacks=[convergence_logger, perplexity_logger])
</code></pre>
<p>I've tested that both in Windows, PyCharm IDE and Ubuntu command line execution of Python cript.</p>
<p><strong>Possible duplicate with the post <a href=""https://stackoverflow.com/questions/69269058/gensim-lda-logging-not-displaying/73002839#73002839"">Gensim LDA logging not displaying</a></strong></p>
","nlp, gensim, lda","<p>The line</p>
<blockquote>
<p>DEBUG - 68/1566 documents converged within 400 iterations</p>
</blockquote>
<p>Can be obtained in the logging file changing the logging configuration to <em>debug</em> in you case would be something like this:</p>
<pre><code>logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',
                   level=logging.DEBUG,
                   filename='content_based_algorithms/training_logs/lda/logs.log')
</code></pre>
<p>Now the line will appear inside the logging file.</p>
",2,1,606,2022-04-22 16:35:50,https://stackoverflow.com/questions/71972018/cannot-see-debug-logs-for-number-of-documents-converged-info-when-running-gens
"Infer &quot;shapes&quot;, or infer analogous relations in Word2Vec","<p>Gensim Word2Vec offers a system for inferring analogous relationships, that is, with the &quot;same shape&quot; as those already found?</p>
<p>Es:
Starting from <strong>King, Queen</strong></p>
<p>I would like to get other couples with male / female gender.</p>
<p>In other word:
most_similar(positive=['king', X], negative=['queen']) -&gt; Y</p>
<p>I would like to find as many xy pairs.</p>
","gensim, word2vec","<p>There's no built-in facility resembling what I think you're asking.</p>
<p>But, you are of course free to cycle through any number of candidate words (as <code>X</code>, or the other arguments to <code>most_similar()</code>), to see what top-neighbors are reported (candidate <code>Y</code> values) - perhaps applying some threshold of similarity.</p>
<p>Note the famous <code>man:king :: woman: _?_</code> is usually presented to a word2vec model in Gensim as <code>most_similar(positive=['king', 'woman'], negative=['man'])</code>, which sort of achieves <code>king - man + woman = _?_</code>. I'm not sure your alternate formulation, effectively <code>king - queen + X = Y</code> has an analogical meaning, for arbitrary <code>X</code> or responses <code>Y</code>.</p>
<p>And, note that <code>most_similar()</code> suppresses the reporting of any candidate wards that are already arguments to <code>positive</code> or <code>negative</code>. Often, the results of the 'artihmetic' are still closer to the input words than anything else - but that won't be reported, showing next-best words instead.</p>
",1,0,17,2022-05-09 00:40:06,https://stackoverflow.com/questions/72165998/infer-shapes-or-infer-analogous-relations-in-word2vec
What is the data type of X in pca.fit_transform(X)?,"<p>I got a word2vec model <code>abuse_model</code> trained by Gensim. I want to apply PCA and make a plot on CERTAIN words that I only care about (vs. all words in the model). Therefore, I created a dict <code>d</code> whose keys are words that I care about and the values are vectors to the key.</p>
<pre><code>vocab = list(abuse_model.wv.key_to_index)
vocab = [v for v in vocab if v in positive_terms]
d = {}
for word in vocab:
    d[word] = abuse_model.wv[word]
</code></pre>
<p>No errors so far.</p>
<p>I encountered an error when passing the dict into <code>pca.fit_transform</code>. I'm new to it and am wondering if the data format that I passed in (list of tuples) is not correct. What data type that the argument has to be?</p>
<pre><code>from sklearn.decomposition import PCA

pca = PCA(n_components=2)
result = pca.fit_transform(list(d.items()))
</code></pre>
<p>Thanks in advance!</p>
","scikit-learn, pca, gensim, word2vec","<p>Per <code>scikit-learn</code> docs – <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform</a> – the argument to <code>.fit_transform()</code>, as is usual for <code>scikit-learn</code> models, is &quot;array-like of shape (n_samples, n_features)&quot;.</p>
<p>Here, that'd mean your samples/rows are words, and features/columns the word-vector dimensions. And, you'll want to remember <em>outside</em> of the <code>PCA</code> object which words correspond to which rows. (In Python 3.x, the fact your <code>d</code> <code>dict</code> will always iterate in the order of insertion should have you covered there.)</p>
<p>So, it may be enough to change your use of <code>.items()</code> to <code>.values()</code>, so that you wind up supplying <code>PCA</code> with your <code>list</code> (which is suitably array-like) of vectors.</p>
<p>A few other notes:</p>
<ul>
<li>the <code>.key_to_index</code> property is already a <code>list</code>, so you don't need to convert/copy it</li>
<li>if your <code>positive_terms</code> is a large <code>list</code>, changing it to a <code>set</code> could offer faster <code>in</code> membership-testing</li>
<li>rather than using a <code>d</code> <code>dict</code>, which involves a little more overhead (including when you then make a <code>list</code> of its values), if your sets-of-words and vectors are large, you might want to preallocate a <code>numpy</code> array of the right size and collect your vectors in it. For example:</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>X = np.empty((len(vocab), abuse_model.wv.vector_size)
for i, word in enumerate(vocab):
    X[i] = abuse_model.wv[word]

#...
#...

result = pca.fit_transform(X)
</code></pre>
<ul>
<li>Even though your hunch is you only want the dimensionality-reduction on your subset of words, you may also want to try keeping all words, or some random subset of other words – it <em>might</em> help retain some of the original structure that otherwise, your subsampling will have prematurely removed. (Unsure of this; just noting it could be a factor.) Even if you do the PCA on a larger set of words, you could still choose to only later plot/analyze your desired subset for clarity.</li>
</ul>
",0,0,611,2022-05-12 19:17:57,https://stackoverflow.com/questions/72221035/what-is-the-data-type-of-x-in-pca-fit-transformx
How to create word embedding using Word2Vec on Python?,"<p>I have seen many tutorials online on how to use Word2Vec (gensim).</p>
<p>Most tutorials are showing on how to find the <code>.most_similar</code> word or similarity between two words.</p>
<p>But, how if I have text data <code>X</code> and I want to produce the word embedding vector <code>X_vector</code>?</p>
<p>So that, this <code>X_vector</code> can be used for classification algorithms?</p>
","python, gensim, word2vec, text-classification, word-embedding","<p>If <code>X</code> is a word (string token), you can look up its vector with <code>word_model[X]</code>.</p>
<p>If <code>X</code> is a text - say, a list-of-words – well, a <code>Word2Vec</code> model only has vectors for words, not texts.</p>
<p>If you have some desired way to use a list-of-words plus per-word-vectors to create a text-vector, you should apply that yourself. There are many potential approaches, some simple, some complicated, but no one 'official' or 'best' way.</p>
<p>One easy popular baseline (a fair starting point especially on very small texts like titles) is to average together all the word vectors. That can be as simple as (assuming <code>numpy</code> is imported as <code>np</code>):</p>
<pre><code>np.mean([word_model[word] for word in word_list], axis=0)
</code></pre>
<p>But, recent versions of Gensim also have a convenience <code>.get_mean_vector()</code> method for averaging together sets of vectors (specified as their word-keys, or raw vectors), with some other options:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector</a></p>
",2,0,1902,2022-05-23 09:42:48,https://stackoverflow.com/questions/72346412/how-to-create-word-embedding-using-word2vec-on-python
I get an (AttributeError: The vocab attribute was removed from KeyedVector in Gensim 4.0.0) when i try to load google news vector embeddings,"<p>I have a class Featurizer that checks for the existance of an embedding_file which has word embeddings from Google news vectors and loads it when called. However when i use the class Feauturizer to load the model.</p>
<p>It gives an error</p>
<pre><code>    AttributeError                            Traceback (most recent call last)
d:\mt 111\QuestionAnswer\training_model.ipynb Cell 11' in &lt;cell line: 2&gt;()
      1 emb_file = os.path.join('D:\mt 111\QuestionAnswer\embedding_file', 'GoogleNews-vectors-negative300.bin')
----&gt; 2 featurizer = Featurizer(emb_file)

d:\mt 111\QuestionAnswer\training_model.ipynb Cell 4' in Featurizer.__init__(self, embedding_file)
     11 print('INFO: Loading word vectors...')
     12 self.word2vec = KeyedVectors.load_word2vec_format(
     13     'GoogleNews-vectors-negative300.bin',
     14     binary=True)
     16 print('INFO: Done! Using %s word vectors from pre-trained word2vec.' \
---&gt; 17     %len(self.word2vec.vocab))

File d:\mt 111\QuestionAnswer\venv\lib\site-packages\gensim\models\keyedvectors.py:735, in KeyedVectors.vocab(self)
    733 @property
    734 def vocab(self):
--&gt; 735     raise AttributeError(
    736         &quot;The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\n&quot;
    737         &quot;Use KeyedVector's .key_to_index dict, .index_to_key list, and methods &quot;
    738         &quot;.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\n&quot;
    739         &quot;See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4&quot;
    740     )

AttributeError: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.
Use KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val)instead.
</code></pre>
<p>See <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4</a></p>
<p>This is the class,</p>
<pre><code>class Featurizer:

    def __init__(self, embedding_file):

        if not os.path.exists(embedding_file):
            raise IOError(&quot;Embeddings file does not exist: %s&quot; %embedding_file)

        punctuation = string.punctuation
        punctuation = punctuation + &quot;’&quot; + &quot;“&quot; + &quot;?&quot; + &quot;‘&quot;
        self.punctuation = punctuation
        print('INFO: Loading word vectors...')
        self.word2vec = KeyedVectors.load_word2vec_format(
            embedding_file,
            binary=True)

        print('INFO: Done! Using %s word vectors from pre-trained word2vec.' \
            %len(self.word2vec.vocab))
</code></pre>
<p>When i try to load the embeddings using the class Featurizer</p>
<pre><code>emb_file = os.path.join('D:\mt 111\QuestionAnswer\embedding_file', 'GoogleNews-vectors-negative300.bin')
featurizer = Featurizer(emb_file)
</code></pre>
<p>Ideally, if it loaded properly. It would give a message output from the Featurizer class such as</p>
<pre><code>emb_file = os.path.join('D:\mt 111\QuestionAnswer\embedding_file', 'GoogleNews-vectors-negative300.bin')
featurizer = Featurizer(emb_file)
</code></pre>
<p>INFO: Loading word vectors...</p>
<p>INFO: Done! Using 3000000 word vectors from pre-trained word2vec.</p>
<p>How can i go about this!!!</p>
","python, flask, gensim","<p>The load succeeded; the failure was in your line of code that tried to report <code>len(self.word2vec.vocab)</code>.</p>
<p>Let me quote the error message for the reason that your code couldn't access a <code>.vocab</code> property:</p>
<blockquote>
<p>The vocab attribute was removed from KeyedVector in Gensim 4.0.0. Use
KeyedVector's .key_to_index dict, .index_to_key list, and methods
.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.
See
<a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4</a></p>
</blockquote>
<p>So, you can't use <code>.vocab</code> anymore, but there are several new properties listed there, like <code>.key_to_index</code> (a dict like <code>vocab</code> was) or <code>.index_to_key</code> (a list of all lookup keys – words – in the set-of-vectors).</p>
<p>Have you tried using any of those specific properties recommended in the error message you received, instead of <code>.vocab</code>?</p>
<p>Or, visiting the recommended URL, which <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes"" rel=""nofollow noreferrer"">makes specific suggestions with before and after code examples</a> how to replace references to the no-longer-available <code>.vocab</code> attribute? Here are the relevant lines of things <em>not</em> to do (🚫), and to do instead (👍), for your case:</p>
<pre class=""lang-py prettyprint-override""><code>vocab_len = len(model.wv.vocab)  # 🚫
…
vocab_len = len(model.wv)  # 👍
</code></pre>
",2,1,1382,2022-06-01 07:39:06,https://stackoverflow.com/questions/72458031/i-get-an-attributeerror-the-vocab-attribute-was-removed-from-keyedvector-in-ge
How to handle KeyError(f&quot;Key &#39;{key}&#39; not present&quot;) wor2vec with gensim,"<p>I have build a model with gensim library and am trying to get the vector of word that not present in the vocabulary but i have an error, and i want to handle this error with the best i way.
If i can get the vector of word not present in the model that well be perfect.</p>
<p><strong>The code</strong></p>
<pre><code>model = KeyedVectors.load('nice.model')
token_vector = model.wv['bla bla bla']
</code></pre>
<p><strong>Error</strong></p>
<pre><code>  File &quot;/home/ahmed/PycharmProjects/WebScarping/venv/lib/python3.9/site-packages/gensim/models/keyedvectors.py&quot;, line 421, in get_index
    raise KeyError(f&quot;Key '{key}' not present&quot;)
KeyError: &quot;Key 'hmed' not present&quot;
</code></pre>
<p>please help me in resolving the error</p>
","python, nlp, gensim, word2vec, keyerror","<p>If the token is not present in the model, it can't give you a vector for it.</p>
<p>Your model doesn't have a vector for the (pseudo-)word <code>'bla bla bla'</code>, all it can do is report that.</p>
<p>You could avoid the exception by pre-checking whether the token is present, and only requesting it if present:</p>
<pre class=""lang-py prettyprint-override""><code>if token in model.wv:
    token_vector = model.wv[token]
else:
    # whatever your next-best step is when a vector not available
    ...
</code></pre>
<p>Or, you could catch the exception:</p>
<pre class=""lang-py prettyprint-override""><code>try:
    token_vector = model.wv[token]
except KeyError:
    # whatever your next-best step is when a vector not available
    ...
</code></pre>
<p>But there's no magic way to create a good vector for an unknown token. You'll have to ignore such words, or make-up some plug stand-in value, or figure some other project-appropriate workaround.</p>
<p>(If you have sufficient training data with varied examples of the token's real usage, you could train a model that includes the token. You could also consider finding or training a word2vec variant model like <code>FastText</code>, which can synthesize guess-vectors for unknown tokens based on which substrings they might share with words learned in training – but such vectors may be quite poor in quality.)</p>
",2,2,7872,2022-06-02 17:42:32,https://stackoverflow.com/questions/72480289/how-to-handle-keyerrorfkey-key-not-present-wor2vec-with-gensim
GoogleNews-vectors-negative300.bin cannot be loaded in gensim models MemoryError,"<p>I have been trying to load GoogleNews Vector file to gensim models. The program never finished loading and I keep getting MemoryError. A few days ago, I didn't have that problem. I don't know why I get this problem all of a sudden.</p>
<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('../../data/GoogleNews-vectors-negative300.bin', binary=True)  

## Above is my simplified python file and how I load the model. 

Traceback (most recent call last):
  File &quot;test.py&quot;, line 4, in &lt;module&gt;
    model = gensim.models.KeyedVectors.load_word2vec_format('../../data/GoogleNews-vectors-negative300.bin', binary=True)  
  File &quot;/usr/local/lib64/python3.6/site-packages/gensim/models/keyedvectors.py&quot;, line 1549, in load_word2vec_format
    limit=limit, datatype=datatype)
  File &quot;/usr/local/lib64/python3.6/site-packages/gensim/models/utils_any2vec.py&quot;, line 286, in _load_word2vec_format
    vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size)
  File &quot;/usr/local/lib64/python3.6/site-packages/gensim/models/utils_any2vec.py&quot;, line 205, in _word2vec_read_binary
    result, counts, chunk, vocab_size, vector_size, datatype, unicode_errors)
  File &quot;/usr/local/lib64/python3.6/site-packages/gensim/models/utils_any2vec.py&quot;, line 190, in _add_bytes_to_result
    _add_word_to_result(result, counts, word, vector, vocab_size)
  File &quot;/usr/local/lib64/python3.6/site-packages/gensim/models/utils_any2vec.py&quot;, line 169, in _add_word_to_result
    result.vocab[word] = gensim.models.keyedvectors.Vocab(index=word_id, count=word_count)
MemoryError

</code></pre>
","python, gensim","<p>You are getting a <code>MemoryError</code> because your system lacks enough RAM to finish the operation.</p>
<p>The <code>GoogleNews</code> vectors are over 3GB on disk, and require more RAM than that to load into the Python object heap. Even if you were doing nothing else on the same machine, it's doubtful you could do much with them on a system with 4GB of RAM - you'd need 8GB, or more, depending on what else is using memory on the machine, and in your Python process(es).</p>
<p>If the same step was succeeding a few days ago, it is certain that the system you were using then (even if the same system as now) had more free memory <em>at the time you attempted the load</em> then, compared to now.</p>
<p>Your options are:</p>
<ul>
<li>move to a system with more RAM (perhaps by upgrading the one yo are using)</li>
<li>load a smaller set of vectors</li>
</ul>
<p>The Gensim <code>KeyedVectors.load_word2vec_format()</code> method takes an optional <code>limit</code> parameter which only reads exactly that many words from the front of the supplied file. As the <code>GoogleNews</code> model includes 3 million words, using something like <code>limit=500000</code> loads jut 1/6th of the words, and thus uses about 1/6th of the RAM.</p>
<p>That's still a ton of words! And, as such models typically list the most-frequently-used words first, a <code>limit=500000</code> only discards less-frequently-used words. Sometimes with natural-language-processing, discarding <em>more</em> of the rare words can even <em>improve</em> results on common tasks. (Rarer word senses-of-meaning can vary more, their vectors are often lower-quality as they've been trained on fewer examples, and yet altogether they are quite numerous overall – sometimes making their inclusion cost more in model size, and processing time, than any incremental meaning they deliver.)</p>
<p>Separately, and unlikely to be a major factor in your issue: it appears you're using a years-old version of Gensim. Generally, efficiency with regard to memory-usage and task-runtimes will improve with later versions. So no matter how you get around this particular <code>MemoryError</code>, you should generally prefer to use a current version of Gensim, such as 4.2.0 (as of this writing in June 2022).</p>
",1,0,370,2022-06-04 19:41:13,https://stackoverflow.com/questions/72502665/googlenews-vectors-negative300-bin-cannot-be-loaded-in-gensim-models-memoryerror
Gensim- KeyError: &#39;word not in vocabulary&#39;,"<p>I am trying to achieve something similar in calculating product similarity used in this example. <a href=""https://www.analyticsvidhya.com/blog/2019/07/how-to-build-recommendation-system-word2vec-python/"" rel=""nofollow noreferrer"">how-to-build-recommendation-system-word2vec-python/</a></p>
<p>I have a dictionary where the key is the item_id and the value is the product associated with it. For eg: <code>dict_items([('100018', ['GRAVY MIX PEPPER']), ('100025', ['SNACK CHEEZIT WHOLEGRAIN']), ('100040', ['CAULIFLOWER CELLO 6 CT.']), ('100042', ['STRIP FRUIT FLY ELIMINATOR'])....)</code></p>
<p>The data structure is the same as in the example (as far as I know). However, I am getting <strong>KeyError: &quot;word '100018' not in vocabulary&quot;</strong> when calling the similarity function on the model using the key present in the dictionary.</p>
<pre><code># train word2vec model
model = Word2Vec(window = 10, sg = 1, hs = 0,
             negative = 10, # for negative sampling
             alpha=0.03, min_alpha=0.0007,
             seed = 14)
model.build_vocab(purchases_train, progress_per=200)
model.train(purchases_train, total_examples = model.corpus_count, 
        epochs=10, report_delay=1)

def similar_products(v, n = 6): #similarity function

# extract most similar products for the input vector
ms = model.similar_by_vector(v, topn= n+1)[1:]

# extract name and similarity score of the similar products
new_ms = []
for j in ms:
    pair = (products_dict[j[0]][0], j[1])
    new_ms.append(pair)
    
return new_ms    
</code></pre>
<p>I am calling the function using:</p>
<pre><code>similar_products(model['100018'])
</code></pre>
<p>Note: I was able to run the example code with the very similar data structure input which was also a dictionary. Can someone tell me what I am missing here?</p>
","python, gensim, word2vec","<p>If you get a <code>KeyError</code> telling you a word isn't in your model, then the word genuinely isn't in the model.</p>
<p>If you've trained the model yourself, and expected the word to be in the resulting model, but it isn't, something went wrong with training.</p>
<p>You should look at the corpus (<code>purchases_train</code> in your code) to make sure each item is of the form the model expects: a list of words. You should enable logging during training, and watch the output to confirm the expected amount of word-discovery and training is happening. You can also look at the exact list-of-words known-to-the-model (in <code>model.wv.key_to_index</code>) to make sure it has all the words you expect.</p>
<p>One common gotcha is that by default, for the best operation of the word2vec algorithm, the <code>Word2Vec</code> class uses a default <code>min_count=5</code>. (Word2vec only works well with multiple varied examples of a word's usage; a word appearing just once, or just a few times, usually won't get a good vector, and further, might make other surrounding word's vectors worse. So the usual best practice is to discard very-rare words.</p>
<p>Is the (pseudo-)word <code>'100018'</code> in your corpus less than 5 times? If so, the model will ignore it as a word too-rare to get a good vector, or have any positive influence on other word-vectors.</p>
<p>Separately, the site you're using example code from may not be a quality source of example code. It's changed a bunch of default values for no good reason - such as changing the <code>alpha</code> and <code>min_alpha</code> values to peculiar non-standard values, with no comment why. This is usually a signal that someone who doesn't know what they're doing is copying someone else who didn't know what they were doing's odd choices.</p>
",1,1,1931,2022-06-06 15:03:32,https://stackoverflow.com/questions/72519612/gensim-keyerror-word-not-in-vocabulary
How to convert small dataset into word embeddings instead of one-hot encoding?,"<p>I have a dataset of 33 words that are a mix of verbs and nouns, for eg. father, sing, etc. I have tried converting them to 1-hot encoding but for my use case, it has been suggested to look into word2vec embedding. I have looked in gensim and glove but struggling to make it work.</p>
<p><strong>How could I convert my data into an embedding?</strong> Such that two words that may be semantically closer may have a lesser distance between their respective vectors. How may this be achieved or any helpful material on the same?</p>
<p>Such as this<img src=""https://miro.medium.com/max/1400/1*sAJdxEsDjsPMioHyzlN3_A.png"" alt=""embedding"" /></p>
","nlp, stanford-nlp, gensim, word2vec","<p>Since your dataset is quite small, and I'm assuming it doesn't contain any jargon, it's best to use a pre-trained model in order to save up on training time.</p>
<p>With gensim, it's as simple as:</p>
<pre><code>import gensim.downloader as api
wv = api.load('word2vec-google-news-300')
</code></pre>
<p>The 'word2vec-google-news-300' model has been pre-trained on a part of the Google News Dataset and generalizes well enough to most tasks. Following this, you can create word embeddings/vectors like so:</p>
<pre><code>vec = wv['father']
</code></pre>
<p>And, finally, for computing word similarity:</p>
<pre><code>similarity_score = wv.similarity('father', 'sing')
</code></pre>
<p>Lastly, one major limitation of Word2Vec is it's inability to deal with words that are OOV(out of vocabulary). For such cases, it's best to train a custom model for your corpus.</p>
",3,1,1014,2022-06-08 12:30:53,https://stackoverflow.com/questions/72545744/how-to-convert-small-dataset-into-word-embeddings-instead-of-one-hot-encoding
Is there a required size of data set for LDA to work in python?,"<p>I am trying to apply LDA on Stack overflow posts for a project. My corpus has about 650 post of interest and I was wondering if the size of the corpus would hinder the functioning of the LDA later on especially when I have to specify the K value?</p>
<p>I have not applied LDA before, therefore some hints on this would really help me.</p>
","python, nlp, gensim, lda, topic-modeling","<p>It might work. What happens when you try it? Are the results useful according to your goals with those 650 posts?</p>
",1,1,403,2022-06-12 07:31:09,https://stackoverflow.com/questions/72590264/is-there-a-required-size-of-data-set-for-lda-to-work-in-python
FastText: Can&#39;t see the representation of words that starts with @ or @,"<p>I am working in an NLP project using FastText. I have some texts which contains words like @.poisonjamak, @aminagabread, @<em>iamquak123</em> and I want to see their FastText representation. I want to mention that the model has the following form:</p>
<pre><code># FastText
ft_model = FastText(word_tokenized_corpus,
                    max_n=0,
                    vector_size=64,
                    window=5,
                    min_count=1,
                    sg=1,
                    workers=20,
                    epochs=50,
                    seed=42)
</code></pre>
<p>Using this I can see their representation, however I have an error</p>
<pre><code>print(ft_model.wv['@.poisonjamak'])

KeyError: 'cannot calculate vector for OOV word without ngrams'
</code></pre>
<p>Of course, these words are in my texts. I have the above error in all these 3 words, however if I do the following this is working.</p>
<pre><code>print(ft_model.wv['@.poisonjamak']) -----&gt; print(ft_model.wv['poisonjamak'])
print(ft_model.wv['@aminagabread']) -----&gt; print(ft_model.wv['aminagabread'])
print(ft_model.wv['@_iamquak123_']) -----&gt; print(ft_model.wv['_iamquak123_'])
</code></pre>
<p><strong>Question: So do you know why I have this problem?</strong></p>
<p><strong>Update:</strong>
My dataset called 'df' and the column with texts called 'text'. I using the following code to prepare the texts for the fast text. The FastText is trained on word_tokenized_corpus</p>
<pre><code>extra_list = df.text.tolist()
final_corpus = [sentence for sentence in extra_list if sentence.strip() !='']

word_punctuation_tokenizer = nltk.WordPunctTokenizer()
word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]
</code></pre>
","python, nlp, gensim, fasttext","<p>As comments note, the main issue is likely with your tokenizer, which won't put <code>'@'</code> characters inside your tokens. As a result, your <code>FastText</code> model isn't seeing the tokens you expect – but probably <em>does</em> have a word-vector for the 'word' <code>'@'</code>.</p>
<p>Separately reviewing your actual <code>word_tokenized_corpus</code>, to see what it truly includes before the mdoel gets to do its training, is a good way to confirm this (or catch this class of error in the future).</p>
<p>There is however another contributing issue: your use of the <code>max_n=0</code> parameter. This essentially <em>turns off</em> subword learning, by qualifying <em>no</em> positive-length word-substrings (aka 'character n-grams') for vector-learning. This setting essentially turns <code>FastText</code> into plain <code>Word2Vec</code>.</p>
<p>If instead you were using <code>FastText</code> in a more usual way, it would've learned subword-vectors for some of the subwords in <code>'aminagabread'</code> etc, and thus would'vbe provided synthetic &quot;guess&quot; word-vectors for the full <code>'@aminagabread'</code> unseen OOV token.</p>
<p>So in a way, you're <em>only</em> seeing the error letting you know about a problem in your tokenization because of this other deviation from usual <code>FastText</code> OOV behavior. If you really want <code>FastText</code> for its unique benefit of synthetic vectors for OOV words, you should return to a more typical <code>max_n</code> setting.</p>
<p>Separate usage tips:</p>
<ul>
<li><code>min_count=1</code> is usually a bad idea with such word2vec-family algorithms, as such rare words don't have enough varied usage examples to get good vectors themselves, but the failed attempt to try degrades training for surrounding words. Often, discarding such words (as with the default <code>min_count=5</code> as if they weren't there at all improves downstream evaluations.</li>
<li>Because of some inherent threading inefficiencies of the Python Global Interpreter Lock (&quot;GIL&quot;), and the Gensim approach to iterating over your corpus in one thread, parcelling work out to worker threads, it is likely you'll get higher training throughput with <em>fewer</em> workers than your <code>workers=20</code> setting, even if you have 20 (or far more) CPU cores. The exact best setting in any situation will vary by a lot of things, including some of the model parameters, and only trial-and-error can narrow the best values. But it's more likely to be in the 6-12 range, even when more cores are available, than 16+.</li>
</ul>
",1,0,369,2022-06-28 09:53:20,https://stackoverflow.com/questions/72784310/fasttext-cant-see-the-representation-of-words-that-starts-with-or
What is the meaning of size(embedding_model)?,"<p>I want to be sure I understand correctly:</p>
<p>Using the length of embedding model means number of different tokens it contains?</p>
<p>i.e:</p>
<pre><code>from gensim import downloader
embedding_model = downloader.load('glove-wiki-gigaword-50')
print(len(embedding_model))
</code></pre>
<p>output:</p>
<pre><code>400000 
</code></pre>
<p>means: <code>glove-wiki-gigaword-50</code> has 400000 different tokens (words) and each token (word) has the size of 50 bytes ?</p>
","gensim, word2vec, word-embedding","<p>Yes, <code>len(model)</code> in this case gives you the count of words inside it.</p>
<p><code>model.vector_size</code> will give you the number of dimensions (not bytes) per vector. (The actual size of the vector in bytes will be 4 times the count of dimensions, as each <code>float32</code>-sized value takes 4 bytes.)</p>
<p>I generally recommend against ever using the Gensim <code>api.downloader</code> functionality: if you instead find &amp; manually download from the original source of the files, you'll better understand their contents, formats, &amp; limitations – and where the file has landed in your local filesystem. And, by then using a specific class/method to load the file, you'll better understand what kinds of classes/objects you're using, rather than whatever mystery-object <code>downloader.load()</code> might have given you.</p>
",1,0,68,2022-07-09 02:59:25,https://stackoverflow.com/questions/72918624/what-is-the-meaning-of-sizeembedding-model
Finding the cosine similarity of two sentences using LSA,"<p>I am trying to use Latent Semantic Indexing to produce the cosine similarity between two sentences based on the topics produced from a large corpus but I'm struggling to find any tutorials that do exactly what I'm looking for - the closest I've found is <a href=""https://stackoverflow.com/questions/31821821/semantic-similarity-between-phrases-using-gensim"">Semantic Similarity between Phrases Using GenSim</a> but I'm not looking to find the most similar sentence to a query, I specifically want to use an LSI model to reduce the dimensionality of two sentences and then measure the cosine similarity of the two sentences. Please can someone help?</p>
<p>From the quoted article, I thought I might be looking at the below code and then having the cosine similarity calculation? But I'm stuck.</p>
<pre><code>import gensim
from gensim import corpora, models, similarities
from gensim.models import LsiModel

# texts = list of list of words from a database
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=400)
doc_1 = &quot;Mary and Samantha arrived at the bus station early but waited until noon for the bus&quot;
doc_2 = &quot;when the seagulls follow the trawler, it is because they think sardines will be dropped in the sea&quot;
vec_bow_1 = dictionary.doc2bow(doc_1.lower().split())
vec_bow_2 = dictionary.doc2bow(doc_2.lower().split())
vec_lsi_1 = lsi[vec_bow_1]
vec_lsi_2 = lsi[vec_bow_2]
</code></pre>
","python, gensim, cosine-similarity, lsa","<p>If you've succeeded in making <code>vec_lsi_1</code> a vector for your <code>doc1</code>, and <code>vec_lsi_2</code> a vector for your <code>doc2</code>, have you tried simply calculating the cosine-similarity between those two vectors? Cosine similarity is calculated by taking the dot-product of two vetors, then dividing that by their unit-norms. EG:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

cossim = (
           np.dot(vec_lsi_1, vec_lsi_2) 
           /
           (np.linalg.norm(vec_lsi_1) * np.linalg.norm(vec_lsi_2))
         )
</code></pre>
<p><strong>Update for completeness:</strong> If <code>vec_lsi_1</code> etc are sparse vectors – some form of list of <code>(index, value)</code> where unmentioned indexes are assumed to be <code>0.0</code> – then <code>np.dot()</code> may not work directly; see <a href=""https://radimrehurek.com/gensim/matutils.html#gensim.matutils.sparse2full"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/matutils.html#gensim.matutils.sparse2full</a> for a helper function to turn Gensim's sparse format into a dense numpy vector.</p>
",0,0,736,2022-07-13 21:46:19,https://stackoverflow.com/questions/72972973/finding-the-cosine-similarity-of-two-sentences-using-lsa
Gensim word2vec model with unordered sentences,"<p>Hello I'm trying to tune word2vec for finding related categories on a large set of categories list.</p>
<p>My main problem compare to natural language is that my categories list are not ordered in a logic manner.</p>
<p>For example I have a lists of fruits:</p>
<pre><code>[banana, mango, apple...], 
[mango, lemon, pineapple...]
</code></pre>
<p>Let's assume mango usually comes in the same list as banana.
I want the model to detect this relationship such that when I call most_similar to mango I'll get banana first.</p>
<p>The problem is that the order of the fruit is meaningless. Mango and banana distance in the list can differate without any meaning.</p>
<p>I thought to set a very high window so &quot;everything is related to everything&quot; but I'm not sure it's the best approach.</p>
<p>I have a dataset of 12M sentences with 500K unique categories.</p>
<p>What is a good started for aloha rate, window and adjusting the model in general? Does word2vec even fits this?</p>
","gensim, word2vec","<p>Setting a very-large window – far larger than any of your texts – does essentially put all words into each others' context-windows. (If your texts are long, it will also significantly increase runtime, especially in skip-gram  mode.) You can also use the optional non-default setting in recent Gensim releases <code>shrink_windows=False</code> tofurther ensure that a step which normally might probabilistically reduce effective window-sizes is skipped.</p>
<p>Whether word2vec will work well for you is something best answered empirically, by trying it versus other approaches on your data and needs. You're not quite using typical texts, with normal natural-language word frequencies &amp; co-occurrences. But lots of people have found success with word2vec-like approaches on not-quite-real-language tokenized data.</p>
<p>While the default values of parameters are often reasonable starting points, with enough data and a typical task, you may have to vary them even more from the defaults, for optimal results, when using other kinds of data, or pursuing less typical goals.</p>
<p>If using the default negative-sampling on categorical data, you may especially want to look at the optional parameter <code>ns_exponent</code> – frozen in early implementations at <code>ns_exponent=0.75</code>, but now adjustable in Gensim – which one paper suggested could have far better values in models for recommendation-systems. (See the <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">class docs for that parameter</a> for a link to that paper.)</p>
<p>But more generally: to find optimal values, you'll need some way to explore many options in an automated fashion. That means some robust repeatable way to score your end model on your real intended end-task (or a close proxy), so that you can run it many different ways then pick the one that scores best. And, parameters like <code>epochs</code>, <code>vector_size</code>, <code>window</code>, <code>negative</code>, and <code>min_count</code> are those most-often tuned.</p>
<p>Note though that if all you really want is to get the ranked list of things that most-often come in the same list as a target query – get back <code>'banana'</code> for <code>'mango'</code>, if and only if <code>'banana'</code> co-appears most-often – then you can use a much simpler approach than word2vec: just count, and retain, <em>all</em> the co-occurrences, or all the top-N co-occurrences for each unique key.</p>
<p>A fairly straightforward &amp; usual way to do this in Python would be to use a <code>dict</code> with one entry per unique key (category), and have the value stored at that key be an instance of the Python utility dictionary <code>Counter</code>. Iterate once over the whole dataset, tallying every co-occurence.</p>
<p>At the end, when you look up the <code>Counter</code> for <code>'mango'</code>, just list its contents in highest-to-lowest count order. You'll have a precise answer, from a single pass over the data – rather than the 'dense' vectors that an algorithm like word2vec builds over many passes, probabilistically.</p>
<p>(If instead of using sorted <code>Counter</code>s per key, but sparse vectors, as wide as your count of unique keys, containing counts of each co-occurence in the respective slots-per-category, then you'll have a 'bag-of-words'-like large sparse vector per primary key. Those can also be pairwise compared by cosine-similarity, similar to how dense embeddings like smaller-dimensional word-vectors can be.)</p>
",1,0,140,2022-07-27 12:04:32,https://stackoverflow.com/questions/73137615/gensim-word2vec-model-with-unordered-sentences
Gensim 4.2.0 downloader function is missing,"<p>I'm using the Gensim package. However, when I want to load the word2vec model, the <code>gensim.downloader</code> function seems not to exist.</p>
<pre><code>w2v = gensim.downloader.load('word2vec-google-news-300')
</code></pre>
<p>Got error message:</p>
<pre><code>AttributeError: module 'gensim' has no attribute 'downloader'
</code></pre>
<p>I checked the directory of gensim using dir() method and here's what I got:</p>
<pre><code>['__builtins__','__cached__','__doc__','__file__','__loader__','__name__','__package__','__path__','__spec__','__version__','_matutils','corpora','interfaces','logger','logging','matutils','models','parsing','similarities','topic_coherence','utils']
</code></pre>
<p>Seems like the downloader method is not in the directory. I wonder if there's another way to download a specific pretrained model with gensim library and also what's wrong with the gensim downloader.</p>
<p>My gensim version is 4.2.0.</p>
","python, nlp, gensim, word2vec","<p>If you're following some example code, you should copy its imports &amp; code exactly. I don't think you'll find any docs/examples suggesting to use the <code>gensim.downloader</code> module the way you've attempted.</p>
<p>More generally: I recommend against using <code>gensim.downloader</code>. It hides the actual sources, local paths, &amp; return types of the data it retrieves, and also runs new code, from the net, that's not part of the Gensim project source-control nor part of versioned Gensim releases. (It's a sketchy software-engineering practice.)</p>
<p>Instead, download the <code>GoogleNews</code> dataset directly from some host, saving the exact original file(s) to a specific place of your choosing. Examine the downloads to understand their filenames/formats (decompressing if necessary).</p>
<p>Then use other Gensim methods – such as <code>KeyedVectors.load_word2vec_format()</code> – to load from a specific known local file path, with a returned object of a specific documented type.</p>
<p>Your code (and your own understanding) will be more clear, robust, &amp; secure.</p>
",0,0,469,2022-08-01 04:19:36,https://stackoverflow.com/questions/73188799/gensim-4-2-0-downloader-function-is-missing
"Can doc2vec training result could change with same input data, and same parameter?","<p>I'm using Doc2Vec in gensim library, and finding similiarity between movie, with its name as input.</p>
<pre><code>model = doc2vec.Doc2Vec(vector_size=100, alpha=0.025, min_alpha=0.025, window=5)
model.build_vocab(tagged_corpus_list)
model.train(tagged_corpus_list, total_examples=model.corpus_count, epochs=50)
</code></pre>
<p>I set parameter like this, and didn't change preprocessing mechanism of input data, didn't changed original data.</p>
<pre><code>similar_doc = model.dv.most_similar(input)
</code></pre>
<p>I also used this code to find most similar movie.
When I restarted code to train this model, the most similar movie has changed, with changed score.
Is this possible? Why? If then, how can I fix the training result?</p>
","python, gensim, doc2vec","<p>Yes, this sort of change from run to run is normal. It's well-explained in question 11 of the Gensim FAQ:</p>
<blockquote>
<h3><a href=""https://github.com/RARE-Technologies/gensim/wiki/Recipes-&amp;-FAQ#q11-ive-trained-my-word2vec--doc2vec--etc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">Q11: I've trained my <code>Word2Vec</code> / <code>Doc2Vec</code> / etc model repeatedly using the exact same text corpus, but the vectors are different each time. Is there a bug or have I made a mistake? (*2vec training non-determinism)</a></h3>
<p><strong>Answer:</strong> The *2vec models (word2vec, fasttext, doc2vec…) begin with random initialization, then most modes use additional randomization
during training. (For example, the training windows are randomly
truncated as an efficient way of weighting nearer words higher. The
negative examples in the default negative-sampling mode are chosen
randomly. And the downsampling of highly-frequent words, as controlled
by the <code>sample</code> parameter, is driven by random choices. These
behaviors were all defined in the original Word2Vec paper's algorithm
description.)</p>
<p>Even when all this randomness comes from a
pseudorandom-number-generator that's been seeded to give a
reproducible stream of random numbers (which gensim does by default),
the usual case of multi-threaded training can further change the exact
training-order of text examples, and thus the final model state.
(Further, in Python 3.x, the hashing of strings is randomized each
re-launch of the Python interpreter - changing the iteration ordering
of vocabulary dicts from run to run, and thus making even the same
string-of-random-number-draws pick different words in different
launches.)</p>
<p>So, it is to be expected that models vary from run to run, even
trained on the same data. There's no single &quot;right place&quot; for any
word-vector or doc-vector to wind up: just positions that are at
progressively more-useful distances &amp; directions from other vectors
co-trained inside the same model. (In general, only vectors that were
trained together in an interleaved session of contrasting uses become
comparable in their coordinates.)</p>
<p>Suitable training parameters should yield models that are roughly as
useful, from run-to-run, as each other. Testing and evaluation
processes should be tolerant of any shifts in vector positions, and of
small &quot;jitter&quot; in the overall utility of models, that arises from the
inherent algorithm randomness. (If the observed quality from
run-to-run varies a lot, there may be other problems: too little data,
poorly-tuned parameters, or errors/weaknesses in the evaluation
method.)</p>
<p>You can try to force determinism, by using <code>workers=1</code> to limit
training to a single thread – and, if in Python 3.x, using the
<code>PYTHONHASHSEED</code> environment variable to disable its usual string hash
randomization. But training will be much slower than with more
threads. And, you'd be obscuring the inherent
randomness/approximateness of the underlying algorithms, in a way that
might make results more fragile and dependent on the luck of a
particular setup. It's better to tolerate a little jitter, and use
excessive jitter as an indicator of problems elsewhere in the data or
model setup – rather than impose a superficial determinism.</p>
</blockquote>
<p>If the change between runs is small – nearest neighbors mostly the same, with a few in different positions – it's best to tolerate it.</p>
<p>If the change is big, there's likely some other problem, like insufficient training data or poorly-chosen parameters.</p>
<p>Notably, <code>min_alpha=0.025</code> isn't a sensible value - the training is supposed to use a gradually-decreasing value, and the usual default (<code>min_alpha=0.0001</code>) usually doesn't need changing. (If you copied this from an online example: that's a bad example! Don't trust that site unless it explains why it's doing an odd thing.)</p>
<p>Increasing the number of training epochs, from the default <code>epochs=5</code> to something like <code>10</code> or <code>20</code> may also help make run-to-run results more consistent, especially if you don't have plentiful training data.</p>
",0,0,170,2022-08-04 05:40:46,https://stackoverflow.com/questions/73230837/can-doc2vec-training-result-could-change-with-same-input-data-and-same-paramete
What is the purpose of Tags in Doc2Vec TaggedDocument?,"<p>Is it to aid in classification tasks? The [docs][1] and tutorials don't explain this; they seem to assume a level of understanding that I don't have. These SO answers get near it do not say explicitly:</p>
<ul>
<li><a href=""https://datascience.stackexchange.com/questions/10216/doc2vec-how-to-label-the-paragraphs-gensim"">https://datascience.stackexchange.com/questions/10216/doc2vec-how-to-label-the-paragraphs-gensim</a></li>
<li><a href=""https://stackoverflow.com/questions/46083322/multiple-tags-for-single-document-in-doc2vec-taggeddocument"">Multiple tags for single document in doc2vec. TaggedDocument</a></li>
</ul>
","gensim, doc2vec","<p>The 'tag' is just the key with which to look-up the learned document vector, after training is done.</p>
<p>The original 'Paragraph Vectors' research papers, on which Gensim's <code>Doc2Vec</code> is based, tended to just assume each document had one unique ID – perhaps, a string token just like any other word. (So, too, did a small patch to the original Google <code>word2vec.c</code> that was once <a href=""https://groups.google.com/g/word2vec-toolkit/c/Q49FIrNOQRo/m/J6KG8mUj45sJ"" rel=""nofollow noreferrer"">shared</a>, long ago, as a limited example of one mode of 'paragraph vectors`.)</p>
<p>In those original formulations, documents had just one unique ID – lookup key for their vector.</p>
<p>However, it was a fairly obvious/straightforward extension to allow these associated vectors to potentially map to other known shared labels, across many documents. (That is, not a unique vector per document, but a unique vector per label, which might appear on multiple texts.) And further, that multiple such range-of-text vectors might be relevant to a single text, that's known to deserve more-than-one label.</p>
<p>So the word 'tag' was used in the Gensim implementation, to convery that this is an association more general than either a unique-ID, or a known-label, though it might in some cases be either.</p>
<p>If you're just starting out, or trying to match early papers, just consider the 'tag' a single unique ID per document. Give every independent document its own unique name – whether it's something natural from your data source (like a unique article title or primary key), or a mere serial number, from <code>'0'</code> to the count of docs in your data.</p>
<p>Only if you're trying expert/experimental other approaches, after understanding the basic approach, would you want to either repeat a 'tag' across multiple documents, or use mroe than one 'tag' per document. Neither to those approaches are necessary, or typical, in the initial application of <code>Doc2Vec</code>.</p>
<p>(And if you start to re-use known tags in training, <code>Doc2Vec</code> is no longer a strictly 'unsupervised' machine-learning technique, but starts to behave more like a 'supervised' or 'semi-supervised' technique, where you're nudging the algorithm towards desired answers. That's sometimes useful, and appropriate, but starts to complicate estimates of how well your steps are working: you then have to use things like held-back test/validation data to get trustworthy estimates of your system's success.)</p>
",1,0,752,2022-08-05 19:54:58,https://stackoverflow.com/questions/73254708/what-is-the-purpose-of-tags-in-doc2vec-taggeddocument
Azure blob storage model access for gensim in python,"<p>I am trying  to load my model files using below code</p>
<pre><code>import gensim
import os
from azure.storage.blob import BlobServiceClient
from smart_open import open

azure_storage_connection_string = &quot;DefaultEndpointsProtocol=https;AccountName=lnipcfdevlanding;AccountKey=xxxxxxxxx&quot;
client = BlobServiceClient.from_connection_string(azure_storage_connection_string)
file_prefix=&quot;azure://landing/TechnologyCluster/VectorCreation/embeddings/&quot;
fin = open(file_prefix+&quot;word2vec.Tobacco.fasttext.model&quot;, transport_params=dict(client=client))
clustering.embedding = gensim.models.Word2Vec.load(fin)
</code></pre>
<p>But it is failing with below error
AttributeError: '_io.TextIOWrapper' object has no attribute 'endswith'</p>
<p>I assume the way I  am passing file to  gensim.models.Word2Vec.load is not the right  way. I could not  find any good  example that how to pass  the filename  which is on Azure blob storage,  if I give complete uri it  does not work, what is the  right way to achieve  this  ?</p>
","python, azure-storage, gensim","<p>Please check :</p>
<p><strong><code>AttributeError</code></strong> usually occurs when <code>save()</code> or <code>load()</code> function is called on an <em><strong><code>object instance instead of class</code></strong></em> as that is a class method..</p>
<p>Please note that the information in file can be  incomplete ,check if the binary tree is missing.In this case, the query may be obtained ,but you may not be able to continue with a <strong>model</strong> loaded with  this way.</p>
<blockquote>
<p>Check if the file path must be saved in <code>word2vec-format</code> file</p>
</blockquote>
<p>binary is bool, if True, indicates data is in binary word2vec format.</p>
<p><strong>Note</strong> from: <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html</a>.</p>
<p>Check if this way can be worked around by <code>importing</code> required models and by checking version compliance : -</p>
<p><code>gensim.models.Word2Vec.load_word2vec_format('model', binary=True)</code></p>
<p>or with<code>KeyedVectors.load</code>  in place of  <code> .Word2Vec.load_...</code> according to what supports fasttext.</p>
<blockquote>
<p>Also check whether the model is correctly supports the
function.</p>
</blockquote>
<p><strong>References:</strong></p>
<ol>
<li><a href=""https://stackoverflow.com/questions/59333165/attributeerror-word2vec-object-has-no-attribute-endswith"">python - AttributeError: 'Word2Vec' object has no attribute
'endswith' - Stack Overflow</a></li>
<li><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#why-use-keyedvectors-instead-of-a-full-model"" rel=""nofollow noreferrer"">models.keyedvectors – Store and query word vectors — gensim
(radimrehurek.com)</a></li>
</ol>
",0,0,141,2022-08-18 19:18:03,https://stackoverflow.com/questions/73408372/azure-blob-storage-model-access-for-gensim-in-python
deleting stopwords with Gensim,"<p>I'm trying to learn Gensim using its site.
There is a function named 'remove_stopword_tokens' which is useful for my research.
Now, although the module is defined and is present on their website (exact link: <a href=""https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.remove_stopword_tokens"" rel=""nofollow noreferrer"">link</a>),I can't import it on my colab</p>
<p>Note: This is my code:</p>
<pre class=""lang-py prettyprint-override""><code>import gensim
from gensim.parsing.preprocessing import remove_stopword_tokens

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-2-dbd838c83237&gt; in &lt;module&gt;
----&gt; 1 from gensim.parsing.preprocessing import remove_stopword_tokens

ImportError: cannot import name 'remove_stopword_tokens' from 'gensim.parsing.preprocessing' (/usr/local/lib/python3.7/dist-packages/gensim/parsing/preprocessing.py)

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
&quot;Open Examples&quot; button below.
</code></pre>
","gensim, stop-words","<p><strong>updated &amp; corrected answer</strong></p>
<p>You've run into a limitation of Google Colab - it may not have the most-recent version of libraries.</p>
<p>You can see this by checking what the value of <code>gensim.__version__</code> is. In my check of Google Colab right now (September 2022), it reports <code>3.6.0</code> – a version of Gensim that's about 4 years old, and lacks later fixes &amp; addtions. The <code>remove_stopwords_tokens()</code> function was only added recently.</p>
<p>Fortunately, you can update the <code>gensim</code> package backing the Colab notebook yourself, using a shell-escape to run <code>pip</code>. Inside a Colab cell, run:</p>
<pre><code>!pip install gensim -U
</code></pre>
<p>If you'd already done an <code>import gensim</code>, it will warn you that you must restart the runtime for the new code to be found.</p>
<p>Note that for clarity reasons you might choose to prefer using more-specific imports, as many project style guides suggest, rather than doing any broad top-level <code>import gensim</code> at all. Just mport the individual classes and/or functions you need, specifically &amp; explicitly. That is, just:</p>
<pre><code>from gensim.parsing.preprocessing import remove_stopword_tokens
# ... other exact class/function/variable imports you'll use...

remove_stopword_tokens(sentence)
</code></pre>
<p>On the other hand, if you want things simple-but-sloppy (not recommended), once you <code>import gensim</code>, it has already (via its own custom initialization routines) imported all of its submodules for you.  So you <em>could</em> do:</p>
<pre><code>import gensim  # parsing &amp; all gensim's other submodules now referenceable!

gensim.parsing.remove_stopword_tokens(sentence)
</code></pre>
<p>(Pro Python programmer style tends <em>not</em> to do this latter approach, of prefixing all in-the-actual-code calls with long dot-paths.)</p>
",1,0,1042,2022-09-08 17:50:30,https://stackoverflow.com/questions/73653227/deleting-stopwords-with-gensim
How to extract matrix together with vocab from gensim word2vec model,"<p>I've trained a word2vec model like so</p>
<pre><code>from gensim.models import Word2Vec

# create model without initializing
model = Word2Vec(min_count=20,
                 window=5,
                 sample=6e-5, 
                 negative=20,
                 workers=cores-1,
                 vector_size=300)

# build vocabulary
w2v_model.build_vocab(sentences, progress_per=10000)

# train model
model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)

</code></pre>
<p>I'd like to export the model as a dataframe, but not sure how to extract the matrix and vocab together correctly, with the right index positions.</p>
<p>Something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>label</th>
<th>V1</th>
<th>V2</th>
<th>V...</th>
</tr>
</thead>
<tbody>
<tr>
<td>government</td>
<td>0.560774564</td>
<td>-0.0464625023</td>
<td>...</td>
</tr>
<tr>
<td>state</td>
<td>0.0106112240</td>
<td>0.0464625023</td>
<td>...</td>
</tr>
<tr>
<td>....</td>
<td>...</td>
<td>..</td>
<td>.</td>
</tr>
</tbody>
</table>
</div>
<p>I've tried this:</p>
<pre><code>tmp = pd.DataFrame(model.syn1neg)
tmp.insert(0, 'label', model.wv.index_to_key)
</code></pre>
<p>which does not square up when comparing</p>
<pre><code>&gt;&gt;&gt; model.wv.get_index('government')
10
&gt;&gt;&gt; tmp.loc[[0]]
0 government 0.329972  0.160003 -0.516633  ...  0.460873 -0.170273 -1.621128  1.255289
</code></pre>
","python, extract, gensim, word2vec","<p>For anyone else looking for a solution to this with gensim 4.x.x here's what I wound up doing:</p>
<pre><code>vocab, vectors = model.wv.key_to_index, model.wv.vectors

# get label and vector index.
label_index = np.array([(voc[0], voc[1]) for voc in vocab.items()])

# init dataframe using embedding vectors and set index as node name
tmp =  pd.DataFrame(vectors[label_index[:,1].astype(int)])
tmp.index = label_index[:, 0]
tmp.to_csv(&quot;matrix_with_labels.csv&quot;)

</code></pre>
<p>Not sure this is the best or proper way but it works.</p>
",1,0,809,2022-09-19 16:08:54,https://stackoverflow.com/questions/73776321/how-to-extract-matrix-together-with-vocab-from-gensim-word2vec-model
Sorted document topic matrix gensim LDA,"<p>I have a corpus that I ran LDA on using gensim, and I'm trying to get a matrix in which rows are documents and columns are topics. I ran used the line of code below, but in the output, scores don't correspond to columns. I want to change this so that in the 0 column, you only have the probability of topic 0, likewise in the 1, 2, etc. columns.</p>
<p>Does anyone know how to do this?</p>
<pre><code>DocTopMat = pd.DataFrame(model.get_document_topics(corpus),columns=[i for i in range(model.num_topics)])
</code></pre>
<p><a href=""https://i.sstatic.net/eHBlQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eHBlQ.jpg"" alt=""enter image description here"" /></a></p>
","python, nlp, gensim, lda, topic-modeling","<p>I'm assuming as of now, the data that you have is in tuples; so try this to get the probabilities in the respective columns;</p>
<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({0:[(1,0.22),(0,0.08),(1,0.34),(1,0.87),(0,0.37)],
                   1:[(2,0.78),(1,0.92),(2,0.66),(3,0.13),(2,0.34)],
                   2:[np.nan,np.nan,np.nan,np.nan,(3,0.28)],
                   3:[np.nan,np.nan,np.nan,np.nan,np.nan],
                   4:[np.nan,np.nan,np.nan,np.nan,(4,0.01)]})

df.fillna(&quot;na&quot;,inplace=True)

df[&quot;topic_0&quot;] = df[[0,1,2,3,4]].apply(lambda x: sum([i[1] for i in x if i[0] == 0]),axis=1)# if i[0] == 0 else np.nan],axis=1)
df[&quot;topic_1&quot;] = df[[0,1,2,3,4]].apply(lambda x: sum([i[1] for i in x if i[0] == 1]),axis=1)# if i[0] == 0 else np.nan],axis=1)
df[&quot;topic_2&quot;] = df[[0,1,2,3,4]].apply(lambda x: sum([i[1] for i in x if i[0] == 2]),axis=1)# if i[0] == 0 else np.nan],axis=1)
df[&quot;topic_3&quot;] = df[[0,1,2,3,4]].apply(lambda x: sum([i[1] for i in x if i[0] == 3]),axis=1)# if i[0] == 0 else np.nan],axis=1)
df[&quot;topic_4&quot;] = df[[0,1,2,3,4]].apply(lambda x: sum([i[1] for i in x if i[0] == 4]),axis=1)# if i[0] == 0 else np.nan],axis=1)
</code></pre>
<p>Output of df;</p>
<p><a href=""https://i.sstatic.net/7hsYA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7hsYA.png"" alt=""enter image description here"" /></a></p>
",0,0,199,2022-09-30 09:20:03,https://stackoverflow.com/questions/73906538/sorted-document-topic-matrix-gensim-lda
Converting word to vector using GloVe,"<p>I loaded my glove package as follows:</p>
<pre><code>import gensim.downloader as api
model = api.load(&quot;glove-wiki-gigaword-100&quot;)
</code></pre>
<p>and would want to create a function where I pass in a word and the GloVe model, and it will return the corresponding vector, for instance,</p>
<pre><code>def convert_word_to_vec(word, model):
</code></pre>
<p>and when I pass in <code>convert_word_to_vec(lol, model)</code> it will return the vectors for the word <code>lol</code></p>
<p>Is there a way around this? Thank you!</p>
","python, deep-learning, stanford-nlp, gensim","<p>Usage:</p>
<pre><code>import gensim.downloader as api
model = api.load(&quot;glove-wiki-gigaword-100&quot;)

vector = model['lol']
print(vector)  # array with shape (100,)
</code></pre>
<p>Please check the documentations for more options:</p>
<p><a href=""https://radimrehurek.com/gensim/models/keyedvectors.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/keyedvectors.html</a></p>
",0,0,611,2022-09-30 18:28:51,https://stackoverflow.com/questions/73912673/converting-word-to-vector-using-glove
Gensim Word2Vec exhausting iterable,"<p>I'm getting the following prompt when calling model.train() from gensim word2vec</p>
<pre><code>INFO : EPOCH 0: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s
</code></pre>
<p>The only solutions I found on my search for an answer point to the <em>itarable</em> vs <em>iterator</em> difference, and at this point, I tried everything I could to solve this on my own, currently, my code looks like this:</p>
<pre><code>class MyCorpus:
    def __init__(self, corpus):
        self.corpus = corpus.copy()

    def __iter__(self):
        for line in self.corpus:
            x = re.sub(&quot;(&lt;br ?/?&gt;)|([,.'])|([^ A-Za-z']+)&quot;, '', line.lower())
            yield utils.simple_preprocess(x)

sentences = MyCorpus(corpus)
w2v_model = Word2Vec(
    sentences = sentences,
    vector_size = w2v_size, 
    window = w2v_window, 
    min_count = w2v_min_freq, 
    workers = -1
    )

</code></pre>
<p>The <code>corpus</code> variable is a list containing sentences, and each sentence is a string.</p>
<p>I tried the numerous &quot;tests&quot; to see if my class is indeed iterable, like:</p>
<pre><code>    print(sum(1 for _ in sentences))
    print(sum(1 for _ in sentences))
    print(sum(1 for _ in sentences))
</code></pre>
<p>For instance, all of them suggest that my class is iterable, so at this point, I think the problem must be something else.</p>
","python, nlp, gensim, word2vec","<p><code>workers=-1</code> is not a supported value for Gensim's <code>Word2Vec</code> model; it essentially means you're using no threads.</p>
<p>Instead, you must specify the actual number of worker threads you'd like to use.</p>
<p>When using an iterable corpus, the optimal number of workers is usually some number up to your number of CPU cores, but not higher than 8-12 if you've got 16+ cores, because of some hard-to-remove inefficiencies in both the Python's Global Interpreter Lock (&quot;GIL&quot;) and the Gensim master-reader-thread approach.</p>
<p>Generally, also, you'll get better throughput if your iterable isn't doing anything expensive or repetitive in its preprocessing - like any regex-based tokenization, or a tokenization that's repeated on every epoch. So best to do such preprocessing <em>once</em>, writing the resulting  simple space-delimited tokens to a new file. Then, read that file with a very-simple, no-regex, space-splitting only tokenization.</p>
<p>(If performance becomes a major concern on a large dataset, you can also look into the alternate <code>corpus_file</code> method of specifying your corpus. It expects a single file, where each text is on its own line, and tokens are already just space-delimited. But it then lets every worker thread read its own range of the file, with far less GIL/reader-thread bottlenecking, so using <code>workers</code> equal to the CPU core count is then roughly optimal for throughput.)</p>
",2,2,131,2022-10-06 02:11:50,https://stackoverflow.com/questions/73968024/gensim-word2vec-exhausting-iterable
Tokenizing a Gensim dataset,"<p>Im trying to tokenize a gensim dataset, which I've never worked with before and Im not sure if its a small bug or im not doing it properly.</p>
<p>I loaded the dataset using</p>
<pre><code>model = api.load('word2vec-google-news-300')
</code></pre>
<p>and from my understanding, to tokenize using nltk all I need to do it call</p>
<pre><code>tokens = word_tokenize(model)
</code></pre>
<p>However, the error im getting is &quot;TypeError: expected string or bytes-like object&quot;. What am I doing wrong?</p>
","python, tokenize, gensim","<p><code>word2vec-google-news-300</code> isn't a dataset that's appropriate to 'tokenize'; it's the pretrained <code>GoogleNews</code> word2vec model released by Google circa 2013 with 3 million word-vectors. It's got lots of word-tokens, each with a 300-dimensional vector, but no multiword texts needing tokenization.</p>
<p>You can run <code>type(model)</code> on the object that <code>api.load()</code> returns to see its Python type, which will offer more clues as to what's appropriate to do with it.</p>
<p>Also, something like <code>nltk</code>'s <code>word_tokenize()</code> appears to take a single string; you'd typically not pass it any full large dataset, in one call, in any case. (You'd be more likely to iterate over many individual texts as strings, tokenizing each in turn.)</p>
<p>Rewind a bit &amp; think more about what kind of dataset you're looking for.</p>
<p>Try to get it in a simple format you can inspect it yourself, as files, before doing extra steps. (Gensim's <code>api.load()</code> is really bad/underdocumented for that, returning who-knows-what depending on what you've requested.)</p>
<p>Try building on well-explained examples that already work, making minimal individual changes that you understand individually, checking continued proper operation after each step.</p>
<p>(Also, for future SO questions that may be any more complicated than this: it's usually best to include the full error message you've received, including all lines of 'traceback' context showing involved files and lines-of-code, in order to better point at relevant lines-of-code in your code, or the libraries you're using, that are most-directly involved.)</p>
",0,0,404,2022-10-22 21:11:01,https://stackoverflow.com/questions/74167136/tokenizing-a-gensim-dataset
Why Doc2vec is slower with multiples cores rather than one?,"<p>I'm trying to train multiple &quot;documents&quot; (here mostly log format), and the Doc2Vec is taking longer if I'm specifying more than one core (which i have).</p>
<p>My data looks like this:</p>
<pre><code>print(len(train_corpus))

7930196
</code></pre>
<pre><code>print(train_corpus[:5])

[TaggedDocument(words=['port', 'ssh'], tags=[0]),
 TaggedDocument(words=['session', 'initialize', 'by', 'client'], tags=[1]),
 TaggedDocument(words=['dfs', 'fsnamesystem', 'block', 'namesystem', 'addstoredblock', 'blockmap', 'update', 'be', 'to', 'blk', 'size'], tags=[2]),
 TaggedDocument(words=['appl', 'selfupdate', 'component', 'amd', 'microsoft', 'windows', 'kernel', 'none', 'elevation', 'lower', 'version', 'revision', 'holder'], tags=[3]),
 TaggedDocument(words=['ramfs', 'tclass', 'blk', 'file'], tags=[4])]
</code></pre>
<p>I have 8 cores available:</p>
<pre><code>print(os.cpu_count())

8
</code></pre>
<p>I am using gensim 4.1.2, on Centos7. Using this approch (stackoverflow.com/a/37190672/130288), It looks like my BLAS library is OpenBlas, so I setted <strong>OPENBLAS_NUM_THREADS=1</strong> on my bashrc (and could be visible from Jupyter, using !echo $OPENBLAS_NUM_THREADS=1 )</p>
<p>This is my test code :</p>
<pre><code>dict_time_workers = dict()
for workers in range(1, 9):

    model =  Doc2Vec(vector_size=20,
                    min_count=1,
                    workers=workers,
                    epochs=1)
    model.build_vocab(train_corpus, update = False)
    t1 = time.time()
    model.train(train_corpus, epochs=1, total_examples=model.corpus_count) 
    dict_time_workers[workers] = time.time() - t1
</code></pre>
<p>And the variable <code>dict_time_workers</code> is equal too :</p>
<pre><code>{1: 224.23211407661438, 
2: 273.408652305603, 
3: 313.1667754650116, 
4: 331.1840877532959, 
5: 433.83785605430603,
6: 545.671571969986, 
7: 551.6248495578766, 
8: 548.430994272232}
</code></pre>
<p>As you can see, the time taking is increasing instead of decreasing. Results seems to be the same with a bigger epochs parameters.
Nothing is running on my Centos7 except this.</p>
<p>If I look at what's happening on my threads using <strong>htop</strong>, I see that the right number of thread are used for each training. But, the more threads are used, less the percentage of usage is (for example, with only one thread, 95% is used, for 2 they both used around 65% of their max power, for 6 threads are 20-25% ...). I suspected an IO issue, but <strong>iotop</strong> showed me that nothing bad is happening at the same disk.</p>
<p>The post seems now to be related to this post
<a href=""https://stackoverflow.com/questions/57532018/not-efficiently-to-use-multi-core-cpu-for-training-doc2vec-with-gensim"">Not efficiently to use multi-Core CPU for training Doc2vec with gensim</a> .</p>
","multithreading, machine-learning, centos7, gensim, doc2vec","<p>When getting <em>no</em> benefit from extra cores like that, it's likely that the BLAS library you've got installed is already configured to try to use all cores for every bulk array operation. That means that other attempts to engage more cores, like Gensim's <code>workers</code> specification, just increase the overhead of contention, when each individual worker thread's individual BLAS callouts also try to use 8 threads.</p>
<p>Depending on the BLAS library in use, its own propensity to use more cores can typically be limited by environment variables named something like <code>OPENBLAS_NUM_THREADS</code> and/or <code>MKL_NUM_THREADS</code>.</p>
<p>If you set these to just <code>1</code> before your process launches, you may see different, and possibly better, multithreaded behavior.</p>
<p>Note, though: <code>1</code> just restores the assumption that every worker-thread only ever engages a single core. Some other mix of BLAS-cores &amp; Gensim-worker-threads might actually achieve the best training throughput &amp; non-contending core-utilization.</p>
<p>And, at least for Gensim <code>workers</code>, the actual thread count value achieving the best throughput will vary based on other model parameters that influence the relative amount of calculation time in highly-parallelizable code-blocks versus highly-contended blocks, especially <code>window</code>, <code>vector_size</code>, &amp; <code>negative</code>. And, there's not really a shortcut to finding the best <code>workers</code> value except via trial-and-error: observing reported training rates in logs over a few minutes of running. (Though: any rate observed in, say, minutes 2-4 of a abbreviated trial run should be representative of the training rate through the whole corpus over multiple epochs.)</p>
<p>(For any system with at least 4 cores, the optimal value with a classic iterable corpus of <code>TaggedDocuments</code> is usually at least 3, no more than the number of cores, but also rarely more than 8-12 threads, due to <em>other</em> inherent sources of contention due to both Gensim's approach to fanning out the work among worker-threads, and the Python 'GIL'.)</p>
<p>Other thoughts:</p>
<ul>
<li>the <code>build_vocab()</code> step is <em>never</em> multi-threaded, so benchmarking alternate <code>workers</code> values will give a truer readout of their effect by only timing the <code>train()</code> step</li>
<li>ensuring your iterable corpus does as little redundant work (like say IO &amp; tokenization) on each pass can help limit any bottlenecks around the single manager thread doing each epoch's iteration &amp; batching texts to the workers</li>
<li>the alternate <code>corpus_file</code> approach can achieve higher core utilization, up to any number of cores, by assigning each thread its own exclusive range of an input-file. But, it also means (a) your whole corpus must be in one uncompressed space-tokenized plain-text file; (b) your documents only get a single integer <code>tag</code> (their line-number); (c) you may be subject to some small as-yet-diagnosed-and-fixed bug(s). (See <a href=""https://github.com/RaRe-Technologies/gensim/issues/2757"" rel=""nofollow noreferrer"">project issue #2747</a>.)</li>
</ul>
",2,0,152,2022-10-27 14:10:52,https://stackoverflow.com/questions/74223504/why-doc2vec-is-slower-with-multiples-cores-rather-than-one
Why isn&#39;t my Gensim fastText model continuing to train on a new corpus?,"<p>I am trying to continue training a fastText model with Gensim, using my own corpus of text.</p>
<p>I've followed along with the documentation here:
<a href=""https://radimrehurek.com/gensim/models/fasttext.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/fasttext.html</a></p>
<p>And I have written the following code:</p>
<p>First, create a small corpus:</p>
<pre><code>corpus = [
    &quot;The brown dog jumps over the kangaroo&quot;,
    &quot;I want to ride my bicycle to Mount Everest&quot;,
    &quot;What a lovely day it is&quot;,
    &quot;When I Wagagamagga, everybody stops to listen&quot;
]

corpus = [sentence.split() for sentence in corpus]
</code></pre>
<p>And then load a testing model:</p>
<pre><code>from gensim.models.fasttext import load_facebook_model
from gensim.test.utils import datapath

model = load_facebook_model(datapath(&quot;crime-and-punishment.bin&quot;))
</code></pre>
<p>Then I do a check to see if the model knows my weird new word in the corpus:</p>
<pre><code>'Wagagamagga' in model.wv.key_to_index
</code></pre>
<p>Which returns False.</p>
<p>Then I try to continue the training:</p>
<pre><code>model.build_vocab(corpus, update=True)
model.train(corpus, total_examples=len(corpus), epochs=model.epochs)
</code></pre>
<p>The model should know about my weird new word now, but this returns False, when I am expecting it to return True:</p>
<pre><code>'Wagagamagga' in model.wv.key_to_index
</code></pre>
<p>What have I missed?</p>
","python, nlp, gensim, fasttext","<p>Models generally have a <code>min_count</code> value of at least 5 - meaning words with fewer occurrences are ignored. Discarding the rarest words typically <em>improves</em> model quality, as both:</p>
<ol>
<li>such rare words have too few usage examples to get a good vector themselves; and further…</li>
<li>by pushing surrounding words outside each others' windows, and spending training cycles &amp; internal-weight updates on a vector that still won't be good, they make other word-vectors worse</li>
</ol>
<p>With larger training data, increasing the <code>min_count</code> even higher makes sense.</p>
<p>So, your problem is likely because a single occurence of that word is insufficient to make it a tracked word. Using a larger, varied corpus with multiple contrasting usage examples, at least as many as the <code>model.min_count</code> value, would be the best fix.</p>
<p>Separately: note that it is always better to train a model with all data at the same time.</p>
<p>Incremental updates will execute, but introduce thorny issues of relative balance between older &amp; newer sessions. To the extent a new session uses only a subset of words and representative word-usages, those words-included can be nudged by training <em>out</em> of comparable alignment with words only known in earlier sessions.</p>
<p>So if trying incremental updates, make sure your quality-evaluations are strong enough to detect if the model is actually improving, or gettings worse, on your real goals.</p>
",1,0,292,2022-11-03 17:19:32,https://stackoverflow.com/questions/74307142/why-isnt-my-gensim-fasttext-model-continuing-to-train-on-a-new-corpus
Problem with training Word2Vec after opening csv,"<p>I'm trying to train Word2Vec model. When I try to train the model directly from the Series I get everything is fine, but when I save the DataFrame to csv and then open it, I have a problem.</p>
<pre><code>data = pd.read_csv('test.txt', sep='\r\n', names=['input'], engine=&quot;python&quot;)
data = data.dropna().drop_duplicates()
data = data['input'].iloc[:1000].apply(lemmatize)
print(data)
</code></pre>
<p>So I get this Series:</p>
<pre><code>0                       [two, households, alike, dignity]
1                          [in, fair, verona, lay, scene]
2             [from, ancient, grudge, break, new, mutiny]
3       [where, civil, blood, makes, civil, hands, unc...
4                  [from, forth, fatal, loins, two, foes]
                              ...                        
1152    [and, therefore, thou, mayst, think, havior, l...
1153              [but, trust, gentleman, i, prove, true]
1154                              [than, coying, strange]
1155                       [i, strange, i, must, confess]
1156             [but, thou, overheard, st, ere, i, ware]
Name: input, Length: 911, dtype: object
</code></pre>
<p>When I try to train Word2Vec directly from this Series, everything is fine:</p>
<pre><code>w2v_model = Word2Vec(
    min_count=10,
    window=2,
    vector_size=300,
    negative=10,
    alpha=0.03,
    min_alpha=0.0007,
    sample=6e-5,
    sg=1)

w2v_model.build_vocab(data)
w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)
print(w2v_model.wv.key_to_index)
</code></pre>
<pre><code>{'i': 0, 'thou': 1, 'and': 2, 'love': 3, 'romeo': 4, 'shall': 5, 'what': 6, 'fair': 7, 'thee': 8, 'but': 9, 'to': 10, 'thy': 11, 'tis': 12, 'the': 13, 'come': 14, 'o': 15, 'night': 16, 'a': 17, 'sampson': 18, 'that': 19, 'capulet': 20, 'montague': 21, 'servingman': 22, 'good': 23, 'gregory': 24, 'one': 25, 'lady': 26, 'my': 27, 'go': 28, 'he': 29, 'nurse': 30, 'she': 31, 'you': 32, 'let': 33, 'would': 34, 'well': 35, 'say': 36, 'sir': 37, 'this': 38, 'for': 39, 'enter': 40, 'old': 41, 'take': 42, 'ay': 43, 'hath': 44, 'benvolio': 45, 'of': 46, 'tell': 47, 'art': 48, 'mercutio': 49, 'know': 50, 'by': 51, 'light': 52, 'see': 53, 'eyes': 54, 'juliet': 55, 'er': 56, 'must': 57, 'word': 58, 'name': 59, 'man': 60, 'men': 61, 'wilt': 62, 'nay': 63, 'peace': 64, 'much': 65, 'it': 66, 'yet': 67, 'house': 68, 'upon': 69, 'which': 70, 'as': 71, 'doth': 72, 'think': 73, 'in': 74}
</code></pre>
<p>But when I try to save the DataFrame to csv and then open it and train Word2Vec, I get this:</p>
<pre><code>data = pd.read_csv('test.txt', sep='\r\n', names=['input'], engine=&quot;python&quot;)
data = data.dropna().drop_duplicates()
data = data['input'].iloc[:1000].apply(lemmatize)
data = data.dropna()
data.to_csv('test.csv', index=False)
</code></pre>
<pre><code>data = pd.read_csv('test.csv')['input']
w2v_model = Word2Vec(
    min_count=10,
    window=2,
    vector_size=300,
    negative=10,
    alpha=0.03,
    min_alpha=0.0007,
    sample=6e-5,
    sg=1)
w2v_model.build_vocab(data)
w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)
print(w2v_model.wv.key_to_index)
</code></pre>
<pre><code>{&quot;'&quot;: 0, ',': 1, ' ': 2, 'e': 3, 'a': 4, 't': 5, 'o': 6, 's': 7, 'r': 8, 'i': 9, 'n': 10, 'h': 11, 'l': 12, ']': 13, '[': 14, 'd': 15, 'u': 16, 'm': 17, 'g': 18, 'c': 19, 'w': 20, 'y': 21, 'p': 22, 'f': 23, 'b': 24, 'v': 25, 'k': 26, 'j': 27, 'q': 28, 'x': 29, 'z': 30}
</code></pre>
<p>Series after opening:</p>
<pre><code>0              ['two', 'households', 'alike', 'dignity']
1               ['in', 'fair', 'verona', 'lay', 'scene']
2      ['from', 'ancient', 'grudge', 'break', 'new', ...
3      ['where', 'civil', 'blood', 'makes', 'civil', ...
4      ['from', 'forth', 'fatal', 'loins', 'two', 'fo...
                             ...                        
906    ['and', 'therefore', 'thou', 'mayst', 'think',...
907    ['but', 'trust', 'gentleman', 'i', 'prove', 't...
908                        ['than', 'coying', 'strange']
909             ['i', 'strange', 'i', 'must', 'confess']
910    ['but', 'thou', 'overheard', 'st', 'ere', 'i',...
Name: input, Length: 911, dtype: object
</code></pre>
<p>What could be the problem?</p>
","python, pandas, machine-learning, gensim, word2vec","<p>First, it'd help to name the variable holding data that's come from a different place different from the original data, for clarity of reference/comparison.</p>
<p>For example, instead of loading your saved data as...</p>
<pre><code>data = pd.read_csv('test.csv')['input']
</code></pre>
<p>...give it a distinctive name instead:</p>
<pre><code>data_from_csv = pd.read_csv('test.csv')['input']
</code></pre>
<p>Then, check whether your original <code>Series</code> <code>data</code> &amp; later <code>data_from_csv</code> actually look the same, as the exact same type of objects in each item of the iterable corpus, to <code>Word2Vec</code>.</p>
<p>For example, to look at the 1st item in each iterable object, look at:</p>
<pre><code>print(next(iter(data)))
</code></pre>
<p>…and also for comparison…</p>
<pre><code>print(next(iter(data_from_csv)))
</code></pre>
<p>If these aren't identical, then your second <code>data_from_csv</code> case <em>isn't</em> showing <code>Word2Vec</code> the same type of corpus. In particular, each individual text item in the <code>Word2Vec</code> training corpus should be a Python <em>list</em> of individual string tokens.</p>
<p>If you instead pass it only strings, it will instead see each individual item in that string (single characters) as if they were string tokens, resulting the problem you've described: all the model's known words are single-characters.</p>
<p>(Are you sure you didn't see a WARNING in your logs/console-output to that effect, on the 2nd run? The <code>.build_vocab()</code> step checks if the 1st item in the corpus is a plain-string, rather than the proper list, and prints a warning when it is.)</p>
<p>Make sure to do one or the other of:</p>
<p>(1) write the CSV as space-delimited texts – not Python <code>list</code> literals – then re-<code>.split()</code> into a list after CSV-reading the raw strings; or</p>
<p>(2) write the CSV as Python list literals – eg with brackets and string-quoting – but then also, after loading those literals as raw strings, interpret them as Python objects (as if using an <code>eval()</code>) to turn them back into lists-of-tokens</p>
<p>Then, the corpus you're feeding to <code>Word2Vec</code> will be of the right format to get the intended individual words. (Option (1) above is usually the preferred approach: space-delimited strings are a simpler/safer/faster format to later re-parse/split, with no risk that and <code>eval()</code> could potentially run unintended code.)</p>
<p>(As a totally separate issue: using odd non-default values like <code>alpha=0.03, min_alpha=0.0007</code> usually indicates you're following a <em>bad</em> tutorial, that's changed these for no good reason. Such a change is unlikely to either hurt or help your results much - it's just an odd &amp; unnecessary choice hinting at random guesswork &amp; unthinking copying rather than true understanding.)</p>
",1,0,53,2022-11-10 17:16:06,https://stackoverflow.com/questions/74392926/problem-with-training-word2vec-after-opening-csv
Metrics for monitoring LDA Model,"<p>We use LDA for topic-modelling in production. I was wondering if there are any metrics which we could use to monitor the quality of this model to understand when model starts to perform poorly and we need to retrain it (for example,if we have too many new topics).</p>
<p>We consider to calculate the ratio of number of words from top-topic(topic which has the highest probability for a document) corpus,which were found in the document, to the general number of words(after all processing) in the document with some theshold, but may be someone can share their experience.</p>
","monitoring, metrics, gensim, lda, mlops","<p>You can calculate its coherence value and compare it with previous one. See <a href=""http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf"" rel=""nofollow noreferrer"">Michael Roeder, Andreas Both and Alexander Hinneburg: “Exploring the space of topic coherence measures</a>, and if you're using <code>gensim</code> with python, check its implementation at <a href=""https://radimrehurek.com/gensim/models/coherencemodel.html"" rel=""nofollow noreferrer""><code>CoherenceModel</code></a>.</p>
",0,0,107,2022-11-15 11:32:12,https://stackoverflow.com/questions/74444916/metrics-for-monitoring-lda-model
Gensim on Google Colab not able to import NMf,"<p>My question is specific for Gensim and Colab, not other notebook.
I am trying to apply Non-Negative Matrix factorization in Colab with Gensim.
<a href=""https://i.sstatic.net/o5zpL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/o5zpL.png"" alt=""enter image description here"" /></a></p>
<p>but I can't get the <code>models.nmf</code></p>
<p><a href=""https://i.sstatic.net/RoF4s.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RoF4s.png"" alt=""enter image description here"" /></a></p>
<p>I also read this <a href=""https://stackoverflow.com/questions/56443667/error-while-importing-gensim-package-in-colab"">question</a> without reaching my expected goal.</p>
<p>I followed these steps:</p>
<pre><code> !pip3 install gensim (and I get the attached screenshot) 
 import gensim
 from gensim import models, interfaces, utils
 from gensim.models import Nmf
</code></pre>
<p>Again I am focused on running the Non-negative Matrix factorization on <a href=""https://colab.research.google.com/"" rel=""nofollow noreferrer"">Google Colab</a>.</p>
","python, nlp, google-colaboratory, gensim","<p>Gensim's development repository history shows the <code>gensim/models/nmf.py</code> file <a href=""https://github.com/RaRe-Technologies/gensim/commits/develop/gensim/models/nmf.py"" rel=""nofollow noreferrer"">arrived in January 2019</a>, just before the <a href=""https://github.com/RaRe-Technologies/gensim/releases/tag/3.7.0"" rel=""nofollow noreferrer"">Gensim 3.7.0 release of January 18, 2019</a>.</p>
<p>If you display <code>gensim.__version__</code> in your Colab notebook, you are likely to find that Google has, by default, only made very-old Gensim 3.6.0 available. That was released September 2018, more than 4 years ago.</p>
<p>If you install a more-recent version of Gensim that actually includes the <code>Nmf</code> class, you should be able to import <code>Nmf</code> without error.</p>
<p>Supplying the <code>-U</code> flag to your install command may be enough to get the most-recent version of Gensim, which will have the most recent functionality, fixes, and optimizations: <code>!pip install gensim -U</code></p>
<p>If you've already imported an older-version gensim into your notebook, you may need to restart your notebook's Python interpreter so it recognizes the updated package is available and can import it. (I think Colab will remind you to do this when necessary.)</p>
",1,0,921,2022-12-09 19:04:28,https://stackoverflow.com/questions/74747316/gensim-on-google-colab-not-able-to-import-nmf
Converting word2vec output into dataframe for sklearn,"<p>I am attempting to use <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim's word2vec</a> to transform a column of a pandas dataframe into a vector that I can pass to a <a href=""https://scikit-learn.org/stable/supervised_learning.html"" rel=""nofollow noreferrer""><code>sklearn</code> classifier</a> to make a prediction.</p>
<p>I understand that I need to average the vectors for each row. I have tried <a href=""https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"" rel=""nofollow noreferrer"">following this guide</a> but I am stuck, as I am getting models back but I don't think I can access the underlying embeddings to find the averages.</p>
<p>Please see <a href=""https://stackoverflow.com/help/minimal-reproducible-example"">a minimal, reproducible example</a> below:</p>
<pre><code>import pandas as pd, numpy as np
from gensim.models import Word2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.feature_extraction.text import CountVectorizer

temp_df = pd.DataFrame.from_dict({'ID': [1,2,3,4,5], 'ContData': [np.random.randint(1, 10 + 1)]*5, 
                                'Text': ['Lorem ipsum dolor sit amet', 'consectetur adipiscing elit.', 'Sed elementum ultricies varius.',
                                         'Nunc vel risus sed ligula ultrices maximus id qui', 'Pellentesque pellentesque sodales purus,'],
                                'Class': [1,0,1,0,1]})
temp_df['text_lists'] = [x.split(' ') for x in temp_df['Text']]

w2v_model = Word2Vec(temp_df['text_lists'].values, min_count=1)

cv = CountVectorizer()
count_model = pd.DataFrame(data=cv.fit_transform(temp_df['Text']).todense(), columns=list(cv.get_feature_names_out()))
</code></pre>
<p>Using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer""><code>sklearn's CountVectorizer</code></a>, I am able to get a simple frequency representation that I can pass to a classifier. How can I get that same format using Word2vec?</p>
<p>This toy example produces:</p>
<pre><code>adipiscing  amet    consectetur dolor   elementum   elit    id  ipsum   ligula  lorem   ... purus   qui risus   sed sit sodales ultrices    ultricies   varius  vel
0   0   1   0   1   0   0   0   1   0   1   ... 0   0   0   0   1   0   0   0   0   0
1   1   0   1   0   0   1   0   0   0   0   ... 0   0   0   0   0   0   0   0   0   0
2   0   0   0   0   1   0   0   0   0   0   ... 0   0   0   1   0   0   0   1   1   0
3   0   0   0   0   0   0   1   0   1   0   ... 0   1   1   1   0   0   1   0   0   1
4   0   0   0   0   0   0   0   0   0   0   ... 1   0   0   0   0   1   0   0   0   0
</code></pre>
<p>While this runs without error, I cannot access the embedding that I can pass with this current format. I would like to produce the same format, with the exception of instead of there being counts, its the <code>word2vec</code> value embeddings</p>
","python, scikit-learn, nlp, gensim, word2vec","<p>While yo might not be able to help it if your original data comes from a Pandas <code>DataFrame</code>, neither Gensim nor Scikit-Learn work with <code>DataFrame</code>-style data natively. Rather, they tend to use raw <code>numpy</code> arrays, or base Python datastructures like <code>list</code>s or iterable sequences.</p>
<p>Trying to shoehorn interim raw vectors into the Pandas style of data structure tends to add code complication &amp; wasteful overhead.</p>
<p>That's especially true if the vectors are dense vectors, where essentially all of a smaller-number of dimensions are nonzero, as in word2vec-like algorthms. But that's also true if the vectors are the kinds of sparse vectors, with a giant number of dimensions, but most dimensions 0, that come from <code>CountVectorizer</code> and various &quot;bag-of-words&quot;-style text models.</p>
<p>So first, I'd recommend <em>against</em> putting the raw outputs of <code>Word2Vec</code> or <code>CountVectorizer</code>, which are usually interim representations on the way to completing some other task, into a <code>DataFrame</code>.</p>
<p>If you want to have the final assigned-labels in the <code>DataFrame</code>, for analysis or reporting in the Pandas style, only add those final outputs in the end. But to understand the interim vector representations, and then to pass them to things like Scikit-Learn classifiers in the formats those classes expect, keep those vectors (and inspect them yourself for clarity) in the their raw <code>numpy</code> vector formats.</p>
<p>In particular, after <code>Word2Vec</code> runs (with the parameters you've shown), there'll be a 100-dimensional vector <em>per word</em>. Not per multi-word text. And the 100-dimensions have no names other than their indexes 0 to 99.</p>
<p>And unlike the dimensions of the <code>CountVectorizer</code> representation, which are counts of individual words, each dimension of the &quot;dense embedding&quot; will be some floating-point decimal value that has no clear or specific interpretation alone: it's only directions/neighborhoods in the whole space, shearing across many dimensions, that vaguely correspond with useful or human-nameable concepts.</p>
<p>If you want to turn the per-word 100-dimensional vectors into vectors for a multi-word text, there are many potential ways to do so – but one simple choice is to simply average together the N word-vectors into 1 summary vector. Gensim's class holding the word-vectors inside the <code>Word2Vec</code> model, <code>KeyedVectors</code>, has a <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector"" rel=""nofollow noreferrer""><code>.get_mean_vector()</code> method</a> that can help. For example:</p>
<pre class=""lang-py prettyprint-override""><code>texts_as_wordlists = [x.split(' ') for x in temp_df['Text']]
text_vectors = [w2v_model.wv.get_mean_vector(wordlist) for wordlist in texts_as_wordlists]
</code></pre>
<p>There are many other potential ways to use word-vectors to model a longer text. For example, you might reweight the words before averagine. But a simple average is a reasonable first baseline approach. (Other algorithms related to word2vec, like the 'Paragraph Vector' algorihtm implemented by the <code>Doc2Vec</code> class, can also create a vector for a multi-word text, and such a vector is <strong>not</strong> just the average of its word-vectors.)</p>
<p>Two other notes on using <code>Word2Vec</code>:</p>
<ul>
<li>word2vec vectors only get good when trained on lots of word-usage data. Toy-sized examples trained on only hundreds, or even tens-of-thousands, of words rarely show anything useful, or anything resembling the power of this algorithm on larger data set.</li>
<li><code>min_count=1</code> is essentially always a bad idea with this algorithm. Related to the point above, the algorithm needs multiple subtly-contrasting usage examples of any word to have any chance of placing it meaningfully in the shared-coordinate space. Words with just one, or even a few, usages tend to get awful vectors not generalizable to the word's real meaning as would be evident from a larger sample of its use. And, in natural-language corpora, such few-example words are very numerous - so they wind up taking a lot of the training time, and achieving their <em>bad</em> representations actually worsens the vectors for <em>surrounding</em> words, that could be better because there are enough training examples. So, the best practice with word2vec is usually to <em>ignore</em> the rarest words – train as if they weren't even there. (The class's default is <code>min_count=5</code> for good reasons, and if that results in your model missing vectors for words you think you need, get more data showing uses of those words in real contexts, rather than lowering <code>min_count</code>.)</li>
</ul>
",2,1,1131,2022-12-12 05:48:08,https://stackoverflow.com/questions/74767053/converting-word2vec-output-into-dataframe-for-sklearn
improve gensim most_similar() return values by using wordnet hypernyms,"<pre><code>import gensim.downloader as api
glove = api.load('glove-wiki-gigaword-200')
</code></pre>
<p>I first ran this code to download the pre-trained model.</p>
<pre><code>glove.most_similar(positive=['sushi', 'uae'], negative=['japan'])
</code></pre>
<p>would then result in:</p>
<pre><code>[('nahyan', 0.5181387066841125),
 ('caviar', 0.4778318405151367),
 ('paella', 0.4497394263744354),
 ('nahayan', 0.44313961267471313),
 ('zayed', 0.4321245849132538),
 ('omani', 0.4285220503807068),
 ('seafood', 0.4279175102710724),
 ('saif', 0.426000714302063),
 ('dirham', 0.4214130640029907),
 ('sashimi', 0.4165934920310974)]
</code></pre>
<p>and in this example, we can see that the method failed to capture the 'type' or 'category' of the query. 'zayed', 'nahyan' are not actually of 'type' food and rather they represent person name.</p>
<p>The approach suggested by my professor is to use wordnet hypernyms to find the 'type'.</p>
<p>With much research, the closest solution I found is to somehow incorporate
<code>lowest_common_hypernyms()</code> that will give the lowest common hypernym between two synsets and use it to filter the results of <code>most_similar()</code>.</p>
<p>I am not sure if my idea make sense and would like the community feedback on this.</p>
<p>My idea is compute the hypernym of, e.g. 'sushi' and the hypernyms of all the similar words returned by <code>most_similar()</code> and only choose the word with 'longest' lowest common hypernym path. I expect this should return the word that best matches the 'type'</p>
<p>Not sure if it makes sense...</p>
","python, nlp, nltk, gensim, wordnet","<p>Does your proposed approach give adequate results when you try it?</p>
<p>That's the only test of whether the idea makes sense.</p>
<p>Word2vec is generally oblivious to the all the variations of category that a lexicon like WordNet can provide – all the words that are similar to another word, in any aspect, will be neighbors.  Even words that people consider opposites – like 'hot' and 'cold' – will be often be fairly close to each other, in some direction in the coordinate space, as they are similar in what they describe and what contexts they're used in. (They can be drop-in replacements for each other.)</p>
<p>Word2vec is also fairly oblivious to polysemy in its standard formulation.</p>
<p>Some other things worth trying might be:</p>
<ul>
<li>if you need only answers of a certain type, mix-in some measurement ranking candidate answers by their closeness to a word either describing that type ('food') or representing multiple examples (say an average vector for many food-names you'd know to be good answers)</li>
<li>choose another vector-set, or train your own. There's no universal &quot;goodness&quot; for word-vectors: their quality for certain tasks will vary based on their training data &amp; parameters. Vectors trained on something broader than Wikipedia (your named vector file), or some text corpus more focused on your domain-of-interest – say, food criticism – might do better on some tasks. Changing training parameters can also change which kinds of similarity are most emphasized in the resulting vectors. For example, some observers have noticed <em>small</em> context-windows tend to put words that are direct drop-in replacements for each other closer-together, while <em>larger</em> context-windows bring words from the same domains-of-use, even if not drop-in replacements of the same 'type', closer. (It sounds like your current need might be best served with a model trained with smaller windows.)</li>
</ul>
",0,1,145,2022-12-13 21:31:11,https://stackoverflow.com/questions/74791093/improve-gensim-most-similar-return-values-by-using-wordnet-hypernyms
MemoryError when extracting articles into list using gensim WikiCorpus,"<p>I wish to build a corpus from a Wikipedia Dump (~19GB compressed .bz2 file). But, I encountered MemoryError when I try to run the code as shown. Is there any solution that can solve this issue?</p>
<pre><code>import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
from gensim.corpora import WikiCorpus
import sys


def make_corpus(in_f, out_f):
    output = open(out_f, 'w')
    print(&quot;File Created!&quot;)
    wiki = WikiCorpus(in_f)
    print(&quot;Wiki Opened!&quot;)
    i = 0

    for text in wiki.get_texts():
        output.write(bytes(' '.join(text).encode('utf-8')).decode('utf-8') + '\n')
        i = i + 1
        if (i % 10000 == 0):
            print('Processed ' + str(i) + ' articles...')

    output.close()
    print('Processing Completed!')


if __name__ == '__main__':
    if len(sys.argv) !=3:
        sys.exit(1)

    in_f = sys.argv[1]
    out_f = sys.argv[2]
    make_corpus(in_f, out_f)
</code></pre>
<blockquote>
<p>Traceback (most recent call last):
File &quot;&quot;, line 1, in 
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\spawn.py&quot;, line 105, in spawn_main
exitcode = _main(fd)
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\spawn.py&quot;, line 114, in _main
prepare(preparation_data)
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\spawn.py&quot;, line 225, in prepare
<em>fixup_main_from_path(data['init_main_from_path'])
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\spawn.py&quot;, line 277, in <em>fixup_main_from_path
run_name=&quot;<strong>mp_main</strong>&quot;)
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\runpy.py&quot;, line 263, in run_path
pkg_name=pkg_name, script_name=fname)
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\runpy.py&quot;, line 96, in <em>run_module_code
mod_name, mod_spec, pkg_name, script_name)
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\runpy.py&quot;, line 85, in <em>run_code
exec(code, run_globals)
File &quot;d:\LeongJC\FYP_Code\Code\wikipedia_transformation.py&quot;, line 3, in 
import gensim
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim_<em>init</em></em>.py&quot;, line 11, in 
from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:F401
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora_<em>init</em></em>.py&quot;, line 6, in 
from .indexedcorpus import IndexedCorpus  # noqa:F401 must appear before the other classes
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\indexedcorpus.py&quot;, line 14, in 
from gensim import interfaces, utils
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\interfaces.py&quot;, line 19, in 
from gensim import utils, matutils
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\matutils.py&quot;, line 19, in 
from scipy.stats import entropy
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\stats_<em>init</em></em>.py&quot;, line 388, in 
from .stats import *
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\stats\stats.py&quot;, line 174, in 
from scipy.spatial.distance import cdist
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\spatial_<em>init</em></em>.py&quot;, line 101, in 
from .<em>procrustes import procrustes
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\spatial_procrustes.py&quot;, line 9, in 
from scipy.linalg import orthogonal_procrustes
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\linalg_<em>init</em></em>.py&quot;, line 194, in 
from .misc import *
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\linalg\misc.py&quot;, line 4, in 
from .lapack import get_lapack_funcs
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\scipy\linalg\lapack.py&quot;, line 783, in 
from scipy.linalg import _flapack
ImportError: DLL load failed: The paging file is too small for this operation to complete.
multiprocessing.pool.RemoteTraceback:
&quot;&quot;&quot;
Traceback (most recent call last):
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\pool.py&quot;, line 119, in worker
result = (True, func(*args, **kwds))
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\wikicorpus.py&quot;, line 530, in _process_article
token_max_len=token_max_len, lower=lower,
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\wikicorpus.py&quot;, line 490, in process_article
result = tokenizer_func(text, token_min_len, token_max_len, lower)
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\wikicorpus.py&quot;, line 361, in tokenize
utils.to_unicode(token) for token in utils.tokenize(content, lower=lower, errors='ignore')
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\utils.py&quot;, line 264, in tokenize
text = text.lower()
MemoryError
&quot;&quot;&quot;</p>
</blockquote>
<blockquote>
<p>The above exception was the direct cause of the following exception:<br />
Traceback (most recent call last):
File &quot;d:/LeongJC/FYP_Code/Code/wikipedia_transformation.py&quot;, line 31, in 
make_corpus(in_f, out_f)
File &quot;d:/LeongJC/FYP_Code/Code/wikipedia_transformation.py&quot;, line 11, in make_corpus
wiki = WikiCorpus(in_f)
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\wikicorpus.py&quot;, line 639, in <strong>init</strong>
self.dictionary = Dictionary(self.get_texts())
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\dictionary.py&quot;, line 78, in <strong>init</strong>
self.add_documents(documents, prune_at=prune_at)
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\dictionary.py&quot;, line 196, in add_documents
for docno, document in enumerate(documents):
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\site-packages\gensim\corpora\wikicorpus.py&quot;, line 693, in get_texts
for tokens, title, pageid in pool.imap(_process_article, group):
File &quot;C:\Users\asus\anaconda3\envs\tensorflow\lib\multiprocessing\pool.py&quot;, line 735, in next
raise value
MemoryError</p>
</blockquote>
<p><a href=""https://i.sstatic.net/PJ0xA.png"" rel=""nofollow noreferrer"">CMD Error Message1</a>
<a href=""https://i.sstatic.net/vHfjo.png"" rel=""nofollow noreferrer"">CMD Error Message2</a></p>
","nlp, gensim","<p>By default, the <code>WikiCorpus</code> class surveys the entire dump file's vocabulary upon creation, even though most users don't need that. And, it's during that step you're hitting this <code>MemoryError</code>.</p>
<p>However, if you supply an empty Python <code>dict</code> at <code>WikiCorpus</code> creation, it'll skip this time-consuming &amp; memory-consuming step. A</p>
<p>Specifically, change your line...</p>
<pre class=""lang-py prettyprint-override""><code>    wiki = WikiCorpus(in_f)
</code></pre>
<p>...to...</p>
<pre class=""lang-py prettyprint-override""><code>    wiki = WikiCorpus(in_f, dictionary={})
</code></pre>
<p>After this change, you may not have any further problems, as it looks like your code is otherwise doing things in an incremental fashion that shouldn't use much memory even on a small-memory machine.</p>
",1,2,140,2022-12-19 08:37:14,https://stackoverflow.com/questions/74847953/memoryerror-when-extracting-articles-into-list-using-gensim-wikicorpus
Evaluation of gensim Doc2Vec model for Recommendations,"<p>I have developed a pipeline to extract text from documents, preprocess the text, and train a gensim Doc2vec model on given documents. Given a document in my corpus, I would like to recommend other documents in the corpus.</p>
<p>I want to know how I can evaluate my model without having a pre-defined list of &quot;good&quot; recommendations. Any ideas?</p>
","nlp, gensim, recommendation-engine, evaluation, doc2vec","<p>One simple self-check that can be used to catch some big problems with a <code>Doc2Vec</code> model training pipeline – like gross misparameterizations, or insufficient data/epochs – is to re-infer vectors for the training texts (using <code>.infer_vector()</code>), and check that generally:</p>
<ol>
<li>the bulk-trained vector for the same text is &quot;close to&quot; the re-inferred vector - such as its nearest-neighbor, or one of the top neighbors, in a <code>.most_similar()</code> operation on the re-inferred text</li>
<li>the overall list of nearest-neighbors (from <code>.most_similar()</code>) for the bulk-trained vector, &amp; the re-inferred vector, are very similar.</li>
</ol>
<p>They won't necessarily be identical, for reasons explained in <a href=""https://github.com/RARE-Technologies/gensim/wiki/Recipes-&amp;-FAQ#q11-ive-trained-my-word2vec--doc2vec--etc-model-repeatedly-using-the-exact-same-text-corpus-but-the-vectors-are-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-2vec-training-non-determinism"" rel=""nofollow noreferrer"">Q11 &amp; Q12 of the Gensim Project FAQ</a>, but if they're wildly-different, then something foundational has gone wrong, like:</p>
<ul>
<li>insufficient (in quantity or quality/form) training data</li>
<li>misparameterizations, like too few epochs or too-large (overfitting-prone) vectors for the quantity of data</li>
</ul>
<p>Ultimately, though, the variety of data sources &amp; intended uses &amp; possible dimensions of &quot;recommendation-worthy&quot; mean that you need cusomt inputs, based on your project's needs, usually from the intended audience (or your own ability to simulate/represent it).</p>
<p>In the <a href=""https://arxiv.org/abs/1405.4053"" rel=""nofollow noreferrer"">original paper introducing the &quot;Paragraph Vector&quot; algorithm</a> (what's inside the <code>Doc2Vec</code> class), and a <a href=""https://arxiv.org/abs/1507.07998"" rel=""nofollow noreferrer"">followup evaluating it on Wikipedia &amp; arXiv articles</a>, several of the evaluations used triplets of documents, where 2 of the triplet were conjectured to be &quot;necessarily similar&quot; based on some preexisting system's groupings, and the 3rd randomly-chosen.</p>
<p>The algorithm's performance, and relative performance under different parameter choices, was scored based on how often it placed the 2 presumptively-related documents closer-together than the 3rd randomly-chosen document.</p>
<p>For example, one of the original paper's evaluations use brief search-engine-result snippets as documents, and considered any 2 documents that appeared as sibling top-10 results for the same query as presumptively-related. Two of the followup paper's evaluation used the human-curated categories of Wikipedia or arXiv as signalling that articles of the same category should be presumptively-related.</p>
<p>It's imperfect, but allowed the creation of large evaluation sets from already-existing systems/data, which generally pointed results in the same direction as human senses-of-relatedness.</p>
<p>Perhaps you can find a similar preexisting guide for your data. Or, as you perform ad-hoc checking, be sure to <em>capture every judgement you make</em>, so that it becomes, over time, a growing dataset of desirable pairings that are either (a) better than some other result that was co-presented; or (b) just &quot;presumably good enough&quot; that they usually should rank higher than other random 3rd documents. A large amount of imprecision in such desirability-data is tolerable, as it can even out as the set of probe-pairings grows, and the power of being able to automate bulk quantitative evaluations (reusing old assessments against new parameters/models) drives far more overall improvement than any small glitches in the evaluations cost.</p>
",1,1,330,2023-01-05 18:59:54,https://stackoverflow.com/questions/75023217/evaluation-of-gensim-doc2vec-model-for-recommendations
&#39;ConcatenatedDoc2Vec&#39; object has no attribute &#39;docvecs&#39;,"<p>I am a beginner in Machine Learning and trying Document Embedding for a university project. I work with Google Colab and Jupyter Notebook (via Anaconda). The problem is that my code is perfectly running in Google Colab but if i execute the same code in Jupyter Notebook (via Anaconda) I run into an error with the ConcatenatedDoc2Vec Object.</p>
<p>With this function I build the vector features for a Classifier (e.g. Logistic Regression).</p>
<pre><code>def build_vectors(model, length, vector_size):
    vector = np.zeros((length, vector_size))
    for i in range(0, length):
        prefix = 'tag' + '_' + str(i)
        vector[i] = model.docvecs[prefix]
    return vector
</code></pre>
<p>I concatenate two Doc2Vec Models (<code>d2v_dm, d2v_dbow</code>), both are working perfectly trough the whole code and have no problems with the function <code>build_vectors()</code>:</p>
<pre><code>d2v_combined = ConcatenatedDoc2Vec([d2v_dm, d2v_dbow])
</code></pre>
<p>But if I run the function <code>build_vectors()</code> with the concatenated model:</p>
<pre><code>#Compute combined Vector size
d2v_combined_vector_size = d2v_dm.vector_size + d2v_dbow.vector_size

d2v_combined_vec= build_vectors(d2v_combined, len(X_tagged), d2v_combined_vector_size)
</code></pre>
<p>I receive this error (but <strong>only</strong> if I run this in Jupyter Notebook (via Anaconda) -&gt; no problem with this code in the Notebook in Google Colab):</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Input In [20], in &lt;cell line: 4&gt;()
      1 #Compute combined Vector size
      2 d2v_combined_vector_size = d2v_dm.vector_size + d2v_dbow.vector_size
----&gt; 4 d2v_combined_vec= build_vectors(d2v_combined, len(X_tagged), d2v_combined_vector_size)

Input In [11], in build_vectors(model, length, vector_size)
      3 for i in range(0, length):
      4     prefix = 'tag' + '_' + str(i)
----&gt; 5     vector[i] = model.docvecs[prefix]
      6 return vector

AttributeError: 'ConcatenatedDoc2Vec' object has no attribute 'docvecs'
</code></pre>
<p>Since this is mysterious (for me) -&gt; Working in Google Colab but not Anaconda and Juypter Notebook -&gt; and I did not find anything to solve my problem in the web.</p>
","jupyter-notebook, google-colaboratory, gensim, doc2vec","<p>If it's working one place, but not the other, you're probably using different versions of the relevant libraries – in this case, <code>gensim</code>.</p>
<p>Does the following show exactly the same version in both places?</p>
<pre class=""lang-py prettyprint-override""><code>import gensim
print(gensim.__version__)
</code></pre>
<p>If not, the most immediate workaround would be to make the place where it doesn't work match the place that it does, by force-installing the same explicit version – <code>pip intall gensim==VERSION</code> (where <code>VERSION</code> is the target version) – then ensuring your notebook is restarted to see the change.</p>
<p>Beware, though, that unless starting from a fresh environment, this could introduce other library-version mismatches!</p>
<p>Other things to note:</p>
<ul>
<li>Last I looked, Colab was using an over-4-year-old version of Gensim (3.6.0), despite more-recent releases with many fixes &amp; performance improvements. It's often best to stay at or closer-to the latest versions of any key libraries used by your project; <a href=""https://stackoverflow.com/a/74748756/130288"">this answer describes how to trigger the installation of a more-recent Gensim at Colab</a>. (Though of course, the initial effects of that might be to cause the same breakage in your code, adapted for the older version, at Colab.)</li>
<li>In more-recent Gensim versions, the property formerly called <code>docvecs</code> is now called just <code>dv</code> - so some older code erroring this way may only need <code>docvecs</code> replaced with <code>dv</code> to work. (Other tips for migrating older code to the latest Gensim conventions are available at: <a href=""https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4</a> )</li>
<li>It's unclear where you're pulling the <code>ConcatenatedDoc2Vec</code> class from. A clas of that name exists in some Gensim demo/test code, as a very minimal shim class that was at one time used in attempts to reproduce the results of the original &quot;Paragaph Vector&quot; (aka <code>Doc2Vec</code>) paper. But beware: that's <em>not</em> a usual way to use <code>Doc2Vec</code>, &amp; the class of that name I know barely does anything outside its original narrow purpose.</li>
<li>Further, beware that as far as I know, noone has ever reproduced the full claimed performance of the two-kinds-of-doc-vectors-concatenated approach reported in that paper, even using the same data/described-technique/evaluation. The claimed results likely relied on some other undisclosed techniques, or some error in the writeup. So if you're trying to mimic that, don't get too frustrated. And know most uses of <code>Doc2Vec</code> just pick one mode.</li>
<li>If you have your own separate reasons for creating concatenated feature-vectors, from multiple algorithms, you should probably write your own code for that, not limited to the peculiar two-modes-of-<code>Doc2Vec</code> code from that one experiment.</li>
</ul>
",1,1,70,2023-01-06 22:53:38,https://stackoverflow.com/questions/75036827/concatenateddoc2vec-object-has-no-attribute-docvecs
Gensim ensemblelda multiprocessing: index -1 is out of bounds for axis 0 with size 0,"<p>I'm using the <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow noreferrer"">gensim library</a> for topic modelling, more precisely the <a href=""https://radimrehurek.com/gensim/models/ensemblelda.html"" rel=""nofollow noreferrer"">Ensemble LDA</a> method. My code is fairly standard (I follow the documentation), the main part is:</p>
<pre><code>           model = models.EnsembleLda(corpus=corpus,
                                   id2word=id2word,
                                   num_topics=ntopics,
                                   passes=2,
                                   iterations = 200,
                                   num_models=ncores,
                                   topic_model_class=models.LdaModel,
                                   ensemble_workers=nworkers,
                                   distance_workers=ncores)
</code></pre>
<p>(full code at <a href=""https://github.com/erwanm/gensim-temporary/blob/main/gensim-topics.py"" rel=""nofollow noreferrer"">https://github.com/erwanm/gensim-temporary/blob/main/gensim-topics.py</a>)</p>
<p>But with my data I <em>sometimes</em> obtain the error below. But it also often runs correctly with a subset of the data, so I don't know if the problem is related to my data?</p>
<pre><code>Process Process-52:
Traceback (most recent call last):
  File &quot;/home/moreaue/anaconda3/envs/twarc2/lib/python3.10/multiprocessing/process.py&quot;, line 314, in _bootstrap
    self.run()
  File &quot;/home/moreaue/anaconda3/envs/twarc2/lib/python3.10/multiprocessing/process.py&quot;, line 108, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/home/moreaue/anaconda3/envs/twarc2/lib/python3.10/site-packages/gensim/models/ensemblelda.py&quot;, line 534, in _asymmetric_distance_matrix_worker
    distance_chunk = _calculate_asymmetric_distance_matrix_chunk(
  File &quot;/home/moreaue/anaconda3/envs/twarc2/lib/python3.10/site-packages/gensim/models/ensemblelda.py&quot;, line 491, in _calculate_asymmetric_distance_matrix_chunk
    mask = masking_method(ttd1, masking_threshold)
  File &quot;/home/moreaue/anaconda3/envs/twarc2/lib/python3.10/site-packages/gensim/models/ensemblelda.py&quot;, line 265, in mass_masking
    smallest_valid = sorted_a[largest_mass][-1]
IndexError: index -1 is out of bounds for axis 0 with size 0
</code></pre>
<p>The error seems related to multiprocessing, since <code>ensemblelda</code> runs a number of threads (each running one instance of LDA).</p>
<p>What can cause this error? Any advice on how I can fix it?</p>
","python, nlp, multiprocessing, gensim","<p>From reading the source code, the <code>mass_masking</code> function is simply returning a boolean array (binary mask) with truth values assigned to its positions that positionally correspond to the elements of <code>a</code>: where those elements of <code>a</code>, if you had to cumulatively summate them, would remain below the <code>threshold</code> set (default is 0.95).</p>
<p>The line <code>sorted_a[largest_mask][-1]</code> is causing problems because nowhere in the <code>mass_masking</code> function are there any checks to make sure that the largest element of <code>a</code> is below the threshold, nor its predecessor <code>ttd1</code>. Whenever <code>ttd1</code>'s, and therefore <code>a</code>'s, largest value is equal to or above the threshold, the <code>IndexError</code> is raised because <code>sorted_a[largest_mass]</code> returns an empty array which is then attempted to be indexed by the <code>[-1]</code>. The minimal examples below showcase this issue.</p>
<p><code>ttd1</code> is created in the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/ensemblelda.py#L451"" rel=""nofollow noreferrer"">_calculate_asymmetric_distance_matrix_chunk()</a> function when it enumerates over <code>ttda1</code>. It calls <code>masking_method(ttd1, masking_threshold)</code>, which subsequently invokes <code>mass_masking</code> which is set as the default method upon initialisation of the <code>EnsembleLda</code> class. I suspect this is a bug that is prevalent during multiprocessing because the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/ensemblelda.py#L545"" rel=""nofollow noreferrer"">_calculate_assymetric_distance_matrix_multiproc()</a> function instantiates the <code>Process</code> with the <code>target</code> set as the <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/ensemblelda.py#L521"" rel=""nofollow noreferrer"">_asymmetric_distance_matrix_worker()</a> function, and this <code>worker</code> function slices the <code>entire_ttda</code> to create <code>ttda1</code> which it passes to the <code>chunk</code> function, but again, there's no check to ensure it hasn't created a slice that has one of its elements equal to or larger than <code>threshold</code>.</p>
<p>Here is a minimal working example of how <code>mass_making</code> should function:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np


def mass_masking(a, threshold=None):
    &quot;&quot;&quot;Original masking method. Returns a new binary mask.&quot;&quot;&quot;
    if threshold is None:
        threshold = 0.95

    sorted_a = np.sort(a)[::-1]  # [0.5   0.449 0.2   0.1  ]
    # sorted_a.cumsum() outputs [0.5   0.949 1.149 1.249] then threshold is taken
    largest_mass = sorted_a.cumsum() &lt; threshold  # [ True  True False False]
    # sorted_a[largest_mass] outputs [0.5   0.449] then last element is taken
    smallest_valid = sorted_a[largest_mass][-1]  # 0.449
    return a &gt;= smallest_valid # [ True False False  True]


test_arr = np.array([0.5, 0.2, 0.1, 0.449])
print(mass_masking(test_arr))
# Output: [ True False False  True]
</code></pre>
<p>When <code>a</code>'s largest element is greater than or equal to 0.95, an <code>IndexError</code> is raised:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np


def mass_masking(a, threshold=None):
    &quot;&quot;&quot;Original masking method. Returns a new binary mask.&quot;&quot;&quot;
    if threshold is None:
        threshold = 0.95

    sorted_a = np.sort(a)[::-1]  # [0.96  0.5   0.449 0.2  ]
    # sorted_a.cumsum() outputs [0.96  1.46  1.909 2.109] then threshold is taken
    largest_mass = sorted_a.cumsum() &lt; threshold  # [False False False False]
    # sorted_a[largest_mass] outputs [] then last element is taken
    smallest_valid = sorted_a[largest_mass][-1]  # IndexError
    return a &gt;= smallest_valid # function fails to return binary mask


test_arr = np.array([0.5, 0.2, 0.96, 0.449])
print(mass_masking(test_arr))
# Output: IndexError: index -1 is out of bounds for axis 0 with size 0
</code></pre>
<p>Knowing this, and assuming I haven't misinterpreted how the whole group of <code>calculate_asymmetric_distance_matrix</code> functions are working during multiprocessing, you can either try to ensure yourself that no values are greater than or equal to the default threshold, or set the <code>masking_threshold</code> higher when initialising the EnsembleLda.</p>
",1,1,206,2023-01-11 15:43:08,https://stackoverflow.com/questions/75085591/gensim-ensemblelda-multiprocessing-index-1-is-out-of-bounds-for-axis-0-with-si
Word2Vec / Doc2Vec training fails: Supplied example count (0) did not equal expected count,"<p>I am learning Word2Vec and was trying to replicate a Word2Vec model from my textbook. Unlike what the textbook shows, however, my model gives a warning saying that <code>supplied example count (0) did not equal expected count (2381)</code>. Apparently, my model was not trained at all. The corpus I fed to the model was apparently an re-usable iterator (it was a list) as it passed this test:</p>
<pre><code>&gt;&gt;&gt; print(sum(1 for _ in corpus))
&gt;&gt;&gt; print(sum(1 for _ in corpus))
&gt;&gt;&gt; print(sum(1 for _ in corpus))

2381
2381
2381
</code></pre>
<p>I tried with gensim 3.6 and gensim 4.3, and both versions gave me the same warning. Here is a code snippet I used with gensim 3.6:</p>
<pre><code>word2vec_model = Word2Vec(size = 300, window=5, min_count = 2, workers = -1)
word2vec_model.build_vocab(corpus)
word2vec_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', lockf=1.0, binary=True)
word2vec_model.train(corpus, total_examples = word2vec_model.corpus_count, epochs = 15)
</code></pre>
<p>This is the warning message:</p>
<pre><code>WARNING:gensim.models.base_any2vec:EPOCH - 1 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 2 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 3 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 4 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 5 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 6 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 7 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 8 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 9 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 10 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 11 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 12 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 13 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 14 : supplied example count (0) did not equal expected count (2381)
WARNING:gensim.models.base_any2vec:EPOCH - 15 : supplied example count (0) did not equal expected count (2381)
(0, 0)
</code></pre>
<p>I tried to train a different model with Doc2Vec with different corpus that is in the form of TaggedDocument, it gave me the same warning message.</p>
","gensim, word2vec, doc2vec","<p>Gensim's <code>Word2Vec</code> &amp; <code>Doc2Vec</code> (&amp; related models) don't take a <code>workers=-1</code> value. You have to set a specific count of worker threads.</p>
<p>Setting <code>-1</code> means no threads, and then the no-training situation you've observed. (There might be some better messaging of what's gone wrong in the latest Gensim or with loggin to at least the INFO level.)</p>
<p>Generally the <code>worker</code> count should never be higher than the number of CPU cores – but also, when training using a corpus iterable on a machine with more than 8 cores, optimal throughput is more likely to be reached in the 6-12 thread range than anything higher, because of some contention/bottlnecking in the single-reader-thread, fan-out-to-many-workers approach Gensim uses, and the Python &quot;GIL&quot;.</p>
<p>Unfortunately, the exact best throughput value will vary based on your other parameters, especially <code>window</code> and <code>vector_size</code> and <code>negative</code>, and can only be found via trial-and-error. I often start with 6 on an 8-core machine, and 12 on any machine with 16 or more cores. (Another key tip is to make sure your corpus iterable is doing as little as possible – such as reading a pre-tokenized file from disk, rather than doing any other preprocessing every iteration, in the main thread.)</p>
<p>If you can get all your text from a pretokenized text file, you can also consider the <code>corpus_file</code> mode, which lets each worker read its own unique range of the file, and thus better achieves maximum throughput by setting workers to the number of cores.</p>
<p>Separate tips:</p>
<ul>
<li><p>A <code>min_count=2</code> value so low usually hurts word2vec results: rare words don't learn good representation for themselves from a small number of usage examples, but can in aggregate dilute/interfere-with other words. Discarding more rare words, as the size of the corpus allows, often improves all surviving words enough to improve overall downstream evaluations.</p>
</li>
<li><p><code>.intersect_word2vec_format()</code> is an advanced/experimental option with no sure best practices; try to understand what it does from the source code, and the weird ways it changes the usual SGD tradeoffs, before trying it – and be sure to run extra checks that it's doing what you want over more typical approaches.</p>
</li>
</ul>
",1,0,480,2023-01-12 02:38:23,https://stackoverflow.com/questions/75091018/word2vec-doc2vec-training-fails-supplied-example-count-0-did-not-equal-expe
Gensim Word2Vec produces different most_similar results through final epoch than end of training,"<p>I'm using gensim's Word2Vec for a recommendation-like task with part of my evaluation being the use of callbacks and the <code>most_similar()</code> method. However, I am noticing a huge disparity between the final few epoch callbacks and that of immediately post-training. In fact, the last epoch callback may often appear worthless, while the post training result is as best as could be desired.</p>
<p>My during-training tracking of most similar entries utilizes gensim's <code>CallbackAny2Vec</code> class. It follows the <a href=""https://radimrehurek.com/gensim/models/callbacks.html"" rel=""nofollow noreferrer"">doc example</a> fairly directly and roughly looks like:</p>
<pre><code>class EpochTracker(CallbackAny2Vec):

  def __init__(self):
    self.epoch = 0

  def on_epoch_begin(self, model):
    print(&quot;Epoch #{} start&quot;.format(self.epoch))

  def on_epoch_end(self, model):
    
    print('Some diagnostics')
    # Multiple terms used in the below
    e = model.wv
    print(e.most_similar(positive=['some term'])[0:3]) # grab the top 3 examples for some term

    print(&quot;Epoch #{} end&quot;.format(self.epoch))
    self.epoch += 1
</code></pre>
<p>As the epochs progress, the <code>most_similar()</code> results given by the callbacks to not seem to indicate an advancement of learning and seem erratic. In fact, often the callback from the first epoch shows the best result.</p>
<p>Counterintuitively, I also have an additional process (not shown) built into the callback that does indicate gradual learning. Following the similarity step, I take the current model's vectors and evaluate them against a down-stream task. In brief, this process is a sklearn <code>GridSearchCV</code> logistic regression check against some known labels.</p>
<p>I find that often the last <code>on_epoch_end</code> callback appears to be garbage. Or perhaps some multi-threading shenanigans. However, if directly after training the model I try the similarity call again:</p>
<pre><code>e = e_model.wv # e_model was the variable assignment of the model overall
print(e.most_similar(positive=['some term'])[0:3])
</code></pre>
<p>I tend to get beautiful results that are in agreement with the downstream evaluation task also used in the callbacks, or are at least vastly different than that of the final epoch end.</p>
<p>I suspect I am missing something painfully apparent or <code>most_similar()</code> has an unusual behavior with epoch-end callbacks. Is this a known issue or is my approach flawed?</p>
","nlp, gensim, word2vec, word-embedding","<p>What version of Gensim are you using?</p>
<p>In older versions – pre 4.0 if I remember correctly? – the <code>most_similar()</code> operation relies on a cached pre-computed set of unit-normalized word-vectors that in some cases will be frozen when you first try a <code>most_similar()</code>.</p>
<p>Thus, incremental updates to vectors won't be reflected in results, unless something happens to flush that cache - which happens at then <em>end</em> of training. But, since mid-training checks weren't an originally-envisioned usage, more-frequent flushing doesn't happen unless forced.</p>
<p>I think if you're sure to use the latest Gensim, the problem may go away - or reviewing this older project issue may provide ideas if you're stuck on an older version: <a href=""https://github.com/RaRe-Technologies/gensim/issues/2260"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/issues/2260</a></p>
<p>(Your other mid-training learning process – if it's accessing the non-normalized per-word vectors directly, rather than via <code>most_similar()</code> – is likely succeeding because it's skipping that normed-vector cache.)</p>
",1,0,122,2023-01-13 03:03:18,https://stackoverflow.com/questions/75104456/gensim-word2vec-produces-different-most-similar-results-through-final-epoch-than
SparseTermSimilarityMatrix().inner_product() throws &quot;cannot unpack non-iterable bool object&quot;,"<p>While working with cosine similarity, I am facing issue calculating the inner product of two vectors.</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.similarities import (
    WordEmbeddingSimilarityIndex,
    SparseTermSimilarityMatrix
)

w2v_model         = api.load(&quot;glove-wiki-gigaword-50&quot;)
similarity_index  = WordEmbeddingSimilarityIndex(w2v_model)
similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary)

score = similarity_matrix.inner_product(
    X = [
        (0, 1), (1, 1), (2, 1), (3, 2), (4, 1), 
        (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), 
        (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), 
        (15, 1), (16, 3)
    ], 
    Y = [(221, 1), (648, 1), (8238, 1)], 
    normalized = True
)
</code></pre>
<p>Error:</p>
<pre class=""lang-bash prettyprint-override""><code>TypeError                                 Traceback (most recent call last)
Input In [77], in &lt;cell line: 1&gt;()
----&gt; 1 similarity_matrix.inner_product(
      2     [(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), 
      3      (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 3)], 
      4     [(221, 1), (648, 1), (8238, 1)], normalized=True)

File ~\Anaconda3\lib\site-packages\gensim\similarities\termsim.py:558, in SparseTermSimilarityMatrix.inner_product(self, X, Y, normalized)
    555 if not X or not Y:
    556     return self.matrix.dtype.type(0.0)
--&gt; 558 normalized_X, normalized_Y = normalized
    559 valid_normalized_values = (True, False, 'maintain')
    561 if normalized_X not in valid_normalized_values:

TypeError: cannot unpack non-iterable bool object
</code></pre>
<p>I am not sure why it says <code>bool</code> objects when both X and Y are <code>list</code>.</p>
","python, nlp, nltk, gensim, cosine-similarity","<p>The <code>normalized</code> parameter should be a 2-tuple which declares for both X and Y separately (<a href=""https://radimrehurek.com/gensim/similarities/termsim.html#gensim.similarities.termsim.SparseTermSimilarityMatrix.inner_product"" rel=""nofollow noreferrer"">as in the docs</a>).</p>
<p>Therefore, the call should look like this:</p>
<pre class=""lang-py prettyprint-override""><code>score = similarity_matrix.inner_product(
    X = [
        (0, 1), (1, 1), (2, 1), (3, 2), (4, 1), 
        (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), 
        (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), 
        (15, 1), (16, 3)
    ], 
    Y = [(221, 1), (648, 1), (8238, 1)], 
    normalized = (True, True)
)
</code></pre>
",1,1,131,2023-01-23 07:39:44,https://stackoverflow.com/questions/75206732/sparsetermsimilaritymatrix-inner-product-throws-cannot-unpack-non-iterable
Gensim: Not able to load the id2word file,"<p>I am working on topic inference on a new corpus given a previously derived lda model. I am able to load the model perfectly, while I am not able to load the id2word file to create the <code>corpora.Dictionary</code> object needed to map the new corpus into numbers: the <code>load</code> method returns a dict attribute error that I don't know why. Below is the minimal code that replicates the situation, and I have attached the code (and packages used) here.</p>
<p>Thank you in advance for your response...</p>
<pre><code>import numpy as np
import os
import pandas as pd
import gensim
from gensim import corpora
import datetime
import nltk

model_name = &quot;lda_sub_full_35&quot;

dictionary_name = &quot;lda_sub_full_35.id2word&quot;

model_for_inference = gensim.models.LdaModel.load(model_name, mmap='r')
print('Successfully load the model')
lda_dictionary = corpora.Dictionary.load(dictionary_name, mmap='r')
</code></pre>
<p>I expect to have both the dictionary and the model loaded, but it turns out that when I load the dictionary, I got the below error:</p>
<pre><code>File &quot;topic_inference.py&quot;, line 31, in &lt;module&gt;
    lda_dictionary = corpora.Dictionary.load(dictionary_name, mmap='r')
File &quot;/topic_modeling/env/lib/python3.8/site-packages/gensim/utils.py&quot;, line 487, in load
    obj._load_specials(fname, mmap, compress, subname)
AttributeError: 'dict' object has no attribute '_load_specials'```
</code></pre>
","nlp, gensim, topic-modeling","<p>How were the contents of the <code>lda_sub_full_35.id2word</code> file originally saved?</p>
<p>Only if it was saved by a Gensim <code>corpora.Dictionary</code> object's <code>.save()</code> method should it be loaded as you've tried, with <code>corpora.Dictionary.load()</code>.</p>
<p>If, by any chance, it was just a plain Python <code>dict</code> saved via some other method of writing a <code>pickle()</code>-created object, then you would need to load it in a symmetrically-matched way. That might be as simple as:</p>
<pre class=""lang-py prettyprint-override""><code>import pickle

with open(path, 'rb') as f:
    lda_dictionary = pickle.load(f)
</code></pre>
",0,0,313,2023-01-30 20:49:37,https://stackoverflow.com/questions/75289969/gensim-not-able-to-load-the-id2word-file
Word2Vec convert a sentence,"<p>I have trained a Word2Vec model using gensim, I have a dataset of tweets that I would like to convert to vectors. What is the best way to convert a sentence to a vector + how can this be done using a word2vec model.</p>
","gensim, word2vec","<p>Formally, the word2vec algorithm only gives you a vector per word, <strong>not</strong> per longer text (like a sentence or paragraph or tweet or article).</p>
<p>One quick &amp; easy baseline approach for turning longer texts into vectors is to just average together the vectors of each word. Recent versions of Gensim have a helper method <a href=""https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector"" rel=""nofollow noreferrer""><code>get_mean_vector()</code></a> to do this on <code>KeyedVectors</code> model objects (sets-of-word-vectors):</p>
<pre class=""lang-py prettyprint-override""><code>text_vector = kv_model.get_mean_vector(list_of_words)
</code></pre>
<p>Of course, such a simpleminded average has no way to model the effects of word-order/grammar. Words may tend to cancel each other out rather than have the compositional effects of real language, and the space of possible multiword-text meanings is much larger than the space of single-word meanings – so just collapsing the text into the same coordinate system as words may lose a lot.</p>
<p>More sophisticated ways of vectorizing text rely on model far more more sophisticated than plain word2vec, such as deep/recurrent neural networks for modelling longer ranges of text.</p>
",1,0,449,2023-02-15 16:32:25,https://stackoverflow.com/questions/75462764/word2vec-convert-a-sentence
Gensim Pickle Error: Enable to Load the Saved Topic Model,"<p>I am working on topic inference that will require to load a previously saved model.</p>
<p>However, I got a pickle error that says</p>
<pre><code>Traceback (most recent call last):
  File &quot;topic_inference.py&quot;, line 35, in &lt;module&gt;
    model_for_inference = gensim.models.LdaModel.load(model_name, mmap = 'r')
  File &quot;topic_modeling/env/lib/python3.8/site-packages/gensim/models/ldamodel.py&quot;, line 1663, in load
    result = super(LdaModel, cls).load(fname, *args, **kwargs)
  File &quot;topic_modeling/env/lib/python3.8/site-packages/gensim/utils.py&quot;, line 486, in load
    obj = unpickle(fname)
  File &quot;topic_modeling/env/lib/python3.8/site-packages/gensim/utils.py&quot;, line 1461, in unpickle
    return _pickle.load(f, encoding='latin1')  # needed because loading from S3 doesn't support readline()
TypeError: __randomstate_ctor() takes from 0 to 1 positional arguments but 2 were given
</code></pre>
<p>The code I use to load the model is simply</p>
<pre><code>gensim.models.LdaModel.load(model_name, mmap = 'r')
</code></pre>
<p>Here is the code that I use to create and save the model</p>
<pre><code> model = gensim.models.ldamulticore.LdaMulticore(
        corpus=comment_corpus,
        id2word=key_word_dict, ## This is now a gensim.corpora.Dictionary Object, previously it was the .id2token attribute
        chunksize=chunksize,
        alpha='symmetric',
        eta='auto',
        iterations=iterations,
        num_topics=num_topics,
        passes=epochs,
        eval_every=eval_every, 
        workers = 15,
        minimum_probability= 0.0)

model.save(output_model)
</code></pre>
<p>where <code>output_model</code> doesn't have an extension like <code>.model</code> or <code>.pkl</code></p>
<p>In the past, I tried the similar approach with the exception that I passed in a <code>.id2token</code> attribute under the <code>gensim.corpora.Dictionary</code> object instead of the full <code>gensim.corpora.Dictionary</code> to the <code>id2word</code> parameter when I created the model, and the method loads the model fine back then. I wonder if passing in a <code>corpora.Dictionary</code> will make a difference in the loading output...? Back that time, I was using regular python, but now I am using anaconda. However, all the versions of the packages are the same.</p>
","pickle, gensim, lda","<p>Another report of an error about <code>__randomstate_ctor</code> (at <a href=""https://github.com/numpy/numpy/issues/14210"" rel=""nofollow noreferrer"">https://github.com/numpy/numpy/issues/14210</a>) suggests the problem may be related to numpy object pickling.</p>
<p>Is there a chance that the configuration where your load is failing is using a later version of <code>numpy</code> than when the save occurred? Could you try, at least temporarily, rolling back to some older <code>numpy</code> (that's still sufficient for whatever Gensim you're using) to see if it helps?</p>
<p>If you find any load that works, even in a suboptimal config, you might be able to null-out whatever <code>random</code>-related objects are causing the problem and re-save, then having a saved version that loads better in your truly-desired configuration. Then, if the <code>random</code>-related objects truly needed after reload, it may be possible to manually re-constitute them. (I haven't looked into this yet, but if you find any workaround allowing a load, but then aren't sure what to manually null/rebuild, I could take a closer look.)</p>
",3,2,3475,2023-02-18 00:02:00,https://stackoverflow.com/questions/75490275/gensim-pickle-error-enable-to-load-the-saved-topic-model
Load Gensim WordVectors into spacy pipeline,"<p>I've generated a Word2Vec model with gensim, bat have a hard time using it in my spacy pipeline.</p>
<pre><code>python -m spacy init vectors de w2v-model-v1.txt.gz path/SpacyModel
</code></pre>
<p>creates a model i can load, but the only component is the vectors.
I am using the model de_core_news_lg with custom pipeline components and would like to simply replace the standard-vectors with my custom trained vectors</p>
","spacy, gensim","<p>I used the vectors in an existing pipeline by adding each vector to a new vocab.</p>
<pre><code>from gensim.models import Word2Vec
from spacy.vocab import Vocab

gensim_model = Word2Vec.load(my_w2vmodel.model)
vocab = Vocab()

for word in gensim_model.wv.index_to_key:
      vector = gensim_model.wv.get_vector(word)
      vocab.set_vector(word, vector)

nlp.vocab.vectors = vocab.vectors
</code></pre>
",0,0,158,2023-02-21 13:20:13,https://stackoverflow.com/questions/75521069/load-gensim-wordvectors-into-spacy-pipeline
How does gensim calculate sentence embeddings when using a pretrained fasttext model?,"<p>According to <a href=""https://github.com/facebookresearch/fastText/issues/323#issuecomment-353167113"" rel=""nofollow noreferrer"">this</a> answer, sentence similarity for FastText is calculated with one of two ways (depending if the embeddings are created superviser or unsupervised)</p>
<ol>
<li>The mean of the normalized word vectors (unsupervised)</li>
<li>The mean of the word vectors (supervised)</li>
</ol>
<p>But I cannot make either of those give the same answer as the sentence embedding</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models import fasttext
import numpy as np

wv = fasttext.load_facebook_vectors(&quot;transtotag/cc.da.300.bin&quot;)

w1 = wv[&quot;til&quot;]
norm_w1 = np.linalg.norm(wv[&quot;til&quot;], ord=2)
s1 = w1/norm_w1

w2 = wv[&quot;skat&quot;]
norm_w2 = np.linalg.norm(wv[&quot;skat&quot;], ord=2)
s2 = w2/norm_w2

w3 = wv[&quot;til skat&quot;]

# Using &quot;raw&quot; embeddings
((w1+w2)/2-w3).max() #0.25
((w1+w2)-w3).max() # 0.5

# using normalized embeddings
((s1+s2)/2-w3).max() # 0.18
((s1+s2)-w3).max() # 0.37

</code></pre>
<p>I even tried to add the EOS (as stated in the answer) aswell</p>
<pre class=""lang-py prettyprint-override""><code>nl = wv[&quot;&lt;/s&gt;&quot;]
norm_nl = np.linalg.norm(wv[&quot;&lt;/s&gt;&quot;],2)
snl = nl/norm_nl

w3 = wv[&quot;til skat&quot;]

((s1+s2+snl)/3-w3).max() #0.12
</code></pre>
<p>If we look in the source code, then <code>wv[]</code> just returns <code>vstack([self.get_vector(key) for key in key_or_keys])</code> i.e it treats <code>til skat</code> a single word.</p>
<p>I cannot find anyting about how sentence embeddings are created in the docs aswell.</p>
","gensim, word-embedding, fasttext","<p>In Gensim, you should use <a href=""https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastTextKeyedVectors.get_sentence_vector"" rel=""nofollow noreferrer"">get_sentence_vector</a> method, which was recently added.</p>
<p>Please read the docs and notice that this method expects a list of words specified by string or int ids.</p>
",0,0,902,2023-03-13 14:19:19,https://stackoverflow.com/questions/75723102/how-does-gensim-calculate-sentence-embeddings-when-using-a-pretrained-fasttext-m
Loading a pretrained fastText model with Gensim,"<p>I'm trying to load a pretrained German fastText model (source: <a href=""https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.bin.gz"" rel=""nofollow noreferrer"">https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.bin.gz</a>) with Gensim. My intention is to fine-tune it using my own dataset. However, there occurs an error when loading the model.</p>
<p>My code:</p>
<pre><code>import gensim

print(&quot;gensim&quot;, gensim.__version__) # Out: gensim 4.3.1
bin_path = &quot;cc.de.300.bin&quot;
model = gensim.models.fasttext.load_facebook_model(bin_path)
</code></pre>
<p>The error:</p>
<pre><code>AssertionError: expected (600000000,),  got (306814511,)
</code></pre>
<p>The whole traceback:</p>
<pre><code>---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
Input In [3], in &lt;module&gt;
----&gt; 1 model = gensim.models.fasttext.load_facebook_model(bin_path)

File /opt/conda/lib/python3.9/site-packages/gensim/models/fasttext.py:728, in load_facebook_model(path, encoding)
    666 def load_facebook_model(path, encoding='utf-8'):
    667     &quot;&quot;&quot;Load the model from Facebook's native fasttext `.bin` output file.
    668 
    669     Notes
   (...)
    726 
    727     &quot;&quot;&quot;
--&gt; 728     return _load_fasttext_format(path, encoding=encoding, full_model=True)

File /opt/conda/lib/python3.9/site-packages/gensim/models/fasttext.py:808, in _load_fasttext_format(model_file, encoding, full_model)
    789 &quot;&quot;&quot;Load the input-hidden weight matrix from Facebook's native fasttext `.bin` output files.
    790 
    791 Parameters
   (...)
    805 
    806 &quot;&quot;&quot;
    807 with utils.open(model_file, 'rb') as fin:
--&gt; 808     m = gensim.models._fasttext_bin.load(fin, encoding=encoding, full_model=full_model)
    810 model = FastText(
    811     vector_size=m.dim,
    812     window=m.ws,
   (...)
    821     max_n=m.maxn,
    822 )
    823 model.corpus_total_words = m.ntokens

File /opt/conda/lib/python3.9/site-packages/gensim/models/_fasttext_bin.py:353, in load(fin, encoding, full_model)
    351     hidden_output = None
    352 else:
--&gt; 353     hidden_output = _load_matrix(fin, new_format=new_format)
    354     assert fin.read() == b'', 'expected to reach EOF'
    356 model.update(vectors_ngrams=vectors_ngrams, hidden_output=hidden_output)

File /opt/conda/lib/python3.9/site-packages/gensim/models/_fasttext_bin.py:284, in _load_matrix(fin, new_format)
    281 else:
    282     matrix = np.fromfile(fin, _FLOAT_DTYPE, count)
--&gt; 284 assert matrix.shape == (count,), 'expected (%r,),  got %r' % (count, matrix.shape)
    285 matrix = matrix.reshape((num_vectors, dim))
    286 return matrix

AssertionError: expected (600000000,),  got (306814511,)
</code></pre>
<p>What's the cause of this error and how can I load the dataset properly?</p>
","python, nlp, gensim, fasttext","<p>That's the sort of error you might get from a file that's been truncated, to not contain everything expected.)</p>
<p>Are you sure your <code>cc.de.300.bin</code> file is complete &amp; undamaged? What's it's size, and can you try re-dowloading it to ensure you have a full copy?</p>
<p>Separately: there's no official support for 'fine-tuning' FastText vectors in Gensim. You can call usual training methods in atypical ways, including on an already-trained model, to attempt an effect like that – but there are no guides for ways to do that effectively in Gensim. Further, I've never seen any good writeup explaining how fine-tuning a FastText model could be attempted &amp; verified.</p>
<p>If you want confidence in the usual benefits of FastText, including its ability to synthesize useful vectors for out-of-vocabulary words, it's safest to use/train it in the usual way: via a single training session which includes representative training texts for all words of interest. If improvising some other approach for patching in other words, or differing word senses for existing words, you should pay special attention to monitoring in what ways the novel steps are helping or hurting the overall model.</p>
",1,0,1578,2023-03-22 14:40:40,https://stackoverflow.com/questions/75813686/loading-a-pretrained-fasttext-model-with-gensim
correct syntax for sklearn&#39;s LogisticRegression where the feature data is arrays and there&#39;s more than one feature,"<p>I have a word2vec encoding for a set of queries and documents. Some queries and documents are relevant to one another and some are not. I am trying to train a logistic regression model to recognise whether a document is relevant to a query.</p>
<p>Currently I have a pandas data frame named training_data that looks like this (simplified version):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>query vector</th>
<th>doc vector</th>
<th>relevant</th>
</tr>
</thead>
<tbody>
<tr>
<td>[1,6,4]</td>
<td>[3,6,2]</td>
<td>1</td>
</tr>
<tr>
<td>[5,2,1]</td>
<td>[1,1,1]</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>Where all the vectors for query and doc vectors columns are the same length.
I have a different similar set-up for my test set also.</p>
<p>My question is, what is the correct syntax for feeding this data into sklearns Logistic Regression model. I am able to do it if the data is a single number and I am able to make one column into a list and feed that in, but how can I do that for 2 sets of features?</p>
<p>for example, if I ignore the query vector column for now I can just do:</p>
<pre><code>
y = training_data[&quot;relevant&quot;]

X = list(training_data[&quot;doc vector&quot;])

clf = LogisticRegression()
clf.fit(X, y)

</code></pre>
<p>and that works, but how can I add the query vector column into this? If I try and go straight from the data frame into the model without converting to a list I get &quot;ValueError: setting an array element with a sequence.&quot;.</p>
<p>I've tried a few combinations of things including making a 2d array representing the 2 columns but that gave me the same error as above. Help!</p>
","scikit-learn, linear-regression, logistic-regression, gensim, word2vec","<p>If you have one set of <code>N</code> numeric features, then another set of <code>M</code> numeric features, you'd generally concatenate them together into one combined  &quot;flat&quot; set of <code>(N+M)</code> feature to make them appropriate for a Scikit-Learn model that expects a single flat array of features as its <code>X</code> inputs.</p>
<p>There are many ways to do that, manually or with the assistance of other library code.</p>
<p>The most simple approach from your setup would probably be a bit of idiomatic Python like:</p>
<pre class=""lang-py prettyprint-override""><code>X = [list1 + list2 for list1, list2 
                   in zip(training_data[&quot;query vector&quot;], 
                          training_data[&quot;doc vector&quot;])]
</code></pre>
<p>If you were building a more extensive Scikit-Learn pipeline, within its helper classes, the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion"" rel=""nofollow noreferrer""><code>FeatureUnion</code></a> class is a utility for combining the results of two different feature-extraction methods. A demo of its use is in the example <a href=""https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html"" rel=""nofollow noreferrer"">Concatenating multiple feature extraction methods</a>.</p>
<p>(Note that in the case of your dataframe's rows, each of the two &quot;feature extractions&quot; could just be pulling the existing arrays from their two different cells. So, any 'transformers' you might use, in that style of approach, might do little more than taking some rough not-yet-correctly structured example – like a Pandas row, or a tuple of your 2 arrays – and indexing to access the chosen input elements.)</p>
",0,0,494,2023-03-27 11:30:36,https://stackoverflow.com/questions/75855226/correct-syntax-for-sklearns-logisticregression-where-the-feature-data-is-arrays
Gensim Dictionary slows down drastically whilst making,"<p>I have 26,000,000 tweets and am creating a Gensim Dictionary with them. The first approach:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.corpora import Dictionary

corpus = []
with open(input_file, &quot;r&quot;) as file:
    for row in file:
        comment = json.loads(row)        
        corpus.append(comment['text'].split())

gdict = Dictionary(corpus)
</code></pre>
<p>Takes about an hour just for the final line. I have also tried the following:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.corpora import Dictionary

gdict = Dictionary()
with open(input_file, &quot;r&quot;) as file:
    for row in file:
        comment = json.loads(row)        
        gdict.add_documents([comment['text'].split()])
</code></pre>
<p>This takes 2 minutes to process the first 11,400,000 tweets, and then suddenly slows down, and predicts over 3000 hours to finish.</p>
<p>I have randomised the order of the tweets and the same happens after around the same number of tweets, so it isn't a particular tweet doing it.</p>
<p>I have also grouped the tweets into various batch sizes before using <code>add_documents</code> and the same issue happens for all of them, around the same stage.</p>
<p>With the first approach, the final size of gdict is 1.4mb so it isn't a size issue.</p>
<p>Any ideas?</p>
","python, dictionary, jupyter-notebook, gensim","<p>Though the questions I asked in a comment are all relevant for diagnosing a general slowdown in this sort of process, looking at the <code>gensim.corpora.Dictionary</code> source, I see another likely culprit for the problem: a rather inefficient &amp; oft-repeated 'pruning', once the <code>Dictionary</code> has 2M entries.</p>
<p>If you have sufficient RAM, supplying a much larger <code>prune_at</code> parameter to either the contructor (when you're also passing the <code>corpus</code> in) or the <code>.add_documents()</code> call (when you're using that) should forestall or eliminate the issue. Ideally, you'd pick a <code>prune_at</code> value larger than the <code>Dictionary</code> ever becomes, so pruning never happens – though that risks exhausting memory, if your corpus has more unique words than can be tracked in RAM.</p>
<p>(Side note: if your <code>comment['text']</code> fields haven't been preprocessed in any way, then a simple <code>.split()</code> tokenization will likely leave you with lots of extra tokens like words connected to surrounding punctuation or with varying capitalization - which would inflate the token count a lot, and perhaps be less useful for many downstream tasks.)</p>
",1,0,83,2023-03-27 14:53:43,https://stackoverflow.com/questions/75857247/gensim-dictionary-slows-down-drastically-whilst-making
Why does the loss of Gensim Word2Vec model deteriorate in every epoch?,"<p>I'm training a Word2vec model using Gensim Word2Vec on twitter data. The loss of the model deteriorates in every epoch. The first epoch gives the lowest loss. Why is it so? Code is shared below:</p>
<pre><code>loss_list = []
class callback(CallbackAny2Vec):
     
    def __init__(self):
        self.epoch = 0
          
    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        loss_list.append(loss)
        print('Loss after epoch {}: {}'.format(self.epoch, loss))
        self.epoch = self.epoch + 1

model = Word2Vec(df['tweet_text'], vector_size=300, window=10, epochs=30, hs=0, negative = 1, compute_loss=True, callbacks=[callback()])
embedding_size = model.wv.vectors.shape[1]
print(&quot;embedding size---&gt;&quot;, embedding_size)
vocab = model.wv.index_to_key
print(&quot;minimum loss {} at epoch {}&quot;.format(min(loss_list), loss_list.index(min(loss_list))))
</code></pre>
<p>The output is:</p>
<pre><code>Loss after epoch 0: 527066.375
Loss after epoch 1: 1038087.0625
Loss after epoch 2: 1510719.75
Loss after epoch 3: 1936163.875
Loss after epoch 4: 2364015.5
Loss after epoch 5: 2779299.75
Loss after epoch 6: 3183956.25
Loss after epoch 7: 3570054.5
Loss after epoch 8: 3966524.75
Loss after epoch 9: 4335994.5
Loss after epoch 10: 4706316.0
Loss after epoch 11: 5046213.0
Loss after epoch 12: 5410604.5
Loss after epoch 13: 5754962.0
Loss after epoch 14: 6080469.0
Loss after epoch 15: 6428622.5
Loss after epoch 16: 6771707.0
Loss after epoch 17: 7105302.0
Loss after epoch 18: 7400089.0
Loss after epoch 19: 7732032.0
Loss after epoch 20: 8059942.5
Loss after epoch 21: 8408386.0
Loss after epoch 22: 8685176.0
Loss after epoch 23: 8959723.0
Loss after epoch 24: 9242788.0
Loss after epoch 25: 9506676.0
Loss after epoch 26: 9752588.0
Loss after epoch 27: 10013168.0
Loss after epoch 28: 10288152.0
Loss after epoch 29: 10550915.0
embedding size---&gt; 300
minimum loss 527066.375 at epoch 0
</code></pre>
","nlp, gensim, word2vec, word-embedding","<p>Unfortunately, the code which totals-up loss for reporting in the Gensim <code>Word2Vec</code> model has a number of known bugs &amp; deviations from reasonable user expectations. You can see an overview of the problems, with links to a number of more-specific bugs, in the project's bug tracking issue [#2617][1].</p>
<p>Among other problems, the default loss reported is a running tally across all epochs – you'd have to do extra comparisons, or resets-to-<code>0.0</code>, to get per-epoch loss. And, insufficient precision in the running tally variables means other inaccuracies that become noticeable in large epochs or large runs.</p>
<p>These bugs <em>don't</em> affect the effectiveness of training, only the accuracy of <code>get_latest_training_loss()</code>. reporting.</p>
<p>Manually resetting the internal tally to <code>0.0</code> at the start of each epoch, from your own callback, may improve the reporting enough for your purposes, if your jobs aren't especially large.</p>
<p>However, other things to note about your apparent setup:</p>
<ul>
<li><p>Keep in mind that a full epoch's loss can hint about whether more SGD training will be beneficial on the model's internal training goals, but is <em>not</em> a reliable indicator of the quality of the final word-vectors for other downstream uses. A model with more loss might give better vectors, a model with less loss might (through overfitting) give word-vector that are less generally-useful for typical purposes. So don't rely on loss as a guide to other meta-optimization, only the choice of <code>epochs</code>/<code>alpha</code> or potential early-stopping.</p>
</li>
<li><p><code>min_count=1</code> is essentially always a mistake with <code>Word2Vec</code>, giving you not just bad vectors for the words that only appear 1 (or a few) times, but also making the other word-vectors, for more common words, worse than they'd be with a more sensible <code>min_count</code> choice. This is especially the case if you truly have enough data to justify large <code>vector_size=300</code> vectors.</p>
</li>
<li><p>The atypical parameter <code>negative=1</code> is also almost certainly sub-optimal, and <code>window=10</code> is another deviation from defaults that will usually only make sense if you've got some repeatable quantitative quality evaluation that can assure you it's an improvement over the default.</p>
</li>
</ul>
",1,0,127,2023-04-01 18:14:07,https://stackoverflow.com/questions/75908046/why-does-the-loss-of-gensim-word2vec-model-deteriorate-in-every-epoch
&#39;pseudocorpus&#39; no longer available from &#39;gensim.models.phrases&#39;?,"<p>Several months ago, I used &quot;pseudocorpus&quot; to create a fake corpus as part of phrase training using Gensim with the following code:</p>
<pre><code>from gensim.models.phrases import pseudocorpus 

corpus = pseudocorpus(bigram_model.vocab, bigram_model.delimiter, bigram_model.common_terms)
bigrams = []
for bigram, score in bigram_model.export_phrases(corpus, bigram_model.delimiter, as_tuples=False):
    if score &gt;= bigram_model.threshold:
        bigrams.append(bigram.decode('utf-8'))
</code></pre>
<p>Now when I run the code, I got the following error message:</p>
<pre><code>ImportError: cannot import name 'pseudocorpus' from 'gensim.models.phrases'
</code></pre>
<p>I'm using Gensim 4.2.0. Is pseudocorpus() no longer available with Gensim 4.2.0?</p>
<p>Thanks a lot!</p>
","python, python-3.x, gensim","<p>I believe the main internal consumer of a <code>pseudocorpus()</code> result, the <code>.export_phrases()</code> method, was improved to achieve the same goals more efficiently, so that method disappeared – as it hadn't really been promoted as part of the public functionality of the module.</p>
<p>Can you make use of <code>.export_phrases()</code> for your purposes?</p>
<p>If not, can you say a bit more about how you were using the (odd synthetic) 'pseudocorpus'?</p>
<p>If all else fails, the prior functionality was a pretty simple extraction from the model's state, and you can view the last version of the function before it was refactored-away at the project's open source repository:</p>
<p><a href=""https://github.com/RaRe-Technologies/gensim/blob/da8847a04f9ee56702cb81a0218cd5a57e1f24e6/gensim/models/phrases.py#L750"" rel=""nofollow noreferrer"">https://github.com/RaRe-Technologies/gensim/blob/da8847a04f9ee56702cb81a0218cd5a57e1f24e6/gensim/models/phrases.py#L750</a></p>
<p>So, you could simply use that as a guide to reimplementing equivalent extraction in your own code.</p>
",1,0,55,2023-04-06 07:12:56,https://stackoverflow.com/questions/75946772/pseudocorpus-no-longer-available-from-gensim-models-phrases
Is it more correct to export bigrams from the bigram model or the trigram model in Gensim?,"<p>After I train a bigram model and a trigram model using Gensim, I can export the bigrams from the bigram model. Alternatively, I can export the bigrams from the trigram model. I find that the bigrams from the two models can be quite different. There is a large overlap. But there is a large number appearing in only one of the lists. What is the right way? Thanks!</p>
<pre><code>bigram_model = gensim.models.Phrases(texts_unigram)
texts_bigram = [bigram_model[sent] for sent in texts]
trigram_model = gensim.models.Phrases(texts_bigram)

# Get from the bigram model
bigrams1 = list(bigram_model.export_phrases().keys())

# Get from the trigram model
ngrams = list(trigram_model.export_phrases().keys()) # This includes both bigrams and trigrams
bigrams2 = [g for g in ngrams if g.count(&quot;_&quot;)==1]
</code></pre>
","python, gensim","<p>When you're applying the <code>Phrases</code>-class statistical bigram-combinations multiple times, you're in experimental territory that's doesn't have well-established rules-of-thumb.</p>
<p>So you should be guided by your own project's evaluations of model effectiveness: for whatever your downstream purposes are, which set of n-grams works better?</p>
<p>Note also:</p>
<ul>
<li>Applying bigram-combinations twice may create not just trigrams (unigrams that are found to combine well with a neighboring bigram) but even quad-grams (bigrams that combine well with neighboring bigrams).</li>
<li>The crude statistical thresholds used by the <code>Phrases</code> class will often combine things that don't match human intuitions, &amp; miss other things you might see as useful multiword n-grams, and tuning will often tend to improve some pairings only at the expense of others. Ultimately, the n-grams created this way may not be appropriate or attractive, for end-user display, but <em>might</em> still help as the input for classification/info-retrieval tasks.</li>
</ul>
",1,0,53,2023-04-08 08:03:52,https://stackoverflow.com/questions/75964037/is-it-more-correct-to-export-bigrams-from-the-bigram-model-or-the-trigram-model
LSTM Model Validation Accuracy not following Training Accuracy,"<p>I'm building a LSTM model to classify homophobic twitters in PT-BR. I have a dataset with 5k tweets already balanced in homophobic and non-homophobic. The thing is, I've already tested with three diffenrent models and all of them the validation accuracy/loss does not follow up the training validation/loss, I would like to know if I'm doing something wrong, or that can be a dataset problem that does not follows a formal writing.</p>
<h1><strong>Params</strong></h1>
<pre><code>dim =300
epochs = 100
lstm = 150
window = 7
</code></pre>
<h1><strong>Word2Vec</strong></h1>
<pre><code>w2v_model = gensim.models.word2vec.Word2Vec(vector_size=dim, 
                                            window=window, 
                                            min_count=10, 
                                            workers=8)
</code></pre>
<h1><strong>Model</strong></h1>
<pre><code>model = Sequential()
model.add(embedding_layer)
model.add(Dropout(0.5))
model.add(LSTM(lstm, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()
model.compile(loss='binary_crossentropy',
              optimizer=&quot;adam&quot;,
              metrics=['accuracy'])
</code></pre>
<h1><strong>Training Example</strong></h1>
<pre><code>model_history=model.fit(X_train, y_train,epochs=epochs,validation_split=0.1,verbose=1)
</code></pre>
<h1><strong>Results</strong></h1>
<p><a href=""https://i.sstatic.net/IYX8g.png"" rel=""nofollow noreferrer"">Accuracy</a>
<a href=""https://i.sstatic.net/hewI8.png"" rel=""nofollow noreferrer"">Loss</a></p>
","python, machine-learning, keras, lstm, gensim","<p>It's unclear why you're instantiating a Gensim <code>Word2Vec</code> model, as it isn't shown to be trained or used by your other code.</p>
<p>But: if you have a mere 5000 tweets – often averaging less than 20 words per tweet – then you've only got about 100,000 training words. That's way too little to train a <code>Word2Vec</code> model from scratch, much less one with a full 300 dimensions per word.</p>
<p>Most generally, when a model shows good performance (like high accuracy) on the data used for training, but then poor performance on held-out validation data, one common cause is 'overfitting'.</p>
<p>Essentially, the model is memorizing learning to give answers based on details in your training data that are, in generalizable truth, irrelevant – but nonetheless turn out to have memorizable but unhelpful correlations with the desired answers strictly within the limited training data. Trying to apply those same misleading correlations on out-of-training data ruins model performance.</p>
<p>Far more data often helps: the nonsense correlations may cancel out. Using a smaller model that's forced to only notice stronger (&amp; thus potentially more-reliable) correlations may also help.</p>
",0,-2,217,2023-04-22 16:08:26,https://stackoverflow.com/questions/76080611/lstm-model-validation-accuracy-not-following-training-accuracy
"Error while loading Word2Vec model using linux, but running well in windows","<p>My code here:</p>
<pre><code>from gensim.models import Word2Vec, KeyedVectors
wv_model = KeyedVectors.load('word2vec.model')
</code></pre>
<p>It raised errors while running in Ubuntu, but it is running well in Windows11.
I tried to change different vesions of it and use pickle and to solve this problem, however raised the same error.</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
File ~/autodl-tmp/test.py:3, in &lt;module&gt;
      1 import gensim
      2 from gensim.models import Word2Vec, KeyedVectors
----&gt; 3 wv_model = KeyedVectors.load('word2vec.model')

File ~/miniconda3/lib/python3.8/site-packages/gensim/utils.py:486, in SaveLoad.load(cls, fname, mmap)
    482 logger.info(&quot;loading %s object from %s&quot;, cls.__name__, fname)
    484 compress, subname = SaveLoad._adapt_by_suffix(fname)
--&gt; 486 obj = unpickle(fname)
    487 obj._load_specials(fname, mmap, compress, subname)
    488 obj.add_lifecycle_event(&quot;loaded&quot;, fname=fname)

File ~/miniconda3/lib/python3.8/site-packages/gensim/utils.py:1461, in unpickle(fname)
   1447 &quot;&quot;&quot;Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.
   1448 
   1449 Parameters
   (...)
   1458 
   1459 &quot;&quot;&quot;
   1460 with open(fname, 'rb') as f:
-&gt; 1461     return _pickle.load(f, encoding='latin1')

TypeError: __randomstate_ctor() takes from 0 to 1 positional arguments but 2 were given
</code></pre>
<p>I had three files pretrained.</p>
<p><a href=""https://i.sstatic.net/ciHTb.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>How could I fix this problem and why it happened?????</p>
","python, deep-learning, nlp, pickle, gensim","<p>There's a fair chance that the root cause of the error is some mismatched interpreter/library versions.</p>
<p>If so, then by ensuring that you're using the exact same versions of Python, Numpy, &amp; Gensim on the Ubuntu system (where you're getting an error) as you've seen succeed on the Windows11 system, it will work on Ubuntu as well.</p>
<p>One other less-likely possibility might be some corruption/truncation of the <code>word2vec.model</code> file (&amp; its supporting files). Checking that the files are identical in both places – by size and secure checksum – could rule-out any problems there.</p>
",0,0,183,2023-05-02 16:23:03,https://stackoverflow.com/questions/76156920/error-while-loading-word2vec-model-using-linux-but-running-well-in-windows
Fine tune a custom word2vec model with gensim 4,"<p>I am new using gensim, especially with gensim 4. To be honest, I found quite hard to understand the docs how to fine-tune a pre-trained word2vec model.
I have a binary pre-trained model saved local. I would like to fine tune this model on new data.</p>
<p>My questions are;</p>
<ul>
<li>how to create the vocab merging both vocabs?</li>
<li>is that the correct approach to fine-tune a word2vec model?</li>
</ul>
<p>So far i have created the following code:</p>
<pre><code># path to pretrained model
pretrained_path = '../models/german.model'

# new data
sentences = df.stem_token_wo_sw.to_list() # Pandas column containing text data

# Create new model
w2v_de = Word2Vec(
    min_count = min_count,
    vector_size = vector_size,
    window = window,
    workers = workers,
)

# Build vocab
w2v_de.build_vocab(sentences)

# Extract number of examples
total_examples = w2v_de.corpus_count

# Load pretrained model
model = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)

# Add previous words from pretrained model
w2v_de.build_vocab([list(model.key_to_index.keys())], update=True)

# Train model
w2v_de.train(sentences, total_examples=total_examples, epochs=2)

# create array of vectors
vectors = np.asarray(w2v_de.wv.vectors)
# create array of labels
labels = np.asarray(w2v_de.wv.index_to_key) 

# create dataframe of vectors for each word
w_emb = pd.DataFrame(
    index = labels,
    columns = [f'X{n}' for n in range(1, vectors.shape[1] + 1)],
    data = vectors,
)
</code></pre>
<p>After training I use PCA to reduce the dimensions from 300 to two, in order to plot the word-embedding space.</p>
<pre><code># create pipeline
pipeline = Pipeline(
    steps = [
        # ('scaler', StandardScaler()),
        ('pca', PCA(n_components=2)),
    ]
)

# fit pipeline
pipeline.fit(w_emb)

# Transform vectors
vectors_transformed = pipeline.transform(w_emb)

w_emb_transformed = (
    pd.DataFrame(
        index = labels,
        columns = ['PC1', 'PC2'],
        data = vectors_transformed,
    )
)
</code></pre>
<p>The <code>labels</code> and <code>vectors</code> do only contain the new words, and not the old + new words and so does my plot and PCA values.</p>
","python, scikit-learn, nlp, gensim, word2vec","<p>There are no official Gnesim docs on how to fine-tune a <code>Word2Vec</code> model because there's no well-established/reliable way to do fine-tuning &amp; be sure it's helping.</p>
<p>There's thus no direct support in Gensim, nor standard recipe that Gensim could recommend to non-expert users.</p>
<p>People have patched together approaches, reaching into Gensim steps/models directly, to try to accomplish fine-tuning. But the average quality of such write-ups that I've seen is <em>very poor</em>, with little evaluation of whether the steps are working, or discussion of the tradeoffs and considerations when expanding beyond the write-up's toy setup.</p>
<p>That is: they're often misleading the unaware into thinking this is a well-esablished process, with dependable results, when it's not.</p>
<p>Regarding your process, some comments:</p>
<ul>
<li>Your initial creation of a vocabulary will get all of your corpus's words into the model, with accurate frequency counts based on your corpus. (Frequencies affect how a model does negative-sampling &amp; frequent-word downsampling, and which words get ignored entirely because they appear fewer times than the configured <code>min_count</code>.)</li>
<li>You are then successfully requesting the model's vocabulary expand with the <code>.build_vocab(..., update=True)</code> call - but by providing a mere list of the words in the new corpus, every word gets an effective occurence-count of just <code>1</code>. With sensible values of <code>min_count</code> (such as the default <code>5</code>, or higher when your corpus is larg enough), none of those word from the pre-trained model will be added to the vocabulary.</li>
<li>But even if you did fix this step – either setting <code>min_count</code> unwisely-low, or artificially repeating the words – the <code>build_vocab()</code> step only makes slots for a word, &amp; randomly-initialized its vector to ready the word for training. You're not doing anything to copy over the actual vectors from the <code>model</code> into <code>w2v_de</code>. So all those 'borrowed' words will just be untrained noise in your actual model. And, these words don't have accurate frequency counts to participate properly in training.</li>
<li>When you train, on just your corpus, only your local-corpus words will appear in the corpus, and thus appear in the positive word-to-word training examples. But some of the imported words (if any) will occasionally be chosen as negative-examples, if you're using negative-sampling mode. (But, they won't be chosen at the typical frequencies - because of the lack of frequency info.) So you'll have a weird training run, primarily updating only your corpus's words, sometimes negative-example updating the other words (but never positive-updating them). The randomly-initialized imported words will thus be skewed further, but not in any useful way.</li>
</ul>
<p>At the end, you <em>might</em> have passable vectors for your in-corpus words. (Though: <code>epochs=2</code> is unlikely to be sufficient training unless your corpus is so vary large that every word of interest appears in many, many diverse contexts.) But the words you tried to import will have just junk vectors, having been initialized randomly, never influenced in their weights by your pretrained <code>model</code> at all, just skewed a bit by sometimes appearing as negative examples.</p>
<p>In short: a mess, with the extra non-standard steps attempting <code>fine-tuning</code> doing nothing useful. (If you've copied this pattern from an online resource faithfully – that resource may have been offered by an author that didn't know what they were doing.)</p>
<p>A far surer approach, if you find your corpus is missing words, is to obtain a larger corpus. As one example, if your pretrained vectors were trained on something like Wikipedia, you can just mix your corpus with Wikipedia texts, to have a combined corpus with good usage example of all the same words. (In some cases, you might be able to find corpus-extending materials that are more appropriate for  your project/domain than generic Wikipedia reference text. Alternatively, you might choose to interleave &amp; repeat your corpus to essentially give your texts greater weight, in the combined corpus.)</p>
<p>A straightforward from-scratch training-run on this new extended corpus will co-train all words in the same model, with accurate counts matching words' appearances in the combined corpus.</p>
<p>Another approach that's sometimes used to re-use word-vectors from elsewhere is to learn a projection between your new/small model, and the pretrained/larger model, based on words that are shared between the two models. Then, use that projection to move the extra words needed – in one or the other model – to new positions, that render them comparable, &quot;in the same coordinate space&quot;, to the other imported vectors. There's an example of doing this in the Gensim <code>TranslationMatrix</code> class &amp; demo notebook.</p>
",3,2,910,2023-05-03 08:16:47,https://stackoverflow.com/questions/76161758/fine-tune-a-custom-word2vec-model-with-gensim-4
Cannot import name &#39;LabeledSentence&#39; from &#39;gensim.models.doc2vec&#39;,"<p><a href=""https://i.sstatic.net/1vBlt.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1vBlt.jpg"" alt=""enter image description here"" /></a></p>
<p>This is the error I get every time I try to execute the code. <strong>cannot import name 'LabeledSentence' from 'gensim.models.doc2vec'</strong>
The source of the problem (the directory path indicated) is this file:</p>
<p><a href=""https://i.sstatic.net/VAV7X.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VAV7X.jpg"" alt=""enter image description here"" /></a></p>
<p>It doesn't seems to have a &quot;from gensim.models.doc2vec import LabeledSentence&quot; so I don't know how to fix the problem. I saw in a similar topic that the library gensim.models.doc2vec LabeledSentence is deprecated so I should substite gensim.models.doc2vec with gensim.models.deprecated.doc2vec but..where? It doesn't appear in the code. This is not the first time I have problems like this with my imported code. Can anyone help me?
The term &quot;LabeledSentence&quot; doesn't even appear in the code</p>
","python, compiler-errors, gensim","<p>It appears you're using some 5-year-old code (perhaps <a href=""https://github.com/FakeNewsDetection/FakeBuster/tree/master"" rel=""nofollow noreferrer"">https://github.com/FakeNewsDetection/FakeBuster/tree/master</a> ?) that's dependent on a much-older version of Gensim.</p>
<p>You <em>might</em> be able to get over this one particular error by updating references to <code>LabeledSentence</code> to instead import/use <code>gensim.models.doc2vec.TaggedDocument</code>.</p>
<p>However, there are likely to be other problems that need fixing to update this code – and some might just silently degrade results, rather than give big errors pointing at exactly the lines-of-code to change.</p>
<p>So you'd need to read &amp; understand the code you're trying to use, and via error messages, logged output, &amp; measured results, verify that it's doing what you wanted. (That is, code this old can't be used as a trustworthy 'black box' that just does-what-it-says.)</p>
<p>You <em>could</em> try guessing exactly which older version of Gensim was used by the project, but as it hasn't formally declared the version it needs (in docs or a <code>requirements.txt</code>-like spec), that would require some trial-and-error based on then-active releases. But, even if it works, it'd mean your Gensim &amp; perhaps other related libraries would be years behind current libraries &amp; online help with regard to performance, bugs, &amp; usage tips.</p>
<p>If you are not specifically wedded to the idea of using that particular bit of outdated code, you might consider building whatever your current task/assignment is on some other more-recently-maintained examples.</p>
",1,0,560,2023-06-26 14:27:25,https://stackoverflow.com/questions/76557640/cannot-import-name-labeledsentence-from-gensim-models-doc2vec
S3 object as gensim LineSentence,"<p>Is it possible to use a txt or jsonl file in an s3 bucket as the <code>corpus_file</code> input for a gensim Doc2Vec model? I am looking for something of the form:</p>
<pre><code>Doc2Vec(corpus_file=&quot;s3://bucket_name/subdir/sample.jsonl&quot;)
</code></pre>
<p>When I run the above line, I get the following error:</p>
<pre><code>TypeError: Parameter corpus_file must be a valid path to a file, got 's3://bucket_name/subdir/sample.jsonl' instead.
</code></pre>
<p>I have also tried creating an iterator object that iterates through the file and yields its lines, and passing it as the <code>corpus_file</code> argument. But I get the same TypeError.</p>
<p>Please note that I am specifically looking to use the <code>corpus_file</code> argument instead of the <code>documents.</code></p>
","python, amazon-s3, nlp, gensim, doc2vec","<p>The <code>corpus_file</code> mode requires random-seek access to the file for its technique, which involves every worker thread opening its own unique file view on distinct ranges of the file. Such access is not well-supported for S3 (HTTP GET) access.</p>
<p>To use <code>corpus_file</code> mode, download the file to a local volume whose filesystem offers efficient seek access.</p>
<p>Or, supply things as a corpus iterable - which can re-iterate over a remote streamed file multiple times, but won't achieve the same high thread utilization. (From an iteratable, even if you have 16+ cores, you'll usually get optimal throughput with no more than 6-12 worker threads – even <em>if</em> you've eliminated IO &amp; expensive in-iterable preprocesing from the setup. The exact optimal number of workers depends on other model parameters – it's especially sensitive to <code>vector_size</code>, <code>negative</code>, &amp; <code>window</code>.)</p>
",1,0,62,2023-06-27 03:47:20,https://stackoverflow.com/questions/76561482/s3-object-as-gensim-linesentence
Infer document vectors for pretrained word vectors,"<p>the following questions refers to the implementation of Word2Vec and Doc2Vec algorithms provided by the great gensim package.</p>
<p>I know similar questions have been asked, however, I feel the given answers seem not to be the best solution for my use-case.</p>
<p>I have a large corpus of 110,000 financial reports with an average length of approx. 30,000 tokens. My goal is to train word vectors first. In a next step, I want to infer doc vectors on sentence level and examine if the vector is similar to the average vector of topic words, e.g., sustainability, environmental, emissions.</p>
<p>My first idea was to use the possibility to train word vectors and doc vectors at the same time. However, if I split the reports in sentences, multiple millions of sentences (documents) result which exceeds my memory (32GB) for saving the arrays of words and documents.</p>
<p>The next idea is to treat every report as a single document for training. I read on github that documents only are trained up to a token limit of 10,000 words but I can split a document into parts of 10,000 token size and use the same tag. So far, this results in a trained model which gives me the ability to train meaningful (in the sense they learned word similarity) word vectors and use the infer_vector method later to infer doc vectors for individual sentences. However, it does not feel as a very good solution because I first train a large number of document vectors which are not used for anything.</p>
<p>My desired goal would be to train a Word2Vec model first, use the word vectors for an &quot;empty&quot; Doc2Vec model which gives me access to the infer_vector method when needed. My understanding is, that this is not easily possible because no pre-trained word vectors can be inizialized for a Doc2Vec model, right? I know this is not necessary under common use-cases related to Doc2Vec, but I hope with this question I could clarify why it would make sense in my case.</p>
<p>I also would appreciate guidance how I could use the internal C-functions which are used by the infer_vector method for training a single docvec given word vectors, unfortunately, I have no C experience at all.</p>
<p>Any help or advice would be highly appreciated, and to be honest I hope Gordon Mohr or someone else from the gensim team might read this;)</p>
<p>Best regards
Ralf</p>
","python, gensim, word2vec, doc2vec","<p>The <code>Doc2Vec</code> algoithm, called &quot;Paragraph Vector&quot; in the papers that introduced it, is not initialized from external pretrained word-vectors, nor is creating word-vectors a distinct 1st step of creating a <code>Doc2Vec</code> model from scratch that could somehow be done separately, or cached/reused across runs. So not even the internal inference routines can do anything with just some external word-vectors - they depend on model weights separate from word-vectors, learned from doc-to-word relations seen in training.</p>
<p>(I've occasionally seen some variants/improvised-changes that move a bit in the direction of taking outside word-vectors, but I've not seen evidence such variations outperform the usual approach, and they're not implemented in Gensim.)</p>
<p>In standard <code>Dov2Vec</code>, rather that taking word-vectors as an input, <em>if</em> the chosen mode of <code>Doc2Vec</code> creates typical per-word word-vectors at all, they get co-trained simultaneously with the doc-vectors.</p>
<ul>
<li><p>In the plain &quot;PV-DBOW&quot; mode – <code>dm=0</code> – no typical word-vectors are trained at all, <em>only</em> doc-vectors &amp; the support for inferencing new doc-vectors. This mode is thus pretty fast and often works quite well for broad topical similarity for short docs of dozens to hundreds of words – because the <em>only</em> thing training is trying to do is predict in-doc words from candidate doc-vectors. In this mode, the <code>window</code> parameter is meaningless - every word in a doc affects its doc-vector.</p>
</li>
<li><p>You can optionally add to that PV-DBOW mode interleaved skip-gram word-vector training, by using the non-default <code>dbow_words=1</code> parameter. This co-training, using a shared output (center word) prediction layer, forces the word-vectors &amp; doc-vectors into a shared coordinate system – so that they're directly comparable to each other. The <code>window</code> parameter then affects the skip-gram word-to-word training, just like in word2vec skip-gram training. Training takes longer, by a factor of about the <code>window</code> value – and in fact the model is spending more total computation making the words predict their neighbors than the doc-vector predicting the doc words. So there's a margin at which improving the word-vectors may be 'crowding out' improvement of the doc-vectors.</p>
</li>
<li><p>The PV-DM mode – the default <code>dm=1</code> parameter – inherently uses a combo of condidate doc-vector &amp; neighbor words to predict each center word. That makes <code>window</code> relevant, and inherently puts the word-vectors &amp; doc-vectors into a shared comparable coordinate space, without as much overhead for larger <code>window</code> values as the interleaved skip-gram above. There <em>may</em> still be some reduction in doc-vector expressiveness to accomodate all the word-to-word influences.</p>
</li>
</ul>
<p>Which is best for a particular set of docs, subject domain, and intended downstream use is really a matter for experimentation. As you've mentioned comparing doc-vectors to word-vectors is an aim, only the latter two modes above – PV-DBOW with optional skip-gram, or PV-DM – would be appropriate. (But if you don't absolutely need that, &amp; have time to run more comparisons, I'd still recommend trying plain PV-DBOW for its speed &amp; strength in some needs.)</p>
<p>Let's assume your sentences are an average of 20 tokens each, so your 110k docs * 30k tokens / (20 tokens/sentence) give you 165 million sentences. Yes, holding (say) 300-dimensional doc-vectors (1200 bytes each) in-training for 165 million texts has prohibitive RAM costs: 198 GB.</p>
<p>As you've noted, you could use a model trained for only the 110k docs to then infer doc-vectors for other smaller texts, liek the sentences. You shouldn't worry about those 'wasted' 110k doc-vectors: they were necessary to create the inferencing capability, you <em>could</em> throw them away after training (&amp; inference will still work), and maybe you will have some reason to compare words or sentences or new docs or other doc-fragments to those full-doc vectors.</p>
<p>You could also consider training on chunks larger than sentences but smaller than your full docs, like paragraphs or sections, if you can segment docs that way. You could conceivably even use arbitrary Ntoken chunks, and it might work well - only way to know is to try. This sort of algorithm isn't super–sensitive to small changes in tokenization/text-segmenting, as it's the bulk of the data, and broad relationships, it's modeling.</p>
<p>You can also simultaneously train doc-vectors for different levels of text, by supplying more than one 'tag' (key for looking up the doc-vector post-training) per example text. That is, if your full document with ID <code>d1</code> has 3 distinct sections <code>d1s1</code>, <code>d1s2</code>, <code>d1s3</code>, you could feed it the doc as 3 texts: the 1st section with tags <code>['d1', 'd1s1']</code>, the 2nd with tags <code>['d1', 'd1s2']</code>, the 3rd with tags <code>['d1', 'd1s3']</code>. Then all the texts contribute to the train-tuning of the <code>d1</code> doc-vector, but the subsections only affect the respective subsection-vectors. Whether that'd be appropriate depends on your goals – &amp; the effect of supplying multiple tags varies a bit between modes – but it may also be worth some experiments.</p>
",1,0,154,2023-07-26 16:37:55,https://stackoverflow.com/questions/76773386/infer-document-vectors-for-pretrained-word-vectors
use gensim with a pyarrow iterable,"<p>Consider this code</p>
<pre><code>import pyarrow.parquet as pq
from gensim.models import Word2Vec

parquet_file = pq.ParquetFile('/mybigparquet.pq')
for i in parquet_file.iter_batches(batch_size=100):
    print(&quot;training on batch&quot;)
    batch =  i.to_pandas()
  
    model = Word2Vec(sentences= batch.tokens, vector_size=100, window=5, workers=40, min_count = 10,epochs = 10)
</code></pre>
<p>As you can see, I am trying to train a <code>word2vec</code> model using a very large parquet file that does not fit entirely into my RAM. I know that <code>gensim</code> can operate on iterables (not generators, as the data need to be scanned twice in word2vec) and I know that <code>pyarrow</code> allows me to generate batches (of even one row) from the file.</p>
<p>Yet, this code is not working correctly. I think I need to write my <code>pyarrow</code> loop as a proper generator but I do not know how to do this.</p>
<p>What do you think?
Thanks!</p>
","python, gensim, pyarrow","<p>You're creating a new model every iteration of your loop, so at best, you'd only have, at the end, a single <code>model</code> from the last iteration – probably not what you want.</p>
<p>Instead, you need to give <code>Word2Vec</code> the kind of re-iterable Python sequence that it wants, where each item is a Python list of string word tokens.</p>
<p>Unless there's something in <code>pyarrow</code>/etc, this likely means your own class, that implements the Python 'iterable' interface, so that each time an <em>iterator</em> is requested from it, it starts a new iteration. Here's a <a href=""https://realpython.com/python-iterators-iterables/#the-iterable-protocol"" rel=""nofollow noreferrer"">reasonable article section about how iterables work</a>. You'd be likely to make use of the generator pattern to achieve this, as mentioned <a href=""https://realpython.com/python-iterators-iterables/#exploring-alternative-ways-to-write-__iter__-in-iterables"" rel=""nofollow noreferrer"">later in the same article</a>.</p>
<p>(Essentially: your <code>__iter__()</code> special function will start the iteration, and loop over the batches, always using <code>yield</code> to return the single next item. If you class is specialized for use as a Gensim <code>Word2Vec</code> corpus, each item yielded should be a Python <code>list</code> with string tokens.)</p>
<p>If you're really only using one column named 'tokens', no need to convert things through a Pandas <code>DataFrame</code> - that's just extra conversion/storage overhead. Just ensure the one column of interest is processed the right way –  if each entry is already a Python list-of-strings, you're fine, but if it's a string, you might have to break into a list-of-tokens.</p>
<p>When you're doing it right, you'll only pass the entire corpus's iterable into the <code>Word2Vec</code> class once - it will then read it, over the many passes required for training, as it needs it by requesting repeated iterations. (It has no idea where the data is coming from, or how your iterable is implemented. It just knows it's got an object that it can request an iterator from, and that iterator gives it one of the right kind of item each <code>next()</code>, until reaching the end of one iteration.)</p>
",1,0,89,2023-09-21 04:59:43,https://stackoverflow.com/questions/77147297/use-gensim-with-a-pyarrow-iterable
What are the &quot;word types&quot; in gensim?,"<p>I am training a large <code>word2vec</code> model with <code>gensim</code> and using logging to follow the training process. The log shows that</p>
<pre><code>PROGRESS: at sentence #3060000, processed 267654284 words, keeping 940042 word types
</code></pre>
<p>What are these word types? The unique words among the 200M+ tokens in the data? I cannot find anything in the documentation.</p>
","python, gensim","<p>Yes, this is reporting progress during the initial 1st vocabulary-survey, and that's the logging's odd terminology for unique word-tokens discovered.</p>
<p>During the scan, this will be a precise count of the unique tokens encountered, <em>unless</em> you're using the <code>max_vocab_size</code> parameter which can trigger some mid-scan purging of rarer tokens. (I <strong>strongly recommend</strong> against using <code>max_vocab_size</code> setting unless there's no way to proceed without it, because of the non-intuitive effects it has on the survey's running count &amp; final vocabulary size.)</p>
<p>At the end of the scan, there will also be a report of the final unique count, then the unique count after your <code>min_count</code> is applied.</p>
<p>If you want to place a hard cap on the known vocabulary – for example to cap the size of your model during training – the <code>max_final_vocab</code> parameter can be used. (It only trims to the exact most-frequent-N words, at the end of the full scan, rather than applying interim larger mid-scan cullings that can be triggered by <code>max_vocab_size</code>.)</p>
",2,1,53,2023-09-21 13:06:36,https://stackoverflow.com/questions/77150535/what-are-the-word-types-in-gensim
Tokenizing and summarizing Textual data by group efficiently in Python,"<p>I have a dataset in Python that look like this one:</p>
<pre><code>data = pd.DataFrame({
    'ID': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],
    'TEXT': [
        &quot;Mouthwatering BBQ ribs cheese, and coleslaw.&quot;,
        &quot;Delicious pizza with pepperoni and extra cheese.&quot;,
        &quot;Spicy Thai curry with cheese and jasmine rice.&quot;,
        &quot;Tiramisu dessert topped with cocoa powder.&quot;,
        &quot;Sushi rolls with fresh fish and soy sauce.&quot;,
        &quot;Freshly baked chocolate chip cookies.&quot;,
        &quot;Homemade lasagna with layers of cheese and pasta.&quot;,
        &quot;Gourmet burgers with all the toppings and extra cheese.&quot;,
        &quot;Crispy fried chicken with mashed potatoes and extra cheese.&quot;,
        &quot;Creamy tomato soup with a grilled cheese sandwich.&quot;
    ],
    'DATE': [
        '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-01', '2023-02-02',
        '2023-02-02', '2023-02-01', '2023-02-01', '2023-02-02', '2023-02-02'
    ]
})
</code></pre>
<p>What I'd like to do is group by DATE and get the frequency of each token after removing punctuation. I'm very new to the Python environment; I come from R, and I have been looking into the gensim library for further reference. It looks quite complicated to me. My desired output would look like this: for each group (DATE), we'll have the frequency of each unique token.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>TOKEN</th>
<th>SUBTOTAL</th>
<th>DATE</th>
</tr>
</thead>
<tbody>
<tr>
<td>cheese</td>
<td>5</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>and</td>
<td>5</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>with</td>
<td>5</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>extra</td>
<td>2</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>mouthwatering</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>bbq</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>ribs</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>coleslaw</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>delicious</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>pizza</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
<tr>
<td>pepperoni</td>
<td>1</td>
<td>1/02/2023</td>
</tr>
</tbody>
</table>
</div>
<p>In R this can be done very easy with quanteda like this:</p>
<pre><code>corpus_food&lt;-corpus(data,
                  docid_field = &quot;ID&quot;,
                  text_field = &quot;TEXT&quot;)

corpus_food %&gt;%
  tokens(remove_punct = TRUE) %&gt;% 
  dfm() %&gt;% 
  textstat_frequency(groups = lubridate::date(DATE)) 
</code></pre>
<p>Which only creates a corpus and then tokenizes to remove punctuation. Later, it creates a document-term matrix and finally summarizes the tokens and their frequencies by group.</p>
<p>I am in no way comparing the two languages, Python and R. They are amazing, but at the moment, I'm interested in a very straightforward and fast method to achieve my results in Python. If perhaps you don't use the gensim library, I'd still be interested in a way to achieve what I'm looking for in a faster and more efficient way in Python. I'm new to Python.</p>
","python, pandas, dataframe, nlp, gensim","<p>I would simply <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extractall.html"" rel=""nofollow noreferrer""><code>extractall</code></a> the words then <a href=""https://pandas.pydata.org/docs/dev/reference/api/pandas.core.groupby.DataFrameGroupBy.value_counts.html"" rel=""nofollow noreferrer""><code>value_counts</code></a> :</p>
<pre><code>out = (
    data[[&quot;DATE&quot;]].join(
        data[&quot;TEXT&quot;].str.extractall(&quot;(\w+)&quot;)[0]
            .droplevel(1).rename(&quot;TOKEN&quot;).str.lower()
    ).groupby([&quot;DATE&quot;, &quot;TOKEN&quot;]).value_counts().reset_index(name=&quot;SUBTOTAL&quot;)
        .sort_values([&quot;DATE&quot;, &quot;SUBTOTAL&quot;], ascending=[True, False])
)
</code></pre>
<p>Output :</p>
<pre><code>print(out)

          DATE   TOKEN  SUBTOTAL
1   2023-02-01     and         5
4   2023-02-01  cheese         5
30  2023-02-01    with         5
10  2023-02-01   extra         2
0   2023-02-01     all         1
..         ...     ...       ...
51  2023-02-02   sauce         1
52  2023-02-02    soup         1
53  2023-02-02     soy         1
54  2023-02-02   sushi         1
55  2023-02-02  tomato         1

[57 rows x 3 columns]
</code></pre>
",0,1,59,2023-10-05 22:16:39,https://stackoverflow.com/questions/77240878/tokenizing-and-summarizing-textual-data-by-group-efficiently-in-python
How to get most similar words to a tagged document in gensim doc2vec,"<p>I have trained a doc2vec model.</p>
<pre><code>doc2vec = Doc2Vec(vector_size= 300,
                    window=10,
                    min_count=100, 
                    dm=1,
                    epochs=40)
doc2vec.build_vocab(corpus_file=train_data, progress_per=1000)
doc2vec.train(....)
</code></pre>
<p>The documents are tagged with incremental integer 0, 1, ...1000.</p>
<p>To get the top-n similar words to a document with tag=0, I used:</p>
<pre><code>doc_vector = doc2vec.dv[tag]
sims = doc2vec.wv.similar_by_vector(doc_vector, top_n=20)
</code></pre>
<p>The similarity makes sense, however, the similarity score are really looked &quot;weird&quot;, all of them are almost <code>1.0</code>. I checked <code>top_n=3000</code> and it is still around 1.0. Does it make sense to get all words with high similarity score.</p>
","nlp, gensim, cosine-similarity, doc2vec","<p>In traditional uses of this algorithm with varies natural-language texts, no, it's not typical to have the most-similar 'nearest neighbors' of texts all have near-<code>1.0</code> similarities.</p>
<p>That suggests something may amiss with your setup – unless of course your data does include lots of 'texts' that are nearly-identical.</p>
<p>Are you perhaps using some atypical corpus, maybe not natural language, where so many super-close similarity scores are still accurate &amp; useful? That's the ultimate test.</p>
<p>That is: if for a bunch of doc probes, the &quot;similar words&quot; vary, and are individually sensible/useful with regard to the origin documents, I'd not worry too much about the absolute magnitudes of the similarity scores.</p>
<p>Such scores have more meaning in comparison to each other than on any absolute scale. A similarity of <code>0.9</code> is not meaningfully interpretable as &quot;X% similar&quot; or even &quot;among top X% most-similar candidates. It only means, &quot;more similar than items with 0.8 similarity, and less similar than items with 0.95 similarity&quot;.</p>
<p>Some things to look at, if wanting to get better sense if things may be going wrong:</p>
<p>Are there at least <em>some</em> words that have far-lower similarity-scores, and do those make sense? Do doc-to-doc comparisons seem roughly sensible?</p>
<p>What is is the rough character &amp; size of your data, and how many docs &amp; unique-words are in your corpus? How many surviving-words (after applying the <code>min_count=100</code> cutoff) remain in the trained model?</p>
<p>If you run with logging enabled, do the various step &amp; progress reports suggest that a model of the expected size, &amp; training effort, is being created?</p>
",1,0,52,2023-10-19 21:08:16,https://stackoverflow.com/questions/77327117/how-to-get-most-similar-words-to-a-tagged-document-in-gensim-doc2vec
Negative values in data passed to MultinomialNB when vectorize using Word2Vec,"<p>I am currently working on a project where I'm attempting to use Word2Vec in combination with Multinomial Naive Bayes (MultinomialNB) for accuracy calculations.</p>
<pre><code>import pandas as pd
import numpy as np, sys
from sklearn.model_selection import train_test_split
from gensim.models import Word2Vec
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score
from datasets import load_dataset

df = load_dataset('celsowm/bbc_news_ptbr', split='train')
X = df['texto']
y = df['categoria']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

sentences = [sentence.split() for sentence in X_train]
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)

def vectorize(sentence):
    words = sentence.split()
    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]
    if len(words_vecs) == 0:
        return np.zeros(100)
    words_vecs = np.array(words_vecs)
    return words_vecs.mean(axis=0)

X_train = np.array([vectorize(sentence) for sentence in X_train])
X_test = np.array([vectorize(sentence) for sentence in X_test])
clf = MultinomialNB()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Precision:', precision_score(y_test, y_pred, pos_label='positive'))
</code></pre>
<p>However, I've encountered an error:</p>
<pre><code>ValueError(&quot;Negative values in data passed to %s&quot; % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)
</code></pre>
<p>I would appreciate any insights into resolving this issue.</p>
","python, scikit-learn, gensim, word2vec, naivebayes","<h2>The Error</h2>
<p>Each word2vec embedding for a word is a vector whose elements can take any real number value. This means that even after you take the mean of all vectors, there might be some negative values in the final vector.</p>
<p>This is not a problem. However, since you are using Multinomial Naive Bayes (MNB), it is causing problems.</p>
<p>Why? MNB assumes that the data follows a multinomial distribution, a generalization of the binomial distribution. It is based entirely on the idea of counts of successes (1s) and failures (0s). Thus, you can imagine why scikit-learn complains about MNB getting negative values.</p>
<h2>The Solution</h2>
<p>If you want to keep the model as MNB, you will have to do away with the negative values. Some ideas (as per <a href=""https://github.com/ClimbsRocks/machineJS/issues/176"" rel=""nofollow noreferrer"">this</a> link):</p>
<ul>
<li>Remove/filter all negative values from the final vector.</li>
<li>Normalize the vector to [0,1] range using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"" rel=""nofollow noreferrer""><code>MinMaxScaler</code></a>.</li>
</ul>
<p>You can also change the vectorization method from Word2Vec to <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorizer</a>, <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfidfVectorizer</a>. Tfidf will work even though it gives fractional values in the final vector. MNB is not designed to work with fractions, only integers, but it works in practice!</p>
<p>If you are okay with using another model, you can try some model options below:</p>
<ul>
<li><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"" rel=""nofollow noreferrer"">Gaussian Naive Bayes</a></li>
<li><a href=""https://scikit-learn.org/stable/modules/svm.html"" rel=""nofollow noreferrer"">Support Vector Machines</a></li>
<li><a href=""https://scikit-learn.org/stable/modules/tree.html"" rel=""nofollow noreferrer"">Decision Trees</a></li>
</ul>
<h2>Code</h2>
<p>Example using MinMaxScaler:</p>
<p>Just switch the line <code>clf = MultinomialNB()</code> to the following.</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline

clf = Pipeline([
    ('scaler', MinMaxScaler()),
    ('clf', MultinomialNB()),
])
</code></pre>
<h2>Transformers</h2>
<p>Depending upon your task, you might also want to check out transformers. They have their own vectorization method, generating dense semantic embeddings instead of working at a purely syntactic level as word2vec does. These models are much bigger, and computationally expensive, but will produce much better results if machine learning models fail to satisfy with accuracy.</p>
<h2>Further Readings</h2>
<ul>
<li>A comprehensive scikitlearn guide: <a href=""https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Binomial_distribution"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Binomial_distribution</a></li>
<li><a href=""https://stats.stackexchange.com/questions/169400/naive-bayes-questions-continus-data-negative-data-and-multinomialnb-in-scikit"">https://stats.stackexchange.com/questions/169400/naive-bayes-questions-continus-data-negative-data-and-multinomialnb-in-scikit</a></li>
<li>Transformers: <a href=""https://huggingface.co/learn/nlp-course/chapter1/4"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/chapter1/4</a></li>
</ul>
<p>Feel free to ask any questions!</p>
",2,1,625,2023-11-09 22:41:04,https://stackoverflow.com/questions/77456745/negative-values-in-data-passed-to-multinomialnb-when-vectorize-using-word2vec
Is it normal for all similarities to be positive in a gensim word2vec model?,"<p>Implementing a standard gensim word2vec model (continuous bag of words) on a series of Chinese characters, and for (comparison between chinese homophones and words of similar frequency) our cosine similarities are positive and weirdly high (&gt;0.3), any clue why this is the case? We have vector size set to 300 and min word set to 1, other than that no modifications made to the gensim's standard implementation of word2vec.</p>
<p>Also if anyone has any resources to look into for how to learn how these embeddings are actually generated that would be really helpful. Thank you very much in advance!</p>
","python, trigonometry, gensim, word2vec, similarity","<p>I don't think it's typical for <em>all</em> pairwise comparisons within a model to be positive, but large sets of tokens-of-interest for one specific investigation might all have positive similarities.</p>
<p><code>0.3</code> isn't necessarily a particularly high similarity, but also note such similarity values don't have any absolute interpretation. Rather, they only have meaning compared to the other similarities in the same model.</p>
<p>Depending on other chosen parameters, especially <code>vector_size</code> dimensionality, the <em>very best</em> nearest-neighbors to a token might be of nearly any positive similirity. That token B is most-similar to A, or that B is more-similar than other tokens, is more meaningful &amp; reliable than whether <code>cossim(a_vector, b_vector)</code> is ~<code>0.3</code> or ~<code>0.9</code>.</p>
<p>Separately, <code>min_count=1</code> is almost always a bad idea for these models. A token that only appears with one single context won't get a good vector from this sort of algorithm, but typical natural-lanaguae corpora may have many such one-off or few-off rare words – which altogether soak up a lot of training time to get nothing of value, while also seeving as 'noise' to worsen the vectors of other tokens which <em>do</em> have adequately numerous/varied contexts. Discarding rarer words, per the default <code>min_count=5</code> (or even higer values as soon as your corpus is large enough) is a best practice which results in far more improvement to the remaining tokens' vectors than loss from ignoring rarer words.</p>
<p>And, <code>vector_size=300</code> would only be appropriate if you have a large-enough corpus to justify it. How large is your corpus, in terms of (1) total tokens; (2) unique words (overall &amp; after applying a reasonable <code>min_count</code>); (3) average text length (in token-count). Most of these stats about your corpus will appear in logging output, as Genim <code>Word2Vec</code> works, if you enable Python logging to the <code>INFO</code> level.</p>
<p>If you continue to have problems, you should either expand (edit) this question, or ask a new question, with more details, including:</p>
<ul>
<li>corpus size, per above</li>
<li>exact <code>Word2Vec</code> parameters used</li>
<li>details on how your corpus is preprocessed/tokenized</li>
<li>example code &amp; output showing what you see as the problem - such as sets of token-to-token similarities that seem 'wrong' to you, or the results of some code that demonstrates some unlikely result like &quot;all&quot; similarities being in some tight positive range</li>
</ul>
",0,0,69,2023-11-30 18:23:58,https://stackoverflow.com/questions/77580859/is-it-normal-for-all-similarities-to-be-positive-in-a-gensim-word2vec-model
topic coherence (w2v) and its trend?,"<p>I tried to use <code>w2v</code> topic coherence score to evaluate the topic model based on <code>NMF</code>.
Below is the <code>w2v</code> coherences I have calculated.</p>
<p>And I want to know, is <code>w2v</code> coherence higher better?
Also, why the coherence scores get lower with more topics?</p>
<p><code>w2v</code> scores with different topic numbers:</p>
<p><img src=""https://i.sstatic.net/TpKpD.png"" alt=""w2v scores with different topic numbers"" /></p>
","python, gensim, word2vec","<blockquote>
<p>Is <code>w2v</code> coherence better if it's higher?</p>
</blockquote>
<p>Yes, generally a higher w2v coherence score means better topic quality. It basically says, that the words that make up each topic are more coherent.</p>
<blockquote>
<p>Why does the coherence scores get lower with more topics?</p>
</blockquote>
<p>Usually as you increase the number of topics, the model tries to divide the words into more and more groups. That then leads to less meaningful or more fragmented topics overall. That's a common trade off in topic modeling, which is why you might wanna look into other metrics like <a href=""https://en.wikipedia.org/wiki/Perplexity"" rel=""nofollow noreferrer"">perplexity</a> <em>in addition</em> to your w2v (or others, depending on your dataset)</p>
",1,0,62,2023-12-08 09:13:19,https://stackoverflow.com/questions/77625499/topic-coherence-w2v-and-its-trend
Python word2vec updates,"<p>I am trying to convert this old snippet of code to be in line with the updated version of gensim. I was able to convert the model.wv.vocab to model.wv.key_to_index but am having issues with the model[model.wv.vocab] and how to convert that.</p>
<p>Here is that the code looks like:</p>
<pre><code>model = Word2Vec(corpus, min_count = 1, vector_size = 5 )

#pass the embeddings to PCA
X = model[model.wv.vocab]

pca = PCA(n_components=2)
result = pca.fit_transform(X)

#create df from the pca results
pca_df = pd.DataFrame(result, columns = ['x','y'])
</code></pre>
<p>I have tried this:</p>
<pre><code>#pass the embeddings to PCA
X = model.wv.key_to_index
pca = PCA(n_components=2)
result = pca.fit_transform(X)

#create df from the pca results
pca_df = pd.DataFrame(result, columns = ['x','y'])
</code></pre>
<p>and keep getting errors. Here is what model.wv.key_to_index looks like:</p>
<pre><code>{'the': 0,
 'in': 1,
 'of': 2,
 'on': 3,
 '': 4,
 'and': 5,
 'a': 6,
 'to': 7,
 'were': 8,
 'forces': 9,
 'by': 10,
 'was': 11,
 'at': 12,
 'against': 13,
 'for': 14,
 'protest': 15,
 'with': 16,
 'an': 17,
 'as': 18,
 'police': 19,
 'killed': 20,
 'district': 21,
 'city': 22,
 'people': 23,
 'al': 24,
 'came': 996,
 'donbass': 997,
 'resulting': 998,
 'financial': 999}
</code></pre>
","python, pca, gensim, word2vec","<p>Your question doesn't show which errors you keep getting, which would usually help identify what's going wrong.</p>
<p>However, it looks like your original (older circa Gensim-3.x.x) code's line…</p>
<pre><code>X = model[model.wv.vocab]
</code></pre>
<p>…intends to assemble (per scikit-learn <code>PCA</code> api) a <em>array-like of shape (n_samples, n_features)</em>, by looking up every key in <code>model.wv.vocab</code> in <code>model</code> to assemble a new array, where each row is one of the <code>vocab</code> word-vectors.</p>
<p>The most direct replacement for that line would thus be to just use the model's existing internal array of word-vectors:</p>
<pre><code>X = model.wv.vectors
</code></pre>
<p>That is: for this use, you don't need to look up words individually, or create a new array of results. The existing in-model array is already exactly what you need.</p>
<p>Of course if you instead want to use subsets of words, you might want to look up mixtures of word individually. Still, for the specific case of using, say, the first 10 words (as in your sibling answer), you could also just use a numpy array 'view' on the existing array, accessed via Python slice notation:</p>
<pre><code>first_ten_word_vectors = model.wv.vectors[:10]
</code></pre>
<p>(As these models typically front-load the storage ordering of words with the most-frequent words, and the &quot;long-tail&quot; of less-frequent words have worse vectors and less utility, working with just the &quot;top-N&quot; of words, while ignoring other words, often improves overall resource usage <em>and</em> evaluated performance. More isn't alway better, when that 'more' is in the less-informative 'noise' of your training data or later texts.)</p>
<p>Two other unrelated notes on your example code – which I recognize may just be a toy demop of some larger exercise, but still useful to remember:</p>
<ul>
<li><p><code>min_count=1</code> is almost always a bad idea with <code>Word2Vec</code> &amp; similar algorithms. Words in your corpus with just 1, or a few, usage examples won't get good vectors – a single usage context won't be representative of the broad/bushy aspects of the word's real meaning – but <em>will</em>, in aggregate, take up a lot of the model's RAM &amp; training-time, and their <em>bad</em> representations will dilute the <em>good</em> representations for more-frequent words. So higher <code>min_count</code> values that actively discard rarer words often improve all three of {training_time, memory_usage, vector_quality} – and if you don't have enough training texts to use, or increase, the class default <code>min_count=5</code>, you may not have a task at which word2vec will work well until you get more data.</p>
</li>
<li><p>Similarly, the usual strengths of the word2vec algorithm are only shown with <em>high-dimensional</em> word-vectors – typically, at least 50-100 dimensions (and often 300+), with a commensurately-sufficient amount of training data. So, except for testing that code <em>runs</em>, or showing syntax, using any value as low as <code>vector_size=5</code> will often mislead about how word2vec behaves at usual scales. It's not even faintly-illustrative of the useful relationships that appear with more-dimensions &amp; plentiful data.</p>
</li>
</ul>
",0,0,89,2023-12-19 17:18:03,https://stackoverflow.com/questions/77686928/python-word2vec-updates
How do I use OML to create a custom conda that contains the gensim python package?,"<p>I'd like to use OML (Oracle Machine Learning) to create a custom python conda that contains the gensim python package. The OML docs suggest</p>
<p><code>%conda create -n gensim_env python==3.10 gensim </code></p>
<p>but that errors out with</p>
<pre><code>Collecting package metadata: ...working... done
Solving environment: ...working... failed

UnsatisfiableError: The following specifications were found to be in conflict:
  - gensim -&gt; libgcc-ng[version='&gt;=11.2.0'] -&gt; __glibc[version='&gt;=2.17']
  - gensim -&gt; libgcc-ng[version='&gt;=11.2.0'] -&gt; _openmp_mutex -&gt; openmp_impl==9999
  - gensim -&gt; libstdcxx-ng[version='&gt;=11.2.0']
  - gensim -&gt; numpy[version='&gt;=1.26.0,&lt;2.0a0'] -&gt; numpy-base==1.26.3=py312he1a6c75_0 -&gt; python[version='&gt;=3.12,&lt;3.13.0a0'] -&gt; libffi[version='&gt;=3.4,&lt;4.0a0']
  - gensim -&gt; numpy[version='&gt;=1.26.0,&lt;2.0a0'] -&gt; numpy-base==1.26.3=py312he1a6c75_0 -&gt; python[version='&gt;=3.12,&lt;3.13.0a0'] -&gt; libuuid[version='&gt;=1.41.5,&lt;2.0a0']
  - gensim -&gt; numpy[version='&gt;=1.26.0,&lt;2.0a0'] -&gt; numpy-base==1.26.3=py312he1a6c75_0 -&gt; python[version='&gt;=3.12,&lt;3.13.0a0'] -&gt; ncurses[version='&gt;=6.4,&lt;7.0a0']
  - gensim -&gt; numpy[version='&gt;=1.26.0,&lt;2.0a0'] -&gt; numpy-base==1.26.3=py312he1a6c75_0 -&gt; python[version='&gt;=3.12,&lt;3.13.0a0'] -&gt; openssl[version='&gt;=3.0.11,&lt;4.0a0']
  - gensim -&gt; numpy[version='&gt;=1.26.0,&lt;2.0a0'] -&gt; numpy-base==1.26.3=py312he1a6c75_0 -&gt; python[version='&gt;=3.12,&lt;3.13.0a0'] -&gt; sqlite[version='&gt;=3.41.2,&lt;4.0a0'] -&gt; zlib[version='&gt;=1.2.13,&lt;1.3.0a0,&gt;=1.2.13,&lt;2.0a0']
  - gensim -&gt; numpy[version='&gt;=1.26.0,&lt;2.0a0'] -&gt; numpy-base==1.26.3=py312he1a6c75_0 -&gt; python[version='&gt;=3.12,&lt;3.13.0a0'] -&gt; xz[version='&gt;=5.4.2,&lt;6.0a0']
  - python==3.10
Use &quot;conda search &lt;package&gt; --info&quot; to see the dependencies for each package.
</code></pre>
","conda, gensim","<p>Use the -c flag to install a Python library from a specific conda channel. By querying gensim in <a href=""https://anaconda.org/search"" rel=""nofollow noreferrer"">https://anaconda.org/search</a>, I found that it resides in the conda-forge channel. To create a conda environment with the name 'gensim_env' in OML Notebooks on Oracle Autonomous Database, run the following command in a %conda paragraph under the ADMIN user:</p>
<p>%conda</p>
<p>create -n gensim_env -c conda-forge --override-channels --strict-channel-priority python=3.10 gensim</p>
<p>Save the conda environment to Object Storage for OML users to download, activate, and use the conda environment:</p>
<p>%conda</p>
<p>upload gensim_env -t application &quot;OML4Py&quot;</p>
<p>Uploading conda environment gensim_env
Upload successful for conda environment gensim_env</p>
<p>Next, the OML user will download and activate the conda environment in a %conda paragraph:</p>
<p>%conda</p>
<p>download gensim_env
activate gensim_env</p>
<p>With the conda environment downloaded and activated, the gensim library, along with other libraries in this conda environment, can be used in a %python paragraph.</p>
<p>Note that these instructions are specific to OML on Autonomous Database. For on-premises installations, third-party packages can be installed using various packages installation techniques, including conda, pip, CRAN, etc.</p>
",0,0,128,2024-01-12 21:09:00,https://stackoverflow.com/questions/77809292/how-do-i-use-oml-to-create-a-custom-conda-that-contains-the-gensim-python-packag
Load word2vec model that is in .tar format,"<p>I want to load a previously trained word2vec model into gensim. The trouble is the file format. It is not a .bin file format but a .tar file. It is the model / file  <em>deu-ch_web-public_2019_1M.tar.gz</em> from the <a href=""https://wortschatz.uni-leipzig.de/en/download/German"" rel=""nofollow noreferrer"">University of Leipzig</a>. The model is also listed <a href=""https://huggingface.co/inovex/Word2Vec-Finetuning-Service-Base-Models"" rel=""nofollow noreferrer"">on HuggingFace</a> where different word2vec models for English and German are listed.</p>
<p><strong>First I tried:</strong></p>
<pre><code>from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format('deu-ch_web-public_2019_1M.tar.gz')
</code></pre>
<p>--&gt; ValueError: invalid literal for int() with base 10: 'deu-ch_web-public_2019_1M</p>
<p><strong>Then I unzipped the file with 7-Zip and tried the following:</strong></p>
<pre><code>from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format('deu-ch_web-public_2019_1M.tar')
</code></pre>
<p>--&gt; ValueError: invalid literal for int() with base 10: 'deu-ch_web-public_2019_1M</p>
<pre><code>from gensim.models import word2vec
model = word2vec.Word2Vec.load('deu-ch_web-public_2019_1M.tar')
</code></pre>
<p>--&gt; UnpicklingError: could not find MARK</p>
<p>Then I got a bit desperate...</p>
<pre><code>import gensim.downloader
model = gensim.downloader.load('deu-ch_web-public_2019_1M.tar')
</code></pre>
<p>--&gt; ValueError: Incorrect model/corpus name</p>
<p>Googling around I found useful information how to load a .bin model with gensim ( <a href=""https://stackoverflow.com/questions/39549248/how-to-load-a-pre-trained-word2vec-model-file-and-reuse-it/39662736#39662736"">see here</a> and <a href=""https://stackoverflow.com/questions/65394022/how-can-a-word2vec-pretrained-model-be-loaded-in-gensim-faster"">here</a> ). Following this <a href=""https://github.com/deeplearning4j/deeplearning4j/issues/4150"" rel=""nofollow noreferrer"">thread</a> it seems tricky to load a .tar file with gensim. Especially if one has not one .txt file but five .txt files as in this case. I found <a href=""https://stackoverflow.com/questions/71733191/how-to-read-and-load-tarfile-to-extract-feature-vector"">one answer how to read a .tar file but with tensorflow</a>. Since I am not familiar with tensorflow, I prefer to use gensim. Any thoughts how to solve the issue is appreciated.</p>
","python, gensim, word2vec, tar.gz","<p>A <code>.tar</code> file is a bundle of one or more directories and files – see <a href=""https://en.wikipedia.org/wiki/Tar_(computing)"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Tar_(computing)</a> – and thus not the sort of single-model file that you should expect Gensim to open directly.</p>
<p>Rather, similar to as with a <code>.zip</code> file, you'd use some purpose-specific software to extract any content inside the <code>.tar</code> into individual files – then point Gensim at those, individually, <em>if</em> they're formats Gensim understands.</p>
<p>A typical command-line operation to extract the individual file(s) from a <code>.tar.gz</code> file (which is both tarred &amp; gzipped) would be:</p>
<p>tar -xvzf deu-ch_web-public_2019_1M.tar.gz</p>
<p>That tells the command to e<code>x</code>tract with <code>v</code>erbose reporting while also un-g<code>z</code>ipping the <code>f</code>ile <code>deu-ch_web-public_2019_1M.tar.gz</code>. Then you'll have one or more new local files, which are the actual (not-packaged-up) files of interest.</p>
<p>In some graphical UI file-explorers, like the MacOS 'Finder', simply double-clicking to perform the default 'open' action on <code>deu-ch_web-public_2019_1M.tar.gz</code> will perform this expansion (no <code>tar</code> command-line needed).</p>
<p>But note: the University of Liepzig page you've linked describes these files as 'corpora' (training texts), <em>not</em> trained sets of word-vectors or word2vec models.</p>
<p>And I looked at the &quot;2019 - switzerland - public web file&quot; you're referring-to, and inside is a directory (folder) <code>deu-ch_web-public_2019_1M</code>, with 7 <code>.txt</code> files inside of various formats, and 1 <code>.sql</code>  file. But none of those are any sort of trained word-vectors - just text &amp; text-statistics.</p>
<p>You could use those to train a model yourself. The <code>deu-ch_web-public_2019_1M-sentences.txt</code> is closest to what you need, as 1 million plain-text sentences.</p>
<p>But it's still not yet in a form fully ready for word2vec training. Each line has a redundant line-number at the front, and the text hasn't yet been tokenized into word-tokens (which would potentially remove punctuation, or sometimes keep punctuation as distinct tokens). And, as a mere 15 million words total, it's still fairly small as a corpus for creating a powerful word2vec model.</p>
",0,0,217,2024-02-07 10:41:01,https://stackoverflow.com/questions/77954019/load-word2vec-model-that-is-in-tar-format
Gensim&#39;s Doc2Vec with documents in multiple languages,"<p>I'm building a content based recommender system using similarities on vector representations of documents.
My documents are descriptions of books. Most of them are in English, but some of them are in other languages. I used gensim's Doc2Vec for building vector representations of the documents.</p>
<p>Based on my understanding of this model, documents in different languages should have very low similarity, because the words are not even overlapping. But this is not what's happening. Documents in different languages in my dataset can have a high similarity as 0.9.</p>
<p>Why is this happening?</p>
<pre><code>import pandas as pd
from pathlib import Path
from gensim.models.doc2vec import TaggedDocument, Doc2Vec
from gensim.utils import simple_preprocess
import numpy as np
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
data_folder = Path.cwd().parent / 'data'
transformed_folder = data_folder / 'transformed'
BOOKS_PATH = Path(transformed_folder, &quot;books_final.csv&quot;)
import seaborn as sns

book_df = pd.read_csv(BOOKS_PATH)
languages=book_df.language.unique()

training_corpus = np.empty(len(book_df), dtype=object)
for i, (isbn, desc) in tqdm(enumerate(zip(book_df['isbn'], book_df['description']))):
    training_corpus[i] = TaggedDocument(simple_preprocess(desc), [str(i)])
model=Doc2Vec(vector_size=50, min_count=2, epochs=40)
model.build_vocab(training_corpus)
#train the model
model.train(training_corpus, total_examples=model.corpus_count, epochs=model.epochs)
#get the keys of the trained model
model_keys=list(model.dv.key_to_index.keys())
matrix = model.dv.vectors
similarity_matrix = cosine_similarity(matrix)

indices_for_language = {}
for language, group_df in book_df.groupby('language'):
    indices_for_language[language] = group_df.index.to_list()

sim_japanese_italian = similarity_matrix[indices_for_language['Japanese']][:, indices_for_language['Italian']]
#make a heatmap
sns.heatmap(sim_japanese_italian)

</code></pre>
<p><a href=""https://i.sstatic.net/9sjkH.png"" rel=""nofollow noreferrer"">Heatmap of similarities between Italian and Japanese documents</a></p>
","python, nlp, gensim, recommendation-engine, doc2vec","<p>You corpus is a bit small compared to published results for the <code>Doc2Vec</code> (&quot;Paragraph Vectors&quot;) algorithm, which is usually demonstrated on set of tens-of-thousands to millions of documents.</p>
<p>In particular, if you only have (4742-4655=) 93 non-English <code>description</code> texts, and those are split over 16 non-English languages, it seems unlikely to me that <em>any</em> of the words from those other languages will have enough in-context usage examples to either (1) survive the <code>min_count</code> cutoff (which generally shouldn't be as low as 2!); or (2) obtain meaningful/generalizable implications in the model.</p>
<p>That is: all those very-rare words will have fairly idiosyncratic/arbitrary meanings, and <code>description</code> texts filled with all rare/weakly-understood words will tend to have weak, almost random doc-vectors. So, spurious similarities aren't that much of a surprise.</p>
<p>If those &quot;non-English&quot; <code>description</code> texts do have a few English-looking or omnilingual tokens in them – stray words or false cognates or punctuation or glitches – those might also dominate the <code>description</code>-to-<code>description</code> comparison.</p>
<p>You should check, on your suspiciously-too-similar pairs, how many of the words in each document are actually in the trained model – keeping in mind words not in the trained model are essentially ignored in training or post-rtraining inference. (A document of 100 words, in a language where there are only a few other 100-word documents, might in fact be dominated by single-appearance words, that don't even meet the <code>min_count=2</code>, and so when slimmed to the surviving words be short docs filled with weakly-understood words.)</p>
<p>If your similarity scores between English documents are generally sensible, then the model is doing what it can, where it has enough data. You really want this family of algorithms – word2vec, <code>Doc2Vec</code>, FastText, etc – to have many, varied, representative examples of any word's usage in sensible contrasting contexts for it to be able to do any meaningful later modeling of those words (and the documents that contain them).</p>
<p>So: if you can't manage to use a <code>min_count</code> of at least the usual default (<code>5</code>), or ideally even higher, you likely don't have sufficient data for these algorithms to show their strengths. Extending your corpus – even with other texts not of direct interest, but of sufficient variety to better represent the languages used in your texts of interest – might help. (That is: even if your universe-of-recommendations is &lt;5000 <code>descriptions</code>, mixing another 200,000 similar short blurbs from other sources, so that words and langguages of interest have enough coverage, could be worthwhile.)</p>
",0,0,136,2024-04-02 15:47:30,https://stackoverflow.com/questions/78262529/gensims-doc2vec-with-documents-in-multiple-languages
&quot;ImportError: cannot import name &#39;triu&#39; from &#39;scipy.linalg&#39;&quot; when importing Gensim,"<p>I am trying to use Gensim, but running <code>import gensim</code> raises this error:</p>
<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.10/dist-packages/gensim/__init__.py&quot;, line 11, in &lt;module&gt;
    from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:F401
  File &quot;/usr/local/lib/python3.10/dist-packages/gensim/corpora/__init__.py&quot;, line 6, in &lt;module&gt;
    from .indexedcorpus import IndexedCorpus  # noqa:F401 must appear before the other classes
  File &quot;/usr/local/lib/python3.10/dist-packages/gensim/corpora/indexedcorpus.py&quot;, line 14, in &lt;module&gt;
    from gensim import interfaces, utils
  File &quot;/usr/local/lib/python3.10/dist-packages/gensim/interfaces.py&quot;, line 19, in &lt;module&gt;
    from gensim import utils, matutils
  File &quot;/usr/local/lib/python3.10/dist-packages/gensim/matutils.py&quot;, line 20, in &lt;module&gt;
    from scipy.linalg import get_blas_funcs, triu
ImportError: cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.10/dist-packages/scipy/linalg/__init__.py)
</code></pre>
<p>Why is this happening and how can I fix it?</p>
","python, scipy, gensim","<p>I found the issue.</p>
<blockquote>
<p>The <a href=""https://docs.scipy.org/doc/scipy/reference/linalg.html#module-scipy.linalg"" rel=""noreferrer""><code>scipy.linalg</code></a> functions <code>tri</code>, <code>triu</code> &amp; <code>tril</code> are deprecated and will be removed in SciPy 1.13.</p>
<p>— <a href=""https://docs.scipy.org/doc/scipy/release/1.11.0-notes.html#deprecated-features"" rel=""noreferrer"">SciPy 1.11.0 Release Notes § Deprecated features</a></p>
</blockquote>
<p>So, I installed SciPy v1.10.1 instead of the latest version and it was working well.</p>
<pre class=""lang-bash prettyprint-override""><code>pip install scipy==1.10.1
</code></pre>
",66,46,63269,2024-04-05 10:01:46,https://stackoverflow.com/questions/78279136/importerror-cannot-import-name-triu-from-scipy-linalg-when-importing-gens
cannot load fine tuned fasttext wiki model after retraining and saving,"<p>I am fine tuning a fastest wiki model as follows. This works fine. After fine tuning I save the retrained model.</p>
<pre><code>from gensim.models import fasttext
model = fasttext.load_facebook_model(datapath(&quot;wiki/wiki.en.bin&quot;))
model.build_vocab(sentences, update=True)
model.train(sentences, total_examples=len(sentences), epochs=5)
model.save(&quot;wiki/wiki.en.updated.bin&quot;)
</code></pre>
<p>Later on when I try and load the model</p>
<pre><code>model = fasttext.load_facebook_model(datapath(&quot;wiki/wiki.en.updated.bin&quot;)
</code></pre>
<p>I get an error</p>
<p>NotImplementedError: Supervised fastText models are not supported</p>
<p>Which is strange since I am not doing supervised training!?!? Any idea why this is happening and how to correctly load the fine tuned model?</p>
<p>Is impossible I am saving or loading the file wrong. I did notice that more than one file is created upon save, but that's just normal due to the underlying representation, or no?</p>
<p>I am running macOS 14.3 on an M2 Mac.</p>
<pre><code>-rw-r--r--@ 1 sail0r  staff  8493673445 Oct 19  2017 wiki.en.bin
-rw-r--r--  1 sail0r  staff    84876251 Apr 14 02:12 wiki.en.updated.bin
-rw-r--r--  1 sail0r  staff  3023571728 Apr 14 02:12 wiki.en.updated.bin.syn1neg.npy
-rw-r--r--  1 sail0r  staff  2400000128 Apr 14 02:11 wiki.en.updated.bin.wv.vectors_ngrams.npy
-rw-r--r--  1 sail0r  staff  3023571728 Apr 14 02:11 wiki.en.updated.bin.wv.vectors_vocab.npy
-rw-rw-r--@ 1 sail0r  staff  6597238061 Sep 19  2016 wiki.en.vec
</code></pre>
<p>This is gensim 4.3.2.</p>
","gensim, fasttext","<p>This change to the code above fixes the problem.</p>
<pre><code>from gensim.models import FastText, fasttext
model = FastText.load(datapath(&quot;wiki/wiki.en.bin&quot;))
</code></pre>
<p>It seems that the retrained model is not saved in fasttext format and so just requires the <code>load()</code> method, otherwise apparently it gets confused.</p>
",1,0,72,2024-04-14 17:06:54,https://stackoverflow.com/questions/78324616/cannot-load-fine-tuned-fasttext-wiki-model-after-retraining-and-saving
SpaCy and Gensim on Jupyter Notebooks,"<p>I have bought a new macbook and have installed python 3.12.5 on it. I am trying to import and run libraries for Natural Langauge Processing in a Jupyter Notebook, and all is well... except when it comes to spaCy and gensim.</p>
<pre><code>import spacy
import gensim
</code></pre>
<pre><code>TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'
</code></pre>
<p>I checked to see if they are imported with :</p>
<pre><code>!pip show spacy
!pip show gensim

Name: spacy
Version: 3.7.5
Summary: Industrial-strength Natural Language Processing (NLP) in Python
Home-page: https://spacy.io
Author: Explosion
Author-email: contact@explosion.ai
License: MIT
Location: /opt/anaconda3/lib/python3.12/site-packages
Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel
Required-by: 
Name: gensim
Version: 4.3.2
Summary: Python framework for fast Vector Space Modelling
Home-page: https://radimrehurek.com/gensim/
Author: Radim Rehurek
Author-email: me@radimrehurek.com
License: LGPL-2.1-only
Location: /opt/anaconda3/lib/python3.12/site-packages
Requires: numpy, scipy, smart-open
Required-by: 
</code></pre>
<p>So they should be loaded, but I don't know what the TypeError refers to and how to fix it!</p>
<p>I read that there is something with the python update...but could anyone help me clarify this?</p>
","python, jupyter-notebook, spacy, huggingface-transformers, gensim","<p>Problem occurs in pydantic library (to be exact <code>pydantic.v1.typing</code> module) the best <a href=""https://github.com/pydantic/pydantic/issues/9609#issuecomment-2155832461"" rel=""nofollow noreferrer"">solution</a> at the moment is to downgrade python version to 3.12.3. <a href=""https://github.com/pydantic/pydantic/pull/9612"" rel=""nofollow noreferrer"">Patches</a> to this problem has been already merged to repository but you need to wait for next release.</p>
",-1,0,262,2024-08-13 09:31:18,https://stackoverflow.com/questions/78865458/spacy-and-gensim-on-jupyter-notebooks
Python error trying to install gensim on MacOS,"<p>trying to install gensim on MacBook Pro (i7, Sonoma 14.7.1) using PyCharm (Python@3.13). I've tried several suggestions from stack, github and other sources but none worked.</p>
<p>Based on what little I found online, it seems that it cannot find <strong>openblas</strong>, even though it is installed. But it is installed and <strong>openBLAS</strong>. Is this just a strict case sensitivity or something else?</p>
<p>How do I resolve this?
Also, when I try to open the full log, path to file does not exist.</p>
<p>Here is the output in the PyCharm Console:</p>
<pre><code>pip install --upgrade gensim        
Collecting gensim
  Using cached gensim-4.3.3.tar.gz (23.3 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting numpy&lt;2.0,&gt;=1.18.5 (from gensim)
  Using cached numpy-1.26.4.tar.gz (15.8 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting scipy&lt;1.14.0,&gt;=1.7.0 (from gensim)
  Using cached scipy-1.13.1.tar.gz (57.2 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─&gt; [45 lines of output]
      + meson setup /private/var/folders/92/9fd9rgg976s7zn44fvbxjw000000gn/T/pip-install-ox0sfkbg/scipy_7a9a4932c4254c9d93723beb50b0e629 /private/var/folders/92/9fd9rgg976s7zn44fvbxjw000000gn/T/pip-install-ox0sfkbg/scipy_7a9a4932c4254c9d93723beb50b0e629/.mesonpy-ry56tq_q -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=/private/var/folders/92/9fd9rgg976s7zn44fvbxjw000000gn/T/pip-install-ox0sfkbg/scipy_7a9a4932c4254c9d93723beb50b0e629/.mesonpy-ry56tq_q/meson-python-native-file.ini
      The Meson build system
      Version: 1.6.0
      Source dir: /private/var/folders/92/9fd9rgg976s7zn44fvbxjw000000gn/T/pip-install-ox0sfkbg/scipy_7a9a4932c4254c9d93723beb50b0e629
      Build dir: /private/var/folders/92/9fd9rgg976s7zn44fvbxjw000000gn/T/pip-install-ox0sfkbg/scipy_7a9a4932c4254c9d93723beb50b0e629/.mesonpy-ry56tq_q
      Build type: native build
      Project name: scipy
      Project version: 1.13.1
      C compiler for the host machine: cc (clang 16.0.0 &quot;Apple clang version 16.0.0 (clang-1600.0.26.4)&quot;)
      C linker for the host machine: cc ld64 1115.7.3
      C++ compiler for the host machine: c++ (clang 16.0.0 &quot;Apple clang version 16.0.0 (clang-1600.0.26.4)&quot;)
      C++ linker for the host machine: c++ ld64 1115.7.3
      Cython compiler for the host machine: cython (cython 3.0.11)
      Host machine cpu family: x86_64
      Host machine cpu: x86_64
      Program python found: YES (/Users/xxxxxxxxxxxx/PycharmProjects/pythonProjectTrainingList/.venv/bin/python)
      Found pkg-config: YES (/usr/local/bin/pkg-config) 2.3.0
      Run-time dependency python found: YES 3.13
      Program cython found: YES (/private/var/folders/92/9fd9rgg976s7zn44fvbxjw000000gn/T/pip-build-env-9qebkipc/overlay/bin/cython)
      Compiler for C supports arguments -Wno-unused-but-set-variable: YES
      Compiler for C supports arguments -Wno-unused-function: YES
      Compiler for C supports arguments -Wno-conversion: YES
      Compiler for C supports arguments -Wno-misleading-indentation: YES
      Library m found: YES
      Fortran compiler for the host machine: gfortran (gcc 14.2.0 &quot;GNU Fortran (Homebrew GCC 14.2.0_1) 14.2.0&quot;)
      Fortran linker for the host machine: gfortran ld64 1115.7.3
      Compiler for Fortran supports arguments -Wno-conversion: YES
      Compiler for C supports link arguments -Wl,-ld_classic: YES
      Checking if &quot;-Wl,--version-script&quot; : links: NO
      Program pythran found: YES 0.15.0 0.15.0 (/private/var/folders/92/9fd9rgg976s7zn44fvbxjw000000gn/T/pip-build-env-9qebkipc/overlay/bin/pythran)
      Did not find CMake 'cmake'
      Found CMake: NO
      Run-time dependency xsimd found: NO (tried pkgconfig, framework and cmake)
      Run-time dependency threads found: YES
      Library npymath found: YES
      Library npyrandom found: YES
      pybind11-config found: YES (/private/var/folders/92/9fd9rgg976s7zn44fvbxjw000000gn/T/pip-build-env-9qebkipc/overlay/bin/pybind11-config) 2.12.1
      Run-time dependency pybind11 found: YES 2.12.1
      Run-time dependency scipy-openblas found: NO (tried pkgconfig)
      Run-time dependency openblas found: NO (tried pkgconfig, framework and cmake)
      Run-time dependency openblas found: NO (tried pkgconfig and framework)
      
      ../scipy/meson.build:163:9: ERROR: Dependency &quot;OpenBLAS&quot; not found, tried pkgconfig and framework
      
      A full log can be found at /private/var/folders/92/9fd9rgg976s7zn44fvbxjw000000gn/T/pip-install-ox0sfkbg/scipy_7a9a4932c4254c9d93723beb50b0e629/.mesonpy-ry56tq_q/meson-logs/meson-log.txt
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─&gt; See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
</code></pre>
","python, macos, scipy, gensim, openblas","<p><a href=""https://pypi.org/project/gensim/#files"" rel=""nofollow noreferrer"">Gensim has precompiled wheels up to Python 3.12</a>.</p>
<p>Your easiest resolution is to downgrade Python from 3.13 to 3.12, so you can use one of those precompiled wheels (<code>gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl</code>; that's a detail you don't need to care about; <code>pip install</code> will choose that wheel when compatible), so you don't need to have an environment in which you can compile Gensim from source.</p>
<p>You might also want to raise an issue on Gensim's GitHub so there'd be wheels for 3.13 in the future.</p>
",3,0,1181,2024-12-01 12:01:13,https://stackoverflow.com/questions/79241502/python-error-trying-to-install-gensim-on-macos
Correct topics from LDA Sequence Model in Gensim,"<p>Python's Gensim package offers a dynamic topic model called <code>LdaSeqModel()</code>. I have run into the same problem as in this <a href=""https://groups.google.com/g/gensim/c/CYmJ05Dp1bk/m/FSqjO7WMAQAJ"" rel=""nofollow noreferrer"">issue</a> from the Gensim mailing list (which has not been solved). The problem is that the model infers a topic that is logically impossible in the sense that it assigns a non-zero probability to a word in a time slice where the word was not used. This is a reproduction of the problem:</p>
<pre><code>from gensim.corpora import Dictionary
from gensim.models import LdaSeqModel

common_texts = [
    ['human', 'interface', 'computer'],
    ['survey', 'user', 'computer', 'system', 'response', 'time'],
    ['eps', 'user', 'interface', 'system'],
    ['system', 'human', 'system', 'eps'],
    ['user', 'response', 'time'],
    ['trees'],
    ['graph', 'trees'],
    ['graph', 'minors', 'trees'],
    ['graph', 'minors', 'survey']
]
common_dictionary = Dictionary(common_texts)
common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]

model = LdaSeqModel(corpus=common_corpus, id2word=common_dictionary, num_topics=1, time_slice=[5, 4])

model.print_topic_times(topic=0)
</code></pre>
<p><code>time_slice=[5, 4]</code> means that the first time slice contains the documents in the first 5 items of the <code>common_texts</code> list. The term <code>graph</code> is not in the first time slice, but <code>print_topic_times()</code> says it is. The output is:</p>
<pre><code>[[('system', 0.13896054593348167),
  ('user', 0.10696589214152682),
  ('trees', 0.10664464447111177),
  ('graph', 0.10643809153102356),
  ('computer', 0.07494460648968987),
  ('human', 0.07494460648968987),
  ('interface', 0.07494460648968987),
  ('response', 0.07494460648968987),
  ('time', 0.07494460648968987),
  ('eps', 0.07494460648968987),
  ('minors', 0.07474199433434457),
  ('survey', 0.01658119265037265)],
 [('system', 0.13882862152464212),
  ('graph', 0.10742799576320598),
  ('trees', 0.10713473662111127),
  ('user', 0.1064043188010877),
  ('minors', 0.07517325760789559),
  ('computer', 0.07474729274679391),
  ('human', 0.07474729274679391),
  ('interface', 0.07474729274679391),
  ('response', 0.07474729274679391),
  ('time', 0.07474729274679391),
  ('eps', 0.07474729274679391),
  ('survey', 0.01654731320129382)]]
</code></pre>
<p>Do I have to set additional parameters to obtain correct results?</p>
<p>I have run this with Python 3.10.12 and Gensim 4.3.3.</p>
<h4>Update January 23, 2025</h4>
<p>I've experimented with the <code>alphas</code>, <code>passes</code>, and <code>em_min_iter</code> parameters, none of which have an effect on the problem.</p>
","python, dynamic, gensim, topic-modeling","<p>Since the dynamic topic model (DTM) is a probabilistic model, word probabilities are never zero even for words that do not occurr in a time slice. But the DTM has a temporal smoothing parameter that influences the temporal continuity of topics. In the <code>LdaSeqModel()</code>, it's the <code>chain_variance</code> parameter. By increasing it, words that do not occurr in a time slice get lower probabilities, also in the toy model given above.</p>
",0,0,79,2025-01-21 16:24:24,https://stackoverflow.com/questions/79375157/correct-topics-from-lda-sequence-model-in-gensim
Trouble getting importing gensim to work in colab,"<p>I am trying to import gensim into colab.</p>
<pre><code>!pip install gensim
</code></pre>
<p>I get the following error:</p>
<pre><code>/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)
    365                 raise AssertionError()
    366         except AssertionError:
--&gt; 367             msg = (&quot;The current Numpy installation ({!r}) fails to &quot;
    368                    &quot;pass simple sanity checks. This can be caused for example &quot;
    369                    &quot;by incorrect BLAS library being linked in, or by mixing &quot;

ModuleNotFoundError: No module named 'numpy.char'
</code></pre>
<p>my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks</p>
","numpy, nlp, dependencies, google-colaboratory, gensim","<p>You have to restart the session for the underlying runtime to notice the package changes. See: <a href=""https://stackoverflow.com/a/79518359/130288"">https://stackoverflow.com/a/79518359/130288</a></p>
<p>I recall in the past Colab offering a warning when you had to do this. And possibly also, in the past, Colab hadn't yet loaded <code>numpy</code>/etc in a fresh environment – and so it was OK for them to downgrade behind the scenes without a problem - the 1st import was only after the downgrade.</p>
<p>But something changed in Colab recently – maybe some fast-start optimization? – with a bunch of reports of problems like this in just the last day or two.</p>
<p>Explicitly restarting after the Gensim-install &amp; <code>numpy</code>/<code>scipy</code> downgrades resolves the errors.</p>
",1,0,133,2025-03-20 14:36:02,https://stackoverflow.com/questions/79523269/trouble-getting-importing-gensim-to-work-in-colab
error when trying to install gensim via pip on windows,"<p>I am trying to install the python package <code>gensim</code>. I installed <code>scipy</code>, <code>smart_open</code> and <code>numpy</code>, and ran <code>pip install gensim</code>.
the installation failed in the <code>preparing metadata</code> stage and a behemoth 954 lines of output were printed.
I'm not sure where is the actual error in it. cntl-f-ing for the word &quot;error&quot; brings
<code>../numpy/core/src/npysort/binsearch.cpp(338): error C2131: expression did not evaluate to a constant</code>
and right at the end there's this block of error-message-y looking text</p>
<pre><code>      &quot;cl&quot; &quot;-Inumpy\core\_multiarray_umath.cp313-win_amd64.pyd.p&quot; &quot;-Inumpy\core&quot; &quot;-I..\numpy\core&quot; &quot;-Inumpy\core\include&quot; &quot;-I..\numpy\core\include&quot; &quot;-I..\numpy\core\src\common&quot; &quot;-I..\numpy\core\src\multiarray&quot; &quot;-I..\numpy\core\src\npymath&quot; &quot;-I..\numpy\core\src\umath&quot; &quot;-IC:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.752.0_x64__qbz5n2kfra8p0\Include&quot; &quot;-IC:\Users\aviv\AppData\Local\Temp\pip-install-schzgq22\numpy_6bd4b901c9874883b7cd71c8defa1392\.mesonpy-helvat64\meson_cpu&quot; &quot;-DNDEBUG&quot; &quot;/MD&quot; &quot;/nologo&quot; &quot;/showIncludes&quot; &quot;/utf-8&quot; &quot;/Zc:__cplusplus&quot; &quot;/W2&quot; &quot;/EHsc&quot; &quot;/std:c++17&quot; &quot;/permissive-&quot; &quot;/O2&quot; &quot;/Gw&quot; &quot;-DNPY_HAVE_SSE2&quot; &quot;-DNPY_HAVE_SSE&quot; &quot;-DNPY_HAVE_SSE3&quot; &quot;-DNPY_INTERNAL_BUILD&quot; &quot;-DHAVE_NPY_CONFIG_H&quot; &quot;-D_FILE_OFFSET_BITS=64&quot; &quot;-D_LARGEFILE_SOURCE=1&quot; &quot;-D_LARGEFILE64_SOURCE=1&quot; &quot;-D__STDC_VERSION__=0&quot; &quot;/Fdnumpy\core\_multiarray_umath.cp313-win_amd64.pyd.p\src_npysort_binsearch.cpp.pdb&quot; /Fonumpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_npysort_binsearch.cpp.obj &quot;/c&quot; ../numpy/core/src/npysort/binsearch.cpp
      ../numpy/core/src/npysort/binsearch.cpp(338): error C2131: expression did not evaluate to a constant
      ../numpy/core/src/npysort/binsearch.cpp(309): note: failure was caused by a read of an uninitialized symbol
      ../numpy/core/src/npysort/binsearch.cpp(309): note: see usage of 'binsearch_base&lt;noarg&gt;::value_type::typenum'
      ../numpy/core/src/npysort/binsearch.cpp(395): note: see reference to class template instantiation 'binsearch_t&lt;noarg&gt;' being compiled
      [369/503] Compiling C object numpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_multiarray_textreading_growth.c.obj
      [370/503] Compiling C++ object numpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_npysort_timsort.cpp.obj
      [371/503] Compiling C object numpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_npymath_arm64_exports.c.obj
      [372/503] Compiling C object numpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_multiarray_textreading_stream_pyobject.c.obj
      [373/503] Compiling C object numpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_multiarray_textreading_str_to_int.c.obj
      [374/503] Compiling C object numpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_multiarray_textreading_rows.c.obj
      [375/503] Compiling C++ object numpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_npysort_selection.cpp.obj
      FAILED: numpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_npysort_selection.cpp.obj
      &quot;cl&quot; &quot;-Inumpy\core\_multiarray_umath.cp313-win_amd64.pyd.p&quot; &quot;-Inumpy\core&quot; &quot;-I..\numpy\core&quot; &quot;-Inumpy\core\include&quot; &quot;-I..\numpy\core\include&quot; &quot;-I..\numpy\core\src\common&quot; &quot;-I..\numpy\core\src\multiarray&quot; &quot;-I..\numpy\core\src\npymath&quot; &quot;-I..\numpy\core\src\umath&quot; &quot;-IC:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.752.0_x64__qbz5n2kfra8p0\Include&quot; &quot;-IC:\Users\aviv\AppData\Local\Temp\pip-install-schzgq22\numpy_6bd4b901c9874883b7cd71c8defa1392\.mesonpy-helvat64\meson_cpu&quot; &quot;-DNDEBUG&quot; &quot;/MD&quot; &quot;/nologo&quot; &quot;/showIncludes&quot; &quot;/utf-8&quot; &quot;/Zc:__cplusplus&quot; &quot;/W2&quot; &quot;/EHsc&quot; &quot;/std:c++17&quot; &quot;/permissive-&quot; &quot;/O2&quot; &quot;/Gw&quot; &quot;-DNPY_HAVE_SSE2&quot; &quot;-DNPY_HAVE_SSE&quot; &quot;-DNPY_HAVE_SSE3&quot; &quot;-DNPY_INTERNAL_BUILD&quot; &quot;-DHAVE_NPY_CONFIG_H&quot; &quot;-D_FILE_OFFSET_BITS=64&quot; &quot;-D_LARGEFILE_SOURCE=1&quot; &quot;-D_LARGEFILE64_SOURCE=1&quot; &quot;-D__STDC_VERSION__=0&quot; &quot;/Fdnumpy\core\_multiarray_umath.cp313-win_amd64.pyd.p\src_npysort_selection.cpp.pdb&quot; /Fonumpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_npysort_selection.cpp.obj &quot;/c&quot; ../numpy/core/src/npysort/selection.cpp
      C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\include\ccomplex(22): warning C4996: '_Header_ccomplex': warning STL4004: &lt;ccomplex&gt;, &lt;cstdalign&gt;, &lt;cstdbool&gt;, and &lt;ctgmath&gt; are deprecated in C++17. You can define _SILENCE_CXX17_C_HEADER_DEPRECATION_WARNING or _SILENCE_ALL_CXX17_DEPRECATION_WARNINGS to acknowledge that you have received this warning.
      ../numpy/core/src/npysort/selection.cpp(441): error C2131: expression did not evaluate to a constant
      ../numpy/core/src/npysort/selection.cpp(425): note: failure was caused by a read of an uninitialized symbol
      ../numpy/core/src/npysort/selection.cpp(425): note: see usage of 'arg_map::typenum'
      [376/503] Compiling C++ object numpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_multiarray_textreading_tokenize.cpp.obj
      [377/503] Compiling C++ object numpy/core/_multiarray_umath.cp313-win_amd64.pyd.p/src_npysort_quicksort.cpp.obj
      ninja: build stopped: subcommand failed.
      Activating VS 16.7.4
      INFO: automatically activated MSVC compiler environment
      INFO: autodetecting backend as ninja
      INFO: calculating backend command to run: C:\Users\aviv\AppData\Local\Temp\pip-build-env-o2e2ghvb\normal\Scripts\ninja.EXE
</code></pre>
<p>but this is complete gibberish to me.</p>
<p>I tried googling some of the lines but nothing came up. adding flags such as <code>-U</code> or <code>--upgrade</code> that i've seen used online didn't seem to help either.
I also encounted <a href=""https://stackoverflow.com/questions/76699105/900-line-error-message-when-pip-installing-numpy"">this</a> answer about a similarly long pip error. but it's for a different package and on Mac so I doubt it's relevant.
frankly, I'm out of Ideas since I can't figure what the error even is.</p>
","windows, pip, gensim, meson-build","<p>From your error messages, you seem to be using Python 3.13. But the stable release of gensim (i.e. 4.3.3) currently has Python 3.8 - 3.12 <a href=""https://pypi.org/project/gensim/#files"" rel=""nofollow noreferrer"">wheels</a> for all major platforms. There's no support yet for Python 3.13. You can downgrade to Python 3.12 to install this package.</p>
",3,1,63,2025-03-21 16:26:56,https://stackoverflow.com/questions/79526058/error-when-trying-to-install-gensim-via-pip-on-windows
